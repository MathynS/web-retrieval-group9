Efficie nt M om e nts-base d Per m utation Tests
Chunxiao Zhou
Dept. of Electrical and Computer Eng.
University of Illinois at Urbana-Champaign
Champaign, IL 61820
czhou4@gmail.com

Huixia Judy Wang
Dept. of Statistics
North Carolina State University
Raleigh, NC 27695
wang@stat.ncsu.edu

Yongmei Michelle Wang
Depts. of Statistics, Psychology, and Bioengineering
University of Illinois at Urbana-Champaign
Champaign, IL 61820
ymw@illinois.edu

Abstract
In this paper, we develop an efficient moments-based permutation test
approach to improve the test?s computational efficiency by approximating
the permutation distribution of the test statistic with Pearson distribution
series. This approach involves the calculation of the first four moments of
the permutation distribution. We propose a novel recursive method to derive
these moments theoretically and analytically without any permutation.
Experimental results using different test statistics are demonstrated using
simulated data and real data. The proposed strategy takes advantage of
nonparametric permutation tests and parametric Pearson distribution
approximation to achieve both accuracy and efficiency.

1

In t ro d u c t i o n

Permutation tests are flexible nonparametric alternatives to parametric tests in small
samples, or when the distribution of a test statistic is unknown or mathematically intractable.
In permutation tests, except exchangeability, no other statistical assumptions are required.
The p-values can be obtained by using the permutation distribution. Permutation tests are
appealing in many biomedical studies, which often have limited observations with unknown
distribution. They have been used successfully in structural MR image analysis [1, 2, 3], in
functional MR image analysis [4], and in 3D face analysis [5].
There are three common approaches to construct the permutation distribution [6, 7, 8]: (1)
exact permutation enumerating all possible arrangements; (2) approximate permutation
based on random sampling from all possible permutations; (3) approximate permutation
using the analytical moments of the exact permutation distribution under the null hypothesis.
The main disadvantage of the exact permutation is the computational cost, due to the
factorial increase in the number of permutations with the increasing number of subjects. The
second technique often gives inflated type I errors caused by random sampling. When a large
number of repeated tests are needed, the random permutation strategy is also
computationally expensive to achieve satisfactory accuracy. Regarding the third approach,
the exact permutation distribution may not have moments or moments with tractability. In
most applications, it is not the existence but the derivation of moments that limits the third
approach.

To the best of our knowledge, there is no systematic and efficient way to derive the moments
of the permutation distribution. Recently, Zhou [3] proposed a solution by converting the
permutation of data to that of the statistic coefficients that are symmetric to the permutation.
Since the test statistic coefficients usually have simple presentations, it is easier to track the
permutation of the test statistic coefficients than that of data. However, this method requires
the derivation of the permutation for each specific test statistic, which is not accessible to
practical users.
In this paper, we propose a novel strategy by employing a general theoretical method to
derive the moments of the permutation distribution of any weighted v-statistics, for both
univariate and multivariate data. We note that any moments of the permutation distribution
for weighted v-statistics [9] can be considered as a summation of the product of data
function term and index function term over a high dimensional index set and all possible
permutations. Our key idea is to divide the whole index set into several permutation
equivalent (see Definition 2) index subsets such that the summation of the data/index
function term over all permutations is invariant within each subset and can be calculated
without conducting any permutation. Then we can obtain the moments by summing up
several subtotals. The proposed method can be extended to equivalent weighted v-statistics
by replacing them with monotonic weighted v-statistics. This is due to the fact that only the
order of test statistics of all permutations matters for obtaining the p-values, so that the
monotonic weighted v-statistics shares the same p-value with the original test statistic. Given
the first four moments, the permutation distribution can be well fitted by Pearson
distribution series. The p-values are then obtained without conducting any real permutation.
For multiple comparison of two-group difference, given the sample size n1 = 21 and n2 = 21,
the number of tests m = 2,000, we need to conduct m?(n1 + n2 )!/n1 !/n2 ! ? 1.1?1015
permutations for the exact permutation test. Even for 20,000 random permutations per test,
we still need m?20,000 ? 4?107 permutations. Alternatively, our moments-based permutation
method using Pearson distribution approximation only involves the calculation of the first
four analytically-derived moments of exact permutation distributions to achieve high
accuracy (see section 3). Instead of calculating test statistics in factorial scale with exact
permutation, our moments-based permutation only requires computation of polynomial
order. For example, the computational cost for univariate mean difference test statistic and
modified multivariate Hotelling's T2 test statistics [8] are O(n) and O(n3 ), respectively, where
n = n 1 + n2 .

2

M e t h o d o lo g y

In this section, we shall mainly discuss how to calculate the moments of the permutation
distribution for weighted v-statistics. For other test statistics, a possible solution is to
replace them with their equivalent weighted v-statistics by monotonic transforms. The
detailed discussion about equivalent test statistics can be found in [7, 8, 10].
2.1

C o m p ut a t i o n a l c h a l l e n g e

Let us first look at a toy example. Suppose we have a two-group univariate data
x = ( x1 , L , xn1 , xn1 +1 ,L , xn1 + n2 ) , where the first n1 elements are in group A and the rest, n2 ,are
in group B. For comparison of the two groups, the hypothesis is typically constructed as:
H 0 : m A = mB vs. H a : m A ? m B , where m A , m B are the population means of the groups A
n1

n

i =1

i = n1 +1

and B, respectively. Define x A = ? xi / n1 and xB = ? xi / n2 as the sample means of two
groups, where n=n1+n2. We choose the univariate group mean difference
statistic,

i.e.,

n

T ( x) = x A - xB = ? w(i ) xi ,
i =1

where

the

index

function

as the test
w(i ) = 1/ n1 ,

if i ? {1, L , n1} and w(i ) = -1/ n2 , if i ? {n1 + 1, L, n} . Then the total number of all possible
permutations of {1, L, n} is n!. To calculate the fourth moment of the permutation
distribution,

n
n n n n
1
1
4
? ( ? w(i ) xp (i ) ) =
? ? ? ? ? w(i1 ) w(i2 ) w(i3 ) w(i4 )xp ( i1 ) xp ( i2 ) xp ( i3 ) xp ( i4 ) ,
n ! p ?S n i =1
n ! p ?S n i1 =1 i2 =1 i3 =1 i4 =1
where ? is the permutation operator and the symmetric group Sn [11] includes all distinct
permutations. The above example shows that the moment calculation can be considered as a
summation over all possible permutations and a large index set. It is noticeable that the
computational challenge here is to go through the factorial level permutations and
polynomial level indices.

Ep (T 4 ( x ))=

2.2

P a r t i t i o n t h e i n de x s e t

In this paper, we assume that the test statistic T can be expressed as a weighted v-statistic of
n

n

i1 =1

id =1

degree d [9], that is, T ( x) = ? L ? w(i1 , L , id ) h( xi1 ,L , xid ) , where x = ( x1 , x 2 , L , x n ) T is a data
with n observations, and w is a symmetric index function. h is a symmetric data function,
i.e., invariant under permutation of (i1 ,L , id ) . Though the symmetry property is not required
for our method, it helps reduce the computational cost. Here, each observation xk can be
either univariate or multivariate. In the above toy example, d=1 and h is the identity
function. Therefore, the r-th moment of the test statistic from the permutated data is:
Ep (T r ( x)) = Ep ( ? w(i1 ,L , id )h( xp (i1 ) ,L , xp ( id ) ))r
i1 ,i2 ,L,id

= Ep [

?
i1(1) ,L, id (1) ,

r

r

k =1

k =1

{ ? w(i1(k ) ,L, id ( k ) ) ? h( xp (i ( k ) ) ,L, xp (i ( k ) ) )}] .

L
i1( r ) ,L,id ( r )

d

1

Then we can exchange the summation order of permutations and that of indices,
Ep (T r ( x)) =

?
i1(1) ,L, id (1) ,

L
i1( r ) ,L,id ( r )

r

r

k =1

k =1

{( ? w(i1( k ) ,L , id ( k ) )) Ep ( ? h( xp (i ( k ) ) ,L, xp (i ( k ) ) ))}.
d

1

Thus any moment of permutation distribution can be considered as a summation of the
product of data function term and index function term over a high dimensional index set and
all possible permutations.
Since all possible permutations map any index value between 1 and n to all possible index
r

values from 1 to n with equal probability, Ep ( ? h( xp (i ( k ) ) ,L , xp (i ( k ) ) )) , the summation of
k =1

1

d

data function over all permutations is only related to the equal/unequal relationship among
indices. It is natural to divide the whole index set U = {i1 ,L , id }r = {(i1(1) , L , id (1) ), L
(r )

, ( i1

, L , id

(r )

r

)} into the union of disjoint index subsets, in which Ep ( ? h( xp (i ( k ) ) ,L , xp (i ( k ) ) ))
k =1

1

d

is invariant.
Definition 1. Since h is a symmetric function, two index elements (i1 ,L , id ) and ( j1 ,L , jd )
are said to be equivalent if they are the same up to the order. For example, for d = 3, (1, 4, 5)
= (1,5,4) = (4,1,5) = (4,5,1) = (5,1,4) = (5,4,1).
Definition 2. Two indices {(i1(1) , L , id (1) ), L , (i1( r ) , L , id ( r ) )} and {( j1(1) , L , jd (1) ), L , ( j1( r ) , L , jd ( r ) )} are
said to be permutation equivalent/? if there exists a permutation p ? Sn such that
{(p (i1(1) ), L , p (id (1) )), L , (p (i1( r ) ), L , p (id ( r ) ))} = {( j1(1) , L , jd (1) ), L , ( j1( r ) , L , jd ( r ) )} . Here "=" means they
have same index elements by Definition 1. For example, for d = 2, n = 4, r = 2, {(1, 2), (2,
3)} ? {(2, 4), (1, 4)} since we can apply ?: 1?1, 2?4, 3?2, 4?3, such that {( ?(1), ?(2)),
(?(2), ?(3))} = {(1, 4), (4, 2)}= {(2, 4), (1, 4)}. As a result, the whole index set for d = 2, r =
2, can be divided into seven permutation equivalent subsets, [{(1, 1), (1, 1)}], [{(1, 1), (1,
2)}], [{(1, 1), (2, 2)}], [{(1, 2), (1, 2)}], [{(1, 1), (2, 3)}], [{(1, 2), (1, 3)}], [{(1, 2), (3, 4)}],
where [ ] denotes the equivalence class. Note that the number of the permutation equivalent
subsets is only related to the order of weighted v-test statistic d and the order of moment r ,

but not related to the data size n, and it is small for the first several moments calculation
(small r) with low order test statistics (small d).
Using the permutation equivalent relationship defined in Definition 2, the whole index set U
can be partitioned into several permutation equivalent index subsets. Then we can calculate
the r-th moment by summing up subtotals of all index subsets. This procedure can be done
without any real permutations based on Proposition 1 and Proposition 2 below.
r

Proposition 1. We claim that the data function sum Ep ( ? h( xp (i ( k ) ) ,L , xp (i ( k ) ) )) is invariant
k =1

d

1

within each equivalent index subset, and
r

?

r

? h( x j ( k ) ,L, x j ( k ) )

1
{( j1(1) ,L, jd (1) ),L,( j1( r ) ,L, jd ( r ) )}?[{(i1(1) ,L,id (1) ),L,(i1( r ) ,L,id ( r ) )}] k =1
1
d
k =1
card ([{(i1(1) ,L, id (1) ),L,(i1(r ) ,L, id (r ) )}])
where card ([{(i1(1) ,L , id (1) ),L , (i1( r ) ,L , id (r ) )}]) is the number of indices falling
permutation equivalent index subset [{(i1(1) ,L , id (1) ),L , (i1(r ) ,L , id ( r ) )}] .

Ep ( ? h( xp (i ( k ) ) ,L, xp (i ( k ) ) )) =

d

,

into the

Proof sketch:
Since all indices in the same permutation equivalent subset are equivalent with respect to the
symmetric group Sn,
r
r
1
Ep ( ? h( xp ( i ( k ) ) ,L , xp (i ( k ) ) )) =
? ? h( xp ( i ( k ) ) ,L , xp (i ( k ) ) ) =
1
d
1
d
n ! p ?Sn k =1
k =1
r

?

=

( ? h ( x j ( k ) ,L , x j

1
{( j1(1) ,L, jd (1) ),L,( j1( r ) ,L, jd ( r ) )}?[{( i1(1) ,Lid (1) ),L,(i1( r ) ,Lid ( r ) )}] k =1
(1)
(1)
(r )
(r)
card ([{(i1 ,L , id ),L , (i1 ,L , id )}])

d

) n !)

(k )

,

n!
r

?

=

( ? h ( x j ( k ) ,L , x j

{( j1(1) ,L, jd (1) ),L,( j1( r ) ,L, jd ( r ) )}?[{(i1(1) ,L,id (1) ),L,( i1( r ) ,L,id ( r ) )}] k =1
card ([{(i1(1) ,L , id (1) ),L , (i1( r ) ,L , id (r ) )}])

d

1

(k )

))
.

Proposition 2. Thus we can obtain the r-th moment by summing up the production of the
data partition sum wl and the index partition sum hl over all permutation equivalent
subsets, i.e.,
Ep (T r ( x)) = ? wl hl , where l = [{(i1(1) ,L , id (1) ),L , (i1( r ) ,L , id ( r ) )}] is any permutation
l?[U ]

equivalent subset of the whole index set U. [U] denotes the set of all distinct permutation
equivalent classes of U. The data partition sum is
r

?

hl =

wl =

( ? h( x j ( k ) , L , x j

{( j1(1) ,L, jd (1) ),L,( j1( r ) ,L, jd ( r ) )}?l k =1

(k )

d

1

))
, and the index partition sum is

card (l )
r

?

( ? w( x j ( k ) , L , x j

{( j1(1) ,L, jd (1) ),L,( j1( r ) ,L, jd ( r ) )}?l k =1

d

1

(k )

)) .

Proof sketch:
r

With Proposition 1, Ep ( ? h( xp (i ( k ) ) ,L , xp (i ( k ) ) )) is invariant within each equivalent index
k =1

d

1

subset, therefore,
Ep (T r ( x)) =

?

i1(1) ,L,id (1) ,
L
i1( r ) ,L,id ( r )

r

r

k =1

k =1

{( ? w(i1( k ) ,L , id ( k ) )) Ep ( ? h( xp (i ( k ) ) ,L , xp (i
1

d

(k )

)

))} =

= ?

?

l?[U ] {( j1(1) ,L, jd (1) ),L,( j1( r ) ,L, jd ( r ) )}?l

= ?

l?[U ] {(

r

r

k =1

k =1

{( ? w( j1( k ) ,L , jd ( k ) )) Ep ( ? h( xp ( j ( k ) ) ,L , xp ( j

d

1

(k )

)

))} =

r

?

j1(1) ,L, jd (1) ),L,( j1( r ) ,L, jd ( r ) )}?l

{ ? w( j1( k ) ,L , jd ( k ) )hl } = ? wl hl .
k =1

l?[U ]

Since both data partition sum wl and the index partition sum hl can be calculated by
summation over all distinct indices within each permutation equivalent index subset, no any
real permutation is needed for computing the moments.
2.3

R e c ur s i v e c a l c ul a t i o n

Direct calculation of the data partition sum and index partition sum leads to traversing
throughout the whole index set. So the computational cost is O(ndr). In the following, we
shall discuss how to reduce the cost by a recursive calculation algorithm.
Definition 3. Let l = [{(i1(1) ,L , id (1) ), L , (i1(r ) ,L , id ( r ) )}] and n = [( j1(1) , L , jd (1) ), L , ( j1( r ) , L , jd ( r ) )] .
l and n are two different permutation equivalent subsets of the whole index set U. We say
that the partition order of n is less than that of l , i.e., n p l , if l can be converted to n
by merging two or more index elements. For instance, n = [(1,1), (2, 3)] p l = [(1, 2), (3, 4)] ,
since by merging 1 and 2, l is converted to [{(1, 1), (3, 4)}] = [{(1, 1), (2, 3)}]. [{(1, 1), (3,
4)}] and [{(1, 1), (2, 3)}] are the same permutation equivalent index subsets because we can
apply the permutation ?: 1?1, 2?4, 3?3, 4?2 to [{(1, 1), (3, 4)}]. Note that the merging
operation may not be unique, for example, n can also be converted to l by merging 3 and
4. To clarify the concept of partition order, we list the order of all partitions when d=2 and
r=2 in figure 1. The partition order of a permutation equivalent subset n is said to be lower
than that of another permutation equivalent subset l if there is a directed path from l to n .

[{(1, 1 ) ,

(1, 1 )}]

[ {(1, 1 ) ,

( 2, 2 )}]

[ {(1, 1 ) ,

(1, 2 )}]

[{(1, 2 ) ,

(1, 2 )}]

[{(1, 1 ) ,

( 2 , 3 )}]

[{(1, 2 ) ,

(1, 3 )}]

[ {(1, 2 ) ,

( 3 , 4 )}]

Figure 1: Order of all permutation equivalent subsets when d = 2 and r = 2.
The difficulty for computing data partition sum and index partition sum comes from two
constraints; equal constraint and unequal constraint. For example, in the permutation
equivalent subset [{(1, 1), (2, 2)}], the equal constraint is that the first and the second index
number are equal and the third and fourth index are also equal. On the other hand, the
unequal constraint requires that the first two index numbers are different from those of the
last two. Due to the difficulties mentioned, we solve this problem by first relaxing the
unequal constraint and then applying the principle of inclusion and exclusion. Thus, the
calculation of a partition sum can be separated into two parts: the relaxed partition sum
without unequal constraint, and lower order partition sums. For example,

wl =[(1,1), (2,2)] = ? ( w(i, i ) w( j , j )) = wl =[(1,1), (2,2)]* - wl =[(1,1), (1,1)] =
i? j

= ? (w(i , i ) w( j , j )) - ? (w(i , i ) w( j , j )) = (? w(i, i )) 2 - ? w(i, i ) 2 , as the relaxed index partition
i= j

i, j

i

i

sum wl =[(1,1), (2,2)]* = ? (w(i , i ) w( j , j )) = (? w(i, i )) 2 .
i, j

i

Proposition 3. The index partition sum wl can be calculated by subtracting all lower order
partition

sums

from

the

corresponding

relaxed

index

partition

sum

wl * ,

i.e.,

#(l )
#(l ? n ) , where #(l ) is the number of distinct order-sensitive
#(n )
n pl
permutation equivalent subsets. For example, there are 2!2!2!/2!/2!=2 order-sensitive index
partition types for l = [(1, 1), (2, 3)] . They are [(1, 1), (2, 3)] and [(2, 3), (1, 1)]. Note that [(1, 1),
(2, 3)] and [(1, 1), (3, 2)] are the same type. #(l ? n ) is the number of different ways of
merging a higher order permutation equivalent subset l to a low order permutation equivalent
subset n .
wl = wl* - ? wn

The calculation of the data index partition sum is similar. Therefore, the computational cost
mainly depends on the calculation of relaxed partition sum and the lowest order partition sum.
Since the computational cost of the lowest order term is O(n), we mainly discuss the calculation
of relaxed partition sums in the following paragraphs.
To reduce the computational cost, we develop a greedy graph search algorithm. For
demonstration, we use the following example.
wl* =[(1,1),(1,2),(1,2),(1,3),(2,3),(1,4)] = #(l ) ? w(i , i ) w(i, j ) w(i, j ) w(i, k )w( j , k ) w(i, l ) . The permutation
i, j ,k ,l

equivalent index subset is represented by an undirected graph. Every node denotes an index
number. We connect two different nodes if these two corresponding index numbers are in the
same index element, i.e., in the same small bracket. In figure 2, the number 2 on the edge ij
denotes that the pair (i, j) is used twice. The self-connected node is also allowed. We assume there
is no isolated subgraph in the following discussion. If any isolated subgraph exists, we only need
to repeat the same procedure for all isolated subgraphs.
Now we shall discuss the steps to compute the wl* =[(1,1),(1,2),(1,2),(1,3),(2,3),(1,4)] . Firstly, we get rid of
? w(i , i ) w(i, j ) w(i, j ) w(i, k ) w( j , k )w(i , l )

the weights of edges and self-connections, i.e.,

i , j ,k ,l

= ? a(i, j ) w(i, k ) w( j , k )w(i , l ) , as a(i, j ) = w(i , i ) w(i, j ) w(i, j ) . Then we search a node with the
i , j ,k ,l

lowest degree and do summation for all indices connected with respect to the chosen node, i.e.,
? a(i, j ) w(i, k ) w( j , k ) w(i , l ) = ? b(i , j ) w(i, k )w( j , k ) , as b(i, j ) = ? a(i , j ) w(i, l ) . The chosen
i , j ,k ,l

l

i , j ,k

nodes and connected edges are deleted after the above computation. We repeat the same step until a
symmetric graph occurs. Since every node in the symmetric graph has the same degree, we randomly choose
any node; for example, k for summation, then ? b(i , j ) w(i, k ) w( j , k ) = ? b(i, j )c(i, j ) , as
i, j ,k

i, j

c(i, j) = ? w(i, k )w( j , k ) . Finally, we clear the whole graph and obtain the relaxed index partition sum.
k

l

i

l

i

i

i

2
j

k

j

k

j

k

j

Figure 2: Greedy Search Algorithm for computing
The most computational-expensive case is the complete graph in which every pair of nodes is
connected. Hence, the computational cost of cl * is determined by the subtotal that has the largest
symmetric subgraph in its graph representation. For example, the most expensive relaxed index
partition sum for d=2 and r=3 is w(i , j ) w(i, k )w( j , k ) , which is a triangle in the graph
representation.
Proposition 4 For d>=2, let m(m -1) / 2 ? r (d - 1)d / 2 < (m + 1)m / 2 , where r is the order of
moment and m is an integer. For a d-th order test statistic, the computational cost of the partition
sum for the r-th moment is bounded by O(nm). When d = 1 the computational complexity of the
partition sum is O(n).
Specifically, the computational cost of the 3rd and 4th moments for a second order test statistic is
O(n3). The computational cost for the 1st and 2nd moments is O(n2).

2.4

Fitting

The Pearson distribution series (Pearson I ~ VII) is a family of probability distributions that are
more general than the normal distribution [12]. It covers all distributions in the (?1, ?2) plane
including normal, beta, gamma, log-normal, and etc., where distribution shape parameters ?1, ?2
are the square of standardized skewness and kurtosis measurements, respectively. Given the first
four moments, the Pearson distribution series can be utilized to approximate the permutation
distribution of the test statistic without conducting real permutation.

3

E xp e ri m e n t a l re su lt s

To evaluate the accuracy and efficiency of our moments-based permutation tests, we generate
simulated data and conduct permutation tests for both linear and quadratic test statistics. We
consider six simulated cases in the first experiment for testing the difference between two groups,
A and B. We use mean difference statistics here. For group A, n1 observations are generated
independently from Normal(0,1) in Cases 1-2, from Gamma(3,3) in Cases 3-4, and from Beta(0.8,
0.8) in Cases 5-6. For group B, n2 independent observations are generated from Normal(1, 0.5) in
Cases 1-2, from Gamma (3,2) in Cases 3-4, and from Beta(0.1, 0.1) in Cases 5-6. The design is
balanced in Cases 1, 3, and 5 with n1 = n2 = 10, and unbalanced in Cases 2, 4, and 6 with n1 = 6, n2
= 18.
Table 1 illustrates the high accuracy of our moments-based permutation technique. Furthermore,
comparing with exact permutation or random 10,000 permutations, the moments-based
permutation tests reduce more than 99.8% of the computation cost, and this efficiency gain
increases with sample size. Table 1 shows the computation time and p-values of three permutation
methods from one simulation. In order to demonstrate the robustness of our method, we repeated
the simulation for 10 times in each case, and calculated the mean and variance of the absolute
biases of p-values of both moments-based permutation and random permutation, treating the pvalues of exact permutation as gold standard. In most cases, our moments-based permutation is
less biased and more stable than random permutation (Table 2), which demonstrates the
robustness and accuracy of our method.
Table 1: Comparison of computation costs and p-values of three permutation methods: Momentsbased permutation (MP), random permutation (RP), and exact permutation (EP). The t_MP, t_RP,
and t_EP denote the computation time (in seconds), and p_MP, p_RP, and p_EP are the p-values
of the three permutation methods.
C a s e 1Ca s e 2Ca s e 3Ca s e 4Ca s e 5Ca s e 6
t _ M P 6 . 7 9 e - 4 5.37e-4 5.54e-4 5.16e-4 5.79e-4 6.53e-4
t _ R P 5 . 0 7 e - 1 5.15e-1 5.06e-1 1.30e-1 2.78e-1 5.99e-1
t _ E P 3 . 9 9 e - 0 1.21e-0 3.71e-0 1.21e-0 3.71e-0 1.22e-0
p _ M P 1 . 1 9 e - 1 2.45e-2 1.34e-1 1.19e-1 3.58e-2 5.07e-5
p _ R P 1 . 2 1 e - 1 2.56e-2 1.36e-1 1.20e-1 3.53e-2 5.09e-2
p _ E P 1 . 1 9 e - 1 2.39e-2 1.34e-1 1.15e-1 3.55e-2 5.11e-2

We consider three simulated cases in the second experiment for testing the difference among three
groups D, E, and F. We use modified F statistics [7] here. For group D, n1 observations are
generated independently from Normal(0,1) in Case 7, from Gamma(3,2) in Case 8, and from
Beta(0.8, 0.8) in Case 9. For group E, n2 independent observations are generated from Normal(0,1)
in Case 7, from Gamma(3,2) in Case 8, and from Beta(0.8, 0.8) in Case 9. For group F, n3
independent observations are generated from Normal(0.1,1) in Case 7, from Gamma(3,1) in Case
8, and from Beta(0.1, 0.1) in Case 9.The design is unbalanced with n1 = 6, n2 = 8, and n3 =12.
Since the exact permutation is too expensive here, we consider the p-values of 200,000 random
permutations (EP) as gold standard. Our methods are more than one hundred times faster than
2,000 random permutation (RP) and also more accurate and robust (Table 3).
We applied the method to the MRI hippocampi belonging to 2 groups, with 21 subjects in
group A and 15 in group B. The surface shapes of different objects are represented by the
same number of location vectors (with each location vector consisting of the spatial x, y, and
z coordinates of the corresponding vertex) for our subsequent statistical shape analysis.
There is no shape difference at a location if the corresponding location vector has an equal

mean between two groups. Evaluation of the hypothesis test using our moments-based
permutation with the modified Hotelling?s T2 test statistics [8] is shown in Fig. 3(a) and 3(b).
It can be seen that the Pearson distribution approximation leads to ignorable discrepancy
with the raw p-value map from real permutation. The false positive error control results are
shown in Fig. 3(c).
Table 2: Robustness and accuracy comparison of moments-based permutation and random
permutation across 10 simulations, considering the p-values of exact permutation as gold standard.
Mean_ABias_MP and VAR_MP are the mean of the absolute biases and the variance of the biases
of p-values of moments-based permutation; Mean_ABias_RP and VAR_RP are the mean of the
absolute biases and the variance of the biases of p-values of random permutation. Mean difference
statistic is used.
Case1Case2Case3Case4Case5Case6
Mean_ABias_MP 1.62e-4 3.04e-4 6.36e-4 8.41e-4 1.30e-3 3.50e-3
Mean_ABias_RP 7.54e-4 3.39e-4 9.59e-4 8.39e-4 1.30e-3 2.00e-3
VAR_MP
6.42e-8 2.74e-7 1.54e-6 1.90e-6 3.76e-6 2.77e-5
VAR_RP
7.85e-7 1.86e-7 1.69e-6 3.03e-6 4.24e-5 1.88e-5

Table 3: Computation cost, robustness, and accuracy comparison of moments-based permutation
and random permutation across 10 simulations. Modified F statistic is used.
Case7Case8Case9
Case7Case8Case9
t_MP 1.03e-3 1.42e-3 1.64e-3 Mean_ABias_MP 9.23e-4 2.37e-4 2.11e-3
t_RP 1.51e-1 1.48e-1 1.38e-1 Mean_ABias_RP 3.94e-3 2.79e-3 3.42e-3
t_EP 1.76e+1 1.86e+1 2.37e+1 VAR_MP
1.10e-6 8.74e-8 1.23e-5
VAR_RP
2.27e-5 1.48e-5 1.85e-5
p-value>0.05
=0.05

(a)

(b)

(c)

(d)

=0.0

(e)

Figure 3. (a) and (b): Comparison of techniques in raw p-value measurement at
a = 0.05 (without correction), through real permutation ((a); number of permutations =
10,000) and using the present moments-based permutation (b). (c) p-map after BH?s FDR
correction of (b). (e) Facial differences between Asian male and white male. Locations in red
on the 3D surface denote significant face shape differences (significance level ? = 0.01 with
false discovery rate control).
We also applied our method to the 3D face comparison between Asian males and white
males. We choose 10 Asian males and 10 white males out of the USF face database to
calculate their differences with the modified Hotelling?s T2 test statistics. Each face surface
is represented by 4,000 voxels. All surfaces are well aligned. Results from our algorithm in
Fig. 3(e) show that significant differences occur at eye edge, nose, lip corners, and cheeks.
They are consistent with anthropology findings and suggest the discriminant surface regions
for ethnic group recognition.

4

C o n c lu si o n

We present and develop novel moments-based permutation tests where the permutation
distributions are accurately approximated through Pearson distributions for considerably reduced
computation cost. Comparing with regular random permutation, the proposed method
considerably reduces computation cost without loss of accuracy. General and analytical
formulations for the moments of permutation distribution are derived for weighted v-test statistics.
The proposed strategy takes advantage of nonparametric permutation tests and parametric Pearson
distribution approximation to achieve both accuracy/flexibility and efficiency.

