query sentence: state-of-art algorithms in theano
<<------------------------------------------------------------------------------------------------------------------------------------------->>
title: 5647-binaryconnect-training-deep-neural-networks-with-binary-weights-during-propagations.pdf

BinaryConnect Training Deep Neural Networks with
binary weights during propagations
Matthieu Courbariaux
Ecole
Polytechnique de Montr?eal
matthieu.courbariaux@polymtl.ca
Yoshua Bengio
Universit?e de Montr?eal CIFAR Senior Fellow
yoshua.bengio@gmail.com
Jean-Pierre David
Ecole
Polytechnique de Montr?eal
jean-pierre.david@polymtl.ca
Abstract
Deep Neural Networks DNN have achieved state-of-the-art results in a wide
range of tasks with the best results obtained with large training sets and large
models In the past GPUs enabled these breakthroughs because of their greater
computational speed In the future faster computation at both training and test
time is likely to be crucial for further progress and for consumer applications on
low-power devices As a result there is much interest in research and development of dedicated hardware for Deep Learning Binary weights weights
which are constrained to only two possible values or would bring great
benefits to specialized DL hardware by replacing many multiply-accumulate operations by simple accumulations as multipliers are the most space and powerhungry components of the digital implementation of neural networks We introduce BinaryConnect a method which consists in training a DNN with binary
weights during the forward and backward propagations while retaining precision
of the stored weights in which gradients are accumulated Like other dropout
schemes we show that BinaryConnect acts as regularizer and we obtain near
state-of-the-art results with BinaryConnect on the permutation-invariant MNIST
CIFAR-10 and SVHN
Introduction
Deep Neural Networks DNN have substantially pushed the state-of-the-art in a wide range of tasks
especially in speech recognition and computer vision notably object recognition from images More recently deep learning is making important strides in natural language processing
especially statistical machine translation Interestingly one of the key factors that enabled
this major progress has been the advent of Graphics Processing Units GPUs with speed-ups on the
order of to starting with and similar improvements with distributed training
Indeed the ability to train larger models on more data has enabled the kind of breakthroughs observed in the last few years Today researchers and developers designing new deep learning algorithms and applications often find themselves limited by computational capability This along with
the drive to put deep learning systems on low-power devices unlike GPUs is greatly increasing the
interest in research and development of specialized hardware for deep networks
Most of the computation performed during training and application of deep networks regards the
multiplication of a real-valued weight by a real-valued activation the recognition or forward
propagation phase of the back-propagation algorithm or gradient the backward propagation
phase of the back-propagation algorithm This paper proposes an approach called BinaryConnect
to eliminate the need for these multiplications by forcing the weights used in these forward and
backward propagations to be binary constrained to only two values not necessarily and We
show that state-of-the-art results can be achieved with BinaryConnect on the permutation-invariant
MNIST CIFAR-10 and SVHN
What makes this workable are two ingredients
Sufficient precision is necessary to accumulate and average a large number of stochastic
gradients but noisy weights and we can view discretization into a small number of values
as a form of noise especially if we make this discretization stochastic are quite compatible
with Stochastic Gradient Descent the main type of optimization algorithm for deep
learning SGD explores the space of parameters by making small and noisy steps and
that noise is averaged out by the stochastic gradient contributions accumulated in each
weight Therefore it is important to keep sufficient resolution for these accumulators
which at first sight suggests that high precision is absolutely required and show
that randomized or stochastic rounding can be used to provide unbiased discretization
have shown that SGD requires weights with a precision of at least to bits and
successfully train DNNs with bits dynamic fixed-point computation Besides the
estimated precision of the brain synapses varies between and bits
Noisy weights actually provide a form of regularization which can help to generalize better
as previously shown with variational weight noise Dropout and DropConnect which add noise to the activations or to the weights For instance DropConnect
which is closest to BinaryConnect is a very efficient regularizer that randomly substitutes half of the weights with zeros during propagations What these previous works show
is that only the expected value of the weight needs to have high precision and that noise
can actually be beneficial
The main contributions of this article are the following
We introduce BinaryConnect a method which consists in training a DNN with binary
weights during the forward and backward propagations Section
We show that BinaryConnect is a regularizer and we obtain near state-of-the-art results on
the permutation-invariant MNIST CIFAR-10 and SVHN Section
We make the code for BinaryConnect available
BinaryConnect
In this section we give a more detailed view of BinaryConnect considering which two values to
choose how to discretize how to train and how to perform inference
or
Applying a DNN mainly consists in convolutions and matrix multiplications The key arithmetic
operation of DL is thus the multiply-accumulate operation Artificial neurons are basically multiplyaccumulators computing weighted sums of their inputs
BinaryConnect constraints the weights to either or during propagations As a result many
multiply-accumulate operations are replaced by simple additions and subtractions This is a huge
gain as fixed-point adders are much less expensive both in terms of area and energy than fixed-point
multiply-accumulators
Deterministic vs stochastic binarization
The binarization operation transforms the real-valued weights into the two possible values A very
straightforward binarization operation would be based on the sign function
if
wb
otherwise
https://github.com/MatthieuCourbariaux/BinaryConnect
Where wb is the binarized weight and the real-valued weight Although this is a deterministic operation averaging this discretization over the many input weights of a hidden unit could compensate
for the loss of information An alternative that allows a finer and more correct averaging process to
take place is to binarize stochastically
with probability
wb
with probability
where is the hard sigmoid function
clip
We use such a hard sigmoid rather than the soft version because it is far less computationally expensive both in software and specialized hardware implementations and yielded excellent results in our
experiments It is similar to the hard tanh non-linearity introduced by It is also piece-wise
linear and corresponds to a bounded form of the rectifier
Propagations vs updates
Let us consider the different steps of back-propagation with SGD udpates and whether it makes
sense or not to discretize the weights at each of these steps
Given the DNN input compute the unit activations layer by layer leading to the top layer
which is the output of the DNN given its input This step is referred as the forward propagation
Given the DNN target compute the training objective?s gradient each layer?s activations starting from the top layer and going down layer by layer until the first hidden
layer This step is referred to as the backward propagation or backward phase of backpropagation
Compute the gradient each layer?s parameters and then update the parameters using
their computed gradients and their previous values This step is referred to as the parameter
update
Algorithm SGD training with BinaryConnect is the cost function for minibatch and the functions binarize(w and clip(w specify how to binarize and clip weights is the number of layers
Require a minibatch of inputs targets previous parameters weights and biases
and learning rate
Ensure updated parameters wt and bt
Forward propagation
wb binarize(wt?1
For to compute ak knowing wb and
Backward propagation
Initialize output layer?s activations gradient a
For to compute knowing ak and wb
Parameter update
Compute
and db?C
knowing a
and
wt clip(wt?1
bt
A key point to understand with BinaryConnect is that we only binarize the weights during the forward and backward propagations steps and but not during the parameter update step as
illustrated in Algorithm Keeping good precision weights during the updates is necessary for SGD
to work at all These parameter changes are tiny by virtue of being obtained by gradient descent
SGD performs a large number of almost infinitesimal changes in the direction that most improves
the training objective plus noise One way to picture all this is to hypothesize that what matters
most at the end of training is the sign of the weights but that in order to figure it out we perform
a lot of small changes to a continuous-valued quantity and only at the end consider its sign
sign
gt
where gt is a noisy estimator of
where C(f is the value
of the objective function on input,target example when are the previous weights and
is its final discretized value of the weights
Another way to conceive of this discretization is as a form of corruption and hence as a regularizer
and our empirical results confirm this hypothesis In addition we can make the discretization errors
on different weights approximately cancel each other while keeping a lot of precision by randomizing
the discretization appropriately We propose a form of randomized discretization that preserves the
expected value of the discretized weight
Hence at training time BinaryConnect randomly picks one of two values for each weight for each
minibatch for both the forward and backward propagation phases of backprop However the SGD
update is accumulated in a real-valued variable storing the parameter
An interesting analogy to understand BinaryConnect is the DropConnect algorithm Just like
BinaryConnect DropConnect only injects noise to the weights during the propagations Whereas
DropConnect?s noise is added Gaussian noise BinaryConnect?s noise is a binary sampling process
In both cases the corrupted value has as expected value the clean original value
Clipping
Since the binarization operation is not influenced by variations of the real-valued weights when its
magnitude is beyond the binary values and since it is a common practice to bound weights usually the weight vector in order to regularize them we have chosen to clip the real-valued weights
within the interval right after the weight updates as per Algorithm The real-valued weights
would otherwise grow very large without any impact on the binary weights
A few more tricks
Optimization
No learning rate scaling
Learning rate scaling
SGD
Nesterov momentum
ADAM
Table Test error rates of a small CNN trained on CIFAR-10 depending on optimization method
and on whether the learning rate is scaled with the weights initialization coefficients from
We use Batch Normalization in all of our experiments not only because it accelerates
the training by reducing internal covariate shift but also because it reduces the overall impact of
the weights scale Moreover we use the ADAM learning rule in all of our CNN experiments
Last but not least we scale the weights learning rates respectively with the weights initialization
coefficients from when optimizing with ADAM and with the squares of those coefficients
when optimizing with SGD or Nesterov momentum Table illustrates the effectiveness of
those tricks
Test-Time Inference
Up to now we have introduced different ways of training a DNN with on-the-fly weight binarization
What are reasonable ways of using such a trained network performing test-time inference on
new examples We have considered three reasonable alternatives
Use the resulting binary weights wb this makes most sense with the deterministic form of
BinaryConnect
Use the real-valued weights the binarization only helps to achieve faster training but
not faster test-time performance
In the stochastic case many different networks can be sampled by sampling a wb for each
weight according to The ensemble output of these networks can then be obtained by
averaging the outputs from individual networks
We use the first method with the deterministic form of BinaryConnect As for the stochastic form
of BinaryConnect we focused on the training advantage and used the second method in the experiments test-time inference using the real-valued weights This follows the practice of Dropout
methods where at test-time the noise is removed
Method
MNIST
CIFAR-10
SVHN
No regularizer
BinaryConnect
BinaryConnect stoch
Dropout
Maxout Networks
Deep L2-SVM
Network in Network
DropConnect
Deeply-Supervised Nets
Table Test error rates of DNNs trained on the MNIST no convolution and no unsupervised
pretraining CIFAR-10 no data augmentation and SVHN depending on the method We see
that in spite of using only a single bit per weight during propagation performance is not worse
than ordinary no regularizer DNNs it is actually better especially with the stochastic version
suggesting that BinaryConnect acts as a regularizer
Figure Features of the first layer of an MLP trained on MNIST depending on the regularizer From left to right no regularizer deterministic BinaryConnect stochastic BinaryConnect
and Dropout
Benchmark results
In this section we show that BinaryConnect acts as regularizer and we obtain near state-of-the-art
results with BinaryConnect on the permutation-invariant MNIST CIFAR-10 and SVHN
Permutation-invariant MNIST
MNIST is a benchmark image classification dataset It consists in a training set of and
a test set of 28 28 gray-scale images representing digits ranging from to Permutationinvariance means that the model must be unaware of the image structure of the data other
words CNNs are forbidden Besides we do not use any data-augmentation preprocessing or unsupervised pretraining The MLP we train on MNIST consists in hidden layers of Rectifier
Linear Units ReLU 24 and a L2-SVM output layer L2-SVM has been shown to perform
better than Softmax on several classification benchmarks The square hinge loss is minimized with SGD without momentum We use an exponentially decaying learning rate We use Batch
Figure Histogram of the weights of the first layer of an MLP trained on MNIST depending on
the regularizer In both cases it seems that the weights are trying to become deterministic to reduce
the training error It also seems that some of the weights of deterministic BinaryConnect are stuck
around hesitating between and
Figure Training curves of a CNN on CIFAR-10 depending on the regularizer The dotted lines
represent the training costs square hinge losses and the continuous lines the corresponding validation error rates Both versions of BinaryConnect significantly augment the training cost slow down
the training and lower the validation error rate which is what we would expect from a Dropout
scheme
Normalization with a minibatch of size to speed up the training As typically done we use the
last samples of the training set as a validation set for early stopping and model selection We
report the test error rate associated with the best validation error rate after epochs we do not
retrain on the validation set We repeat each experiment times with different initializations The
results are in Table They suggest that the stochastic version of BinaryConnect can be considered
a regularizer although a slightly less powerful one than Dropout in this context
CIFAR-10
CIFAR-10 is a benchmark image classification dataset It consists in a training set of and
a test set of 32 32 color images representing airplanes automobiles birds cats deers
dogs frogs horses ships and trucks We preprocess the data using global contrast normalization
and ZCA whitening We do not use any data-augmentation which can really be a game changer for
this dataset The architecture of our CNN is
Where C3 is a ReLU convolution layer is a max-pooling layer a fully
connected layer and SVM a L2-SVM output layer This architecture is greatly inspired from VGG
The square hinge loss is minimized with ADAM We use an exponentially decaying learning
rate We use Batch Normalization with a minibatch of size to speed up the training We use the
last samples of the training set as a validation set We report the test error rate associated with
the best validation error rate after training epochs we do not retrain on the validation set The
results are in Table and Figure
SVHN
SVHN is a benchmark image classification dataset It consists in a training set of and a test set
of 32 32 color images representing digits ranging from to We follow the same procedure
that we used for CIFAR-10 with a few notable exceptions we use half the number of hidden units
and we train for epochs instead of because SVHN is quite a big dataset The results are in
Table
Related works
Training DNNs with binary weights has been the subject of very recent works 38 39 Even
though we share the same objective our approaches are quite different do not train their
DNN with Backpropagation but with a variant called Expectation Backpropagation
EBP is based on Expectation Propagation which is a variational Bayes method used to do
inference in probabilistic graphical models Let us compare their method to ours
It optimizes the weights posterior distribution which is not binary In this regard our
method is quite similar as we keep a real-valued version of the weights
It binarizes both the neurons outputs and weights which is more hardware friendly than
just binarizing the weights
It yields a good classification accuracy for fully connected networks on MNIST but not
yet for ConvNets
retrain neural networks with ternary weights during forward and backward propagations
They train a neural network with high-precision
After training they ternarize the weights to three possible values and and adjust
to minimize the output error
And eventually they retrain with ternary weights during propagations and high-precision
weights during updates
By comparison we train all the way with binary weights during propagations our training procedure could be implemented with efficient specialized hardware avoiding the forward and backward
propagations multiplications which amounts to about of the multiplications Algorithm
Conclusion and future works
We have introduced a novel binarization scheme for weights during forward and backward propagations called BinaryConnect We have shown that it is possible to train DNNs with BinaryConnect on
the permutation invariant MNIST CIFAR-10 and SVHN datasets and achieve nearly state-of-the-art
results The impact of such a method on specialized hardware implementations of deep networks
could be major by removing the need for about of the multiplications and thus potentially allowing to speed-up by a factor of at training time With the deterministic version of BinaryConnect
the impact at test time could be even more important getting rid of the multiplications altogether
and reducing by a factor of at least from bits single-float precision to single bit precision
the memory requirement of deep networks which has an impact on the memory to computation
bandwidth and on the size of the models that can be run Future works should extend those results to
other models and datasets and explore getting rid of the multiplications altogether during training
by removing their need from the weight update computation
Acknowledgments
We thank the reviewers for their many constructive comments We also thank Roland Memisevic for
helpful discussions We thank the developers of Theano a Python library which allowed
us to easily develop a fast and optimized code for GPU. We also thank the developers of Pylearn2
and Lasagne two Deep Learning libraries built on the top of Theano We are also grateful for
funding from NSERC the Canada Research Chairs Compute Canada and CIFAR

<<----------------------------------------------------------------------------------------------------------------------->>

title: 6573-binarized-neural-networks.pdf

Binarized Neural Networks
Itay Hubara1
itayh@technion.ac.il
Matthieu Courbariaux2
matthieu.courbariaux@gmail.com
Ran El-Yaniv1
rani@cs.technion.ac.il
Daniel Soudry3
daniel.soudry@gmail.com
Yoshua Bengio2,4
yoshua.umontreal@gmail.com
Technion Israel Institute of Technology
Columbia University
Indicates equal contribution
Universit de Montr?al
CIFAR Senior Fellow
Abstract
We introduce a method to train Binarized Neural Networks BNNs neural
networks with binary weights and activations at run-time At train-time the binary
weights and activations are used for computing the parameter gradients During the
forward pass BNNs drastically reduce memory size and accesses and replace most
arithmetic operations with bit-wise operations which is expected to substantially
improve power-efficiency To validate the effectiveness of BNNs we conducted
two sets of experiments on the Torch7 and Theano frameworks On both BNNs
achieved nearly state-of-the-art results over the MNIST CIFAR-10 and SVHN
datasets We also report our preliminary results on the challenging ImageNet
dataset Last but not least we wrote a binary matrix multiplication GPU kernel
with which it is possible to run our MNIST BNN times faster than with an
unoptimized GPU kernel without suffering any loss in classification accuracy The
code for training and running our BNNs is available on-line
Introduction
Deep Neural Networks DNNs have substantially pushed Artificial Intelligence limits in a wide
range of tasks LeCun Today DNNs are almost exclusively trained on one or many very
fast and power-hungry Graphic Processing Units GPUs Coates As a result it is often
a challenge to run DNNs on target low-power devices and substantial research efforts are invested in
speeding up DNNs at run-time on both general-purpose Gong Han and
specialized computer hardware Chen Esser
This paper makes the following contributions
We introduce a method to train Binarized-Neural-Networks BNNs neural networks with binary
weights and activations at run-time and when computing the parameter gradients at train-time
Section
We conduct two sets of experiments each implemented on a different framework namely Torch7
and Theano which show that it is possible to train BNNs on MNIST CIFAR-10 and SVHN and
achieve near state-of-the-art results Section Moreover we report preliminary results on the
challenging ImageNet dataset
We show that during the forward pass both at run-time and train-time BNNs drastically reduce
memory consumption size and number of accesses and replace most arithmetic operations with
bit-wise operations which potentially lead to a substantial increase in power-efficiency Section
Conference on Neural Information Processing Systems NIPS Barcelona Spain
Moreover a binarized CNN can lead to binary convolution kernel repetitions we argue that
dedicated hardware could reduce the time complexity by
Last but not least we programed a binary matrix multiplication GPU kernel with which it is
possible to run our MNIST BNN times faster than with an unoptimized GPU kernel without
suffering any loss in classification accuracy Section
The code for training and running our BNNs is available on-line both Theano1 and Torch framework2
Binarized Neural Networks
In this section we detail our binarization function show how we use it to compute the parameter
gradients,and how we backpropagate through it
Deterministic vs Stochastic Binarization When training a BNN we constrain both the weights
and the activations to either or Those two values are very advantageous from a hardware
perspective as we explain in Section In order to transform the real-valued variables into those
two values we use two different binarization functions as in Courbariaux Our first
binarization function is deterministic
if
xb Sign(x
otherwise
where xb is the binarized variable weight or activation and the real-valued variable It is very
straightforward to implement and works quite well in practice Our second binarization function is
stochastic
with probability
with probability
where is the hard sigmoid function
The stochastic binarization is more appealing than the sign function but harder to implement as
it requires the hardware to generate random bits when quantizing As a result we mostly use the
deterministic binarization function the sign function with the exception of activations at
train-time in some of our experiments
clip
Gradient Computation and Accumulation Although our BNN training method uses binary
weights and activation to compute the parameter gradients the real-valued gradients of the weights
are accumulated in real-valued variables as per Algorithm Real-valued weights are likely required
for Stochasic Gradient Descent SGD to work at all SGD explores the space of parameters in small
and noisy steps and that noise is averaged out by the stochastic gradient contributions accumulated
in each weight Therefore it is important to maintain sufficient resolution for these accumulators
which at first glance suggests that high precision is absolutely required
Moreover adding noise to weights and activations when computing the parameter gradients provide
a form of regularization that can help to generalize better as previously shown with variational
weight noise Graves Dropout Srivastava and DropConnect Wan
Our method of training BNNs can be seen as a variant of Dropout in which instead of randomly
setting half of the activations to zero when computing the parameter gradients we binarize both the
activations and the weights
Propagating Gradients Through Discretization The derivative of the sign function is zero almost
everywhere making it apparently incompatible with back-propagation since the exact gradient of
the cost with respect to the quantities before the discretization pre-activations or weights would
https://github.com/MatthieuCourbariaux/BinaryNet
https://github.com/itayhubara/BinaryNet
be zero Note that this remains true even if stochastic quantization is used Bengio studied
the question of estimating or propagating gradients through stochastic discrete neurons He found in
his experiments that the fastest training was obtained when using the straight-through estimator
previously introduced in Hinton?s lectures Hinton We follow a similar approach but use the
version of the straight-through estimator that takes into account the saturation effect and does use
deterministic rather than stochastic sampling of the bit Consider the sign function quantization
Sign(r
and assume that an estimator gq of the gradient
has been obtained with the straight-through
estimator when needed
Algorithm Training a BNN. is the cost function Algorithm Shift based AdaMax learning
for minibatch the learning rate decay factor and rule Kingma Ba gt2 indicates the
the number of layers indicates element-wise mul element-wise square gt gt and stands for
tiplication The function Binarize specifies how to both left and right bit-shift Good default
stochastically or deterministically binarize the activa settings are
tions and weights and Clip specifies how to clip the All operations on vectors are
weights BatchNorm specifies how to batch-normalize element-wise With and we denote
the activations using either batch normalization Ioffe and to the power
Szegedy or its shift-based variant we describe in
Algorithm BackBatchNorm specifies how to back Require Previous parameters and
their gradient gt and learning rate
propagate through the normalization Update specifies
Ensure
Updated parameters
how to update the parameters when their gradients are
Biased 1st and 2nd moment estimates
known using either ADAM Kingma Ba or
the shift-based AdaMax we describe in Algorithm
mt gt
vt gt
Require a minibatch of inputs and targets a
Updated parameters
previous weights previous BatchNorm parameters weight initialization coefficients from Glorot
Bengio and previous learning rate
Ensure updated weights updated BatchNorm
parameters and updated learning rate
Algorithm Shift based Batch Normaliz{1 Computing the gradients
ing Transform applied to activation over
Forward propagation
a mini-batch The approximate power-offor to do
is3 AP sign(x)2round(log2|x and
Wkb Binarize(Wk sk abk?1 Wkb
stands for both left and right binary shift
ak BatchNorm(sk
Require Values of over a mini-batch
if then abk Binarize(ak
parameters to learn
Backward propagation
Ensure
BN(xi
Please note that the gradients are not binary
Mini-batch
Pm mean
Compute gaL aL knowing aL and a
for to do
Centered input
if then gak gabk
C(xi
gsk g?k BackBatchNorm(gak sk
Approximate
variance
Pm
gabk?1 gsk Wkb gWkb gs>k abk?1
AP
Normalize
Accumulating the gradients
x?i C(xi AP
for to do
Scale
and
shift
Update(?k g?k
AP x?i
Wkt+1 Clip(Update(Wk gWkb
Then our straight-through estimator of
is simply
gr gq
Note that this preserves the gradient?s information and cancels the gradient when is too large
Not cancelling the gradient when is too large significantly worsens the performance The use of
this straight-through estimator is illustrated in Algorithm The derivative can also be seen
as propagating the gradient through hard tanh which is the following piece-wise linear activation
function
Htanh(x Clip(x
For hidden units we use the sign function nonAlgorithm Running a BNN. layers
linearity to obtain binary activations and for
Require a vector of 8-bit inputs a0 the binary
weights we combine two ingredients
weights and the BatchNorm parameters
Constrain each real-valued weight between Ensure the MLP output aL
and by projecting wr to or when the
First layer
weight update brings wr outside of
a1
clipping the weights during training as
for to do
per Algorithm The real-valued weights
a1 a1 XnorDotProduct(an0 W1b
would otherwise grow very large without any
ab1 Sign(BatchNorm(a1
impact on the binary weights
Remaining hidden layers
for to do
When using a weight wr quantize it using
ak XnorDotProduct(abk?1 Wkb
wb Sign(wr
abk Sign(BatchNorm(ak
This is consistent with the gradient canceling
Output layer
when wr according to
aL XnorDotProduct(abL?1 WLb
aL BatchNorm(aL
Shift-based Batch Normalization Batch
Normalization Ioffe Szegedy accelerates the training and also seems to reduces
the overall impact of the weight scale The normalization noise may also help to regularize the
model However at train-time BN requires many multiplications calculating the standard deviation
and dividing by namely dividing by the running variance the weighted mean of the training
set activation variance Although the number of scaling calculations is the same as the number of
neurons in the case of ConvNets this number is quite large For example in the CIFAR-10 dataset
using our architecture the first convolution layer consisting of only filter masks
converts an image of size 32 32 to size 28 28 which is two orders of magnitude
larger than the number of weights To achieve the results that BN would obtain we use a shift-based
batch normalization SBN technique detailed in Algorithm SBN approximates BN almost
without multiplications In the experiment we conducted we did not observe accuracy loss when
using the shift based BN algorithm instead of the vanilla BN algorithm
Shift based AdaMax The ADAM learning rule Kingma Ba also seems to reduce the
impact of the weight scale Since ADAM requires many multiplications we suggest using instead the
shift-based AdaMax we detail in Algorithm In the experiment we conducted we did not observe
accuracy loss when using the shift-based AdaMax algorithm instead of the vanilla ADAM algorithm
First Layer In a BNN only the binarized values of the weights and activations are used in all
calculations As the output of one layer is the input of the next all the layers inputs are binary
with the exception of the first layer However we do not believe this to be a major issue First in
computer vision the input representation typically has far fewer channels red green and blue
than internal representations As a result the first layer of a ConvNet is often the smallest
convolution layer both in terms of parameters and computations Szegedy Second it is
relatively easy to handle continuous-valued inputs as fixed point numbers with bits of precision
For example in the common case of 8-bit fixed point inputs
wb
wb
where is a vector of 8-bit inputs is the most significant bit of the first input wb is a vector
of 1-bit weights and is the resulting weighted sum This trick is used in Algorithm
Benchmark Results
We conduct two sets of experiments each based on a different framework namely Torch7 and Theano
Implementation details are reported in Appendix A and code for both frameworks is available online
Results are reported in Table
Hardware implementation of AP2 is as simple as extracting the index of the most significant bit from the
number?s binary representation
Table Classification test error rates of DNNs trained on MNIST fully connected architecture
CIFAR-10 and SVHN convnet No unsupervised pre-training or data augmentation was used
Data set
MNIST
SVHN
Binarized activations+weights during training and test
BNN Torch7
BNN Theano
Committee Machines Array Baldassi
Binarized weights during training and test
BinaryConnect Courbariaux
Binarized activations+weights during test
EBP Cheng
Bitwise DNNs Kim Smaragdis
Ternary weights binary activations during test
Hwang Sung
No binarization standard results
No regularization
Gated pooling Lee
CIFAR-10
Preliminary Results on ImageNet To Figure Training curves for different methods on
test the strength of our method we applied CIFAR-10 dataset The dotted lines represent the trainit to the challenging ImageNet classifica ing costs square hinge losses and the continuous lines
tion task Considerable research has been the corresponding validation error rates Although
concerned with compressing ImageNet ar BNNs are slower to train they are nearly as accurate as
chitectures while preserving high accuracy float DNNs
performance Han Previous approaches that have been tried include pruning near zero weights using matrix factorization techniques quantizing
the weights and applying Huffman codes
among others To the best of the our knowledge so far there are no reports on successfully quantizing the network?s activations
Moreover a recent work Han
showed that accuracy significantly deteriorates when trying to quantize convolutional
layers weights below bits FC layers are
more robust to quantization and can operate
quite well with only bits In the present
work we attempted to tackle the difficult task of binarizing both weights and activations Employing
the well known AlexNet and GoogleNet architectures we applied our techniques and achieved
top-1 and top-5 accuracies using AlexNet and top-1 and top-5 accuracies
using GoogleNet While this performance leaves room for improvement relative to full precision
nets they are by far better than all previous attempts to compress ImageNet architectures using less
than bits precision for the weights Moreover this advantage is achieved while also binarizing
neuron activations Detailed descriptions of these results as well as full implementation details
of our experiments are reported in the supplementary material Appendix B). In our latest work
Hubara we relaxed the binary constrains and allowed more than 1-bit per weight and
activations The resulting QNNs achieve prediction accuracy comparable to their counterparts
For example our quantized version of AlexNet with 1-bit weights and 2-bit activations achieves
top-1 accuracy and GoogleNet with 4-bits weighs and activation achived Moreover we
quantize the parameter gradients to 6-bits as well which enables gradients computation using only
bit-wise operation Full details can be found in Hubara
Table Energy consumption of multiplyaccumulations in pico-joules Horowitz
Operation
MUL ADD
8bit Integer
32bit Integer
16bit Floating Point
32tbit Floating Point
Table Energy consumption of memory accesses
in pico-joules Horowitz
Memory size memory access
8K
1M
DRAM
High Power Efficiency during the Forward Pass
Computer hardware be it general-purpose or specialized is composed of memories arithmetic
operators and control logic During the forward pass both at run-time and train-time BNNs
drastically reduce memory size and accesses and replace most arithmetic operations with bit-wise
operations which might lead to a great increase in power-efficiency Moreover a binarized CNN can
lead to binary convolution kernel repetitions and we argue that dedicated hardware could reduce the
time complexity by
Memory Size and Accesses Improving computing performance has always been and remains a
challenge Over the last decade power has been the main constraint on performance Horowitz
This is why much research effort has been devoted to reducing the energy consumption of neural
networks Horowitz provides rough numbers for the energy consumed by the computation the
given numbers are for technology as summarized in Tables and Importantly we can see
that memory accesses typically consume more energy than arithmetic operations and memory access
cost augments with memory size In comparison with DNNs BNNs require 32 times smaller
memory size and 32 times fewer memory accesses This is expected to reduce energy consumption
drastically more than 32 times
XNOR-Count Applying a DNN mainly consists of convolutions and matrix multiplications The
key arithmetic operation of deep learning is thus the multiply-accumulate operation Artificial neurons
are basically multiply-accumulators computing weighted sums of their inputs In BNNs both the
activations and the weights are constrained to either or As a result most of the floating
point multiply-accumulations are replaced by 1-bit XNOR-count operations This could have a big
impact on dedicated deep learning hardware For instance a floating point multiplier costs
about Xilinx FPGA slices Govindu Beauchamp whereas a 1-bit XNOR
gate only costs a single slice
Exploiting Filter Repetitions When using a ConvNet architecture with binary weights the number
of unique filters is bounded by the filter size For example in our implementation we use filters of
size so the maximum number of unique 2D filters is 29 Since we now have binary
filters many 2D filters of size repeat themselves By using dedicated hardware/software we
can apply only the unique 2D filters on each feature map and sum the results to receive each 3D
filter?s convolutional result For example in our ConvNet architecture trained on the CIFAR-10
benchmark there are only unique filters per layer on average Hence we can reduce the number
of the XNOR-popcount operations by
Seven Times Faster on GPU at Run-Time
It is possible to speed up GPU implementations of BNNs by using a method sometimes called
SIMD single instruction multiple data within a register SWAR The basic idea of SWAR is to
concatenate groups of 32 binary variables into registers and thus obtain a 32-times speed-up
on bitwise operations XNOR Using SWAR it is possible to evaluate 32 connections with only
instructions
a1 popcount(xnor(a32b
w1
where a1 is the resulting weighted sum and
and are the concatenated inputs and weights
Those instructions accumulation popcount xnor take clock cycles on recent
Nvidia GPUs and if they were to become a fused instruction it would only take a single clock cycle
Consequently we obtain a theoretical Nvidia GPU speed-up of factor of In practice this
speed-up is quite easy to obtain as the memory bandwidth to computation ratio is also increased by
times
In order to validate those theoretical results we
programed two GPU kernels
Figure The first three columns represent the
time it takes to perform a The first kernel baseline is an unoptimized nary matrix multiplication on a Nvidia
matrix multiplication kernel
GPU depending on which kernel is used We
The second kernel XNOR is nearly identical can see that our XNOR kernel is 23 times faster
to the baseline kernel except that it uses the than our baseline kernel and times faster than
cuBLAS The next three columns represent the
SWAR method as in Equation
time it takes to run the MLP from Section on the
The two GPU kernels return identical outputs full MNIST test set As MNIST?s images are not
when their inputs are constrained to or binary the first layer?s computations are always
but not otherwise The XNOR kernel is about performed by the baseline kernel The last three
23 times faster than the baseline kernel and columns show that the MLP accuracy does not
times faster than cuBLAS as shown in Figure depend on which kernel is used
Last but not least the MLP from Section runs
times faster with the XNOR kernel than with
the baseline kernel without suffering any loss
in classification accuracy Figure
Discussion and Related Work
Until recently the use of extremely lowprecision networks binary in the extreme case
was believed to be highly destructive to the network performance Courbariaux
Soudry and Cheng
proved the contrary by showing that good performance could be achieved even if all neurons
and weights are binarized to This was done
using Expectation BackPropagation a
variational Bayesian approach which infers networks with binary weights and neurons by updating the posterior distributions over the weights
These distributions are updated by differentiating their parameters mean values via the back
propagation algorithm Esser implemented a fully binary network at run time using
a very similar approach to EBP showing significant improvement in energy efficiency The drawback
of EBP is that the binarized parameters are only used during inference
The probabilistic idea behind EBP was extended in the BinaryConnect algorithm of Courbariaux
In BinaryConnect the real-valued version of the weights is saved and used as a key

<<----------------------------------------------------------------------------------------------------------------------->>

title: 6310-phased-lstm-accelerating-recurrent-network-training-for-long-or-event-based-sequences.pdf

Phased LSTM Accelerating Recurrent Network
Training for Long or Event-based Sequences
Daniel Neil Michael Pfeiffer and Shih-Chii Liu
Institute of Neuroinformatics
University of Zurich and ETH Zurich
Zurich Switzerland
dneil pfeiffer shih}@ini.uzh.ch
Abstract
Recurrent Neural Networks RNNs have become the state-of-the-art choice for
extracting patterns from temporal sequences However current RNN models are
ill-suited to process irregularly sampled data triggered by events generated in
continuous time by sensors or other neurons Such data can occur for example
when the input comes from novel event-driven artificial sensors that generate
sparse asynchronous streams of events or from multiple conventional sensors with
different update intervals In this work we introduce the Phased LSTM model
which extends the LSTM unit by adding a new time gate This gate is controlled
by a parametrized oscillation with a frequency range that produces updates of the
memory cell only during a small percentage of the cycle Even with the sparse
updates imposed by the oscillation the Phased LSTM network achieves faster
convergence than regular LSTMs on tasks which require learning of long sequences
The model naturally integrates inputs from sensors of arbitrary sampling rates
thereby opening new areas of investigation for processing asynchronous sensory
events that carry timing information It also greatly improves the performance of
LSTMs in standard RNN applications and does so with an order-of-magnitude
fewer computes at runtime
Introduction
Interest in recurrent neural networks RNNs has greatly increased in recent years since larger
training databases more powerful computing resources and better training algorithms have enabled
breakthroughs in both processing and modeling of temporal sequences Applications include speech
recognition natural language processing and attention-based models for structured
prediction RNNs are attractive because they equip neural networks with memories and
the introduction of gating units such as LSTM and GRU has greatly helped in making the
learning of these networks manageable RNNs are typically modeled as discrete-time dynamical
systems thereby implicitly assuming a constant sampling rate of input signals which also becomes
the update frequency of recurrent and feed-forward units Although early work such as
has realized the resulting limitations and suggested continuous-time dynamical systems approaches
towards RNNs the great majority of modern RNN implementations uses fixed time steps
Although fixed time steps are perfectly suitable for many RNN applications there are several
important scenarios in which constant update rates impose constraints that affect the precision and
efficiency of RNNs Many real-world tasks for autonomous vehicles or robots need to integrate input
from a variety of sensors for vision audition distance measurements or gyroscopes Each sensor
may have its own data sampling rate and short time steps are necessary to deal with sensors with
high sampling frequencies However this leads to an unnecessarily higher computational load and
Conference on Neural Information Processing Systems NIPS Barcelona Spain
Input it
Gate
Output Gate ot
Output
Gate
it Input
Gate
ct
ht
c~t
ct
kt
Forget Gate ft
Forget Gate ft
ot
kt
ht
Figure Model architecture Standard LSTM model Phased LSTM model with time gate kt
controlled by timestamp In the Phased LSTM formulation the cell value ct and the hidden output
ht can only be updated during an open phase otherwise the previous values are maintained
power consumption so that all units in the network can be updated with one time step An interesting
new application area is processing of event-based sensors which are data-driven and record stimulus
changes in the world with short latencies and accurate timing Processing the asynchronous outputs of
such sensors with time-stepped models would require high update frequencies thereby counteracting
the potential power savings of event-based sensors And finally there is an interest coming from
computational neuroscience since brains can be viewed loosely as very large RNNs However
biological neurons communicate with spikes and therefore perform asynchronous event-triggered
updates in continuous time This work presents a novel RNN model which can process inputs sampled
at asynchronous times and is described further in the following sections
Model Description
Long short-term memory LSTM units are an important ingredient for modern deep
RNN architectures We first define their update equations in the commonly-used version from
it
ft
ct
ot
ht
Wxi Whi wci bi
Wxf Whf wcf bf
ft it Wxc Whc bc
Wxo Who wco ct bo
ot ct
The main difference to classical RNNs is the use of the gating functions it ft ot which represent
the input forget and output gate at time respectively ct is the cell activation vector whereas
and ht represent the input feature vector and the hidden output vector respectively The gates use the
typical sigmoidal nonlinearities and tanh nonlinearities and with weight parameters
Whi Whf Who Wxi Wxf and Wxo which connect the different inputs and gates with the memory
cells and outputs as well as biases bi bf and bo The cell state ct itself is updated with a fraction of
the previous cell state that is controlled by ft and a new input state created from the element-wise
Hadamard product denoted by of it and the output of the cell state nonlinearity Optional
peephole connection weights wci wcf wco further influence the operation of the input forget
and output gates
The Phased LSTM model extends the LSTM model by adding a new time gate kt The
opening and closing of this gate is controlled by an independent rhythmic oscillation specified by
three parameters updates to the cell state ct and ht are permitted only when the gate is open The
first parameter controls the real-time period of the oscillation The second ron controls the ratio
of the duration of the open phase to the full period The third controls the phase shift of the
oscillation to each Phased LSTM cell All parameters can be learned during the training process
Though other variants are possible we propose here a particularly successful linearized formulation
Output
Output
Output
Layer
Layer
Layer
Input
Layer
Layer
Layer
Input
kt Openness
closed
ct State
open
Input
Input
tj
Time
Figure Diagram of Phased LSTM behaviour Top The rhythmic oscillations to the time gates of
different neurons the period and the phase shift is shown for the lowest neuron The parameter
ron is the ratio of the open period to the total period Bottom Note that in a multilayer scenario
the timestamp is distributed to all layers which are updated at the same time point Illustration of
Phased LSTM operation A simple linearly increasing function is used as an input The time gate
kt of each neuron has a different identical phase shift and an open ratio ron of Note that
the input top panel flows through the time gate kt middle panel to be held as the new cell state ct
bottom panel only when kt is open
of the time gate with analogy to the rectified linear unit that propagates gradients well
if ron
ron
mod
kt
if ron ron
on
otherwise
is an auxiliary variable which represents the phase inside the rhythmic cycle The gate kt has three
phases in the first two phases the openness of the gate rises from to first phase
and drops from to second phase During the third phase the gate is closed and the previous cell
state is maintained The leak with rate is active in the closed phase and plays a similar role as the
leak in a parametric leaky rectified linear unit by propagating important gradient information
even when the gate is closed Note that the linear slopes of kt during the open phases of the time gate
allow effective transmission of error gradients
In contrast to traditional RNNs and even sparser variants of RNNs updates in Phased LSTM
can optionally be performed at irregularly sampled time points tj This allows the RNNs to work with
event-driven asynchronously sampled input data We use the shorthand notation cj ctj for cell
states at time tj analogously for other gates and units and let denote the state at the previous
update time We can then rewrite the regular LSTM cell update equations for cj and hj from
and using proposed cell updates cej and hej mediated by the time gate kj
cej fj ij Wxc Whc bc
cj kj cej kj
hj oj cej
hj kj hej kj
A schematic of Phased LSTM with its parameters can be found in accompanied by an
illustration of the relationship between the time the input the time gate kt and the state ct in
One key advantage of this Phased LSTM formulation lies in the rate of memory decay For the simple
task of keeping an initial memory state c0 as long as possible without receiving additional inputs
ij at all time steps tj a standard LSTM with a nearly fully-opened forget gate fj
after update steps would contain
cn fn c0
18 22 24 26 28
Time
Accuracy at 70 Epochs
18 22 24 26 28
Time
90
Phased LSTM
BN LSTM
LSTM
70
18 22 24 26 28
Time
High
Standard resolution Async
sampling sampling sampling
Figure Frequency discrimination task The network is trained to discriminate waves of different
frequency sets shown in blue and gray every circle is an input point Standard condition the
data is regularly sampled every ms High resolution sampling condition new input points
are gathered every Asynchronous sampling condition new input points are presented at
intervals of ms to ms The accuracy of Phased LSTM under the three sampling conditions
is maintained but the accuracy of the BN-LSTM and standard LSTM drops significantly in the
sampling conditions and Error bars indicate standard deviation over runs
This means the memory for decays exponentially with every time step Conversely the Phased
LSTM state only decays during the open periods of the time gate but maintains a perfect memory
during its closed phase cj if kt for tj Thus during a single oscillation
period of length the units only update during a duration of ron which will result in substantially
fewer than update steps Because of this cyclic memory Phased LSTM can have much longer and
adjustable memory length via the parameter
The oscillations impose sparse updates of the units therefore substantially decreasing the total number
of updates during network operation During training this sparseness ensures that the gradient is
required to backpropagate through fewer updating timesteps allowing an undecayed gradient to be
backpropagated through time and allowing faster learning convergence Similar to the shielding of
the cell state ct and its gradient by the input gates and forget gates of the LSTM the time gate
prevents external inputs and time steps from dispersing and mixing the gradient of the cell state
Results
In the following sections we investigate the advantages of the Phased LSTM model in a variety
of scenarios that require either precise timing of updates or learning from a long sequence For all
the results presented here the networks were trained with Adam set to default learning rate
parameters using Theano with Lasagne Unless otherwise specified the leak rate was set to
during training and during test The phase shift for each neuron was uniformly
chosen from the interval The parameters and were learned during training while the open
ratio ron was fixed at and not adjusted during training except in the first task to demonstrate
that the model can train successfully while learning all parameters
Frequency Discrimination Task
In this first experiment the network is trained to distinguish two classes of sine waves from different
frequency sets those with a period in a target range and those outside the range
using for the uniform distribution on the interval This
task illustrates the advantages of Phased LSTM since it involves a periodic stimulus and requires
fine timing discrimination The inputs are presented as pairs hy ti where is the amplitude and
the timestamp of the sample from the input sine wave
Figure illustrates the task the blue curves must be separated from the lighter curves based on
the samples shown as circles We evaluate three conditions for sampling the input signals In the
standard condition the sine waves are regularly sampled every ms in the oversampled
90
Phased LSTM
BN LSTM
LSTM
75
70
MSE
Accuracy
85
65
55
45
Epoch
LSTM
PLSTM
PLSTM
PLSTM
PLSTM
Epoch
Figure Accuracy during training for the superimposed frequencies task The Phased LSTM
outperforms both LSTM and BN-LSTM while exhibiting lower variance Shading shows maximum
and minimum over runs while dark lines indicate the mean Mean-squared error over training
on the addition task with an input length of Note that longer periods accelerate learning
convergence
condition the sine waves are regularly sampled every ms resulting in ten times
as many data points Finally in the asynchronously sampled condition samples are
collected at asynchronous times over the duration of the input Additionally the sine waves have
a uniformly drawn random phase shift from all possible shifts random numbers of samples drawn
from a random duration drawn from and a start time drawn from
duration The number of samples in the asynchronous and standard sampling condition is equal
The classes were approximately balanced yielding a chance success rate
Single-layer RNNs are trained on this data each repeated with five random initial seeds We compare
our Phased LSTM configuration to regular LSTM and batch-normalized LSTM which has
found success in certain applications For the regular LSTM and the BN-LSTM the timestamp
is used as an additional input feature dimension for the Phased LSTM the time input controls
the time gates kt The architecture consists of neurons for the LSTM and BN-LSTM and
for the Phased LSTM The oscillation periods of the Phased LSTMs are drawn uniformly in
the exponential space to give a wide variety of applicable frequencies All
other parameters match between models where applicable The default LSTM parameters are given
in the Lasagne Theano implementation and were kept for LSTM BN-LSTM and Phased LSTM
Appropriate gate biasing was investigated but did not resolve the discrepancies between the models
All three networks excel under standard sampling conditions as expected as seen in 3d left
However for the same number of epochs increasing the data sampling by a factor of ten has
devastating effects for both LSTM and BN-LSTM dropping their accuracy down to near chance
middle Presumably if given enough training iterations their accuracies would return to
the normal baseline However for the oversampled condition Phased LSTM actually increases in
accuracy as it receives more information about the underlying waveform Finally if the updates are
not evenly spaced and are instead sampled at asynchronous times even when controlled to have the
same number of points as the standard sampling condition it appears to make the problem rather
challenging for traditional state-of-the-art models right However the Phased LSTM has
no difficulty with the asynchronously sampled data because the time gates kt do not need regular
updates and can be correctly sampled at any continuous time within the period
We extend the previous task by training the same RNN architectures on signals composed of two
sine waves The goal is to distinguish signals composed of sine waves with periods T1
and T2 each with independent phase from signals composed of sine waves with
periods T1 and T2 again with independent
phase Despite being significantly more challenging 4a demonstrates how quickly the Phased
LSTM converges to the correct solution compared to the standard approaches using exactly the same
parameters Additionally the Phased LSTM appears to exhibit very low variance during training
Time
Figure N-MNIST experiment Sketch of digit movement seen by the image sensor
Frame-based representation of an digit from the N-MNIST dataset obtained by integrating all
input spikes for each pixel Spatio-temporal representation of the digit presented in three saccades
as in Note that this representation shows the digit more clearly than the blurred frame-based one
Adding Task
To investigate how introducing time gates helps learning when long memory is required we revisit
an original LSTM task called the adding task In this task a sequence of random numbers
is presented along with an indicator input stream When there is a in the indicator input stream
the presented value should be ignored a indicates that the value should be added At the end of
presentation the network produces a sum of all indicated values Unlike the previous tasks there is no
inherent periodicity in the input and it is one of the original tasks that LSTM was designed to solve
well This would seem to work against the advantages of Phased LSTM but using a longer period for
the time gate kt could allow more effective training as a unit opens only a for a few timesteps during
training
In this task a sequence of numbers of length to was drawn from Two
numbers in this stream of numbers are marked for addition one from the first of numbers
drawn with uniform probability and one in the last half drawn with uniform probability producing
a model of a long and noisy stream of data with only few significant points Importantly this should
challenge the Phased LSTM model because there is no inherent periodicity and every timestep could
contain the important marked points
The same network architecture is used as before The period was drawn uniformly in the exponential domain comparing four sampling intervals and
Note that despite different values the total number of LSTM updates remains approximately the same since the overall sparseness is set by ron However a longer period provides
a longer jump through the past timesteps for the gradient during backpropagation-through-time
Moreover we investigate whether the model can learn longer sequences more effectively when longer
periods are used By varying the period the results in 4b show longer accelerates training of
the network to learn much longer sequences faster
N-MNIST Event-Based Visual Recognition
To test performance on real-world asynchronously sampled data we make use of the publiclyavailable N-MNIST dataset for neuromorphic vision The recordings come from an event-based
vision sensor that is sensitive to local temporal contrast changes An event is generated from
a pixel when its local contrast change exceeds a threshold Every event is encoded as a 4-tuple
hx ti with position of the pixel a polarity bit indicating a contrast increase or decrease
and a timestamp indicating the time when the event is generated The recordings consist of events
generated by the vision sensor while the sensor undergoes three saccadic movements facing a static
digit from the MNIST dataset An example of the event responses can be seen in
In previous work using event-based input data the timing information was sometimes
removed and instead a frame-based representation was generated by computing the pixel-wise
event-rate over some time period as shown in Note that the spatio-temporal surface of
Table Accuracy on N-MNIST
CNN
BN-LSTM
Phased LSTM
Accuracy at Epoch
Train/test
Test with
Test with
per neuron
per neuron
LSTM Updates
events in reveals details of the digit much more clearly than in the blurred frame-based
representation.The Phased LSTM allows us to operate directly on such spatio-temporal event streams
Table summarizes classification results for three different network types a CNN trained on framebased representations of N-MNIST digits and two RNNs a BN-LSTM and a Phased LSTM trained
directly on the event streams Regular LSTM is not shown as it was found to perform worse The
CNN was comprised of three alternating layers of kernels of convolution with a leaky ReLU
nonlinearity and max-pooling which were then fully-connected to neurons and finally fullyconnected to the output classes The event pixel address was used to produce a 40-dimensional
embedding via a learned embedding matrix and combined with the polarity to produce the input
Therefore the network architecture was for the Phased LSTM and for the
BN-LSTM with the time given as an extra input dimension to the BN-LSTM
Table shows that Phased LSTM trains faster than alternative models and achieves much higher
accuracy with a lower variance even within the first epoch of training We further define a factor
which represents the probability that an event is included means all events are included
The RNN models are trained with and again the Phased LSTM achieves slightly higher
performance than the BN-LSTM model When testing with fewer events and more
events without retraining both RNN models perform well and greatly outperform the CNN. This is
because the accumulated statistics of the frame-based input to the CNN change drastically when the
overall spike rates are altered The Phased LSTM RNNs seem to have learned a stable spatio-temporal
surface on the input and are only slightly altered by sampling it more or less frequently
Finally as each neuron of the Phased LSTM only updates about of the time on average
updates are needed in comparison to the updates needed per neuron of the BN-LSTM leading
to an approximate twenty-fold reduction in run time compute cost It is also worth noting that these
results form a new state-of-the-art accuracy for this dataset
Visual-Auditory Sensor Fusion for Lip Reading
Finally we demonstrate the use of Phased LSTM on a task involving sensors with different sampling
rates Few RNN models ever attempt to merge sensors of different input frequencies although the
sampling rates can vary substantially For this task we use the GRID dataset This corpus contains
video and audio of speakers each uttering sentences composed of a fixed grammar and a
constrained vocabulary of 51 words The data was randomly divided into a train-test set
An OpenCV implementation of a face detector was used on the video stream to extract the face
which was then resized to grayscale pixels The goal here is to obtain a model that can use
audio alone video alone or both inputs to robustly classify the sentence However since the audio
alone is sufficient to achieve greater than accuracy sensor modalities were randomly masked to
zero during training to encourage robustness towards sensory noise and loss
The network architecture first separately processes video and audio data before merging them in
two RNN layers that receive both modalities The video stream uses three alternating layers of
kernels of convolution and subsampling to reduce the input of to which is
then used as the input to recurrent units The audio stream connects the 39-dimensional MFCCs
MFCCs with first and second derivatives to recurrent units Both streams converge into
the Merged-1 layer with recurrent units and is connected to a second hidden layer with
recurrent units named Merged-2 The output of the Merged-2 layer is fully-connected to 51 output
nodes which represent the vocabulary of GRID For the Phased LSTM network all recurrent units
are Phased LSTM units
Low Res.
Loss
MFCCs
Video
Frames
Phased LSTM
BN LSTM
LSTM
Video
PLSTM
Merged-1
PLSTM
Merged-2
PLSTM
Time
35
High Res.
Loss
Audio
PLSTM
MFCC
kj Openness
Inputs
Time
Time
Epoch
Figure Lip reading experiment Inputs and openness of time gates for the lip reading experiment
Note that the 25fps video frame rate is a multiple of the audio input frequency Hz). Phased
LSTM timing parameters are configured to align to the sampling time of their inputs Example
input of video top and audio bottom Test loss using the video stream alone Video frame rate
is Top low resolution condition MFCCs computed every with a network update every
ms Bottom high resolution condition MFCCs every ms with a network update every ms
In the audio and video Phased LSTM layers we manually align the open periods of the time gates
to the sampling times of the inputs and disable learning of the and parameters
This prevents presenting zeros or artificial interpolations to the network when data is not present
In the merged layers however the parameters of the time gate are learned with the period of the
first merged layer drawn from and the second from 6b shows a
visualization of one frame of video and the complete duration of an audio sample
During evaluation all networks achieve greater than accuracy on audio-only and combined
audio-video inputs However video-only evaluation with an audio-video capable network proved
the most challenging so the results in 6c focus on these results though result rankings are
representative of all conditions Two differently-sampled versions of the data were used In the first
low resolution version top the sampling rate of the MFCCs was matched to the sampling
rate of the fps video In the second high-resolution condition the sampling rate was set to the
more common value of Hz sampling frequency bottom and shown in The
higher audio sampling rate did not increase accuracy but allows for a faster latency instead of
The Phased LSTM again converges substantially faster than both LSTM and batch-normalized
LSTM The peak accuracy of compares favorably against lipreading-focused state-of-the-art
approaches while avoiding manually-crafted features
Discussion
The Phased LSTM has many surprising advantages With its rhythmic periodicity it acts like a
learnable gated Fourier transform on its input permitting very fine timing discrimination Alternatively the rhythmic periodicity can be viewed as a kind of persistent dropout that preserves state
enhancing model diversity The rhythmic inactivation can even be viewed as a shortcut to the past
for gradient backpropagation accelerating training The presented results support these interpretations demonstrating the ability to discriminate rhythmic signals and to learn long memory traces
Importantly in all experiments Phased LSTM converges more quickly and theoretically requires
only of the computes at runtime while often improving in accuracy compared to standard LSTM
The presented methods can also easily be extended to GRUs and it is likely that even simpler
models such as ones that use a square-wave-like oscillation will perform well thereby making even
more efficient and encouraging alternative Phased LSTM formulations An inspiration for using
oscillations in recurrent networks comes from computational neuroscience where rhythms have
been shown to play important roles for synchronization and plasticity Phased LSTMs were
not designed as biologically plausible models but may help explain some of the advantages and
robustness of learning in large spiking recurrent networks

<<----------------------------------------------------------------------------------------------------------------------->>

title: 6215-on-multiplicative-integration-with-recurrent-neural-networks.pdf

On Multiplicative Integration with
Recurrent Neural Networks
Yuhuai Saizheng Zhang2 Ying Zhang2 Yoshua Bengio2,4 and Ruslan Salakhutdinov3,4
University of Toronto MILA Universit de Montr?al Carnegie Mellon University CIFAR
ywu@cs.toronto.edu,2 firstname.lastname}@umontreal.ca,rsalakhu@cs.cmu.edu
Abstract
We introduce a general and simple structural design called Multiplicative Integration to improve recurrent neural networks RNNs MI changes the way in
which information from difference sources flows and is integrated in the computational building block of an RNN while introducing almost no extra parameters
The new structure can be easily embedded into many popular RNN models including LSTMs and GRUs We empirically analyze its learning behaviour and conduct
evaluations on several tasks using different RNN models Our experimental results
demonstrate that Multiplicative Integration can provide a substantial performance
boost over many of the existing RNN models
Introduction
Recently there has been a resurgence of new structural designs for recurrent neural networks RNNs
Most of these designs are derived from popular structures including vanilla RNNs Long
Short Term Memory networks LSTMs and Gated Recurrent Units GRUs Despite of their
varying characteristics most of them share a common computational building block described by the
following equation
Uz
where Rn and Rm are state vectors coming from different information sources Rd?n
and Rd?m are state-to-state transition matrices and is a bias vector This computational
building block serves as a combinator for integrating information flow from the and by a sum
operation followed by a nonlinearity We refer to it as the additive building block Additive
building blocks are widely implemented in various state computations in RNNs hidden state
computations for vanilla-RNNs gate/cell computations of LSTMs and GRUs
In this work we propose an alternative design for constructing the computational building block by
changing the procedure of information integration Specifically instead of utilizing sum operation
we propose to use the Hadamard product to fuse Wx and Uz
Uz
The result of this modification changes the RNN from first order to second order while introducing
no extra parameters We call this kind of information integration design a form of Multiplicative
Integration The effect of multiplication naturally results in a gating type structure in which Wx
and Uz are the gates of each other More specifically one can think of the state-to-state computation
Uz where for example represents the previous state as dynamically rescaled by Wx where
for example represents the input Such rescaling does not exist in the additive building block in
which Uz is independent of This relatively simple modification brings about advantages over the
additive building block as it alters RNN?s gradient properties which we discuss in detail in the next
section as well as verify through extensive experiments
Equal contribution
Conference on Neural Information Processing Systems NIPS Barcelona Spain
In the following sections we first introduce a general formulation of Multiplicative Integration We
then compare it to the additive building block on several sequence learning tasks including character
level language modelling speech recognition large scale sentence representation learning using a
Skip-Thought model and teaching a machine to read and comprehend for a question answering
task The experimental results together with several existing state-of-the-art models show that
various RNN structures including vanilla RNNs LSTMs and GRUs equipped with Multiplicative
Integration provide better generalization and easier optimization Its main advantages include it
enjoys better gradient properties due to the gating effect Most of the hidden units are non-saturated
the general formulation of Multiplicative Integration naturally includes the regular additive
building block as a special case and introduces almost no extra parameters compared to the additive
building block and it is a drop-in replacement for the additive building block in most of the
popular RNN models including LSTMs and GRUs It can also be combined with other RNN training
techniques such as Recurrent Batch Normalization We further discuss its relationship to existing
models including Hidden Markov Models HMMs second order RNNs and Multiplicative
RNNs
Structure Description and Analysis
General Formulation of Multiplicative Integration
The key idea behind Multiplicative Integration is to integrate different information flows Wx and Uz
by the Hadamard product A more general formulation of Multiplicative Integration includes
two more bias vectors and added to Wx and Uz
Uz
where are bias vectors Notice that such formulation contains the first order terms as
in a additive building block Uht?1 Wxt In order to make the Multiplicative
Integration more flexible we introduce another bias vector Rd to gate2 the term Wx Uz
obtaining the following formulation
Wx Uz Uz Wx
Note that the number of parameters of the Multiplicative Integration is about the same as that of the
additive building block since the number of new parameters and are negligible compared
to total number of parameters Also Multiplicative Integration can be easily extended to LSTMs
and GRUs3 that adopt vanilla building blocks for computing gates and output states where one can
directly replace them with the Multiplicative Integration More generally in any kind of structure
where information flows are involved residual networks one can implement
pairwise Multiplicative Integration for integrating all information sources
Gradient Properties
The Multiplicative Integration has different gradient properties compared to the additive building
block For clarity of presentation we first look at vanilla-RNN and RNN with Multiplicative
Integration embedded referred to as MI-RNN That is ht Wxt Uht?1 versus
ht
ht Wxt Uht?1 In a vanilla-RNN the gradient
can be computed as follows
t?n
ht
UT diag(?0k
ht?n
where
Wxk Uhk?1 The equation above shows that the gradient flow through time
heavily depends on the hidden-to-hidden matrix but and xk appear to play a limited role they
ht
only come in the derivative of mixed with Uhk?1 On the other hand the gradient
of a
t?n
MI-RNN is4
ht
UT diag(Wxk diag(?0k
ht?n
If the Multiplicative Integration will degenerate to the vanilla additive building block
See exact formulations in the Appendix
Here we adopt the simplest formulation of Multiplicative Integration for illustration In the more general
case diag(Wxk in will become diag Wxk
where Wxk Uhk?1 By looking at the gradient we see that the matrix and
the current input xk is directly involved in the gradient computation by gating the matrix hence
more capable of altering the updates of the learning system As we show in our experiments with
Wxk directly gating the gradient the vanishing/exploding problem is alleviated Wxk dynamically
reconciles making the gradient propagation easier compared to the regular RNNs For LSTMs
and GRUs with Multiplicative Integration the gradient propagation properties are more complicated
But in principle the benefits of the gating effect also persists in these models
Experiments
In all of our experiments we use the general form of Multiplicative Integration for any hidden
state/gate computations unless otherwise specified
Exploratory Experiments
To further understand the functionality of Multiplicative Integration we take a simple RNN for
illustration and perform several exploratory experiments on the character level language modeling
task using Penn-Treebank dataset following the data partition in The length of the
training sequence is All models have a single hidden layer of size and we use Adam
optimization algorithm with learning rate Weights are initialized to samples drawn from
Performance is evaluated by the bits-per-character BPC metric which is
log2 of perplexity
Gradient Properties
To analyze the gradient flow of the model we divide the gradient in into two parts the
gated matrix products UT diag(Wxk and the derivative of the nonlinearity We separately
analyze the properties of each term compared to the additive building block We first focus on the
gating effect brought by diag(Wxk In order to separate out the effect of nonlinearity we chose
to be the identity map hence both vanilla-RNN and MI-RNN reduce to linear models referred to as
lin-RNN and lin-MI-RNN
For each model we monitor the log-L2-norm of the gradient log averaged over the
training set after every training epoch where ht is the hidden state at time step and is the
negative log-likelihood of the single character prediction at the final time step Figure
shows the evolution of the gradient norms for small as they better reflect the gradient
propagation behaviour Observe that the norms of lin-MI-RNN orange increase rapidly and soon
exceed the corresponding norms of lin-RNN by a large margin The norms of lin-RNN stay close to
zero and their changes over time are almost negligible This observation implies that with
the help of diag(Wxk term the gradient vanishing of lin-MI-RNN can be alleviated compared to
lin-RNN The final test BPC bits-per-character of lin-MI-RNN is which is comparable to a
vanilla-RNN with stabilizing regularizer while lin-RNN performs rather poorly achieving a test
BPC of over
Next we look into the nonlinearity We chose tanh for both vanilla-RNN and MI-RNN
Figure and shows a comparison of histograms of hidden activations over all time steps on
the validation set after training Interestingly in for vanilla-RNN most activations are saturated
with values around whereas in for MI-RNN most activations are non-saturated with values
around This has a direct consequence in gradient propagation non-saturated activations imply
that diag(?0k for tanh which can help gradients propagate whereas saturated activations
imply that diag(?0k resulting in gradients vanishing
Scaling Problem
When adding two numbers at different order of magnitude the smaller one might be negligible for the
sum However when multiplying two numbers the value of the product depends on both regardless
of the scales This principle also applies when comparing Multiplicative Integration to the additive
building blocks In this experiment we test whether Multiplicative Integration is more robust to the
scales of weight values Following the same models as in Section we first calculated the norms
of Wxk and Uhk?1 for both vanilla-RNN and MI-RNN for different after training We found that
in both structures Wxk is a lot smaller than Uhk?1 in magnitude This might be due to the fact that
xk is a one-hot vector making the number of updates for columns of be smaller than U. As a
result in vanilla-RNN the pre-activation term Wxk Uhk?1 is largely controlled by the value of
Uhk?1 while Wxk becomes rather small In MI-RNN on the other hand the pre-activation term
Wxk Uhk?1 still depends on the values of both Wxk and Uhk?1 due to multiplication
validation BPC
lin-RNN
lin-RNN
lin-RNN
lin-MI-RNN
lin-MI-RNN
lin-MI-RNN
number of epochs
activation values of h_t
vanilla-RNN
MI-RNN-simple
MI-RNN-general
normalized fequency
normalized fequency
log||dC
number of epochs
activation values of h_t
Figure Curves of log-L2-norm of gradients for lin-RNN blue and lin-MI-RNN orange Time gradually
changes from Validation BPC curves for vanilla-RNN MI-RNN-simple using and MIRNN-general using Histogram of vanilla-RNN?s hidden activations over the validation set most
activations are saturated Histogram of MI-RNN?s hidden activations over the validation set most activations
are not saturated
We next tried different initialization of and to test their sensitivities to the scaling For each
model we fix the initialization of to and initialize to uniform[?rW rW
where rW varies in Table top left panel shows results As we increase
the scale of performance of the vanilla-RNN improves suggesting that the model is able to
better utilize the input information On the other hand MI-RNN is much more robust to different
initializations where the scaling has almost no effect on the final performance
On different choices of the formulation
In our third experiment we evaluated the performance of different computational building blocks
which are vanilla-RNN MI-RNN-simple and MI-RNN-general)5 From the
validation curves in Figure we see that both MI-RNN simple and MI-RNN-general yield much
better performance compared to vanilla-RNN and MI-RNN-general has a faster convergence speed
compared to MI-RNN-simple We also compared our results to the previously published models
in Table bottom left panel where MI-RNN-general achieves a test BPC of which is to our
knowledge the best result for RNNs on this task without complex gating/cell mechanisms
Character Level Language Modeling
In addition to the Penn-Treebank dataset we also perform character level language modeling on two
larger datasets text86 and Hutter Challenge Wikipedia7 Both of them contain characters from
Wikipedia while text8 has an alphabet size of 27 and Hutter Challenge Wikipedia has an alphabet
size of For both datasets we follow the training protocols in and respectively We use
Adam for optimization with the starting learning rate grid-searched in If the
validation BPC bits-per-character does not decrease for epochs we half the learning rate
We implemented Multiplicative Integration on both vanilla-RNN and LSTM referred to as MIRNN and MI-LSTM The results for the text8 dataset are shown in Table bottom middle panel
All five models including some of the previously published models have the same number of
We perform hyper-parameter search for the initialization of in MI-RNN-general
http://mattmahoney.net/dc/textdata
http://prize.hutter1.net
rW
std
RNN
MI-RNN
WSJ Corpus
CER WER
DRNN+CTCbeamsearch
Encoder-Decoder
LSTM+CTCbeamsearch
Eesen
LSTM+CTC+WFST ours
MI-LSTM+CTC+WFST ours
Penn-Treebank
BPC
text8
BPC
RNN
HF-MRNN
RNN+stabalization
MI-RNN ours
linear MI-RNN ours
RNN+smoothReLu
HF-MRNN
MI-RNN ours
LSTM ours
MI-LSTM(ours
HutterWikipedia
BPC
stacked-LSTM
GF-LSTM
grid-LSTM
MI-LSTM ours
Table Top test BPCs and the standard deviation of models with different scales of weight initializations Top
right test CERs and WERs on WSJ corpus Bottom left test BPCs on character level Penn-Treebank dataset
Bottom middle test BPCs on character level text8 dataset Bottom right test BPCs on character level Hutter
Prize Wikipedia dataset
parameters For RNNs without complex gating/cell mechanisms the first three results our
MI-RNN with initialized as performs the best our MI-LSTM with
initialized as outperforms all other models by a large margin8
On Hutter Challenge Wikipedia dataset we compare our MI-LSTM single layer with unit
with initialized as to the previous stacked LSTM layers
GF-LSTM layers and grid-LSTM layers Table bottom
right panel shows results Despite the simple structure compared to the sophisticated connection
designs in GF-LSTM and grid-LSTM our MI-LSTM outperforms all other models and achieves the
new state-of-the-art on this task
Speech Recognition
We next evaluate our models on Wall Street Journal WSJ corpus available as LDC corpus
LDC93S6B and where we use the full 81 hour set for training set for
validation and set for test We follow the same data preparation process and model setting
as in and we use 59 characters as the targets for the acoustic modelling Decoding is done with
the CTC based weighted finite-state transducers WFSTs as proposed by
Our model referred to as MI-LSTM+CTC+WFST consists of bidirectional MI-LSTM layers each with units for each direction CTC is performed on top to resolve the alignment
issue in speech transcription For comparison we also train a baseline model referred to as
LSTM+CTC+WFST with the same size but using vanilla LSTM Adam with learning rate
is used for optimization and Gaussian weight noise with zero mean and standard deviation
is injected for regularization We evaluate our models on the character error rate CER without
language model and the word error rate WER with extended trigram language model
Table top right panel shows that MI-LSTM+CTC+WFST achieves quite good results on both CER
and WER compared to recent works and it has a clear improvement over the baseline model Note
that we did not conduct a careful hyper-parameter search on this task hence one could potentially
obtain better results with better decoding schemes and regularization techniques
Learning Skip-Thought Vectors
Next we evaluate our Multiplicative Integration on the Skip-Thought model of Skip-Thought is
an encoder-decoder model that attempts to learn generic distributed sentence representations The
model produces sentence representation that are robust and perform well in practice as it achieves
excellent results across many different NLP tasks The model was trained on the BookCorpus dataset
that consists of books with sentences Not surprisingly a single pass through
reports better results but they use much larger models which is not directly comparable
Semantic-Relatedness
MSE
Paraphrase detection Acc F1
uni-skip
bi-skip
combine-skip
uni-skip
bi-skip
combine-skip
uni-skip ours
MI-uni-skip ours
uni-skip ours
MI-uni-skip ours
Classification
MR CR SUBJ MPQA
uni-skip
bi-skip
combine-skip
uni-skip ours
MI-uni-skip ours
Attentive Reader
Val. Err.
LSTM
BN-LSTM
BN-everywhere
LSTM ours
MI-LSTM ours
MI-LSTM+BN ours
MI-LSTM+BN-everywhere ours
Table Top left skip-thought+MI on Semantic-Relatedness task Top Right skip-thought+MI on Paraphrase
Detection task Bottom left skip-thought+MI on four different classification tasks Bottom right Multiplicative
Integration with batch normalization on Teaching Machines to Read and Comprehend task
the training data can take up to a week on a high-end GPU as reported in Such training
speed largely limits one to perform careful hyper-parameter search However with Multiplicative
Integration not only the training time is shortened by a factor of two but the final performance is
also significantly improved
We exactly follow the authors Theano implementation of the skip-thought model9 Encoder and
decoder are single-layer GRUs with hidden-layer size of all recurrent matrices adopt orthogonal
initialization while non-recurrent weights are initialized from uniform distribution Adam is used
for optimization We implemented Multiplicative Integration only for the encoder GRU embedding
MI into decoder did not provide any substantial gains We refer our model as MI-uni-skip with
initialized as We also train a baseline model with the same size referred
to as uni-skip(ours which essentially reproduces the original model of
During the course of training we evaluated the skip-thought vectors on the semantic relatedness
task using SICK dataset every updates for both MI-uni-skip and the baseline model each
iteration processes a mini-batch of size The results are shown in Figure Note that MI-uni-skip
significantly outperforms the baseline not only in terms of speed of convergence but also in terms
of final performance At around updates MI-uni-skip already exceeds the best performance
achieved by the baseline which takes about twice the number of updates
We also evaluated both models after one week of training with the best results being reported on six
out of eight tasks reported in semantic relatedness task on SICK dataset paraphrase detection
task on Microsoft Research Paraphrase Corpus and four classification benchmarks movie review
sentiment customer product reviews subjectivity/objectivity classification SUBJ and
opinion polarity MPQA We also compared our results with the results reported on three models in
the original skip-thought paper uni-skip bi-skip combine-skip Uni-skip is the same model as our
baseline bi-skip is a bidirectional model of the same size and combine-skip takes the concatenation
of the vectors from uni-skip and bi-skip to form a dimension vector for task evaluation Table
shows that MI-uni-skip dominates across all the tasks Not only it achieves higher performance
than the baseline model but in many cases it also outperforms the combine-skip model which has
twice the number of dimensions Clearly Multiplicative Integration provides a faster and better way
to train a large-scale Skip-Thought model
Teaching Machines to Read and Comprehend
In our last experiment we show that the use of Multiplicative Integration can be combined with
other techniques for training RNNs and the advantages of using MI still persist Recently
introduced Recurrent Batch-Normalization They evaluated their proposed technique on a uni9
https://github.com/ryankiros/skip-thoughts
MSE
uni-skip ours
MI-uni-skip ours
validation error
number of iterations
LSTM
BN-LSTM
MI-LSTM ours
MI-LSTM+BN ours
number of iterations
Figure MSE curves of uni-skip ours and MI-uni-skip ours on semantic relatedness task on SICK
dataset MI-uni-skip significantly outperforms baseline uni-skip Validation error curves on attentive reader
models There is a clear margin between models with and without MI.
directional Attentive Reader Model for the question answering task using the CNN corpus10 To
test our approach we evaluated the following four models A vanilla LSTM attentive reader model
with a single hidden layer size same as as our baseline referred to as LSTM ours A
multiplicative integration LSTM with a single hidden size referred to as MI-LSTM MILSTM with Batch-Norm referred to as MI-LSTM+BN MI-LSTM with Batch-Norm everywhere
as detailed in referred to as MI-LSTM+BN-everywhere We compared our models to results
reported in referred to as LSTM BN-LSTM and BN-LSTM everywhere
For all MI models were initialized to We follow the experimental
protocol of and use exactly the same settings as theirs except we remove the gradient clipping
for MI-LSTMs Figure 2b shows validation curves of the baseline LSTM MI-LSTM BN-LSTM
and MI-LSTM+BN and the final validation errors of all models are reported in Table bottom right
panel Clearly using Multiplicative Integration results in improved model performance regardless
of whether Batch-Norm is used However the combination of MI and Batch-Norm provides the
best performance and the fastest speed of convergence This shows the general applicability of
Multiplication Integration when combining it with other optimization techniques
Relationship to Previous Models
Relationship to Hidden Markov Models
One can show that under certain constraints MI-RNN is effectively implementing the forward
algorithm of the Hidden Markov Model(HMM A direct mapping can be constructed as follows
for a similar derivation Let Rm?m be the state transition probability matrix with Uij
Pr[ht+1 i|ht Rm?n be the observation probability matrix with Wij Pr[xt
i|ht When is a one-hot vector in many of the language modelling tasks multiplying
it by is effectively choosing a column of the observation matrix Namely if the th entry of
is one then Wxt Pr[xt ht Let h0 be the initial state distribution with h0 Pr[h0 and
ht be the alpha values in the forward algorithm of HMM ht Pr[x1 ht Then
Uht Pr[x1 Thus Wxt+1 Uht Pr[xt+1 Pr[x1
Pr[x1 To exactly implement the forward algorithm using Multiplicative Integration
the matrices and have to be probability matrices and needs to be a one-hot vector The
function needs to be linear and we drop all the bias terms Therefore RNN with Multiplicative
Integration can be seen as a nonlinear extension of HMMs The extra freedom in parameter values
and nonlinearity makes the model more flexible compared to HMMs
Relations to Second Order RNNs and Multiplicative RNNs
MI-RNN is related to the second order RNN and the multiplicative RNN MRNN We first
describe the similarities with these two models
The second order RNN involves a second order term st in a vanilla-RNN where the ith element
st,i is computed by the bilinear form st,i xTt where Rn?m is
Note that used a truncated version of the original dataset in order to save computation
Learning curves and the final result number are obtained by emails correspondence with authors of
https://github.com/cooijmanstim/recurrent-batch-normalization.git
the ith slice of a tensor Rm?n?m Multiplicative Integration also involve a second order term
st Wxt Uht?1 but in our case st,i xTt ui
where and ui are ith row in and and is the ith element of Note that the outer product
ui is a rank-1 matrix The Multiplicative RNN is also a second order RNN but which
Pdiag(Vxt For MI-RNN we can
approximates by a tensor decomposition
also think of the second order term as a tensor decomposition Wxt Uht?1 U(xt
diag(?)diag(Wxt
There are however several differences that make MI a favourable model Simpler Parametrization
MI uses a rank-1 approximation compared to the second order RNNs and a diagonal approximation
compared to Multiplicative RNN. Moreover MI-RNN shares parameters across the first and second
order terms whereas the other two models do not As a result the number of parameters are largely
reduced which makes our model more practical for large scale problems while avoiding overfitting
Easier Optimization In tensor decomposition methods the products of three different low-rank
matrices generally makes it hard to optimize However the optimization problem becomes
easier in MI as discussed in section and General structural design vanilla-RNN design
Multiplicative Integration can be easily embedded in many other RNN structures LSTMs and
GRUs whereas the second order RNN and MRNN present a very specific design for modifying
vanilla-RNNs
Moreover we also compared MI-RNN?s performance to the previous HF-MRNN?s results Multiplicative RNN trained by Hessian-free method in Table bottom left and bottom middle panels on
Penn-Treebank and text8 datasets One can see that MI-RNN outperforms HF-MRNN on both tasks
General Multiplicative Integration
Multiplicative Integration can be viewed as a general way of combining information flows from
two different sources In particular proposed the ladder network that achieves promising
results on semi-supervised learning In their model they combine the lateral connections and the
backward connections via the combinator function by a Hadamard product The performance would
severely degrade without this product as empirically shown by explored neural embedding
approaches in knowledge bases by formulating relations as bilinear and/or linear mapping functions
and compared a variety of embedding models on the link prediction task Surprisingly the best
results among all bilinear functions is the simple weighted Hadamard product They further carefully
compare the multiplicative and additive interactions and show that the multiplicative interaction
dominates the additive one
Conclusion
In this paper we proposed to use Multiplicative Integration a simple Hadamard product to
combine information flow in recurrent neural networks MI can be easily integrated into many popular
RNN models including LSTMs and GRUs while introducing almost no extra parameters Indeed
the implementation of MI requires almost no extra work beyond implementing RNN models We also
show that MI achieves state-of-the-art performance on four different tasks or datasets of varying
sizes and scales We believe that the Multiplicative Integration can become a default building block
for training various types of RNN models
Acknowledgments
The authors acknowledge the following agencies for funding and support NSERC Canada Research
Chairs CIFAR Calcul Quebec Compute Canada Disney research and ONR Grant
The authors thank the developers of Theano and Keras and also thank Jimmy Ba for many
thought-provoking discussions

<<----------------------------------------------------------------------------------------------------------------------->>

title: 6279-natural-parameter-networks-a-class-of-probabilistic-neural-networks.pdf

Natural-Parameter Networks
A Class of Probabilistic Neural Networks
Hao Wang Xingjian Shi Dit-Yan Yeung
Hong Kong University of Science and Technology
hwangaz,xshiab,dyyeung}@cse.ust.hk
Abstract
Neural networks have achieved state-of-the-art performance in various applications Unfortunately in applications where training data is insufficient they are
often prone to overfitting One effective way to alleviate this problem is to exploit
the Bayesian approach by using Bayesian neural networks Another shortcoming of NN is the lack of flexibility to customize different distributions for the
weights and neurons according to the data as is often done in probabilistic graphical models To address these problems we propose a class of probabilistic neural
networks dubbed natural-parameter networks as a novel and lightweight
Bayesian treatment of NN. NPN allows the usage of arbitrary exponential-family
distributions to model the weights and neurons Different from traditional NN
and BNN NPN takes distributions as input and goes through layers of transformation before producing distributions to match the target output distributions As
a Bayesian treatment efficient backpropagation is performed to learn the
natural parameters for the distributions over both the weights and neurons The
output distributions of each layer as byproducts may be used as second-order
representations for the associated tasks such as link prediction Experiments on
real-world datasets show that NPN can achieve state-of-the-art performance
Introduction
Recently neural networks have achieved state-of-the-art performance in various applications
ranging from computer vision to natural language processing However NN trained by
stochastic gradient descent SGD or its variants is known to suffer from overfitting especially
when training data is insufficient Besides overfitting another problem of NN comes from the
underestimated uncertainty which could lead to poor performance in applications like active learning
Bayesian neural networks BNN offer the promise of tackling these problems in a principled way
Early BNN works include methods based on Laplace approximation variational inference
and Monte Carlo sampling but they have not been widely adopted due to their lack of
scalability Some recent advances in this direction seem to shed light on the practical adoption of
BNN. proposed a method based on VI in which a Monte Carlo estimate of a lower bound on the
marginal likelihood is used to infer the weights Recently used an online version of expectation
propagation called probabilistic back propagation for the Bayesian learning of NN
and proposed Bayes by Backprop which can be viewed as an extension of based on
the reparameterization trick More recently an interesting Bayesian treatment called Bayesian
dark knowledge BDK was designed to approximate a teacher network with a simpler student
network based on stochastic gradient Langevin dynamics SGLD
Although these recent methods are more practical than earlier ones several outstanding problems
remain to be addressed most of these methods require sampling either at training time or
at test time incurring much higher cost than a vanilla NN as mentioned in methods
Conference on Neural Information Processing Systems NIPS Barcelona Spain
based on online EP or VI do not involve sampling but they need to compute the predictive density
by integrating out the parameters which is computationally inefficient these methods assume
Gaussian distributions for the weights and neurons allowing no flexibility to customize different
distributions according to the data as is done in probabilistic graphical models
To address the problems we propose natural-parameter networks NPN as a class of probabilistic
neural networks where the input target output weights and neurons can all be modeled by arbitrary
exponential-family distributions Poisson distributions for word counts instead of being limited
to Gaussian distributions Input distributions go through layers of linear and nonlinear transformation
deterministically before producing distributions to match the target output distributions previous
work shows that providing distributions as input by corrupting the data with noise plays the
role of regularization As byproducts output distributions of intermediate layers may be used as
second-order representations for the associated tasks Thanks to the properties of the exponential
family distributions in NPN are defined by the corresponding natural parameters which can
be learned efficiently by backpropagation Unlike NPN explicitly propagates the estimates of
uncertainty back and forth in deep networks This way the uncertainty estimates for each layer of
neurons are readily available for the associated tasks Our experiments show that such information is
helpful when neurons of intermediate layers are used as representations like in autoencoders In
summary our main contributions are
We propose NPN as a class of probabilistic neural networks Our model combines the merits
of NN and PGM in terms of computational efficiency and flexibility to customize the types
of distributions for different types of data
Leveraging the properties of the exponential family some sampling-free backpropagationcompatible algorithms are designed to efficiently learn the distributions over weights by
learning the natural parameters
Unlike most probabilistic NN models NPN obtains the uncertainty of intermediate-layer
neurons as byproducts which provide valuable information to the learned representations
Experiments on real-world datasets show that NPN can achieve state-of-the-art performance
on classification regression and unsupervised representation learning tasks
Natural-Parameter Networks
The exponential family refers to an important class of distributions with useful algebraic properties
Distributions in the exponential family have the form exp where is
the random variable denotes the natural parameters is a vector of sufficient statistics and
is the normalizer For a given type of distributions different choices of lead to different shapes
For example a univariate Gaussian distribution with d)T corresponds to 2d
2d
Motivated by this observation in NPN only the natural parameters need to be learned to model the
distributions over the weights and neurons Consider an NPN which takes a vector random distribution
a multivariate Gaussian distribution as input multiplies it by a matrix random distribution
goes through nonlinear transformation and outputs another distribution Since all three distributions
in the process can be specified by their natural parameters given the types of distributions learning
and prediction of the network can actually operate in the space of natural parameters For example if
we use element-wise factorized gamma distributions for both the weights and neurons the NPN
counterpart of a vanilla network only needs twice the number of free parameters weights and
neurons since there are two natural parameters for each univariate gamma distribution
Notation and Conventions
We use boldface uppercase letters like to denote matrices and boldface lowercase letters like
for vectors Similarly a boldface number or represents a row vector or a matrix with
identical entries In NPN is used to denote the values of neurons in layer before nonlinear
transformation and is for the values after nonlinear transformation As mentioned above NPN
tries to learn distributions over variables rather than variables themselves Hence we use letters
without subscripts and and to denote random variables with corresponding
distributions Subscripts and are used to denote natural parameter pairs such as Wc and Wd
Similarly subscripts and are for mean-variance pairs Note that for clarity many operations used
below are implicitly element-wise for example the square z2 division bz partial derivative
the
gamma function logarithm log factorial and z1 For the data
we set am as Input distributions with as resemble AE?s denoising effect as
input of the network and denotes the output targets labels and word counts In the following
text we drop the subscript and sometimes the superscript for clarity The bracket denotes
concatenation or pairs of vectors
Linear Transformation in NPN
Here we first introduce the linear form of a general NPN. For simplicity we assume distributions
with two natural parameters gamma distributions beta distributions and Gaussian distributions d)T in this section Specifically we have factorized distributions on the weight
matrices Wc Wd i,j p(Wij Wc,ij Wd,ij where the pair Wc,ij Wd,ij is the
corresponding natural parameters For and we assume similar factorized distributions
In a traditional NN the linear transformation follows where is the
output from the previous layer In NN and are deterministic variables while in
NPN they are exponential-family distributions meaning that the result is also a distribution For
convenience of subsequent computation it is desirable to approximate using another exponentialfamily distribution We can do this by matching the mean and variance Specifically after computing
Wm Ws Wc Wd and bm bs bc bd we can get oc and od through
the mean om and variance os of as follows
ad
Ws(l
Wm
am
Wm
Wm
os
where denotes the element-wise product and the bijective function maps the natural paramec+1
ters of a distribution into its mean and variance
d2 in gamma distributions
Similarly we use to denote the inverse transformation Wm Ws bm and bs are the
mean and variance of and obtained from the natural parameters The computed om and
os can then be used to recover oc and od which will subsequently facilitate the feedforward
computation of the nonlinear transformation described in Section
Nonlinear Transformation in NPN
After we obtain the linearly transformed distribution over defined by natural parameters oc and
od an element-wise nonlinear transformation with a well defined inverse function will
be imposed The resulting activation distribution is pa po where po
is the factorized distribution over defined by oc od
Though pa may not be an exponential-family distribution we can approximate it with one
ac ad by matching the first two moments Once the mean am and variance as of pa
are obtained we can compute corresponding natural parameters with approximation
accuracy is sufficient according to preliminary experiments The feedforward computation is
am
po o|oc od as
po o|oc od do a2m ac ad am as
Here the key computational challenge is computing the integrals in Equation Closed-form
solutions are needed for their efficient computation If po o|oc od is a Gaussian distribution closedform solutions exist for common activation functions like tanh(x and details are in
Section Unfortunately this is not the case for other distributions Leveraging the convenient
form of the exponential family we find that it is possible to design activation functions so that the
integrals for non-Gaussian distributions can also be expressed in closed form
Theorem Assume an exponential-family distribution po exp where
the vector u2 uM is the number of natural parameters
If activaR
tion function ui is used the first two moments of po
Table Activation Functions for Exponential-Family Distributions
Distribution
Probability Density Function
Beta Distribution
Rayleigh Distribution
Gamma Distribution
Poisson Distribution
Gaussian Distribution
exp
exp{?dx
cx exp{?c
exp
Activation Function
Support
qx
Nonnegative interger
ReLU tanh and sigmoid
and po dx can be expressed in closed form Here different ui
corresponds to a different set of activation functions and and are constants
and
Proof We first let
The first moment of is
exp ui dx
g(e
exp{e
dx
g(e
g(e
Similarly the second moment can be computed as r2
2rq
A more detailed proof is provided in the supplementary material With Theorem what remains is to
find the constants that make strictly increasing and bounded Table shows some exponentialfamily distributions and their possible activation functions For example in Equation if
am odo
oc for the gamma distribution
In the backpropagation for distributions with two natural parameters the gradient consists of two
as
terms For example
a
a
oc as oc where is the error term of the network
Algorithm Deep Nonlinear NPN
Input Data
number of iterations learning rate number of layers L.
for do
for do
Apply Equation to compute the linear and nonlinear transformation in layer
end for
Compute the error from oc od or ac ad
for do
Compute and Compute and
Wm
Ws
bm
bs
Wc
Wd
bc
bd
end for
Update Wc Wd bc and bd in all layers
end for
Deep Nonlinear NPN
Naturally layers of nonlinear NPN can be stacked to form a deep NPN1 as shown in Algorithm A
deep NPN is in some sense similar to a PGM with a chain structure Unlike PGM in general however
NPN does not need costly inference algorithms like variational inference or Markov chain Monte
Carlo For some chain-structured PGM hidden Markov models efficient inference algorithms
also exist due to their special structure Similarly the Markov property enables NPN to be efficiently
trained in an end-to-end backpropagation learning fashion in the space of natural parameters
PGM is known to be more flexible than NN in the sense that it can choose different distributions to
depict different relationships among variables A major drawback of PGM is its scalability especially
Although the approximation accuracy may decrease as NPN gets deeper during feedforward computation it
can be automatically adjusted according to data during backpropagation
Note that since the first part of Equation and the last part of Equation are canceled out we can
directly use am as without computing ac ad here
when the PGM is deep Different from PGM NN stacks relatively simple computational layers and
learns the parameters using backpropagation which is computationally more efficient than most
algorithms for PGM. NPN has the potential to get the best of both worlds In terms of flexibility
different types of exponential-family distributions can be chosen for the weights and neurons Using
gamma distributions for both the weights and neurons in NPN leads to a deep and nonlinear version
of nonnegative matrix factorization while an NPN with the Bernoulli distribution and sigmoid
activation resembles a Bayesian treatment of sigmoid belief networks If Poisson distributions
are chosen for the neurons NPN becomes a neural analogue of deep Poisson factor analysis
Note that similar to the weight decay in NN we may add the KL divergence between the prior
distributions and the learned distributions on the weights to the error for regularization we use
isotropic Gaussian priors in the experiments In NPN the chosen prior distributions correspond to
priors in Bayesian models and the learned distributions correspond to the approximation of posterior
distributions on weights Note that the generative story assumed here is that weights are sampled
from the prior and then output is generated given all data from these weights
Variants of NPN
In this section we introduce three NPN variants with different properties to demonstrate the flexibility
and effectiveness of NPN. Note that in practice we use a transformed version of the natural parameters
referred to as proxy natural parameters here instead of the original ones for computational efficiency
For example in gamma distributions dc exp(?dx we use proxy natural
parameters during computation rather than the natural parameters
Gamma NPN
The gamma distribution with support over positive values is an important member of the exponential
family The corresponding probability density function is dc exp(?dx with
as its natural parameters we use as proxy natural parameters If we assume
gamma distributions for and an AE formed by NPN becomes a deep and
nonlinear version of nonnegative matrix factorization To see this note that this AE with
activation and zero biases is equivalent to finding a factorization of matrix such that
QL
where denotes the middle-layer neurons and has nonnegative entries
from gamma distributions In this gamma NPN parameters Wc Wd bc and bd can be learned
following Algorithm We detail the algorithm as follows
Linear Transformation Since gamma distributions are assumed here we can use the function
dc dc2 to compute Wm Ws Wc Wd bm bs bc bd and
oc od om os during the probabilistic linear transformation in Equation
Nonlinear Transformation With the proxy natural parameters for the gamma distributions over
the mean am and variance as for the nonlinearly transformed distribution over would
be obtained with Equation Following Theorem closed-form solutions are possible with
and ui where and are constants Using this new
activation function we have Section and of the supplementary material for details on the
function and derivation
ooc
od oc
am po o|oc od v(o)do od
od
od
od 2oc
as r2
oc
od
od
Error With oc
and od we can compute the regression error as the negative log-likelihood
log
log od
oc
log od
where is the observed output corresponding to For classification cross-entropy loss can be used
as E. Following the computation flow above BP can be used to learn Wc Wd bc and bd
Figure Predictive distributions for PBP BDK dropout NN and NPN. The shaded regions correspond to standard deviations The black curve is the data-generating function and blue curves
show the mean of the predictive distributions Red stars are the training data
Gaussian NPN
Different from the gamma distribution which has support over positive values only the Gaussian
distribution also an exponential-family distribution can describe real-valued random variables This
makes it a natural choice for NPN. We refer to this NPN variant with Gaussian distributions over both
the weights and neurons as Gaussian NPN. Details of Algorithm for Gaussian NPN are as follows
Linear Transformation Besides support over real values another property of Gaussian distributions
is that the mean and variance can be used as proxy natural parameters leading to an identity mapping
function which cuts the computation cost We can use this function to compute
Wm Ws Wc Wd bm bs bc bd and oc od om os
during the probabilistic linear transformation in Equation
is used am in
Nonlinear Transformation If the sigmoid activation
Equation would be convolution of Gaussian with sigmoid is approximated by another sigmoid
oc
am o|oc diag(od
od
as o|oc diag(od do a2m
a2m
od
where log and Similar approximation can be applied for
activation tanh(x since tanh(x
If the ReLU activation is used we can use the techniques in to obtain the first
two moments of max(z1 z2 where z1 and z2 are Gaussian random variables Full derivation for
tanh(x and is left to the supplementary material
Error With oc and od in the last layer we can then compute the error as the KL divergence
KL(N oc diag(od ym where is a vector with all entries equal to a small
1T y)T log log For
value Hence the error
od
od
classification tasks cross-entropy loss is used Following the computation flow above BP can be
used to learn Wc Wd bc and bd
Poisson NPN
The Poisson distribution as another member of the exponential family is often used to model counts
counts of words topics or super topics in documents Hence for text modeling it is natural to
assume Poisson distributions for neurons in NPN. Interestingly this design of Poisson NPN can be
seen as a neural analogue of some Poisson factor analysis models
Besides closed-form nonlinear transformation another challenge of Poisson NPN is to map the pair
om os to the single parameter
oc of Poisson distributions According to the central limit theorem
we have oc 8os Section and of the supplementary
material for proofs justifications and detailed derivation of Poisson NPN
Experiments
In this section we evaluate variants of NPN and other state-of-the-art methods on four real-world
datasets We use Matlab with GPU to implement NPN AE variants and the vanilla NN trained
with dropout SGD dropout NN). For other baselines we use the Theano library and MXNet
Table Test Error Rates on MNIST
Method
Error
BDK
BBB
Dropout1
Dropout2
gamma NPN
Gaussian NPN
Table Test Error Rates for Different Size of Training Data
Size
NPN
Dropout
BDK
Toy Regression Task
To gain some insights into NPN we start with a toy 1d regression task so that the predicted mean and
variance can be visualized Following we generate points in one dimension from a uniform
distribution in the interval The target outputs are sampled from the function x3
where We fit the data with the Gaussian NPN BDK and PBP the supplementary
material for detailed hyperparameters Figure shows the predicted mean and variance of NPN
BDK and PBP along with the mean provided by the dropout NN for larger versions of figures please
refer to the end of the supplementary materials As we can see the variance of PBP BDK and NPN
diverges as is farther away from the training data Both NPN?s and BDK?s predictive distributions
are accurate enough to keep most of the x3 curve inside the shaded regions with relatively low
variance An interesting observation is that the training data points become more scattered when
Ideally the variance should start diverging from which is what happens in NPN.
However PBP and BDK are not sensitive enough to capture this dispersion change In another dataset
Boston Housing the root mean square error for PBP BDK and NPN is and
MNIST Classification
The MNIST digit dataset consists of training images and test images All images
are labeled as one of the digits We train the models with images and use images
for validation Networks with a structure of are used for all methods since
works best for the dropout NN denoted as Dropout1 in Table and BDK BDK with a structure of
achieves an error rate of We also try the dropout NN with twice the number
of hidden neurons Dropout2 in Table for fair comparison For BBB we directly quote their results
from We implement BDK and NPN using the same hyperparameters as in whenever possible
Gaussian priors are used for NPN the supplementary material for detailed hyperparameters
Accuracy
As shown in Table BDK and BBB achieve comparable performance
with dropout NN similar to PBP is not included in the comparison
since it supports regression only and gamma NPN slightly outperforms
dropout NN. Gaussian NPN is able to achieve a lower error rate of
Note that BBB with Gaussian priors can only achieve an error
rate of is the result of using Gaussian mixture priors For

<<----------------------------------------------------------------------------------------------------------------------->>

title: 5271-pre-training-of-recurrent-neural-networks-via-linear-autoencoders.pdf

Pre-training of Recurrent Neural Networks via
Linear Autoencoders
Luca Pasa Alessandro Sperduti
Department of Mathematics
University of Padova Italy
pasa,sperduti}@math.unipd.it
Abstract
We propose a pre-training technique for recurrent neural networks based on linear
autoencoder networks for sequences linear dynamical systems modelling the
target sequences We start by giving a closed form solution for the definition of
the optimal weights of a linear autoencoder given a training set of sequences This
solution however is computationally very demanding so we suggest a procedure
to get an approximate solution for a given number of hidden units The weights
obtained for the linear autoencoder are then used as initial weights for the inputto-hidden connections of a recurrent neural network which is then trained on the
desired task Using four well known datasets of sequences of polyphonic music
we show that the proposed pre-training approach is highly effective since it allows
to largely improve the state of the art results on all the considered datasets
Introduction
Recurrent Neural Networks RNN constitute a powerful computational tool for sequences modelling and prediction However training a RNN is not an easy task mainly because of the well
known vanishing gradient problem which makes difficult to learn long-term dependencies Although alternative architectures LSTM networks and more efficient training procedures
such as Hessian Free Optimization have been proposed to circumvent this problem reliable and
effective training of RNNs is still an open problem
The vanishing gradient problem is also an obstacle to Deep Learning In that context
there is a growing evidence that effective learning should be based on relevant and robust internal
representations developed in autonomy by the learning system This is usually achieved in vectorial
spaces by exploiting nonlinear autoencoder networks to learn rich internal representations of input
data which are then used as input to shallow neural classifiers or predictors see for example
The importance to start gradient-based learning from a good initial point in the parameter space has
also been pointed out in Relationship between autoencoder networks and Principal Component
Analysis PCA is well known since late especially in the case of linear hidden units
More recently linear autoencoder networks for structured data have been studied in
where an exact closed-form solution for the weights is given in the case of a number of hidden units
equal to the rank of the full data matrix
In this paper we borrow the conceptual framework presented in to devise an effective pretraining approach based on linear autoencoder networks for sequences to get a good starting point
into the weight space of a RNN which can then be successfully trained even in presence of longterm dependencies Specifically we revise the theoretical approach presented in by giving
a simpler and direct solution to the problem of devising an exact closed-form solution full rank
case for the weights of a linear autoencoder network for sequences highlighting the relationship
between the proposed solution and PCA of the input data ii introducing a new formulation of
the autoencoder learning problem able to return an optimal solution also in the case of a number
of hidden units which is less than the rank of the full data matrix iii proposing a procedure for
approximate learning of the autoencoder network weights under the scenario of very large sequence
datasets More importantly we show how to use the linear autoencoder network solution to derive a
good initial point into a RNN weight space and how the proposed approach is able to return quite
impressive results when applied to prediction tasks involving long sequences of polyphonic music
Linear Autoencoder Networks for Sequences
In it is shown that principal directions of a set of vectors Rk are related to solutions
obtained by training linear autoencoder networks
oi Woutput Whidden
where Whidden Rp?k Woutput Rk?p and the network is trained so to get oi
When considering a temporal sequence of input vectors where is a discrete time
index a linear autoencoder can be defined by considering the coupled linear dynamical systems
Cyt
Axt Byt?1
It should be noticed that eqs and extend the linear transformation defined in eq by
introducing a memory term involving matrix Rp?p In fact is inserted in the right part
of equation to keep track of the input history through time this is done exploiting a state space
representation represents the decoding part of the autoencoder when a state is multiplied
by the observed input at time and state at time are generated Decoding
can then continue from This formulation has been proposed for example in where an
iterative procedure to learn weight matrices A and based on Oja?s rule is presented No proof
of convergence for the proposed procedure is however given More recently an exact closed-form
solution for the weights has been given in the case of a number of hidden units equal to the rank of
the full data matrix full rank case In this section we revise this result In addition we
give an exact solution also for the case in which the number of hidden units is strictly less than the
rank of the full data matrix
The basic idea of is to look for directions of high variance into the state space of the
dynamical linear system Let start by considering a single sequence and
the state vectors of the corresponding induced state sequence collected as rows of a matrix
y2 y3 yn By using the initial condition y0 the null vector and the dynamical
linear system we can rewrite the matrix as
A
xT xT
AT BT
2T
x3
A
A
n?s
where given kn
is a data matrix collecting all the inverted input subsequences
including the whole sequence as rows and is the parameter matrix of the dynamical system
Now we are interested in using a state space of dimension Rp such that as
much information as contained in is preserved We start by factorizing using SVD obtaining
V?UT where Rn?n is an unitary matrix Rn?s is a rectangular diagonal matrix
with nonnegative real numbers on the diagonal with the singular values
and UT Rs?n is a unitary matrix
It is important to notice that columns of UT which correspond to nonzero singular values apart
some mathematical technicalities basically correspond to the principal directions of data PCA.
If the rank of is then only the first elements of the diagonal of are not null and the
above decomposition can be reduced to where Rn?p Rp?p
and Rp?n Now we can observe that I where I is the identity matrix of
dimension since by definition the columns of are orthogonal and by imposing
we can derive optimal matrices A Rp?k and Rp?p for our dynamical system which will
have corresponding state space matrix
Thus if we represent as composed of submatrices Ui each of size the problem
reduces to find matrices A and such that
A
U1
AT BT
U2
2T
A
U3
AT
Un
The reason to impose is to get a state space where the coordinates are uncorrelated so
to diagonalise the empirical sample covariance matrix of the states Please note that in this way
each state row of the matrix corresponds to a row of the data matrix the unrolled
sub)sequence read up to a given time If the rows of were vectors this would correspond to
compute PCA keeping only the fist principal directions
In the following we demonstrate that there exists a solution to the above equation We start
by observing that owns a special structure given where
Rn?k then for Rn
and
Rn the null matrix of size Moreover by singular value decomposition we
have Ui for Using the fact that I and
combining the above equations we get Ui+t Ui Qt for and
where RT
Moreover we have that Un since
nV
Rn Thus eq is satisfied by
Rn
Un Un
U1
A
and It is interesting to note that the original data can be recovered by
computing which can be achieved by running the system
AT
BT
AT
starting from yn
is the matrix defined in eq
BT
Finally it is important to remark that the above construction works not only for a single sequence
but also for a set of sequences of different length For example let consider the two sequences
xa2 xa3 and xb xb Then we have
aT
bT
and
a xa2 xa1
bT
xb1
aT
aT
aT
x3
a
R4
and
which can be collected together to obtain
R2
As a final remark it should be stressed that the above construction only works if is equal to the
rank of In the next section we treat the case in which
Optimal solution for low dimensional autoencoders
and
When rank the solution given above breaks down because
Rn
So the question is whether the proposed solutions for A and still
consequently
hold the best reconstruction error when
In this paper we answer in negative terms to this question by resorting to a new formulation of our
problem where we introduce slack-like matrices Ei Rk?p collecting the
reconstruction errors which need to be minimised
min
Q?Rp?p Ei
subject to
U1
U2
U3
kEi k2F
Un
Un En(p
U2
U3
Notice that the problem above is convex both in the objective function and in the constraints thus
it only has global optimal solutions E?i and from which we can derive AT U1 and
Specifically when Rs,k is in the span of and the optimal
solution is given by E?i and RT
the solution we have already
s,k
described If the optimal solution cannot have E?i However it is not
difficult to devise an iterative procedure to reach the minimum Since in the experimental section we
do not exploit the solution to this problem for reasons that we will explain later here we just sketch
such procedure It helps to observe that given a fixed the optimal solution for Ei is given by
U2 U1 U3 U1 U4 MQ
I
I
where MQ is the pseudo inverse of MQ
I
In general
can be decomposed into a component in the
span of and a component orthogonal to it Notice that cannot be reduced while
and taking
part of the other component can be absorbed into by defining
the new optimal values for are obtained and the process iterated till convergence
Given
Pre-training of Recurrent Neural Networks
Here we define our pre-training procedure for recurrent neural networks with one hidden layer of
units and output units
ot Woutput h(xt RO h(xt Winput Whidden Rp
where Woutput RO?p Whidden Rp?k for a vector Rm
zi
and here we consider the symmetric sigmoid function
The idea is to exploit the hidden state representation obtained by eqs as initial hidden state representation for the RNN described by eqs This is implemented by initialising the weight matrices
Winput and Whidden of by using the matrices that jointly solve eqs and eqs A and
since is function of A and B). Specifically we initialize Winput with A and Whidden with
B. Moreover the use of symmetrical sigmoidal functions which do give a very good approximation
of the identity function around the origin allows a good transferring of the linear dynamics inside
RNN. For what concerns Woutput we initialise it by using the best possible solution the pseudoinverse of times the target matrix which does minimise the output squared error Learning
is then used to introduce nonlinear components that allow to improve the performance of the model
More formally let consider a prediction task where for each sequence sq xq2 xqlq
of length lq in the training set a sequence tq of target vectors is defined a training sequence is given by hsq tq h(xq1 tq1 tq2 xqlq tqlq where tqi RO Given a trainPN
ing set with sequences let define the target matrix RL?O where
lq as
t1 t2 tl1 t1 tlN The input matrix will have size Let be the desired number of hidden units for the recurrent neural network Then the pre-training procedure can be defined as follows compute the linear autoencoder for using principal direc
tions obtaining the optimal matrices A Rp and Rp set Winput A and
Whidden iii run the RNN over the training sequences collecting the hidden activities vec
tors computed using symmetrical sigmoidal functions over time as rows of matrix RL?p
iv set Woutput where is the left pseudoinverse of H.
Computing an approximate solution for large datasets
In real world scenarios the application of our approach may turn difficult because of the size of
the data matrix In fact stable computation of principal directions is usually obtained by SVD decomposition of the data matrix that in typical application domains involves a number of rows
and columns which is easily of the order of hundreds of thousands Unfortunately the computational complexity of SVD decomposition is basically cubic in the smallest of the matrix dimensions
Memory consumption is also an important issue Algorithms for approximate computation of SVD
have been suggested however since for our purposes we just need matrices and
with a predefined number of columns here we present an ad-hoc algorithm for approximate
computation of these matrices Our solution is based on the following four main ideas divide
in slices of size of input at time columns so to exploit SVD decomposition at each slice
separately ii compute approximate and matrices with columns incrementally via truncated
SVD of temporary matrices obtained by concatenating the current approximation of with a new
slice iii compute the SVD decomposition of a temporary matrix via either its kernel or covariance
matrix depending on the smallest between the number of rows and the number of columns of the
temporary matrix iv exploit QR decomposition to compute SVD decomposition
Algorithm shows in pseudo-code the main steps of our procedure It maintains a temporary matrix
which is used to collect incrementally an approximation of the principal subspace of dimension
of Initially line is set equal to the last slices of in a number sufficient to get a number
of columns larger than line Matrices and from the p-truncated SVD decomposition of
are computed line via the procedure described in Algorithm and used to define a
new matrix by concatenation with the last unused slice of When all slices are processed the
current and matrices are returned The procedure described in Algorithm reduces
the computational burden by computing the p-truncated SVD decomposition of the input matrix
via its kernel matrix lines if the number of rows of is no larger than the number of
columns otherwise the covariance matrix is used lines In both cases the p-truncated SVD
decomposition is implemented via QR decomposition by the INDIRECT SVD procedure described in
Algorithm This allows to reduce computation time when large matrices must be processed
Finally matrices and both kernel and covariance matrices have squared singular values of
are returned
We use the strategy to process slices of in reverse order since moving versus columns with larger
indices the rank as well as the norm of slices become smaller and smaller thus giving less and less
contribution to the principal subspace of dimension This should reduce the approximation error
cumulated by dropping the components from to during computation As a final
remark we stress that since we compute an approximate solution for the principal directions of
it makes no much sense to solve the problem given in eq learning will quickly compensate
for the approximations and/or sub-optimality of A and obtained by matrices and returned
by Algorithm Thus these are the matrices we have used for the experiments described in next
section
Algorithm Approximated and with components
function SVF OR IG DATA
nStart dp/ke
Number of starting slices
nSlice columns/k nStart
Number of remaining slices
nSlice columns
Computation of and for starting slices
for in REVERSED(range(nSlice do
Computation of and for remaining slices
end for
return
end function
Algorithm Kernel vs covariance computation Algorithm Truncated SVD by QR
function
function INDIRECT SVD(M
if M.rows columns then
MMT
Vr UT SVD(R
Ssqr INDIRECT SVD(K
QVr
else
MT
Ssqr UT INDIRECT SVD(C
UT UT
return UT
MUT Ssqr2
end function
end if
return Ssqr
end function
Experiments
In order to evaluate our pre-training approach we decided to use the four polyphonic music sequences datasets used in for assessing the prediction abilities of the RNN-RBM model The
prediction task consists in predicting the notes played at time given the sequence of notes played
till time The RNN-RBM model achieves state-of-the-art in such demanding prediction task
As performance measure we adopted the accuracy measure used in and described in Each
dataset is split in training set validation set and test set Statistics on the datasets including largest
sequence length are given in columns of Table Each sequence in the dataset represents a song
having a maximum polyphony of notes average each time step input spans the whole range
of piano from A0 to C8 and it is represented by using 88 binary values
Our pre-training approach PreT-RNN has been assessed by using a different number of hidden
units is set in turn to and epochs of RNN training1 using the
Theano-based stochastic gradient descent software available at
Random initialisation Rnd has also been used for networks with the same number of hidden units
Specifically for networks with hidden units we have evaluated the performance of different
random initialisations Finally in order to verify that the nonlinearity introduced by the RNN is
actually useful to solve the prediction task we have also evaluated the performance of a network
with linear units hidden units initialised with our pre-training procedure
To give an idea of the time performance of pre-training with respect to the training of a RNN in
column of Table we have reported the time in seconds needed to compute pre-training matrices
on Intel
Xeon
CPU with GB and to perform training of a
RNN with for epochs on GPU NVidia Please note that for larger values of
the increase in computation time of pre-training is smaller than the increment in computation time
needed for training a RNN.
Due to early overfitting for the Muse dataset we used epochs
Dataset
Nottingham
Piano-midi.de
MuseData
JSB Chorales
Set
Training
Test
Validation
Training
Test
Validation
Training
Test
Validation
Training
Test
Validation
Samples
Max length
87
77
76
Pre-)Training Time
seconds
epochs
seconds
epochs
seconds
epochs
seconds
epochs
Model
RNN HF
RNN-RBM
PreT-RNN
PreT-Lin250
RNN HF
RNN-RBM
PreT-RNN
PreT-Lin250
RNN HF
RNN-RBM
PreT-RNN
PreT-Lin250
RNN HF
RNN-RBM
PreT-RNN
PreT-Lin250
ACC
Table Datasets statistics including data matrix size for the training set columns computational times in seconds to perform pre-training and training for epochs with column
and accuracy results for state-of-the-art models vs our pre-training approach columns
The acronym HF is used to identify an RNN trained by Hessian Free Optimization
Training and test curves for all the models described above are reported in Figure It is evident that
random initialisation does not allow the RNN to improve its performance in a reasonable amount of
epochs Specifically for random initialisation with Rnd we have reported the average
and range of variation over the different trails different initial points do not change substantially
the performance of RNN. Increasing the number of hidden units allows the RNN to slightly increase
its performance Using pre-training on the other hand allows the RNN to start training from a quite
favourable point as demonstrated by an early sharp improvement of performances Moreover the
more hidden units are used the more the improvement in performance is obtained till overfitting is
observed In particular early overfitting occurs for the Muse dataset It can be noticed that the linear
model Linear reaches performances which are in some cases better than RNN without pre-training
However it is important to notice that while it achieves good results on the training set JSB and
Piano-midi the corresponding performance on the test set is poor showing a clear evidence of overfitting Finally in column of Table we have reported the accuracy obtained after validation on
the number of hidden units and number of epochs for our approaches PreT-RNN and PreT-Lin250
versus the results reported in for RNN also using Hessian Free Optimization and RNN-RBM
In any case the use of pre-training largely improves the performances over standard RNN with
or without Hessian Free Optimization Moreover with the exception of the Nottingham dataset
the proposed approach outperforms the state-of-the-art results achieved by RNN-RBM Large improvements are observed for the Muse and JSB datasets Performance for the Nottingham dataset
is basically equivalent to the one obtained by RNN-RBM For this dataset also the linear model
with pre-training achieves quite good results which seems to suggest that the prediction task for
this dataset is much easier than for the other datasets The linear model outperforms RNN without
pre-training on Nottingham and JSB datasets but shows problems with the Muse dataset
Conclusions
We have proposed a pre-training technique for RNN based on linear autoencoders for sequences
For this kind of autoencoders it is possible to give a closed form solution for the definition of the
optimal weights which however entails the computation of the SVD decomposition of the full
data matrix For large data matrices exact SVD decomposition cannot be achieved so we proposed
a computationally efficient procedure to get an approximation that turned to be effective for our
goals Experimental results for a prediction task on datasets of sequences of polyphonic music
show the usefulness of the proposed pre-training approach since it allows to largely improve the
state of the art results on all the considered datasets by using simple stochastic gradient descend for
learning Even if the results are very encouraging the method needs to be assessed on data from
other application domains Moreover it is interesting to understand whether the analysis performed
in on linear deep networks for vectors can be extended to recurrent architectures for sequences
and in particular to our method
Rnd trials
Linear
Rnd
PreT
PreT
Nottingham
Test Set
Epoch
Accuracy
Accuracy
PreT
PreT
PreT
Nottingham Training
Set
Epoch
Epoch
Piano-Midi.de Training Set
Piano-Midi.de Test Set
Accuracy
Epoch
Epoch
Muse Dataset Training Set
Muse Dataset Test Set
Accuracy
Accuracy
Epoch
Epoch
JSB Chorales Training Set
JSB Chorales Test Set
Accuracy
Rnd
Rnd
Rnd
Accuracy
Accuracy
Accuracy
Epoch
Epoch
Figure Training left column and test right column curves for the assessed approaches on the
four datasets Curves are sampled at each epoch till epoch and at steps of epochs afterwards

<<----------------------------------------------------------------------------------------------------------------------->>

title: 5954-convolutional-networks-on-graphs-for-learning-molecular-fingerprints.pdf

Convolutional Networks on Graphs
for Learning Molecular Fingerprints
David Duvenaud Dougal Maclaurin Jorge Aguilera-Iparraguirre
Rafael G?omez-Bombarelli Timothy Hirzel Al?an Aspuru-Guzik Ryan P. Adams
Harvard University
Abstract
We introduce a convolutional neural network that operates directly on graphs
These networks allow end-to-end learning of prediction pipelines whose inputs
are graphs of arbitrary size and shape The architecture we present generalizes
standard molecular feature extraction methods based on circular fingerprints We
show that these data-driven features are more interpretable and have better predictive performance on a variety of tasks
Introduction
Recent work in materials design used neural networks to predict the properties of novel molecules
by generalizing from examples One difficulty with this task is that the input to the predictor a
molecule can be of arbitrary size and shape Currently most machine learning pipelines can only
handle inputs of a fixed size The current state of the art is to use off-the-shelf fingerprint software
to compute fixed-dimensional feature vectors and use those features as inputs to a fully-connected
deep neural network or other standard machine learning method This formula was followed by
During training the molecular fingerprint vectors were treated as fixed
In this paper we replace the bottom layer of this stack the function that computes molecular
fingerprint vectors with a differentiable neural network whose input is a graph representing the
original molecule In this graph vertices represent individual atoms and edges represent bonds The
lower layers of this network is convolutional in the sense that the same local filter is applied to each
atom and its neighborhood After several such layers a global pooling step combines features from
all the atoms in the molecule
These neural graph fingerprints offer several advantages over fixed fingerprints
Predictive performance By using data adapting to the task at hand machine-optimized
fingerprints can provide substantially better predictive performance than fixed fingerprints
We show that neural graph fingerprints match or beat the predictive performance of standard fingerprints on solubility drug efficacy and organic photovoltaic efficiency datasets
Parsimony Fixed fingerprints must be extremely large to encode all possible substructures
without overlap For example used a fingerprint vector of size after having
removed rarely-occurring features Differentiable fingerprints can be optimized to encode
only relevant features reducing downstream computation and regularization requirements
Interpretability Standard fingerprints encode each possible fragment completely distinctly with no notion of similarity between fragments In contrast each feature of a neural
graph fingerprint can be activated by similar but distinct molecular fragments making the
feature representation more meaningful
Equal contribution
Figure Left A visual representation of the computational graph of both standard circular fingerprints and neural graph fingerprints First a graph is constructed matching the topology of the
molecule being fingerprinted in which nodes represent atoms and edges represent bonds At each
layer information flows between neighbors in the graph Finally each node in the graph turns on
one bit in the fixed-length fingerprint vector Right A more detailed sketch including the bond
information used in each operation
Circular fingerprints
The state of the art in molecular fingerprints are extended-connectivity circular fingerprints
ECFP Circular fingerprints are a refinement of the Morgan algorithm designed
to encode which substructures are present in a molecule in a way that is invariant to atom-relabeling
Circular fingerprints generate each layer?s features by applying a fixed hash function to the concatenated features of the neighborhood in the previous layer The results of these hashes are then treated
as integer indices where a is written to the fingerprint vector at the index given by the feature
vector at each node in the graph Figure 1(left shows a sketch of this computational architecture
Ignoring collisions each index of the fingerprint denotes the presence of a particular substructure
The size of the substructures represented by each index depends on the depth of the network Thus
the number of layers is referred to as the radius of the fingerprints
Circular fingerprints are analogous to convolutional networks in that they apply the same operation
locally everywhere and combine information in a global pooling step
Creating a differentiable fingerprint
The space of possible network architectures is large In the spirit of starting from a known-good configuration we designed a differentiable generalization of circular fingerprints This section describes
our replacement of each discrete operation in circular fingerprints with a differentiable analog
Hashing The purpose of the hash functions applied at each layer of circular fingerprints is to
combine information about each atom and its neighboring substructures This ensures that any
change in a fragment no matter how small will lead to a different fingerprint index being activated
We replace the hash operation with a single layer of a neural network Using a smooth function
allows the activations to be similar when the local molecular structure varies in unimportant ways
Indexing Circular fingerprints use an indexing operation to combine all the nodes feature vectors
into a single fingerprint of the whole molecule Each node sets a single bit of the fingerprint to one
at an index determined by the hash of its feature vector This pooling-like operation converts an
arbitrary-sized graph into a fixed-sized vector For small molecules and a large fingerprint length
the fingerprints are always sparse We use the softmax operation as a differentiable analog of
indexing In essence each atom is asked to classify itself as belonging to a single category The sum
of all these classification label vectors produces the final fingerprint This operation is analogous to
the pooling operation in standard convolutional neural networks
Algorithm Circular fingerprints
Input molecule radius fingerprint
length
Initialize fingerprint vector 0S
for each atom a in molecule
ra
lookup atom features
for to
for each layer
for each atom a in molecule
r1 rN neighbors(a
ra r1 rN concatenate
ra hash(v
hash function
mod(ra convert to index
fi
Write at index
Return binary vector
Algorithm Neural graph fingerprints
Input molecule radius hidden weights
HR
output weights W1 WR
Initialize fingerprint vector 0S
for each atom a in molecule
ra
lookup atom features
for to
for each layer
for each atom a in molecule
r1 rN Pneighbors(a
ra
sum
ri
ra vHLN
smooth function
softmax(ra WL
sparsify
add to fingerprint
Return real-valued vector
Figure Pseudocode of circular fingerprints left and neural graph fingerprints right Differences
are highlighted in blue Every non-differentiable operation is replaced with a differentiable analog
Canonicalization Circular fingerprints are identical regardless of the ordering of atoms in each
neighborhood This invariance is achieved by sorting the neighboring atoms according to their
features and bond features We experimented with this sorting scheme and also with applying the
local feature transform on all possible permutations of the local neighborhood An alternative to
canonicalization is to apply a permutation-invariant function such as summation In the interests of
simplicity and scalability we chose summation
Circular fingerprints can be interpreted as a special case of neural graph fingerprints having large
random weights This is because in the limit of large input weights tanh nonlinearities approach
step functions which when concatenated form a simple hash function Also in the limit of large
input weights the softmax operator approaches a one-hot-coded argmax operator which is analogous to an indexing operation
Algorithms and summarize these two algorithms and highlight their differences Given a fingerprint length and features at each layer the parameters of neural graph fingerprints consist of
a separate output weight matrix of size for each layer as well as a set of hidden-to-hidden
weight matrices of size at each layer one for each possible number of bonds an atom can
have up to in organic molecules
Experiments
We ran two experiments to demonstrate that neural fingerprints with large random weights behave
similarly to circular fingerprints First we examined whether distances between circular fingerprints
were similar to distances between neural fingerprint-based distances Figure left shows a scatterplot of pairwise distances between circular neural fingerprints Fingerprints had length
and were calculated on pairs of molecules from the solubility dataset Distance was measured
using a continuous generalization of the Tanimoto Jaccard similarity measure given by
distance(x
min(xi
max(xi
There is a correlation of between the distances The line of points on the right of the plot
shows that for some pairs of molecules binary ECFP fingerprints have exactly zero overlap
Second we examined the predictive performance of neural fingerprints with large random weights
that of circular fingerprints Figure right shows average predictive performance on the solubility dataset using linear regression on top of fingerprints The performances of both methods
follow similar curves In contrast the performance of neural fingerprints with small random weights
follows a different curve and is substantially better This suggests that even with random weights
the relatively smooth activation of neural fingerprints helps generalization performance
RMSE log Mol/L
Neural fingerprint distances
Neural vs Circular distances
Circular fingerprints
Random conv with large parameters
Random conv with small parameters
Circular fingerprint distances
Fingerprint radius
Figure Left Comparison of pairwise distances between molecules measured using circular fingerprints and neural graph fingerprints with large random weights Right Predictive performance
of circular fingerprints neural graph fingerprints with fixed large random weights green and
neural graph fingerprints with fixed small random weights blue The performance of neural graph
fingerprints with large random weights closely matches the performance of circular fingerprints
Examining learned features
To demonstrate that neural graph fingerprints are interpretable we show substructures which most
activate individual features in a fingerprint vector Each feature of a circular fingerprint vector can
each only be activated by a single fragment of a single radius except for accidental collisions
In contrast neural graph fingerprint features can be activated by variations of the same structure
making them more interpretable and allowing shorter feature vectors
Solubility features Figure shows the fragments that maximally activate the most predictive features of a fingerprint The fingerprint network was trained as inputs to a linear model predicting
solubility as measured in The feature shown in the top row has a positive predictive relationship
with solubility and is most activated by fragments containing a hydrophilic R-OH group a standard
indicator of solubility The feature shown in the bottom row strongly predictive of insolubility is
activated by non-polar repeated ring structures
Fragments most
activated by
pro-solubility
feature
OH
NH
OH
OH
Fragments most
activated by
anti-solubility
feature
Figure Examining fingerprints optimized for predicting solubility Shown here are representative
examples of molecular fragments highlighted in blue which most activate different features of the
fingerprint Top row The feature most predictive of solubility Bottom row The feature most
predictive of insolubility
Toxicity features We trained the same model architecture to predict toxicity as measured in two
different datasets in Figure shows fragments which maximally activate the feature most
predictive of toxicity in two separate datasets
Fragments most
activated by
toxicity feature
on SR-MMP
dataset
Fragments most
activated by
toxicity feature
on NR-AHR
dataset
Figure Visualizing fingerprints optimized for predicting toxicity Shown here are representative
samples of molecular fragments highlighted in red which most activate the feature most predictive
of toxicity Top row the most predictive feature identifies groups containing a sulphur atom attached
to an aromatic ring Bottom row the most predictive feature identifies fused aromatic rings also
known as polycyclic aromatic hydrocarbons a well-known carcinogen
constructed similar visualizations but in a semi-manual way to determine which toxic fragments activated a given neuron they searched over a hand-made list of toxic substructures and chose
the one most correlated with a given neuron In contrast our visualizations are generated automatically without the need to restrict the range of possible answers beforehand
Predictive Performance
We ran several experiments to compare the predictive performance of neural graph fingerprints to
that of the standard state-of-the-art setup circular fingerprints fed into a fully-connected neural
network
Experimental setup Our pipeline takes as input the SMILES string encoding of each
molecule which is then converted into a graph using RDKit We also used RDKit to produce
the extended circular fingerprints used in the baseline Hydrogen atoms were treated implicitly
In our convolutional networks the initial atom and bond features were chosen to be similar to those
used by ECFP Initial atom features concatenated a one-hot encoding of the atom?s element its
degree the number of attached hydrogen atoms and the implicit valence and an aromaticity indicator The bond features were a concatenation of whether the bond type was single double triple
or aromatic whether the bond was conjugated and whether the bond was part of a ring
Training and Architecture Training used batch normalization We also experimented with
tanh vs relu activation functions for both the neural fingerprint network layers and the fullyconnected network layers relu had a slight but consistent performance advantage on the validation set We also experimented with dropconnect a variant of dropout in which weights are
randomly set to zero instead of hidden units but found that it led to worse validation error in general Each experiment optimized for minibatches of size using the Adam algorithm
a variant of RMSprop that includes momentum
Hyperparameter Optimization To optimize hyperparameters we used random search The hyperparameters of all methods were optimized using trials for each cross-validation fold The
following hyperparameters were optimized log learning rate log of the initial weight scale the log
penalty fingerprint length fingerprint depth up to and the size of the hidden layer in the
fully-connected network Additionally the size of the hidden feature vector in the convolutional
neural fingerprint networks was optimized
Dataset
Units
Predict mean
Circular FPs linear layer
Circular FPs neural net
Neural FPs linear layer
Neural FPs neural net
Solubility
log Mol/L
Drug efficacy
in nM
Photovoltaic efficiency
percent
Table Mean predictive accuracy of neural fingerprints compared to standard circular fingerprints
Datasets We compared the performance of standard circular fingerprints against neural graph fingerprints on a variety of domains
Solubility The aqueous solubility of molecules as measured by
Drug efficacy The half-maximal effective concentration in vitro of
molecules against a sulfide-resistant strain of P. falciparum the parasite that causes malaria
as measured by
Organic photovoltaic efficiency The Harvard Clean Energy Project uses expensive
DFT simulations to estimate the photovoltaic efficiency of organic molecules We used a
subset of molecules from this dataset
Predictive accuracy We compared the performance of circular fingerprints and neural graph fingerprints under two conditions In the first condition predictions were made by a linear layer using
the fingerprints as input In the second condition predictions were made by a one-hidden-layer
neural network using the fingerprints as input In all settings all differentiable parameters in the
composed models were optimized simultaneously Results are summarized in Table
In all experiments the neural graph fingerprints matched or beat the accuracy of circular fingerprints
and the methods with a neural network on top of the fingerprints typically outperformed the linear
layers
Software Automatic differentiation software packages such as Theano significantly
speed up development time by providing gradients automatically but can only handle limited control
structures and indexing Since we required relatively complex control flow and indexing in order
to implement variants of Algorithm we used a more flexible automatic differentiation package
for Python called Autograd github.com/HIPS/autograd This package handles standard
Numpy code and can differentiate code containing while loops branches and indexing
Code for computing neural fingerprints and producing visualizations is available at
github.com/HIPS/neural-fingerprint
Limitations
Computational cost Neural fingerprints have the same asymptotic complexity in the number of
atoms and the depth of the network as circular fingerprints but have additional terms due to the
matrix multiplies necessary to transform the feature vector at each step To be precise computing
the neural fingerprint of depth fingerprint length of a molecule with atoms using a molecular
convolutional net having features at each layer costs O(RN RN In practice training
neural networks on top of circular fingerprints usually took several minutes while training both the
fingerprints and the network on top took on the order of an hour on the larger datasets
Limited computation at each layer How complicated should we make the function that goes
from one layer of the network to the next In this paper we chose the simplest feasible architecture
a single layer of a neural network However it may be fruitful to apply multiple layers of nonlinearities between each message-passing step as in or to make information preservation easier by
adapting the Long Short-Term Memory architecture to pass information upwards
Limited information propagation across the graph The local message-passing architecture developed in this paper scales well in the size of the graph due to the low degree of organic molecules
but its ability to propagate information across the graph is limited by the depth of the network This
may be appropriate for small graphs such as those representing the small organic molecules used in
this paper However in the worst case it can take a depth N2 network to distinguish between graphs
of size To avoid this problem proposed a hierarchical clustering of graph substructures A
tree-structured network could examine the structure of the entire graph using only log(N layers
but would require learning to parse molecules Techniques from natural language processing
might be fruitfully adapted to this domain
Inability to distinguish stereoisomers Special bookkeeping is required to distinguish between
stereoisomers including enantomers mirror images of molecules and cis/trans isomers rotation
around double bonds Most circular fingerprint implementations have the option to make these
distinctions Neural fingerprints could be extended to be sensitive to stereoisomers but this remains
a task for future work
Related work
This work is similar in spirit to the neural Turing machine in the sense that we take an existing
discrete computational architecture and make each part differentiable in order to do gradient-based
optimization
Neural nets for quantitative structure-activity relationship QSAR The modern standard for
predicting properties of novel molecules is to compose circular fingerprints with fully-connected
neural networks or other regression methods used circular fingerprints as inputs to an ensemble
of neural networks Gaussian processes and random forests used circular fingerprints of depth
as inputs to a multitask neural network showing that multiple tasks helped performance
Neural graph fingerprints The most closely related work is who build a neural network
having graph-valued inputs Their approach is to remove all cycles and build the graph into a tree
structure choosing one atom to be the root A recursive neural network is then run from
the leaves to the root to produce a fixed-size representation Because a graph having nodes
has possible roots all possible graphs are constructed The final descriptor is a sum of the
representations computed by all distinct graphs There are as many distinct graphs as there are
atoms in the network The computational cost of this method thus grows as O(F where
is the size of the feature vector and is the number of atoms making it less suitable for large
molecules
Convolutional neural networks Convolutional neural networks have been used to model images
speech and time series However standard convolutional architectures use a fixed computational graph making them difficult to apply to objects of varying size or structure such as molecules
More recently and others have developed a convolutional neural network architecture for modeling sentences of varying length
Neural networks on fixed graphs introduce convolutional networks on graphs in the regime
where the graph structure is fixed and each training example differs only in having different features
at the vertices of the same graph In contrast our networks address the situation where each training
input is a different graph
Neural networks on input-dependent graphs propose a neural network model for graphs
having an interesting training procedure The forward pass consists of running a message-passing
scheme to equilibrium a fact which allows the reverse-mode gradient to be computed without storing
the entire forward computation They apply their network to predicting mutagenesis of molecular
compounds as well as web page rankings also propose a neural network model for graphs
with a learning scheme whose inner loop optimizes not the training loss but rather the correlation
between each newly-proposed vector and the training error residual They apply their model to a
dataset of boiling points of molecular compounds Our paper builds on these ideas with the
following differences Our method replaces their complex training algorithms with simple gradientbased optimization generalizes existing circular fingerprint computations and applies these networks in the context of modern QSAR pipelines which use neural networks on top of the fingerprints
to increase model capacity
Unrolled inference algorithms and others have noted that iterative inference procedures
sometimes resemble the feedforward computation of a recurrent neural network One natural extension of these ideas is to parameterize each inference step and train a neural network to approximately
match the output of exact inference using only a small number of iterations The neural fingerprint
when viewed in this light resembles an unrolled message-passing algorithm on the original graph
Conclusion
We generalized existing hand-crafted molecular features to allow their optimization for diverse tasks
By making each operation in the feature pipeline differentiable we can use standard neural-network
training methods to scalably optimize the parameters of these neural molecular fingerprints end-toend We demonstrated the interpretability and predictive performance of these new fingerprints
Data-driven features have already replaced hand-crafted features in speech recognition machine
vision and natural-language processing Carrying out the same task for virtual screening drug
design and materials design is a natural next step
Acknowledgments
We thank Edward Pyzer-Knapp Jennifer Wei and Samsung Advanced Institute of Technology for
their support This work was partially funded by NSF

<<----------------------------------------------------------------------------------------------------------------------->>

title: 5961-neural-adaptive-sequential-monte-carlo.pdf

Neural Adaptive Sequential Monte Carlo
Shixiang
Zoubin Ghahramani
Richard E. Turner
University of Cambridge Department of Engineering Cambridge UK
MPI for Intelligent Systems T?ubingen Germany
sg717@cam.ac.uk zoubin@eng.cam.ac.uk ret26@cam.ac.uk
Abstract
Sequential Monte Carlo or particle filtering is a popular class of methods for sampling from an intractable target distribution using a sequence of simpler intermediate distributions Like other importance sampling-based methods
performance is critically dependent on the proposal distribution a bad proposal
can lead to arbitrarily inaccurate estimates of the target distribution This paper
presents a new method for automatically adapting the proposal using an approximation of the Kullback-Leibler divergence between the true posterior and the
proposal distribution The method is very flexible applicable to any parameterized proposal distribution and it supports online and batch variants We use the
new framework to adapt powerful proposal distributions with rich parameterizations based upon neural networks leading to Neural Adaptive Sequential Monte
Carlo NASMC Experiments indicate that NASMC significantly improves inference in a non-linear state space model outperforming adaptive proposal methods
including the Extended Kalman and Unscented Particle Filters Experiments also
indicate that improved inference translates into improved parameter learning when
NASMC is used as a subroutine of Particle Marginal Metropolis Hastings Finally
we show that NASMC is able to train a latent variable recurrent neural network
LV-RNN achieving results that compete with the state-of-the-art for polymorphic music modelling NASMC can be seen as bridging the gap between adaptive
SMC methods and the recent work in scalable black-box variational inference
Introduction
Sequential Monte Carlo SMC is a class of algorithms that draw samples from a target distribution
of interest by sampling from a series of simpler intermediate distributions More specifically the sequence constructs a proposal for importance sampling SMC is particularly well-suited for
performing inference in non-linear dynamical models with hidden variables since filtering naturally
decomposes into a sequence and in many such cases it is the state-of-the-art inference method
Generally speaking inference methods can be used as modules in parameter learning systems SMC
has been used in such a way for both approximate maximum-likelihood parameter learning and
in Bayesian approaches such as the recently developed Particle MCMC methods
Critically in common with any importance sampling method the performance of SMC is strongly
dependent on the choice of the proposal distribution If the proposal is not well-matched to the target distribution then the method can produce samples that have low effective sample size and this
leads to Monte Carlo estimates that have pathologically high variance The SMC community
has developed approaches to mitigate these limitations such as resampling to improve particle diversity when the effective sample size is low and applying MCMC transition kernels to improve
particle diversity A complementary line of research leverages distributional approximate
inference methods such as the extended Kalman Filter and Unscented Kalman Filter to construct
better proposals leading to the Extended Kalman Particle Filter EKPF and Unscented Particle Fil1
ter UPF In general however the construction of good proposal distributions is still an open
question that severely limits the applicability of SMC methods
This paper proposes a new gradient-based black-box adaptive SMC method that automatically tunes
flexible proposal distributions The quality of a proposal distribution can be assessed using the intractable Kullback-Leibler divergence between the target distribution and the parametrized
proposal distribution We approximate the derivatives of this objective using samples derived from
SMC. The framework is very general and tractably handles complex parametric proposal distributions For example here we use neural networks to carry out the parameterization thereby leveraging
the large literature and efficient computational tools developed by this community We demonstrate
that the method can efficiently learn good proposal distributions that significantly outperform existing adaptive proposal methods including the EKPF and UPF on standard benchmark models used
in the particle filter community We show that improved performance of the SMC algorithm translates into improved mixing of the Particle Marginal Metropolis-Hasting PMMH Finally we
show that the method allows higher-dimensional and more complicated models to be accurately handled using SMC such as those parametrized using neural networks that are challenging for
traditional particle filtering methods
The focus of this work is on improving SMC but many of the ideas are inspired by the burgeoning
literature on approximate inference for unsupervised neural network models These connections are
explored in section
Sequential Monte Carlo
We begin by briefly reviewing two fundamental SMC algorithms sequential importance sampling
SIS and sequential importance resampling Consider a probabilistic model comprising possibly multi-dimensional hidden and observed states and respectively whose joint disQT
tribution factorizes as p(zt p(xt This
general form subsumes common state-space models such as Hidden Markov Models HMMs as
well as non-Markovian models for the hidden state such as Gaussian processes
The goal of the sequential importance sampler is to approximate the posterior distribution over
PN
the hidden state sequence
through a weighted set of
sampled trajectories drawn from a simpler proposal distribution
Any form of proposal distribution can be used in principle but a particularly convenient one takes
QT
the same factorisation as the true posterior q(zt with
filtering dependence on A short derivation supplementary material then shows that the
normalized importance weights are defined by a recursion
p(zT p(xT
w(z
w(z
q(zT
SIS is elegant as the samples and weights can be computed in sequential fashion using a single
forward pass However na??ve implementation suffers from a severe pathology the distribution
of importance weights often become highly skewed as increases with many samples attaining
very low weight To alleviate the problem the Sequential Importance Resampling SIR algorithm
adds an additional step that resamples zt at time from a multinomial distribution given by
w(z
and gives the new particles equal weight.1 This replaces degenerated particles that have low
weight with samples that have more substantial importance weights without violating the validity of
the method SIR requires knowledge of the full trajectory of previous samples at each stage to draw
the samples and compute the importance weights For this reason when carrying out resampling
each new particle needs to update its ancestry information Letting represent the ancestral
index of particle at time for state where and collecting these into the set
At
A
A
zt where z1:tt
at,t where a a
the resampled trajectory can be denoted
a
a
zt t,t Finally to lighten notation we use the shorthand
More advanced implementations resample only when the effective sample size falls below a threshold
wt for the weights Note that when employing resampling these do not depend on
the previous weights since resampling has given the previous particles uniform weight The
implementation of SMC is given by Algorithm in the supplementary material
The Critical Role of Proposal Distributions in Sequential Monte Carlo
The choice of the proposal distribution in SMC is critical Even when employing the resampling
step a poor proposal distribution will produce trajectories that when traced backwards quickly
collapse onto a single ancestor Clearly this represents a poor approximation to the true posterior
These effects can be mitigated by increasing the number of particles and/or applying
more complex additional MCMC moves but these strategies increase the computational cost
The conclusion is that the proposal should be chosen with care The optimal choice for an unconstrained proposal that has access to all of the observed data at all times is the intractable posterior
distribution Given the restrictions imposed by the factorization
this becomes q(zt p(zt which is still typically intractable The bootstrap filter instead uses the prior q(zt p(zt which is often tractable
but fails to incorporate information from the current observation A halfway-house employs
distributional approximate inference techniques to approximate p(zt Examples include the EKPF and UPF However these methods suffer from three main problems First
the extended and unscented Kalman Filter from which these methods are derived are known to be
inaccurate and poorly behaved for many problems outside of the SMC setting Second these
approximations must be applied on a sample by sample basis leading to significant additional computational overhead Third neither approximation is tuned using an SMC-relevant criterion In the
next section we introduce a new method for adapting the proposal that addresses these limitations
Adapting Proposals by Descending the Inclusive KL Divergence
In this work the quality of the proposal distribution will be optimized using the
inclusive KL-divergence between the true posterior distribution and the proposal
KL[p Parameters are made explicit since we will shortly be
interested in both adapting the proposal and learning the model This objective is chosen for
four main reasons First this is a direct measure of the quality of the proposal unlike those typically
used such as effective sample size Second if the true posterior lies in the class of distributions
attainable by the proposal family then the objective has a global optimum at this point Third if
the true posterior does not lie within this class then this KL divergence tends to find proposal
distributions that have higher entropy than the original which is advantageous for importance
sampling the exclusive KL is unsuitable for this reason Fourth the derivative of the objective
can be approximated efficiently using a sample based approximation that will now be described
The gradient of the negative KL divergence with respect to the parameters of the proposal distribution takes a simple form
KL[p
log
The expectation over the posterior can be approximated using samples from SMC. One option would
use the weighted sample trajectories at the final time-step of SMC but although asymptotically
unbiased such an estimator would have high variance due to the collapse of the trajectories An
alternative that reduces variance at the cost of introducing some bias uses the intermediate ancestral
trees a filtering approximation the supplementary material for details
KL[p
log zt
The simplicity of the proposed approach brings with it several advantages and opportunities
Online and batch variants Since the derivatives distribute over time it is trivial to apply this
update in an online way updating the proposal distribution every time-step Alternatively when
learning parameters in a batch setting it might be more appropriate to update the proposal parameters after making a full forward pass of SMC. Conveniently when performing approximate
maximum-likelihood learning the gradient update for the model parameters can be efficiently
approximated using the same sample particles from SMC supplementary material and Algorithm A similar derivation for maximum likelihood learning is also discussed in
log[p
log zt
Algorithm Stochastic Gradient Adaptive SMC batch inference and learning variants
Require proposal model observations number of particles
repeat
NextMiniBatch(X
PTj
log zt
PTj
log zt
optional
Optimize
Optimize optional
until convergence
Efficiency of the adaptive proposal In contrast to the EPF and UPF the new method employs an
analytic function for propagation and does not require costly particle-specific distributional approximation as an inner-loop Similarly although the method bears similarity to the assumed-density filter
ADF which minimizes a local inclusive KL the new method has the advantage of minimizing
a global cost and does not require particle-specific moment matching
Training complex proposal models The adaptation method described above can be applied to any
parametric proposal distribution Special cases have been previously treated by We propose
a related but arguably more straightforward and general approach to proposal adaptation In the
next section we describe a rich family of proposal distributions that go beyond previous work
based upon neural networks This approach enables adaptive SMC methods to make use of the rich
literature and optimization tools available from supervised learning
Flexibility of training One option is to train the proposal distribution using samples from SMC
derived from the observed data However this is not the only approach For example the proposal
could be trained using data sampled from the generative model instead which might mitigate overfitting effects for small datasets Similarly the trained proposal does not need to be the one used to
generate the samples in the first place The bootstrap filter or more complex variants can be used
Flexible and Trainable Proposal Distributions Using Neural Networks
The proposed adaption method can be applied to any parametric proposal distribution Here we
briefly describe how to utilize this flexibility to employ powerful neural network-based parameterizations that have recently shown excellent performance in supervised sequence learning tasks
Generally speaking applications of these techniques to unsupervised sequence modeling settings is
an active research area that is still in its infancy and this work opens a new avenue in this wider
research effort
In a nutshell the goal is to parameterize zt the proposal?s stochastic mapping from
all previous hidden states and all observations up to and including the current observation
to the current hidden state zt in a flexible computationally efficient and trainable way Here
we use a class of functions called Long Short-Term Memory LSTM that define a deterministic
mapping from an input sequence to an output sequence using parameter-efficient recurrent dynamics and alleviate the common vanishing gradient problem in recurrent neural networks
The distributions zt ht can be a mixture of Gaussians mixture density network MDN
in which the mixing proportions means and covariances are parameterised through another neural
network the supplementary for details on LSTM MDN and neural network architectures
Experiments
The goal of the experiments is three fold First to evaluate the performance of the adaptive method
for inference on standard benchmarks used by the SMC community with known ground truth Second to evaluate the performance when SMC is used as an inner loop of a learning algorithm Again
we use an example with known ground truth Third to apply SMC learning to complex models that
would normally be challenging for SMC comparing to the state-of-the-art in approximate inference
One way of assessing the success of the proposed method would be to evaluate
KL[p(z1:T However this quantity is hard to accurately compute Instead
we use a number of other metrics For the experiments where ground truth states are known
we can evaluate the root mean square error RMSE between the approximate
posterior mean of the
latent variables and the true value RMSE(z1:T T1 zt
z?t More generally the estimate of the log-marginal likelihood LML log log p(xt
log
wt and its variance is also indicative of performance Finally we also employ a
common metric called the effective sample size ESS to measure the effectiveness of our SMC
method ESS of particles at time is given by ESSt
If
expected ESS is maximized and equals the number of particles equivalently the
normalized importance weights are uniform Note that ESS alone is not a sufficient metric since it
does not measure the absolute quality of samples but rather the relative quality
Inference in a Benchmark Nonlinear State-Space Model
In order to evaluate the effectiveness of our adaptive SMC method we tested our method on a
standard nonlinear state-space model often used to benchmark SMC algorithms The model is
given by where The posterior distribution is highly multi-modal
due to uncertainty about the signs of the latent states
p(zt zt
p(xt zt
g(zt
zt2
The experiments investigated how the new proposal adaptation method performed in comparison to
standard methods including the bootstrap filter EKPF and UKPF In particular we were interested
in the following questions Do rich multi-modal proposals improve inference For this we compared
a Gaussian proposal with a diagonal Gaussian to a mixture density network with three components Does a recurrent parameterization of the proposal help For this we compared a non-recurrent
neural network with hidden units to a recurrent neural network with LSTM units Can injecting information about the prior dynamics into the proposal improve performance
similar in spirit to for variational methods To assess this we parameterized proposals for vt
process noise instead of zt and let the proposal have access to the prior dynamics
For
all experiments the parameters in the non-linear state-space model were fixed to
Adaptation of the proposal was performed on samples from the generative process
at each iteration Results are summarized in and Table supplementary material for
additional results Average run times for the algorithms over a sequence of length were
bootstrap EKPF UPF NN-NASMC and RNN-NASMC where
EKPF and UPF implementations are provided by Although these numbers should only be taken
as a guide as the implementations had differing levels of acceleration
The new adaptive proposal methods significantly outperform the bootstrap EKPF and UPF methods in terms of ESS RMSE and the variance in the LML estimates The multi-modal proposal
outperforms a simple Gaussian proposal compare RNN-MD-f to RNN-f indicating multi-modal
proposals can improve performance Moreover the RNN outperforms the non-recurrent NN compare RNN to NN). Although the proposal models can effectively learn the transition function injecting information about the prior dynamics into the proposal does help compare RNN-f to RNN
Interestingly there is no clear cut winner between the EKPF and UPF although the UPF does return
LML estimates that have lower variance All methods converged to similar LMLs that were close
to the values computed using large numbers of particles indicating the implementations are correct
70
effective sample size
log marginal likelihood
EKPF
NN-MD
prior
RNN-f
RNN-MD-f
RNN-MD
RNN
UPF
EKP
NN-M
prio
RNN
MD NN-M
RNN
UPF
RNN
iteration
Figure Left Box plots for LML estimates from iteration to Right Average ESS over
the first iterations
ESS iter
mean std
prior
EKPF
UPF
RNN
RNN-f
RNN-MD
RNN-MD-f
NN-MD
LML
mean std
79
34
36
32
36
RMSE
mean std
Table Left Middle Average ESS and log marginal likelihood estimates over the last iterations Right The RMSE over new sequences with no further adaptation
Inference in the Cart and Pole System
As a second and more physically meaningful system we considered a cart-pole system that consists
of an inverted pendulum that rests on a movable base The system was driven by a white noise
input An ODE solver was used to simulate the system from its equations of motion We considered
the problem of inferring the true position of the cart and orientation of the pendulum along with
their derivatives and the input noise from noisy measurements of the location of the tip of the pole
The results are presented in The system is significantly more intricate than the model in
Sec. and does not directly admit the usage of EKPF or UPF. Our RNN-MD proposal model
successfully learns good proposals without any direct access to the prior dynamics
ESS
RNN-MD
prior
prior
prior
rad
ESS
iteration
prior
RNN-MD
ground-truth
time
time
Figure Left Normalized ESS over iterations Middle Right Posterior mean ground-truth
for the horizontal location of the cart and the change in relative angle of the pole RNN-MD
learns to have higher ESS than the prior and more accurately estimates the latent states
prior
RNN-MD-f-pre
RNN-MD-f
RNN-MD-pre
RNN-MD
iteration
iteration
Figure PMMH samples of values for particles For small numbers of particles
right PMMH is very slow to burn in and mix when proposing from the prior distribution due to the
large variance in the marginal likelihood estimates it returns
Bayesian learning in a Nonlinear SSM
SMC is often employed as an inner loop of a more complex algorithm One prominent example
is Particle Markov Chain Monte Carlo a class of methods that sample from the joint posterior
over model parameters and latent state trajectories Here we consider the Particle
Marginal Metropolis-Hasting sampler PMMH In this context SMC is used to construct a proposal
distribution for a Metropolis-Hasting accept/reject step The proposal is formed by sampling a
proposed set of parameters by perturbing the current parameters using a Gaussian random walk
then SMC is used to sample a proposed set of latent state variables resulting in a joint proposal
The MH step uses the SMC marginal likelihood
estimates to determine acceptance Full details are given in the supplementary material
In this experiment we evaluate our method in a PMMH sampler on the same model from Section following A random walk proposal is used to sample
The prior over is set as is initialized as
and the PMMH is run for iterations
Two of the adaptive models considered section are used for comparison RNN-MD and RNNMD-f where models are pre-trained for iterations using samples from the initial
The results are shown in and were typical for a range of parameter settings Given a
sufficient number of particles there is almost no difference between the prior proposal
and our method However when the number of particles gets smaller NASMC enables
significantly faster burn-in to the posterior particularly on the measurement noise and for similar
reasons NASMC mixes more quickly The limitation with the NASMC-PMMH is that the model
needs to continuously adapt as the global parameter is sampled but note this is still not as costly as
adapting on a particle-by-particle basis as is the case for the EKPF and UPF.
Polyphonic Music Generation
Finally the new method is used to train a latent variable recurrent neural network LV-RNN for
modelling four polymorphic music datasets of varying complexity These datasets are often
used to benchmark RNN models because of their high dimensionality and the complex temporal
dependencies involved at different time scales 18 Each dataset contains at least hours of
polyphonic music with an average polyphony number of simultaneous notes of out of 88 LVRNN contains a recurrent neural network with LSTM layers that is driven by stochastic latent
variables zt at each time-point and stochastic outputs that are fed back into the dynamics full
details in the supplementary material Both the LSTM layers in the generative and proposal models
are set as units and Adam is used as the optimizer The bootstrap filter is compared to
the new adaptive method NASMC particles are used in the training The hyperparameters
are tuned using the validation set A diagonal Gaussian output is used in the proposal model
with an additional hidden layer of size The log likelihood on the test set a standard metric
for comparison in generative models is approximated using SMC with particles
Only the prior proposal is compared since Sec. shows the advantage of our method over EKPF/UPF
The results are reported in Table The adaptive method significantly outperforms the bootstrap
filter on three of the four datasets On the piano dataset the bootstrap method performs marginally
better In general the NLLs for the new methods are comparable to the state-of-the-art although
detailed comparison is difficult as the methods with stochastic latent states require approximate
marginalization using importance sampling or SMC.
Dataset
Piano-midi-de
Nottingham
MuseData
JSBChorales
LV-RNN
NASMC
LV-RNN
Bootstrap
STORN
SGVB
FD-RNN
sRNN
RNN-NADE
Table Estimated negative log likelihood on test data FD-RNN and STORN are from
and sRNN and RNN-NADE are results from
Comparison of Variational Inference to the NASMC approach
There are several similarities between NASMC and Variational Free-energy methods that employ recognition models Variational Free-energy methods refine an approximation to
the posterior distribution by optimising the exclusive variational KL-divergence
KL[q It is common to approximate this integral using samples from the approximate posterior 22 This general approach is similar in spirit to the way that the proposal is
adapted in NASMC except that the inclusive KL-divergence is employed KL[p and
this entails that sample based approximation requires simulation from the true posterior Critically
NASMC uses the approximate posterior as a proposal distribution to construct a more accurate posterior approximation The SMC algorithm therefore can be seen as correcting for the deficiencies in
the proposal approximation We believe that this can lead to significant advantages over variational
free-energy methods especially in the time-series setting where variational methods are known to
have severe biases Moreover using the inclusive KL avoids having to compute the entropy
of the approximating distribution which can prove problematic when using complex approximating
distributions mixtures and heavy tailed distributions in the variational framework There is a
close connection between NASMC and the wake-sleep algorithm The wake-sleep algorithm
also employs the inclusive KL divergence to refine a posterior approximation and recent generalizations have shown how to incorporate this idea into importance sampling In this context the
NASMC algorithm extends this work to SMC.
Conclusion
This paper developed a powerful method for adapting proposal distributions within general SMC
algorithms The method parameterises a proposal distribution using a recurrent neural network
to model long-range contextual information allows flexible distributional forms including mixture
density networks and enables efficient training by stochastic gradient descent The method was
found to outperform existing adaptive proposal mechanisms including the EKPF and UPF on a standard SMC benchmark it improves burn in and mixing of the PMMH sampler and allows effective
training of latent variable recurrent neural networks using SMC. We hope that the connection between SMC and neural network technologies will inspire further research into adaptive SMC methods In particular application of the methods developed in this paper to adaptive particle smoothing
high-dimensional latent models and adaptive PMCMC for probabilistic programming are particular
exciting avenues
Acknowledgments
SG is generously supported by Cambridge-T?ubingen Fellowship the ALTA Institute and Jesus
College Cambridge RET thanks the EPSRC grants and We thank
Theano developers for their toolkit the authors of for releasing the source code and Roger
Frigola Sumeet Singh Fredrik Lindsten and Thomas Sch?on for helpful suggestions on experiments
Results for RNN-NADE are separately provided for

<<----------------------------------------------------------------------------------------------------------------------->>

title: 5278-general-stochastic-networks-for-classification.pdf

General Stochastic Networks for Classification
Matthias Z?ohrer and Franz Pernkopf
Signal Processing and Speech Communication Laboratory
Graz University of Technology
matthias.zoehrer@tugraz.at pernkopf@tugraz.at
Abstract
We extend generative stochastic networks to supervised learning of representations In particular we introduce a hybrid training objective considering a generative and discriminative cost function governed by a trade-off parameter We use
a new variant of network training involving noise injection walkback training to jointly optimize multiple network layers Neither additional regularization
constraints such as norms or dropout variants nor pooling or convolutional layers were added Nevertheless we are able to obtain state-of-the-art performance on the MNIST dataset without using permutation invariant digits and
outperform baseline models on sub-variants of the MNIST and rectangles dataset
significantly
Introduction
Since there has been a boost in machine learning due to improvements in the field of unsupervised learning of representations Most accomplishments originate from variants of restricted
Boltzmann machines RBMs auto-encoders and sparse-coding Deep models in representation learning also obtain impressive results in supervised learning problems such
as speech recognition and computer vision tasks
If no a-priori knowledge is modeled in the architecture cf convolutional layers or pooling layers
generatively pre-trained networks are among the best when applied to supervised learning tasks
Usually a generative representation is obtained through a greedy-layerwise training procedure
called contrastive divergence In this case the network layer learns the representation from
the layer below by treating the latter as static input Despite of the impressive results achieved with
CD we identify two minor drawbacks when used for supervised learning Firstly after obtaining
a representation by pre-training a network a new discriminative model is initialized with the trained
weights splitting the training into two separate models This seems to be neither biologically plausible nor optimal when it comes to optimization as carefully designed early stopping criteria have to
be implemented to prevent over or under-fitting Secondly generative and discriminative objectives
might influence each other beneficially when combined during training CD does not take this into
account
In this work we introduce a new training procedure for supervised learning of representations In
particular we define a hybrid training objective for general stochastic networks dividing the
cost function into a generative and discriminative part controlled by a trade-off parameter It turns
out that by annealing when solving this unconstrained non-convex multi-objective optimization
problem we do not suffer from the shortcomings described above We are able to obtain stateof-the-art performance on the MNIST dataset without using permutation invariant digits and
significantly outperform baseline models on sub-variants of the MNIST and rectangle database
Our approach is related to the generative-discriminative training approach of RBMs However
a different model and a new variant of network training involving noise injection walkback
training is used to jointly optimize multiple network layers Most notably we did not
apply any additional regularization constraints such as norms or dropout variants
unlocking further potential for possible optimizations The model can be extended to learn multiple
tasks at the same time using jointly trained weights and by introducing multiple objectives This
might also open a new prospect in the field of transfer learning and multi-task learning
beyond classification
This paper is organized as follows Section presents mathematical background material the
GSN and a hybrid learning criterion In Section we empirically study the influence of hyper
parameters of GSNs and present experimental results Section concludes the paper and provides a
perspective on future work
General Stochastic Networks
Recently a new supervised learning algorithm called walkback training for generalized autoencoders GAE was introduced A follow-up study defined a new network model
generative stochastic networks extending the idea of walkback training to multiple layers When
applied to image reconstruction they were able to outperform various baseline systems due to its
ability to learn multi-modal representations In this paper we extend the work of
First we provide mathematical background material for generative stochastic networks Then we
introduce modifications to make the model suitable for supervised learning In particular we present
a hybrid training objective dividing the cost into a generative and discriminative part This paves
the way for any multi-objective learning of GSNs We also introduce a new terminology general stochastic networks a model class including generative discriminative and hybrid stochastic
network variants
General Stochastic Networks for Unsupervised Learning
Restricted Boltzmann machines RBM and denoising autoencoders DAE share the following commonality The input distribution is sampled to convergence in a Markov chain
In the case of the DAE the transition operator first samples the hidden state Ht from a corruption
distribution and generates a reconstruction from the parametrized model i.e the density
Figure DAE Markov chain
The resulting DAE Markov chain shown in Figure is defined as
and
where is the input sample fed into the chain at time step and is the reconstruction
of at time step In the case of a GSN an additional dependency between the latent variables Ht
over time is introduced to the network graph The GSN Markov chain is defined as follows
and
Figure shows the corresponding network graph
This chain can be expressed with deterministic functions of random variables In
particular the density is used to model specified for some independent noise source with the condition that cannot be recovered exactly from
Figure GSN Markov chain
We introduce as a back-probable stochastic non-linearity of the form out g(?in a
with noise processes Zt out for layer The variable a
is the activation for unit where
a
Iti bi with a weight matrix and bias bi representing the parametric distribution It is
embedded in a non-linear activation function The input Iti is either the realization xit of observed
sample Xti or the hidden realization hit of Hti In general Iti specifies an upward path in a GSN
for a specific layer In the case of
we define Hti out g(?in a
as a downward path in the network a
Ht using the transpose of the weight
matrix and the bias bi This formulation allows to directly back-propagate the reconstruction log-likelihood for all parameters b0 bd where is the
number of hidden layers In Figure the GSN includes a simple hidden layer This can be
extended to multiple hidden layers requiring multiple deterministic functions of random variables
Figure visualizes the Markov chain for a multi-layer GSN inspired by the unfolded computational
graph of a deep Boltzmann machine Gibbs sampling process
Lt
Lt
Lt
Lt
Figure GSN Markov chain with multiple layers and backprop-able stochastic units
In the training case alternatively even or odd layers are updated at the same time The information
is propagated both upwards and downwards for steps allowing the network to build higher order
representations An example for this update process is given in Figure In the even update marked
and
in red
In the odd update marked in blue
for In the case of and
and
in the odd update
in the even update and
In case of and in the even update and
and
in the odd update
The cost function of a generative GSN can be written as
Lt Xt+k
Lt is a specific loss-function such as the mean squared error MSE at time step In general any
arbitrary loss function could be used as long as they can be seen as a log-likelihood Xt+k
is the reconstruction of the input at layer after steps Optimizing the loss function by
building the sum over the costs of multiple corrupted reconstructions is called walkback training
This form of network training leads to a significant performance boost when used for input
reconstruction The network is able to handle multi-modal input representations and is therefore
considerably more favorable than standard generative models
General Stochastic Networks for Supervised Learning
In order to make a GSN suitable for a supervised learning task we introduce the output to the
network graph In this case log log Although the target is not fed into the
network it is introduced as an additional cost term The layer update-process stays the same
Lt
Lt
Lt
Lt
Lt
Lt
Figure GSN Markov chain for input and target with backprop-able stochastic units
We define the following cost function for a 3-layer GSN
Lt Xt+k
Lt Ht+k
k=d
generative
discriminative
This is a non-convex multi-objective optimization problem where weights the generative and
discriminative part of C. The parameter specifies the number of network layers depth of the
network Scaling the mean loss in is not mandatory but allows to equally balance both loss terms
with for input and target scaled to the same range Again Figure shows the
corresponding network graph for supervised learning with red and blue edges denoting the even and
odd network updates
In general the hybrid objective optimization criterion is not restricted to hX as additional input
and output terms could be introduced to the network This setup might be useful for transfer-learning
or multi-task scenarios which is not discussed in this paper
Experimental Results
In order to evaluate the capabilities of GSNs for supervised learning we studied MNIST digits
variants of MNIST digits and the rectangle datasets The first database consists of
labeled training and labeled test images of handwritten digits The second dataset includes variants of MNIST digits mnist-basic mnist-rot mnist-back-rand mnist-back-image
mnist-rot-back-image with additional factors of variation added to the original data Each variant
includes labeled training labeled validation and labeled test images The third
dataset involves two subsets rectangle rectangle-image The dataset rectangle consists of
labeled training labeled validation and labeled test images The dataset rectangleimage includes labeled train labeled validation and labeled test images
In a first experiment we focused on the multi-objective optimization problem defined in Next we
evaluated the number of walkback steps in a GSN necessary for convergence In a third experiment
we analyzed the influence of different Gaussian noise settings during walkback training improving
the generalization capabilities of the network Finally we summarize classification results for all
datasets and compare to baseline systems
Multi-Objective Optimization in a Hybrid Learning Setup
In order to solve the non-convex multi-objective optimization problem variants of stochastic gradient descent SGD can be used We applied a search over fixed values on all problems Furthermore we show that the use of an annealed factor during training works best in practice
In all experiments a three layer GSN with neurons in each layer randomly initialized with small Gaussian noise and an MSE loss function for both inputs and
targets was used Regarding optimization we applied SGD with a learning rate a momentum term of and a multiplicative annealing factor per epoch for the learning
rate A rectifier unit was chosen as activation function Following the ideas of no explicit
sampling was applied at the input and output layer In the test case the zero-one loss was computed
averaging the network?s output over walkback steps
Analysis of the Hybrid Learning Parameter
Concerning the influence of the trade-off parameter we tested fixed values in the range
where low values emphasize the discriminative part in the objective and vice versa Walkback training with steps using zero-mean pre and postactivation Gaussian noise with zero mean and variance was performed for training epochs In a more dynamic scenario was annealed by to reach
within epochs simulating generative pre-training to a
certain extend
Figure Influence of dynamic and static on MNIST variants basic left rotated middle and
background right where denotes the training the validation and the test-set The dashed
line denotes the static setup the bold line the dynamic setup
Figure compares the results of both GSNs using static and dynamic setups on the MNIST
variants basic rotated and background The use of a dynamic annealed achieved
the best validation and test error in all experiments In this case more attention was given to the
generative proportion of the objective in the early stage of training After approximately
epochs discriminative training fine-tuning dominates This setup is closely related to DBN
training where emphasis is on optimizing at the beginning of the optimization whereas
is important at the last stages In case of the GSN the annealed achieves a more smooth
transition by shifting the weight in the optimization criterion from to within one
model
Analysis of Walkback Steps
In a next experiment we tested the influence of walkback steps for GSNs Figure shows the
results for different GSNs trained with walkback steps and annealed with
In all cases the information was at least propagated once up and once downwards in the
layer network using fixed Gaussian pre and post-activation noise with and
Figure Evaluating the number of walkback steps on MNIST variants basic left rotated middle
and background right where denotes the training the validation and the test-set
Figure shows that increasing the walkback steps does not improve the generalization capabilities
of the used GSNs The setup is sufficient for convergence and achieves the best validation
and test result in all experiments
Analysis of Pre and Post-Activation Noise
Injecting noise during the training process of GSNs serves as a regularizer and improves the generalization capabilities of the model In this experiment the influence of Gaussian pre and
post-activation noise with and and deactivated noise
during training was tested on a GSN-3 trained for walkback steps The trade-off factor
was annealed with Figure summarizes the results of the different GSNs for the
MNIST variants basic rotated and background Setting achieved the best overall result
on the validation and test-set for all three experiments In all other cases the GSNs either over or
underfitted the data
Figure Evaluating noise injections during training on MNIST variants basic left rotated middle
and background right where denotes the training the validation and the test-set
MNIST results
Table presents the average classification error of three runs of all MNIST variation datasets obtained by a using fixed Gaussian pre and post-activation noise with and
walkback steps The hybrid learning parameter was annealed with and
A small grid test was performed in the range of with neurons per
layer for layers to find the optimal network configuration
Dataset
SVMrbf
SVMpoly NNet
DBN-1
SAA-3
DBN-3
GSN-3
mnist-basic
mnist-rot
mnist-back-rand
mnist-back-image
mnist-rot-back-image
rectangles
rectangles-image
Table MNIST variations and recangle results For datasets marked by updated results are
shown
Table shows that a three layer GSN clearly outperforms all other models except for the MNIST
random-background dataset In particular when comparing the GSN-3 to the radial basis function
support vector machine SVMrbf the second best model on MNIST basic the GSN-3 achieved
an relative improvement of on the test set On the MNIST rotated dataset the GSN-3 was
able to beat the second best model by on the test set On the MNIST rotatedbackground there is an relative improvement of on the test set between the second best model
and the All results are statistically significant Regarding the number of model
parameters although we cannot directly compare the models in terms of network parameters it is
worth to mention that a far smaller grid test was used to generate the results for all GSNs cf
When comparing the classification error of the GSN-3 trained without noise obtained in the previous
experiments with Table the GSN-3 achieved the test error of on the MNIST variant
basic outperforming all other models on this task On the MNIST variant rotated the GSN-3 also
outperformed the obtaining a test error of This indicates that not only the Gaussian
regularizer in the walkback training improves the generalization capabilities of the network but also
the hybrid training criterion of the GSN.
Table lists the results for the MNIST dataset without additional affine transformations applied to
the data permutation invariant digits A three layer GSN achieved the state-of-the-art test error
of
Network
Result
Rectifier MLP dropout
DBM
Maxout MLP dropout
MP-DBM
Deep Convex Network
Manifold Tangent Classifier
DBM dropout
GSN-3
Table MNIST results
It might be worth noting that in addition to the noise process in walkback training no other regularizers such as norms and dropout variants were used in the GSNs In general
training epochs with early-stopping are necessary for GSN training
All simulations1 were executed on a GPU with the help of the mathematical expression compiler
Theano
Conclusions and Future Work
We have extended GSNs for classification problems In particular we defined an hybrid multiobjective training criterion for GSNs dividing the cost function into a generative and discriminative
part This renders the need for generative pre-training unnecessary We analyzed the influence of
the objective?s trade-off parameter empirically showing that by annealing we outperform a
static choice of Furthermore we discussed effects of noise injections and sampling steps during
walkback training As a conservative starting point we restricted the model to use only rectifier
units Neither additional regularization constraints such as norms or dropout variants
nor pooling or convolutional layers were added Nevertheless the GSN was
able to outperform various baseline systems in particular a deep belief network a multi
layer perceptron a support vector machine SVM and a stacked auto-associator on
variants of the MNIST dataset Furthermore we also achieved state-of-the-art performance on the
original MNIST dataset without permutation invariant digits The model not only converges faster
in terms of training iterations but also show better generalization behavior in most cases Our
approach opens a wide field of new applications for GSNs In future research we explore adaptive
noise injection methods for GSNs and non-convex multi-objective optimization strategies

<<----------------------------------------------------------------------------------------------------------------------->>

title: 4443-algorithms-for-hyper-parameter-optimization.pdf

Algorithms for Hyper-Parameter Optimization
R?emi Bardenet
Laboratoire de Recherche en Informatique
Universit?e Paris-Sud
bardenet@lri.fr
James Bergstra
The Rowland Institute
Harvard University
bergstra@rowland.harvard.edu
Yoshua Bengio
D?ept d?Informatique Recherche Op?erationelle
Universit?e de Montr?eal
yoshua.bengio@umontreal.ca
Bal?azs K?egl
Linear Accelerator Laboratory
Universit?e Paris-Sud CNRS
balazs.kegl@gmail.com
Abstract
Several recent advances to the state of the art in image classification benchmarks
have come from better configurations of existing techniques rather than novel approaches to feature learning Traditionally hyper-parameter optimization has been
the job of humans because they can be very efficient in regimes where only a few
trials are possible Presently computer clusters and GPU processors make it possible to run more trials and we show that algorithmic approaches can find better
results We present hyper-parameter optimization results on tasks of training neural networks and deep belief networks DBNs We optimize hyper-parameters
using random search and two new greedy sequential methods based on the expected improvement criterion Random search has been shown to be sufficiently
efficient for learning neural networks for several datasets but we show it is unreliable for training DBNs The sequential algorithms are applied to the most difficult
DBN learning problems from and find significantly better results than the best
previously reported This work contributes novel techniques for making response
surface models in which many elements of hyper-parameter assignment
are known to be irrelevant given particular values of other elements
Introduction
Models such as Deep Belief Networks DBNs stacked denoising autoencoders convolutional networks as well as classifiers based on sophisticated feature extraction techniques
have from ten to perhaps fifty hyper-parameters depending on how the experimenter chooses to
parametrize the model and how many hyper-parameters the experimenter chooses to fix at a reasonable default The difficulty of tuning these models makes published results difficult to reproduce
and extend and makes even the original investigation of such methods more of an art than a science
Recent results such as and demonstrate that the challenge of hyper-parameter optimization in large and multilayer models is a direct impediment to scientific progress These works
have advanced state of the art performance on image classification problems by more concerted
hyper-parameter optimization in simple algorithms rather than by innovative modeling or machine
learning strategies It would be wrong to conclude from a result such as that feature learning
is useless Instead hyper-parameter optimization should be regarded as a formal outer loop in the
learning process A learning algorithm as a functional from data to classifier taking classification
problems as an example includes a budgeting choice of how many CPU cycles are to be spent
on hyper-parameter exploration and how many CPU cycles are to be spent evaluating each hyperparameter choice by tuning the regular parameters The results of and suggest that
with current generation hardware such as large computer clusters and GPUs the optimal alloca1
tion of CPU cycles includes more hyper-parameter exploration than has been typical in the machine
learning literature
Hyper-parameter optimization is the problem of optimizing a loss function over a graph-structured
configuration space In this work we restrict ourselves to tree-structured configuration spaces Configuration spaces are tree-structured in the sense that some leaf variables the number of hidden
units in the 2nd layer of a DBN are only well-defined when node variables a discrete choice of
how many layers to use take particular values Not only must a hyper-parameter optimization algorithm optimize over variables which are discrete ordinal and continuous but it must simultaneously
choose which variables to optimize
In this work we define a configuration space by a generative process for drawing valid samples
Random search is the algorithm of drawing hyper-parameter assignments from that process and
evaluating them Optimization algorithms work by identifying hyper-parameter assignments that
could have been drawn and that appear promising on the basis of the loss function?s value at other
points This paper makes two contributions Random search is competitive with the manual
optimization of DBNs in and Automatic sequential optimization outperforms both manual
and random search
Section covers sequential model-based optimization and the expected improvement criterion Section introduces a Gaussian Process based hyper-parameter optimization algorithm Section introduces a second approach based on adaptive Parzen windows Section describes the problem of
DBN hyper-parameter optimization and shows the efficiency of random search Section shows
the efficiency of sequential optimization on the two hardest datasets according to random search
The paper concludes with discussion of results and concluding remarks in Section and Section
Sequential Model-based Global Optimization
Sequential Model-Based Global Optimization SMBO algorithms have been used in many applications where evaluation of the fitness function is expensive In an application where the true
fitness function is costly to evaluate model-based algorithms approximate with a surrogate that is cheaper to evaluate Typically the inner loop in an SMBO algorithm is the numerical
optimization of this surrogate or some transformation of the surrogate The point that maximizes
the surrogate its transformation becomes the proposal for where the true function should be
evaluated This active-learning-like algorithm template is summarized in Figure SMBO algorithms differ in what criterion they optimize to obtain given a model surrogate of and in
they model via observation history H.
SMBO M0
For to
argminx
Evaluate
Expensive step
Fit a new model Mt to H.
return
Figure The pseudo-code of generic Sequential Model-Based Optimization
The algorithms in this work optimize the criterion of Expected Improvement Other criteria have been suggested such as Probability of Improvement and Expected Improvement
minimizing the Conditional Entropy of the Minimizer and the bandit-based criterion described
in We chose to use the EI criterion in our work because it is intuitive and has been shown to
work well in a variety of settings We leave the systematic exploration of improvement criteria for
future work Expected improvement is the expectation under some model of RN that
will exceed negatively some threshold
EIy
max(y
The contribution of this work is two novel strategies for approximating by modeling a hierarchical Gaussian Process and a tree-structured Parzen estimator These are described in Section
and Section respectively
The Gaussian Process Approach
Gaussian Processes have long been recognized as a good method for modeling loss functions in
model-based optimization literature Gaussian Processes GPs are priors over functions
that are closed under sampling which means that if the prior distribution of is believed to be a GP
with mean and kernel the conditional distribution of knowing a sample
of its values is also a GP whose mean and covariance function are analytically derivable GPs with
generic mean functions can in principle be used but it is simpler and sufficient for our purposes
to only consider zero mean processes We do this by centering the function values in the considered data sets Modelling linear trends in the GP mean leads to undesirable extrapolation in
unexplored regions during SMBO
The above mentioned closedness property along with the fact that GPs provide an assessment of
prediction uncertainty incorporating the effect of data scarcity make the GP an elegant candidate
for both finding candidate Figure step and fitting a model Mt Figure step The runtime
of each iteration of the GP approach scales cubically in and linearly in the number of variables
being optimized however the expense of the function evaluations typically dominate even
this cubic cost
Optimizing EI in the GP
We model with a GP and set to the best value found after observing min{f
The model pM in is then the posterior GP knowing H. The EI function in encapsulates a compromise between regions where the mean function is close to or better than and
under-explored regions where the uncertainty is high
EI functions are usually optimized with an exhaustive grid search over the input space or a Latin
Hypercube search in higher dimensions However some information on the landscape of the EI criterion can be derived from simple computations it is always non-negative and zero at training
points from it inherits the smoothness of the kernel which is in practice often at least once
differentiable and noticeably the EI criterion is likely to be highly multi-modal especially as
the number of training points increases The authors of used the preceding remarks on the
landscape of EI to design an evolutionary algorithm with mixture search specifically aimed at optimizing EI that is shown to outperform exhaustive search for a given budget in EI evaluations We
borrow here their approach and go one step further We keep the Estimation of Distribution EDA
approach on the discrete part of our input space categorical and discrete hyper-parameters
where we sample candidate points according to binomial distributions while we use the Covariance
Matrix Adaptation Evolution Strategy CMA-ES for the remaining part of our input space
continuous hyper-parameters CMA-ES is a state-of-the-art gradient-free evolutionary algorithm
for optimization on continuous domains which has been shown to outperform the Gaussian search
EDA. Notice that such a gradient-free approach allows non-differentiable kernels for the GP regression We do not take on the use of mixtures in but rather restart the local searches several times
starting from promising places The use of tesselations suggested by is prohibitive here as our
task often means working in more than dimensions thus we start each local search at the center
of mass of a simplex with vertices randomly picked among the training points
Finally we remark that all hyper-parameters are not relevant for each point For example a DBN
with only one hidden layer does not have parameters associated to a second or third layer Thus it
is not enough to place one GP over the entire space of hyper-parameters We chose to group the
hyper-parameters by common use in a tree-like fashion and place different independent GPs over
each group As an example for DBNs this means placing one GP over common hyper-parameters
including categorical parameters that indicate what are the conditional groups to consider three
GPs on the parameters corresponding to each of the three layers and a few 1-dimensional GPs over
individual conditional hyper-parameters like ZCA energy Table for DBN parameters
Tree-structured Parzen Estimator Approach TPE
Anticipating that our hyper-parameter optimization tasks will mean high dimensions and small fitness evaluation budgets we now turn to another modeling strategy and EI optimization scheme for
the SMBO algorithm Whereas the Gaussian-process based approach modeled directly this
strategy models and
Recall from the introduction that the configuration space is described by a graph-structured generative process first choose a number of DBN layers then choose the parameters for each
The tree-structured Parzen estimator TPE models by transforming that generative process
replacing the distributions of the configuration prior with non-parametric densities In the experimental section we will see that the configuation space is described using uniform log-uniform
quantized log-uniform and categorical variables In these cases the TPE algorithm makes the
following replacements uniform truncated Gaussian mixture log-uniform exponentiated
truncated Gaussian mixture categorical re-weighted categorical Using different observations
in the non-parametric densities these substitutions represent a learning algorithm
that can produce a variety of densities over the configuration space The TPE defines
using two such densities
if
if
where is the density formed by using the observations such that corresponding loss
was less than and is the density formed by using the remaining observations
Whereas the GP-based approach favoured quite an aggressive typically less than the best observed loss the TPE algorithm depends on a that is larger than the best observed so that
some points can be used to form The TPE algorithm chooses to be some quantile of the
observed values so that p(y but no specific model for is necessary By maintaining sorted lists of observed variables in the runtime of each iteration of the TPE algorithm can
scale linearly in and linearly in the number of variables dimensions being optimized
Optimizing EI in the TPE algorithm
The parametrization of as in the TPE algorithm was chosen to facilitate the
optimization of EI.
EIy
y)p(y|x)dy
dy
By construction p(y and p(x|y)p(y)dy Therefore
y)p(x|y)p(y)dy
y)p(y)dy
p(y)dy
so that finally EIy
p(y)dy
This last expression
shows that to maximize improvement we would like points with high probability under
and low probability under The tree-structured form of and makes it easy to draw many
candidates according to and evaluate them according to On each iteration the algorithm
returns the candidate with the greatest EI.
Details of the Parzen Estimator
The models and are hierarchical processes involving discrete-valued and continuousvalued variables The Adaptive Parzen Estimator yields a model over by placing density in
the vicinity of observations H. Each continuous hyper-parameter was
specified by a uniform prior over some interval or a Gaussian or a log-uniform distribution
The TPE substitutes an equally-weighted mixture of that prior with Gaussians centered at each of
the B. The standard deviation of each Gaussian was set to the greater of the distances to the
left and right neighbor but clipped to remain in a reasonable range In the case of the uniform the
points a and were considered to be potential neighbors For discrete variables supposing the prior
was a vector of probabilities pi the posterior vector elements were proportional to pi Ci
where Ci counts the occurrences of choice in B. The log-uniform hyper-parameters were treated
as uniforms in the log domain
Table Distribution over DBN hyper-parameters for random sampling Options separated by
such as pre-processing and including the random seed are weighted equally Symbol means
uniform means Gaussian-distributed and log means uniformly distributed in the log-domain
CD also known as stands for contrastive divergence the algorithm used to initialize the layer
parameters of the DBN.
Whole model
Per-layer
Parameter
Prior
Parameter
Prior
pre-processing
raw or ZCA
hidden units
log
ZCA energy
init
or a2
random seed
choices
a
algo A or text
classifier learn rate
log
algo A coef
classifier anneal start log
CD epochs
log
classifier penalty
or log
CD learn rate
log
layers
to
CD anneal start log
batch size
or
CD sample data yes or no
Random Search for Hyper-Parameter Optimization in DBNs
One simple but recent step toward formalizing hyper-parameter optimization is the use of random
search showed that random search was much more efficient than grid search for optimizing
the parameters of one-layer neural network classifiers In this section we evaluate random search
for DBN optimization compared with the sequential grid-assisted manual search carried out in
We chose the prior listed in Table to define the search space over DBN configurations The details
of the datasets the DBN model and the greedy layer-wise training procedure based on CD are
provided in This prior corresponds to the search space of except for the following differences
we allowed for ZCA pre-processing we allowed for each layer to have a different size
we allowed for each layer to have its own training parameters for CD we allowed for the
possibility of treating the continuous-valued data as either as Bernoulli means more theoretically
correct or Bernoulli samples more typical in the CD algorithm and we did not discretize the
possible values of real-valued hyper-parameters These changes expand the hyper-parameter search
problem while maintaining the original hyper-parameter search space as a subset of the expanded
search space
The results of this preliminary random search are in Figure Perhaps surprisingly the result of
manual search can be reliably matched with 32 random trials for several datasets The efficiency
of random search in this setting is explored further in Where random search results match
human performance it is not clear from Figure whether the reason is that it searched the original
space as efficiently or that it searched a larger space where good performance is easier to find But
the objection that random search is somehow cheating by searching a larger space is backward
the search space outlined in Table is a natural description of the hyper-parameter optimization
problem and the restrictions to that space by were presumably made to simplify the search
problem and make it tractable for grid-search assisted manual search Critically both methods train
DBNs on the same datasets
The results in Figure indicate that hyper-parameter optimization is harder for some datasets For
example in the case of the MNIST rotated background images dataset MRBI random sampling
appears to converge to a maximum relatively quickly best models among experiments of 32 trials
show little variance in performance but this plateau is lower than what was found by manual search
In another dataset convex the random sampling procedure exceeds the performance of manual
search but is slow to converge to any sort of plateau There is considerable variance in generalization
when the best of 32 models is selected This slow convergence indicates that better performance is
probably available but we need to search the configuration space more efficiently to find it The
remainder of this paper explores sequential optimization strategies for hyper-parameter optimization
for these two datasets convex and MRBI
Sequential Search for Hyper-Parameter Optimization in DBNs
We validated our GP approach of Section by comparing with random sampling on the Boston
Housing dataset a regression task with points made of 13 scaled input variables and a scalar
mnist basic
mnist background images
mnist rotated background images
accuracy
accuracy
accuracy
32
64
experiment size trials
32
64
32
64
experiment size trials
32
64
accuracy
accuracy
accuracy
rectangles images
experiment size trials
rectangles
experiment size trials
convex
32
64
experiment size trials
32
64
experiment size trials
Figure Deep Belief Network DBN performance according to random search Random
search is used to explore up to 32 hyper-parameters Table Results found using a
grid-search-assisted manual search over a similar domain with an average 41 trials are
given in green 1-layer DBN and red 3-layer DBN Each box-plot for
shows the distribution of test set performance when the best model among random trials
is selected The datasets convex and mnist rotated background images are used for
more thorough hyper-parameter optimization
regressed output We trained a Multi-Layer Perceptron MLP with hyper-parameters including
learning rate and penalties size of hidden layer number of iterations whether a PCA preprocessing was to be applied whose energy was the only conditional hyper-parameter Our
results are depicted in Figure The first iterations were made using random sampling while
from the on we differentiated the random samples from the GP approach trained on the updated
history The experiment was repeated times Although the number of points is particularly small
compared to the dimensionality the surrogate modelling approach finds noticeably better points than
random which supports the application of SMBO approaches to more ambitious tasks and datasets
Applying the GP to the problem of optimizing DBN performance we allowed random restarts to
the CMA+ES algorithm per proposal and up to iterations of conjugate gradient method in
fitting the length scales of the GP. The squared exponential kernel was used for every node
The CMA-ES part of GPs dealt with boundaries using a penalty method the binomial sampling part
dealt with it by nature The GP algorithm was initialized with randomly sampled points in H.
After trials the prediction of a point using this GP took around seconds
For the TPE-based algorithm we chose and picked the best among candidates drawn
from on each iteration as the proposal After trials the prediction of a point using
this TPE algorithm took around seconds TPE was allowed to grow past the initial bounds used
with for random sampling in the course of optimization whereas the GP and random search were
restricted to stay within the initial bounds throughout the course of optimization The TPE algorithm
was also initialized with the same randomly sampled points as were used to seed the GP.
Parallelizing Sequential Search
Both the GP and TPE approaches were actually run asynchronously in order to make use of multiple
compute nodes and to avoid wasting time waiting for trial evaluations to complete For the GP approach the so-called constant liar approach was used each time a candidate point was proposed
a fake fitness evaluation equal to the mean of the y?s within the training set was assigned temporarily until the evaluation completed and reported the actual loss For the TPE approach
we simply ignored recently proposed points and relied on the stochasticity of draws from to
provide different candidates from one iteration to the next The consequence of parallelization is
that each proposal is based on less feedback This makes search less efficient though faster in
terms of wall time
26
Best value so far
24
22
TPE
GP
Manual
Random
18
convex
MRBI
Table The test set classification error of
the best model found by each search algorithm on each problem Each search algorithm was allowed up to trials The manual searches used 82 trials for convex and 27
trials MRBI
Time
Figure After time GP optimizing
the MLP hyper-parameters on the Boston
Housing regression task Best minimum
found so far every iterations against
time Red GP Blue Random Shaded
areas one-sigma error bars
Runtime per trial was limited to hour of GPU computation regardless of whether execution was on
a GTX or The difference in speed between the slowest and fastest machine was
roughly two-fold in theory but the actual efficiency of computation depended also on the load of the
machine and the configuration of the problem the relative speed of the different cards is different in
different hyper-parameter configurations With the parallel evaluation of up to five proposals from
the GP and TPE algorithms each experiment took about 24 hours of wall time using five GPUs
Discussion
The trajectories constructed by each algorithm up to steps are illustrated in Figure and
compared with random search and the manual search carried out in The generalization scores
of the best models found using these algorithms and others are listed in Table On the convex
dataset classification both algorithms converged to a validation score of error In
generalization TPE?s best model had error and GP?s best had TPE?s best was significantly better than both manual search and random search with trials On the
MRBI dataset classification random search was the worst performer error the GP
approach and manual search approximately tied error while the TPE algorithm found a new
best result error The models found by the TPE algorithm in particular are better than previously found ones on both datasets The GP and TPE algorithms were slightly less efficient than
manual search GP and EI identified performance on par with manual search within trials the
manual search of used 82 trials for convex and 27 trials for MRBI
There are several possible reasons for why the TPE approach outperformed the GP approach in
these two datasets Perhaps the inverse factorization of is more accurate than the in
the Gaussian process Perhaps conversely the exploration induced by the TPE?s lack of accuracy
turned out to be a good heuristic for search Perhaps the hyper-parameters of the GP approach itself
were not set to correctly trade off exploitation and exploration in the DBN configuration space More
empirical work is required to test these hypotheses Critically though all four SMBO runs matched
or exceeded both random search and a careful human-guided search which are currently the state
of the art methods for hyper-parameter optimization
The GP and TPE algorithms work well in both of these settings but there are certainly settings
in which these algorithms and in fact SMBO algorithm in general would not be expected to do
well Sequential optimization algorithms work by leveraging structure in observed pairs It is
possible for SMBO to be arbitrarily bad with a bad choice of It is also possible to be slower
than random sampling at finding a global optimum with a apparently good if it extracts
structure in that leads only to a local optimum
Conclusion
This paper has introduced two sequential hyper-parameter optimization algorithms and shown them
to meet or exceed human performance and the performance of a brute-force random search in two
difficult hyper-parameter optimization tasks involving DBNs We have relaxed standard constraints
equal layer sizes at all layers on the search space and fall back on a more natural hyperparameter space of 32 variables including both discrete and continuous variables in which many
Dataset convex
Dataset mnist rotated background images
manual
GP
TPE
error fraction incorrect
error fraction incorrect
manual
GP
TPE
time trials
time trials
Figure Efficiency of Gaussian Process-based and graphical model-based TPE sequential optimization algorithms on the task of optimizing the validation set performance
of a DBN of up to three layers on the convex task left and the MRBI task right The
dots are the elements of the trajectory produced by each SMBO algorithm The solid
coloured lines are the validation set accuracy of the best trial found before each point in
time Both the TPE and GP algorithms make significant advances from their random initial conditions and substantially outperform the manual and random search methods A
confidence interval about the best validation means on the convex task extends
above and below each point and on the MRBI task extends above and below each
point The solid black line is the test set accuracy obtained by domain experts using a
combination of grid search and manual search The dashed line is the quantile of validation performance found among trials sampled from our prior distribution
Table estimated from and random trials on the two datasets respectively
variables are sometimes irrelevant depending on the value of other parameters the number of
layers In this 32-dimensional search problem the TPE algorithm presented here has uncovered new
best results on both of these datasets that are significantly better than what DBNs were previously
believed to achieve Moreover the GP and TPE algorithms are practical the optimization for each
dataset was done in just 24 hours using five GPU processors Although our results are only for
DBNs our methods are quite general and extend naturally to any hyper-parameter optimization
problem in which the hyper-parameters are drawn from a measurable set
We hope that our work may spur researchers in the machine learning community to treat the hyperparameter optimization strategy as an interesting and important component of all learning algorithms The question of How well does a DBN do on the convex task is not a fully specified
empirically answerable question different approaches to hyper-parameter optimization will give
different answers Algorithmic approaches to hyper-parameter optimization make machine learning
results easier to disseminate reproduce and transfer to other domains The specific algorithms we
have presented here are also capable at least in some cases of finding better results than were previously known Finally powerful hyper-parameter optimization algorithms broaden the horizon of
models that can realistically be studied researchers need not restrict themselves to systems of a few
variables that can readily be tuned by hand
The TPE algorithm presented in this work as well as parallel evaluation infrastructure is available
as BSD-licensed free open-source software which has been designed not only to reproduce the
results in this work but also to facilitate the application of these and similar algorithms to other
hyper-parameter optimization problems.1
Acknowledgements
This work was supported by the National Science and Engineering Research Council of Canada
Compute Canada and by the grant of the French National Research Agency
GPU implementations of the DBN model were provided by Theano
Hyperopt software package https://github.com/jaberg/hyperopt

<<----------------------------------------------------------------------------------------------------------------------->>

