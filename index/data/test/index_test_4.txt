query sentence: 2dimensional non-randomized column-vector
---------------------------------------------------------------------
title: 5053-firing-rate-predictions-in-optimal-balanced-networks.pdf

Firing rate predictions in optimal balanced networks
Sophie Den`eve
Group for Neural Theory
Ecole
Normale Sup?erieure
Paris France
sophie.deneve@ens.fr
David G.T. Barrett
Group for Neural Theory
Ecole
Normale Sup?erieure
Paris France
david.barrett@ens.fr
Christian K. Machens
Champalimaud Neuroscience Programme
Champalimaud Centre for the Unknown
Lisbon Portugal
christian.machens@neuro.fchampalimaud.org
Abstract
How are firing rates in a spiking network related to neural input connectivity and
network function This is an important problem because firing rates are a key
measure of network activity in both the study of neural computation and neural
network dynamics However it is a difficult problem because the spiking mechanism of individual neurons is highly non-linear and these individual neurons
interact strongly through connectivity We develop a new technique for calculating firing rates in optimal balanced networks These are particularly interesting
networks because they provide an optimal spike-based signal representation while
producing cortex-like spiking activity through a dynamic balance of excitation and
inhibition We can calculate firing rates by treating balanced network dynamics
as an algorithm for optimising signal representation We identify this algorithm
and then calculate firing rates by finding the solution to the algorithm Our firing
rate calculation relates network firing rates directly to network input connectivity
and function This allows us to explain the function and underlying mechanism of
tuning curves in a variety of systems
Introduction
The firing rate of a neuron is arguably the most important characterisation of both neural network
dynamics and neural computation and has been ever since the seminal recordings of Adrian and
Zotterman in which the firing rate of a neuron was observed to increase with muscle tension A
large sometimes bewildering diversity of firing rate responses to stimuli have since been observed
ranging from sigmoidal-shaped tuning curves to bump-shaped tuning curves with
much diversity in between What is the computational role of these firing rate responses and how
are firing rates determined by neuron dynamics network connectivity and neural input
There have been many attempts to answer these questions using a variety of experimental and
theoretical techniques However most approaches have struggled to deal with the non-linearity of
neural spike-generation mechanisms and the strong interaction between neurons as mediated through
network connectivity Significant progress has been made using linear approximations For example
experimentally recorded firing rates in a variety of systems have been described using the linear
receptive field which captures the linear relationship between stimulus and firing rate response
However in recent years it has been found that this linear approximation often fails to capture
important aspects of neural activity Similarly in theoretical studies linear approximations
have been used to simplify non-linear firing rate calculations in a variety of network models using
Taylor Series approximations and more recently using linear response theory These
calculations have led to important insights into how neural network connectivity and input determine
firing rates Again however these calculations only apply to a restricted subset of situations where
the linearising assumptions apply
We develop a new technique for calculating firing rates by directly identifying the non-linear structure of tightly balanced networks Balanced network theory has come to be regarded as the standard
model of cortical activity accounting for a large proportion of observed activity through
a dynamic balance of excitation and inhibition Recently it was found that tightly balanced
networks are synonymous with efficient coding in which a signal is represented optimally subject
to metabolic costs This observation allows us here to interpret balanced network activity as
an optimisation algorithm We can then directly identify that the non-linear relationship between
firing rates input connectivity and neural computation is provided by this algorithm We use this
technique to calculate firing rates in a variety of balanced network models thereby exploring the
computational role and underlying network mechanisms of monotonic firing rate tuning curves
bump-shaped tuning curves and tuning curve inhomogeneity
Optimal balanced network models
We calculate firing rates in a balanced network consisting of recurrently connected leaky
integrate-and-fire neurons
The network is driven by an input signal I
Ik IM where Ik is the th input and is the dimension of the input In response toPthis input neurons produce spike trains denoted by
si sN where
si tik is the spike train of neuron with spike times tik A spike is produced
whenever the membrane potential Vi exceeds the spiking threshold Ti of neuron This simple
spike rule captures the essence of a neural spike-generation mechanism The membrane potential
has the following dynamics
dVi
Fij Ij
ik sk
dt
where is the neuron leak ik is connection strength from neuron to neuron and Fij is the
connection strength from input to neuron When a neuron spikes the membrane potential
is reset to Ri Ti ii This is written in equation as a self-connection Throughout this work
we focus on networks where connectivity is symmetric this simplifies our analysis although in
certain cases we can generalise to non-symmetric matrices
We are interested in networks where a balance of excitation and inhibition coincides with optimal signal representation Not all choices of network connectivity and spiking thresholds will give
both but if certain conditions are satisfied this can be possible Before we proceed to our
firing rate calculation we must derive these conditions
We begin by calculating the sum total of excitatory and inhibitory input received by neurons in our
network This is given by solving equation implicitly
Vi
ik rk
Fij
where rk is a temporal filtering of the
th
neuron?s spike train
sk t0 dt0
and is a temporal filtering of the th input
Ij t0 dt0
rk
All the excitatory and inhibitory inputs received by neuron are included in this summation Eqn
This can be rewritten as the slope of a loss function as follows
dE(r
Vi
dri
where
rT 2rT Fx
and is a constant
Now we can use this expression to derive the conditions that connectivity must satisfy so that the
network operates in an optimal balanced state In balanced networks excitation and inhibition cancel
to produce an input that is the same order of magnitude as the spiking threshold This is very small
relative to the magnitude of excitation or inhibition alone In tightly balanced networks
which we consider this cancellation is so precise that Vi in the large network limit for all
active neurons 17 Now using equation we can see that this tight balance condition is
equivalent to saying that our loss function Eqn is minimised
This has two implications for our choice of network connectivity and spiking thresholds First
the loss function must have a minimum To guarantee this we require to be positive definite
Secondly the spiking threshold of each neuron must be chosen so that each spike acts to minimise
the cost function This spiking condition can be written as E(no spike E(with spike Using
equation this can be rewritten as E(no spike E(no spike kk Finally
time sec
Figure Optimal balanced network example Schematic of a balanced neural network providing an optimal spike-based representation
of a signal A tightly balanced network can
produce an output
blue top panel that closely matches the signal black top panel Population spiking activity is represented here using a raster plot middle panel where each spike is
represented with a dot For a randomly chosen neuron red middle panel we plot the total excitatory input green bottom panel and the total inhibitory input red bottom panel The sum of
excitation and inhibition black bottom panel fluctuates about the spiking threshold thin black line
bottom panel indicating that this network is tightly balanced A spike is produced whenever this
sum exceeds the spiking threshold Firing rate tuning curves are measured during simulations of
our balanced network Each line represents the tuning curve of a single neuron The representation
error at each value of is given by equation
cancelling terms and using equation we can write our spiking condition as Vk
Therefore the spiking threshold for each neuron must be set to Tk though this condition
can be relaxed considerably if our loss function has an additional linear cost term1 Once these
conditions are satisfied our network is tightly balanced
We are interested in networks that are both tightly balanced and optimal Now we can see from
equation that the balance of excitation and inhibition coincides with the optimisation of our loss
function Eqn This is an important result because it relates balanced network dynamics to a
neural computation Specifically it allows us to interpret the spiking activity of our tightly balanced
network as an algorithm that optimises a loss function Eqn
This is interesting because this optimisation can be easily mapped onto many useful computations
A particularly interesting example is given by FFT where I is the identity matrix
17 In recent work it was shown that this connectivity can be learnt using a spike timingdependent plasticity rule Here we use this connectivity to rewrite our loss function Eqn
as follows
ri2
where
FT
The second term of equation is a metabolic cost term that penalises neurons for spiking excessively
where
and the first term quantifies the difference between the signal value and a linear read-out
is computed using the linear decoder FT Eqn Therefore a network with this connectivity
that is close to the
produces spike trains that optimise equation thereby producing an output
signal value Throughout the remainder of this work we will focus on optimal balanced networks
with this form of connectivity
We illustrate the properties of this system by simulating a network of neurons We find that
our network produces spike trains middle panel that represent with great accuracy
across a broad range of signal values top panel As expected this optimal performance
coincides with a tight balance of excitation and inhibition bottom panel reminiscent
of cortical observations In this example our network has been optimised to represent a 2dimensional signal We measure firing rate tuning curves using a fixed value of
while varying We use this signal because it can produce interesting non-linear tuning curves
especially at signal values where neurons fall silent In the next section we will attempt
to understand this tuning curve non-linearity by calculating firing rates analytically
Firing rate analysis with quadratic programming
Our goal is to calculate the firing rates of all the neurons in these tightly balanced network models as a function of the network input the recurrent network connectivity and the feedforward
connectivity F. On the surface this may seem to be a difficult problem because individual neurons
have complicated non-linear integrate-and-fire dynamics and they interact strongly through network
connectivity However the loss function relationship that we developed above allows us now to
circumvent these problems
There are many possible firing rate measures used in experiments and theoretical studies Usually a
box-shaped temporal averaging window is used We define the firing rate of a neuron to be
fk
sk t0 dt0
This is an exponentially weighted temporal average2 with timescale We have chosen this
temporal average because it matches the dynamics of synaptic filters in our neural network Eqn
Suppose that our network optimises the following cost function rT 2rT Fx bT
where is a vector of positive linear weights Then we find that the optimal spiking thresholds for this network
are given by Ti bi Therefore we can apply our techniques to all networks with
thresholds Ti
In this case the firing rate timescale is very short because is the membrane potential leak However we
can easily generalise our framework so that this timescale can be as long as the slowest synaptic process
allowing us to write fi ri Here we need to multiply by to ensure that our firing rates
are reported in units of spikes per second
We can now calculate firing rates using this relationship and by exploiting the algorithmic nature
of tightly balanced networks These networks produce spike trains that minimise our loss function
Eqn Therefore the firing rates of our network are those that minimise E(f under the
constraint that firing rates must be positive
fi arg min E(f
fi
This firing rate prediction is the solution to a constrained optimisation problem known as quadratic
programming The optimisation is quadratic because our loss function is a quadratic function
of and it is constrained because firing rates are positive valued quantities by definition
We illustrate this firing rate prediction using a simple two-neuron network with recurrent connectivity given by FT I as before We simulate this system and measure the spike-train firing
rates for both neurons a left panel We then use equation to obtain a theoretical prediction for firing rates We find that our firing rate prediction matches the spike-train measurement with
great accuracy a middle panel and right panel
We can now use our firing rate solution to understand the relationship between firing rates input
connectivity and function When both neurons are active we can solve equation exactly to see
that firing rates are related to network connectivity according to Fx. When one of the
neurons becomes silent the other neuron must compensate by adjusting its firing rate slope For
example when neuron becomes silent we have f1 and the firing rate of neuron increases
to f2 FT2 where F2 denotes the second row of F. Similarly when neuron
prediction
simulation measurement
f2
f2
f2
f1
f1
f1
Figure Calculating firing rates in a two-neuron example Tuning curve measurements are
obtained from a simulation of a two-neuron network left top The representation error for
this network is given at each signal value left bottom Tuning curve predictions are obtained
using quadratic programming middle top with predicted representation error middle bottom
Predicted firing rates closely match measured firing rates for both neurons and for all signal values
right A phase diagram of the network activity during a simulation left panel Firing rates
evolve from a silent state towards the minimum of the cost function red cross left
panel Here they fluctuate about the minimum increasing in discrete steps of size and decreasing
exponentially left panel inset).We also measure the firing rate trajectory right panel as the network
evolves towards the minimum of the cost function blue cross right panel where neuron
is silent
becomes silent we have f2 and the firing rate of neuron increases to f1 FT1
where F1 is the first row of F. This non-linear change in firing rates is caused by the positivity
constraint It can be understood functionally as an attempt by the network to represent accurately
within the constraints of the system
In larger networks our firing rate prediction is more difficult to write down analytically because there
are so many interactions between individual neurons and the positivity constraint Nonetheless we
can make a number of general observations about tuning curve shape In general we can interpret
tuning curve shape to be the solution of a quadratic programming problem which can be written as
a piece-wise linear function where is a matrix whose entries depend on the
region of signal space occupied by For example in the two-neuron system that we just discussed
the signal space is partitioned into three regions one region where neuron is active and where
neuron is silent a second region where both neurons are active and a third region where neuron
is silent and neuron is active a left panel In each region there is a different linear
relationship between the signal and the firing rates The boundaries of these regions occur at points
in signal space where an active neuron becomes silent where a silent neuron becomes active At
most there will be such regions
We can also use quadratic programming to describe the spiking dynamics underlying these nonlinear networks Returning to our two-neuron example we measure the temporal evolution of the
firing rates f1 and f2 We find that if we initialise the network to a sub-optimal state the firing rates
rapidly evolve toward the optimum in a series of discrete steps of size left panel The
step-size is because when neuron spikes ri ri according to equation and therefore
fi fi according to equation Once the network has reached the optimal state it is impossible
for it to remain there The firing rates begin to decay exponentially because our firing rate definition
is an exponentially weighted summation Eqn middle panel Eventually when the
firing rate has decayed too far from the optimal solution another spike is fired and the network moves
closer to the optimum In this way spiking dynamics can be interpreted as a quadratic programming
algorithm The firing rate continues to fluctuate around the optimal spiking value These fluctuations
are noisy in that they are dependent on initial conditions of the network However this noise has an
unusual algorithmic structure that it is not well characterised by standard probabilistic descriptions
of spiking irregularity
Analysing tuning curve shape with quadratic programming
Now that we have a framework for relating firing rates to network connectivity and input we can
explore the computational function of tuning curve shapes and the network mechanisms that generate these tuning curves We will investigate systems that have monotonic tuning curves and systems
that have bump-shaped tuning curves which together constitute a large proportion of firing rate
observations
We begin by considering a system of monotonic tuning curves similar to the examples that we have
considered already where recurrent connectivity is given by FFT In these systems
the recurrent connectivity and hence the tuning curve shape is largely determined by the form of the
feedforward matrix F. This matrix also determines the contribution of tuning curves to computational function through its role as a linear decoder for signal representation Eqn We illustrate
this by simulating the response of our network to a 2-dimensional signal where
is varied and is fixed using three different configurations of This system produces
monotonically increasing and decreasing tuning curves We find that neurons with positive
values of have positive firing rate slopes blue tuning curves and neurons with negative
values have negative firing rate slopes red tuning curves If the values of are regularly
spaced then the tuning curves of individual neurons are regularly spaced and if we manipulate this
regularity by adding some random noise to the connectivity we obtain inhomogeneous and highly
irregular tuning curves This inhomogeneity has little effect on the representation error
This inhomogeneous monotonic tuning is reminiscent of tuning in many neural systems including
the oculomotor system The oculomotor system represents eye position using neurons with
negative slopes to represent left side eye positions and neurons with positive slopes to represent
right side eye positions To relate our model to this system the signal variable can be interpreted
as eye-position with zero representing the central eye position and with positive and negative values
simulation measurement
prediction
Figure The relationship between firing rates stimulus and connectivity in a network of neurons
Each dot represents the contribution of a neuron to a signal representation when the firing rate
is Hz column Here we consider signals along a straight line thin black line We
simulate a network of neurons and measure firing rates column These measurements closely
match our algorithmically predicted firing rates column where each point in the 4th column
represents the firing rate of an individual neuron for a given stimulus Similar to except
that some noise is added to the connectivity The representation error bottom panels column
and column is similar to the network without connectivity noise Similar to except
that we consider signals along a circle thin black line Each dot represents the contribution of a
neuron to a signal representation when the firing rate is Hz column This signal
produces bump-shaped tuning curves column which we can also predict accurately and
4th column
leak
membrane potential noise
Figure Performance of quadratic programming in firing rate prediction The mean prediction
error absolute difference between each prediction and measurement averaged over neurons and
over seconds increases with bottom line The standard deviation of the prediction becomes
much larger with top line The mean prediction error bottom line and standard deviation of
the prediction error top line also increase with noise However the prediction error remains less
that Hz.
of representing right and left side eye positions respectively Now we can use the relationship
that we have developed between tuning curves and computational function to interpret oculomotor
tuning as an attempt to represent eye positions optimally
Bump-shaped tuning curves can be produced by networks representing circular variables cos
sin where is the orientation of the signal As before the tuning curves of
individual neurons are regularly spaced if the values of are regularly spaced If we add some
noise to the connectivity the tuning curves become inhomogeneous and highly irregular Again
this inhomogeneity has little effect on the signal representation error
In all the above examples our firing rate predictions closely match firing rate measurements from
network simulations The success of our algorithmic approach in calculating firing rates
depends on the success of spiking networks in algorithmically optimising a cost function The
resolution of this spiking algorithm is determined by the leak and membrane potential noise If
is large the firing rate prediction error will have large fluctuations about the optimal firing rate
value However the average prediction error averaged over time and neurons remains
small Similarly membrane potential noise3 increases fluctuations about the optimal firing rate but
the average prediction error remains small until the noise is large enough to generate spikes without
any input
Discussion and Conclusions
We have developed a new algorithmic technique for calculating firing rates in tightly balanced networks Our approach does not require us to make any linearising approximations Rather we directly identify the non-linear relationship between firing rates connectivity input and optimal signal
representation Identifying such relationships is a long-standing problem in systems neuroscience
largely because the mathematical language that we use to describe information representation is
very different to the language that we use to describe neural network spiking statistics For tightly
balanced networks we have essentially solved this problem by matching the firing rate statistics of
neural activity to the structure of neural signal representation The non-linear relationship that we
identify is the solution to a quadratic programming problem
Previous studies have also interpreted firing rates to be the result of a constrained optimisation
problem but for a population coding model not for a network of spiking neurons In a more
recent study a spiking network was used to solve an optimisation problem although this network
required positive and negative spikes which is difficult to reconcile with biological spiking
The firing rate tuning curves that we calculate have allowed us to investigate poorly understood
features of experimentally recorded tuning curves In particular we have been able to evaluate
the impact of tuning curve inhomogeneity on neural computation This inhomogeneity often goes
unreported in experimental studies because it is difficulty to interpret and in theoretical studies it
is often treated as a form of noise that must be averaged out We find that tuning curve inhomogeneity
is not necessarily noise because it does not necessarily harm signal representation Therefore we
propose that tuning curves are inhomogeneous simply because they can be
Beyond the interpretation of tuning curve shape our quadratic programming approach to firing rate
calculations promises to be useful in other areas of neuroscience from data analysis where it may
be possible to train our framework using neural data so as to predict firing rate responses to sensory
stimuli to the study of computational neurodegeneration where the impact of neural damage on
tuning curves and computation may be characterised
Acknowledgements
We would like to thank Nuno Calaim for helpful comments on the manuscript Also we are grateful for generous funding from the Emmy-Noether grant of the Deutsche Forschungs-gemeinschaft
CKM and the Chaire dexcellence of the Agence National de la Recherche CKM as well as
a James Mcdonnell Foundation Award and EU grants BACS BIND and ERC FP7-PREDSPIKE
Membrane potential noise can be included in our network model by adding a Wiener process noise term to
our membrane potential equation Eqn We parametrise this noise with a constant

----------------------------------------------------------------

title: 5113-sketching-structured-matrices-for-faster-nonlinear-regression.pdf

Sketching Structured Matrices for
Faster Nonlinear Regression
David P. Woodruff
IBM Almaden Research Center
San Jose CA
dpwoodru@us.ibm.com
Haim Avron
Vikas Sindhwani
IBM T.J. Watson Research Center
Yorktown Heights NY
haimav,vsindhw}@us.ibm.com
Abstract
Motivated by the desire to extend fast randomized techniques to nonlinear lp regression we consider a class of structured regression problems These problems
involve Vandermonde matrices which arise naturally in various statistical modeling settings including classical polynomial fitting problems additive models and
approximations to recently developed randomized techniques for scalable kernel
methods We show that this structure can be exploited to further accelerate the
solution of the regression problem achieving running times that are faster than
input sparsity We present empirical results confirming both the practical value
of our modeling framework as well as speedup benefits of randomized regression
Introduction
Recent literature has advocated the use of randomization as a key algorithmic device with which
to dramatically accelerate statistical learning with lp regression or low-rank matrix approximation
techniques Consider the following class of regression problems
arg min kZx bkp where
x?C
where is a convex constraint set Rn?k is a sample-by-feature design matrix and Rn
is the target vector We assume henceforth that the number of samples is large relative to data
dimensionality The setting corresponds to classical least squares regression while
leads to least absolute deviations fit which is of significant interest due to its robustness
properties The constraint set can incorporate regularization When Rk and an optimal solution can be obtained in time O(nk log poly(k using randomization
which is much faster than an O(nk deterministic solver when is not too small dependence on
can be improved to if higher accuracy is needed Similarly a randomized solver
for l1 regression runs in time O(nk log poly(k
In many settings what makes such acceleration possible is the existence of a suitable oblivious
subspace embedding An OSE can be thought of as a data-independent random sketching
matrix Rt?n whose approximate isometry properties over a subspace over the column
space of imply that
kS(Zx b)kp kZx bkp for all
which in turn allows to be optimized over a sketched dataset of much smaller size without losing
solution quality Sketching matrices include Gaussian random matrices structured random matrices
which admit fast matrix multiplication via FFT-like operations and others
This paper is motivated by two questions which in our context turn out to be complimentary
Can additional structure in be non-trivially exploited to further accelerate runtime Clarkson
and Woodruff have recently shown that when is sparse and has nnz(Z nk non-zeros it
is possible to achieve much faster input-sparsity runtime using hashing-based sketching matrices Is it possible to further beat this time in the presence of additional structure on
Can faster and more accurate sketching techniques be designed for nonlinear and nonparametric
regression To see that this is intertwined
Pq with the previous question consider the basic problem
of fitting a polynomial model to a set of samples zi bi
Then the design matrix has Vandermonde structure which can potentially be exploited in a
regression solver It is particularly appealing to estimate non-parametric models on large datasets
Sketching algorithms have recently been explored in the context of kernel methods for nonparametric function estimation
To be able to precisely describe the structure on that we consider in this paper and outline our
contributions we need the following definitions
Definition Vandermonde Matrix Let be real numbers The Vandermonde matrix denoted Vq,n has the form
Vq,n
Vandermonde matrices of dimension require only implicit storage and admit
log2 matrix-vector multiplication time We also define the following matrix operator Tq which
maps a matrix A to a block-Vandermonde structured matrix
Definition Matrix Operator Given a matrix A Rn?d we define the following matrix
Tq Vq,n
Vq,n
Vq,n An,d
In this paper we consider regression problems Eqn. where can be written as
Tq
for an matrix A so that dq The operator Tq expands each feature column of the
original dataset A to columns of by applying monomial transformations upto degree This
lends a block-Vandermonde structure to Z. Such structure naturally arises in polynomial regression
problems but also applies more broadly to non-parametric additive models and kernel methods as
we discuss below With this setup the goal is to solve the following problem
Structured Regression Given A and with constant probability output a vector for which
kTq bkp kTq bkp
for an accuracy parameter where arg minx?C kTq bkp
Our contributions in this paper are as follows
For we provide an algorithm that solves the structured regression problem above in time
O(nnz(A log2 poly(dq?1 By combining our sketching methods with preconditioned
iterative solvers we can also obtain logarithmic dependence on For we provide an
algorithm with runtime O(nnz(A log log2 poly(dq?1 log This implies that moving
from linear A to nonlinear regression Tq incurs only a mild additional log2
runtime cost while requiring no extra storage Since nnz(Tq nnz(A this provides to
our knowledge the first sketching approach that operates faster than input-sparsity time
we sketch Tq in time faster than nnz(Tq
Our algorithms apply to a broad class of nonlinear models for both least squares regression and
their robust l1 regression counterparts While polynomial regression and additive models with
monomial basis functions are immediately covered by our methods we also show that under a
suitable choice of the constraint set the structured regression problem with Tq for
a Gaussian random matrix approximates non-parametric regression using the Gaussian kernel
We argue that our approach provides a more flexible modeling framework when compared to
randomized Fourier maps for kernel methods
Empirical results confirm both the practical value of our modeling framework as well as speedup
benefits of sketching
Polynomial Fitting Additive Models and Random Fourier Maps
Our primary goal in this section is to motivate sketching approaches for a versatile class of BlockVandermonde structured regression problems by showing that these problems arise naturally in various statistical modeling settings
The most basic application is the one-dimensional polynomial regression
In multivariate additive regression models a continuous target variable and input variables
Pd
Rd are related through the model fi zi where is an intercept term
are zero-mean Gaussian error terms
Pqand fi are smooth univariate functions The basic idea is
to expand each function as fi hi,t using basis functions hi,t and estimate the
unknown parameter vector dq typically by a constrained or penalized least
squares model argminx?C kZx where yn and Hq Rn?dq for
Hi hi,t zj on a training sample zi The constraint set typically imposes
smoothing sparsity or group sparsity constraints It is easy to see that choosing a monomial basis
hi,s us immediately maps the design matrix to the structured regression form of Eqn.
For our algorithms also provide fast solvers for robust polynomial additive models
Additive models impose a restricted form of univariate nonlinearity which ignores interactions
betweenPcovariates Let us denote an interaction term as zd?d
where
A degree-q multivariate polynomial function space Pq is
spanned by q}d Pq admits all possible degree-q interactions
but has dimensionality dq which is computationally infeasible to explicitly work with except for
low-degrees and low-dimensional
or sparse datasets Kernel methods with polynomial kernels
provide an implicit mechanism to compute inner products in the
feature space associated with Pq However they require computation for solving associated
kernelized ridge regression problems and storage of dense Gram matrices given
by Kij k(zi zj and therefore do not scale well
For a matrix let SG be the subspace spanned by
Gij zi
Assuming dq and that is a random matrix of i.i.d Gaussian variables then almost surely we
have SG Pq An intuitively appealing explicit scalable approach is then to use dq In that
case SG essentially spans a random subspace of Pq The design matrix for solving the multivariate
polynomial regression restricted to SG has the form Tq where A znT
This scheme can be in fact related to the idea of random Fourier features introduced by Rahimi
and Recht in the context of approximating shift-invariant kernel functions with the Gaussian
Kernel exp as the primary example By appealing to Bochner?s Theorem it is shown that the Gaussian kernel is the Fourier transform of a zero-mean multivariate
Gaussian distribution with covariance matrix Id where Id denotes the d-dimensional identity
matrix
exp Id
where ei An empirical approximation to this expectation can be obtained by sampling
PD
frequencies Id and setting
This implies
that the Gram matrix of the Gaussian kernel Kij exp kzi zj may be approximated
with high concentration as RRT where cos(AG sin(AG Rn?2D sine and cosine are applied elementwise as scalar functions This randomized explicit feature mapping for
the Gaussian kernel implies that standard linear regression with as the design matrix can then
be used to obtain a solution in time O(nD2 By taking the Maclaurin series expansion of sine
and cosine upto degree we can see that a restricted structured regression problem of the form
argminx?range(Q kTq AG)x bkp where the matrix R2Dq?2D contains appropriate coefficients of the Maclaurin series will closely approximate the randomized Fourier features construction
of By dropping or modifying the constraint set range(Q the setup above in principle
can define a richer class of models A full error analysis of this approach is the subject of a separate
paper
Fast Structured Regression with Sketching
We now develop our randomized solvers for block-Vandermonde structured lp regression problems
In the theoretical developments below we consider unconstrained regression though our results
generalize straightforwardly to convex constraint sets C. For simplicity we state all our results
for constant failure probability One can always repeat the regression procedure times
each time with independent randomness and choose the best solution found This reduces the failure
probability to
Background
We begin by giving some notation and then provide necessary technical background
Given a matrix Rn?d let M1 Md be the columns of and
be the rows
of Define kM k1 to be the element-wise norm of That is kM k1 kMi k1 Let
kM kF
be the Frobenius norm of Let
i,j
Well-Conditioning and Sampling of A Matrix
Definition 1)-well-conditioning Given a matrix Rn?d we say is 1)well-conditioned if kxk kM xk1 for any Rd and kM k1
Lemma Implicit in Suppose is an matrix so that for all Rd
kM xk1 kSM xk1 kM xk1
Let be a QR-decomposition
of SM so that QR SM and has orthonormal columns
Then is 1)-well-conditioned
Theorem Theorem of SupposeU is an 1)-well-conditioned basis of an
kUi k1
matrix A. For each let pi min tkU
k1 where
Suppose we independently sample each row with probability pi and create a diagonal matrix
where Si,i if is not sampled and Si,i if is sampled Then with probability at least
simultaneously for all Rd we have
kSAxk1 kAxk1 kAxk1
We also need the following method of quickly obtaining approximations to the pi in Theorem
which was originally given in Mahoney
Theorem Let Rn?d be an 1)-well-conditioned basis
matrix A. Suppose
of an
kUi Gk1
is a O(log matrix of Gaussians Let pi min
dkU Gk
for all where is
as in Theorem Then with probability over the choice of the following occurs If we
sample each row with probability pi and create as in Theorem then with probability at least
over our choice of sampled rows simultaneously for all Rd we have
kSAxk1 kAxk1 kAxk1
Oblivious Subspace Embeddings
Let A Rn?d We assume that Let nnz(A denote the number of non-zero entries of A.
We can assume nnz(A and that there are no all-zero rows or columns in A.
Norm The following family of matrices is due to Charikar also For a parameter define a random linear map Rn Rt as follows
is a random map so that for each t0 for t0 with probability
is a binary matrix with and all remaining entries
is an random diagonal matrix with each diagonal entry independently chosen to be
or with equal probability
We will refer to as a sparse embedding matrix
For certain it was recently shown that with probability at least 99 over the choice of and
for any fixed A Rn?d we have simultaneously for all Rd
kAxk2 k?Axk2 kAxk2
that is the entire column space of A is preserved The best known value of is
We will also use an oblivious subspace embedding known as the subsampled randomized Hadamard
transform or SRHT See Boutsidis and Gittens?s recent article for a state-the-art analysis
Theorem Lemma in There is a distribution over linear maps such that with probability
99 over the choice of for any fixed A Rn?d we have simultaneously for all Rd
kAxk2 Axk2 kAxk2
where the number of rows of is t0 log log and the time to compute
A is O(nd log t0
Norm The results can be generalized to subspace embeddings with respect to the norm
The best known bounds are due to Woodruff and Zhang so we use their family of
embedding matrices in what follows Here the goal is to design a distribution over matrices so
that with probability at least 99 for any fixed A Rn?d simultaneously for all Rd
kAxk1 k?Axk1 kAxk1
where is a distortion parameter The best known value of independent of for which
A can be computed in O(nnz(A time is log2 Their family of matrices is
chosen to be of the form where is as above with parameter for arbitrarily small
constant and is a diagonal matrix with Ei,i where u1 un are independent
standard exponentially distributed random variables
Recall that an exponential distribution has support probability density function PDF
e?x and cumulative distribution function CDF We say a random variable
is exponential if is chosen from the exponential distribution
Fast Vandermonde Multipication
Lemma Let and Vq,n For any Rn and Rq the
matrix-vector products and can be computed in log2 time
Main Lemmas
We handle and separately Our algorithms uses the subroutines given by the next lemmas
Lemma Efficient Multiplication of a Sparse Sketch and Tq Let A Rn?d Let
be a sparse embedding matrix for the norm with associated hash function for an
arbitrary value of and let be any diagonal matrix There is a deterministic algorithm to compute
the product Tq in O((nnz(A dtq log2 time
Proof By definition of Tq it suffices to prove this when Indeed if we can prove for a
column vector a that the product Tq can be computed in O((nnz(a tq log2 time
then by linearity if will follow that the product Tq can be computed in O((nnz(A
Algorithm StructRegression-2
Input An matrix A with nnz(A non-zero entries an vector an integer degree and an accuracy parameter
Output With probability at least 98 a vector Rd for which kTq bk2 minx kTq bk2
Let be a sparse embedding matrix for the norm with
Compute Tq using the efficient algorithm of Lemma with set to the identity matrix
Compute
Compute and where is a subsampled randomized Hadamard transform of Theorem with t0
dq log rows
Output the minimizer of Tq
dtq log2 time for general Hence in what follows we assume that and our matrix A is a
column vector a Notice that if a is just a column vector then Tq is equal to Vq,n an
For each define the ordered list Lk such that and Let Lk
We define an dimensional vector as follows If pk is the i-th element of Lk we set ik
Dpk Epk Let be the submatrix of Vq,n an whose rows are in the set
Lk Notice that is itself the transpose of a Vandermonde matrix where the number of rows of
is By Lemma the product can be computed in log2 time Notice that
is equal to the k-th row of the product DETq Therefore the entire product DETq
can be computed in
log O((nnz(a tq log time
Lemma Efficient Multiplication of Tq on the Right Let A Rn?d For any vector
there is a deterministic algorithm to compute the matrix vector product Tq in O((nnz(A
dq log2 time
The proof is provided in the supplementary material
Lemma Efficient Multiplication of Tq on the Left Let A Rn?d For any vector
there is a deterministic algorithm to compute the matrix vector product Tq in O((nnz(A
dq log2 time
The proof is provided in the supplementary material
Fast regression
We start by considering the structured regression problem in the case We give an algorithm
for this problem in Algorithm
Theorem Algorithm TRUCT EGRESSION solves w.h.p the structured regression with
in time
O(nnz(A log2 poly(dq
Proof By the properties of a sparse embedding matrix Section with probability at least
99 for we have simultaneously for all in the span of the columns of Tq
adjoined with
k?yk2
since the span of this space has dimension at most dq By Theorem we further have that with
probability 99 for all vectors in the span of the columns of
zk2
It follows that for all vectors Rd
bk2 bk2
It follows by a union bound that with probability at least 98 the output of TRUCT EGRESSION
is a approximation
For the time complexity Tq can be computed in O((nnz(A dtq log2 by Lemma while
can be computed in time The remaining steps can be performed in poly(dq time and
therefore the overall time is O(nnz(A log2 poly(dq
Algorithm StructRegression-1
Input An matrix A with nnz(A non-zero entries an vector an integer degree and an accuracy parameter
Output With probability at least 98 a vector Rd for which kTq bk1 minx kTq bk1
Let DE be a subspace embedding matrix for the norm with dq for an arbitrarily small constant
Compute Tq ETq using the efficient algorithm of Lemma
Compute
Compute a QR-decomposition of where denotes the adjoining of column vector to Tq
Let be a dq O(log matrix of Gaussians
Compute G.
Compute Tq using the efficient algorithm of Lemma applied to each of the columns of G.
rows of Tq and corresponding entries of
Let be the diagonal matrix of Theorem formed by sampling O(q
using the scheme of Theorem
Output the minimizer of kSTq Sbk1
Logarithmic Dependence on
The TRUCT EGRESSION algorithm can be modified to obtain a running time with a logarithmic
dependence on by combining sketching-based methods with iterative ones
Theorem 13 There is an algorithm which solves the structured regression problem with in
time O((nnz(A dq poly(dq
Due to space limitations the proof is provided in Supplementary material
Fast regression
We now consider the structured regression in the case The algorithm in this case is more
complicated than that for and is given in Algorithm
Theorem Algorithm TRUCT EGRESSION solves w.h.p the structured regression in problem
with in time
O(nnz(A log log2 poly(dq??1 log
The proof is provided in supplementary material
We note when there is a convex constraint set the only change in the above algorithms is to
optimize over C.
Experiments
We report two sets of experiments on classification and regression datasets The first set of experiments compares generalization performance of our structured nonlinear least squares regression models against standard linear regression and nonlinear regression with random fourier features The second set of experiments focus on scalability benefits of sketching We used
Regularized Least Squares Classification RLSC for classification
Generalization performance is reported in Table As expected ordinary linear regression is
very fast especially if the matrix is sparse However it delivers only mediocre results The results
improve somewhat with additive polynomial regression Additive polynomial regression maintains
the sparsity structure so it can exploit fast sparse solvers Once we introduce random features
thereby introducing interaction terms results improve considerably When compared with random
Fourier features for the same number of random features additive polynomial regression with
random features get better results than regression with random Fourier features If the number of
random features is not the same then if DF ourier DP oly where DF ourier is the number of
Fourier features and DP oly is the number of random features in the additive polynomial regression
then regression with random Fourier features seems to outperform additive polynomial regression
with random features However computing the random features is one of the most expensive steps
so computing better approximations with fewer random features is desirable
Figure reports the benefit of sketching in terms of running times and the trade-off in terms of
accuracy In this experiment we use a larger sample of the MNIST dataset with examples
Ord. Reg.
Fourier Features
sec
sec
sec
sec
sec
sec
sec
sec
sec
sec
sec
sec
sec
sec
sec
sec
Ord. Reg.
Add. Poly Reg.
MNIST
sec
classification
CPU
regression
ADULT
classification
32
CENSUS
regression
18
sec
FOREST COVER
classification
54
58
Table
sec
Add. Poly Reg.
Random Features
sec
Dataset
Comparison of testing error and training time of the different methods In the table is number of training instances is the
number of features per instance and is the number of instances in the test set Ord Reg stands for ordinary regression Add Poly
Reg stands for additive polynomial regression For classification tasks the percent of testing points incorrectly predicted is reported For
Sketching
Sampling
Speedup
Sketch Size of Examples
Sketching
Sampling
Sketch Size of Examples
Classification Error on Test Set
Suboptimality of residual
regression tasks we report kyp yk2 kyk where yp is the predicted values and is the ground truth
Sketching
Sampling
Exact
Sketch Size of Examples
Figure Examining the performance of sketching
We compute random features and then solve the corresponding additive polynomial regression problem with both exactly and with sketching to different number of rows We also
tested a sampling based approach which simply randomly samples a subset of the rows no sketching Figure plots the speedup of the sketched method relative to the exact solution In these
experiments we use a non-optimized straightforward implementation that does not exploit fast Vandermonde multiplication or parallel processing Therefore running times were measured using a
sequential execution We measured only the time required to solve the regression problem For
this experiment we use a machine with two quad-core Intel @ and DDR2
RAM. Figure explores
in solving
the regression problem More
the sub-optimality
specifically we plot kYp kF
Yp
Yp
where is the labels matrix Yp is
the best approximation exact solution and Yp is the sketched solution We see that indeed the error
decreases as the size of the sampled matrix grows and that with a sketch size that is not too big we
get to about a larger objective In Figure we see that this translates to an increase in error
rate Encouragingly a sketch as small as of the number of examples is enough to have a very
small increase in error rate while still solving the regression problem more than times faster the
speedup is expected to grow for larger datasets
Acknowledgements
The authors acknowledge the support from XDATA program of the Defense Advanced Research
Projects Agency DARPA administered through Air Force Research Laboratory contract

----------------------------------------------------------------

title: 4196-sparse-features-for-pca-like-linear-regression.pdf

Sparse Features for PCA-Like Linear Regression
Petros Drineas
Computer Science Department
Rensselaer Polytechnic Institute
Troy NY
drinep@cs.rpi.edu
Christos Boutsidis
Mathematical Sciences Department
IBM T. J. Watson Research Center
Yorktown Heights New York
cboutsi@us.ibm.com
Malik Magdon-Ismail
Computer Science Department
Rensselaer Polytechnic Institute
Troy NY
magdon@cs.rpi.edu
Abstract
Principal Components Analysis PCA is often used as a feature extraction procedure Given a matrix Rn?d whose rows represent data points with respect
to features the top right singular vectors of the so-called eigenfeatures
are arbitrary linear combinations of all available features The eigenfeatures are
very useful in data analysis including the regularization of linear regression Enforcing sparsity on the eigenfeatures forcing them to be linear combinations
of only a small number of actual features as opposed to all available features can
promote better generalization error and improve the interpretability of the eigenfeatures We present deterministic and randomized algorithms that construct such
sparse eigenfeatures while provably achieving in-sample performance comparable
to regularized linear regression Our algorithms are relatively simple and practically efficient and we demonstrate their performance on several data sets
Introduction
Least-squares analysis was introduced by Gauss in and has since has bloomed into a staple of
the data analyst Assume the usual setting with tuples y1 yn in Rd where are
points and are targets The vector of regression weights Rd minimizes over all Rd
the RMS in-sample error
uX
kXw yk2
In the above Rn?d is the data matrix whose rows are the vectors Xij and
Rn is the target vector We will use the more convenient matrix formulation1
namely given and we seek a vector that minimizes kXw yk2 The minimal-norm vector
can be computed via the Moore-Penrose pseudo-inverse of Then the optimal
in-sample error is equal to
ky XX yk2
For the sake of simplicity we assume and rank in our exposition neither assumption is
necessary
When the data is noisy and is ill-conditioned becomes unstable to small perturbations and
overfitting can become a serious problem Practitioners deal with such situations by regularizing
the regression Popular regularization methods include for example the Lasso Tikhonov
regularization and top-k PCA regression or truncated SVD regularization In general
such methods are encouraging some form of parsimony thereby reducing the number of effective
degrees of freedom available to fit the data Our focus is on top-k PCA regression which can be
viewed as regression onto the top-k principal components or equivalently the top-k eigenfeatures
The eigenfeatures are the top-k right singular vectors of and are arbitrary linear combinations
of all available input features The question we tackle is whether one can efficiently extract sparse
eigenfeatures eigenfeatures that are linear combinations of only a small number of the available
features that have nearly the same performance as the top-k eigenfeatures
Basic notation A are matrices a are vectors are integers In is the
identity matrix is the matrix of zeros ei is the standard basis whose dimensionality
will be clear from the context For vectors we use the Euclidean norm for matrices the
Frobenius and the spectral norms kXkF i,j X2ij and kXk2 the largest singular
value of
Top-k PCA Regression Let U?V be the singular value decomposition of where
resp is the matrix of left resp right singular vectors of with singular values in the diagonal
matrix For let Uk and Vk contain only the top-k singular vectors and associated
singular values The best rank-k reconstruction of in the Frobenius norm can be obtained from
this truncated singular value decomposition as Xk Uk VkT The right singular vectors in Vk
are called the top-k eigenfeatures The projections of the data points onto the top eigenfeatures are
obtained by projecting the onto the columns of Vk to obtain Fk XVk U?V Vk
Now each data point row in Fk only has dimensions Each column of Fk contains a particular
eigenfeature?s value for every data point and is a linear combination of the columns of
The top-k PCA regression uses Fk as the data matrix and as the target vector to produce regression
weights wk
The in-sample error of this k-dimensional regression is equal to
ky Fk wk ky Fk
yk2 ky yk2 ky yk2
The weights wk are k-dimensional and cannot be applied to but the equivalent weights Vk wk
can be applied to and they have the same in-sample error with respect to
E(V wk ky XVk wk ky wk ky Uk UkT yk2
Hence we will refer to both wk and Vk wk as the top-k PCA regression weights the dimension will
make it clear which one we are talking about and for simplicity we will overload wk to refer to both
these weight vectors the dimension will make it clear which In practice is chosen to measure
the effective dimension of the data and typically rank(X One way to choose is so
that kX Xk kF the energy in the k-th principal component is large compared to the
energy in all smaller principal components We do not argue the merits of top-k PCA regression
we just note that top-k PCA regression is a common tool for regularizing regression
Problem Formulation Given Rn?d the number of target eigenfeatures for top-k PCA
regression and the sparsity parameter we seek to extract a set of at most sparse eigenfea which use at most of the actual dimensions Let
XV
Rn?k denote the matrix
tures
whose columns are the extracted sparse eigenfeatures which are a linear combination of a set of at
most actual features Our goal is to obtain sparse features for which the vector of sparse regression
weights
results in an in-sample error ky yk2 that is close to the top-k PCA
regression error ky
yk2 Just as with top-k PCA regression we can define the equivalent
kw
we will overload
to refer to these weights as well
d-dimensional weights
Finally we conclude by noting that while our discussion above has focused on simple linear regression the problem can also be defined for multiple regression where the vector is replaced by a
matrix with The weight vector becomes a weight matrix where each
column of contains the weights from the regression of the corresponding column of onto the
features All our results hold in this general setting as well and we will actually present our main
contributions in the context of multiple regression
Our contributions
Recall from our discussion at the end of the introduction that we will present all our results in the
general setting where the target vector is replaced by a matrix Our first theorem
argues that there exists a polynomial-time deterministic algorithm that constructs a feature matrix
Rn?k such that each feature column of
is a linear combination of at most actual
features columns from and results in small in-sample error Again this should be contrasted
with top-k PCA regression which constructs a feature matrix Fk such that each feature column of
Fk is a linear combination of all features columns in Our theorems argue that the in-sample
error of our features is almost as good as the in-sample error of top-k PCA regression which uses
dense features
Theorem Deterministic Feature Extraction Let Rn?d and be the input matrices
in a multiple regression problem Let be a target rank for top-k PCA regression on and Y.
XV
Rn?k such
For any there exists an algorithm that constructs a feature matrix
that every column of Fk is a linear combination of the same at most columns of and
9k kX Xk kF
kYk2
Wk
kY Fk YkF kY XW kF
is the k-th
singular value of The running time of the proposed algorithm is Vk
ndk nrk where Vk is the time required to compute the matrix Vk the top-k right singular vectors of
Theorem says that one can construct features with sparsity and obtain a comparble regression error to that attained by the dense top-k PCA features up to additive term that is proportional
to kX Xk kF
To construct the features satisfying the guarantees of the above theorem we first employ the Algorithm DSF-Select Table and Section to select columns of and form the matrix
Rn?r Now let denote the best rank-k approximation with respect to the Frobenius
norm to in the column-span of C. In other words is a rank-k matrix that minimizes
kY kF over all rank-k matrices in the column-span of C. Efficient algorithms are known
for computing and have been described in Given the sparse eigenfeatures
can be computed efficiently as follows first set Observe that
CC
The last equality follows because CC projects onto the column span of and is already
in the column span of C. has rank at most because has rank at most Let the
CU Rn?k Clearly each column of
is a
SVD of be
and set
linear combination of the same at most columns of the columns in C). The sparse features
XV
so
themselves can also be obtained because
are a good set of sparse features we first relate the regression error from using
To prove that
to how well approximates Y.
kY Y)kF kY C?kF kY CU
kF kY
kF kY
YkF
are the optimal regression weights for the features
The
The last inequality follows because
reverse inequality also holds because is the best rank-k approximation to in the column
span of C. Thus
kY
YkF kY Y)kF
The upshot of the above discussion is that if we can find a matrix consisting of columns of for
which kY Y)kF is small then we immediately have good sparse eigenfeatures Indeed all
that remains to complete the proof of Theorem is to bound kY Y)kF for the columns
returned by the Algorithm DSF-Select
Our second result employs the Algorithm RSF-Select Table and Section to select
columns of and again form the matrix Rn?r One then proceeds to construct and
as described above The advantage of this approach is simplicity better efficiency and a slightly
better error bound at the expense of logarithmically worse sparsity
Theorem Randomized Feature Extraction Let Rn?d and be the input matrices
in a multiple regression problem Let be a target rank for top-k PCA regression on and
Y. For any there exists a randomized algorithm that constructs a feature matrix
XV
Rn?k such that every column of
is a linear combination of at most columns
of and with probability at least over random choices made in the algorithm
kY
Yk kY XWk kX Xk kF kYk
The running time of the proposed algorithm is Vk O(dk log
Connections with prior work
A variant of our problem is the identification of a matrix consisting of a small number say
columns of such that the regression of onto as opposed to features from gives small insample error This is the sparse approximation problem where the number of non-zero weights in the
regression vector is restricted to This problem is known to be NP-hard Sparse approximation
has important applications and many approximation algorithms have been presented
proposed algorithms are typically either greedy or are based on convex optimization relaxations of
the objective An important difference between sparse approximation and sparse PCA regression is
that our goal is not to minimize the error under a sparsity constraint but to match the top-k PCA
regularized regression under a sparsity constraint We argue that it is possible to achieve a provably
accurate sparse PCA-regression use sparse features instead of dense ones
If approximating using the columns of then this is the column-based matrix reconstruction problem which has received much attention in existing literature 18 26
In this paper we study the more general problem where which turns out to be considerably
more difficult
Input sparseness is closely related to feature selection and automatic relevance determination Research in this area is vast and we refer the reader to for a high-level view of the field Again
the goal in this area is different than ours namely they seek to reduce dimensionality and improve
out-of-sample error Our goal is to provide sparse PCA features that are almost as good as the exact principal components While it is definitely the case that many methods outperform top-k PCA
regression especially for this discussion is orthogonal to our work
The closest result to ours in prior literature is the so-called rank-revealing QR RRQR factorization The authors use a QR-like decomposition to select exactly columns of and compare
with the top-k PCA regularized solution wk They show that
their sparse solution vector
kwk
kX Xk
k(n
ky Xwk This bound is similar to our bound in Theorem
where kw
but
only applies to and is considerably weaker For example k(n kX Xk
kX Xk kF note also that the dependence of the above bound on is generally worse
than ours
The importance of the right singular vectors in matrix reconstruction problems including PCA
has been heavily studied in prior literature going back to work by Jolliffe in The idea of
sampling columns from a matrix with probabilities that are derived from VkT as we do in Theorem
was introduced in in order to construct coresets for regression problems by sampling data
points rows of the matrix as opposed to features columns of the matrix X). Other prior work
including 13 27 has employed variants of this sampling scheme indeed we borrow
proof techniques from the above papers in our work Finally we note that our deterministic feature
selection algorithm Theorem uses a sparsification tool developed in for column based matrix
reconstruction This tool is a generalization of algorithms originally introduced in
Our algorithms
Our algorithms emerge from the constructive proofs of Theorems and Both algorithms necessitate access to the right singular vectors of namely the matrix Vk Rd?k In our experiments we
used PROPACK in order to compute Vk iteratively PROPACK is a fast alternative to the exact
SVD. Our first algorithm DSF-Select is deterministic while the second algorithm RSF-Select
is randomized requiring logarithmically more columns to guarantee the theoretical bounds Prior
to describing our algorithms in detail we will introduce useful notation on sampling and rescaling
matrices as well as a matrix factorization lemma Lemma that will be critical in our proofs
Sampling and rescaling matrices
Let Rn?r contain columns of Rn?d We can express the matrix as where
the sampling matrix Rd?r is equal to eir and ei are standard basis vectors in Rd In
our proofs we will make use of Rr?r a diagonal rescaling matrix with positive entries on the
diagonal Our column selection algorithms return a sampling and a rescaling matrix so that X?S
contains a subset of rescaled columns from The rescaling is benign since it does not affect the
span of the columns of and thus the quantity of interest namely
A structural result using matrix factorizations
We now present a matrix reconstruction lemma that will be the starting point for our algorithms
Let be a target matrix and let Rn?d be the basis matrix that we will use in order
to reconstruct Y. More specifically we seek a sparse reconstruction of from or in other
words we would like to choose columns from and form a matrix Rn?r such that
kY Y)kF is small Let Rd?k be an orthogonal matrix Ik and express the
matrix as follows
HZT
where is some matrix in Rn?k and Rn?d is the residual error of the factorization It is easy
to prove that the Frobenius or spectral norm of is minimized when XZ. Let Rd?r and
Rr?r be a sampling and a rescaling matrix respectively as defined in the previous section and
let Rn?r Then the following lemma holds for a detailed proof
Lemma Generalized Column Reconstruction Using the above notation if the rank of the matrix
is equal to then
kY Y)kF kY HH YkF kE?S(Z YkF
We now parse the above lemma carefully in order to understand its implications in our setting For
our goals the matrix essentially contains a subset of features from the data matrix Recall that
is the best rank-k approximation to within the column space of and the difference
measures the error from performing regression using sparse eigenfeatures that are
constructed as linear combinations of the columns of C. Moving to the right-hand side of eqn
the two terms reflect a tradeoff between the accuracy of the reconstruction of using and the
error in approximating by the product HZT Ideally we would like to choose so that can
be accurately approximated and at the same time the matrix is approximated by the product HZ
with small residual error E. In general these two goals might be competing and a balance must be
struck Here we focus on one extreme of this trade off namely choosing so that the Frobenius
norm of the matrix is minimized More specifically since has rank the best choice for HZT in
order to minimize kEkF is Xk then Xk Using the SVD of Xk namely Xk Uk VkT
we apply Lemma setting Uk and Vk The following corollary is immediate
Lemma Generalization of Lemma in Using the above notation if the rank of the matrix
VkT is equal to then
kY Y)kF kY UkT YkF k(X Xk kT
YkF
Our main results will follow by carefully choosing and in order to control the right-hand side of
the above inequality
Algorithm DSF-Select
Input
Output columns of in C.
Compute Vk and
Xk XVk VkT
Algorithm DetSampling
Input vd A ad
Output Sampling and rescaling matrices S].
Initialize B0 and
for to
do
Set rk
Pick index and such that
Run DetSampling to construct sampling and rescaling matrices and
DetSampling(VkT
Return X?.
L(vi
Update tvi v?iT
Set and
end for
Return and S.
Table DSF-Select Deterministic Sparse Feature Selection
DSF-Select Deterministic Sparse Feature Selection
DSF-Select deterministically selects columns of the matrix to form the matrix Table
and note that the matrix might contain duplicate columns which can be removed without
any loss in accuracy The heart of DSF-Select is the subroutine DetSampling a near-greedy
algorithm which selects columns of VkT iteratively to satisfy two criteria the selected columns should
form an approximately orthogonal basis for the columns of VkT so that VkT is well-behaved
and E?S should also be well-behaved These two properties will allow us to prove our results via
Lemma The implementation of the proposed algorithm is quite simple since it relies only on
standard linear algebraic operations
DetSampling takes as input two matrices Rk?d satisfying Ik and A Rn?d In
order to describe the algorithm it is convenient to view these two matrices as two sets of column
Pd
vectors vd satisfying vi viT Ik and A ad In DSF-Select
we set Vk and A Xk Given and the algorithm iterates from up to
and its main operation is to compute the functions and that are defined
as follows
vT Ik
vT Ik
In the above Rk?k is a symmetric matrix with eigenvalues and is a parameter
We also define the function for a vector a Rn as follows
aT a
kAk2F
At every step the algorithm selects a column such that L(vi note that
is a matrix which is also updated at every step of the algorithm Table The
existence of such a column is guaranteed by results in
It is worth noting that in practical implementations of the proposed algorithm there might exist
multiple columns which satisfy the above requirement In our implementation we chose to break
such ties arbitrarily However more careful and informed choices such as breaking the ties in a way
that makes maximum progress towards our objective might result in considerable savings This is
indeed an interesting open problem
The running time of our algorithm is dominated by the search for a column which satisfies
L(vi To compute the function we first need to compute which
necessitates the eigenvalues of and then we need to compute the inverse of Ik
These computations need O(k time per iteration for a total of O(rk time over all iterations
Now in order to compute the function for each vector vi for all we need an additional
Algorithm RSF-Select
Input
Output columns of in C.
Compute Vk
Run RandSampling to construct sampling and rescaling matrices and
RandSampling(VkT
Return X?.
Algorithm RandSampling
Input vd and
Output Sampling and rescaling matrices S].
For compute probabilities
pi
kvi
Initialize and
for to do
Select an index where the
probability of selecting index is equal to pi
Set and rpi
end for
Return and S.
Table RSF-Select Randomized Sparse Feature Selection
O(dk time per iteration the total time for all iterations is O(drk Next in order to compute
the function we need to compute aiT for all which necessitates O(nnz(A time
where nnz(A is the number of non-zero elements of A. In our setting A Rn?d so the
overall running time is O(drk In order to get the final running time we also need to account
for the computation of Vk and E.
The theoretical properties of DetSampling were analyzed in detail in building on the original
analysis of The following lemma from summarizes important properties of
Lemma DetSampling with inputs and A returns a sampling matrix Rd?r and a
rescaling matrix Rr?r satisfying
k(V
kA?SkF kAkF
We apply Lemma with VTk and A and we combine it with Lemma to conclude the
proof of Theorem see for details
RSF-Select Randomized Sparse Feature Selection
RSF-Select is a randomized algorithm that selects columns of the matrix in order to form the
matrix Table The main differences between RSF-Select and DSF-Select are two first
RSF-Select only needs access to kT and second RSF-Select uses a simple sampling procedure in
order to select the columns of to include in C. This sampling procedure is described in algorithm
RandSampling and essentially selects columns of with probabilities that depend on the norms of
the columns of VkT Thus RandSampling first computes a set of probabilities that are proportional
to the norms of the columns of VkT and then samples columns of in independent identical trials
with replacement where in each trial a column is sampled according to the computed probabilities
Note that a column could be selected multiple times In terms of running time and assuming that
the matrix Vk that contains the top right singular vectors of has already been computed the
proposed algorithm needs O(dk time to compute the sampling probabilities and an additional
log time to sample columns from Similar to Lemma we can prove analogous properties
for the matrices and that are returned by algorithm RandSampling Again combining with
Lemma we can prove Theorem see for details
Experiments
The goal of our experiments is to illustrate that our algorithms produce sparse features which perform as well in-sample as the top-k PCA regression It turns out that the out-of-sample performance
is comparable if not better in many cases perhaps due to the sparsity to top-k PCA-regression
Data
wk
Arcene
I-sphere
LibrasMov
Madelon
HillVal
Spambase
kDSF
kRSF
krnd
wk
2k
kDSF
kRSF
krnd
Table Comparison of DSF-select and RSF-select with top-k PCA. The top entry in each cell
is the in-sample error and the bottom entry is the out-sample error In bold is the method achieving
the best out-sample error
Compared to top-k PCA our algorithms are efficient and work well in practice even better than the
theoretical bounds suggest
We present our findings in Table using data sets from the UCI machine learning repository We
used a five-fold cross validation design with random splits we computed regression weights
using of the data and estimated out-sample error in the remaining of the data We set
in the experiments no attempt was made to optimize Table shows the in and out-sample error
kDSF
for four methods top-k PCA regression wk r-sparse features regression using DSF-select
RSF
r-sparse features regression using random
r-sparse features regression using RSF-select
krnd
columns
Discussion
The top-k PCA regression constructs features without looking at the targets it is target-agnostic
So are all the algorithms we discussed here as our goal was to compare with top-k PCA. However
there is unexplored potential in Lemma We only explored one extreme choice for the factorization
namely the minimization of some norm of the matrix E. Other choices in particular non-targetagnostic choices could prove considerably better Such investigations are left for future work
As mentioned when we discussed our deterministic algorithm it will often be the case that in some
steps of the greedy selection process multiple columns could satisfy the criterion for selection In
such a situation we are free to choose any one we broke ties arbitrarily in our implementation
and even as is the algorithm performed as well or better than top-k PCA. However we expect that
breaking the ties so as to optimize the ultimate objective would yield considerable additional benefit
this would also be non-target-agnostic
Acknowledgments
This work has been supported by two NSF CCF and DMS grants to Petros Drineas and Malik
Magdon-Ismail

----------------------------------------------------------------

title: 4745-a-scalable-cur-matrix-decomposition-algorithm-lower-time-complexity-and-tighter-bound.pdf

A Scalable CUR Matrix Decomposition Algorithm
Lower Time Complexity and Tighter Bound
Shusen Wang and Zhihua Zhang
College of Computer Science Technology
Zhejiang University
Hangzhou China
wss,zhzhang}@zju.edu.cn
Abstract
The CUR matrix decomposition is an important extension of Nystr?om approximation to a general matrix It approximates any data matrix in terms of a small number of its columns and rows In this paper we propose a novel randomized CUR
algorithm with an expected relative-error bound The proposed algorithm has the
advantages over the existing relative-error CUR algorithms that it possesses tighter
theoretical bound and lower time complexity and that it can avoid maintaining the
whole data matrix in main memory Finally experiments on several real-world
datasets demonstrate significant improvement over the existing relative-error algorithms
Introduction
Large-scale matrices emerging from stocks genomes web documents web images and videos everyday bring new challenges in modern data analysis Most efforts have been focused on manipulating understanding and interpreting large-scale data matrices In many cases matrix factorization
methods are employed to construct compressed and informative representations to facilitate computation and interpretation A principled approach is the truncated singular value decomposition
SVD which finds the best low-rank approximation of a data matrix Applications of SVD such as
eigenface and latent semantic analysis have been illustrated to be very successful
However the basis vectors resulting from SVD have little concrete meaning which makes it very
difficult for us to understand and interpret the data in question
An example in has well
shown this viewpoint that is the vector 2)height the sum of the
significant uncorrelated features from a dataset of people?s features is not particularly informative
The authors of have also claimed it would be interesting to try to find basis vectors for all
experiment vectors using actual experiment vectors and not artificial bases that offer little insight
Therefore it is of great interest to represent a data matrix in terms of a small number of actual
columns and/or actual rows of the matrix
The CUR matrix decomposition provides such techniques and it has been shown to be very useful
in high dimensional data analysis Given a matrix A the CUR technique selects a subset of
columns of A to construct a matrix and a subset of rows of A to construct a matrix and
CUR best approximates A. The typical CUR algorithms
computes a matrix such that A
work in a two-stage manner Stage is a standard column selection procedure and Stage
does row selection from A and simultaneously Thus Stage is more complicated than Stage
The CUR matrix decomposition problem is widely studied in the literature 13
18 19 Perhaps the most widely known work on the CUR problem is in which the authors
devised a randomized CUR algorithm called the subspace sampling algorithm Particularly the
algorithm has relative-error ratio with high probability
Unfortunately all the existing CUR algorithms require a large number of columns and rows to be
chosen For example for an matrix A and a target rank min{m the state-ofthe-art CUR algorithm the subspace sampling algorithm in requires exactly O(k
rows or log2 rows in expectation to achieve relative-error ratio Moreover
the computational cost of this algorithm is at least the cost of the truncated SVD of A that is
O(min{mn2 nm2 The algorithms are therefore impractical for large-scale matrices
In this paper we develop a CUR algorithm which beats the state-of-the-art algorithm in both theory
and experiments In particular we show in Theorem a novel randomized CUR algorithm with
lower time complexity and tighter theoretical bound in comparison with the state-of-the-art CUR
algorithm in
The rest of this paper is organized as follows Section introduces several existing column selection
algorithms and the state-of-the-art CUR algorithm Section describes and analyzes our novel
CUR algorithm Section empirically compares our proposed algorithm with the state-of-the-art
algorithm
Notations
For a matrix A aij Rm?n let be its i-th row and aj be its j-th column Let
be the Frobenius norm and be the spectral
i,j aij be the norm
i,j aij
norm Moreover let Im denote an identity matrix and 0mn denotes an zero matrix
UA,k VA,k
UA,k VA,k
be the
Let A UA A VA
uA,i vA,i
SVD of A where rank(A and UA,k and VA,k correspond to the top singular values
We denote Ak UA,k VA,k
Furthermore let A
be the Moore-Penrose
inverse of A
Related Work
Section introduces several relative-error column selection algorithms related to this work Section describes the state-of-the-art CUR algorithm in Section discusses the connection
between the column selection problem and the CUR problem
Relative-Error Column Selection Algorithms
Given a matrix A Rm?n column selection is a problem of selecting columns of A to construct
Rm?c to minimize A CC A?F Since there are nc possible choices of constructing
so selecting the best subset is a hard problem In recent years many polynomial-time approximate
algorithms have been proposed among which we are particularly interested in the algorithms with
relative-error bounds that is with columns selected from A there is a constant such that
A CC A?F Ak
We call the relative-error ratio We now present some recent results related to this work
We first introduce a recently developed deterministic algorithm called the dual set sparsification
proposed in We show their results in Lemma Furthermore this algorithm is a building
block of some more powerful algorithms Lemma and our novel CUR algorithm also relies
on this algorithm We attach the algorithm in Appendix A.
Lemma Column Selection via Dual Set Sparsification Algorithm Given a matrix A Rm?n
of rank and a target rank there exists a deterministic algorithm to select columns
of A and form a matrix Rm?c such that
Ak
A CC A
A
Although some partial SVD algorithms such as Krylov subspace methods require only O(mnk time
they are all numerical unstable See for more discussions
Moreover the matrix can be computed in TVA,k O(mn+nck where TVA,k is the time needed
to compute the top right singular vectors of A.
There are also a variety of randomized column selection algorithms achieving relative-error bounds
in the literature
An randomized algorithm in selects only 2k
columns to achieve the expected
relative-error ratio The algorithm is based on the approximate SVD via random projection the dual set sparsification algorithm and the adaptive sampling algorithm Here we
present the main results of this algorithm in Lemma Our proposed CUR algorithm is motivated
by and relies on this algorithm
Lemma Near-Optimal Column Selection Algorithm Given a matrix A Rm?n of rank a
target rank and there exists a randomized algorithm to select at most
2k
columns of A to form a matrix Rm?c such that
E2 A CC A?F E?A CC Ak
where the expectations are taken C. Furthermore the matrix can be computed in O((mnk
nk
The Subspace Sampling CUR Algorithm
Drineas proposed a two-stage randomized CUR algorithm which has a relative-error
bound Given a matrix A Rm?n and a target rank in the first stage the algorithm
chooses exactly O(k log columns log log in expectation of
A to construct Rm?c in the second stage it chooses exactly log rows
log log in expectation of A and simultaneously to construct and U. With
probability at least the relative-error ratio is The computational cost is dominated by
the truncated SVD of A and C.
Though the algorithm is optimal with high probability it requires too many rows get chosen at
least log2 rows in expectation In this paper we seek to devise an algorithm with
mild requirement on column and row numbers
Connection between Column Selection and CUR Matrix Decomposition
The CUR problem has a close connection with the column selection problem As aforementioned
the first stage of existing CUR algorithms is simply a column selection procedure However the
second stage is more complicated If the second stage is na??vely solved by a column selection
algorithm on AT then the error ratio will be at least
For a relative-error CUR algorithm the first stage seeks to bound a construction error ratio of
A?F
AR R?F
given C. Actually the first
A?Ak while the section stage seeks to bound
A?F
stage is a special case of the second stage where Ak Given a matrix A if an algorithm solv
AR R?F
ing the second stage results in a bound A?CC
then this algorithm also solves the
A?F
column selection problem for A with an relative-error ratio Thus the second stage of CUR is a
generalization of the column selection problem
Main Results
In this section we introduce our proposed CUR algorithm We call it the fast CUR algorithm because
it has lower time complexity compared with SVD. We describe it in Algorithm and give a theoretical analysis in Theorem Theorem relies on Lemma and Theorem and Theorem relies on
Theorem Theorem is a generalization of Theorem and Theorem is a generalization
of Theorem
Algorithm The Fast CUR Algorithm
Input a real matrix A Rm?n target rank target column number 2k
target
row number 2c
Stage select columns of A to construct Rm?c
kV
Compute approximate truncated SVD via random projection such that Ak
kV
V1 columns of
kT
Construct U1 columns of A
Compute s1 Dual Set Spectral-Frobenius Sparsification Algorithm V1
Construct C1 ADiag(s1 and then delete the all-zero columns
Residual matrix A C1 A
Compute sampling probabilities pi di
Sampling c2 columns from A with probability pn to construct C2
Stage select rows of A to construct Rr?n
kV
V2 columns of
Tk
Construct U2 columns of A
Compute s2 Dual Set Spectral-Frobenius Sparsification Algorithm V2
Construct R1 Diag(s2 and then delete the all-zero rows
Residual matrix A R1 Compute qj
Sampling r2 rows from A with probability qm to construct R2
return C2 RT2 and AR
Adaptive Sampling
The relative-error adaptive sampling algorithm is established in Theorem The algorithm
is based on the following idea after selecting a proportion of columns from A to form C1 by
an arbitrary algorithm the algorithms randomly samples additional c2 columns according to the
residual A C1 A. Boutsidis used the adaptive sampling algorithm to decrease the
residual of the dual set sparsification algorithm and obtained an relative-error bound Here
we prove a new bound for the adaptive sampling algorithm Interestingly this new bound is a
generalization of the original one in Theorem In other words Theorem of is a direct
corollary of our following theorem in which Ak is set
Theorem The Adaptive Sampling Algorithm Given a matrix A Rm?n and a matrix
Rm?c such that rank(C rank(CC A we let R1 Rr1 consist of r1
rows of A and define the residual A R1 Additionally for we define
pi
We further sample r2 rows from A in each trial of which the i-th row is chosen with probability
pi Let R2 Rr2 contains the r2 sampled rows and let RT2 Then
the following inequality holds
E?A CC AR A CC A R1
r2
where the expectation is taken R2
The Fast CUR Algorithm
Based on the dual set sparsification algorithm of of Lemma and the adaptive sampling algorithm
of Theorem we develop a randomized algorithm to solve the second stage of CUR problem We
present the results of the algorithm in Theorem Theorem of is a special case of the following
theorem where Ak
Theorem The Fast Row Selection Algorithm Given a matrix A Rm?n and a matrix
Rm?c such that rank(C rank(CC A and a target rank the
r?n
proposed randomized algorithm selects
such
rows of A to construct
that
E?A CC AR A CC Ak
where the expectation is taken R. Furthermore the matrix can be computed in O((mnk
mk time
Based on Lemma and Theorem here we present the main theorem for the fast CUR algorithm
Table A summary of the datasets
Dataset
Type
size
Source
Redrocknatural
http://www.agarwala.org/efficient gdc
Arcene
biology
http://archive.ics.uci.edu/ml/datasets/Arcene
Dexter bag of words 2600http://archive.ics.uci.edu/ml/datasets/Dexter
Theorem The Fast CUR Algorithm Given a matrix A Rm?n and a positive integer
min{m the fast CUR algorithm described in Algorithm randomly selects 2k
columns of A to construct Rm?c with the near-optimal column selection algorithm of Lemma
r?n
and then selects 2c
with the fast row selection
rows of A to construct
algorithm of Theorem Then we have
E?A CUR?F E?A AR Ak
Moreover the algorithm runs in time n)k mk nk
Since min{m by the assumptions so the time complexity of the fast CUR algorithm
is lower than that of the SVD of A. This is the main reason why we call it the fast CUR algorithm
Another advantage of this algorithm is avoiding loading the whole data matrix A into main
memory None of three steps the randomized SVD the dual set sparsification algorithm and the
adaptive sampling algorithm requires loading the whole of A into memory The most memoryexpensive operation throughout the fast CUR Algorithm is computing the Moore-Penrose inverse
of and which requires maintaining an matrix or an matrix in memory In
comparison the subspace sampling algorithm requires loading the whole matrix into memory to
compute its truncated SVD.
Empirical Comparisons
In this section we provide empirical comparisons among the relative-error CUR algorithms on several datasets We report the relative-error ratio and the running time of each algorithm on each data
set The relative-error ratio is defined by
A CUR?F
Relative-error ratio
A Ak
where is a specified target rank
We conduct experiments on three datasets including natural image biology data and bags of words
Table briefly summarizes some information of the datasets Redrock is a large size natural image
Arcene and Dexter are both from the UCI datasets Arcene is a biology dataset with
instances and attributes Dexter is a bag of words dataset with a 20000-vocabulary and
documents Each dataset is actually represented as a data matrix upon which we apply the CUR
algorithms
We implement all the algorithms in MATLAB We conduct experiments on a workstation
with Intel Xeon CPUs memory and Ubuntu system According to the
analysis in and this paper and should be integers far less than and For each data
set and each algorithm we set or and where ranges in each set of
experiments We repeat each set of experiments for times and report the average and the standard
deviation of the error ratios The results are depicted in Figures
The results show that the fast CUR algorithm has much lower relative-error ratio than the subspace
sampling algorithm The experimental results well match our theoretical analyses in Section As
for the running time the fast CUR algorithm is more efficient when and are small When and
become large the fast CUR algorithm becomes less efficient This is because the time complexity
of the fast CUR algorithm is linear in and large and imply small However the purpose
of CUR is to select a small number of columns and rows from the data matrix that is and
So we are not interested in the cases where and are large compared with and say
and
Running Time
Running Time
Subspace Sampling Exactly
Subspace Sampling Expected
Fast CUR
Subspace Sampling Exactly
Subspace Sampling Expected
Fast CUR
Time
Time
Time
Running Time
18 22 24 26 28 32 34 36
Construction Error Frobenius Norm
18 22 24
Subspace Sampling Exactly
Subspace Sampling Expected
Fast CUR
18
18 22 24 26 28 32 34 36
Subspace Sampling Exactly
Subspace Sampling Expected
Fast CUR
Construction Error Frobenius Norm
Relative Error Ratio
Subspace Sampling Exactly
Subspace Sampling Expected
Fast CUR
Relative Error Ratio
Construction Error Frobenius Norm
Relative Error Ratio
Subspace Sampling Exactly
Subspace Sampling Expected
Fast CUR
18 22 24
18
and and and
Figure Empirical results on the Redrock data set
Running Time
Running Time
18
Time
Time
Time
Running Time
Subspace Sampling Exactly
Subspace Sampling Expected
Fast CUR
18 22 24 26 28
Construction Error Frobenius Norm
18
Subspace Sampling Exactly
Subspace Sampling Expected
Fast CUR
Construction Error Frobenius Norm
Construction Error Frobenius Norm
Subspace Sampling Exactly
Subspace Sampling Expected
Fast CUR
Relative Error Ratio
Subspace Sampling Exactly
Subspace Sampling Expected
Fast CUR
Subspace Sampling Exactly
Subspace Sampling Expected
Fast CUR
Relative Error Ratio
Relative Error Ratio
Subspace Sampling Exactly
Subspace Sampling Expected
Fast CUR
18 22 24 26 28
18
and and and
Figure Empirical results on the Arcene data set
Running Time
Running Time
Running Time
Subspace Sampling Exactly
Subspace Sampling Expected
Fast CUR
Subspace Sampling Exactly
Subspace Sampling Expected
Fast CUR
Subspace Sampling Exactly
Subspace Sampling Expected
Fast CUR
Time
Time
Time
18 22 24 26 28
18 22 24
18 22 24 26 28
18
18 22 24
Subspace Sampling Exactly
Subspace Sampling Expected
Fast CUR
Relative Error Ratio
Relative Error Ratio
Subspace Sampling Exactly
Subspace Sampling Expected
Fast CUR
Construction Error Frobenius Norm
Subspace Sampling Exactly
Subspace Sampling Expected
Fast CUR
Construction Error Frobenius Norm
Construction Error Frobenius Norm
Relative Error Ratio
18
and and and
Figure Empirical results on the Dexter data set
Conclusions
In this paper we have proposed a novel randomized algorithm for the CUR matrix decomposition
problem This algorithm is faster more scalable and more accurate than the state-of-the-art algorithm the subspace sampling algorithm Our algorithm requires only
columns and rows to achieve relative-error ratio To achieve the same
relative-error bound the subspace sampling algorithm requires log columns and
log rows selected from the original matrix Our algorithm also beats the subspace
sampling algorithm in time-complexity Our algorithm costs n)k
mk nk time which is lower than O(min{mn2 m2 of the subspace sampling algorithm when is small Moreover our algorithm enjoys another advantage of avoiding loading the
whole data matrix into main memory which also makes our algorithm more scalable Finally the
empirical comparisons have also demonstrated the effectiveness and efficiency of our algorithm
A The Dual Set Sparsification Algorithm
For the sake of completeness we attach the dual set sparsification algorithm here and describe
some implementation details The dual set sparsification algorithms are deterministic algorithms
established in The fast CUR algorithm calls the dual set spectral-Frobenius sparsification algorithm Lemma in both stages We show this algorithm in Algorithm and its bounds in
Lemma
Lemma Dual Set Spectral-Frobenius Sparsification Let Rl
l?n
contains the columns of an arbitrary matrix
RT Let vn
be a decompositions of the identity
I
Given
an
integer
with
Algorithm deterministically computes a set of weights si at most of which
are non-zero such that
and
tr
si xTi
si vi vi
Algorithm Deterministic Dual Set Spectral-Frobenius Sparsification Algorithm
Input
vi with
vi vi Ik
Initialize s0 A0
Compute for and then compute
k/r
for to do
Compute the eigenvalue decomposition of A
Find an index in and compute a weight such that
vjT A
vj
vj
vjT A
A A
where
A
rk
Update the j-th component of and A
end for
k/r
return
sr
A A tvj vjT
The weights si can be computed deterministically in O(rnk nl time
Here we would like to mention the implementation of Algorithm which is not described in detailed
by In each iteration the algorithm performs once eigenvalue decomposition A W?WT
is guaranteed to be positive semi-definite in each iteration Since
A Ik WDiag WT
we can efficiently compute based on the eigenvalue decomposition of A With
the eigenvalues at hand A can also be computed directly
Acknowledgments
This work has been supported in part by the Natural Science Foundations of China
the Google visiting faculty program and the Scholarship Award for Excellent Doctoral Student
granted by Ministry of Education

----------------------------------------------------------------

title: 5735-randomized-block-krylov-methods-for-stronger-and-faster-approximate-singular-value-decomposition.pdf

Randomized Block Krylov Methods for Stronger and
Faster Approximate Singular Value Decomposition
Christopher Musco
Massachusetts Institute of Technology EECS
Cambridge MA USA
cpmusco@mit.edu
Cameron Musco
Massachusetts Institute of Technology EECS
Cambridge MA USA
cnmusco@mit.edu
Abstract
Since being analyzed by Rokhlin Szlam and Tygert and popularized by
Halko Martinsson and Tropp randomized Simultaneous Power Iteration has
become the method of choice for approximate singular value decomposition It is
more accurate than simpler sketching algorithms yet still converges quickly for
any matrix independently of singular value gaps After
iterations it gives
a low-rank approximation within of optimal for spectral norm error
We give the first provable runtime improvement on Simultaneous Iteration a randomized block Krylov method closely related to the classic Block Lanczos algo iterations and performs substanrithm gives the same guarantees in just
tially better experimentally Our analysis is the first of a Krylov subspace method
that does not depend on singular value gaps which are unreliable in practice
Furthermore while it is a simple accuracy benchmark even error for
spectral norm low-rank approximation does not imply that an algorithm returns
high quality principal components a major issue for data applications We address
this problem for the first time by showing that both Block Krylov Iteration and
Simultaneous Iteration give nearly optimal PCA for any matrix This result further
justifies their strength over non-iterative sketching methods
Introduction
Any matrix A Rn?d with rank can be written using a singular value decomposition SVD as
A U?VT Rn?r and Rd?r have orthonormal columns left and right singular
vectors and Rr?r is a positive diagonal matrix containing A?s singular values
A rank partial SVD algorithm returns just the top left or right singular vectors of A. These are
the first columns of or denoted Uk and Vk respectively
Among countless applications the SVD is used for optimal low-rank approximation and principal
component analysis Specifically for a partial SVD can be used to construct a rank
approximation Ak such that both kA Ak kF and kA Ak are as small as possible We simply
set Ak Uk UTk A. That is Ak is A projected onto the space spanned by its top singular vectors
For principal component analysis A?s top singular vector u1 provides a top principal component
which describes the direction of greatest variance within A. The ith singular vector ui provides the
ith principal component which is the direction of greatest variance orthogonal to all higher principal
components Formally denoting A?s ith singular value as
uTi AAT ui
max
x:kxk2 x?uj
xT AAT
Traditional SVD algorithms are expensive typically running in O(nd2 time so there has been substantial research on randomized techniques that seek nearly optimal low-rank approximation and
PCA These methods are quickly becoming standard tools in practice and implementations are widely available including in popular learning libraries
Recent work focuses on algorithms whose runtimes do not depend on properties of A. In contrast
classical literature typically gives runtime bounds that depend on the gaps between A?s singular
values and become useless when these gaps are small which is often the case in practice see
Section This limitation is due to a focus on how quickly approximate singular vectors converge
to the actual singular vectors of A. When two singular vectors have nearly identical values they are
difficult to distinguish so convergence inherently depends on singular value gaps
Only recently has a shift in approximation goal along with an improved understanding of randomization allowed for algorithms that avoid gap dependence and thus run provably fast for any matrix
For low-rank approximation and PCA we only need to find a subspace that captures nearly as much
variance as A?s top singular vectors distinguishing between two close singular values is overkill
Prior Work
The fastest randomized SVD algorithms run in O(nnz(A time1 are based on non-iterative
sketching methods and return a rank matrix with orthonormal columns z1 zk satisfying
Frobenius Norm Error
kA ZZT AkF Ak kF
Unfortunately as emphasized in prior work Frobenius norm error is often hopelessly
insufficient especially for data analysis and learning applications
When A has a heavy-tail of
singular values which is common for noisy data kA Ak k2F i>k can be huge potentially
much larger than A?s top singular value This renders meaningless since does not need to
align with any large singular vectors to obtain good multiplicative error
To address this shortcoming a number of papers target spectral norm low-rank approximation error
Spectral Norm Error
kA ZZT Ak2 Ak
which is intuitively stronger When looking for a rank approximation A?s top singular vectors
are often considered data and the remaining tail is considered noise A spectral norm guarantee
roughly ensures that ZZT A recovers A up to this noise threshold
A series of work 13 shows that the decades old Simultaneous Power Iteration also
called subspace iteration or orthogonal iteration implemented with random start vectors achieves
after
iterations Hence this method which was popularized by Halko Martinsson and
Tropp in has become the randomized SVD algorithm of choice for practitioners
Our Results
Algorithm IMULTANEOUS I TERATION
Algorithm LOCK RYLOV I TERATION
n?d
input A
error rank input A Rn?d error rank
n?k
output
output Rn?k
log
d?k
log
AAT A
AAT AAT A
Orthonormalize the columns of to obtain Orthonormalize the columns of to obtain
Rn?k
Rn?qk
k?k
Compute AA
Compute QT AAT Rqk?qk
to the top singular vectors of M.
Set Uk to the top singular vectors of M.
Set
return QU
return QU
Faster Algorithm
We show that Algorithm a randomized relative of the Block Lanczos algorithm which
we call Block Krylov Iteration gives the same guarantees as Simultaneous Iteration Algorithm
iterations This not only gives the fastest known theoretical runtime for achieving
in just
but also yields substantially better performance in practice Section
Here nnz(A is the number of non-zero entries in A and this runtime hides lower order terms
Even though the algorithm has been discussed and tested for potential improvement over Simultaneous Iteration 19 theoretical bounds for Krylov subspace and Lanczos methods are much
more limited As highlighted in
Despite decades of research on Lanczos methods the theory for randomized
power iteration is more complete and provides strong guarantees of excellent
accuracy whether or not there exist any gaps between the singular values
Our work addresses this issue giving the first gap independent bound for a Krylov subspace method
Stronger Guarantees
In addition to runtime improvements we target a much stronger notion of approximate SVD that is
needed for many applications but for which no gap-independent analysis was known
Specifically as noted in while intuitively stronger than Frobenius norm error spectral norm low-rank approximation error does not guarantee any accuracy in for many matrices2
Consider A with its top squared singular values all equal to followed by a tail of smaller
singular values at kA Ak but in fact kA ZZT for any rank
leaving the spectral norm bound useless At the same time kA Ak k2F is large so Frobenius
error is meaningless as well For example any obtains kA ZZT Ak2F Ak k2F
With this scenario in mind it is unsurprising that low-rank approximation guarantees fail as an
accuracy measure in practice We ran a standard sketch-and-solve approximate SVD algorithm
Section on SNAP AMAZON an Amazon product co-purchasing dataset and
achieved very good low-rank approximation error in both norms for
kA ZZT AkF Ak kF and kA ZZT Ak2 Ak
However the approximate principal components given by are of significantly lower quality than
A?s true singular vectors Figure We saw similar results for a number of other datasets
uT(AAT)u
Singular Value
zTi AAT)zi
Index
Figure Poor per vector error for SNAP AMAZON returned by a sketch-and-solve approximate SVD that gives very good low-rank approximation in both spectral and Frobenius norm
We address this issue by introducing a per vector guarantee that requires each approximate singular
vector z1 zk to capture nearly as much variance as the corresponding true singular vector
Per Vector Error
uTi AAT ui zTi AAT zi
The error bound is very strong in that it depends on
which is better then relative error
for A?s large singular values While it is reminiscent of the bounds sought in classical numerical
analysis we stress that does not require each zi to converge to ui in the presence of small
singular value gaps In fact we show that both randomized Block Krylov Iteration and our slightly
modified Simultaneous Iteration algorithm achieve in gap-independent runtimes
Main Result
Our contributions are summarized in Theorem Its detailed proof is relegated to the full version of
this paper The runtimes are given in Theorems and and the three error bounds shown in
Theorems and In Section we provide a sketch of the main ideas behind the result
In fact it does not even imply Frobenius norm error
Theorem Main Theorem With high probability Algorithms and find approximate singular
vectors zk satisfying guarantees and for low-rank approximation and for
PCA. For?error Algorithm requires O(log iterations while Algorithm requires
O(log iterations Excluding lower order terms both algorithms run in time O(nnz(A)kq
In the full version of this paper we also use our results to give an alternative analysis that does
depend on singular value gaps and can offer significantly faster convergence when A has decaying
singular values It is possible to take further advantage of this result by running Algorithms and
with a that has columns a simple modification for accelerating either method
In Section we test both algorithms on a number of large datasets We justify the importance of gap
independent bounds for predicting algorithm convergence and we show that Block Krylov Iteration
in fact significantly outperforms the more popular Simultaneous Iteration
Comparison to Classical Bounds
Decades of work has produced a variety of gap dependent bounds for Krylov methods Most
relevant to our work are bounds for block Krylov methods with block size equal to Roughly
speaking with randomized initialization these results offerp
guarantees equivalent to our strong equation for the top singular directions after iterations
This bound is recovered in Section of this paper?s full version When the target accuracy
is smaller than the relative singular value gap it is tighter than our gap independent
results However as discussed in Section for high dimensional data problems where is set far
above machine precision gap independent bounds more accurately predict required iteration count
Prior work also attempts to analyze algorithms with block size smaller than While small
block algorithms offer runtime advantages it is well understood that with duplicate singular
values it is impossible to recover the top singular directions with a block of size More
generally large singular value clusters slow convergence so any small block algorithm must have
runtime dependence on the gaps between each adjacent pair of top singular values
Analyzing Simultaneous Iteration
Before discussing our proof of Theorem we review prior work on Simultaneous Iteration to
demonstrate how it can achieve the spectral norm guarantee
Algorithms for Frobenius norm error typically work by sketching A into very few dimensions
using a Johnson-Lindenstrauss random projection matrix with poly(k columns
An?d
is usually a random Gaussian or possibly sparse random sign matrix and is computed using
the SVD of A or of A projected onto A This sketch-and-solve approach is very
efficient the computation of A is easily parallelized and regardless pass-efficient in a single
processor setting Furthermore once a small compression of A is obtained it can be manipulated
in fast memory for the final computation of Z.
However Frobenius norm error seems an inherent limitation of sketch-and-solve methods The
noise from A?s lower singular values corrupts making it impossible to extract a good
partial SVD if the sum of these singular values equal to kA Ak k2F is too large
In order to achieve spectral norm error Simultaneous Iteration must reduce this noise down to
the scale of kA Ak It does this by working with the powered matrix Aq By the
spectral theorem Aq has exactly the same singular vectors as A but its singular values are equal to
those of A raised to the th power Powering spreads the values apart and accordingly Aq lower
singular values are relatively much smaller than its top singular values example in Figure
Specifically log is sufficient to increase any singular value to be significantly poly(d times larger than any value This effectively denoises our problem
if we use a sketching method to find a good for approximating Aq up to Frobenius norm error
will have to align very well with every singular vector with value It thus provides
an accurate basis for approximating A up to small spectral norm error
For nonsymmetric matrices we work with AAT A but present the symmetric case here for simplicity
45
Spectrum of A
Spectrum of Aq
Singular Value
35
Index
An degree Chebyshev polynomial pushes low values nearly
as close to zero as
A?s singular values compared to those of
Aq rescaled to match on Notice the significantly reduced tail after
Figure Replacing A with a matrix polynomial facilitates higher accuracy approximation
Computing Aq directly is costly so Aq is computed iteratively start with a random and
repeatedly multiply by A on the left Since even a rough Frobenius norm approximation for Aq
suffices can be chosen to have just columns Each iteration thus takes O(nnz(A)k time
When analyzing Simultaneous Iteration uses the following randomized sketch-and-solve result
to find a that gives a coarse Frobenius norm approximation to Aq and therefore a good
spectral norm approximation to A. The lemma is numbered for consistency with our full paper
Lemma Frobenius Norm Low-Rank Approximation For any Rn?d and Rd?k where
the entries of are independent Gaussians drawn from If we let be an orthonormal
basis for span then with probability at least for some fixed constant
kB ZZT Bk2F dkkB Bk k2F
For analyzing block methods results like Lemma can effectively serve as a replacement for earlier
random initialization analysis that applies to single vector power and Krylov methods
Aq
poly(d A
for any with Plugging into Lemma
kAq ZZT Aq k2F cdk
Aq cdk
Aq
Aq poly(d
Aq
Rearranging using Pythagorean theorem we have kZZT Aq k2F kAq k2F poly(d
That is Aq
projection onto captures nearly all of its Frobenius norm This is only possible if aligns very
well with the top singular vectors of Aq and hence gives a good spectral norm approximation for A.
Proof Sketch for Theorem
The intuition for beating Simultaneous Iteration with Block Krylov Iteration matches that of many
accelerated iterative methods Simply put there are better polynomials than Aq for denoising tail
singular values In particular we can use a lower degree polynomial allowing us to compute fewer
powers of A and thus leading
to an algorithm with fewer iterations For example an appropriately
shifted O(log(d degree Chebyshev polynomial can push the tail of A nearly as close to
zero as AO(log even if the long run growth of the polynomial is much lower Figure
Specifically we prove the following scalar polynomial lemma in the full version of our paper
which can then be applied to effectively denoising A?s singular value tail
Lemma Chebyshev Minimizing Polynomial For and O(log there exists
a degree polynomial such that and
for
poly(d
for
Furthermore we can choose the polynomial to only contain monomials with odd powers
Block Krylov Iteration takes advantage of such polynomials by working with the Krylov subspace
A A2 A3 Aq
from which we can construct pq for any polynomial pq of degree Since the polynomial
from Lemma must be scaled and shifted based on the value of we cannot easily compute it
directly Instead we argue that the very best rank approximation to A lying in the span of at
least matches the approximation achieved by projecting onto the span of pq Finding this best
approximation will therefore give a nearly optimal low-rank approximation to A.
Unfortunately there?s a catch Surprisingly it is not clear how to efficiently compute the best spectral
norm error low-rank approximation to A lying in a given subspace K?s span This
challenge precludes an analysis of Krylov methods parallel to recent work on Simultaneous Iteration
Nevertheless since our analysis shows that projecting to captures nearly all the Frobenius norm
of pq we can show that the best Frobenius norm low-rank approximation to A in the span of
gives good enough spectral norm approximation By the following lemma this optimal Frobenius
norm low-rank approximation is given by ZZT A where is exactly the output of Algorithm
Lemma Lemma of Given A Rn?d and Rm?n with orthonormal columns
min
kA QCkF
kA QQT A)k kF kA QT A kF
C|rank(C)=k
QT A can be obtained using an SVD of the matrix QT AAT Specifically
2U
be the SVD of and QU
then QT A ZZT A.
letting
Stronger Per Vector Error Guarantees
Achieving the per vector guarantee of requires a more nuanced understanding of how Simultaneous Iteration and Block Krylov Iteration denoise the spectrum of A. The analysis for spectral norm
low-rank approximation relies on the fact that Aq pq for Block Krylov Iteration blows up
any singular value to much larger than any singular value This ensures that
our output aligns very well with the singular vectors corresponding to these large singular values
If then aligns well with all top singular vectors of A and we get good
Frobenius norm error and the per vector guarantee Unfortunately when there is a small gap
between and could miss intermediate singular vectors whose values lie between
and This is the case where gap dependent guarantees of classical analysis break down
However Aq or for Block Krylov Iteration some q-degree polynomial in our Krylov subspace also
significantly separates singular values from those Thus each column of
at least aligns with A nearly as well as So even if we miss singular values between and
they will be replaced with approximate singular values enough for
For Frobenius norm low-rank approximation we prove that the degree to which falls outside of
the span of A?s top singular vectors depends on the number of singular values between and
These are the values that could be swapped in for the true top singular values Since
their weight counts towards A?s tail our total loss compared to optimal is at worst kA Ak k2F
Implementation and Runtimes
For both Algorithm and can be replaced by a random sign matrix or any matrix achieving
the guarantee of Lemma may also be chosen with columns In our full paper we
discuss in detail how this approach can give improved accuracy
Simultaneous Iteration
which is necessary for achieving per vector guarantees for
In our implementation we set QU
approximate PCA. However for near optimal low-rank approximation we can simply set Q.
is equivalent to projecting to as these matrices have the same column spans
Projecting A to QU
Since powering A spreads its singular values AAT A could be poorly conditioned To
improve stability we orthonormalize after every iteration every few iterations This does not
change K?s column span so it gives an equivalent algorithm in exact arithmetic
Algorithm in fact only constructs odd powered terms in which is sufficient for our choice of pq
Theorem Simultaneous Iteration Runtime Algorithm runs in time
nnz(A)k nk
Proof Computing requires first multiplying A by which takes O(nnz(A)k time Computing
A then takes O(nnz(A)k time to first multiply our
AAT A given AAT
matrix by AT and then by A. Reorthogonalizing after each iteration takes O(nk time via GramSchmidt This gives a total runtime of O(nnz(A)kq nk for computing K. Finding takes
O(nk time Computing by multiplying from left to right requires O(nnz(A)k nk time
by takes
M?s SVD then requires O(k time using classical techniques Finally multiplying
time O(nk Setting log gives the claimed runtime
Block Krylov Iteration
In the traditional Block Lanczos algorithm one starts by computing an orthonormal basis for
the first block in K. Bases for subsequent blocks are computed from previous blocks using a three
term recurrence that ensures QT AAT is block tridiagonal with sized blocks This
technique can be useful if qk is large since it is faster to compute the top singular vectors of a block
tridiagonal matrix However computing using a recurrence can introduce a number of stability
issues and additional steps may be required to ensure that the matrix remains orthogonal
An alternative uesd in and our Algorithm is to compute explicitly and then find
using a QR decomposition This method does not guarantee that QT AAT is block tridiagonal
but avoids stability issues Furthermore if qk is small taking the SVD of QT AAT will still be
fast and typically dominated by the cost of computing K.
As with Simultaneous Iteration we orthonormalize each block of after it is computed avoiding
poorly conditioned blocks and giving an equivalent algorithm in exact arithmetic
Theorem Block Krylov Iteration Runtime Algorithm runs in time
nnz(A)k log(d nk log2 log3
Proof Computing including reorthogonalization requires O(nnz(A)kq nk time The remaining steps are analogous to those in Simultaneous Iteration except somewhat more costly as we
work with a rather than dimensional subspace Finding takes time Computing
take O(nnz(A)(kq time and its SVD then requires
time Finally multi
plying Uk by takes time Setting log gives the claimed runtime
Experiments
We close with several experimental results A variety of empirical papers not to mention widespread
adoption already justify the use of randomized SVD algorithms Prior work focuses in particular on
benchmarking Simultaneous Iteration and due to its improved accuracy over sketch-andsolve approaches this algorithm is popular in practice As such we focus on demonstrating
that for many data problems Block Krylov Iteration can offer significantly better convergence
We implement both algorithms in MATLAB using Gaussian random starting matrices with exactly
columns We explicitly compute for both algorithms as described in Section and use reorthonormalization at each iteration to improve stability We test the algorithms with varying
iteration count on three common datasets SNAP AMAZON SNAP EMAIL NRON
and EWSGROUPS computing column principal components in all cases We plot
error iteration count for metrics and in Figure For per vector error we plot the
maximum deviation amongst all top approximate principal components relative to
Unsurprisingly both algorithms obtain very accurate Frobenius norm error kA ZZT AkF kA
Ak kF with very few iterations This is our intuitively weakest guarantee and in the presence of a
heavy singular value tail both iterative algorithms will outperform the worst case analysis
On the other hand for spectral norm low-rank approximation and per vector error we confirm that
Block Krylov Iteration converges much more rapidly than Simultaneous Iteration as predicted by
Block Krylov Frobenius Error
Block Krylov Spectral Error
Block Krylov Per Vector Error
Simult Iter Frobenius Error
Simult Iter Spectral Error
Simult Iter Per Vector Error
Error
Block Krylov Frobenius Error
Block Krylov Spectral Error
Block Krylov Per Vector Error
Simult Iter Frobenius Error
Simult Iter Spectral Error
Simult Iter Per Vector Error
Error
Iterations
SNAP AMAZON
Error
Block Krylov Frobenius Error
Block Krylov Spectral Error
Block Krylov Per Vector Error
Simult Iter Frobenius Error
Simult Iter Spectral Error
Simult Iter Per Vector Error
Error
SNAP EMAIL NRON
Block Krylov Frobenius Error
Block Krylov Spectral Error
Block Krlyov Per Vector Error
Simult Iter Frobenius Error
Simult Iter Spectral Error
Simult Iter Per Vector Error
Iterations
Iterations
Runtime seconds
EWSGROUPS
EWSGROUPS runtime cost
Figure Low-rank approximation and per vector error convergence rates for Algorithms and
our theoretical analysis It it often possible to achieve nearly optimal error with iterations where
as getting to within say error with Simultaneous Iteration can take much longer
The final plot in Figure shows error verses runtime for the dimensional EWS GROUPS dataset We averaged over trials and ran the experiments on a commodity laptop with
of memory As predicted because its additional memory overhead and post-processing costs
are small compared to the cost of the large matrix multiplication required for each iteration Block
Krylov Iteration outperforms Simultaneous Iteration for small
More generally these results justify the importance of convergence bounds that are independent of
singular value gaps Our analysis in Section of the full paper predicts that once is small in
comparison to the gap
we should see much more rapid convergence since will depend
on instead of However for Simultaneous Iteration we do not see this behavior with
SNAP AMAZON and it only just begins to emerge for EWSGROUPS
While all three datasets have rapid singular value decay a careful look confirms that their singular
value gaps are actually quite small For example is for SNAP AMAZON
and for EWSGROUPS in comparison to for SNAP EMAIL NRON Accordingly the
frequent claim that singular value gaps can be taken as constant is insufficient even for small

----------------------------------------------------------------

title: 6004-secure-multi-party-differential-privacy.pdf

Secure Multi-party Differential Privacy
Peter Kairouz1
Sewoong Oh2
Pramod Viswanath1
Department of Electrical Computer Engineering
Department of Industrial Enterprise Systems Engineering
University of Illinois Urbana-Champaign
Urbana IL USA
kairouz2,swoh,pramodv}@illinois.edu
Abstract
We study the problem of interactive function computation by multiple parties
each possessing a bit in a differential privacy setting there remains an uncertainty in any party?s bit even when given the transcript of interactions and all the
other parties bits Each party wants to compute a function which could differ
from party to party and there could be a central observer interested in computing
a separate function Performance at each party is measured via the accuracy of
the function to be computed We allow for an arbitrary cost metric to measure
the distortion between the true and the computed function values Our main result is the optimality of a simple non-interactive protocol each party randomizes
its bit sufficiently and shares the privatized version with the other parties This
optimality result is very general it holds for all types of functions heterogeneous
privacy conditions on the parties all types of cost metrics and both average and
worst-case over the inputs measures of accuracy
Introduction
Multi-party computation MPC is a generic framework where multiple parties share their information in an interactive fashion towards the goal of computing some functions potentially different at
each of the parties In many situations of common interest the key challenge is in computing the
functions as privately as possible without revealing much about one?s information to the other
potentially colluding parties For instance an interactive voting system aims to compute the majority of say binary opinions of each of the parties with each party being averse to declaring their
opinion publicly Another example involves banks sharing financial risk exposures the banks need
to agree on quantities such as the overnight lending rate which depends on each bank?s exposure
which is a quantity the banks are naturally loath to truthfully disclose A central learning theory
question involves characterizing the fundamental limits of interactive information exchange such
that a strong and suitably defined adversary only learns as little as possible while still ensuring that
the desired functions can be computed as accurately as possible
One way to formulate the privacy requirement is to ensure that each party learns nothing more
about the others information than can be learned from the output of the function computed This
topic is studied under the rubric of secure function evaluation the SFE formulation has
been extensively studied with the goal of characterizing which functions can be securely evaluated One drawback of SFE is that depending on what auxiliary information the
adversary might have disclosing the exact function output might reveal each party?s data For example consider computing the average of the data owned by all the parties Even if we use SFE a
party?s data can be recovered if all the other parties collaborate To ensure protection of the private
data under such a strong adversary we want to impose a stronger privacy guarantee of differential
privacy Recent breaches of sensitive information about individuals due to linkage attacks prove
the vulnerability of existing ad-hoc privatization schemes such as anonymization of the records In
linkage attacks an adversary matches up anonymized records containing sensitive information with
public records in a different dataset Such attacks have revealed the medical record of a former governor of Massachusetts the purchase history of Amazon users[7 genomic information
and movie viewing history of Netflix users
An alternative formulation is differential privacy a relatively recent formulation that has received
considerable attention as a formal mathematical notion of privacy that provides protection against
such strong adversaries recent survey is available at The basic idea is to introduce enough
randomness in the communication so that an adversary possessing arbitrary side information and
access to the entire transcript of the communication will still have some residual uncertainty in
identifying any of the bits of the parties This privacy requirement is strong enough that non-trivial
functions will be computed only with some error Thus there is a great need for understanding
the fundamental tradeoff between privacy and accuracy and for designing privatization mechanisms
and communication protocols that achieve the optimal tradeoffs The formulation and study of an
optimal framework addressing this tradeoff is the focus of this paper
We study the following problem of multi-party computation under differential privacy each party
possesses a single bit of information and the information bits are statistically independent Each
party is interested in computing a function which could differ from party to party and there could
be a central observer observing the entire transcript of the interactive communication protocol
interested in computing a separate function Performance at each party and the central observer is
measured via the accuracy of the function to be computed We allow an arbitrary cost metric to
measure the distortion between the true and the computed function values Each party imposes a
differential privacy constraint on its information bit the privacy level could be different from party
to party there remains an uncertainty in any specific party?s bit even to an adversary that has
access to the transcript of interactions and all the other parties bits The interactive communication
is achieved via a broadcast channel that all parties and the central observer can hear this modeling
is without loss of generality since the differential privacy constraint protects against an adversary
that can listen to the entire transcript the communication between any two parties might as well be
revealed to all the others It is useful to distinguish between two types of communication protocols
interactive and non-interactive We say a communication protocol is non-interactive if a message
broadcasted by one party does not depend on the messages broadcasted by other parties In contrast
interactive protocols allow the messages at any stage of the communication to depend on all the
previous messages
Our main result is the exact optimality of a simple non-interactive protocol in terms of maximizing
accuracy for any given privacy level when each party possesses one bit each party randomizes
sufficiently and publishes its own bit In other words
non-interactive randomized response is exactly optimal
Each party and the central observer then separately compute their respective decision functions to
maximize the appropriate notion of their accuracy measure This optimality result is very general
it holds for all types of functions heterogeneous privacy conditions on the parties all types of
cost metrics and both average and worst-case over the inputs measures of accuracy Finally the
optimality result is simultaneous in terms of maximizing accuracy at each of the parties and the
central observer Each party only needs to know its own desired level of privacy its own function
to be computed and its measure of accuracy Optimal data release and optimal decision making is
naturally separated
The key technical result is a geometric understanding of the space of conditional probabilities of a
given transcript the interactive nature of the communication constrains the space to be a rank-1 tensor special case of Equation in and perhaps implicitly used in the two-party analog
of this result is in while differential privacy imposes linear constraints on the singular vectors
of this tensor We characterize the convex hull of such manifolds of rank-1 tensors and show that
their corner-points exactly correspond to the transcripts that arise from a non-interactive randomized response protocol This universal for all functionalities characterization is then used to argue
that both average-case and worst-case accuracies are maximized by non-interactive randomized responses
Technically we prove that non-interactive randomized response is the optimal solution of the rankconstrained and non-linear optimization of The rank constraints on higher order tensors arises
from the necessary condition of possibly interactive multi-party protocols known as protocol compatibility Section for details To solve this non-standard optimization we transform into
a novel linear program of and The price we pay is the increased dimension the resulting LP is now infinite dimensional The idea is that we introduce a new variable for each possible
rank-one tensor and optimize over all of them
Formulating utility maximization under differential privacy as linear programs has been previously
studied in under the standard client-server model where there is a single data publisher and a single data analyst These approaches exploit the fact that both the differential privacy
constraints and the utilities are linear in the matrix representing a privatization mechanism A similar technique of transforming a non-linear optimization problem into an infinite dimensional LP has
been successfully applied in where optimal privatization mechanisms under local differential
privacy has been studied We generalize these techniques to rank-constrained optimizations
Further perhaps surprisingly we prove that this infinite dimensional linear program has a simple
optimal solution which we call randomized response Upon receiving the randomized responses
each party can compute the best approximation of its respective function The main technical innovation is in proving that the optimal solution of this LP corresponds to corner points of a convex
hull of a particular manifold defined by a rank-one tensor Lemma in the supplementary material for details and the respective manifold has a simple structure such that the corner points
correspond to particular protocols that we call randomized responses
When the accuracy is measured via average accuracy both the objective and the constraints are
linear and it is natural to expect the optimal solution to be at the corner points Equation
A surprising aspect of our main result is that the optimal solution is still at the corner points even
though the worst-case accuracy is a concave function over the protocol Equation
This work focuses on the scenario where each party possesses a single bit of information With
multiple bits of information at each of the parties the existence of a differentially private protocol
with a fixed accuracy for any non-trivial functionality implies the existence of a protocol with the
same level of privacy and same level of accuracy for a specific functionality that only depends on
one bit of each of the parties as in Thus if we can obtain lower bounds on accuracy for
functionalities involving only a single bit at each of the parties we obtain lower bounds on accuracy
for all non-trivial general functionalities However non-interactive communication is unlikely to be
exactly optimal in this general case where each party possesses multiple bits of information and we
provide a further discussion in Section We move a detailed discussion of related work Section
to the supplementary material focusing on the problem formulation next
Problem formulation
Consider the setting where we have parties each with its own private binary data
generated independently The independence assumption here is necessary because without it each
party can learn something about others which violates differential privacy even without revealing
any information We discuss possible extensions to correlated sources in Section Differential
privacy implicitly imposes independence in a multi-party setting The goal of the private multi-party
computation is for each party to compute an arbitrary function fi of interest
by interactively broadcasting messages while preserving the privacy of each party There might be
a central observer who listens to all the messages being broadcasted and wants to compute another
arbitrary function f0 Y. The parties are honest in the sense that once they agree on
what protocol to follow every party follows the rules At the same time they can be curious and
each party needs to ensure other parties cannot learn his bit with sufficient confidence The privacy
constraints here are similar to the local differential privacy setting studied in in the sense that
there are multiple privacy barriers each one separating each individual party and the rest of the
world However the main difference is that we consider multi-party computation where there are
multiple functions to be computed and each node might possess a different function to be computed
Let xk denote the vector of bits and x?i
xk is the vector of bits except for the i-th bit The parties
agree on an interactive protocol to achieve the goal of multi-party computation A transcript is
the output of the protocol and is a random instance of all broadcasted messages until all the communication terminates The probability that a transcript is broadcasted via a series of interactive
communications when the data is is denoted by for and for
Then a protocol can be represented as a matrix denoting the probability distribution over a set of
transcripts conditioned on
In the end each party makes a decision on what the value of function fi is based on its own bit
and the transcript that was broadcasted A decision rule is a mapping from a transcript and
private bit to a decision represented by a function f?i We allow randomized
decision rules in which case f?i can be a random variable For the central observer a decision
rule is a function of just the transcript denoted by a function
We consider two notions of accuracy the average accuracy and the worst-case accuracy For the
i-th party consider an accuracy measure equivalently a negative cost function
such that fi f?i measures the accuracy when the function to be computed is fi and
the approximation is f?i Then the average accuracy for this i-th party is defined as
ACCave fi f?i
Ef?i fi f?i
2k
where the expectation is taken over the random transcript distribution as and also any randomness in the decision function f?i We want to emphasize that the input is deterministic we impose
no distribution on the input data and the expectation is not over the data sets Compared to assuming a distribution over the data this is a weaker assumption on the data and hence makes our main
result stronger For example if the accuracy measure is an indicator such that
then ACCave measures the average probability of getting the correct function output For a given
protocol it takes operations to compute the optimal decision rule
fi,ave
arg max
fi
y?Y
x?i
for each The computational cost of for computing the optimal decision rule is
unavoidable in general since that is the inherent complexity of the problem describing the distribution of the transcript requires the same cost We will show that the optimal protocol requires a set
of transcripts of size 2k and the computational complexity of the decision rule for general
a function is However for a fixed protocol this decision rule needs to be computed only once
before any message is transmitted Further it is also possible to find a closed form solution for the
decision rule when has a simple structure One example is the XOR function studied in detail in
Section where the optimal decision rule is as simple as evaluating the XOR of all the received bits
which requires operations When there are multiple maximizers we can choose arbitrarily
and it follows that there is no gain in randomizing the decision rule for average accuracy Similarly
the worst-case accuracy is defined as
ACCwc fi f?i
min Ef?i fi f?i
For worst-case accuracy given a protocol the optimal decision rule of the i-th party with a bit
can be computed by solving the following convex program
XX
Q(xi arg max
min
fi
R|T
subject to
x?i
y?Y
y?Y
and
The optimal random decision rule fi,wc
is to output given transcript according to
Q?,yi This can be formulated as a linear program with variables and
constraints Again it is possible to find a closed form solution for the decision rule
when has a simple structure for the XOR function the optimal decision rule is again evaluating
the XOR of all the received bits requiring operations For a central observer the accuracy
measures are defined similarly and the optimal decision rule is now
f0,ave
arg max
w0
y?Y
and for worst-case accuracy the optimal random decision rule f0,wc
is to output given tranP
script according to Subject to y?Y and
XX
arg max
min
w0
R|T
y?Y
where w0 is the measure of accuracy for the central observer
Privacy is measured by differential privacy Since we allow heterogeneous privacy constraints we use to denote the desired privacy level of the i-th party We say a protocol is
differentially private for the i-th party if for and all x0i x?i and
x?i e?i x?i
This condition ensures no adversary can infer the private data with high enough confidence no
matter what auxiliary information he might have and independent of his computational power To
lighten notations we let e?i and say a protocol is differentially private for the i-th party
If the protocol is differentially private for all then we say that the protocol is differentially private for all parties
A necessary condition on the multi-party protocols when the bits are generated independent of
each other is protocol compatibility conditioned on the transcript of the protocol the input
bits stay independent of each other In our setting input bits are deterministic hence independent
Mathematically a protocol is protocol compatible if each column is a rank-one tensor when
reshaped into a k-th order tensor where
Precisely there exist vectors such that where denotes the
standard outer-product Pi1 ui1 uik This is crucial in deriving the main results
and it is a well-known fact in the secure multi-party computation literature This follows from the
fact that when the bits are generated
independently all the bits are still independent conditioned
on the transcript which follows implicitly from and directly from
Equation of Notice that using the rank-one tensor representation of each column of the
protocol we have x?i x?i u1 It follows that is
differentially private if and only if
u2 u1 u2
Randomized response Consider the following simple protocol known as the randomized response
which is a term first coined by and commonly used in many private communications including
the multi-party setting We will show in Section that this is the optimal protocol for simultaneously maximizing the accuracy of all the parties Each party broadcasts a randomized version of
its bit denoted by
such that
with probability
x?i with probability
where x?i is the logical complement of Each transcript can be represented by the output of the
protocol which in this case is
where is now the set of all
broadcasted bits
Accuracy maximization Consider the problem of maximizing the average accuracy for a centralized observer with function Up to the scaling of in the accuracy can be written as
XX
EP
y?Y
Wx
where denotes the randomized decision up on receiving the transcript In the following we
define Wx to represent the accuracy measure and to represent
the decision rule
Focusing on this single central observer for the purpose of illustration we want to design protocols
and decision rules that maximize the above accuracy Further this protocol has to be
compatible with interactive communication satisfying the rank one condition discussed above and
satisfy the differential privacy condition in Hence we can formulate the accuracy maximization
can be formulated as follows Given Wx in terms of the function f0 to be computed an
accuracy measure w0 and required privacy level we solve
maximize
Wx(y
subject to
and are row-stochastic matrices
rank(P
P(xi P(x0i x?i
where is defined as a k-th order tensor defined from the th column of matrix as defined
in Equation Notice that the rank constraint is only a necessary condition for a protocol to be
compatible with interactive communication schemes a valid interactive communication protocol
implies the rank-one condition but not all rank-one protocols are valid interactive communication
schemes Therefore the above is a relaxation with larger feasible set of protocols but it turns out that
the optimal solution of the above optimization problem is the randomized response which is a valid
non-interactive communication protocol Hence there is no loss in solving the above relaxation
The main challenge in solving this optimization is that it is a rank-constrained tensor optimization
which is notoriously difficult Since the rank constraint is over a k-th order tensor k-dimensional
array with possibly common approaches of convex relaxation from for matrices which
are 2nd order tensors does not apply Further we want to simultaneously apply similar optimizations to all the parties with different functions to be computed
We introduce a novel transformation of the above rank-constrained optimization into a linear program in and The price we pay is in the increased dimensionality the LP has an infinite
dimensional decision variable However combined with the geometric understanding of the the
manifold of rank-1 tensors we can identify the exact optimal solution We show in the next section
that given desired level of privacy there is a single universal protocol that simultaneously
maximizes the accuracy for all parties any functions of interest any accuracy measures
and both worst-case and average case accuracy Together with optimal decision rules performed
at each of the receiving ends this gives the exact optimal multi-party computation scheme
Main Result
We show perhaps surprisingly that the simple randomized response presented in is the optimal
protocol in a very general sense For any desired privacy level and arbitrary function fi for any
accuracy measure and any notion of accuracy either average or worst case we show that the
randomized response is universally optimal The proof of the following theorem can be found in the
supplementary material
Theorem Let the optimal decision rule be defined as in for the average accuracy and for
the worst-case accuracy Then for any any function fi and any accuracy
measure for the randomized response for given with the optimal decision
function achieves the maximum accuracy for the i-th party among all differentially private
interactive protocols and all decision rules For the central observer the randomized response with
the optimal decision rule defined in and achieves the maximum accuracy among all differentially private interactive protocols and all decision rules for any arbitrary function f0 and
any measure of accuracy w0
This is a strong universal optimality result Every party and the central observer can simultaneously
achieve the optimal accuracy using a universal randomized response Each party only needs to know
its own desired level of privacy its own function to be computed and its measure of accuracy Optimal data release and optimal decision making are naturally separated However it is not immediate
at all that a non-interactive scheme such as the randomized response would achieve the maximum
accuracy The fact that interaction does not help is counter-intuitive and might as well be true only
for the binary data scenario we consider in this paper The key technical innovation is the convex
geometry in the proof which does not generalize to larger alphabet case
Once we know interaction does not help we can make an educated guess that the randomized response should dominate over other non-interactive schemes This intuition follows from the dominance of randomized response in the single-party setting that was proved using a powerful operational interpretation of differential privacy first introduced in This intuition can in fact be made
rigorous as we show in Section of our supplemental material with a simple two-party example
However we want to emphasize that our main result for multi-party computation does not follow
from any existing analysis of randomized responses in particular those seemingly similar analyses
in The challenge is in proving that interaction does not help which requires the technological
innovations presented in this paper
Multi-party XOR computation For a given function and a given accuracy measure analyzing
the performance of the optimal protocol provides the exact nature of the privacy-accuracy tradeoff
Consider a scenario where a central observer wants to compute the XOR of all the k-bits each of
which is differentially private In this special case we can apply our main theorem to analyze the
accuracy exactly in a combinatorial form and we provide a proof in Section A.1.
Corollary Consider k-party computation for f0 xk and the accuracy measure
is one if correct and zero if not w0 w0 and w0 w0
For any differentially private protocol and any decision rule the average and worst-case
accuracies are bounded by
Pbk/2c
Pbk/2c
2i
2i
ACCave w0 f0 f0
ACCwc w0 f0 f0
and the equality is achieved by the randomized response and optimal decision rules in and
The optimal decision for both accuracies is simply to output the XOR of the received privatized bits
This is a strict generalization of a similar result in where XOR computation was studied but
only for a two-party setting In the high privacy regime where equivalently this
implies that ACCave The leading term is due to the fact that we are
considering an accuracy measure of a Boolean function The second term of captures the
effect that we are essentially observing the XOR through consecutive binary symmetric channels
with flipping probability Hence the accuracy gets exponentially worse in On the other
hand if those k-parties are allowed to collaborate then they can compute the XOR in advance and
only transmit the privatized version of the XOR achieving accuracy of
This is always better than not collaborating which is the bound in Corollary
Discussion
In this section we discuss a few topics each of which is interesting but non-trivial to solve in any
obvious way Our main result is general and sharp but we want to ask how to push it further
Generalization to multiple bits When each party owns multiple bits it is possible that interactive protocols improve over the randomized response protocol This is discussed with examples in
Section the supplementary material
Approximate differential privacy A common generalization of differential privacy known as the
approximate differential privacy is to allow a small slack of in the privacy condition[14
In the multi-party context a protocol is differentially private for the i-th party if for all
and all x0i x?i and for all subset
x?i
e?i x?i
It is natural to ask if the linear programming approach presented in this paper can be extended to
identify the optimal multi-party protocol under differential privacy The LP formulations
of and heavily rely on the fact that any differentially private protocol can be decomposed
as the combination of the matrix and the Since the differential privacy constraints are
invariant under scaling of one can represent the scale-free pattern of the distribution with
and the scaling with This is no longer true for differential privacy and the analysis
technique does not generalize
Correlated sources When the data are correlated each party observe a noisy version
of the state of the world knowing reveals some information on other parties bits In general
revealing correlated data requires careful coordination between multiple parties The analysis techniques developed in this paper do not generalize to correlated data since the crucial rank-one tensor
structure of is no longer present
Extensions to general utility functions A surprising aspect of the main result is that even though
the worst-case accuracy is a concave function over the protocol the maximum is achieved at an
extremal point of the manifold of rank-1 tensors This suggests that there is a deeper geometric
structure of the problem leading to possible universal optimality of the randomized response for a
broader class of utility functions It is an interesting task to understand the geometric structure of the
problem and to ask what class of utility functions lead to optimality of the randomized response
Acknowledgement
This research is supported in part by NSF CISE award NSF SaTC award NSF CMMI award and NSF ENG award

----------------------------------------------------------------

title: 3890-cur-from-a-sparse-optimization-viewpoint.pdf

CUR from a Sparse Optimization Viewpoint
Jacob Bien
Department of Statistics
Stanford University
Stanford CA
Ya Xu
Department of Statistics
Stanford University
Stanford CA
Michael W. Mahoney
Department of Mathematics
Stanford University
Stanford CA
jbien@stanford.edu
yax.stanford@gmail.com
mmahoney@cs.stanford.edu
Abstract
The CUR decomposition provides an approximation of a matrix that has low
reconstruction error and that is sparse in the sense that the resulting approximation
lies in the span of only a few columns of In this regard it appears to be similar to many sparse PCA methods However CUR takes a randomized algorithmic
approach whereas most sparse PCA methods are framed as convex optimization
problems In this paper we try to understand CUR from a sparse optimization
viewpoint We show that CUR is implicitly optimizing a sparse regression objective and furthermore cannot be directly cast as a sparse PCA method We also
observe that the sparsity attained by CUR possesses an interesting structure which
leads us to formulate a sparse PCA method that achieves a CUR-like sparsity
Introduction
CUR decompositions are a recently-popular class of randomized algorithms that approximate a data
matrix Rn?p by using only a small number of actual columns of CUR decompositions are often described as SVD-like low-rank decompositions that have the additional advantage of
being easily interpretable to domain scientists The motivation to produce a more interpretable lowrank decomposition is also shared by sparse PCA SPCA methods which are optimization-based
procedures that have been of interest recently in statistics and machine learning
Although CUR and SPCA methods start with similar motivations they proceed very differently For
example most CUR methods have been randomized and they take a purely algorithmic approach
By contrast most SPCA methods start with a combinatorial optimization problem and they then
solve a relaxation of this problem Thus far it has not been clear to researchers how the CUR and
SPCA approaches are related It is the purpose of this paper to understand CUR decompositions
from a sparse optimization viewpoint thereby elucidating the connection between CUR decompositions and the SPCA class of sparse optimization methods
To do so we begin by putting forth a combinatorial optimization problem below which
CUR is implicitly approximately optimizing This formulation will highlight two interesting features
of CUR first CUR attains a distinctive pattern of sparsity which has practical implications from
the SPCA viewpoint and second CUR is implicitly optimizing a regression-type objective These
two observations then lead to the three main contributions of this paper first we formulate a
non-randomized optimization-based version of CUR Problem GL EG in Section that is
based on a convex relaxation of the CUR combinatorial optimization problem second we show
that in contrast to the original PCA-based motivation for CUR CUR?s implicit objective cannot
be directly expressed in terms of a PCA-type objective Theorem in Section and third
we propose an SPCA approach Problem GL SPCA in Section that achieves the sparsity
structure of CUR within the PCA framework We also provide a brief empirical evaluation of our
two proposed objectives While our proposed GL EG and GL SPCA methods are promising in
and of themselves our purpose in this paper is not to explore them as alternatives to CUR instead
our goal is to use them to help clarify the connection between CUR and SPCA methods
Jacob Bien and Ya Xu contributed equally
We conclude this introduction with some remarks on notation Given a matrix A we use to
denote its ith row as a row-vector and its ith column Similarly given a set of indices I
AI and AI denote the submatrices of A containing only these I rows and columns respectively
Finally we let Lcol denote the column space of A.
Background
In this section we provide a brief background on CUR and SPCA methods with a particular emphasis on topics to which we will return in subsequent sections Before doing so recall that given
an input matrix Principal Component Analysis PCA seeks the k-dimensional hyperplane with
the lowest reconstruction error That is it computes a orthogonal matrix that minimizes
ERR(W XWW
Writing the SVD of as U?VT the minimizer of is given by Vk the first columns of V. In
the data analysis setting each column of provides a particular linear combination of the columns
of These linear combinations are often thought of as latent factors In many applications interpreting such factors is made much easier if they are comprised of only a small number of actual
columns of which is equivalent to Vk only having a small number of nonzero elements
CUR matrix decompositions
CUR decompositions were proposed by Drineas and Mahoney to provide a low-rank approximation to a data matrix by using only a small number of actual columns and/or rows of Fast
randomized variants deterministic variants Nystr?om-based variants and heuristic
variants have also been considered Observing that the best rank-k approximation to the SVD
provides the best set of linear combinations of all the columns one can ask for the best set of
actual columns Most formalizations of best lead to intractable combinatorial optimization problems but one can take advantage of oversampling choosing slightly more than columns and
randomness as computational resources to obtain strong quality-of-approximation guarantees
Theorem Relative-error CUR Given an arbitrary matrix Rn?p and an integer
there exists a randomized algorithm that chooses a random subset I of size
O(k log such that XI the submatrix containing those columns of satisfies
XI XI min
XI Xk
c?p
B?R
with probability at least where Xk is the best rank approximation to
The algorithm referred to by Theorem is very simple
Compute the normalized statistical leverage scores defined below in
Form I by randomly sampling columns of using these normalized statistical leverage scores
as an importance sampling distribution
Return the matrix XI consisting of these selected columns
The key issue here is the choice of the importance sampling distribution Let the matrix Vk
be the top-k right singular vectors of Then the normalized statistical leverage scores are
for all where Vk(i denotes the i-th row of Vk These scores proportional to the
Euclidean norms of the rows of the top-k right singular vectors define the relevant nonuniformity
structure to be used to identify good the sense of Theorem columns In addition these scores
are proportional to the diagonal elements of the projection matrix onto the top-k right singular
subspace Thus they generalize the so-called hat matrix and they have a natural interpretation
as capturing the statistical leverage or influence of a given column on the best low-rank fit of
the data matrix
Regularized sparse PCA methods
SPCA methods attempt to make PCA easier to interpret for domain experts by finding sparse approximations to the columns of There are several variants of SPCA For example Jolliffe
For SPCA we only consider sparsity in the right singular vectors and not in the left singular vectors U.
This is similar to considering only the choice of columns and not of both columns and rows in CUR.
and Witten use the maximum variance interpretation of PCA and provide an optimization
problem which explicitly encourages sparsity in based on a Lasso constraint d?Aspremont
take a similar approach but instead formulate the problem as an SDP.
Zou use the minimum reconstruction error interpretation of PCA to suggest a different
approach to the SPCA problem this formulation will be most relevant to our present purpose They
begin by formulating PCA as the solution to a regression-type problem
Theorem Zou Given an arbitrary matrix Rn?p and an integer let A and
be matrices Then for any let
Vk argminA,W?Rp?k XWAT AT A Ik
Then the minimizing matrices A and Vk satisfy si and Vk
si or
si where
ii
That is up to signs A consists of the top-k right singular vectors of and Vk consists of
those same vectors shrunk by a factor depending on the corresponding singular value Given this
regression-type characterization of PCA Zou then sparsify the formulation by adding
an L1 penalty on
Vk argminA,W?Rp?k XWAT AT A Ik
where
ij Wij This regularization tends to sparsify element-wise so that the
solution Vk gives a sparse approximation of Vk
Expressing CUR as an optimization problem
In this section we present an optimization formulation of CUR. Recall from Section that CUR
takes a purely algorithmic approach to the problem of approximating a matrix in terms of a small
number of its columns That is it achieves sparsity indirectly by randomly selecting columns and
it does so in such a way that the reconstruction error is small with high probability Theorem By
contrast SPCA methods are generally formulated as the exact solution to an optimization problem
From Theorem it is clear that CUR seeks a subset I of size for which minB?Rc?p
is small In this sense CUR can be viewed as a randomized algorithm for approximately solving the
following combinatorial optimization problem
min
min XI
B?Rc?p
In words this objective asks for the subset of columns of which best describes the entire matrix
Notice that relaxing to does not affect the optimum This optimization problem
is analogous to all-subsets multivariate regression which is known to be NP-hard
However by using ideas from the optimization literature we can approximate this combinatorial
problem as a regularized regression problem that is convex First notice that is equivalent to
min XB||F
B?Rp?p
where we now optimize over a matrix B. To see the equivalence between and note that
the constraint in is the same as finding some subset I with such that BI
The formulation in provides a natural entry point to proposing a convex optimization approach
corresponding to CUR. First notice that uses an L0 norm on the rows of which is not convex
However we can approximate the L0 constraint by a group lasso penalty which uses a well-known
convex heuristic proposed by Yuan that encourages prespecified groups of parameters
to be simultaneously sparse Thus the combinatorial problem in can be approximated by the
following convex and thus tractable problem
Problem Group lasso regression GL EG). Given an arbitrary matrix Rn?p let
Rp?p and The GL EG problem is to solve
argminB XB||F
where is chosen to get nonzero rows in
Pp
Since the rows of are grouped together in the penalty the row vector will tend
to be either dense or entirely zero Note also that the algorithm to solve Problem is a special case
of Algorithm below which solves the GL SPCA problem to be introduced later Finally
as a side remark note that our proposed GL EG is strikingly similar to a recently proposed method
for sparse inverse covariance estimation
Distinguishing CUR from SPCA
Our original intention in casting CUR in the optimization framework was to understand better
whether CUR could be seen as an SPCA-type method So far we have established CUR?s connection to regression by showing that CUR can be thought of as an approximation algorithm for the
sparse regression problem In this section we discuss the relationship between regression and
PCA and we show that CUR cannot be directly cast as an SPCA method
To do this recall that regression in particular self regression finds a Rp?p that minimizes
XB||F
On the other hand PCA-type methods find a set of directions that minimize
ERR(W
XWW
Here unlike in we do not assume that is orthogonal since the minimizer produced from
SPCA methods is often not required to be orthogonal recall Section
Clearly with no constraints on or we can trivially achieve zero reconstruction error in both
cases by taking Ip and any full-rank matrix However with additional constraints
these two problems can be very different It is common to consider sparsity and/or rank constraints
We have seen in Section that CUR effectively requires to be row-sparse in the standard PCA
setting is taken to be rank with in which case is minimized by Vk and obtains
the optimal value ERR(Vk Xk finally for SPCA is further required to be sparse
To illustrate the difference between the reconstruction errors and when extra constraints
are imposed consider the 2-dimensional toy example in Figure In this example we compare
regression with a row-sparsity constraint to PCA with both rank and sparsity constraints With
we plot against as the solid points in both plots of Figure Constraining
giving row-sparsity as with CUR methods becomes minB12
which is a simple linear regression represented by the black thick line and minimizing the sum
of squared vertical errors as shown The red line left plot shows the first principal component
direction which minimizes ERR(W among all rank-one matrices W. Here ERR(W is the sum
of squared projection distances red dotted lines Finally if is further required to be sparse in
the direction as with SPCA methods we get the rank-one sparse projection represented by
the green line in Figure right The two sets of dotted lines in each plot clearly differ indicating
that their corresponding reconstruction errors are differ ent as well Since we have shown that CUR
is minimizing a regression-based objective this toy example suggests that CUR may not in fact be
optimizing a PCA-type objective such as Next we will make this intuition more precise
The first step to showing that CUR is an SPCA method would be to produce a matrix VCUR for
which XI XI XVCUR
CUR to express CUR?s approximation in the form of an SPCA
I
approximation However this equality implies Lcol XVCUR
CUR Lcol meaning that
I
VCUR I If such a VCUR existed then clearly ERR(VCUR XI and so
CUR could be regarded as implicitly performing sparse PCA in the sense that VCUR is sparse
and by Theorem with high probability ERR(VCUR ERR(Vk Thus the existence
of such a VCUR would cast CUR directly as a randomized approximation algorithm for SPCA However the following theorem states that unless an unrealistic constraint on holds there does not
exist a matrix VCUR for which ERR(VCUR XI XI The larger implication of this
theorem is that CUR cannot be directly viewed as an SPCA-type method
Theorem Let I be an index set and suppose Rp?p satisfies WI Then
XWW XI XI
unless Lcol XI Lcol XI in which case holds
Regression
Regression
error
error
error
error
SPCA
PCA
Figure Example of the difference in reconstruction errors and when additional constraints
imposed Left regression with row-sparsity constraint black compared with PCA with low rank
constraint Right regression with row-sparsity constraint black compared with PCA with
low rank and sparsity constraint green In both plots the corresponding errors are represented by
the dotted lines
Proof
XWW XI WI XI WI WIT WI WT
XI WI WI
XI XI XI XI XI
XI XI XI XI XI XI
The last inequality is strict unless XI XI XI
CUR-type sparsity and the group lasso SPCA
Although CUR cannot be directly cast as an SPCA-type method in this section we propose a sparse
PCA approach which we call the group lasso SPCA or GL SPCA that accomplishes something
very close to CUR. Our proposal produces a that has rows that are entirely zero and it is motivated by the following two observations about CUR. First following from the definition of the
leverage scores CUR chooses columns of based on the norm of their corresponding rows of
Vk Thus it essentially zeros-out the rows of Vk with small norms a probabilistic sense
Second as we have noted in Section if CUR could be expressed as a PCA method its principal
directions matrix VCUR would have rows that are entirely zero corresponding to removing
those columns of
Recall that Zou obtain a sparse by including in an additional L1 penalty from
the optimization problem Since the L1 penalty is on the entire matrix viewed as a vector
it encourages only unstructured sparsity To achieve the CUR-type row sparsity we propose the
following modification of
Problem Group lasso SPCA GL SPCA Given an arbitrary matrix Rn?p and an integer
let A and be matrices and let The GL SPCA problem is to solve
A argminA,W XWA
AT A Ik
Thus
the lasso penalty in is replaced in by a group lasso penalty
where rows of are grouped together so that each row of will tend to
be either dense or entirely zero
Importantly the GL SPCA problem is not convex in and A together it is however convex in
and it is easy to solve in A. Thus analogous to the treatment in Zou we propose
an iterative alternate-minimization algorithm to solve GL SPCA This is described in Algorithm
and the justification of this algorithm is given in Section Note that if we fix A to be I throughout
then Algorithm can be used to solve the GL EG problem discussed in Section
Algorithm Iterative algorithm for solving the GL SPCA and GL EG problems
For the GL EG problem fix A I throughout this algorithm
Input Data matrix and initial estimates for A and
Output Final estimates for A and
repeat
Compute SVD of XT XW as UDVT and then A UVT
for do
Compute bi X(j)T
if bi then
else
AT XT bi
until convergence
We remark that such row-sparsity in can have either advantages or disadvantages Consider for
example when there are a small number of informative columns in and the rest are not important
for the task at hand In such a case we would expect that enforcing entire rows to be zero
would lead to better identification of the signal columns and this has been empirically observed in
the application of CUR to DNA SNP analysis The unstructured by contrast would not
be able to borrow strength across all columns of to differentiate the signal columns from the
noise columns On the other hand requiring such structured sparsity is more restrictive and may
not be desirable For example in microarray analysis in which we have measured genes on
patients our goal may be to find several underlying factors Biologists have identified pathways
of interconnected genes and it would be desirable if each sparse factor could be identified with
a different pathway that is a different set of genes Requiring all factors of to exclude the same
genes does not allow a different sparse subset of genes to be active in each factor
We finish this section by pointing out that while most SPCA methods only enforce unstructured
zeros in the idea of having a structured sparsity in the PCA context has very recently been
explored Our GL SPCA problem falls within the broad framework of this idea
Empirical Comparisons
In this section we evaluate the performance of the four methods discussed above on both synthetic and real data In particular we compare the randomized CUR algorithm of Mahoney and
Drineas to our GL EG of Problem and we compare the SPCA algorithm proposed
by Zou to our GL SPCA of Problem We have also compared against the SPCA
algorithm of Witten and we found the results to be very similar to those of Zou
Simulations
where
is the underlying signal
We first consider synthetic examples of the form
matrix and is a matrix of noise In all our simulations has entries while the
has one of the following forms
signal
where the matrix
is the nonzero part of
In other words
Case I
has nonzero columns and does not necessarily have a low-rank structure
UVT where and each consist of orthogonal columns In addition to
Case
being low-rank has entire rows equal to zero it is row-sparse
UVT where and each consist of orthogonal columns Here is
Case III
low-rank and sparse but the sparsity is not structured it is scattered-sparse
and has high precision in
A successful method attains low reconstruction error of the true signal
identifying correctly the zeros in the underlying model As previously discussed the four methods
optimize for different types of reconstruction error Thus in comparing CUR and GL EG we
XI XI whereas for the
use the regression-type reconstruction error ERRreg
XVV
comparison of SPCA and GL SPCA we use the PCA-type error ERR(V
Table presents the simulation results from the three cases All comparisons use and
In Case and III the signal matrix has rank The underlying sparsity level is
Case I and Case II&III are zeros Note that all methods
of the entries of
except for GL EG require the rank as an input and we always take it to be even in Case I. For
easy comparison we have tuned each method to have the correct total number of zeros The results
are averaged over trials
Methods
Case I
Case
Case III
ERR reg
CUR
GL EG
ERR
SPCA
GL SPCA
Table Simulation results The reconstruction errors and the percentages of correctly identified
zeros parentheses
We notice in Table that the two regression-type methods CUR and GL EG have very similar
performance As we would expect since CUR only uses information in the top singular vectors it
does slightly worse than GL EG in terms of precision when the underlying signal is not low-rank
Case I). In addition both methods perform poorly if the sparsity is not structured as in Case III. The
two PCA-type methods perform similarly as well Again the group lasso method seems to work
better in Case I. We note that the precisions reported here are based on element-wise sparsity?if we
were measuring row-sparsity methods like SPCA would perform poorly since they do not encourage
entire rows to be zero
Microarray example
We next consider a microarray dataset of soft tissue tumors studied by Nielsen Mahoney and Drineas apply CUR to this dataset of 31 tissue samples and genes
As with the simulation results we use two sets of comparisons we compare CUR with GL EG
we take
and we compare SPCA with GL SPCA Since we do not observe the underlying truth
ERRreg XI XI and ERR(V XVV Also since we do not observe
the true sparsity we cannot measure the precision as we do in Table The left plot in Figure
shows ERRreg as a function of We see that CUR and GL EG perform similarly However
since CUR is a randomized algorithm on every run it gives a different result From a practical
standpoint this feature of CUR can be disconcerting to biologists wanting to report a single set of
important genes In this light GL EG may be thought of as an attractive non-randomized alternative to CUR The right plot of Figure compares GL SPCA to SPCA specifically Zou
Since SPCA does not explicitly enforce row-sparsity for a gene to be not used in the model requires
all of the columns of to exclude it This likely explains the advantage of GL SPCA over
SPCA seen in the figure
Justification of Algorithm
The algorithm alternates between minimizing with respect to A and until convergence
Solving for A given If is fixed then the regularization penalty in can be ignored in
which case the optimization problem becomes minA XBAT subject to AT A I. This
problem was considered by Zou who showed that the solution is obtained by computing
UVT This explains step in
the SVD of XT X)B as XT X)B UDVT and then setting A
Algorithm
Solving for given A If A is fixed then becomes an unconstrained convex optimization
problem in B. The subgradient equations using that AT A Ik are
2BT XT 2AT XT si
Microarray Dataset
GL SPCA
SPCA
ERR
GL EG
CUR
ERR reg
Microarray Dataset
Number of genes used
Figure Left Comparison of CUR multiple runs with
SPCA with SPCA specifically Zou
Number of genes used
GL EG
Right Comparison of
GL
where the subgradient vectors si BT(i if or if Let us
define bi BT XT BT(i so that the subgradient equations
can be written as
bi AT XT
The following claim explains Step in Algorithm
Claim if and only if XT bi
Proof First if the subgradient equations become bi AT XT
Since if we have XT bi To prove the other
direction recall that implies si BT(i Substituting this expression into
rearranging terms and taking
the norm on both sides we get XT bi
By Claim XT bi implies that which further implies si
BT(i Substituting into gives Step in Algorithm
Conclusion
In this paper we have elucidated several connections between two recently-popular matrix decomposition methods that adopt very different perspectives on obtaining interpretable low-rank matrix
decompositions In doing so we have suggested two optimization problems GL EG and GL SPCA that highlight similarities and differences between the two methods In general SPCA
methods obtain interpretability by modifying an existing intractable objective with a convex regularization term that encourages sparsity and then exactly optimizing that modified objective On
the other hand CUR methods operate by using randomness and approximation as computational resources to optimize approximately an intractable objective thereby implicitly incorporating a form
of regularization into the steps of the approximation algorithm Understanding this concept of implicit regularization via approximate computation is clearly of interest more generally in particular
for applications where the size scale of the data is expected to increase
Acknowledgments
We would like to thank Art Owen and Robert Tibshirani for encouragement and helpful suggestions
Jacob Bien was supported by the Urbanek Family Stanford Graduate Fellowship and Ya Xu was
supported by the Melvin and Joan Lane Stanford Graduate Fellowship In addition support from
the NSF and AFOSR is gratefully acknowledged

----------------------------------------------------------------

title: 4928-understanding-variable-importances-in-forests-of-randomized-trees.pdf

Understanding variable importances
in forests of randomized trees
Gilles Louppe Louis Wehenkel Antonio Sutera and Pierre Geurts
Dept of EE CS University of Li`ege Belgium
g.louppe l.wehenkel a.sutera p.geurts}@ulg.ac.be
Abstract
Despite growing interest and practical use in various scientific areas variable importances derived from tree-based ensemble methods are not well understood from
a theoretical point of view In this work we characterize the Mean Decrease Impurity MDI variable importances as measured by an ensemble of totally randomized trees in asymptotic sample and ensemble size conditions We derive a
three-level decomposition of the information jointly provided by all input variables about the output in terms of the MDI importance of each input variable ii
the degree of interaction of a given input variable with the other input variables
iii the different interaction terms of a given degree We then show that this MDI
importance of a variable is equal to zero if and only if the variable is irrelevant
and that the MDI importance of a relevant variable is invariant with respect to
the removal or the addition of irrelevant variables We illustrate these properties
on a simple example and discuss how they may change in the case of non-totally
randomized trees such as Random Forests and Extra-Trees
Motivation
An important task in many scientific fields is the prediction of a response variable based on a set
of predictor variables In many situations though the aim is not only to make the most accurate
predictions of the response but also to identify which predictor variables are the most important
to make these predictions in order to understand the underlying process Because of their
applicability to a wide range of problems and their capability to both build accurate models and
at the same time to provide variable importance measures Random Forests Breiman and
variants such as Extra-Trees Geurts have become a major data analysis tool used with
success in various scientific areas
Despite their extensive use in applied research only a couple of works have studied the theoretical
properties and statistical mechanisms of these algorithms Zhao Breiman Biau
Biau Meinshausen and Lin and Jeon investigated simplified to very
realistic variants of these algorithms and proved the consistency of those variants Little is known
however regarding the variable importances computed by Random Forests like algorithms and
as far as we know the work of Ishwaran is indeed the only theoretical study of tree-based
variable importance measures
In this work we aim at filling this gap and present a theoretical analysis of the Mean Decrease
Impurity importance derived from ensembles of randomized trees The rest of the paper is organized
as follows in section we provide the background about ensembles of randomized trees and recall
how variable importances can be derived from them in section we then derive a characterization
in asymptotic conditions and show how variable importances derived from totally randomized trees
offer a three-level decomposition of the information jointly contained in the input variables about the
output section shows that this characterization only depends on the relevant variables and section
discusses theses ideas in the context of variants closer to the Random Forest algorithm section
then illustrates all these ideas on an artificial problem finally section includes our conclusions
and proposes directions of future works
Background
In this section we first describe decision trees as well as forests of randomized trees Then we
describe the two major variable importances measures derived from them including the Mean
Decrease Impurity MDI importance that we will study in the subsequent sections
Single classification and regression trees and random forests
A binary classification resp regression tree Breiman is an input-output model
represented by a tree structure from a random input vector Xp taking its values in
X1 Xp to a random output variable Any node in the tree represents a subset
of the space with the root node being itself Internal nodes are labeled with a binary test
split st Xm dividing their subset in two subsets1 corresponding to their two children
tL and tR while the terminal nodes leaves are labeled with a best guess value of the output
variable2 The predicted output for a new instance is the label of the leaf reached by the instance
when it is propagated through the tree A tree is built from a learning sample of size drawn from
Xp using a recursive procedure which identifies at each node the split st for
which the partition of the Nt node samples into tL and tR maximizes the decrease
pL i(tL pR i(tR
of some impurity measure the Gini index the Shannon entropy or the variance of
and where pL NtL Nt and pR NtR Nt The construction of the tree stops when nodes
become pure in terms of or when all variables are locally constant
Single trees typically suffer from high variance which makes them not competitive in terms of
accuracy A very efficient and simple way to address this flaw is to use them in the context of
randomization-based ensemble methods Specifically the core principle is to introduce random
perturbations into the learning procedure in order to produce several different decision trees from
a single learning set and to use some aggregation technique to combine the predictions of all these
trees In Bagging Breiman trees are built on random bootstrap copies of the original data
hence producing different decision trees In Random Forests Breiman Bagging is extended
and combined with a randomization of the input variables that are used when considering candidate
variables to split internal nodes In particular instead of looking for the best split among all
variables the Random Forest algorithm selects at each node a random subset of variables and
then determines the best split over these latter variables only
Variable importances
In the context of ensembles of randomized trees Breiman proposed to evaluate the
importance of a variable Xm for predicting by adding up the weighted impurity decreases
p(t)?i(st for all nodes where Xm is used averaged over all NT trees in the forest
Imp(Xm
p(t)?i(st
NT
t?T v(st
and where is the proportion Nt of samples reaching and v(st is the variable used in split
st When using the Gini index as impurity function this measure is known as the Gini importance or
Mean Decrease Gini However since it can be defined for any impurity measure we will refer
to Equation as the Mean Decrease Impurity importance no matter the impurity measure
We will characterize and derive results for this measure in the rest of this text
More generally splits are defined by a not necessarily binary partition of the range Xm of possible values
of a single variable Xm
determined as the majority class resp the average value within the subset of the leaf
In addition to MDI Breiman also proposed to evaluate the importance of a variable
Xm by measuring the Mean Decrease Accuracy MDA of the forest when the values of Xm are
randomly permuted in the out-of-bag samples For that reason this latter measure is also known as
the permutation importance
Thanks to popular machine learning softwares Breiman Liaw and Wiener Pedregosa
both of these variable importance measures have shown their practical utility in an
increasing number of experimental studies Little is known however regarding their inner workings
Strobl compare both MDI and MDA and show experimentally that the former is biased
towards some predictor variables As explained by White and Liu in case of single decision
trees this bias stems from an unfair advantage given by the usual impurity functions towards
predictors with a large number of values Strobl later showed that MDA is biased as
well and that it overestimates the importance of correlated variables although this effect was not
confirmed in a later experimental study by Genuer From a theoretical point of view
Ishwaran provides a detailed theoretical development of a simplified version of MDA giving
key insights for the understanding of the actual MDA.
Variable importances derived from totally randomized tree ensembles
Let us now consider the MDI importance as defined by Equation and let us assume a set
Xp of categorical input variables and a categorical output For the sake of simplicity
we will use the Shannon entropy as impurity measure and focus on totally randomized trees later
on we will discuss other impurity measures and tree construction algorithms
Given a training sample of joint observations of X1 Xp independently drawn from the
joint distribution Xp let us assume that we infer from it an infinitely large ensemble
of totally randomized and fully developed trees In this setting a totally randomized and fully
developed tree is defined as a decision tree in which each node is partitioned using a variable
picked uniformly at random among those not yet used at the parent nodes of and where each is
split into sub-trees one for each possible value of and where the recursive construction
process halts only when all variables have been used along the current branch Hence in such a
tree leaves are all at the same depth and the set of leaves of a fully developed tree is in bijection
with the set of all possible joint configurations of the input variables For example if the input
variables are all binary the resulting tree will have exactly 2p leaves
Theorem The MDI importance of Xm for as computed with an infinite ensemble of fully
developed totally randomized trees and an infinitely large training sample is
Imp(Xm
Cpk
I(Xm
B?Pk
where denotes the subset Xm Pk is the set of subsets of of cardinality
and I(Xm is the conditional mutual information of Xm and given the variables in B.
Proof See Appendix B.
Theorem For any ensemble of fully developed trees in asymptotic learning sample size conditions
in the same conditions as those of Theorem we have that
Imp(Xm Xp
Proof See Appendix C.
Together theorems and show that variable importances derived from totally randomized trees
in asymptotic conditions provide a three-level decomposition of the information Xp
contained in the set of input variables about the output variable The first level is a decomposition
among input variables Equation of Theorem the second level is a decomposition along the
degrees of interaction terms of a variable with the other ones the outer sum in Equation of
Theorem and the third level is a decomposition along the combinations of interaction terms of
fixed size of possible interacting variables the inner sum in Equation
We observe that the decomposition includes for each variable each and every interaction term
of each and every degree weighted in a fashion resulting only from the combinatorics of possible
interaction terms In particular since all I(Xm terms are at most equal to H(Y the prior
entropy of the terms of the outer sum of Equation are each upper bounded by
H(Y
H(Y H(Y
Cp
B?Pk
As such the second level decomposition
resulting from totally randomized trees makes the subP
importance terms C1k p?k
I(Xm to equally contribute at most to the total
B?P
importance even though they each include a combinatorially different number of terms
Importances of relevant and irrelevant variables
Following Kohavi and John let us define as relevant to with respect to a variable Xm for
which there exists at least one subset possibly empty such that I(Xm Thus we
define as irrelevant to with respect to a variable for which for all I(Xi
Remark that if is irrelevant to with respect to then by definition it is also irrelevant to
with respect to any subset of
Theorem is irrelevant to with respect to if and only if its infinite sample size
importance as computed with an infinite ensemble of fully developed totally randomized trees built
on for is
Proof See Appendix D.
Lemma Let
be an irrelevant variable for with respect to The infinite sample size
importance of Xm as computed with an infinite ensemble of fully developed totally randomized
trees built on for is the same as the importance derived when using to build the
ensemble of trees for
Proof See Appendix E.
Theorem Let VR be the subset of all variables in that are relevant to with respect
to The infinite sample size importance of any variable Xm VR as computed with an infinite
ensemble of fully developed totally randomized trees built on VR for is the same as its importance
computed in the same conditions by using all variables in That is
Imp(Xm
Cpk
Crl
I(Xm
B?Pk
I(Xm
B?Pl VR?m
where is the number of relevant variables in VR
Proof See Appendix F.
Theorems and show that the importances computed with an ensemble of totally randomized
trees depends only on the relevant variables Irrelevant variables have a zero importance and do not
affect the importance of relevant variables Practically we believe that such properties are desirable
conditions for a sound criterion assessing the importance of a variable Indeed noise should not be
credited of any importance and should not make any other variable more less important
Among the relevant variables we have the marginally relevant ones for which I(Xm the strongly
relevant ones for which I(Xm and the weakly relevant variables which are relevant but not
strongly relevant
Random Forest variants
In this section we consider and discuss variable importances as computed with other types of ensembles of randomized trees We first show how our results extend to any other impurity measure
and then analyze importances computed by depth-pruned ensemble of randomized trees and those
computed by randomized trees built on random subspaces of fixed size Finally we discuss the case
of non-totally randomized trees
Generalization to other impurity measures
Although our characterization in sections and uses Shannon entropy as the impurity measure
we show in Appendix I that theorems and hold for other impurity measures simply substituting conditional mutual information for conditional impurity reduction in the different formulas
and in the definition of irrelevant variables In particular our results thus hold for the Gini index in
classification and can be extended to regression problems using variance as the impurity measure
Pruning and random subspaces
In sections and we considered totally randomized trees that were fully developed until all
variables were used within each branch When totally randomized trees are developed only up to
some smaller depth we show in Proposition that the variable importances as computed by
these trees is limited to the first terms of Equation We then show in Proposition that these
latter importances are actually the same as when each tree of the ensemble is fully developed over a
random subspace of variables drawn prior to its construction
Proposition The importance of Xm for as computed with an infinite ensemble of pruned
totally randomized trees built up to depth and an infinitely large training sample is
Imp(Xm
Cp
I(Xm
B?Pk
Proof See Appendix G.
Proposition The importance of Xm for as computed with an infinite ensemble of pruned
totally randomized trees built up to depth and an infinitely large training sample is identical
to the importance as computed for with an infinite ensemble of fully developed totally randomized
trees built on random subspaces of variables drawn from
Proof See Appendix H.
As long as where denotes the number of relevant variables in it can easily be shown
that all relevant variables will still obtain a strictly positive importance which will however differ
in general from the importances computed by fully grown totally randomized trees built over all
variables Also each irrelevant variable of course keeps an importance equal to zero which means
that in asymptotic conditions these pruning and random subspace methods would still allow us
identify the relevant variables as long as we have a good upper bound on
Non-totally randomized trees
In our analysis in the previous sections trees are built totally at random and hence do not directly
relate to those built in Random Forests Breiman or in Extra-Trees Geurts To
better understand the importances as computed by those algorithms let us consider a close variant
of totally randomized trees at each node let us instead draw uniformly at random
variables and let us choose the one that maximizes Notice that for this procedure
amounts to building ensembles of totally randomized trees as defined before while for it
amounts to building classical single trees in a deterministic way
First the importance of Xm as computed with an infinite ensemble of such randomized trees
is not the same as Equation For masking effects indeed appear at some variables are
never selected because there always is some other variable for which is larger Such effects
tend to pull the best variables at the top of the trees and to push the others at the leaves As a result
the importance of a variable no longer decomposes into a sum including all I(Xm terms
The importance of the best variables decomposes into a sum of their mutual information alone or
conditioned only with the best others but not conditioned with all variables since they no longer
ever appear at the bottom of trees By contrast the importance of the least promising variables
now decomposes into a sum of their mutual information conditioned only with all variables but
not alone or conditioned with a couple of others since they no longer ever appear at the top of
trees In other words because of the guided structure of the trees the importance of Xm now takes
into account only some of the conditioning sets which may over or underestimate its overall
relevance
To make things clearer let us consider a simple example Let X1 perfectly explains and let X2 be
a slightly noisy copy of X1 and
Using totally randomized trees the importances of X1 and X2 are nearly equal the importance of
X1 being slightly higher than the importance of X2
Imp(X1
Imp(X2
In non-totally randomized trees for X1 is always selected at the root node and X2 is
always used in its children Also since X1 perfectly explains all its children are pure and the
reduction of entropy when splitting on X2 is null As a result ImpK=2 and
ImpK=2 Masking effects are here clearly visible the true importance
of X2 is masked by X1 as if X2 were irrelevant while it is only a bit less informative than X1
As a direct consequence of the example above for it is no longer true that a variable is
irrelevant if and only if its importance is zero In the same way it can also be shown that the
importances become dependent on the number of irrelevant variables Let us indeed consider the
following counter-example let us add in the previous example an irrelevant variable with respect
to X2 and let us keep The probability of selecting X2 at the root node now becomes
positive which means that ImpK=2 now includes and is therefore strictly larger
than the importance computed before For fixed adding irrelevant variables dampens masking
effects which thereby makes importances indirectly dependent on the number of irrelevant variables
In conclusion the importances as computed with totally randomized trees exhibit properties that do
not possess by extension neither random forests nor extra-trees With totally randomized trees the
importance of Xm only depends on the relevant variables and is if and only if Xm is irrelevant
As we have shown it may no longer be the case for Asymptotically the use of totally
randomized trees for assessing the importance of a variable may therefore be more appropriate In
a finite setting a limited number of samples and a limited number of trees guiding the choice
of the splitting variables remains however a sound strategy In such a case I(Xm cannot be
measured neither for all Xm nor for all B. It is therefore pragmatic to promote those that look the
most promising even if the resulting importances may be biased
Illustration on a digit recognition problem
In this section we consider the digit recognition problem of Breiman for illustrating
variable importances as computed with totally randomized trees We verify that they match with our
theoretical developments and that they decompose as foretold We also compare these importances
with those computed by an ensemble of non-totally randomized trees as discussed in section
for increasing values of K.
Let us consider a seven-segment indicator displaying numerals using horizontal and vertical lights
in on-off combinations as illustrated in Figure Let be a random variable taking its value in
with equal probability and let X1 X7 be binary variables whose values are each
determined univocally given the corresponding value of in Table
Since Table perfectly defines the data distribution and given the small dimensionality of the problem it is practicable to directly apply Equation to compute variable importances To verify our
X1
X2
X3
X4
X5
X6
X7
Eqn.
x3
x4
x5
x6
x7
Table Values of X1 X7
Figure 7-segment display
X1
X2
X3
X4
X5
X6
P7
Table Variable importances as computed with an ensemble of randomized trees for increasing values of K.
Importances at follow their theoretical values as predicted by Equation in Theorem However as
increases importances diverge due to masking effects In accordance with Theorem their sum is also always
equal to X7 H(Y log2 since inputs allow to perfectly predict the output
theoretical developments we then compare in Table variable importances as computed by Equation and those yielded by an ensemble of totally randomized trees Note that
given the known structure of the problem building trees on a sample of finite size that perfectly
follows the data distribution amounts to building them on a sample of infinite size At best trees
can thus be built on a 10-sample dataset containing exactly one sample for each of the equiprobable
outcomes of As the table illustrates the importances yielded by totally randomized trees match
those computed by Equation which confirms Theorem Small differences stem from the fact
that a finite number of trees were built in our simulations but those discrepancies should disappear
as the size of the ensemble grows towards infinity It also shows that importances indeed add up to
which confirms Theorem Regarding the actual importances they indicate that
X5 is stronger than all others followed in that order by X2 X4 and X3 which also show large
importances X1 X7 and X6 appear to be the less informative The table also reports importances
for increasing values of K. As discussed before we see that a large value of yields importances
that can be either overestimated at the importances of X2 and X5 are larger than at
or underestimated due to masking effects at the importances of X1 X3 X4
and X6 are smaller than at as if they were less important It can also be observed that
masking effects may even induce changes in the variable rankings compare the rankings at
and at which thus confirms that importances are differently affected
To better understand why a variable is important it is also insightful to look at its decomposition into
its sub-importances terms as shown in Figure Each row in the plots of the figure corresponds
to one the variables and each column to a size of conditioning sets As such the value at
row and column corresponds
the importance of Xm when conditioned with other variables
to the term C1k p?k
I(Xm in Equation in the case of totally randomized
B?P
trees In the left plot for the figure first illustrates how importances yielded by totally
randomized trees decomposes along the degrees of interactions terms We can observe that they
each equally contribute at most the total importance of a variable The plot also illustrates why
X5 is important it is informative either alone or conditioned with any combination of the other
variables all of its terms are significantly larger than By contrast it also clearly shows why
X1
X1
X2
X2
X3
X3
X4
X4
X5
X5
X6
X6
X7
X7
Figure Decomposition of variable importances along the degrees of interactions of one variable with the
other ones At all I(Xm are accounted for in the total importance while at only some
of them are taken into account due to masking effects
X6 is not important neither alone nor combined with others X6 seems to be very informative
all of its terms are close to More interestingly this figure also highlights redundancies X7
is informative alone or conditioned with a couple of others the first terms are significantly larger
than but becomes uninformative when conditioned with many others the last terms are closer
to The right plot for illustrates the decomposition of importances when variables are
chosen in a deterministic way The first thing to notice is masking effects Some of the I(Xm
terms are indeed clearly never encountered and their contribution is therefore reduced to in the
total importance For instance for the sub-importances of X2 and X5 are positive while
all others are null which means that only those two variables are ever selected at the root node
hence masking the others As a consequence this also means that the importances of the remaining
variables is biased and that it actually only accounts of their relevance when conditioned to X2
or X5 but not of their relevance in other contexts At masking effects also amplify the
contribution of resp since X2 resp X5 appears more frequently at the root
node than in totally randomized trees In addition because nodes become pure before reaching
depth conditioning sets of size are never actually encountered which means that we can no
longer know whether variables are still informative when conditioned to many others All in all this
figure thus indeed confirms that importances as computed with non-totally randomized trees take
into account only some of the conditioning sets hence biasing the measured importances
Conclusions
In this work we made a first step towards understanding variable importances as computed with
a forest of randomized trees In particular we derived a theoretical characterization of the Mean
Decrease Impurity importances as computed by totally randomized trees in asymptotic conditions
We showed that they offer a three-level decomposition of the information jointly provided by all
input variables about the output Section We then demonstrated Section that MDI importances
as computed by totally randomized trees exhibit desirable properties for assessing the relevance of
a variable it is equal to zero if and only if the variable is irrelevant and it depends only on the
relevant variables We discussed the case of Random Forests and Extra-Trees Section and finally
illustrated our developments on an artificial but insightful example Section
There remain several limitations to our framework that we would like address in the future First our
results should be adapted to binary splits as used within an actual Random Forest-like algorithm In
this setting any node is split in only two subsets which means that any variable may then appear
one or several times within a branch and thus should make variable importances now dependent on
the cardinalities of the input variables In the same direction our framework should also be extended
to the case of continuous variables Finally results presented in this work are valid in an asymptotic
setting only An important direction of future work includes the characterization of the distribution
of variable importances in a finite setting
Acknowledgements Gilles Louppe is a research fellow of the FNRS Belgium and acknowledges its financial
support This work is supported by PASCAL2 and the IUAP DYSCO initiated by the Belgian State Science
Policy Office

----------------------------------------------------------------

title: 6555-sdp-relaxation-with-randomized-rounding-for-energy-disaggregation.pdf

SDP Relaxation with Randomized Rounding for
Energy Disaggregation
Kiarash Shaloudegi
Imperial College London
k.shaloudegi16@imperial.ac.uk
Csaba Szepesv?ri
University of Alberta
szepesva@ualberta.ca
Andr?s Gy?rgy
Imperial College London
a.gyorgy@imperial.ac.uk
Wilsun Xu
University of Alberta
wxu@ualberta.ca
Abstract
We develop a scalable computationally efficient method for the task of energy
disaggregation for home appliance monitoring In this problem the goal is to
estimate the energy consumption of each appliance over time based on the total
energy-consumption signal of a household The current state of the art is to model
the problem as inference in factorial HMMs and use quadratic programming to
find an approximate solution to the resulting quadratic integer program Here we
take a more principled approach better suited to integer programming problems
and find an approximate optimum by combining convex semidefinite relaxations
randomized rounding as well as a scalable ADMM method that exploits the special
structure of the resulting semidefinite program Simulation results both in synthetic
and real-world datasets demonstrate the superiority of our method
Introduction
Energy efficiency is becoming one of the most important issues in our society Identifying the
energy consumption of individual electrical appliances in homes can raise awareness of power
consumption and lead to significant saving in utility bills Detailed feedback about the power
consumption of individual appliances helps energy consumers to identify potential areas for energy
savings and increases their willingness to invest in more efficient products Notifying home owners
of accidentally running stoves ovens etc may not only result in savings but also improves safety
Energy disaggregation or non-intrusive load monitoring NILM uses data from utility smart meters
to separate individual load consumptions a load signal from the total measured power the
mixture of the signals in households
The bulk of the research in NILM has mostly concentrated on applying different data mining and
pattern recognition methods to track the footprint of each appliance in total power measurements
Several techniques such as artificial neural networks ANN Prudenzi Chang
Liang deep neural networks Kelly and Knottenbelt k-nearest neighbor
Figueiredo Weiss sparse coding Kolter or ad-hoc heuristic
methods Dong have been employed Recent works rather than turning electrical events
into features fed into classifiers consider the temporal structure of the data[Zia Kolter
and Jaakkola Kim Zhong Egarter Guo
resulting in state-of-the-art performance Kolter and Jaakkola These works usually model the
individual appliances by independent hidden Markov models HMMs which leads to a factorial
HMM FHMM model describing the total consumption
Conference on Neural Information Processing Systems NIPS Barcelona Spain
FHMMs introduced by Ghahramani and Jordan are powerful tools for modeling times series
generated from multiple independent sources and are great for modeling speech with multiple people
simultaneously talking Rennie or energy monitoring which we consider here Kim
Doing exact inference in FHMMs is NP hard therefore computationally efficient approximate
methods have been the subject of study Classic approaches include sampling methods such as
MCMC or particle filtering Koller and Friedman and variational Bayes methods Wainwright
and Jordan Ghahramani and Jordan In practice both methods are nontrivial to make
work and we are not aware of any works that would have demonstrated good results in our application
domain with the type of FHMMs we need to work and at practical scales
In this paper we follow the work of Kolter and Jaakkola to model the NILM problem by
FHMMs The distinguishing features of FHMMs in this setting are that the output is the sum of
the output of the underlying HMMs perhaps with some noise and the number of transitions
are small in comparison to the signal length FHMMs with the first property are called additive In
this paper we derive an efficient convex relaxation based method for FHMMs of the above type
which significantly outperforms the state-of-the-art algorithms Our approach is based on revisiting
relaxations to the integer programming formulation of Kolter and Jaakkola In particular
we replace the quadratic programming relaxation of Kolter and Jaakkola with a relaxation
to an semi-definite program which based on the literature of relaxations is expected to be
tighter and thus better While SDPs are convex and could in theory be solved using interior-point
methods in polynomial time Malick IP scales poorly with the size of the problem
and is thus unsuitable to our large scale problem which may involve as many a million variables To
address this problem capitalizing on the structure of our relaxation coming from our FHMM model
we develop a novel variant of ADMM Boyd that uses Moreau-Yosida regularization
and combine it with a version of randomized rounding that is inspired by the the recent work of
Park and Boyd Experiments on synthetic and real data confirm that our method significantly
outperforms other algorithms from the literature and we expect that it may find its applications in
other FHMM inference problems too
Notation
Throughout the paper we use the following notation denotes the set of real numbers Sn denotes
the set of positive semidefinite matrices denotes the indicator function of an event
that is it is if the event is true and zero otherwise denotes a vector of appropriate dimension
whose entries are all For an integer denotes the set K}. denotes the
Gaussian distribution with mean and covariance matrix For a matrix A trace(A denotes its
trace and diag(A denotes the vector formed by the diagonal entries of A.
System Model
Following Kolter and Jaakkola the energy usage of the household is modeled using an additive
factorial HMM Ghahramani and Jordan Suppose there are appliances in a household
Each of them is modeled via an HMM let Pi RKi Ki denote the transition-probability matrix of
appliance and assume that for each state Ki the energy consumption of the appliance
is constant denotes the corresponding Ki dimensional column vector i,Ki
Denoting by xt,i the indicator vector of the state
st,i of appliance at time
xt,i,s I{st,i the total power consumption at time is
xt,i which we assume is
observed with some additive zero mean Gaussian noise of variance
xt,i
Given this model the maximum likelihood estimate of the appliance state vector sequence can be
obtained by minimizing the log-posterior function
PM
X1
xt,i
arg min
t,i log Pi
xt,i
subject to
xt,i xt,i and
Alternatively we can assume that the power
consumption yt,i ofPeach appliance is normally distributed with
mean
i2 and yt,i
xt,i and variance where
where log Pi denotes a matrix obtained from Pi by taking the logarithm of each entry
In our particular application in addition to the signal?s temporal structure large changes in total power
comparison to signal noise contain valuable information that can be used to further improve the
inference results fact solely this information was used for energy disaggregation by Dong
Figueiredo This observation was used by Kolter and Jaakkola
to amend the posterior with a term that tries to match the large signal changes to the possible changes
in the power level when only the state of a single appliance changes
Formally let
and define the matrices Et,i RKi Ki
diff
by Et,i
for some constant diff Intuitively Et,i is
the negative log-likelihood up to a constant of observing a change in the power level when
appliance transitions from state to state under some zero-mean Gaussian noise with variance
diff Making the heuristic approximation that the observation noise and this noise are independent
which clearly does not hold under the previous model Kolter and Jaakkola added the term
PT PM
xt,i Et,i to the objective of arriving at
arg min
xt,i
xT
PM
xt,i
X1
t,i Et,i log Pi
subject to xt,i xt,i and
In the rest of the paper we derive an efficient approximate solution to and demonstrate that it is
superior to the approximate solution derived by Kolter and Jaakkola with respect to several
measures quantifying the accuracy of load disaggregation solutions
SDP Relaxation and Randomized Rounding
There are two major challenges to solve the optimization problem exactly the optimization is
over binary vectors xt,i and the objective function even when considering its extension to a
convex domain is in general non-convex due to the second term As a remedy we will relax to
make it an integer quadratic programming problem then apply an SDP relaxation and randomized
rounding to solve approximately the relaxed problem We start with reviewing the latter methods
Approximate Solutions for Integer Quadratic Programming
In this section we consider approximate solutions to the integer quadratic programming problem
minimize
subject to
Dx
where Sn is positive semidefinite and Rn While an exact solution of can be found
by enumerating all possible combination of binary values within a properly chosen box or ellipsoid
the running time of such exact methods is nearly exponential in the number of binary variables
making these methods unfit for large scale problems
One way to avoid exponential running times is to replace with a convex problem with the hope that
the solutions of the convex problems can serve as a good starting point to find high-quality solutions
to The standard approach to this is to linearize by introducing a new variable Sn
tied to trough xx so that Dx trace(DX and then relax the nonconvex constraints
xx to xx diag(X This leads to the relaxed SDP
problem
minimize
subject to
trace(D
diag(X
By introducing
this can be written in the compact SDP form
trace(D
AX
minimize
subject to
and A is an appropriate linear operator This
general SDP optimization problem can be solved with arbitrary precision in polynomial time using
interior-point methods Malick Wen As discussed before this approach
becomes impractical in terms of both the running time and the required memory if either the number
of variables or the optimization constraints are large Wen We will return to the issue of
building scaleable solvers for NILM in Section
where
Note that introducing the new variable the problem is projected into a higher dimensional space
which is computationally more challenging than just simply relaxing the integrality constraint in
but leads to a tighter approximation of the optimum Park and Boyd see also Lov?sz and
Schrijver Burer and Vandenbussche
To obtain a feasible point of from the solution of we still need to change the solution to
a binary vector This can be done via randomized rounding Park and Boyd Goemans and
Williamson Instead of letting the integrality constraint in can be
replaced by the inequalities for all Although these constraints are nonconvex
they admit an interesting probabilistic interpretation the optimization problem
minimize
Ew?N Dw
subject to
Ew?N
is equivalent to
minimize
trace
subject to
Rn
which
is in the form of with and above Ex?P stands for
x)dP This leads to the rounding procedure starting from a solution of
we randomly draw several samples from round to or to obtain
and keep the with the smallest objective value In a series of experiments Park and Boyd
found this procedure to be better than just naively rounding the coordinates of
An Efficient Algorithm for Inference in FHMMs
To arrive at our method we apply the results of the previous subsection to To do so as mentioned
at the beginning of the section we need to change the problem to a convex one since the elements of
the second term in the objective of
t,i Et,i log Pi are not convex To address this
issue we relax the problem by introducing new variables Zt,i xt,i
and replace the constraint
Zt,i xt,i
with
two
new
ones
and Zt,i
Zt,i xt,i
To simplify the presentation we will assume that Ki for all Then problem becomes
arg min
xt,i
subject to
z?t
KK
xt,i
Zt,i xt,i
zt
and
Zt,i
and
Algorithm ADMM-RR Randomized rounding algorithm for suboptimal solution to
Given number of iterations itermax length of input data
Solve the optimization problem Run Algorithm to get Xt and zt
Set xbest
zt and Xtbest Xt for
for do
best
Set xbest
xbest
best
best
Set block(Xtbest
where block constructs block diagonal matrix from input
Xt
arguments
Set best
Form the covariance matrix xxT and find its Cholesky factorization LL
for itermax do
Random sampling Lw where I
Round to the nearest integer point xk that satisfies the constraints of
If best ft xk then update xbest
and Xtbest from the corresponding entries of xk and xk xk
respectively
end for
end for
where
xt,M
zt vec(Zt,1 vec(Zt,M and
pt vec(Et,1 log P1 vec(log PT with vec(A denoting the column vector obtained
by concatenating the columns of A for a matrix A. Expanding the first term of and following the
relaxation method of Section we get the following SDP problem:2
arg min
Xt zt
subject to
trace(Dt Xt
zt
AXt
Xt
BXt Czt EXt+1
Xt zt
Here A SM
Rm SM
Rm and RM KK?m are all appropriate linear
operators and the integers and are determined by the number of equality constraints while
Dt
and dt pt Notice that is a simple though huge-dimensional SDP
has a special block structure
problem in the form of where
Next we apply the randomized rounding method from Section to provide an approximate solution
to our original problem Starting from an optimal solution of and utilizing that
we have an SDP problem for each time step we obtain Algorithm that performs the rounding
sequentially for However we run the randomized method for three consecutive time
steps since Xt appears at both time steps and in addition to time equation
Following Park and Boyd in the experiments we introduce a simple greedy search within
Algorithm after finding the initial point xk we greedily try to objective the target value by change
the status of a single appliance at a single time instant The search stops when no such improvement
is possible and we use the resulting point as the estimate
ADMM Solver for Large-Scale Sparse Block-Structured SDP Problems
Given the relaxation and randomized rounding presented in the previous subsection all that remains
is to find Xt zt to initialize Algorithm Although interior point methods can solve SDP problems
efficiently even for problems with sparse constraints as the running time to obtain an optimal
solution is of the order of Nesterov Section which becomes prohibitive
in our case since the number of variables scales linearly with the time horizon
As an alternative solution first-order methods can be used for large scale problems Wen
Since our problem is an SDP problem where the objective function is separable ADMM is a
promising candidate to find a near-optimal solution To apply ADMM we use the Moreau-Yosida
quadratic regularization Malick which is well suited for the primal formulation we
The only modification is that we need to keep the equality constraints in that are missing from
Algorithm ADMM for sparse SDPs of the form
Given length of input data number of iterations itermax
Set the initial values to zero Wt0 Pt0 0t and rt0 h0t
Set Default step-size value
for itermax do
for do
Update Ptk Wtk Stk rtk hkt and tk respectively according to Appendix A).
end for
end for
consider When implementing ADMM over the variables Xt zt the sparse structure of our
constraints allows to consider the SDP problems for each time step sequentially
arg min
trace(Dt Xt
zt
subject to
AXt
BXt Czt EXt+1
BXt Czt EXt
Xt
Xt zt
Xt zt
The regularized Lagrangian function for is3
trace(D
kX Sk2F
kz
BX Cz EX BX
trace(W
trace(P
Cz
EX
AX
where
and are dual variables and is a constant By taking the
derivatives of and computing the optimal values of and one can derive the standard ADMM
updates which due to space constraints are given in Appendix A. The final algorithm which updates
the variables for each sequentially is given by Algorithm
Algorithms and together give an efficient algorithm for finding an approximate solution to and
thus also to the inference problem of additive FHMMs
Learning the Model
The previous section provided an algorithm to solve the inference part of our energy disaggregation
problem However to be able to run the inference method we need to set up the model To learn
the HMMs describing each appliance we use the method of Kontorovich to learn the
transition matrix and the spectral learning method of Anandkumar following Mattfeld
to determine the emission parameters
However when it comes to the specific application of NILM the problem of unknown time-varying
bias also needs to be addressed which appears due to the presence of unknown/unmodeled appliances
in the measured signal A simple idea which is also followed by Kolter and Jaakkola is to
use a generic model whose contribution to the objective function is downweighted Surprisingly
incorporating this idea in the FHMM inference creates some unexpected challenges.4
Therefore in this work we come up with a practical heuristic solution tailored to NILM First we
identify all electric events defined by a large change in the power usage using some ad-hoc
threshold Then we discard all events that are similar to any possible level change The
remaining large jumps are regarded as coming from a generic HMM model describing the unregistered
appliances they are clustered into clusters and an HMM model is built where each cluster is
regarded as power usage coming from a single state of the unregistered appliances We also allow an
off state with power usage
We drop the subscript and replace and with and signs respectively
For example the incorporation of this generic model breaks the derivation of the algorithm of Kolter and
Jaakkola See Appendix for a discussion of this
Experimental Results
We evaluate the performance of our algorithm in two setups:5 we use a synthetic dataset to test
the inference method in a controlled environment while we used the REDD dataset of Kolter and
Johnson to see how the method performs on non-simulated real data The performance of
our algorithm is compared to the structured variational inference SVI method of Ghahramani and
Jordan the method of Kolter and Jaakkola and that of Zhong we shall
refer to the last two algorithms as KJ and ZGS respectively
Experimental Results Synthetic Data
The synthetic dataset was generated randomly the exact procedure is described in Appendix C).
To evaluate the performance we use normalized disaggregation error as suggested by Kolter and
Jaakkola and also adopted by Zhong This measures the reconstruction error for
each individual appliance Given the true output yt,i and the estimated output y?t,i y?t,i
the error measure is defined as
qP
NDE
y?t,i t,i yt,i
t,i yt,i
Figures and show the performance of the algorithms as the number HMMs resp number of
states is varied Each plot is a report for steps averaged over random models and
realizations showing the mean and standard deviation of NDE. Our method shown under the label
ADMM-RR runs ADMM for iterations runs the local search at the end of each iterations
and chooses the result that has the maximum likelihood ADMM is the algorithm which applies naive
rounding It can be observed that the variational inference method is significantly outperformed by
all other methods while our algorithm consistently obtained better results than its competitors KJ
coming second and ZGS third
Number of states Data length Number of samples
Number of states Data length Number of samples
ADMM-RR
KJ method
ADMM
ZGS method
Normalized error
Normalized error
ADMM-RR
KJ method
ADMM
Variational Approx
ZGS method
Figure Disaggregation error varying the number of HMMs
Number of appliances Data length Number of samples
ADMM-RR
KJ method
ADMM
ZGS method
Normalized error
Normalized error
Number of appliances Data length Number of samples
ADMM-RR
KJ method
ADMM
Variational Approx
ZGS method
Figure Disaggregation error varying the number of states
Experimental Results Real Data
In this section we also compared the best methods on the real dataset REDD Kolter and Johnson
We use the first half of the data for training and the second half for testing Each HMM
Our code is available online at https://github.com/kiarashshaloudegi/FHMM_inference
Appliance
Oven-3
Fridge
Microwave
Bath
Kitch
Wash./Dry.-20-A
Unregistered-A
Oven-4
Dishwasher-6
Wash./Dryer-10
Kitch
Wash./Dry.-20-B
13 Unregistered-B
Average
ADMM-RR
KJ method
ZGS method
Table Comparing the disaggregation performance of three different algorithms precision/recall
Bold numbers represent statistically better performance on both measures
appliance is trained separately using the associated circuit level data and the HMM corresponding
to unregistered appliances is trained using the main panel data In this set of experiments we monitor
appliances consuming more than watts ADMM-RR is run for iterations and the local
search is run at the end of each iterations and the result with the largest likelihood is chosen
To be able to use the ZGS method on this data we need to have some prior information about the
usage of each appliance the authors suggestion is to us national energy surveys but in the lack of
this information also about the number of residents type of houses etc we used the training data to
extract this prior knowledge which is expected to help this method
Detailed results about the precision and recall of estimating which appliances are at any given
time are given in Table In Appendix we also report the error of the total power usage assigned
to different appliances Table as well as the amount of assigned power to each appliance as
a percentage of total power Figure As a summary we can see that our method consistently
outperformed the others achieving an average precision and recall of and with about
better precision than KJ with essentially the same recall while significantly
improving upon ZGS Considering the error in assigning the power consumption to
different appliances our method achieved about smaller error ADMM-RR KJ
ZGS than its competitors
In our real-data experiments there are about million decision variables or appliances
for phase A and power respectively with states each and for about time
steps for one day sample every seconds KJ and ZGS solve quadratic programs increasing their
memory usage vs 6GB in our case On the other hand our implementation of their method
using the commercial solver MOSEK inside the Matlab-based YALMIP L?fberg runs in
minutes while our algorithm which is purely Matlab-based takes hours to finish We expect that an
optimized version of our method could achieve a significant speed-up compared to our current
implementation
Conclusion
FHMMs are widely used in energy disaggregation However the resulting model has a huge
factored state space making standard inference FHMM algorithms infeasible even for only a
handful of appliances In this paper we developed a scalable approximate inference algorithm based
on a semidefinite relaxation combined with randomized rounding which significantly outperformed
the state of the art in our experiments A crucial component of our solution is a scalable ADMM
method that utilizes the special block-diagonal-like structure of the SDP relaxation and provides a
good initialization for randomized rounding We expect that our method may prove useful in solving
other FHMM inference problems as well as in large scale integer quadratic programming
Acknowledgements
This work was supported in part by the Alberta Innovates Technology Futures through the Alberta Ingenuity
Centre for Machine Learning and by NSERC K. is indebted to Pooria Joulani and Mohammad Ajallooeian
whom provided much useful technical advise while all authors are grateful for Zico Kolter for sharing his code

----------------------------------------------------------------

title: 4933-pass-efficient-unsupervised-feature-selection.pdf

Pass-Efficient Unsupervised Feature Selection
Haim Schweitzer
Department of Computer Science
The University of Texas at Dallas
HSchweitzer@utdallas.edu
Crystal Maung
Department of Computer Science
The University of Texas at Dallas
Crystal.Maung@gmail.com
Abstract
The goal of unsupervised feature selection is to identify a small number of important features that can represent the data We propose a new algorithm a modification of the classical pivoted QR algorithm of Businger and Golub that requires a
small number of passes over the data The improvements are based on two ideas
keeping track of multiple features in each pass and skipping calculations that can
be shown not to affect the final selection Our algorithm selects the exact same
features as the classical pivoted QR algorithm and has the same favorable numerical stability We describe experiments on real-world datasets which sometimes
show improvements of several orders of magnitude over the classical algorithm
These results appear to be competitive with recently proposed randomized algorithms in terms of pass efficiency and run time On the other hand the randomized
algorithms may produce more accurate features at the cost of small probability of
failure
Introduction
Work on unsupervised feature selection has received considerable attention See
In numerical linear algebra unsupervised feature selection is known as the column
subset selection problem where one attempts to identify a small subset of matrix columns that can
approximate the entire column space of the matrix See Chapter The distinction
between supervised and unsupervised feature selection is as follows In the supervised case one
is given labeled objects as training data and features are selected to help predict that label in the
unsupervised case nothing is known about the labels
We describe an improvement to the classical Businger and Golub pivoted QR algorithm We
refer to the original algorithm as the QRP and to our improved algorithm as the IQRP The QRP
selects features one by one using passes in order to select features In each pass the selected
feature is the one that is the hardest to approximate by the previously selected features We achieve
improvements to the algorithm run time and pass efficiency without affecting the selection and the
excellent numerical stability of the original algorithm Our algorithm is deterministic and runs in a
small number of passes over the data It is based on the following two ideas
In each pass we identify multiple features that are hard to approximate with the previously
selected features A second selection step among these features uses an upper bound on
unselected features that enables identifying multiple features that are guaranteed to have
been selected by the QRP. See Section for details
Since the error of approximating a feature can only decrease when additional features are
added to the selection there is no need to evaluate candidates with error that is already too
small This allows a significant reduction in the number of candidate features that need to
be considered in each pass See Section for details
Algorithms for unsupervised feature selection
The algorithms that we consider take as input large matrices of numeric values We denote by
the number of rows by the number of columns features and by the number of features to be
selected Criteria for evaluating algorithms include their run time and memory requirements the
number of passes over the data and the algorithm accuracy The accuracy is a measure of the error
of approximating the entire data matrix as a linear combination of the selection We review some
classical and recent algorithms for unsupervised feature selection
Related work in numerical linear algebra
Businger and Golub QRP was established by Businger and Golub We discuss it in detail
in Section It requires passes for selecting features and its run time is 4kmn 2k
4k A recent study compares experimentally the accuracy of the QRP as a feature selection
algorithm to some recently proposed state-of-the-art algorithms Even though the accuracy of the
QRP is somewhat below the other algorithms the results are quite similar The only exception was
the performance on the Kahan matrix where the QRP was much less accurate
Gu and Eisenstat algorithm was considered the most accurate prior to the work on randomized
algorithms that had started with It computes an initial selection typically by using the QRP
and then repeatedly swaps selected columns with unselected column The swapping is done so that
the product of singular values of the matrix formed by the selected columns is increased with each
swapping The algorithm requires random access memory and it is not clear how to implement it
by a series of passes over the data Its run time is
Randomized algorithms
Randomized algorithms come with a small probability of failure but otherwise appear to be more
accurate than the classical deterministic algorithms Frieze al have proposed a randomized
algorithm that requires only two passes over the data This assumes that the norms of all matrix
columns are known in advance and guarantees only an additive approximation error We discuss
the run time and the accuracy of several generalizations that followed their studies
Volume sampling Deshpande al have studied a randomized algorithm that samples k-tuples
of columns with probability proportional to their volume The volume is the square of the product
of the singular values of the submatrix formed by these columns They show that this sampling
scheme gives rise to a randomized algorithm that computes the best possible solution in the Frobenius norm They describe an efficient O(kmn randomized algorithm that can be implemented in
passes and approximates this sampling scheme These results were improved terms of accuracy
in by computing the exact volume sampling The resulting algorithm is slower but much more
accurate Further improvements to the speed of volume sampling in have reduced the run time
complexity to O(km2 As shown in this optimal terms of accuracy algorithm can also
be derandomized with a deterministic run time of O(km3
Leverage sampling The idea behind leverage sampling is to randomly select features with probability proportional to their leverage Leverage values are norms of the rows of the right
eigenvector matrix in the truncated SVD expansion of the data matrix See In particular
the two stage algorithm described in requires only passes if the leverage values are known
Its run time is dominated by the calculation of the leverage values To the best of our knowledge
the currently best algorithms for estimating leverage values are randomized One run takes
passes and O(mn log m3 time This is dominated by the mn term and show that it
can be further reduced to the number of nonzero values We note that these algorithms do not compute reliable leverage in passes since they may fail at a relatively high probability As
stated in the success probability can be amplified by independent repetition and taking the
coordinate-wise median Therefore accurate estimates of leverage can be computed in constant
number of passes But the constant would be larger than
Input The features matrix columns and an integer
Output An ordered list of indices
In the initial pass compute
For set x?i vi
is the error vector of
approximating by a linear combination of the columns in
At the end of the pass set z1 arg max vi and initialize
For each pass
For set vi to the square error of
approximating by a linear combination of the columns in S.
At the end of pass set zj arg max vi and add zj to S.
Figure The main steps of the QRP algorithm
Randomized ID
In a recent survey Halko et.al describe how to compute QR factorization using their randomized Interpolative Decomposition Their approach produces an accurate as a basis of the data
matrix column space They propose an efficient row extraction method for computing that
works when the desired rank is similar to the rank of the data matrix Otherwise the row extraction introduces unacceptable inaccuracies which led Halko et.al to recommend using an alternative
O(kmn technique in such cases
Our result the complexity of the IQRP
The savings that the IQRP achieves depend on the data The algorithm takes as input an integer value
the length of a temporary buffer As explained in Section our implementation requires temporary
storage of which takes floats The following values depend on the data the number
of passes the number of IO-passes explained below and a unit cost of orthogonalization
Section
In terms of and the run time is 2mn 4mnc 4mlk Our experiments show that for typical
datasets the value of is below For our experiments show that the number of passes is
typically much smaller than The number of passes is even smaller if one considers IO-passes To
explain what we mean by IO-passes consider as an example a situation where the algorithm runs
three passes over the data In the first pass all features are being accessed In the second only two
features are being accessed In the third only one feature is being accessed In this case we take
the number of IO-passes to be n3 We believe that is a relevant measure of the algorithm pass
complexity when skipping is cheap so that the cost of a pass over the data is the amount of data that
needs to be read
The Businger Golub algorithm QRP
In this section we describe the QRP which forms the basis to the IQRP The main steps
are described in Figure There are two standard implementations for Step in Figure The
first is by means of the Modified Gram-Schmidt and the second is by Householder
orthogonalization Both methods require approximately the same number of flops but
error analysis shows that the Householder approach is significantly more stable
Memory-efficient implementations
The implementations shown in Figure update the memory where the matrix A is stored Specifically A is overwritten by the component of the QR factorization Since we are not interested in
overwriting A may not be acceptable The procedure shown in Figure does not overwrite A
but it is more costly The flops count is dominated by Steps and which cost at most
at pass Summing up for this gives a total flops count of approximately 2k mn flops
Compute zj qj Qj
for
vi vi wi2
At the end of the pass
zj arg max vi
Compute zj hj Hj
for
the j?th coordinate of
vi vi wi2
At the end of the pass
zj arg max vi
qj xzj xzj
Qj qj
Create the Householder matrix hj from
Hj hj
Modified Gram-Schmidt
Householder orthogonalization
Figure Standard implementations of Step of the QRP
Compute zj qj Qj
for
QTj?1
vi
At the end of the pass
zj arg max vi
Compute zj hj Hj
for
Pm
vi yi2
At the end of the pass
zj arg max vi
q?j xzj wzj qj q?j
qj
Qj qj
Create hj from yzj
Hj hj
Modified Gram-Schmidt
Householder orthogonalization
Figure Memory-efficient implementations of Step of the QRP
The IQRP algorithm
In this section we describe our main result the improved QRP. The algorithm maintains three ordered lists of columns The list is the input list containing all columns The list contains
columns that have already been selected The list is of size where is a user defined parameter
For each column in the algorithm maintains an integer value ri and a real value vi These
values can be kept in core or a secondary memory They are defined as follows
ri
vi vi ri kxi Qri QTri
where Qri qri is an orthonormal basis to the first ri columns in S. Thus vi ri is
the squared error of approximating with the first ri columns in S. In each pass the algorithm
identifies the candidate columns corresponding to the largest values of vi That is the vi
values are computed as the error of predicting each candidate by all columns currently in S. The
identified columns with the largest vi are stored in the list L. In addition the value of the
largest vi is kept as the constant BF Thus after a pass is terminated the following
condition holds
BF for all L.
The list and the value BF can be calculated in one pass using a binary heap data structure with
the cost of at most log(l comparisons See Chapter The main steps of the algorithm
are described in Figure
Details of Steps of the IQRP The threshold value is defined by
if the heap is not full
top of the heap if the heap is full
Input The matrix columns features and an integer
Output An ordered list of indices
The initial pass over
Create a min-heap of size
In one pass go over
Set vi ri
Fill the heap with the candidates corresponding to the largest vi
At the end of the pass
Set BF to the value at the top of the heap
Set to heap content excluding the top element
Add to as many candidates from as possible using BF
Repeat until has candidates
Create a min-heap of size
Let be defined by
In one pass go over
Skip if vi ri Otherwise update vi ri heap
At the end of the pass
Set BF
Set to heap content excluding the top element
Add to as many candidates from as possible using BF
Figure The main steps of the IQRP algorithm
Thus when the heap is full is the value of associated with the largest candidate encountered so far The details of Step are shown in Figure Step can be computed using
either Gram-Schmidt or Householder as shown in Figures and
A.1. If vi ri skip
A.2. Otherwise check ri
If ri conditionally insert into the heap
If ri
Compute vi Set ri
Conditionally insert into the heap
Figure Details of Step
Details of Steps and of the IQRP Here we are given the list and the value of BF
satisfying To move candidates from to run the QRP on as long as the pivot value is above
BF The pivot value is the largest value of vi in The details are shown in Figure
B.1. arg max vi
i?L
B.2. If vz BF we are done exploiting L.
B.3. Otherwise
Move from to S.
Update the remaining candidates in using either Gram-Schmidt or
the Householder procedure
For example with Householder
Create the Householder matrix hj from xz
for all in replace with hj
Figure Details of Steps and
Correctness
In this section we show that the IQRP computes the same selection as the QRP. The proof
is by induction on the number of columns in S. For the QRP selects with
vj max The IQRP selects vj0 as the largest among the largest values in Therei
fore vj0 maxxi maxxi vj Now assume that for the QRP and the
IQRP select the same columns in this is the inductive assumption Let vj be the value of
the selection by the QRP and let vj0 be the value of the selection by the IQRP We
need to show that vj0 vj The QRP selection of satisfies vj maxxi vi
Observe that if then ri Initially is created from the heap elements that have
ri Once is increased in Step the columns in are updated according to so that
they all satisfy ri The IQRP selection satisfies
vj0 max vi and vj0 BF
Additionally for all
BF
This follows from the observation that is monotonically decreasing in and
Therefore combining and we get
vj0 max vi vj
This completes the proof by induction
Termination
To see that the algorithm terminates it is enough to observe that at least one column is selected in
each pass The condition at Step in Figure cannot hold at the first time in a new L. The value
of BF is the largest vi while the maximum at is among the largest vi
Complexity
The formulas in this section describe the complexity of the IQRP in terms of the following
the number of features matrix columns
the number of selected features
number of passes
a unit cost of orthogonalizing
the number of objects matrix rows
user provided parameter
number of IO-passes
The value of depends on the implementation of Step in Figure We write cmemory for the
value of in the memory-efficient implementation and cflops for the faster implementation terms
of flops We use the following notation At pass the number of selected columns is kj and the
number of columns that were not skipped in Step of the IQRP same as Step is nj
The number of flops in the memory-efficient implementation can be shown to be
flopsmemory 2mn 4mnc 4mlk
where
nj
kj
Observe that so that for the worst case behavior is the same as the memoryoptimized QRP algorithm which is O(k We show in Section that the typical run time is
much faster In particular the dependency on appears to be linear and not quadratic
For the faster implementation that overwrites the input it can be shown that
flopstime 2mn 4m
r?i
where r?i is the value of ri at termination
Since r?i it follows that flopstime 4kmn Thus the worst case behavior is the same as the
flops-efficient QRP algorithm
Memory in the memory-efficient implementation requires km in-core floats and additional memory
for the heap that can be reused for the list L. Additional memory to store and manipulate vi ri
for is roughly 2n floats Observe that these memory locations are being accessed
consecutively and can be efficiently stored and manipulated out-of-core The data itself the matrix
A is stored out-of-core When the method of Figure is used in these matrix values are
read-only
IO-passes We wish to distinguish between a pass where the entire data is accessed and a pass where
mostP
of the data is skipped
Pp This suggests the following definition for the number of IO-passes
nj nj
Number of floating point comparisons Testing for the skipping and manipulating the heap requires
floating point comparisons The number of comparisons is n(p log2 This
does not affect the asymptotic complexity since the number of flops is much larger
Experimental results
We describe results on several commonly used datasets with and
is part of the URL reputation collection at the UCI Repository thrombin with
and is the data used in KDD Cup Amazon with
and is part of the Amazon Commerce reviews set and was obtained from the UCI
Repository gisette with and was used in NIPS selection challenge
Measurements We vary and report the following flopsmemory flopstime are the ratios between
the number of flops used by the IQRP and kmn for the memory-efficient orthogonalization and
the time-efficient orthogonalization passes is the number of passes needed to select features
IO-passes is discussed in sections and It is the number of times that the entire data is read
Thus the ratio between the number of IO-passes and the number of passes is the fraction of the data
that was not skipped
Run time The number of flops of the QRP is between 2kmn and 4kmn We describe experiments
with the list size taken as For Day1 the number of flops beats the QRP by a factor of more
than For the other datasets the results are not as impressive There are still significant savings
for small and moderate values of say up to but for larger values the savings are smaller
Most interesting is the observation that the memory-efficient implementation of Step is not much
slower than the optimization for time Recall that the memory-optimized QRP is times slower than
the time-optimized QRP. In our experiments they differ by no more than a factor of
Number of passes We describe experiments with the list size taken as and also with
regardless of the value of The QRP takes passes for selecting features For the
Day1 dataset we observed a reduction by a factor of between to in the number of passes For
IO-passes the reduction goes up to a factor of almost Similar improvements are observed for
the Amazon and the gisette datasets For the thrombin it is slightly worse typically a reduction by
a factor of about 70 The number of IO-passes is always significantly below the number of passes
giving a reduction by factors up to For the recommended setting of we observed the
following In absolute terms the number of passes was below for most of the data the number of
IO-passes was below for most of the data
Concluding remarks
This paper describes a new algorithm for unsupervised feature selection Based on the experiments
we recommend using the memory-efficient implementation and setting the parameter As
explained earlier the algorithm maintains numbers for each column and these can also be kept
in-core This gives a memory footprint
Our experiments show that for typical datasets the number of passes is significantly smaller than
In situations where memory can be skipped the notion of IO-passes may be more accurate than
passes IO-passes indicate the amount of data that was actually read and not skipped
Day1
flopsmemory
flopstime
passes
IO-passes
number of passes
flops/kmn
passes
IO-passes
number of passes
thrombin
flopsmemory
flopstime
passes
IO-passes
passes
IO-passes
number of passes
number of passes
flops/kmn
Amazon
flopsmemory
flopstime
number of passes
flops/kmn
passes
IO-passes
passes
IO-passes
number of passes
gisette
flopsmemory
flopstime
passes
IO-passes
passes
IO-passes
number of passes
number of passes
flops/kmn
Figure Results of applying the IQRP to several datasets with varying and
The performance of the IQRP depends on the data Therefore the improvements that we observe
can also be viewed as an indication that typical datasets are easy This appears to suggest that
worst case analysis should not be considered as the only criterion for evaluating feature selection
algorithms Comparing the IQRP to the current state-of-the-art randomized algorithms that were
reviewed in Section we observe that the IQRP is competitive in terms of the number of passes
and appears to outperform these algorithms in terms of the number of IO-passes On the other hand
it may be less accurate

----------------------------------------------------------------

