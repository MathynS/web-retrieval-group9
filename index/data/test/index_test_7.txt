query sentence: genomic anomaly
<<------------------------------------------------------------------------------------------------------------------------------------------->>
title: 6456-multi-view-anomaly-detection-via-robust-probabilistic-latent-variable-models.pdf

Multi-view Anomaly Detection via Robust
Probabilistic Latent Variable Models
Tomoharu Iwata
NTT Communication Science Laboratories
iwata.tomoharu@lab.ntt.co.jp
Makoto Yamada
Kyoto University
makoto.m.yamada@ieee.org
Abstract
We propose probabilistic latent variable models for multi-view anomaly detection which is the task of finding instances that have inconsistent views given
multi-view data With the proposed model all views of a non-anomalous instance
are assumed to be generated from a single latent vector On the other hand an
anomalous instance is assumed to have multiple latent vectors and its different
views are generated from different latent vectors By inferring the number of latent vectors used for each instance with Dirichlet process priors we obtain multiview anomaly scores The proposed model can be seen as a robust extension of
probabilistic canonical correlation analysis for noisy multi-view data We present
Bayesian inference procedures for the proposed model based on a stochastic EM
algorithm The effectiveness of the proposed model is demonstrated in terms of
performance when detecting multi-view anomalies
Introduction
There has been great interest in multi-view learning in which data are obtained from various information sources In a wide variety of applications data are naturally comprised of multiple views
For example an image can be represented by color texture and shape information a web page can
be represented by words images and URLs occurring on in the page and a video can be represented
by audio and visual features In this paper we consider the task of finding anomalies in multi-view
data The task is called horizontal anomaly detection or multi-view anomaly detection
Anomalies in multi-view data are instances that have inconsistent features across multiple views
Multi-view anomaly detection can be used for many applications such as information disparity management purchase behavior analysis malicious insider detection and user aggregation
from multiple databases In information disparity management multiple views can be obtained from
documents written in different languages such as Wikipedia Multi-view anomaly detection tries to
find documents that contain different information across different languages which would be helpful for editors to select documents to be updated or beneficial for cultural anthropologists to analyze
social difference across different languages In purchase behavior analysis multiple views for each
item can be defined as its genre and its purchase history a set of users who purchased the item
Multi-view anomaly detection can find movies inconsistently purchased by users based on the movie
genre which would assist creating marketing strategies
Multi-view anomaly detection is different from standard single-view anomaly detection Singleview anomaly detection finds instances that do not conform to expected behavior Figure
shows the difference between a multi-view anomaly and a single-view anomaly in a two-view data
set is a multi-view anomaly since belongs to different clusters in different views
cluster in View and cluster in View and views of are not consistent is a singleview anomaly since is located far from other instances in each view However both views of
have the same relationship with the others they are far from the other instances and then
Conference on Neural Information Processing Systems NIPS Barcelona Spain
Latent space
BA
MC
GI
HE
W1
BD
MC
A
W2
a
GI
Observed view
I EH
AD
Observed view
Figure A multi-view anomaly and a single-view anomaly in a two-view data set Each
letter represents an instance and the same letter indicates the same instance Wd is a projection
matrix for view Graphical model representation of the proposed model
is not a multi-view anomaly Single-view anomaly detection methods such as one-class support
vector machines or tensor-based anomaly detection consider that is anomalous On
the other hand we would like to develop a multi-view anomaly detection method that detects as
anomaly but not Note that although single-view anomalies are uncommon instances multi-view
anomalies can be majority if they are inconsistent across multiple views
We propose a probabilistic latent variable model for multi-view anomaly detection With the proposed model there is a latent space that is shared across all views We assume that all views of a
non-anomalous normal instance are generated using a single latent vector On the other hand an
anomalous instance is assumed to have multiple latent vectors and its different views are generated
using different latent vectors which indicates inconsistency across different views of the instance
Figure shows an example of a latent space shared by the two-view data Two views of every
non multi-view anomaly can be generated from a latent vector using view-dependent projection matrices On the other hand since two views of multi-view anomaly are not consistent two latent
vectors are required to generate the two views using the projection matrices
Since the number of latent vectors for each instance is unknown we automatically infer it from the
given data by using Dirichlet process priors The inference of the proposed model is based on a
stochastic EM algorithm In the E-step a latent vector is assigned for each view of each instance
using collapsed Gibbs sampling while analytically integrating out latent vectors In the M-step
projection matrices for mapping latent vectors into observations are estimated by maximizing the
joint likelihood By alternately iterating and M-steps we infer the number of latent vectors used
in each instance and calculate its anomaly score from the probability of using more than one latent
vector
Proposed Model
Suppose that we are given instances with views Xn
where Xn xnd
Md
is a set of multi-view observation vectors for the nth instance and xnd
is the observation
vector of the dth view The task is to find anomalous instances that have inconsistent observation
features across multiple views We propose a probabilistic latent variable model for this task The
proposed model assumes that each instance has potentially a countably infinite number of latent
vectors Zn znj
where znj Each view of an instance xnd is generated depending
on a view-specific projection matrix Wd RMd and a latent vector znsnd that is selected from
a set of latent vectors Zn Here snd is the latent vector assignment of xnd When
the instance is non-anomalous and all its views are consistent all of the views are generated from
a single latent vector In other words the latent vector assignments for all views are the same
sn1 sn2 snD When it is an anomaly and some views are inconsistent different views
are generated from different latent vectors and some latent vector assignments are different
snd snd for some
Specifically the proposed model is an infinite mixture model where the probability for the dth view
of the nth instance is given by
p(xnd Zn Wd
nj xnd Wd znj
where
are the mixture weights nj represents the probability of choosing the jth
latent vector is a precision parameter denotes the Gaussian distribution with mean
and covariance matrix and I is the identity matrix Information of non-anomalous instances that
cannot be handled by a single latent vector is modeled in Gaussian noise which is controlled by
Since we assume the same observation noise across different views the observations need to be
normalized We use a Dirichlet process for the prior of mixture weight Its use enables us to
automatically infer the number of latent vectors for each instance from the given data
The complete generative process of the proposed model for multi-view instances is as follows
Draw a precision parameter Gamma(a
For each instance
Draw mixture weights Stick
For each latent vector
Draw a latent vector znj I
For each view
Draw a latent vector assignment snd Discrete(?n
ii Draw an observation vector xnd Wd znsnd I
Here Stick is the stick-breaking process that generates mixture weights for a Dirichlet
process with concentration parameter and is the relative precision for latent vectors is shared
for observation and latent vector precision because it makes it possible to analytically integrate out
as shown in Figure shows a graphical model representation of the proposed model where
the shaded and unshaded nodes indicate observed and latent variables respectively
The joint probability of the data and the latent vector assignments snd
is given
by
S|W a a
where Wd
Because we use conjugate priors we can analytically integrate out mixture
weights
latent vectors and precision parameter Here we use a Dirichlet
process prior for multinomial parameter and a Gaussian-Gamma prior for latent vector znj By
integrating out mixture weights the first factor is calculated by
Jn
Nnj
where Nnj represents the number of views assigned to the jth latent vector in the nth instance and
Jn is the number of latent vectors of the nth instance for which Nnj By integrating out latent
vectors and precision parameter the second factor of is calculated by
a
Md
Jn
Jn
ba
Cnj
a
where
a
PD
Md
Jn
XX
XX
nj
xnd xnd
nj nj
nj Cnj
Cnj
Wd xnd
d:snd
Wd Wd rI
d:snd
The posterior for the precision parameter and that for the latent vector znj are given by
a Gamma(a
p(znj Cnj
respectively
Inference
We describe inference procedures for the proposed model based on a stochastic EM algorithm in
which collapsed Gibbs sampling of latent vector assignments and the maximum joint likelihood
estimation of projection matrices are alternately iterated while analytically integrating out the
latent vectors mixture weights and precision parameter By integrating out latent vectors
we do not need to explicitly infer the latent vectors leading to a robust and fast-mixing inference
Let be the index of the dth view of the nth instance for notational convenience In the
E-step given the current state of all but one latent assignment a new value for is sampled from
according to the following probability
a
a
a
where represents a value or set excluding the dth view of the nth instance The first factor is given
by
Nnj
if
if
using where is for existing latent vectors and is for a new latent vector
By using the second factor is given by
Md
a
rI(j=Jn
a
bs
where represents the indicator function if A is true and otherwise and subscript
indicates the value when is assigned to the jth latent vector as follows
nj,s nj,s
a
Cnj,s Cnj
Cnj,s
Wd Wd Cnj
Intuitively if the current view cannot be modeled well by existing latent vectors a new latent vector
is used which indicates that the view is inconsistent with the other views
In the M-step the projection matrices are estimated by maximizing the logarithm of the joint
likelihood while fixing cluster assignment variables S. By setting the gradient of the joint log
likelihood with respect to equal to zero an estimate of is obtained as follows
Wd
a
xnd
nsnd
Jn
Cnj
a
nsnd nsnd
When we iterate the E-step that samples the latent vector assignment snd by employing for
each view in each instance and the M-step that maximizes the joint
likelihood using with respect to the projection matrix Wd for each view we
obtain an estimate of the latent vector assignments and projection matrices
In Section we defined that an instance is an anomaly when its different views are generated from
different latent vectors Therefore for an anomaly score we use the probability that the instance
uses more than one latent vector It is estimated by using the samples obtained in the inference as
PH
follows vn H1 I(Jn where Jn is the number of latent vectors used by the nth
instance in the hth iteration of the Gibbs sampling after the burn-in period and is the number
of the iterations The output of the proposed method is a ranked list of anomalies based on their
anomaly scores An analyst would investigate top few anomalies or use a threshold to select the
anomalies The threshold can be determined based on a targeted false alarm and detection rate
We can use cross-validation to select an appropriate dimensionality for the latent space K. With
cross-validation we assume that some features are missing from the given data and infer the model
with a different K. Then we select the smallest value that has performed the best at predicting
missing values
Related Work
Anomaly detection has had a wide variety of applications such as credit card fraud detection
intrusion detection for network security and analysis for healthcare data However most
existing anomaly detection techniques assume data with a single view a single observation
feature set
A number of anomaly detection methods for two-view data have been proposed
However they cannot be used for data with more than two views Gao proposed a
HOrizontal Anomaly Detection algorithm HOAD for finding anomalies from multi-view data In
HOAD there are hyperparameters including a weight for the constraint that require the data to be
labeled as anomalous or not for tuning and the performance is sensitive to the hyperparameters On
the other hand the parameters with the proposed model can be estimated from the given multi-view
data without label information by maximizing the likelihood In addition because the proposed
model is a probabilistic generative model we can extend it in a probabilistically principled manner
for example for handling missing data and combining with other probabilistic models
Liu and Lam proposed multi-view anomaly detection methods using consensus clustering They
found anomalies based on the inconsistency of clustering results across multiple views Therefore
they cannot find inconsistency within a cluster Christoudias proposed a method for filtering
instances that are corrupted by background noise from multi-view data The multi-view anomalies
considered in this paper include not only instances corrupted by background noise but also instances
categorized into different foreground classes across views and instances with inconsistent views
even if they belong to the same cluster Recently Alvarez proposed a multi-view anomaly
detection method However since the method is based on clustering it cannot find anomalies when
there are no clusters in the given data
The proposed model is a generalization of either probabilistic principal component analysis
PPCA or probabilistic canonical correlation analysis PCCA When all views are generated from different latent vectors for every instance the proposed model corresponds to PPCA
that is performed independently for each view When all views are generated from a single latent
vector for every instance the proposed model corresponds to PCCA with spherical noise
PCCA or canonical correlation analysis can be used for multi-view anomaly detection With
PCCA a latent vector that is shared by all views for each instance and a linear projection matrix for
each view are estimated by maximizing the likelihood or minimizing the reconstruction error of the
given data The reconstruction error for each instance can be used as an anomaly score However the
reconstruction errors are not reliable because they are calculated from parameters that are estimated
using data with anomalies by assuming that all of the instances are non-anomalous On the other
hand because the proposed model simultaneously estimates the parameters and infers anomalies
the estimated parameters are not contaminated by the anomalies With PPCA and PCCA Gaussian
distributions are used for observation noise which are sensitive to atypical observations Robust
PPCA and PCCA use Student-t distributions instead of Gaussian distributions which are stable
to data containing single-view anomalies The proposed model assumes Gaussian observation noise
and its precision is parameterized by a Gamma distributed variable Since we marginalize out
in the inference as written in the observation noise becomes a Student-t distribution Therefore
the proposed model is robust to single-view anomalies
With some CCA-related methods each latent vector is factorized into shared and private components
across different views They assume that every instance has shared and private parts that are the
same dimensionality for all instances In contrast the proposed model assumes that non-anomalous
instances have only shared latent vectors and anomalies have private latent vectors The proposed
model can be seen as CCA with private latent vectors where latent vectors across views are clustered for each instance When CCA with private latent vectors are inferred without clustering the
inferred private latent vectors do not become the same even if it is generated from a single latent vector because switching latent dimension or rotating the latent space does not change the likelihood
Therefore difference of the latent vectors cannot be used for multi-view anomaly detection
Experiments
Data We evaluated the proposed model quantitatively by using data sets which we obtained
from the LIBSVM data sets We generated two views by randomly splitting the features where
each feature can belong to only a single view and anomalies were added by swapping views of
two randomly selected instances regardless of their class labels for each view Splitting data does
not generate anomalies Therefore we can evaluate methods while controlling the anomaly rate
properly By swapping although single-view anomalies cannot be created since the distribution for
each view does not change multi-view anomalies are created
Comparing methods We compared the proposed model with probabilistic canonical correlation
analysis PCCA horizontal anomaly detection HOAD consensus clustering based anomaly
detection and one-class support vector machine OCSVM For PCCA we used
the proposed model in which the number of latent vectors was fixed at one for every instance The
anomaly scores obtained with PCCA were calculated based on the reconstruction errors HOAD requires to select an appropriate hyperparameter value for controlling the constraints whereby different
views of the same instance are embedded close together We ran HOAD with different hyperparameter settings and show the results that achieved the highest performance for each
data set For CC first we clustered instances for each view using spectral clustering We set the
number of clusters at which achieved a good performance in preliminary experiments Then we
calculated anomaly scores by the likelihood of consensus clustering when an instance was removed
since it indicates inconsistency of the instance across different views OCSVM is a representative
method for single-view anomaly detection To investigate the performance of a single-view method
for multi-view anomaly detection we included OCSVM as a comparison method For OCSVM
multiple views are concatenated in a single vector then use it for the input We used Gaussian kernel In the proposed model we used a and for all experiments The number of
iterations for the Gibbs sampling was and the anomaly score was calculated by averaging over
the multiple samples
Multi-view anomaly detection For the evaluation measurement we used the area under the ROC
curve A higher AUC indicates a higher anomaly detection performance Figure shows
AUCs with different rates of anomalies using two-view data sets which are averaged over
experiments For the dimensionality of the latent space we used for the proposed model
PCCA and HOAD In general as the anomaly rate increases the performance decreases The
proposed model achieved the best performance with eight of the data sets This result indicates
that the proposed model can find anomalies effectively by inferring a number of latent vectors for
each instance The performance of CC was low because it assumes that there are clusters for each
view and it cannot find anomalies within clusters The AUC of OCSVM was low because it is a
single-view anomaly detection method which considers instances anomalous that are different from
others within a single view Multi-view anomaly detection is the task to find instances that have
inconsistent features across views but not inconsistent features within a view The computational
time needed for PCCA was sec and that needed for the proposed model was 35 sec with wine
data
Figure shows AUCs with different dimensionalities of latent vectors using data sets whose anomaly
rate is When the dimensionality was very low or the AUC was low in most of the data
sets because low-dimensional latent vectors cannot represent the observation vectors well With all
the methods the AUCs were relatively stable when the latent dimensionality was higher than four
breast-cancer
diabetes
glass
Proposed
PCCA
HOAD
CC
OCSVM
AUC
AUC
AUC
anomaly rate
heart
anomaly rate
ionosphere
anomaly rate
sonar
svmguide2
AUC
AUC
AUC
AUC
anomaly rate
svmguide4
anomaly rate
anomaly rate
vehicle
vowel
anomaly rate
wine
AUC
AUC
AUC
anomaly rate
AUC
anomaly rate
anomaly rate
anomaly rate
Figure Average AUCs with different anomaly rates and their standard errors A higher AUC is
better
breast-cancer
diabetes
glass
AUC
AUC
AUC
Proposed
PCCA
HOAD
latent dimensionality
heart
latent dimensionality
ionosphere
latent dimensionality
sonar
svmguide2
AUC
AUC
AUC
AUC
latent dimensionality
svmguide4
latent dimensionality
vehicle
latent dimensionality
wine
AUC
AUC
AUC
latent dimensionality
vowel
AUC
latent dimensionality
latent dimensionality
latent dimensionality
latent dimensionality
Figure Average AUCs with different dimensionalities of latent vectors and their standard errors
Single-view anomaly detection We would like to find multi-view anomalies but woul not like to
detect single-view anomalies We illustrated that the proposed model does not detect single-view
anomalies using synthetic single-view anomaly data With the synthetic data latent vectors for
Table Average AUCs for single-view anomaly detection
Proposed
PCCA
OCSVM
Table High and low anomaly score movies calculated by the proposed model
Title
Score Title
Score
The Full Monty
Star Trek VI
Liar Liar
Star Trek III
The Professional
The Saint
Mr. Holland?s Opus Heat
Contact
Conspiracy Theory
single-view anomalies were generated from and those for non-anomalous instances
were generated from I). Since each of the anomalies has only one single latent vector it is
not a multi-view anomaly The numbers of anomalous and non-anomalous instances were and 95
respectively The dimensionalities of the observed and latent spaces were five and three respectively
Table shows the average AUCs with the single-view anomaly data which are averaged over
different data sets The low AUC of the proposed model indicates that it does not consider singleview anomalies as anomalies On the other hand the AUC of the one-class SVM OCSVM was
high because OCSVM is a single-view anomaly detection method and it leads to low multi-view
anomaly detection performance
Application to movie data For an application of multi-view anomaly detection we analyzed inconsistency between movie rating behavior and genre in MovieLens data An instance corresponds to a movie where the first view represents whether the movie is rated or not by users and the
second view represents the movie genre Both views consist of binary features where some movies
are categorized in multiple genres We used movies users and 19 genres Table shows
high and low anomaly score movies when we analyzed the movie data by the proposed method with
The Full Monty and Liar Liar were categorized in Comedy genre They are rated
by not only users who likes Comedy but also who likes Romance and Action-Thriller The
Professional was anomaly because it was rated by two different user groups where a group prefers
Romance and the other prefers Action Since Star Trek series are typical Sci-Fi and liked by
specific users its anomaly score was low
Conclusion
We proposed a generative model approach for multi-view anomaly detection which finds instances
that have inconsistent views In the experiments we confirmed that the proposed model could
perform much better than existing methods for detecting multi-view anomalies There are several
avenues that can be pursued for future work Since the proposed model assumes the linearity of
observations with respect to their latent vectors it cannot find anomalies when different views are
in a nonlinear relationship We can relax this assumption by using Gaussian processes We can
also relax the assumption that non-anomalous instances have the same latent vector across all views
by introducing private latent vectors The proposed model assumes Gaussian observation noise
Our framework can be extended for binary or count data by using Bernoulli or Poisson distributions
instead of Gaussian
Acknowledgments
MY was supported by KAKENHI

<<----------------------------------------------------------------------------------------------------------------------->>

title: 3145-geometric-entropy-minimization-gem-for-anomaly-detection-and-localization.pdf

Geometric entropy minimization GEM for anomaly
detection and localization
Alfred Hero III
University of Michigan
Ann Arbor MI
hero@umich.edu
Abstract
We introduce a novel adaptive non-parametric anomaly detection approach called
GEM that is based on the minimal covering properties of K-point entropic graphs
when constructed on training samples from a nominal probability distribution Such graphs have the property that as their span recovers the
entropy minimizing set that supports at least K/N of the mass of the
Lebesgue part of the distribution When a test sample falls outside of the entropy
minimizing set an anomaly can be declared at a statistical level of significance
A method for implementing this non-parametric anomaly detector is
proposed that approximates this minimum entropy set by the influence region of a
K-point entropic graph built on the training data By implementing an incremental
leave-one-out k-nearest neighbor graph on resampled subsets of the training data
GEM can efficiently detect outliers at a given level of significance and compute
their empirical p-values We illustrate GEM for several simulated and real data
sets in high dimensional feature spaces
Introduction
Anomaly detection and localization are important but notoriously difficult problems In such problems it is crucial to identify a nominal or baseline feature distribution with respect to which statistically significant deviations can be reliably detected However in most applications there is seldom
enough information to specify the nominal density accurately especially in high dimensional feature spaces for which the baseline shifts over time In such cases standard methods that involve
estimation of the multivariate feature density from a fixed training sample are inapplicable high
dimension or unreliable shifting baseline In this paper we propose an adaptive non-parametric
method that is based on a class of entropic graphs called K-point minimal spanning trees
and overcomes the limitations of high dimensional feature spaces and baseline shift This method
detects outliers by comparing them to the most concentrated subset of points in the training sample
It follows from that this most concentrated set converges to the minimum entropy set of probability as and K/N Thus we call this approach to anomaly detection the geometric
entropy minimization GEM method
Several approaches to anomaly detection have been previously proposed Parametric approaches
such as the generalized likelihood ratio test lead to simple and classical algorithms such as the Student t-test for testing deviation of a Gaussian test sample from a nominal mean value and the Fisher
F-test for testing deviation of a Gaussian test sample from a nominal variance These methods fall
under the statistical nomenclature of the classical slippage problem and have been applied to
detecting abrupt changes in dynamical systems image segmentation and general fault detection applications The main drawback of these algorithms is that they rely on a family of parameterically
defined nominal no-fault distributions
An alternative to parametric methods of anomaly detection are the class of novelty detection algorithms and include the GEM approach described herein Scholkopf and Smola introduced a kernelbased novelty detection scheme that relies on unsupervised support vector machines SVM The
single class minimax probability machine of Lanckriet etal derives minimax linear decision regions that are robust to unknown anomalous densities More closely related to our GEM approach is
that of Scott and Nowak who derive multiscale approximations of minimum-volume-sets to estimate a particular level set of the unknown nominal multivariate density from training samples For a
simple comparative study of several of these methods in the context of detecting network intrusions
the reader is referred to
The GEM method introduced here has several features that are summarized below Unlike the
MPM method of Lanckriet etal the GEM anomaly detector is not restricted to linear or even
convex decision regions This translates to higher power for specified false alarm level GEMs
computational complexity scales linearly in dimension and can be applied to level set estimation
in feature spaces of unprecedented high dimensionality GEM has no complicated tuning parameters or function approximation classes that must be chosen by the user Like the method
of Scott and Nowak GEM is completely non-parametric learning the structure of the nominal
distribution without assumptions of linearity smoothness or continuity of the level set boundaries
Like Scott and Nowak?s method GEM is provably optimal indeed uniformly most powerful
of specified level for the case that the anomaly density is a mixture of the nominal and a uniform
density GEM easily adapts to local structure changes in local dimensionality of the support
of the nominal density
We introduce an incremental Leave-one-out kNNG as a particularly versatile and fast anomaly detector in the GEM class Despite the similarity in nomenclature the L1O kNNG is different
from nearest neighbor kNN anomaly detection of The kNN anomaly detector is based on
thresholding the distance from the test point to the k-th nearest neighbor The L1O kNNG detector
computes the change in the topology of the entire kNN graph due to the addition of a test sample and
does not use a decision threshold Furthermore the parent GEM anomaly detection methodology
has proven theoretical properties the restricted optimality property for uniform mixtures and
general consistency properties
We introduce the statistical framework for anomaly detection in the next section We then describe
the GEM approach in Section Several simulations are presented Section
Statistical framework
The setup is the following Assume that a training sample Xn Xn of d-dimensional
vectors is available Given a new sample the objective is to declare to be a nominal
sample consistent with Xn or an anomalous sample that is significantly different from Xn This
declaration is to be constrained to give as few false positives as possible To formulate this problem
we adopt the standard statistical framework for testing composite hypotheses Assume that Xn is
an independent identically distributed sample from a multivariate density f0 supported on
the unit d-dimensional cube Let have density Anomaly detection can be formulated
as testing the hypotheses H0 fo versus H0 fo at a prescribed level of significance
declare H1
The minimum-volume-set
of level is defined as a set in IRd which minimizes the volume
dx subject to the constraint f0 x)dx The minimum-entropy-set of level
x)dx
is defined as a set in IRd which minimizes the R?enyi entropy
subject to the constraint f0 x)dx Here is any real valued parameter between
When is a Lebesgue density in IRd it is easy to show that these three sets are identical
almost everywhere
The test decide anomaly if is equivalent to implementing the test function
This test has a strong optimality property when f0 is Lebesgue continuous it is a uniformly most
powerful UMP level for testing anomalies that follow a uniform mixture distribution Specif
ically let have density where is the uniform density over
and Consider testing the hypotheses
H0
H1
Proposition Assume that under H0 the random vector has a Lebesgue continuous density f0
and that f0 is also a continuous random variable Then the level-set test of level is
uniformly most powerful for testing Furthermore its power function is
given by
A sufficient condition for the random variable above to be continuous is that the density f0
have no flat spots over its support set The proof of this proposition is omitted
There are two difficulties with implementing the level set test First for known f0 the level set
may be very difficult if not impossible to determine in high dimensions Second when only
a training sample from f0 is available and f0 is unknown the level sets have to be learned from
the training data There are many approaches to doing this for minimum volume tests and these
are reviewed in These methods can be divided into two main approaches density estimation
followed by plug in estimation of via variational methods and direct estimation of the level
set using function approximation and non-parametric estimation Since both approaches involve
explicit approximation of high dimensional quantities the multivariate density or the boundary
of the set these methods are difficult to apply in high dimensional problems The
GEM method we propose in the next section overcomes these difficulties
GEM and entropic graphs
GEM is a method that directly estimates the critical region for detecting anomalies using minimum coverings of subsets of points in a nominal training sample These coverings are obtained by
constructing minimal graphs a MST or kNNG covering a K-point subset that is a given proportion of the training sample Points not covered by these K-point minimal graphs are identified
as tail events and allow one to adaptively set a pvalue for the detector
For a set of points Xn in IRd a graph over Xn is a pair where Xn is the set of vertices
and is the set of
edges of the graph The total power weighted length or more simply the
length of is L(Xn e?E where is a specified edge exponent parameter
K-point MST
The MST with power weighting is defined as the graph that spans Xn with minimum total length
LM ST Xn min
e?T
where is the set of all trees spanning Xn
subsets of distinct points from Xn
Definition K-point MST Let Xn,K denote one of the
Among all of the MST?s spanning these sets the K-MST is defined as the one having minimal length
minXn,K Xn LM ST Xn,k
The K-MST thus specifies the minimal subset of points in addition to specifying the minimum
length This subset of points which we call a minimal graph covering of Xn of size can be viewed
as capturing the densest region of Xn Furthermore if Xn is a sample from a multivariate
density and if limK,n K/n and a greedy version of the K-MST is implemented this
set converges to the minimum entropy set containing a proportion of at least K/n of
the mass of the Lebesgue component of where This fact was used in to
motivate the greedy K-MST as an outlier resistant estimator of entropy for finite K.
Define the K-point subset
Xn,K
argminXn,K Xn LM ST Xn,K
selected by the greedy K-MST Then we have the following As the minimum entropy set and minimum volume set are identical this suggests the following minimal-volume-set anomaly detection
algorithm which we call the K-MST anomaly detector
K-MST anomaly detection algorithm
1]Process training sample Given a level of significance and a training sample Xn
Xn construct the greedy K-MST and retain its vertex set Xn,K
2]Process test sample Given a test sample run the K-MST on the merged training-test sample
Xn and store the minimal set of points
3]Make decision Using the test function defined below decide H1 if and decide H0
if
When the density f0 generating the training sample is Lebesgue continuous it follows from Theorem that as the K-MST anomaly detector has false alarm probability that converges
to K/n and power that converges to that of the minimum-volume-set test of level When
the density f0 is not Lebesgue continuous some optimality properties of the K-MST anomaly detector still hold Let this nominal density have the decomposition f0 where is Lebesgue
continuous and is singular Then according to Theorem the K-MST anomaly detector will
have false alarm probability that converges to where is the mass of the singular component of f0 and it is a uniformly most powerful test for anomalies in the continuous component
for the test of H0 against H1
It is well known that the K-MST construction is of exponential complexity in In fact even
for a case one can call the leave-one-out MST there is no simple fast algorithm
for computation However the leave-one-out kNNG described below admits a fast incremental
algorithm
K-point kNNG
Let Xn Xn be a set of points The nearest neighbors kNN Xi(k
of a point Xn are the closest points to points in Xn Here the measure of
closeness is the Euclidean distance Let ei(k be the set of edges between and its
nearest neighbors The kNN graph kNNG over Xn is defined as the union of all of the kNN edges
ei(k and the total power weighted edge length of the kNN graph is
LkN Xn
subsets of distinct points from Xn
Definition K-point kNNG Let Xn,K denote one of the
Among all of the kNNG over each of these sets the K-kNNG is defined as the one having minimal
length minXn,K Xn LkN Xn,k
As the kNNG length is also a quasi additive continuous functional the asymptotic KMST
theory of extends to the K-point kNNG Of course computation of the K-point kNNG also has
exponential complexity However the same type of greedy approximation introduced by Ravi
for the K-MST can be implemented to reduce complexity of the K-point kNNG This approximation
to the K-point kNNG will satisfy the tightly coverable graph property of Defn We have the
following result that justifies the use of such an approximation as an anomaly detector of level
where
Proposition Let Xn,K
be the set of points in Xn that results from any approximation to the
point kNNG that satisfies the property Defn Then limn P0 Xn,K
and
limn P0 Xn,K where is a minimum-volume-set of
level and
Proof We provide a rough sketch using the terminology of Recall that a set
of resolution is representable by a union of elements of the uniform partition of into
hypercubes of volume Lemma of asserts that there exists an such that for the
limits claimed in Proposition hold with replaced by Am
a minimum volume set of resolution
that contains As limm Am
this
establishes
the proposition
Figures illustrate the use of the K-point kNNG as an anomaly detection algorithm
K?point kNNG
Bivariate Gaussian mixture density
Figure Left level sets of the nominal bivariate mixture density used to illustrate the point kNNG
anomaly detection algorithms Right K-point kNNG over random training samples drawn
from the nominal bivariate mixture at left Here and corresponding to a significance
level of
K?point kNNG
K?point kNNG
Figure Left The test point is declared anomalous at level as it is not captured by the
K-point kNNG constructed over the combined test sample and the training samples drawn
from the nominal bivariate mixture shown in Right A different test point is declared
non-anomalous as it is captured by this K-point kNNG
Leave-one-out kNNG L1O-kNNG
The theoretical equivalence between the K-point kNNG and the level set anomaly detector motivates
a low complexity anomaly detection scheme which we call the leave-one-out kNNG discussed
in this section and adopted for the experiments below As before assume a single test sample
and a training sample Xn Fix and assume that the kNNG over the set Xn has been
computed To determine the kNNG over the combined sample Xn one can
execute the following algorithm
L1O kNNG anomaly detection algorithm
For each compute the kNNG total length difference
LkN LkN LkN by the following steps For each
Find the edges
of all of the kNN?s of
Find the edges of other points in that have as one of their
kNNs For these points find the edges to their respective 1st NN point
Compute LkN e?E e?E
Define the kNNG most outlying point as Xo LkN
Declare the test sample an anomaly if Xo
This algorithm will detect anomalies with a false alarm level of approximately Thus larger
sizes of the training sample will correspond to more stringent false alarm constraints Furthermore
the p-value of each test point is easily computed by recursing over the size of the training
sample In particular let vary from to and define as the minimum value of for which
is declared an anomaly Then the p-value of is approximately
A useful relative influence coefficient can be defined for each point in the combined sample
LkN
maxi LkN
The coefficient when the test point is declared an anomaly
Using matlab?s matrix sort algorithm step of this algorithm can be computed an order of magnitude
faster than the K-point MST logN vs logN For example the experiments below have
shown that the above algorithm can find and determine the p-value of outliers among test
samples in a few seconds on a Dell 2GHz processor running Matlab
Illustrative examples
Here we focus on the L1O kNNG algorithm due to its computational speed We show a few representative experiments for simple Gaussian and Gaussian mixture nominal densities f0
L1O kNN scores detection
iteration pvalue
iteration pvalue
score max
iteration pvalue
iteration pvalue
iteration pvalue
iteration pvalue
iteration pvalue
iteration pvalue
iteration pvalue
sample number
Figure Left The plot of the anomaly curve for the L1O kNNG anomaly detector for detecting
deviations from a nominal 2D Gaussian density with mean and correlation coefficient
The boxes on peaks of curve correspond to positions of detected anomalies and the height of the
boxes are equal to one minus the computed p-value Anomalies were generated on the average
every samples and drawn from a 2D Gaussian with correlation coefficient The parameter
is equal to where is the user defined false alarm rate Right the resampled nominal
distribution and anomalous points detected at the iterations indicated at left
First we illustrate the L1O kNNG algorithm for detection of non-uniformly distributed anomalies
from training samples following a bivariate Gaussian nominal density Specifically a 2D Gaussian
density with mean and correlation coefficient was generated to train of the L1O kNNG
detector The test sample consisted of a mixture of this nominal and a zero mean 2D Gaussian with
correlation coefficient with mixture coefficient In the results of simulation with
a training sample of samples and tests samples are shown is a plot of the relative
influence curve over the test samples as compared to the most outlying point in the resampled
training sample When the relative influence curve is equal to the corresponding test sample is the
most outlying point and is declared an anomaly The detected anomalies in have p-values
less than and therefore one would expect an average of only one false alarm at this level of
significance In the right panel of the detected anomalies asterisks are shown along with the
training sample dots used to grow the L1O kNNG for that particular iteration note that to protect
against bias the training sample is resampled at each iteration
Next we compare the performance of the L1O kNNG detector to that of the UMP test for the
hypotheses We again trained on a bivariate Gaussian f0 with mean zero but this time with
identical component variances of This distribution has essential support on the unit
square For this?simple case the minimum volume set of level is a disk centered at the origin with radius and the power of the of the UMP can be computed in closed form
We implemented the GEM anomaly detector with the incremental leave-one-out kNNG using The training set consisted of samples from f0 and
the test set consisted of samples from the mixture of a uniform density and f0 with parameter
ranging from to Figure shows the empirical ROC curves obtained using the GEM test vs
the theoretical curves labeled clairvoyant for several different values of the mixing parameter
Note the good agreement between theoretical prediction and the GEM implementation of the UMP
using the kNNG
ROC curves for Gaussian+uniform mixture Nrep=10
L1O?kNN
Clairvoyant
Figure ROC curves for the leave-one-out kNNG anomaly detector described in Sec. The
labeled clairvoyant curve is the ROC of the UMP anomaly detector The training sample is a zero
mean 2D spherical Gaussian distribution with standard deviation and the test sample is a this
2D Gaussian and a 2D uniform-[0 mixture density The plot is for various values of the mixture
parameter
Conclusions
A new and versatile anomaly detection method has been introduced that uses geometric entropy
minimization GEM to extract minimal set coverings that can be used to detect anomalies from a
set of training samples This method can be implemented through the K-point minimal spanning tree
MST or the K-point nearest neighbor graph kNNG The L1O kNNG is significantly less computationally demanding than the K-point MST. We illustrated the L1O kNNG method on simulated
data containing anomalies and showed that it comes close to achieving the optimal performance of
the UMP detector for testing the nominal against a uniform mixture with unknown mixing parameter As the L1O kNNG computes p-values on detected anomalies it can be easily extended to
account for false discovery rate constraints By using a sliding window the methodology derived in
this paper is easily extendible to on-line applications and has been applied to non-parametric intruder
detection using our Crossbow sensor network testbed reported elsewhere
Acknowledgments
This work was partially supported by NSF under Collaborative ITR grant

<<----------------------------------------------------------------------------------------------------------------------->>

title: 4612-multi-criteria-anomaly-detection-using-pareto-depth-analysis.pdf

Multi-criteria Anomaly Detection using
Pareto Depth Analysis
Ko-Jen Hsiao Kevin S. Xu Jeff Calder and Alfred Hero III
University of Michigan Ann Arbor MI USA
coolmark,xukevin,jcalder,hero}@umich.edu
Abstract
We consider the problem of identifying patterns in a data set that exhibit anomalous behavior often referred to as anomaly detection In most anomaly detection
algorithms the dissimilarity between data samples is calculated by a single criterion such as Euclidean distance However in many cases there may not exist a
single dissimilarity measure that captures all possible anomalous patterns In such
a case multiple criteria can be defined and one can test for anomalies by scalarizing the multiple criteria using a linear combination of them If the importance
of the different criteria are not known in advance the algorithm may need to be
executed multiple times with different choices of weights in the linear combination In this paper we introduce a novel non-parametric multi-criteria anomaly
detection method using Pareto depth analysis PDA uses the concept of
Pareto optimality to detect anomalies under multiple criteria without having to
run an algorithm multiple times with different choices of weights The proposed
PDA approach scales linearly in the number of criteria and is provably better than
linear combinations of the criteria
Introduction
Anomaly detection is an important problem that has been studied in a variety of areas and used in diverse applications including intrusion detection fraud detection and image processing Many
methods for anomaly detection have been developed using both parametric and non-parametric approaches Non-parametric approaches typically involve the calculation of dissimilarities between
data samples For complex high-dimensional data multiple dissimilarity measures corresponding
to different criteria may be required to detect certain types of anomalies For example consider the
problem of detecting anomalous object trajectories in video sequences Multiple criteria such as
dissimilarity in object speeds or trajectory shapes can be used to detect a greater range of anomalies
than any single criterion In order to perform anomaly detection using these multiple criteria one
could first combine the dissimilarities using a linear combination However in many applications
the importance of the criteria are not known in advance It is difficult to determine how much weight
to assign to each dissimilarity measure so one may have to choose multiple weights using for example a grid search Furthermore when the weights are changed the anomaly detection algorithm
needs to be re-executed using the new weights
In this paper we propose a novel non-parametric multi-criteria anomaly detection approach using
Pareto depth analysis PDA uses the concept of Pareto optimality to detect anomalies without
having to choose weights for different criteria Pareto optimality is the typical method for defining
optimality when there may be multiple conflicting criteria for comparing items An item is said to
be Pareto-optimal if there does not exist another item that is better or equal in all of the criteria An
item that is Pareto-optimal is optimal in the usual sense under some combination not necessarily
linear of the criteria Hence PDA is able to detect anomalies under multiple combinations of the
criteria without explicitly forming these combinations
Figure Left Illustrative example with training samples blue and test samples red circle
and triangle in R2 Center Dyads for the training samples black dots along with first Pareto
fronts green lines under two criteria and The Pareto fronts induce a partial ordering on
the set of dyads Dyads associated with the test sample marked by the red circle concentrate around
shallow fronts near the lower left of the figure Right Dyads associated with the test sample
marked by the red triangle concentrate around deep fronts
The PDA approach involves creating dyads corresponding to dissimilarities between pairs of data
samples under all of the dissimilarity measures Sets of Pareto-optimal dyads called Pareto fronts
are then computed The first Pareto front depth one is the set of non-dominated dyads The second
Pareto front depth two is obtained by removing these non-dominated dyads peeling off the
first front and recomputing the first Pareto front of those remaining This process continues until
no dyads remain In this way each dyad is assigned to a Pareto front at some depth for
illustration Nominal and anomalous samples are located near different Pareto front depths thus
computing the front depths of the dyads corresponding to a test sample can discriminate between
nominal and anomalous samples The proposed PDA approach scales linearly in the number of criteria which is a significant improvement compared to selecting multiple weights via a grid search
which scales exponentially in the number of criteria Under assumptions that the multi-criteria dyads
can be modeled as a realizations from a smooth K-dimensional density we provide a mathematical
analysis of the behavior of the first Pareto front This analysis shows in a precise sense that PDA
can outperform a test that uses a linear combination of the criteria Furthermore this theoretical prediction is experimentally validated by comparing PDA to several state-of-the-art anomaly detection
algorithms in two experiments involving both synthetic and real data sets
The rest of this paper is organized as follows We discuss related work in Section In Section we
provide an introduction to Pareto fronts and present a theoretical analysis of the properties of the first
Pareto front Section relates Pareto fronts to the multi-criteria anomaly detection problem which
leads to the PDA anomaly detection algorithm Finally we present two experiments in Section to
evaluate the performance of PDA.
Related work
Several machine learning methods utilizing Pareto optimality have previously been proposed an
overview can be found in These methods typically formulate machine learning problems as
multi-objective optimization problems where finding even the first Pareto front is quite difficult
These methods differ from our use of Pareto optimality because we consider multiple Pareto fronts
created from a finite set of items so we do not need to employ sophisticated methods in order to find
these fronts
Hero and Fleury introduced a method for gene ranking using Pareto fronts that is related to our
approach The method ranks genes in order of interest to a biologist by creating Pareto fronts of
the data samples the genes In this paper we consider Pareto fronts of dyads which correspond
to dissimilarities between pairs of data samples rather than the samples themselves and use the
distribution of dyads in Pareto fronts to perform multi-criteria anomaly detection rather than ranking
Another related area is multi-view learning which involves learning from data represented by
multiple sets of features commonly referred to as views In such case training in one view helps to
improve learning in another view The problem of view disagreement where samples take different
classes in different views has recently been investigated The views are similar to criteria in
our problem setting However in our setting different criteria may be orthogonal and could even
give contradictory information hence there may be severe view disagreement Thus training in one
view could actually worsen performance in another view so the problem we consider differs from
multi-view learning A similar area is that of multiple kernel learning which is typically applied
to supervised learning problems unlike the unsupervised anomaly detection setting we consider
Finally many other anomaly detection methods have previously been proposed Hodge and Austin
and Chandola both provide extensive surveys of different anomaly detection methods
and applications Nearest neighbor-based methods are closely related to the proposed PDA approach Byers and Raftery proposed to use the distance between a sample and its kth-nearest
neighbor as the anomaly score for the sample similarly Angiulli and Pizzuti and Eskin
proposed to the use the sum of the distances between a sample and its nearest neighbors
Breunig used an anomaly score based on the local density of the nearest neighbors
of a sample Hero and Sricharan and Hero introduced non-parametric adaptive anomaly
detection methods using geometric entropy minimization based on random k-point minimal spanning trees and bipartite k-nearest neighbor graphs respectively Zhao and Saligrama
proposed an anomaly detection algorithm k-LPE using local p-value estimation LPE based on a
k-NN graph These k-NN anomaly detection schemes only depend on the data through the pairs of
data points dyads that define the edges in the k-NN graphs
All of the aforementioned methods are designed for single-criteria anomaly detection In the multicriteria setting the single-criteria algorithms must be executed multiple times with different weights
unlike the PDA anomaly detection algorithm that we propose in Section
Pareto depth analysis
The PDA method proposed in this paper utilizes the notion of Pareto optimality which has been
studied in many application areas in economics computer science and the social sciences among
others We introduce Pareto optimality and define the notion of a Pareto front
Consider the following problem given items denoted by the set and criteria for evaluating
each item denoted by functions f1 fK select that minimizes fK In
most settings it is not possible to identify a single item that simultaneously minimizes fi
for all K}. A minimizer can be found by combining the criteria using a linear
combination of the fi and finding the minimum of the combination Different choices of nonnegative weights in the linear combination could result in different minimizers a set of items that
are minimizers under some linear combination can then be created by using a grid search over the
weights for example
A more powerful approach involves finding the set of Pareto-optimal items An item is said to
strictly dominate another item if is no greater than in each criterion and is less than
in at least one criterion This relation can be written as if fi fi for each
and fi fi for some The set of Pareto-optimal items called the Pareto front is the set
of items in that are not strictly dominated by another item in S. It contains all of the minimizers
that are found using linear combinations but also includes other items that cannot be found by linear
combinations Denote the Pareto front by F1 which we call the first Pareto front The second Pareto
front can be constructed by finding items that are not strictly dominated by any of the remaining
items which are members of the set F1 More generally define the ith Pareto front by
Fi Pareto front of the set
Fj
For convenience we say that a Pareto front Fi is deeper than Fj if
Mathematical properties of Pareto fronts
The distribution of the number of points on the first Pareto front was first studied by BarndorffNielsen and Sobel in their seminal work The problem has garnered much attention since for a
survey of recent results see We will be concerned here with properties of the first Pareto front
that are relevant to the PDA anomaly detection algorithm and thus have not yet been considered in
the literature Let Y1 Yn be independent and identically distributed on Rd with density
function Rd R. For a measurable set A Rd we denote by FA the points on the first Pareto
front of Y1 Yn that belong to A. For simplicity we will denote F1 by and use for the
cardinality of F. In the general Pareto framework the points Y1 Yn are the images in Rd of
feasible solutions to some optimization problem under a vector of objective functions of length
In the context of this paper each point Yl corresponds to a dyad Dij which we define in Section
and is the number of criteria A common approach in multi-objective optimization is linear
scalarization which constructs a new single criterion as a convex combination of the criteria
It is well-known and easy to see
that linear scalarization will only identify Pareto points on the
boundary of the convex hull of x?F Rd where Rd Rd
Although this is a common motivation for Pareto methods there are to the best of our knowledge
no results in the literature regarding how many points on the Pareto front are missed by scalarization
We present such a result here We define
argmin
Sn Yn
x?Sn
The subset contains all Pareto-optimal points that can be obtained by some selection of
weights for linear scalarization We aim to study how large can get compared to in expectation
In the context of this paper if some Pareto-optimal points are not identified then the anomaly
score defined in section will be artificially inflated making it more likely that a non-anomalous
sample will be rejected Hence the size of is a measure of how much the anomaly score is
inflated and the degree to which Pareto methods will outperform linear scalarization
Pareto points in are a result of non-convexities in the Pareto front We study two kinds of
non-convexities those induced by the geometry of the domain of Y1 Yn and those induced by
randomness We first consider the geometry of the domain Let Rd be bounded and open with
a smooth boundary and suppose the density vanishes outside of For a point we
denote by the unit inward normal to For define Th by
Th Given it is not hard to see that all Pareto-optimal points
will almost surely lie in for large enough provided the density is strictly positive on
Hence it is enough to study the asymptotics for E|FTh for and
Theorem Let with inf Let be open and connected such that
inf
z?T
and
for T.
Then for sufficiently small we have
E|FTh
as
where
dz
The proof of Theorem is postponed to Section of the supplementary material Theorem shows
asymptotically how many Pareto points are contributed on average by the segment The
number of points contributed depends only on the geometry of through the direction of its normal
vector and is otherwise independent of the convexity of Hence by using Pareto methods we
will identify significantly more Pareto-optimal points than linear scalarization when the geometry
of includes non-convex regions For example if is non-convex left panel of
Figure and satisfies the hypotheses of Theorem then for large enough all Pareto points in
a neighborhood of will be unattainable by scalarization Quantitatively if on then
E|F O(n as where and
is the dimensional Hausdorff measure of It has recently come to our attention that Theorem
appears in a more general form in an unpublished manuscript of Baryshnikov and Yukich
We now study non-convexities in the Pareto front which occur due to inherent randomness in the
samples We show that even in the case where is convex there are still numerous small-scale
non-convexities in the Pareto front that can only be detected by Pareto methods We illustrate this in
the case of the Pareto box problem for
Figure Left Non-convexities in the Pareto front induced by the geometry of the domain Theorem Right Non-convexities due to randomness in the samples Theorem In each case the
larger points are Pareto-optimal and the large black points cannot be obtained by scalarization
Theorem Let Y1 Yn be independent and uniformly distributed on Then
as
The proof of Theorem is also postponed to Section of the supplementary material A proof that
as can be found in Hence Theorem shows that asymptotically
and in expectation only between and 65 of the Pareto-optimal points can be obtained by linear
scalarization in the Pareto box problem Experimentally we have observed that the true fraction of
points is close to This means that at least and likely more of the Pareto points can only be
obtained via Pareto methods even when is convex Figure gives an example of the sets and
from the two theorems
Multi-criteria anomaly detection
Assume that a training set XN XN of nominal data samples is available Given a test
sample the objective of anomaly detection is to declare to be an anomaly if is significantly
different from samples in XN Suppose that different evaluation criteria are given Each criterion is associated with a measure for computing dissimilarities Denote the dissimilarity between
and Xj computed using the measure corresponding to the lth criterion by dl
We define a dyad by Dij dK RK
Each dyad
corresponds
to
a
connection
between
samples
and Xj Therefore there are in
ij
total N2 different dyads For convenience denote the set of all dyads by and the space of all
dyads RK
by D. By the definition of strict dominance in Section a dyad Dij strictly dominates
another dyad Di if dl dl for all and dl dl for some
The first Pareto front F1 corresponds to the set of dyads from that are not strictly dominated by
any other dyads from D. The second Pareto front F2 corresponds to the set of dyads from F1
that are not strictly dominated by any other dyads from F1 and so on as defined in Section
Recall that we refer to Fi as a deeper front than Fj if
Pareto fronts of dyads
For each sample Xn there are dyads corresponding to its connections with the other
samples Define the set of dyads associated with Xn by Dn If most dyads in Dn are located
at shallow Pareto fronts then the dissimilarities between Xn and the other samples are small
under some combination of the criteria Thus Xn is likely to be a nominal sample This is the basic
idea of the proposed multi-criteria anomaly detection method using PDA.
We construct Pareto fronts F1 FM of the dyads from the training set where the total number
of fronts is the required number of fronts such that each dyad is a member of a front When a test
sample is obtained we create new dyads corresponding to connections between and training
samples as illustrated in Figure Similar to many other anomaly detection methods we connect
each test sample to its nearest neighbors could be different for each criterion so we denote ki
PK
as the choice of for criterion We create ki new dyads which we denote by the set
Algorithm PDA anomaly detection algorithm
Training phase
for do
Calculate pairwise dissimilarities dl between all training samples and Xj
Create dyads Dij dK for all training samples
Construct Pareto fronts on set of all dyads until each dyad is in a front
Testing phase
nb empty list
for do
Calculate dissimilarities between test sample and all training samples in criterion
nbl kl nearest neighbors of
nb nbl append neighbors to list
Create new dyads Dinew between and training samples in nb
for do
Calculate depth ei of Dinew
Ps
Declare an anomaly if ei
Dnew D1new D2new Dsnew corresponding to the connections between and the union of the
ki nearest neighbors in each criterion In other words we create a dyad between and Xj if Xj
is among the ki nearest neighbors1 of in any criterion We say that Dinew is below a front Fl if
Dinew Dl for some Dl Fl Dinew strictly dominates at least a single dyad in Fl Define the
depth of Dinew by
ei min{l Dinew is below Fl
Therefore if ei is large then Dinew will be near deep fronts and the distance between and the
corresponding training sample is large under all combinations of the criteria If ei is small then
Dinew will be near shallow fronts so the distance between and the corresponding training sample
is small under some combination of the criteria
Anomaly detection using depths of dyads
In k-NN based anomaly detection algorithms such as those mentioned in Section the anomaly
score is a function of the nearest neighbors to a test sample With multiple criteria one could define an anomaly score by scalarization From the probabilistic properties of Pareto fronts discussed
in Section we know that Pareto methods identify more Pareto-optimal points than linear scalarization methods and significantly more Pareto-optimal points than a single weight for scalarization2
This motivates us to develop a multi-criteria anomaly score using Pareto fronts We start with the
observation from Figure that dyads corresponding to a nominal test sample are typically located
near shallower fronts than dyads corresponding to an anomalous test sample Each test sample is
associated with new dyads where the ith dyad Dinew has depth ei For each test sample we
define the anomaly score to be the mean of the ei which corresponds to the average depth
of the dyads associated with Thus the anomaly score can be easily computed and compared to
the decision threshold using the test
H1
ei
H0
Pseudocode for the PDA anomaly detector is shown in Algorithm In Section of the supplementary material we provide details of the implementation as well as an analysis of the time complexity
and a heuristic for choosing the ki that performs well in practice Both the training time and the
If a training sample is one of the ki nearest neighbors in multiple criteria then multiple copies of the dyad
corresponding to the connection between the test sample and the training sample are created
Theorems and require samples but dyads are not independent However there are O(N dyads
and each dyad is only dependent on O(N other dyads This suggests that the theorems should also hold for the
non-i.i.d dyads as well and it is supported by experimental results presented in Section of the supplementary
material
Table AUC comparison of different methods for both experiments Best AUC is shown in bold
PDA does not require selecting weights so it has a single AUC. The median and best AUCs over all
choices of weights selected by grid search are shown for the other four methods PDA outperforms
all of the other methods even for the best weights which are not known in advance
Four-criteria simulation standard error
Method
PDA
k-NN
k-NN sum
k-LPE
LOF
Pedestrian trajectories
AUC by weight
Median
Best
Method
PDA
k-NN
k-NN sum
k-LPE
LOF
AUC by weight
Median Best
time required to test a new sample using PDA are linear in the number of criteria K. To handle
multiple criteria other anomaly detection methods such as the ones mentioned in Section need
to be re-executed multiple times using different non-negative linear combinations of the criteria If a grid search is used for selection of the weights in the linear combination then the required
computation time would be exponential in K. Such an approach presents a computational problem
unless is very small Since PDA scales linearly with it does not encounter this problem
Experiments
We compare the PDA method with four other nearest neighbor-based single-criterion anomaly detection algorithms mentioned in Section For these methods we use linear combinations of the
criteria with different weights selected by grid search to compare performance with PDA.
Simulated data with four criteria
First we present an experiment on a simulated data set The nominal distribution is given by the
uniform distribution on the hypercube The anomalous samples are located just outside of
this hypercube There are four classes of anomalous distributions Each class differs from the
nominal distribution in one of the four dimensions the distribution in the anomalous dimension is
uniform on We draw training samples from the nominal distribution followed by
test samples from a mixture of the nominal and anomalous distributions with a probability of
selecting any particular anomalous distribution The four criteria for this experiment correspond to
the squared differences in each dimension If the criteria are combined using linear combinations
the combined dissimilarity measure reduces to weighted squared Euclidean distance
The different methods are evaluated using the receiver operating characteristic ROC curve and
the area under the curve The mean AUCs with standard errors over simulation runs
are shown in Table A grid of six points between and in each criterion corresponding to
64 different sets of weights is used to select linear combinations for the single-criterion
methods Note that PDA is the best performer outperforming even the best linear combination
Pedestrian trajectories
We now present an experiment on a real data set that contains thousands of pedestrians trajectories
in an open area monitored by a video camera Each trajectory is approximated by a cubic spline
curve with seven control points We represent a trajectory with time samples by
y1 y2 yl
where denote a pedestrian?s position at time step
Shape dissimilarity
True positive rate
PDA method
k?LPE with best AUC weight
k?LPE with worst AUC weight
Attainable region of k?LPE
False positive rate
Walking speed dissimilarity
Figure Left ROC curves for PDA and attainable region for k-LPE over choices of weights
PDA outperforms k-LPE even under the best choice of weights Right A subset of the dyads for the
training samples along with the first Pareto fronts The fronts are highly non-convex partially
explaining the superior performance of PDA.
We use two criteria for computing the dissimilarity between trajectories The first criterion is to
compute the dissimilarity in walking speed We compute the instantaneous speed at all time steps
along
each trajectory by finite differencing the speed of trajectory at time step is given
by A histogram of speeds for each trajectory is obtained in this
manner We take the dissimilarity between two trajectories to be the squared Euclidean distance
between their speed histograms The second criterion is to compute the dissimilarity in shape For
each trajectory we select points uniformly positioned along the trajectory The dissimilarity
between two trajectories and is then given by the sum of squared Euclidean distances between
the positions of and over all points
The training sample for this experiment consists of trajectories and the test sample consists of
trajectories Table shows the performance of PDA as compared to the other algorithms
using uniformly spaced weights for linear combinations Notice that PDA has higher AUC than
the other methods under all choices of weights for the two criteria For a more detailed comparison
the ROC curve for PDA and the attainable region for k-LPE the region between the ROC curves
corresponding to weights resulting in the best and worst AUCs is shown in Figure along with
the first Pareto fronts for PDA. k-LPE performs slightly better at low false positive rate when
the best weights are used but PDA performs better in all other situations resulting in higher AUC.
Additional discussion on this experiment can be found in Section of the supplementary material
Conclusion
In this paper we proposed a new multi-criteria anomaly detection method The proposed method
uses Pareto depth analysis to compute the anomaly score of a test sample by examining the Pareto
front depths of dyads corresponding to the test sample Dyads corresponding to an anomalous
sample tended to be located at deeper fronts compared to dyads corresponding to a nominal sample
Instead of choosing a specific weighting or performing a grid search on the weights for different
dissimilarity measures the proposed method can efficiently detect anomalies in a manner that scales
linearly in the number of criteria We also provided a theorem establishing that the Pareto approach
is asymptotically better than using linear combinations of criteria Numerical studies validated our
theoretical predictions of PDA?s performance advantages on simulated and real data
Acknowledgments
We thank Zhaoshi Meng for his assistance in labeling the pedestrian trajectories We also thank
Daniel DeWoskin for suggesting a fast algorithm for computing Pareto fronts in two criteria This
work was supported in part by ARO grant

<<----------------------------------------------------------------------------------------------------------------------->>

title: 3723-anomaly-detection-with-score-functions-based-on-nearest-neighbor-graphs.pdf

Anomaly Detection with Score functions based on
Nearest Neighbor Graphs
Manqi Zhao
ECE Dept
Boston University
Boston MA
mqzhao@bu.edu
Venkatesh Saligrama
ECE Dept
Boston University
Boston MA
srv@bu.edu
Abstract
We propose a novel non-parametric adaptive anomaly detection algorithm for high
dimensional data based on score functions derived from nearest neighbor graphs
on n-point nominal data Anomalies are declared whenever the score of a test
sample falls below which is supposed to be the desired false alarm level The
resulting anomaly detector is shown to be asymptotically optimal in that it is uniformly most powerful for the specified false alarm level for the case when
the anomaly density is a mixture of the nominal and a known density Our algorithm is computationally efficient being linear in dimension and quadratic in
data size It does not require choosing complicated tuning parameters or function
approximation classes and it can adapt to local structure such as local change in
dimensionality We demonstrate the algorithm on both artificial and real data sets
in high dimensional feature spaces
Introduction
Anomaly detection involves detecting statistically significant deviations of test data from nominal
distribution In typical applications the nominal distribution is unknown and generally cannot be
reliably estimated from nominal training data due to a combination of factors such as limited data
size and high dimensionality
We propose an adaptive non-parametric method for anomaly detection based on score functions that
maps data samples to the interval Our score function is derived from a K-nearest neighbor
graph K-NNG on n-point nominal data Anomaly is declared whenever the score of a test sample
falls below the desired false alarm error The efficacy of our method rests upon its close connection to multivariate p-values In statistical hypothesis testing p-value is any transformation of the
feature space to the interval that induces a uniform distribution on the nominal data When test
samples with p-values smaller than are declared as anomalies false alarm error is less than
We develop a novel notion of p-values based on measures of level sets of likelihood ratio functions
Our notion provides a characterization of the optimal anomaly detector in that it is uniformly most
powerful for a specified false alarm level for the case when the anomaly density is a mixture of the
nominal and a known density We show that our score function is asymptotically consistent namely
it converges to our multivariate p-value as data length approaches infinity
Anomaly detection has been extensively studied It is also referred to as novelty detection
outlier detection one-class classification and single-class classification in the literature Approaches to anomaly detection can be grouped into several categories In parametric
approaches the nominal densities are assumed to come from a parameterized family and generalized likelihood ratio tests are used for detecting deviations from nominal It is difficult to use
parametric approaches when the distribution is unknown and data is limited A K-nearest neighbor
anomaly detection approach is presented in There an anomaly is declared whenever
the distance to the K-th nearest neighbor of the test sample falls outside a threshold In comparison
our anomaly detector utilizes the global information available from the entire K-NN graph to detect
deviations from the nominal In addition it has provable optimality properties Learning theoretic
approaches attempt to find decision regions based on nominal data that separate nominal instances
from their outliers These include one-class SVM of Sch?olkopf where the basic idea
is to map the training data into the kernel space and to separate them from the origin with maximum margin Other algorithms along this line of research include support vector data description
linear programming approach and single class minimax probability machine While
these approaches provide impressive computationally efficient solutions on real data it is generally
difficult to precisely relate tuning parameter choices to desired false alarm probability
Scott and Nowak derive decision regions based on minimum volume sets which does
provide Type I and Type error control They approximate appropriate function classes level
sets of the unknown nominal multivariate density from training samples Related work by Hero
based on geometric entropic minimization GEM detects outliers by comparing test samples
to the most concentrated subset of points in the training sample This most concentrated set is the
K-point minimum spanning tree(MST for n-point nominal data and converges asymptotically to
the minimum entropy set which is also the MV set Nevertheless computing K-MST for n-point
data is generally intractable To overcome these computational limitations proposes heuristic
greedy algorithms based on leave-one out K-NN graph which while inspired by K-MST algorithm
is no longer provably optimal Our approach is related to these latter techniques namely MV sets
of and GEM approach of We develop score functions on K-NNG which turn out to be the
empirical estimates of the volume of the MV sets containing the test point The volume which is a
real number is a sufficient statistic for ensuring optimal guarantees In this way we avoid explicit
high-dimensional level set computation Yet our algorithms lead to statistically optimal solutions
with the ability to control false alarm and miss error probabilities
The main features of our anomaly detector are summarized Like our algorithm scales
linearly with dimension and quadratic with data size and can be applied to high dimensional feature
spaces Like our algorithm is provably optimal in that it is uniformly most powerful for
the specified false alarm level for the case that the anomaly density is a mixture of the nominal
and any other density not necessarily uniform We do not require assumptions of linearity
smoothness continuity of the densities or the convexity of the level sets Furthermore our algorithm
adapts to the inherent manifold structure or local dimensionality of the nominal density Like
and unlike other learning theoretic approaches such as we do not require choosing complex
tuning parameters or function approximation classes
Anomaly Detection Algorithm Score functions based on K-NNG
In this section we present our basic algorithm devoid of any statistical context Statistical analysis
appears in Section Let be the nominal training set of size belonging to
the unit cube For notational convenience we use and interchangeably to denote a test
point Our task is to declare whether the test point is consistent with nominal data or deviates from
the nominal data If the test point is an anomaly it is assumed to come from a mixture of nominal
distribution underlying the training data and another known density Section
Let be a distance function denoting the distance between any two points For
simplicity we denote the distances by dij d(xi In the simplest case we assume the distance
function to be Euclidean However we also consider geodesic distances to exploit the underlying manifold structure The geodesic distance is defined as the shortest distance on the manifold
The Geodesic Learning algorithm a subroutine in Isomap can be used to efficiently and
consistently estimate the geodesic distances In addition by means of selective weighting of different coordinates note that the distance function could also account for pronounced changes in local
dimensionality This can be accomplished for instance through Mahalanobis distances or as a by
product of local linear embedding However we skip these details here and assume that a
suitable distance metric is chosen
Once a distance function is defined our next step is to form a nearest neighbor graph K-NNG or
alternatively an neighbor graph K-NNG is formed by connecting each to the closest
points xiK in We then sort the nearest distances for each in increasing
order di,i1 di,iK and denote RS di,iK that is the distance from to its K-th
nearest neighbor We construct where and are connected if and only if dij In this
case we define NS as the degree of point in the
For the simple case when the anomalous density is an arbitrary mixture of nominal and uniform
density1 we consider the following two score functions associated with the two graphs K-NNG and
NNG respectively The score functions map the test data to the interval
K-LPE p?K
1X
I{RS
1X
I{NS
where is the indicator function
Finally given a pre-defined significance level we declare to be anomalous if
p?K We call this algorithm Localized p-value Estimation LPE algorithm This
choice is motivated by its close connection to multivariate p-values(see Section
The score function K-LPE measures the relative concentration of point compared to
the training set Section establishes that the scores for nominally generated data is asymptotically
uniformly distributed in Scores for anomalous data are clustered around Hence when scores
below level are declared as anomalous the false alarm error is smaller than asymptotically since
the integral of a uniform distribution from to is
anomaly detection via K?LPE
empirical distribution of the scoring function K?LPE
nominal data
anomaly data
empirical density
Bivariate Gaussian mixture distribution
level set at
labeled as anomaly
labeled as nominal
value of K?LPE
Figure Left Level sets of the nominal bivariate Gaussian mixture distribution used to illustrate the KLPE algorithm Middle Results of K-LPE with and Euclidean distance metric for test
points drawn from a equal mixture of 2D uniform and the nominal bivariate distributions Scores for the test
points are based on nominal training samples Scores falling below a threshold level are declared as
anomalies The dotted contour corresponds to the exact bivariate Gaussian density level set at level
Right The empirical distribution of the test point scores associated with the bivariate Gaussian appear to be
uniform while scores for the test points drawn from 2D uniform distribution cluster around zero
Figure illustrates the use of K-LPE algorithm for anomaly detection when the nominal data is a
2D Gaussian mixture The middle panel of figure shows the detection results based on K-LPE are
consistent with the theoretical contour for significance level The right panel of figure
shows the empirical distribution derived from the kernel density estimation of the score function
K-LPE for the nominal solid blue and the anomaly dashed red data We can see that the curve for
the nominal data is approximately uniform in the interval and the curve for the anomaly data
has a peak at Therefore choosing the threshold will approximately control the Type I
error within and minimize the Type error We also take note of the inherent robustness of our
algorithm As seen from the figure right small changes in lead to small changes in actual false
alarm and miss levels
Pn
When the mixing density is not uniform but say f1 the score functions must be modified to
I
NS
NS
Pn
and
for the two graphs K-NNG and NNG respectively
I
To summarize the above discussion our LPE algorithm has three steps
Inputs Significance level distance metric Euclidean geodesic weighted
Score computation Construct K-NNG based on dij and compute the score function
K-LPE from Equation LPE from Equation
Make Decision Declare to be anomalous if and only if p?K
Computational Complexity To compute each pairwise distance requires operations and
operations for all the nodes in the training set In the worst-case computing the K-NN graph
for small and the functions RS NS requires operations over all the nodes in the
training data Finally computing the score for each test data requires O(nd+n operations(given that
RS NS have already been computed
Remark LPE is fundamentally different from non-parametric density estimation or level set estimation schemes MV-set These approaches involve explicit estimation of high dimensional
quantities and thus hard to apply in high dimensional problems By computing scores for each test
sample we avoid high-dimensional computation Furthermore as we will see in the following section the scores are estimates of multivariate p-values These turn out to be sufficient statistics for
optimal anomaly detection
Theory Consistency of LPE
A statistical framework for the anomaly detection problem is presented in this section We establish
that anomaly detection is equivalent to thresholding p-values for multivariate data We will then
show that the score functions developed in the previous section is an asymptotically consistent estimator of the p-values Consequently it will follow that the strategy of declaring an anomaly when a
test sample has a low score is asymptotically optimal
Assume that the data belongs to the d-dimensional unit cube and the nominal data is sampled from a multivariate density f0 supported on the d-dimensional unit cube Anomaly
detection can be formulated as a composite hypothesis testing problem Suppose test data comes
from a mixture distribution namely where f1 is a mixing density
supported on Anomaly detection involves testing the nominal hypotheses H0 versus
the alternative anomaly H1 The goal is to maximize the detection power subject to false
alarm level namely P(declare H1 H0
Definition Let P0 be the nominal probability measure and f1 be P0 measurable Suppose the
likelihood ratio f1 does not have non-zero flat spots on any open ball in Define
the p-value of a data point as
f1
f1
P0
f0
f0
Note that the definition naturally accounts for singularities which may arise if the support of f0
is a lower dimensional manifold In this case we encounter f1 f0 and the p-value
Here anomaly is always declared(low score
The above formula can be thought of as a mapping of Furthermore the distribution of
under H0 is uniform on However as noted in the introduction there are other such transformations To build intuition about the above transformation and its utility consider the following
example When the mixing density is uniform namely f1 where is uniform over
note that is a density level set at level It is well known
that such a density level set is equivalent to a minimum volume set of level The minimum volume
set at level is known to be the uniformly most powerful decision region for testing H0
versus the alternative H1 The generalization to arbitrary f1 is described next
Theorem The uniformly most powerful test for testing H0 versus the alternative
anomaly H1 at a prescribed level of significance P(declare H1 H0 is
H1
H0 otherwise
Proof We provide the main idea for the proof First measure theoretic arguments are used to
establish as a random variable over under both nominal and anomalous distributions
Next when f0 distributed with nominal density it follows that the random variable
When with the random variable where
is a monotonically decreasing PDF supported on Consequently the uniformly most powerful
test for a significance level is to declare p-values smaller than as anomalies
Next we derive the relationship between the p-values and our score function By definition RS
and RS are correlated because the neighborhood of and might overlap We modify our
algorithm to simplify our analysis We assume is odd say and can be written as 2m
We divide training set into two parts
S1 S2 xm x2m
We modify LPE to
I{NS2 K-LPE to
I
Now
and
are
independent
S2
S1
Furthermore we assume f0 satisfies the following two smoothness conditions
the Hessian matrix of f0 is always dominated by a matrix with largest eigenvalue
and max
In the support of f0 its value is always lower bounded by some
We have the following theorem
Theorem Consider the setup above with the training data generated from f0
Let be an arbitrary test sample It follows that for a suitable choice and under the
above smoothness conditions
pK almost surely
For simplicity we limit ourselves to the case when f1 is uniform The proof of Theorem consists
of two steps
We show that the expectation ES1
Lemma This result is then exn
tended to K-LPE ES1
pK in Lemma
Next we show that p?K ES1
pK via concentration inequality Lemma
with probability at least
Lemma By picking 5d
lm ES1
um
where
lm P0
um P0
and
Proof We only prove the lower bound since the upper bound follows along similar lines By interchanging the expectation with the summation
ES1
ES1
I{NS2
Exi ES1 I{NS2
Ex1 NS1
where the last inequality follows from the symmetric structure of xm
Clearly the objective of the proof is to show PS1 NS1
Skipping technical details this can be accomplishedRin two steps Note that NS is a binomial
random variable with success probability f0 t)dt This relates PS1
NS1 to We relate to based on the function
smoothness condition The details of these two steps are shown in the below
Note that NS1 Binom(m By Chernoff bound of binomial distribution we have
2mq(x
PS1 mq(x1
that is NS1 is concentrated around mq(x1 This implies
PS1 NS1 I{NS
2mq(x
We choose will be specified later and reformulate equation as
PS1 NS1 I
NS
q(x
Vol(B1
mVol(B
Next we relate f0 t)dt to f0 via the Taylor?s expansion and the smoothness
condition of f0
f0 t)dt
f0
ktk2 dt
Vol(B
Vol(B
and then equation becomes
PS1 NS1 I
NS
f0
mVol(B
By applying the same steps to NS2 as equation Chernoff bound and equation Taylor?s
explansion we have with probability at least
Ex1 NS1
Finally by choosing 5d
f0
Px1 f0
and we prove the lemma
Lemma By picking with probability at least
lm ES1
pK um
Proof The proof is very similar to the proof to Lemma and we only give a brief outline here Now
the objective is to show PS1 RS1 The basic idea is to use
the result of Lemma To accomplish this we note that RS1 contains the events
or equivalently
By the tail probability of Binomial distribution the probability of the above two events converges to
exponentially fast if and By using the same two-step bounding
techniques developed in the proof to Lemma these two inequalities are implied by
and
Therefore if we choose we have with probability at least
PS1 RS1
Remark Lemma and Lemma were proved with specific choices for and K. However they
can be chosen in a range of values but will lead to different lower and upper bounds We will show
in Section via simulation that our LPE algorithm is generally robust to choice of parameter K.
Lemma Suppose and denote p?K
I{RS2 We have
c2
P0
pK p?K 2e
where is a constant and is defined as the minimal number of cones centered at the origin of angle
that cover Rd
Proof We can not apply Law of Large Number in this case because I{RS2 are correlated Instead we need to use the more generalized concentration-of-measure
inequality such
as MacDiarmid?s inequality[17 Denote xm
I{RS2 From
Corollary in
sup
xm
xm x0i K?d
Then the lemma directly follows from applying McDiarmid?s inequality
Theorem directly follows from the combination of Lemma and Lemma and a standard application of the first Borel-Cantelli lemma We have used Euclidean distance in Theorem When the
support of f0 lies on a lower dimensional manifold say d0 adopting the geodesic metric leads
to faster convergence It turns out that d0 replaces in the expression for in Lemma
Experiments
First to test the sensitivity of K-LPE to parameter changes we run K-LPE on the benchmark dataset Banana with varying from to We randomly pick points with label and
regard them as the nominal training data The test data comprises of points and
points ground truth and the algorithm is supposed to predict data as nominal and data
as anomalous Scores computed for test set using Equation is oblivious to true f1 density
labels Euclidean distance metric is adopted for this experiment
To control false alarm at level points with score smaller than are predicted as anomaly Empirical false alarm and true positives are computed from ground truth We vary to obtain the empirical
ROC curve The above procedure is followed for the rest of the experiments in this section As
shown in the LPE algorithm is insensitive to K. For comparison we plot the empirical ROC
curve of the one-class SVM of For our OC-SVM implementation for a fixed bandwidth we
obtain the empirical ROC curve by varying We then vary the bandwidth to obtain the best
terms of AUC ROC curve The optimal bandwidth turns out to be In LPE if we set
we get empirical A and for empirical A For OC-SVM we
are unaware of any natural way of picking and to control FA rate based on training data
Next we apply our K-LPE to the problem where the nominal and anomalous data are generated in
the following way
49
f0
f1
49
We call ROC curve corresponding to the optimal Bayesian classifier as the Clairvoyant ROC the
red dashed curve in Figure The other two curves are averaged over trials empirical ROC
curves via LPE. Here we set and or We see that for a relatively small
training set of size the average empirical ROC curve is very close to the clairvoyant ROC curve
Finally we ran LPE on three real-world datasets Wine Ionosphere[20 and MNIST US Postal
Service USPS database of handwritten digits If there are more than labels in the data set we
artificially regard points with one particular label as nominal and regard the points with other labels
as anomalous For example for the USPS dataset we regard instances of digit as nominal and
instances of digits as anomaly The data points are normalized to be within and we
2D Gaussian mixture
true positives
true positives
banana data set
ROC of LPE
ROC of LPE
ROC of LPE
ROC of LPE
ROC of LPE
ROC of LPE
ROC of
ROC of
Clairvoyant ROC
ROC of one?class SVM
false positives
SVM K-LPE for Banana Data
false positives
Clairvoyant K-LPE
Figure Empirical ROC curve of LPE on the banana dataset with with
vs the empirical ROC curve of one class SVM developed in Empirical ROC curves of
LPE algorithm vs clairvoyant ROC curve is given by Equation for and for or
true positive
true positive
true positive
use geodesic distance The ROC curves are shown in Figure The feature dimension of Wine
is 13 and we apply the LPE algorithm with and 39 The test set is a mixture of
nominal points and anomaly points The feature dimension of Ionosphere is 34 and we
apply the K-LPE algorithm with and The test set is a mixture of nominal points
and anomaly points The feature dimension of USPS is and we apply the K-LPE algorithm
with and The test set is a mixture of nominal points and 33 anomaly points
In USPS setting induces empirical false-positive and empirical false alarm rate
In contrast and A with for OC-SVM as reported in Practically we
find that K-LPE is more preferable to LPE and as a rule of thumb setting is generally
effective
false positive
Wine
false positive
Ionosphere
false positive
USPS
Figure ROC curves on real datasets via LPE Wine dataset with 13 39
Ionosphere dataset with 34 USPS dataset with
Conclusion
In this paper we proposed a novel non-parametric adaptive anomaly detection algorithm which leads
to a computationally efficient solution with provable optimality guarantees Our algorithm takes a
K-nearest neighbor graph as an input and produces a score for each test point Scores turn out to be
empirical estimates of the volume of minimum volume level sets containing the test point While
minimum volume level sets provide an optimal characterization for anomaly detection they are
high dimensional quantities and generally difficult to reliably compute in high dimensional feature
spaces Nevertheless a sufficient statistic for optimal tradeoff between false alarms and misses is
the volume of the MV set itself which is a real number By computing score functions we avoid
computing high dimensional quantities and still ensure optimal control of false alarms and misses
The computational cost of our algorithm scales linearly in dimension and quadratically in data size

<<----------------------------------------------------------------------------------------------------------------------->>

title: 4287-efficient-anomaly-detection-using-bipartite-k-nn-graphs.pdf

Efficient anomaly detection using
bipartite k-NN graphs
Kumar Sricharan
Department of EECS
University of Michigan
Ann Arbor MI
kksreddy@umich.edu
Alfred Hero III
Department of EECS
University of Michigan
Ann Arbor MI
hero@umich.edu
Abstract
Learning minimum volume sets of an underlying nominal distribution is a very effective approach to anomaly detection Several approaches to learning minimum
volume sets have been proposed in the literature including the K-point nearest
neighbor graph K-kNNG algorithm based on the geometric entropy minimization GEM principle The K-kNNG detector while possessing several desirable characteristics suffers from high computation complexity and in a
simpler heuristic approximation the leave-one-out kNNG L1O-kNNG was proposed In this paper we propose a novel bipartite k-nearest neighbor graph BPkNNG anomaly detection scheme for estimating minimum volume sets Our
bipartite estimator retains all the desirable theoretical properties of the K-kNNG
while being computationally simpler than the K-kNNG and the surrogate L1OkNNG detectors We show that BP-kNNG is asymptotically consistent in recovering the p-value of each test point Experimental results are given that illustrate
the superior performance of BP-kNNG as compared to the L1O-kNNG and other
state of the art anomaly detection schemes
Introduction
Given a training set of normal events the anomaly detection problem aims to identify unknown
anomalous events that deviate from the normal set This novelty detection problem arises in applications where failure to detect anomalous activity could lead to catastrophic outcomes for example
detection of faults in mission-critical systems quality control in manufacturing and medical diagnosis
Several approaches have been proposed for anomaly detection One class of algorithms assumes a
family of parametrically defined nominal distributions Examples include Hotelling?s test and the
Fisher F-test which are both based on a Gaussian distribution assumption The drawback of these
algorithms is model mismatch the supposed distribution need not be a correct representation of the
nominal data which can then lead to poor false alarm rates More recently several non-parametric
methods based on minimum volume set estimation have been proposed These methods aim to
find the minimum volume set that recovers a certain probability mass with respect to the unknown
probability density of the nominal events If a new event falls within the MV set it is classified as
normal and otherwise as anomalous
Estimation of minimum volume sets is a difficult problem especially for high dimensional data
There are two types of approaches to this problem transform the MV estimation problem to an
equivalent density level set estimation problem which requires estimation of the nominal density
and directly identify the minimal set using function approximation and non-parametric estimation Both types of approaches involve explicit approximation of high dimensional quant1
ities the multivariate density function in the first case and the boundary of the minimum volume
set in the second and are therefore not easily applied to high dimensional problems
The GEM principle developed by Hero for determining MV sets circumvents the above difficulties by using the asymptotic theory of random Euclidean graphs instead of function approximation However the GEM based K-kNNG anomaly detection scheme proposed in is computationally difficult To address this issue a surrogate L1O-kNNG anomaly detection scheme was proposed
in L1O-kNNG is computationally simpler than K-kNNG but loses some desirable properties of
the K-kNNG including asymptotic consistency as shown below
In this paper we use the GEM principle to develop a bipartite k-nearest neighbor graphbased anomaly detection algorithm BP-kNNG retains the desirable properties of the GEM principle
and as a result inherits the following features it is not restricted to linear or even convex decision
regions it is completely non-parametric iii it is optimal in that it converges to the uniformly
most powerful UMP test when the anomalies are drawn from a mixture of the nominal density and
the uniform density it does not require knowledge of anomalies in the training sample it is
asymptotically consistent in recovering the p-value of the test point and it produces estimated
p-values allowing for false positive rate control
K-LPE and RRS are anomaly detection methods which are also based on k-NN graphs BPkNNG differs from L1O-kNNG K-LPE and RRS in the following respects L1O-kNNG K-LPE
and RRS do not use bipartite graphs We will show that the bipartite nature of BP-kNNG results
in significant computational savings In addition the K-LPE and RRS test statistics involve only
the k-th nearest neighbor distance while the statistic in BP-kNNG like the L1O-kNNG involves
summation of the power weighted distance of all the edges in the k-NN graph This will result
in increased robustness to outliers in the training sample Finally we will show that the mean
square rate of convergence of p-values in BP-kNNG is faster as compared to the
convergence rate of K-LPE where is the size of the nominal training sample
and is the dimension of the data
The rest of this paper is organized as follows In Section we outline the statistical framework
for minimum volume set anomaly detection In Section we describe the GEM principle and the
K-kNNG and L1O-kNNG anomaly detection schemes proposed in Next in Section we
develop our bipartite k-NN graph BP-kNNG method for anomaly detection We show consistency
of the method and compare its computational complexity with that of the K-kNNG L1O-kNNG and
K-LPE algorithms In Section we show simulation results that illustrate the superior performance
of BP-kNNG over L1O-kNNG We also show that our method compares favorably to other state of
the art anomaly detection schemes when applied to real world data from the UCI repository We
conclude with a short discussion in Section
Statistical novelty detection
The problem setup is as follows We assume that a training sample XT of ddimensional vectors is available Given a new sample the objective is to declare to either be
a nominal event consistent with or an anomalous event which deviates from We seek to
find a functional and corresponding detection rule so that is declared to be nominal if
holds and anomalous otherwise The acceptance region is given by A
We seek to further constrain the choice of to allow as few false negatives as possible for a fixed
allowance of false positives
To formulate this problem we adopt the standard statistical framework for testing composite hypotheses We assume that the training sample is an i.i.d sample draw from an unknown ddimensional probability distribution on Let have density on The anomaly
detection problem can be formulated as testing the hypotheses f0 versus H1 f0
For a given we seek an acceptance region A that satisfies r(X A|H
This
requirement maintains the false positive rate at a level no greater than Let A A
x)dx denote the collection of acceptance regions of level The most suitable
A
acceptance region from the collection A would be the set which minimizes the false negative rate
Assume that the density is bounded above by some constant C. In this case the false negative rate
is bounded by where is the Lebesgue measure in Consider the relaxed problem of
minimizing the upper bound or equivalently the volume of A. The optimal acceptance
region with a maximum
false alarm rate is therefore given by the minimum volume set of level
A f0 x)dx
Define the minimum entropy
set of level to be min{H A f0 x)dx where
A x)dx is the R?enyi entropy of the density over the set A. It can be
shown that when is a Lebesgue density in the minimum volume set and the minimum entropy
set are equivalent and are identical Therefore the optimal decision rule for a given level
of false alarm is to declare an anomaly if
This decision rule has a strong optimality property when is Lebesgue continuous and has
no flat regions over its support this decision rule is a uniformly most powerful UMP test at level
for the null hypothesis that the test point has density equal to the nominal versus
the alternative hypothesis that where is the uniform density
over and Furthermore the power function is given by r(X
GEM principle
In this section we briefly review the geometric entropy minimization GEM principle method
for determining minimum entropy sets of level The GEM method directly estimates the critical region for detecting anomalies using minimum coverings of subsets of points in a nominal
training sample These coverings are obtained by constructing minimal graphs the k-minimal
spanning tree or the k-nearest neighbor graph covering a K-point subset that is a given proportion
of the training sample Points in the training sample that are not covered by the K-point minimal
graphs are identified as tail events
In particular let K,T denote one of the
point subsets of XT The k-nearest neighbors
of a point XK,T are the closest points to among XK,T Denote the
corresponding set of edges between and its k-NN by ei(k For any subset XK,T
define the total power weighted edge length of the k-NN graph on K,T with power weighting
as
LkN XK,T
eti
where tK are the indices of XK,T Define the K-kNNG
to be the K-point
graph
k-NN graph having minimal length min XT XT LkN XT,K over all subsets XK,T Denote
argmin LkN XK,T
the corresponding length minimizing subset of points by T,K
XT
minimal graph covering K,T
of size K. This graph can be viewed as
The K-kNNG thus specifies a
capturing the densest regions of If XT is an sample from a multivariate density and
converges to the minimum entropy set containing
if limK,T K/T then the set XK,T
a proportion of at least of the mass of where This set can be used to
perform anomaly detection
K-kNNG anomaly detection
Given a test sample denote the pooled sample XT and determine the K-kNNG
K,T
graph over Declare to be an anomaly if
and nominal otherwise When the
density f0 is Lebesgue continuous it follows from that as this anomaly detection
algorithm has false alarm rate that converges to K/T and power that converges to that of
the minimum volume set test of level An identical detection scheme based on the K-minimal
spanning tree has also been developed in
The K-kNNG anomaly detection scheme therefore offers a direct approach to detecting outliers
while bypassing the more difficult problems of density estimation and level set estimation in high dimensions However this
algorithm requires construction of k-nearest neighbor graphs k-minimal
spanning trees over
different subsets For each input test point the runtime of this algorithm
As a result the K-kNNG method is not well suited for anomaly detection
is therefore O(dK
for large sample sizes
L1O-kNNG
To address the computational problems of K-kNNG Hero proposed implementing the K-kNNG
for the simplest case The runtime of this algorithm for each input test point is O(dT
Clearly the L1O-kNNG is of much lower complexity that the K-kNNG scheme However the L1OkNNG detects anomalies at a fixed false alarm rate where is the training sample size
To detect anomalies at a higher false alarm rate one would have to subsample the training set
and only use training samples This destroys any hope for asymptotic consistency
of the L1O-kNNG
In the next section we propose a different GEM based algorithm that uses bipartite graphs The
algorithm has algorithm has a much faster runtime than the L1O-kNNG and unlike the L1O-kNNG
is asymptotically consistent and can operate at any specified alarm rate We describe our algorithm
below
BP-kNNG
Let XN XM be a partition of with card{XN
and card{XM
respectively As above let K,N denote one of the subsets of distinct points from
Define the bipartite k-NN graph on K,N XM to be the set of edges linking each XK,N
to its nearest neighbors in Define the total power weighted edge length of this bipartite
k-NN graph with power weighting and a fixed number of edges
corresponding to each vertex XK,N to be
Ls,k XK,N XM
eti
where tK are the indices of XK,N and eti eti are the k-NN edges in
the bipartite graph originating from ti XK,N Define the bipartite K-kNNG graph to be the one
having minimal weighted length min XN,K XN Ls,k XN,K XM over all
subsets XK,N Define
argmin Ls,k XK,N XM
the corresponding minimizing subset of points of K,N by XK,N
XK,N
Using the theory of partitioned k-NN graph entropy estimators it follows that as k/M
and for fixed the set K,N
converges to the minimum entropy set
containing a proportion of at least of the mass of where limK,N K/N and
This suggests using the bipartite k-NN graph to detect anomalies in the following way Given a
test point denote the pooled sample XN and determine the optimal bipartite
K,N
K-kNNG graph K,N
over XK,N XM Now declare to be an anomaly if
and nominal otherwise It is clear that by the GEM principle this algorithm detects false alarms at
a rate that converges to K/T and power that converges to that of the minimum volume set
test of level
We can equivalently determine K,N
as follows For each XN construct ds,k
For
each
test
point
define s,k
where
eX(k are the k-NN edges from to Now choose the points among
with the smallest of the edge lengths s,k XN ds,k Because of
the bipartite nature of the construction this is equivalent to choosing K,N
This leads to the
proposed BP-kNNG anomaly detection algorithm described by Algorithm
BP-kNNG p-value estimates
The p-value is a score between and that is associated with the likelihood that a given point
comes from a specified nominal distribution The BP-kNNG generates an estimate of the p-value
Algorithm Anomaly detection scheme using bipartite k-NN graphs
Input Training samples test samples false alarm rate
Training phase
a Create partition XN XM
Construct k-NN bipartite graph on partition
Compute k-NN lengths s,k for each XN ds,k
Test phase detect anomalous points
for each input test sample do
Compute k-NN length s,k
if
ds,k
XN
then
Declare to be anomalous
else
Declare to be non-anomalous
end if
end for
that is asymptotically consistent guaranteeing that the BP-kNNG detector is a consistent novelty
detector
Specifically for a given test point the
true p-value associated with a point in a minimum
volume set test is given by true f0 z)dz where f0 f0 and
f0 f0 ptrue is the minimal level at which would be rejected
The empirical p-value associated with the BP-kNNG is defined as
XN ds,k
pbp
Asymptotic consistency and optimal convergence rates
Here we prove that the BP-kNNG detector is asymptotically consistent by showing that for a fixed
number of edges bp ptrue as k/M In the process
we also obtain rates of convergence of this mean-squared error These rates depend on and
and result in the specification of an optimal number of neighbors and an optimal partition ratio
N/M that achieve the best trade-off between bias and variance of the p-value estimates bp
We assume that the density is bounded away from and and is continuous on its support
has no flat spots over its support set and iii has a finite number of modes Let denote the
expectation the density and denote the bias and variance operators Throughout this
section assume without loss of generality that XN XN and XN XT XM
Bias We first introduce the oracle p-value orac XN f0
and note that E[p orac ptrue The distance ei(l of a point XN to its l-th
nearest neighbor in is related to the bipartite l-nearest neighbor density estimate f?l
cd edi(l section where is the unit ball volume in dimensions Let
f?l
s(f
and
X0
We then have
B[pbp
E[pbp ptrue E[pbp porac(X0
ds,k
This bias will be non-zero when First we investigate
this condition when In this case for we need
Likewise when occurs
when
From the theory developed in for any fixed O(k/M with probability greater than This implies that
B[pbp
where the last step follows from our assumption that the density is continuous and has a finite
number of modes
Variance Define We can compute the variance
in a similar manner to the bias as follows for additional details please refer to the supplementary
material
V[pbp
Cov[b1 b2
b2
Consistency of p-values From and we obtain an asymptotic representation of the estimated p-value bp ptrue This implies that
pbp converges in mean square to true for a fixed number of edges as k/M
Optimal choice of parameters The optimal choice of to minimize the MSE is given by
For fixed to minimize MSE should then be chosen to be of the
order O(M which implies that The mean square convergence rate for
this optimal choice of and partition ratio N/M is given by O(T In comparison the
K-LPE method requires that grows with the sample size at rate The mean square
rate of convergence of the p-values in K-LPE is then given by O(T The rate of
convergence of the p-values is therefore faster in the case of BP-kNNG as compared to K-LPE
Comparison of run time complexity
Here we compare complexity of BP-kNNG with that ofK-kNNG
L1O-kNNG and K-LPE For a
single query point the runtime of K-kNNG is O(dK
while the complexity of the surrogate
L1O-kNN algorithm and the K-LPE is O(dT On the other hand the complexity of the proposed
BP-kNNG algorithm is dominated by the computation of for each XN and dk
which is O(dN O(dT o(dT
For the K-kNNG L1O-kNNG and K-LPE a new k-NN graph has to be constructed on
for every new query point On the other hand because of the bipartite construction of our k-NN
graph dk for each XN needs to be computed and stored only once For every new query
that comes in the cost to compute is only O(dM O(dT For a total of query points
the overall runtime complexity of our algorithm is therefore much smaller than the L1O-kNNG
LPE and K-kNNG anomaly
compared to O(dLT
detection schemes O(dT
O(dLT and O(dLK respectively
Simulation comparisons
We compare the L1O-kNNG and the bipartite K-kNNG schemes on a simulated data set The
training set contains realizations drawn from a 2-dimensional Gaussian density with mean
and diagonal covariance with identical component variances of The test set contains
realizations drawn from where is the uniform density on Samples from the
uniform distribution are classified to be anomalies The percentage of anomalies in the test set is
therefore
Observed
True positive rate
BP?kNNG
L10?kNNG
Clairvoyant
False positive rate
BP?kNNG
L10?kNNG
Desired
ROC curves for L1O-kNNG and BP-kNNG Comparison of observed false alarm rates for
The labeled clairvoyant curve is the ROC of the L1O-kNNG and BP-kNNG with the desired false
UMP anomaly detector
alarm rates
Figure Comparison of performance of L1O-kNNG and BP-kNNG
Data set
HTTP
Forest
Mulcross
SMTP
Shuttle
Sample size
Dimension
Anomaly class
attack
class vs class
clusters
attack
class vs class
Table Description of data used in anomaly detection experiments
The distribution has essential support on the unit square For
this simple case the minimum
volume set of level is a disk centered at the origin with radius The power of the
uniformly most powerful UMP test is
L1O-kNNG and BP-kNNG were implemented in Matlab on an GHz Intel processor with
GB of RAM. The value of was set to For the BP-kNNG we set and
In we compare the detection performance of L1O-kNNG and BP-kNNG
against the clairvoyant UMP detector in terms of the ROC. We note that the proposed BP-kNNG
is closer to the optimal UMP test as compared to the L1O-kNNG In we note the close
agreement between desired and observed false alarm rates for BP-kNNG Note that the L1O-kNNG
significantly underestimates its false alarm rate for higher levels of true false alarm In the case
of the L1O-kNNG it took an average of to test each instance for possible anomaly The
total run-time was therefore For the BP-kNNG for a single instance it took an
average of When all the instances were processed together the total run time was only
This significant savings in runtime is due to the fact that the bipartite graph does not have to be
constructed separately for each new test instance it suffices to construct it once on the entire data
set
Experimental comparisons
In this section we compare our algorithm to several other state of the art anomaly detection algorithms namely MassAD isolation forest iForest two distance-based methods
ORCA and K-LPE a density-based method LOF and the one-class support vector
machine All the methods are tested on the five largest data sets used in The
data characteristics are summarized in Table One of the anomaly data generators is Mulcross
and the other four are from the UCI repository Full details about the data can be found in
The comparison performance is evaluated in terms of averaged AUC area under ROC curve and
processing time total of training and test time Results for BP-kNNG are compared with results
for L1O-kNNG K-LPE MassAD iForest and ORCA in Table The results for MassAD iForest
and ORCA are reproduced from MassAD and iForest were implemented in Matlab and tested
on an AMD Opteron machine with a GHz processor and GB memory The results for ORCA
Data sets
HTTP
Forest
Mulcross
SMTP
Shuttle
BP
NA
NA
NA
NA
NA
K-LPE
NA
NA
NA
NA
NA
AUC
Mass
iF
ORCA
BP
Time secs
K-LPE
Mass
34
18
17
iF
79
75
26
ORCA
Table Comparison of anomaly detection schemes in terms of AUC and run-time for BP-kNNG
against L1O-kNNG K-LPE MassAD Mass iForest and ORCA When reporting
results for L1O-kNNG and K-LPE we report the processing time per test instance We are
unable to report the AUC for K-LPE and L1O-kNNG because of the large processing time We note
that BP-kNNG compares favorably in terms of AUC while also requiring the least run-time
Data sets
HTTP
Forest
Mulcross
SMTP
Shuttle
Desired false alarm
Table Comparison of desired and observed false alarm rates for BP-kNNG There is good agreement between the desired and observed rates
LOF and 1-SVM were conducted using the same experimental setting but on a faster GHz
machine We exclude the results for LOF and 1-SVM in table because MassAD iForest and
ORCA have been shown to outperform LOF and 1-SVM in
We implemented BP-kNNG L1O-kNNG and K-LPE in Matlab on an Intel GHz processor with
GB RAM. We note that this machine is comparable to the AMD Opteron machine with a GHz
processor We choose training samples and fix in all three cases For BP-kNNG
we fix and When reporting results for L1O-kNNG and K-LPE we report the
processing time per test instance We are unable to report the AUC for K-LPE because of the
large processing time and for L1O-kNNG because it cannot operate at high false alarm rates
From the results in Table we see that BP-kNNG performs comparably in terms of AUC to the
other algorithms while having the least processing time across all algorithms implemented on
different but comparable machines In addition BP-kNNG allows the specification of a threshold
for anomaly detection at a desired false alarm rate This is corroborated by the results in Table
where we see that the observed false alarm rates across the different data sets are close to the desired
false alarm rate
Conclusions
The geometric entropy minimization GEM principle was introduced in to extract minimal set
coverings that can be used to detect anomalies from a set of training samples In this paper we
propose a bipartite k-nearest neighbor graph BP-kNNG anomaly detection algorithm based on the
GEM principle BP-kNNG inherits the theoretical optimality properties of GEM methods including
consistency while being an order of magnitude faster than the methods proposed in
We compared BP-kNNG against state of the art anomaly detection algorithms and showed that BPkNNG compares favorably in terms of both ROC performance and computation time In addition
BP-kNNG enjoys several other advantages including the ability to detect anomalies at a desired false
alarm rate In BP-kNNG the p-values of each test point can also be easily computed making
BP-kNNG easily extendable to incorporating false discovery rate constraints

<<----------------------------------------------------------------------------------------------------------------------->>

title: 4299-group-anomaly-detection-using-flexible-genre-models.pdf

Group Anomaly Detection using Flexible Genre Models
Liang Xiong
Machine Learning Department
Carnegie Mellon University
lxiong@cs.cmu.edu
Barnab?as P?oczos
Robotics Institute
Carnegie Mellon University
bapoczos@cs.cmu.edu
Jeff Schneider
Robotics Institute
Carnegie Mellon University
schneide@cs.cmu.edu
Abstract
An important task in exploring and analyzing real-world data sets is to detect
unusual and interesting phenomena In this paper we study the group anomaly
detection problem Unlike traditional anomaly detection research that focuses on
data points our goal is to discover anomalous aggregated behaviors of groups of
points For this purpose we propose the Flexible Genre Model FGM is
designed to characterize data groups at both the point level and the group level so
as to detect various types of group anomalies We evaluate the effectiveness of
FGM on both synthetic and real data sets including images and turbulence data
and show that it is superior to existing approaches in detecting group anomalies
Introduction
Anomaly detection is a crucial problem in processing large-scale data sets when our goal is to
find rare or unusual events These events can either be outliers that should be ignored or novel
observations that could lead to new discoveries See for a recent survey of this field Traditional
research often focuses on individual data points In this paper however we are interested in finding
group anomalies where a set of points together exhibit unusual behavior For example consider
text data where each article is considered to be a set group of words points While the phrases
machine learning or gummy bears will not surprise anyone on their own an article containing
both of them might be interesting
We consider two types of group anomalies A point-based group anomaly is a group of individually
anomalous points A distribution-based anomaly is a group where the points are relatively normal
but as a whole they are unusual Most existing work on group anomaly detection focuses on pointbased anomalies A common way to detect point-based anomalies is to first identify anomalous
points and then find their aggregations using scanning or segmentation methods This
paradigm clearly does not work well for distribution-based anomalies where the individual points
are normal To handle distribution-based anomalies we can design features for groups and then treat
them as points However this approach relies on feature engineering that is domain specific
and can be difficult Our contribution is to propose a new method FGM for detecting both types of
group anomalies in an integral way
Group anomalies exist in many real-world problems In astronomical studies modern telescope
pipelines1 produce descriptions for a vast amount of celestial objects Having these data we want
to pick out scientifically valuable objects like planetary nebulae or special clusters of galaxies that
could shed light on the development of the universe In physics researchers often simulate the
motion of particles or fluid In these systems a single particle is seldom interesting but a group of
particles can exhibit interesting motion patterns like the interweaving of vortices Other examples
are abundant in the fields of computer vision text processing time series and spatial data analysis
For example the Sloan Digital Sky Survey SDSS http://www.sdss.org
We take a generative approach to address this problem If we have a model to generate normal
data then we can mark the groups that have small probabilities under this model as anomalies
Here we make the bag-of-points assumption points in the same group are unordered and
exchangeable Under this assumption mixture models are often used to generate the data due to
De Finetti?s theorem The most famous class of mixture models for modeling group data is
the family of topic models In topic models distributions of points in different groups are
mixtures of components topics which are shared among all the groups
Our proposed method is closely related to the class of topic models but it is designed specifically for
the purpose of detecting group anomalies We use two levels of concepts/latent variables to describe
a group At the group level a flexible structure based on genres is used to characterize the topic
distributions so that complex normal behaviors are allowed and can be recognized At the point level
each group has its own topics to accommodate and capture the variations of points distributions
while global topic information is still shared among groups We call this model the Flexible Genre
Model Given a group of points we can examine whether or not it conforms to the normal
behavior defined by the learned genres and topics We will also propose scoring functions that can
detect both point-based and distribution-based group anomalies Exact inference and learning for
FGM is intractable so we resort to approximate methods Inference for the FGM model will be
done by Gibbs sampling which is efficient and simple to implement due to the application of
conjugate distributions Single-sample Monte Carlo EM is used to learn parameters based on
samples produced by the Gibbs sampler
We demonstrate the effectiveness of the FGM on synthetic and on real-world data sets including
scene images and turbulence data Empirical results show that FGM is superior to existing approaches in finding group anomalies
The paper is structured as follows In Section we review related work and discuss the limitations
with existing algorithms and why a new method is needed for group anomaly detection Section introduces our proposed model The parameter learning of our model and inference on it are explained
in Section Section describes how to use our method for group anomaly detection Experimental
results are shown in Section We finish that paper by drawing conclusions Section
Background and Related Work
In this section we provide background about topic models and explain the limitation of existing
methods in detecting group anomalies For intuition we introduce the problem in the context of
detecting anomalous images rare galaxy clusters and unusual motion in a dynamic fluid simulation
We consider a data set with pre-defined groups G1 GM spatial clusters of galaxies patches in an image or fluid motions in a local region Group Gm contains Nm points
galaxies image patches simulation grid points The features of these points are denoted by
xm xm,n Rf where is the dimensionality of the points features These would
be spectral features of each galaxy SIFT features of each image patch or velocities at each grid
point of a simulation We assume that points in the same group are unordered and exchangeable
Having these data we ask the question whether in group Gm the distribution of features xm looks
anomalous
Topic models such as Latent Dirichlet Allocation LDA are widely used to model data having
this kind of group structure The original LDA model was proposed for text processing It represents
the distribution of points words in a group document as a mixture of global topics
each of which is a distribution Sf where Sf is the dimensional probability simplex
Let be the multinomial distribution parameterized by SK and be the Dirichlet
distribution with parameter RK
LDA generates the mth group by first drawing its topic
distribution from the prior distribution Then for each point xmn in the mth group
it draws one of the topics from zmn and then generates the point
according to this topic xmn zmn
In our examples the topics can represent galaxy types blue?,?red or emissive with
image features edge detectors representing various orientations or common motion patterns in the fluid fast left slow right etc Each point in the group has its own topic We
consider points that have multidimensional continuous feature vectors In this case topics can be
modeled by Gaussian distributions and each point is generated from one of the Gaussian topics
At a higher level a group is characterized by the distribution of topics the proportion of
different types in the group Gm The concepts of topic and topic distribution help us define group
anomalies a point-based anomaly contains points that do not belong to any of the normal topics
and a distribution-based anomaly has a topic distribution that is uncommon
Although topic models are very useful in estimating the topics and topic distributions in groups the
existing methods are incapable of detecting group anomalies comprehensively In order to detect
anomalies the model should be flexible enough to enable complex normal behaviors For example
it should be able to model complex and multi-modal distributions of the topic distribution LDA
however only uses a single Dirichlet distribution to generate topic distributions and cannot effectively define what normal and abnormal distributions should be It also uses the same topics for
every group which makes groups indifferentiable when looking at their topics In addition these
shared topics are not adapted to each group either
The Mixture of Gaussian Mixture Model MGMM firstly uses topic modeling for group
anomaly detection It allows groups to select their topic distributions from a dictionary of multinomials which is learned from data to define what is normal employed the same idea but
did not apply their model to anomaly detection The problem of using multinomials is that it does
not consider the uncertainty of topic distributions The Theme Model ThM lets a mixture
of Dirichlets generate the topic distributions and then uses the memberships in this mixture to do
clustering on groups This idea is useful for modeling group-level behaviors but fails to capture
anomalous point-level behaviors The topics are still shared globally in the same way as in LDA. In
contrast proposed to use different topics for different groups in order to account for the burstiness of the words points These adaptive topics are useful in recognizing point-level anomalies
but cannot be used to detect anomalous behavior at the group level For the group anomaly detection
problem we propose a new method the Flexible Genre Model and demonstrate that it is able to cope
with the issues mentioned above and performs better than the existing state-of-the-art algorithms
Model Specification
The flexible genre model FGM extends LDA such that the generating processes of topics and topic
distributions can model more complex distributions To achieve this goal two key components are
added To model the behavior of topic distributions we use several genres each of which is a
typical distribution of topic distributions We use topic generators to generate adaptive topics
for different groups We will also use them to learn how the normal topics have been generated The
generative process of FGM is presented in Algorithm A graphical representation of FGM is given
in Figure
Algorithm Generative process of FGM
for Groups to do
Draw a genre ym
Draw a topic distribution according to the genre ym SK Dir(?ym
Draw topics m,k m,k
for Points to Nm do
Draw a topic membership zm,n m,zmn topic will be active
Generate a point xm,n xm,n m,zmn
end for
end for
We assume there are genres and topics denotes the global distribution of genres Each
genre is a Dirichlet distribution for generating the topic distributions and is the
set of genre parameters Each group has topics m,k The topic generators
are the global distributions for generating the corresponding topics
Having the topic distribution and the topics m,k points are generated as in LDA.
By comparing FGM to LDA the advantages of FGM become evident In FGM each group has a
latent genre attribute ym which determines how the topic distribution in this group should look like
Dir(?ym and each group has its own topics m,k
but they are still tied through the
ym
zmn
xmn
Figure The Flexible Genre Model
global distributions Thus the topics can be adapted to local group data but the information
is still shared globally Moreover the topic generators determine how the topics m,k
should look like In turn if a group uses unusual topics to generate its points it can be identified
To handle real-valued multidimensional data we set the point-generating distributions the topics to be Gaussians xm,n m,k xm,n m,k where m,k includes
the mean and covariance parameters For computational convenience the topic generators are
Gaussian-Inverse-Wishart GIW distributions which are conjugate to the Gaussian topics Hence
parameterizes the GIW distribution See the supplementary materials
for more details Let denote the model parameters We can write the complete
likelihood of data and latent variables in group Gm under FGM as follows
Gm ym
M(ym
GIW
m,k
M(zmn xmn
m,zmn
By integrating out and summing out ym we get the marginal likelihood of Gm
YX
Gm
Dir(?m
GIW m,k
mk xmn m,k d?m
Finally the data-set?s likelihood is just the product of all groups likelihoods
Inference and Learning
To learn FGM we update the parameters to maximize the likelihood of data The inferred latent
states?including the topic distributions the topics and the topic and genre memberships
zm ym can be used for detecting anomalies and exploring the data Nonetheless the inference
and learning in FGM is intractable so we train FGM using an approximate method described below
Inference
The approximate inference of the latent variables can be done using Gibbs sampling In Gibbs
sampling we iteratively update one variable at a time by drawing samples from its conditional
distribution when all the other parameters are fixed Thanks to the use of conjugate distributions
Gibbs sampling in FGM is simple and easy to implement The sampling distributions of the latent
variables in group are given below We use to denote the distribution of one variable
conditioned on all the others For the genre membership ym we have that
ym ym Dir(?m
For the topic distribution
zm ym M(zm Dir(?m Dir(?ym nm
where nm denotes the histogram of the values in vector zm The last equation follows from the
Dirichlet-Multinomial conjugacy For m,k the kth topic in group one can find that
m,k
m,k m,k
m,k GIW m,k
GIW
m,k
where xm are points in group Gm from topic zm,n The last equation follows from
the Gaussian-Inverse-Wishart-Gaussian conjugacy is the parameter of the posterior GIW distri(k
bution given xm its exact form can be found in the supplementary material For zmn the topic
membership of point in group is as follows
zmn xmn zmn
zmn
xmn
m,k
Learning
Learning the parameters of FGM helps us identify the groups and points normal behaviors Each of
the genres captures one typical distribution of topic distributions as Dir(?t
The topic generators determine how the normal topics m,k should look like
We use single-sample Monte Carlo EM to learn parameters from the samples provided by
the Gibbs sampler Given sampled latent variables we update the parameters to their maximum
likelihood estimations we learn from and from and from
can easily be estimated from the histogram of is learned by the MLE of a Dirichlet distribution given the multinomials ym the topic distributions having
genre which can be solved using the Newton?Raphson method The kth topic-generator?s
parameter 0k is the MLE of a GIW distribution given the parameters
m,k the kth topics of all groups We have derived an efficient solution for this MLE problem The details can be found in the supplementary material
The overall learning algorithm works by repeating the following procedure until convergence
do Gibbs sampling to infer the states of the latent variables update the model parameters using
the estimations above To select appropriate values for the parameters and the number of
genres and topics we can apply the Bayesian information criterion BIC or use the values
that maximize the likelihood of a held-out validation set
Scoring Criteria
The learned FGM model can easily be used for anomaly detection on test data Given a test group
we first infer its latent variables including the topics and the topic distribution Then we treat these
latent states as the group?s characteristicsand examine if they are compatible with the normal behaviors defined by the FGM parameters
Point-based group anomalies can be detected by examining the topics of the groups If a group
contains anomalous points with rare feature values xmn then the topics m,k
that generate these points will deviate from the normal behavior defined by the topic generators Let
QK
GIW m,k The point-based anomaly score PB score of group Gm is
Gm
The posterior Gm can again be approximated using Gibbs sampling and the expectation
can be done by Monte Carlo integration
Distribution-based group anomalies can be detected by examining the topic distributions The genres
capture the typical distributions of topic distributions If a group?s topic distribution
is unlikely to be generated from any of these genres we call it anomalous Let
PT
Dir(?m The distribution-based anomaly score DB score of group Gm is defined as
E?m
Gm
Again this expectation can be approximated using Gibbs sampling and Monte Carlo integration
Using a combination of the point-based and distribution-based scores we can detect both pointbased and distribution-based group anomalies
Experiments
In this section we provide empirical results produced by FGM on synthetic and real data We show
that FGM outperforms several sate-of-the-art competitors in the group anomaly detection task
Synthetic Data
In the first experiment we compare FGM with the Mixture of Gaussian Mixture Model
MGMM and with an adaptation of the Theme Model ThM on synthetic data sets The
original ThM handles only discrete data and was proposed for clustering To handle continuous data
and detect anomalies we modified it by using Gaussian topics and applied the distribution-based
anomaly scoring function To detect both distribution-based and point-based anomalies we can
use the data?s likelihood under ThM as the scoring function
Using the synthetic data sets described below we can demonstrate the behavior of the different
models and scoring functions We generated the data using 2-dimensional GMMs as in Here
each group has a GMM to generate its points All GMMs share three Gaussian components with
covariance I2 and centered at points and respectively A group?s
mixing weights are randomly chosen from w1 or w2 Thus
a group is normal if its points are sampled from these three Gaussians and their mixing weights are
close to either w1 or w2 To test the detectors we injected both point-based and distribution-based
anomalies Point-based anomalies were groups of points sampled from I2 Distributionbased anomalies were generated by GMMs consisting of normal Gaussian components but with
mixing weights and which were different from w1 and w2 We
generated groups each of which had Nm oisson(100 points One point-based
anomalous group and two distribution-based anomalous groups were injected into the data set
The detection results of MGMM ThM and FGM are shown in We show out of the
groups Normal groups are surrounded by black solid boxes point-based anomalies have green
dashed boxes and distribution-based anomalies have red/magenta dashed boxes Points are colored by the anomaly scores of the groups darker color means more anomalous An ideal detector
would make dashed boxes points dark and solid boxes points light gray We can see that all the
MGMM
ThM
FGM
ThM Likelihood
Figure Detection results on synthetic data
models can find the distribution-based anomalies since they are able to learn the topic distributions
However MGMM and ThM miss the point-based anomaly The explanation is simple the anomalous points are distributed in the middle of the topics thus the inferred topic distribution is around
which is exactly w1 As a result MGMM and ThM infer this group to be normal
although it is not This example shows one possible problem of scoring groups based on topic distributions only On the contrary using the sum of point-based and distribution-based scores FGM
found all of the group anomalies thanks to its ability to characterize groups both at the point-level
and the group-level We also show the result of scoring the groups by the ThM likelihood Only
point anomalies are found This is because the data likelihood under ThM is dominated by the
anomalousness of points thus a few eccentric points will overshadow group-level behaviors
Figures show the density estimations given by MGMM ThM and FGM respectively for
the point-based anomalous group We can see that FGM gives a better estimation due to its adaptive
topics while MGMM and ThM are limited to use their global topics Figure shows the learned
PT
genres visualized as the distribution on the topic simplex This distribution summarizes the normal topic distributions in this data set Observe that the two peaks in the probability
simplex are very close to w1 and w2 indeed
Figure show the density of the point-based anomaly estimated by MGMM ThM and
FGM respectively In MGMM and ThM topics must be shared globally therefore their perform
badly The genres in the synthetic data set learned by FGM.
Image Data
In this experiment we test the performance of our method on detecting anomalous scene images We
use the data set from We selected the first images from categories mountain coast
and inside city These images are randomly divided are used for training and the rest
for testing We created anomalies by stitching random normal test images from different categories
For example an anomaly may be a picture that is half mountain and half city street These anomalies are challenging since they have the same local patches as the normal images We mixed the
anomalies with normal test images and asked the detectors to find them Some examples are shown
in The images are represented as in we treat each of them as a group of local points
On each image we randomly sample patches on each patch extract the 128-dimensional SIFT
feature and then reduce its dimension to using PCA. Points near the stitching boundaries are
discarded to avoid boundary artifacts
We compare FGM with several other methods We implemented a simple detector based on Gaussian
mixture models it is able to detect point-based anomalies This method fits a GMM to all
data points calculates the points scores as their likelihood under this GMM and finally scores
a group by averaging these numbers To be able to detect distribution-based anomalies we also
implemented two other competitors The first one called LDA-KNN uses LDA to estimate the topic
distributions of the groups and treats these topic distributions vector parameters of multinomials
as the groups features Then a k-nearest neighbor KNN based point detector is used to score
the groups features The second method uses symmetrized Kullback-Leibler divergences
between densities For each group DD uses a GMM to estimate the distribution of its points
Then KL divergences between these GMMs are estimated using Monte Carlo method and then the
KNN-based detector is used to find anomalous GMMs groups
For all algorithms we used topics and genres as it was suggested by BIC searches We
set for FGM. The performance is measured by the area under the ROC curve AUC
of retrieving the anomalies from the test set In the supplementary material we also show results
using the average precision performance measure The performances from random runs are
shown in Figure GMM cannot detect the group anomalies that do not have anomalous points
The performance of LDA-KNN was also close to the random baseline A possible reason is
that the KNN detector did not perform well in the dimensional space MGMM ThM and
FGM show improvements over the random baseline and FGM achieves significantly better results
than others the paired t-test gives a p-value of for FGM ThM. We can also see that
the DD method performs poorly possibly due to many error-prone steps including fitting the GMMs
and estimating divergences using Monte Carlo method
Turbulence Data
We present an explorative study of detecting group anomalies on turbulence data from the JHU Turbulence Database Cluster2 TDC TDC simulates fluid motion through time on a 3-dimensional
grid and here we perform our experiment on a continuous sub-grid In each time step and each
http://turbulence.pha.jhu.edu
AUC
Sample images and stitched anomalies
LDA?KNN MGMM
ThM
FGM?DB
DD
Detection performance
Figure Detection of stitched images Images samples Green boxes first row contain natural
images and yellow boxes second row contain stitched anomalies The detection AUCs
vertex of the grid TDC records the 3-dimensional velocity of the fluid We consider the vertices in a
local cubic region as a group and the goal is to find groups of vertices whose velocity distributions
moving patterns are unusual and potentially interesting The following steps were used to extract the groups We chose the grid points as centers of our groups Around
these centers the points in 73 sized cubes formed our groups The feature of a point in the cube
was its velocity relative to the velocity at its cube?s center point After these pre-processing steps
we had groups each of which had 3-dimensional feature vectors
We applied MGMM ThM and FGM to find anomalies in this group data genres and
topics were used for all methods We do not have a groundtruth for anomalies in this data set
However we can compute the vorticity score for each vertex that indicates the tendency of
the fluid to spin Vortices and especially their interactions are uncommon and of great interest in
the field of fluid dynamics This vorticity can be considered as a hand crafted anomaly score based
on expert knowledge of this fluid data We do not want an anomaly detector to match this score
perfectly because there are other non-vortex anomalous events it should find as well However
we do think higher correlation with this score indicates better anomaly detection performance
Figure visualizes the anomaly scores of FGM and the vorticity We can see that these pictures are
highly correlated which implies that FGM was able to find interesting turbulence activities based on
velocity only and without using the definition of vorticity or any other expert knowledge Correlation
values between vorticity and the MGMM ThM and FGM scores from random runs are displayed
in showing that FGM is better at finding regions with high vorticity
Correlation with Vorticity
MGMM
FGM-DB Score
Vorticity
ThM
FGM?DB
Figure Detection results for the turbulence data FGM-DB anomaly score and vorticity
visualized on one slice of the cube Correlations of the anomaly scores with the vorticity
Conclusion
We presented the generative Flexible Genre Model FGM for the group anomaly detection problem
Compared to traditional topic models FGM is able to characterize groups behaviors at multiple
levels This detailed characterization makes FGM an ideal tool for detecting different types of group
anomalies Empirical results show that FGM achieves better performance than existing approaches
In the future we will examine other possibilities as well For model selection we can extend FGM
by using nonparametric Bayesian techniques such as hierarchical Dirichlet processes It would
also be interesting to study structured groups in which the exchangeability assumption is not valid

<<----------------------------------------------------------------------------------------------------------------------->>

title: 3156-in-network-pca-and-anomaly-detection.pdf

In-Network PCA and Anomaly Detection
Ling Huang
University of California
Berkeley CA
hling@cs.berkeley.edu
Michael I. Jordan
University of California
Berkeley CA
jordan@cs.berkeley.edu
XuanLong Nguyen
University of California
Berkeley CA
xuanlong@cs.berkeley.edu
Anthony Joseph
University of California
Berkeley CA
adj@cs.berkeley.edu
Minos Garofalakis
Intel Research
Berkeley CA
minos.garofalakis@intel.com
Nina Taft
Intel Research
Berkeley CA
nina.taft@intel.com
Abstract
We consider the problem of network anomaly detection in large distributed systems In this
setting Principal Component Analysis PCA has been proposed as a method for discovering anomalies by continuously tracking the projection of the data onto a residual subspace
This method was shown to work well empirically in highly aggregated networks that is
those with a limited number of large nodes and at coarse time scales This approach however has scalability limitations To overcome these limitations we develop a PCA-based
anomaly detector in which adaptive local data filters send to a coordinator just enough data
to enable accurate global detection Our method is based on a stochastic matrix perturbation analysis that characterizes the tradeoff between the accuracy of anomaly detection and
the amount of data communicated over the network
Introduction
The area of distributed computing systems provides a promising domain for applications of machine
learning methods One of the most interesting aspects of such applications is that learning algorithms
that are embedded in a distributed computing infrastructure are themselves part of that infrastructure
and must respect its inherent local computing constraints constraints on bandwidth latency
reliability while attempting to aggregate information across the infrastructure so as to improve
system performance availability in a global sense
Consider for example the problem of detecting anomalies in a wide-area network While it is
straightforward to embed learning algorithms at local nodes to attempt to detect node-level anomalies these anomalies may not be indicative of network-level problems Indeed in recent work
demonstrated a useful role for Principal Component Analysis PCA to detect network anomalies
They showed that the minor components of PCA the subspace obtained after removing the components with largest eigenvalues revealed anomalies that were not detectable in any single node-level
trace This work assumed an environment in which all the data is continuously pushed to a central
site for off-line analysis Such a solution cannot scale either for networks with a large number of
monitors nor for networks seeking to track and detect anomalies at very small time scales
Designing scalable solutions presents several challenges Viable solutions need to process data innetwork to intelligently control the frequency and size of data communications The key underlying
problem is that of developing a mathematical understanding of how to trade off quantization arising
from local data filtering against fidelity of the detection analysis We also need to understand how
this tradeoff impacts overall detection accuracy Finally the implementation needs to be simple if it
is to have impact on developers
In this paper we present a simple algorithmic framework for network-wide anomaly detection that
relies on distributed tracking combined with approximate PCA analysis together with supporting
theoretical analysis In brief the architecture involves a set of local monitors that maintain parameterized sliding filters These sliding filters yield quantized data streams that are sent to a coordinator
The coordinator makes global decisions based on these quantized data streams We use stochastic
matrix perturbation theory to both assess the impact of quantization on the accuracy of anomaly
detection and to design a method that selects filter parameters in a way that bounds the detection
error The combination of our theoretical tools and local filtering strategies results in an in-network
tracking algorithm that can achieve high detection accuracy with low communication overhead for
instance our experiments show that by choosing a relative eigen-error of yielding approximately a missed detection rate and a false alarm rate we can filter out more than of
the traffic from the original signal
Prior Work The original work on a PCA-based method by Lakhina has been extended
by who show how to infer network anomalies in both spatial and temporal domains As with
this work is completely centralized and propose distributed PCA algorithms distributed
across blocks of rows or columns of the data matrix however these methods are not applicable to
our case Furthermore neither nor address the issue of continuously tracking principal
components within a given error tolerance or the issue of implementing a communication/accuracy
tradeoff issues which are the main focus of our work Other initiatives in distributed monitoring
profiling and anomaly detection aim to share information and foster collaboration between widely
distributed monitoring boxes to offer improvements over isolated systems Work in
posits the need for scalable detection of network attacks and intrusions In the setting of simpler
statistics such as sums and counts in-network detection methods related to ours have been explored
by Finally recent work in the machine learning literature considers distributed constraints
in learning algorithms such as kernel-based classification and graphical model inference
See for a survey
Problem description and background
We consider a monitoring system comprising a set of local monitor nodes Mn each of
which collects a locally-observed time-series data stream For instance the monitors
may collect information on the number of TCP connection requests per second the number of
DNS transactions per minute or the volume of traffic at port per second A central coordinator
node aims to continuously monitor the global collection of time series and make global decisions
such as those concerning matters of network-wide health Although our methodology is generally
applicable in this paper we focus on the particular application of detecting volume anomalies A
volume anomaly refers to unusual traffic load levels in a network that are caused by anomalies such
as worms distributed denial of service attacks device failures misconfigurations and so on
Each monitor collects a new data point at every time step and assuming a naive continuous push
protocol sends the new point to the coordinator Based on these updates the coordinator keeps track
of a sliding time window of size the most recent data points for each monitor time series
organized into a matrix of size where the ith column Yi captures the data from monitor
see The coordinator then makes its decisions based solely on this global matrix
In the network-wide volume anomaly detection algorithm of the local monitors measure the total
volume of traffic bytes on each network link and periodically every minutes centralize
the data by pushing all recent measurements to the coordinator The coordinator then performs
PCA on the assembled matrix to detect volume anomalies This method has been shown to work
remarkably well presumably due to the inherently low-dimensional nature of the underlying data
However such a periodic push approach suffers from inherent limitations To ensure fast
detection the update periods should be relatively small unfortunately small periods also imply
increased monitoring communication overheads which may very well be unnecessary if there
are no significant local changes across periods Instead in our work we study how the monitors
can effectively filter their time-series updates sending as little data as possible yet enough so as
to allow the coordinator to make global decisions accurately We provide analytical bounds on the
errors that occur because decisions are made with incomplete data and explore the tradeoff between
reducing data transmissions communication overhead and decision accuracy
18
Anomaly
State Vector
Data Flow
Result
Mon
Tue
Wed
Thu
Fri
Sat
Sun
Tue
Wed
Thu
Fri
Sat
Sun
17
M2
M3
Mn
Residual Vector
M1
Mon
Abilene network traffic data
The system setup
Figure The distributed monitoring system Data sample kyk2 collected over one week its
projection in residual subspace bottom Dashed line represents a threshold for anomaly detection
Using PCA for centralized volume anomaly detection As observed by Lakhina due to
the high level of traffic aggregation on ISP backbone links volume anomalies can often go unnoticed by being buried within normal traffic patterns the circle dots shown in the top plot in
Fig On the other hand they observe that although the measured data is of seemingly high
dimensionality number of links normal traffic patterns actually lie in a very low-dimensional
subspace furthermore separating out this normal traffic subspace using PCA to find the principal
traffic components makes it much easier to identify volume anomalies in the remaining subspace
bottom plot of
As before let be the global time-series data matrix centered to have zero mean and let
denote a n-dimensional vector of measurements for all links from a single time step
Formally PCA is a projection method that maps a given set of data points onto principal components ordered by the amount of data variance that they capture The set of principal components
vi are defined as
vi arg max k(Y
Yvj vjT xk
kxk=1
YT Y. As shown in
and are the eigenvectors of the estimated covariance matrix A
PCA reveals that the Origin-Destination flow matrices of ISP backbones have low intrinsic
dimensionality For the Abilene network with 41 links most data variance can be captured by the
first principal components Thus the underlying normal OD flows effectively reside in a
low k-dimensional subspace of Rn This subspace is referred to as the normal traffic subspace
Sno The remaining principal components constitute the abnormal traffic subspace ab
Detecting volume anomalies relies on the decomposition of link traffic at any time step into
normal and abnormal components yno yab such that yno corresponds to modeled normal
traffic the projection of onto Sno and yab corresponds to residual traffic the projection of
onto Sab Mathematically yno and yab can be computed as
yno PPT Cno and yab I PPT Cab
where v2 vk is formed by the first principal components which capture the dominant variance in the data The matrix Cno PPT represents the linear operator that performs
projection onto the normal subspace Sno and Cab projects onto the abnormal subspace Sab
As observed in a volume anomaly typically results in a large change to ab thus a useful metric
for detecting abnormal traffic patterns is the squared prediction error
SPE kyab kCab yk2
essentially a quadratic residual function More formally their proposed algorithm signals a volume anomaly if SPE where denotes the threshold statistic for the SPE residual function
at the confidence level Such a statistical test for the SPE residual function known as the
Q-statistic can be computed as a function of the non-principal
eigenvalues of the covariance matrix A.
Distr Monitors
Filter
Predict
Filter
Predict
Anomaly
R1
Input
R2
Perturbation
Analysis
Yn
Filter
Predict
Rn
Subspace
Method
Adaptive
Coordinator
Figure Our in-network tracking and detection framework
In-network PCA for anomaly detection
We now describe our version of an anomaly detector that uses distributed tracking and approximate
PCA analysis A key idea is to curtail the amount of data each monitor sends to the coordinator
Because our job is to catch anomalies rather than to track ongoing state we point out that the
coordinator only needs to have a good approximation of the state when an anomaly is near It need
not track global state very precisely when conditions are normal This observation makes it intuitive
that a reduction in data sharing between monitors and the coordinator should be possible We curtail
the amount of data flow from monitors to the coordinator by installing local filters at each monitor
These filters maintain a local constraint and a monitor only sends the coordinator an update of its
data when the constraint is violated The coordinator thus receives an approximate or perturbed
view of the data stream at each monitor and hence of the global state We use stochastic matrix
perturbation theory to analyze the effect on our PCA-based anomaly detector of using a perturbed
global matrix Based on this we can choose the filtering parameters the local constraints so as
to limit the effect of the perturbation on the PCA analysis and on any deterioration in the anomaly
detector?s performance All of these ideas are combined into a simple adaptive distributed protocol
Overview of our approach
illustrates the overall architecture of our system We now describe the functionality at the
monitors and the coordinator The goal of a monitor is to track its local raw time-series data and to
decide when the coordinator needs an update Intuitively if the time series does not change much
or doesn?t change in a way that affects the global condition being tracked then the monitor does not
send anything to the coordinator The coordinator assumes that the most recently received update
is still approximately valid The update message can be either the current value of the time series
or a summary of the most recent values or any function of the time series The update serves as a
prediction of the future data because should the monitor send nothing in subsequent time intervals
then the coordinator uses the most recently received update to predict the missing values
For our anomaly detection application we filter as follows Each monitor maintains a filtering
window Fi of size centered at a value Ri Fi Ri Ri At each
time the monitor sends both Yi and Ri to the coordinator only if Yi
Fi otherwise it
sends nothing The window parameter is called the slack it captures the amount the time series
can drift before an update to the coordinator needs to be sent The center parameter denotes
the approximate representation or summary of Yi In our implementation we set Ri equal
to the average of last five signal values observed locally at monitor Let denote the time of the
most recent update happens The monitor needs to send both and Ri to the coordinator
when it does an update because the coordinator will use Yi at time and Ri for all
until the next update arrives For any subsequent when the coordinator receives no update
from that monitor it will use Ri as the prediction for Yi
The role of the coordinator is twofold First it makes global anomaly-detection decisions based
upon the received updates from the monitors Secondly it computes the filtering parameters the
slacks for all the monitors based on its view of the global state and the condition for triggering an
anomaly It gives the monitors their slacks initially and updates the value of their slack parameters
when needed Our protocol is thus adaptive Due to lack of space we do not discuss here the
method for deciding when slack updates are needed The global detection task is the same as in the
centralized scheme In contrast to the centralized setting however the coordinator does not have
instead The PCA analysis
an exact version of the raw data matrix it has the approximation
A The
including the computation of Sab is done on the perturbed covariance matrix A
magnitude of the perturbation matrix is determined by the slack variables
Selection of filtering parameters
A key ingredient of our framework is a practical method for choosing the slack parameters This
choice is critical because these parameters balance the tradeoff between the savings in data communication and the loss of detection accuracy Clearly the larger the slack the less the monitor needs
to send thus leading to both more reduction in communication overhead and potentially more information loss at the coordinator We employ stochastic matrix perturbation theory to quantify the
effects of the perturbation of a matrix on key quantities such as eigenvalues and the eigen-subspaces
which in turn affect the detection accuracy
Our approach is as follows We measure the size of a perturbation using a norm on We derive
an upper bound on the changes to the eigenvalues and the residual subspace Cab as a function of
We choose to ensure that an approximation to this upper bound on is not exceeded This
in turn ensures that and Cab do not exceed their upper bounds Controlling these latter terms we
are able to bound the false alarm probability
Recall that the coordinator?s view of the global data matrix is the perturbed matrix
where all elements of the column vector Wi are bounded within the interval Let and
denote the eigenvalues of the covariance matrix A YT and its perturbed
Applying the classical theorems of Mirsky and Weyl we obtain bounds
Y.
version A
on the eigenvalue perturbation in terms of the Frobenius norm k.k and the spectral norm of
respectively
A A
uX
k?kF and max
eig
Applying the sin theorem and results on bounding the angle of projections to subspaces
for more details we can bound the perturbation of the residual subspace ab in terms of the
Frobenius norm of
ab kF 2k?kF
kCab
where denotes the eigengap between the th and eigenvalues of the estimated covariance
matrix A.
To obtain practical computable bound on the norms of we derive expectation bounds
instead of worst case bounds We make the following assumptions on the error matrix
The column vectors W1 Wn are independent and radially symmetric m-vectors
For each all elements of column vector Wi are random variables with
mean variance and fourth moment
Note that the independence assumption is imposed only on the error?this by no means implies that
the signals received by different
monitors are statistically independent Under the above assumption
we can show that k?kF is upper bounded in expectation by the following quantity
olF
mn
mn
Similar results can be obtained for the spectral norm as well In practice these upper bounds are
very tight because tend to be small compared to the top eigenvalues Given the tolerable
perturbation olF we can use Eqn. to select the slack variables For example we can divide the
overall tolerance across monitors either uniformly or in proportion to their observed local variance
Guarantee on false alarm probability
Because our approximation perturbs the eigenvalues it also impacts the accuracy with which the
trigger is fired Since the trigger condition is kCab yk2 we must assess the impact on both
of these terms We can compute an upper bound on the perturbation of the SPE statistic SPE
kCab yk2 as follows First note that
uX
kCab yk k(Cab Cab
kCab
yk kCab
kCab
uX
2k?kF
yk
ab 2k?kF
kC
ab
ab
kCab yk2
kC
The dependency of the threshold on the eigenvalues can be expressed as
h1
h0
where is the percentile of the standard normal distribution h0
Pn
for
To assess the perturbation in false alarm probability we start by considering the following random
variable derived from Eqn.
h0
The random variable essentially normalizes the random quantity kC ab yk2 and is known to approximately follow a standard normal distribution The false alarm probability in the centralized
system is expressed as
Pr kCab yk2 Pr
where the lefthand term of this equation is conditioned upon the SPE statistics being inside the
ab
normal range In our distributed setting the anomaly detector fires a trigger if
We thus only observe a perturbed version for the random variable Let denote the bound on
The deviation of the false alarm probability in our approximate detection scheme can then
be approximated as where is a standard normal random variable
Evaluation
We implemented our algorithm and developed a trace-driven simulator to validate our methods We
used a one-week trace collected from the Abilene network1 The traces contains per-link traffic
loads measured every minutes for all 41 links of the Abilene network With a time unit of
minutes data was collected for time units This data was used to feed the simulator There
are anomalies in the data that were detected by the centralized algorithm and verified by hand
to be true anomalies We also injected 70 synthetic anomalies into this dataset using the method
described in so that we would have sufficient data to compute error rates We used a threshold
corresponding to an confidence level Due to space limitations we present
results only for the case of uniform monitor slack
The input parameter for our algorithm is the tolerable relative error of theqeigenvalues relative
eigen-error for short which acts as a tuning knob Precisely it is ol n1
where olF
is defined in Eqn. Given this parameter and the input data we can compute the filtering slack
for the monitors using Eqn. We then feed in the data to run our protocol in the simulator with the
Abilene is an Internet2 high-performance backbone network that interconnects a large number of universities as well as a few other research institutes
Fal. Alarm Rate
Rel. Threshold Error
Missed Detec Rate
Rel. Eigen Error
Upper Bound
Actual Accrued
Comm Overhead
Slack
Figure In all plots the x-axis is the relative eigen-error The filtering slack Actual accrued eigenerror Relative error of detection threshold False alarm rates Missed detection rates Communication overhead
computed The simulator outputs a set of results including the actual relative eigen errors and
the relative errors on the detection threshold the missed detection rate false alarm rate and
communication cost achieved by our method The missed-detection rate is defined as the fraction of
missed detections over the total number of real anomalies and the false-alarm rate as the fraction
of false alarms over the total number of detected anomalies by our protocol which is defined in
Sec. rescaled as a rate rather than a probability The communication cost is computed as the
fraction of number of messages that actually get through the filtering window to the coordinator
The results are shown in In all plots the x-axis is the relative eigen-error In we plot
the relationship between the relative eigen-error and the filtering slack when assuming filtering
errors are uniformly distributed on interval With this model the relationship between the
relative eigen-error and the slack is determined by a simplified version of Eqn. with all i2
The results make intuitive sense As we increase our error tolerance we can filter more at the monitor
and send less to the coordinator The slack increases almost linearly with the relative eigen-error
because the first term in the right hand side of Eqn. dominates all other terms
In
we compare the relative eigen-error to the actual accrued relative eigen-error defined as
where eig is defined in Eqn These were computed using the slack parameters
eig
as computed by our coordinator We can see that the real accrued eigen-errors are always less than
the tolerable eigen errors The plot shows a tight upper bound indicating that it is safe to use our
model?s derived filtering slack In other words the achieved eigen-error always remains below the
requested tolerable error specified as input and the slack chosen given the tolerable error is close
to being optimal shows the relationship between the relative eigen-error and the relative
error of detection threshold We see that the threshold for detecting anomalies decreases as we
tolerate more and more eigen-errors In these experiments an error of in the eigenvalues leads
to an error of approximately in our estimate of the appropriate cutoff threshold
We now examine the false alarm rates achieved In the curve with triangles represents
the upper bound on the false alarm rate as estimated by the coordinator The curve with circles
is the actual accrued false alarm rate achieved by our scheme Note that the upper bound on the
false alarm rate is fairly close to the true values especially when the slack is small The false alarm
rate increases with increasing eigen-error because as the eigen-error increases the corresponding
detection threshold will decrease which in turn causes the protocol to raise an alarm more
rather than the relative threshold difference we would obviously see a
often If we had plotted
where
is computed from
Precisely it is
with increasing eigen-error We see in that the missed detection rates remain
decreasing
below for various levels of communication overhead
The communication overhead is depicted in Clearly the larger the errors we can tolerate
the more overhead can be reduced Considering these last three plots together we observe
several tradeoffs For example when the relative eigen-error is our algorithm reduces the data
sent through the network by more than This gain is achieved at the cost of approximately a
missed detection rate and a false alarm rate This is a large reduction in communication for
a small increase in detection error These initial results illustrate that our in-network solution can
dramatically lower the communication overhead while still achieving high detection accuracy
Conclusion
We have presented a new algorithmic framework for network anomaly detection that combines distributed tracking with PCA analysis to detect anomalies with far less data than previous methods
The distributed tracking consists of local filters installed at each monitoring site whose parameters
are selected based upon global criteria The idea is to track the local monitoring data only enough so
as to enable accurate detection The local filtering reduces the amount of data transmitted through
the network but also means that anomaly detection must be done with limited or partial views of the
global state Using methods from stochastic matrix perturbation theory we provided an analysis for
the tradeoff between the detection accuracy and the data communication overhead We were able
to control the amount of data overhead using the the relative eigen-error as a tuning knob To the
best of our knowledge this is the first result in the literature that provides upper bounds on the false
alarm rate of network anomaly detection

<<----------------------------------------------------------------------------------------------------------------------->>

title: 1077-a-neural-network-autoassociator-for-induction-motor-failure-prediction.pdf

A Neural Network Autoassociator for
Induction Motor Failure Prediction
Thomas Petsche Angelo Marcantonio Christian Darken
Stephen J. Hanson Gary M. Kuhn and Iwan Santoso
PETSCHE ANGELO DARKEN JOSE GMK NIS]@SCR.SIEMENS.COM
Siemens Corporate Research Inc.
College Road East
Princeton NJ
Abstract
We present results on the use of neural network based autoassociators
which act as novelty or anomaly detectors to detect imminent motor
failures The autoassociator is trained to reconstruct spectra obtained
from the healthy motor In laboratory tests we have demonstrated that the
trained autoassociator has a small reconstruction error on measurements
recorded from healthy motors but a larger error on those recorded from a
motor with a fault We have designed and built a motor monitoring system
using an autoassociator for anomaly detection and are in the process of
testing the system at three industrial and commercial sites
Introduction
An unexpected breakdown of an electric induction motor can cause financial loss significantly in excess of the cost of the motor For example the breakdown of a motor in a
production line during a production run can cause the loss of work in progress as well as
loss of production time
When a motor does fail it is not uncommon to replace it with an oversized motor based on
the assumption that if a motor is not running at its design limit then it will survive longer
While this is frequently effective this leads to significantly lower operating efficiencies and
higher initial and operating costs
The primary motivation behind this project is the observation that if a motor breakdown and
be predicted before the actual breakdown occurs then the motor can be replaced in a more
orderly way with minimal interruption of the process in which it is involved The goal is
to produce a system that is conceptually similar to a fuel gauge on an automobile When
the system detects conditions that indicate that the motor is approaching its end-of-life the
operators are notified that a replacement is necessary in the near future
A Neural Network Autoassociator for Induction Motor Failure Prediction
Background
At present motors in critical operations that are subject to mechanical failures for example
fire pump motors on US Navy vessels are typically monitored by a human expert who
periodically listens to the vibrations of the motor and based on experience determines
whether the motor sounds healthy or sounds like a problem is developing Since mechanical
probiems in motors typically lead to increased or changed vibrations this technique can
werk well Unfortunately it depends on a competent and expensive expert
In an attempt to automate motor monitoring several vendors have automated motor monitoring equipment available For mechanical failure monitoring such systems typically rely
on several accelerometers to measure the vibration of the motor at various points and along
various axes The systems then display information primarily about the vibration spectrum
to an operator who determines whether the motor is functioning properly These systems
are expensive since they rely on several accelerometers each of which is itself expensive
as well as data collection hardware and a computer Further the systems require an expert
operator and frequently require that the motor be tested only when it is driving a known load
Neither the human motor expert nor the existing motor monitoring systems provide an
affordable solution for continuous on-line mechanical failure monitoring However the
success of the human expert and existing vibration monitors does demonstrate that in fact
there is sufficient information in the vibration of an electric induction motor to detect
imminent mechanical failures
Siemens Energy and Automation has proposed a new product the Siemens Advanced Motor
Master System SAMMS that will continuously monitor and protect an electric induction motor while it is operating on-line Like the presently available SAMMS the SAMMS
is designed to provide protection against thermal and electrical overload an in addition
it will provide detection of insulation deterioration and mechanical fault monitoring
In contrast to existing systems and techniques the SAMMS is designed to require
no human expert to determine if a motor is developing problems be inexpensive and
provide continuous on-line monitoring of the motor in normal operation
The requirements for the SAMMS in partiCUlar the cost constraint require that several
issues be resolved First in order to produce a low cost system it is necessary to eliminate
the need for expensive accelerometers Second wiring should be limited to the motor control
center it should not be necessary to run new signal wires from the motor control center
to the motor Third the SAMMS is to provide continuous on-line monitoring so the
system must adapt to or factor out the effect of changing loads on the motor Finally since
the SAMMS would not necessarily be bundled with a motor and so might be used to
control and monitor an arbitrary motor from an arbitrary manufacturer the design can not
assume that a full description of the motor construction is available
Approach
The first task was to determine how to eliminate the accelerometers Based on work done
elsewhere Schoen Habetler Bartheld SE&A determined that it might be possible
to use measurements of the current on a single phase of the power supply to estimate the
vibration of the motor This depends on the assumption that any vibration of the motor will
cause the rotor to move radially relative to the stator which will cause changes in the airgap
which in tum will induce changes in the current
Experiments were done at the Georgia Institute of Technology to determine the feasibility
of this idea using the same sort of data collection system described later Early experiments
indicated that for a single motor driving a variety of loads it is possible to distinguish
T. PETSCHE A. MARCANTONIO C. DARKEN S. J. HANSON G. M. KUHN I. SANTOSO
Table Loads for motors and
Load type
constant
sinusoidal oscillation at rotating frequency
sinusoidal oscillation at twice the rotating frequency
switching load duty cycle at rotating frequency
sinusoidal oscillation 28 Hz
sinusoidal oscillation at Hz
switching load duty cycle at Hz
Load Magnitude
half and full rated
half and full rated
full rated
full rated
half and full rated
full rated
full rated
Table Neural network classifier experiment
Features
Performance on motor
Performance on motor
48
63
64
between a current spectrum obtained from the motor while it is healthy and another obtained
when the motor contains a fault Moreover it is also possible to automatically generate a
classifiers that correctly determine the presence or absence of a fault in the motor
The first obvious approach to this monitoring task would seem to be to build a classifier
that would be used to distinguish between a healthy motor and one that has developed a
fault that is likely to lead to a breakdown Unfortunately this approach does not work
As described above we have successfully built classifiers of various sorts using manual and
automatic techniques to distinguish between current spectra obtained from a motor when it
is healthy and those obtained when it contains a fault
However since the SAMMS will be connected to a motor before it fails and will be asked
to identify a failure without ever seeing a labeled example of a failure from that motor a
classifier can only be used if it can be trained on data collected from one or more motors
and then used to monitor the motor of interest Unfortunately experiments indicate that
this will not work
One of these experiments is illustrated in table Several feedforward neural network classifiers were trained using examples from a single motor under four conditions healthy
unbalanced containing a broken rotor bar and containing a hole in the outer
bearing race The ten different loads listed in table were applied to the motor for each of
these conditions
The networks contained inputs where is given in table hidden units and
outputs There were training examples where each example is the average of distinct
magnitude scaled FFrs obtained from motor from a single load/fault combination The
test data for which the results are reported in the table consisted of averaged FFfs from
motor and averaged FFfs balanced and unbalanced only from motor The test
set for motor is completely distinct from the training set
In the case where the FFf components were selected to include the frequencies
identified by the theory of motor physics as interesting for the three fault conditions and
exclude all other components This led to an improvement over the other cases where a
single contiguous set of components was chosen but the performance still degrades to about
random chance instead of
This experiment clearly illustrates that is is possible to distinguish between healthy and
faulty spectra obtained from the same motor However it also clearly illustrates that a
A Neural Network Autoassociator for Induction Motor Failure Prediction
Measurements
Novelty
detection
Novelty
Decision
Diagnosis
Adaptation
AlgOrithm
Figure The basic form of an anomaly detection system
classifier trained on one motor does not perform well on another motor since the error rates
increase immensely Based on results such as these we have concluded that it is not feasible
to build a single classifier that would be trained once and then placed in the field to monitor
a motor Instead we are pursuing an alternative based on anomaly detection which adapts
a monitor to the particular motor for which it is responsible
Anomaly detection
The basic notion of anomaly detection for monitoring is illustrated in figure Statistical
anomaly detection centers around a model of the data that was seen while the motor was
operating normally This model is produced by collecting spectra from the motor while
it is operating normally Once trained the system compares each new spectrum to the
model to determine how similar to or different from the training set it is This similarity
is described by an anomaly metric which in the simplest case can be thresholded to
determine whether the motor is still normal or has developed a fault Once the anomaly
metric has been generated various statistical techniques can be used to determine if there
has been a change in the distribution of values
A Neural Network-based Anomaly Detector
The core of the most successful monitoring system we have built to date is a neural network
designed to function as an autoassociator Rumelhart Hinton Williams called it
an encoder We use a simple three layer feedforward network with inputs outputs
and hidden units The input layer is fully connected to the hidden layer which is
fully connected to the output layer Each unit in the hidden and output layers computes
Wi,jXj where is the output of neuron which receives inputs from Mi other
neurons and Wi,j is the weight on the connection from neuron to neuron The network is
trained using the backpropagation algorithm to reconstruct the input vector on the output
units Specifically if is one of input vectors and is the corresponding output vector
the network is trained to minimize the sum of squared errors Ilxi xdl Once
training is complete the anomaly metric is mi IIXi
Anomaly Detection Test
We have tested the effectiveness of the neural network autoassociator as an anomaly detector
on several motors For all these tests the autoasociator had hidden units The hidden
layer size was chosen after some experimentation and data analysis on motor but no
attempt was made to tune the hidden layer size for motor or motor
Motor was tested using the ten different loads listed in table and four different
T. PETSCHE A. MARCANTONIO C. DARKEN S. HANSON M. KUHN I SANTOSO
Xl
Xl
C\I
o.oooos
Threshold
balanced
unbalanced
Threshold
Figure Probability of error as a function of threshold using individual FFfs on motor with inputs and motor with inputs
health/fault conditions healthy balanced unbalanced broken rotor bar and a hole in
the outer bearing race Motor was tested while driving the same ten loads but for one
healthy and one faulty condition healthy balanced and unbalanced
For both motors and recordings of a single current phase were made as follows For
each fault condition a load was selected and applied and the motor was run and the current
signal recorded for five minutes Then a new load was introduced and the motor was run
again The load was constant during any five minute recording session
Motor was tested using thirteen different loads but only two fault conditions healthy
balanced and unbalanced In this case however load changes occurred at random times
We preprocessed this data to to identify where the load changes occurred to generate the
training set and the healthy motor test sets
Preprocessing
Recordings were made on a digital audio tape The current on a single phase was
measured with a current transformer amplified notch filtered to reduce the magnitude of
the component amplified again and then applied as input to the OAT. The notch filter
was a switched capacitor filter which reduced the magnitude at by about
The time series obtained from the OAT was processed to reduce the sampling rate and then
dividing the data into non-overlapping blocks and computing the FFT of each block A
subset of the FFf magnitude coefficients was selected and for each FFT independent of
any other FFf the components were linearly scaled and translated to the interval typically That is for each FFT consisting of coefficients to
we selected a subset the same for all FFTs of the components and computed a
2e)(maxiEFh miniEFh)-t and miniEFh Then the input vector to the
network is Xj a(fij where for allj ij ik and ij ik
Experimental Results
In figure we illustrate the results of a typical anomaly detection experiment on motor
using an autoassociator with inputs and hidden units This graph illustrates the
performance false alarm and miss rates of a very simple anomaly detection system which
thresholds the anomaly metric to determine if the motor is good or bad The decreasing
curve that starts at threshold P(error is the false alarm rate as a function of the
threshold Each increasing curve is the miss rate for a particular fault type
In figure 2b we illustrate the performance of an autoassociator on motor using an
A Neural Network Autoassociator for Induction Motor Failure Prediction
I
Xl
ci
ii
ci
Threshold
Figure Probability of error for motor using individual FFTs and inputs
Xl
Xl
ci
balanced
unbalanced
ci
Threshold
Threshold
Figure Probability of error using averaged FFTs for motor and inputs
motor and inputs
autoassociator with inputs and hidden units Figure shows our results on motor
using an autoassociator with inputs
We have found significant performance improvements by averaging several consecutive
FFTs In figure we show the results for motors and when we averaged FFTs to
produce the input features Compare these curves to those in figure In particular notice
that the probability of error is much lower for the averaged FFTs when the good motor
curve crosses anyone of the faulty motor curves
Candor System Design
Based on our experiments with autoassociators we designed a prototype mechanical motor
condition monitoring system The functional system architecture is shown in figure In
order to control costs the system is implemented on a PC. The system is designed so that
each PC can monitor up to motors using one analog to digital converter The
signals are collected filtered and multiplexed on custom external signal processing cards
Each card supports up to eight motors with up to cards per PC).
The system records current measurements from one motor at a time For each motor
measurements are collected four FFTs are computed on non-overlapping time series and
the four FFTs are averaged to produce a vector that is input to the neural network The system
reports that a motor is bad only if more than five of the last ten averaged FFTs produced an
anomaly metric more than five standard deviations greater than the mean metric computed
on the training set Otherwise the motor is reported to be normal In addition to monitoring
the motors the prototype systems are designed to record all measurements on tape to support
T. PETSCHE A. MARCANTONIO C. DARKEN S. HANSON G. M. KUHN I. SANTOSO
GOOD
BAD
Figure Functional architecture of Candor
future experiments with alternative algorithms and tuning to improve performance
To date three monitoring systems have been installed in an oil refinery in a testing
laboratory and on an office building ventilation system The system has correctly detected
the only failure it has seen so far when a filter on the inlet to a water circulation pump
became clogged the spectrum changed so much that the average daily novelty metric jumped
from less than one standard deviation above the training set average to more than twenty
standard deviations We hope to have further test results in a year or so
Related work
Gluck and Myers proposed a model oflearning in the hippocampus based in part on
an autoassociator which is used to detect novel stimuli and to compress the representation
of the stimuli This model has accurately predicted many of the classical conditioning
behaviors that have been observed in normal and hippocampal-damaged animals Based on
this work Japkowicz Myers and Gluck independently derived an autoassociatorbased novelty detector for machine learning tasks similar to that used in our system
Together with Gluck we have tested an autoassociator based anomaly detector on helicopter
gearbox failures for the US Navy In this case the autoassociator is given inputs
consisting of 64 vibration based features from each of accelerometers mounted at different
locations on the gearbox In a blind test the autoassociator was able to correctly distinguish
between feature vectors taken from a damaged gearbox and other feature vectors taken
from normal gearboxes all recorded in flight Our anomaly detector will be included in
test flights of a gearbox monitoring system later this year

<<----------------------------------------------------------------------------------------------------------------------->>

title: 5236-localized-data-fusion-for-kernel-k-means-clustering-with-application-to-cancer-biology.pdf

Localized Data Fusion for Kernel k-Means Clustering
with Application to Cancer Biology
Adam A. Margolin
margolin@ohsu.edu
Department of Biomedical Engineering
Oregon Health Science University
Portland OR USA
Mehmet G?onen
gonen@ohsu.edu
Department of Biomedical Engineering
Oregon Health Science University
Portland OR USA
Abstract
In many modern applications from for example bioinformatics and computer vision samples have multiple feature representations coming from different data
sources Multiview learning algorithms try to exploit all these available information to obtain a better learner in such scenarios In this paper we propose a novel
multiple kernel learning algorithm that extends kernel k-means clustering to the
multiview setting which combines kernels calculated on the views in a localized
way to better capture sample-specific characteristics of the data We demonstrate
the better performance of our localized data fusion approach on a human colon
and rectal cancer data set by clustering patients Our method finds more relevant
prognostic patient groups than global data fusion methods when we evaluate the
results with respect to three commonly used clinical biomarkers
Introduction
Clustering algorithms aim to find a meaningful grouping of the samples at hand in an unsupervised
manner for exploratory data analysis k-means clustering is one of the classical algorithms Hartigan which uses prototype vectors centers or centroids of clusters to characterize
the data and minimizes a sum-of-squares cost function to find these prototypes with a coordinate
descent optimization method However the final cluster structure heavily depends on the initialization because the optimization scheme of k-means clustering is prone to local minima Fortunately
the sum-of-squares minimization can be formulated as a trace maximization problem which can
not be solved easily due to binary decision variables used to denote cluster memberships but this
hard optimization problem can be reduced to an eigenvalue decomposition problem by relaxing the
constraints Zha Ding and He In such a case overall clustering algorithm can be
formulated in two steps performing principal component analysis PCA Pearson on the
covariance matrix and recovering cluster membership matrix using the eigenvectors that correspond to the largest eigenvalues Similar to many other learning algorithms k-means clustering
is also extended towards a nonlinear version with the help of kernel functions which is called kernel
k-means clustering Girolami The kernelized variant can also be optimized with a spectral
relaxation approach using kernel PCA KPCA Sch?olkopf instead of canonical PCA.
In many modern applications samples have multiple feature representations views coming
from different data sources Instead of using only one of the views it is better to use all available information and let the learning algorithm decide how to combine these data sources which is known
as multiview learning There are three main categories for the combination strategy Noble
combination at the feature level by concatenating the views early integration combination at the decision level by concatenating the outputs of learners trained on each view separately
late integration and iii combination at the learning level by trying to find a unified distance
kernel or latent matrix using all views simultaneously intermediate integration
Related work
When we have multiple views for clustering we can simply concatenate the views and train a standard clustering algorithm on the concatenated view which is known as early integration However
this approach does not assign weights to the views and the view with the highest number of features
might dominate the clustering step due to the unsupervised nature of the problem
Late integration algorithms obtain a clustering on each view separately and combine these clustering
results using an ensemble learning scheme Such clustering algorithms are also known as cluster
ensembles Strehl and Ghosh However they do not exploit the dependencies between the
views during clustering and these dependencies might already be lost if we combine only clustering
results in the second step
Intermediate integration algorithms combine the views in a single learning scheme to collectively
find a unified clustering Chaudhuri propose to extract a unifying feature representation
from the views by performing canonical correlation analysis CCA Hotelling and to train
a clustering algorithm on this common representation Similarly Blaschko and Lampert extract a common feature representation but with a nonlinear projection step using kernel CCA Lai
and Fyfe and then perform clustering Such CCA-based algorithms assume that all views are
informative and if there are some noisy views this can degrade the clustering performance drastically Lange and Buhmann propose to optimize the weights of a convex combination of
view-specific similarity measures within a nonnegative matrix factorization framework and to assign samples to clusters using the latent matrices obtained in the factorization step Valizadegan and
Jin extend the maximum margin clustering formulation of Xu to perform kernel combination and clustering jointly by formulating a semidefinite programming SDP problem
Chen further improve this idea by formulating a quadratically constrained quadratic
programming problem instead of an SDP problem Tang convert the views into graphs
by placing samples into vertices and creating edges using the similarity values between samples
in each view and then factorize these graphs jointly with a shared factor common to all graphs
which is used for clustering at the end Kumar propose a co-regularization strategy
for multiview spectral clustering by enforcing agreement between the similarity matrices calculated
on the latent representations obtained from the spectral decomposition of each view Huang
formulate another multiview spectral clustering method that finds a weighted combination
of the affinity matrices calculated on the views Yu develop a multiple kernel k-means
clustering algorithm that optimizes the weights in a conic sum of kernels calculated on the views
However their formulation uses the same kernel weights for all of the samples
Multiview clustering algorithms have attracted great interest in cancer biology due to the availability
of multiple genomic characterizations of cancer patients Yuan formulate a patientspecific data fusion algorithm that uses a nonparametric Bayesian model coupled with a Markov
chain Monte Carlo inference scheme which can combine only two views and is computationally
very demanding due to the high dimensionality of genomic data Shen and Mo
find a shared latent subspace across genomic views and cluster cancer patients using their
representations in this subspace Wang construct patient networks from patient?patient
similarity matrices calculated on the views combine these into a single unified network using a
network fusion approach and then perform clustering on the final patient network
Our contributions
Intermediate integration using kernel matrices is also known as multiple kernel learning MKL
G?onen and Alpayd?n Most of the existing MKL algorithms use the same kernel weights
for all samples which may not be a good idea due to sample-specific characteristics of the data or
measurement noise present in some of the views In this work we study kernel k-means clustering under the multiview setting and propose a novel MKL algorithm that combines kernels with
sample-specific weights to obtain a better clustering We demonstrate the better performance of our
algorithm on the human colon and rectal cancer data set provided by TCGA consortium The Cancer
Genome Atlas Network where we use three genomic characterizations of the patients
DNA copy number mRNA gene expression and DNA methylation for clustering Our localized
data fusion approach obtains more relevant prognostic patient groups than global fusion approaches
when we evaluate the results with respect to three commonly used clinical biomarkers microsatellite instability hypermutation and mutation in BRAF gene of colon and rectal cancer
Kernel k-means clustering
We first review kernel k-means clustering Girolami before extending it to the multiview
setting Given independent and identically distributed samples we assume that
there is a function that maps the samples into a feature space in which we try to minimize a
sum-of-squares cost function over the cluster assignment variables zic
The optimization
problem defines kernel k-means clustering as a binary integer programming problem where
nc is the number of samples assigned to cluster and is the centroid of cluster
minimize
zic k?(xi
with respect to zic
subject to
zic
where nc
zic
zic
nc
We can convert this optimization problem into an equivalent matrix-vector form problem as follows
minimize tr
with respect to
subject to Z1k 1n
where
ZLZ
diag
n2 nk
Using that tr tr and the objective function of the
optimization problem can be rewritten as
tr tr ZLZ ZLZ
tr ZLZ ZLZ ZLZ
tr 2KZLZ KZLZ ZLZ tr KZL
where is the kernel matrix that holds the similarity values between the samples and is defined
as taking the square root of the diagonal elements The resulting optimization problem is a
trace maximization problem but it is still very difficult to solve due to the binary decision variables
maximize tr KZL
with respect to
subject to Z1k 1n
However we can formulate a relaxed version of this optimization problem by renaming ZL as
and letting take arbitrary real values subject to orthogonality constraints
maximize tr KH
with respect to Rn?k
subject to Ik
The final optimization problem can be solved by performing KPCA on the kernel matrix
and setting to the eigenvectors that correspond to largest eigenvalues Sch?olkopf
We can finally extract a clustering solution by first normalizing all rows of to be on the
unit sphere and then performing k-means clustering on this normalized matrix Note that after the
normalization step contains k-dimensional representations of the samples on the unit sphere and
k-means is not very sensitive to initialization in such a case
Multiple kernel k-means clustering
In a multiview learning scenario we have multiple feature representations where we assume that
each representation has its own mapping function Instead of an unweighted
combination of these views simple concatenation we can obtain a weighted mapping function
by concatenating views using a convex sum nonnegative weights that sum up to This
corresponds to replacing with
where Rp is the vector of kernel weights that we need to optimize during training The kernel
function defined over the weighted mapping function becomes
h?m
km
where we combine kernel functions using a conic sum nonnegative weights which guarantees
to have a positive semi-definite kernel function at the end The optimization problem gives
the trace maximization problem we need to solve
maximize tr
with respect to Rn?k Rp
subject to Ik 1p
where
Km
We solve this problem using a two-step alternating optimization strategy Optimize given
If we know the kernel weights initialize randomly in the first iteration solving reduces
to solving with the combined kernel matrix which requires performing KPCA on
Optimize given H. If we know the eigenvectors from the first step solving reduces to
solving which is a convex quadratic programming problem with decision variables
and one equality constraint and is solvable with any standard QP solver up to a moderate number
of kernels
minimize
tr Km Km
with respect to Rp
subject to 1p
Note that
Ppusing a convex combination of kernels in is not a viable option because if we set
to Km there would be a trivial solution to the trace maximization problem with a
single active kernel and others with zero weights which is also observed by Yu
Localized multiple kernel k-means clustering
Instead of using the same kernel weights for all samples we propose to use a localized data fusion approach by assigning sample-specific weights to kernels which enables us to capture samplespecific characteristics of the data and to get rid of sample-specific noise that may be present in
some of the views In our localized combination approach the mapping function is represented as
ip where Rn?p
is the matrix of
sample-specific kernel weights which are nonnegative and sum up to for each sample G?onen and
Alpayd?n The locally combined kernel function can be written as
h?im jm
im jm km
where we are guaranteed to have a positive semi-definite kernel function The optimization problem
gives the trace maximization problem with the locally combined kernel matrix where
Rn is the vector of kernel weights assigned to kernel and denotes the Hadamard product
maximize tr
with respect to Rn?k Rn?p
subject to Ik 1n
where
Km
We solve this problem using a two-step alternating optimization strategy Optimize given
If we know the sample-specific kernel weights initialize randomly in the first iteration solving
reduces to solving with the combined kernel matrix which requires performing
KPCA on Optimize given H. If we know the eigenvectors from the first step using that
tr solving reduces to solving which is
a convex QP problem with decision variables and equality constraints
minimize
HH Km
n?p
with respect to
subject to 1n
Training the localized combination approach requires more computational effort than training the
global approach due to the increased size of QP problem in the second step However the blockdiagonal structure of the Hessian matrix in can be exploited to solve this problem much
more efficiently Note that the objective function of can be written as
In HH K1
In HH K2
In HH Kp
where we have an matrix for each kernel on the diagonal of the Hessian matrix
Experiments
Clustering patients is one of the clinically important applications in cancer biology because it helps
to identify prognostic cancer subtypes and to develop personalized strategies to guide therapy Making use of multiple genomic characterizations in clustering is critical because different patients may
manifest their disease in different genomic platforms due to cancer heterogeneity and measurement
noise We use the human colon and rectal cancer data set provided by TCGA consortium The Cancer Genome Atlas Network which contains several genomic characterizations of the patients
to test our new clustering algorithm in a challenging real-world application
We use DNA copy number mRNA gene expression and DNA methylation data of the patients
for clustering In order to evaluate the clustering results we use three commonly used clinical
biomarkers of colon and rectal cancer The Cancer Genome Atlas Network micro-satellite
instability a hypermutable phenotype caused by the loss of DNA mismatch repair activity
hypermutation defined as having mutations in more than or equal to genes and iii mutation in BRAF gene Note that these three biomarkers are not directly identifiable from the input
data sources used The preprocessed genomic characterizations of the patients can be downloaded
from a public repository at https://www.synapse.org/#!Synapse:syn300013 where
DNA copy number mRNA gene expression DNA methylation and mutation data consist of
and features respectively The micro-satellite instability data can be downloaded from https://tcga-data.nci.nih.gov/tcga/dataAccessMatrix.htm In
the resulting data set there are patients with available genomic and clinical biomarker data
We implement kernel k-means clustering and its multiview variants in Matlab Our implementations
are publicly available at https://github.com/mehmetgonen/lmkkmeans We solve the
QP problems of the multiview variants using the Mosek optimization software Mosek For
all methods we perform replications of k-means with different initializations as the last step and
use the solution with the lowest sum-of-squares cost to decide cluster memberships
We calculate four different kernels to use in our experiments KC the Gaussian kernel on DNA
copy number data KG the Gaussian kernel on mRNA gene expression data iii KM the
Gaussian kernel on DNA methylation data and KCGM the Gaussian kernel on concatenated
data early combination Before calculating each kernel the input data is normalized to have
zero mean and unit standard deviation z-normalization for each feature For each kernel we
set the kernel width parameter to the square root of the number of features in its corresponding view
We compare seven clustering algorithms on this colon and rectal cancer data set kernel k-means
clustering with KC kernel k-means clustering with KG iii kernel k-means clustering with KM
kernel k-means clustering with KCGM kernel k-means clustering with KC KG KM
multiple kernel k-means clustering with KC KG KM and vii localized multiple kernel kmeans clustering with KC KG KM The first three algorithms are single-view clustering methods
that work on a single genomic characterization The fourth algorithm is the early integration approach that combines the views at the feature level The fifth and sixth algorithms are intermediate
integration approaches that combine the kernels using unweighted and weighted sums respectively
where the latter is very similar to the formulations of Huang and Yu The
last algorithm is our localized MKL approach that combines the kernels in a sample-specific way
We assign three different binary labels to each sample as the ground truth using the three clinical
biomarkers mentioned and evaluate the clustering results using three different performance metrics
normalized mutual information purity and iii the Rand index We set the number
of clusters to for all of the algorithms because each ground truth label has only two categories
Cluster
pre
ex
ion
lat
Ge
ne
thy
Me
sio
We first show the kernel weights assigned to
colon and rectal cancer patients by our localized
data fusion approach As we can see from Figure some of the patients are very well characterized by their DNA copy number data Our localized algorithm assigns weights larger than
to DNA copy number data for most of the patients
in the second cluster whereas all three views are
used with comparable weights for the remaining
patients Note that the kernel weights of each patient are strictly nonnegative and sum up to
defined on the unit simplex Our proposed clustering algorithm can identify the most informative genomic platforms in an unsupervised and
patient-specific manner Together with the better clustering performance and biological interpretation presented next this particular application from cancer biology shows the potential for
localized combination strategy
Copy number
Figure Kernel weights assigned to patients
by our localized data fusion approach Each dot
denotes a single cancer patient and patients in
the same cluster are drawn with the same color
Figure summarizes the results obtained by seven clustering algorithms on the colon and rectal cancer data set For each algorithm the cluster assignment and the values of three clinical biomarkers
are aligned to each other and we report the performance values of nine biomarker?metric pairs We
see that DNA copy number KC is the most informative genomic characterization when we
compare the performance of single-view clustering algorithms where it obtains better results than
mRNA gene expression KG and DNA methylation KM in terms of NMI and RI on all
biomarkers We also see that the early integration strategy KCGM does not improve the results because mRNA gene expression and DNA methylation dominate the clustering step due to the
unsupervised nature of the problem However when we combine the kernels using an unweighted
combination strategy KC KG KM the performance values are significantly improved
compared to single-view clustering methods and early integration in terms of NMI and RI on all
biomarkers Instead of using an unweighted sum we can optimize the combination weights using
the multiple kernel k-means clustering of Section In this case the performance values are slightly
improved compared to the unweighted sum in terms of NMI and RI on all biomarkers Our localized data fusion approach significantly outperforms the other algorithms in terms of NMI and RI on
micro-satellite instability and hypermutation biomarkers and it is the only algorithm that can
obtain purity values higher than the ratio of the majority class samples on mutation in BRAF gene
biomarker These results validate the benefit of our localized approach for the multiview setting
Algorithm Kernel means clustering with KC
Clusters
patients
MSI high
Hypermutation
BRAF mutation
patients
Algorithm Kernel means clustering with KG
Clusters
patients
MSI high
Hypermutation
BRAF mutation
NMI
Purity
RI
NMI
Purity
RI
NMI
Purity
RI
NMI
Purity
RI
85 patients
NMI
Purity
RI
82 patients
NMI
Purity
RI
NMI
Purity
RI
87 patients
Algorithm Kernel means clustering with KM
Clusters
83 patients
MSI high
Hypermutation
BRAF mutation
patients
Algorithm Kernel means clustering with KCGM
Clusters
87 patients
MSI high
Hypermutation
BRAF mutation
patients
Algorithm Kernel means clustering with KC KG KM
Clusters
patients
MSI high
Hypermutation
BRAF mutation
Algorithm Multiple kernel means clustering with KG KM
Clusters
patients
MSI high
Hypermutation
BRAF mutation
Algorithm Localized multiple kernel means clustering with KG KM
Clusters
patients
MSI high
Hypermutation
BRAF mutation
46 patients
Figure Results obtained by seven clustering algorithms on the colon and rectal cancer data set
provided by TCGA consortium The Cancer Genome Atlas Network For each algorithm we
first display the cluster assignment and report the number of patients in each cluster We then display
the values of three clinical biomarkers aligned with the cluster assignment where MSI high shows
the patients with high micro-satellite instability status in darker color Hypermutation shows the
patients with mutations in more than or equal to genes in darker color and BRAF mutation
shows the patients with a mutation in their BRAF gene in darker color We compare the algorithms
in terms of their clustering performance on three clinical biomarkers under three metrics normalized
mutual information purity and the Rand index For all performance metrics a higher
value means better performance and for each biomarker?metric pair the best result is reported in
bold face We see that our localized clustering algorithm obtains the best result for eight out of nine
biomarker?metric pairs whereas all algorithms have the same purity value for BRAF mutation
Copy number
Gene expression
Methylation
Clusters
Mutation
Figure Important features in genomic views determined using the solution of multiple kernel
k-means clustering together with cluster assignment and mutations in frequently mutated genes
For each genomic view we calculate the Pearson correlation values between features and clustering
assignment and display topmost positively correlated and bottommost negatively correlated
features red high blue low We also display the mutation status black mutated white wildtype of patients for most frequently mutated genes which are mutated in at least patients
Copy number
Gene expression
Methylation
Clusters
Mutation
Figure Important features in genomic views determined using the solution of localized multiple
kernel k-means clustering together with cluster assignment and mutations in frequently mutated
genes See Figure for details
We perform an additional biological interpretation step by looking at the features that can be used
to differentiate the clusters found Figures and show features in genomic views that are highly
positively or negatively correlated with the cluster assignments of the two best performing algorithms in terms of clustering performance namely multiple kernel k-means clustering and localized
multiple kernel k-means clustering We clearly see that the genomic signatures of the hyper-mutated
cluster especially the one for DNA copy number obtained using our localized data fusion approach
are much less noisy than those of global data fusion Identifying clear genomic signatures are clinically important because they can be used for diagnostic and prognostic purposes on new patients
Discussion
We introduce a localized data fusion approach for kernel k-means clustering to better capture
sample-specific characteristics of the data in the multiview setting which can not be captured using
global data fusion strategies such as Huang and Yu The proposed method
is from the family of MKL algorithms and combines the kernels defined on the views with samplespecific weights to determine the relative importance of the views for each sample We illustrate the
practical importance of the method on a human colon and rectal cancer data set by clustering patients
using their three different genomic characterizations The results show that our localized data fusion
strategy can identify more relevant prognostic patient groups than global data fusion strategies
The interesting topics for future research are exploiting the special structure of the Hessian
matrix in our formulation by developing a customized solver instead of using an off-the-shelf optimization software to improve the time complexity and integrating prior knowledge about the
samples that we may have into our formulation to be able to find more relevant clusters
Acknowledgments This study was financially supported by the Integrative Cancer Biology Program grant no and the Cancer Target Discovery and Development CTDD Network grant no of the National Cancer Institute

<<----------------------------------------------------------------------------------------------------------------------->>

title: 1459-intrusion-detection-with-neural-networks.pdf

Intrusion Detection with Neural Networks
Jake Ryan
Department of Computer Sciences
The University of Texas at Austin
Austin TX
Department of Electrical and Computer Engineering
The University of Texas at Austin
Austin TX
raven@cs.utexas.edu
mj@orac.ece utexas.edu
Meng-Jang Lin
Risto Miikkulainen
Department of Computer Sciences
The University of Texas at Austin
Austin TX
risto@cs.utexas.edu
Abstract
With the rapid expansion of computer networks during the past few years
security has become a crucial issue for modern computer systems A
good way to detect illegitimate use is through monitoring unusual user
activity Methods of intrusion detection based on hand-coded rule sets or
predicting commands on-line are laborous to build or not very reliable
This paper proposes a new way of applying neural networks to detect
intrusions We believe that a user leaves a print when using the system
a neural network can be used to learn this print and identify each user
much like detectives use thumbprints to place people at crime scenes If
a user's behavior does not match hislher print the system administrator
can be alerted of a possible security breech A backpropagation neural
network called NNID Neural Network Intrusion Detector was trained
in the identification task and tested experimentally on a system of
users The system was accurate in detecting unusual activity with
false alarm rate These results suggest that learning user profiles is
an effective way for detecting intrusions
INTRODUCTION
Intrusion detection schemes can be classified into two categories misuse and anomaly
intrusion detection Misuse refers to known attacks that exploit the known vulnerabilities
of the system Anomaly means unusual activity in general that could indicate an intrusion
Currently MCI Communications Corp N. IH 35 Austin TX jake.ryan@mci.com
Ryan M-J. Lin and R. Miikkulainen
If the observed activity of a user deviates from the expected behavior an anomaly is said
to occur
Misuse detection can be very powerful on those attacks that have been programmed in
to the detection system However it is not possible to anticipate all the different attacks
that could occur and even the attempt is laborous Some kind of anomaly detection is
ultimately necessary One problem with anomaly detection is that it is likely to raise many
false alarms Unusual but legitimate use may sometimes be considered anomalous The
challenge is to develop a model of legitimate behavior that would accept novel legitimate
use
It is difficult to build such a model for the same reason that it is hard to build a comprehensive misuse detection system it is not possible to anticipate aU possible variations of such
behavior The task can be made tractable in three ways Instead of general legitimate
use the behavior of individual users in a particular system can be modeled The task of
characterizing regular patterns in the behavior of an individual user is an easier task than
trying to do it for aU users simultaneously The patterns of behavior can be learned
for examples of legitimate use instead of having to describe them by hand-COding possible
behaviors Detecting an intrusion real-time as the user is typing commands is very
difficult because the order of commands can vary a lot In many cases it is enough to recognize that the distribution of commands over the entire login session or even the entire
day differs from the usual
The system presented in this paper NNID Neural Network Intrusion Detector is based on
these three ideas NNID is a backpropagation neural network trained to identify users based
on what commands they use during a day The system administrator runs NNID at the end
of each day to see if the users sessions match their normal pattern If not an investigation
can be launched The NNID model is implemented in a UNIX environment and consists of
keeping logs of the commands executed forming command histograms for each user and
learning the users profiles from these histograms NNID provides an elegant solution to
off-line monitoring utilizing these user profiles In a system of users NNID was
accurate in detecting anomalous behavior random usage patterns with a false alarm
rate of These results show that a learning offline monitoring system such as NNID
can achieve better performance than systems that attempt to detect anomalies on-line in the
command sequences and with computationally much less effort
The rest of the paper outlines other approaches to intrusion detection and motivates the
NNID approach in more detail sections and presents the implementation and an
evaluation on a real-world computer system sections and and outlines some open
issues and avenues for future work section
INTRUSION DETECTION SYSTEMS
Many misuse and anomaly intrusion detection systems lDSs are based on the general
model proposed by Denning This model is independent of the platform system vulnerability and type of intrusion It maintains a set of historical profiles for users matches
an audit record with the appropriate profile updates the profile whenever necessary and reports any anomalies detected Another component a rule set is used for detecting misuse
Actual systems implement the general model with different techniques Frank
Mukherjee for an overview Often statistical methods are used to measure how
anomalous the behavior is that is how different the commands used are from normal
behavior Such approaches require that the distribution of subjects behavior is known
The behavior can be represented as a rule-based model Garvey and Lunt in terms
of predictive pattern generation Teng or using state transition analysis Porras
Intrusion Detection with Neural Networks
Pattern matching techniques are then used to detennine whether the sequence
of events is part of normal behavior constitutes an anomaly or fits the description of a
known attack
IDSs also differ in whether they are on-line or off-line Off-line IDSs are run periodically and they detect intrusions after-the-fact based on system logs On-line systems are
designed to detect intrusions while they are happening thereby allowing for quicker intervention On-line IDSs are computationally very expensive because they require continuous
monitoring Decisions need to be made quickly with less data and therefore they are not as
reliable
Several IDSs that employ neural networks for on-line intrusion detection have been proposed Debar Fox These systems learn to predict the next command based on a sequence of previous commands by a specific user Through a shifting
window the network receives the most recent commands as its input The network is
recurrent that is part of the output is fed back as the input for the next step thus the
network is constantly observing the new trend and forgets old behavior over time The
size of the window is an important parameter If is too small there will be many false
positives if it is too big the network may not generalize well to novel sequences The most
recent of such systems Debar can predict the next command correctly around
of the time and accept a command as predictable among the three most likely next
commands of the time
One problem with the on-line approach is that most of the effort goes into predicting the
order of commands In many cases the order does not matter much but the distribution of
commands that are used is revealing A possibly effective approach could therefore be to
collect statistics about the users command usage over a period of time such as a day and
try to recognize the distribution of commands as legitimate or anomalous off-line This is
the idea behind the NNID system
THE NNID SYSTEM
The NNID anomaly intrusion detection system is based on identifying a legitimate user
based on the distribution of commands she or he executes This is justifiable because
different users tend to exhibit different behavior depending on their needs of the system
Some use the system to send and receive e-mail only and do not require services such as
programming and compilation Some engage in all kinds of activities including editing
programming e-mail Web browsing and so on However even two users that do the same
thing may not use the same application program For example some may prefer the
editor to emacs favor pine over elm as their mail utility program or use gcc more
often than to compile programs Also the frequency with which a command is
used varies from user to user The set of commands used and their frequency therefore
constitutes a print of the user reflecting the task performed and the choice of application
programs and it should be possible to identify the user based on this information
It should be noted that this approach works even if some users have aliases set up as shorthands for long commands they use frequently because the audit log records the actual
commands executed by the system Users privacy is not violated since the arguments to
a command do not need to be recorded That is we may know that a user sends e-mail five
times a day but we do not need to know to whom the mail is addressed
Building NNID for a particular computer system consists of the following three phases
Collecting training data Obtain the audit logs for each user for a period of several
days For each day and user form a vector that represents how often the user
executed each command
Ryan M-J. Un and R. Miikkulainen
as
cut
expr
ghostview
Id
man
netstat
rm
tcsh
vi
awk
cvs
fgrep
gmake
fess
mesg
nm
rsh
tee
virtex
be
date
filter
grep
look
metamail
objdump
sed
test
61btex
df
find
gs
Ipq
rillCdir
perl
sendmail
tgif
wc
calendar
diff
finger
gzip
Ipr
more
pgp
sh
top
whereis
cat
du
fmt
hostname
Iprm
movemail
ping
sort
tput
xbiff
chmOd
dvips
from
id
Is
mpage
ps
strip
tr
xca1c
comsat
egrep
ftp
ifConfig
machine
mt
pwd
stty
tty
xdvi
cp
elm
gcc
Ispell
mail
mv
rcp
tail
uname
xhost
cpp
emacs
gdb
fast
make
netscape
resize
tar
vacation
xterm
Table The commands used to describe user behavior The number of times the user
executed each of these commands during the day was recorded mapped into a nonlinear scale of
intervals and concatenated into a l00-dimensional input vector representing the usage pattern for
that user for that day
Training Train the neural network to identify the user based on these command
distribution vectors
Perfonnance Let the network identify the user for each new command distribution vector If the network's suggestion is different from the actual user or if the
network does not have a clear suggestion signal an anomaly
The particular implementation of NNID and the environment where it was tested is described in the next section
EXPERIMENTS
The NNID system was built and tested on a machine that serves a particular research group
at the Department of Electrical and Computer Engineering at the University of Texas at
Austin This machine has total users some are regular users with several other users
logging in intennittently This platfonn was chosen for three reasons
The operating system NetBSD provides audit trail logging for accounting purposes and this option had been enabled on this system
The number of users and the total number of commands executed per day are on
an order of magnitude that is manageable Thus the feasibility of the approach
could be tested with real-world data without getting into scalability issues
The system is relatively unknown to outsiders and the users are all known to us so
that it is likely that the data collected on it consists of nonnal user behavior free
of intrusions
Data was collected on this system for days resulting in 89 user-days Instead of trying
to optimize the selection of features commands for the input we decided to simply use
a set of most common commands in the logs listed in Table and let the network
figure out what infonnation was important and what superfluous Intelligent selection of
features might improve the results some but the current approach is easy to implement and
proves the point
In order to introduce more overlap between input vectors and therefore better generalization the number of times a command was used was divided into intervals There were
intervals non-linearly spaced so that the representation is more accurate at lower frequencies where it is most important The first interval meant the command was never used the
second that it was used once or twice and so on until the last interval where the command
was used more than times The intervals were represented by values from to
in increments These values one for each command were then concatenated into a
100-dimensional command distribution vector also called user vector below to be used as
input to the neural network
Intrusion Detection with Neural Networks
The standard three-layer backpropagation architecture was chosen for the neural network
The idea was to get results on the most standard and general architecture so that the feasibility of the approach could be demonstrated and the results would be easily replicable
More sophisticated architectures could be used and they would probably lead to slightly
better results The input layer consisted of units representing the user vector the hidden layer had units and the output layer units one for each user The network was
implemented in the PlaNet Neural Network simulator Miyata
RESULTS
To avoid overtraining several training sessions were run prior to the actual experiments to
see how many training cycles would give the highest performance The network was trained
on randomly chosen days of data user vectors and its performance was tested on the
remaining days vectors after epochs and of which gave
the best performance Four splits of the data into training and testing sets were created by
randomly picking days for training The reSUlting four networks were tested in two tasks
Identifying the user vectors of the remaining days If the activation of the output
unit representing the correct user was higher than those of all other units and
also higher than the identification was counted as correct Otherwise a false
positive was counted
Identifying randomly-generated user vectors If all output units had an activation less than the network was taken to correctly identify the vector as an
anomaly not any of the known users in the system Otherwise the most
highly active output unit identifies the network's suggestion Since all intrusions
occur under one of the user accounts there is a chance that the suggestion
would accidentally match the compromised user account and the intrusion would
not be detected Therefore of all such cases were counted as false negatives
The second test is a suggestive measure of the accuracy of the system It is not possible to
come up with vectors that would represent a good sampling of actual intrusions the idea
here was to generate vectors where the values for each command were randomly drawn
from the distribution of values for that command in the entire data set In other words the
random test vectors had the same first-order statistics as the legitimate user vectors but
had no higher-order correlations Therefore they constitute a neutral but realistic sample of
unusual behavior
All four splits led to similar results On average the networks rejected of the random
user vectors leading to an anomaly detection rate of They correctly identified the
legitimate user vectors of the time giving a false alarm rate of
Figure shows the output of the network for one of the splits Out of 24 legitimate user
vectors the network identified 22 Most of the time the correct output unit is very highly
activated indicating high certainty of identification However the activation of the highest
unit was below for two of the inputs resulting in a false alarm
Interestingly in all false alarms in all splits the falsely-accused user was always the same
A closer look at the data set revealed that there were only days of data on this user He
used the system very infrequently and the network could not learn a proper profile for him
While it would be easy to fix this problem by collecting more data in this case we believe
this is a problem that would be difficult to rule out in general No matter how much data
one collects there may still not be enough for some extremely infrequent user Therefore
we believe the results obtained in this rather small data set give a realistic picture of the
performance of the NNID system
Ryan M-l. lin and R. Miikkulainen
D.itrut
Eo
tI
tI
O.lt('t-rt
xr~ct
ti
Figure User identification with the NNID Network The output layer of NNID is shown for
each of the 24 test vectors in one of the splits tested The output units are lined up from left to
right and their activations are represented by the size of the squares In this split there were two false
alarms one is displayed in the top right with activation and one in the second row from the
bottom second column from the left with All the other test vectors are identified correctly with
activation higher than
DISCUSSION AND FUTURE WORK
An important question is how well does the performance of NNID scale with the number
of users Although there are many computer systems that have no more than a dozen
users most intrusions occur in larger systems with hundreds of users With more users
the network would have to make finer distinctions and it would be difficult to maintain the
same low level of false alarms However the rate of detecting anomalies may not change
much as long as the network can learn the user patterns well Any activity that differs from
the user's normal behavior would still be detected as an anomaly
Training the network to represent many more users may take longer and require a larger
network but it should be possible because the user profiles share a lot of common structure and neural networks in general are good at learning such data Optimizing the set of
commands included in the user vector and the size of the value intervals might also have a
large impact on performance It would be interesting to determine the curve of performance
Intrusion Detection with Neural Networks
versus the number of users and also see how the size of the input vector and the granularity
of the value intervals affect that curve This is the most important direction of future work
Another important issue is how much does a user's behavior change over time If behavior
changes dramatically NNID must be recalibrated often or the number of false positives
would increase Fortunately such retraining is easy to do Since NNID parses daily activity
of each user into a user-vector the user profile can be updated daily NNID could then be
retrained periodically In the current system it takes only about 90 seconds and would not
be a great burden on the system
CONCLUSION
Experimental evaluation on real-world data shows that NNID can learn to identify users
simply by what commands they use and how often and such an identification can be used
to detect intrusions in a network computer system The order of commands does not need
to be taken into account NNID is easy to train and inexpensive to run because it operates
off-line on daily logs As long as real-time detection is not required NNID constitutes a
promising practical approach to anomaly intrusion detection
Acknowledgements
Special thanks to Mike Dahlin and Tom Ziaja for feedback on an earlier version of this paper and to
Jim Bednar for help with the PlaNet simulator This research was supported in part by DOD-ARPA
contract NSF grant and the Texas Higher Education Coordinating
board grant

<<----------------------------------------------------------------------------------------------------------------------->>

