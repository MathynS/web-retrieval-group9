query sentence: over-simplified ethics of webspam
---------------------------------------------------------------------
title: 4778-one-permutation-hashing.pdf

One Permutation Hashing
Ping Li
Department of Statistical Science
Cornell University
Art Owen
Department of Statistics
Stanford University
Cun-Hui Zhang
Department of Statistics
Rutgers University
Abstract
Minwise hashing is a standard procedure in the context of search for efficiently
estimating set similarities in massive binary data such as text Recently b-bit
minwise hashing has been applied to large-scale learning and sublinear time nearneighbor search The major drawback of minwise hashing is the expensive preprocessing as the method requires applying to permutations
on the data This paper presents a simple solution called one permutation hashing
Conceptually given a binary data matrix we permute the columns once and divide
the permuted columns evenly into bins and we store for each data vector the
smallest nonzero location in each bin The probability analysis illustrates that this
one permutation scheme should perform similarly to the original k-permutation
minwise hashing Our experiments with training SVM and logistic regression confirm that one permutation hashing can achieve similar even better accuracies
compared to the k-permutation scheme See more details in
Introduction
Minwise hashing is a standard technique in the context of search for efficiently computing
set similarities Recently b-bit minwise hashing which stores only the lowest bits of
each hashed value has been applied to sublinear time near neighbor search and learning
on large-scale high-dimensional binary data text A drawback of minwise hashing is that it
requires a costly preprocessing step for conducting permutations on the data
Massive High-Dimensional Binary Data
In the context of search text data are often processed to be binary in extremely high dimensions A
standard procedure is to represent documents Web pages using w-shingles contiguous
words where in several studies This means the size of the dictionary needs to be
substantially increased from common English words to super-words In current
practice it appears sufficient to set the total dimensionality to be for convenience Text
data generated by w-shingles are often treated as binary The concept of shingling can be naturally
extended to Computer Vision either at pixel level for aligned images or at Visual feature level
In machine learning practice the use of extremely high-dimensional data has become common For
example discusses training datasets with on average items and distinct
features experimented with a dataset of potentially trillion unique features
Minwise Hashing and b-Bit Minwise Hashing
Minwise hashing was mainly designed for binary data A binary data vector can be viewed as
a set locations of the nonzeros Consider sets Si where the size of
the space is often set as in industrial applications The similarity between two sets S1 and
S2 is commonly measured by the resemblance which is a version of the normalized inner product
a
S2
where f1 f2 a S2
S2
f1 f2 a
For large-scale applications the cost of computing resemblances exactly can be prohibitive in time
space and energy-consumption The minwise hashing method was proposed for efficient computing
resemblances The method requires applying independent random permutations on the data
Denote a random permutation The hashed values are the two minimums of
and The probability at which the two hashed values are equal is
S2
Pr
S2
One can then estimate from independent permutations
min(?j
Var
Because the indicator function min(?j can be written as an inner product
between two binary vectors each having only one in dimensions
min(?j
we know that minwise hashing can be potentially used for training linear SVM and logistic regression on high-dimensional binary data by converting the permuted data into a new data matrix in
dimensions This of course would not be realistic if
The method of b-bit minwise hashing provides a simple solution by storing only the lowest
bits of each hashed data reducing the dimensionality of the expanded hashed data matrix to just
2b applied this idea to large-scale learning on the webspam dataset and demonstrated that
using and to could achieve very similar accuracies as using the original data
The Cost of Preprocessing and Testing
Clearly the preprocessing of minwise hashing can be very costly In our experiments loading the
webspam dataset samples about million features and about in Libsvm/svmlight
text format used in took about seconds when the data were stored in text format and
took about seconds after we converted the data into binary In contrast the preprocessing cost for
was about seconds Note that compared to industrial applications the webspam
dataset is very small For larger datasets the preprocessing step will be much more expensive
In the testing phrase search or learning if a new data point a new document or a new
image has not been processed then the total cost will be expensive if it includes the preprocessing
This may raise significant issues in user-facing applications where the testing efficiency is crucial
Intuitively the standard practice of minwise hashing ought to be very wasteful in that all the
nonzero elements in one set are scanned permuted but only the smallest one will be used
Our Proposal One Permutation Hashing
13
Figure Consider S1 S2 S3 We apply one permutation on the
sets and present and as binary vectors where
and We divide the space evenly into bins select the smallest
nonzero in each bin and re-index the selected elements as and For
now we use for empty bins which occur rarely unless the number of nonzeros is small compared to
As illustrated in Figure the idea of one permutation hashing is simple We view sets as vectors
in dimensions so that we can treat a collection of sets as a binary data matrix in dimensions
After we permute the columns features of the data matrix we divide the columns evenly into
parts bins and we simply take for each data vector the smallest nonzero element in each bin
In the example in Figure which concerns sets the sample selected from is
where we use to denote an empty bin for the time being Since only want to compare elements
with the same bin number so that we can obtain an inner product we can actually re-index the
elements of each bin to use the smallest possible representations For example for after
re-indexing the sample becomes 13
We will show that empty bins occur rarely unless the total number of nonzeros for some set is small
compared to and we will present strategies on how to deal with empty bins should they occur
Advantages of One Permutation Hashing
Reducing permutations to just one permutation a few is much more computationally
efficient From the perspective of energy consumption this scheme is desirable especially considering that minwise hashing is deployed in the search industry Parallel solutions GPU
which require additional hardware and software implementation will not be energy-efficient
In the testing phase if a new data point a new document or a new image has to be first processed with permutations then the testing performance may not meet the demand in for example
user-facing applications such as search or interactive visual analytics
One permutation hashing should be easier to implement from the perspective of random number
generation For example if a dataset has one billion features we can simply generate a
permutation vector of length the memory cost of which is not significant
On the other hand it would not be realistic to store a permutation matrix of size if
and instead one usually has to resort to approximations such as universal hashing
Universal hashing often works well in practice although theoretically there are always worst cases
One permutation hashing is a better matrix sparsification scheme In terms of the original binary data
matrix the one permutation scheme simply makes many nonzero entries be zero without further
damaging the matrix Using the k-permutation scheme we store for each permutation and each
row only the first nonzero and make all the other nonzero entries be zero and then we have to
concatenate such data matrices This significantly changes the structure of the original data matrix
Related Work
One of the authors worked on another one permutation scheme named Conditional Random Sampling CRS since Basically CRS continuously takes the bottom-k nonzeros after
applying one permutation on the data then it uses a simple trick to construct a random sample for
each pair with the effective sample size determined at the estimation stage By taking the nonzeros
continuously however the samples are no longer aligned and hence we can not write the estimator as an inner product in a unified fashion commented that using CRS for linear learning
does not produce as good results compared to using b-bit minwise hashing Interestingly in the
original minwise hashing paper we use quotes because the scheme was not called minwise
hashing at that time only one permutation was used and a sample was the first nonzeros after
the permutation Then they quickly moved to the k-permutation minwise hashing scheme
We are also inspired by the work on very sparse random projections and very sparse stable
random projections The regular random projection method also has the expensive preprocessing cost as it needs a large number of projections showed that one can substantially reduce the preprocessing cost by using an extremely sparse projection matrix The preprocessing cost of very sparse random projections can be as small as merely doing one projection See www.stanford.edu/group/mmds/slides2012/s-pli.pdf for the experimental results on clustering/classification/regression using very sparse random projections
This paper focuses on the fixed-length scheme as shown in Figure The technical report
also describes a variable-length scheme Two schemes are more or less equivalent although the fixed-length scheme is more convenient to implement and it is slightly more
accurate The variable-length hashing scheme is to some extent related to the Count-Min
sketch and the Vowpal Wabbit hashing algorithms
Applications of Minwise Hashing on Efficient Search and Learning
In this section we will briefly review two important applications of the k-permutation b-bit minwise
hashing sublinear time near neighbor search and large-scale linear learning
Sublinear Time Near Neighbor Search
The task of near neighbor search is to identify a set of data points which are most similar to
a query data point Developing efficient algorithms for near neighbor search has been an active
research topic since the early days of modern computing In current practice methods
for approximate near neighbor search often fall into the general framework of Locality Sensitive
Hashing LSH The performance of LSH largely depends on its underlying implementation
The idea in is to directly use the bits from b-bit minwise hashing to construct hash tables
Specifically we hash the data points using random permutations and store each hash value using
bits For each data point we concatenate the resultant bk bits as a signature bk
This way we create a table of 2B buckets and each bucket stores the pointers of the data points
whose signatures match the bucket number In the testing phrase we apply the same permutations
to a query data point to generate a bk-bit signature and only search data points in the corresponding
bucket Since using only one table will likely miss many true near neighbors as a remedy we
independently generate tables The query result is the union of data points retrieved in tables
Index
01
Index
01
Data Points
38
empty
01
31 74
Data Points
99
32 97
01 49
33
26 79
Figure An example of hash tables with and
Figure provides an example with bits permutations and tables The size of
each hash table is 24 Given data points we apply permutations and store bits of
each hashed value to generate signatures times Consider data point For Table left
panel of Figure the lowest b-bits of its two hashed values are and and thus its signature
is in binary hence we place a pointer to data point in bucket number For Table right
panel of Figure we apply another permutations This time the signature of data point
becomes in binary and hence we place it in the last bucket Suppose in the testing phrase the
two signatures of a new data point are and respectively We then only search for
the near neighbors in the set 26 79 instead of the original set of data points
Large-Scale Linear Learning
The recent development of highly efficient linear learning algorithms is a major breakthrough Popular packages include SVMperf Pegasos Bottou?s SGD SVM and LIBLINEAR
Given a dataset RD the regularized logistic regression solves
the following optimization problem where is the regularization parameter
min wT
log e?yi
The regularized linear SVM solves a similar problem
min
w+C
max wT
In they apply random permutations on each binary feature vector and store the lowest
bits of each hashed value to obtain a new dataset which can be stored using merely nbk bits At
run-time each new data point has to be expanded into a 2b k-length vector with exactly
To illustrate this simple procedure provided a toy example with permutations Suppose for one data vector the hashed values are whose binary digits
are respectively Using bits
the binary digits are stored as which corresponds to in decimals At
run-time the b-bit hashed data are expanded into a new feature vector of length 2b
The same procedure is then applied to all feature vectors
Clearly in both applications near neighbor search and linear learning the hashed data have to be
aligned in that only the hashed data generated from the same permutation are interacted Note
that with our one permutation scheme as in Figure the hashed data are indeed aligned
Theoretical Analysis of the One Permutation Scheme
This section presents the probability analysis to provide a rigorous foundation for one permutation
hashing as illustrated in Figure Consider two sets S1 and S2 We first introduce two definitions
for the number of jointly empty bins and the number of matched bins respectively
Nemp
Iemp,j
Nmat
Imat,j
where Iemp,j and Imat,j are defined for the j-th bin as
if both and are empty in the j-th bin
Iemp,j
otherwise
if both and are not empty and the smallest element of
matches the smallest element of in the j-th bin
Imat,j
otherwise
Recall the notation f1 f2 a S2 We also use S2 f1 f2 a
Lemma
fY
j+s
Pr Nemp
D?t
Assume k1 f1 f2 a
fY
k1
Nemp
D?j
fY
Nmat
Nemp
k?j
Cov Nmat Nemp
In practical scenarios the data are often sparse f1 f2 a D. In this case the upper
E(Nemp
bound k1 is a good approximation to the true value of
Since k1
e?f we know that the chance of empty bins is small when For example if then
k1 For practical applications we would expect that for most data pairs
otherwise hashing probably would not be too useful anyway This is why we do not expect empty
bins will significantly impact if at all the performance in practical settings
mat of the resemblance is unbiased
Lemma shows the following estimator
Lemma
Nmat
mat
Nemp
mat
ar
Nemp
Pr Nemp
Nemp
k?j
E(Nemp
mat
mat may seem surprising as in general ratio estimators are not unbiased
The fact that
Note that Nemp because we assume the original data vectors are not completely empty
all
zero As expected when f1 f2 a Nemp is essentially zero and hence ar Rmat
mat is a bit smaller than
In fact ar
especially for large
It is probably not surprising that our one permutation scheme slightly outperforms the original
k-permutation scheme at merely of the preprocessing cost because one permutation hashing
which is sampling-without-replacement provides a better strategy for matrix sparsification
Strategies for Dealing with Empty Bins
In general we expect that empty bins should not occur often because E(Nemp e?f which
is very close to zero if Recall S2 If the goal of using minwise hashing is for
data reduction reducing the number of nonzeros then we would expect that anyway
Nevertheless in applications where we need the estimators to be inner products we need strategies
to deal with empty bins in case they occur Fortunately we realize a retrospect simple strategy
which can be nicely integrated with linear learning algorithms and performs well
Frequency
Figure plots the histogram of the numbers of
Webspam
nonzeros in the webspam dataset which has
samples The average number of nonzeros is about
which should be much larger than
for the hashing procedure On the other hand about
of the samples have
nonzeros Thus we must deal with empty bins
nonzeros
if we do not want to exclude those data points For
Figure
Histogram
of the numbers of nonzeros
example if then Nemp
in the webspam dataset samples
which is not small
The strategy we recommend for linear learning is zero coding which is tightly coupled with the
strategy of hashed data expansion as reviewed in Sec. More details will be elaborated in
Sec. Basically we can encode as zero in the expanded space which means Nmat will
remain the same after taking the inner product in the expanded space This strategy which is
sparsity-preserving essentially corresponds to the following modified estimator
mat
Nmat
Nemp
Nemp
Pk
Pk
where Nemp Iemp,j and Nemp Iemp,j are the numbers of empty bins in
and respectively This modified estimator makes sense for a number of reasons
Basically since each data vector is processed and coded separately we actually do not know Nemp
the number of jointly empty bins until we see both and In other words we can not
really compute Nemp if we want toq
use linear estimators
On the other hand Nemp and Nemp are
always available In fact the use of Nemp Nemp in the denominator corresponds to the
normalizing step which is needed before feeding the data to a solver for SVM or logistic regression
mat When two original vectors
When Nemp Nemp Nemp is equivalent to the original
are very similar large Nemp and Nemp will be close to Nemp When two sets are highly
unbalanced using will overestimate however in this case Nmat will be so small that the
absolute error will not be large
The m-Permutation Scheme with
If one would like to further significantly reduce the chance of the occurrences of empty bins
here we shall mention that one does not really have to strictly follow one permutation since one
can always conduct permutations with k/m and concatenate the hashed data Once the
preprocessing is no longer the bottleneck it matters less whether we use permutation or
permutations The chance of having empty bins decreases exponentially with increasing
An Example of The Zero Coding Strategy for Linear Learning
Sec. reviewed the data-expansion strategy used by for integrating b-bit minwise hashing
with linear learning We will adopt a similar strategy with modifications for considering empty bins
We use a similar example as in Sec. Suppose we apply our one permutation hashing scheme and
use bins For the first data vector the hashed values are the
bin is empty Suppose again we use bits With the zero coding strategy our procedure
is summarized as follows
Original hashed values
Original binary representations
Lowest binary digits
Expanded 2b binary digits
01
New feature vector fed to a solver
We apply the same procedure to all feature vectors in the data matrix to generate a new data matrix
The normalization factor varies depending on the number of empty bins in the i-th vector
k?Nemp
Experimental Results on the Webspam Dataset
The webspam dataset has samples and features Each feature vector has on
average about nonzeros see Figure Following we use of samples for training
and the remaining for testing We conduct extensive experiments on linear SVM and logistic
regression using our proposed one permutation hashing scheme with 27 28 29 and
For convenience we use which is divisible by
There is one regularization parameter in linear SVM and logistic regression Since our purpose
is to demonstrate the effectiveness of our proposed hashing scheme we simply provide the results
for a wide range of values and assume that the best performance is achievable if we conduct
cross-validations This way interested readers may be able to easily reproduce our experiments
SVM 64
Webspam Accuracy
Original
Perm
Perm
SVM
Webspam Accuracy
logit 64
Webspam Accuracy
Original
Perm
Perm
SVM
Webspam Accuracy
98
96
94
92
90
88
86
84
82
logit
Webspam Accuracy
98
96
94
92
90
88
86
84
82
Original
Perm
Perm
SVM
Webspam Accuracy
98
96
94
92
90
88
86
84
82
Original
Perm
Perm
logit
Webspam Accuracy
Accuracy
Accuracy
98
96
94
92
90
88
86
84
82
98
96
94
92
90
88
86
84
82
Accuracy
98
96
94
92
90
88
86
84
82
Accuracy
Accuracy
Accuracy
98
96
94
92
90
88
86
84
82
Accuracy
Accuracy
Figure presents the test accuracies for both linear SVM upper panels and logistic regression bottom panels Clearly when even and b-bit one permutation hashing achieves
similar test accuracies as using the original data Also compared to the original k-permutation
scheme as in our one permutation scheme achieves similar even slightly better accuracies
98
96
94
92
90
88
86
84
82
Original
Perm
Perm
logit
Webspam Accuracy
Figure Test accuracies of SVM upper panels and logistic regression bottom panels averaged
over repetitions The accuracies of using the original data are plotted as dashed red if color is
available curves with diamond markers is the regularization parameter Compared with the
original k-permutation minwise hashing dashed and blue if color is available the one permutation
hashing scheme achieves similar accuracies or even slightly better accuracies when is large
The empirical results on the webspam datasets are encouraging because they verify that our proposed
one permutation hashing scheme performs as well as even slightly better than the original kpermutation scheme at merely of the original preprocessing cost On the other hand it would
be more interesting from the perspective of testing the robustness of our algorithm to conduct
experiments on a dataset news20 where the empty bins will occur much more frequently
Experimental Results on the News20 Dataset
The news20 dataset with samples and features is a very small dataset in not-toohigh dimensions The average number of nonzeros per feature vector is about which is also
small Therefore this is more like a contrived example and we use it just to verify that our one
permutation scheme with the zero coding strategy still works very well even when we let be
as large as most of the bins are empty In fact the one permutation schemes achieves
noticeably better accuracies than the original k-permutation scheme We believe this is because the
one permutation scheme is sample-without-replacement and provides a better matrix sparsification
strategy without contaminating the original data matrix too much
We experiment with 26 27 28 29 and for both one permutation scheme and k-permutation scheme We use samples for training and the other
samples for testing For convenience we let which is larger than
SVM 32
News20 Accuracy
Accuracy
Accuracy
90
85
75
70
65
SVM 64
News20 Accuracy
90
SVM
News20 Accuracy
75
90
85
75
SVM
News20 Accuracy
95
85
Original
Perm
Perm
75
65
70
SVM
News20 Accuracy
95
90
65
95
70
85
65
70
SVM
News20 Accuracy
95
95
95
90
85
75
70
65
55
Accuracy
Accuracy
95
90
85
75
70
65
55
Accuracy
Accuracy
95
90
85
75
70
65
55
Accuracy
Accuracy
Figure and Figure present the test accuracies for linear SVM and logistic regression respectively
When is small both the one permutation scheme and the original k-permutation
scheme perform similarly For larger values especially as however our one permutation scheme noticeably outperforms the k-permutation scheme Using the original data the test
accuracies are about Our one permutation scheme with and essentially achieves
the original test accuracies while the k-permutation scheme could only reach about
Original
Perm
Perm
75
65
85
70
SVM
News20 Accuracy
90
SVM
News20 Accuracy
logit 32
News20 Accuracy
95
Accuracy
90
85
75
70
65
logit 64
News20 Accuracy
90
75
85
Original
Perm
Perm
75
65
75
95
70
logit
News20 Accuracy
90
95
85
65
90
logit
News20 Accuracy
90
Original
Perm
Perm
85
75
70
logit
News20 Accuracy
70
logit
News20 Accuracy
95
85
65
70
logit
News20 Accuracy
95
Accuracy
95
90
85
75
70
65
55
Accuracy
Accuracy
95
90
85
75
70
65
55
Accuracy
Accuracy
95
90
85
75
70
65
55
Accuracy
Accuracy
Figure Test accuracies of linear SVM averaged over repetitions The one permutation scheme
noticeably outperforms the original k-permutation scheme especially when is not small
65
logit
News20 Accuracy
Figure Test accuracies of logistic regression averaged over repetitions The one permutation
scheme noticeably outperforms the original k-permutation scheme especially when is not small
Conclusion
A new hashing algorithm is developed for large-scale search and learning in massive binary data
Compared with the original k-permutation minwise hashing which is a standard
procedure in the context of search our method requires only one permutation and can achieve
similar or even better accuracies at merely of the original preprocessing cost We expect that one
permutation hashing its variant will be adopted in practice See more details in
Acknowledgement The research of Ping Li is partially supported by and The research of Art Owen
is partially supported by The research of Cun-Hui Zhang is partially supported by
and

----------------------------------------------------------------

title: 6145-a-multi-batch-l-bfgs-method-for-machine-learning.pdf

A Multi-Batch L-BFGS Method for Machine
Learning
Albert S. Berahas
Northwestern University
Evanston IL
albertberahas@u.northwestern.edu
Jorge Nocedal
Northwestern University
Evanston IL
j-nocedal@northwestern.edu
Martin Tak??c
Lehigh University
Bethlehem PA
takac.mt@gmail.com
Abstract
The question of how to parallelize the stochastic gradient descent SGD method
has received much attention in the literature In this paper we focus instead on batch
methods that use a sizeable fraction of the training set at each iteration to facilitate
parallelism and that employ second-order information In order to improve the
learning process we follow a multi-batch approach in which the batch changes
at each iteration This can cause difficulties because L-BFGS employs gradient
differences to update the Hessian approximations and when these gradients are
computed using different data points the process can be unstable This paper shows
how to perform stable quasi-Newton updating in the multi-batch setting illustrates
the behavior of the algorithm in a distributed computing platform and studies its
convergence properties for both the convex and nonconvex cases
Introduction
It is common in machine learning to encounter optimization problems involving millions of parameters
and very large datasets To deal with the computational demands imposed by such applications high
performance implementations of stochastic gradient and batch quasi-Newton methods have been
developed In this paper we study a batch approach based on the L-BFGS method that
strives to reach the right balance between efficient learning and productive parallelism
In supervised learning one seeks to minimize empirical risk
1X
def
fi
where denote the training examples and Rd is the composition of a
prediction function parametrized by and a loss function The training problem consists of finding
an optimal choice of the parameters Rd with respect to
min
w?Rd
1X
fi
At present the preferred optimization method is the stochastic gradient descent SGD method
and its variants 24 which are implemented either in an asynchronous manner when
Conference on Neural Information Processing Systems NIPS Barcelona Spain
using a parameter server in a distributed setting or following a synchronous mini-batch approach that
exploits parallelism in the gradient evaluation 22 A drawback of the asynchronous approach
is that it cannot use large batches as this would cause updates to become too dense and compromise
the stability and scalability of the method As a result the algorithm spends more time in
communication as compared to computation On the other hand using a synchronous mini-batch
approach one can achieve a near-linear decrease in the number of SGD iterations as the mini-batch
size is increased up to a certain point after which the increase in computation is not offset by the
faster convergence
An alternative to SGD is a batch method such as L-BFGS which is able to reach high training
accuracy and allows one to perform more computation per node so as to achieve a better balance
with communication costs Batch methods are however not as efficient learning algorithms as
SGD in a sequential setting To benefit from the strength of both methods some high performance
systems employ SGD at the start and later switch to a batch method
Multi-Batch Method In this paper we follow a different approach consisting of a single method
that selects a sizeable subset batch of the training data to compute a step and changes this batch at
each iteration to improve the learning abilities of the method We call this a multi-batch approach
to differentiate it from the mini-batch approach used in conjunction with SGD which employs a
very small subset of the training data When using large batches it is natural to employ a quasiNewton method as incorporating second-order information imposes little computational overhead
and improves the stability and speed of the method We focus here on the L-BFGS method which
employs gradient information to update an estimate of the Hessian and computes a step in flops
where is the number of variables The multi-batch approach can however cause difficulties to
L-BFGS because this method employs gradient differences to update Hessian approximations When
the gradients used in these differences are based on different data points the updating procedure can
be unstable Similar difficulties arise in a parallel implementation of the standard L-BFGS method if
some of the computational nodes devoted to the evaluation of the function and gradient are unable to
return results on time as this again amounts to using different data points to evaluate the function
and gradient at the beginning and the end of the iteration The goal of this paper is to show that stable
quasi-Newton updating can be achieved in both settings without incurring extra computational cost or
special synchronization The key is to perform quasi-Newton updating based on the overlap between
consecutive batches The only restriction is that this overlap should not be too small something that
can be achieved in most situations
Contributions We describe a novel implementation of the batch L-BFGS method that is robust in
the absence of sample consistency when different samples are used to evaluate the objective
function and its gradient at consecutive iterations The numerical experiments show that the method
proposed in this paper which we call the multi-batch L-BFGS method achieves a good balance
between computation and communication costs We also analyze the convergence properties of the
new method using a fixed step length strategy on both convex and nonconvex problems
The Multi-Batch Quasi-Newton Method
In a pure batch approach one applies a gradient based method such as L-BFGS to the
deterministic optimization problem When the number of training examples is large it is
natural to parallelize the evaluation of and by assigning the computation of the component
functions fi to different processors If this is done on a distributed platform it is possible for some
of the computational nodes to be slower than the rest In this case the contribution of the slow
unresponsive computational nodes could be ignored given the stochastic nature of the objective
function This leads however to an inconsistency in the objective function and gradient at the
beginning and at the end of the iteration which can be detrimental to quasi-Newton methods Thus
we seek to find a fault-tolerant variant of the batch L-BFGS method that is capable of dealing with
slow or unresponsive computational nodes
A similar challenge arises in a multi-batch implementation of the L-BFGS method in which the entire
training set is not employed at every iteration but rather a subset of the data is
used to compute the gradient Specifically we consider a method in which the dataset is randomly
divided into a number of batches say or and the minimization is performed with
respect to a different batch at every iteration At the k-th iteration the algorithm chooses a batch
Sk computes
fi wk
Sk wk
Sk
i?Sk
Sk wk gkSk
fi wk
Sk
i?Sk
and takes a step along the direction Hk gkSk where Hk is an approximation to wk Allowing the sample Sk to change freely at every iteration gives this approach flexibility of implementation
and is beneficial to the learning process as we show in Section We refer to Sk as the sample of
training points even though Sk only indexes those points
The case of unresponsive computational nodes and the multi-batch method are similar The main
difference is that node failures create unpredictable changes to the samples Sk whereas a multi-batch
method has control over sample generation In either case the algorithm employs a stochastic approximation to the gradient and can no longer be considered deterministic We must however distinguish
our setting from that of the classical SGD method which employs small mini-batches and noisy
gradient approximations Our algorithm operates with much larger batches so that distributing the
function evaluation is beneficial and the compute time of gkSk is not overwhelmed by communication
costs This gives rise to gradients with relatively small variance and justifies the use of a second-order
method such as L-BFGS
Robust Quasi-Newton Updating The difficulties created by the use of a different sample Sk at each
iteration can be circumvented if consecutive samples Sk and overlap so that Ok Sk
One can then perform stable quasi-Newton updating by computing gradient differences based on
this overlap by defining
Ok
gkOk
wk
in the notation given in The correction pair yk sk can then be used in the BFGS update
When the overlap set Ok is not too small yk is a useful approximation of the curvature of the objective
function along the most recent displacement and will lead to a productive quasi-Newton step This
observation is based on an important property of Newton-like methods namely that there is much
more freedom in choosing a Hessian approximation than in computing the gradient Thus a
smaller sample Ok can be employed for updating the inverse Hessian approximation Hk than for
computing the batch gradient gkSk in the search direction Hk gkSk In summary by ensuring that
unresponsive nodes do not constitute the vast majority of all working nodes in a fault-tolerant parallel
implementation or by exerting a small degree of control over the creation of the samples Sk in the
multi-batch method one can design a robust method that naturally builds upon the fundamental
properties of BFGS updating
We should mention in passing that a commonly used strategy for ensuring stability of quasi-Newton
updating in machine learning is to enforce gradient consistency to use the same sample
Sk to compute gradient evaluations at the beginning and the end of the iteration Another popular
remedy is to use the same batch Sk for multiple iterations alleviating the gradient inconsistency
problem at the price of slower convergence In this paper we assume that achieving such sample
consistency is not possible the fault-tolerant case or desirable a multi-batch framework and
wish to design a new variant of L-BFGS that imposes minimal restrictions in the sample changes
Specification of the Method
At the k-th iteration the multi-batch BFGS algorithm chooses a set Sk and computes a
new iterate
wk Hk gkSk
where is the step length gkSk is the batch gradient and Hk is the inverse BFGS Hessian
matrix approximation that is updated at every iteration by means of the formula
VkT Hk Vk sk sTk
Ts
yk
Vk I yk sTk
To compute the correction vectors sk yk we determine the overlap set Ok Sk consisting
of the samples that are common at the k-th and iterations We define
Ok wk
fi wk
Ok wk gkOk
fi wk
Ok
Ok
i?Ok
i?Ok
and compute the correction vectors as in In this paper we assume that is constant
In the limited memory version the matrix Hk is defined at each iteration as the result of applying
BFGS updates to a multiple of the identity matrix using a set of correction pairs si
kept in storage The memory parameter is typically in the range to When computing the
matrix-vector product in it is not necessary to form that matrix Hk since one can obtain this
product via the two-loop recursion using the most recent correction pairs si After the
step has been computed the oldest pair sj yj is discarded and the new curvature pair is stored
A pseudo-code of the proposed method is given below and depends on several parameters The
parameter denotes the fraction of samples in the dataset used to define the gradient
The parameter denotes the length of overlap between consecutive samples and is defined as a
fraction of the number of samples in a given batch
Algorithm Multi-Batch L-BFGS
Input w0 initial iterate for training set memory parameter
batch fraction of overlap fraction of batch iteration counter
Create initial batch S0
As shown in Firgure
for do
Calculate the search direction pk Hk gkSk
Using L-BFGS formula
Choose the step length
Compute wk pk
Create the next batch
Ok
Compute the curvature pairs wk and
gkOk
Replace the oldest pair si by
end for
Sample Generation
We now discuss how the sample is created at each iteration Line in Algorithm
Distributed Computing with Faults Consider a distributed implementation in which slave nodes
read the current iterate wk from the master node compute a local gradient on a subset of the
dataset and send it back to the master node for aggregation in the calculation Given a time
computational budget it is possible for some nodes to fail to return a result The schematic in
Figure 1a illustrates the gradient calculation across two iterations and in the presence of faults
Here Bi denote the batches of data that each slave node receives where Bi
is the gradient calculation using all nodes that responded within the preallocated time
and
MASTER
NODE
wk
S0
B1
B2
B1 wk
rf
B3
SLAVE
NODES
BB
B3 wk rf
BB wk
rf
wk
B1
B2
B3
B1
rf
MASTER
NODE
wk
rf
SHUFFLED DATA
O0
SHUFFLED DATA
S3
O3
S6
O6
BB
S1
O1
S4
O4
BB
rf
S2
O2
S5
O5
rf
Figure Sample and Overlap formation
Let Jk and be the set of indices of all nodes that returned a
gradient at the k-th and iterations respectively Using this notation Sk j?Jk Bj and
Bj and we define Ok j?Jk Bj The simplest implementation in this
setting preallocates the data on each compute node requiring minimal data communication only
one data transfer In this case the samples Sk will be independent if node failures occur randomly
On the other hand if the same set of nodes fail then sample creation will be biased which is harmful
both in theory and practice One way to ensure independent sampling is to shuffle and redistribute the
data to all nodes after a certain number of iterations
Multi-batch Sampling We propose two strategies for the multi-batch setting
Figure 1b illustrates the sample creation process in the first strategy The dataset is shuffled and
batches are generated by collecting subsets of the training set in order Every set except S0 is
of the form Sk Nk Ok where and Ok are the overlapping samples with batches
and respectively and Nk are the samples that are unique to batch Sk After each pass
through the dataset the samples are reshuffled and the procedure described above is repeated In our
implementation samples are drawn without replacement guaranteeing that after every pass epoch
all samples are used This strategy has the advantage that it requires no extra computation in the
Ok
evaluation of gkOk and
but the samples Sk are not independent
The second sampling strategy is simpler and requires less control At every iteration a batch Sk is
created by randomly selecting Sk elements from The overlapping set Ok is then formed
by randomly selecting Ok elements from Sk subsampling This strategy is slightly more expensive
Ok
since
requires extra computation but if the overlap is small this cost is not significant
Convergence Analysis
In this section we analyze the convergence properties of the multi-batch L-BFGS method Algorithm
when applied to the minimization of strongly convex and nonconvex objective functions using a
fixed step length strategy We assume that the goal is to minimize the empirical risk given in
but note that a similar analysis could be used to study the minimization of the expected risk
Strongly Convex case
Due to the stochastic nature of the multi-batch approach every iteration of Algorithm employs a
gradient that contains errors that do not converge to zero Therefore by using a fixed step length
strategy one cannot establish convergence to the optimal solution but only convergence to a
neighborhood of Nevertheless this result is of interest as it reflects the common practice of
using a fixed step length and decreasing it only if the desired testing error has not been achieved It
also illustrates the tradeoffs that arise between the size of the batch and the step length
In our analysis we make the following assumptions about the objective function and the algorithm
Assumptions A.
is twice continuously differentiable
and
I
such that I
for all Rd and all
There exist positive constants
sets
There is a constant such that ES k?F for all Rd and all sets
The samples are drawn independently and is an unbiased estimator of the true
gradient for all Rd ES
Note that Assumption implies that the entire Hessian also satisfies
I
Rd
for some constants Assuming that every sub-sampled function is strongly convex
is not unreasonable as a regularization term is commonly added in practice when that is not the case
We begin by showing that the inverse Hessian approximations Hk generated by the multi-batch
L-BFGS method have eigenvalues that are uniformly bounded above and away from zero The proof
technique used is an adaptation of that in
Lemma If Assumptions above hold there exist constants such that the
Hessian approximations Hk generated by Algorithm satisfy
I Hk I
for
Utilizing Lemma we show that the multi-batch L-BFGS method with a constant step length
converges to a neighborhood of the optimal solution
Theorem Suppose that Assumptions hold and let where is the
minimizer of Let wk be the iterates generated by Algorithm with starting
from w0 Then for all
E[F wk
The bound provided by this theorem has two components a term decaying linearly to zero and
a term identifying the neighborhood of convergence Note that a larger step length yields a
more favorable constant in the linearly decaying term at the cost of an increase in the size of the
neighborhood of convergence We will consider again these tradeoffs in Section where we also
note that larger batches increase the opportunities for parallelism and improve the limiting accuracy
in the solution but slow down the learning abilities of the algorithm
One can establish convergence of the multi-batch L-BFGS method to the optimal solution by
employing a sequence of step lengths that converge to zero according to the schedule proposed
by Robbins and Monro However that provides only a sublinear rate of convergence which is of
little interest in our context where large batches are employed and some type of linear convergence is
expected In this light Theorem is more relevant to practice
Nonconvex case
The BFGS method is known to fail on noconvex problems Even for L-BFGS which
makes only a finite number of updates at each iteration one cannot guarantee that the Hessian
approximations have eigenvalues that are uniformly bounded above and away from zero To establish
convergence of the BFGS method in the nonconvex case cautious updating procedures have been
proposed Here we employ a cautious strategy that is well suited to our particular algorithm we
skip the update set Hk if the curvature condition
ykT sk ksk
is not satisfied where is a predetermined constant Using said mechanism we show that the
eigenvalues of the Hessian matrix approximations generated by the multi-batch L-BFGS method are
bounded above and away from zero Lemma The analysis presented in this section is based on
the following assumptions
Assumptions B.
is twice continuously differentiable
The gradients of are Lipschitz continuous and the gradients of are Lipschitz
continuous for all Rd and all sets
The function is bounded below by a scalar Fb
There exist constants and such that ES k?F for all
Rd and all sets
The samples are drawn independently and is an unbiased estimator of the true
gradient for all Rd
Lemma Suppose that Assumptions hold and let be given Let Hk be the
Hessian approximations generated by Algorithm with the modification that Hk whenever
is not satisfied Then there exist constants such that
I Hk I
for
We can now follow the analysis in Chapter to establish the following result about the behavior
of the gradient norm for the multi-batch L-BFGS method with a cautious update strategy
Theorem Suppose that Assumptions above hold and let be given Let wk be
the iterates generated by Algorithm with
starting from w0 and with the
modification that Hk whenever is not satisfied Then
Fb
k?F wk
This result bounds the average norm of the gradient of after the first iterations and shows
that the iterates spend increasingly more time in regions where the objective function has a small
gradient
Numerical Results
In this Section we present numerical results that evaluate the proposed robust multi-batch L-BFGS
scheme Algorithm on logistic regression problems Figure shows the performance on the
webspam dataset1 where we compare it against three methods multi-batch L-BFGS without
enforcing sample consistency L-BFGS where gradient differences are computed using different
samples yk
gkSk multi-batch gradient descent Gradient Descent which is
obtained by setting Hk I in Algorithm and iii serial SGD where at every iteration one
sample is used to compute the gradient We run each method with different random seeds and
where applicable report results for different batch and overlap sizes The proposed method
is more stable than the standard L-BFGS method this is especially noticeable when is small On
the other hand serial SGD achieves similar accuracy as the robust L-BFGS method and at a similar
rate at the cost of communications per epochs versus
communications per
epoch Figure also indicates that the robust L-BFGS method is not too sensitive to the size of
overlap Similar behavior was observed on other datasets in regimes where was not too small
We mention in passing that the L-BFGS step was computed using the a vector-free implementation
proposed in
webspam
Epochs
webspam
Epochs
webspam
Robust L?BFGS
L?BFGS
Gradient Descent
SGD
Epochs
webspam
Robust L?BFGS
L?BFGS
Gradient Descent
SGD
Robust L?BFGS
L?BFGS
Gradient Descent
SGD
k?F
k?F
k?F
Robust L?BFGS
L?BFGS
Gradient Descent
SGD
webspam
Robust L?BFGS
L?BFGS
Gradient Descent
SGD
k?F
k?F
webspam
Robust L?BFGS
L?BFGS
Gradient Descent
SGD
k?F
Epochs
Epochs
Epochs
Figure webspam dataset Comparison of Robust L-BFGS L-BFGS multi-batch L-BFGS without
enforcing sample consistency Gradient Descent multi-batch Gradient method and SGD for various
batch and overlap sizes Solid lines show average performance and dashed lines show worst
and best performance over runs per algorithm MPI processes
We also explore the performance of the robust multi-batch L-BFGS method in the presence of node
failures faults and compare it to the multi-batch variant that does not enforce sample consistency
L-BFGS Figure illustrates the performance of the methods on the webspam dataset for various
LIBSVM https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html
probabilities of node failures and suggests that the robust L-BFGS variant is
more stable
webspam
webspam
Robust L?BFGS
L?BFGS
webspam
Robust L?BFGS
L?BFGS
Robust L?BFGS
L?BFGS
k?F
k?F
k?F
Iterations/Epochs
Iterations/Epochs
Iterations/Epochs
Figure webspam dataset Comparison of Robust L-BFGS and L-BFGS multi-batch L-BFGS
without enforcing sample consistency for various node failure probabilities Solid lines show
average performance and dashed lines show worst and best performance over runs per algorithm
MPI processes
Lastly we study the strong and weak scaling properties of the robust L-BFGS method on artificial
data Figure We measure the time needed to compute a gradient Gradient and the associated
communication Gradient+C as well as the time needed to compute the L-BFGS direction LBFGS and the associated communication L-BFGS+C for various batch sizes The figure
on the left shows strong scaling of multi-batch LBFGS on a dimensional problem with
samples The size of input data is and we vary the number of MPI processes
The time it takes to compute the gradient decreases with however for small
values of the communication time exceeds the compute time The figure on the right shows weak
scaling on a problem of similar size but with varying sparsity Each sample has non-zero
elements thus for any the size of local problem is roughly for size of data
We observe almost constant time for the gradient computation while the cost of computing
the L-BFGS direction decreases with however if communication is considered the overall time
needed to compute the L-BFGS direction increases slightly
Strong Scaling
Elapsed Time
Gradient
Gradient+C
L?BFGS
L?BFGS+C
Elapsed Time
Number of MPI processes
Weak Scaling Fix problem dimensions
Gradient
Gradient+C
L?BFGS
L?BFGS+C
Number of MPI processes
Figure Strong and weak scaling of multi-batch L-BFGS method
Conclusion
This paper describes a novel variant of the L-BFGS method that is robust and efficient in two settings
The first occurs in the presence of node failures in a distributed computing implementation the second
arises when one wishes to employ a different batch at each iteration in order to accelerate learning
The proposed method avoids the pitfalls of using inconsistent gradient differences by performing
quasi-Newton updating based on the overlap between consecutive samples Numerical results show
that the method is efficient in practice and a convergence analysis illustrates its theoretical properties
Acknowledgements
The first two authors were supported by the Office of Naval Research award the
Department of Energy grant and the National Science Foundation grant
Martin Tak??c was supported by National Science Foundation grant

----------------------------------------------------------------

title: 4403-hashing-algorithms-for-large-scale-learning.pdf

Hashing Algorithms for Large-Scale Learning
Ping Li
Cornell University
pingli@cornell.edu
Anshumali Shrivastava
Cornell University
anshu@cs.cornell.edu
Joshua Moore
Cornell University
jlmo@cs.cornell.edu
Arnd Christian K?onig
Microsoft Research
chrisko@microsoft.com
Abstract
Minwise hashing is a standard technique in the context of search for efficiently
computing set similarities The recent development of b-bit minwise hashing provides a substantial improvement by storing only the lowest bits of each hashed
value In this paper we demonstrate that b-bit minwise hashing can be naturally integrated with linear learning algorithms such as linear SVM and logistic
regression to solve large-scale and high-dimensional statistical learning tasks especially when the data do not fit in memory We compare b-bit minwise hashing
with the Count-Min and Vowpal Wabbit algorithms which have essentially the same variances as random projections Our theoretical and empirical
comparisons illustrate that b-bit minwise hashing is significantly more accurate at
the same storage cost than VW and random projections for binary data
Introduction
With the advent of the Internet many machine learning applications are faced with very large and
inherently high-dimensional datasets resulting in challenges in scaling up training algorithms and
storing the data Especially in the context of search and machine translation corpus sizes used in
industrial practice have long exceeded the main memory capacity of single machine For example
discusses training sets with items and distinct features requiring novel algorithmic
approaches and architectures As a consequence there has been a renewed emphasis on scaling up
machine learning techniques by using massively parallel architectures however methods relying
solely on parallelism can be expensive both with regards to hardware requirements and energy
costs and often induce significant additional communication and data distribution overhead
This work approaches the challenges posed by large datasets by leveraging techniques from the area
of similarity search where similar increases in data sizes have made the storage and computational requirements for computing exact distances prohibitive thus making data representations that
allow compact storage and efficient approximate similarity computation necessary
The method of b-bit minwise hashing is a recent progress for efficiently both time and
space computing resemblances among extremely high-dimensional binary vectors In
this paper we show that b-bit minwise hashing can be seamlessly integrated with linear Support
Vector Machine SVM 18 31 and logistic regression solvers
Ultra High-Dimensional Large Datasets and Memory Bottlenecks
In the context of search a standard procedure to represent documents Web pages is to use
w-shingles contiguous words where in several studies This procedure can
generate datasets of extremely high dimensions For example suppose we only consider common English words Using may require the size of dictionary to be
In practice often suffices as the number of available documents may not be large enough
to exhaust the dictionary For w-shingle data normally only abscence/presence information
is used as it is known that word frequency distributions within documents approximately follow
a power-law meaning that most single terms occur rarely thereby making a w-shingle is unlikely to occur more than once in a document Interestingly even when the data are not too highdimensional empirical studies 17 achieved good performance with binary-quantized data
When the data can fit in memory linear SVM training is often extremely efficient after the data are
loaded into the memory It is however often the case that for very large datasets the data loading
time dominates the computing time for solving the SVM problem A more severe problem
arises when the data can not fit in memory This situation can be common in practice The publicly
available webspam dataset LIBSVM format needs about disk space which exceeds the
memory capacity of many desktop PCs. Note that webspam which contains only documents represented by 3-shingles is still very small compared to industry applications
Our Proposal
We propose a solution which leverages b-bit minwise hashing Our approach assumes the data
vectors are binary high-dimensional and relatively sparse which is generally true of text documents
represented via shingles We apply b-bit minwise hashing to obtain a compact representation of the
original data In order to use the technique for efficient learning we have to address several issues
We need to prove that the matrices generated by b-bit minwise hashing are positive definite
which will provide the solid foundation for our proposed solution
If we use b-bit minwise hashing to estimate the resemblance which is nonlinear how can
we effectively convert this nonlinear problem into a linear problem
Compared to other hashing techniques such as random projections Count-Min
sketch or Vowpal Wabbit does our approach exhibits advantages
It turns out that our proof in the next section that b-bit hashing matrices are positive definite naturally
provides the construction for converting the otherwise nonlinear SVM problem into linear SVM.
Review of Minwise Hashing and b-Bit Minwise Hashing
Minwise hashing has been successfully applied to a wide range of real-world problems
for efficiently computing set similarities Minwise hashing mainly works well
with binary data which can be viewed either as vectors or as sets Given two sets S1 S2
a widely used measure of similarity is the resemblance
S2
a
S2
f1 f2 a
where f1 f2 a S2
Applying a random permutation on S1 and S2 the collision probability is simply
Pr
S2
R.
S2
One can repeat the permutation times to estimate without bias The common
practice is to store each hashed value and using 64 bits The
storage and computational cost will be prohibitive in truly large-scale industry applications
b-bit minwise hashing provides a strikingly simple solution to this storage and computational
problem by storing only the lowest bits instead of 64 bits of each hashed value
For convenience denote z1 min and z2 min and denote z1 the
integer value corresponding to the lowest bits of of z1 For example if z1 then z1
Theorem Assume is large
Pb Pr z1 z2
r1
f1
f2
r2
f1 f2
r1
r1
r2
r2
r1 r2
r1 r2
r1 r2
r1 r2
r1 r1
2b
r1
r2 r2
r2
This approximate formula is remarkably accurate even for very small see Figure in
We can then estimate Pb and from independent permutations
Pb
Var
Var P?b
It turns out that our method only needs P?b for linear learning no need to explicitly estimate R.
Kernels from Minwise Hashing b-Bit Minwise Hashing
Definition A symmetric matrix satisfying ij ci cj Kij for all real vectors is called
positive definite Note that here we do not differentiate PD from nonnegative definite
Theorem Consider sets S1 Sn Apply one permutation to each
set Define zi min{?(Si and zi the lowest bits of zi The following three matrices are PD.
The resemblance matrix Rn?n whose j)-th entry is the resemblance between set
Sj
Si and set Sj Rij Sii Sjj Si
Sj
The minwise hashing matrix Rn?n Mij zj
The b-bit minwise hashing matrix Rn?n Mij zi zj
Consequently consider independent permutations and denote the b-bit minwise hashing
Pk
matrix generated by the s-th permutation Then the summation is also PD.
Proof A matrix A is PD if it can be written as an inner product BT B. Because
Mij zj
Mij is the inner product of two D-dim vectors Thus is PD. Similarly is PD because
P2b
Mij
is PD because Rij Pr{Mij Mij and
Mij is the j)-th element of the PD matrix M. Note that the expectation is a linear operation
Integrating b-Bit Minwise Hashing with Linear Learning Algorithms
Linear algorithms such as linear SVM and logistic regression have become very powerful and extremely popular Representative software packages include SVMperf Pegasos Bottou?s
SGD SVM and LIBLINEAR Given a dataset RD The
regularized linear SVM solves the following optimization problem
min
max wT
w+C
and the regularized logistic regression solves a similar problem
min
w+C
log e?yi
Here is a regularization parameter Since our purpose is to demonstrate the effectiveness of
our proposed scheme using b-bit hashing we simply provide results for a wide range of values
and assume that the best performance is achievable if we conduct cross-validations
In our approach we apply random permutations on each feature vector and store the lowest
bits of each hashed value This way we obtain a new dataset which can be stored using merely nbk
bits At run-time we expand each new data point into a 2b k-length vector with exactly
For example suppose and the hashed values are originally whose binary digits are Consider Then
the binary digits are stored as which corresponds to in decimals At run-time
we need to expand them into a vector of length 2b to be
which will be the new feature vector fed to a solver such as LIBLINEAR Clearly this expansion is
directly inspired by the proof that the b-bit minwise hashing matrix is PD in Theorem
Experimental Results on Webspam Dataset
Our experiment settings closely follow the work in They conducted experiments on three
datasets of which only the webspam dataset is public and reasonably high-dimensional
Therefore our experiments focus on webspam Following we
randomly selected of samples for testing and used the remaining samples for training
We chose LIBLINEAR as the workhorse to demonstrate the effectiveness of our algorithm All
experiments were conducted on workstations with Xeon(R CPU and
RAM under Windows System Thus in our case the original data about in LIBSVM
format fit in memory In applications when the data do not fit in memory we expect that b-bit
hashing will be even more substantially advantageous because the hashed data are relatively very
small In fact our experimental results will show that for this dataset using and can
achieve similar testing accuracies as using the original data The effective storage for the reduced
dataset with examples using and would be merely about
Experimental Results on Nonlinear Kernel SVM
We implemented a new resemblance kernel function and tried to use LIBSVM to train an SVM using
the webspam dataset The training time well exceeded 24 hours Fortunately using b-bit minswise
hashing to estimate the resemblance kernels provides a substantial improvement For example with
and the training time is about seconds and the testing accuracy is quite
close to the best results given by LIBLINEAR on the original webspam data
Experimental Results on Linear SVM
There is an important tuning parameter C. To capture the best performance and ensure repeatability
we experimented with a wide range of values from to with fine spacings in
svm
Spam Accuracy
svm
Spam Accuracy
98
96
94
92
90
88
86
svm
84
82
Spam Accuracy
Accuracy
Accuracy
98
96
94
92
90
88
86
84
82
Accuracy
98
96
94
92
90
88
86
svm
84
82
Spam Accuracy
98
96
94
92
90
88
86
svm
84
82
Spam Accuracy
Accuracy
98
96
94
92
90
88
86
84
82
Accuracy
Accuracy
98
96
94
92
90
88
86
svm
84
Spam Accuracy
82
Accuracy
Accuracy
We experimented with to and and Figure average
and Figure std standard deviation provide the test accuracies Figure demonstrates that using
and achieves similar test accuracies as using the original data Since our method
is randomized we repeated every experiment times We report both the mean and std values
Figure illustrates that the stds are very small especially with In other words our algorithm
produces stable predictions For this dataset the best performances were usually achieved at
98
96
94
92
90
88
86
svm
84
82
Spam Accuracy
98
96
94
92
90
88
86
svm
84
82
Spam Accuracy
Figure SVM test accuracy averaged over repetitions With and b-bit
hashing achieves very similar accuracies as using the original data dashed red if color is available
svm
Spam accuracy std
svm
Spam accuracy std
Accuracy std
Accuracy std
Accuracy std
Accuracy std
svm
Spam accuracy std
svm
Spam accuracy std
Figure SVM test accuracy The standard deviations are computed from repetitions
When the standard deviations become extremely small
Compared with the original training time about seconds Figure upper panels shows that
our method only needs about seconds near Note that our reported training time did not
include data loading about minutes for the original data and seconds for the hashed data
Compared with the original testing time about seconds Figure bottom panels shows that
our method needs merely about seconds Note that the testing time includes both the data loading
time as designed by LIBLINEAR The efficiency of testing may be very important in practice for
example when the classifier is deployed in a user-facing application such as search while the cost
of training or preprocessing may be less critical and can be conducted off-line
svm
Spam Testing time
svm
Spam Testing time
svm
Spam Testing time
svm
Spam Training time
Testing time sec
Testing time sec
Testing time sec
svm
Spam Training time
Training time sec
svm
Spam Training time
Testing time sec
Training time sec
Training time sec
svm
Spam Training time
Training time sec
svm
Spam Testing time
Figure SVM training time upper panels and testing time bottom panels The original costs
are plotted using dashed red if color is available curves
logit
Spam Accuracy
logit
Spam Training time
logit
Spam Training time
Training time sec
98
96
94
92
90
88
86
logit
84
82
Spam Accuracy
Training time sec
Training time sec
98
96
94
92
90
88
86
logit
84
82
Spam Accuracy
Accuracy
Training time sec
98
96
94
92
90
88
86
84
82
Accuracy
98
96
94
92
90
88
86
84
logit
82
Spam Accuracy
Accuracy
Accuracy
Experimental Results on Logistic Regression
Figure presents the test accuracies and training time using logistic regression Again with
and b-bit minwise hashing can achieve similar test accuracies as using the original data The
training time is substantially reduced from about seconds to about seconds only
logit
Spam Training time
logit
Spam Training time
Figure Logistic regression test accuracy upper panels and training time bottom panels
In summary it appears b-bit hashing is highly effective in reducing the data size and speeding up the
training and testing for both SVM and logistic regression We notice that when using the
training time can be much larger than using Interestingly we find that b-bit hashing can be
easily combined with Vowpal Wabbit to further reduce the training time when is large
Random Projections Count-Min Sketch and Vowpal Wabbit
Random projections Count-Min sketch and Vowpal Wabbit as
popular hashing algorithms for estimating inner products for high-dimensional datasets are naturally
applicable in large-scale learning In fact those methods are not limited to binary data Interestingly
the three methods all have essentially the same variances Note that in this paper we use
particularly for the hashing algorithm in not the influential online learning platform
Random Projections
Denote the first two rows of a data matrix by u1 u2 RD The task is to estimate the inner
PD
product a The general idea is to multiply the data vectors by a random matrix
rij RD?k where rij is sampled from the following generic distribution with
Note that
E(rij
E(rij ar(rij E(rij
ar(rij
E(rij
rij
rij
This generates two k-dim vectors v1 and v2
rij
The general family of distributions includes the standard normal
case
distribution this
with
prob
2s
and the sparse projection distribution specified as rij
with prob 1s
with prob 2s
provided the following unbiased estimator a
rp,s of a and the general variance formula
1X
arp,s a
a
arp,s
a
rp,s
which means achieves the smallest variance The only elementary distribution we know that
satisfies with is the two point distribution in with equal probabilities
proposed an improved estimator for random projections as the solution to a cubic equation
Because it can not be written as an inner product that estimator can not be used for linear learning
Count-Min Sketch and Vowpal Wabbit
Again in this paper always refers to the hashing algorithm in VW may be viewed as
a bias-corrected version of the Count-Min sketch In the original CM algorithm the
key step is to independently and uniformly hash elements of the data vectors to buckets and the
hashed value is the sum of the elements
in the bucket That is with probability k1 where
if
By writing Iij
we can write the hashed data as
otherwise
Iij
Iij
Pk
The estimate a
cm is severely biased for estimating inner products The original
paper suggested a count-min step for positive data by generating multiple independent estimates a
cm and taking the minimum as the final estimate That step
but can not remove
can reduce
PD
the bias Note that the bias can be easily removed by using a
cm
proposed a creative method for bias-correction which consists of pre-multiplying elementwise the original data vectors with a random vector whose entries are sampled from the twopoint distribution in with equal probabilities Here we consider the general distribution
After applying multiplication and hashing on u1 and u2 the resultant vectors g1 and g2 are
ri Iij
ri Iij
where E(ri E(ri2 E(ri3 E(ri4 We have the following Lemma
Theorem
a
vw,s
avw,s
a
avw,s
a
Interestingly the variance says we do need otherwise the additional term
PD
will not vanish even as the sample size In other words the choice of
random distribution in VW is essentially the only option if we want to remove the bias by premultiplying the data vectors element-wise with a vector of random variables Of course once we
let the variance becomes identical to the variance of random projections
Comparing b-Bit Minwise Hashing with VW and Random Projections
svm VW vs hashing
Training time sec
98
96
94
92
90
88
86
84
logit VW vs hashing
82
Spam Accuracy
Training time sec
98
96
94
92
90
88
86
84
svm VW vs hashing
82
Spam Accuracy
Accuracy
Accuracy
We implemented VW and experimented it on the same webspam dataset Figure shows that b-bit
minwise hashing is substantially more accurate at the same sample size and requires significantly
less training time to achieve the same accuracy Basically for 8-bit minwise hashing with
achieves similar test accuracies as VW with note that we only stored the non-zeros
Spam Training time
logit VW vs hashing
Spam Training time
Figure The dashed red if color is available curves represent b-bit minwise hashing results only
for while solid curves for VW. We display results for
This empirical finding is not surprising because the variance of b-bit hashing is usually substantially
smaller than the variance of VW and random projections In the technical report
which also includes the complete proofs of the theorems presented in this paper we show that at
the same storage cost b-bit hashing usually improves VW by to by assuming each
sample of VW needs 32 bits to store Of course even if VW only stores each sample using bits
an improvement of to 50-fold would still be very substantial
There is one interesting issue here Unlike random projections and minwise hashing VW is a
sparsity-preserving algorithm meaning that in the resultant sample vector of length the number
of non-zeros will not exceed the number of non-zeros in the original vector
In fact it is easy
to see
that the fraction of zeros in the resultant vector would be at least k1 exp kc where
is the number of non-zeros in the original data vector In this paper we mainly focus on the scenario
in which we use b-bit minwise hashing or VW for the purpose of data reduction
However in some cases we care about because VW is also an excellent tool for compact
indexing In fact our b-bit minwise hashing scheme for linear learning may face such an issue
Combining b-Bit Minwise Hashing with VW
In Figures and when the training time becomes substantially larger than Recall
that in the run-time we expand the b-bit minwise hashed data to sparse binary vectors of length 2b
with exactly When the vectors are very sparse On the other hand once we have
expanded the vectors the task is merely computing inner products for which we can use VW.
Therefore in the run-time after we have generated the sparse binary vectors of length 2b we hash
them using VW with sample size to differentiate from How large should be Theorem
of the resemblance
may provide an insight Recall Section provides the estimator denoted by
using b-bit minwise hashing Now suppose we first apply VW hashing with size on the binary
vector of length 2b before estimating which will introduce some additional randomness We
b,vw Theorem provides its theoretical variance
denote the new estimator by
90
85
90
Logit hashing
Spam:Accuracy
95
SVM hashing VW
Training time sec
Accuracy
Accuracy
85
Spam Accuracy
Spam:Training Time
SVM hashing VW
95
Training time sec
Logit hashing
Spam Training Time
Figure We apply VW hashing on top of the binary vectors of length 2b generated by b-bit
hashing with size 22 23 28 for and The numbers on the solid
curves are the exponents The dashed red if color if available curves are the results
from only using b-bit hashing When 28 this method achieves similar test accuracies left
panels while substantially reducing the training time right panels
Theorem
Pb Pb
Var Rb,vw ar Rb
Pb
Pb is given by and is the constant defined in Theorem
where ar
Compared to the original variance ar the additional term in can be relatively large if
is small Therefore we should choose and 2b If then 28 may be a
good trade-off Figure provides an empirical study to verify this intuition
Limitations
While using b-bit minwise hashing for training linear algorithms is successful on the webspam
dataset it is important to understand the following three major limitations of the algorithm
Our method is designed for binary sparse data Our method requires an expensive
preprocessing step for generating permutations of the data For most applications we expect the
preprocessing cost is not a major issue because the preprocessing can be conducted off-line combined with the data-collection step and is easily parallelizable However even if the speed is not a
concern the energy consumption might be an issue especially considering b-bit minwise hashing
is mainly used for industry applications In addition testing an new unprocessed data vector
a new document will be expensive Our method performs only reasonably well in terms of
dimension reduction The processed data need to be mapped into binary vectors in 2b dimensions which is usually not small Note that the storage cost is just bk bits For example for the
webspam dataset using and seems to suffice and 28 is quite large
although it is much smaller than the original dimension of million It would be desirable if we
can further reduce the dimension because the dimension determines the storage cost of the model
and moderately increases the training time for batch learning algorithms such as LIBLINEAR
In hopes of fixing the above limitations we experimented with an implementation using another
hashing technique named Conditional Random Sampling CRS which is not limited to
binary data and requires only one permutation of the original data no expensive preprocessing
We achieved some limited success For example CRS compares favorably to VW in terms of storage to achieve the same accuracy on the webspam dataset However so far CRS can not compete
with b-bit minwise hashing for linear learning terms of training speed storage cost and model
size The reason is because even though the estimator of CRS is an inner product the normalization
factors the effective sample size of CRS to ensure unbiased estimates substantially differ pairwise which is a significant advantage in other applications In our implementation we could not
to use fully correct normalization factors which lead to severe bias of the inner product estimates
and less than satisfactory performance of linear learning compared to b-bit minwise hashing
Conclusion
As data sizes continue to grow faster than the memory and computational power statistical learning
tasks in industrial practice are increasingly faced with training datasets that exceed the resources on
a single server A number of approaches have been proposed that address this by either scaling out
the training process or partitioning the data but both solutions can be expensive
In this paper we propose a compact representation of sparse binary data sets based on b-bit minwise
hashing which can be naturally integrated with linear learning algorithms such as linear SVM and
logistic regression leading to dramatic improvements in training time and/or resource requirements
We also compare b-bit minwise hashing with the Count-Min sketch and Vowpal Wabbit
algorithms which according to our analysis all have essentially the same variances as random
projections Our theoretical and empirical comparisons illustrate that b-bit minwise hashing is
significantly more accurate at the same storage for binary data There are various limitations
expensive preprocessing in our proposed method leaving ample room for future research
Acknowledgement
This work is supported by NSF ONR and a grant from
Microsoft We thank John Langford and Tong Zhang for helping us better understand the VW hashing algorithm and Chih-Jen Lin for his patient explanation of LIBLINEAR package and datasets

----------------------------------------------------------------

title: 5129-stochastic-majorization-minimization-algorithms-for-large-scale-optimization.pdf

Stochastic Majorization-Minimization Algorithms
for Large-Scale Optimization
Julien Mairal
LEAR Project-Team INRIA Grenoble
julien.mairal@inria.fr
Abstract
Majorization-minimization algorithms consist of iteratively minimizing a majorizing surrogate of an objective function Because of its simplicity and its wide
applicability this principle has been very popular in statistics and in signal processing In this paper we intend to make this principle scalable We introduce
a stochastic majorization-minimization scheme which is able to deal with largescale or possibly infinite data sets When applied to convex optimization problems
under suitable
assumptions we show that it achieves an expected convergence
rate of after iterations and of for strongly convex functions
Equally important our scheme almost surely converges to stationary points for
a large class of non-convex problems We develop several efficient algorithms
based on our framework First we propose a new stochastic proximal gradient
method which experimentally matches state-of-the-art solvers for large-scale logistic regression Second we develop an online DC programming algorithm for
non-convex sparse estimation Finally we demonstrate the effectiveness of our
approach for solving large-scale structured matrix factorization problems
Introduction
Majorization-minimization is a simple optimization principle for minimizing an objective function It consists of iteratively minimizing a surrogate that upper-bounds the objective thus monotonically driving the objective function value downhill This idea is used in many existing procedures
For instance the expectation-maximization algorithm builds a surrogate for a
likelihood model by using Jensen?s inequality Other approaches can also be interpreted under the
majorization-minimization point of view such as DC programming where stands for difference of convex functions variational Bayes techniques or proximal algorithms 23
In this paper we propose a stochastic majorization-minimization algorithm which is is suitable for
solving large-scale problems arising in machine learning and signal processing More precisely we
address the minimization of an expected cost?that is an objective function that can be represented
by an expectation over a data distribution For such objectives online techniques based on stochastic
approximations have proven to be particularly efficient and have drawn a lot of attraction in machine
learning statistics and optimization 17 19 22
Our scheme follows this line of research It consists of iteratively building a surrogate of the expected
cost when only a single data point is observed at each iteration this data point is used to update the
surrogate which in turn is minimized to obtain a new estimate Some previous works are closely
related to this scheme the online EM algorithm for latent data models and the online matrix
factorization technique of involve for instance surrogate functions updated in a similar fashion
Compared to these two approaches our method is targeted to more general optimization problems
Another related work is the incremental majorization-minimization algorithm of for finite training sets it was indeed shown to be efficient for solving machine learning problems where storing
dense information about the past iterates can be afforded Concretely this incremental scheme requires to store O(pn values where is the variable size and is the size of the training set.1
This issue was the main motivation for us for proposing a stochastic scheme with a memory load
independent of thus allowing us to possibly deal with infinite data sets or a huge variable size
We study the convergence properties of our algorithm when the surrogates are strongly convex and
chosen among the class of first-order surrogate functions introduced in which consist of approximating the possibly non-smooth objective up to a smooth error When the objective is convex
we obtain expected convergence rates that are asymptotically
optimal or close to optimal
More precisely the convergence rate is of order in a finite horizon setting and for
a strongly convex objective in an infinite horizon setting Our second analysis shows that for nonconvex problems our method almost surely converges to a set of stationary points under suitable
assumptions We believe that this result is equally valuable as convergence rates for convex optimization To the best of our knowledge the literature on stochastic non-convex optimization is rather
scarce and we are only aware of convergence results in more restricted settings than ours?see for
instance for the stochastic gradient descent algorithm for online EM for online matrix
factorization or which provides stronger guarantees but for unconstrained smooth problems
We develop several efficient algorithms based on our framework The first one is a new stochastic
proximal gradient method for composite or constrained optimization This algorithm is related to a
long series of work in the convex optimization literature 22 and we demonstrate that it performs as well as state-of-the-art solvers for large-scale logistic regression The
second one is an online DC programming technique which we demonstrate to be better than batch
alternatives for large-scale non-convex sparse estimation Finally we show that our scheme can
address efficiently structured sparse matrix factorization problems in an online fashion and offers
new possibilities to such as the use of various loss or regularization functions
This paper is organized as follows Section introduces first-order surrogate functions for batch
optimization Section is devoted to our stochastic approach and its convergence analysis Section
presents several applications and numerical experiments and Section concludes the paper
Optimization with First-Order Surrogate Functions
Throughout the paper we are interested in the minimization of a continuous function Rp
min
where Rp is a convex set The majorization-minimization principle consists of computing a majorizing surrogate gn of at iteration and updating the current estimate by arg gn
The success of such a scheme depends on how well the surrogates approximate In this paper we
consider a particular class of surrogate functions introduced in and defined as follows
Definition Strongly Convex First-Order Surrogate Functions
Let be in We denote by the set of strongly convex functions such that
the approximation error is differentiable and the gradient is LLipschitz continuous We call the functions in first-order surrogate functions
Among the first-order surrogate functions presented in we should mention the following ones
Lipschitz Gradient Surrogates
When is differentiable and is L-Lipschitz admits the following surrogate in S2L,L
When is convex is in SL,L and when is strongly convex is in
Minimizing amounts to performing a classical classical gradient descent step L1
Proximal Gradient Surrogates
Assume that splits into f1 f2 where f1 is differentiable is L-Lipschitz and f2 is
To alleviate this issue it is possible to cut the dataset into mini-batches reducing the memory load to
which remains cumbersome when is very large
convex Then the function below is in S2L,L
f2
When f1 is convex is in SL,L If f1 is strongly convex is in Minimizing
amounts to a proximal gradient step 23 arg min L1 L1 f2
f1
DC Programming Surrogates
Assume that f1 f2 where f2 is concave and differentiable is Lipschitz and g1 is in
SL1 Then the following function is a surrogate in SL1
f1 f2
When f1 is convex f1 f2 is a difference of convex functions leading to a DC program
With the definition of first-order surrogates and a basic batch algorithm in hand we now introduce
our main contribution a stochastic scheme for solving large-scale problems
Stochastic Optimization
As pointed out in one is usually not interested in the minimization of an empirical cost on a
finite training set but instead in minimizing an expected cost Thus we assume from now on that
has the form of an expectation
min Ex
where from some set represents a data point which is drawn according to some unknown
distribution and is a continuous loss function As often done in the literature we assume that
the expectations are well defined and finite valued we also assume that is bounded below
We present our approach for tackling in Algorithm At each iteration we draw a training
point assuming that these points are samples from the data distribution Note that in
practice since it is often difficult to obtain true samples the points are computed by
cycling on a randomly permuted training set Then we choose a surrogate gn for the function
and we use it to update a function g?n that behaves as an approximate surrogate for the
expected cost The function g?n is in fact a weighted average of previously computed surrogates
and involves a sequence of weights wn that will be discussed later Then we minimize g?n and
obtain a new estimate For convex problems we also propose to use averaging schemes denoted
by option and option in Alg. Averaging is a classical technique for improving convergence
rates in convex optimization for reasons that are clear in the convergence proofs
Algorithm Stochastic Majorization-Minimization Scheme
input initial estimate number of iterations wn weights in
initialize the approximate surrogate
for do
draw a training point define fn
choose a surrogate function gn in fn
update the approximate surrogate g?n wn
wn gn
update the current estimate
arg min g?n
for option update the averaged iterate
for option update the averaged iterate
wk
end for
output option current estimate no averaging
output option first averaging scheme
output option second averaging scheme
We remark that Algorithm is only practical when the functions g?n can be parameterized with a
small number of variables and when they can be easily minimized over Concrete examples are
discussed in Section Before that we proceed with the convergence analysis
Convergence Analysis Convex Case
First We study the case of convex functions fn and make the following assumption
for all in the functions fn are R-Lipschitz continuous Note that for convex functions
this is equivalent to saying that subgradients of fn are uniformly bounded by R.
Assumption is classical in the stochastic optimization literature Our first result shows that
with the averaging scheme corresponding to option in Alg. we obtain an expected convergence
rate that makes explicit the role of the weight sequence wn
Proposition Convergence Rate
When the functions fn are convex under assumption and when we have
Pn
RL
wk
for all
E[f
wk
where is defined in Algorithm is a minimizer of on and
Such a rate is similar to the one of stochastic gradient descent with averaging see for example
Note that the constraint here is compatible with the proximal gradient surrogate
From Proposition it is easy to obtain a bound for a finite horizon?that is when the
total number of iterations is known in advance
When is fixed such a bound can indeed be
obtained by
plugging constant weights wk for all in Note that the upperbound cannot be improved in general without making further assumptions on the objective
function The next corollary shows that in an infinite horizon setting and with decreasing
weights we lose a logarithmic factor compared to an optimal convergence rate of
Corollary Convergence Rate Infinite Horizon Decreasing Weights
Let us make the same assumptions as in Proposition and choose the weights wn Then
R2 log(n
E[f
2L
Our analysis
suggests
to use weights of the form In practice we have found that choosing
wn n0 n0 performs well where n0 is tuned on a subsample of the training set
Convergence Analysis Strongly Convex Case
In this section we introduce an additional assumption
the functions fn are strongly convex
We show that our method achieves a rate which is optimal up to a multiplicative constant
for strongly convex functions
Proposition Convergence Rate
Under assumptions and with Define and wn
Then
2R
for all
E[f max
where is defined in Algorithm when choosing the averaging scheme called option
The averaging scheme is slightly different than in the previous section and the weights decrease
at a different speed Again this rate applies to the proximal gradient surrogates which satisfy the
constraint In the next section we analyze our scheme in a non-convex setting
Convergence Analysis Non-Convex Case
Convergence results for non-convex problems are by nature weak and difficult to obtain for stochastic optimization In such a context proving convergence to a global local minimum is out
of reach and classical analyses study instead asymptotic stationary point conditions which involve
directional derivatives Concretely we introduce the following assumptions
and the support of the data are compact
The functions fn are uniformly bounded by some constant
The weights wn are non-increasing w1 wn and wn2
The directional derivatives fn and exist for all and in
Assumptions and combined with are useful because they allow us to use some uniform
convergence results from the theory of empirical processes In a nutshell these assumptions
ensure that the function class is simple enough such that a uniform law
of large numbers applies The assumption is more technical it resembles classical conditions
used for proving the convergence of stochastic gradient descent algorithms usually stating that the
weights wnPshould be
the summand of a diverging sum while the sum of wn should be finite the
constraint wn is slightly stronger Finally is a mild assumption which is
useful to characterize the stationary points of the problem A classical necessary first-order condition for to be a local minimum of is indeed to have non-negative for all in
We call such points the stationary points of the function The next proposition is a generalization
of a convergence result obtained in in the context of sparse matrix factorization
Proposition Non-Convex Analysis Almost Sure Convergence
Under assumptions converges with probability one Under assumption we also have that
lim inf inf
where the function f?n is a weighted empirical risk recursively defined as f?n wn fn
It can be shown that f?n uniformly converges to
Even though f?n converges uniformly to the expected cost Proposition does not imply that the
limit points of are stationary points of We obtain such a guarantee when the surrogates
that are parameterized an assumption always satisfied when Algorithm is used in practice
Proposition Non-Convex Analysis Parameterized Surrogates
Let us make the same assumptions as in Proposition and let us assume that the functions g?n are
parameterized by some variables living in a compact set of Rd In other words g?n can be
written as g?n with in K. Suppose there exists a constant such that
Kk for all in and in K. Then every limit point of the sequence is a
stationary point of that is for all in
Finally we show that our non-convex convergence analysis can be extended beyond first-order surrogate functions?that is when gn does not satisfy exactly Definition This is possible when
the objective has a particular partially separable structure as shown in the next proposition This
extension was motivated by the non-convex sparse estimation formulation of Section where such
a structure appears
Proposition Non-Convex Analysis Partially Separable Extension
PK
Assume that the functions fn split into fn fk,n where the functions
are convex and R-Lipschitz and the fk,n are non-decreasing for Consider
in SL0 and some non-decreasing functions gk,n in SLk fk,n Instead
of choosing gn in fn in Alg replace it by gn
Then Propositions and still hold
Applications and Experimental Validation
In this section we introduce different applications and provide numerical experiments A
C++/Matlab implementation is available in the software package SPAMS All experiments
were performed on a single core of a 2GHz Intel CPU with of RAM.
http://spams-devel.gforge.inria.fr
Stochastic Proximal Gradient Descent Algorithm
Our first application is a stochastic proximal gradient descent method which we call SMM Stochastic Majorization-Minimization for solving problems of the form
min Ex
where is a convex deterministic regularization function and the functions are differentiable and their gradients are L-Lipschitz continuous We can thus use the proximal gradient
surrogate presented in Section Assume that a weight sequence wn is chosen such that
w1 By defining some other weights wni recursively as wni wn for and
wnn wn our scheme yields the update rule
arg min
wni fi
SMM
Our algorithm is related to FOBOS to SMIDAS or the truncated gradient method
when is the norm These three algorithms use indeed the following update rule
FOBOS
arg min fn
Another related scheme is the regularized dual averaging RDA of which can be written as
1X
RDA
fi
arg min
Compared to these approaches our scheme includes a weighted average of previously seen gradients and a weighted average of the past iterates Some links can also be drawn with approaches
such as the approximate follow the leader algorithm of and other works
We now evaluate the performance of our method for logistic regression In summary the datasets
consist of pairs
where the are in and the are in with unit norm The function in is the norm and is a regularization parameter
the functions fi are logistic losses fi log(1 e?yi One part of each
dataset is devoted
to training and another part to testing We used weights of the form wn n0
where n0 is automatically adjusted at the beginning of each experiment by performing one pass on
of the training data We implemented SMM in and exploited the sparseness of the datasets
such that each update has a computational complexity of the order where is the number of
non zeros in fn such an implementation is non trivial but proved to be very efficient
We consider three datasets described in the table below rcv1 and webspam are obtained from the
Pascal large-scale learning challenge.3 is available from the LIBSVM website.4
name
rcv1
webspam
Ntr train
Nte test
23
47
28
density
size
We compare our implementation with state-of-the-art publicly available solvers the batch algorithm
FISTA of implemented in the SPAMS toolbox and LIBLINEAR LIBLINEAR
is based on a working-set algorithm and to the best of our knowledge is one of the most efficient
available solver for logistic regression with sparse datasets Because is large the incremental
majorization-minimization method of could not run for memory reasons We run every method
on and epochs passes over the training set for three regularization regimes
respectively yielding a solution with approximately and non-zero coefficients
We report results for the medium regularization in Figure and provide the rest as supplemental
material FISTA is not represented in this figure since it required more than epochs to achieve
reasonable values Our conclusion is that SMM often provides a reasonable solution after one epoch
and outperforms LIBLINEAR in the low-precision regime For high-precision regimes LIBLINEAR
should be preferred Such a conclusion is often obtained when comparing batch and stochastic
algorithms but matching the performance of LIBLINEAR is very challenging
http://largescale.ml.tu-berlin.de
http://www.csie.ntu.edu.tw/?cjlin/libsvm
LIBLINEAR
SMM
Objective on Training Set
Objective on Training Set
LIBLINEAR
SMM
LIBLINEAR
SMM
Epochs Dataset kddb
LIBLINEAR
SMM
Epochs Dataset rcv1
Epochs Dataset webspam
Computation Time sec Dataset webspam
LIBLINEAR
SMM
Epochs Dataset webspam
LIBLINEAR
SMM
Computation Time sec Dataset rcv1
Objective on Training Set
Objective on Training Set
Epochs Dataset rcv1
Objective on Testing Set
Objective on Testing Set
LIBLINEAR
SMM
Objective on Testing Set
Objective on Training Set
Objective on Training Set
LIBLINEAR
SMM
LIBLINEAR
SMM
Epochs Dataset kddb
Computation Time sec Dataset kddb
Figure Comparison between LIBLINEAR and SMM for the medium regularization regime
Online DC Programming for Non-Convex Sparse Estimation
We now consider the same
Pp experimental setting as in the previous section but with a non-convex
regularizer where is the j-th entry in A classical way for
PN
minimizing the regularized empirical cost N1 fi is to resort to DC programming It
consists of solving a sequence of reweighted-?1 problems A current estimate is updated
PN
Pp
as a solution of N1 fi where
In contrast to this batch methodology we can use our framework to address the problem online
At iteration of Algorithm we define the function gn according to Proposition
Pp
gn fn fn
Online DC
Batch DC
Iterations Epochs Dataset rcv1
Iterations Epochs Dataset rcv1
Online DC
Batch DC
Online DC
Batch DC
Objective on Test Set
Online DC
Batch DC
Objective on Train Set
Objective on Test Set
Objective on Train Set
We compare our online DC programming algorithm against the batch one and report the results in
Figure with set to We conclude that the batch reweighted-?1 algorithm always converges
after or weight updates but suffers from local minima issues The stochastic algorithm exhibits
a slower convergence but provides significantly better solutions Whether or not there are good
theoretical reasons for this fact remains to be investigated Note that it would have been more
rigorous to choose a bounded set which is required by Proposition In practice it turns not to
be necessary for our method to work well the iterates have indeed remained in a bounded set
Iterations Epochs Dataset webspam
Iterations Epochs Dataset webspam
Figure Comparison between batch and online DC programming with medium regularization for
the datasets rcv1 and webspam Additional plots are provided in the supplemental material Note
that each iteration in the batch setting can perform several epochs passes over training data
Online Structured Sparse Coding
In this section we show that we can bring new functionalities to existing matrix factorization techm
niques We are given a large collection of signals
in and we want to find a
dictionary in Rm?K that can represent these signals in a sparse way The quality of is measured through the loss min??RK kx where the norm
can be replaced by any convex regularizer and the squared loss by any convex smooth loss
Then we are interested in minimizing the following expected cost
min
D?Rm?K
Ex
where is a regularizer for D. In the online learning approach of the only way to regularize
is to use a constraint set on which we need to be able to project efficiently this is unfortunately not
always possible In the matrix factorization framework of it is argued that some applications
can benefit from a structured penalty but the approach of is not easily amenable to stochastic
optimization Our approach makes it possible by using the proximal gradient surrogate
gn Tr kD k2F
It is indeed possible to show that is differentiable and its gradient is Lipschitz
continuous with a constant that can be explicitly computed
We now design a proof-of-concept experiment We consider a set of whitened natural
image patches of size pixels We visualize some elements from a dictionary
trained by SPAMS on the left of Figure the dictionary elements are almost sparse but have
some residual noise among the small coefficients Following we propose to use a regularization
function encouraging neighbor pixels to be set to zero together thus leading to a sparse structured
dictionary We consider the collection of all groups of variables corresponding to squares of
PK
neighbor pixels in Then we define g?G maxk?g dj kDk2F
where dj is the j-th column of D. The penalty is a structured sparsity-inducing penalty that encourages groups of variables to be set to zero together Its proximal operator can be computed
efficiently and it is thus easy to use the surrogates We set and after
trying a few values for and at a reasonable computational cost we obtain dictionaries with the
desired regularization effect as shown in Figure Learning one dictionary of size took
a few minutes when performing one pass on the training data with mini-batches of size This
experiment demonstrates that our approach is more flexible and general than and Note that
it is possible to show that when is large enough the iterates Dn necessarily remain in a bounded
set and thus our convergence analysis presented in Section applies to this experiment
Figure Left Two visualizations of elements from a larger dictionary obtained by the toolbox
SPAMS the second view amplifies the small coefficients Right the corresponding views of
the dictionary elements obtained by our approach after initialization with the dictionary on the left
Conclusion
In this paper we have introduced a stochastic majorization-minimization algorithm that gracefully
scales to millions of training samples We have shown that it has strong theoretical properties and
some practical value in the context of machine learning We have derived from our framework
several new algorithms which have shown to match or outperform the state of the art for solving
large-scale convex problems and to open up new possibilities for non-convex ones In the future
we would like to study surrogate functions that can exploit the curvature of the objective function
which we believe is a crucial issue to deal with badly conditioned datasets
Acknowledgments
This work was supported by the Gargantua project program Mastodons CNRS

----------------------------------------------------------------

title: 6070-asynchronous-parallel-greedy-coordinate-descent.pdf

Asynchronous Parallel Greedy Coordinate Descent
Yang You XiangRu Lian Ji Liu Hsiang-Fu Yu
Inderjit S. Dhillon James Demmel Cho-Jui Hsieh
equally contributed
University of California Davis
University of Rochester
University of Texas Austin
University of California Berkeley
youyang@cs.berkeley.edu xiangru@yandex.com jliu@cs.rochester.edu
rofuyu,inderjit}@cs.utexas.edu demmel@eecs.berkeley.edu
chohsieh@cs.ucdavis.edu
Abstract
In this paper we propose and study an Asynchronous parallel Greedy Coordinate
Descent Asy-GCD algorithm for minimizing a smooth function with bounded
constraints At each iteration workers asynchronously conduct greedy coordinate
descent updates on a block of variables In the first part of the paper we analyze the
theoretical behavior of Asy-GCD and prove a linear convergence rate In the second
part we develop an efficient kernel SVM solver based on Asy-GCD in the shared
memory multi-core setting Since our algorithm is fully asynchronous?each core
does not need to idle and wait for the other cores?the resulting algorithm enjoys
good speedup and outperforms existing multi-core kernel SVM solvers including
asynchronous stochastic coordinate descent and multi-core LIBSVM
Introduction
Asynchronous parallel optimization has recently become a popular way to speedup machine learning
algorithms using multiple processors The key idea of asynchronous parallel optimization is to allow
machines work independently without waiting for the synchronization points It has many successful
applications including linear SVM deep neural networks matrix completion
linear programming and its theoretical behavior has been deeply studied in the past few
years
The most widely used asynchronous optimization algorithms are stochastic gradient method
and coordinate descent 13 where the workers keep selecting a sample or a
variable randomly and conduct the corresponding update asynchronously Although these stochastic
algorithms have been studied deeply in some important machine learning problems a greedy
approach can achieve much faster convergence speed A very famous example is greedy coordinate
descent instead of randomly choosing a variable at each iteration the algorithm selects the most
important variable to update If this selection step can be implemented efficiently greedy coordinate
descent can often make bigger progress compared with stochastic coordinate descent leading to a
faster convergence speed For example the decomposition method variant of greedy coordinate
descent is widely known as best solver for kernel SVM which is implemented in LIBSVM
and SVMLight Other successful applications can be found in
In this paper we study asynchronous greedy coordinate descent algorithm framework The variable is
partitioned into subsets and each worker asynchronously conducts greedy coordinate descent in one
of the blocks To our knowledge this is the first paper to present a theoretical analysis or practical
applications of this asynchronous parallel algorithm In the first part of the paper we formally define
the asynchronous greedy coordinate descent procedure and prove a linear convergence rate under
mild assumption In the second part of the paper we discuss how to apply this algorithm to solve the
kernel SVM problem on multi-core machines Our algorithm achieves linear speedup with number of
cores and performs better than other multi-core SVM solvers
Conference on Neural Information Processing Systems NIPS Barcelona Spain
The rest of the paper is outlined as follows The related work is discussed in Section We propose
the asynchronous greedy coordinate descent algorithm in Section and derive the convergence rate
in the same section In Section we show the details how to apply this algorithm for training kernel
SVM and the experimental comparisons are presented in Section
Related Work
Coordinate Descent Coordinate descent has been extensively studied in the optimization
community and has become widely used in machine learning At each iteration only one variable
is chosen and updated while all the other variables remain fixed CD can be classified into stochastic
coordinate descent cyclic coordinate descent CCD and greedy coordinate descent GCD
based on their variable selection scheme In SCD variables are chosen randomly based on some
distribution and this simple approach has been successfully applied in solving many machine learning
problems The theoretical analysis of SCD has been discussed in Cyclic coordinate
descent updates variables in a cyclic order and has also been applied to several applications
Greedy Coordinate Descent GCD).The idea of GCD is to select a good instead of random
coordinate that can yield better reduction of objective function value This can often be measured by
the magnitude of gradient projected gradient for constrained minimization or proximal gradient
for composite minimization Since the variable is carefully selected at each iteration GCD can
reduce objective function more than SCD or CCD which leads to faster convergence in practice
Unfortunately selecting a variable with larger gradient is often time consuming so one needs to
carefully organize the computation to avoid the overhead and this is often problem dependent
The most famous application of GCD is the decomposition method used in kernel SVM.
By exploiting the structure of quadratic programming selecting the variable with largest gradient
magnitude can be done without any overhead as a result GCD becomes the dominant technique in
solving kernel SVM and is implemented in LIBSVM and SVMLight There are also other
applications of GCD such as non-negative matrix factorization large-scale linear SVM
and proposed an approximate way to select variables in GCD. Recently proved an improved
convergence bound for greedy coordinate descent We focus on parallelizing the GS-r rule in this
paper but our analysis can be potentially extended to the GS-q rule mentioned in that paper
To the best of our knowledge the only literature discussing how to parallelize GCD was in
A thread-greedy/block-greedy coordinate descent is a synchronized parallel GCD for L1 regularized
empirical risk minimization At an iteration each thread randomly selects a block of coordinates
from a pre-partitioned block partition and proposes the best coordinate from this block along with its
increment step size Then all the threads are synchronized to perform the actual update to the
variables However the method can potentially diverge indeed this is mentioned in about the
potential divergence when the number of threads is large establishes sub-linear convergence for
this algorithm
Asynchronous Parallel Optimization Algorithms.In a synchronous algorithm each worker conducts local updates and in the end of each round they have to stop and communicate to get the new
parameters This is not efficient when scaling to large problem due to the curse of last reducer all
the workers have to wait for the slowest one In contrast in asynchronous algorithms there is no
synchronization point so the throughput will be much higher than a synchronized system As a result
many recent work focus on developing asynchronous parallel algorithms for machine learning as well
as providing theoretical guarantee for those algorithms 13 19 28
In distributed systems asynchronous algorithms are often implemented using the concept of parameter
servers In such setting each machine asynchronously communicates with the server to
read or write the parameters In our experiments we focus on another multi-core shared memory
setting where multiple cores in a single machine conduct updates independently and asynchronously
and the communication is implicitly done by reading/writing to the parameters stored in the shared
memory space This has been first discussed in for the stochastic gradient method and recently
proposed for parallelizing stochastic coordinate descent
This is the first work proposing an asynchronous greedy coordinate decent framework The closest
work to ours is for asynchronous stochastic coordinate descent ASCD In their algorithm each
worker asynchronously conducts the following updates randomly select a variable compute
the update and write to memory or server In our AGCD algorithm each worker will select the best
variable to update in a block which leads to faster convergence speed We also compare with ASCD
algorithm in the experimental results for solving the kernel SVM problem
Asynchronous Greedy Coordinate Descent
We consider the following constrained minimization problem
min
where is convex and smooth RN is the constraint set and each
is a closed subinterval of the real line
Notation We denote to be the optimal solution set for and PS to be the Euclidean
projection of onto respectively We also denote to be the optimal objective function value
for
We propose the following Asynchronous parallel Greedy Coordinate Descent Asy-GCD for solving Assume coordinates are divided into non-overlapping sets S1 Sn Let be the
global counter of total number of updates In Asy-GCD each processor repeatedly runs the following
GCD updates
Randomly select a set Sk Sn and pick the coordinate ik Sk where the projected
gradient defined in has largest absolute value
Update the parameter by
xk
rik xk
where is the step size
Here the projected gradient defined by
xk xk
ik
xk
rik
xk
is a measurement of optimality for each variable where
is current point stored in memory used
to calculate the update The processors will run concurrently without synchronization In order to
analyze Asy-GCD we capture the system-wise global view in Algorithm
Algorithm Asynchronous Parallel Greedy Coordinate Descent Asy-GCD
Input
Output
Initialize
while do
Choose Sk from Sn with equal probability
Pick ik arg maxi2Sk kr
xk
rik
xk
end while
The update in the th iteration is
th
xk
rik
xk
where ik is the selected coordinate in iteration
is the point used to calculate the gradient and
rik
xk is a zero vector where the ik th coordinate is set to the corresponding coordinate of the
gradient of at
Note that
may not be equal to the current value of the optimization variable
xk due to asynchrony Later in the theoretical analysis we will need to assume
is close to xk using
the bounded delay assumption
In the following we prove the convergence behavior of Asy-GCD We first make some commonly
used assumptions
Assumption
Bounded Delay There is a set for each iteration such that
xk
where is the upper bound of the staleness In this inconsistent read model we assume
some of the latest updates are not yet written back to memory This is also used in some
previous papers and is more general than the consistent read model that assumes
is equal to some previous iterate
For simplicity we assume each set Si has coordinates
Lipschitzian Gradient The gradient function of the objective rf is Lipschitzian That
is to say
krf rf Lkx yk
Under the Lipschitzian gradient assumption we can define three more constants Lres Ls and
Lmax Define Lres to be the restricted Lipschitz constant satisfying the following inequality
8i and with tei
Let ri be the operator calculating a zero vector where the ith coordinate is set to the ith
coordinate of the gradient Define for as the minimum constant that
satisfies
kri ri ei
Define Lmax It can be seen that Lmax Lres L.
Let be any positive integer bounded by Define Ls to be the minimal constant satisfying
the following inequality 8x 8S where
rf rf i2S ei Ls
i2S ei
krf
rf ei Lres
Global Error Bound We assume that our objective has the following property when
3L1max there exists a constant such that
PS
xk 8x
Where
is defined by argminx0 hrf kx0 xk2 This is satisfied by
strongly convex objectives and some weakly convex objectives For example it is proved
in that the kernel SVM problem satisfies the global error bound even when the
kernel is not strictly positive definite
Independence All random variables in Sk in Algorithm are independent to
each other
kx
We then have the following convergence result
Theorem Convergence Choose in Algorithm Suppose
upper bound for staleness satisfies the following condition
nLmax
4eLres
Under Assumption we have the following convergence rate for Algorithm
2Lmax
E(f xk
where is defined as
18 nLmax Lres
and that the
This theorem
indicates a linear convergence rate under the global error bound and the condition
Since is usually proportional to the total number cores involved in the computation
this result suggests that one can have linear speedup as long as the total number of cores is smaller
than Note that for Algorithm reduces to the standard asynchronous coordinate
descent algorithm ASCD and our result is essentially consistent with the one in although they
use the optimally strong convexity assumption for The optimally strong convexity is a similar
condition to the global error bound assumption
Here we briefly discuss the constants involved in the convergence rate Using Gaussian kernel SVM on
covtype as a concrete sample Lmax for Gaussian kernel Lres is the maximum norm of columns
of kernel matrix is the 2-norm of for covtype and conditional number
As number of samples increased the conditional number will become a dominant term and this
also appears in the rate of serial greedy coordinate descent In terms of speedup when increasing
L2T
number of threads although LT may grow it only appears in 18pnLmax
where
Lres
the first term inside is usually small since there is a in the demominator Therefore in
most cases which means the convergence rate does not slow down too much when we increase
Application to Multi-core Kernel SVM
In this section we demonstrate how to apply asynchronous parallel greedy coordinate descent to
solve kernel SVM We follow the conventional notations for kernel SVM where the variables
for the dual form are Rn instead of in the previous section Given training samples
with corresponding labels kernel SVM solves the following quadratic minimization
problem
minn
eT
where is an by symmetric matrix with Qij yj K(ai aj and K(ai aj is the kernel
function Gaussian kernel is a widely-used kernel function where K(ai aj kai aj
Greedy coordinate descent is the most popular way to solve kernel SVM. In the following we first
introduce greedy coordinate descent for kernel SVM and then discuss the detailed update rule and
implementation issues when applying our proposed Asy-GCD algorithm on multi-core machines
Kernel SVM and greedy coordinate descent
When we apply coordinate descent to solve the dual form of kernel SVM the one variable update
rule for any index can be computed by
rfi
where is the projection to the interval and the gradient is rfi Note
that this update rule is slightly different from by setting the step size to be 1/Qii For
quadratic functions this step size leads to faster convergence because obtained by is the closed
form solution of
arg min ei
and ei is the i-th indicator vector
As in Algorithm we choose the best coordinate based on the magnitude of projected gradient In
this case by definition
ri
The success of GCD in solving kernel SVM is mainly due to the maintenance of the gradient
ri
Consider the update rule it requires time to compute which is the cost for stochastic
coordinate descent or cyclic coordinate descent However in the following we show that GCD has
the same time complexity per update by using the trick of maintaining during the whole procedure
If is available in memory each element of the projected gradient can be computed in
time so selecting the variable with maximum magnitude of projected gradient only costs time
The single variable update can be computed in time After the update
the
has to be updated by
qi where qi is the i-th column of Q. This also costs time
Therefore each GCD update only costs using this trick of maintaining
Therefore for solving kernel SVM GCD is faster than SCD and CCD since there is no additional
cost for selecting the best variable to update Note that in the above discussion we assume can be
stored in memory Unfortunately this is not the case for large scale problems because is an by
dense matrix where can be millions We will discuss how to deal with this issue in Section
With the trick of maintaining the GCD for solving can be summarized in Algorithm
Algorithm Greedy Coordinate Descent GCD for Dual Kernel SVM
Initial
For
step Pick arg maxi
using
step Compute by eq
step
qi
step
See eq
Asynchronous greedy coordinate descent
When we have threads in a multi-core shared memory machine and the dual variables corresponding training samples are partitioned into the same number of blocks
S1 S2 Sn and Si Sj
for all
Now we apply Asy-GCD algorithm to solve For better memory allocation of kernel cache
Section we bind each thread to a partition The behavior of our algorithm still follows
Asy-GCD because the sequence of updates are asynchronously random The algorithm is summarized
in Algorithm
Algorithm Asy-GCD for Dual Kernel SVM
Initial
Each thread repeatedly performs the following updates in parallel
step Pick arg maxi2St
using
step Compute by eq
step For
gj
gj Qj,i using atomic update
step
See eq
Note that each thread will read the dimensional vector in step and update in step in the
shared memory For the read we do not use any atomic operations For the writes we maintain the
correctness of by atomic writes otherwise some updates to might be overwritten by others and
the algorithm cannot converge to the optimal solution Theorem suggests a linear convergence rate
of our algorithm and in the experimental results we will see the algorithm is much faster than the
widely used Asynchronous Stochastic Coordinate Descent Asy-SCD algorithm
Implementation Issues
In addition to the main algorithm there are some practical issues we need to handle in order to
make Asy-GCD algorithm scales to large-scale kernel SVM problems Here we discuss these
implementation issues
Kernel Caching.The main difficulty for scaling kernel SVM to large dataset is the memory requirement for storing the matrix which takes memory In the GCD algorithm step eq
requires a diagonal element of which can be pre-computed and stored in memory However the
main difficulty is to conduct step where a column of denoted by qi is needed If qi is in the
memory the algorithm only takes time however if qi is not in the memory re-computing it
from scratch takes O(dn time As a result how to maintain most important columns of in memory
is an important implementation issues in SVM software
In LIBSVM the user can specify the size of memory they want to use for storing columns of Q. The
columns of are stored in a linked-list in the memory and when memory space is not enough the
Least Recent Used column will be kicked out LRU technique
In our implementation instead of sharing the same LRU for all the cores we create an individual
LRU for each core and make the memory space used by a core in a contiguous memory space As
a result remote memory access will happen less in the NUMA system when there are more than
CPU in the same computer Using this technique our algorithm is able to scale up in a multi-socket
machine Figure
Variable Partitioning.The theory of Asy-GCD allows any non-overlapping partition of the dual
variables However we observe a better partition that minimizes the between-cluster connections can
often lead to faster convergence This idea has been used in a divide-and-conquer SVM algorithm
and we use the same idea to obtain the partition More specifically we partition the data by running
kmeans algorithm on a subset of training samples to obtain cluster centers cr and then
assign each to the nearest center argminr kcr This steps can be easily parallelized
and costs less than seconds in all the datasets used in the experiments Note that we include this
kmeans time in all our experimental comparisons
Experimental Results
We conduct experiments to show that the proposed method Asy-GCD achieves good speedup in
parallelizing kernel SVM in multi-core systems We consider three datasets ijcnn1 covtype and
webspam Table for detailed information We follow the parameter settings in where
Table Data statistics is number of training samples is dimensionality is number of testing
samples
ijcnn1
22 32
covtype
54 32 32
webspam 32
ijcnn1 time vs obj
webspam time vs obj
covtype time vs obj
Figure Comparison of Asy-GCD with threads on ijcnn1 covtype and webspam datasets
and are selected by cross validation All the experiments are run on the same system with CPUs
and memory where the CPU has two sockets each with cores We locate for kernel
caching for all the algorithms In our algorithm the will distribute to each core for example
for Asy-GCD with cores each core will have kernel cache
We include the following algorithms/implementations into our comparison
Asy-GCD Our proposed method implemented by with OpenMP Note that the preprocessing time for computing the partition is included in all the timing results
PSCD We implement the asynchronous stochastic coordinate descent approach for
solving kernel SVM. Instead of forming the whole kernel matrix in the beginning which
cannot scale to all the dataset we are using we use the same kernel caching technique as
Asy-GCD to scale up PSCD
LIBSVM In LIBSVM there is an option to speedup the algorithm in multi-core environment using OpenMP http://www.csie.ntu.edu.tw/~cjlin/libsvm
faq.html#f432 This approach uses multiple cores when computing a column of kernel
matrix qi used in step of Algorithm
All the implementations are modified from LIBSVM they share the similar LRU cache class
so the comparison is very fair We conduct the following two sets of experiments Note that another
recent proposed DC-SVM solver is currently not parallelizable however since it is a meta
algorithm and requires training a series of SVM problems our algorithm can be naturally served as a
building block of DC-SVM
Scaling with number of cores
In the first set of experiments we test the speedup of our algorithm with varying number of cores
The results are presented in Figure and Figure We have the following observations
Time vs obj for cores From we observe that when we use
more CPU cores the objective decreases faster
Cores vs speedup From we can observe that we got good strong scaling when we
increase the number of threads Note that our computer has two sockets each with cores
and our algorithm can often achieve times speedup This suggests our algorithm can
scale to multiple sockets in a Non-Uniform Memory Access NUMA system Previous
asynchronous parallel algorithms such as HogWild or PASSCoDe often struggle
when scaling to multiple sockets
Comparison with other methods
Now we compare the efficiency of our proposed algorithm with other multi-core parallel kernel SVM
solvers on real datasets in Figure All the algorithms in this comparison are using cores and
memory space for kernel caching Note that LIBSVM is solving the kernel SVM problem with
the bias term so the objective function value is not showing in the figures
We have the following observations
ijcnn1 cores vs speedup
webspam cores vs speedup
covtype cores vs speedup
Figure The scalability of Asy-GCD with up to threads
ijcnn1 time vs accuracy
covtype time vs accuracy
webspam time vs accuracy
ijcnn1 time vs objective
covtype time vs objective
webspam time vs objective
Figure Comparison among multi-core kernel SVM solvers All the solvers use cores and the
same amount of memory
Our algorithm achieves much faster convergence in terms of objective function value
compared with PSCD This is not surprising because using the trick of maintaining
details in Section greedy approach can select the best variable to update while stochastic
approach just chooses variables randomly In terms of accuracy PSCD is sometimes good in
the beginning but converges very slowly to the best accuracy For example in covtype data
the accuracy of PSCD remains after seconds while our algorithm can achieve
accuracy after seconds
LIBSVM OMP is slower than our method The main reason is that they only use multiple
cores when computing kernel values so the computational power is wasted when the column
of kernel qi needed is available in memory
Conclusions In this paper we propose an Asynchronous parallel Greedy Coordinate Descent AsyGCD algorithm and prove a linear convergence rate under mild condition We show our algorithm
is useful for parallelizing the greedy coordinate descent method for solving kernel SVM and the
resulting algorithm is much faster than existing multi-core SVM solvers
Acknowledgement XL and JL are supported by the NSF grant HFY and ISD
are supported by the NSF grants and YY and JD are
supported by the U.S. Department of Energy Office of Science Office of Advanced Scientific
Computing Research Applied Mathematics program under Award Number by the
U.S. Department of Energy Office of Science Office of Advanced Scientific Computing Research
under Award Numbers and by DARPA Award Number Intel Google HP Huawei LGE Nokia NVIDIA Oracle and Samsung Mathworks
and Cray CJH also thank the XSEDE and Nvidia support

----------------------------------------------------------------

title: 5924-a-dual-augmented-block-minimization-framework-for-learning-with-limited-memory.pdf

A Dual-Augmented Block Minimization Framework
for Learning with Limited Memory
Ian E.H. Yen Shan-Wei Lin Shou-De Lin
University of Texas at Austin
National Taiwan University
ianyen@cs.utexas.edu r03922067,sdlin}@csie.ntu.edu.tw
Abstract
In past few years several techniques have been proposed for training of linear
Support Vector Machine SVM in limited-memory setting where a dual blockcoordinate descent dual-BCD method was used to balance cost spent on I/O and
computation In this paper we consider the more general setting of regularized
Empirical Risk Minimization ERM when data cannot fit into memory In particular we generalize the existing block minimization framework based on strong
duality and Augmented Lagrangian technique to achieve global convergence for
general convex ERM. The block minimization framework is flexible in the sense
that given a solver working under sufficient memory one can integrate it with
the framework to obtain a solver globally convergent under limited-memory condition We conduct experiments on L1-regularized classification and regression
problems to corroborate our convergence theory and compare the proposed framework to algorithms adopted from online and distributed settings which shows superiority of the proposed approach on data of size ten times larger than the memory
capacity
Introduction
Nowadays data of huge scale are prevalent in many applications of statistical learning and data
mining It has been argued that model performance can be boosted by increasing both number
of samples and features and through crowdsourcing technology annotated samples of terabytes
storage size can be generated As a result the performance of model is no longer limited by the
sample size but the amount of available computational resources In other words the data size can
easily go beyond the size of physical memory of available machines Under this setting most of
learning algorithms become slow due to expensive I/O from secondary storage device
When it comes to huge-scale data two settings are often considered online and distributed learning In the online setting each sample is processed only once without storage while in the distributed setting one has several machines that can jointly fit the data into memory However the
real cases are often not as extreme as these two there are usually machines that can fit part of the
data but not all of them In this setting an algorithm can only process a block of data at a time
Therefore balancing the time spent on I/O and computation becomes the key issue Although
one can employ an online-fashioned learning algorithm in this setting it has been observed that online method requires large number of epoches to achieve comparable performance to batch method
and at each epoch it spends most of time on I/O instead of computation The situation
for online method could become worse for problem of non-smooth non-strongly convex objective
function where a qualitatively slower convergence of online method is exhibited than that
proved for strongly-convex problem like SVM
In the past few years several algorithms have been proposed to solve large-scale linear Support Vector Machine SVM in the limited memory setting These approaches are based on a dual
Block Coordinate Descent dual-BCD algorithim which decomposes the original problem into a
series of block sub-problems each of them requires only a block of data loaded into memory The
approach was proved linearly convergent to the global optimum and demonstrated fast convergence
empirically However the convergence of the algorithm relies on the assumption of a smooth dual
problem which as we show does not hold generally for other regularized Empirical Risk Minimizaton ERM problem As a result although the dual-BCD approach can be extended to the more
general setting it is not globally convergent except for a class of problems with L2-regularizer
In this paper we first show how to adapt the dual block-coordinate descnet method of to
the general setting of regularized Empirical Risk Mimization which subsumes most of supervised learning problems ranging from classification regression to ranking and recommendation
Then we discuss the convergence issue arises when the underlying ERM is not strongly-convex A
Primal Proximal Point or Dual Augmented Lagrangian method is then proposed to address this
issue which as we show results in a block minimization algorithm with global convergence to optimum for convex regularized ERM problems The framework is flexible in the sense that given a
solver working under sufficient-memory condition it can be integrated into the block minimization
framework to obtain a solver globally convergent under limited-memory condition
We conduct experiments on L1-regularized classification and regression problems to corroborate
our convergence theory which shows that the proposed simple dual-augmented technique changes
the convergence behavior dramatically We also compare the proposed framework to algorithms
adopted from online and distributed settings In particular we describe how to adapt a distributed optimization framework Alternating Direction Method of Multiplier ADMM to the limitedmemory setting and show that although the adapted algorithm is effective it is not as efficient as the
proposed framework specially designed for limited-memory setting Note our experiment does not
adapt into comparison some recently proposed distributed learning algorithms CoCoA etc
that only apply to ERM with L2-regularizer or some other distributed method designed for some
specific loss function
Problem Setup
In this work we consider the regularized Empirical Risk Minimization problem which given a data
set
estimates a model through
min
w?Rd Rp
Ln
where Rd is the model parameter to be estimated is a by design matrix that encodes
features of the n-th data sample Ln is a convex loss function that penalizes the discrepancy
between ground truth and prediction vector Rp and is a convex regularization term
penalizing model complexity
The formulation subsumes a large class of statistical learning problems ranging from classification regression ranking and convex clustering For example in classification
problem we have where YPconsists of the set of all possible labels and Ln can be defined
as the logistic loss Ln log k?Y exp(?k yn as in logistic regression or the hinge loss
Ln maxk?Y k,yn yn as used in support vector machine in a multi-task regression problem the target variable consists of real values RK the prediction vector has
dimensions and a square loss Ln is often used There are also a variety of regularizers employed in different applications which includes the L2-regularizer kwk2
in ridge regression L1-regularizer kwk1 in Lasso nuclear-norm kwk in
matrix completion and a family of structured group norms kwkG Although the
specific form of Ln does not affect the implementation of the limited-memory training
procedure two properties of the functions strong convexity and smoothness have key effects
on the behavior of the block minimization algorithm
Definition Strong Convexity A function is strongly convex iff it is lower bounded by a
simple quadratic function
kx yk2
for some constant and dom(f
Definition Smoothness A function is smooth iff it is upper bounded by a simple quadratic
function
kx yk2
for some constant and dom(f
For instance the square loss and logistic loss are both smooth and strongly convex while the hingeloss satisfies neither of them On the other hand most of regularizers such as L1-norm structured
group norm and nuclear norm are neither smooth nor strongly convex except for the L2-regularizer
which satifies both In the following we will demonstrate the effects of these properties to Block
Minimization algorithms
Throughout this paper we will assume that a solver for that works in sufficient-memory condition
is given and our task is to design an algorithmic framework that integrates with the solver to efficiently solve when data cannot fit into memory We will assume however that the d-dimensional
parameter vector can be fit into memory
Dual Block Minimization
In this section we extend the block minimization framework of from linear SVM to the general
setting of regularized ERM dual of can be expressed as
min
Rp
L?n
Tn
where is the convex conjugate of and L?n is the convex conjugate of Ln
The block minimization algorithm of basically performs a dual Block-Coordinate Descent
dual-BCD over by dividing the whole data set into blocks DB1 DBK and optimizing a block of dual variables at a time where DBk and
Bk Bk
In the dual problem is derived explicitly in order to perform the algorithm However
for many sparsity-inducing regularizer such as L1-norm and nuclear norm it is more efficient and
convenient to solve in the primal Therefore here instead of explicitly forming the dual
problem we express it implicitly as
min
where is the Lagrangian function of and maximize a block of variables
Bk from the primal instead of dual by strong duality
max min min max
Bk
Bk
tBj
with other dual variables
fixed The maximization of dual variables Bk in
then enforces the primal equalities Bk which results in the block minimization
problem
min
Ln tT
Bk
w?Rd Rp
n?Bk
Bk
The logistic loss is strongly convex when its input are within a bounded range which is true as long as
we have a non-zero regularizer
where tBk n?B
have been dropped since they
Note that in variables
are not relevant to the block of dual variables Bk and thus given the dimensional vector tBk
one can solve without accessing data
outside the block Bk Throughout the
PN
dual-BCD algorithm we maintain d-dimensional vector Tn tn and compute tB via
tB
Tn tn
n?Bk
in the beginning of solving each block subproblem Since subproblem is of the same form to
the original problem except for one additional linear augmented term TBk one can adapt the
solver of to solve easily by providing an augmented version of the gradient
tBk
to the solver where denotes the function with augmented terms and denotes the function
without augmented terms Note the augmented term tBk is constant and separable coordinates so it adds little overhead to the solver After obtaining solution Bk from we can
derive the corresponding optimal dual variables Bk for according to the KKT condition and
maintain subsequently by
Ln Bk
tBk
Tn
n?Bk
The procedure is summarized in Algorithm which requires a total memory capacity of O(d
DBk p|Bk The factor comes from the storage of wt factor DBk comes from the
storage of data block and the factor p|Bk comes from the storage of Bk Note this requires the
same space complexity as that required in the original algorithm proposed for linear SVM
where for the binary classification setting
Dual-Augmented Block Minimization
The Block Minimization Algorithm though can be applied to the general regularized ERM problem it is not guaranteed that the sequence
produced by Algorithm converges to global
optimum of In fact the global convergence of Algorithm only happens for some special cases
One sufficient condition for the global convergence of a Block-Coordinate Descent algorithm is that
the terms in objective function that are not separable blocks must be smooth Definition
The dual objective function expressed using only comprises two terms
PN
PN
Tn L?n where second term is separable to
and thus is also separable while the first term couples variables BK
involving all the blocks As a result if is a smooth function according to Definition then
Algorithm has global convergence to the optimum However the following theorem states this is
true only when is strongly convex
Theorem Strong/Smooth Duality Assume is closed and convex Then is smooth with
parameter if and only if its convex conjugate is strongly convex with parameter
A proof of above theorem can be found in According to Theorem the Block Minimization
Algorithm is not globally convergent if is not strongly convex which however is the case
for most of regularizers other than the L2-norm kwk2 as discussed in Section
In this section we propose a remedy to this problem which by a Dual-Augmented Lagrangian
method equivalently Primal Proximal Point method creates a dual objective function of desired
property that iteratively approaches the original objective and results in fast global convergence
of the dual-BCD approach
Algorithm Dual Block Minimization
Split data into blocks B1 B2 BK
Initialize
for do
Draw uniformly from
Load DBk and tBk into memory
Compute tBk from
Solve to obtain Bk
Compute
Bk by
Maintain through
Save
Bk out of memory
end for
Algorithm Dual-Aug Block Minimization
Split data into blocks B1 B2 BK
Initialize w0
for outer iteration do
for do
Draw uniformly from
Load DBk sBk into memory
Compute sBk from
Solve to obtain Bk
Compute
Bk by
Maintain through
Save
Bk out of memory
end for
end for
Algorithm
The Dual Augmented Lagrangian DAL method equivalently Proximal Point Method modifies
the original problem by introducing a sequence of Proximal Maps
arg min
kw wt
where denotes the ERM problem Under this simple modification instead of doing BlockCoordinate Descent in the dual of original problem we perform Dual-BCD on the proximal subproblem As we show in next section the dual formulation of has the required property
for global convergence of the Dual BCD algorithm all terms involving more than one block of
variables Bk are smooth Given the current iterate wt the Dual-Augmented Block Minimization
algorithm optimizes the dual of proximal-point problem one block of variables Bk at a
time keeping others fixed Bj
max min min max
Bk
where is the Lagrangian of
Bk
Tn
kw wt
Once again the maximization Bk in enforces the equalities Bk and
thus leads to a primal sub-problem involving only data in block Bk
min
Ln Bk
kw wt
w?Rd Rp
n?Bk
Bk
where
Note that is almost the same as except that it has a
n?B
proximal-point augmented term Therefore one can follow the same procedure as in Algorithm to
PN
maintain the vector Tn and computes
Bk
Tn
Bk
n?Bk
before solving each block subproblem After obtaining solution Bk from we update
dual variables Bk as
Ln Bk
and maintain subsequently as
Bk
Tn
n?Bk
The sub-problem is of similar form to the original ERM problem Since the augmented
term is a simple quadratic function separable each coordinate given a solver for working
in sufficient-memory condition one can easily adapt it by modifying
tBk wt
where denotes the function with augmented terms and denotes the function without augmented terms The Block Minimization procedure is repeated until every sub-problem reaches
a tolerance in Then the proximal point method update is performed where
is the solution of for the latest dual iterate The resulting algorithm is summarized in Algorithm
Analysis
In this section we analyze the convergence rate of Algorithm to the optimum of First we
show that the proximal-point formulation has a dual problem with desired property for the
global convergence of Block-Coordinate Descent In particular since the dual of takes the form
minp
Tn
L?n
is the convex conjugate of
where
kw wt and since
is strongly
convex with parameter the convex conjugate is smooth with parameter
according to Theorem Therefore is in the composite form of a convex smooth function plus
a convex block-separable function This type of function has been widely studied in the literature
of Block-Coordinate Descent In particular one can show that a Block-Coordinate Descent
applied on has global convergence to optimum with a fast rate by the following theorem
Theorem BCD Convergence Let the sequence
be the iterates produced by Block
Coordinate Descent in the inner loop of Algorithm and be the number of blocks Denote
the optimal value of Then with probability
as the dual objective function of and F?opt
F?opt
for some constant if Ln is smooth or Ln is polyhedral function and is also
polyhedral or smooth Otherwise for any convex Ln we have
F?opt
for log
F?opt
cK
F?opt
for
log
for some constant
Note the above analysis appendix does not assume exact solution of each block subproblem
Instead it only assumes each block minimization step leads to a dual ascent amount proportional to
that produced by a single dual proximal gradient ascent step on the block of dual variables For
the outer loop of Primal Proximal-Point Dual Augmented Lagrangian iterates we show the
following convergence theorem
Theorem Proximal Point Convergence Let be objective of the regularized ERM problem
and maxv maxw kv wk be the radius of initial
level set The sequence wt
produced by the Proximal-Point update with has
Fopt for log
for some constant if both Ln and are strictly convex and smooth or polyhedral Otherwise for any convex we have
Fopt R2
The following theorem further shows that solving sub-problem inexactly with tolerance
suffices for convergence to overall precision where is the number of outer iterations required
Theorem Inexact Proximal Map Suppose for a given dual iterate wt each sub-problem
has
is solved inexactly the solution
prox?t wt
kw
Then let
be the sequence of iterates produced by inexact proximal updates and
as that generated by exact updates After iterations we have
wt
kw
wt
Note for Ln being strictly convex and smooth or polyhedral is of order
and thus it only requires O(K O(K log2 overall number of block minimization steps to achieve suboptimality Otherwise as long as Ln is smooth for any convex
regularizer is of order so it requires
total
number of block minimization steps
Practical Issues
Solving Sub-Problem Inexactly
While the analysis in Section assumes exact solution of subproblems in practice the Block
Minimization framework does not require solving subproblem exactly In our experiments
it suffices for the fast convergence of proximal-point update to solve subproblem for only a
single pass of all blocks of variables BK and limit the number of iterations the designated
solver spends on each subproblem to be no more than some parameter Tmax
Random Selection w/o Replacement
In Algorithm and the block to be optimized is chosen uniformly at random from
which eases the analysis for proving a better convergence rate However in practice to avoid
unbalanced update frequency among blocks we do random sampling without replacement for both
Algorithm and that is for every iterations we generate a random permutation of
block index and optimize block subproblems according to the order This
also eases the checking of inner-loop stopping condition
Storage of Dual Variables
Both the algorithms and need to store the dual variables Bk into memory and load/save them
from/to some secondary storage units which requires a time linear to p|Bk For some problems
such as multi-label classification with large number of labels or structured prediction with large
number of factors this can be very expensive In this situation one can instead maintain
has I/O and storage cost linear to which can be
n?Bk Bk directly Note
much smaller than p|Bk in a low-dimensional problem
Experiment
In this section we compare the proposed Dual Augmented Block Minimization framework Algorithm to the vanilla Dual Block Coordinate Descent algorithm and methods adopted from
Online and Distributed Learning The experiments are conducted on the problem of L1-regularized
L2-loss SVM and the Lasso L1-regularized Regression problem in the limited-memory
setting with data size times larger than the available memory For both problems we use stateof-the-art randomized coordinate descent method as the solver for solving sub-problems
and we set parameter of L1-regularizer for all experiments
Four public benchmark data sets are used webspam rcv1-binary for classification and year-pred
for regression which can be obtained from the LIBSVM data set collections For year-pred
and the features are generated from Random Fourier Features that approximate the
effect of Gaussian RBF kernel Table summarizes the data statistics The algorithms in comparison and their shorthands are listed below where all solvers are implemented in and run on
machine with Intel(R Xeon(R CPU. We constrained the process to use no more
than of memory required to store the whole data
OnlineMD Stochastic Mirror Descent method specially designed for L1-regularized problem proposed in with step size chosen from for best performance
Table Data Statistics Summary of data statistics when stored using sparse format The last two
columns specify memory consumption in of the whole data and that of a block when data is
split into partitions
Data
train
test
dimension
non-zeros
Memory Block
webspam
rcv1
year-pred
Figure Relative function value difference to the optimum and Testing RMSE Accuracy on
LASSO top and L1-regularized L2-SVM bottom RMSE best for year-pred for
Accuracy best for for webspam best for rcv1
year?pred?obj
year?rmse
ADMM
BC?ADMM
DA?BCD
D?BCD
onlineMD
ADMM
BC?ADMM
DA?BCD
D?BCD
onlineMD
ADMM
BC?ADMM
DA?BCD
D?BCD
onlineMD
ADMM
BC?ADMM
DA?BCD
D?BCD
onlineMD
obj
RMSE
rmse
objective
time
webspam?obj
time
time
ADMM
BC?ADMM
DA?BCD
D?BCD
onlineMD
rcv1?obj
webspam?error
ADMM
BC?ADMM
DA?BCD
D?BCD
onlineMD
time
ADMM
BC?ADMM
DA?BCD
D?BCD
onlineMD
rcv1?error
ADMM
BC?ADMM
DA?BCD
D?BCD
onlineMD
error
obj
obj
error
time
time
time
time
D-BCD2 Dual Block-Coordinate Descent method Algorithm
DA-BCD Dual-Augmented Block Minimization Algorithm
ADMM ADMM for limited-memory learning Algorithm in appendix-B
BC-ADMM Block-Coordinate ADMM that updates a randomly chosen block of dual variables at a time for limited-memory learning Algorithm in appendix-B
We use wall clock time that includes both I/O and computation as measure for training time in all
experiments In Figure three measures are plotted versus the training time Relative objective
function difference to the optimum Testing RMSE and Accuracy Figure shows the results where
as expected the dual Block Coordinate Descent D-BCD method without augmentation cannot improve the objective after certain number of iterations However with extremely simple modification
the Dual-Augmented Block Minimization DA-BCD algorithm becomes not only globally convergent but with a rate several times faster than other approaches Among all methods the convergence
of Online Mirror Descent SMIDAS is significantly slower which is expected since the online
Mirror Descent on a non-smooth non-strongly convex function converges at a rate qualitatively
slower than the linear convergence rate of DA-BCD and ADMM and Online method
does not utilize the available memory capacity and thus spends unbalanced time on I/O and computation For methods adopted from distributed optimization the experiment shows BC-ADMM
consistently but only slightly improves ADMM and both of them converge much slower than the
DA-BCD approach presumably due to the conservative updates on the dual variables
Acknowledgement We thank to the support of Telecommunication Lab Chunghwa Telecom
Ltd via AOARD via No. Ministry of Science and Technology
National Taiwan University and Intel Co. via
The objective value obtained from D-BCD fluctuates a lot in figures we plot the lowest values achieved by
D-BCD from the beginning to time

----------------------------------------------------------------

title: 4476-see-the-tree-through-the-lines-the-shazoo-algorithm.pdf

See the Tree Through the Lines
The Shazoo Algorithm
Nicol`o Cesa-Bianchi
DSI University of Milan Italy
nicolo.cesa-bianchi@unimi.it
Fabio Vitale
DSI University of Milan Italy
fabio.vitale@unimi.it
Giovanni Zappella
Dept of Mathematics Univ of Milan Italy
giovanni.zappella@unimi.it
Claudio Gentile
DICOM University of Insubria Italy
claudio.gentile@uninsubria.it
Abstract
Predicting the nodes of a given graph is a fascinating theoretical problem with applications in several domains Since graph sparsification via spanning trees retains
enough information while making the task much easier trees are an important
special case of this problem Although it is known how to predict the nodes of an
unweighted tree in a nearly optimal way in the weighted case a fully satisfactory
algorithm is not available yet We fill this hole and introduce an efficient node
predictor HAZOO which is nearly optimal on any weighted tree Moreover we
show that HAZOO can be viewed as a common nontrivial generalization of both
previous approaches for unweighted trees and weighted lines Experiments on
real-world datasets confirm that HAZOO performs well in that it fully exploits
the structure of the input tree and gets very close to and sometimes better than
less scalable energy minimization methods
Introduction
Predictive analysis of networked data is a fast-growing research area whose application domains
include document networks online social networks and biological networks In this work we view
networked data as weighted graphs and focus on the task of node classification in the transductive
setting when the unlabeled graph is available beforehand Standard transductive classification
methods such as label propagation work by optimizing a cost or energy function defined
on the graph which includes the training information as labels assigned to training nodes Although
these methods perform well in practice they are often computationally expensive and have performance guarantees that require statistical assumptions on the selection of the training nodes
A general approach to sidestep the above computational issues is to sparsify the graph to the largest
possible extent while retaining much of its spectral properties see Inspired
by this paper reduces the problem of node classification from graphs to trees by extracting
suitable spanning trees of the graph which can be done quickly in many cases The advantage
of performing this reduction is that node prediction is much easier on trees than on graphs This
fact has recently led to the design of very scalable algorithms with nearly optimal performance
guarantees in the online transductive model which comes with no statistical assumptions Yet the
current results in node classification on trees are not satisfactory The REE PT strategy of is
optimal to within constant factors but only on unweighted trees No equivalent optimality results
are available for general weighted trees To the best of our knowledge the only other comparable
result is WTA by which is optimal within log factors only on weighted lines In fact WTA can
still be applied to weighted trees by exploiting an idea contained in This is based on linearizing
the tree via a depth-first visit Since linearization loses most of the structural information of the tree
This work was supported in part by Google Inc. through a Google Research Award and by the PASCAL2
Network of Excellence under EC grant This publication only reflects the authors views
this approach yields suboptimal mistake bounds This theoretical drawback is also confirmed by
empirical performance throwing away the tree structure negatively affects the practical behavior of
the algorithm on real-world weighted graphs
The importance of weighted graphs as opposed to unweighted ones is suggested by many practical
scenarios where the nodes carry more information than just labels vectors of feature values A
natural way of leveraging this side information is to set the weight on the edge linking two nodes to
be some function of the similariy between the vectors associated with these nodes In this work we
bridge the gap between the weighted and unweighted cases by proposing a new prediction strategy
called HAZOO achieving a mistake bound that depends on the detailed structure of the weighted
tree We carry out the analysis using a notion of learning bias different from the one used in and
more appropriate for weighted graphs More precisely we measure the regularity of the unknown
node labeling via the weighted cutsize induced by the labeling on the tree Section for a precise
definition This replaces the unweighted cutsize that was used in the analysis of WTA. When the
weighted cutsize is used a cut edge violates this inductive bias in proportion to its weight This
modified bias does not prevent a fair comparison between the old algorithms and the new one
HAZOO specializes to REE PT in the unweighted case and to WTA when the input tree is a
weighted line By specializing HAZOO?s analysis to the unweighted case we recover REE PT?s
optimal mistake bound When the input tree is a weighted line we recover WTA?s mistake bound
expressed through the weighted cutsize instead of the unweighted one The effectiveness of HAZOO
on any tree is guaranteed by a corresponding lower bound Section
HAZOO can be viewed as a common nontrivial generalization of both REE PT and WTA. Obtaining this generalization while retaining and extending the optimality properties of the two algorithms
is far from being trivial from a conceptual and technical standpoint Since HAZOO works in the
online transductive model it can easily be applied to the more standard train/test batch transductive setting one simply runs the algorithm on an arbitrary permutation of the training nodes and
obtains a predictive model for all test nodes However the implementation might take advantage
of knowing the set of training nodes beforehand For this reason we present two implementations
of HAZOO one for the online and one for the batch setting Both implementations result in fast
algorithms In particular the batch one is linear in This is achieved by a fast algorithm for
weighted cut minimization on trees a procedure which lies at the heart of HAZOO
Finally we test HAZOO against WTA label propagation and other competitors on real-world
weighted graphs In almost all cases as expected we report improvements over WTA due to the
better sensitivity to the graph structure In some cases we see that HAZOO even outperforms standard label propagation methods Recall that label propagation has a running time per prediction
which is proportional to where is the graph edge set On the contrary HAZOO can typically
be run in constant amortized time per prediction by using Wilson?s algorithm for sampling random
spanning trees By disregarding edge weights in the initial sampling phase this algorithm is
able to draw a random unweighted spanning tree in time proportional to on most graphs Our
experiments reveal that using the edge weights only in the subsequent prediction phase causes in
practice only a minor performance degradation
Preliminaries and basic notation
Let be an undirected and weighted tree with nodes positive edge weights
Wi,j for and Wi,j for
E. A binary labeling of is any assignment
yn of binary labels to its nodes We use to denote the resulting
labeled weighted tree The online learning protocol for predicting is defined as follows The
learner is given while is kept hidden The nodes of are presented to the learner one by one
according to an unknown and arbitrary permutation i1 in of At each time step
node it is presented and the learner must issue a prediction ybit for the label yit Then
yit is revealed and the learner knows whether a mistake occurred The learner?s goal is to minimize
the total number of prediction mistakes
Following previous works we measure the regularity of a labeling of in terms of
edges where a edge for is any such that yP
yj The overall amount of
irregularity in a labeled tree is the weighted cutsize Wi,j where
is the subset of edges in the tree We use the weighted cutsize as our learning bias that is we
want to design algorithms whose predictive performance scales with Unlike the edge count
which is a good measure of regularity for unweighted graphs the weighted cutsize takes
the edge weight Wi,j into account when measuring the irregularity of a edge In the sequel
when we measure the distance between any pair of nodes
and on the input tree we always use
the resistance distance metric that is W1r,s where is the unique
path connecting to
A lower bound for weighted trees
In this section we show that the weighted cutsize can be used as a lower bound on the number of
online mistakes made by any algorithm on any tree In order to do so and unlike previous papers
on this specific subject see we need to introduce a more refined notion of adversarial
budget Given let be the maximum
number of edges of such that
the
sum of their weights does not exceed max wi,j
We have the following simple lower bound all proofs are omitted from this extended abstract
Theorem For any weighted tree there exists a randomized label assignment to
such that any algorithm can be forced to make at least online mistakes in expectation
while
Specializing Theorem to trees gives the lower bound under the constraint
The main difference between the two bounds is the measure of label regularity being used Whereas
Theorem uses which depends on the weights Theorem uses the weight-independent
quantity This dependence of the lower bound on the edge weights is consistent with our learning
bias stating that a heavy edge violates the bias more than a light one Since is nondecreasing
the lower bound implies a number of mistakes of at least Note that for any
labeled tree Hence whereas a constraint on implies forcing at least mistakes a
constraint on allows the adversary to force a potentially larger number of mistakes
In the next section we describe an algorithm whose mistake bound nearly matches the above lower
bound on any weighted tree when using as the measure of label regularity
The Shazoo algorithm
In this section we introduce the HAZOO algorithm and relate it to previously proposed methods
for online prediction on unweighted trees REE PT from and weighted line graphs WTA from
In fact HAZOO is optimal on any weighted tree and reduces to REE PT on unweighted trees
and to WTA on weighted line graphs Since REE PT and WTA are optimal on any unweighted tree
and any weighted line graph respectively HAZOO necessarily contains elements of both of these
algorithms
In order to understand our algorithm we now define some relevant structures of the input tree See
Figure left for an example These structures evolve over time according to the set of observed
labels First we call revealed a node whose label has already been observed by the online learner
otherwise a node is unrevealed A fork is any unrevealed node connected to at least three different
revealed nodes by edge-disjoint paths A hinge node is either a revealed node or a fork A hinge
tree is any component of the forest obtained by removing from all edges incident to hinge nodes
hence any fork or labeled node forms a 1-node hinge tree When a hinge tree contains only one
hinge node a connection node for is the node contained in H. In all other cases we call a
connection node for any node outside which is adjacent to a node in H. A connection fork is
a connection node which is also a fork Finally a hinge line is any path connecting two hinge nodes
such that no internal node is a hinge node
Given an unrevealed node and a label value the cut function cut(i is the value
of the minimum weighted cutsize of over all labelings consistent with the labels
seen so far and such that Define cut(i cut(i if is unrevealed and
otherwise The algorithm?s pseudocode is given in Algorithm At time in order
to predict the label yit of node it HAZOO calculates for all connection nodes of H(it
where H(it is the hinge tree containing it Then the algorithm predicts yit using the label of the
connection node of H(it which is closest to it and such that recall from Section
that all distances/lengths are measured using the resistance metric Ties are broken arbitrarily If
for all connection nodes in H(it then HAZOO predicts a default value in the
Figure Left An input tree Revealed nodes are dark grey forks are doubly circled and hinge
lines have thick black edges The hinge trees not containing hinge nodes the ones that are not
singletons are enclosed by dotted lines The dotted arrows point to the connection node(s of such
hinge trees Middle The predictions of HAZOO on the nodes of a hinge tree The numbers on the
edges denote edge weights At a given time HAZOO uses the value of on the two hinge nodes
the doubly circled ones which are also forks in this case and is required to issue a prediction on
node it the black node in this figure Since it is between a positive hinge node and a negative
hinge node HAZOO goes with the one which is closer in resistance distance hence predicting
ybit Right A simple example where the mincut prediction strategy does not work well in the
weighted case In this example mincut mispredicts all labels yet and the ratio of to the
total weight of all edges is about The labels to be predicted are presented according to the
numbers on the left of each node Edge weights are also displayed where a is a very small constant
pseudocode If it is a fork which is also a hinge node then H(it it In this case it is
a connection node of H(it and obviously
the one closest to itself Hence in this case HAZOO
predicts simply by ybit sgn See Figure middle for an example On unweighted
Algorithm HAZOO
for
Let be the set of the connection nodes of H(it for which
if H(it
Let be the node ofC H(it closest to it
Set ybit sgn
else Set ybit default value
trees computing for a connection node reduces to the Fork Label Estimation Procedure in
Lemma On the other hand predicting with the label of the connection node closest to it
in resistance distance is reminiscent of the nearest-neighbor prediction of WTA on weighted line
graphs In fact as in WTA this enables to take advantage of labelings whose edges are light
weighted An important limitation of WTA is that this algorithm linearizes the input tree On the
one hand this greatly simplifies the analysis of nearest-neighbor prediction on the other hand this
prevents exploiting the structure of thereby causing logaritmic slacks in the upper bound of WTA.
The REE PT algorithm instead performs better when the unweighted input tree is very different
from a line graph more precisely when the input tree cannot be decomposed into long edge-disjoint
paths a star graph Indeed REE PT?s upper bound does not suffer from logaritmic slacks
and is tight up to constant factors on any unweighted tree Similar to REE PT HAZOO does
not linearize the input tree and extends to the weighted case REE PT?s superior performance also
confirmed by the experimental comparison reported in Section
In Figure right we show an example that highlights the importance of using the function to
compute the fork labels Since predicts a fork it with the label that minimizes the weighted cutsize
of consistent with the revealed labels one may wonder whether computing through mincut
based on the number of edges rather than their weighted sum could be an effective prediction
strategy Figure right illustrates an example of a simple tree where such a mispredicts the
labels of all nodes when both and are small
Remark We would like to stress that HAZOO can also be used to predict the nodes of an arbitrary graph by first drawing a random spanning tree of the graph and then predicting optimally
on see The resulting mistake bound is simply the expected value of HAZOO?s
mistake bound over the random draw of By using a fast spanning tree sampler the involved
computational overhead amounts to constant amortized time per node prediction on most graphs
Remark In certain real-world input graphs the presence of an edge linking two nodes may also
carry information about the extent to which the two nodes are dissimilar rather than similar This
information can be encoded by the sign of the weight and the resulting network is called a signed
graph The regularity measure is naturally extended to signed graphs by counting the weight of
frustrated edges where is frustrated if yj sgn(wi,j Many of the existing
algorithms for node classification can in principle be run on signed graphs
However the computational cost may not always be preserved For example mincut is in general
NP-hard when the graph is signed Since our algorithm sparsifies the graph using trees it can
be run efficiently even in the signed case We just need to re-define the function as
fcut(i fcut(i where fcut is the minimum total weight of frustrated edges consistent
with the labels seen so far The argument contained in Section for the positive edge weights see
therein allows us to show that also this version of can be computed efficiently The
prediction rule has to be re-defined as well We count the parity of thenumber of negative-weighted
edges along the path connecting it to the closest node H(it ybit sgn
Remark In the authors note that REE PT approximates a version space Halving algorithm on the set of tree labelings Interestingly HAZOO is also an approximation to a more general
Halving algorithm for weighted trees This generalized Halving gives a weight to each labeling
consistent with the labels seen so far and with the sign of for each fork These weighted
labelings which depend on the weights of the edges generated by each labeling are used for computing the predictions One can show details omitted due to space limitations that this generalized
Halving algorithm has a mistake bound within a constant factor of HAZOO?s
Mistake bound analysis and implementation
We now show that HAZOO is nearly optimal on every weighted tree We obtain an upper bound
in terms of and the structure of nearly matching the lower bound of Theorem We now
give some auxiliary notation that is strictly needed for stating the mistake bound
Given a labeled tree a cluster is any maximal subtree whose nodes have the same label An
in-cluster line graph is anyP
line graph that is entirely contained in a single cluster Finally given a
W1i,j the resistance distance between its terminal nodes
line graph we set RL
Theorem For any labeled and weighted tree there exists a set LT of edgedisjoint in-cluster line graphs such that the number of mistakes made by HAZOO is at most of the
order of
min log RL
L?LT
The above mistake bound depends on the tree structure through LT The sum contains
terms each one being at most logarithmic
in the scale-free products RL
The bound is governed
occurring in the lower bound of Theorem However Theorem
by the same key quantity
also shows that HAZOO can take advantage of trees that cannot be covered by long line graphs For
example if the input tree is a weighted
long
line graph then it is likely to contain
in-cluster lines
Hence the factor multiplying may be of the order of log RL
If instead has
constant diameter a star graph then the in-cluster lines can only contain a constant number of
nodes and the number of mistakes can never exceed This is a log factor improvement
over WTA which by its very nature cannot exploit the structure of the tree it operates
As for the implementation we start by describing a method for calculating cut(v for any unlabeled node and label value Let be the maximal subtree of rooted at such that no internal
node is revealed For any node of let Tiv be the subtree of rooted at Let vi be the
minimum weighted cutsize of Tiv consistent with the revealed nodes and such that Since
One might wonder whether an arbitrarily large gap between upper Theorem and lower Theorem
bounds exists due to the extra factors depending on RL
One way to get around this is to follow the
analysis of WTA in Specifically we can adapt here the more general analysis from that paper Lemma
therein that allows us to drop for any integer the resistance contribution of arbitrary non edges of
the line graphs in LT thereby reducing RL
for any containing any of these edges at the cost of increasing
the mistake bound by K. The details will be given in the full version of this paper
cut(v cut(v vv vv our goal is to compute vv It is easy to
see by induction that the quantity vi can be recursively defined as follows where Civ is the set
of all children of in and Yj yj if yj is revealed and Yj otherwise:2
min
I
if is an internal node of
i,j
Yj
vi
j?Ci
otherwise
Now vv can be computed through a simple depth-first visit of In all backtracking steps of
this visit the algorithm uses to compute vi for each node the values vj for all children
of being calculated during the previous backtracking steps The total running time is therefore
linear in the number of nodes of
Next we describe the basic implementation of HAZOO for the on-line setting A batch learning
implementation will be given at the end of this section The online implementation is made up of
three steps
Find the hinge nodes of subtree it Recall that a hinge-node is either a fork or a revealed
node Observe that a fork is incident to at least three nodes lying on different hinge lines Hence in
this step we perform a depth-first visit of it marking each node lying on a hinge line In order to
accomplish this task it suffices to single out all forks marking each labeled node and recursively
each parent of a marked node of it At the end of this process we are able to single out the forks
by counting the number of edges of each marked node such that has been marked too The
remaining hinge nodes are the leaves of it whose labels have currently been revealed
Compute for all connection forks of H(it From the previous step we can easily
find the connection node(s of H(it Then we simply exploit the above-described technique for
computing the cut function obtaining for all connection forks of H(it
Propagate the labels of the nodes of C(H(it only if it is not a fork We perform a visit of
H(it starting from every node C(H(it During these visits we mark each node of H(it
with the label of computed in the previous step together with the length of which is what
we need for predicting any label of H(it at the current time step
The overall running time is dominated
by the first step and the calculation of Hence the worst
case running time is proportional to it This quantity can be quadratic in though
this is rarely encountered in practice if the node presentation order is not adversarial For example
it is easy to show that in a line graph if the node presentation order is random then the total time is
of the order of log For a star graph the total time complexity is always linear in even
on adversarial orders
In many real-world scenarios one is interested in the more standard problem of predicting the labels
of a given subset of test nodes based on the available labels of another subset of training nodes
Building on the above on-line implementation we now derive an implementation of HAZOO for
this train/test batch learning setting We first show that computing and for
all unlabeled nodes in takes time This allows us to compute for all forks
in time and then use the first and the third steps of the on-line implementation Overall we
show that predicting all labels in the test set takes time
Consider tree as rooted at Given any unlabeled node we perform a visit of starting at
During the backtracking steps of this visit we use to calculate for each node in
and label Observe now that for any pair of adjacent unlabeled nodes and any
label once we have obtained ii and we can compute ji in
constant time as ji ii miny0 I wi,j In fact all children
of in are descendants of while the children of in but are descendants of in
HAZOO computes ii we can compute in constant time ji for all child nodes of in
and use this value for computing jj Generalizing this argument it is easy to see that in the next
phase we can compute kk in constant time for all nodes of such that for all ancestors of
and all the values of uu have previously been computed
The recursive computations contained in this section are reminiscent of the sum-product algorithm
The time for computing ss for all nodes of and any label is therefore linear in the time
of performing a breadth-first depth-first visit of linear in the number of nodes of
Since each labeled node with degree is part of at most trees for some we have that the total
number of nodes of all distinct edge-disjoint trees across is linear in
Finally we need to propagate the connection node labels of each hinge tree as in the third step of
the online implementation Since also this last step takes linear time we conclude that the total time
for predicting all labels is linear in
Experiments
We tested our algorithm on a number of real-world weighted graphs from different domains character recognition text categorization bioinformatics Web spam detection against the following
baselines
Online Majority Vote This is an intuitive and fast algorithm for sequentially predicting the
node labels is via a weighted majorityP
vote over the labels of the adjacent nodes seen so far Namely
OMV predicts yit through the sign of
yis wis it where ranges over such that is it E.
Both the total time and space required by OMV are
Label Propagation AB ROP AB ROP is a batch transductive learning method computed by solving a system of linear equations which requires total time of the order of This
relatively high computational cost should be taken into account when comparing AB ROP to faster
online algorithms Recall that OMV can be viewed as a fast online approximation to AB ROP.
Weighted Tree Algorithm As explained in the introductory section WTA can be viewed
as a special case of HAZOO When the input graph is not a line WTA turns it into a line by first
extracting a spanning tree of the graph and then linearizing it The implementation described in
runs in constant amortized time per prediction whenever the spanning tree sampler runs in time
The Graph Perceptron algorithm is another readily available baseline This algorithm has been
excluded from our comparison because it does not seem to be very competitive in terms of performance see and is also computationally expensive
In our experiments we combined HAZOO and WTA with spanning trees generated in different ways
note that OMV and AB ROP do not need to extract spanning trees from the input graph
Random Spanning Tree Following Ch. of we draw a weighted spanning tree with
probability proportional to the product of its edge weights We also tested our algorithms combined
with random spanning trees generated uniformly at random ignoring the edge weights the
weights were only used to compute predictions on the randomly generated tree we call these
spanning trees NWRST no-weight RST On most graphs this procedure can be run in time linear
in the number of nodes Hence the combinations HAZOO+NWRST and WTA+NWRST run in
time on most graphs
Minimum Spanning Tree This is just the minimal weight spanning tree where the weight
of a spanning tree is the sum of its edge weights This is the tree that best approximates the original
graph trace norm distance of the corresponding Laplacian matrices
Following we also ran HAZOO and WTA using committees of spanning trees and then
aggregating predictions via a majority vote The resulting algorithms are denoted by k*S HAZOO
and k*WTA where is the number of spanning trees in the aggregation We used either
or depending on the dataset size
For our experiments we used five datasets RCV1 USPS KROGAN COMBINED and WEBSPAM WEBSPAM is a big dataset nodes and edges of inter-host links created
for the Web Spam Challenge KROGAN nodes and edges and COMBINED nodes and edges are high-throughput protein-protein interaction networks of
budding yeast taken from see for a more complete description Finally USPS and RCV1
are graphs obtained from the USPS handwritten characters dataset all ten categories and the first
documents in chronological order of Reuters Corpus Vol. the four most frequent categories respectively In both cases we used Euclidean 10-Nearest Neighbor to create edges each
We do not compare our results to those obtained within the challenge since we are only exploiting the
graph weighted topology here disregarding content features
weight wi,j being equal to e?kxi We set
squared distance between and its nearest neighbours
where is the average
Following previous experimental settings we associate binary classification tasks with the five
datasets/graphs via a standard one-vs-all reduction Each error rate is obtained by averaging over ten
randomly chosen training sets and ten different trees in the case of RST and NWRST WEBSPAM
is natively a binary classification problem and we used the same train/test split provided with the
dataset training nodes and test nodes the remaining nodes being unlabeled
In the below table we show the macro-averaged classification error rates percentages achieved by
the various algorithms on the first four datasets mentioned in the main text For each dataset we
trained ten times over a random subset of and of the total number of nodes and tested
on the remaining ones In boldface are the lowest error rates on each column excluding AB ROP
which is used as a yardstick comparison Standard deviations averaged over the binary problems
are small most of the times less than
Datasets
Predictors
HAZOO+RST
HAZOO+NWRST
HAZOO+MST
WTA RST
WTA NWRST
WTA MST
HAZOO+RST
HAZOO+NWRST
7*WTA+RST
7*WTA+NWRST
HAZOO+RST
HAZOO+NWRST
11*WTA+RST
11*WTA+NWRST
OMV
AB ROP
USPS
RCV1
KROGAN
COMBINED
Next we extract from the above table a specific comparison among HAZOO WTA and AB ROP.
HAZOO and WTA use a single minimum spanning tree the best performing tree type for both
algorithms Note that HAZOO consistently outperforms WTA.
We then report the results on WEBSPAM HAZOO and WTA use only non-weighted random spanning trees NWRST to optimize scalability Since this dataset is extremely unbalanced positive
labels we use the average test set F-measure instead of the error rate
HAZOO
WTA
OMV
AB ROP
3*WTA
HAZOO
7*WTA
HAZOO
Our empirical results can be briefly summarized as follows
Without using committees HAZOO outperforms WTA on all datasets irrespective to the type
of spanning tree being used With committees HAZOO works better than WTA almost always
although the gap between the two reduces
The predictive performance of HAZOO+MST is comparable to and sometimes better than that
of AB ROP though the latter algorithm is slower
k*S HAZOO with on WEBSPAM seems to be especially effective outperforming AB ROP with a small training set size
NWRST does not offer the same theoretical guarantees as RST but it is extremely fast to generate
linear in on most graphs and in our experiments is only slightly inferior to RST.

----------------------------------------------------------------

title: 5983-estimating-jaccard-index-with-missing-observations-a-matrix-calibration-approach.pdf

Estimating Jaccard Index with Missing Observations
A Matrix Calibration Approach
Wenye Li
Macao Polytechnic Institute
Macao SAR China
wyli@ipm.edu.mo
Abstract
The Jaccard index is a standard statistics for comparing the pairwise similarity between data samples This paper investigates the problem of estimating a Jaccard
index matrix when there are missing observations in data samples Starting from
a Jaccard index matrix approximated from the incomplete data our method calibrates the matrix to meet the requirement of positive semi-definiteness and other
constraints through a simple alternating projection algorithm Compared with
conventional approaches that estimate the similarity matrix based on the imputed
data our method has a strong advantage in that the calibrated matrix is guaranteed to be closer to the unknown ground truth in the Frobenius norm than the
un-calibrated matrix except in special cases they are identical We carried out a
series of empirical experiments and the results confirmed our theoretical justification The evaluation also reported significantly improved results in real learning
tasks on benchmark datasets
Introduction
A critical task in data analysis is to determine how similar two data samples are The applications
arise in many science and engineering disciplines For example in statistical and computing sciences similarity analysis lays a foundation for cluster analysis pattern classification image analysis
and recommender systems
A variety of similarity models have been established for different types of data When data samples
can be represented as algebraic vectors popular choices include cosine similarity model linear
kernel model and so on When each vector element takes a value of zero or one the
Jaccard index model is routinely applied which measures the similarity by the ratio of the number
of unique elements common to two samples against the total number of unique elements in either of
them
Despite the wide applications the Jaccard index model faces a non-trivial challenge when data
samples are not fully observed As a treatment imputation approaches may be applied which
replace the missing observations with substituted values and then calculate the Jaccard index based
on the imputed data Unfortunately with a large portion of missing observations imputing data
samples often becomes un-reliable or even infeasible as evidenced in our evaluation
Instead of trying to fill in the missing values this paper investigates a completely different approach
based on matrix calibration Starting from an approximate Jaccard index matrix that is estimated
from incomplete samples the proposed method calibrates the matrix to meet the requirement of
positive semi-definiteness and other constraints The calibration procedure is carried out with a
simple yet flexible alternating projection algorithm
The proposed method has a strong theoretical advantage The calibrated matrix is guaranteed to be
better than or at least identical to special cases the un-calibrated matrix in terms of a shorter
Frobenius distance to the true Jaccard index matrix which was verified empirically as well Besides our evaluation of the method also reported improved results in learning applications and the
improvement was especially significant with a high portion of missing values
A note on notation Throughout the discussion a data sample Ai is treated as a set of
features Let fd be the set of all possible features Without causing ambiguity Ai
also represents a binary-valued vector If the j-th element of vector Ai is one it means
fj Ai feature fj belongs to sample Ai if the element is zero fj Ai if the element is marked
as missing it remains unknown whether feature fj belongs to sample Ai or not
Background
The Jaccard index
The Jaccard index is a commonly used statistical indicator for measuring the pairwise similarity
For two nonempty and finite sets Ai and Aj it is defined to be the ratio of the number of
elements in their intersection against the number of elements in their union
Jij
Ai Aj
Ai Aj
where denotes the cardinality of a set
The Jaccard index has a value of when the two sets have no elements in common when they have
exactly the same elements and strictly between and otherwise The two sets are more similar
have more common elements when the value gets closer to
For
sets A1 An the Jaccard index matrix is defined as an matrix
Jij The matrix is symmetric and all diagonal elements of the matrix are
Handling missing observations
When data samples are fully observed the accurate Jaccard index can be obtained trivially by enumerating the intersection and the union between each pair of samples if both the number of samples
and the number of features are small For samples with a large number of features the index can
often be approximated by MinHash and related methods which avoid the explicit counting
of the intersection and the union of the two sets
When data samples are not fully observed however obtaining the accurate Jaccard index generally
becomes infeasible One na??ve approximation is to ignore the features with missing values Only
those features that have no missing values in all samples are used to calculate the Jaccard index
Obviously for a large dataset with missing-at-random features it is very likely that this method will
throw away all features and therefore does not work at all
The mainstream work tries to replace the missing observations with substituted values and then
calculates the Jaccard index based on the imputed data Several simple approaches including zero
median and k-nearest neighbors kNN methods are popularly used A missing element is set to
zero often implying the corresponding feature does not exist in a sample It can also be set to the
median value the mean value of the feature over all samples or sometimes over a number of
nearest neighboring instances
A more systematical imputation framework is based on the classical expectation maximization
algorithm which generalizes maximum likelihood estimation to the case of incomplete data
Assuming the existence of un-observed latent variables the algorithm alternates between the expectation step and the maximization step and finds maximum likelihood or maximum a posterior
estimates of the un-observed variables In practice the imputation is often carried out through iterating between learning a mixture of clusters of the filled data and re-filling missing values using
cluster means weighted by the posterior probability that a cluster generates the samples
Solution
Our work investigates the Jaccard index matrix estimation problem for incomplete data Instead
of throwing away the un-observed features or imputing the missing values a completely different
solution based on matrix calibration is designed
Initial approximation
For a sample Ai denote by Oi the set of features that are known to be in Ai and denote by Oi the
set of features that are known to be not in Ai Let Oi Oi Oi If Oi Ai is fully observed
without missing values otherwise Ai is not fully observed with missing values The complement
of Oi with respect to denoted by Oi gives Ai unknown features and missing values
For two samples Ai and Aj with missing values we approximate their Jaccard index by
Oj Oi
Jij
Oj Oi
Oj Oi
Here we assume that each sample has at least one observed feature It is obvious that Jij
is equal to
the ground truth Jij if the samples are fully observed
There exists an interval that the true value Jij
lies in where
if
otherwise
Oi Oj
and
if
Oi Oj
Oi Oj
otherwise
The lower bound is obtained from the extreme case of setting the missing values in a way that the
two sets have the fewest features in their intersection while having the most features in their union
On the contrary the upper bound is obtained from the other extreme When the samples are fully
observed the interval shrinks to a single point Jij
Matrix calibration
Denote by Jij
the true Jaccard index matrix for a set of data samples An
we have
Theorem For a given set of data samples its Jaccard index matrix is positive semi-definite
For data samples with missing values the matrix Jij
often loses positive semiij=1
definiteness Nevertheless it can be calibrated to ensure the property by seeking an matrix
Jij to minimize
L0
subject to the constraints
and Jij
where requires to be positive semi-definite and k?kF denotes the Frobenius norm of a
matrix and kJkF ij Jij
Let Mn be the set of symmetric matrices The feasible region defined by the constraints
denoted by is a nonempty closed and convex subset of Mn Following standard results in optimization theory the problem of minimizing L0 is convex Denote by PR the
projection onto R. Its unique solution is given by the projection of J0 onto JR
PR
For JR
we have
The equality holds iff
Theorem
JR
Proof Define an inner product on Mn that induces the Frobenius norm
hX trace for Mn
Then
JR
JR
JR
JR
JR
JR
JR
JR
JR
JR
The second holds due to the Kolmogrov?s criterion which states that the projection of onto
JR
is unique and characterized by
for all R.
JR
and JR
JR
JR
JR
and JR
The equality holds iff
JR
This key observation shows that projecting onto the feasible region will produce an improved
estimate towards although this ground truth matrix remains unknown to us
Projection onto subsets
Based on the results in Section we are to seek a minimizer to L0 to improve the estimate
Define two nonempty closed and convex subsets of Mn
Mn
and
Mn Xij
Obviously Now our minimization problem becomes finding the projection of onto
the intersection of two sets and with respect to the Frobenius norm This can be done by
studying the projection onto the two sets individually Denote by PS the projection onto and PT
the projection onto For projection onto a straightforward result based on the Kolmogrov?s
criterion is
Theorem For a given matrix Mn its projection onto XT PT is given by
Xij if Xij
XT ij if Xij
if Xij
For projection onto a well known result is the following
Theorem For Mn and its singular value decomposition where
diag the projection of onto is given by XS PS where
diag and
if
otherwise
The matrix XS PS gives the positive semi-definite matrix that most closely approximates
with respect to the Frobenius norm
Dykstra?s algorithm
To study the orthogonal projection onto the intersection of subspaces a classical result is von Neumann?s alternating projection algorithm Let be a Hilbert space with two closed subspaces C1
and C2 The orthogonal projection onto the intersection C1 C2 can be obtained by the product of
the two projections PC1 PC2 when the two projections commute PC2 PC2 PC1 When they
do not commute the work shows that for each the projection of onto the intersection
can be obtained by the limit
point of a sequence
of projections onto each subspace respectively
limk PC1 PC1 The algorithm generalizes to any finite number of subspaces and projections onto them
Unfortunately different from the application in in our problem both and are not subspaces
but subsets and von Neumann?s convergence result does not apply The limit point of the generated
sequence may converge to non-optimal points
To handle the difficulty Dykstra extended von Neumann?s
work and proposed an algorithm that
Tr
works with subsets Consider the case of Ci where is nonempty and each Ci is
a closed and convex subset in H. Assume that for any obtaining PC is hard while
obtaining each
Dykstra?s algorithm produces two sequences
from
kP Ci is easy Starting
the iterates and the increments Ii The two sequences are generated by
xk0
xrk?1
xki
PCi xki?1 Iik?1
Iik
xki xki?1 Iik?1
where and The initial values are given by x0r Ii0
The sequence of xki converges to the optimal solution with a theoretical guarantee
Theorem Let C1 Cr be closed and convex subsets of a Hilbert space such that
Ck For any and any the sequence xki converges strongly to
x0C PC
xki x0C
as
The convergent rate of Dykstra?s algorithm for polyhedral sets is linear which coincides with
the convergence rate of von Neumann?s alternating projection method
An iterative method
Based on the discussion in Section we have a simple approach shown in Algorithm that finds
the projection of an initial matrix onto the nonempty set Here the projections onto
and are given by the two theorems in Section The algorithm stops when falls into the
feasible region or when a maximal number of iterations is achieved For practical implementation
a more robust stopping criterion can be adopted
Related work
It is a known study in mathematical optimization field to find a positive semi-definite matrix that
is closest to a given matrix A number of methods have been proposed recently The idea of alternating projection method was firstly applied in a financial application The problem can also
be phrased as a semi-definite programming SDP model and be solved via the interior-point
method In the work of and the quasi-Newton method and the projected gradient method
to the Lagrangian dual of the original problem were applied which reported faster results than the
SDP formulation An even faster Newton?s method was developed in by investigating the dual
problem which is unconstrained with a twice continuously differentiable objective function and has
a quadratically convergent solution
Algorithm Projection onto
Require Initial matrix
JT0
IS0
IT0
while NOT CONVERGENT
do
JSk+1 PS JTk ISk
ISk+1 JSk+1 JTk ISk
JTk+1 PT JSk+1 ITk
ITk+1 JTk+1 JSk+1 ITk
end while
return JTk
Evaluation
To evaluate the performance of the proposed method four benchmark datasets were used in our
experiments
MNIST a grayscale image database of handwritten digits to After binarization
each image is represented as a 784-dimensional vector
USPS another grayscale image database of handwritten digits After binarization each
image is represented as a 256-dimensional vector
PROTEIN a bioinformatics database with three classes of instances Each instance is represented as a sparse 357-dimensional vector
WEBSPAM a dataset with both spam and non-spam web pages Each page is represented
as a vector The data are highly sparse On average one vector has about non-zero
values out of more than million features
Our experiments have two objectives One is to verify the effectiveness of the proposed method in
estimating the Jaccard index matrix by measuring the derivation of the calibrated matrix from the
ground truth in Frobenius norm The other is to evaluate the performance of the calibrated matrix in
general learning applications The comparison is made against the popular imputation approaches
listed in Section including the zero kNN and EM approaches As the median approach gave
very similar performance as the zero approach its results were not reported separately
Jaccard index matrix estimation
The experiment was carried out under various settings For each dataset we experimented with
and samples respectively For each sample different portions from to
of feature values were marked as missing which was assumed to be missing at random and all
features had the same probability of being marked
As mentioned in Section for the proposed calibration approach an initial Jaccard index matrix
was firstly built based on the incomplete data Then the matrix was calibrated to meet the positive
semi-definite requirement and the lower and upper bounds requirement While for the imputation
approaches the Jaccard index matrix was calculated directly from the imputed data
Note that for the kNN approach we iterated different from to and the best result was collected
which actually overestimated its performance Under some settings the results of the EM approach
were not available due to its prohibitive computational requirement to our platform
The results are presented through the comparison of mean square deviations from the ground truth
of the Jaccard index matrix For an estimated matrix its mean square deviation from
ftp://ftp.cs.toronto.edu/pub/zoubin/old/EMcode.tar.Z
FBO4RVBSF%FWJBUJPO Samples
ZERO/MEDIAN
kNN
EM
NO_CALIBRATION
CALIBRATION
ZERO/MEDIAN
kNN
EM
NO_CALIBRATION
CALIBRATION
Mean Square Deviation log?scale
0HDQ6TXDUH'HYLDWLRQ log?scale
Mean Square Deviation log?scale
FBO4RVBSF%FWJBUJPO Samples
Mean Square Deviation Samples
ZERO/MEDIAN
kNN
EM
NO_CALIBRATION
CALIBRATION
ZERO/MEDIAN
kNN
NO_CALIBRATION
CALIBRATION
0HDQ6TXDUH'HYLDWLRQ log?scale
Mean Square Deviation Samples
Ratio of Observed Features
MNIST
Ratio of Observed Features
Ratio of Observed Features
WEBSPAM
FBO4RVBSF%FWJBUJPO Samples
ZERO/MEDIAN
kNN
NO_CALIBRATION
CALIBRATION
Mean Square Deviation log?scale
Mean Square Deviation Samples
ZERO/MEDIAN
kNN
EM
NO_CALIBRATION
CALIBRATION
PROTEIN
FBO4RVBSF%FWJBUJPO Samples
ZERO/MEDIAN
kNN
EM
NO_CALIBRATION
CALIBRATION
0HDQ6TXDUH'HYLDWLRQ log?scale
Mean Square Deviation log?scale
Ratio of Observed Features
USPS
Mean Square Deviation Samples
ZERO/MEDIAN
kNN
NO_CALIBRATION
CALIBRATION
0HDQ6TXDUH'HYLDWLRQ log?scale
Ratio of Observed Features
MNIST
Ratio of Observed Features
USPS
Ratio of Observed Features
PROTEIN
Ratio of Observed Features
WEBSPAM
Figure Mean square deviations from the ground truth on benchmark datasets by different methods
Horizontal percentages of observed values from to Vertical mean square deviations
in log-scale samples samples For better visualization effect of the
results shown in color the reader is referred to the soft copy of this paper
is defined as the square Frobenius distance between the two matrices divided by the number of
Pn
Jij
elements nij
In addition to the comparison with the popular approaches the mean
square deviation between the un-calibrated matrix and shown as NO CALIBRATION is
also reported as a baseline
Figure shows the results It can be seen that the calibrated matrices reported the smallest derivation
from the ground truth in nearly all experiments The improvement is especially significant when the
ratio of observed features is low the missing ratio is high It is guaranteed to be no worse than the
un-calibrated matrix As evidenced in the results for all the imputation approaches there is no such
a guarantee
Supervised learning
Knowing the improved results in reducing the deviation from the ground truth matrix we would like
to further investigate whether this improvement indeed benefits practical applications specifically
in supervised learning
We applied the calibrated results in nearest neighbor classification tasks Given a training set of
labeled samples we tried to predict the labels of the samples in the testing set For each testing
sample its label was determined by the label of the sample in the training set that had the largest
Jaccard index value with it
Similarly the experiment was carried out with samples and different portions of missing values from to respectively In each run of the samples were randomly chosen as
the training set and the remaining were used as the testing set The mean and standard deviation
of the classification errors in runs were reported As a

----------------------------------------------------------------

title: 5216-beyond-pairwise-provably-fast-algorithms-for-approximate-k-way-similarity-search.pdf

Beyond Pairwise Provably Fast Algorithms for
Approximate k-Way Similarity Search
Anshumali Shrivastava
Department of Computer Science
Computing and Information Science
Cornell University
Ithaca NY USA
anshu@cs.cornell.edu
Ping Li
Department of Statistics Biostatistics
Department of Computer Science
Rutgers University
Piscataway NJ USA
pingli@stat.rutgers.edu
Abstract
We go beyond the notion of pairwise similarity and look into search problems
with k-way similarity functions In this paper we focus on problems related to
3-way Jaccard similarity R3way
S1 S2 S3 where is a
size collection of sets binary vectors We show that approximate R3way
similarity search problems admit fast algorithms with provable guarantees analogous to the pairwise case Our analysis and speedup guarantees naturally extend
to k-way resemblance In the process we extend traditional framework of locality
sensitive hashing LSH to handle higher-order similarities which could be of independent theoretical interest The applicability of R3way search is shown on the
Google Sets application In addition we demonstrate the advantage of R3way
resemblance over the pairwise case in improving retrieval quality
Introduction and Motivation
Similarity search near neighbor search is one of the fundamental problems in Computer Science
The task is to identify a small set of data points which are most similar to a given input query
Similarity search algorithms have been one of the basic building blocks in numerous applications
including search databases learning recommendation systems computer vision etc
One widely used notion of similarity on sets is the Jaccard similarity or resemblance 18
Given two sets S1 S2 the resemblance R2way between S1 and S2 is
defined as R2way
Existing notions of similarity in search problems mainly work with
pairwise similarity functions In this paper we go beyond this notion and look at the problem of
k-way similarity search where the similarity function of interest involves sets Our work
exploits the fact that resemblance can be naturally extended to k-way resemblance similarity
defined over sets S2 Sk as Rk?way
Binary high-dimensional data
The current web datasets are typically binary sparse and extremely high-dimensional largely due to the wide adoption of the Bag of Words BoW representations for documents and images It is often the case in BoW representations that just the presence
or absence of specific feature words captures sufficient information especially
with 3-grams or higher-order models And so the web can be imagined as a giant storehouse
of ultra high-dimensional sparse binary vectors Of course binary vectors can also be equivalently
viewed as sets containing locations of the nonzero features
We list four practical scenarios where k-way resemblance search would be a natural choice
Google Sets
http://googlesystem.blogspot.com/2012/11/google-sets-still-available.html
Google Sets is among the earliest google projects which allows users to generate list of similar
words by typing only few related keywords For example if the user types mazda and honda
the application will automatically generate related words like ford toyota etc This
application is currently available in google spreadsheet If we assume the term document binary
representation of each word in the database then given query w1 and w2 we show that
turns out to be a very good similarity measure for this application Section
Joint recommendations Users A and would like to watch a movie together The profile of
each person can be represented as a sparse vector over a giant universe of attributes For example
a user profile may be the set of actors actresses genres directors etc which she/he likes On the
other hand we can represent a movie in the database over the same universe based on attributes
associated with the movie If we have to recommend movie jointly to users A and then a
natural measure to maximize is
The problem of group recommendation is applicable
in many more settings such as recommending people to join circles etc
iii Improving retrieval quality We are interested in finding images of a particular type of object and we have two or three possibly noisy representative images In such a scenario a natural
expectation is that retrieving images simultaneously similar to all the representative images should
be more refined than just retrieving images similar to any one of them In Section we demonstrate that in cases where we have more than one element to search for we can refine our search
quality using k-way resemblance search In a dynamic feedback environment we can improve
subsequent search quality by using k-way similarity search on the pages already clicked by the user
Beyond pairwise clustering While machine learning algorithms often utilize the data
through pairwise similarities inner product or resemblance there are natural scenarios where
the affinity relations are not pairwise but rather triadic tetradic or higher The computational
cost of course will increase exponentially if we go beyond pairwise similarity
Efficiency is crucial With the data explosion in modern applications the brute force way of scanning all the data for searching is prohibitively expensive specially in user-facing applications like
search The need for k-way similarity search can only be fulfilled if it admits efficient algorithms
This paper fulfills this requirement for k-way resemblance and its derived similarities In particular
we show fast algorithms with provable query time guarantees for approximate k-way resemblance
search Our algorithms and analysis naturally provide a framework to extend classical LSH framework to handle higher-order similarities which could be of independent theoretical interest
Organization
In Section we review approximate near neighbor search and classical Locality
Sensitive Hashing In Section we formulate the 3-way similarity search problems Sections and describe provable fast algorithms for several search problems Section demonstrates the applicability of 3-way resemblance search in real applications
Classical c-NN and Locality Sensitive Hashing LSH
Initial attempts of finding efficient sub-linear time algorithms for exact near neighbor search based
on space partitioning turned out to be a disappointment with the massive dimensionality of current
datasets Approximate versions of the problem were proposed to break the linear
query time bottleneck One widely adopted formalism is the c-approximate near neighbor
Definition c-Approximate Near Neighbor or Consider a set of points denoted by in a
D-dimensional space RD and parameters R0 The task is to construct a data structure
which given any query point if there exist an R0 near neighbor of in it reports some cR0 near
neighbor of in with probability
The usual notion of c-NN is for distance Since we deal with similarities we define R0 near neighbor
of point as a point with Sim(q R0 where Sim is the similarity function of interest
Locality sensitive hashing LSH is a popular framework for c-NN problems LSH is a
family of functions with the property that similar input objects in the domain of these functions
have a higher probability of colliding in the range space than non-similar ones In formal terms
consider a family of hash functions mapping RD to some set
Definition Locality Sensitive Hashing A family is called cR0 p1 p2 sensitive if
for any two points RD and chosen uniformly from satisfies the following
if Sim(x R0 then rH p1
if Sim(x cR0 then rH p2
For approximate nearest neighbor search typically p1 p2 and is needed Note as
we are defining neighbors in terms of similarity Basically LSH trades off query time with extra
preprocessing time and space which can be accomplished off-line
Fact Given a family of cR0 p1 p2 sensitive hash functions one can construct a data struc1/p1
ture for c-NN with log1/p2 query time and space where log
log
Minwise Hashing for Pairwise Resemblance One popular choice of LSH family of functions
associated with resemblance similarity is Minwise Hashing family Minwise Hashing
family applies an independent random permutation on the given set and looks
at the minimum element under Given two sets S1 S2
it can be shown by elementary probability argument that
S2
R2way
S2
The recent work on b-bit minwise hashing provides an improvement by storing only the
lowest bits of the hashed values implemented the idea of building
hash tables for near neighbor search by directly using the bits from b-bit minwise hashing
3-way Similarity Search Formulation
Our focus will remain on binary vectors which can also be viewed as sets We illustrate our method
The algorithm and
using 3-way resemblance similarity function Sim(S1 S2 S3
guarantees naturally extend to k-way resemblance Given a size collection of sets
binary vectors we are particularly interested in the following three problems
Given two query sets S1 and S2 find S3 that maximizes Sim(S1 S2 S3
Given a query set S1 find two sets S2 S3 maximizing Sim(S1 S2 S3
Find three sets S1 S2 S3 maximizing Sim(S1 S2 S3
The brute force way of enumerating all possibilities leads to the worst case query time of
and for problem and respectively In a hope to break this barrier just like the
case of pairwise near neighbor search we define the c-approximate versions of the above
three problems As in the case of c-NN we are given two parameters R0 and For each
of the following three problems the guarantee is with probability at least
c-Near Neighbor or 3-way c-NN Given two query sets S1 and S2 if there
exists S3 with Sim(S1 S2 S3 R0 then we report some so that
Sim(S1 S2 cR0
c-Close Pair or 3-way c-CP Given a query set S1 if there exists a pair of
set S2 S3 with Sim(S1 S2 S3 R0 then we report sets so that
Sim(S1 cR0
c-Best Cluster or 3-way c-BC If there exist sets S1 S2 S3 with
Sim(S1 S2 S3 R0 then we report sets so that Sim(S1 cR0
Sub-linear Algorithm for 3-way c-NN
The basic philosophy behind sub-linear search is bucketing which allows us to preprocess dataset
in a fashion so that we can filter many bad candidates without scanning all of them LSH-based
techniques rely on randomized hash functions to create buckets that probabilistically filter bad candidates This philosophy is not restricted for binary similarity functions and is much more general
Here we first focus on 3-way c-NN problem for binary data
Theorem For R3way c-NN one can construct a data structure with log1/cR0 query time
and space where
log
log 1/c+log
The argument for 2-way resemblance can be naturally extended to k-way resemblance Specifically
given three sets S1 S2 S3 and an independent random permutation we have
R3way
shows that minwise hashing although it operates on sets individually preserves all 3-way
fact k-way similarity structure of the data The existence of such a hash function is the key
requirement behind the existence of efficient approximate search For the pairwise case the probability event was a simple hash collision and the min-hash itself serves as the bucket index In case
of 3-way and higher c-NN problem we have to take care of a more complicated event to create an
indexing scheme In particular during preprocessing we need to create buckets for each individual
S3 and while querying we need to associate the query sets S1 and S2 to the appropriate bucket We
need extra mechanisms to manipulate these minwise hashes to obtain a bucketing scheme
Proof of Theorem We use two additional functions f1 for manipulating
and f2 for manipulating both and Let a such that
We define f1 and f2 This choice ensures
that given query S1 and S2 for any S3 f1 f2 holds
if and only if and thus we get a bucketing scheme
To complete the proof we introduce two integer parameters and L. Define a new hash function
by concatenating events To be more precise while preprocessing for every element S3
create buckets g1 f1 hK where hi is chosen uniformly from minwise
hashing family For given query points S1 and S2 retrieve only points in the bucket g2 S2
h1 f2 hK hK Repeat this process times independently For any
S3 with Sim(S1 S2 S3 R0 is retrieved with probability at least R0K Using
log
log
log
and log where log 1/c+log the proof can be obtained
cR0
using standard concentration arguments used to prove Fact see It is worth noting that
the probability guarantee parameter gets absorbed in the constants as log Note the process is
stopped as soon as we find some element with R3way cR0
Theorem can be easily extended to k-way resemblance with same query time and space guarantees
Note that k-way c-NN is at least as hard as way c-NN for any because we can always
choose identical query sets in k-way c-NN and it reduces to way c-NN problem So
any improvements in R3way c-NN implies improvement in the classical min-hash LSH for Jaccard
similarity The proposed analysis is thus tight in this sense
The above observation makes it possible to also perform the traditional pairwise c-NN search using
the same hash tables deployed for 3-way c-NN In the query phase we have an option if we have
two different queries S1 S2 then we retrieve from bucket g2 S2 and that is usual 3-way c-NN
search If we are just interested in pairwise near neighbor search given one query S1 then we will
look into bucket g2 S1 and we know that the 3-way resemblance between S1 S1 S3 boils
down to the pairwise resemblance between S1 and S3 So the same hash tables can be used for
both the purposes This property generalizes and hash tables created for k-way c-NN can be used
for any way similarity search so long as The approximation guarantees still holds This
flexibility makes k-way c-NN bucketing scheme more advantageous over the pairwise scheme
One of the peculiarity of LSH based techniques is that the
query complexity exponent is dependent on the choice
of the threshold R0 we are interested in and the value of
which is the approximation ratio that we will tolerate Figure
log
plots log 1/c+log with respect to for selected R0
values from to For instance if we are interested in
highly similar pairs R0 then we are looking at near
O(log query complexity for c-NN problem as On
the other hand for very lower threshold R0 there is no much
log
of hope of time-saving because is close to
Figure log 1/c+log
Other Efficient k-way Similarities
We refer to the k-way similarities for which there exist sub-linear algorithms for c-NN search with
query and space complexity exactly as given in Theorem as efficient We have demonstrated
existence of one such example of efficient similarities which is the k-way resemblance This leads
to a natural question Are there more of them
analyzed all the transformations on similarities that preserve existence of efficient LSH search In
particular they showed that if is a similarity for which there exists an LSH family then there also
exists an LSH family for any similarity which is a probability generating
function PGF transfor
mation on S. PGF
transformation
on
is
defined
as
GF
where and
pi satisfies pi Similar theorem can also be shown in the case of 3-way resemblance
Theorem Any PGF transformation on 3-way resemblance R3way is efficient
Recall in the proof of Theorem we created hash assignments f1 and
f2 which lead to a bucketing scheme for the 3-way resemblance search
where the collision event f2 happens with
probability R3way To prove the above Theorem we will need to create hash events
having probability GF R3way pi R3way Note that GF R3way We will
make use of the following simple lemma
Lemma R3way is efficient for all N.
Proof Define new hash assignments g1n f1 hn and g2n S2
h1 f2 hn hn The collision event g1n g2n S2 has
probability R3way We now use the pair g1n g2n instead of f1 f2 and obtain same
guarantees as in Theorem for R3way as well
Proof of Theorem From Lemma let g1i g2i be the hash pair corresponding to R3way
as used in above lemma We sample one hash pair from the set g1i g2i where
the probability of sampling g1i g2i is proportional to pi Note that pi and satisfies
is valid It is not difficult to see that the collision of the
pi and so the above sampling
sampled hash pair has probability exactly pi R3way
Theorem can be naturally extended to k-way similarity for any Thus we now have
infinitely many k-way similarity functions admitting efficient sub-linear search One that might be
interesting because of its radial basis kernel like nature is shown in the following corollary
Corollary eR
k?way
is efficient
Proof Use the expansion of eR
k?way
normalized by to see that eR
k?way
is a PGF on Rk?way
Fast Algorithms for 3-way c-CP and 3-way c-BC Problems
For 3-way c-CP and 3-way c-BC problems using bucketing scheme with minwise hashing family
will save even more computations
Theorem For R3way c-Close Pair Problem c-CP one can construct a data structure with
log
log1/cR0 query time and space where log 1/c+log
Note that we can switch the role of f1 and f2 in the proof of Theorem We are thus left with a c-NN
problem with search space all pairs instead of A bit of analysis similar to Theorem
will show that this procedure achieves the required query time log1/cR0 but uses a lot
more space than shown in the above theorem It turns out that there is a better way of
doing c-CP that saves us space
Proof of Theorem We again start with constructing hash tables For every element Sc we
create a hash-table and store Sc in bucket B(Sc Sc h2 Sc hK Sc where hi is chosen
uniformly from minwise independent family of hash functions H. We create such hash-tables For
a query element Sq we look for all pairs in bucket B(Sq Sq h2 Sq hK Sq and repeat
this for each of the tables Note we do not form pairs of elements retrieved from different tables
as they do not satisfy If there exists a pair S1 S2 with Sim(Sq S1 S2 R0 using
we can see that we will find that pair in bucket B(Sq with probability R0K
Here we cannot use traditional choice of and similar to what we did in Theorem as there
log
log
are instead of possible pairs We instead use log
and
cR0
log
with log 1/c+log
With this choice of and the result follows Note the process
is stopped as soon as we find pairs S1 and S2 with Sim(Sq S1 S2 cR0 The key argument that
saves space from to is that we hash points individually makes it
clear that hashing all possible pairs is not needed when every point can be processed individually
and pairs formed within each bucket itself filter out most of the unnecessary combinations
Theorem For R3way c-Best Cluster Problem c-BC there exist an algorithm with running time
log
log1/cR0 where log 1/c+log
The argument similar to one used in proof of Theorem leads to the running time of
log1/cR0 as we need and we have to processes all points at least once
Proof of Theorem Repeat c-CP problem times for every element in collection acting
as query once We use the same set of hash tables and hash functions every time The preprocessing time is log1/cR0 evaluations of hash functions and the total querying time is
O(n log1/cR0 which makes the total running time log1/cR0
For k-way c-BC Problem we can achieve log1/cR0 running time If we are interested in very high similarity cluster with R0 then and the running time is around
O(n log This is a huge saving over the brute force O(nk In most practical cases specially in
big data regime where we have enormous amount of data we can expect the k-way similarity of
good clusters to be high and finding them should be efficient We can see that with increasing
hashing techniques save more computations
Experiments
In this section we demonstrate the usability of 3-way and higher-order similarity search using
Google Sets and Improving retrieval quality
Google Sets Generating Semantically Similar Words
Here the task is to retrieve words which are semantically similar to the given set of query words
We collected million random documents from Wikipedia and created a standard term-doc binary vector representation of each term present in the collected documents after removing standard
stop words and punctuation marks More specifically every word is represented as a million dimension binary vector indicating its presence or absence in the corresponding document The total
number of terms words was around in this experiment
Since there is no standard benchmark available for this task we show qualitative evaluations For
querying we used the following four pairs of semantically related words jaguar and tiger
artificial and intelligence iii milky and way finger and lakes Given the
query words w1 and w2 we compare the results obtained by the following four methods
Google Sets We use Google?s algorithm and report words from Google spreadsheets
This is Google?s algorithm which uses its own data
3-way Resemblance We use 3-way resemblance
to rank every word
and report top words based on this ranking
Sum Resemblance Another intuitive method is to use the sum of pairwise resem|w2
blance
and report top words based on this ranking
Pairwise Intersection We first retrieve top words based on pairwise resemblance
for each w1 and w2 independently We then report the words common in both If there is
no word in common we do not report anything
The results in Table demonstrate that using 3-way resemblance retrieves reasonable candidates
for these four queries An interesting query is finger and lakes Finger Lakes is a region in
upstate New York Google could only relate it to New York while 3-way resemblance could even
retrieve the names of cities and lakes in the region Also for query milky and we can
see some perhaps unrelated words like dance returned by Google We do not see such random
behavior with 3-way resemblance Although we are not aware of the algorithm and the dataset used
by Google we can see that 3-way resemblance appears to be a right measure for this application
The above results also illustrate the problem with using the sum of pairwise similarity method The
similarity value with one of the words dominates the sum and hence we see for queries artificial
and intelligence that all the retrieved words are mostly related to the word intelligence Same is
the case with query finger and lakes as well as jaguar and tiger Note that jaguar is also a
car brand In addition for all queries there was no common word in the top words similar to
the each query word individually and so PI method never returns anything
Table Top five words retrieved using various methods for different queries
JAGUAR AND TIGER
OOGLE
WAY
SR
LION
LEOPARD
CHEETAH
CAT
DOG
LEOPARD
CHEETAH
LION
PANTHER
CAT
CAT
LEOPARD
LITRE
BMW
CHASIS
MILKY AND WAY
OOGLE
WAY
SR
DANCE
STARS
SPACE
THE
UNIVERSE
GALAXY
STARS
EARTH
LIGHT
SPACE
EVEN
ANOTHER
STILL
BACK
TIME
PI
ARTIFICIAL AND INTELLIGENCE
OOGLE
WAY
SR
PI
COMPUTER
COMPUTER
SECURITY
PROGRAMMING
SCIENCE
WEAPONS
INTELLIGENT
SECRET
SCIENCE
ROBOT
HUMAN
ATTACKS
ROBOTICS
TECHNOLOGY
HUMAN
PI
OOGLE
NEW
YORK
NY
PARK
CITY
FINGER AND LAKES
WAY
SR
SENECA
CAYUGA
ERIE
ROCHESTER
IROQUOIS
RIVERS
FRESHWATER
FISH
STREAMS
FORESTED
PI
We should note the importance of the denominator term in 3-way resemblance without which frequent words will be blindly favored The exciting contribution of this paper is that 3-way resemblance similarity search admits provable sub-linear guarantees making it an ideal choice On the
other hand no such provable guarantees are known for SR and other heuristic based search methods
Improving Retrieval Quality in Similarity Search
We also demonstrate how the retrieval quality of traditional similarity search can be boosted by utilizing more query candidates instead of just one For the evaluations we choose two public datasets
MNIST and WEBSPAM which were used in a recent related paper for near neighbor search
with binary data using b-bit minwise hashing
The two datasets reflect diversity both in terms of task and scale that is encountered in practice
The MNIST dataset consists of handwritten digit samples Each sample is an image of 28 28
pixel yielding a dimension vector with the associated class label digit We binarize the
data by settings all non zeros to be We used the standard partition of MNIST which consists
of samples in one set and in the other The WEBSPAM dataset with
features consists of sparse vector representation of emails labeled as spam or not We randomly
sample data points and partitioned them into two independent sets of size each
Table Percentage of top candidates with the same labels as that of query retrieved using various
similarity criteria More indicates better retrieval quality Best marked in bold
OP
Pairwise
3-way NNbor
4-way NNbor
MNIST
WEBSPAM
For evaluation we need to generate potential similar search query candidates for k-way search It
makes no sense in trying to search for object simultaneously similar to two very different objects To
generate such query candidates we took one independent set of the data and partition it according
to the class labels We then run a cheap k-mean clustering on each class and randomly sample
triplets x3 from each cluster for evaluating 3-way and 4-way similarity search
For MNIST dataset the standard test set was partitioned according to the labels into sets
each partition was then clustered into clusters and we choose triplets randomly from each
cluster In all we had such triplets for each class and thus overall query triplets For
WEBSPAM which consists only of classes we choose one of the independent set and performed
the same procedure We selected triplets from each cluster We thus have triplets from
each class making the total number of query candidates
The above procedures ensure that the elements in each triplets x3 are not very far from
each other and are of the same class label For each triplet x3 we sort all the points
in the other independent set based on the following
Pairwise We only use the information in and rank based on resemblance
3-way NN We rank based on 3-way resemblance
4-way NN We rank based on 4-way resemblance
We look at the top and points based on orderings described above Since all the
query triplets are of the same label The percentage of top retrieved candidates having same label as
that of the query items is a natural metric to evaluate the retrieval quality This percentage values
accumulated over all the triplets are summarized in Table
We can see that top candidates retrieved by 3-way resemblance similarity using query points
are of better quality than vanilla pairwise similarity search Also 4-way resemblance with query
points further improves the results compared to 3-way resemblance similarity search This clearly
demonstrates that multi-way resemblance similarity search is more desirable whenever we have
more than one representative query in mind Note that for MNIST which contains classes the
boost compared to pairwise retrieval is substantial The results follow a consistent trend
Future Work
While the work presented in this paper is promising for efficient 3-way and k-way similarity search
in binary high-dimensional data there are numerous interesting and practical research problems we
can study as future work In this section we mention a few such examples
One-permutation hashing Traditionally building hash tables for near neighbor search required
many independent hashes This is both time and energy-consuming not only for building tables but also for processing un-seen queries which have not been processed One permutation
hashing provides the hope of reducing many permutations to merely one The version in
however was not applicable to near neighbor search due to the existence of many empty bins which
offer no indexing capability The most recent work is able to fill the empty bins and works
well for pairwise near neighbor search It will be interesting to extend to k-way search
Non-binary sparse data This paper focuses on minwise hashing for binary data Various extensions
to real-valued data are possible For example our results naturally apply to consistent weighted
sampling which is one way to handle non-binary sparse data The problem however is not
solved if we are interested in similarities such as normalized k-way inner products although the
line of work on Conditional Random Sampling CRS may be promising CRS works on
non-binary sparse data by storing a bottom subset of nonzero entries after applying one permutation
to real-valued sparse data matrix CRS performs very well for certain applications but it does not
work in our context because the bottom nonzero subsets are not properly aligned
Building hash tables by directly using bits from minwise hashing This will be a different approach
from the way how the hash tables are constructed in this paper For example directly used
the bits from b-bit minwise hashing to build hash tables and demonstrated the significant
advantages compared to sim-hash and spectral hashing It would be interesting to see
the performance of this approach in k-way similarity search
k-Way sign random projections It would be very useful to develop theory for k-way sign random
projections For usual real-valued random projections it is known that the volume which is related
to the determinant is approximately preserved We speculate that the collision probability
of k-way sign random projections might be also a monotonic function of the determinant
Conclusions
We formulate a new framework for k-way similarity search and obtain fast algorithms in the case of
k-way resemblance with provable worst-case approximation guarantees We show some applications
of k-way resemblance search in practice and demonstrate the advantages over traditional search Our
analysis involves the idea of probabilistic hashing and extends the well-known LSH family beyond
the pairwise case We believe the idea of probabilistic hashing still has a long way to go
Acknowledgement
The work is supported by and
Ping Li thanks Kenneth Church for introducing Google Sets to him in
the summer of at Microsoft Research

----------------------------------------------------------------

title: 2534-different-cortico-basal-ganglia-loops-specialize-in-reward-prediction-at-different-time-scales.pdf

Different Cortico-Basal Ganglia Loops
Specialize in Reward Prediction on
Different Time Scales
Saori Tanaka
Kenji Doya
Nara Institute of Science and Technology
ATR Computational Neuroscience Laboratories
CREST Japan Science and Technology Corporation
Kyoto Japan
xsaori@atr.co.jp
doya@atr.co.jp
Go Okada
Kazutaka Ueda
Yasumasa Okamoto
Shigeto Yamawaki
Hiroshima University School of Medicine
CREST Japan Science and Technology Corporation
Hiroshima Japan
Abstract
To understand the brain mechanisms involved in reward prediction
on different time scales we developed a Markov decision task that
requires prediction of both immediate and future rewards and analyzed subjects brain activities using functional MRI. We estimated
the time course of reward prediction and reward prediction error on
different time scales from subjects performance data and used them
as the explanatory variables for SPM analysis We found topographic maps of different time scales in medial frontal cortex and
striatum The result suggests that different cortico-basal ganglia
loops are specialized for reward prediction on different time scales
Intro du ction
In our daily life we make decisions based on the prediction of rewards on different
time scales immediate and long-term effects of an action are often in conflict and
biased evaluation of immediate or future outcome can lead to pathetic behaviors
Lesions in the central serotonergic system result in impulsive behaviors in humans
and animals which can be attributed to deficits in reward prediction on a long
time scale Damages in the ventral part of medial frontal cortex MFC also cause
deficits in decision-making that requires assessment of future outcomes
A possible mechanism underlying these observations is that different brain areas are
specialized for reward prediction on different time scales and that the ascending
serotonergic system activates those specialized for predictions in longer time scales
The theoretical framework of temporal difference learning successfully
explains reward-predictive activities of the midbrain dopaminergic system as well as
those of the cortex and the striatum In TD learning theory the predicted
amount of future reward starting from a state is formulated as the value function
E[r(t r(t
and learning is based on the TD error
V(t
The discount factor controls the time scale of prediction while only the immediate
reward r(t is considered with rewards in the longer future are taken into
account with closer to
In order to test the above hypothesis we developed a reinforcement learning task
which requires a large value of discount factor for successful performance and analyzed subjects brain activities using functional MRI. In addition to conventional
block-design analysis a novel model-based regression analysis revealed topographic
representation of prediction time scale with in the cortico-basal ganglia loops
Methods
Markov Decision Task
In the Markov decision task markers on the corners of a square present four
states and the subject selects one of two actions by pressing a button left button
a2 right button The action determines both the amount of reward and the
movement of the marker In the REGULAR condition the next trial is
started from the marker position at the end of the previous trial Therefore in order to
maximize the reward acquired in a long run the subject has to select an action by
taking into account both the immediate reward and the future reward expected from
the subsequent state The optimal behavior is to receive small negative rewards at
states and s4 to obtain a large positive reward at state s1 In the
RANDOM condition next trial is started from a random marker position so that the
subject has to consider only immediate reward Thus the optimal behavior is to collect a larger reward at each state In the baseline condition NO condition
the reward is always zero
In order to learn the optimal behaviors the discount factor has to be larger than
in REGULAR condition while it can be arbitrarily small in RANDOM condition
fMRI imaging
Eighteen healthy right-handed volunteers males and females gave informed
consent to take part in the study with the approval of the ethics and safety committees
of ATR and Hiroshima University
A
Time
s2
s1
REGULAR condition
s2
s1
RANDOM condition
s2
s1
s4
s3
a1
a2
r1 yen
r2 yen
s4
s3
s4
s3
Sequence of stimulus and response events in the Markov decision
task First one of four squares representing present state turns green As
the fixation point turns green the subject presses either the right or left
button within second After 1s delay the green square changes its position
and then a reward for the current action is presented by a number
and a bar graph showing cumulative reward during the block is updated
One trial takes four seconds Subjects performed five trials in the NO
condition 32 trials in the RANDOM condition five trials in the NO condition and 32 trials in the REGULAR condition in one block They repeated
four blocks thus the entire experiment consisted of trials taking about
minutes The rule of the reward and marker movement In the
REGULAR condition the optimal behavior is to receive small negative rewards or yen at states and s4 to obtain a large positive
reward or yen at state In the RANDOM condition
the next trial is started from random state Thus the optimal behavior is to
select a larger reward at each state
A 1.5-Tesla scanner Marconi MAGNEX ECLIPSE Japan was used to acquire both
structural T1-weighted images TR TE ms flip angle deg matrix
FoV mm thickness mm slice gap mm and T2*-weighted
echo planar images TR TE 55 msec flip angle 90 deg 38 transverse slices
matrix 64 64 FoV mm thickness mm slice gap mm slice gap
mm with blood oxygen level-dependent BOLD contrast
Data analysis
The data were preprocessed and analyzed with SPM99 Friston Wellcome Department of Cognitive Neurology London UK). The first three volumes of
images were discarded to avoid T1 equilibrium effects The images were realigned to
the first image as a

----------------------------------------------------------------

