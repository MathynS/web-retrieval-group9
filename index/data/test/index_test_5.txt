query sentence: digital microelectronic amplifiers
<<------------------------------------------------------------------------------------------------------------------------------------------->>
title: 11-microelectronic-implementations-of-connectionist-neural-networks.pdf

MICROELECTRONIC IMPLEMENTATIONS OF CONNECTIONIST
NEURAL NETWORKS
Stuart Mackie Hans P. Graf Daniel B. Schwartz and John S. Denker
AT&T Bell Labs Holmdel NJ
Abstract
In this paper we discuss why special purpose chips are needed for useful
implementations of connectionist neural networks in such applications as pattern
recognition and classification Three chip designs are described a hybrid
digital/analog programmable connection matrix an analog connection matrix with
adjustable connection strengths and a digital pipe lined best-match chip The common
feature of the designs is the distribution of arithmetic processing power amongst the
data storage to minimize data movement
RAMs
Distributed
co mputati on
chips
ZO
Conventional
CPUs
Node Complexity
of Transistors
Figure A schematic graph of addressable node complexity and size for conventional
computer chips Memories can contain millions of very simple nodes each
with a very few transistors but with no processing power CPU chips are
essentially one very complex node Neural network chips are in the
distributed computation region where chips contain many simple fixed
instruction processors local to data storage After Reece and Treleaven
American Institute of Physics
Introduction
It is clear that conventional computers lag far behind organic computers when it
comes to dealing with very large data rates in problems such as computer vision and
speech recognition Why is this The reason is that the brain performs a huge number
of operations in parallel whereas in a conventional computer there is a very fast
processor that can perform a variety of instructions very quickly but operates on only
two pieces of data at a time
The rest of the many megabytes of RAM is idle during any instruction cycle The
duty cycle of the processor is close to but that of the stored data is very close to
zero If we wish to make better use of the data we have to distribute processing
power amongst the stored data in a similar fashion to the brain Figure illustrates
where distributed computation chips lie in comparison to conventional computer chips
as regard number and complexity of addressable nodes per chip
In order for a distributed strategy to work each processing element must be small
in order to accommodate many on a chip and communication must be local and hardwired Whereas the processing element in a conventional computer may be able to
execute many hundred different operations in our scheme the processor is hard-wired
to perform just one This operation should be tailored to some particular application
In neural network and pattern recognition algorithms the dot products of an input
vector with a series of stored vectors referred to as features or memories is often
required The general calculation is
Sum of Products
L. v.J f..IJ
where is the input vector and is one of the stored feature vectors Two
variations of this are of particular interest In feature extraction we wish to find all the
features for which the dot product with the input vector is greater than some threshold
in which case we say that such features are present in the input vector
Feature Extraction
L. v.J f..IJ
In pattern classification we wish to find the stored vector that has the largest dot
product with the input vector and we say that the the input is a member of the class
represented by that feature or simply that that stored vector is closest to input vector
Classification
max(V
LV.
IJ
The chips described here are each designed to perform one or more of the above
functions with an input vector and a number of feature vectors in parallel The overall
strategy may be summed up as follows we recognize that in typical pattern recognition
applications the feature vectors need to be changed infrequently compared to the input
vectors and the calculation that is perfonned is fixed and low-precision we therefore
distribute simple fixed-instruction processors throughout the data storage area thus
minimizing the data movement and optimizing the use of silicon Our ideal is to have
every transistor on the chip doing something useful during every instruction cycle
Analog Sum-or-Products
sing an idea slightly reminiscent of synapses and neurons from the brain in two
of the chips we store elements of features as connections from input wires on which the
elements of the input vectors appear as voltages to summing wires where a sum-ofproducts is perfonned The voltage resulting from the current summing is applied to
the input of an amplifier whose output is then read to determine the result of the
calculation A schematic arrangement is shown in Figure with the vertical inputs
connected to the horizontal summing wires through resistors chosen such that the
conductance is proportional to the magnitude of the feature element When both
positive and negative values are required inverted input lines are also necessary
Resistor matrices have been fabricated using amorphous silicon connections and metal
linewidths These were programmed during fabrication by electron beam lithography
to store names using the distributed feedback method described by Hopfield2 This
work is described more fully elsewhere Hard-wired resistor matrices are very
compact but also very inflexible In many applications it is desirable to be able to
reprogram the matrix without having to fabricate a new chip For this reason a series
of programmable chips has been designed
Input lines
Feature
Feature
oc
Feature
I
Feature
Figure A schematic arrangement for calculating parallel sum-of-products with a
resistor matrix Features are stored as connections along summing wires and
the input elements are applied as voltages on the input wires The voltage
generated by the current summing is thresholded by the amplifer whose
output is read out at the end of the calculation Feedback connections may be
made to give mutual inhibition and allow only one feature amplifier to tum
on or allow the matrix to be used as a distributed feedback memory
Programmable Connection Matrix
Figure is a schematic diagram of a programmable connection using the contents of
two RAM cells to control current sinking or sourcing into the summing wire The
switches are pass transistors and the resistors are transistors with gates connected to
their drains Current is sourced or sunk if the appropriate RAM cell contains a and
the input Vi is high thus closing both switches in the path Feature elements can
therefore take on values where the values of a and are determined by the
conductivities of the and p-transistors obtained during processing A matrix with
such connections allowing full interconnection of the inputs and outputs of 54
amplifiers was designed and fabricated in CMOS Figure Each connection
is about the chip is 7x7mm and contains about transistors When
loaded with 49 features kernel and presented with a input vector
the chip performs 49 dot products in parallel in under 1Jls This is equivalent to
billion bit operations/sec The flexibility of the design allows the chip to be operated in
several modes The chip was programmed as a distributed feedback memory
associative memory but this did not work well because the current sinking capability
of the n-type transistors was times that of the p-types An associative memory was
implemented by using a grandmother cell representation where the memories were
stored along the input lines of amplifiers as for feature extraction but mutually
inhibitory connections were also made that allowed only one output to tum on With
stored vectors each bits long the best match was found in depending
on the data The circuit can also be programmed to recognize sequences of vectors and
to do error correction when vectors were omitted or wrong vectors were inserted into
the sequences The details of operation of the chip are described more fully
elsewhere This chip has been interfaced to a UNIX minicomputer and is in everyday
use as an accelerator for feature extraction in optical character recognition of handwritten numerals The chip speeds up this time consuming calculation by a factor of
more than The use of the chip enables experiments to be done which would be
too time consuming to simulate
Experience with this device has led to the design of four new chips which are
currently being tested These have no feedback capability and are intended exclusively
for feature extraction The designs each incorporate new features which are being
tested separately but all are based on a connection matrix which stores 46 vectors each
96 bits long The chip will perform a full parallel calculation in lOOns
VDD
Output
Vj
Excitatory
Inhibitory
Ivss
Figure Schematic diagram of a programmable connection A current sourcing or
sinking connection is made if a RAM cell contains a and the input Vi is
high The currents are summed on the input wire of the amplifier
Pads
Row Decoders
Connections
ITII1 Amplifie rs
Figure Programmable connection matrix chip The chip contains transistors
in and was fabricated using design rules
Adaptive Connection Matrix
Many problems require analog depth in the connection strengths and this is
especially important if the chip is to be used for learning where small adjustments are
required during training Typical approaches which use transistors sized in powers of
two to give conductance variability take up an area equivalent to the same number of
minimum sized transistors as the dynamic range which is expensive in area and
enables only a few connections to be put on a chip We have designed a fully analog
connection based on a DRAM structure that can be fabricated using conventional
CMOS technology A schematic of a connection and a connection matrix is shown in
Figure The connection strength is represented by the difference in voltages stored
on two MOS capacitors The capacitors are 33Jlm on edge and lose about of their
charge in five minutes at room temperature The leakage rate can be reduced by three
orders of magnitude by cooling the the capacitors to and by five orders of
magnitude by cooling to The output is a current proportional to the product of
the input voltage and the connection strength The output currents are summed on a
wire and are sent off chip to external amplifiers The connection strengths can be
adjusted using transferring charge between the capacitors through a chain of transistors
The connections strengths may be of either polarity and it is expected that the
connections will have about bits of analog depth A chip has been designed in
CMOS containing connections in an array with 46 inputs and 24 outputs
Input
Weight update and decay
by shifting charge
02
Input
Output=w*lnput
Output through external amplifiers
Figure Analog connection The connection strength is represented by the difference
in voltages stored on two capacitors The output is a current proprtional to
the product of the input voltage and the connection strength
Each connection is The design has been sent to foundry and testing is
expected to start in April The chip has been designed to perform a network
calculation in the chip will perform at a rate of 33 billion multiplies/sec It
can be used simply as a fast analog convolver for feature extraction or as a learning
engine in a gradient descent algorithm using external logic for connection strength
adjustment Because the inputs and outputs are true analog larger networks may be
formed by tiling chips and layered networks may be made by cascading through
amplifiers acting as hidden units
Digital Classifier Chip
The third design is a digital implementation of a classifier whose architecture is not
a connectionist matrix It is nearing completion of the design stage and will be
fabricated using CMOS It calculates the largest five using an alldigital pipeline of identical processors each attached to one stored word Each
processor is also internally pipelined to the extent that no stage contains more than two
gate delays This is important since the throughput of the processor is limited by the
speed of the slowest stage Each processor calculates the Hamming distance number
of difference bits between an input word and its stored word and then compares that
distance with each of the smallest values previously found for that input word An
updated list of best matches is then passed to the next processor in the pipeline At
the end of the pipeline the best matches overall are output
Features stored in
Data
pipeline
ring shift register
it
Best match list
pipeline
Tag register
it Ii
Jr it mIfl
I[HI tm
Pf
Input and feature
Accumulator
are compared
dumps
distance
bit-serially
into comparison
register at end
of input word
Pig.
Comparator inserts
new match and tag into
list when better than
old match
Schematic of one of the processors in the digital classifier chip The
Hamming distance of the input vector to the feature vector is calculated and
if better than one of the five best matches found so far is inserted into the
match list together with the tag and passed onto the next processor At the
end of the pipeline the best five matches overall are output
The data paths on chip are one bit wide and all calculations are bit serial This
means that the processing elements and the data paths are compact and maximizes the
number of stored words per chip The layout of a single processor is shown in
The features are stored as words in ring shift registers and
associated with each feature is a tag or name string that is stored in a static
register The input vector passes through the chip and is compared bit-by-bit to each
stored vector whose shift registers are cycled in tum The total number of bits
difference is summed in an accumulator After a vector has passed through a processor
the total Hamming distance is loaded into the comparison register together with the tag
At this time the match list for the input vector arrives at the comparator It is an
ordered list of the lowest Hamming distances found in the pipeline so far together
with associated tag strings The distance just calculated is compared bit-serially with
each of the values in the list in turn If the current distance is smaller than one of the
ones in the list the output streams of the comparator are switched having the effect of
inserting the current match and tag into the list and deleting the previous fifth best
match After the last processor in the pipeline the list stream contains the best five
distances overall together with the tags of the stored vectors that generated them The
data stream and the list stream are loaded into wide registers ready for output
The design enables chips to be connected together to extend the pipeline if more than
stored vectors are required The throughput is constant irrespective of the number of
chips connected together only the latency increases as the number of chips increases
The chip has been designed to operate with an on-chip clock frequency of at least
l00MHz This high speed is possible because stage sizes are very small and data paths
have been kept short The computational efficiency is not as high as in the analog chips
because each processor only deals with one bit of stored data at a time However the
overall throughput is high because of the high clock speed Assuming a clock
frequency of l00MHz the chip will produce a list of best distances with tag strings
every with a latency of about Even if a thousand chips containing
stored vectors were pipelined together the latency would be low
enough for most real time applications The chip is expected to perform billion bit
operation/sec
While it is important to have high clock frequencies on the chip it is also important
to have them much lower off the chip since frequencies above 50MHz are hard to deal
on circuit boards The wide communication paths onto and off the chip ensure
that this is not a problem here
Conclusion
The two approaches discussed here analog and digital represent opposites in
computational approach In one a single global computation is performed for each
match in the other many local calculations are done Both the approaches have their
advantages and it remains to be seen which type of circuit will be more efficient in
applications and how closely an electronic implementation of a neural network should
resemble the highly interconnected nature of a biolOgical network
These designs represent some of the first distributed computation chips They are
characterized by having simple processors distributed amongst data storage The
operation performed by the processor is tailored to the application It is interesting to
note some of the reasons why these designs can now be made minimum linewidths on
circuits are now small enough that enough processors can be put on one chip to make
these designs of a useful size sophisticated design tools are now available that enable a
single person to design and simulate a complete circuit in a matter of months and
fabrication costs are low enough that highly speculative circuits can be made without
requiring future volume production to offset prototype costs
We expect a flurry of similar designs in the coming years with circuits becoming
more and more optimized for particular applications However it should be noted that
the impressive speed gain achieved by putting an algorithm into custom silicon can only
be done once Further gains in speed will be closely tied to mainstream technological
advances in such areas as transistor size reduction and wafer-scale integration It
remains to be seen what influence these kinds of custom circuits will have in useful
technology since at present their functions cannot even be simulated in reasonable time
What can be achieved with these circuits is very limited when compared with a three
dimensional highly complex biological system but is a vast improvement over
conventional computer architectures
The authors gratefully acknowledge the contributions made by L.D. Jackel and
R.E. Howard

<<----------------------------------------------------------------------------------------------------------------------->>

title: 586-ann-based-classification-for-heart-defibrillators.pdf

ANN Based Classification for Heart Defibrillators
M. Jabri S. Pickard P. Leong Z. Chi B. Flower and Y. Xie
Sydney University Electrical Engineering
NSW Australia
Abstract
Current Intra-Cardia defibrillators make use of simple classification algorithms to determine patient conditions and subsequently to enable proper
therapy The simplicity is primarily due to the constraints on power dissipation and area available for implementation Sub-threshold implementation
of artificial neural networks offer potential classifiers with higher performance than commercially available defibrillators In this paper we explore
several classifier architectures and discuss micro-electronic implementation
issues
INTRODUCTION
Intra-Cardia Defibrillators lCDs represent an important therapy for people with heart disease These devices are implanted and perform three types of actions
l.monitor the heart
pace the heart
apply high energy/high voltage electric shock
1bey sense the electrical activity of the heart through leads attached to the heart tissue Two
types of sensing are commooly used
Single Chamber Lead attached to the Right Ventricular Apex RVA
Dual Chamber An additional lead is attached to the High Right Atrium
The actions performed by defibrillators are based on the outcome of a classification procedure
based on the heart rhythms of different heart diseases abnormal rhythms or arrhythmias
Jabri Pickard Leong Chi Flower and Xie
There are tens of different arrhythmias of interest to cardiologists They are clustered into
three groups according to the three therapies actions that ICDs perform
Figure shows an example of a Normal Sinus Rhythm Note the regularity in the beats Of
interest to us is what is called the QRS complex which represents the electrical activity in the
ventricle during a beat The point represents the peak and the distance between two heart
beats is usually referred to as the RR interval
FIGURE A Normal Sinus Rhythm NSR waveform
Figure shows an example of a Ventricular Tachycardia more precisely a Ventricular Tachycardia Slow or VTS Note that the beats are faster in comparison with an NSR.
Ventricular Fibrillation is shown in Figure Note the chaotic behavior and the absence
of well defined heart beats
FIGURE A Ventricular Tachycardia wavefonn
The three waveforms discussed above are examples of Intra-Cardia Electro-Grams ICEG
NSR VT and VF are representative of the type of action a defibrillator has to takes For an
NSR. an action of continue monitoring is used For a VT an action of pacing is perfonned whereas for VF a high energy/high voltage shock is issued Because they are nearfield signals ICEGs are different from external Eltro-Cardio-Grams As a result classification algorithms developed for ECG patterns may not be necessarily valuable for lCEG recognition
The difficulties in ICEG classification lie in that many arrhythmias share similar features and
fuzzy situations often need to be dealt with For instance many ICDs make use of the heart
rate as a fundamental feature in the arrhythmia classificatioo process But several arrhythmias
that require different type of therapeutic actions have similar heart rates For example a Sinus
Tachycardia is an arrhythmia characterized with a heart rate that is higher than that of an
NSR and in the vicinity of a VT. Many classifier would classify an ST as VT leading to a therapy of pacing whereas an ST is supposed to be grouped under an NSR type of therapy
Another example is a fast VT which may be associated with heart rates that are indicative of
ANN Based Classification for Heart Defibrillators
VF. In this case the defibrillator would apply a VF type of therapy when only a vr type therapy is required pacing
FIGURE A Ventricular Fibrillation waveform
The overlap of the classes when only heart rate is used as the main classification feature highlights the necessity of the consideration of further features with higher discrimination capabilities Features that are commonly used in addition to the heart rate are
I.average heart rate over a period of time
2.arrhythmia onset
3.arrhythmia probability density nmctions
Because of the limited power budget and area arrhythmia classifiers in ICDs are kept
extremely simple with respect to what could be achieved with more relaxed implementation
constraints As a result false positive pacing or defibrillation when none is required may be
high and error rates may reach
Artificial neural network techniques offer the potential of higher classification performance In
order to maintain as lower power consumption as possible VLSI micro-power implementation
techniques need to be considered
In this paper we discuss several classifier architectures and sub-threshold implementation
techniques Both single and dual chamber based classifications are considered
DATA
Data used in our experiments were collected from Electro-Physiological Studies EPS at hospitals in Australia and the UK. Altogether and depending on whether single or dual chamber
is considered data from over 70 patients is available Cardiologists from our commercial collaborator have labelled this data All tests were performed on a testing set that was not used in
classifier Mlding Arrhythmias recorded during EPS are produced by stimulation As a result
no natural transitions are captured
SINGLE CHAMBER CLASSIFICATION
We have evaluated several approaches for single chamber classification It is important to note
here that in the case of single chamber not all arrhythmias could be correctly classified not
even by truman experts This is because data from the RVA lead represents mainly the ventricular electrical activity and many atrial arrhythmias require atrial information for proper diagnosis
Jabri Pickard Leong Chi Flower and Xie
MULTI?LAYER PERCEPTRONS
Table shows the performance of multi-layer perceptrons MLP trained using vanilla backpropagation coojugate gradient and a specialized limited precision training algorithm that we
call Combined Search Algorithm Xie and labri The input to the MLP are features
extracted from the time domain There are three outputs representing three main groupings
NSR VT and VF. We do not have the space here to elaborate on the choice of the input features Interested readers are referenced to Oli and labri Leong and labri
TABLE Perfonnance of Multi-layer Perceptron Based CIMsifiers
Network
Training
Algorithm
Precision
Average
Performance
backprop
unlimited
conj.-grad
unlimited
CSA
bits
The summary here indicates that a high performance single chamber based c1assificatioo can
be achieved for ventricular arrhythmias It also indicates that limited precision training does
not Significantly degrade this performance In the case of limited precision MLP bits plus a
sign bit are used to represent network activities and weights
INDUCTION OF DECISION TREES
The same training data used to train the MLP was used to create a decision tree using the
program developed by Ross Quinlan derivative of the ID3 algorithm The resultant tree
was then tested and the performance was correct classification In order to achieve this
high performance the whole training data had to be used in the induction process windowing
disabled This has a negative side effect in that the trees generated tend to be large
The implementation of decision trees in VLSI is not a difficult procedure The problem however is that because of the binary decision process the branching thresholds are difficult to be
implemented in digital for large trees and even more difficult to be implemented in micropower analog The latter implementation technique would be possible if the induction process
can take advantage of the hardware characteristics in a similar way that in-loop training of
sub-threshold VLSI MLP achieves the same objective
DUAL CHAMBER BASED CLASSIFIERS
Two architectures for dual chamber based classifiers have been investigated Multi-ModuleNeural Networks and a hybrid Decision Tree/MLP The difference between the classifier architectures is a function of which arrhythmia group is being targetted for classification
ANN Based Classification for Heart Defibrillators
MULTI-MODULE NEURAL NETWORK
The multi-module neural network architecture aims at improving the performance with respect
to the classification of Supra-Ventricular Tachycardia The architecture is shown in
Figure with the classifier being the right most block
I
I
I
I
I
I
I
I
I
I
I
IU1
I
I
NSI
I
I
I
I
I
I
I
I
I
I
I
1I
I
MIn
I
I
I
VT
vr
FIGURE Multi-Module Neural Network Classifier
The idea behind this architecture is to divide the classification problem into that of discriminating between NSR and SVT on one hand and VF and VT on the other The details of the
operation and the training of this structured classifier can be fotllld in Oli and Jabri
In order to evaluate the performance of the MMNN classifier a single large MLP was also
developed The single large MLP makes use of the same input features as the MMNN and targets the same classes The performance comparison is shown in Table which clearly shows
that a higher performance is achieved using the MMNN
HYBRID DECISION TREEIMLP
The hybrid decision tree/multi-Iayer perceptron mimics the classification process as performed by cardiologists The architecture of the classifier is shown in Figure The decision
tree is used to produce a judgement on
1.The rate aspects of the ventricular and atrial channels
2.The relative timing between the atrial and ventricular beats
In parallel with the decision tree a morphology based classifier is used to perform template
matching The morphology classifier is a simple MLP with input that are signal samples sampled at half the speed of the normal sampling rate of the signal
The output of the timing and morphology classifiers are fed into an arbitrator which produces
the class of the arrhythmia being observed An out of classifier is used to smooth out the
Jabri Pickard Leong Chi Flower and Xie
TABLE Performance of Multi-Module Neural Network
Classifier and comparison with that of a single large MLP.
Rhythms
MMNN
Best
MMNN
Worst
Single
MLP
NSR
ST
svr
AT
AF
vr
VTF
VF
97
97
Average
SD
classification output by the arbitrator and to produce an averaged final output class Further
details on the implementation and the operation of the hybrid classifier can be fOtllld in Leong
and Jabri
This classifier achieves a high performance classification over several types of arrhythmia
Table shows the performance on a multi-patient database and indicate a performance of over
correct classification
born
RVA
plobr
QRS
Deuct
funl Hrtwo:rk Clusifiu
Hz
Albitntr
XoutofY
FIHAL
CLASS
fiom
plobr
Dtttct
TitniDi Clmif"lu
FIGURE Architecture of the hybrid dec~ion treelneural network classifier
MICROELECTRONIC IMPLEMENTATIONS
In all our classifier architecture investigations micro-electronic implementation consider
ations were a constant constraint Many other architectures that can achieve competitive performance were not discussed in this paper because of their unsuitability for low power/small
area implementation The main challenge in a low power/small area VLSI implementation of
classifiers similar to those discussed above is how to implement in very low power a MLP
architecture that can reliably learn and achieve a performance comparable to that of the func
ANN Based Classification for Heart Defibrillators
tiona similations Several design strategies can achieve the low power and small area objectives
TABLE Performance 01 the hybrid decision treelMLP dassifier for dual
chamber classification
SubClass
Class
NSR
svr
VT
VF
NSR
NSR
ST
NSR
24
svr
SVT
AT
SVT
52
AP
SVT
VT
VT
VT
VT
VF
VF
VTF
VF
Both digital and analog implementation techniques are bemg investigated and we report here
on our analog implementation efforts only Our analog implementations make use of the subthreshold operation mode of MOS transistors in order to maintain a very low power dissipation
MASTER PERTURBATOR CHIP
The architecture of this chip is shown in Figure Weights are implemented using a differ
ential capacitor scheme refreshed from digital RAM. Synapses are implemented as four quadrant Gilbert multipliers Pickard al The chip has been fabricated and is currently being
tested TIle building blocks have so far been successfully tested Two networks are implemented a total of synapses and a small single layer network The single layer network has been successfully trained to perfonn simple logic operations using the Weight
Perturbation algorithm Jabri and Flower
THE BOURKE CHIP
The BOURKE chip Leong and Jahri makes use of Multiplying Digital to Analog Con
verters to implement the synapses Weights are stored in digital registers All neurons were
implemented as external resistors for the sake of evaluation Figure shows the schematics
of a synapse The BOURKE chip has a small network and has been successfully tested
it was successfully trained to perfonn an XOR A larger version of this chip with a
network is being fabricated
Conclusions
We have presented in this paper several architectures for single and dual chamber arrhythmia
Jabri Pickard Leong Chi Flower and Xie
I
hi
ID
I
I
FIGURE Architecture of the Master Perturbator chip Schematics 01 the
BOURKE chip synapse implementation
classifiers In both cases a good classification performance was achieved In particular for the
case of dual chamber classification the complexity of the problem calls on more structured
classifier architectures Two microelectronic low power implementation were briefly presented Progress so far indicates that micro-power VLSI ANNs offer a technology that will
enable the use of powerful classification strategies in implantable defibrillators
Acknowledgment
Work presented in this paper was supported by the Australian Department of Industry Technology Commerce Telectronics Pacing Systems and the Australian Research Council

<<----------------------------------------------------------------------------------------------------------------------->>

title: 377-proximity-effect-corrections-in-electron-beam-lithography-using-a-neural-network.pdf

Proximity Effect Corrections in Electron Beam
Lithography Using a Neural Network
Robert C. Frye
AT Bell Laboratories
Mountain Avenue
Murray Hill NJ
Kevin D. Cummings
AT&T Bell Laboratories
Mountain Avenue
Murray Hill NJ
Edward A. Rietman
AT&T Bell Laboratories
Mountain Avenue
Murray Hill NJ
Abstract
We have used a neural network to compute corrections for images written
by electron beams to eliminate the proximity effects caused by electron
scattering Iterative methods are effective but require prohibitively
computation time We have instead trained a neural network to perform
equivalent corrections resulting in a significant speed-up We have
examined hardware implementations using both analog and digital
electronic networks Both had an acceptably small error of compared
to the iterative results Additionally we verified that the neural network
correctly generalized the solution of the problem to include patterns not
contained in its training set We have experimentally verified this approach
on a Cambridge Instruments EBMF exposure system
INTRODUCTION
Scattering imposes limitations on the minImum feature sizes that can be reliably
obtained with electron beam lithography Linewidth corrections can be used to control
the dimensions of isolated features intraproximity Sewell but meet with
little success when dealing with the same features in a practical context where they are
surrounded by other features interproximity Local corrections have been
proposed using a self-consistent method of computation for the desired incident dose
pattern parikh Such techniques require inversion of large matrices and
prohibitive amounts of computation time Lynch al have proposed an
analytical method for proximity corrections based on a solution of a set of approximate
equations resulting in a considerable improvement in speed
The method that we present here using a neural network combines the computational
simplicity of the method of Lynch with the accuracy of the self-consistent
methods The first step is to determine the scattered energy profile of the electron
beam which depends on the substrate structure beam size and electron energy This is
Present address Motorola Inc. Phoenix Corporate Research Laboratories East Elliot Rd. Tempe
AZ
Frye Cummings and Rietman
then used to compute spatial vanatlons in the dosage that result when a particular
image is scattered This can be used to iteratively compute a corrected image for the
input pattern The goal of the correction is to adjust the written image so that the
incident pattern of dose after scattering approximates the desired one as closely as
possible We have used this iterative method on a test image to form a training set for
a neural network The architecture of this network was chosen to incorporate the basic
mathematical structure as the analytical method of Lynch but relies on an
adaptive procedure to determine its characteristic parameters
CALCULATING PROXIMITY CORRECTED PATTERNS
We determined the radial distribution of scattered dose from a single pixel by using a
Monte-Carlo simulation for a variety of substrates and electron beam energies
Cummings As an example problem we looked at resist on a heavy metal
substrate These are of interest in the fabrication of masks for x-ray lithography For
a KeV electron beam this distribution or proximity function can be approximated
by the analytical expression
I
fer
where
a Jlm Jlm
Jlm
and
The unscattered image is assumed to be composed of an array of pixels For a
beam with a proximity function fer like the one given above the image after scattering
will be
ls(x,y
Io(x-m,y-n
which is the discrete convolution of the original image with the lineshape fer The
approach suggested by analogy with signal processing is to deconvolve the image by
an inverse filtering operation This method cannot be used however because it is
impossible to generate negative amounts of electron exposure Restricting the beam to
positive exposures makes the problem inherently nonlinear and we must rely instead
on an iterative rather than analytical solution
Figure shows the pattern that we used to generate a training set for the neural
network This pattern was chosen to include examples of the kinds of features that are
difficult to resolve because of proximity effects Minimum feature sizes in the pattern
ore Jlm and the overall image using Jlm pixels is pixels J.lm on a
side for a total of pixels The initial incident dose pattern for the iterative
correction of this image started with a relative exposure value of for exposed
pixels and for unexposed ones The scattered intensity distribution was computed
from this incident dose using the discrete two-dimensional convolution with the
summation truncated to a finite range roo For the example proximity function of
the scattered intensity is contained within a radius of J.lm pixels and this
value was used for roo The scattered intensity distribution was computed and compared
with the desired pattern of for exposed and for unexposed pixels The
Proximity Effect Corrections in Electron Beam Lithography
difference between the resulting scattered and desired distributions is the error This
error was subtracted from the dose pattern to be used for the next iteration However
since negative doses are not allowed negative regions in the correction were truncated
to zero
IIIII
IIIII
pixels
Figure Training pattern
Using this algorithm a pixel that receives a dosage that is too small will have a
negative error and on the next iteration its intensity will be increased Unexposed
pixels regions where resist is to be removed will always have some dosage
scattered into them from adjacent features and will consequently always show a
positive error Because the written dose in these regions is always zero rather than
negative it is impossible for the iterative solution to completely eliminate the error in
the final scattered distribution However the nonlinear exposure properties of the resist
will compensate for this Moreover since all exposed features receive a uniform dose
after correction it is possible to choose a resist with the optimal contrast properties for
the pattern
Although this iterative method is effective it is also time consuming Each iteration on
the test pattern required about hour to run on a based computer Four iterations
were required before the smallest features in the resist were properly resolved Even
the expected order of magnitude speed increase from a large mainframe computer is
not sufficient to correct the image from a full sized chip consisting of several billion
pixels The purpose of the neural network is to do these same calculations but in a
much shorter time
NETWORK ARCHITECTURE AND TRAINING
Figure shows the relationship between the image being corrected and the neural
network The correction for one pixel takes into account the image surrounding it
Since the neighborhood must include all of the pixels that contribute appreciable
scattered intensity to the central pixel being corrected the size of the network was
determined by the same maximum radius ro Ilm that characterized the
scattering proximity function The large number of inputs would be difficult to manage
in an analog network if these inputs were general analog signals but fortunately the
input data are binary and can be loaded into an analog network using digital shift
registers
Frye Cummings and Rietman
Figure shows a schematic diagram of the analog network The binary signals from
the shift registers representing a portion of the image were connected to the buffer
amplifiers through Kil resistors Each was connected to only one summing node
corresponding to its radial distance from the center pixel This stage converted the 19
19 binary representation of the image into analog voltages that represented the
radial distribution of the surrounding intensity The summing amplifier at the output
was connected to these nodes by variable resistors This resulted in an output that
was a weighted sum of the radial components
summing
node
corrected
pixel
pixel to be
corrected
Figure Network configuration
fixed
weights
binary
inputs
analog
output
buffer
amplifiers
Figure Schematic diagram of the analog network
Proximity Effect Corrections in Electron Beam Lithography
Functionally this network does the operation
VO\1t wr<Io>r
where wr are the weight coefficients set by the adjustable resistors and are the
average values of the pixel intensity at radius The form of this relationship is
identical to the one proposed by Lynch but uses an adaptive method rather than
an analytical one to detennine the coefficients wr
The prototype analog hardware network was built on a wire wrap board using
bit CMOS static shift registers and quad operational amplifiers for
the active devices The resistors in the first layer were KO thin-film resistors in
dual-in-line packages and had a tolerance of The ten adjustable resistors in the
second layer of the network were turn precision trimmers Negative weights were
made by inverting the sign of the voltage at the buffer amplifiers For comparison we
also evaluated a digital hardware implementation of this network It was implemented
on a floating point array processor built by Eighteen Eight Laboratories using an
AT chip operating at MFLOPs peak rate The mathematical operation
perfonned by the network is equivalent to a two-dimensional convolution of the input
image with an adaptively learned floating point kernel
The adjustable weight values for both networks were determined using the delta rule of
Widrow and Hoff For each pixel in the trial pattern of Figure I there was a
corresponding desired output computed by the iterative method Each pixel in the test
image its surroundings and corresponding analog corrected value computed by the
iterative method constituted a single learning trial and the overall image contained
of them We found that the weight values stabilized after two passes through
the test image
NEURAL NETWORK PERFORMANCE
The accuracy of both the analog and digital networks compared to the iterative
solution was comparable Both showed an average error for the test image of
and a maximum error of on any particular pixel The accuracy of the networks on
images other than the one used to train them was comparable averaging about
overall
Convolution with an adaptively-leamed kernel is itself a relatively efficient
computational algorithm The iterative method required hours to compute the
correction for the pixel example Equivalent results were obtained by
convolution in about minutes using the same computer Examination of the
assembled code for the software network showed that the correction for each pixel
required the execution of about times fewer instructions than for the iterative
method
The analog hardware generated corrections for the same example in seconds
Almost of this time was used for input/output operations between the network and
the computer It was the time required for the I/O. rather than the speed of the circuit
that limited the dynamic perfonnance of this system Clearly with improved I/O
hardware the analog network could be made to compute these corrections much more
quickly
Frye Cummings and Rietman
The same algorithm running on the digital floating point array processor perfonned the
correction for this example problem in seconds The factor of three improvement
over the analog hardware was primarily a result of the decreased time needed for I/O
in the DSP-based network The digital network was not appreciably more accurate than
the analog one indicating that the overall accuracy of operation was determined
primarily by the network architecture rather than by limitations in the implementation
These results are summarized in Table
Table Comparison of computational speed for various methods
METHOD
Iteration
Software network
Analog hardware network
Digital hardware network
SPEED
years
days
days
18 hours
EXPERIMENTAL VERIFICATION
Recently we have evaluated this method experimentally using a Cam bridge
Instruments exposure system Cummings The test image
was mm and contained Cambridge shapes and pixels The substrate
was silicon with Jlm of resist exposed at KeV beam energy The
range of the scattered electrons is more than three times greater for these conditions
than in the tests described above requiring a network about ten times larger The
neural network computations were done using the digital floating point array processor
and required about 18 hours to correct the entire image Input to the program was
Cambridge source code which was converted to a bit-mapped array corrected by the
neural network and then decomposed into new Cambridge source code
Figure shows SEM micrographs comparing one of the test structures written with
and without the neural network correction This test structure consists of a Jlm
square pad next to a Jlm wide line separated by a gap of Jlm. Note in the
uncorrected pattern that the line widens in the region adjacent to the large pad and the
webs of resist extending into the gap This is caused by excess dosage scattered into
these regions from the large pad In the corrected pattern the dosage in these regions
has been adjusted resulting in a uniform exposure after scattering and greatly
improved pattern resolution
CONCLUSIONS
The results of our trial experiments clearly demonstrate the computational benefits of a
neural network for this particular application The trained analog hardware network
performed the corrections more than times faster than the iterative method using
the same computer and the digital processor was times faster This technique is
readily applicable to a variety of direct write exposure systems that have the capability
to write with variable exposure times Implementation of the network on more
sophisticated computers with readily available coprocessors can directly lead to another
order of magnitude improvement in speed making it practical to correct full chip-sized
images
Proximity Effect Corrections in Electron Beam Lithography
The performance of the analog network suggests that with improved speed of I/O
between the computer and the network it would be possible to obtain much faster
operation The added flexibility and generality of the digital approach however is a
considerable advantage
I
Figure Comparison of a test structure written with and without correction
Acknowledgments
We thank S. Waaben and W. T. Lynch for useful discussions suggestions and
information and J. Brereton who assisted in building the hardware and trial patterns
for initial evaluation We also thank C. Biddick C. Lockstampfor S. Moccio and B.
Vogel for technical support in the experimental verification

<<----------------------------------------------------------------------------------------------------------------------->>

title: 342-an-analog-vlsi-splining-network.pdf

An Analog VLSI Splining Network
Daniel B. Schwartz and Vijay K. Samalam
GTE Laboratories Inc.
Sylvan Rd.
Waltham MA
Abstract
We have produced a VLSI circuit capable of learning to approximate arbitrary smooth of a single variable using a technique closely related to
splines The circuit effectively has knots space on a uniform grid and
has full support for learning The circuit also can be used to approximate
multi-variable functions as sum of splines
An interesting and as of yet nearly untapped set of applications for VLSI implementation of neural network learning systems can be found in adaptive control and
non-linear signal processing In most such applications the learning task consists
of approximating a real function of a small number of continuous variables from
discrete data points Special purpose hardware is especially interesting for applications of this type since they generally require real time on-line learning and there
can be stiff constraints on the power budget and size of the hardware Frequently
the already difficult learning problem is made more complex by the non-stationary
nature of the underlying process
Conventional feed-forward networks with sigmoidal units are clearly inappropriate
for applications of this type Although they have exhibited remarkable performance
in some types of time series prediction problems for example Wiegend and
Atlas their learning rates in general are too slow for on-line learning On-line
performance can be improved most easily by using networks with more constrained
architecture effectively making the learning problem easier by giving the network a
hint about the learning task Networks that build local representations of the data
such as radial basis functions are excellent candidates for these type of problems
One great advantage of such networks is that they require only a single layer of
units If the position and width of the units are fixed the learning problem is linear
An Analog VLSI Splining Network
in the coefficients and local By local we mean the computation of a weight change
requires only information that is locally available to each weight a highly desirable
property for VLSI implementation If the learning algorithm is allowed to adjust
both the position and width of the units then many of the advantages of locally
tuned units are lost
A number of techniques have been proposed for the determination of the width
and placement of the units One of the most direct is to center a unit at every
data point and to adjust the widths of the units so the receptive fields overlap
with those of neighboring data points Broom head The proliferation of
units can be limited by using unsupervised clustering techniques to clump the data
followed by the allocation of units to fit the clumps Moody Others have
advocated assigning new units only when the error on a new data point is larger than
a threshold and otherwise making small adjustments in the weights and parameters
of the existing units Platt All of these methods suffer from the common
problem of requiring an indeterminate quantity of resources in contrast with the
fixed resources available from most VLSI circuits Even worse when used with
non-stationary processes a mechanism is needed to deallocate units as well as to
allocate them The resource allocation/deallocation problem is a serious barrier to
implementing these algorithms as autonomous VLSI microsystems
A Splining Network
To avoid the resource allocation problem we propose a network that uses all of
its weights and units regardless of the problem We avoid over parameterization
of the training data by building constraints on smoothness into the network thus
reducing the number of degrees of freedom available to the training process In
its simplest guise the network approximates arbitrary I-d smooth functions with a
linear superposition of locally tuned units spaced on a uniform grid
LWifC7(z
where is the radius of the unit's receptive field and the Wi are the weights fC7 is a
bump of width such as a gaussian or a cubic spline basis function Mathematically
the network is closely related to function approximation using B-splines Lancaster
with uniformly spaced knots However in B-spline interpolation the overlap
of the basis functions is normally determined by the degree of the spline whereas
we use the degree of overlap as a free parameter to constrain the smoothness of
the network's output As mentioned earlier the network is linear in its weights
so gradient descent with a quadratic cost function LMS is an effective training
procedure
The weights needed for this network can easily be implemented in CMOS with an
array of transconductance amplifiers The amplifiers are wired as voltage followers
with their outputs tied together and the weights are represented by voltages lti
at the non-inverting inputs of the amplifiers If the outputs of the locally tuned
units are represented by unipolar currents Ii these currents can be used to bias the
Schwartz and Samalam
transconductance amplifiers and the result is
t7
You
Ei IiYi
L"i
Ii
provided that care is taken to control the non-linearities of the amplifiers However
while the weights have a simple implementation in analog VLSI circuitry the input
units du not A number of circuits exist whose transfer characteristics can be shaped
to be a suitable bump but none of those known to the authors allow the width of
the bump to be adjusted over a wide range without the use of resistors
Generating the Receptive Fields
Input units with tunable receptive fields can be generated quite efficiently by breaking them up into two layers of circuitry as shown in figure The input layer place
encodes the input signal only one or perhaps a small cluster of units is active
at a time The output of the place encoding units either injects or controls the
output
weight
spreading
layer
place
encoding
Input
Figure An architecture that allows the width and shape of the receptive fields to
be varied over a wide range The elements of the spreading layer are passive and
can sink current to ground
injection of current into the laterally connected spreading layer The elements in
the spreading layer all contain ground terminals and the current sunk by each one
determines the bias current applied to the associated weight Clearly the distribution of currents flowing to ground through the spreading layer form a smooth bump
such that when excitation is applied to tap of the spreading layer
Ii
where is the bump called for by equation In our earliest realizations of
this network the input layer was a crude flash A-to-D converter and the input
to the circuit was analog In the current generation the input is digital with the
place encoding performed by a conventional address decoder If desired input
quantization can be avoided by using a layer of amplifiers that generate smooth
bumps of fixed width to generate the input place encoding
An Analog VLSI Splining Network
The simplest candidate to implement the spreading layer in conventional CMOS
is a set of diode connected n-channel transistors laterally connected by n-channel
pass transistors The gate voltages of the diode connected transistors determine
the bias currents Ii of the weights Ignoring the body effect and assuming weak
inversion in the current sink this type of networks tends to gives bumps with rather
sharp peaks Ii Ioe aul where Iii is the distance from the point where the
excitation is applied Figure shows a more sophisticated version of this circuit
in which the output of the place encoding units applies excitation to the spreading
network through a p-channel transistor The shape of the bumps can be softened by
to weights
bias
voltages
from place encoder
Figure A schematic of a section of the spreading layer Roughly speaking the
n-channel pass transistor controls the extent of the tails of the bumps and the
p-channel pass transistor and the cascode transistor control its width
limiting the amount of current drawn by the current sinks with an n-channel cascode
transistor in series with the current sink Some experimental results for this type of
circuit are shown in figure More control can be obtained by using complementary
pass transistors The use of p-channel pass transistors alone unexpectedly results
in bumps that are nearly square figure These can be smoothed by using a
using both flavors of pass transistor simultaneously figure
The Weights
As described earlier the implementation of the output weights is based on the
computation of means by the well known follower-aggregation circuit With typical
transconductance amplifiers this averaging is linear only when the voltages being
averaged are distributed over a voltage range of no more than a few time UQ kT/e
in weak inversion In the circuits described here the linear range has been widened
to nearly a volt by reducing the transconductance of the readout amplifiers through
the combination of low width to length ratio input transistors and relatively large
tail currents
The weights Vi are stored on MOS capacitors and are programmed by the gated
transconductance amplifier shown in figure Since this amplifier computes the
Schwartz and Samalam
I
I
I
I
I
I
I
I
I
I
I
I
I
I
cCD
I
I
I
Tap Number
Figure Experimental measurements of the receptive field shapes obtained from
different types of networks n-channel transistors for several gate voltages
p-channel transistors for several gate voltages Both n-channel and p-channel
pass transistors
exdmkm
Figure Schematic of an output weight including the circuitry to generate weight
updates To minimize leakage and charge injection simultaneously the pass transistors used to gate the weight change amplifier are of minimum size and a separate
transistor turns off the output transistors of the amplifier
difference between the target voltage and the actual output of the network the
learning rule is just LMS
where is the capacitance of the storage capacitor and is the duration of weight
changes The transconductance gi of the weight change amplifier is determined by
the strength of excitation current from the spreading layer gi oc Ii in weak inversion
Since the weight changes are governed by strengths of the excitation currents from
the spreading layer clusters of weights are changed at a time This enhances the
fault tolerance of the circuit since the group of weights surrounding a bad one can
compensate for it
An Analog VLSI Splining Network
Experimental Evaluation
Several different chips have been fabricated in p-well CMOS and tested to evaluate the principles described here The most recent of these has weights arranged
in a 64 matrix connected to form a one dimensional array The active area of
this chip is The input signal is digital with the place encoding
performed by a conventional address decoder To maximize the flexibility of the
chip the excitation is applied to the spreading layer by a register located in each
cell By writing to multiple registers between resets the spreading layer can be excited at multiple points simultaneously This feature allows the chip to be treated
as a single I-dimensional spline with weights or for example as the sum of
four distinct I-dimensional splines each made up of weights One of the most
noticeable virtues of this design is the simplicity of the layout due to the absence
of any dear distinction between weights and units The primitive cell consists of
a register a piece of the spreading network a weight change amplifier a storage
capacitor and output amplifier All but a tiny fraction of the chip is a tiling of
this primitive cell The excess circuitry consists of the address decoders a timing
circuit to control the duration of weight changes and some biasing circuitry for the
spreading layer
To execute LMS learning the user need only provide a sequence of target voltages
and a current proportional to the duration of weight changes Under reasonable
operating conditions a weight updates cycle takes less than implying a weight
change rate of connections/second The response of the chip to a single
weight change after initialization is shown in in figure One feature of this plot
is striking even though the distribution of offsets in the individual amplifiers has
a variance of the ripple in the output of the chip is about a ImV. For some
computations it appears the limiting factor on the accuracy of the chip is the rate
of weight decay about IOmV/s
As a more strenuous test of the functionality of the chip we trained it to predict
chaotic time series generated by the well know logistic equation
Xt+l a
Some experimental results for the mean prediction error are shown in figure
In these experiments a mean prediction error of is achieved which is well
above the intrinsic accuracy of the circuit A detailed examination of the error
rate as a function of the size and shape of the bumps indicates that the problem
lies in the long tails exhibited by the spreading layer when the n-channel pass
transistors are turned on This tail falls off very slowly due to the body effect
One remedy to this problem is to actively bias the gates of the n-channel pass
transistors to be a programmed offset above their source voltages Mead A
simpler solution is to subtract a fixed current from each of the bias current defined
by the spreading layer This solution costs a mere transistors and has the added
benefit of guaranteeing that the bumps will always have a finite support
Conclusion
We have demonstrated that neural network learning can be efficiently mapped onto
analog VLSI provided that the network architecture and training procedure are
Schwartz and Samalam
co
ci
ci
ci
as
ci
ci
ci
input value
70
time
Figure Some experimental results from a splining circuit The response of
the circuit to learning one data point after initialization of the weights to a constant
value Experimental mean prediction while learning a chaotic time series
tailored to match the constraints imposed by VLSI Besides the computational
speed and low power consumption that follow directly from this mapping
onto VLSI the circuit also demonstrates intrinsic fault tolerance to defects in the
weights
Acknowledgements
This work was initially inspired by a discussion with A. G. Barto and R. S. Sutton
A discussion with J. Moody was also helpful

<<----------------------------------------------------------------------------------------------------------------------->>

title: 413-an-analog-vlsi-chip-for-finding-edges-from-zero-crossings.pdf

An Analog VLSI Chip for Finding Edges
from Zero-crossings
Wyeth Bair
Christof Koch
Computation and Neural Systems Program
Caltech
Pasadena CA
Abstract
We have designed and tested a one-dimensional 64 pixel analog CMOS
VLSI chip which localizes intensity edges in real-time This device exploits
on-chip photoreceptors and the natural filtering properties of resistive networks to implement a scheme similar to and motivated by the Difference
of Gaussians DOG operator proposed by Marr and Hildreth Our
chip computes the zero-crossings associated with the difference of two exponential weighting functions If the derivative across this zero-crossing
is above a threshold an edge is reported Simulations indicate that this
technique will extend well to two dimensions
INTRODUCTION
The zero-crossings of the Laplacian of the Gaussian,V are often used for detecting edges Marr and Hildreth argued that the Mexican-hat shape of the
operator can be approximated by the difference of two Gaussians In this
spirit we have built a chip that takes the difference of two resistive-network smoothings of photoreceptor input and finds the resulting zero-crossings The Green's function of the resistive network a symmetrical decaying exponential differs from the
Gaussian filter Figure shows the Mexican-hat shape of the DOG superimposed
on the witch-hat shape of the difference of exponentials DOE filter implemented
by our chip
This implementation has the particular advantage of exploiting the smoothing operation performed by a linear resistive network shown in Figure In such a network
data voltages are applied to the nodes along the network via conductances and
the nodes are connected by resistances R. Following Kirchhoff's laws the network
Bair and Koch
node voltages settle to values such that power dissipation is minimized One
may think of the network node voltages as the convolution of the input with the
symmetrical decaying exponential filter function The characteristic length of this
filter function is approximately where is the data conductance and
the network resistance
Figure The Mexican-hat shape of the difference of Gaussians dotted and the
witch-hat shape of the filter implemented by our chip
Such a network is easily implemented in silicon and avoids the burden of additional
circuitry which others have used to implement Gaussian kernels Our simulations
with digitized camera images show only minor differences between the zero-crossings
from the DOE filter and those from the DOG.
Figure I-D resistive network
ANALOG VLSI IMPLEMENTATION
This chip was implemented with a CMOS n-well process available through the
MOSIS silicon foundry Intensity edges are detected using four stages of circuitry
photoreceptors capture incoming light a pair of I-D resistive networks smooth the
input image transconductance amplifiers subtract the smoothed images and digital
circuitry detects zero-crossings Figures and show block diagrams for two pixels
of the 64 pixel chip
An Analog VLSI Chip for Finding Edges from Zero-crossings
VP.
VP.
R1
Vl
V2.
Vli
R1
R2
R2
Figure Block circuit diagram for two of 64 pixels as described in Section
Bair and Koch
Processing begins at a line of photoreceptors spaced apart which encode the
logarithm of light intensity as a voltage shown in Figure The set of voltages from the photoreceptors are reported to corresponding nodes of two resistive
networks via transconductance amplifiers connected as followers The followers
voltage biases VGI and can be adjusted off-chip to independently set the data
conductances for each resistive network The network resistors are implemented
as Mead's saturating resistors Mead Voltage biases VRI and VR2 allow
independent off-chip adjustment of the two network resistances The data conductance and network resistance values determine the space constant of the smoothing
filter which each network implements The sets of voltages VI and shown in
Figure represent the two filtered versions of the image Wide-range transconductance amplifiers Mead produce currents I proportional to the difference
Vl V2.
Figure Zero-crossing detection and threshold circuitry
Figure shows the final stage of processing which detects zero-crossings in the
sequence of currents I and implements a threshold on the slope of those zerocrossings Currents Ii and Ii+l charge or discharge the inputs of an exclusive
OR gate The output of this gate is the first input to a NAND gate which is
used to implement the threshold A current proportional to the magnitude of the
difference Ii I charges the second input of the NAND gate while a threshold
current discharges this input If the charging current representing the slope of the
zero-crossing is greater than the threshold current set off-chip by the bias voltage
V,hruh this NAND input is charged to logical otherwise this input is discharged
to logical The output of the NAND gate Zi indicates the presence logical
or the absence logical of a zero-crossing with slope greater than Ithruh
A final stage of circuitry is used to multiplex the sequence of 63 bits and
corresponding currents Ii I indicating the slope of the zero-crossings
An Analog VLSI Chip for Finding Edges from Zero-crossings
BEHAVIOR
We tested the behavior of the chip by placing a small lens above the silicon wafer
to focus an image onto the array of photoreceptors The input light profile that we
used is shown in Figure Figure is an oscilloscope trace showing the smoothed
voltages VI and V2 of Figure corresponding to the filtered versions of the image
The difference of these two smoothed voltage traces is shown in Figure Arrows
indicate the locations of two zero-crossings which the chip reports at the output
The reported zero-crossings accurately localize the positions of the edges in the
image The trace in Figure 5c crosses zero at other locations but zero-crossings
with slope less than the adjustable threshold are masked by the circuitry shown in
Figure This allows for noise and imperfections in the circuitry and can be used
to filter out weaker edges which are not relevant to the application
Figure tj shows the response when two fingers are held one meter from the lens
and swept across the field of view The fingers appear as bright regions against a
darker background The chip accurately localizes the four edges two per finger as
indicated by the pulses below each voltage trace As the fingers move quickly back
and forth across the field of view the image and the zero-crossings follow the object
with no perceived delay The measured response time of the chip to the appearance
of a detectable discontinuity in light intensity varies from about 100j.lsec in bright
indoor illumination to about 10msec in a dark room The time constant is longer
for lower illumination due to the design of the logarithmic photoreceptor Mead
The chip has been proven to be a reliable and robust edge detector through its
use in two systems It provides data for a system designed at the Hughes Aircraft
Artificial Intelligence Center which tracks edges and reports their velocities at over
Also we have built a hand-held battery powered device which displays the
locations of edges on a bank of 63 LEDs This device accurately detects edges
in many different environments ranging from a dimly lit room to bright outdoor
sunlight
SIMULATIONS OF A VERSION
We have used a computer simulation of rectangular networks of ideal linear resistors
to test the extension of this technique in two dimensions Results indicate that the
zero-crossings from the difference of two symmetrical exponential filters are qualitatively similar to those from the DOG. Figure compares the zero-crossing from
a difference of Gaussians filter left to those from a difference of resistive networks
filter right For the DOG a Gaussian of pixels is subtracted from a pixels For the resistive networks a filter of characteristic length
sian of
was subtracted from one with characteristic length Weaker zero-crossings
are masked from both output images by thresholding on the slope to emphasize
comparison of the stronger edges
Bair and Koch
I
Figure Chip response to a light bar stimulus
Figure Chip response to two moving stimuli
An Analog VLSI Chip for Finding Edges from Zero-crossings
Figure Zero--crossings from the difference of two Gaussians left and similar
output from a difference of decaying exponentials right
CONCLUSION
Our analog VLSI chip shows that finding the thresholded zero--crossings of the
difference of exponential filters is a robust technique for localizing intensity edges in
real-time The robust behavior of the chip in systems to track edges and determine
velocity demonstrates the usefulness of implementing simple algorithms in analog
VLSI and the advantages of avoiding large more general digital systems for these
purposes
Acknowledgements
Many thanks to Carver Mead Our laboratory is partially supported by grants
from the Office of Naval Research the Rockwell International Science Center and
the Hughes Aircraft Artificial Intelligence Center Wyeth Bair is supported by a
National Science Foundation Graduate Fellowship Thanks also to Steve DeWeerth
and John Harris

<<----------------------------------------------------------------------------------------------------------------------->>

title: 110-adaptive-neural-networks-using-mos-charge-storage.pdf

Adaptive Neural Networks Using MOS Charge Storage
D. B. Schwartz R. E. Howard and W. E. Hubbard
AT&T Bell Laboratories
Crawfords Corner Rd.
Holmdel N.J.
Abstract
MOS charge storage has been demonstrated as an effective method to store
the weights in VLSI implementations of neural network models by several
workers However to achieve the full power of a VLSI implementation of
an adaptive algorithm the learning operation must built into the circuit We
have fabricated and tested a circuit ideal for this purpose by connecting a
pair of capacitors with a CCD like structure allowing for variable size weight
changes as well as a weight decay operation A CMOS version achieves
better than bits of dynamic range in a area A chip
based upon the same cell has weights on a die and is
capable of peak learning rates of at least weight changes per second
Adaptive Networks
Much of the recent excitement about neural network models of computation has
been driven by the prospect of new architectures for fine grained parallel computation using analog VLSI Adaptive systems are espescially good targets for analog
VLSI because the ada.ptive process can compensate for the inaccuracy of individual
devices as easily as for the variability of the signal However silicon VLSI does not
provide us with an ideal solution for weight storage Among the properties of an
ideal storage technology for analog VLSI adaptive systems are
The minimum available weight change must be small The simplest adaptive algorithms optimize the weights by minimizing the output error with a
steepest descent search in weight space Iterative improvement algorithms
such as steepest descent are based on the heuristic assumption of better
weights being found in the neighborhood of good ones a heuristic that fails
when the granularity of the weights is not fine enough In the worst case the
resolution required just to represent a function can grow exponentially in the
dimension of the input space
The weights must be able to represent both positive and negative values and
the changes must be easily reversible Frequently the weights may cycle up
and down while the adaptive process is converging and millions of incremental
changes during a single training session is not unreasonable If the weights
cannot easily follow all of these changes then the learning must be done off
chip
Now at GTE Laboratories Sylvan Waltham Mass dbs@gte.com%relay.cs.net
2For example see the papers by Mann and Gilbert Walker and Akers and Murray in
this proceedings
Schwartz Howard and Hubbard
The parallelism of the network can be exploited to the fullest only if the
mechanism controlling weight changes is simple enough to be reproduced at
each weight Ideally the change is determined by some easily computed combination of information local to each weight and signals global to the entire
system This type of locality which is as much a property of the algorithm as
of the hardware is necessary to keep the wiring cost associated with learning
small
Weight decay Wi
aw with a is useful although not essential Global
decay of all the weights can be used to extend their dynamic range by rescaling
when the average magnitude becomes too large Decay of randomly chosen
weights can be used both to control their magnitude and to help gradient
searches escape from local minima
To implement an analog storage cell with MOS VLSI the most obvious choices
are non-volatile devices like floating gate and MNOS transistors multiplying DAC's
with conventional digital storage and dynamic analog storage on MOS capacitors
Most non-volatile devices rely upon electron tunneling to change the amount of
stored charge typically requiring a large amount of circuitry to control weight
changes DAC's have already proven themselves in situations where bits or less
of resolution are sufficient but higher resolution is prohibitively expensive in
terms of area We will show the disadvantage of MOS charge storage its volatility
is more than outweighed by the resolution available and ease of making weight
changes
Representation of both positive and negative weights can be obtained by storing
the weights Wi differentially on a pair of capacitors in which case
Differential storage can be used to obtain some degree of rejection of leakage and
can guarantee that leakage will reduce the magnitude of the weights as compared
with a scheme where the weights are defined with respect to a fixed level in which
case as a weight decays it can change signs A constant common mode voltage also
eases the design constraints on the differential input multiplier used to read out the
weights An elegant way to manipulate the weights is to transfer charge from one
capacitor to the other keeping constant the total charge on the system and thus
maximizing the dynamic range available from the readout circuit
Weight Changes
Small packets of charge can easily be transferred from one capacitor to the other by
exploiting charge injection a phenomena carefully avoided by designers of switched
capacitor circuits as a source of sampling error An example of a
storage cell with the simplest configuration for a charge transfer system is shown
in figure A pair of MOS capacitors are connected by a string of narrow MOS
transistors a long one to transfer charge and two minimum length ones to isolate
Adaptive Neural Networks Using MOS Charge Storage
TA
TP
TC
ru
IL
Ul~v
I
TA
TP
TA
TM
I
ru
TCP
TI
TCM
TA
TM
UlJ
I
I
Figure The simplest storage cell with provisions for only a single size increment decrement operations and no weight decay A more sophisticated cell with
facilities for weight decay By suitable manipulation of the clock signals the two
charge transfer transistors can be used to obtain different sizes of weight changes
Both circuits are initialized by turning on the access transistors TA and charging
the capacitors up to a convenient voltage typically Vnn
the charge transfer transistor from the storage nodes For the sake of discussion we
can treat the isolation transistors as ideal switches and concentrate on the charge
transfer transistor that we here assume to be an n-channel device To increase the
weight See figure the charge transfer transistor and isolation transistor
attached to the positive storage node are turned on When the system has
reached electrostatic equilibrium the charge transfer transistor is disconnected
from the plus storage node by turning off TP and connected to the minus storage
node by turning on TM. If the charge transfer transistor TC is slowly turned off the
mobile charge in its channel will diffuse into the minus node lowering its voltage
A detailed analysis of the charge transfer mechanism has been given elsewhere
but for the purpose of qualitative understanding of the circuit the inversion charge
in the charge transfer transistor's channel can be approximated by
qNinv
Cox(VG
VTE
where VT is the effective threshold voltage and Cox the gate to channel capacitance
of the charge transfer transistor The effective threshold voltage is then given by
where VTO is the threshold voltage in the absence of body effect the fermi level
Vs the source to substrate voltage and the usual body effect coefficient An even
Schwartz Howard and Hubbard
rougher model can be obtained by linearizing the body effect term
where Cell co.n tains both the gate oxide capacitance and the effects of parasitic
capacitance and I I Within the linearized approximation the change
in voltage on a storage node with capacitance Cstore after transfers is
Vn Va
VT exp(-an
with a Cell Cstore and where Va is the initial voltage on the storage node Due
to the dependence of the size of the transfer on the stored voltage when the transfer
direction is reversed the increment size changes unless the stored voltages on the
capacitors are equal This can be partially compensated for by using complementary
pairs of p-channel and n-channel charge transfer transistors in effect using a string
of transmission gates to perform charge transfers A weight decay operation can be
introduced by using the more complex string of charge transfer transistors shown
in figure lb A weight decay is initiated by turning off the transistor in the middle
of the string and turning on all the other transistors When the two sides of
the charge transfer string have equilibrated with their respective storage nodes the
connections to the storage nodes TM and TP are turned off and the two cha.rge
transfer transistors TCP and TCM are allowed to exchange charge by turning
on the transistor TI which separates them When two equal charge packets have
been obtained TI is turned off again and the charge packets held by TCP and TCM
are injected back into the storage capacitors The resulting change in the stored
weight is
tl vdecay CCeff
ox
which corresponds to multiplying the weight by a constant a as desired Besides
allowing for weight decay the more complex charge string shown in figure Ib ca.n also
be used to obtain different size weight changes by using different clock sequences
Experimental Evaluation
Test chips have been fabricated in both and CMOS using the AT&T
Twin Tub technology[ll To evaluate the properties of an individual cell especially
the charge transfer mechanism an isolated test structure consisting of five storage
cells was built on one section of the chip The storage cells were differentially read out by two quadrant transconductance amplifiers whose input-output
characteristics are shown in figure By using the bias current of the amplifiers as
an input the amplifiers were used as two quadrant multipliers Since many neural
network models call for a sigmoidal nonlinearity no attempt was made to linearize
the operation of the multiplier The output currents of the five multipliers were
summed by a single output wire and the voltages on each of the ten capacitors were
Adaptive Neural Networks Using MOB Charge Storage
Input Voltage
Figure A family of transfer characteristics from one of the transconductance
multipliers for several different values of stored weight The different branches of
the curves are each separated by ten large charge transfers No attempt was made
to linearize the input/output characteristic since many neural network models call
for non-linearities
buffered by voltage followers to allow for detailed examination of the inner workings
of the cell
After trading off between hold time resolution and area we decided upon
long charge transfer transistors and storage capacitors with technology
based upon the minimum channel width of For a long channel and a
gate to source voltage the channel transit time To is approximately ns and
charge transfer clock frequencies exceeding 1oMHz are possible without measurable
pumping of charge into the substrate The wide access transistors were
long leading to leakage rates from the individual capacitors of about of the
stored value in limited by surface leakage in our unpassivated test structures
Even with uncapped wafers the leakage was small enough to allow all the tests
described here to be made without special provisions for environmental control of
either temperature or humidity As mentioned earlier the more complex set of
charge transfer transistors needed to introduce weight decay can also be used to
obtain several different size of charge transfers a small weight change by using
the two long transistors in sequence and a coarse one by treating the two long
transistors and the isolation transistor separating them as a single device Using
the small weight changes the worst case resolution was bits near
and the results where in excellent agreement with the predictions of equation
Schwartz Howard and Hubbard
as
as
Q.
Charge transfers
Figure The voltage on the two storage capacitors when the weight is initially
set to saturation using large increments and then reduced back towards zero using
weight decay The granularity of the curves is an experimental artifact of the digital
voltmeter's resolution
using the effective capacitance as a fitting parameter In the figure we use large
charge transfers to quickly increment the weight up to its maximum value and then
reduce it back to zero with weight decays demonstrating the expected exponential
dependence of the stored voltage on the number of weight decays Even under
repeated cycling up and down through the entire differential voltage range of the
cell the total amount of charge on the cell remained constant for frequencies under
with the exception of the expected losses due to leakage
The long term goal of this work is to develop analog VLSI chips that are complete
learning machines capable of modified their own weights when provided with input
data and some feedback based on the output of the network However the study
of learning algorithms is in a state of flux and few if any algorithms have been
optimized for VLSI implementation Rather than cast an inappropriate algorithm
in silicon we have designed our first chips to be used as adaptive systems with
an external controller allowing us to develop algorithms that are appropriate for
the medium once we understand its properties The networks are organized as
rectangular matrix multipliers with voltage inputs and current outputs with 46
inputs and 24 outputs in a 96 pin package for the chip Since none of the
analog input/output lines of the chip are multiplexed larger and more complicated
networks can be built by cascading several chips
To the digital controller the chip looks like a static RAM with some
extra clock inputs to drive the charge transfers The charge transfer clock signals are
Adaptive Neural Networks Using MOS Charge Storage
distributed globally and are connected to the individual strings of charge transfer
transistors through a pair of cross bar switches controlled by two bits of static
RAM local to each cell The use of a pair of cross bar switches is necessitated
by the faciltities for weight decay if the simpler charge transfer string shown in
figure la were used then only a single switch would be needed When both a
cell's RAMs are zeroed the global charge transfer lines are not connected to the
charge transfer transistors The global lines are connected to the individual strings
of charge transfer transistors either normally or in reverse depending upon which
RAM cell contains a one By reversing the order of the signals on the charge
transfer lines a weight change can also be reversed Neglecting the dependence of
the size of the charge transfer upon stored weight the RAM's represent a weight
change vector ij with components f).wij Once a weight change vector
has been written serially to the RAM's the weight changes along that vector are
made in parallel by manipulating the charge transfer lines This architecture is
also a powerful way to implement programable networks of fixed weights since an
arbitrary matrix of bit weights can be written to the chip in a few milliseconds
or less if an efficient decomposition of the desired weight vector into global charge
transfers is made In view of the speed with which the chip can evaluate the output
of a network an overhead of less than a percent for a refresh operation is acceptable
in many applications
Conclusions
We have implemented a generic chip to facilitate studying adaptive networks by
building them in analog VLSI By exploiting the well known properties of charge
storage and charge injection in a novel way we have achieved a high enough level of
complexity weights and bits of analog depth to be interesting in spite
of the limitation to a modest die size required by a multi-project
fabrication run If the cell were optimized to represent fixed weight networks by
eliminating weight decay and bi-directional weight changes the density could easily
be increased by a factor of two with no loss in resolution Once a weight change
vector has been written to the RAM cells charge transfers can be clocked at a
rate of 2M chip corresponds to a peak learning rate of updates/second
exceeding the speeds of digital neurocomputers based upon DSP chips by two
orders of magnitude
Acknowledgements
A large group of people assisted the authors in taking this work from concept to
silicon a few of whom we single out for mention here The IDA design tools used
for the layouts were provided and supported by D. D. Hill and D. D. Shugard at
Murray Hill and the process was supported by D. Wroge and R. Ashton The
first author wishes to acknowledge helpful discussions with H. P. Graf S. Mackie
and G. Taylor with special thanks to R. G. Swartz
Schwartz Howard and Hubbard

<<----------------------------------------------------------------------------------------------------------------------->>

title: 1541-active-noise-canceling-using-analog-neuro-chip-with-on-chip-learning-capability.pdf

Active Noise Canceling using Analog NeuroChip with On-Chip Learning Capability
Jung-Wook Cho and Soo-Young Lee
Computation and Neural Systems Laboratory
Department of Electrical Engineering
Korea Advanced Institute of Science and Technology
Kusong-dong Yusong-gu Taejon Korea
sylee@ee.kaist.ac.kr
Abstract
A modular analogue neuro-chip set with on-chip learning capability is
developed for active noise canceling The analogue neuro-chip set
incorporates the error backpropagation learning rule for practical
applications and allows pin-to-pin interconnections for multi-chip
boards The developed neuro-board demonstrated active noise
canceling without any digital signal processor Multi-path fading of
acoustic channels random noise and nonlinear distortion of the loud
speaker are compensated by the adaptive learning circuits of the
neuro-chips Experimental results are reported for cancellation of car
noise in real time
INTRODUCTION
Both analog and digital implementations of neural networks have been reported
Digital neuro-chips can be designed and fabricated with the help of well-established
CAD tools and digital VLSI fabrication technology Although analogue neurochips have potential advantages on integration density and speed over digital chips[2
they suffer from non-ideal characteristics of the fabricated chips such as offset and
nonlinearity and the fabricated chips are not flexible enough to be used for many
different applications Also much careful design is required and the fabricated chip
characteristics are fairly dependent upon fabrication processes
For the implementation of analog neuro-chips there exist two different approaches
with and without on-chip learning capability Currently the majority of analog
neuro-chips does not have learning capability while many practical applications
require on-line adaptation to continuously changing environments and must have online adaptation learning capability Therefore neuro-chips with on-chip learning
capability are essential for such practical applications Modular architecture is also
Active Noise Canceling with Analog On-Chip Learning Neuro-Chip
advantageous to provide flexibility of implementing many large complex systems from
same chips
Although many applications have been studied for analog neuro-chips it is very
important to find proper problems where analog neuro-chips may have potential
advantages over popular DSPs We believe applications with analog input/output
signals and high computational requirements are those good problems For example
active noise controls and adaptive equalizers are good applications for analog
neuro-chips
In this paper we report a demonstration of the active noise canceling which may have
many applications in real world A modular analog neuro-chip set is developed with
on-chip learning capability and a neuro-board is fabricated from multiple chips with
PC interfaces for input and output measurements Unlike our previous implementations
for adaptive equalizers with binary outputs both input and output values are
analogue in this noise canceling
xl
Figure Block diagram of a synapse cell
Figure Block diagram of a neuron cell
ANALOG NEURO-CHIP WITH ON-CHIP LEARNING
We had developed analog neuro-chips with error backpropagation learning capability
With the modular architecture the developed analog neuro-chip set consists of a
synapse chip and a neuron The basic cell of the synapse chip is shown in
Figure Each synapse cell receives two inputs pre-synaptic neural activation
and error correction term and generates two outputs feed-forward signal wx and
back-propagated error Also it updates a stored weight by the amount of
Therefore a synapse cell consists of three multiplier circuits and one analogue storage
for the synaptic weight Figure shows the basic cell in the neuron chip which collects
signals from synapses in the previous layer and distributes to synapses in the following
layer Each neuron body receives two inputs post-synaptic neural activation and
back-propagated error from the following layer and generates two outputs
Sigmoid-squashed neural activation and a new backpropagated error multiplied by
a bell-shaped Sigmoid-derivative The backpropagated error may be input to the
synapse cells in the previous layer
To provide easy connectivity with other chips the two inputs of the synapse cell are
represented as voltage while the two outputs are as currents for simple current
summation On the other hand the inputs and outputs of the neuron cell are represented
as currents and voltages respectively For simple pin-to-pin connections between chips
one package pin is maintained to each input and output of the chip No time
Cho and Lee
multiplexing is introduced and no other control is required for multi-chip and multilayer systems However it makes the number of package pins the main limiting factor
for the number of synapse and neuron cells in the developed chip sets
Although many simplified multipliers had been reported for high-density integration
their performance is limited in linearity resolution and speed For on-chip learning it
is desirable to have high precision and a faithful implementation of the 4-quadranr
Gilbert multipliers is used Especially the mUltiplier for weight updates in the synapse
cell requires high precision.[9 The synaptic weight is stored on a capacitor and an
MaS switch is used to allow current flow from the multiplier to the capacitor during a
short time interval for weight adaptation For applications like active noise controls
and telecommunications tapped analog delay lines are also designed and
integrated in the synapse chip To reduce offset accumulation a parallel analog delay
line is adopted Same offset voltage is introduced for operational amplifiers at all
nodes Diffusion capacitors with pF are used for the storage of the tapped
analog delay line
In a synapse chip synapse cells are integrated in a array with a
analog delay line Inputs may be applied either from the analog delay line or from
external pins in parallel To select a capacitor in the cell for refresh decoders are
placed in columns and rows The actual size of the synapse cell is
and the size of the synapse chip is The chip is fabricated in a
single-poly CMOS process On the other hand the neuron chip has a very
simple structure which consists of neuron cells without additional circuits The
Sigmoid circuit in the neuron cell uses a differential pair and the slope and
amplitude are controlled by a voltage-controlled resistor Sigmoid-derivative
circuit is also using differential pair with min-select circuit The size of the neuron cell
is
Synapse
Chip
PC
Neuron
PC
Chip
Target
I
I
I
DSP
I
Output
I
Input
ANN Board
Figure Block diagram of the analog neuro-board
GDAB
tv.c
ArIC
D1 16bitll
DO 48bitll
Active Noise Canceling with Analog On-Chip Learning Neuro-Chip
Using these chip sets an analog neuro-system is constructed Figure shows a brief
block diagram of the analog neuro-system where an analogue neuro-board is
interfaced to a host computer through a GDAB General Data Acquisition Board The
GDAB board is specially designed for the data interface with the analogue neuro-chips
The neuro-board has synapse chips and neuron chips with the 2-layer Perceptron
architecture For test and development purposes a DSP ADC and DAC are installed
on the neuro-board to refresh and adjust weights
Forward propagation time of the layers Perceptron is measured as about f..lsec
Therefore the computation speed of the neuro-board is about MCPS Mega
Connections Per Second for recall and about MCUPS Mega Connections
Updates Per Second for error backpropagation learning To achieve this speed with a
DSP about MIPS is required for recall and at least MIPS for error-back
propagation learning
Channel
Error
Signal
Noise
Source
Adaptive Filter
or
Multilayer Perceptron
Figure Structure of a feedforward active noise canceling
ACTIVE NOISE CANCELING USING NEURO-CHIP
Basic architecture of the feed forward active noise canceling is shown in Figure An
area near the microphone is called quiet zone which actually means noise should be
small in this area Noise propagates from a source to the quiet zone through a
dispersive medium of which characteristics are modeled as a finite impulse response
FIR filter with additional random noise An active noise canceller should generate
electric signals for a loud speaker which creates acoustic signals to cancel the noise at
the quiet zone In general the electric-to-acoustic signal transfer characteristics of the
loud speaker is nonlinear and the overall active noise canceling ANC system also
becomes nonlinear Therefore multilayer Perceptron has a potential advantage over
popular transversal adaptive filters based on linear-mean.-square LMS error
minimization
Experiments had been conducted for car noise canceling The

<<----------------------------------------------------------------------------------------------------------------------->>

title: 1912-smart-vision-chip-fabricated-using-three-dimensional-integration-technology.pdf

Smart Vision Chip Fabricated Using Three
Dimensional Integration Technology
H.Kurino M.Nakagawa K.W Lee T.Nakamura
Y.Yamada K.T.Park and M.Koyanagi
Dept of Machine Intelligence
and Systems Engineering
Tohoku University
01 Aza-Aramaki Aoba-ku Sendai
Japan
kurino@sd.mech.tohoku.ac.jp
Abstract
The smart VISIOn chip has a large potential for application in
general purpose high speed image processing systems In order to
fabricate smart vision chips including photo detector compactly
we have proposed the application of three dimensional LSI
technology for smart vision chips Three dimensional technology
has great potential to realize new neuromorphic systems inspired
by not only the biological function but also the biological structure
In this paper we describe our three dimensional LSI technology for
neuromorphic circuits and the design of smart vision chips
Introduction
Recently the demand for very fast image processing systems with real time
operation capability has significantly increased Conventional image processing
systems based on the system level integration of a camera and a digital processor
do not have the potential for application in general purpose consumer electronic
products This is simply due to the cost size and complexity of these systems
Therefore the smart vision chip will be an inevitable component of future intelligent
systems In smart vision chips 2D images are simultaneously processed in parallel
Therefore very high speed image processing can be realized Each pixel includes a
photo-detector In order to receive a light signal as much as possible the
photo-detector should occupy a large proportion of the pixel area However the
successi ve processing circuits must become larger in each pixel to realize high level
image processing It is very difficult to achieve smart vision chips by using
conventional two dimensional LSI technology because such smart vision chips
have low fill-factor and low resolution This problem can be overcome if three
dimensional integration technology can be employed for the smart vision
chip In this paper we propose a smart vision chip fabricated by three dimensional
integration technology We also discuss the key technologies for realizing three
dimensional integration and preliminary test results of three dimensional image
sensor chips
Three Dimensional Integrated Vision Chips
Figure shows the cross-sectional structure of the three dimensional integrated
vision chip Several circuit layers with different functions are stacked into one chip
in 3D LSI. For example the first layer consists of a photo detector array acting like
photo receptive cells in the retina the second layer is horizontal/bipolar cell
circuits the third layer is ganglion cell circuits and so on Each circuit layer is
stacked and electrically connected vertically using buried interconnections and
micro bumps By using three dimensional integration technology a photo detector
can be formed with a high fill-factor and high resolution because several successive
processing circuits with large areas are formed on the lower layers underneath the
photo detector layer Every photo detector is directly connected with successive
processing circuits ie horizontal and bipolar cell circuits in parallel via the
vertical interconnections The signals in every pixel are simultaneously transferred
in the vertical direction and processed in parallel in each layer Therefore high
performance real time vision chips can be realized We considered the 3D LSI
suitable for realizing neuromorphic LSI because the three dimensional structure is
quite similar to the structure of the retina or cortex Three dimensional technology
will realize new neuromorphic systems inspired by not only the biological function
but also the biological structure
Glass Wafer
Photoreceptors
Layer
Horizontal and
Bipolar Cells
Layer
Ganglion Cells
Layer
Fig.1 Cross-sectional structure of three dimensional vision chip
Figure shows the neuromorphic analog circuits implemented into 3D LSI. The
circuits are divided into three circuit layers Photodiodes and photocircuits are
designed on the first layer Horizontal/bipolar cell circuits and ganglion cells are
circuit layer is fabricated
If.t
I
I
I
I
I
I
I
a
Th ird LOU
I
Fig.2
Circuit diagram of three dimensional vision chip
Photodiode
Third Layer
Fig.3
Layout of the three dimensional vision chip
on different Si wafers and stacked into a 3D LSI. Light signals are converted into
electrical analog signals by photodiodes and photocircuits on the first layer The
electric signals are transferred from the first layer to the second layer through the
vertical interconnections The operational amplifiers and resistor network on the

<<----------------------------------------------------------------------------------------------------------------------->>

title: 29-basins-of-attraction-for-electronic-neural-networks.pdf

BASINS OF ATTRACTION FOR
ELECTRONIC NEURAL NETWORKS
C. M. Marcus
R. M. Westervelt
Division of Applied Sciences and Department of Physics
Harvard University Cambridge MA
ABSTRACT
We have studied the basins of attraction for fixed point and
oscillatory attractors in an electronic analog neural network Basin
measurement circuitry periodically opens the network feedback loop
loads raster-scanned initial conditions and examines the resulting
attractor
Plotting the basins for fixed points memories we show
that overloading an associative memory network leads to irregular
basin shapes The network also includes analog time delay circuitry
and we have shown that delay in symmetric networks can introduce
basins for oscillatory attractors Conditions leading to oscillation
are related to the presence of frustration reducing frustration by
diluting the connections can stabilize a delay network
INTRODUCTION
The dynamical system formed from an interconnected network of
nonlinear neuron-like elements can perform useful parallel
computation
Recent progress in controlling the dynamics has
focussed on algorithms for encoding the location of fixed pOints
and on the stability of the flow to fixed points An equally
important aspect of the dynamics is the structure of the basins of
attraction which describe the location of all pOints in initial
condition space which flow to a particular attractor 22
In a useful associative memory an initial state should lead
reliably to the closest memory
This requirement suggests that a
well-behaved basin of attraction should evenly surround its attractor
and have a smooth and regular shape One dimensional basin maps
plotting pull in probability against Hamming distance from an
attract or do not reveal the shape of the basin in the high
dimensional space of initial states 19 Recently a numerical study
of a Hopfield network with discrete time and two-state neurons showed
rough and irregular basin shapes in a two dimensional Hamming space
suggesting that the high dimensional basin has a complicated
structure It is not known how the basin shapes change with the
size of the network and the connection rule
We have investigated the basins of attraction in a network with
continuous state dynamics by building an electronic neural network
with eight variable gain sigmoid neurons and a three level
interconnection matrix
We have also built circuitry that can map
out the basins of attraction in two dimensional slices of initial
state space Fig
The network and the basin measurements are
described in section
@ American Institute of Physics
In section we show that the network operates well as an
associative memory and can retrieve up to four memories eight fixed
points without developing spurious attractors but that for storage
of three or more memories the basin shapes become irregular
In section we consider the effects of time delay Real network
components cannot switch infinitely fast or propagate signals
instantaneously so that delay is an intrinsic part of any hardware
implementation of a neural network We have included a controllable
CCD charge coupled device analog time delay in each neuron to
investigate how time delay affects the dynamics of a neural network
We find that networks with symmetric interconnection matrices which
are guaranteed to converge to fixed points for no delay show
collective sustained oscillations when time delay is present By
discovering which configurations are maximally unstable to
oscillation and looking at how these configurations appear in
networks we are able to show that by diluting the interconnection
matrix one can reduce or eliminate the oscillations in neural
networks with time delay
NETWORK AND BASIN MEASUREMENT
A block diagram of the network and basin measurement circuit is
shown in
digital
comparator
and
oscillation
detector
desired
memory
sigmoid amplifiers
with
Block diagram
of the network and
basin measurement
system
Fi~.l
The main feedback loop consists of non-linear amplifiers
neurons see with capacitive inputs and a resistor matrix
allowing interconnection strengths of In
all basin measurements the input capacitance was nF giving a
time constant of ms A charge coupled device CCD analog time
del ay was built into each neuron providing an adjustable delay per
neuron over a range ms
Inverting
output
Fig.2 Electronic neuron
Non-linear gain provided
by feedback diodes
Inset Nonlinear
behavior at several
different values of
gain
Analog switches allow the feedback path to be periodically
disconnected and each neuron input charged to an initial voltage The
network is then reconnected and settles to the attractor associated
with that set of initial conditions Two of the initial voltages are
raster scanned on a time scale that is long compared to the load/run
switching time with function generators that are also connected to
the and axes of a storage scope
The beam of the scope is
activated when the network settles into a de sired at tractor
producing an image of the basin for that attractor in a twodimensional slice of initial condition space
The attractor of
interest can be one of the fixed points or an oscillatory
attractor
A simple example of this technique is the case of three neurons
with symmetric non-inverting connection shown in
STATE
SPACE
G??gBASIN FORt
BASIN FOR
1V
CIRCUIT
1V
Basin
of
Fig.3
attraction for three
neurons
with
symmetric non-inverting
coupling Slices are
in
plane
of
the
initial voltages on
neurons and The
two fixed points are
all neurons saturated
all
positive
or
negative
The data
are photographs of
the scope screen
BASINS FOR FIXED POINTS ASSOCIATIVE MEMORY
Two dimensional slices of the eight dimensional initial condition
space for the full network reveal important qualitative features
about the high dimensional basins
shows a typical slice for
a network programmed with three memories according to a clipped Hebb
rule1
where is an N-component memory vector of l's and and is
the number of memories The memories were chosen to be orthogonal
MEMORIES
I
IV
IV
Fi~.
A slice of initial condition space shows the basins of
attraction for five of the six fixed points for three memories
in eight-neuron Hopfield net Learning rule was clipped Hebb
Neuron gain
Because the Hebb rule makes a and stable attractors a
three-memory network will have six fixed point attractors In
the basins for five of these attractors are visible each produced
with a different rastering pattern to make it distinctive Several
characteristic features should be noted
All initial conditions lead to one of the memories
inverses no spurious attractors were seen for three or four
memories
This is interesting in light of the well documented
emergence of spurious attractors at miN in larger networks with
discrete time
The basins have smooth and continuous edges
The shapes of the basins as seen in this slice are irregular
Ideally a slice with attractors at each of the corners should have
rectangular basins one basin in each quadrant of the slice and the
location of the lines dividing quadrants determined by the initial
conditions on the other neurons the unseen dimensions With three
or more memories the actual basins do not resemble this ideal form
TIME DELAY FRUSTRATION AND SUSTAINED OSCILLATION
Arguments defining conditions which guarantee convergence to
fixed points based for example on the construction of a
Liapunov function generally assume instantaneous communication
between elements of the network
In any hardware implementation
these assumptions break down due to the finite switching speed of
amplifiers and the charging time of long interconnect lines 13 It is
the ratio of delay/RC which is important for stability so keeping
this ratio small limits how fast a neural network chip can be
designed to run
Time delay is also relevant to biological neural
nets where propagation and response times are comparable lS
Our particular interest in this section is how time delay can
lead to sustained oscillation in networks which are known to be
stable when there is no delay
We therefore restrict our attention
to networks with symmetric interconnection matrices Tlj Tjl
An obvious ingredient in producing oscillations in a delay
network is feedback or stated another way a graph representing the
connections in a network must contain loops
The simplest oscillatory structure made of delay elements is the
ring oscillator fig.Sa
Though not a symmetric configuration the
ring oscillator illustrates an important point the ring will
oscillate only when there is negative feedback at dc that is when
the product of'interconnection around the loop is negative Positive
feedback at dc loop product of connections will lead to
saturation
Observing various symmetric configurations fig.Sb in the
delayed-neuron network we find that a negative product of
connections around a loop is also a necessary condition for sustained
oscillation in symmetric circuits
An important difference between
the ring fig.Sa and the symmetric loop fig.Sb is that the period
of oscillation for the ring is the total accumulated delay around the
ring the larger the ring the longer the period In contrast for
those symmetric configurations which have oscillatory attractors the
period of oscillation is roughly twice the delay regardless of the
size of the configuration or the value of delay This indicates that
for symmetric configurations the important feedback path is local
not around the loop
I illator
NEGATIVE
FEEDBACK
bJmmetric
FI~~~TRATED
lime delay
neuron
non-inverting
connection
inverting
connection
Fir;;r A ring oscillator
needs negative feedback at dc
to oscillate Symmetrically connected triangle This
configuration is frustrated
defined in text and has
both oscillatory and fixed
point attractors when neurons
have delay
otl
Configurations with loop connection product are important in
the theory of spin glasses where such configurations are called
frustrated Frustration in magnetic spin systems gives a measure
of serious bond disorder disorder that cannot be removed by a
change of variables which can lead to a spin glass state
Recent results based on the similarity between spin glasses and
symmetric neural networks has shown that storage capacity limitations
can be understood in terms of this bond disorder 18 19 Restating our
observation above We only find stable oscillatory modes in symmetric
networks with delay when there is frustration A similar result for a
sign-symmetric network Tlj Tjl both or with no delay is
described by Hirsch
We can set up the basin measurement system fig.l to plot the
basin of attraction for the oscillatory mode Fig.6 shows a slice of
the oscillatory basin for a frustrated triangle of delay neurons
I
I
I
I
I
Fig.6 Basin for oscillatory attractor cross-hatched region
in frustrated triangle of delay-neurons
Connections were
all symmetric and inverting other frustrated configurations
two non-inverting one inverting all symmetric were
similar delay inset shows trajectory to fixed
point and oscillatory mode for two close-lying initial
conditions
delay basin size increases
A fully connected feedback associative network with more that one
memory will contain frustration As more memories are added the
amount of frustration will increases until memory retrieval
disappears But before this point of memory saturation is reached
delay could cause an oscillatory basin to open
In order to design
out this possibility one must understand how frustration delay and
global stability are related A first step in determining the
stability of a delay network is to consider which small
configurations are most prone to oscillation and then see how these
dangerous configurations show up in the network As described
above we only need to consider frustrated configurations
A frustrated configuration of neurons can be sparsely connected
as in a loop or densely connected with all neurons connected to all
others forming what is called in graph theory a clique
Representing a network with inverting and non-inverting connections
as a signed graph edges carry and we define a frustrated clique
as a fully connected set of vertices vertices edges
with all sets of three vertices in the clique forming frustrated
triangles Some examples of frustrated loops and cliques are shown in
fig
Notice that neurons connected with all inverting symmetric
connections a configuration that is useful as a winner-take-all
circuit is a frustrated clique
FiQ.7 Examples of frustrated
loops
and
frustrated
cliques
In
the
graph
representation
vertices
inverting
non-inverting
black dots are neurons
symmetric connection symmetric connection
FRUSTRATED with delay and undirected
edges
are
symmetric
CLIQUES
connections
fully connected
LOOPS
all triangles
frustrated
We find that delayed neurons connected in a frustrated loop
longer than three neurons do not show sustained oscillation for any
value of delay tested up to delay
In contrast when delayed
neurons are connected in any frustrated clique configuration we do
find basins of attraction for sustained oscillation as well as fixed
point attractors and that the larger the frustrated clique the more
easily it oscillates in the following ways For a given value of
delay/RC the size of the oscillatory basin increases with the
size of the frustrated clique fig The critical value of
delay at which the volume of the oscillatory basin goes to zero
decreases with increasing For the critical delay is
already less than RC.
Fig.8
Size of basin
for
oscillatory mode
increases with size of
frustrated clique
The
delay
is
per
neuron in each picture
Slices are in the space
of initial voltages on
neurons and other
initial voltages near
zero
iG
co
Fig.9 The critical valu of delay
where the oscillatory mode vanishes
Measured by reducing delay until
system leaves oscillatory attractor
Delay plotted in units of the
characteristic time RioC where Rio
Lj Rij and
indicating that the critical delay
decreases faster than
size of frustrated clique
Having identified frustrated cliques as the maximally unstable
configuration of time delay neurons we now ask how many cliques of a
given size do we expect to find in a large network
A set of vertices neurons can be fully connected by
edges of two types or to form different cliques Of
these will be frustrated cliques
Fig shows all
cases for
r.III
A
All graphs of size that are frustrated cliques
fully connected every triangle frustrated Solid lines
positive edges dashed lines negative edges
For a randomly connected network this result combined with
results from random graph th eory gives an expected number of
frustrated cliques of size in a network of size
EN(r
where
is the binomial coefficient and
is defined as the
concentration of frustrated cliques is the connectance of the
network defined as the probability that any two neurons are
connected is the special case where and edges noninverting inverting connections are equally probable We have also
generalized this result to the case
shows the dramatic reduction in the concentration of all
frustrated configurations in a diluted random network For the
general case
we find that the negative connections
affect the concentrations of frustrated cliques more strongly than
the positive connections
as expected Frustration requires
negatives not positives see fig
Concentration of
frustrated cliques of size
in an unbiased
random network from
Concentrations
decrease
rapidly as the network is
diluted
especially for
large cliques note log
scale
connectance
When the interconnections in a network are specified by a
learning rule rather than at random the expected numbers of any
configuration will differ from the above results
We have compared
the number of frustrated triangles in large three-valued
Hebb interconnection matrices to the expected number
in a random matrix of the same size and connectance The Hebb matrix
was constructed according to the rule
Tij Zk La=l,m ia Tii
Zk(X for for for
is the number of memories Zkis a threshold function with cutoff
and a is a random string of l's and The matrix constructed
by is roughly unbiased equal number of positive and negative
connections and has a connectance
shows the ratio of
frustrated triangles in a diluted Hebb matrix to the expected number
in a random graph with the same connectance for different numbers of
memories stored in the Hebb matrix At all values of connectance the
Hebb matrix has fewer frustrated triangles than the random matrix by
a ratio that is decreased by diluting the matrix or storing fewer
memories The curves do not seem to depend on the size of the matrix
N.
This result suggests that diluting a Hebb matrix breaks up
frustration even more efficiently than diluting a random matrix
CD
a
ratio
ratio
ratio
ratio
ratio
maS5
connectance
The number of frustrated
triangles in a Hebb rule
matrix divided by the
expected number in a random
with
equal
signed
graph
connectance
The different sets
of points are for different
numbers of random memories in the
The lines are
Hebb matrix
guides to the eye
The sensitive dependence of frustration on connectance suggests
that oscillatory modes in a large neural network with delay can be
eliminated by diluting the interconnection matrix
As an example
consider a unbiased random network with delay
From
only frustrated cliques of size or larger have oscillatory basins
for this value of delay frustration in smaller configurations in the
network cannot lead to sustained oscillation in the network
Diluting the connectance to will reduce the concentration of
frustrated cliques with by a factor of over and by a
factor of
The reduction would be even greater for a clipped
Hebb matrix
Results from spin glass theory suggest that diluting a clipped
Hebb matrix can actually improve the storage capacity for moderated
dilution with a maximum in the capacity at a connectance of To
the extent this treatment applies to an analog continuous-time
network we should expect that by diluting connections oscillatory
modes can be killed before memory capacity is compromised
We have confirmed the stabilizing effect of dilution in our
network For a fully connected eight neuron network programmed with
three orthogonal memories according to eq.l adding a delay of
opens large basins for sustained oscillation
By randomly diluting
the interconnections to
we were able to close the
oscillatory basins and recover a useful associative memory
SUMMARY
We have investigated the structure of fixed point and oscillatory
basins of attraction in an electronic network of eight non-linear
amplifiers with controllable time delay and a three value
interconnection matrix
For fixed point attractors we find that the network performs
well as an associative memory no spurious attractors were seen for
up to four stored memories but for three or more memories the
shapes of the basins of attraction became irregular
A network which is stable with no delay can have basins for
oscillatory at tractors when time delay is present For symmetric
networks with time delay we only observe sustained oscillation when
there
is
frustration
Frustrated cliques
fully connected
configurations with all triangles frustrated and not loops are
most prone to oscillation and the larger the frustrated clique the
more easily it oscillates The number of the se dangerous
configurations in a large network can be greatly reduced by diluting
the connections We have demonstrated that a network with a large
basin for an oscillatory attractor can be stabilized by dilution
ACKNOWLEDGEMENTS
We thank K.L.Babcock S.W.Teitsworth S.Strogatz and P.Horowitz for
useful discussions One of us acknowledges support as an AT&T
Bell Laboratories Scholar
This work was supported by JSEP contract
no

<<----------------------------------------------------------------------------------------------------------------------->>

title: 22-new-hardware-for-massive-neural-networks.pdf

NEW HARDWARE FOR MASSIVE NEURAL NETWORKS
D. D. Coon and A. G. U. Perera
Applied Technology Laboratory
University of Pittsburgh
Pittsburgh PA
ABSTRACT
Transient phenomena associated with forward biased silicon structures at show remarkable similarities with biological neurons The devices play
a role similar to the two-terminal switching elements in Hodgkin-Huxley equivalent
circuit diagrams The devices provide simpler and more realistic neuron emulation
than transistors or op-amps They have such low power and current requirements
that they could be used in massive neural networks Some observed properties of
simple circuits containing the devices include action potentials refractory periods
threshold behavior excitation inhibition summation over synaptic inputs synaptic
weights temporal integration memory network connectivity modification based on
experience pacemaker activity firing thresholds coupling to sensors with graded signal outputs and the dependence of firing rate on input current Transfer functions
for simple artificial neurons with spiketrain inputs and spiketrain outputs have been
measured and correlated with input coupling
INTRODUCTION
Here we discuss the simulation of neuron phenomena by electronic processes in
silicon from the point of view of hardware for new approaches to electronic processing
of information which parallel the means by which information is processed in intelligent organisms Development of this hardware basis is pursued through exploratory
work on circuits which exhibit some basic features of biological neural networks
shows the basic circuit used to obtain spiketrain outputs A distinguishing feature
of this hardware basis is the spontaneous generation of action potentials as a device
physics feature
JLJLL
Figure Spontaneous
neuronlike spiketrain
generating circuit The
spikes are nearly equal in
amplitude so that
information is contained in
the frequency and
temporal pattern of the
spiketrain generation
American Institute of Physics
TWO-TERMINAL SWITCHING ELEMENTS
The use of transistor based circuitry is avoided because transistor electrical
characteristics are not similar to neuron characteristics The use of devices with
fundamentally non-neuronlike character increases the complexity of artificial neural
networks Complexity would be an important drawback for massive neural networks
and most neural networks in nature achieve their remarkable performance through
their massive size In addition transistors have three terminals whereas the switching
elements of Hodgkin-Huxley equivalent circuits have two terminals Motivated in
part by Hodgkin-Huxley equivalent circuit diagrams we employ two-terminal devices which execute transient switching between low conductance and high
conductance states See We call these devices injection mode devices IMDs
In the OFF-STATE a typical current through the devices is and
in the ON-STATE a typical current is Hence this device is an
extremely good switch with a ON ratio of As in real neurons the current
in the device is a function of voltage and time not only voltage The devices require
cryogenic cooling but this results in an advantageously low quiescent power drain of
nanowatt/cm2 of chip area and the very low leakage currents mentioned above
In addition the highly unique ability of the neural networks described here to operate
in a cryogenic environment is an important advantage for infrared image processing
at the focal plane and further discussion below Vision systems begin
processing at the focal plane and there are many benefits to be gained from the
vision system approach to IR image processing
I
I
I
VD
IR
SS:Ulse
Output
Figure
Switching element
in Hodgkin-Huxley equivalent circuits
Figure Single stage conversion of
infrared intensity to spiketrain frequency with a neuron-like semiconductor device No pre-amplifiers
are necessary
Coding of graded input signals such as photocurrents into action potential spike trains with millimeter scale devices has been experimentally
demonstrated3 with currents from IlA down to about picoampere with coding
noise referred to input of femtoamperes Coding of much smaller current levels
should be possible with smaller devices Figure clearly shows the threshold behavior
of the IMD. For devices studied to date a transition from action potential output to
graded signal output is observed for input currents of the order of picoamperes
CURRENT AMPERES
Figure Coding of NIR-VISmLE-UV intensity into firing frequency of a spiketrain
and the experimentally determined firing rate the input current for one device
Note that the dynamic range is about
UBI
E2
Figure mustration of the threshold firing of the
device in response to input step functions
fLS div
This transition is remarkably well described in von Neumann's discussion of
the mixed character of neural elements which he relates to the concept of subliminal stimulation levels which are too low to produce the stereotypical all-or-nothing
response Neural network modelers frequently adopt viewpoints which ignore this
interesting mixed character The von Neumann viewpoint links the mixed character
to concepts of nonlinear dynamics in a way which is not apparent in recent neural
network modeling literature The scaling down of IMD size should result in even
lower current requirements for all-or-nothing response
DEVICE PHYSICS
Recently neuronlike action potential transients in IMDs have been the subject
of considerable research3 In the simple circuits of the IMD
gives rise to a spontaneous neuronlike spiketrain output Between pulses the IMD is
polarized in the sense that it is in a low conductance state with a substantial voltage
occurring across it even though it is forward biased The low conductance has been
attributed to small interfacial work functions due to band offsets at the and
interfaces
Low temperatures inhibit thermionic injection of electrons and holes into the
n-region from the layer and layer impurity bands Pulses are caused by
switching to depolarized states with low diode potential drops and large injection
currents which are believed to be triggered by the slow buildup of a small thermionic
injection current from the layer into the n-region The injection current can cause
impact ionization of n-region donor impurities resulting in an increasingly positive
space charge which further enhances the injection current to the point where the IMD
abruptly switches to the low conductance state with large injection current Switching
times are typically under lOOns Charging of the load capacitance CL cuts off the
large injection current and resets the diode to its low conductance state The load
capacitor CL then discharges through RL. During the CL discharging time constant
RLCL the voltage across the IMD itself is low and therefore the bias voltage would
have to be raised substantially to cause further firing Thus RLCL is analogous to
the refractory period of a neuron The output pulses of an IMD generally have about
the same amplitude while the rate of pulsing varies over a wide range depending on
the bias voltage and the presence of electromagnetic radiation
DETECTOR ARRAY
TRANSIENT SENSING
MOTION
SENSING
TRACKING
PARALLEL OUTPUT
Figure lllustrative
laminar architecture
showing stacked wafers in
3-dimensions
LAMINAR
NEURAL NETWORK
REAL TIME PARALLEL ASYNCHRONOUS PROCESSING
The devices described here could form the hardware basis for a parallel asynchronous processor in much the same way that transistors form the basis for digital
computers The devices could be used to construct networks which could perform real
time signal processing Pulse propagation through silicon chips parallel firethrough
see as opposed to the lateral planar propagation in conventional integrated
circuits has been proposed 1S This would permit the use of laminar stacked wafer
architectures See
Such architectures would eliminate the serial processing limitations of standard processors which utilize multiplexing and charge transfer There are additional
advantages in terms of elimination of pre-amplifiers and reduction in power consumption The approach would utilize the low power low noise devices lO described here
to perform input signal-to-frequency conversion in every processing channel
POWER CONSUMPTION FOR A BRAIN SCALE SYSTEM
The low power and low current requirements together with the electronic simplicity lower parts-count as compared with transistor and op-amp approaches and
INPUTS
Siwafer
Siwafer
Siwaf.r
Siwaf.r
Figure Schematic illustration of the signal flow
pattern through a real time
parallel asynchronous processor consisting of stacked
silicon wafers
wafer
I I I I I I I I I I I I I 1SiSiwaf.r
OUTPUTS
the natural emulation of neuron features means that the approach described here
would be especially advantageous for very large neural networks systems comparable to supercomputers in which power dissipation and system complexity are important considerations The power consumption of large scale analog and digital 17
systems is always a major concern For example the power consumption of the
CRAY is of the order of kilowatts For the devices described here the
power consumption is very low For these devices we have observed quiescent power
drains of about cm and pulse power consumption of about nJ/pulse/cm
We estimate that a system with active elements comparable
to the number of neurons in the brain 18 all firing with an average pulse rate of
KHz corresponding to a high neuronal firing rateS would consume about watts
The quiescent power drain for this system would be milliwatts Thus power
requirements for such an artificial neural network with the size scale pulse
generating elements of the human brain and a range of activity between zero and
the maximum conceivable sustained activity for neurons in the brain would be
milliwatts watts for micron technology For comparison we note that
von Neumann's estimate for the power dissipation of the brain is of order to
watts Fabrication of a element artificial neural network would require
processing of about four inch wafers
NETWORK CONNECTIVITY
For a network with coupling between many IMD's3 we have shown that
where Vj is the voltage across the diode and the input capacitance Cj of the i-th
network node Rj represents a leakage resistance in parallel with Cil and Ij represents
an external current input to the i-th diode label different network nodes
and Tij incoporates coupling between network elements Equation has the same
form as equations which occur in the Hopfield modeI 2o for neural networks
Sejnowski has also discussed similar equations in connection with skeleton filters in
OUTPUTS
INPUTS
TRANSMISSION LINE
Figure Main features of a typical neuron from Kandel and Schwartz 19 Our
artificial neuron which shows the summation over synaptic inputs and fan-out
the brain 24 Nonlinear threshold behavior of IMD)s enters through as it does
in the neural network models
In a range of input capacitances is possible This range of capacitances
is related to the range of possible synaptic weights The circuit in accomplishes
pulse height discrimination and each pulse can contribute to the charge stored on
the central node capacitance C. The charge added to during each input pulse is
linearly related to the input capacitance except at extreme limits The range of input
capacitances for a particular experiment was J-lF to J-lF which differ by a factor
of about The effect of various input capacitance values synaptic weights on
input-output firing rates is shown in Also the shows many capacitive
inputs/outputs to/from a single IMD. fan-in and fan-out For pulses which arrive
at different inputs at about the same time the effect of the pulses is additive The
time within which inputs are summed is just the stored charge lifetime Summation
over many inputs is an important feature of neural information processing
EXCITATION INHIBITION MEMORY
Both excitatory and inhibitory input circuits are shown in Input pulses
cause the accumulation of charge on in excitatory circuits and the depletion of
charge on in inhibitory circuits Charge associated with input spiketrains is integrated/stored on C. The temporally integrated charge is depleted by the firing of the
IMD. Thus the storage time is related to the firing rate After an input spiketrain
raises the potential across to a value above the firing threshold the resulting IMD
I
Figure Output pulse
rate the input
pulse rate for different
input capacitance
values Ci values
Vl
CL
INPUT PULSE RATE Hz
I NP
OUTP UT
Figure Circuits which incorporate rectifying synaptic inputs an excitatory
input an inhibitory input
INP
R'L
output spiketrain codes the input information The output firing rate is linearly related to the input firing rate times the synaptic coupling strength linearly related to
Ci). See If the input ceases then the potential across relaxes back to a value
just below the firing threshold When not firing the IMD has a high impedance If
there is negligible leakage of charge from then can remain near threshold
voltage for a long time and a new input signal will quickly take the IMD over the
firing threshold See We have observed stored charge lifetimes of 56 days and
longer times may be acheivable The lifetime of charge stored on can be reduced
by adding a resistance in parallel with C.
From the discussion of integration we see that long term storage of charge on
is equivalent to long term memory The memory can be read by seeing if a new input
pulse or spiketrain produces a prompt output pulse or spiketrain The read signal
input channel in can be the same as or different from the channel which
resulted in the charge storage In either case memory would produce a change in the
pattern of connectivity if the circuit was imbedded in a neural network Changes in
patterns of connectivity are similar to Hebb's ruie considerations26 in which memory
is associated with increases in the strength weight of synaptic couplings Frequently
QJ 13
Figure Firing rate the bias voltage
The region where the firing is negligible is
associated with memory The state of the
memory is associated with the proximity
to the firing threshold
Input Potential
the increase in synaptic weights is modeled by increased conductance whereas in the
circuits in Figs lO(a and memory is achieved by integration and charge storage
Note that for these particular circuits the memory is not eraseable although volatile
short term memory can easily be constructed by adding a resistor in parallel with
C. Thus a continuous range of memory lifetimes can be achieved
PARALLEL ASYNCHRONOUS CHIP-TO-CHIP TRANSMISSION
For many IMD's the output pulse heights for a circuit like that in are
volts Thus output from the first stage or any later stage of the network could
easily be transmitted to other parts of an overall system Two-dimensional arrays
of devices on different chips could be coupled by indium bump bonding to form
the laminar architecture described above Planar technology could be used for local
lateral interconnections in the processor See In addition to transmission of
electrical pulses optical transmission is possible because the pulses can directly drive
LED's
Emerging GaAs-on-Si technology is interesting as a means of fabricating two
dimensional emitter arrays Optical transmission is not necessary but it might be
useful for processed image data transfer for coupling to an optical processor or to provide optical interconnects between chips bearing arrays of
diodes Note that with optical interconnects between chips the circuits
employed here would be internal receivers The p-i-n diodes employed in the present
work would be well suited to the receiver role An interesting possibility would entail the use optical interconnects between chips to achieve local lateral interaction
This would be accomplished by having each optical emitter in a array broadcast
locally to multiple receivers rather than to a single receiver Similarly each receiver
would have a reeeptive field extending over multiple transmitters It is also possible
that an optical element could be placed in the gap between parallel transmitter and
receiver planes to structure control or alter patterns of interconnection This
would be an alternative to a planar technology approach to lateral interconnection
IT the optical elements were active then the system would constitute a hybrid optical/electronic processor whereas if passive optical elements were employed we would
regard the system as an optoelectronic processor In either case we picture the processing functions of temporal integration spatial summation over inputs coding and
pulse generation as residing on-chip
ACKNOWLEDGEMENTS
The work was supported in part by U.S. DOE under contract and NSF under grant

<<----------------------------------------------------------------------------------------------------------------------->>

