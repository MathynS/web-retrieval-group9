query sentence: neural network computers
<<------------------------------------------------------------------------------------------------------------------------------------------->>
title: 235-computational-efficiency-a-common-organizing-principle-for-parallel-computer-maps-and-brain-maps.pdf

Nelson and Bower
Computational Efficiency
A Common Organizing Principle for
Parallel Computer Maps and Brain Maps
Mark E. Nelson James M. Bower
Computation and Neural Systems Program
Division of Biology
California Institute of Technology
Pasadena CA
ABSTRACT
It is well-known that neural responses in particular brain regions
are spatially organized but no general principles have been developed that relate the structure of a brain map to the nature of
the associated computation On parallel computers maps of a sort
quite similar to brain maps arise when a computation is distributed
across multiple processors In this paper we will discuss the relationship between maps and computations on these computers and
suggest how similar considerations might also apply to maps in the
brain
INTRODUCTION
A great deal of effort in experimental and theoretical neuroscience is devoted to
recording and interpreting spatial patterns of neural activity A variety of map
patterns have been observed in different brain regions and presumably these patterns reflect something about the nature of the neural computations being carried
out in these regions To date however there have been no general principles for
interpreting the structure of a brain map in terms of properties of the associated
computation In the field of parallel computing analogous maps arise when a computation is distributed across multiple processors and in this case the relationship
Computational Eftkiency
between maps and computations is better understood In this paper we will attempt to relate some of the mapping principles from the field of parallel computing
to the organization of brain maps
MAPS ON PARALLEL COMPUTERS
The basic idea of parallel computing is to distribute the computational workload
for a single task across a large number of processors Dongarra Fox and
Messina In principle a parallel computer has the potential to deliver computing power equivalent to the total computing power of the processors from which
it is constructed a processor machine can potentially deliver times the
computing power of a single processor In practice however the performance that
can be achieved is always less efficient than this ideal A perfectly efficient implementation with processors would give a factor speed up in computation time
the ratio of the actual speedup to the ideal speedup can serve as a measure of
the efficiency of a parallel implementation
For a given computation one of the factors that most influences the overall performance is the way in which the computation is mapped onto the available processors
The efficiency of any particular mapping can be analyzed in terms of two principal
factors load-balance and communication overhead Load-balance is a measure of
how uniformly the computational work load is distributed among the available processors Communication overhead on the other hand is related to the cost in time
of communicating information between processors
On parallel computers the load imbalance A is defined in terms of the average
calculation time per processor atJg and the maximum calculation time required by
the busiest processor maz
A
Tmaz
atJg
atJg
The communication overhead is defined in terms of the maximum calculation time
and the maximum communication time Tcomm
maz
Tcomm
Tcomm
Assuming that the calculation and communication phases of a computation do not
overlap in time as is the case for many parallel computers the relationship between
efficiency load-imbalance A and communicaticn overhead is given by Fox
61
62
Nelson and Bower
When both load-imbalance A and communication overhead are small the inefficiency is approximately the sum of the contributions from load-imbalance and
communication overhead
When attempting to achieve maximum performance from a parallel computer a
programmer tries to find a mapping that minimizes the combined contributions of
load-imbalance and communication overhead In some cases this is accomplished by
applying simple heuristics Fox while in others it requires the explicit
use of optimization techniques like simulated annealing Kirkpatrick
or even artificial neural network approaches Fox and Furmanski In any
case the optimal tradeoff between load imbalance and communication overhead
depends on certain properties of the computation itself Thus different types of
computations give rise to different kinds of optimal maps on parallel computers
AN EXAMPLE
In order to illustrate how different mappings can give rise to different computational
efficiencies we will consider the simulation of a single neuron using a multicompartment modeling approach Segev In such a simulation the model neuron is divided into a large number of compartments each of which is assumed to be
isopotential Each compartment is represented by an equivalent electric circuit that
embodies information about the local membrane properties In order to update the
voltage of an individual compartment it is necessary to know the local properties
as well as the membrane voltages of the neighboring compartments Such a model
gives rise to a system of differential equations of the following form
where em is the membrane capacitance Vi is the membrane voltage of compartment
9k and Ek are the local conductances and their reversal potentials and are
coupling conductances to neighboring compartments
When carrying out such a simulation on a parallel computer where there are more
compartments than processors each processor is assigned responsibility for updating
a subset of the compartments Nelson If the compartments represent
equivalent computational loads then the load-imbalance will be proportional to
the difference between the maximum and the average number of compartments per
processor If the computer processors are fully interconnected by communication
channels then the communication overhead will be proportional to the number
of interprocessor messages providing the voltages of neighboring compartments If
Computational Efficiency
A
A
A
Figure Tradeoffs between load-imbalance A and communication overhead
giving rise to different efficiencies for different mappings of a multicompartment neuron model a minimum-cut mapping that minimizes communication
overhead but suffers from a significant load-imbalance a scattered mapping
that minimizes load-imbalance but has a large communication overhead and
a near-optimal mapping that simultaneously minimizes both load-imbalance and
communication overhead
neighboring compartments are mapped to the same processor then this information
is available without any interprocessor communication and thus no communication
overhead is incurred
shows three different ways of mapping a compartment neuron model
onto a group of processors In each case the load-imbalance and communication
overhead are calculated using the assumptions listed above and the computational
efficiency is computed using eq The map in 1A minimizes the communication
overhead of the mapping by making a minimum number of cuts in the dendritic
tree but is rather inefficient because a significant load-imbalance remains even
after optimizing the location of each cut The map is on the other hand
minimizes the load-imbalance by using a scattered mapping technique Fox
but is inefficient because of a large communication overhead The map in
1C strikes a balance between load-imbalance and communication overhead that
results in a high computational efficiency Thus this particular mapping makes the
best use of the available computing resources for this particular computational task
63
64
Nelson and Bower
A
Figure Three classes of map topologies found in the brain of the rat
continuous map of tactile inputs in somatosensory cortex patchy map of tactile
inputs to cerebellar cortex and scattered mapping of olfactory inputs to olfactory
cortex as represented by the unstructured pattern of 2DG uptake in a single section
of this cortex
MAPS IN THE BRAIN
Since some parallel computer maps are clearly more efficient than others for particular problems it seems natural to ask whether a similar relationship might hold for
brain maps and neural computations Namely for a given computational task does
one particular brain map topology make more efficient use of the available neural
computing resources than another If so does this impose a significant constraint
on the evolution and development of brain map topologies
It turns out that there are striking similarities between the kinds of maps that
arise on parallel computers and the types of maps that have been observed in
the brain In both cases the map patterns can be broadly grouped into three
categories continuous maps patchy maps and scattered non-topographic maps
shows examples of brain maps that fall into these categories 2A shows
an example of a smooth and continuous map representing the pattern of afferent
tactile projections to the primary somatosensory cortex of a rat Welker
The patchy map in 2B represents the spatial pattern of tactile projections to
the granule cell layer of the rat cerebellar hemispheres Shambes Bower
and Woolston Finally 2C represents an extreme case in which a brain
region shows no apparent topographic organization This figure shows the pattern
of metabolic activity in one section of the olfactory piriform cortex as assayed by
2-deoxyglucose uptake in response to the presentation of a particular odor
Sharp As suggested by the uniform label in the cortex no discernible
Computational Eftkiency
odor-specific patterns are found in this region of cortex
On parallel computers maps in these different categories arise as optimal solutions
to different classes of computations Continuous maps are optimal for computations
that are local in the problem space patchy maps are optimal for computations that
involve a mixture of local and non-local interactions and scattered maps are optimal or near-optimal for computations characterized by a high degree of interaction
throughout the problem space especially if the patterns of interaction are dynamic
or cannot be easily predicted Interestingly it turns out that the intrinsic neural circuitry associated with different kinds of brain maps also reflects these same
patterns of interaction Brain regions with continuous maps like somatosensory
cortex tend to have predominantly local circuitry regions with patchy maps like
cerebellar cortex tend to have a mixture of local and non-local circuitry and regions
with scattered maps like olfactory cortex tend to be characterized by wide-spread
connectivity
The apparent correspondence between brain maps and computer maps raises the
general question of whether or not there are correlates of load-imbalance and communication overhead in the nervous system In general these factors are much more
difficult to identify and quantify in the brain than on parallel computers Parallel
computer systems are after all human-engineered while the nervous system has
evolved under a set of selection criteria and constraints that we know very little
about Furthermore fundamental differences in the organization of digital computers and brains make it difficult to translate ideas from parallel computing directly
into neural equivalents Nelson For example it is far from clear
what should be taken as the neural equivalent of a single processor Depending on
the level of analysis it might be a localized region of a dendrite an entire neuron or
an assembly of many neurons Thus one must consider multiple levels of processing
in the nervous system when trying to draw analogies with parallel computers
First we will consider the issue of load-balancing in the brain The map in
while smooth and continuous is obviously quite distorted In particular the regions
representing the lips and whiskers are disproportionately large in comparison to
the rest of the body It turns out that similar map distortions arise on parallel
computers as a result of load-balancing If different regions of the problem space
require more computation than other regions load-balance is achieved by distorting
the map until each processor ends up with an equal share of the workload Fox
In brain maps such distortions are most often explained by variations
in the density of peripheral receptors However it has recently been shown in
the monkey that prolonged increased use of a particular finger is accompanied by
an expansion of the corresponding region of the map in the somatosensory cortex
Merzenich Presumably this is not a consequence of a change in peripheral
receptor density but instead reflects a use-dependent remapping of some tactile
computation onto available cortical circuitry
Although such map reorganization phenomena are suggestive of load-balancing effects we cannot push the analogy too far because we do not know what actually
6S
66
Nelson and Bower
corresponds to computational load in the brain One possibility is that it is associated with the metabolic load that arises in response to neural activity Yarowsky
and Ingvar Since metabolic activity necessitates the delivery of an adequate
supply of oxygen and glucose via a network of small capillaries the efficient use of
the capillary system might favor mappings that tend to avoid metabolic hot spots
which would overload the delivery capabilities of the system
When discussing communication overhead in the brain we also run into the problem of not knowing exactly what corresponds to communication cost On parallel
computers communication overhead is usually associated with the time-cost of exchanging information between processors In the nervous system the importance of
such time-costs is probably quite dependent on the behavioral context of the computation There is evidence for example that some brain regions actually make use
of transmission delays to process information Carr and Konishi However
there is another aspect of communication overhead that may be more generally
applicable having to do with the space-costs of physically connecting processors together In the design of modern parallel computers and in the design of individual
computer processor chips space-costs associated with interconnections pose a very
serious constraint for the design engineer In the nervous system the extremely
large numbers of potential connections combined with rather strict limitations on
cranial capacity are likely to make space-costs a very important factor
CONCLUSIONS
The view that computational efficiency is an important constraint on the organization of brain maps provides a potentially useful new perspective for interpretting
the structure of those maps Although the available evidence is largely circumstantial it seems likely that the topology of a brain map affects the efficiency with
which neural resources are utilized Furthermore it seems reasonable to assume
that network efficiency would impose a constraint on the evolution and development of map topologies that would tend to favor maps that are near-optimal for
the computational tasks being performed The very substantial task before us in
the case of the nervous system is to carry out further experiments to better understand the detailed relationships between brain maps neural architectures and
associated computations Bower
Acknowledgements
We would like to acknowledge Wojtek Furmanski and Geoffrey Fox of the Caltech
Concurrent Computation Program CCCP for their parallel computing support
We would also like to thank Geoffrey for his comments on an earlier version of this
manuscript This effort was supported by the NSF the Lockheed
Corporation and the Department of Energy

<<----------------------------------------------------------------------------------------------------------------------->>

title: 4718-analog-readout-for-optical-reservoir-computers.pdf

Analog readout for optical reservoir computers
A. Smerieri1 F. Duport1 Y. Paquot1 B. Schrauwen2 M. Haelterman1 S. Massar3
Service OPERA-photonique Universit Libre de Bruxelles Avenue F. D.
Roosevelt CP Bruxelles Belgium
Department of Electronics and Information Systems ELIS Ghent University
Sint-Pietersnieuwstraat 41 Ghent Belgium
Laboratoire d?Information Quantique Universit Libre de Bruxelles
Avenue F. D. Roosevelt CP Bruxelles Belgium
Abstract
Reservoir computing is a new powerful and flexible machine learning technique that is easily implemented in hardware Recently by using a timemultiplexed architecture hardware reservoir computers have reached performance comparable to digital implementations Operating speeds allowing for real time information operation have been reached using optoelectronic systems At present the main performance bottleneck is the readout
layer which uses slow digital postprocessing We have designed an analog
readout suitable for time-multiplexed optoelectronic reservoir computers
capable of working in real time The readout has been built and tested experimentally on a standard benchmark task Its performance is better than
non-reservoir methods with ample room for further improvement The
present work thereby overcomes one of the major limitations for the future
development of hardware reservoir computers
Introduction
The term reservoir computing encompasses a range of similar machine learning techniques
independently introduced by H. Jaeger and by W. Maass While these techniques
differ in implementation details they share the same core idea that one can leverage the
dynamics of a recurrent nonlinear network to perform computation on a time dependent
signal without having to train the network itself This is done simply by adding an external
generally linear readout layer and training it instead The result is a powerful system that
can outperform other techniques on a range of tasks for example the ones reported
in and is significantly easier to train than recurrent neural networks Furthermore
it can be quite easily implemented in hardware although it is only recently that
hardware implementations with performance comparable to digital implementations have
been reported
One great advantage of this technique is that it places almost no requirements on the
structure of the recurrent nonlinear network The topology of the network as well as
the characteristics of the nonlinear nodes are left to the user The only requirements are
that the network should be of sufficiently high dimensionality and that it should have
suitable rich dynamics The last requirement essentially means that the dynamics allows
the exploration of a large number of network states when new inputs come in while at
the same time retaining for a finite time information on the previous inputs For this
reason the reservoir computers appearing in literature use widely different nonlinear units
see for example and in particular the time multiplexing architecture proposed
in
Optical reservoir computers are particularly promising as they can provide an alternative
path to optical computing They could leverage the inherent high speeds and parallelism
granted by optics without the need for strong nonlinear interaction needed to mimic traditional electronic components Very recently optoelectronic reservoir computers have been
demonstrated by different research teams conjugating good computational performances with the promise of very high operating speeds However one major drawback in
these experiments as well as all preceding ones was the absence of readout mechanisms
reservoir states were collected on a computer and post-processed digitally severely limiting
the processing speeds obtained and hence the applicability
An analog readout for experimental reservoirs would remove this major bottleneck as
pointed out in The modular characteristics of reservoir computing imply that hardware reservoirs and readouts can be optimized independently and in parallel Moreover
an analog readout opens the possibility of feeding back the output of the reservoir into the
reservoir itself which in turn allows the use of different training techniques and to apply
reservoir computing to new categories of tasks such as pattern generation
In this paper we present a proposal for the readout mechanism for opto-electronic reservoirs
using an optoelectronic intensity modulator The design that we propose will drastically
cut down their operation time specially in the case of long input sequences Our proposal
is suited to optoelectronic or all-optical reservoirs but the concept can be easily extended
to any experimental time-multiplexed reservoir computer The mechanism has been tested
experimentally using the experimental reservoir reported in and compared to a digital
readout Although the results are preliminary they are promising while not as good as
those reported in they are however already better than non-reservoir methods for the
same task
Reservoir computing and time multiplexing
Principles of Reservoir Computing
The main component of a reservoir computer is a recurrent network of nonlinear
elements usually called nodes or neurons The system typically works in discrete time
and the state of each node at each time step is a function of the input value at that time
step and of the states of neighboring nodes at the previous time step The network output
is generated by a readout layer a set of linear nodes that provide a linear combination of
the instantaneous node states with fixed coefficients
The equation that describes the evolution of the reservoir computer is
wij
where is the state of the i-th node at discrete time is the total number of nodes
is the reservoir input at time mi and wij are the connection coefficients that describe
the network topology and are two parameters that regulate the network?s dynamics
and is a nonlinear function One generally tunes and to have favorable dynamics
when the input to be treated is injected in the reservoir The network output is then
constructed using a set of readout weights Wi and a bias weight Wb as
Wi Wb
Training a reservoir computer only involves the readout layer and consists in finding the
best set of readout weights Wi and bias Wb that minimize the error between the desired
output and the actual network output Unlike conventional recurrent neural networks the
Figure Scheme of the experimental setup including the optoelectronic reservoir Input
and Reservoir layers and the analog readout Output layer The red and green parts
represent respectively the optical and electronic components Arbitrary waveform
generator LiN bO3 Mach-Zehnder modulator Feedback photodiode
Amplifier Scope NI PXI acquisition card
strength of connections mi and wij are left untouched As the output layer is made only of
linear units given the full set of reservoir states for all the time steps the training
procedure is a basic regularized linear regression
Time multiplexing
The number of nodes in a reservoir computer determines an upper limit to the reservoir
performance this can be an obstacle when designing physical implementations of RCs
which should contain a high number of interconnected nonlinear units A solution to this
problem proposed in is time multiplexing the are computed one by one by
a single nonlinear element which receives a combination of the input and a previous
state In addition an input mask mi is applied to the input to enrich the
reservoir dynamics The value of is then stored in a delay line to be used at a later
time step The interaction between different neurons can be provided by either having
a slow nonlinear element which couples state to the previous states or
by using an instantaneous nonlinear element and desynchronizing the input with respect to
the delay line
Hardware RC with digital readout
The hardware reservoir computer we use in the present work is identical to the one reported
in also It uses the time-multiplexing with desynchronisation technique described in the previous paragraph We give a brief description of the experimental system
represented in the left part of Figure It uses a LiN bO3 Mach-Zehnder modulator
operating on a constant power nm laser as the nonlinear component A MZ modulator
is a voltage controlled optoelectronic device the amount of light that it transmits is a sine
function of the voltage applied to it The resulting state is encoded in a light intensity
level at the MZ output It is then stored in a spool of optical fiber acting as delay line of
duration while all the subsequent states are being computed by the MZ
modulator When a state reaches the end of the fiber spool it is converted into a
voltage by a photodiode
The input is multiplied by the input mask mi and encoded in a voltage level by an
Arbitrary Waveform Generator The two voltages corresponding to the state
at the end of the fiber spool and the input mi are added amplified and the resulting
voltage is used to drive the MZ modulator thereby producing the state and so
on for all values of
In the experiment reported in a portion of the light coming out of the MZ is deviated
to a second photodiode not shown in Figure that converts it into a voltage and sends
it to a digital oscilloscope The Mach-Zehnder output can be represented as steps of light
intensities of duration Figure each one representing the value of a single node
state at discrete time The value of each is recovered by taking an average of the
measured voltage for each state at each time step The optimal readout weights Wi and bias
Wb are then calculated on a computer from a subset training set of the recorded states
using ridge regression and the output is then calculated using equation for all
the states collected The performance of the reservoir is then calculated by comparing the
reservoir output with the desired output
Analog readout
Readout scheme
Developing an analog readout for the reservoir computer described in section means designing a device that multiplies the reservoir states shown in Figure 2a by the readout
weights Wi and that sums them together in such a way that the reservoir output
can be retrieved directly from its output However this is not straightforward to do since
obtaining good performance requires positive and negative readout weights Wi In optical
implementations the states are encoded as light intensities which are always positive so they cannot be subtracted one from another Moreover the summation over the
states must include only the values of pertaining to the same discrete time step and reject all other values This is difficult in time-multiplexed reservoirs where the states xN
and follow seamlessly
Here we show how to resolve both difficulties using the scheme depicted in the right panel of
Figure Reservoir states encoded as light intensities in the optical reservoir computer and
represented in Figure 2a are fed to the input of a second MZ modulator with two outputs
A second function generator governs the bias of the second Mach-Zehnder providing the
modulation voltage The modulation voltage controls how much of the input light
passing through the readout Mach-Zehnder is sent to each output keeping constant the
sum of the two output intensities The two outputs are connected to the two inputs of
a balanced photodiode which in turn gives as output a voltage level proportional to the
difference of the light intensities received at its two inputs1 This allows us to multiply the
reservoir states by both positive and negative weights
The time average of the output voltage of the photodiode is obtained by using a capacitor
The characteristic time of the analog integrator is proportional to the capacity The
role of this time scale is to include in the readout output all the pertinent contributions and
exclude the others The final output of the reservoir is the voltage across the capacitor at
the end of each discretized time
What follows is a detailed description of the readout design
Multiplication by arbitrary weights
The multiplication of the reservoir states by arbitrary weights positive or negative is realized by the second MZ modulator followed by the balanced photodiode The modulation
voltage that drives the second Mach Zehnder is piecewise constant with a step duration equal to the duration of the reservoir states transitions in voltages and in reservoir
states are synchronized The modulation voltage is also a periodic function of period
so that each reservoir state is paired with a voltage level Vi that doesn?t depend on
The light intensities O1 and O2 at the two outputs of the Mach-Zehnder modulator
A balanced photodiode consists of two photodiodes which convert the two light intensities
into two electric currents followed by an electronic circuit which produces as output a voltage
proportional to the difference of the two currents
In the case where the impedance of the coaxial cable is matched with the output
impedance of the photodiode we have RC
are
O1
cos((V Vbias
O2
cos((V Vbias
where is the light intensity coming from the reservoir Vbias is a constant voltage that
drives the modulator is an arbitrary constant phase value and is the half-wave
voltage of the modulator Neglecting the effect of any bandpass filter in the photodiode
and choosing Vbias appropriately the output from the photodiode can be written as
O2 sin
I(t)W
with a constant gain factor In other words by setting the right bias and driving the
modulator with a voltage we multiply the signal by an arbitrary coefficient
Note that if is piecewise constant then is as well This allows us to achieve the
multiplication of the states encoded in the light intensity by the weights Wi
just by choosing the right voltage as shown in Figure
Summation of weighted states
To achieve the summation over all the states pertaining to the same discrete time step
which according to equation will give us the reservoir output minus the bias Wb we use
the capacitor at the right side of the Output layer in Figure The capacitor provides the
integration of the photodiode output given by eq with an exponential kernel and time
constant If is significantly less than the amount of time needed for the system to
process all the nodes relative to a single time step we can minimize the crosstalk between
node states relative to different time steps
Let us consider the input of the readout and let be the instant where the state of
the first node for a given discrete time step begins to be encoded in Using equation
we can write the voltage on the capacitor at time as
I(s)W ds
For we have
for
Integrating equation yields
Equation shows that at time the voltage on the capacitor is a linear combination of
the reservoir states for the discrete time with node-dependent coefficients plus a
residual of the voltage at time multiplied by an extinction coefficient At time
the voltage on the capacitor would be a linear combination of the states for discrete time
multiplied by the same coefficients plus a residual of the voltage at time and so
on for all values of and corresponding multiples of
A simple procedure would encode the weights
onto the voltage that drives the
modulator provide an external constant bias Wb and have the output of the reservoir
defined by equation effectively encoded on the capacitor This simple procedure would
however be unsatisfactory because unavoidably some of the would be very small and
therefore the would be large spanning several orders of magnitude This is undesirable
as it requires a very precise control of the modulation voltage in order to recreate all
the values leaving the system vulnerable to noise and to any non-ideal behavior of the
modulator itself
a
Readout
Output
Voltage
Voltage
Time
Figure Reservoir output The gray line represents the output as measured by
a photodiode and an oscilloscope We indicated for

<<----------------------------------------------------------------------------------------------------------------------->>

title: 256-performance-of-connectionist-learning-algorithms-on-2-d-simd-processor-arrays.pdf

Nunez and Fortes
Performance of Connectionist Learning Algorithms
on SIMD Processor Arrays
Fernando J. Nunez and Jose A.B. Fortes
School of Electrical Engineering
Purdue University
West Lafayette IN
ABSTRACT
The mapping of the back-propagation and mean field theory
learning algorithms onto a generic SIMD computer is
described This architecture proves to be very adequate for these
applications since efficiencies close to the optimum can be
attained Expressions to find the learning rates are given and
then particularized to the DAP array procesor
INTRODUCTION
The digital simulation of connectionist learning algorithms is flexible and
accurate However with the exception of very small networks conventional
computer architectures spend a lot of time in the execution of simulation
software Parallel computers can be used to reduce the execution time Vectorpipelined multiprocessors and array processors are some of the most important
classes of parallel computers Connectionist or neural net learning
algorithms have been mapped onto all of them
The focus of this contribution is on the mapping of the back-propagation
and mean field theory MFT learning algorithms onto the subclass of SIMD
computers with the processors arranged in a square two-dimensional mesh and
interconnected by nearest-neighbor links
The material is organized as follows In section the execution cost of BP and
MFT on sequential computers is found Two-dimensional SIMD processor arrays
are described in section and the costs of the two dominanting operations in the
simulations are derived In section the mapping of BP and MFT is I;ommented
Current address Motorola Inc
Algonquin Schaumburg IL
Performance of Connectionist Learning Algorithms
and expressions for the learning rates are obtained These expressions are
particularized to the DAP computer in section Section concludes this work
BACK-PROPAGATION AND MEAN FIELD THEORY
In this paper two learning algorithms Bp and MFT4 and 3-layer nets are
considered The number of neurons in the input hidden and output layer is I
and respectively BP has been used in many applications Probably NETtalk8
is the best known MFT can also be used to learn arbitrary mappings between
two sets and remarkably to find approximate solutions to hard optimization
problems much more efficiently than a Boltzmann Machine does
The
Vj
output
ajjvj
of
a
neuron
will be denoted as
Vi
and
called
value
OJ). The summation represents the net input received and will
j'l"j
be called activation The neuron thresold is OJ. A sigmoid-like function is
applied to find the value The weight of the link from neuron to neuron is ajj
Since input patterns are the values of the I layer only neuron values and
activations of the Hand layers must be computed In BP the activation error
and the value error of the Hand layers are calculated and used to change the
weights
In a conventional computer the execution time of BP is approximately the time
spent in finding the activations back-propagating the activation error of the
layer and modifying the I-H and H-O weights The result is
where tm is the time required to perform a multiply/accumulate operation Since
the net has I O)H connections the learning rate in connections per second is
fNBP
CPS
In the MFT algorithm only from the neuron values in equilibrium at the end of
the clamped and free annealing phases we can compute the weight increments It
is assumed that in both phases there are A annealing temperature nd that
iterations are enough to reach equilibrium at each temperature With these
changes MFT is now a deterministic algorithm where the anne ling phases are
composed of AE sweeps The MFT execution time can be apprl?"jmated by the
time spent in computing activations in the annealing loops J,ing into account
that in the clamped phase only the layer is updated and tha in the free phase
both the Hand layers change their values the MFT leaning performance is
found to be
ft
tMFT
tBP
AE
CPS
MFT is AE times more expensive than BP. However the learning qualities of
both algorithms are different and such a direct cOP'tJarison is simplistic
Nunez and Fortes
SIMD PROCESSOR ARRAYS
Two-dimensional single instruction multiple data stream SIMD computers
are very efficient in the simulation of NN learning algorithms They can provide
massive parallelism at low cost An SIMD computer is an array of processing
elements PEs that execute the same instruction in each cycle There is a single
control unit that broadcasts instructions to all the PEs. SIMD architectures
operate in a synchronous lock-step fashion They are also called array procesors
because their raison cfetre is to operate on vectors and matrices
Example SIMD computers are the Illiac-IV the Massively Parallel Processor
the Connection Machine and the Distributed Array Processor
With the exception of the CM whose PE interconnection topology is a
hypercube the other three machines are SThAD arrays because their PEs are
interconnected by a mesh with wrap-around links figure
CONTROL
UNIT
pp
Figure A SIMD Processor Array
Each PE has its own local memory The instruction has an address field to access
it The array memory space can be seen as a volume This volume is
generated by the PE plane and the depth is the number of memory words that
each PE can address When the control unit issues an address a plane of the
memory volume is being referenced Then square blocks of PxP elements are the
natural addressing unit of SThAD processor arrays There is an activity bit
register in each PE to disable the execution of instructions This is useful to
perform operations with a subset of the PEs. It is assumed that there is no
Performance of Connectionist Learning Algorithms
overlapping between data processing an data moving operations In other words
PEs can be either performing some operation on data this includes accessing the
local memory or exchanging data with other processors
MAPPING THE TWO BASIC OPERATIONS
It is characteristic of array processors that the way data is allocated into the PEs
memories has a very important effect on performance For our purposes two
data structures must be considered vectors and matrices The storage of vectors
is illustrated in figure There are two modes row and column A vector is
split into P-element subvectors stored in the same memory plane Very large
vectors will require two or more planes The storage of matrices is also very
simple They must be divided into square PXP blocks figure The shading
in figure indicates that in general the sizes of vectors and matrices do not fit
the array dimensions perfectly
row
IIJ
column
Figure Vector and Matrix Storage
The execution time of BP and MFT in a SIMD computer is spent almost
completely
in
matrix-vector
multiply
MVM
and
vector
outer
multiply/accumulate VOM operations They can be decomposed in the
following simpler operations involving PxP blocks
Addition A such that eij aij bij
Point multiply/accumulate
A-B such that e'ij eij aijb ij
Unit rotation The result block has the same elements than the original but
rotated one place in one of the four possible directions and S).
Row column broadcast The result of the row column broadcast of a vector
stored in row column mode is a block such that xii Xj Xi).
a
The time required to execute a and will be denoted as tll tm and t6
respectively Next let us see how the operation Ax MVM is decomposed in
simpler steps using the operations above Assume that and yare P-element
vectors and A is a PXP block
Nunez and Fortes
Row-broadcast vector
Point multiply A?X.
Row addition of block
Yi f'llij
aijxj
j-l
This requires flOg2pl steps In
each step multiple rotations and one addition are performed Figure shows how
eight values in the same row are added using the recursive doubling technique
Note that the number of rotations doubles in each step The cost is
Pt log2Pto Row addition is an inefficient operation because of the large cost
due to communication Fortunately for larger data its importance can be
diminished by using the scheduling described nextly
Figure Recursive Doubling
Suppose that and A have dimensions MP NP and nxm
respectively Then Ax must be partitioned into a sequence of nonpartitioned block operations as the one explained above We can write
Aijxj Aij?Xj)u
Aij.Xj)u
In this expression and represent the i-th and i-th P-element subvector of
and respectively and A ij is the PxP block of A with indices and Block
is the result of row-broadcasting is stored in row mode Finally is a
vector with all its P-elements equal to Note that in the second term column
additions are implicit while only one is required in the third term because blocks
instead of vectors are accumulated Since has subvectors and the
subvectors of are broadcast only once the total cost of the MVM operation is
Mter a similar development the cost of the YOM At A
yx
operation is
Performance of Connectionist Learning Algorithms
If the number of neurons in each layer is not an integer multiple of the storage
and execution efficiencies decrease This effect is less important in large networks
LEARNING RATES ON SIMD COMPUTERS
BACK-PROPAGATION
The neuron val~es activations value errors activation errors and thresolds of
the Hand layers are organized as vectors The weights are grouped into two
matrices I-H and H-O. Then the scalar operations of the original algorithm are
transformed into matrix-vector operations
From now on the size of the input hidden and output layers will be IP HP and
OP. commented before the execution time is mostly spent in computing
activations values their errors and in changing the weights To compute
activations and to back-propagate the activation error of the layer MVM
operations are performed The change of weights requires YOM operations Alter
substituting the expressions of the previous section the time required to learn a
pattern simulating BP on a SIMD computer is
The time spent in data communication is given by the factors in tr and The
larger they are the smaller is the efficiency For array processors with fast
broadcast facilities and for nets large enough in terms of the array dimensions
the efficiency grows since a smaller fraction of the total execution time is
dedicated to moving data Since the net has I O)HP2 connections the
learning rate is p2 times greater than using a single PE
I
NSIMD-BP
CPS
MEAN FIELD THEORY
The operations outside the annealing loops can be neglected with small error In
consequence only the computation of activations in the clamped and free
annealing phases is accounted for
O)(Ptr log2Pta
Under the same favorable conditions above mentioned the learning rate is
SIMD-MFT
I
CPS
Nunez and Fortes
LEARNING PERFORMANCE ON THE DAP
The DAP is a commercial SIMD processor array developed by lCL It is a
massively parallel computer with bit-level PEs built around a single-bit full
adder In addition to the PE interconnection mesh there are row and column
broadcast buses that allow the direct transfer of data from any processor row or
column to an edge register Many instructions require a single clock cycle leading
to very efficient codings of loop bodies The computer features
PEs with a maximum local memory of 1Mbit per PE. The has 26
PEs and the maximum local memory IS 64Kbit The clock cycle in both
machines is nsl
With bit-level processors it is possible to tailor the preCISIon of fixed-point
computations to the minimum required by the application The costs in cycles
required by several basic operations are given below These expressions are
function of the number of bits of the operands that has been assumed to be the
same for all of them bits
The time required by the DAP to perform a block addition point
multiplication/accumulation and broadcast is to tm 2b and t6 8b
clock cycles respectively On the other hand 2b log2P cycles is the duration
of a row addition Let us take bits and AE 24 This values have been
found adequate in many applications Then the maximum learning rates of the
are
MCPS
BP
MFT MCPS
where MCPS CPS. These figures are times smaller for the It is
worth to mention that the performance decreases quadratically with The two
learning rates of each algorithm correspond to the worst and best case topology
EXAMPLES
Let us consider a one-thousand neuron net with and neurons in the
input hidden and output layer For the we have and
The other parameters are the same than used above After substituting
we see that the communication costs are less than of the total
demonstrating the efficiency of the DAP in this type of applications The learning
rates are
BP
MCPS
MFT MCPS
NETtalk is frequently used as a benchmark in order to compare the
performance achieved on different computers Here a network with similar
dimensions is considered input 64 hidden and 32 output neurons These
dimensions fit perfectly into the since 32 before a data
precision of bits has been taken However the fact than the input patterns are
binary has been exploited to obtain some savings
The performance reached in this case is MCPS Even though NETtalk is a
relatively small network only of the total execution time is spent in data
communication If the were used somewhat less than MCPS would
be learnt since the output layer is smaller than what causes some inefficiency
Performance of Connectionist Learning Algorithms
Finally BP learning rates of the with and operands are
compared to those obtained by other machines below
COMPUTER
VAX
CRAY-2
CM PEs
bits
bits
MCPS
13
CONCLUSIONS
Two-dimensional SThfl array processors are very adequate for the simulation of
connectionist learning algorithms like BP and MFT These architectures can
execute them at nearly optimum speed if the network is large enough and there is
full connectivity between layers Other much more costly parallel architectures
are outperformed
The mapping approach described in this paper can be easily extended to any
network topology with dense blocks in its global interconnection matrix
However it is obvious that SIMD arrays are not a good option to simulate
networks with random sparse connectivity
Acknow ledgements
This work has been supported by the Ministry of Education and Science of Spain

<<----------------------------------------------------------------------------------------------------------------------->>

title: 2571-using-machine-learning-to-break-visual-human-interaction-proofs-hips.pdf

Using Machine Learning to Break Visual
Human Interaction Proofs HIPs
Kumar Chellapilla
Microsoft Research
One Microsoft Way
Redmond WA
kumarc@microsoft.com
Patrice Y. Simard
Microsoft Research
One Microsoft Way
Redmond WA
patrice@microsoft.com
Abstract
Machine learning is often used to automatically solve human tasks
In this paper we look for tasks where machine learning algorithms
are not as good as humans with the hope of gaining insight into
their current limitations We studied various Human Interactive
Proofs HIPs on the market because they are systems designed to
tell computers and humans apart by posing challenges presumably
too hard for computers We found that most HIPs are pure
recognition tasks which can easily be broken using machine
learning The harder HIPs use a combination of segmentation and
recognition tasks From this observation we found that building
segmentation tasks is the most effective way to confuse machine
learning algorithms This has enabled us to build effective HIPs
which we deployed in MSN Passport as well as design
challenging segmentation tasks for machine learning algorithms
In rod ct on
The OCR problem for high resolution printed text has virtually been solved years
ago On the other hand cursive handwriting recognition today is still too poor
for most people to rely on Is there a fundamental difference between these two
seemingly similar problems
To shed more light on this question we study problems that have been designed to
be difficult for computers The hope is that we will get some insight on what the
stumbling blocks are for machine learning and devise appropriate tests to further
understand their similarities and differences
Work on distinguishing computers from humans traces back to the original Turing
Test which asks that a human distinguish between another human and a machine
by asking questions of both Recent interest has turned to developing systems that
allow a computer to distinguish between another computer and a human These
systems enable the construction of automatic filters that can be used to prevent
automated scripts from utilizing services intended for humans Such systems
have been termed Human Interactive Proofs HIPs or Completely Automated
Public Turing Tests to Tell Computers and Humans Apart CAPTCHAs An
overview of the work in this area can be found in Construction of HIPs that are
of practical value is difficult because it is not sufficient to develop challenges at
which humans are somewhat more successful than machines This is because the
cost of failure for an automatic attacker is minimal compared to the cost of failure
for humans Ideally a HIP should be solved by humans more than of the time
while an automatic script with reasonable resource use should succeed less than
of the time This latter ratio in is a function of the cost of an
automatic trial divided by the cost of having a human perform the attack
This constraint of generating tasks that are failed of the time by all
automated algorithms has generated various solutions which can easily be sampled
on the internet Seven different HIPs namely Mailblocks MSN before April
Ticketmaster Yahoo Yahoo v2 after Register and Google will
be given as examples in the next section We will show in Section that machinelearning-based attacks are far more successful than in Yet some of these
HIPs are harder than others and could be made even harder by identifying the
recognition and segmentation parts and emphasizing the latter Section presents
examples of more difficult HIPs which are much more respectable challenges for
machine learning and yet surprisingly easy for humans The final section discusses
a known weakness of machine learning algorithms and suggests designing simple
artificial datasets for studying this weakness
Exa mp les I Ps
The HIPs explored in this paper are made of characters symbols rendered to an
image and presented to the user Solving the HIP requires identifying all characters
in the correct order The following HIPs can be sampled from the web
Mailblocks While signing up for free email service with
www.mailblocks.com you will find HIP challenges of the type
mailblocks
MSN While signing up for free e-mail with MSN Hotmail www.hotmail.com you
will find HIP challenges of the type
Register.com While requesting a whois lookup for a domain at www.register.com
you will HIP challenges of the type
Yahoo!/EZ-Gimpy While signing up for free e-mail service with Yahoo
www.yahoo.com you will receive HIP challenges of the type
Yahoo version Starting in August Yahoo introduced their second
generation HIP. Three examples are presented below
Ticketmaster While looking for concert tickets at www.ticketmaster.com you
will receive HIP challenges of the type
Google/Gmail While signing up for free e-mail with Gmail at www.google.com
one will receive HIP challenges of the type
While solutions to Yahoo HIPs are common English words those for ticketmaster
and Google do not necessarily belong to the English dictionary They appear to have
been created using a phonetic generator
Usi ma ch lea rn rea IP
Breaking HIPs is not new Mori and Malik have successfully broken the EZGimpy success and Gimpy success HIPs from CMU. Our approach
aims at an automatic process for solving multiple HIPs with minimum human
intervention using machine learning In this paper our main goal is to learn more
about the common strengths and weaknesses of these HIPs rather than to prove that
we can break any one HIP in particular with the highest possible success rate We
have results for six different HIPs EZ-Gimpy/Yahoo Yahoo mailblocks
register ticketmaster and Google
To simplify our study we will not be using language models in our attempt to break
HIPs For example there are only about words in the EZ-Gimpy dictionary
which means that a random guess attack would get a success rate of in more
than enough to break the HIP greater than success HIPs become harder
when no language model is used Similarly when a HIP uses a language model to
generate challenges success rate of attacks can be significantly improved by
incorporating the language model Further since the language model is not common
to all HIPs studied it was not used in this paper
Our generic method for breaking all of these HIPs is to write a custom algorithm to
locate the characters and then use machine learning for recognition Surprisingly
segmentation or finding the characters is simple for many HIPs which makes the
process of breaking the HIP particularly easy Gimpy uses a single constant
predictable color black for letters even though the background color changes We
quickly realized that once the segmentation problem is solved solving the HIP
becomes a pure recognition problem and it can trivially be solved using machine
learning Our recognition engine is based on neural networks It yielded a
error rate on the MNIST database uses little memory and is very fast for
recognition important for breaking HIPs
For each HIP we have a segmentation step followed by a recognition step It
should be stressed that we are not trying to solve every HIP of a given type our
goal is not success rate but something efficient that can achieve much better
than
In each of the following experiments HIPs were hand labeled and used as
follows recognition for training for validation and for testing
and segmentation for testing segmentation For each of the five HIPs a
convolution neural network identical to the one described in was trained and
tested on gray level character images centered on the guessed character positions
below The trained neural network became the recognizer
a oc
To solve the HIP we select the red channel binarize and erode it extract the largest
connected components and breakup CCs that are too large into two or three
adjacent CCs. Further vertically overlapping half character size CCs are merged
The resulting rough segmentation works most of the time Here is an example
For instance in the example above the NN would be trained and tested on the
following images
The end-to-end success rate is for segmentation for recognition
given correct segmentation and total Note that most of
the errors come from segmentation even though this is where all the custom
programming was invested
Register
The procedure to solve HIPs is very similar The image was smoothed binarized
and the largest connected components were identified Two examples are
presented below
The end-to-end success rate is for segmentation for recognition
given correct segmentation and total
a oo mp
Unlike the mailblocks and register HIPs the Yahoo/EZ-Gimpy HIPs are richer in
that a variety of backgrounds and clutter are possible Though some amount of text
warping is present the text color size and font have low variability Three simple
segmentation algorithms were designed with associated rules to identify which
algorithm to use The goal was to keep these simple yet effective
No mesh Convert to grayscale image threshold to black and white select
large CCs with sizes close to HIP char sizes One example
Black mesh Convert to grayscale image threshold to black and white
remove vertical and horizontal line pixels that don?t have neighboring
pixels select large CCs with sizes close to HIP char sizes One example
White mesh Convert to grayscale image threshold to black and white add
black pixels white line locations if there exist neighboring pixels select
large CCs with sizes close to HIP char sizes One example
Tests for black and white meshes were performed to determine which segmentation
algorithm to use The end-to-end success rate was for segmentation
came from from and from for recognition given
correct segmentation and total The average length of a
Yahoo HIP solution is characters
ma
The procedure that solved the Yahoo HIP is fairly successful at solving some of the
ticket master HIPs These HIPs are characterized by cris-crossing lines at random
angles clustered around 45 90 and degrees A multipronged attack as in the
Yahoo case section has potential In the interests of simplicity a single attack
was developed Convert to grayscale threshold to black and white up-sample
image dilate first then erode select large CCs with sizes close to HIP char sizes
One example
The dilate-erode combination causes the lines to be removed along with any thin
objects but retains solid thick characters This single attack is successful in
achieving an end-to-end success rate of for segmentation the recognition rate
was spite of interfering lines and total The
average HIP solution length is characters
a oo ve on
The second generation HIP from Yahoo had several changes it did not use words
from a dictionary or even use a phonetic generator it uses only black and white
colors uses both letters and digits and uses connected lines and arcs as clutter
The HIP is somewhat similar to the MSN/Passport HIP which does not use a
dictionary uses two colors uses letters and digits and background and foreground
arcs as clutter Unlike the MSN/Passport HIP several different fonts are used A
single segmentation attack was developed Remove pixel border up-sample dilate
first then erode select large CCs with sizes close to HIP char sizes The attack is
practically identical to that used for the ticketmaster HIP with different
preprocessing stages and slightly modified parameters Two examples
This single attack is successful in achieving an end-to-end success rate of for
segmentation the recognition rate was and total
The average HIP solution length is characters
oog a
The Google HIP is unique in that it uses only image warp as a means of distorting
the characters Similar to the MSN/Passport and Yahoo version HIPs it is also
two color The HIP characters are arranged closed to one another they often touch
and follow a curved baseline The following very simple attack was used to segment
Google HIPs Convert to grayscale up-sample threshold and separate connected
components
This very simple attack gives an end-to-end success rate of for segmentation
the recognition rate was giving total probability
of breaking a HIP. Average Google HIP solution length is characters This can
be significantly improved upon by judicious use of dilate-erode attack A direct
application doesn?t do as well as it did on the ticketmaster and yahoo HIPs because
of the shear and warp of the baseline of the word More successful and complicated
attacks might estimate and counter the shear and warp of the baseline to achieve
better success rates
Lesso lea rn ed ro rea ki IPs
From the previous section it is clear that most of the errors come from incorrect
segmentations even though most of the development time is spent devising custom
segmentation schemes This observation raises the following questions Why is
segmentation a hard problem Can we devise harder HIPs and datasets Can we
build an automatic segmentor Can we compare classification algorithms based on
how useful they are for segmentation
me a on ob
As a review segmentation is difficult for the following reasons
Segmentation is computationally expensive In order to find valid patterns a
recognizer must attempt recognition at many different candidate locations
The segmentation function is complex To segment successfully the system
must learn to identify which patterns are valid among the set of all possible
valid and non-valid patterns This task is intrinsically more difficult than
classification because the space of input is considerably larger Unlike the space
of valid patterns the space of non-valid patterns is typically too vast to sample
This is a problem for many learning algorithms which yield too many false
positives when presented non-valid patterns
Identifying valid characters among a set of valid and invalid candidates is a
combinatorial problem For example correctly identifying which characters
among candidates assuming false positives has a in
choose chances of success by random guessing
ui te a de I
We can use what we have learned to build better HIPs For instance the HIP below
was designed to make segmentation difficult and a similar version has been
deployed by MSN Passport for hotmail registrations www.hotmail.com
The idea is that the additional arcs are themselves good candidates for false
characters The previous segmentation attacks would fail on this HIP. Furthermore
simple change of fonts distortions or arc types would require extensive work for
the attacker to adjust to We believe HIPs that emphasize the segmentation problem
such as the above example are much stronger than the HIPs we examined in this
paper which rely on recognition being difficult Pushing this to the extreme we can
easily generate the following HIPs
Despite the apparent difficulty of these HIPs humans are surprisingly good at
solving these indicating that humans are far better than computers at segmentation
This approach of adding several competing false positives can in principle be used
to automatically create difficult segmentation problems or benchmarks to test
classification algorithms
ui a a ut ma me or
To build an automatic segmentor we could use the
following procedure Label characters based on
their correct position and train a recognizer Apply
the trained recognizer at all locations in the HIP
image Collect all candidate characters identified
with high confidence by the recognizer Compute
the probability of each combination of candidates
going from left to right and output the solution
string with the highest probability This is better
illustrated with an example
Consider the following HIP to the right The
trained neural network has these maps warm
colors indicate recognition that show that
and so on are correctly identified However the
maps for and show several false positives In
general we would get the following color coded
map for all the different candidates
HIP
With a threshold of on the network?s outputs the map obtained is
We note that there are several false positives for each true positive The number of
false positives per true positive character was found to be between and giving a
in to in random chance of guessing the
correct segmentation for the HIP characters These numbers can be improved upon
by constraining solution strings to flow sequentially from left to right and by
restricting overlap For each combination we compute a probability by multiplying
the probabilities of the classifier for each position The combination with the
highest probability is the one proposed by the classifier We do not have results for
such an automatic segmentor at this time It is interesting to note that with such a
method a classifier that is robust to false positives would do far better than one that
is not This suggests another axis for comparing classifiers
Con clu si on
In this paper we have successfully applied machine learning to the problem of
solving HIPs We have learned that decomposing the HIP problem into
segmentation and recognition greatly simplifies analysis Recognition on even
unprocessed images given segmentation is a solved can be done automatically
using neural networks Segmentation on the other hand is the difficulty
differentiator between weaker and stronger HIPs and requires custom intervention
for each HIP. We have used this observation to design new HIPs and new tests for
machine learning algorithms with the hope of improving them
A ow ge me
We would like to acknowledge Chau Luu and Eric Meltzer for their help with
labeling and segmenting various HIPs We would also like to acknowledge Josh
Benaloh and Cem Paya for stimulating discussions on HIP security

<<----------------------------------------------------------------------------------------------------------------------->>

title: 383-back-propagation-implementation-on-the-adaptive-solutions-cnaps-neurocomputer-chip.pdf

Back Propagation Implementation on the
Adaptive Solutions CNAPS Neurocomputer Chip
Hal McCartor
Adaptive Solutions Inc.
N.W. Compton Drive
Suite
Beaverton OR
Abstract
The Adaptive Solutions CN APS architecture chip is a general purpose
neurocomputer chip It has 64 processors each with bytes of local
memory running at megahertz It is capable of implementing most
current neural network algorithms with on chip learning This paper discusses the implementation of the Back Propagation algorithm on an array
of these chips and shows performance figures from a clock accurate hardware simulator An eight chip configuration on one board can update
billion connections per second in learning mode and process billion
connections per second in feed forward mode
Introduction
The huge computational requirements of neural networks and their natural parallelism have led to a number of interesting hardware innovations for executing such
networks Most investigators have created large parallel computers or special purpose chips limited to a small subset of algorithms The Adaptive Solutions CNAPS
architecture describes a general-purpose 64-processor chip which supports on chip
learning and is capable of implementing most current algorithms Implementation
of the popular Back Propagation algorithm will demonstrate the speed and
versatility of this new chip
Back Propagation Implementation
The Hardware Resources
The Adaptive Solutions CNAPS architecture is embodied in a single chip digital
neurocomputer with 64 processors running at megahertz All processors receive
the same instruction which they conditionally execute Multiplication and addition
are performed in parallel allowing billion inner product steps per second per
chip Each processor has a adder 9-bit by multiplier by in two
clock cycles shifter logic unit 32 registers and bytes oflocal memory
Input and output are accomplished over 8-bit input and output buses common
to all processors The output bus is tied to the input bus so that output of one
processor can be broadcast to all others When multiple chips are used they appear
to the user as one chip with more processors Special circuits support finding the
maximum of values held in each processor and conserving weight space for sparsely
connected networks An accompanying sequencer chip controls instruction flow
input and output
The Back Propagation Algorithm Implementation
Three critical issues must be addressed in the parallel implementation of BP on efficient hardware These are the availability of weight values for back propagating the
error the scaling and precision of computations and the efficient implementation
of the output transfer function
BP requires weight values at different nodes during the feed forward and back
propagation phases of computation This problem is solved by having a second set
of weights which is the transpose of the output layer weights These are located on
hidden node processors The two matrices are updated identically The input to the
hidden layer weight matrix is not used for error propagation and is not duplicated
BP implementations typically use floating point math This largely eliminates
scaling precision and dynamic range issues Efficient hardware implementation
dictates integer arithmetic units with precision no greater than required Baker
has shown integer weights are sufficient for BP training and much
lower values adequate for use after training
With fixed point integer math the position of the binary point must be chosen In
this implementation weights are bits and use bits to the right of the binary
point and four to the left including a sign bit They range from to Input
and output are represented as 8-bit unsigned integers with binary point at the left
The leaning rate is represented as an 8-bits integer with two bits to the left of the
binary point and values ranging from to Error is represented as bit
signed integers at the output layer and with the same representation as the weights
at the hidden layer
This data representation has been used to train benchmark BP applications with
results comparable to the floating point versions
The BP sigmoid output function is implemented as an 8-bit by lookup table
During the forward pass input values are broadcast to all processors from off chip
via the input bus or from hidden nodes via the output bus to the input bus During
McCartor
the backward error propagation error values are broadcast from the output nodes
to hidden nodes
The typical BP network has two computational layers the hidden and output layers
They can be assigned to the same or different processor nodes PN depending on
available memory for weights PNs used for the hidden layer contain the transpose
weights of the output layer for back propagating error If momentum or periodic
weight update are used additional storage space is allocated with each weight
In this implementation BP can be mapped to any set of contiguous processors
allowing multiple networks in CNAPS memory simultaneously Thus the output
of one algorithm can be directly used as input to another For instance in speech
recognition a Fourier transform performed on the PN array could be input to a
series of matched BP networks whose hidden layers run concurrently Their output
could be directed to an LVQ2 network for final classification This can all be
accomplished without any intermediate results leaving the chip array
Results
BP networks have been successfully run on a hardware clock accurate simulator
which gives the following timing results In this example an eight-chip implementation processors was used The network had inputs hidden nodes
and outputs Weights were updated after each input and no momentum was
used The following calculations show BP performance
TRAINING PHASE
Overhead clock cycles per input vector
Cycles per input vector element
Cycles per hidden node
Cycles per output node
Cycles per vector
Vectors per second
Total forward weights
Weight updates per second
FEED FORWARD ONLY
Overhead cycles per input vector 59
Cycles per input vector element
Cycles per hidden node
Cycles per output node for output of data
Cycles per vector
Vectors per second
Connections per second
Back Propagation Implementation
Comparative Performance
An array of eight Adaptive Solutions CN APS chips would execute the preceding BP
network at billion training weight updates per second or billion feed forward
connections per second These results can be compared with the results on other
computers shown in table
MACHINE
SUN lD88j
SAle SIGMA-llD88j
WARP
CRAY lPGTK88J
CRAY X-MP lD88J
lWZ89j
ADAPTIVE CN APS chips
MCUPS
MCPS
17
WTS
fp
fp
fp
fp
fp
fp
fp
bit int
Table Comparison of BP performance for various computers and Adaptive
Solutions CNAPS chips on one board MCUPS is Millions of BP connection updates
per second in training mode MCPS is millions of connections processed per second
in feed forward mode WTS is representation used for weights
Summary
The Adaptive Solutions CN APS chip is a very fast general purpose digital neurocomputer chip It is capable of executing the Back Propagation algorithm quite
efficiently An chip configuration can train billion connections per second and
evaluate billion BP feed forward connections per second

<<----------------------------------------------------------------------------------------------------------------------->>

title: 6261-visual-question-answering-with-question-representation-update-qru.pdf

Visual Question Answering with
Question Representation Update QRU
Ruiyu Li
Jiaya Jia
The Chinese University of Hong Kong
ryli,leojia}@cse.cuhk.edu.hk
Abstract
Our method aims at reasoning over natural language questions and visual images
Given a natural language question about an image our model updates the question
representation iteratively by selecting image regions relevant to the query and
learns to give the correct answer Our model contains several reasoning layers
exploiting complex visual relations in the visual question answering VQA task
The proposed network is end-to-end trainable through back-propagation where its
weights are initialized using pre-trained convolutional neural network CNN and
gated recurrent unit Our method is evaluated on challenging datasets of
COCO-QA and VQA and yields state-of-the-art performance
Introduction
Visual question answering VQA is a new research direction as intersection of computer vision and
natural language processing Developing stable systems for VQA attracts increasing interests in
multiple communities Possible applications include bidirectional image-sentence retrieval human
computer interaction blind person assistance etc It is now still a difficult problem due to many
challenges in visual object recognition and grounding natural language representation and common
sense reasoning
Most recently proposed VQA models are based on image captioning 24 These methods
have been advanced by the great success of deep learning on building language models image
classification and on visual object detection Compared with image captioning where a
plausible description is produced for a given image VQA requires algorithms to give the correct
answer to a specific human-raised question regarding the content of a given image It is a more
complex research problem since the method is required to answer different types of questions
An example related to image content is What is the color of the There are also
questions requiring extra knowledge or commonsense reasoning such as Does it appear to be
rainy
Properly modeling questions is essential for solving the VQA problem A commonly employed
strategy is to use a CNN or an RNN to extract semantic vectors The general issue is that the resulting
question representation lacks detailed information from the given image which however is vital
for understanding visual content We take the question and image in Figure as an example To
answer the original question What is sitting amongst things have been abandoned
one needs to know the target object location Thus the question can be more specific as What is
discarded on the side of a building near an old book shelf
In this paper we propose a neural network based reasoning model that is able to update the question
representation iteratively by inferring image information With this new system it is now possible
to make questions more specific than the original ones focusing on important image information
automatically Our approach is based on neural reasoner which has recently shown remarkable
Conference on Neural Information Processing Systems NIPS Barcelona Spain
Question What is sitting
amongst things have been
abandoned
Answer Toilet
Before
What sits in the
room that appears to be
partially abandoned
Updated What is discarded
on the side of a building
near an old book shelf
Figure The questions asked by human can be ambiguous given an image containing various objects
The Before and Updated questions are the most similar ones based on the cosine similarity to the
original Question before and after applying our algorithm to update representation shows the
attention masks generated by our model
success in text question answering tasks Neural reasoner updates the question by interacting it with
supporting facts through multiple reasoning layers We note applying this model to VQA is nontrivial
since the facts are in the form of an image Thus image region information is extracted in our
model To determine the relevance between question and each image region we employ the attention
mechanism to generate the attention distribution over regions of the image Our contributions are as
follows
We present a reasoning network to iteratively update the question representation after each
time the question interacts with image content
Our model utilizes object proposals to obtain candidate image regions and has the ability to
focus on image regions relevant to the question
We evaluate and compare the performance of our model on two challenging VQA datasets
COCO-QA and VQA Experiments demonstrate the ability of our model to infer image
regions relevant to the question
Related Work
Research on visual question answering is mostly driven by text question answering and image
captioning methods In natural language processing question answering is a well-studied problem In
an end-to-end memory network was used with a recurrent attention model over a large external
memory Compared with the original memory network it has less supervision and shows comparable
results on the QA task The neural reasoning system proposed in named neural reasoner can
utilize multiple supporting facts and find an answer Decent performance was achieved on positional
reasoning and path finding QA tasks
VQA is closely related to image captioning 24 28 In a set of likely words are detected
in several regions of the image and are combined together using a language model to generate image
description In a structured max-margin objective was used for deep neural networks It learns to
embed both visual and language data into a common multi-modal space Vinyals extracted
high-level image feature vectors from CNN and took them as the first input to the recurrent network
to generate caption Xu integrated visual attention in the recurrent network The proposed
algorithm predicts one word at a time by looking at local image regions relevant to the currently
generated word
Malinowski first introduced a solution addressing the VQA problem It combines natural
language processing with semantic segmentation in a Bayesian framework for automatic question
answering Since it several neural network based models 19 were proposed to solve the
VQA problem These models use CNN to extract image features and recurrent neural networks to
embed questions The embedded image and question features are then fused by concatenation
Image Understanding
Region
Image
Question
What are they playing
Query
Query
GRU
Query
Query
SoftMax
Region
Query
CNN
Region
Reasoning
Question Encoding
Answering
Figure The overall architecture of our model with single reasoning layer for VQA.
or element-wise addition to predict answers Recently several models integrated the attention
mechanism 27 and showed the ability of their networks to focus on image regions related
to the question
There also exist other approaches for VQA. For example Xiong proposed an improved
dynamic memory network to fuse the question and image region representations using bi-directional
GRU. The algorithm of learns to compose a network from a collection of composable modules
Ma made use of CNN and proposed a model with three CNNs to capture information of
the image question and multi-modal representation
Our Model
The overall architecture of our model is illustrated in Figure The model is derived from the neural
reasoner which is able to update the representation of question recursively by inferring over
multiple supporting facts Our model yet contains a few inherently different components Since
VQA involves only one question and one image each time instead of a set of facts we use object
proposal to obtain candidate image regions serving as the facts in our model Moreover in the
pooling step we employ an attention mechanism to determine the relevance between representation
of original questions and updated ones Our network consists of four major components image
understanding question encoding reasoning and answering layers
Image Understanding Layer
The image understanding layer is designed for modeling image content into semantic vectors We
build this layer upon the VGG model with 19 weight layers It is pre-trained on ImageNet
The network has sixteen convolutional layers and five max-pooling layers of kernel size with
stride followed by two fully-connected layers with neurons
Using a global representation of the image may fail to capture all necessary information for answering
the question involving multiple objects and spatial configuration Moreover since most of the
questions are related to objects we utilize object proposal generator to produce a set of
candidate regions that are most likely to be an object For each image we choose candidate regions
by extracting the top 19 detected edge boxes We choose intersection over union IoU value
when performing non-maximum suppression which is a common setting in object detection
Additionally the whole image region is added to capture the global information in the image
understanding layer resulting in candidate regions per image We extract features from each
candidate region through the above mentioned CNN bringing a dimension of image region
features The extracted features however lack spatial information for object location To remedy this
issue we follow the method of to include an 8D representation
xmin ymin xmax ymax xcenter ycenter wbox hbox
where wbox and hbox are the width and height of the image region We set the image center as the
origin The coordinates are normalized to range from to Then each image region is represented
as a feature denoted as fi where For modeling convenience we use a single layer
perceptron to transform the image representation into a common latent space shared with the question
feature
vi Wvf fi bvf
where is the rectified activation function
Question Encoding Layer
To encode the natural language question we resort to the recurrent neural network which has
demonstrated great success on sentence embedding The question encoding layer is composed of a
word embedding layer and GRU cells Given a question wT where wt is the tth word
in the question and is the length of the question we first embed each word wt to a vector space
with an embedding matrix We wt Then for each time step we feed into GRU sequentially
At each step the GRU takes one input vector and updates and outputs a hidden state ht The final
hidden state hT is considered as the question representation We also embed it into the common
latent space same as image embedding through a single layer perceptron
Wqh hT bqh
We utilize the pre-trained network with skip-thought vectors model designed for general sentence
embedding to initialize our question encoding layer as used in Note that the skip-thought vectors
model is trained in an unsupervised manner on large language corpus By fine-tuning the GRU we
transfer knowledge from natural language corpus to the VQA problem
Reasoning Layer
The reasoning layer includes question-image interaction and weighted pooling
Question-Image Interaction Given that multilayer perceptron MLP has the ability to determine
the relationship between two input sentences according to supervision We examine image
region features and question representation to acquire a good understanding of the question In a
memory network these image region features are akin to the input memory representation
which can be retrieved for multiple times according to the question
There are a total of reasoning layers In the lth reasoning layer the ith interaction happens between
and vi through an MLP resulting in updated question representation qil as
qil LPl vi
with being the model parameter of interaction at the lth reasoning layer In the simplest case with
one single layer in LPl the updating process is given by
qil vi bl
where indicates element-wise multiplication which performs better in our experiments than other
strategies concatenation and element-wise addition
Generally speaking qil contains update of network focus towards answering the question after its
interaction with image feature vi This property is important for the reasoning process
Weighted Pooling Pooling aims to fuse components of the question after its interaction with all
image features to update representation Two common strategies for pooling are max and mean
pooling However when answering a specifical question it is often the case the correct answer is only
related to particular image regions Therefore using max pooling may lead to unsatisfying results
since questions may involve interaction between human and object while mean pooling may also
cause inferior performance due to noise introduced by regions irrelevant to the question
To determine the relevance between question and each image region we resort to the attention
mechanism used in to generate the attention distribution over image regions For each updated
question qil after interaction with the ith image region it is chosen close to the original question
representation Hence the attention weights take the following forms
Ci tanh(WA qil WB bB
sof tmax(WP bP
where is a matrix and its ith column is Ci RM is a dimensional vector representing the
attention weights is the number of image regions set to Based on the attention distribution
we calculate weighted average of qil resulting in the updated question representation as
ql
Pi qil
The updated question representation after weighted pooling serves as the question input to the next
reasoning or answering layer
Answering Layer
Following we model VQA as a classification problem with pre-defined classes Given the
updated question representation at last reasoning layer a softmax layer is employed to classify
into one of the possible answers as
pans sof tmax(Wans bans
Note instead of the softmax layer for predicting the correct answer it is also possible to utilize LSTM
or GRU decoder taking as input to generate free-form answers
Experiments
Datasets and Evaluation Metrics
We conduct experiments on COCO-QA and VQA The COCO-QA dataset is based on
Microsoft COCO image data There are training questions and test ones based
on a total of images Four types of questions are provided including Object Number Color
and Location Each type takes and of the whole dataset respectively
In the VQA dataset each image from the COCO data is annotated by Amazon Mechanical Turk
AMT with three questions It is the largest for VQA benchmark so far There are
and questions for training validation and testing respectively For each question ten answers
are provided to take consensus of annotators Following we choose the top most frequent
answers as candidate outputs which constitutes of the train+val answers
Since we formulate VQA as a classification problem mean classification accuracy is used to evaluate
the model on the COCO-QA dataset Besides Wu-Palmer similarity WUPS measure is also
reported on COCO-QA dataset WUPS calculates similarity between two words based on their
longest common subsequence in the taxonomy tree Following we use thresholds and in
our evaluation VQA dataset provides a different kind of evaluation metric Since ten ground truth
answers are given a predicted answer is considered to be correct when three or more ground truth
answers match it Otherwise partial score is given
Implementation Details
We implement our network using the public Torch computing framework Before training all question
sentences are normalized to lower case where question marks are removed These words are fed into
GRU one by one The whole answer with one or more words is regarded as a separate class For
extracting image features each candidate region is cropped and resized to before feeding
into CNN.
For the COCO-QA dataset we set the dimension of common latent space to Since VQA
dataset is larger than COCO-QA we double the dimension of common latent space to adapt the data
and classes On each reasoning layer we use one single layer in MLP. We test up to two reasoning
layers No further improvement is observed when using three or more layers
Methods
Mean Pooling
Max Pooling
W/O Global
W/O Coord
Full Model
ACC.
Object
Number
Color
Location
Table Comparison of ablation models Models are trained and tested on COCO-QA with one
reasoning layer
Methods
IMG+BOW
2VIS+BLSTM
Ensemble
ABC-CNN
DPPnet
SAN
QRU
QRU
ACC.
Object
Number
Color
Location
WUPS
WUPS
Table Evaluation results on COCO-QA dataset QRU and QRU refer to and
reasoning layers incorporated in the system
The network is trained in an end-to-end fashion using stochastic gradient descent with mini-batches
of samples and momentum The learning rate starts from and decreases by a factor of
when validation accuracy stops improving We use dropout and gradient clipping to regularize the
training process Our model is denoted as QRU in following experiments
Ablation Results
We conduct experiments to exam the usefulness of each component in our model Specifically we
compare different question representation pooling mechanisms mean pooling and max pooling
We also train two controlled models devoid of global image feature and spatial coordinate denoted
as W/O Global and W/O Coord Table shows the results
The performance of mean and max pooling models are substantially worse than the full model which
uses weighted pooling This indicates that our model benefits from the attention mechanism by
looking at several image regions rather than only one or all of them A drop of in accuracy is
observed if the global image feature is not modeled confirming that inclusion of the whole image
is important for capturing the global information Without modeling spatial coordinates also leads
to a drop in accuracy Notably the greatest deterioration is on the question type of Object This is
because the Object type seeks information around the object like What is next to the stop
sign Spatial coordinates help our model reason spatial relationship among objects
Comparison with State-of-the-art
We compare performance in Tables and with experimental results on COCO-QA and VQA
respectively Table shows that our model with only one reasoning layer already outperforms
state-of-the-art 2-layer stacked attention network SAN Two reasoning layers give the best
performance We also report the per-category accuracy to show the strength and weakness of our
model in Table Our best model outperforms SAN by and in the question types of
Color and Location respectively and by in Object
Our analysis is that the SAN model puts its attention on coarser regions obtained from the activation
of last convolutional layer which may include cluttered and noisy background In contrast our model
only deals with selected object proposal regions which have the good chance to be objects When
answering questions involving objects our model gives reasonable results For the question type
Number since an object proposal may contain several objects our counting ability is weakened In
fact the counting task is a complete computer vision problem on its own
Methods
BOWIMG
LSTMIMG
iBOWIMG
DPPnet
SAN
WR Sel
FDA
DMN
QRU
QRU
Open-Ended test-dev
All
Y/N
Num Other
test-std
All
Multiple-Choice test-dev
All
Y/N Num Other
test-std
All
Table Evaluation results on VQA dataset QRU and QRU refer to and reasoning
layers incorporated in the system
Original
Before updating
After updating
with one
reasoning layer
After updating
with two
reasoning layers
What next to two other open laptops
What next to each other dipicting smartphones
What next to two boys
What hooked up to two computers
What next to each other with visible piping
What next to two pair of shoes
What are there laying down with two remotes
What next to each other depicting smartphones
What hooked up to two computers
What next to each other with monitors
What cubicle with four differnet types of computers
What plugged with wires
What next to each other with monitors
What are open at the table with cell phones
What is next to the monitor
What sits on the desk along with monitors
Figure Retrieved questions before and after update from COCO-QA dataset
Table shows that our model yields prominent improvement on the Other type when compared
with other models that use global representation of the image Object proposals in our
model are useful since the Other type contains questions such as What color What kind
Where is etc Our model outperforms that of by where the latter also exploits
object proposals Compared with we use less number of object proposals demonstrating the
effectiveness of our approach This table also reveals that our model with two reasoning layers
achieve state-of-the-art results for both open-ended and multiple-choice tasks
Qualitative Analysis
To understand the ability of our model in updating question representation we show an image and
several questions in Figure The retrieved questions from the test set are based on the cosine
similarities to the original question before and after our model updates the representation It is notable
that before update out of the top similar questions begin with What next This is because GRU
acts as the language model making the obtained questions share similar language structure After we
update question representation the resulting ones are more related to image content regarding objects
computers and monitors while the originally retrieved questions contain irrelevant words like boys
and shoes The retrieved questions become even more informative using two reasoning layers
We visualize a few attention masks generated by our model in Figure Visualization is created
by soft masking the image with a mask created by summing weights of each region The mask
is normalized with maximum value followed by small Gaussian blur Our model is capable of
putting attention on important regions closely relevant to the question To answer the question What
is the color of the snowboard the proposed model finds the snowboard For the other
question The man holding what on top of a snow covered hill it is required to infer
the relation among person snow covered hill and snowboard With these attention masks it is
possible to predict correct answers since irrelevant image regions are ruled out More examples are
shown in Figure
What is the color of the The man holding what on
snowboard
top of a snow covered hill
A Yellow
A Snowboard
Figure Visualization of attention masks Our model learns to attend particular image regions that
are relevant to the question
What is the color What is sitting on What is the man in What are hogging
of the sunflower
top of table in a
stadium style seats a bed by themselfs
workshop
using
A Yellow
A Boat
A Phone
A Dogs
What next to a
large building
A Clock
Figure Visualization of more attention masks
Conclusion
We have proposed an end-to-end trainable neural network for VQA. Our model learns to answer
questions by updating question representation and inferring over a set of image regions with multilayer
perceptron Visualization of attention masks demonstrates the ability of our model to focus on image
regions highly related to questions Experimental results are satisfying on the two challenging VQA
datasets Future work includes improving object counting ability and word-region relation
Acknowledgements
This work is supported by a grant from the Research Grants Council of the Hong Kong SAR project
No. and by the National Science Foundation China under Grant We thank
NVIDIA for providing Ruiyu Li a Tesla GPU accelerator for this work

<<----------------------------------------------------------------------------------------------------------------------->>

title: 89-neural-network-implementation-approaches-for-the-connection-machine.pdf

Neural Network Implementation Approaches
for the
Connection Machine
Nathan H. Brown Jr.
MRJlPerkin Elmer White Granite Dr. Suite Oakton Va.
ABSlRACf
The SIMD parallelism of the Connection Machine allows the construction of
neural network simulations by the use of simple data and control structures Two
approaches are described which allow parallel computation of a model's nonlinear
functions parallel modification of a model's weights and parallel propagation of a
model's activation and error Each approach also allows a model's interconnect
structure to be physically dynamic A Hopfield model is implemented with each
approach at six sizes over the same number of CM processors to provide a performance
comparison
INTRODUCflON
Simulations of neural network models on digital computers perform various
computations by applying linear or nonlinear functions defined in a program to
weighted sums of integer or real numbers retrieved and stored by array

<<----------------------------------------------------------------------------------------------------------------------->>

title: 1259-are-hopfield-networks-faster-than-conventional-computers.pdf

Are Hopfield Networks Faster Than
Conventional Computers
Ian Parberry and Hung-Li Tsengt
Department of Computer Sciences
University of North Texas
P.O. Box
Denton TX
Abstract
It is shown that conventional computers can be exponentiallx faster
than planar Hopfield networks although there are planar Hopfield
networks that take exponential time to converge a stable state of an
arbitrary planar Hopfield network can be found by a conventional
computer in polynomial time The theory of P.cS-completeness
gives strong evidence that such a separation is unlikely for nonplanar Hopfield networks and it is demonstrated that this is also the
case for several restricted classes of nonplanar Hopfield networks
including those who interconnection graphs are the class of bipartite graphs graphs of degree the dual of the knight's graph the
8-neighbor mesh the hypercube the butterfly the cube-connected
cycles and the shuffle-exchange graph
Introduction
Are Hopfield networks faster than conventional computers This apparently
straightforward question is complicated by the fact that conventional computers
are universal computational devices that is they are capable of simulating any
discrete computational device including Hopfield networks Thus a conventional
computer could in a sense cheat by imitating the fastest Hopfield network possible
Email ianGcs unt edu URL http://hercule csci unt edu/ian
Email ht sengGponder csci unt edu
I. Parberry and H. Tseng
But the question remains is it faster for a computer to imitate a Hopfield network
or to use other computational methods Although the answer is likely to be different for different benchmark problems and even for different computer architectures
we can make our results meaningful in the long term by measuring scalability that
is how the running time of Hopfield networks and conventional computers increases
with the size of any benchmark problem to be solved
Stated more technically we are interested in the computational complexity of the
stable state problem for Hopfield networks which is defined succinctly as follows
given a Hopfield network determine a stable configuration As previously stated
this stable configuration can be determined by imitation or by other means The
following results are known about the scalability of Hopfield network imitation Any
imitative algorithm for the stable state problem must take exponential time on som
Hopfield networks since there exist Hopfield networks that require exponential time
to converge Haken and Luby Goles and Martinez It is unlikely that even
non-imitative algorithms can solve the stable state problem in polynomial time
since the latter is PeS-complete Papadimitriou Schaffer and Yannakakis
However the stable state problem is more difficult for some classes of Hopfield
networks than others Hopfield networks will converge in polynomial time if their
weights are bounded in magnitude by a polynomial of the number of nodes for
an expository proof see Parberry Corollary In contrast the stable
state problem for Hopfield networks whose interconnection graph is bipartite is
peS-complete this can be proved easily by adapting techniques from Bruck and
Goodman which is strong evidence that it too requires superpolynomial time
to solve even with a nonimitative algorithm
We show in this paper that although there exist planar Hopfield networks that ake
exponential time to converge in the worst case the stable state problem for planar
Hopfield networks can be solved in polynomial time by a non-imitative algorithm
This demonstrates that imitating planar Hopfield networks is exponentially slower
than using non-imitative algorithmic techniques In contrast we discover that the
stable state problem remains peS-complete for many simple classes of nonplanar
Hopfield network including bipartite networks networks of degree and some
networks that are popular in neurocomputing and parallel computing
The main part of this manuscript is divided into four sections Section contains
some background definitions and

<<----------------------------------------------------------------------------------------------------------------------->>

title: 804-asynchronous-dynamics-of-continuous-time-neural-networks.pdf

Asynchronous Dynamics of Continuous
Time Neural Networks
Xin Wang
Computer Science Department
University of California at Los Angeles
Los Angeles CA
Qingnan Li
Department of Mathematics
University of Southern California
Los Angeles CA
Edward K. Blum
Department of Mathematics
University of Southern California
Los Angeles CA
ABSTRACT
Motivated by mathematical modeling analog implementation and
distributed simulation of neural networks we present a definition of
asynchronous dynamics of general CT dynamical systems defined
by ordinary differential equations based on notions of local times
and communication times We provide some preliminary results
on globally asymptotical convergence of asynchronous dynamics
for contractive and monotone CT dynamical systems When applying the results to neural networks we obtain some conditions
that ensure additive-type neural networks to be asynchronizable
INTRODUCTION
Neural networks are massively distributed computing systems A major issue in parallel and distributed computation is synchronization versus asynchronization Bertsekas and Tsitsiklis To fix our idea we consider a much studied additive-type
model Cohen and Grossberg Hopfield Hirsch of a continuoustime neural network of neurons whose dynamics is governed by
Xi(t
ajXi(t WijO'j Jlj Xj Ii
Wang Li. and Blum
with neuron states at time constant decay rates external inputs gains
neuron activation functions Uj and synaptic connection weights Wij. Simulation and implementation of idealized models of neural networks such as on
centralized computers not only limit the size of networks but more importantly
preclude exploiting the inherent massive parallelism in network computations A
truly faithful analog implementation or simulation of neural networks defined by
over a distributed network requires that neurons follow a global clock communicate timed states Xj(t to all others instantaneously and synchronize global
dynamics precisely all the time the same Xj(t should be used in evolution of
all Xi(t at time Clearly hardware and software realities make it very hard and
sometimes impossible to fulfill these requirements any mechanism used to enforce
such synchronization may have an important effect on performance of the network Moreover absolutely insisting on synchronization contradicts the biological
manifestation of inherent asynchrony caused by delays in nerve signal propagation
variability of neuron parameters such as refractory periods and adaptive neuron
gains On the other hand introduction of asynchrony may change network dynamics for example from convergent to oscillatory Therefore validity of asynchronous
dynamics of neural networks must be assessed in order to ensure desirable dynamics
in a distributed environment
JJj
Motivated by the above issues we study asynchronous dynamics of general CT dynamical systems with neural networks in particular Asynchronous dynamics has
been thoroughly studied in the context of iterative maps or discrete-time dynamical systems see Bertsekas and Tsitsiklis and

<<----------------------------------------------------------------------------------------------------------------------->>

title: 313-applications-of-neural-networks-in-video-signal-processing.pdf

Applications of Neural Networks in
Video Signal Processing
John C. Pearson Clay D. Spence and Ronald Sverdlove
David Sarnoff Research Center
Princeton NJ
Abstract
Although color TV is an established technology there are a number of
longstanding problems for which neural networks may be suited Impulse
noise is such a problem and a modular neural network approach is presented in this paper The training and analysis was done on conventional
computers while real-time simulations were performed on a massively parallel computer called the Princeton Engine The network approach was
compared to a conventional alternative a median filter Real-time simulations and quantitative analysis demonstrated the technical superiority of
the neural system Ongoing work is investigating the complexity and cost
of implementing this system in hardware
THE POTENTIAL FOR NEURAL NETWORKS IN
CONSUMER ELECTRONICS
Neural networks are most often considered for application in emerging new technologies such as speech recognition machine vision and robotics The fundamental
ideas behind these technologies are still being developed and it will be some time
before products containing neural networks are manufactured As a result research
in these areas will not drive the development of inexpensive neural network hardware which could serve as a catalyst for the field of neural networks in general
In contrast neural networks are rarely considered for application in mature technologies such as consumer electronics These technologies are based on established
principles of information processing and communication and they are used in millions of products per year The embedding of neural networks within such mass
Pearson Spence and Sverdlove
market products would certainly fuel the development oflow-cost network hardware
as economics dictates rigorous cost-reduction in every component
IMPULSE NOISE IN TV
The color television signaling standard used in the U.S. was adopted in McIlwain and Dean Pearson The video information is first broadcast as an
amplitude modulated radio-frequency signal and is then demodulated
in the receiver into what is called the composite video signal The composite signal
is comprised of the high-bandwidth MHz luminance black and white signal
and two low-bandwidth color signals whose amplitudes are modulated in quadrature
on a MHz subcarrier This signal is then further decoded into the red green
and blue signals that drive the display One image frame is formed by interlacing
two successive fields of horizontal lines
Electric sparks create broad-band RF emissions which are transformed into oscillatory waveforms in the composite video signal called AM impulses See Figure
These impulses appear on a television screen as short horizontal multi-colored
streaks which clearly stand out from the picture Such sparks are commonly created by electric motors There is little spatial within a frame or temporal between
frames correlation between impulses
General considerations suggest a two step approach for the removal of impulses from
the video signal detect which samples have been corrupted and replace them with
values derived from their spatio-temporal neighbors Although impulses are quite
visible they form a small fraction of the data so only those samples detected as
corrupted should be altered An interpolated average of some sort will generally be a
good estimate of impulse-corrupted samples because images are generally smoothly
varying in space and time
There are a number of difficulties associated with this detection/replacement approach to the problem There are many impulse-like waveforms present in normal
video which can cause false positives or false alarms See Figure The algorithms that decode the composite signal into RGB spread impulses onto neighboring
lines so it is desirable to remove the impulses in the composite signal However
the color encoding within the composite signal complicates matters The sub carrier
frequency is near the ringing frequency of the impulses and tends to hide the impulses Furthermore the replacement function cannot simply average the nearest
Figure Seven Representative AM Impulse Waveforms They have been digitized
and displayed at the intervals used in digital receivers bits 07 usec The largest
amplitude impulses are samples wide approximately of the width of one
line of active video samples
Applications of Neural Networks in Video Signal Processing
Figure Corrupted Video Scan Line Top Scan line of a composite video signal
containing six impulse waveforms Bottom The impulse waveforms derived by
subtracting the uncorrupted signal from the corrupted signal Note the presence of
many impulse-like features in the video signal
samples because they represent different color components The impulses also have
a wide variety of waveforms Figure including some variation caused by clipping
in the receiver
MODULAR NEURAL NETWORK SYSTEM
The impulse removal system incorporates three small multi-layer perceptron networks Rumelhart and McClelland and all of the processing is confined to
one field of data See Figure The replacement function is performed by one
network termed the i-net denotes interpolation Its input is consecutive
samples each from the two lines above and the two lines below the current line
The network consists of units in the first hidden layer in the second and-one
output node trained to estimate the center sample of the current line
The detection function employs networks in series A single network detector
has been tried but it has never performed as well as this two-stage detector The
inputs to the first network are consecutive samples from the current line centered
on the sample of interest It has nodes in the first layer and one output node
trained to compute a moving average of the absolute difference between the clean
and noisy signals of the current inputs It is thus trained to function as a filter for
impulse energy and is termed the e-net The output of the e-net is then low-pass
filtered and sub-sampled to remove redundant information
The inputs to the second network are lines of consecutive samples each drawn
from the post-processed output of the e-net centered on the sample of interest
This network like the e-net has nodes in the first layer and one output node It
is trained to output if the sample of interest is contaminated with impulse noise
and otherwise It is thus an impulse detector and is called the d-net
The output of the d-net is then fed to a binary switch which passes through to the
final system output either the output of the i-net or the original signal depending
on whether the input exceeds an adjustable threshold
Pearson Spence and Sverdlove
Original Dirty Picture
small
Impu_lse
pseudo
Impulse
I
big Impulse
false negative
potential false negalive
potenllaltrue positive
Interpolated Original
true positive
false positives
Restored Picture
r+smallimpulse let through
blurred eyes
Impulse removed
Figure The Neural Network AM Impulse Removal System The cartoon face is
used to illustrate salient image processing characteristics of the system The e-net
correctly signals the presence of the large impulse chin misses the small impulse
forehead and incorrectly identifies edges nose and points eyes as impulses
The d-net correctly disregards the vertically correlated impulse features nose and
detects the large impulse chin but incorrectly misses the small impulse forehead
and the non-correlated impulse-like features eyes The i-net produces a fuzzy
doubled version of the original which is used to replace segments identified as
corrupted by the d-net
Experience showed that the d-net tended to produce narrow spikes in response to
impulse-like features of the image To remove this source of false positives the
output of the d-net is averaged over a 19 sample region centered on the sample of
interest This reduces the peak amplitude of signals due to impulse-like features
much more than the broad signals produced by true impulses An impulse is considered to be present if this smoothed signal exceeds a threshold the level of which
is chosen so as to strike a balance between low false positive rates high threshold
and high true positive rates low threshold
Experience also showed that the fringes of the impulses were not being detected
To compensate for this sub-threshold d-net output samples are set high if they are
within samples of a super-threshold d-net sample Figure shows the output of
the resulting trained system for one scan line
The detection networks were trained on one frame of video containing impulses of
different amplitudes with the largest twenty times the smallest Visually these
ranged from non-objectionable to brightly colored Standard incremental backpropagation and conjugate gradient NAG were the training proceedures
used The complexity of the e-net and d-net were reduced in phases These nets
Applications of Neural Networks in Video Signal Processing
INPUT
NOISE
SMOOTHED D-NET
THRESHOL
Figure Input and Network Signals
began as layer nets After a phase of training redundant nodes were identified
and removed and training re-started This process was repeated until there were
no redundant nodes
REAL-TIME SIMULATION ON THE PRINCETON
ENGINE
The trained system was simulated in real-time on the Princeton Engine Chin
and a video demonstration was presented at the conference The Princeton Engine is a GIPS image processing system consisting of up to
processing elements in a SIMD configuration Each processor is responsible for the
output of one column of pixels and contains a arithmetic unit multiplier a
64-word triple-port register stack and words of local processor memory In
addition an interprocessor communication bus permits exchanges of data between
neighboring processors during one instruction cycle
While the i-net performs better than conventional interpolation methods the difference is not significant for this problem because of the small amount of signal
which is replaced If the whole image is replaced the neural net interpolator gave
about dB better performance than a conventional method Thus it has not
been implemented on the PE. The i-net may be of value in other video tasks such
as converting from an interlaced to a non-interlaced display
fixed point arithmetic was used in these simulations with bits of fraction
and bit sigmoid function look-up tables Comparison with the double-precision
arithmetic used on the conventional computers showed no significant reduction in
Pearson Spence and Sverdlove
zo
02
04
08
08
FALSE DETtcnoN
02
04
08
08
FALSE DETtcnON
Figure ROC Analysis of Neural Network and Median Detectors
performance Current work is exploring the feasibility of implementing training on
the PE.
PERFORMANCE ANALYSIS
The mean squared error MSE is well known to be a poor measure of subjective
image quality Roufs and Bouma A better measure of detection performance
is given by the receiver operating characteristic or ROC Green and Swets
The ROC is a parametric plot of the fraction of corrupted samples correctly
detected versus the fraction of clean samples that were falsely detected In this case
the decision threshold for the smoothed output of the d-net was the parameter
varied Figure left shows the neural network detector ROC for five different
impulse amplitudes tested on a video frame that it was not trained This
quantifies the sharp breakdown in performance observed in real-time simulations at
low impulse amplitude This breakdown is not observed in analysis of the MSE.
Median filters are often suggested for impulse removal tasks and have been applied
to the removal of impulses from FM TV transmission systems Perlman aI
In order to assess the relative merits of the neural network detector a
median detector was designed and analyzed This detector computes the m~dian of
the current sample and its nearest neighbors with the same color sub-carrier phase
A detection is registered if the difference between the median and the current sample
is above threshold the same additional measures were taken to insure that impulse
fringes were detected as were described above for the neural network detector
Figure right shows both the neural network and median detector ROC's for
two different video frames each of which contained a mixture of all impulse
amplitudes One frame was used in training the network TRAIN and the other
was not TEST This verifies that the network was not overtrained and quantifies
the superior performance of the network detector observed in real-time simulations
Applications of Neural Networks in Video Signal P1'ocessing
CONCLUSIONS
We have presented a system using neural network algorithms that outperforms a
conventional method median filtering in removing AM impulses from television
signals Of course an additional essential criterion is the cost and complexity of
hardware implementations Median filter chips have been successfully fabricated
Christopher We are currently investigating the feasibility of casting
small neural networks into special purpose chips We are also applying neural nets
to other television signal processing problems
Acknowledgements
This work was supported by Thomson Consumer Electronics under Erich Geiger
and Dietrich Westerkamp This work was part of a larger team effort and we
acknowledge their help in particular Nurit Binenbaum Jim Gibson Patrick Hsieh
and John Ju.

<<----------------------------------------------------------------------------------------------------------------------->>

