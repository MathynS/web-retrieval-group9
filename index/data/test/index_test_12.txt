query sentence: Nearest neighbours search algorithm for similarity comparison
<<------------------------------------------------------------------------------------------------------------------------------------------->>
title: 6387-cliquecnn-deep-unsupervised-exemplar-learning.pdf

CliqueCNN Deep Unsupervised Exemplar Learning
Miguel A. Bautista Artsiom Sanakoyeu Ekaterina Sutter Bj?rn Ommer
Heidelberg Collaboratory for Image Processing
IWR Heidelberg University Germany
firstname.lastname@iwr.uni-heidelberg.de
Abstract
Exemplar learning is a powerful paradigm for discovering visual similarities in
an unsupervised manner In this context however the recent breakthrough in
deep learning could not yet unfold its full potential With only a single positive
sample a great imbalance between one positive and many negatives and unreliable
relationships between most samples training of Convolutional Neural networks is
impaired Given weak estimates of local distance we propose a single optimization
problem to extract batches of samples with mutually consistent relations Conflicting relations are distributed over different batches and similar samples are grouped
into compact cliques Learning exemplar similarities is framed as a sequence of
clique categorization tasks The CNN then consolidates transitivity relations within
and between cliques and learns a single representation for all samples without
the need for labels The proposed unsupervised approach has shown competitive
performance on detailed posture analysis and object classification
Introduction
Visual similarity learning is the foundation for numerous computer vision subtasks ranging from
low-level image processing to high-level object recognition or posture analysis A common paradigm
has been category-level recognition where categories and the similarities of all their instances
to other classes are jointly modeled However large intra-class variability has recently spurred
exemplar methods which split this problem into simpler sub-tasks Therefore separate
exemplar classifiers are trained by learning the similarities of individual exemplars against a large
set of negatives The exemplar paradigm has been successfully employed in diverse areas such as
segmentation grouping instance retrieval and object recognition Learning
similarities is also of particular importance for posture analysis and video parsing
Among the many approaches for similarity learning supervised techniques have been particularly
popular in the vision community leading to the formulation as a ranking regression and
classification task With the recent advances of convolutional neural networks two-stream
architectures and ranking losses have shown great improvements However to achieve their
performance gain CNN architectures require millions of samples of supervised training data or at
least the fine-tuning on large datasets such as PASCAL VOC. Although the amount of accessible
image data is increasing at an enormous rate supervised labeling of similarities is very costly In
addition not only similarities between images are important but especially between objects and their
parts Annotating the fine-grained similarities between all these entities is hopelessly complex in
particular for the large datasets typically used for training CNNs
Unsupervised deep learning of similarities that does not requiring any labels for pre-training or
fine-tuning is therefore of great interest to the vision community This way we can utilize large
image datasets without being limited by the need for costly manual annotations However CNNs for
Both authors contributed equally
Conference on Neural Information Processing Systems NIPS Barcelona Spain
exemplar-based learning have been rare due to limitations resulting from the widely used softmax
loss The learning task suffers from only a single positive instance it is highly unbalanced with many
more negatives and the relationships between samples are unknown cf Sec. Consequentially
stochastic gradient descend SGD gets corrupted and has a bias towards negatives thus forfeiting
the benefits of deep learning
Outline of the proposed approach We overcome these limitations by updating similarities and
CNNs Typically at the beginning only a few local estimates of dis-)similarity are easily available
pairs of samples that are highly similar near duplicates or that are very distant Most of the
similarities are however unknown or mutually contradicting so that transitivity does not hold
Therefore we initially can only gather small compact cliques of mutually similar samples around an
exemplar but for most exemplars we know neither if they are similar nor dissimilar To nevertheless
define balanced classification tasks suited for CNN training we formulate an optimization problem
that builds training batches for the CNN by selecting groups of compact cliques so that all cliques in
a batch are mutually distant Thus for all samples of a batch dis-)similarity is defined?they either
belong to the same compact clique or are far away and belong to different cliques However pairs of
samples with no reliable similarities end up in different batches so they do not yield false training
signal for SGD. Classifying if a sample belongs to a clique serves as a pretext task for learning
exemplar similarity Training the network then implicitly reconciles the transitivity relations between
samples in different batches Thus the learned CNN representations impute similarities that were
initially unavailable and generalize them to unseen data
In the experimental evaluation the proposed approach significantly improves over state-of-the-art
approaches for posture analysis and retrieval by learning a general feature representation for human
pose that can be transferred across datasets
Exemplar Based Methods for Similarity Learning
The Exemplar Support Vector Machine Exemplar-SVM has been one of the driving methods for
exemplar based learning Each Exemplar-SVM classifier is defined by a single positive instance
and a large set of negatives To improve performance Exemplar-SVMs require several round of hard
negative mining increasing greatly the computational cost of this approach To circumvent this high
computational cost proposes to train Linear Discriminant Analysis LDA over Histogram of
Gradient HOG features LDA whitened HOG features with the common covariance matrix
estimated for all the exemplars removes correlations between the HOG features which tend to amplify
the background of the image
Recently several CNN approaches have been proposed for supervised similarity learning using either
pairs or triplets of images However supervised formulations for learning similarities
require that the supervisory information scales quadratically for pairs of images or cubically for
triplets This results in very large training times
Literature on exemplar based learning in CNNs is very scarce In the authors of ExemplarCNN tackle the problem of unsupervised feature learning A patch-based categorization problem is
designed by randomly extracting patch for each image in the training set and defining it as surrogate
class Hence since this approach does not take into account dis-)similarities between exemplars it
fails to model their transitivity relationships resulting in poor performances Sect
Furthermore recent works by Wang and Doersh showed that temporal information
in videos and spatial context information in images can be utilized as a convenient supervisory
signal for learning feature representation with CNNs However the computational cost of the
training algorithm is enormous since the approach in needs to tackle all possible pair-wise image
relationships requiring training set that scales quadratically with the number of samples On the
contrary our approach leverages the relationship information between compact cliques defining
a multi-class classification problem As each training batch contains mutually distinct cliques the
computational cost of the training algorithm is greatly decreased
Approach
We will now discuss how we can employ a CNN for learning similarities between all pairs of a large
number of exemplars Exemplar learning in CNNs has been a relatively unexplored approach for
multiple reasons First and foremost deep learning requires large amounts of training data thus
conflicting with having only a single positive exemplar in a setup that we now abbreviate as 1-sample
True positive rate
False positive rate
Figure Average AUC for posture retrieval in the Olympic Sports dataset Similarities learnt
by 1-sample CNN using NN-CNN and for the proposed approach The plots show a
magnified crop of the full similarity matrix Note the more detailed fine structure in
CNN. Such a 1-sample CNN faces several issues The within-class variance of an individual
exemplar cannot be modeled The ratio of one exemplar and many negatives is highly imbalanced
so that the softmax loss over SGD batches overfits against the negatives iii An SGD batch for
training a CNN on multiple exemplars can contain arbitrarily similar samples with different label the
different exemplars may be similar or dissimilar resulting in label inconsistencies The proposed
method overcomes these issues as follows In Sect we discuss why simply merging an exemplar
with its nearest neighbors and data augmentation similar in spirit to the Clustered Exemplar-SVM
is not sufficient to address Sect compares this NN-CNN approach against other methods
Sect deals with and iii by generating batches of cliques that maximize the intra-clique
similarity while minimizing inter-clique similarity
To show the effectiveness of the proposed method we give empirical proof by training CNNs in both
1-sample CNN and NN-CNN manners shows the average ROC curve for posture retrieval
in the Olympic Sports dataset refer to Sec. for further details for 1-sample CNN NN-CNN
and the proposed method which clearly outperforms both exemplar based strategies In addition
show an excerpt of the similarity matrix learned for each method It becomes evident
that the proposed approach captures more detailed similarity structures the diagonal structures
correspond to repetitions of the same gait cycle within a long jump
Initialization
Since deep learning benefits from large amounts of data and requires more than a single exemplar
to avoid biased gradients we now reframe exemplar-based learning of similarities so that it can
be handled by a CNN. Given a single exemplar di we thus strive for related samples to enable a
CNN training that then further improves the similarities between samples To obtain this initial
set of few mutually similar samples for an exemplar we now briefly discuss the reliability of
standard feature distances such as whitening HOG features using LDA HOG-LDA is a
computationally effective foundation for estimating similarities sij between large numbers of samples
sij s(di dj Here is the initial HOG-LDA representation of the exemplar
and is the resulting kernel
Most of these initial similarities are unreliable and thus the majority of samples
cannot be properly ranked their similarity to an exemplar di However highly similar samples
and those that are far away can be reliably identified as they stand out from the similarity distribution
Subsequently we utilize these few reliable relationships to build groups of compact cliques
Compact Cliques
Simply assigning the same label to all the nearest and another label to all the furthest neighbors
of an exemplar is inappropriate The samples in these groups may be close to di distant for
the negative group but not to another due to lacking transitivity Moreover mere augmentation of
the exemplar with synthetic data does not add transitivity relations to other samples Therefore to
learn within-class similarities we need to restrict the model to compact cliques of samples so that all
samples in a clique are also mutually close to another and deserve the same label
Query
Ours
Alexnet
HOG-LDA
Figure Averaging of the nearest neighbours for a given query frame using similarities obtained
by our approach Alexnet[13 and HOG-LDA
To build candidate cliques we apply complete-linkage clustering starting at each di to merge the
sample with its local neighborhood so that all merged samples are mutually similar Thus cliques are
compact differ in size and may be mutually overlapping To reduce redundancy highly overlapping
cliques are subsequently merged by clustering cliques using farthest-neighbor clustering This
agglomerative grouping is terminated if intra-clique similarity of a cluster is less than half that of its
constituents Let be the resulting number of clustered cliques and the number of samples di
Then is the resulting assignment matrix of samples to cliques
Selecting Batches of Mutually Consistent Cliques
We now have a set of compact cliques that comprise all training data Thus one may consider to train
a CNN to assign all samples of a clique with the same label However since only the highest/lowest
similarities are reliable samples in different cliques are not necessarily dissimilar Forcing them into
different classes can consequently entail incorrect similarities Therefore we now seek batches of
mutually distant cliques so that all samples in a batch can be labeled consistently because they are
either similar same compact clique or dissimilar different distant clique Samples with unreliable
similarity then end up in different batches and we train a CNN successively on these batches
We now formulate an optimization problem that produces a set of consistent batches of cliques Let
be an indicator matrix that assigns cliques to batches the rows xb of are the
cliques in batch and S0 RK?K be the similarity between cliques We enforce cliques in the same
batch to be dissimilar by minimizing tr which is regularized for the diagonal elements of
the matrix S0 selected for each batch Moreover each batch should maximize sample
coverage the number of distinct samples in all cliques of a batch kxb Ckpp should be maximal
Finally the number of distinct points covered by all batches k1XCkpp should be maximal so that
the different potentially overlapping batches together comprise as much samples as possible We
select so that our penalty function roughly approximate the non-linear step function The
objective of the optimization problem then becomes
min
tr tr diag
kxb Ckpp k1XCkpp
r1B
where is the desired number of cliques in one batch for CNN training The number of batches
can be set arbitrarily high to allow for as many rounds of SGD training as desired If it is too low
this can be easily spotted as only limited coverage of training data can be achieved in the last term of
Since is discrete the optimization problem is not easier than the Quadratic Assignment
Proble which is known to be hardm To overcome this issue we relax the binary constraints
and force instead the continuous solution to the boundaries of the feasible range by maximizing the
additional term kX using the Frobenius norm
We condition S0 to be positive semi-definite by thresholding its eigenvectors and projecting onto the
resulting base Since also the previous objective function is a difference of convex functions
Figure Visual example of a resulting batch of cliques for long jump category of Olympic Sports
dataset Each clique contains at least samples and is represented as their average
where
tr
kxb Ckpp k1XCkpp
tr(X diag kX
It can be solved using the CCCP Algorithm In each iteration of CCCP the following convex
optimization problem is solved
argmin vec vec
r1B
where v(Xt 2X diag 2X and denotes the Hadamard product We solve
this constrained optimization problem by means of the interior-point method shows a visual
example of a selected batch of cliques
CNN Training
We successively train a CNN on the different batches xb obtained using In each batch
classifying samples according to the clique they are in then serves as a pretext task for learning
sample similarities One of the key properties of CNNs is the training using SGD and backpropagation
The backpropagated gradient is estimated only over a subset batch of training samples so it
depends only on the subset of cliques in xb Following this observation the clique categorization
problem is effectively decoupled into a set of smaller sub-tasks?the individual batches of cliques
During training we randomly pick a batch in each iteration and compute the stochastic gradient
using the softmax loss
fW dj
j?x
Vt
Wt
where is the SGD batch size Wt denotes the CNN weights at iteration and Vt denotes the
weight update of the previous iteration Parameters and denote the learning rate and momentum
respectively We then compute similarities between exemplars by simply measuring correlation on
the learned feature representation extracted from the CNN Sect for details
Similarity Imputation
By alternating between the different batches which contain cliques with mutually inconsistent
similarities the CNN learns a single representation for samples from all batches In effect this
consolidates similarities between cliques in different batches It generalizes from a subset of initial
cliques to new previously unreliable relations between samples in different batches by utilizing
transitivity relationships implied by the cliques
After a training round over all batches we impute the similarities using the representation learned
by the CNN. The resulting similarities are more reliable and enable the grouping algorithm from
Sect to find larger cliques of mutually related samples As there are fewer unreliable similarities
Frames sorted by exemplar similarity score
Query exemplar
Similarity score
Frame ranking
Figure Cumulative distribution of the spectrum of the similarity matrices obtained by our
method and the HOG-LDA initialization Sorted similarities with respect to one exemplar
where only similarities at the ends
of the distribution can be trusted
more samples can be comprised in a batch and overall less batches already cover the same fraction of
data as before Consequently we alternately train the CNN and recompute cliques and batches using
the similarities inferred in the previous iteration of CNN training This alternating imputation of
similarities and update of the classifier follows the idea of multiple-instance learning and has shown
to converge quickly in less than four iterations
To evaluate the improvement of the similarities analyzes the eigenvalue spectrum of on
the Olympic Sports dataset see Sect The plot shows the normalized cumulative sum of the
eigenvalues as the function of the number of eigenvectors Compared to the initialization transitivity
relations are learned and the approach can generalize from an exemplar to more related samples
Therefore the similarity matrix becomes more structured and random noisy relations
disappear As a consequence it can be represented using very few basis vectors In a further
experiment we evaluate the number of reliable similarities and dissimilarities within and between
cliques per batch Recall that samples can only be part of the same batch if their similarity is
reliable So the goal of similarity learning is to remove transitivity conflicts and reconcile relations
between samples to yield larger batches We now observe that after the iterative update of similarities
the average number of similarities and dissimilarities in a batch has increased by a factor of
compared to the batches at initialization
Experimental Evaluation
We provide a quantitative and qualitative analysis of our exemplar-based approach for unsupervised
similarity learning For evaluation three different settings are considered posture analysis on
Olympic Sports pose estimation on Leeds Sports and object classification on PASCAL
VOC
Olympic Sports Dataset Posture Analysis
The Olympic Sports dataset is a video compilation of different sports competitions To evaluate
fine-scale pose similarity for each sports category we had independent annotators manually label
positive similar and negative dissimilar samples for around exemplars Note that these
annotations are solely used for testing since we follow an unsupervised approach
We compare the proposed method with the Exemplar-CNN the two-stream approach of Doersch
al 1-sample CNN and NN-CNN models a very similar spirit to Alexnet
Exemplar-SVMs and HOG-LDA Due to its performance in object and person detection
we use the approach of to compute person bounding boxes The evaluation should investigate
the benefit of the unsupervised gathering of batches of cliques for deep learning of exemplars using
standard CNN architectures Therefore we incarnate our approach by adopting the widely used model
of Krizhevsky Batches for training the network are obtained by solve the optimization
problem in with and and fine-tuning the model for iterations
Thereafter we compute similarities using features extracted from layer fc7 in the caffe implementation
of Exemplar-CNN is trained using the best performing parameters reported in and the
architecture Then we use the output of fc4 and compute 4-quadrant max
pooling iii Exemplar-SVM was trained on the exemplar frames using the HOG descriptor The
samples for hard negative mining come from all categories except the one that an exemplar is from
We performed cross-validation to find an optimal number of negative mining rounds less than three
The class weights of the linear SVM were set as C1 and C2 LDA whitened HOG
HOG-LDA
Ex-SVM
Ex-CNN
Alexnet
CNN
NN-CNN
Doersch al
Ours
Table Avg. AUC for each method on Olympic Sports dataset
was computed as specified in The 1-sample CNN was trained by defining a separate class for
each exemplar sample plus a negative category containing all other samples In a similar fashion
the NN-CNN was trained using the exemplar plus nearest neighbours obtained using the whitened
HOG similarities As implementation for both CNNs we again used the model of fine-tuned
for iterations Each image in the training set is augmented with transformed versions by
performing random translation scaling rotation and color transformation to improve invariance with
respect to these
Tab. reports the average AuC for each method over all categories of the Olympic Sports dataset
Our approach obtains a performance improvement of at least the other methods In
particular the experiments show that the 1-sample CNN fails to model the positive distribution
due to the high imbalance between positives and negatives and the resulting biased gradient In
comparison additional nearest neighbours to the exemplar NN-CNN yield a better model of withinclass variability of the exemplar leading to a performance increase over the 1-sample CNN.
However NN-CNN also sees a large set of negatives which are partially similar and dissimilar Due
to this unstructuredness of the negative set the approach fails to thoroughly capture the fine-grained
similarity structure over the negative samples To circumvent this issue we compute sets of mutually
distant compact cliques resulting in a relative performance increase of over NN-CNN
Furthermore presents the similarity structures which the different approaches extract when
analyzing human postures further highlights the similarities and the relations between
neighbors For each method the top nearest neighbours for a randomly chosen exemplar frame in
the Olympic Sports dataset are blended We can see how the neighbors obtained by our approach
depict a sharper average posture since they result from compact cliques of mutually similar samples
Therefore they retain more details and are more similar to the original than in case of the other
methods
Leeds Sports Dataset Pose Estimation
The Leeds Sports Dataset is the most widely used benchmark for pose estimation For training
we employ images from the dataset combined with images from the extended version of
this dataset where each image is annotated with joint locations We use the visual similarities
learned by our approach to find frames similar in posture to a query frame Since our training is
unsupervised joint labels are not available At test time we therefore estimate the pose of a query
person by identifying the nearest neighbor from the training set To compare against the supervised
methods the pose of the nearest neighbor is then compared against ground-truth
Now we evaluate our visual similarity learning and the resulting identification of nearest postures
For comparison similar postures are also retrieved using HOG-LDA and Alexnet In
addition we also report an upper bound on the performance that can be achieved by the nearest
neighbor using ground-truth similarities Therefore the nearest training pose for a query is identified
by minimizing the average distance between their ground-truth pose annotation This is the best one
can do by finding the most similar frame when not provided with a supervised parametric model the
performance gap to shows the difference between training and test poses For completeness
we compare with a fully supervised state-of-the-art approach for pose estimation We use the
same experimental settings described in Sect Tab. reports the Percentage of Correct Parts
PCP for the different methods The prediction for a part is considered correct when its endpoints
are within part length of the corresponding ground truth endpoints Our approach significantly
improves the visual similarities learned using Alexnet and HOG-LDA It is note-worthy that even
though our approach for estimating the pose is fully unsupervised it attains a competitive performance
when compared to the upper-bound of supervised ground truth similarities
In addition presents success and failure cases of our method In we can see
that the pose is correctly transferred from the nearest neighbor from the training set resulting in a
PCP score of for that particular image Moreover show that the representation learnt
by our method is invariant to front-back flips matching a person facing away from the camera to one
Method
Ours
HOG-LDA[10
Alexnet[13
Ground Truth
Pose Machines
Torso
Upper legs
Lower legs
Upper arms
Lower arms
Head
Total
Table PCP measure for each method on Leeds Sports dataset
Figure Pose prediction results and are test images with the superimposed ground truth
skeleton depicted in red and the predicted skeleton in green and are corresponding nearest
neighbours which were used to transfer pose
facing the camera Since our approach learns pose similarity in an unsupervised manner it becomes
invariant to changes in appearance as long as the shape is similar thus explaining this confusion
Adding additional training data or directly incorporating face detection-based features could resolve
this
PASCAL VOC Object Classification
The previous sections have analyzed the learning of pose similarities Now we evaluate the learning
of similarities over object categories Therefore we classify object bounding boxes of the PASCAL
VOC dataset To initialize our model we now use the visual similarities of Wang
without applying any fine tuning on PASCAL and also compare against this approach Thus neither
ImageNet nor Pascal VOC labels are utilized For comparison we evaluate against HOG-LDA
and R-CNN For our method and HOG-LDA we use the same experimental settings as
described in Sect initializing our method and network with the similarities obtained by
For all methods the nearest neighbors are computed using similarities Pearson correlation based
on In Tab. we show the classification accuracies for all approaches for Our approach
improves upon the initial similarities of the unsupervised approach of to yield a performance
gain of without requiring any supervision information or fine-tuning on PASCAL
HOG-LDA
Wang al
Wang al Ours
RCNN
Table Classification results for PASCAL VOC
Conclusion
We have proposed an approach for unsupervised learning of similarities between large numbers
of exemplars using CNNs CNN training is made applicable in this context by addressing crucial
problems resulting from the single positive exemplar setup the imbalance between exemplar and
negatives and inconsistent labels within SGD batches Optimization of a single cost function yields
SGD batches of compact mutually dissimilar cliques of samples Learning exemplar similarities is
then posed as a categorization task on individual batches In the experimental evaluation the approach
has shown competitive performance compared to the state-of-the-art providing significantly finer
similarity structure that is particularly crucial for detailed posture analysis
This research has been funded in part by the Ministry for Science Baden-W?rttemberg and the Heidelberg
Academy of Sciences Heidelberg Germany We are grateful to the NVIDIA corporation for donating a Titan
GPU.

<<----------------------------------------------------------------------------------------------------------------------->>

title: 5425-a-multiplicative-model-for-learning-distributed-text-based-attribute-representations.pdf

A Multiplicative Model for Learning Distributed
Text-Based Attribute Representations
Ryan Kiros Richard S. Zemel Ruslan Salakhutdinov
University of Toronto
Canadian Institute for Advanced Research
rkiros zemel rsalakhu}@cs.toronto.edu
Abstract
In this paper we propose a general framework for learning distributed representations of attributes characteristics of text whose representations can be jointly
learned with word embeddings Attributes can correspond to a wide variety of
concepts such as document indicators to learn sentence vectors language indicators to learn distributed language representations meta-data and side information such as the age gender and industry of a blogger or representations of
authors We describe a third-order model where word context and attribute vectors
interact multiplicatively to predict the next word in a sequence This leads to the
notion of conditional word similarity how meanings of words change when conditioned on different attributes We perform several experimental tasks including
sentiment classification cross-lingual document classification and blog authorship attribution We also qualitatively evaluate conditional word neighbours and
attribute-conditioned text generation
Introduction
Distributed word representations have enjoyed success in several NLP tasks More recently
the use of distributed representations have been extended to model concepts beyond the word level
such as sentences phrases and paragraphs entities and relationships and embeddings of semantic categories
In this paper we propose a general framework for learning distributed representations of attributes
characteristics of text whose representations can be jointly learned with word embeddings The use
of the word attribute in this context is general Table illustrates several of the experiments we
perform along with the corresponding notion of attribute For example an attribute can represent
an indicator of the current sentence or language being processed This allows us to learn sentence
and language vectors similar to the proposed model of Attributes can also correspond to side
information or metadata associated with text For instance a collection of blogs may come with
information about the age gender or industry of the author This allows us to learn vectors that can
capture similarities across metadata based on the associated body of text The goal of this work
is to show that our notion of attribute vectors can achieve strong performance on a wide variety of
NLP related tasks In particular we demonstrate strong quantitative performance on three highly
diverse tasks sentiment classification cross-lingual document classification and blog authorship
attribution
To capture these kinds of interactions between attributes and text we propose to use a third-order
model where attribute vectors act as gating units to a word embedding tensor That is words are
represented as a tensor consisting of several prototype vectors Given an attribute vector a word
embedding matrix can be computed as a linear combination of word prototypes weighted by the
attribute representation During training attribute vectors reside in a separate lookup table which
can be jointly learned along with word features and the model parameters This type of three-way
Table Summary of tasks and attribute types used in our experiments The first three are quantitative while the second three are qualitative
Task
Sentiment Classification
Cross-Lingual Classification
Authorship Attribution
Conditional Text Generation
Structured Text Generation
Conditional Word Similarity
Dataset
Sentiment Treebank
RCV1/RCV2
Blog Corpus
Gutenberg Corpus
Gutenberg Corpus
Blogs Europarl
Attribute type
Sentence Vector
Language Vector
Author Metadata
Book Vector
Part of Speech Tags
Author Metadata Language
interaction can be embedded into a neural language model where the three-way interaction consists
of the previous context the attribute and the score distribution of the next word after the context
Using a word embedding tensor gives rise to the notion of conditional word similarity More specifically the neighbours of word embeddings can change depending on which attribute is being conditioned on For example the word joy when conditioned on an author with the industry attribute
religion appears near rapture and god but near delight and comfort when conditioned on
an author with the industry attribute science Another way of thinking of our model would be
the language analogue of They used a factored conditional restricted Boltzmann machine for
modelling motion style defined by real or continuous valued style variables When our factorization
is embedded into a neural language model it allows us to generate text conditioned on different
attributes in the same manner as could generate motions from different styles As we show in
our experiments if attributes are represented by different books samples generated from the model
learn to capture associated writing styles from the author Furthermore we demonstrate a strong
performance gain for authorship attribution when conditional word representations are used
Multiplicative interactions have also been previously incorporated into neural language models
introduced a multiplicative model where images are used for gating word representations Our
framework can be seen as a generalization of and in the context of their work an attribute would
correspond to a fixed representation of an image introduced a multiplicative recurrent neural
network for generating text at the character level In their model the character at the current timestep
is used to gate the network?s recurrent matrix This led to a substantial improvement in the ability to
generate text at the character level as opposed to a non-multiplicative recurrent network
Methods
In this section we describe the proposed models We first review the log-bilinear neural language
model of as it forms the basis for much of our work Next we describe a word embedding
tensor and show how it can be factored and introduced into a multiplicative neural language model
This is concluded by detailing how our attribute vectors are learned
Log-bilinear neural language models
The log-bilinear language model LBL is a deterministic model that may be viewed as a feedforward neural network with a single linear hidden layer Each word in the vocabulary is represented as a K-dimensional real-valued vector rw RK Let denote the matrix of
word representation vectors where is the vocabulary size Let be a tuple of
words where is the context size The LBL model makes a linear prediction of the next word
representation as
rwi
where are context parameter matrices Thus
is the predicted
representation of rwn The conditional probability wn of wn given w1 is
exp
rT ri bi
wn PV
rT rj bj
exp
where RV is a bias vector Learning can be done using backpropagation
NLM
Multiplicative NLM
Multiplicative NLM with language switch
Figure Three different formulations for predicting the next word in a neural language model Left
A standard neural language model Middle The context and attribute vectors interact via
a multiplicative interaction Right When words are unshared across attributes a one-hot attribute
vector gates the factors-to-vocabulary matrix
A word embedding tensor
Traditionally word representation matrices are represented as a matrix RV such as in
the case of the log-bilinear model Throughout this work we instead represent words as a tensor
RV where corresponds to the number of tensor slices Given an attribute vector
PD
RD we can compute attribute-gated word representations as word
representations with respect to are computed as a linear combination of slices weighted by each
component of
It is often unnecessary to use a fully unfactored tensor Following we re-represent in
terms of three matrices Wf RF Wf RF and Wf RF such that
Wf diag(Wf Wf
where diag denotes the matrix with its argument on the diagonal These matrices are parametrized
by a pre-chosen number of factors
Multiplicative neural language models
We now show how to embed our word representation tensor into the log-bilinear neural language
model Let Wf Wf denote a folded matrix of word embeddings Given the
context w1 the predicted next word representation
is given by
where denotes the column of for the word representation of and
are context matrices Given a predicted next word representation
the factor outputs are
Wf
Wf
where is a component-wise product The conditional probability wn of wn
given w1 and can be written as
exp Wf bi
wn PV
fv
exp bj
Here Wf denotes the column of Wf corresponding to word In contrast to the log-bilinear
model the matrix of word representations from before is replaced with the factored tensor as
shown in
Unshared vocabularies across attributes
Our formulation for assumes that word representations are shared across all attributes In some
cases words may only be specific to certain attributes and not others An example of this is crosslingual modelling where it is necessary to have language specific vocabularies As a running example consider the case where each attribute corresponds to a language representation vector Let
Table Samples generated from the model when conditioning on various attributes For the last
example we condition on the average of the two vectors symbol corresponds to a number
Attribute
Bible
Caesar
Bible
Caesar
Sample
for thus enquired unto thee saying the lord had not come unto
him when see them shall see me greater am that under the name
of the king on israel
to tell vs pindarus shortly pray now hence a word comes hither and
let vs exclaim once by him fear till loved against caesar till you are now which
have kept what proper deed there is an ant for caesar not wise cassi
let our spring tiger as with less for tucking great fellowes at ghosts of broth
industrious time with golden glory employments but are far in men
soft from bones assur too set and blood of smelling and there they cost
learned love no guile his word downe the mystery of possession
denote the attribute vector for language and for language English and French We
can then compute language-specific word representations by breaking up our decomposition into
language dependent and independent components
diag(Wf Wf
where is a language specific matrix The matrices Wf and Wf do not depend
on the language or the vocabulary whereas is language specific Moreover since each language may have a different sized vocabulary we use to denote the vocabulary size of language
Observe that this model has an interesting property in that it allows us to share statistical strength
across word representations of different languages In particular we show in our experiments how
we can improve cross-lingual classification performance between English and German when a large
amount of parallel data exists between English and French and only a small amount of parallel data
exists between English and German
Learning attribute representations
We now discuss how to learn representation vectors Recall that when training neural language
models the word representations of w1 are updated by backpropagating through the
word embedding matrix We can think of this as being a linear layer where the input to this layer
is a one-hot vector with the i-th position active for word Then multiplying this vector by the
embedding matrix results in the word vector for Thus the columns of the word representations
matrix consisting of words from w1 will have non-zero gradients with respect to the loss
This allows us to consistently modify the word representations throughout training
We construct attribute representations in a similar way Suppose that is an attribute lookup table
where and is an optional non-linearity We often use a rectifier non-linearity in
order to keep sparse and positive which we found made training much more stable Initially the
entries of are generated randomly During training we treat in the same way as the word embedding matrix This way of learning language representations allows us to measure how similar
attributes are as opposed to using a one-hot encoding of attributes for which no such similarity could
be computed
In some cases attributes that are available during training may not also be available at test time
An example of this is when attributes are used as sentence indicators for learning representations
of sentences To accommodate for this we use an inference step similar to that proposed by
That is at test time all the network parameters are fixed and stochastic gradient descent is used for
inferring the representation of an unseen attribute vector
Experiments
In this section we describe our experimental evaluation and results Throughout this section we refer
to our model as Attribute Tensor Decomposition All models are trained using stochastic gradient descent with an exponential learning rate decay and linear per epoch increase in momentum
We first demonstrate initial qualitative results to get a sense of the tasks our model can perform For
these we use the small project Gutenberg corpus which consists of 18 books some of which have
the same author We first trained a multiplicative neural language model with a context size of
Table A modified version of the game Mad Libs Given an initialization the model is to generate
the next words according to the part-of-speech sequence note that these are not hard constraints
NN IN DT JJ
the meaning of life
the cure of the bad
the truth of the good
a penny for the fourth
the globe of those modern
all man upon the same
VB VBD JJS NNS
my greatest accomplishment
to keep sold most wishes
to make manned most magnificent
to keep wounded best nations
to be allowed best arguments
to be mentioned most people
PRP NN JJ NN
could not live without
his regard willing tenderness
her french serious friend
her father good voice
her heart likely beauty
her sister such character
Table Classification accuracies on various tasks Left Sentiment classification on the treebank dataset Competing methods include the Neural Bag of words NBoW Recursive Network RNN Matrix-Vector Recursive Network MV-RNN Recursive Tensor Network
RTNN Dynamic Convolutional Network DCNN and Paragraph Vector Right
Cross-lingual classification on RCV2 Methods include statistical machine translation IMatrix Bag-of-words autoencoders and BiCVM BiCVM The use of
on cross-lingual tasks indicate the use of a third language French for learning embeddings
Method
SVM
BiNB
NBoW
RNN
MVRNN
RTNN
DCNN
PV
ATD
Fine-grained
Positive Negative
Method
SMT
I-Matrix
BAE-cr
BAE-tree
BiCVM
BiCVM
BAE-corr
ATD
ATD
EN DE
DE EN
where each attribute is represented as a book This results in 18 learned attribute vectors one for
each book After training we can condition on a book vector and generate samples from the model
Table illustrates some the generated samples Our model learns to capture the style associated
with different books Furthermore by conditioning on the average of book representations the
model can generate reasonable samples that represent a hybrid of both attributes even though such
attribute combinations were not observed during training
Next we computed POS sequences from sentences that occur in the training corpus We trained
a multiplicative neural language model with a context size of to predict the next word from its
context given knowledge of the POS tag for the next word That is we model wn
where denotes the POS tag for word wn After training we gave the model an initial input and
a POS sequence and proceeded to generate samples Table shows some results for this task
Interestingly the model can generate rather funny and poetic completions to the initial context
Sentiment classification
Our first quantitative experiments are performed on the sentiment treebank of A common challenge for sentiment classification tasks is that the global sentiment of a sentence need not correspond
to local sentiments exhibited in sub-phrases of the sentence To address this issue collected annotations from the movie reviews corpus of of all subphrases extracted from a sentence parser
By incorporating local sentiment into their recursive architectures was able to obtain significant
performance gains with recursive networks over bag of words baselines
We follow the same experimental procedure proposed by for which evaluation is reported on
two tasks fine-grained classification of categories very negative negative neutral positive very
positive and binary classification positive negative We extracted all subphrases of sentences
that occur in the training set and used these to train a multiplicative neural language model Here
each attribute is represented as a sentence vector as in In order to compute subphrases for
unseen sentences we apply an inference procedure similar to where the weights of the network
are frozen and gradient descent is used to infer representations for each unseen vector We trained a
logistic regression classifier using all training subphrases in the training set At test time we infer a
representation for a new sentence which is used for making a review prediction We used a context
size of dimensional word vectors initialized from and dimensional sentence vectors
initialized by averaging vectors of words from the corresponding sentence
Table left panel illustrates our results on this task in comparison to all other proposed approaches
Our results are on par with the highest performing recursive network on the fine-grained task and
outperforms all bag-of-words baselines and recursive networks with the exception of the RTNN on
the binary task Our method is outperformed by the two recently proposed approaches of
convolutional network trained on sentences and Paragraph Vector
Cross-lingual document classification
We follow the experimental procedure of for which several existing baselines are available to
compare our results The experiment proceeds as follows We first use the Europarl corpus for
inducing word representations across languages Let be a sentence with words in language
and let be the corresponding language vector Let
diag(Wf Wf
w?S
w?S
denote the sentence representation of defined as the sum of language conditioned word representations for each S. Equivalently we define a sentence representation for the translation of
denoted as We then optimize the following ranking objective
XX
minimize
max
Ck
subject to the constraints that each sentence vector has unit norm Each Ck is a constrastive nontranslation sentence of and denotes all model parameters This type of cross-language ranking
loss was first used by but without the norm constraint which we found significantly improved
the stability of training The Europarl corpus contains roughly million parallel sentence pairs
between English and German as well as English and French for which we induce dimensional
word representations Evaluation is then performed on English and German sections of the Reuters
RCV1/RCV2 corpora Note that these documents are not parallel The Reuters dataset contains
multiple labels for each document Following we only consider documents which have been
assigned to one of the top categories in the label hierarchy These are CCAT Corporate/Industrial
ECAT Economics GCAT Government/Social and MCAT Markets There are a total of
English documents and German documents with vocabulary sizes of English words
and German words We consider both training on English and evaluating on German and
vice versa To represent a document we sum over the word representations of words in that document followed by a unit-ball projection Following we use an averaged perceptron classifier
Classification accuracy is then evaluated on a held-out test set in the other language We used a
monolingual validation set for tuning the margin which was set to Five contrastive terms
were used per example which were randomly assigned per epoch
Table right panel shows our results compared to all proposed methods thus far We are competitive with the current state-of-the-art approaches being outperformed only by BiCVM and
BAE-corr on EN DE. The BAE-corr method combines both a reconstruction term and a
correlation regularizer to match sentences while our method does not consider reconstruction We
also performed experimentation on a low resource task where we assume the same conditions as
above with the exception that we only use parallel sentence pairs between English and German while still incorporating all English and French parallel sentences For this task we compare
against a separation baseline which is the same as our model but with no parameter sharing across
languages and thus resembles Here we achieve and accuracies EN?DE and
DE?EN while the separation baseline obtains and This indicates that parameter sharing across languages can be useful when only a small amount of parallel data is available
Figure further shows t-SNE embeddings of English-German word pairs.1
Another interesting consideration is whether or not the learned language vectors can capture any
interesting properties of various languages To look into this we trained a multiplicative neural
language model simultaneously on languages English French German Czech and Slovak To
our knowledge this is the most languages word representations have been jointly learned on We
We note that Germany and Deutschland are nearest neighbours in the original space
Months
Countries
uncondit ioned ATD
Correlation matrix
uncondit ioned ATD
LBL
condit ioned ATD
Inferred at ribut es difference
Im provem ent over init ial odel
Figure t-SNE embeddings of English-German word pairs learned from Europarl
Docum ent housands
Docum ent housands
Effect of conditional embeddings Effect of inferring attribute vectors
Figure Results on the Blog classification corpus For the middle and right plots each pair of same
coloured bars corresponds to the non-inclusion or inclusion of inferred attribute vectors respectively
computed a correlation matrix from the language vectors illustrated in Interestingly we
observe high correlation between Czech and Slovak representations indicating that the model may
have learned some notion of lexical similarity That being said additional experimentation for future
work is necessary to better understand the similarities exhibited through language vectors
Blog authorship attribution
For our final task we use the Blog corpus of which contains blog posts from
authors For our experiments we break the corpus into two separate datasets one containing the
most prolific authors most blog posts and the other containing all the rest Each author comes
with an attribute tag corresponding to a tuple age gender industry indicating the age range of the
author or whether the author is male or female and what industry the author works
in Note that industry does not necessary correspond to the topic of blog posts We use the dataset
of non-prolific authors to train a multiplicative language model conditioned on an attribute tuple
of which there are unique tuples in total We used dimensional word vectors initialized
from dimensional attribute vectors with random initialization and a context size of A
classification task is then performed on the prolific author subset and evaluation is done
using 10-fold cross-validation Our initial experimentation with baselines found that tf-idf performs
well on this dataset accuracy Thus we consider how much we can improve on the tf-idf
baseline by augmenting word and attribute features
For the first experiment we determine the effect conditional word embeddings have on classification
performance assuming attributes are available at test time For this we compute two embedding
matrices from a trained ATD model one without and with attribute knowledge
unconditioned ATD
conditioned ATD
Wf Wf
Wf diag(Wf Wf
We represent a blog post as the sum of word vectors projected to unit norm and augment these with
tf-idf features As an additional baseline we include a log-bilinear language model Figure
3b illustrates the results from which we observe that conditioned word embeddings are significantly
more discriminative over word embeddings computed without knowledge of attribute vectors
The log-bilinear model has no concept of attributes
Table Results from a conditional word similarity task using Blog attributes and language vectors
Query,A,B
school
f/10/student
m/20/tech
journal
f/10/student
create
f/30/arts
f/30/internet
joy
m/30/religion
m/20/science
cool
m/10/student
f/10/student
Common
work
church
college
diary
blog
webpage
build
develop
maintain
happiness
sadness
pain
nice
funny
awesome
Unique to A
choir
prom
skool
project
book
yearbook
provide
acquire
generate
rapture
god
heartbreak
beautiful
amazing
neat
Unique to
therapy
tech
job
zine
app
referral
compile
follow
analyse
delight
comfort
soul
sexy
hott
lame
English
january
june
october
market
markets
internal
war
weapons
global
said
stated
told
two
two-thirds
both
French
janvier
decembre
juin
marche
marches
interne
guerre
terrorisme
mondaile
dit
disait
declare
deux
deuxieme
seconde
German
januar
dezember
juni
markt
binnenmarktes
marktes
krieg
globale
krieges
sagte
gesagt
sagten
zwei
beiden
zweier
For the second experiment we determine the effect of inferring attribute vectors at test time if they
are not assumed to be available To do this we train a logistic regression classifier within each fold
for predicting attributes We compute an inferred vector by averaging each of the attribute vectors
weighted by the log-probabilities of the classifier In 3c we plot the difference in performance
when an inferred vector is augmented when it is not These results show consistent albeit small
improvement gains when attribute vectors are inferred at test time
To get a better sense of the attribute features learned from the model the supplementary material
contains a t-SNE embedding of the learned attribute vectors Interestingly the model learns features
which largely isolate the vectors of all teenage bloggers independent of gender and topic
Conditional word similarity
One of the key properties of our tensor formulation is the notion of conditional word similarity
namely how neighbours of word representations change depending on the attributes that are conditioned on In order to explore the effects of this we performed two qualitative comparisons one
using blog attribute vectors and the other with language vectors These results are illustrated in
Table For the first comparison on the left we chose two attributes from the blog corpus and a
query word We identify each of these attribute pairs as A and B. Next we computed a ranked list of
the nearest neighbours by cosine similarity of words conditioned on each attribute and identified
the top words in each Out of these words we display the top words which are common
to both ranked lists as well as words that are unique to a specific attribute Our results illustrate
that the model can capture distinctive notions of word similarities depending on which attributes
are being conditioned On the right of Table we chose a query word in English italicized and
computed the nearest neighbours when conditioned on each language vector This results in neighbours that are either direct translations of the query word or words that are semantically similar The
supplementary material includes additional examples with nearest neighbours of collocations
Conclusion
There are several future directions from which this work can be extended One application area
of interest is in learning representations of authors from papers they choose to review as a way of
improving automating reviewer-paper matching Since authors contribute to different research
topics it might be more useful to instead consider a mixture of attribute vectors that can allow for
distinctive representations of the same author across research areas Another interesting application
is learning representations of graphs Recently proposed an approach for learning embeddings
of nodes in social networks Introducing network indicator vectors could allow us to potentially
learn representations of full graphs Finally it would be interesting to train a multiplicative neural
language model simultaneously across dozens of languages
Acknowledgments
We would also like to thank the anonymous reviewers for their valuable comments and suggestions
This work was supported by NSERC Google Samsung and ONR Grant

<<----------------------------------------------------------------------------------------------------------------------->>

title: 4143-pose-sensitive-embedding-by-nonlinear-nca-regression.pdf

Pose-Sensitive Embedding
by Nonlinear NCA Regression
Graham W. Taylor Rob Fergus George Williams Ian Spiro and Christoph Bregler
Courant Institute of Mathematics New York University
New York USA
gwtaylor,fergus,spiro,bregler@cs.nyu.edu
Abstract
This paper tackles the complex problem of visually matching people in similar
pose but with different clothes background and other appearance changes We
achieve this with a novel method for learning a nonlinear embedding based on
several extensions to the Neighborhood Component Analysis NCA framework
Our method is convolutional enabling it to scale to realistically-sized images By
cheaply labeling the head and hands in large video databases through Amazon
Mechanical Turk crowd-sourcing service we can use the task of localizing
the head and hands as a proxy for determining body pose We apply our method
to challenging real-world data and show that it can generalize beyond hand localization to infer a more general notion of body pose We evaluate our method
quantitatively against other embedding methods We also demonstrate that realworld performance can be improved through the use of synthetic data
Introduction
Determining the pose of a human body from one or more images is a central problem in Computer
Vision The complex multi-jointed nature of the body makes the determination of pose challenging
particularly in natural settings where ambiguous and unusual configurations may be observed The
ability to localize the hands is particularly important they provide tight constraints on the layout of
the upper body yielding a strong cue as to the action and intent of a person
A huge range of techniques both parametric and non-parametric exist for inferring body pose from
2D images and 3D datasets 39 28 33 We propose a non-parametric approach to
Figure Query image left column and the eight nearest neighbours found by our method
Distance in the learned embedded space is shown bottom right Matches are based on the location
of the hands and more generally body pose not the individual or the background
estimating body pose by localizing the hands using a parametric nonlinear multi-layered embedding
of the raw pixel images Unlike many other metric learning approaches ours is designed for use with
real-world images having a convolutional architecture that scales gracefully to large images and is
invariant to local geometric distortions
Our embedding trained on both real and synthetic data is a functional mapping that projects images
with similar head and hand positions to lie close-by in a low-dimensional output space Efficient
nearest-neighbour search can then be performed in this space to find images in a large training
corpus that have similar pose Specifically for this task we have designed an interface to obtain
and verify head and hand labels for thousands of frames through Amazon Mechanical Turk with
minimal user intervention We find that our method is able to cope with the terse and noisy labels
provided by crowd-sourcing It succeeds in generalizing to body and hand pose when such cues are
not explicitly provided in the labels
Related work
Our application domain is related to several approaches in the computer vision literature that propose
hand or body pose tracking Many techniques rely on sliding-window part detectors based on color
and other features applied to controlled recording conditions 39 to name a few we
refer to for a complete survey In our domain hands might only occupy a few pixels and the
only body-part that can reliably be detected is the human face Many techniques have
been proposed that extract learn or reason over entire body features Some use a combination of
local detectors and structural reasoning for coarse tracking and for person-dependent
tracking In a similar spirit more general techniques using pictorial structures poselets
and other part-models have received increased attention An entire new stream of kinematic
model-based techniques based on the HumanEva dataset has been proposed but this area differs
from our domain in that the images considered are of higher quality and less cluttered
More closely related to our task are nearest-neighbour and locally-weighted regression-based techniques Some extract shape-context edge based histograms from the human body or just
silhouette features Shakhnarovich use HOG features and boosting for learning a parameter sensitive hash function All these approaches rely on good background subtraction
or recordings with clear backgrounds Our domain contains clutter lighting variations and low
resolution such that it is impossible to separate body features from background successfully We
instead learn relevant features directly from pixels instead of pre-coded edge or gradient histogram
features and discover implicitly background invariance from training data
Several other works have used synthetically created data as a training set We show in
this paper several experiments with challenging real video with crowd-sourced Amazon Mechanical
Turk labels synthetic training data and hybrid datasets Our final system after training is always
applied to the cluttered non-background subtracted real video input without any labels
Our technique is also related to distance metric learning an important area of machine learning
research especially due to recent interest in analyzing complex high-dimensional data A subset
of approaches for dimensionality reduction implicitly learn a distance metric by learning
a function mapping from high-dimensional pixel space to low-dimensional feature space
such that perceptually similar observations are mapped to nearby points on a manifold Neighbourhood Components Analysis NCA proposes a solution where the transformation from input
to feature space is linear and the distance metric is Euclidean NCA learns the transformation that
is optimal for performing KNN in the feature space NCA has also been recently extended to the
nonlinear case using MNIST class labels and to linear 1D regression for reinforcement learning
Dimensionality Reduction by Learning an Invariant Mapping DrLIM also learns a nonlinear mapping Like NCA DrLIM uses class neighbourhood structure to drive the optimization
observations with the same class label are driven to be close-by in feature space Our approach
is also inspired by recent hashing methods 34 although those techniques are restricted to
binary codes for fast lookup
Learning an invariant mapping by nonlinear embedding
We first discuss Neighbourhood Components Analysis and its nonlinear variants We then propose an alternative objective function optimized for performing nearest neighbour regression
rather than classification Next we describe our convolutional architecture which maps images from
high-dimensional to low-dimensional space Finally we introduce a related but different objective
for our model based on DrLIM
Neighbourhood Components Analysis
NCA both linear and nonlinear and DrLIM do not presuppose the existence of a meaningful and
computable distance metric in the input space They only require that neighbourhood relationships
be defined between training samples This is well-suited for learning a metric for non-parametric
classification KNN on high-dimensional data If the original data does not contain discrete
class labels but real-valued labels pose information for images of people one alternative is to
define neighbourhoods based on the distance in the real-valued label space and proceed as usual
However if classification is not our ultimate goal we may wish to exploit the soft nature of the
labels and use an alternative objective one that does not optimize KNN performance
Suppose we are given a set of labeled training cases where RD
and RL Each training point selects another point as its neighbour with some probability
defined by normalizing distances in the transformed feature space
exp(?d2ij
exp(?dik
pij
pii
dij zj
where we use a Euclidean distance metric dij and zi is the mapping parametrized
by from input space to feature space For NCA this is typically linear but it can be extended
to be nonlinear through back-propagation for example in it is a multi-layer neural network
NCA assumes that the labels are discrete rather than real-valued and seeks to
maximize the expected number of correctly classified points on the training data which minimizes
LNCA
pij
j:yi yj
The parameters are found by minimizing LNCA with respect to back-propagating in the case of
a multi-layer parametrization Instead of seeking to optimize KNN classification performance we
can use the NCA regression NCAR objective
LNCAR
pij yj
Intuitively this states that if with high probability and are neighbours in feature space then
they should also lie close-by in label space While we use the Euclidean distance in label space our
approach generalizes to other metrics which may be more appropriate for a different domain
Keller consider the linear case of NCAR where is a weight matrix and is a scalar
representing Bellman error to map states with similar Bellman errors close together Similar to
NCA we can extend this objective to the nonlinear multi-layer case We simply need to compute
the derivative of LNCAR with respect to the output of the mapping zi and backpropagate through
the remaining layers of the network The gradient can be computed efficiently as
LNCAR
zi zj pij yij
pji yij
zi
where we use yij
yj and pij yij
See the supplementary material for details
Convolutional architectures
As points out nonlinear NCA was originally proposed in but with the exception of a
modest success with a two-layer network in extracting 2D codes that explicitly represented the
size and orientation of face images attempts to extract more complex properties using multi-layer
feature extraction were less successful This was due in part to the difficulty in training multi-layer
networks and the fact that many data pairs are required to fit the large number of network parameters
Though both and were successful in learning a multi-layer nonlinear mapping of the data
there is still a fundamental limitation of using fully-connected networks that must be addressed
Such an architecture can only be applied to relatively small image patches typically less than 64
64 pixels because they do not scale well with the size of the input Salakhutdinov and Hinton
escaped this issue by training only on the MNIST dataset 28 images of digits and Torralba
used a global image descriptor as an initial feature representation rather than pixels
However to avoid such hand-crafted features which may not be suitable for the task and to scale to
realistic sized inputs models should take advantage of the pictorial nature of the image input This is
addressed by convolutional architectures which exploit the fact that salient motifs can appear
anywhere in the image By employing successive stages of weight-sharing and feature-pooling
deep convolutional architectures can achieve stable latent representations at each layer that preserve
locality provide invariance to small variations of the input and drastically reduce the number of free
parameters
Our proposed method which we call Convolutional NCA regression C-NCAR is based on a standard convolutional architecture alternating convolution and subsampling layers followed
by a single fully-connected layer It differs from typical convolutional nets in the objective function with which it is trained minimizing Because the loss is defined on pairs of
examples we use a siamese network Pairs of frames are processed by separate networks with
equal weights The loss is then computed on the output of both networks Hadsell also
use a siamese convolutional network with yet a different objective They use their method for visualization but not any discriminative task Mobahi have also recently used a convolutional
siamese network in which temporal coherence between pairs of frames drives the regularization of
the model rather than the objective More details of training our network are given in Sec.
Input
Layer
Layer
Layer
Layer
Output
d(z
Convolutions
tanh abs
Average
pooling
Convolutions
tanh abs
Average
pooling
Fully
connected
Figure Convolutional NCA regression C-NCAR Each image is processed by two convolutional
and subsampling layers and one fully-connected layer A loss computed on the distance
between resulting codes drives parameter learning
Adding a contrastive loss function
Like NCA DrLIM assumes a discrete notion of similarity or dissimilarity between data pairs
and It defines both a similarity loss Ls which penalizes similar points which are far apart
in code space and a dissimilarity loss LD which penalizes dissimilar points which lie within a
user-defined margin of each other
LD dij
LS d2ij
where dij is given by Let be an indicator such that if and are deemed
similar and if and are deemed dissimilar For example if labels are discrete
then for yj and otherwise The total loss is defined by
LDrLIM
Ls LD
When faced with real-valued labels we can avoid explicitly defining similarity and dissimilarity
via thresholding by defining a soft notion of similarity
exp(?||yi yj
exp(?||yi yj
Replacing the indicator variables with in yields what we call the soft DrLIM loss
Experimental results
We evaluate our approach in real and synthetic environments by performing 1-nearest neighbour
regression using a variety of standard and learned metrics described below For every query
image in a test set we compute its distance under the metric to each of the training points in a
database We then copy the label position of the head and hands of the neighbour to the
query example For evaluation we compare the ground-truth label of the query to the label of the
nearest neighbour Errors are reported in terms of mean pixel error over each query and each marker
the head if it is tracked and each hand Errors are absolute with respect to the original image size
We acknowledge that improved results could potentially be obtained by using more than one neighbour or with more sophisticated techniques such as locally weighted regression However we
focus on learning a good metric for performing this task rather than the regression problem The
approaches compared are
Pixel distance can be used to find nearest neighbours though it is not practical in real situations due
to the intractability of computing distances in such a high-dimensional space
GIST descriptors are a global representation of image content.We are motivated to use GIST
by its previous use in nonlinear NCA for image retrieval The resulting image representation
is a length-512 vector We note that this is still too large for efficient NN search and that the GIST
features are not domain-adaptive
Linear NCA regression NCAR is described in Section We pre-compute GIST for each image
and use that as our input representation We learn a 32 matrix of weights by minimizing
using nonlinear conjugate gradients with randomly sampled mini-batches of size We
perform three line-searches per mini-batch and stop learning after mini-batches We found that
our results slightly improved when we applied a form of local contrast normalization LCN prior
to computing GIST Each pixel?s response was normalized by the integrated response of a
window of neighbouring pixels For more details see
Convolutional NCA regression C-NCAR See for a summary of our architecture Images
are pre-processed using LCN. Convolutions are followed by pixel-wise tanh and absolute value
rectification The abs prevents cancellations in local neighbourhoods during average downsampling
Our architectural parameters size of filters number of filter banks etc are chosen to produce
a 32-dimensional output Derivations of parameter updates are presented as supplementary material
Soft DrLIM S-DrLIM and Convolutional soft DrLIM CS-DrLIM We also experiment with a
variant of an alternative energy-based method that adds an explicit contrastive loss to the objective
rather than implicitly through normalization The contrastive loss only operates on dissimilar points
which lie within a specified margin of each other We use as suggested by
In both the linear and nonlinear case the architecture and training procedure remains the same as
NCAR and C-NCAR respectively We use a different objective minimizing with respect to
the parameters
Estimating 2D head and hand pose from synthetic data
We extracted frames of training data and frames of test data from Poser renderings
of several hours of real motion capture data Our synthetic data is similar to that considered in
however we use a variety of backgrounds rather than a constant background Furthermore subjects
are free to move around the frame and are rendered at various scales The training set contains
different characters superimposed on different backgrounds The test set contains characters and
backgrounds not present in the training set The inputs are images and the labels
are 6D vectors the true locations of the head and hands
Results are shown in Table column SY). Simple linear NCAR performs well compared to the
baselines while our nonlinear methods C-NCAR and CS-DrLIM which are not restricted to the
GIST descriptor significantly outperform all other approaches Pixel-based matching though extremely slow does surprisingly well This is perhaps an artifact of the synthetic data
Estimating 2D hand pose from real video
We digitally recorded all of the contributing and invited speakers at the Learning Workshop Snowbird held in April The set consisted of speakers with talks ranging from minutes
each After each session of talks blocks of frames were distributed as Human Intelligence Tasks
Table regression performance on the synthetic dataset and the real dataset Results are divided into baselines no learning linear embeddings and nonlinear embeddings Errors
are the mean pixel distance between the nearest neighbour and the ground truth label of the query
For SY we locate the head and both hands For RE we assume the location and scale of the head is
given by a face detector and only locate the hands The images at right indicate top a radius of
pixels with respect to the SY input bottom a radius of pixels with respect to
the RE input Images have been scaled for the plot
Embedding
None
None
PCA
PCA
NCAR
NCAR
S-DrLIM
Boost-SSC
C-NCAR
CS-DrLIM
Input
Pixels
GIST
GIST
GIST
GIST
LCN+GIST
GIST
LCN+GIST
LCN
LCN
Dim
32
32
32
32
32
32
32
Error-SY
Error-RE
on Amazon Mechanical Turk We were able to obtain accurate hand and head tracks for each of the
speakers within a few hours of their talks For the following experiments we divided the speakers
into a training set odd numbered speakers and test set even numbered speakers
Since current state-of-the-art face detection algorithms work reasonably well we concentrate on the
harder problem of tracking the speakers hands We first run a commercial face detection algorithm
on all frames which provides an estimate of scale for every frame We use the average scale per
video estimated by the face detector to crop and rescale each frame to a image centered
on the head that contains the speaker at roughly the same scale as other speakers there is some
variability due to using average scale per video as speakers move throughout their talks A similar
preprocessing step was used in We do not consider cases in which the hands lie outside
the frame or are occluded This yields and training and test images respectively
containing the head and both hands Since the images are head-centered the labels used during
training are the 4-dimensional vector containing the relative offset of each hand from the head
We emphasize that finding the hands is an extremely difficult task sometimes even for human subjects Frames are low-resolution typically the hands are pixels in diameter and contain
camera movement as well as frequently poor lighting While previous work has assumed static
backgrounds we confront the changing backgrounds and aim to learn invariance to both scene and
subject identity
Results are shown in Table column RE). They are organized into three groups baselines highdimensional and learning-based methods both linear and nonlinear The linear methods are able
to achieve performance comparable to the baseline with the important attribute that distances are
computed in a 32-dimensional space If the codes are made binary as in we could use fast
approximate hashing techniques to permit real-time tracking using a database of well over million
examples The nonlinear methods show a dramatic improvement over the linear methods especially
our convolutional architectures which learn features from pixels Boost-SSC is based on a
global representation similar to GIST and so it is restricted in domain adaptivity We also investigate
the performance of C-NCAR on code size Performance is impressive even when the
dimension in which we compute distances is reduced from 32 to A visualization of the 2D
embedding is shown in
shows some examples of nearest-neighbour matches under several different metrics Most
apparent is that our methods and in particular C-NCAR develop invariance to background and focus
on the subject?s pose Both pixel-based and GIST-batch matching are highly driven by the scene
including lighting and background Though our method is trained only on the relative positions
of the hands from the head it appears to capture something more substantial about body pose in
general We plan on evaluating this result quantitatively using synthetic data in which we have
access to an articulated skeleton
px
c1
c1
c1
c2
c2
c2
c3
c3
c3
c4
c4
Figure Visualization of the 2D C-NCAR embedding of points from the RE training set We
show the data points and their local geometry within four example clusters Note that even
with a 2D embedding we are able to capture pose similarity invariant of subject and background
Query
C-NCAR
NCAR
GIST
Pixels
Figure Nearest neighbour pose estimation The leftmost column shows the query image and
the remaining columns left to right show the nearest neighbour found by nonlinear C-NCAR
regression linear NCAR GIST pixel distance Circles mark the pose obtained by crowd-sourcing
we superimpose the pose estimated by C-NCAR onto the query with crosses
Improving real-world performance with synthetic data
There has been recent interest in using synthetic examples to improve performance on real-world
vision tasks The subtle differences between real and synthetic data make it difficult to
apply existing techniques to a dataset comprised of both types of examples This problem falls under
the domain of transfer learning but to the best of our knowledge transfer learning between real and
synthetic pairings is relatively unexplored While previous work has attempted to learn representations that are invariant to such effects as geometric distortions of the input and temporal shifts
we know of no previous work that has explicitly attempted to learn features that are invariant
to the nature of the input that is real or synthetic
Relative error test
Pixel error test
19
18
17
No synthetic
NCAR?1
NCAR?2
Dimension of code
32
Number of Synthetic Examples
Figure Effect of code size on the performance of Convolutional NCA regression Adding
synthetic data to a fixed dataset of real examples to improve test performance measured on
real data Error is expressed relative to a training set with no synthetic data NCAR-1 does not
re-initialize weights when more synthetic examples are added NCAR-2 reinitializes weights to
the same random seed for each run The curves show that adding synthetic examples improve
performance up to a point at which the synthetic examples outnumber the real examples
The pairwise nature of our approach is well-suited to learning such invariance provided that we have
established correspondences between real and synthetic examples In our case of pose estimation
this comes from the labels By forcing examples with similar poses regardless of whether they are
real or synthetic to lie close-by in code space we can implicitly produce a representation at each
layer that is invariant to the nature of the input We have not made an attempt to restrict pairings to
be only between real and synthetic examples though this may further aid in learning invariance
demonstrates the effect of gradually adding synthetic examples from SY to the RE training
dataset We use a reduced-size set of real examples for training which is gradually modified
to contain synthetic examples and a fixed set of real examples for testing Error is expressed
relative to the case of no synthetic examples We use Linear NCA for this experiment and train as
described above We follow two different regimes In NCAR-1 we do not reset the weights of the
model to random each time we adjust the training set to add more synthetic examples We simply
add more synthetic data and continue learning In NCAR-2 we reset the weights to the same random
seed for each run The overall result is the same for each regime the addition of synthetic examples
to the training set improves test performance on real data up to a level at which the number of
synthetic examples is double the number of real examples
Conclusions
We have presented a nonparametric approach for pose estimation in realistic challenging video
datasets At the core of our method is a learned parametric mapping from high-dimensional space to
a low-dimensional space in which distance is efficiently computed Our work differs from previous
attempts at learning invariant mappings in that it is optimized for nearest neighbour regression rather
than classification and it scales to realistic sized images through the use of convolution and weightsharing This permits us to learn domain-adaptive features directly from pixels rather than relying
on hand-crafted features or global descriptors
In our experiments we have restricted ourselves to matching but we plan to investigate other
more sophisticated approaches such as locally weighted regression or using the match as an initialization for a gradient descent search in a parametric model Though we work with video our model
does not rely on any type of temporal coherence Integrating temporal knowledge in the form of a
prior would benefit our approach Alternatively temporal context could be integrated at the input
level from simple frame differencing to more sophisticated temporal feature extraction
Our entire network is trained end-to-end with a single objective and we do not perform any network pre-training as in Recent work has demonstrated that pre-training can successfully
be applied to convolutional architectures both in the context of RBMs and sparse coding We intend to investigate the effect of pre-training as well as the use of mixed generative
and discriminative objectives

<<----------------------------------------------------------------------------------------------------------------------->>

title: 5017-the-power-of-asymmetry-in-binary-hashing.pdf

The Power of Asymmetry in Binary Hashing
Behnam Neyshabur
Payman Yadollahpour
Yury Makarychev
Toyota Technological Institute at Chicago
btavakoli,pyadolla,yury]@ttic.edu
Ruslan Salakhutdinov
Departments of Statistics and Computer Science
University of Toronto
rsalakhu@cs.toronto.edu
Nathan Srebro
Toyota Technological Institute at Chicago
and Technion Haifa Israel
nati@ttic.edu
Abstract
When approximating binary similarity using the hamming distance between short
binary hashes we show that even if the similarity is symmetric we can have
shorter and more accurate hashes by using two distinct code maps I.e. by approximating the similarity between and as the hamming distance between
and for two distinct binary codes rather than as the hamming distance
between and
Introduction
Encoding high-dimensional objects using short binary hashes can be useful for fast approximate
similarity computations and nearest neighbor searches Calculating the hamming distance between
two short binary strings is an extremely cheap computational operation and the communication cost
of sending such hash strings for lookup on a server sending hashes of all features or patches in
an image taken on a mobile device is low Furthermore it is also possible to quickly look up nearby
hash strings in populated hash tables Indeed it only takes a fraction of a second to retrieve a shortlist
of similar items from a corpus containing billions of data points which is important in image video
audio and document retrieval tasks Moreover compact binary codes are remarkably
storage efficient and allow one to store massive datasets in memory It is therefore desirable to find
short binary hashes that correspond well to some target notion of similarity Pioneering work on
Locality Sensitive Hashing used random linear thresholds for obtaining bits of the hash Later
work suggested learning hash functions attuned to the distribution of the data
More recent work focuses on learning hash functions so as to optimize agreement with the target
similarity measure on specific datasets It is important to obtain accurate and short
hashes?the computational and communication costs scale linearly with the length of the hash and
more importantly the memory cost of the hash table can scale exponentially with the length
In all the above-mentioned approaches similarity between two objects is approximated by
the hamming distance between the outputs of the same hash function between and
for some The emphasis here is that the same hash function is applied to both and
methods like LSH multiple hashes might be used to boost accuracy but the comparison is still
between outputs of the same function
The only exception we are aware of is where a single mapping of objects to fractional vectors
is used its thresholding signDf?(x
is used in the database
and similarity between and is approximated using This has become known
as asymmetric hashing but even with such a-symmetry both mappings are based on the
same fractional mapping That is the asymmetry is in that one side of the comparison gets
thresholded while the other is fractional but not in the actual mapping
In this paper we propose using two distinct mappings and approximating the
similarity by the hamming distance between and We refer to such hashing
schemes as asymmetric Our main result is that even if the target similarity function is symmetric and well behaved even if it is based on Euclidean distances between objects using
asymmetric binary hashes can be much more powerful and allow better approximation of the target similarity with shorter code lengths In particular we show extreme examples of collections
of points in Euclidean space where the neighborhood similarity can be realized using an
asymmetric binary hash based on a pair of distinct functions of length bits but where a symmetric hash based on a single function would require at least bits Although actual data is
not as extreme our experimental results on real data sets demonstrate significant benefits from using
asymmetric binary hashes
Asymmetric hashes can be used in almost all places where symmetric hashes are typically used
usually without any additional storage or computational cost Consider the typical application of
storing hash vectors for all objects in a database and then calculating similarities to queries by
computing the hash of the query and its hamming distance to the stored database hashes Using
an asymmetric hash means using different hash functions for the database and for the query This
neither increases the size of the database representation nor the computational or communication
cost of populating the database or performing a query as the exact same operations are required
In fact when hashing the entire database asymmetric hashes provide even more opportunity for
improvement We argue that using two different hash functions to encode database objects and
queries allows for much more flexibility in choosing the database hash Unlike the query hash
which has to be stored compactly and efficiently evaluated on queries as they appear if the database
is fixed an arbitrary mapping of database objects to bit strings may be used We demonstrate that
this can indeed increase similarity accuracy while reducing the bit length required
Minimum Code Lengths and the Power of Asymmetry
Let be a binary similarity function over a set of objects where we can
interpret to mean that and are similar or dissimilar or to indicate whether they are
neighbors A symmetric binary coding of is a mapping where is the bitlength of the code We are interested in constructing codes such that the hamming distance between
and corresponds to the similarity That is for some threshold
sign(hf Although discussing the hamming distance it is more convenient for us
to work with the inner product hu vi which is equivalent to the hamming distance dh since
hu vi 2dh for
In this section we will consider the problem of capturing a given similarity using an arbitrary binary
code That is we are given the entire similarity mapping as a matrix over
a finite domain of objects with Sij S(xi We ask for an encoding
ui of each object and a threshold such that Sij sign(hui uj
or at least such that equality holds for as many pairs as possible It is important to emphasize
that the goal here is purely to approximate the given matrix using a short binary code?there is no
out-of-sample generalization
We now ask Can allowing an asymmetric coding enable approximating a symmetric similarity
matrix with a shorter code length
Denoting for the matrix whose columns contain the codewords ui the minimal
binary code length that allows exactly representing is then given by the following matrix factorization problem
ks min
s.t
Sij Yij
where 1n is an matrix of ones
We begin demonstrating the power of asymmetry by considering an asymmetric variant of the above
problem That is even if is symmetric we allow associating with each object two distinct
binary codewords ui and vi we can think of this as having two arbitrary
mappings ui and vi g(xi such that Sij sign(hui vj The minimal asymmetric
binary code length is then given by
ka min s.t
Sij Yij
Writing the binary coding problems as matrix factorization problems is useful for understanding
the power we can get by asymmetry even if is symmetric and even if we seek a symmetric
insisting on writing as a square of a binary matrix might be a tough constraint This is captured
in the following Theorem which establishes that there could be an exponential gap between the
minimal asymmetry binary code length and the minimal symmetric code length even if the matrix
is symmetric and very well behaved
Theorem
For any there exists a set of points in Euclidean space with similarity matrix
if kxi
Sij
such that ka 2r but ks 2r
if kxi
Proof Let I1 and I2 Consider the matrix defined by
Gii Gij if I1 or I2 and Gij otherwise Matrix is
diagonally dominant By the Gershgorin circle theorem is positive definite Therefore there exist
vectors such that hxi Gij for every and Define
if kxi
Sij
if kxi
Note that if then Sij if and I1 I1 I2 I2 then kxi
Gii Gjj 2Gij and therefore Sij Finally if and I1
then kxi Gii Gjj 2Gij and therefore Sij We show that
ka Let be an matrix whose column vectors are the vertices of the cube
any
let be anr matrix defined by Cij if I1 and Cij if I2 Let
order
and
For where threshold we have that Yij
if Sij and Yij if Sij Therefore ka
Now we show that ks ks Consider and as in Let Note
that Yij0 ks and thus ks Let ones
followed by minus ones We have
Yii0
Yij0
Yij0
i,j:Sij
ks
i,j:Sij
i,j:Sij
i,j:Sij
nks
nks n2
2nks n2
We conclude that ks
The construction of Theorem shows that there exists data sets for which an asymmetric binary hash
might be much shorter then a symmetric hash This is an important observation as it demonstrates
that asymmetric hashes could be much more powerful and should prompt us to consider them
instead of symmetric hashes The precise construction of Theorem is of course rather extreme
fact the most extreme construction possible and we would not expect actual data sets to have this
exact structure but we will show later significant gaps also on real data sets
Uniform
Symmetric
Asymmetric
Symmetric
Asymmetric
LabelMe
70
bits
bits
35
Average Precision
Average Precision
Figure Number of bits required for approximating two similarity matrices as a function of average precision Left uniform data in the 10-dimensional hypercube similarity represents a thresholded Euclidean
distance set such that of the similarities are positive Right Semantic similarity of a subset of LabelMe
images thresholded such that of the similarities are positive
Approximate Binary Codes
As we turn to real data sets we also need to depart from seeking a binary coding that exactly
captures the similarity matrix Rather we are usually satisfied with merely approximating and
for any fixed code length seek the symmetric or asymmetric k-bit code that best captures the
similarity matrix S. This is captured by the following optimization problem
min L(Y
Yij
i,j:Sij
i,j:Sij
where is the zero-one-error and is a parameter that allows us to weight positive
and negative errors differently Such weighting can compensate for Sij being imbalanced typically
many more pairs of points are non-similar rather then similar and allows us to obtain different
balances between precision and recall
The optimization problem is a discrete discontinuous and highly non-convex problem In our
experiments we replace the zero-one loss with a continuous loss and perform local search
by greedily updating single bits so as to improve this objective Although the resulting objective
let alone the discrete optimization problem is still not convex even if is convex we found it
beneficial to use a loss function that is not flat on so as to encourage moving towards the
correct sign In our experiments we used the square root of the logistic loss
Before moving on to out-of-sample generalizations we briefly report on the number of bits needed
empirically to find good approximations of actual similarity matrices with symmetric and asymmetric codes We experimented with several data sets attempting to fit them with both symmetric and
asymmetric codes and then calculating average precision by varying the threshold while keeping
and fixed Results for two similarity matrices one based on Euclidean distances between
points uniformly distributed in a hypoercube and the other based on semantic similarity between
images are shown in Figure
Out of Sample Generalization Learning a Mapping
So far we focused on learning binary codes over a fixed set of objects by associating an arbitrary
code word with each object and completely ignoring the input representation of the objects
We discussed only how well binary hashing can approximate the similarity but did not consider
generalizing to additional new objects However in most applications we would like to be able to
have such an out-of-sample generalization That is we would like to learn a mapping
over an infinite domain using only a finite training set of objects and then apply the
mapping to obtain binary codes for future objects to be encountered such that
sign(hf Thus the mapping is usually limited to some constrained
parametric class both so we could represent and evaluate it efficiently on new objects and to ensure
good generalization For example when Rd we can consider linear threshold mappings
fW sign(W where Rk?d and sign operates elementwise as in Minimal Loss
Hashing Or we could also consider more complex classes such as multilayer networks
We already saw that asymmetric binary codes can allow for better approximations using shorter
codes so it is natural to seek asymmetric codes here as well That is instead of learning a single
parametric map we can learn a pair of maps and both
constrained to some parametric class and a threshold such that sign(hf
This has the potential of allowing for better approximating the similarity and thus better overall
accuracy with shorter codes despite possibly slightly harder generalization due to the increase in
the number of parameters
In fact in a typical application where a database of objects is hashed for similarity search over
future queries asymmetry allows us to go even further Consider the following setup We are given
objects from some infinite domain and the similarities S(xi between
these objects Our goal is to hash these objects using short binary codes which would allow us to
quickly compute approximate similarities between these objects the database and future objects
the query That is we would like to generate and store compact binary codes for objects in a
database Then given a new query object we would like to efficiently compute a compact binary
code for a given query and retrieve similar items in the database very fast by finding binary codes
in the database that are within small hamming distance from the query binary code Recall that it
is important to ensure that the bit length of the hashes are small as short codes allow for very fast
hamming distance calculations and low communication costs if the codes need to be sent remotely
More importantly if we would like to store the database in a hash table allowing immediate lookup
the size of the hash table is exponential in the code length
The symmetric binary hashing approach would be to find a single parametric mapping
such that sign(hf for future queries and database
objects calculate for all database objects and store these hashes perhaps in a hash table
allowing for fast retrieval of codes within a short hamming distance The asymmetric approach
described above would be to find two parametric mappings and
such that sign(hf g(xi and then calculate and store g(xi
But if the database is fixed we can go further There is actually no need for to be in a constrained
parametric class as we do not need to generalize to future objects nor do we have to efficiently
calculate it on-the-fly nor communicate to the database Hence we can consider allowing the
database hash function to be an arbitrary mapping That is we aim to find a simple parametric
mapping and arbitrary codewords v1 vn for each
in the database such that sign(hf vi for future queries and for the objects
in the database This form of asymmetry can allow us for greater approximation power
and thus better accuracy with shorter codes at no additional computational or storage cost
In Section we evaluate empirically both of the above asymmetric strategies and demonstrate their
benefits But before doing so in the next Section we discuss a local-search approach for finding the
mappings or the mapping and the codes v1 vn
Optimization
We focus on Rd and linear threshold hash maps of the form sign(W where
Rk?d Given training points we consider the two models discussed above
IN IN We learn two linear threshold functions sign(Wq and sign(Wd
I.e. we need to find the parameters Wq Wd Rk?d
IN We learn a single linear threshold function sign(Wq and codewords
v1 vn Rk I.e. we need to find Wq Rk?d as well as Rk?n where vi
are the columns of
In either case we denote ui and in IN IN also vi g(xi and learn by attempting to
minimizing the objective in where is again a continuous loss function such as the square
root of the logistic That is we learn by optimizing the problem with the additional constraint
sign(Wq and possibly also sign(Wd for IN where
Rd?n
We optimize these problems by alternatively updating rows of Wq and either rows of Wd for
IN IN or of for IN To understand these updates let us first return to with un5
constrained and consider updating a row Rn of Denote
the prediction matrix with component subtracted away It is easy to verify that we can write
L(U
where does not depend on and and Rn?n
also does not depend on and is given by
Sij Yij Sij Yij
Mij
with or depending on Sij This implies that we can optimize over the entire
row concurrently by maximizing and so the optimum conditioned on and all
other rows of is given by
sign(M
Symmetrically we can optimize over the row conditioned on and the rest of or in the
case of IN conditioned on Wq and the rest of
Similarly optimizing over a row of Wq amount to optimizing
XD
arg max sign(w(t X)M arg max
Mi sign
Rd
Rd
This is a weighted zero-one-loss binary classification problem with targets sign Mi and
weights Mi We approximate it as a weighted logistic regression problem and at each
update iteration attempt to improve the objective using a small number epochs of stochastic
gradient descent on the logistic loss For IN IN we also symmetrically update rows of Wd
When optimizing the model for some bit-length we initialize to the optimal 1-length model
We initialize the new bit either randomly or by thresholding the rank-one projection of for
unconstrained or the rank-one projection after projecting the columns of for IN or
both rows and columns of for IN IN to the column space of We take the initialization
random or rank-one based that yields a lower objective value
Empirical Evaluation
In order to empirically evaluate the benefits of asymmetry in hashing we replicate the experiments
of which were in turn based on on six datasets using learned symmetric linear threshold
codes These datasets include LabelMe and Peekaboom are collections of images represented as
GIST features Photo-tourism is a database of image patches represented as SIFT
features MNIST is a collection of greyscale handwritten images and Nursery contains
8D features Similar to we also constructed a synthetic Uniform dataset containing
uniformly sampled points for a hypercube We used points for training and for
testing
For each dataset we find the Euclidean distance at which each point has on average neighbours
This defines our ground-truth similarity in terms of neighbours and non-neighbours So for each
dataset we are given a set of points represented as vectors in Rd and the binary
similarities S(xi between the points with corresponding to and being neighbors and
otherwise Based on these training points present a sophisticated optimization approach
for learning a thresholded linear hash function of the form sign(W where Rk?d
This hash function is then applied and are stored in the database evaluate
the quality of the hash by considering an independent set of test points and comparing to
sign(hf on the test points and the database objects training points
In our experiments we followed the same protocol but with the two asymmetric variations IN IN
and IN using the optimization method discussed in Sec. In order to obtain different balances
between precision and recall we should vary in obtaining different codes for each value of
Uniform
LIN:V
LIN:LIN
MLH
KSH
BRE
LSH
24
28
32
36
44
48
52
56
LIN:V
LIN:LIN
MLH
KSH
BRE
LSH
64
24
Number of Bits
36
44
48
52
56
24
28
32
36
44
24
48
52
56
32
36
44
48
52
56
64
LIN:V
LIN:LIN
MLH
KSH
BRE
LSH
64
28
Nursery
Number of Bits
24
28
32
36
44
48
52
56
Average Precision
LIN:V
LIN:LIN
MLH
KSH
BRE
LSH
Average Precision
Number of Bits
LIN:V
LIN:LIN
MLH
KSH
BRE
LSH
64
Photo-tourism
Average Precision
32
Number of Bits
Peekaboom
28
Average Precision
MNIST
Average Precision
Average Precision
LabelMe
LIN:V
LIN:LIN
MLH
KSH
BRE
LSH
64
Number of Bits
24
28
32
36
44
48
52
56
64
Number of Bits
Figure Average Precision of points retrieved using Hamming distance as a function of code length
for six datasets Five curves represent LSH BRE KSH MLH and two variants of our method Asymmetric
LIN-LIN and Asymmetric LIN-V Best viewed in color
LabelMe
MNIST
45
35
35
35
Average Precision
LIN:V
LIN:LIN
MLH
KSH
Bits Required
45
Average Precision
LIN:V
LIN:LIN
MLH
KSH
Peekaboom
45
Bits Required
Bits Required
LIN:V
LIN:LIN
MLH
KSH
Average Precision
Figure Code length required as a function of Average Precision for three datasets
However as in the experiments of we actually learn a code mappings and or
a mapping and matrix using a fixed value of and then only vary the threshold to
obtain the precision-recall curve
In all of our experiments in addition to Minimal Loss Hashing we also compare our approach to three other widely used methods Kernel-Based Supervised Hashing KSH of Binary
Reconstructive Embedding BRE of and Locality-Sensitive Hashing LSH of
In our first set of experiments we test performance of the asymmetric hash codes as a function of
the bit length Figure displays Average Precision of data points retrieved using Hamming
distance as a function of code length These results are similar to ones reported by where MLH
yields higher precision compared to BRE and LSH. Observe that for all six datasets both variants
of our method asymmetric IN IN and asymmetric IN consistently outperform all other
methods for different binary code length The gap is particularly large for short codes For example
for the LabelMe dataset MLH and KSH with bits achieve AP of and respectively
whereas IN already achieves AP of with only bits Figure shows similar performance
gains appear for a number of other datasets We also note across all datasets IN improves upon
IN IN for short-sized codes These results clearly show that an asymmetric binary hash can be
much more compact than a symmetric hash
We used the BRE KSH and MLH implementations available from the original authors For each method
we followed the instructions provided by the authors More specifically we set the number of points for each
hash function in BRE to and the number of anchors in KSH to the default values For MLH we learn
the threshold and shrinkage parameters by cross-validation and other parameters are initialized to the suggested
values in the package
LabelMe
64 bits
MNIST
bits
LIN:V
LIN:LIN
MLH
KSH
BRE
LSH
LIN:V
LIN:LIN
MLH
KSH
BRE
LSH
Recall
Precision
Precision
Precision
Precision
bits
LIN:V
LIN:LIN
MLH
KSH
BRE
LSH
Recall
64bits
LIN:V
LIN:LIN
MLH
KSH
BRE
LSH
Recall
Recall
Figure Precision-Recall curves for LabelMe and MNIST datasets using and 64 binary codes Best
viewed in color
LIN:V
MLH
KSH
Recall
Precision
LIN:V
MLH
KSH
Recall
Number Retrieved
Figure Left Precision-Recall curves for the Semantic LabelMe dataset Right Percentage of
ground-truth neighbours as a function of retrieved images Best viewed in color
Next we show in Figure the full Precision-Recall curves for two datasets LabelMe and MNIST
and for two specific code lengths and 64 bits The performance of IN IN and IN is almost
uniformly superior to that of MLH KSH and BRE methods We observed similar behavior also for
the four other datasets across various different code lengths
Results on previous datasets show that asymmetric binary codes can significantly outperform
other state-of-the-art methods on relatively small scale datasets We now consider a much larger
LabelMe dataset called Semantic LabelMe It contains training images and
test images where each image is represented by a GIST descriptor The dataset also provides a
semantic similarity between two images based on semantic content object labels overlap in
two images As argued by hash functions learned using semantic labels should be more useful
for content-based image retrieval compared to Euclidean distances Figure shows that IN with
64 bits substantially outperforms MLH and KSH with 64 bits
Summary
The main point we would like to make is that when considering binary hashes in order to approximate similarity even if the similarity measure is entirely symmetric and well behaved much power
can be gained by considering asymmetric codes We substantiate this claim by both a theoretical
analysis of the possible power of asymmetric codes and by showing in a fairly direct experimental
replication that asymmetric codes outperform state-of-the-art results obtained for symmetric codes
The optimization approach we use is very crude However even using this crude approach we could
find asymmetric codes that outperformed well-optimized symmetric codes It should certainly be
possible to develop much better and more well-founded training and optimization procedures
Although we demonstrated our results in a specific setting using linear threshold codes we believe
the power of asymmetry is far more widely applicable in binary hashing and view the experiments
here as merely a demonstration of this power Using asymmetric codes instead of symmetric codes
could be much more powerful and allow for shorter and more accurate codes and is usually straightforward and does not require any additional computational communication or significant additional
memory resources when using the code We would therefore encourage the use of such asymmetric
codes with two distinct hash mappings wherever binary hashing is used to approximate similarity
Acknowledgments
This research was partially supported by NSF CAREER award and NSF grant

<<----------------------------------------------------------------------------------------------------------------------->>

title: 251-a-self-organizing-associative-memory-system-for-control-applications.pdf

Hormel
A Sell-organizing Associative
Memory System lor Control
Applications
Michael Bormel
Department of Control Theory and Robotics
Technical University of Darmstadt
Schlossgraben
Darmstadt/W.-Ger.any
ABSTRACT
The CHAC storage scheme has been used as a basis
for a software implementation of an associative
emory system AHS which itself is a major part
of the learning control loop LERNAS A major
disadvantage of this CHAC-concept is that the
degree of local generalization area of interpolation is fixed This paper deals with an algorithm for self-organizing variable generalization for the AKS based on ideas of T. Kohonen
INTRODUCTION
For several years research at the Department of Control Theory and Robotics at the Technical University of Darmstadt
has been concerned with the design of a learning real-time
control loop with neuron-like associative memories LERNAS
A Self-organizing Associative Memory System for Control Applications
for the control of unknown nonlinear processes Ersue
Tolle This control concept uses an associative memory system AHS based on the cerebellar cortex model CHAC by
Albus Albus for the storage of a predictive nonlinear process model and an appropriate nonlinear control strategy
e&;ected process response
I>.
planne
control inputs
red setpoint
co
predictive
process
IF
opti.iud
control input
eValuation
opti.ization
actual/past
process infor.ation
control strate
actual control input
I
unknown process
I>.
I
short ter
e.ory
process infor.ation
I
I
laSSOCialive lIe.ory syste
Figure The learning control loop LERNAS
One problem for adjusting the control loop to a process is
however to find a suitable set of parameters for the associative memory The parameters in question determine the
degree of generalization within the memory and therefore
have a direct influence on the number of training steps required to learn the process behaviour For a good performance of the control loop it is desirable to have a very
small generalization around a given setpoint but to have a
large generalization elsewhere Actually the amount of collected data is small during the transition phase between two
Hormel
setpoints but is large during setpoint control Therefore a
self-organizing variable generalization adapting itself to
the amount of available data would be very advantageous
Up to now when working with fixed generalization finding
the right parameters has meant to find the best compromise
between performance and learning time required to generate a
process model This paper will show a possibility to introduce a self-organizing variable generalization capability
into the existing AMS/CMAC algorithm
THE AMS-CONCEPT
The associative memory syste AMS is based on the Cerebellar Model Articulation Controller CMAC as presented by J.S.
Albus The information processing structure of AMS can be
divided into three stages
Each component of a n-dimensional input vector stimulus activates a fixed number of sensory cells the
receptive fields of which are overlapping So n?p sensory cells become active
The active sensory cells are grouped to form n-dimensional vectors These vectors are mapped to association cells The merged receptive fields of the sensory
cells described by one vector can be seen as a hypercube
in the n-dimensional input space and therefore as the
receptive field of the association cell In normal applications the total number of available association
cells is about
The association cells are connected to the output cells
by modifiable synaptic weights The output cell computes
the mean value of all weights that are connected to active association cells active weights
Figure shows the basic principle of the associative memory
system AMS.
A Self-organizing Associative Memory System for Control Applications
output value
input space
adjustable weights
Figure The basic aechanism of AMS
During training the generated output is compared with a desired output the error is computed and equally distributed
over all active weights For the mapping of sensory cells to
association cells a hash-coding mechanism is used
THE SELF-ORGANIZING FEATURE MAP
An approach for explaining the self-organizing capabilities
of the nervous system has been presented by T. Kohonen Kohonen
In his self-organizing feature mapft a network of laterally
interconnected neurons can adapt itself according to the
density of trained points in the input space Presenting a
n-diaensional input vector to the network causes every neuron to produce an output signal which is correlated with the
similarity between the input vector and a template vector
which may be stored in the synaptic weights of the neuron
Due to the mexican-hat coupling function between the neurons the one with the maximum output activity will excite
its nearest neighbours but will inhibit neurons farther away therefore generating a localized response in the network The active cells can now adapt their input weights in
order to increase their similarity to the input vector If
we define the receptive field of a neuron by the number of
input vectors for which the neurons activity is greater than
Hormel
that of any other neuron in the net this yields the effect
that in areas with a high density of trained points the receptive fields become small whereas in areas with a low density of trained points the size of the receptive fields is
large Is mentioned above this is a desired effect when
workin with a learning control loop
SELF-ORGANIZING VARIABLE GENERALIZATION
Both of the approaches above have several advantages and
disadvantages when using them for real-time control applications
In the AKS algorithm one does not have to care for predefining a network and the coupling functions or coupling matrices among the elements of the network Association and
weight cells are generated when they are needed during
training and can be adressed very quietly to produce a memory response One of the disadvantages is the fixed generalization once the parameters of a eaory unit have been
chosen
Unlike AHS the feature map allows the adaption of the network according to the input data This advantage has to be
payed for by extensive search for the best matching neuron
in the network and therefore the response time of the network aay be too large for real-tiae control when working
with big networks
These problems can be overcome when allowing that the mapping of sensory cells to association cells in AKS is no
longer fixed but can be changed during training
To accomplish this a template vector is introduced for
every association cell This vector serves as an indicator
for the stimuli by which the association cell has been accessed previously During an associative recall for a stimulus a preliminary set of association cells is activated
by the hash coding mechanism Due to the self-organizing
process during training the template vectors do not need to
correspond to the input vector For the search for the
A Self-organizing Associative Memory System for Control Applications
best aatching cell the template vector of the accessed
association cell is compared to the stiaulus and a difference vector is calculated
L.v
number of searching steps
This vector can now be used to compute a virtual stimulus
which compensates the mapping errors of the hash-coding
mechanism
The best matching cell is found for
ain
ns
and can be adressed by the virtual stimulus when using
the hash coding mechanism This search mechanism ensures
that the best matching cell is found even if self organization is in effect
During training the template
cells are updated by
vectors of
the
association
lateral distance of neurons in the network
where denotes the value of the teaplate vector at time
and denotes the stimulus is a monotonic decreasing function of time and the lateral distance between
neurons in the network
SIMULATION RESULTS
Figure and show some simulation results of the presented
algorithm for the dase of a two dimensional stimulus vector
Hormel
Figure shows the expected positions in input space of the
untrained template vectors denotes untrained association
cells
Figure Untrained etwork
Figure shows the network after training steps with
stimuli of gaussian distribution in input space The position of the template vectors of trained cells has shifted
into the direction of the better trained areas so that more
association cells are used to represent this area than before Therefore the stored information will be more exact in
this area
Figure Network after training steps
A Self-organizing Associative Memory System for Control Applications
CONCLUSION
The ney algorithm presented above introduces the capability
to adapt the storage mechanisms of a CMAC-type associative
memory according to the arriving stimuli This will result
in various degrees of generalization depending on the number
of trained points in a given area It therefore will make it
unnecessary to choose a generalization factor as a compromise between several constraints when representing nonlinear
functions by storing them in this type of associative memory Some results on tests will be presented together with a
comparison on respective results for the original AMS.
Acknowledgements
This work was sponsored by the German inistry for
and Technology BMFT under grant no ITR
Research

<<----------------------------------------------------------------------------------------------------------------------->>

title: 3705-an-online-algorithm-for-large-scale-image-similarity-learning.pdf

An Online Algorithm for
Large Scale Image Similarity Learning
Gal Chechik
Google
Mountain View CA
gal@google.com
Varun Sharma
Google
Bengalooru Karnataka India
vasharma@google.com
Uri Shalit
ICNC The Hebrew University
Israel
uri.shalit@mail.huji.ac.il
Samy Bengio
Google
Mountain View CA
bengio@google.com
Abstract
Learning a measure of similarity between pairs of objects is a fundamental problem in machine learning It stands in the core of classification methods like kernel
machines and is particularly useful for applications like searching for images
that are similar to a given image or finding videos that are relevant to a given
video In these tasks users look for objects that are not only visually similar but
also semantically related to a given object Unfortunately current approaches for
learning similarity do not scale to large datasets especially when imposing metric
constraints on the learned similarity We describe OASIS a method for learning
pairwise similarity that is fast and scales linearly with the number of objects and
the number of non-zero features Scalability is achieved through online learning
of a bilinear model over sparse representations using a large margin criterion and
an efficient hinge loss cost OASIS is accurate at a wide range of scales on a standard benchmark with thousands of images it is more precise than state-of-the-art
methods and faster by orders of magnitude On million images collected
from the web OASIS can be trained within days on a single CPU. The nonmetric similarities learned by OASIS can be transformed into metric similarities
achieving higher precisions than similarities that are learned as metrics in the first
place This suggests an approach for learning a metric from data that is larger by
orders of magnitude than was handled before
Introduction
Learning a pairwise similarity measure from data is a fundamental task in machine learning Pair
distances underlie classification methods like nearest neighbors and kernel machines and similarity
learning has important applications for query-by-example in information retrieval For instance
a user may wish to find images that are similar to but not identical copies of an image she has
a user watching an online video may wish to find additional videos about the same subject In all
these cases we are interested in finding a semantically-related sample based on the visual content
of an image in an enormous search space Learning a relatedness function from examples could be
a useful tool for such tasks
A large number of previous studies of learning similarities have focused on metric learning like
in the case of a positive semidefinite matrix that defines a Mahalanobis distance However
similarity learning algorithms are often evaluated in a context of ranking When the amount
of training data available is very small adding positivity constraints for enforcing metric properties
is useful for reducing overfitting and improving generalization However when sufficient data is
available as in many modern applications adding positive semi-definitiveness constraints is very
costly and its benefit in terms of generalization may be limited With this view we take here an
approach that avoids imposing positivity or symmetry constraints on the learned similarity measure
Some similarity learning algorithms assume that the available training data contains real-valued pairwise similarities or distances Here we focus on a weaker supervision signal the relative similarity
of different pairs This signal is also easier to obtain here we extract similarity information from
pairs of images that share a common label or are retrieved in response to a common text query in an
image search engine
The current paper presents an approach for learning semantic similarity that scales up to two orders
of magnitude larger than current published approaches Three components are combined to make
this approach fast and scalable First our approach uses an unconstrained bilinear similarity Given
two images p1 and p2 we measure similarity through a bilinear form p1 Wp2 where the matrix
is not required to be positive or even symmetric Second we use a sparse representation of
the images which allows to compute similarities very fast Finally the training algorithm that
we developed OASIS Online Algorithm for Scalable Image Similarity learning is an online dual
approach based on the passive-aggressive algorithm It minimizes a large margin target function
based on the hinge loss and converges to high quality similarity measures after being presented with
a small fraction of the training pairs
We find that OASIS is both fast and accurate at a wide range of scales for a standard benchmark with
thousands of images it achieves better or comparable results than existing state-of-the-art methods
with computation times that are shorter by an order of magnitude For web-scale datasets OASIS
can be trained on more than two million images within three days on a single CPU. On this large
scale dataset human evaluations of OASIS learned similarity show that of the ten nearest
neighbors of a given image are semantically relevant to that image
Learning Relative Similarity
We consider the problem of learning a pairwise similarity function given supervision on the relative similarity between two pairs of images The algorithm is designed to scale well with the number
of samples and the number of features by using fast online updates and a sparse representation
Formally we are given a set of images where each image is represented as a vector Rd We
assume that we have access to an oracle that given a query image pi can locate two other
images
and pi such that pi is more relevant to pi than pi P. Formally
we could write that relevance(pi pi relevance(pi pi However unlike methods that assume
that a numerical value of the similarity is available relevance(pi pj we use this weaker
form of supervision and only assume that some pairs of images can be ranked by their relevance
to a query image pi The relevance measure could reflect that the relevant image
belongs to the
same class of images as the query image or reflect any other semantic property of the images
Our goal is to learn a similarity function SW pi pj parameterized by that assigns higher similarity scores to the pairs of more relevant images with a safety margin
S(pi
S(pi pi
pi
pi
In this paper we consider a parametric similarity function that has a bi-linear form
SW pi pj pTi pj
with Rd?d Importantly if the image vectors pi Rd are sparse namely the number of
non-zero entries ki kpi k0 is small ki then the value of the score defined in can be
computed very efficiently even when is large Specifically SW can be computed with complexity
of O(ki kj regardless of the dimensionality To learn a scoring function that obeys the constraints
in we define a global
hinge losses over all possible triplets in
loss LW that accumulates
the training set LW
with the loss for a single triplet being
pi pi pi
lW pi pi pi max SW pi pi SW pi pi
To minimize the global loss LW we propose an algorithm that is based on the Passive-Aggressive
family of algorithms First is initialized to the identity matrix W0 Id?d Then the
algorithm iteratively draws a random triplet pi
pi and solves the following convex problem
with a soft margin
Wi argmin
kW k2F ro
lW pi
pi
and
where k?kF ro is the Frobenius norm point-wise norm At the ith iteration Wi is updated to
optimize a trade-off between staying close to the previous parameters and minimizing the
loss on the current triplet lW pi
pi The aggressiveness parameter controls this trade-off
To solve the problem in we follow the derivation in When lW pi
pi it is clear
that Wi satisfies directly Otherwise we define the Lagrangian
kW k2F ro pTi
pi
where and are the Lagrange multipliers The optimal solution is obtained when the
gradient vanishes
Vi where Vi is the gradient matrix at the
lW
current step Vi
pi pi pi pi When image vectors are sparse the
gradient Vi is also sparse hence the update step costs only O(|pi
k0 kpi k0 where the
L0 norm kxk0 is the number of nonzero values in Differentiating the Lagrangian with respect to
which knowing that means that C. Plugging
we obtain
back into the Lagrangian in we obtain kVi pTi
pi
Finally taking the derivative of this second Lagrangian with respect to and using we obtain
Vi
lWi?1 pi
pi
min
kVi
The optimal update for the new therefore has a form of a gradient descent step with a step size
that can be computed exactly Applying this algorithm for classification tasks was shown to yield a
small cumulative online loss and selecting the best Wi during training using a hold-out validation
set was shown to achieve good generalization
It should be emphasized that OASIS is not guaranteed to learn a parameter matrix that is positive
or even symmetric We study variants of OASIS that enforce symmetry or positivity in Sec.
Related Work
Learning similarity using relative relevance has been intensively studied and a few recent approaches aim to address learning at large scale For small-scale data there are two main groups of
similarity learning approaches The first approach learning Mahalanobis distances can be viewed
as learning a linear projection of the data into another space often of lower dimensionality where a
Euclidean distance is defined among pairs of objects Such approaches include Fisher?s Linear Discriminant Analysis relevant component analysis RCA supervised global metric learning large margin nearest neighbor LMNN and metric learning by collapsing classes
MLCC Other constraints like sparseness are sometimes induced over the learned metric See
also a review in for more details
The second family of approaches learning kernels is used to improve performance of kernel based
classifiers Learning a full kernel matrix in a non parametric way is prohibitive except for very
small data sets As an alternative several studies suggested learning a weighted sum of pre-defined
kernels where the weights are learned from data In some applications this was shown to be
inferior to uniform weighting of the kernels The work in further learns a weighting over
local distance functions for every image in the training set Non linear image similarity learning was
also studied in the context of dimensionality reduction as in
Finally Jain al based on Davis al aim to learn metrics in an online setting This work
is one of the closest work with respect to OASIS it learns online a linear model of a dis-]similarity
Query image
Top relevant images retrieved by OASIS
Table OASIS Successful cases from the web dataset The relevant text queries for each image
are shown beneath the image not used in training
function between documents images the main difference is that Jain al try to learn a true
distance imposing positive definiteness constraints which makes the algorithm more complex and
more constrained We argue in this paper that in the large scale regime imposing these constraints
throughout could be detrimental
Learning a semantic similarity function between images was also studied in There semantic
similarity is learned by representing each image by the posterior probability distribution over a
predefined set of semantic tags and then computing the distance between two images as the distance
between the two underlying posterior distributions The representation size of each image therefore
grows with the number of semantic classes
Experiments
We tested OASIS on two datasets spanning a wide regime of scales First we tested its scalability on
million images collected from the web Then to quantitatively compare the precision of OASIS
with other small-scale metric-learning methods we tested OASIS using Caltech-256 a standard
machine vision benchmark
Image representation We use a sparse representation based on bags of visual words These
features were systematically tested and found to outperform other features in related tasks but the
details of the visual representation is outside the focus of this paper Broadly speaking features are
extracted by dividing each image into overlapping square blocks representing each block by edge
and color histograms and finding the nearest block in a predefined set dictionary of
vectors of such features An image is thus represented as the number of times each dictionary visual
word was present in it yielding vectors in Rd with an average of 70 non-zero values
Evaluation protocol We evaluated the performance of all algorithms using precision-at-top-k a
standard ranking precision measure based on nearest neighbors For each query image in the test set
all other test images were ranked according to their similarity to the query image and the number of
same-class images among the top images the nearest neighbors is computed and then averaged
across test images We also calculated the mean average precision a measure that is widely
used in the information retrieval community
Web-Scale Experiment
We first tested OASIS on a set of million images scraped from the Google image search engine
We collected a set of anonymized text queries and for each of these queries we had access
to a set of relevant images To compute an image-image relevance measure we first obtained measures of relevance between images and text queries This was achieved by collecting anonymized
clicks over images collected from the set of text queries We used this query-image click counts
C(query,image to compute the unnormalized probability that two images are co-queried as Relevance(image,image C. The relevance matrix was then thresholded to keep only the top
percent values We trained OASIS on a training set of million images and tested performance on
million images The number of training iterations each corresponding to sampling one triplet
was selected using a second validation set of around images over which the performance
saturated after million iterations Overall training took a total of minutes on a single
CPU of a standard modern machine
Table shows the top five images as ranked by OASIS on two examples of query-images in the test
set In these examples OASIS captures similarity that goes beyond visual appearance most top
ranked images are about the same concept as the query image even though that concept was never
provided in a textual form and is inferred in the viewers mind This shows that
learning similarity across co-queried images can indeed capture the semantics of queries even if the
queries are not explicitly used during training
To obtain a quantitative evaluation of the ranking obtained by OASIS we created an evaluation
benchmark by asking human evaluators to mark if a set of candidate images were semantically
relevant to a set of popular image queries For each query image evaluators were presented with
the images ranked by OASIS mixed with random images Given the relevance ranking
from evaluators we computed the precision of each OASIS rank as the fraction of people that
marked each image as relevant to the query image On average across all queries and evaluators
OASIS rankings yielded precision of at the top ranked images
As an estimate of an upper bound on the difficulty of the task we also computed the precision
obtained by human evaluators For every evaluator we used the rankings of all other evaluators
as ground truth to compute his precision As with the ranks of OASIS we computed the fraction
of evaluators that marked an image as relevant and repeated this separately for every query and
human evaluator providing a measure of coherence per query shows the mean precision
obtained by OASIS and human evaluators for every query in our data For some queries OASIS
achieves precision that is very close to that of the mean human evaluator In many cases OASIS
achieves precision that is as good or better than some evaluators
Human precision
OASIS precision
fast LMNN MNIST categories days
nd
projected extrapolation poly
OASIS Web data
runtime min
precision
2days
hrs
5min
days
hrs
3hrs
min
37sec
9sec
query ID sorted by precision
2M
number of images log scale
Figure Precision of OASIS and human evaluators per query using rankings of all remaining
human evaluators as a ground truth Comparison of the runtime of OASIS and fast-LMNN[17
over a wide range of scales LMNN results on MNIST data are faster than OASIS results on
subsets of the web data However LMNN scales quadratically with the number of samples hence is
three times slower on images and may be infeasible for handling million images
We further studied how the runtime of OASIS scales with the size of the training set Figure
shows that the runtime of OASIS as found by early stopping on a separate validation set grows
linearly with the train set size We compare this to the fastest result we found in the literature based
on a fast implementation of LMNN The LMNN algorithm scales quadratically with the number
of objects although their experiments with MNIST data show that the active set of constraints grows
linearly This could be because MNIST has classes only
classes
classes
classes
precision
precision
precision
OASIS
MCML
LEGO
LMNN
Euclidean
Random
number of neighbors
OASIS
MCML
LEGO
LMNN
Euclidean
Random
number of neighbors
OASIS
LEGO
LMNN
Euclidean
Random
number of neighbours
Figure Comparison of the performance of OASIS LMNN MCML LEGO and the Euclidean
metric in feature space Each curve shows the precision at top as a function of neighbors The
results are averaged across train/test partitions training images test images per class error
bars are standard error of the means black dashed line denotes chance performance
Caltech256 Dataset
To compare OASIS with small-scale methods we used the Caltech256 dataset containing images collected from Google image search and from PicSearch.com Images were assigned to
categories and evaluated by humans in order to ensure image quality and relevance After we have
pre-processed the images and filtered images that were too small we were left with images
in categories To allow comparisons with methods that were not optimized for sparse representation we also reduced the block vocabulary size from to
We compared OASIS with the following metric learning methods
Euclidean The standard Euclidean distance in feature space equivalent to using the identity
matrix Id?d MCML Learning a Mahalanobis distance such that same-class samples are mapped to the same point formulated as a convex problem LMNN learning a
Mahalanobis distance for aiming to have the k-nearest neighbors of a given sample belong to the
same class while separating different-class samples by a large margin As a preprocessing phase
images were projected to a basis of the principal components PCA of the data with no dimensionality reduction LEGO Online learning of a Mahalanobis distance using a Log-Det
regularization per instance loss that is guaranteed to yield a positive semidefinite matrix We used a
variant of LEGO that like OASIS learns from relative distances.1
We tested all methods on subsets of classes taken from the Caltech256 repository For OASIS
images from the same class were treated as similar Each subset was built such that it included
semantically diverse categories controlled for classification difficulty We tested sets containing
and classes each spanning the range of difficulties
We used two levels of 5-fold cross validation one to train the model and a second to select
hyper parameters of each method early stopping time for OASIS the parameter for LMNN
and the regularization parameter for LEGO
Results reported below were obtained by selecting the best value of the hyper parameter and then
training again on the full training set images per class
Figure compares the precision obtained with OASIS with the four competing approaches OASIS
achieved consistently superior results throughout the full range of number of neighbors tested
and on all four sets studied LMNN performance on the training set was often high suggesting that
it overfits the training set as was also observed sometimes by
Table shows the total CPU time in minutes for training all algorithms compared and for four
subsets of classes at sizes and Data is not given when runtime was longer than
days or performance was worse than the Euclidean baseline For the purpose of a fair comparison
we tested two implementations of OASIS The first was fully implemented Matlab The second had
the core loop of the algorithm implemented in and called from Matlab All other methods used
We have also experimented with the methods of which we found to be too slow and with RCA
whose precision was lower than other methods These results are not included in the evaluations below
Table Runtime minutes on a standard CPU of all compared methods
num
classes
OASIS
Matlab
42
45
OASIS
Matlab+C
03
02
04
MCML
Matlab+C
LEGO
Matlab
44
49
28
LMNN
Matlab+C
fastLMNN
Matlab+C
62
67
code supplied by the authors implemented in Matlab with core parts implemented in C. Due to
compatibility issues fast-LMNN was run on a different machine and the given times are rescaled to
the same time scale as all other algorithms LEGO is fully implemented in Matlab All other code
was compiled mex to C. The implementation of OASIS is significantly faster since Matlab does
not use the potential speedup gained by sparse images
OASIS is significantly faster with a runtime that is shorter by orders of magnitudes than MCML
even on small sets and about one order of magnitude faster than LMNN The run time of OASIS
and LEGO was measured until the point of early stopping OASIS memory requirements grow
quadratically with the size of the dictionary For a large dictionary of the parameters matrix
takes floats or Giga bytes of memory
mean average precision
precision
OASIS
PROJ OASIS
ONLINE?PROJ OASIS
DISSIM?OASIS
Euclidean
Random
number of neighbors
proj every
proj every
proj after complete
learning steps
Figure Comparing symmetric variants of OASIS on the 20-class subset similar results obtained with other sets mAP along training for three PSD projection schemes
Symmetry and positivity
The similarity matrix learned by OASIS is not guaranteed to be positive or even symmetric
Some applications like ranking images by semantic relevance to a given image query are known to
be non-symmetric when based on human judgement However in some applications symmetry
or positivity constraints reflects a prior knowledge that may help in avoiding overfitting We now
discuss variants of OASIS that learn a symmetric or positive matrices
Symmetric similarities
A simple approach to enforce symmetry is to project the OASIS model onto the set of symmetric
matrices sym(W WT Projection can be done after each update denoted
Online-Proj-Oasis or after learning is completed Proj-Oasis Alternatively the asymmetric score
function SW pi pj in lW can be replaced with a symmetric score
SW
pi pj pj pi pj
and used to derive an OASIS-like algorithm which we name Dissim-Oasis The optimal update for
this loss has a symmetric gradient V?i pi
pi pi pi pi Therefore
if is initialized with a symmetric matrix the identity all are guaranteed to remain
symmetric Dissim-Oasis is closely related to LMNN This can be seen be casting the batch
objective of LMNN into an online setup which has the form err(W SW
pi
lW pi pi pi This online version of LMNN becomes equivalent to Dissim-Oasis for
Figure compares the precision of the different symmetric variants with the original OASIS All symmetric variants performed slightly worse or equal to the original asymmetric OASIS The precision of Proj-Oasis was equivalent to that of OASIS most likely since asymmetric OASIS actually converged to an almost-symmetric model as measured by a symmetry index
ksym(W)k
kWk2
Positive similarity
Most similarity learning approaches focus on learning metrics In the context of OASIS when is
positive semi definite it defines a Mahalanobis distance over the images The matrix squareroot of AT A can then be used to project the data into a new space in which the Euclidean
distance is equivalent to the distance in the original space
We experimented with positive variants of OASIS where we repeatedly projected the learned model
onto the set of PSD matrices once every iterations Projection is done by taking the eigen decomposition VT where is the eigenvector matrix and is a the diagonal eigenvalues
matrix limited to positive eigenvalues Figure traces precision on the test set throughout learning
for various values of
The effect of positive projections is complex First continuously projecting at every step helps
to reduce overfitting as can be observed by the slower decline of the blue curve upper smooth
curve compared to the orange curve lowest curve However when projection is performed after
many steps instead of continuously performance of the projected model actually outperforms
the continuous-projection model upper jittery curve The reason for this effect is likely to be
that estimating the positive sub-space is very noisy when only based on a few samples Indeed
accurate estimation of the negative subspace is known to be a hard problem in that the estimated
eigenvalues of eigenvectors near zero is relatively large We found that this effect was so strong
that the optimal projection strategy is to avoid projection throughout learning completely Instead
projecting into PSD after learning namely after a model was chosen using early stopping provided
the best performance in our experiments
An interesting alternative to obtain a PSD matrix was explored by Using a LogDet divergence between two matrices Dld tr(XY log(det(XY ensures that given an
initial PSD matrix all subsequent matrices will be PSD as well It will be interesting to test the
effect of using LogDet regularization in the OASIS setup
Discussion
We have presented OASIS a scalable algorithm for learning image similarity that captures both
semantic and visual aspects of image similarity Three key factors contribute to the scalability of
OASIS First using a large margin online approach allows training to converge even after seeing
a small fraction of potential pairs Second the objective function of OASIS does not require the
similarity measure to be a metric during training although it appears to converge to a near-symmetric
solution whose positive projection is a good metric Finally we use a sparse representation of low
level features which allows to compute scores very efficiently
OASIS learns a class-independent model it is not aware of which queries or categories were shared
by two similar images As such it is more limited in its descriptive power and it is likely that classdependent similarity models could improve precision On the other hand class-independent models
could generalize to handle classes that were not observed during training as in transfer learning
Large scale similarity learning applied to images from a large variety of classes could therefore be
a useful tool to address real-world problems with a large number of classes
This paper focused on the training part of metric learning To use the learned metric for ranking an
efficient procedure for scoring a large set of images is needed Techniques based on locality-sensitive
hashing could be used to speed up evaluation but this is outside the scope of this paper

<<----------------------------------------------------------------------------------------------------------------------->>

title: 2566-neighbourhood-components-analysis.pdf

Neighbourhood Components Analysis
Jacob Goldberger Sam Roweis Geoff Hinton Ruslan Salakhutdinov
Department of Computer Science University of Toronto
jacob,roweis,hinton,rsalakhu}@cs.toronto.edu
Abstract
In this paper we propose a novel method for learning a Mahalanobis
distance measure to be used in the KNN classification algorithm The
algorithm directly maximizes a stochastic variant of the leave-one-out
KNN score on the training set It can also learn a low-dimensional linear embedding of labeled data that can be used for data visualization
and fast classification Unlike other methods our classification model
is non-parametric making no assumptions about the shape of the class
distributions or the boundaries between them The performance of the
method is demonstrated on several data sets both for metric learning and
linear dimensionality reduction
Introduction
Nearest neighbor KNN is an extremely simple yet surprisingly effective method for classification Its appeal stems from the fact that its decision surfaces are nonlinear there
is only a single integer parameter which is easily tuned with cross-validation and the
expected quality of predictions improves automatically as the amount of training data increases These advantages shared by many non-parametric methods reflect the fact that
although the final classification machine has quite high capacity since it accesses the entire
reservoir of training data at test time the trivial learning procedure rarely causes overfitting
itself
However KNN suffers from two very serious drawbacks The first is computational since
it must store and search through the entire training set in order to classify a single test point
Storage can potentially be reduced by editing or thinning the training data and in low
dimensional input spaces the search problem can be mitigated by employing data structures
such as KD-trees or ball-trees[4 The second is a modeling issue how should the distance
metric used to define the nearest neighbours of a test point be defined In this paper we
attack both of these difficulties by learning a quadratic distance metric which optimizes the
expected leave-one-out classification error on the training data when used with a stochastic
neighbour selection rule Furthermore we can force the learned distance metric to be low
rank thus substantially reducing storage and search costs at test time
Stochastic Nearest Neighbours for Distance Metric Learning
We begin with a labeled data set consisting of real-valued input vectors in RD
and corresponding class labels c1 cn We want to find a distance metric that maximizes
the performance of nearest neighbour classification Ideally we would like to optimize
performance on future test data but since we do not know the true data distribution we
instead attempt to optimize leave-one-out LOO performance on the training data
In what follows we restrict ourselves to learning Mahalanobis quadratic distance metrics
which can always be represented by symmetric positive semi-definite matrices We estimate such metrics through their inverse square roots by learning a linear transformation
of the input space such that in the transformed space KNN performs well If we denote
the transformation by a matrix A we are effectively learning a metric A A such that
Q(x Ax Ax Ay).
The actual leave-one-out classification error of KNN is quite a discontinuous function of the
transformation A since an infinitesimal change in A may change the neighbour graph and
thus affect LOO classification performance by a finite amount Instead we adopt a more
well behaved measure of nearest neighbour performance by introducing a differentiable
cost function based on stochastic neighbour assignments in the transformed space
In particular each point selects another point as its neighbour with some probability pij
and inherits its class label from the point it selects We define the pij using a softmax over
Euclidean distances in the transformed space
exp(?kAxi Axj
exp(?kAxi Axk
pij
pii
Under this stochastic selection rule we can compute the probability pi that point will be
correctly classified denote the set of points in the same class as by Ci j|ci cj
pij
pi
j?Ci
The objective we maximize is the expected number of points correctly classified under this
scheme
XX
pi
pij
j?Ci
Differentiating with respect to the transformation matrix A yields a gradient rule which
we can use for learning denote xij
XX
pij xij
pik xik
ij
ik
A
j?Ci
Reordering the terms we obtain a more efficiently computed expression
pi
2A
pik xik
pij xij
ik
ij
A
j?Ci
Our algorithm which we dub Neighbourhood Components Analysis is extremely
simple maximize the above objective using a gradient based optimizer such as deltabar-delta or conjugate gradients Of course since the cost function above is not convex
some care must be taken to avoid local maxima during training However unlike many
other objective functions where good optima are not necessarily deep but rather broad it
has been our experience that the larger we can drive during training the better our test
performance will be In other words we have never observed an overtraining effect
Notice that by learning the overall scale of A as well as the relative directions of its rows
we are also effectively learning a real-valued estimate of the optimal number of neighbours
This estimate appears as the effective perplexity of the distributions pij If the learning
procedure wants to reduce the effective perplexity consult fewer neighbours it can scale
up A uniformly similarly by scaling down all the entries in A it can increase the perplexity
of and effectively average over more neighbours during the stochastic selection
Maximizing the objective function is equivalent to minimizing the L1 norm between
the true class distribution having probability one on the true class and the stochastic class
distribution induced by pij via A. A natural alternative distance is the KL-divergence which
induces the following objective function
log
pij
log(pi
j?Ci
Maximizing this objective would correspond to maximizing the probability of obtaining a
perfect error free classification of the entire training set The gradient of is even
simpler than that of
j?Ci pij xij xij
2A
pik xik xik
A
j?Ci pij
We have experimented with optimizing this cost function as well and found both the transformations learned and the performance results on training and testing data to be very
similar to those obtained with the original cost function
To speed up the gradient computation the sums that appear in equations and over
the data points and over the neigbours of each point can be truncated one because we
can do stochastic gradient rather than exact gradient and the other because pij drops off
quickly
Low Rank Distance Metrics and Nonsquare Projection
Often it is useful to reduce the dimensionality of input data either for computational savings or for regularization of a subsequent learning algorithm Linear dimensionality reduction techniques which apply a linear operator to the original data in order to arrive
at the reduced representation are popular because they are both fast and themselves relatively immune to overfitting Because they implement only affine maps linear projections
also preserve some essential topology of the original data Many approaches exist for linear dimensionality reduction ranging from purely unsupervised approaches such as factor
analysis principal components analysis and independent components analysis to methods
which make use of class labels in addition to input features such as linear discriminant
analysis possibly combined with relevant components analysis
By restricting A to be a nonsquare matrix of size NCA can also do linear dimensionality reduction In this case the learned metric will be low rank and the transformed inputs
will lie in Rd Since the transformation is linear without loss of generality we only consider the case D. By making such a restriction we can potentially reap many further
benefits beyond the already convenient method for learning a KNN distance metric In particular by choosing we can vastly reduce the storage and search-time requirements
of KNN. Selecting or we can also compute useful low dimensional visualizations on labeled datasets using only a linear projection The algorithm is exactly the
same optimize the cost function using gradient descent on a nonsquare A. Our method
requires no matrix inversions and assumes no parametric model Gaussian or otherwise
for the class distributions or the boundaries between them For now the dimensionality of
the reduced representation the number of rows in A must be set by the user
By using an highly rectangular A so that we can significantly reduce the computational load of KNN at the expense of restricting the allowable metrics to be those of
rank at most To achieve this we apply the NCA learning algorithm to find the optimal
transformation A and then we store only the projections of the training points yn Axn
as well as their labels At test time we classify a new point xtest by first computing its
projection ytest Axtest and then doing KNN classification on ytest using the yn and
a simple Euclidean metric If is relatively small say less than we can preprocess
the yn by building a KD-tree or a ball-tree to further increase the speed of search at test
time The storage requirements of this method are O(dN Dd compared with O(DN
for KNN in the original input space
Experiments in Metric Learning and Dimensionality Reduction
We have evaluated the NCA algorithm against standard distance metrics for KNN and other
methods for linear dimensionality reduction In our experiments we have used data sets
from the UC Irvine repository We compared the NCA transformation obtained from
optimizing for square A on the training set with the default Euclidean distance A I
the whitening transformation A where is the sample data covariance matrix
and the RCA transformation A where is the average of the within-class
covariance matrices We also investigated the behaviour of NCA when A is restricted to
be diagonal allowing only axis aligned Mahalanobis measures
Figure shows that the training and more importantly testing performance of NCA is
consistently the same as or better than that of other Mahalanobis distance measures for
KNN despite the relative simplicity of the NCA objective function and the fact that the
distance metric being learned is nothing more than a positive definite matrix A>A.
We have also investigated the use of linear dimensionality reduction using NCA with nonsquare A for visualization as well as reduced-complexity classification on several datasets
In figure we show examples of visualization First we generated a synthetic threedimensional dataset shown in top row of figure which consists of classes shown by
different colors In two dimensions the classes are distributed in concentric circles while
the third dimension is just Gaussian noise uncorrelated with the other dimensions or the
class label If the noise variance is large enough the projection found by PCA is forced
to include the noise as shown on the top left of figure A full rank Euclidean metric
would also be misled by this dimension The classes are not convex and cannot be linearly separated hence the results obtained from LDA will be inappropriate as shown in
figure In contrast NCA adaptively finds the best projection without assuming any parametric structure in the low dimensional representation We have also applied NCA to the
UCI wine dataset which consists of points labeled into classes and to a database
of gray-scale images of faces consisting of 18 classes each a separate individual and
dimensions image size is The face dataset consists of images for each
person Finally we applied our algorithm to a subset of the USPS dataset of handwritten
digit images consisting of the first five digit classes through The grayscale
images were downsampled to pixel resolution resulting in 64 dimensions
As can be seen in figure when a two-dimensional projection is used the classes are consistently much better separated by the NCA transformation than by either PCA which is
unsupervised or LDA which has access to the class labels Of course the NCA transformation is still only a linear projection just optimized with a cost function which explicitly
encourages local separation To further quantify the projection results we can apply a
nearest-neighbor classification in the projected space Using the same projection learned
at training time we project the training set and all future test points and perform KNN in
the low-dimensional space using the Euclidean measure The results under the PCA LDA
LDA followed by RCA and NCA transformations using appear in figure The
NCA projection consistently gives superior performance in this highly constrained low
distance metric learning training
distance metric learning testing
NCA
diag?NCA
RCA
whitened
Euclidean
bal
ion
iris
wine
NCA
diag?NCA
RCA
whitened
Euclidean
hous
digit
bal
rank transformation training
iris
wine
hous
digit
rank transformation testing
NCA
LDA+RCA
LDA
PCA
bal
ion
iris
wine
hous
digit
NCA
LDA+RCA
LDA
PCA
ion
bal
ion
iris
wine
hous
digit
Figure KNN classification accuracy left train right test on UCI datasets balance ionosphere iris wine and housing and on the USPS handwritten digits Results are averages
over realizations of splitting each dataset into training and testing subsets
for USPS images for each of the digit classes were used for training and for
testing Top panels show distance metric learning square A and bottom panels show
linear dimensionality reduction down to
rank KNN setting In summary we have found that when labeled data is available NCA
performs better both in terms of classification performance in the projected representation
and in terms of visualization of class separation as compared to the standard methods of
PCA and LDA.
Extensions to Continuous Labels and Semi-Supervised Learning
Although we have focused here on discrete classes linear transformations and fully supervised learning many extensions of this basic idea are possible Clearly a nonlinear
transformation function could be learned using any architecture such as a multilayer
perceptron trainable by gradient methods Furthermore it is possible to extend the classification framework presented above to the case of a real valued continuous supervision
signal by defining the set of correct matches Ci for point to be those points having
similar continuous targets This naturally leads to the idea of soft matches in which
the objective function becomes a sum over all pairs each weighted by their agreement according to the targets Learning under such an objective can still proceed even in settings
where the targets are not explicitly provided as long as information identifying close pairs
PCA
LDA
NCA
Figure Dataset visualization results of PCA LDA and NCA applied to from top the
concentric rings wine faces and digits datasets The data are reduced from their
original dimensionalities respectively to the dimensions
show
Figure The two dimensional outputs of the neural network on a set of test cases On the left each
point is shown using a line segment that has the same orientation as the input face On the right the
same points are shown again with the size of the circle representing the size of the face
is available Such semi-supervised tasks often arise in domains with strong spatial or temporal continuity constraints on the supervision in a video of a person?s face we may
assume that pose and expression vary slowly in time even if no individual frames are ever
labeled explicitly with numerical pose or expression values
To illustrate this we generate pairs of faces in the following way First we choose two faces
at random from the FERET-B dataset isolated faces that have a standard orientation
and scale The first face is rotated by an angle uniformly distributed between and
scaled to have a height uniformly distributed between and 35 pixels The second face
which is of a different person is given the same rotation and scaling but with Gaussian
noise of and pixels The pair is given a weight wab which is the probability
density of the added noise divided by its maximum possible value We then trained a neural
network with one hidden layer of logistic units to map from the pixel intensities
of a face to a point in a output space Backpropagation was used to minimize the
cost function in which encourages the faces in a pair to be placed close together
exp(?||ya yb
Cost
wab log
c,d exp(?||yc yd
pair(a,b
where and are indices over all of the faces not just the ones
that form a pair Four example faces are shown to the right horizontally the pairs agree and vertically they do not Figure above
shows that the feedforward neural network discovered polar coordinates without the user having to decide how to represent scale
and orientation in the output space
Relationships to Other Methods and Conclusions
Several papers recently addressed the problem of learning Mahalanobis distance functions
given labeled data or at least side-information of the form of equivalence constraints Two
related methods are RCA and a convex optimization based algorithm RCA is
implicitly assuming a Gaussian distribution for each class so it can be described using
only the first two moments of the class-conditional distribution Xing al attempt to
find a transformation which minimizes all pairwise squared distances between points in the
same class this implicitly assumes that classes form a single compact connected set For
highly multimodal class distributions this cost function will be severely penalized Lowe[6
proposed a method similar to ours but used a more limited idea for learning a nearest
neighbour distance metric In his approach the metric is constrained to be diagonal as
well it is somewhat redundantly parameterized and the objective function corresponds to
the average squared error between the true class distribution and the predicted distribution
which is not entirely appropriate in a more probabilistic setting
In parallel there has been work on learning low rank transformations for fast classification
and visualization The classic LDA algorithm[3 is optimal if all class distributions are
Gaussian with a single shared covariance this assumption however is rarely true LDA
also suffers from a small sample size problem when dealing with high-dimensional data
when the within-class scatter matrix is nearly singular[2 Recent variants of LDA
make the transformation more robust to outliers and to numerical instability when
not enough datapoints are available This problem does not exist in our method since there
is no need for a matrix inversion
In general there are two classes of regularization assumption that are common in linear
methods for classification The first is a strong parametric assumption about the structure of
the class distributions typically enforcing connected or even convex structure the second
is an assumption about the decision boundary typically enforcing a hyperplane Our
method makes neither of these assumptions relying instead on the strong regularization
imposed by restricting ourselves to a linear transformation of the original inputs
Future research on the NCA model will investigate using local estimates of as derived
from the entropy of the distributions pij the possible use of a stochastic classification rule
at test time and more systematic comparisons between the objective functions and
To conclude we have introduced a novel non-parametric learning method NCA that
handles the tasks of distance learning and dimensionality reduction in a unified manner
Although much recent effort has focused on non-linear methods we feel that linear embedding has still not fully fulfilled its potential for either visualization or learning
Acknowledgments
Thanks to David Heckerman and Paul Viola for suggesting that we investigate the alternative cost and the case of diagonal A.

<<----------------------------------------------------------------------------------------------------------------------->>

title: 1357-the-canonical-distortion-measure-in-feature-space-and-1-nn-classification.pdf

The Canonical Distortion Measure in Feature
Space and I-NN Classification
Jonathan Baxter*and Peter Bartlett
Department of Systems Engineering
Australian National University
Canberra Australia
jon,bartlett}@syseng.anu.edu.au
Abstract
We prove that the Canonical Distortion Measure CDM is the
optimal distance measure to use for I nearest-neighbour classification and show that it reduces to squared Euclidean distance in feature
space for function classes that can be expressed as linear combinations
of a fixed set of features PAC-like bounds are given on the samplecomplexity required to learn the CDM. An experiment is presented in
which a neural network CDM was learnt for a Japanese OCR environment and then used to do I-NN classification
INTRODUCTION
Let be an input space a distribution on a class of functions mapping into
called the environment a distribution on and a function
The Canonical Distortion Measure CDM between two inputs Xl is defined to be
Xl
f(x
Throughout this paper we will be considering real-valued functions and squared loss so
and yl The CDM was introduced in where it was
analysed primarily from a vector quantization perspective In particular the CDM was
proved to be the optimal distortion measure to use in vector quantization in the sense of
producing the best approximations to the functions in the environment F. In some
experimental results were also presented a toy domain showing how the CDM may be
learnt
The purpose of this paper is to investigate the utility of the CDM as a classification tool
In Section we show how the CDM for a class of functions possessing a common feature
The first author was supported in part by EPSRC grants and
Baxter and P. Bartlett
set reduces via a change of variables to squared Euclidean distance in feature space A
lemma is then given showing that the CDM is the optimal distance measure to use for 1nearest-neighbour classification Thus for functions possessing a common feature
set optimall-NN classification is achieved by using squared Euclidean distance in feature
space
In general the CDM will be unknown so in Section we present a technique for learning
the CDM by minimizing squared loss and give PAC-like bounds on the sample-size required for good generalisation In Section we present some experimental results in which
a set of features was learnt for a machine-printed Japanese OCR environment and then
squared Euclidean distance was used to do I-NN classification in feature space The experiments provide strong empirical support for the theoretical results in a difficult real-world
application
THE CDM IN FEATURE SPACE
can be expressed as a linear combination of a fixed set of features
That is for all there exists Wk such that
Suppose each
Wi?i In this case the distribution over the environment is a
distribution over the weight vectors Measuring the distance between function values by
the CDM becomes
iE.k
dQ(w
where
fw w'w
is a matrix Making the change of variable
we have Thus the assumption that the functions in
the environment can be expressed as linear combinations of a fixed set of features means
that the CDM is simply squared Euclidean distance in a feature space related to the original
by a linear transformation
I-NN CLASSIFICATION AND THE CDM
Suppose the environment consists of classifiers 1}-valued functions Let be
some function in and f(x a training set of examples of
In I-NN classification the classification of a novel is computed by where
argmin the classification of is the classification of the nearest training
point to under some distance measure If both and are chosen at random the
expected misclassification error of the scheme using and the training points
xn)is
er(x EF Ex
where is the nearest neighbour to from The following lemma is now
immediate from the definitions
Lemma For all sequences
er{x is minimized ifd is the CDM
Remarks Lemma combined with the results of the last section shows that for function
classes possessing a common feature set optimall-NN classification is achieved by using
squared Euclidean distance in feature space In Section some experimental results on
Japanese OCR are presented supporting this conclusion
The property of optimality of the CDM for I-NN classification may not be stable to small
perturbations That is if we learn an approximationg to then even ifE xxx
The Canonical Distortion Measure in Feature Space and I-NN Classification
is small it may not be the case that l-NN classification using is also small
However one can show that stability is maintained for classifier environments in which
positive examples of different functions do not overlap significantly as is the case for the
Japanese OCR environment of Section face recognition environments speech recognition environments and so We are currently investigating the general conditions under
which stability is maintained
LEARNING THE CDM
For most environments encountered in practice speech recognition or image recognition will be unknown In this section it is shown how may be estimated or learnt using
function approximation techniques feedforward neural networks
SAMPLING THE ENVIRONMENT
To learn the CDM the learner is provided with a class of functions neural networks
M]. The goal of the learner is to find a such
that the error between and the CDM is small For the sake of argument this error will
be measured by the expected squared loss
where each maps
erp(g Exxx
where the expectation is with respect to
Ordinarily the learner would be provided with training data in the form
and would use this data to minimize an empirical version of However is unknown
so to generate data of this form must be estimated for each training pair Hence to
generate training sets for learning the CDM both the distribution over the environment
and the distribution over the input space must be sampled So let
be samples from according to and let be samples
from according to P. For any pair Xj an estimate of Xj is given by
P(Xi Xj
J'(fdxd,fk(Xj
k=l
This gives training triples
which can be used as data to generate an empirical estimate of er
Only n(n of the possible training triples are used because the functions
are assumed to already be symmetric and to satisfy for all if this is not
the case then set if and and use
instead
In an experiment was presented in which was a neural network class and was
minimized directly by gradient descent In Section we present an alternative technique
in which a set of features is first learnt for the environment and then an estimate of in
feature space is constructed explicitly
J. Baxter and P. Bartlett
UNIFORM CONVERGENCE
We wish to ensure good generalisation from a minimizing
small
Pr
lerx,f(g
I
erp(g
e~r
in the sense that for
The following theorem shows that this occurs if both the number of functions and the
number of input samples are sufficiently large Some exotic but nonetheless benign
measurability restrictions have been ignored in the statement of the theorem In the statement of the theorem denotes the smallest 6-cover of under the norm
where gl gN is an 6-cover of9 iffor all there exists gi such that Ilgi gil
Theorem Assume the range of the functions in the environment is no more than
and in the class used to approximate the CDM is no more than
VB). For all and if
62
and
log log;5
then
Proof For each define
erx(g
If for any
l~i<j~n
Pr sup ler erx(g I
gE9
and
Pr
sup lerx(g erp(g)1
gE9
then by the triangle inequality will hold We treat and separately
Equation To simplify the notation let gij Pij and Pij denote Xj and
p(Xi Xj respectively Now
ij pij
l~i<j~n
n(n
4B
n(n
Pij Pij ij Pij Pij
Pij Pij
E.rx(J
X(Jk
k=l
gij Pij)2
l~i<j~n
The Canonical Distortion Measure in Feature Space and J-NN Classification
where is defined by
4B
n(n
Thus
Pr
I
Si<J:S
Pr
EJ'x(f
which is exp by Hoeffding's inequality Setting this less than
gives the bound on in theorem
Equation Without loss of generality suppose that is even The trick here is to split
the sum over all pairs Xj with appearing in the definition of er into a
double sum
erx(g
Xj p(Xi
i=l
g(xo
j=l
where for each and are permutations on such that
is empty That there exist permutations with
this property such that the sum can be broken up in this way can be proven easily by induction Now conditional on each the pairs
are an sample from according to So by standard results from real-valued
function learning with squared loss
Pr
su
gEQ
erp(g
J=l
4N exp
Hence by the union bound
Pr
erx(g
erp(g I
l)N
exp
Setting as in the statement of the theorem ensures this is less than
Remark The bound on the number of functions that need to be sampled from the
environment is independent of the complexity of the class This should be contrasted
with related bias learning equivalently learning to learn results in which the number
of functions does depend on the complexity The heuristic explanation for this is that here
we are only learning a distance function on the input space the CDM whereas in bias
learning we are learning an entire hypothesis space that is appropriate for the environment
However we shall see in the next section how for certain classes of problems the CDM can
also be used to learn the functions in the environment Hence in these cases learning the
CDM is a more effective method of learning to learn
EXPERIMENT JAPANESE OCR
To verify the optimality of the CDM for I-NN classification and also to show how
it can be learnt in a non-trivial domain only a toy example was given in the
Baxter and Bartlett
COM was learnt for a Japanese OCR environment Specifically there were functions I in the environment each one a classifier for a different Kanji character A
database containing segmented machine-printed Kanji characters scanned from
various sources was purchased from the CEDAR group at the State University of New
York Buffalo The quality of the images ranged from clean to very degraded
http://www cedar buffalo edu/Databases/JOcR
The main reason for choosing Japanese OCR rather than English OCR as a test-bed was
the large number of distinct characters in Japanese Recall from Theorem that to get good
generalisation from a learnt COM sufficiently many functions must be sampled from the
environment If the environment just consisted of English characters then it is likely that
sufficiently many characters would mean all characters and so it would be impossible to
test the learnt COM on novel characters not seen in training
Instead of learning the COM directly by minimizing it was learnt implicitly by first
learning a set of neural network features for the functions in the environment The features
were learnt using the method outlined in which essentially involves learning a set of
classifiers with a common final hidden layer The features were learnt on out of the
classifiers in the environment using of the data in training and in testing
Each resulting classifier was a linear combination of the neural network features The
average error of the classifiers was on the test set which is an accurate estimate as
there were test examples
Recall from Section that if all can be expressed as I for a fixed feature
set then the COM reduces to I where
w/w The result of the learning procedure above is a set of features
ci and weight vectors such that for each of the character classifiers fi
used in training Ii Wi Thus is
an empirical estimate of the true CDM where W:Wi With a linear change
of variable ci ci>VW becomes This was used to do
I-NN classification on the test examples in two different experiments
fw
In the first experiment all testing and training examples that were not an example of one
of the training characters were lumped into an extra category for the purpose of classification All test examples were then given the label of their nearest neighbour in the
training set under initially all training examples were mapped into feature space
to give Then each test example was mapped into feature space and
assigned the same label as argminx.llci The total misclassification error was
which can be directly compared with the misclassification error of the original classifiers of The COM does better because it uses the training data explicitly and the
information stored in the network to make a comparison whereas the classifiers only use
the information in the network The learnt COM was also used to do k-NN classification
with However this afforded no improvement For example the error of the
classifier was and the error of the classifier was This provides an
indication that the COM may not be the optimal distortion measure to use if NN classification is the aim
In the second experiment was again used to do I-NN classification on the test set but
this time all characters were distinguished So in this case the learnt COM was being
asked to distinguish between characters that were treated as a single character when
it was being trained The misclassification error was a surprisingly low The
error compares favourably with the error achieved on the same data by the CEDAR
group using a carefully selected feature set and a hand-tailored nearest-neighbour routine
In our case the distance measure was learnt from raw-data input and has not been the
subject of any optimization or tweaking
The Canonical Distortion Measure in Feature Space and I-NN Classification
Figure Six Kanji characters first character in each row and examples of their four
nearest neighbours remaining four characters in each row
As a final more qualitative assessment the learnt CDM was used to compute the distance between every pair of testing examples and then the distance between each pair of
characters an individual character being represented by a number of testing examples
was computed by averaging the distances between their constituent examples The nearest neighbours of each character were then calculated With this measure every character
turned out to be its own nearest neighbour and in many cases the next-nearest neighbours
bore a strong subjective similarity to the original Some representative examples are shown
in Figure
CONCLUSION
We have shown how the Canonical Distortion Measure CDM is the optimal distortion
measure for I-NN classification and that for environments in which all the functions can
be expressed as a linear combination of a fixed set of features the Canonical Distortion
Measure is squared Euclidean distance in feature space A technique for learning the CDM
was presented and PAC-like bounds on the sample complexity required for good generalisation were proved
Experimental results were presented in which the CDM for a Japanese OCR environment
was learnt by first learning a common set of features for a subset of the character classifiers
in the environment The learnt CDM was then used as a distance measure in I-NN neighbour classification and performed remarkably well both on the characters used to train it
and on entirely novel characters

<<----------------------------------------------------------------------------------------------------------------------->>

title: 4441-generalized-lasso-based-approximation-of-sparse-coding-for-visual-recognition.pdf

Generalized Lasso based Approximation of Sparse
Coding for Visual Recognition
Nobuyuki Morioka
The University of New South Wales NICTA
Sydney Australia
nmorioka@cse.unsw.edu.au
Shin?ichi Satoh
National Institute of Informatics
Tokyo Japan
satoh@nii.ac.jp
Abstract
Sparse coding a method of explaining sensory data with as few dictionary bases
as possible has attracted much attention in computer vision For visual object category recognition regularized sparse coding is combined with the spatial pyramid representation to obtain state-of-the-art performance However because of its
iterative optimization applying sparse coding onto every local feature descriptor
extracted from an image database can become a major bottleneck To overcome
this computational challenge this paper presents Generalized Lasso based Approximation of Sparse coding GLAS By representing the distribution of sparse
coefficients with slice transform we fit a piece-wise linear mapping function with
the generalized lasso We also propose an efficient post-refinement procedure to
perform mutual inhibition between bases which is essential for an overcomplete
setting The experiments show that GLAS obtains a comparable performance to
regularized sparse coding yet achieves a significant speed up demonstrating its
effectiveness for large-scale visual recognition problems
Introduction
Recently sparse coding has attracted much attention in computer vision research Its applications range from image denoising to image segmentation and image classification
achieving state-of-the-art results Sparse coding interprets an input signal with
a sparse vector whose linear combination with an overcomplete set of bases
also known as dictionary RD?K reconstructs the input as precisely as possible To
enforce sparseness on the norm is a popular choice due to its computational convenience and its
interesting connection with the NP-hard norm in compressed sensing Several efficient regularized sparse coding algorithms have been proposed and are adopted in visual recognition
In particular Yang compute the spare codes of many local feature descriptors
with sparse coding However due to the norm being non-smooth convex the sparse coding algorithm needs to optimize iteratively until convergence Therefore the local feature descriptor coding
step becomes a major bottleneck for large-scale problems like visual recognition
The goal of this paper is to achieve state-of-the-art performance on large-scale visual recognition
that is comparable to the work of Yang but with a significant improvement in efficiency
To this end we propose Generalized Lasso based Approximation of Sparse coding GLAS for
short Specifically we encode the distribution of each dimension in sparse codes with the slice
transform representation and learn a piece-wise linear mapping function with the generalized
lasso to obtain the best fit to approximate regularized sparse coding We further propose
an efficient post-refinement procedure to capture the dependency between overcomplete bases The
effectiveness of our approach is demonstrated with several challenging object and scene category
datasets showing a comparable performance to Yang and performing better than other
fast algorithms that obtain sparse codes While there have been several supervised dictionary
learning methods for sparse coding to obtain more discriminative sparse representations
they have not been evaluated on visual recognition with many object categories due to its computational challenges Furthermore Ranzato have empirically shown that unsupervised
learning of visual features can obtain a more general and effective representation Therefore in this
paper we focus on learning a fast approximation of sparse coding in an unsupervised manner
The paper is organized as follows Section reviews some related work including the linear spatial
pyramid combined with sparse coding and other fast algorithms to obtain sparse codes Section
presents GLAS This is followed by the experimental results on several challenging categorization
datasets in Section Section concludes the paper with discussion and future work
Related Work
Linear Spatial Pyramid Matching Using Sparse Coding
This section reviews the linear spatial pyramid matching based on sparse coding by Yang
Given a collection of local feature descriptors randomly sampled from training images
xN RD?N an over-complete dictionary b2 bK RD?K is learned
by
min
B,U
kxi Bui kui k1
kbk K.
The cost function above is a combination of the reconstruction error and the sparsity penalty
which is controlled by The norm on each bk is constrained to be less than or equal to
to avoid a trival solution Since both and u2 uN are unknown a priori an alternating
optimization technique is often used to optimize over the two parameter sets
Under the spatial pyramid matching framework each image is divided into a set of sub-regions
r2 rR For example if and partitions are used on an image we have
sub-regions Then we compute the sparse solutions of all local feature descriptors denoted as
Urj appearing in each sub-region rj by
min kXrj BUrj kUrj k1
Urj
The sparse solutions are max pooled for each sub-region and concatenated with other sub-regions to
build a statistic of the image by
max(|Ur1 max(|Ur2 max(|UrR
where is a function that finds the maximum value at each row of a matrix and returns a
column vector Finally a linear SVM is trained on a set of image statistics for classification
The main advantage of using sparse coding is that state-of-the-art results can be achieved with a
simple linear classifier as reported in Compared to kernel-based methods this dramatically
speeds up training and testing time of the classifier However the step of finding a sparse code for
each local descriptor with sparse coding now becomes a major bottleneck Using the efficient sparse
coding algorithm based on feature-sign search the time to compute the solution for one local
descriptor is O(KZ where is the number of non-zeros in This paper proposes an approximation method whose time complexity reduces to With the post-refinement procedure its
time complexity is O(K which is still much lower than
Predictive Sparse Decomposition
Predictive sparse decomposition PSD described in is a feedforward network that applies a
non-linear mapping function on linearly transformed input data to match the optimal sparse coding
Gg(Wxi where
solution as accurate as possible Such feedfoward network is defined as
denotes a non-linear parametric mapping function which can be of any form but to name
a few there are hyperbolic tangent tanh(z and soft shrinkage sign(z max(|z The
function is applied to linearly transformed data Wxi and subsequently scaled by a diagonal matrix
G. Given training samples
the parameters can be estimated either jointly or separately from
the dictionary B. When learning jointly we minimize the cost function given below
min
kxi Bui kui k1 kui Gg(Wxi
When learning separately and are obtained with Eqn. first Then other remaining parameters and are estimated by solving the last term of Eqn. only Gregor and LeCun have
later proposed a better but iterative approximation scheme for regularized sparse coding
One downside of the parametric approach is its accuracy is largely dependent on how well its parametric function fits the target statistical distribution as argued by Hel-Or and Shaked This paper
explores a non-parametric approach which can fit any distribution as long as data samples available
are representative The advantage of our approach over the parametric approach is that we do not
need to seek an appropriate parametric function for each distribution This is particularly useful in
visual recognition that uses multiple feature types as it automatically estimates the function form
for each feature type from data We demonstrate this with two different local descriptor types in our
experiments
Locality-constrained Linear Coding
Another notable work that overcomes the bottleneck of the local descriptor coding step is localityconstrained linear coding LLC proposed by Wang a fast version of local coordinate
coding Given a local feature descriptor LLC searches for nearest dictionary bases of
each local descriptor and these nearest bases stacked in columns are denoted as B?i RD?M
where indicates the index list of the bases Then the coefficients u?i RM whose linear
combination with B?i reconstructs is solved by
min kxi B?i u?i
u?i
u?i
This is the least squares problem which can be solved quite efficiently The final sparse code ui is
obtained by setting its elements indexed at to u?i The time complexity of LLC is O(K
This excludes the time required to find nearest neighbours While it is fast the resulting sparse
solutions obtained are not as discriminative as the ones obtained by sparse coding This may be due
to the fact that is fixed across all local feature descriptors Some descriptors may need more bases
for accurate representation and others may need less bases for more distinctiveness In contrast the
number of bases selected with our post-refinement procedure to handle the mutual inhibition is
different for each local descriptor
Generalized Lasso based Approximation of Sparse Coding
This section describes GLAS We first learn a dictionary from a collection of local feature descriptors
as given Eqn. Then based on slice transform representation we fit a piece-wise linear mapping
function with the generalized lasso to approximate the optimal sparse solutions of the local feature
descriptors under regularized sparse coding Finally we propose an efficient post-refinement
procedure to perform the mutual inhibition
Slice Transform Representation
Slice transform representation is introduced as a way to discretize a function space so to fit a piecewise linear function for the purpose of image denoising by Hel-Or and Shaked This is later
adopted by Adler for single image super resolution In this paper we utilise the representation to approximate sparse coding to obtain sparse codes for local feature descriptor as fast as
possible
Given a local descriptor we can linearly combine with to obtain For the moment we just
consider one dimension of denoted as which is a real value and lies in a half open interval of
The interval is divided into equal-sized bins whose boundaries form a vector
q2 qQ such that a q1 q2 qQ
Data
RLS
L1?SC
GLAS
Data
RLS
L1?SC
GLAS
Data
RLS
L1?SC
GLAS
Figure Different approaches to fit a piece-wise linear mapping function Regularized least squares
RLS in red Eqn. regularized sparse coding in magenta Eqn. GLAS
in green Eqn. All three methods achieving a good fit A case when L1-SC fails to
extrapolate well at the end and RLS tends to align itself to in black A case when data samples
at around are removed artificially to illustrate that RLS fails to interpolate as no neighoring
prior is used In contrast GLAS can both interpolate and extrapolate well in the case of missing or
noisy data
The interval into which the value of falls is expressed as if qj and its
corresponding residue is calculated by
Based on the above we can re-express as
Sq
where Sq
If we now come back to the multivariate case of then we have the following
Sq Sq Sq zK where zk implies the th dimension of Then we replace the
boundary vector with p2 pK such that resulting vector approximates the optimal
sparse solution of obtained by regularized sparse coding as much as possible This is written as
Sq Sq Sq zK pK
Hel-Or and Shaked have formulated the problem of learning each pk as regularized least squares
either independently in a transform domain or jointly in a spatial domain Unlike their setting
we have significantly large number of bases which makes joint optimization of all pk difficult
Moreover since we are interested in approximating the sparse solutions which are in the transform
domain we learn each pk independently Given local descriptors xN RD?N
and their corresponding sparse solutions u2 uN y2 yK RK?N
obtained with regularized sparse coding we have an optimization problem given as
min kyk Sk pk kq pk
pk
where Sk Sq zk The regularization of the second term is essential to avoid singularity when
computing the inverse and its consequence is that pk is encouraged to align itself to when not many
data samples are available This might have been a reasonable prior for image denoising but not
desirable for the purpose of approximating sparse coing as we would like to suppress most of the
coefficients in to zero Figure shows the distribution of one dimension of sparse coefficients
obtained from a collection of SIFT descriptors and does not look similar to the distribution This
motivates us to look at the generalized lasso as an alternative for obtaining a better fit of the
distribution of the coefficients
Generalized Lasso
In the previous section we have argued that regularized least squares stated in Eqn. does not give
the desired result Instead most intervals need to be set to zero This naturally leads us to consider
regularized sparse coding also known as the lasso which is formulated as
min kyk Sk pk kpk k1
pk
However the drawback of this is that the learnt piece-wise linear function may become unstable in
cases when training data is noisy or missing as illustrated in Figure and It turns out
trend filtering generally known as the generalized lasso can overcome this problem This
is expressed as
min kyk Sk pk kDpk k1
pk
where
is referred to as a penalty matrix and defined as
To solve the above optimization problem we can turn it into the sparse coding problem Since
A
is not invertible the key is to augment with A to build a square matrix
Q?Q
and the rows of A are orthogonal to the rows of D. To satisfy such
such that rank(D
constraints A can for example be set to If we let Dp
where Dpk and Apk then Sk pk Sk Sk1 Sk2 After some substitutions
Sk2 yk Sk1 given is solved already Now
we see that can be solved by
Sk2
to solve we have the following sparse coding problem
min k(I P)yk I P)Sk1 k1
Sk2 Having computed both and we can recover the solution of pk
where Sk2
Sk2
Further details can be found in
by
Given the learnt we can approximate sparse solution of by Eqn. However explicitly computing Sq and multiplying it by is somewhat redundant Thus we can alternatively compute
as follows
each component of
r(zk pk r(zk pk
whose time complexity becomes In Eqn. since we are essentially using pk as a lookup
table the complexity is independent from Q. This is followed by normalization on
can readily be used for the spatial max pooling as stated in Eqn. it does not yet capture
While
any explaining away effect where the corresponding coefficients of correlated bases are mutually
inhibited to remove redundancy This is because each pk is estimated independently in the transform
domain In the next section we propose an efficient post-refinement technique to mutually inhibit
between the bases
Capturing Dependency Between Bases
To handle the mutual inhibition between overcomplete bases this section explains how to refine the
sparse codes by solving regularized least squares on a significantly small active basis set Given a
estimated with above method we set the non-zero
local descriptor and its initial sparse code
components of the code to be active By denoting a set of these active components as we have
and which are the subsets of the sparse code and dictionary bases respectively The goal is
denoted as such that reconstructs as accurately as
to compute the refined code of
possible We formulate this as regularised least squares given below
min kx
where is the weight parameter of the regularisation This is convex and has the following analytical
solution
is considered as a good
The intuition behind the above formulation is that the initial sparse code
starting point for refinement to further reduce the reconstruction error by allowing redundant bases to
is substantially
compete against each other Empirically the number of active components for each
small compared to the whole basis set Hence a linear system to be solved becomes much smaller
Methods
Train
Train
Time sec
KM
Methods
Train
Train
Time sec
KM
SIFT Dim
LLC PSD
SC
GLAS
GLAS
Local Self-Similarity Dim
LLC PSD
SC
GLAS
GLAS
Table Recognition accuracy on Caltech-101 The dictionary sizes for all methods are set to
We also report the time taken to process local descriptors for each method
which is computationally cheap We also make sure that we do not deviate too much from the initial
solution by introducing the regularization on This refinement procedure may be similar to LLC
However in our case we do not preset the number of active bases and determine by non-zero
More importantly we base our final solution on
and do not perform nearest
components of
neighbor search With this refinement procedure the total time complexity becomes O(K
We refer GLAS with this post-refinement procedure as GLAS
Experimental Results
This section evaluates GLAS and GLAS on several challenging categorization datasets To learn
the mapping function we have used local descriptors as data samples The parameters
and are fixed to and respectively for all experiments unless otherwise stated For
comparison we have implemented methods discussed in Section SC is our re-implementation
of Yang LLC is locality-constrained linear coding proposed by Wang The
number of nearest neighors to consider is set to PSD is predictive sparse decomposition
Shrinkage function is used as its parametric mapping function We also include KM which builds
its codebook with k-means clustering and adopts hard-assignment as its local descriptor coding
For all methods exactly the same local feature descriptors spatial max pooling technique and linear
SVM are used to only compare the difference between the local feature descriptor coding techniques
As for the descriptors SIFT and Local Self-Similarity are used SIFT is a histogram of
gradient directions computed over an image patch capturing appearance information We have
sampled a patch at every pixel step In contrast Local Self-Similarity computes correlation
between a small image patch of interest and its surrounding region which captures the geometric
layout of a local region Spatial max pooling with and image partitions is used
The implementation is all done in MATLAB for fair comparison
Caltech-101
The Caltech-101 dataset consists of images which are divided into object categories
The images are scaled down to preserving their aspect ratios We train with images
per class and test with images per class The dictionary size of each method is set to for
both SIFT and Local Self-Similarity
The results are averaged over eight random training and testing splits and are reported in Table
For SIFT GLAS is consistently better than GLAS demonstrating the effectiveness of mutual
inhibition by the post-refinement procedure Both GLAS and GLAS performs better than other
fast algorithms that produces sparse codes In addition GLAS and GLAS performs competitively
against SC. In fact GLAS is slightly better when training images per class are used While
sparse codes for both GLAS and GLAS are learned from the solutions of SC the approximated
codes are not exactly the same as the ones of SC. Moreover SC sometimes produces unstable codes
due to the non-smooth convex property of norm as previously observed in In contrast GLAS
72
70
68
SC
GLAS
GLAS
66
74
Average Recognition
74
Average Recognition
Average Recognition
74
72
70
68
SC
GLAS
GLAS
66
Alpha
72
70
68
66
64
62
SC
RLS
GLAS
GLAS
of Missing Data
Figure the number of bins to quantize the interval of each sparse code component
the parameter that controls the weight of the norm used for the generalized lasso When some
data samples are missing GLAS is more robust than regularized least squares given in Eqn.
approximates its sparse codes with a relatively smooth piece-wise linear mapping function learned
with the generalized lasso note that the norm penalizes on changes in the shape of the function
and performs smooth post-refinement We suspect these differences may be contributing to the
slightly better results of GLAS on this dataset
Although PSD performs quite close to GLAS for SIFT this is not the case for Local Self-Similarity
GLAS outperforms PSD probably due to the distribution of sparse codes is not captured well by a
simple shrinkage function Therefore GLAS might be more effective for a wide range of distributions This is useful for recognition using multiple feature types where speed is critical GLAS
performs worse than SC but GLAS closes the gap between GLAS and SC. We suspect that due to
Local Self-Similarity dim being relatively low-dimensional than SIFT the mutual
inhibition becomes more important This might also explain why LLC has performed reasonably
well for this descriptor
Table also reports computational time taken to process local descriptors for each method
GLAS and GLAS are slower than KM and PSD but are slightly faster than LLC and significantly
faster than SC. This demonstrates the practical importance of our approach where competitive recognition results are achieved with fast computation
Different values for and are evaluated one parameter at a time Figure shows the results
of different Q. The results are very stable after bins As sparse codes are computed by Eqn.
the time complexity is not affected by what is chosen Figure shows the results for different
which look very stable We also observe similar stability for
We also validate if the generalized lasso given in Eqn. is more robust than the regularized least
squares solution given in Eqn. when some data samples are missing When learning each qk
we artificially remove data samples from an interval centered around a randomly sampled point
as also illustrated in Figure We evaluate with different numbers of data samples removed in
terms of percentages of the whole data sample set The results are shown in Figure where the
performance of RLS significantly drops as the number of missing data is increased However both
GLAS and GLAS are not affected that much
Caltech-256
Caltech-256 contains images and object categories in total Like Caltech-101 we
scale the images down to preserving their aspect ratios The results are averaged over
eight random training and testing splits and are reported in Table We use testing images per
class This time for SIFT GLAS performs slightly worse than SC but GLAS outperforms SC
probably due to the same argument given in the previous experiments on Caltech-101 For Local
Self-Similarity results similar to Caltech-101 are obtained The performance of PSD is close to KM
and is outperformed by GLAS suggesting the inadequate fitting of sparse codes LLC performs
slightly better than GLAS but could not perform better than GLAS While SC performed the best
the performance of GLAS is quite close to SC. We also plot a graph of the computational time
taken for each method with its achieved accuracy on SIFT and Local Self-Similarity in Figure
and respectively
Methods
Train
Train
KM
Methods
Train
Train
KM
SIFT Dim
LLC PSD
SC
GLAS
GLAS
Local Self-Similarity Dim
LLC PSD
SC
GLAS
GLAS
Table Recognition accuracy on Caltech-256 The dictionary sizes are all set to for SIFT and
for Local Self-Similarity
35
KM
LLC
PSD
SC
GLAS
GLAS
Computational Time
81
Average Recognition
36
Average Recognition
Average Recognition
34
KM
LLC
PSD
SC
GLAS
GLAS
32
28
Computational Time
79
KM
LLC
PSD
SC
GLAS
GLAS
78
77
76
Computational Time
Figure Plotting computational time average recognition and are SIFT and Local-Self
Similarity respectively evaluated on Caltech-256 with training images The dictionary size is set
to is SIFT evaluated on Scenes The dictionary size is set to
Scenes
The Scenes dataset contains images divided into scene classes ranging from indoor
scenes to outdoor scenes training images per class are used for training and the rest for testing
We used SIFT to learn dictionary bases for each method The results are plotted with computational time taken in Figure The result of GLAS are very similar to that of SC
yet the former is significantly faster In summary we show that our approach works well
on three different challenging datasets
Conclusion
This paper has presented an approximation of sparse coding based on the generalized lasso called
GLAS This is further extended with the post-refinement procedure to handle mutual inhibition
between bases which are essential in an overcomplete setting The experiments have shown competitive performance of GLAS against SC and achieved significant computational speed up We have
also demonstrated that the effectiveness of GLAS on two local descriptor types namely SIFT and
Local Self-Similarity where LLC and PSD only perform well on one type GLAS is not restricted
to only approximate sparse coding but should be applicable to other variations of sparse coding
in general For example it may be interesting to try GLAS on Laplacian sparse coding that
achieves smoother sparse codes than sparse coding
Acknowledgment
NICTA is funded by the Australian Government as represented by the Department of Broadband Communications and the Digital Economy and the Australian Research Council through the ICT Centre of Excellence
program

<<----------------------------------------------------------------------------------------------------------------------->>

title: 5950-skip-thought-vectors.pdf

Skip-Thought Vectors
Ryan Kiros Yukun Zhu Ruslan Salakhutdinov Richard S. Zemel
Antonio Torralba Raquel Urtasun Sanja Fidler
University of Toronto
Canadian Institute for Advanced Research
Massachusetts Institute of Technology
Abstract
We describe an approach for unsupervised learning of a generic distributed sentence encoder Using the continuity of text from books we train an encoderdecoder model that tries to reconstruct the surrounding sentences of an encoded
passage Sentences that share semantic and syntactic properties are thus mapped
to similar vector representations We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training allowing us
to expand our vocabulary to a million words After training our model we extract and evaluate our vectors with linear models on tasks semantic relatedness
paraphrase detection image-sentence ranking question-type classification and
benchmark sentiment and subjectivity datasets The end result is an off-the-shelf
encoder that can produce highly generic sentence representations that are robust
and perform well in practice
Introduction
Developing learning algorithms for distributed compositional semantics of words has been a longstanding open problem at the intersection of language understanding and machine learning In recent
years several approaches have been developed for learning composition operators that map word
vectors to sentence vectors including recursive networks recurrent networks convolutional
networks and recursive-convolutional methods among others All of these methods
produce sentence representations that are passed to a supervised task and depend on a class label in
order to backpropagate through the composition weights Consequently these methods learn highquality sentence representations but are tuned only for their respective task The paragraph vector
of is an alternative to the above models in that it can learn unsupervised sentence representations
by introducing a distributed sentence indicator as part of a neural language model The downside is
at test time inference needs to be performed to compute a new vector
In this paper we abstract away from the composition methods themselves and consider an alternative loss function that can be applied with any composition operator We consider the following
question is there a task and a corresponding loss that will allow us to learn highly generic sentence
representations We give evidence for this by proposing a model for learning high-quality sentence
vectors without a particular supervised task in mind Using word vector learning as inspiration we
propose an objective function that abstracts the skip-gram model of to the sentence level That
is instead of using a word to predict its surrounding context we instead encode a sentence to predict
the sentences around it Thus any composition operator can be substituted as a sentence encoder
and only the objective function becomes modified Figure illustrates the model We call our model
skip-thoughts and vectors induced by our model are called skip-thought vectors
Our model depends on having a training corpus of contiguous text We chose to use a large collection
of novels namely the BookCorpus dataset for training our models These are free books written
by yet unpublished authors The dataset has books in different genres Romance
books Fantasy Science fiction Teen etc Table highlights the summary
statistics of the book corpus Along with narratives books contain dialogue emotion and a wide
range of interaction between characters Furthermore with a large enough collection the training
set is not biased towards any particular domain or application Table shows nearest neighbours
Figure The skip-thoughts model Given a tuple si of contiguous sentences with si
the i-th sentence of a book the sentence si is encoded and tries to reconstruct the previous sentence
and next sentence In this example the input is the sentence triplet I got back home I
could see the cat on the steps This was strange Unattached arrows are connected to the encoder
output Colors indicate which components share parameters heosi is the end of sentence token
of books
of sentences
of words
of unique words
mean of words per sentence
13
Table Summary statistics of the BookCorpus dataset We use this corpus to training our
model
of sentences from a model trained on the BookCorpus dataset These results show that skip-thought
vectors learn to accurately capture semantics and syntax of the sentences they encode
We evaluate our vectors in a newly proposed setting after learning skip-thoughts freeze the model
and use the encoder as a generic feature extractor for arbitrary tasks In our experiments we consider tasks semantic-relatedness paraphrase detection image-sentence ranking and standard
classification benchmarks In these experiments we extract skip-thought vectors and train linear
models to evaluate the representations directly without any additional fine-tuning As it turns out
skip-thoughts yield generic representations that perform robustly across all tasks considered
One difficulty that arises with such an experimental setup is being able to construct a large enough
word vocabulary to encode arbitrary sentences For example a sentence from a Wikipedia article
might contain nouns that are highly unlikely to appear in our book vocabulary We solve this problem
by learning a mapping that transfers word representations from one model to another Using pretrained word2vec representations learned with a continuous bag-of-words model we learn a
linear mapping from a word in word2vec space to a word in the encoder?s vocabulary space The
mapping is learned using all words that are shared between vocabularies After training any word
that appears in word2vec can then get a vector in the encoder word embedding space
Approach
Inducing skip-thought vectors
We treat skip-thoughts in the framework of encoder-decoder models That is an encoder maps
words to a sentence vector and a decoder is used to generate the surrounding sentences Encoderdecoder models have gained a lot of traction for neural machine translation In this setting an
encoder is used to map an English sentence into a vector The decoder then conditions on this
vector to generate a translation for the source English sentence Several choices of encoder-decoder
pairs have been explored including ConvNet-RNN RNN-RNN and LSTM-LSTM
The source sentence representation can also dynamically change through the use of an attention
mechanism to take into account only the relevant words for translation at any given time In our
model we use an RNN encoder with GRU activations and an RNN decoder with a conditional
GRU. This model combination is nearly identical to the RNN encoder-decoder of used in neural
machine translation GRU has been shown to perform as well as LSTM on sequence modelling
tasks while being conceptually simpler GRU units have only gates and do not require the use
of a cell While we use RNNs for our model any encoder and decoder can be used so long as we
can backpropagate through it
Assume we are given a sentence tuple si Let wit denote the t-th word for sentence si
and let xti denote its word embedding We describe the model in three parts the encoder decoder
and objective function
Encoder Let wi1 wiN be the words in sentence si where is the number of words in the
sentence At each time step the encoder produces a hidden state hti which can be interpreted as the
representation of the sequence wi1 wit The hidden state hN
thus represents the full sentence
A preliminary version of our model was developed in the context of a computer vision application
Query and nearest sentence
he ran his hand inside his coat double-checking that the unopened letter was still there
he slipped his hand between his coat and his shirt where the folded copies lay in a brown envelope
im sure youll have a glamorous evening she said giving an exaggerated wink
im really glad you came to the party tonight he said turning to her
although she could tell he had n?t been too invested in any of their other chitchat he seemed genuinely curious about this
although he had n?t been following her career with a microscope he definitely taken notice of her appearances
an annoying buzz started to ring in my ears becoming louder and louder as my vision began to swim
a weighty pressure landed on my lungs and my vision blurred at the edges threatening my consciousness altogether
if he had a weapon he could maybe take out their last imp and then beat up errol and vanessa
if he could ram them from behind send them sailing over the far side of the levee he had a chance of stopping them
then with a stroke of luck they saw the pair head together towards the portaloos
then from out back of the house they heard a horse scream probably in answer to a pair of sharp spurs digging deep into its flanks
ll take care of it goodman said taking the phonebook
ll do that julia said coming in
he finished rolling up scrolls and placing them to one side began the more urgent task of finding ale and tankards
he righted the table set the candle on a piece of broken plate and reached for his flint steel and tinder
Table In each example the first sentence is a query while the second sentence is its nearest
neighbour Nearest neighbours were scored by cosine similarity from a random sample of
sentences from our corpus
To encode a sentence we iterate the following sequence of equations dropping the subscript
rt
Ur
Uz
ht
tanh(Wx U(r
ht
zt zt
is the proposed state update at time zt is the update gate rt is the reset gate denotes
where
a component-wise product Both update gates takes values between zero and one
Decoder The decoder is a neural language model which conditions on the encoder output hi The
computation is similar to that of the encoder except we introduce matrices Cz Cr and that are
used to bias the update gate reset gate and hidden state computation by the sentence vector One
decoder is used for the next sentence while a second decoder is used for the previous sentence
Separate parameters are used for each decoder with the exception of the vocabulary matrix
which is the weight matrix connecting the decoder?s hidden state for computing a distribution over
words In what follows we describe the decoder for the next sentence although an analogous
computation is used for the previous sentence Let hti+1 denote the hidden state of the decoder
at time Decoding involves iterating through the following sequence of equations dropping the
subscript
rt
ht
hti+1
Wrd Udr Cr hi
Wzd Udz
tanh(W
Cz hi
Chi
zt zt
Given hti+1 the probability of word
given the previous words and the encoder vector is
hi exp(vwi+1
hti+1
where vwi+1
denotes the row of corresponding to the word of
An analogous computation
is performed for the previous sentence
Objective Given a tuple si the objective optimized is the sum of the log-probabilities
for the forward and backward sentences conditioned on the encoder representation
logP
hi
logP
hi
The total objective is the above summed over all such training tuples
Vocabulary expansion
We now describe how to expand our encoder?s vocabulary to words it has not seen during training
Suppose we have a model that was trained to induce word representations such as word2vec Let
Vw2v denote the word embedding space of these word representations and let Vrnn denote the RNN
word embedding space We assume the vocabulary of Vw2v is much larger than that of Vrnn Our
goal is to construct a mapping Vw2v Vrnn parameterized by a matrix such that v0 Wv
for Vw2v and v0 Vrnn Inspired by which learned linear mappings between translation
word spaces we solve an un-regularized linear regression loss for the matrix W. Thus any word
from Vw2v can now be mapped into Vrnn for encoding sentences
Experiments
In our experiments we evaluate the capability of our encoder as a generic feature extractor after
training on the BookCorpus dataset Our experimentation setup on each task is as follows
Using the learned encoder as a feature extractor extract skip-thought vectors for all sentences
If the task involves computing scores between pairs of sentences compute component-wise features between pairs This is described in more detail specifically for each experiment
Train a linear classifier on top of the extracted features with no additional fine-tuning or backpropagation through the skip-thoughts model
We restrict ourselves to linear classifiers for two reasons The first is to directly evaluate the representation quality of the computed vectors It is possible that additional performance gains can be
made throughout our experiments with non-linear models but this falls out of scope of our goal Furthermore it allows us to better analyze the strengths and weaknesses of the learned representations
The second reason is that reproducibility now becomes very straightforward
Details of training
To induce skip-thought vectors we train two separate models on our book corpus One is a unidirectional encoder with dimensions which we subsequently refer to as uni-skip The other is
a bidirectional model with forward and backward encoders of dimensions each This model
contains two encoders with different parameters one encoder is given the sentence in correct order while the other is given the sentence in reverse The outputs are then concatenated to form a
dimensional vector We refer to this model as bi-skip For training we initialize all recurrent
matricies with orthogonal initialization Non-recurrent weights are initialized from a uniform
distribution in Mini-batches of size are used and gradients are clipped if the norm of
the parameter vector exceeds We used the Adam algorithm for optimization Both models were trained for roughly two weeks As an additional experiment we also report experimental
results using a combined model consisting of the concatenation of the vectors from uni-skip and
bi-skip resulting in a dimensional vector We refer to this model throughout as combine-skip
After our models are trained we then employ vocabulary expansion to map word embeddings into
the RNN encoder space The publically available CBOW word vectors are used for this purpose
The skip-thought models are trained with a vocabulary size of words After removing
multiple word examples from the CBOW model this results in a vocabulary size of words
Thus even though our skip-thoughts model was trained with only words after vocabulary
expansion we can now successfully encode possible words
Since our goal is to evaluate skip-thoughts as a general feature extractor we keep text pre-processing
to a minimum When encoding new sentences no additional preprocessing is done other than basic
tokenization This is done to test the robustness of our vectors As an additional baseline we also
consider the mean of the word vectors learned from the uni-skip model We refer to this baseline as
bow This is to determine the effectiveness of a standard baseline trained on the BookCorpus
Semantic relatedness
Our first experiment is on the SemEval Task semantic relatedness SICK dataset Given
two sentences our goal is to produce a score of how semantically related these sentences are based
on human generated scores Each score is the average of different human annotators Scores
take values between and A score of indicates that the sentence pair is not at all related while
http://code.google.com/p/word2vec
MSE
Method
Acc
Illinois-LH
UNAL-NLP
Meaning Factory
ECNU
feats
RAE+DP
RAE+feats
RAE+DP+feats
Mean vectors
DT-RNN
SDT-RNN
LSTM
Bidirectional LSTM
Dependency Tree-LSTM
FHS
PE
WDDP
MTMETRICS
TF-KLD
bow
uni-skip
bi-skip
combine-skip
combine-skip+COCO
bow
uni-skip
bi-skip
combine-skip
combine-skip feats
Method
F1
Table Left Test set results on the SICK semantic relatedness subtask The evaluation metrics
are Pearson?s Spearman?s and mean squared error The first group of results are SemEval
submissions while the second group are results reported by Right Test set results on the
Microsoft Paraphrase Corpus The evaluation metrics are classification accuracy and F1 score Top
recursive autoencoder variants Middle the best published results on this dataset
a score of indicates they are highly related The dataset comes with a predefined split of
training pairs development pairs and testing pairs All sentences are derived from existing
image and video annotation datasets The evaluation metrics are Pearson?s Spearman?s and
mean squared error
Given the difficulty of this task many existing systems employ a large amount of feature engineering
and additional resources Thus we test how well our learned representations fair against heavily engineered pipelines Recently showed that learning representations with LSTM or Tree-LSTM
for the task at hand is able to outperform these existing systems We take this one step further
and see how well our vectors learned from a completely different task are able to capture semantic
relatedness when only a linear model is used on top to predict scores
To represent a sentence pair we use two features Given two skip-thought vectors and we
compute their component-wise product and their absolute difference and concatenate
them together These two features were also used by To predict a score we use the same
setup as Let be an integer vector from to We compute a distribution
as a function of prediction scores given by pi byc if byc pi byc if
byc and otherwise These then become our targets for a logistic regression classifier At test
time given new sentence pairs we first compute targets and then compute the related score as
As an additional comparison we also explored appending features derived from an image-sentence
embedding model trained on COCO section Given vectors and we obtain vectors u0
and from the learned linear embedding model and compute features u0 and These
are then concatenated to the existing features
Table left presents our results First we observe that our models are able to outperform all
previous systems from the SemEval competition It highlights that skip-thought vectors learn
representations that are well suited for semantic relatedness Our results are comparable to LSTMs
whose representations are trained from scratch on this task Only the dependency tree-LSTM of
performs better than our results We note that the dependency tree-LSTM relies on parsers whose
training data is very expensive to collect and does not exist for all languages We also observe
using features learned from an image-sentence embedding model on COCO gives an additional
performance boost resulting in a model that performs on par with the dependency tree-LSTM To
get a feel for the model outputs Table shows example cases of test set pairs Our model is able to
accurately predict relatedness on many challenging cases On some examples it fails to pick up on
small distinctions that drastically change a sentence meaning such as tricks on a motorcycle versus
tricking a person on a motorcycle
Paraphrase detection
The next task we consider is paraphrase detection on the Microsoft Research Paraphrase Corpus On this task two sentences are given and one must predict whether or not they are
Sentence
Sentence
GT
pred
A little girl is looking at a woman in costume
A little girl is looking at a woman in costume
A little girl is looking at a woman in costume
A young girl is looking at a woman in costume
The little girl is looking at a man in costume
A little girl in costume looks like a woman
A sea turtle is hunting for fish
A sea turtle is not hunting for fish
A sea turtle is hunting for food
A sea turtle is hunting for fish
A man is driving a car
There is no man driving the car
The car is being driven by a man
A man is driving a car
A large duck is flying over a rocky stream
A large duck is flying over a rocky stream
A duck which is large is flying over a rocky stream
A large stream is full of rocks ducks and flies
A person is performing acrobatics on a motorcycle
A person is performing tricks on a motorcycle
A person is performing tricks on a motorcycle
The performer is tricking a person on a motorcycle
Someone is pouring ingredients into a pot
Nobody is pouring ingredients into a pot
Someone is pouring ingredients into a pot
Someone is adding ingredients to a pot
Someone is pouring ingredients into a pot
A man is removing vegetables from a pot
Table Example predictions from the SICK test set GT is the ground truth relatedness scored
between and The last few results show examples where slight changes in sentence structure
result in large changes in relatedness which our model was unable to score correctly
Model
Random Ranking
DVSA
GMM+HGLMM
m-RNN
bow
uni-skip
bi-skip
combine-skip
R@1
COCO Retrieval
Image Annotation
R@5 Med
R@1
Image Search
R@5
Med
Table COCO test-set results for image-sentence retrieval experiments R@K is Recall@K high
is good Med is the median rank low is good
paraphrases The training set consists of sentence pairs which are positive and the
test set has pairs are positive We compute a vector representing the pair of sentences
in the same way as on the SICK dataset using the component-wise product and their absolute
difference which are then concatenated together We then train logistic regression on top to
predict whether the sentences are paraphrases Cross-validation is used for tuning the penalty
As in the semantic relatedness task paraphrase detection has largely been dominated by extensive
feature engineering or a combination of feature engineering with semantic spaces We report experiments in two settings one using the features as above and the other incorporating basic statistics
between sentence pairs the same features used by These are referred to as feats in our results
We isolate the results and baselines used in as well as the top published results on this task
Table right presents our results from which we can observe the following skip-thoughts
alone outperform recursive nets with dynamic pooling when no hand-crafted features are used
when other features are used recursive nets with dynamic pooling works better and when skipthoughts are combined with basic pairwise statistics it becomes competitive with the state-of-the-art
which incorporate much more complicated features and hand-engineering This is a promising result
as many of the sentence pairs have very fine-grained details that signal if they are paraphrases
Image-sentence ranking
We next consider the task of retrieving images and their sentence descriptions For this experiment
we use the Microsoft COCO dataset which is the largest publicly available dataset of images
with high-quality sentence descriptions Each image is annotated with captions each from different annotators Following previous work we consider two tasks image annotation and image
search For image annotation an image is presented and sentences are ranked based on how well
they describe the query image The image search task is the reverse given a caption we retrieve
images that are a good fit to the query The training set comes with over images each with
captions For development and testing we use the same splits as The development and test sets
each contain images and captions Evaluation is performed using Recall@K namely the
mean number of images for which the correct caption is ranked within the top-K retrieved results
and vice-versa for sentences We also report the median rank of the closest ground truth result
from the ranked list
The best performing results on image-sentence ranking have all used RNNs for encoding sentences
where the sentence representation is learned jointly Recently showed that by using Fisher
vectors for representing sentences linear CCA can be applied to obtain performance that is as strong
as using RNNs for this task Thus the method of is a strong baseline to compare our sentence
representations with For our experiments we represent images using 4096-dimensional OxfordNet
features from their 19-layer model For sentences we simply extract skip-thought vectors for
each caption The training objective we use is a pairwise ranking loss that has been previously
used by many other methods The only difference is the scores are computed using only linear
transformations of image and sentence inputs The loss is given by
XX
XX
s(Ux Vy s(Ux Vyk
s(Vy Ux s(Vy Uxk
where is an image vector is the skip-thought vector for the groundtruth sentence yk are vectors
for constrastive incorrect sentences and is the image-sentence score Cosine similarity is
used for scoring The model parameters are where is the image embedding matrix and
is the sentence embedding matrix In our experiments we use a dimensional embedding
margin and contrastive terms We trained for epochs and saved our model
anytime the performance improved on the development set
Table illustrates our results on this task Using skip-thought vectors for sentences we get performance that is on par with both and except for R@1 on image annotation where other methods perform much better Our results indicate that skip-thought vectors are representative enough
to capture image descriptions without having to learn their representations from scratch Combined
with the results of it also highlights that simple scalable embedding techniques perform very
well provided that high-quality image and sentence vectors are available
Classification benchmarks
For our final quantitative experiments we report results on several classification benchmarks which
are commonly used for evaluating sentence representation learning methods
We use datasets movie review sentiment customer product reviews subjectivity/objectivity classification SUBJ opinion polarity MPQA and question-type classification
TREC On all datasets we simply extract skip-thought vectors and train a logistic regression classifier on top 10-fold cross-validation is used for evaluation on the first datasets while TREC has
a pre-defined train/test split We tune the penality using cross-validation and thus use a nested
cross-validation for the first datasets
Method
MR
CR
SUBJ
MPQA
TREC
NB-SVM
MNB
cBoW
GrConv
RNN
BRNN
CNN
AdaSent
Paragraph-vector
bow
uni-skip
bi-skip
combine-skip
combine-skip NB
On these tasks properly tuned bag-ofwords models have been shown to perform exceptionally well In particular
the NB-SVM of is a fast and robust performer on these tasks Skipthought vectors potentially give an alternative to these baselines being just as
fast and easy to use For an additional
comparison we also see to what effect augmenting skip-thoughts with bigram Naive Bayes features improves performance
Table presents our results On most
tasks skip-thoughts performs about as
well as the bag-of-words baselines but
Table Classification accuracies on several standard bench fails to improve over methods whose
marks Results are grouped as follows bag-of-words mod sentence representations are learned diels supervised compositional models Paragraph Vector rectly for the task at hand This indicates
unsupervised learning of sentence representations ours that for tasks like sentiment classificaBest results overall are bold while best results outside of group tion tuning the representations even on
are underlined
small datasets are likely to perform better than learning a generic unsupervised
We use the code available at https://github.com/mesnilgr/nbsvm
TREC
SUBJ
SICK
Figure t-SNE embeddings of skip-thought vectors on different datasets Points are colored based
on their labels question type for TREC subjectivity/objectivity for SUBJ On the SICK dataset
each point represents a sentence pair and points are colored on a gradient based on their relatedness
labels Results best seen in electronic form
sentence vector on much bigger datasets Finally we observe that the skip-thoughts-NB combination is effective particularly on MR. This results in a very strong new baseline for text classification
combine skip-thoughts with bag-of-words and train a linear model
Visualizing skip-thoughts
As a final experiment we applied t-SNE to skip-thought vectors extracted from TREC SUBJ
and SICK datasets and the visualizations are shown in Figure For the SICK visualization each
point represents a sentence pair computed using the concatenation of component-wise and absolute
difference of features Even without the use of relatedness labels skip-thought vectors learn to
accurately capture this property
Conclusion
We evaluated the effectiveness of skip-thought vectors as an off-the-shelf sentence representation
with linear classifiers across tasks Many of the methods we compare against were only evaluated
on task The fact that skip-thought vectors perform well on all tasks considered highlight the
robustness of our representations
We believe our model for learning skip-thought vectors only scratches the surface of possible objectives Many variations have yet to be explored including deep encoders and decoders larger
context windows encoding and decoding paragraphs other encoders such as convnets It is
likely the case that more exploration of this space will result in even higher quality representations
Acknowledgments
We thank Geoffrey Hinton for suggesting the name skip-thoughts We also thank Felix Hill Kelvin
Xu Kyunghyun Cho and Ilya Sutskever for valuable comments and discussion This work was
supported by NSERC Samsung CIFAR Google and ONR Grant

<<----------------------------------------------------------------------------------------------------------------------->>

