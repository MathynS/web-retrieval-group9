query sentence: backpropagated error in neural-network
<<------------------------------------------------------------------------------------------------------------------------------------------->>
title: 1100-tempering-backpropagation-networks-not-all-weights-are-created-equal.pdf

Tempering Backpropagation Networks
Not All Weights are Created Equal
Nicol N. Schraudolph
EVOTEC BioSystems GmbH
Grandweg 64
Hamburg Germany
nici@evotec.de
Terrence J. Sejnowski
Computational Neurobiology Lab
The Salk Institute for BioI Studies
San Diego CA USA
terry@salk.edu
Abstract
Backpropagation learning algorithms typically collapse the network's
structure into a single vector of weight parameters to be optimized We
suggest that their performance may be improved by utilizing the structural information instead of discarding it and introduce a framework for
tempering each weight accordingly
In the tempering model activation and error signals are treated as approximately independent random variables The characteristic scale of weight
changes is then matched to that ofthe residuals allowing structural properties such as a node's fan-in and fan-out to affect the local learning rate
and backpropagated error The model also permits calculation of an upper
bound on the global learning rate for batch updates which in turn leads
to different update rules for bias non-bias weights
This approach yields hitherto unparalleled performance on the family relations benchmark a deep multi-layer network for both batch learning
with momentum and the delta-bar-delta algorithm convergence at the
optimal learning rate is sped up by more than an order of magnitude
Introduction
Although neural networks are structured graphs learning algorithms typically view them
as a single vector of parameters to be optimized All information about a network's architecture is thus discarded in favor of the presumption of an isotropic weight space the
notion that a priori all weights in the network are created equal This serves to decouple
the learning process from network design and makes a large body of function optimization
techniques directly applicable to backpropagation learning
But what if the discarded structural information holds valuable clues for efficient weight
optimization Adaptive step size and second-order gradient techniques Battiti may
N. N. SCHRAUDOLPH T. J. SEJNOWSKI
recover some of it at considerable computational expense Ad hoc attempts to incorporate
structural information such as the fan-in Plaut into local learning rates have become a familiar part of backpropagation lore here we deri ve a more comprehensi ve framework which we call tempering and demonstrate its effectiveness
Tempering is based on modeling the acti vities and error signals in a backpropagation network as independent random variables This allows us to calculate activity and weightinvariant upper bounds on the effect of synchronous weight updates on a node's activity
We then derive appropriate local step size parameters by relating this maximal change in a
node's acti vi ty to the characteristic scale of its residual through a global learning rate
Our subsequent derivation of an upper bound on the global learning rate for batch learning
suggests that the component of the error signal be given special treatment Our experiments show that the resulting method of error shunting allows the global learning rate to
approach its predicted maximum for highly efficient learning performance
Local Learning Rates
Consider a neural network with feedforward activation given by
Yj
Wij
iEAj
where Aj denotes the set of anterior nodes feeding directly into node and is a nonlinear
typically sigmoid activation function We imply that nodes are activated in the appropriate
sequence and that some have their values clamped so as to represent external inputs
With a local learning rate of'1j for node gradient descent in an objective function produces the weight update
Linearizing
Ij
around
Yj
approximates the resultant change in activation
Xj
as
iEAj
iEAj
Our goal is to put the scale of Xj in relation to that of the error signal tSj Specifically when
averaged over many training samples we want the change in output activity of each node
in response to each pattern limited to a certain proportion given by the global learning
rate of its residual We achieve this by relating the variation of over the training
set to that of the error signal
where denotes averaging over training samples Formally this approach may be interpreted as a diagonal approximation of the inverse Fischer information matrix Amari
We implement by deriving an upper bound for the left-hand side which is then equated
with the right-hand side Replacing the acti vity-dependent slope of Ij by its maximum value
maxl/j(u)1
and assuming that there are no correlations between inputs and error tSj we obtain
Note
that such correlations are minimized by the local weight update
Tempering Backpropagation Networks Not All Weights Are Created Equal
from provided that
ej
lEA
We can now satisfy by setting the local learning rate to
TJ
fj
TJ
ej
There are several approaches to computing an upper bound on the total squared input
power
One option would be to calculate the latter empirically during training though
this raises sampling and stability issues For external inputs we may precompute orderive
an upper bound based on prior knowledge of the training data For inputs from other nodes
in the network we assume independence and derive
from the range of their activation
functions
p(fd where p(fd ffiuax/i(u)2
ej
ej
iEAj
Note that when all nodes use the same activation function
I
we obtain the well-known
Vfan-in heuristic Plaut as a special case of
Error Backpropagation
In deriving local learning rates above we have tacitly used the error signal as a stand-in for
the residual proper the distance to the target For output nodes we can scale the error to
never exceed the residual
Note that for the conventional quadratic error this simplifies to Pj What about
the remainder of the network Unlike Krogh we do not wish to prescribe
definite targets and hence residuals for hidden nodes Instead we shall use our bounds
and independence arguments to scale backpropagated error signals to roughly appropriate
magnitude For this purpose we introduce an attenuation coefficient aj into the error backpropagation equation
c5j
aj
Wjj
c5j
jEP
where Pi denotes the set of posterior nodes fed directly from node We posit that the appropriate variation for c5i be no more than the weighted average of the variation of backpropagated errors
whereas assuming independence between the c5j and replacing the slope of Ii by its maximum value gives us
a
jEP
Again we equate the right-hand sides of both inequalities to satisfy yielding
8(fdJiP;T
N. N. SCHRAUDOLPH T. J. SEJNOWSKI
Note that the incorporation ofthe weights into is ad hoc as we have no a priori reason
to scale a node's step size in proportion to the size of its vector of outgoing weights We
have chosen simply because it produces a weight-invariant value for the attenuation
coefficient The scale of the backpropagated error could be controlled more rigorously at
the expense of having to recalculate after each weight update
Global Learning Rate
We now derive the appropriate global learning rate for the batch weight update
LiWij
dj
tET
over a non-redundant training sample T. Assuming independent and zero-mean residuals
we then have
by virtue of Under these conditions we can ensure
dj
that the variation of the batch weight update does not exceed that of the residual by
using a global learning rate of
l/JiTf
Even when redundancy in the training set forces us to use a lower rate knowing the upper
bound effectively allows an educated guess at saving considerable time in practice
Error Shunting
It remains to deal with the assumption made above that the residuals be zero-mean that
Any component in the error requires a learning rate inversely proportional to
the batch size far below the rate permissible for zero-mean residuals This suggests
handling the component of error signals separately This is the proper job of the bias
weight so we update it accordingly
In order to allow learning at rates close to
then centered by subtracting the mean
for all other weights their error signals are
tET
T/j
tET
dj
dj
tET
Note that both sums in must be collected in batch implementations of backpropagation
anyway the only additional statistic required is the average input activity Indeed
for batch update centering errors is equivalent to centering inputs which is known to assist
learning by removing a large eigenvalue of the Hessian LeCun We expect
online implementations to perform best when both input and error signals are centered so
as to improve the stochastic approximation
Tempering Backpropagation Networks Not All Weights Are Created Equal
person
OO~O~OOOOOOO
TJeff
TJ
A BI?.
if;i
person OOOOO~OOOOOO
TJ
05 TJ
OOOOOOOOOO~O
relationship
Figure Backpropagation network for learning family relations Hinton
Experimental Setup
We tested these ideas on the family relations task Hinton a backpropagation network is given examples of a family member and relationship as input and must indicate
on its output which family members fit the relational description according to an underlying family tree Its architecture Figure consists of a central association layer of hidden
units surrounded by three encoding layers that act as informational bottlenecks forcing the
network to make the deep structure of the data explicit
The input is presented to the network in a canonical local encoding for any given training
example exactly one input in each of the two input layers is active On account of the always
active bias input the squared input power for tempering at these layers is thus Since
the output uses the same local code only one or two targets at a time will be active we
therefore do not attenuate error signals in the immediately preceding layer We use crossentropy error and the logistic squashing function at the output giving
but prefer the hyperbolic tangent for hidden units with p(tanh s(tanh
To illustrate the impact of tempering on this architecture we translate the combined effect
of local learning rate and error attenuation into an effective learning rate for each layer
shown on the right in Figure We observe that effective learning rates are largest near the
output and decrease towards the input due to error attenuation Contrary to textbook opinion
LeCun Haykin page we find that such unequal step sizes are in fact the
key to efficient learning here We suspect that the logistic squashing function may owe its
popUlarity largely to the error attenuation side-effect inherent in its maximum slope of expect tempering to be applicable to a variety of backpropagation learning algorithms
here we present first results for batch learning with momentum and the delta-bar-delta
rule Jacobs Both algorithms were tested under three conditions conventional
tempered as described in Sections and and tempered with error shunting All experiments were performed with a customized simulator based on Xerion
For each condition the global learning rate TJ was empirically optimized to single-digit precision for fastest reliable learning performance as measured by the sum of empirical mean
and standard deviation of epochs required to reach a given low value of the cost function
All other parameters were held in variant across experiments their values shown in Table
were chosen in advance so as not to bias the results
2This is possible only for strictly layered networks those with no shortcut skip-through
connections between topologically non-adjacent layers
At the time of writing the Xerion neural network simulator and its successor UTS are available
by anonymous file transfer from ai.toronto.edu directory pub/xerion
N. N. SCHRAUDOLPH T. SEJNOWSKI
Parameter
Val ue
training set size epoch
momentum parameter
uniform initial weight range
weight decay rate per epoch
I Value I
Parameter
zero-error radius around target
acceptable error weight cost
delta-bar-delta gain increment
delta-bar-delta gain decrement
Table Invariant parameter settings for our experim~nts
Experimental Results
Table lists the empirical mean and standard deviation over ten restarts of the number
of epochs required to learn the family relations task under each condition and the optimal
learning rate that produced this performance Training times for conventional backpropagation are quite long this is typical for deep multi-layer networks For comparison Hinton
reports around epochs on this problem when both learning rate and momentum have
been optimized personal communication Much faster convergence though to a far
looser criterion has recently been observed for online algorithms O'Reilly
Tempering on the other hand is seen here to speed up two batch learning methods by almost an order of magnitude It reduces not only the average training time but also its coefficient of variation indicating a more reliable optimization process Note that tempering
makes simple batch learning with momentum run about twice as fast as the delta-bar-delta
algorithm This is remarkable since delta-bar-delta uses online measurements to continually adapt the learning rate for each individual weight whereas tempering merely prescales
it based on the network's architecture We take this as evidence that tempering establishes
appropriate local step sizes upfront that delta-bar-delta must discover empirically
This suggests that by using tempering to set the initial equilibrium learning rates for deltabar-delta it may be possible to reap the benefits of both prescaling and adaptive step size
control Indeed Table confirms that the respective speedups due to tempering and deltabar-delta multiply when the two approaches are combined in this fashion Finally the addition of error shunting increases learning speed yet further by allowing the global learning
rate to be brought close to the maximum of
that we would predict from
Discussion
In our experiments we have found tempering to dramatically improve speed and reliability
of learning More network architectures data sets and learning algorithms will have to be
tempered to explore the general applicability and limitations of this approach we also
hope to extend it to recurrent networks and online learning Error shunting has proven useful
in facilitating of near-maximal global learning rates for rapid optimization
Algorithm
Condition
conventional
with tempering
tempering shunting
batch momentum
mean
st.d
delta-bar-delta
mean
st.d
Table Epochs required to learn the family relations task
Tempering Backpropagation Networks Not All Weights Are Created Equal
Although other schemes may speed up backpropagation by comparable amounts our approach has some unique advantages It is computationally cheap to implement local learning and error attenuation rates are invariant with respect to network weights and activities
and thus need to be recalculated only when the network architecture is changed
More importantly even advanced gradient descent methods typically retain the isotropic
weight space assumption that we improve upon one would therefore expect them to benefit from tempering as much as delta-bar-delta did in the experiments reported here For
instance tempering could be used to set non-isotropic model-trust regions for conjugate and
second-order gradient descent algorithms
Finally by restricting ourselves to fixed learning rates and attenuation factors for now we
have arrived at a simplified method that is likely to leave room for further improvement
Possible refinements include taking weight vector size into account when attenuating error
signals or measuring quantities such as online instead of relying on invariant upper
bounds How such adaptive tempering schemes will compare to and interact with existing
techniques for efficient backpropagation learning remains to be explored
Acknowledgements
We would like to thank Peter Dayan Rich Zemel and Jenny Orr for being instrumental in
discussions that helped shape this work Geoff Hinton not only offered invaluable comments but is the source of both our simulator and benchmark problem N. Schraudolph
received financial support from the McDonnell-Pew Center for Cognitive Neuroscience in
San Diego and the Robert Bosch Stiftung GmbH

<<----------------------------------------------------------------------------------------------------------------------->>

title: 358-continuous-speech-recognition-by-linked-predictive-neural-networks.pdf

Continuous Speech Recognition by
Linked Predictive Neural Networks
Joe Tebelskis Alex Waibel Bojan Petek and Otto Schmidbauer
School of Computer Science
Carnegie Mellon University
Pittsburgh PA
Abstract
We present a large vocabulary continuous speech recognition system based
on Linked Predictive Neural Networks LPNN's The system uses neural networks as predictors of speech frames yielding distortion measures
which are used by the One Stage DTW algorithm to perform continuous
speech recognition The system already deployed in a Speech to Speech
Translation system currently achieves and word accuracy
on tasks with perplexity and respectively outperforming several simple HMMs that we tested We also found that the accuracy and
speed of the LPNN can be slightly improved by the judicious use of hidden
control inputs We conclude by discussing the strengths and weaknesses
of the predictive approach
INTRODUCTION
Neural networks are proving to be useful for difficult tasks such as speech recognition because they can easily be trained to compute smooth nonlinear nonparametric functions from any input space to output space In speech recognition the
function most often computed by networks is classification in which spectral frames
are mapped into a finite set of classes such as phonemes In theory classification
networks approximate the optimal Bayesian discriminant function and in practice they have yielded very high accuracy However integrating a phoneme
classifier into a speech recognition system is nontrivial since classification decisions
tend to be binary and binary phoneme-level errors tend to confound word-level
hypotheses To circumvent this problem neural network training must be carefully
integrated into word level training An alternative function which can be com
Tebelskis Waibel Petek and Schmidbauer
puted by networks is prediction where spectral frames are mapped into predicted
spectral frames This provides a simple way to get non-binary distortion measures
with straightforward integration into a speech recognition system Predictive networks have been used successfully for small vocabulary and large vocabulary
speech recognition systems In this paper we describe our prediction-based
LPNN system which performs large vocabulary continuous speech recognition
and which has already been deployed within a Speech to Speech Translation system
We present our experimental results and discuss the strengths and weaknesses
of the predictive approach
LINKED PREDICTIVE NEURAL NETWORKS
The LPNN system is based on canonical phoneme models which can be logically
concatenated in any order using a linkage pattern to create templates for different words this makes the LPNN suitable for large vocabulary recognition
Each canonical phoneme is modeled by a short sequence of neural networks The
number of nets in the sequence corresponds to the granularity of the
phoneme model These phone modeling networks are nonlinear multilayered feedforward and predictive in the sense that given a short section of speech the
networks are required to extrapolate the raw speech signal rather than to classify
it Thus each predictive network produces a time-varying model of the speech
signal which will be accurate in regions corresponding to the phoneme for which
that network has been trained but inaccurate in other regions which are better
modeled by other networks Phonemes are thus recognized indirectly by virtue
of the relative accuracies of the different predictive networks in various sections of
speech Note however that phonemes are not classified at the frame level Instead
continuous scores prediction errors are accumulated for various word candidates
and a decision is made only at the word level where it is finally appropriate
TRAINING AND TESTING ALGORITHMS
The purpose of the training procedure is both to train the networks to become
better predictors and to cause the networks to specialize on different phonemes
Given a known training utterance the training procedure consists of three steps
Forward Pass All the networks make their predictions across the speech sample and we compute the Euclidean distance matrix of prediction errors between
predicted and actual speech frames See Figure
Alignment Step We compute the optimal time-alignment path between the
input speech and corresponding predictor nets using Dynamic Time Warping
Backward Pass Prediction error is backpropagated into the networks according
to the segmentation given by the alignment path See Figure
Hence backpropagation causes the nets to become better predictors and the alignment path induces specialization of the networks for different phonemes
Testing is performed using the One Stage algorithm which is a classical extension of the Dynamic Time Warping algorithm for continuous speech
Continuous Speech Recognition by Linked ftedictive Neural Networks
prediction errors
CtJ
phoneme
predictors
phoneme
predictors
Figure The forward pass during training Canonical phonemes are modeled by
sequences of predictive networks shown as triangles here Words are
represented by linkage patterns over these canonical phoneme models shown in
the area above the triangles according to the phonetic spelling of the words Here
we are training on the word In the forward pass prediction errors shown
as black circles are computed for all predictors for each frame of the input speech
As these prediction errors are routed through the linkage pattern they fill a distance
matrix upper right
CtJ
Figure The backward pass during training After the DTW alignment path has
been computed error is backpropagated into the various predictors responsible for
each point along the alignment path The back propagated error signal at each such
point is the vector difference between the predicted and actual frame This teaches
the networks to become better predictors and also causes the networks to specialize
on different phonemes
Tebelskis Waibel Petek and Schmidbauer
RECOGNITION EXPERIMENTS
We have evaluated the LPNN system on a database of continuous speech recorded
at CMU. The database consists of English sentences using a vocabulary of
words comprising dialogs in the domain of conference registration Training
and testing versions of this database were recorded in a quiet office by multiple
speakers for speaker-dependent experiments Recordings were digitized at a sampling rate of KHz. A Hamming window and an FFT were computed to produce
melscale spectral coefficients every msec In our experiments we used
context-independent phoneme models including one for silence each of which had
a 6-state phoneme topology similar to the one used in the SPICOS system
lIE
lit
tV>o I?UD IS THIS Il IFfll
iii EH
ClII'EREKI
IH
seero
1ft
17 61
III 1ft
KRAHfRIftNS
ER
I
lh lllIltlll III
llll IIIIIIIIU
lu III.,IIII.III
ll
I.II UI
IIIII IIII IIIIUIIIlIlIl"I IU IIUIIlIlIlIl IIIU I.
III"hl
UIlIlI
I
IUIlIl
I
I
Figure Actual and predicted spectrograms
Figure shows the result of testing the LPNN system on a typical sentence The
top portion is the actual spectrogram for this utterance the bottom portion shows
the frame-by-frame predictions made by the networks specified by each point along
the optimal alignment path The similarity of these two spectrograms indicates that
the hypothesis forms a good acoustic model of the unknown utterance fact the
hypothesis was correct in this case In our speaker-dependent experiments using
two males speakers our system averaged and word accuracy on
tasks with perplexity and respectively
In order to confirm that the predictive networks were making a positive contribution to the overall system we performed a set of comparisons between the LPNN
and several pure HMM systems When we replaced each predictive network by a
univariate Gaussian whose mean and variance were determined analytically from
the labeled training data the resulting HMM achieved word accuracy compared to achieved by the LPNN under the same conditions single speaker
perplexity When we also provided the HMM with delta coefficients which
were not directly available to the LPNN it achieved Thus the LPNN was
outperforming each of these simple HMMs
Continuous Speech Recognition by Linked R-edictive Neural Networks
HIDDEN CONTROL EXPERIMENTS
In another series of experiments we varied the LPNN architecture by introducing
hidden control inputs as proposed by Levin The idea illustrated in Figure
is that a sequence of independent networks is replaced by a single network which is
modulated by an equivalent number of hidden control input bits that distinguish
the state
Sequence of
Predictive Networks
Hidden Control
Network
Figure A sequence of networks corresponds to a single Hidden Control network
A theoretical advantage of hidden control architectures is that they reduce the
number offree parameters in the system As the number of networks is reduced each
one is exposed to more training data and up to a certain point generalization
may improve The system can also run faster since partial results of redundant
forward pass computations can be saved Notice however that the total number
of forward passes is unchanged Finally the savings in memory can be significant
In our experiments we found that by replacing 2-state phoneme models by equivalent Hidden Control networks recognition accuracy improved slightly and the system ran much faster On the other hand when we replaced all of the phonemic
networks in the entire system by a single Hidden Control network whose hidden
control inputs represented the phoneme as well as its state recognition accuracy
degraded significantly Hence hidden control may be useful but only if it is used
judiciously
CURRENT LIMITATIONS OF PREDICTIVE NETS
While the LPNN system is good at modeling the acoustics of speech it presently
tends to suffer from poor discrimination In other words for a given segment
of speech all of the phoneme models tend to make similarly good predictions
rendering all phoneme models fairly confusable For example Figure shows an
actual spectrogram and the frame-by-frame predictions made by the model
and the model Disappointingly both models are fairly accurate predictors for
the entire utterance
This problem arises because each predictor receives training in only a small region
of input acoustic space those frames corresponding to that phoneme Consequently when a predictor is shown any other input frames it will compute an
Tebelskis Waibel Petek and Schmidbauer
lti1"rl
nu IIII UI I I IIU I U. lh
I III'II
II'".II
IIH IIII'I I1
I'II
r"TT~!"."""'""'"TmmTmTmTnm""'TnT""'IJII1""'rrm~!!!'I1TT!".""rrmrrm""'...,.,nT11""';nnIlPl_rm""'m;1,m,mn'""rrmTm?nnTTi1TtTTT11TTTm mml
I1
I
I
I.II
III III I HIIIIII
I IIII
IUff'.""I".'.IIIIIIIIIII
tt'"IIII I
I
I IIl I
I
01
tu
I.H III I I
Figure Actual spectrogram and corresponding predictions by the and
phoneme models
undefined output which may overlap with the outputs of other predictors In other
words the predictors are currently only trained on positive instances because it is
not obvious what predictive output target is meaningful for negative instances and
this leads to problematic undefined regions for the predictors Clearly some type
of discriminatory training technique should be introduced to yield better performance in prediction based recognizers
CONCLUSION
We have studied the performance of Linked Predictive Neural Networks for large vocabulary continuous speech recognition Using a 6-state phoneme topology without
duration modeling or other optimizations the LPNN achieved an average of
and accuracy on tasks with perplexity and respectively This
was better than the performance of several simple HMMs that we tested Further
experiments revealed that the accuracy and speed of the LPNN can be slightly
improved by the judicious use of hidden control inputs
The main advantages of predictive networks are that they produce non-binary distortion measures in a simple and elegant way and that by virtue of their nonlinearity
they can model the dynamic properties of speech curvature better than linear predictive models Their main current weakness is that they have poor
discrimination since their strictly positive training causes them all to make confusably accurate predictions in any context Future research should concentrate
on improving the discriminatory power of the LPNN by such techniques as corrective training explicit context dependent phoneme modeling and function word
modeling
Continuous Speech Recognition by Linked ltedictive Neural Networks
Acknowledgements
The authors gratefully acknowledge the support of DARPA the National Science
Foundation ATR Interpreting Telephony Research Laboratories and NEC Corporation B. Petek also acknowledges support from the University of Ljubljana and
the Research Council of Slovenia Schmidbauer acknowledges support from his
employer Siemens AG Germany

<<----------------------------------------------------------------------------------------------------------------------->>

title: 2137-relative-density-nets-a-new-way-to-combine-backpropagation-with-hmms.pdf

Relative Density Nets A New Way to
Combine Backpropagation with HMM's
Andrew D. Brown
Department of Computer Science
University of Toronto
Toronto Canada M5S
andy@cs.utoronto.ca
Geoffrey E. Hinton
Gatsby Unit UCL
London UK WCIN 3AR
hinton@gatsby.ucl.ac.uk
Abstract
Logistic units in the first hidden layer of a feedforward neural network compute the relative probability of a data point under two
Gaussians This leads us to consider substituting other density
models We present an architecture for performing discriminative
learning of Hidden Markov Models using a network of many small
HMM's Experiments on speech data show it to be superior to the
standard method of discriminatively training HMM's
Introduction
A standard way of performing classification using a generative model is to divide the
training cases into their respective classes and hen train a set of class conditional
models This unsupervised approach to classification is appealing for two reasons It
is possible to reduce overfitting because he model learns the class-conditional input
densities P(xlc rather han the input conditional class probabilities P(clx Also
provided that the model density is a good match to the underlying data density
then the decision provided by a probabilistic model is Bayes optimal The problem
with this unsupervised approach to using probabilistic models for classification is
that for reasons of computational efficiency and analytical convenience very simple
generative models are typically used and the optimality of the procedure no longer
holds For this reason it is usually advantageous to train a classifier discriminatively
In this paper we will look specifically at the problem of learning HMM for classifying speech sequences It is an application area where the assumption that the HMM
is the correct generative model for the data is inaccurate and discriminative methods
of training have been successful The first section will give an overview of current
methods of discriminatively training HMM classifiers We will then introduce a new
type of multi-layer backpropagation network which takes better advantage of the
HMM for discrimination Finally we present some simulations comparing the two
methods
19 S1
c1
HMM
Sequence
Figure An Alphanet with one HMM per class Each computes a score for the
sequence and this feeds into a softmax output layer
Alphanets and Discriminative Learning
The unsupervised way of using an HMM for classifying a collection of sequences is to
use the Baum-Welch algorithm to fit one HMM per class Then new sequences
are classified by computing the probability of a sequence under each model and
assigning it to the one with the highest probability Speech recognition is one of the
commonest applications of HMM but unfortunately an HMM is a poor model of
the speech production process For this reason speech researchers have looked at the
possibility of improving the performance of an HMM classifier by using information
from negative examples examples drawn from classes other than the one which
the HMM was meant to model One way of doing this is to compute the mutual
information between the class label and the data under the HMM density and
maximize that objective function
It was later shown that this procedure could be viewed as a type of neural network
Figure in which the inputs to the network are the log-probability scores
C(Xl:TIH of the sequence under hidden Markov model In such a model
there is one HMM per class and the output is a softmax non-linearity
Training this model by maximizing the log probability of correct classification leads
to a classifier which will perform better than an equivalent HMM model trained
solely in a unsupervised manner Such an architecture has been termed an AIphanet because it may be implemented as a recurrent neural network which mimics
the forward pass of the forward-backward algorithm.l
Backpropagation Networks as Density Comparators
A multi-layer feedforward network is usually thought of as a flexible non-linear
regression model but if it uses the logistic function non-linearity in the hidden
layer there is an interesting interpretation of the operation performed by each
hidden unit Given a mixture of two Gaussians where we know the component
priors and the component densities then the posterior probability that
Gaussian 90 generated an observation is a logistic function whose argument is
the negative log-odds of the two classes This can clearly be seen by rearranging
lThe results of the forward pass are the probabilities of the hidden states conditioned
on the past observations or alphas in standard HMM terminology
the expression for the posterior
P(xI9o)P(Qo
P(xI9o)P(Qo P(xI9d Qd
P(Qolx
exp log
P(x IQo P(x lQd
log
P(Qo
P(Ql
If the class conditional densities in question are multivariate Gaussians
P(xI9k
exp
with equal covariance matrices
written in this familiar form
Pk)T
then the posterior class probability may be
P(Qo Ix
where
Thus the multi-layer perceptron can be viewed as computing pairwise posteriors
between Gaussians in the input space and then combining these in the output layer
to compute a decision
A New Kind of Discriminative Net
This view of a feedforward network suggests variations in which other kinds of
density models are used in place of Gaussians in the input space In particular
instead of performing pairwise comparisons between Gaussians the units in the
first hidden layer can perform pairwise comparisons between the densities of an
input sequence under different HMM's For a given sequence the log-probability
of a sequence under each HMM is computed and the difference in log-probability
is used as input to the logistic hidden unit This is equivalent to computing the
posterior responsibilities of a mixture of two HMM's with equal prior probabilities
In order to maximally leverage the information captured by the HMM's we use
hidden units so that all possible pairs are included The output of a hidden unit
is given by
where we have used as an index over the set of all unordered pairs of
the HMM's The results of this hidden layer computation are then combined using
a fully connected layer of free weights and finally passed through a soft max
function to make the final decision
ak
W(m n)kh(mn
2We take the time averaged log-probability so that the scale of the inputs is independent
of the length of the sequence
Density
Comparator
Units
Figure A multi-layer density net with HMM's in the input layer The hidden
layer units perform all pairwise comparisons between the HMM
where we have used as shorthand for the logistic function and Pk is the value
of the kth output unit The resulting architecture is shown in figure Because
each unit in the hidden layer takes as input the difference in log-probability of two
HMM this can be thought of as a fixed layer of weights connecting each hidden
unit to a pair of HMM's with weights of
In contrast to the Alphanet which allocates one HMM to model each class this network does not require a one-to-one alignment between models and classes and it gets
maximum discriminative benefit from the HMM's by comparing all pairs Another
benefit of this architecture is that it allows us to use more HMM's than there are
classes The unsupervised approach to training HMM classifiers is problematic because it depends on the assumption that a single HMM is a good model of the data
and in the case of speech this is a poor assumption Training the classifier discriminatively alleviated this drawback and the multi-layer classifier goes even further in
this direction by allowing many HMM's to be used to learn the decision boundaries
between the classes The intuition here is that many small HMM's can be a far
more efficient way to characterize sequences than one big HMM. When many small
HMM's cooperate to generate sequences the mutual information between different
parts of generated sequences scales linearly with the number of HMM's and only
logarithmically with the number of hidden nodes in each HMM
Derivative Updates for a Relative Density Network
The learning algorithm for an RDN is just the backpropagation algorithm applied
to the network architecture as defined in equations and The output layer is
a distribution over class memberships of data point Xl:T and this is parameterized
as a softmax function We minimize the cross-entropy loss function
tk logpk
where Pk is the value of the kth output unit and tk is an indicator variable which is
equal to if is the true class Taking derivatives of this expression with respect
to the inputs of the output units yields
of
Pk
oak
Oak
tk Pk)h(mn
OW(mn
oak OW(mn
The derivative of the output of the mn)th hidden unit with respect to the output
of ith HMM is
oh(mn
bin
where bim bin is an indicator which equals if if and zero
otherwise This derivative can be chained with the the derivatives backpropagated
from the output to the hidden layer
For the final step of the backpropagation procedure we need the derivative of the
log-likelihood of each HMM with respect to its parameters In the experiments we
use HMM with a single axis-aligned Gaussian output density per state We use
the following notation for the parameters
A aij is the transition probability from state to state
7ri is the initial state prior
mean vector for state
Vi vector of variances for state
set of HMM parameters A
We also use the variable St to represent the state of the HMM at time We make
use of the property of all latent variable density models that the derivative of the
log-likelihood is equal to the expected derivative of the joint log-likelihood under
the posterior distribution For an HMM this means that
P(Sl:Tlxl:T log P(Xl:T Sl:TI1-l
Sl:T
The joint likelihood of an HMM is
logP(Xl:T
i)log 7ri
LL(b jb
i)log aij
i,j
IOgVi'd
canst
where denotes expectations under the posterior distribution and and
are the expected state occupancies and transitions under this distribution All the necessary expectations are computed by the forward backward algorithm We could take derivatives with respect to this functional directly but that would require doing constrained gradient descent on the probInstead we reparameterize the model using a
abilities and the variances
softmax basis for probability vectors and an exponential basis for the variance parameters
This choice of basis allows us to do unconstrained optimization in the new basis
The new parameters are defined as follows
a
exp(e
JI
exp
1JI
exp(e
if
exp
V"d exp(Oi,d
This results in the following derivatives
O?(Xl
oO(a
jb i)aij
S1
1fi
st
f..li
f..li
IJ
When chained with the error signal backpropagated from the output these derivatives give us the direction in which to move the parameters of each HMM in order
to increase the log probability of the correct classification of the sequence
Experiments
To evaluate the relative merits of the RDN we compared it against an Alphanet
on a speaker identification task The data was taken from the CSLU Speaker
Recognition corpus It consisted of speakers uttering phrases consisting of
different sequences of connected digits recorded multiple times over the course
of recording sessions The data was pre-emphasized and Fourier transformed
in frames at a frame rate of lOms It was then filtered using 24 bandpass
mel-frequency scaled filters The log magnitude filter response was then used as the
feature vector for the HMM's This pre-processing reduced the data dimensionality
while retaining its spectral structure
While mel-cepstral coefficients are typically recommended for use with axis-aligned
Gaussians they destroy the spectral structure of the data and we would like to
allow for the possibility that of the many HMM's some of them will specialize on
particular sub-bands of the frequency domain They can do this by treating the
variance as a measure of the importance of a particular frequency band using
large variances for unimportant bands and small ones for bands to which they pay
particular attention
We compared the RDN with an Alphanet and three other models which were implemented as controls The first of these was a network with a similar architecture
to the RDN as shown in figure except that instead of fixed connections of
the hidden units have a set of adaptable weights to all of the HMM's We refer
to this network as a comparative density net A second control experiment
used an architecture similar to a CDN without the hidden layer there is a single
layer of adaptable weights directly connecting the HMM's with the softmax output
units We label this architecture a CDN-l The CDN-l differs from the Alphanet
in that each softmax output unit has adaptable connections to the HMM's and we
can vary the number of HMM's whereas the Alphanet has just one HMM per class
directly connected to each softmax output unit Finally we implemented a version
of a network similar to an Alphanet but using a mixture of Gaussians as the input density model The point of this comparison was to see if the HMM actually
achieves a benefit from modelling the temporal aspects of the speaker recognition
task
In each experiment an RDN constructed out of a set of 4-state HMM's was
compared to the four other networks all matched to have the same number of free
parameters except for the MoGnet In the case of the MoGnet we used the same
number of Gaussian mixture models as HMM's in the Alphanet each with the
same number of hidden states Thus it has fewer parameters because it is lacking
the transition probabilities of the HMM. We ran the experiment four times with
Alphanet
MaGnet
CDN
EJ
RDN
CDN-1
RDN
Alphanet
Architecture
CDN-1
in
gj
Ci
gj
CiO.5
MeG net
Architecture
CDN
in
Alphanet
a
RDN
CDN
a
MaGnet
Architecture
CDN-1
RDN
Alphanet
MeGnet
CDN
CDN-1
Architecture
Figure Results of the experiments for an RDN with and
24 HMM's
values of of and 24 For the Alphanet and MoGnet we varied the
number of states in the HMM's and the Gaussian mixtures respectively For the
CDN model we used the same number of 4-state HMM's as the RDN and varied
the number of units in the hidden layer of the network Since the CDN-1 network
has no hidden units we used the same number of HMM's as the RDN and varied
the number of states in the HMM. The experiments were repeated times with
different training-test set splits All the models were trained using 90 iterations of
a conjugate gradient optimization procedure
Results
The boxplot in figure shows the results of the classification performance on the
runs in each of the experiments Comparing the Alphanet and the RDN we
see that the RDN consistently outperforms the Alphanet In all four experiments
the difference in their performance under a paired t-test was significant at the level
This indicates that given a classification network with a fixed number of
parameters there is an advantage to using many small HMM and using all the
pairwise information about an observed sequence as opposed to using a network
with a single large HMM per class
In the third experiment involving the MoGnet we see that its performance is comparable to that of the Alphanet This suggests that the HMM's ability to model the
temporal structure of the data is not really necessary for the speaker classification
task as we have set it Nevertheless the performance of both the Alphanet and
3If we had done text-dependent speaker identification instead of multiple digit phrases
the MoGnet is less than the RDN.
Unfortunately the CDN and CDN-l networks perform much worse than we expected While we expected these models to perform similarly to the RDN it seems
that the optimization procedure takes much longer with these models This is probably because the small initial weights from the HMM's to the next layer severely
attenuate the backpropagated error derivatives that are used to train the HMM's
As a result the CDN networks do not converge properly in the time allowed
Conclusions
We have introduced relative density networks and shown that this method of discriminatively learning many small density models in place of a single density model
per class has benefits in classification performance In addition there may be a
small speed benefit to using many smaller HMM compared to a few big ones
Computing the probability of a sequence under an HMM is order O(TK where
is the length of the sequence and is the number of hidden states in the network
Thus smaller HMM can be evaluated faster However this is somewhat counterbalanced by the quadratic growth in the size of the hidden layer as increases
Acknowledgments
We would like to thank John Bridle Chris Williams Radford Neal Sam Roweis
Zoubin Ghahramani and the anonymous reviewers for helpful comments

<<----------------------------------------------------------------------------------------------------------------------->>

title: 1820-combining-ica-and-top-down-attention-for-robust-speech-recognition.pdf

Combining ICA and top-down attention
for robust speech recognition
Un-Min Bae and Soo-Young Lee
Department of Electrical Engineering and Computer Science
and Brain Science Research Center
Korea Advanced Institute of Science and Technology
Kusong-dong Yusong-gu Taejon Korea
bum@neuron.kaist.ac.kr sylee@ee.kaist.ac.kr
Abstract
We present an algorithm which compensates for the mismatches
between characteristics of real-world problems and assumptions of
independent component analysis algorithm To provide additional
information to the ICA network we incorporate top-down selective attention An MLP classifier is added to the separated signal
channel and the error of the classifier is backpropagated to the
ICA network This backpropagation process results in estimation
of expected ICA output signal for the top-down attention Then
the unmixing matrix is retrained according to a new cost function
representing the backpropagated error as well as independence It
modifies the density of recovered signals to the density appropriate
for classification For noisy speech signal recorded in real environments the algorithm improved the recognition performance and
showed robustness against parametric changes
Introduction
Independent Component Analysis ICA is a method for blind signal separation
ICA linearly transforms data to be statistically as independent from each other as
possible ICA depends on several assumptions such as linear mixing and
source independence which may not be satisfied in many real-world applications
In order to apply ICA to most real-world problems it is necessary either to release
of all assumptions or to compensate for the mismatches with another method
In this paper we present a complementary approach to compensate for the mismatches The top-down selective attention from a classifier to the ICA network
provides additional information of the signal-mixing environment A new cost function is defined to retrain the unmixing matrix of the ICA network considering the
propagated information Under a stationary mixing environment the averaged
adaptation by iterative feedback operations can adjust the feature space to be more
helpful to classification performance This process can be regarded as a selective
attention model in which input patterns are adapted according to top-down infor
mation The proposed algorithm was applied to noisy speech recognition in real
environments and showed the effectiveness of the feedback operations
The proposed algorithm
Feedback operations based on selective attention
As previously mentioned ICA supposes several assumptions For example one
assumption is a linearly mixing condition but in general there is inevitable nonlinearity of microphones to record input signals Such mismatches between the
assumptions of ICA and real mixing conditions cause unsuccessful separation of
sources To overcome this problem a method to supply valuable information to
the rcA network was proposed In the learning phase of ICA the unmixing matrix
is subject to the signal-mixing matrix not the input patterns Under stationary
mixing environment where the mixing matrix is fixed iteratively providing additional information of the mixing matrix can contribute to improving blind signal
separation performance The algorithm performs feedback operations from a classifier to the ICA network in the test phase which adapts the unmixing matrices
of ICA according to a newly defined measure considering both independence and
classification error This can result in adaptation of input space of the classifier and
so improve recognition performance This process is inspired from the selective attention model which calculates expected input signals according to top-down
information
In the test phase as shown in Figure ICA separates signal and noise and Melfrequency cepstral coefficients MFCCs extracted as a feature vector are delivered
to a classifier multi-layer perceptron After classification the error function
of the classifier is defined as
m1p
Ymlp,i
where tmlp,i is target value of the output neuron Ymlp,i In general the target
values are not known and should be determined from the outputs Ymlp Only the
target value of the highest output is set to and the others are set to when the
nonlinear function of the classifier is the bipolar sigmoid function The algorithm
performs gradient-descent calculation by error backpropagation To reduce the
error it computes the required changes of the input values of the classifier and
finally those of the unmixed signals of the ICA network Then the leaning rule
of the ICA algorithm should be changed considering these variations The newly
defined cost function of the ICA network includes the error backpropagated term
as well as the joint entropy Yica of the outputs Yica
Eica
H(Yica Utarget u)(Utarget
H(Yica
where are the estimate recovered sources and is a coefficient which represents the
relative importance of two terms The learning rule derived using gradient descent
on the cost function in is
ex I p(u)uH]W
where are the input signals of the rcA network The first term in is the
learning rule of ICA which is applicable to complex-valued data in the frequency
Input Speech
Block into Frames
Hamming Window
Fourier Transform
ICA
Retraining ICA
Linear-to-Mel Freq
Filter Bank Conversion
Mel-to-Linear Freq
Filter Bank Conversion
Frame Normalization
Frame Re-normalization
MLP Classification
MLP Backpropagation
Figure Real-world speech recognition with feedback operations from a classifier
to the ICA network
domain In real environments where substantial time delays occur the observed input signals are convolved mixtures of sources not linear mixtures and the
mixing model no longer is a matrix In this case blind signal separation using ICA
can be achieved in the frequency domain The complex score function is
tanh(Re{z tanh(Im{z
The procedure in the test phase is summarized as follows
For a test input perform the forward operation and classify the pattern
Define the error function of the classifier in and perform error backpropagation to find the required changes of unmixed signals of ICA.
Define the cost function of the ICA network in and update unmixing
matrix with the learning rule in Then go to step
The newly defined error function ofthe classifier in does not cause overfitting
problems because it is used for updating the unmixing matrix of ICA only once
If classification performance is good the averaged changes of the unmixing matrix
over the total input patterns can contribute to improving recognition performance
Considering the assumptions of ICA
The assumptions of ICA are summarized as follows
Figure a nonlinear mixing model due to the distortions of microphones
The sources are linearly mixed
The sources are mutually independent
At most one source is normally distributed
The number of sensors is equal to or greater than the number of sources
No sensor noise or only low additive noise signals are permitted
The assumptions and can be released if there are enough sensors The assumption is also negligible because the source distribution is usually approximated as
super-Gaussian or Laplacian distributions in the speech recognition problem
As to speech recognition in real mixing environments the nonlinearity of microphones is an inevitable problem Figure shows a nonlinear mixing model the
nonlinear functions and denote the distortions of microphones are original sources are observed signals and are the estimates of the recovered sources
If the sources 81 and 82 are mutually independent the random variables 8r and
are still independent each other and so are Voo and VlO. The density of Zl VOO+VlO
equals the convolution of the densities of Voo and VlO
Pvoo(Zl VlO)PVIO(VlO)dvlO
P(Zl
After all the observed signal Xl is not a linear mixture of two independent components due to the nonlinear distortion The assumption of source independence
is violated In this situation it is hard to expect what would be the leA solution
and to assert the solution is reliable Even if Xl has two independent components
which is the case of linear distortion of microphones there is a conflict between independence and source density approximation because the densities of independent
components of observed signals are different from those of original sources by
and and may be far from the density approximated by
The proposed algorithm can be a solution to this problem In the training phase a
classifier learns noiseless data and the density of Xl used for the learning is
p(xd
aoo
The second backpropagated term in the cost function changes the unmixing
matrix to adapt the density of unmixed signals to the density that the classifier
Table The recognition rates of noisy speech recorded with fighter noise
SNR
MLP
leA
The proposed
algorithm
lJlean
Training data
lOdB
5dB
lJlean
Test data
5dB
learned This can be a clue to what should be the leA solution Iterative operations
over the total data induce that the averaged change of the unmixing matrix becomes
roughly a function of the nonlinearity and not a certain density P(Sl
subject to every pattern
Noisy Speech Recognition in Real Environments
The proposed algorithm was applied to isolated-word speech recognition The input
data are convolved mixtures of speech and noise recorded in real environments The
speech data set consists of 75 Korean words of 48 speakers and fighter noise
and speech babbling noise were used as noise sources Each leA network has two
inputs and two outputs for the signal and noise sources Tables and show the
recognition results for the three methods MLP only MLP with standard leA
and the proposed algorithm Training data mean the data used for learning of
the classifier and Test data are the rest leA improves classification performance
compared to MLP only in the heavy-noise cases but in the cases of clean data
leA does not contribute to recognition and the recognition rates are lower than
those of MLP only The proposed algorithm shows better recognition performance
than standard leA for both training and test data Especially for the clean data
the proposed algorithm improves the recognition rates to be the same as those of
MLP only in most cases The algorithm reduces the false recognition rates by about
to in comparison with standard leA when signal to noise ratios SNRs
are or higher With such low noise the classification performance of MLP is
relatively reliable and MLP can provide the leA network for helpful information
However with heavy noise the recognition rates of MLP sharply decrease and the
error backpropagation can hardly provide valuable information to the leA network
The overall improvement for the training data is higher than that for the test data
This is because the the recognition performance of MLP is better for the training
data
As shown in Figure iterative feedback operations decrease the false recognition
rates and the variation of the unknown parameter in doesn't affect the
final recognition performance The variation of the learning rate for updating the
unmixing matrix also doesn't affect the final performance and it only influences on
the converging time to reach the final recognition rates The learning rate was fixed
regardless of SNR in all of the experiments
Discussion
The proposed algorithm is an approach to complement leA by providing additional
information based on top-down selective attention with a pre-trained MLP classifier The error backpropagation operations adapt the density of recovered signals
Table The recognition rates of noisy speech recorded with speech babbling noise
SNR
MLP
ICA
The proposed
algorithm
lJlean
Training data
15dtl lOdtl
5dtl
lJlean
Test data
15dtl 10dtl
5dtl
according to the new cost function of ICA. This can help ICA find the solution
proper for classification under the nonlinear and independence violations but this
needs the stationary condition For nonstationary environments a mixture model
like the ICA mixture model can be considered The ICA mixture model can
assign class membership to each environment category and separate independent
sources in each class To completely settle the nonlinearity problem in real environment it is necessary to introduce a scheme which models the nonlinearity such
as the distortions of microphones Multi-layered ICA can be an approach to model
nonlinearity
In the noisy recognition problem the proposed algorithm improved recognition performance compared to ICA alone Especially in moderate noise cases the algorithm
remarkably reduced the false recognition rates This is due to the high classification performance of the pre-trained MLP. In the case of heavy noise the expected
ICA output estimated from the top-down attention may not be accurate and the
selective attention does not help much It is natural that we only put attention to
familiar subjects Therefore more robust classifiers may be needed for signals with
heavy noise
Acknowledgments
This work was supported as a Brain Science Engineering Research Program
sponsored by Korean Ministry of Science and Technology

<<----------------------------------------------------------------------------------------------------------------------->>

title: 5953-natural-neural-networks.pdf

Natural Neural Networks
Guillaume Desjardins Karen Simonyan Razvan Pascanu Koray Kavukcuoglu
gdesjardins,simonyan,razp,korayk}@google.com
Google DeepMind London
Abstract
We introduce Natural Neural Networks a novel family of algorithms that speed up
convergence by adapting their internal representation during training to improve
conditioning of the Fisher matrix In particular we show a specific example that
employs a simple and efficient reparametrization of the neural network weights by
implicitly whitening the representation obtained at each layer while preserving
the feed-forward computation of the network Such networks can be trained efficiently via the proposed Projected Natural Gradient Descent algorithm PRONG
which amortizes the cost of these reparametrizations over many parameter updates and is closely related to the Mirror Descent online learning algorithm We
highlight the benefits of our method on both unsupervised and supervised learning tasks and showcase its scalability by training on the large-scale ImageNet
Challenge dataset
Introduction
Deep networks have proven extremely successful across a broad range of applications While their
deep and complex structure affords them a rich modeling capacity it also creates complex dependencies between the parameters which can make learning difficult via first order stochastic gradient
descent As long as SGD remains the workhorse of deep learning our ability to extract highlevel representations from data may be hindered by difficult optimization as evidenced by the boost
in performance offered by batch normalization on the Inception architecture
Though its adoption remains limited the natural gradient appears ideally suited to these difficult
optimization issues By following the direction of steepest descent on the probabilistic manifold
the natural gradient can make constant progress over the course of optimization as measured by the
Kullback-Leibler divergence between consecutive iterates Utilizing the proper distance measure ensures that the natural gradient is invariant to the parametrization of the model Unfortunately
its application has been limited due to its high computational cost Natural gradient descent NGD
typically requires an estimate of the Fisher Information Matrix FIM which is square in the number
of parameters and worse it requires computing its inverse Truncated Newton methods can avoid
explicitly forming the FIM in memory but they require an expensive iterative procedure to
compute the inverse Such computations can be wasteful as they do not take into account the highly
structured nature of deep models
Inspired by recent work on model reparametrizations our approach starts with a simple question can we devise a neural network architecture whose Fisher is constrained to be
identity This is an important question as SGD and NGD would be equivalent in the resulting
model The main contribution of this paper is in providing a simple theoretically justified network
reparametrization which approximates via first-order gradient descent a block-diagonal natural gradient update over layers Our method is computationally efficient due to the local nature of the
reparametrization based on whitening and the amortized nature of the algorithm Our second contribution is in unifying many heuristics commonly used for training neural networks under the roof
of the natural gradient while highlighting an important connection between model reparametrizations and Mirror Descent Finally we showcase the efficiency and the scalability of our method
across a broad-range of experiments scaling our method from standard deep auto-encoders to large
convolutional models on ImageNet[20 trained across multiple GPUs This is to our knowledge the
first-time a non-diagonal natural gradient algorithm is scaled to problems of this magnitude
The Natural Gradient
This section provides the necessary background and derives a particular form of the FIM whose
structure will be key to our efficient approximation While we tailor the development of our method
to the classification setting our approach generalizes to regression and density estimation
Overview
We consider the problem of fitting the parameters RN of a model p(y to an empirical
distribution under the log-loss We denote by the observation vector and its
associated label Concretely this stochastic optimization problem aims to solve
argmin log p(y
Defining the per-example loss as Stochastic Gradient Descent SGD performs the above
minimization by iteratively following the direction of steepest descent given by the column vector
Parameters are updated using the rule
where is a
learning rate An equivalent proximal form of gradient descent reveals the precise nature of
argmin ri
Namely each iterate is the solution to an auxiliary optimization problem where controls
the distance between consecutive iterates using an distance In contrast the natural gradient
relies on the KL-divergence between iterates a more appropriate distance measure for probability
distributions Its metric is determined by the Fisher Information matrix
@ log
@ log
@
@
the covariance of the gradients of the model log-probabilities wrt its parameters The natural
gradient direction is then obtained as rN See for a recent overview of the topic
Fisher Information Matrix for MLPs
We start by deriving the precise form of the Fisher for a canonical multi-layer perceptron MLP
composed of layers We consider the following deep network for binary classification though our
approach generalizes to an arbitrary number of output classes
p(y hL
h1
fL WL hL
f1 b1
bL
The parameters of the MLP denoted b1 WL bL are the weights Wi RNi Ni
connecting layers and and the biases bi RNi fi is an element-wise non-linear function
Let us define to be the backpropagated gradient through the i-th non-linearity We ignore the
off block-diagonal components of the Fisher matrix and focus on the block FWi corresponding to
interactions between parameters of layer This block takes the form
FWi vec hTi vec hTi
y?p
where vec(X is the vectorization function yielding a column vector from the rows of matrix
Assuming that
and activations hi
are independent random variables we can write
FWi hi
y?p
m)hi
Figure A 2-layer natural neural network Illustration of the projections involved in PRONG
where is the element at row and column of matrix and is the i-th element of vector
FWi is the entry in the Fisher capturing interactions between parameters Wi
and Wj Our hypothesis verified experimentally
in Sec. is that we can greatly improve
conditioning of the Fisher by enforcing that hi hTi I for all layers of the network despite
ignoring possible correlations in the and off block diagonal terms of the Fisher
Projected Natural Gradient Descent
This section introduces Whitened Neural Networks which perform approximate whitening
of their internal representations We begin by presenting a novel whitened neural layer with the
assumption that the network statistics E[hi and E[hi hTi are fixed We then show
how these layers can be adapted to efficiently track population statistics over the course of training
The resulting learning algorithm is referred to as Projected Natural Gradient Descent PRONG We
highlight an interesting connection between PRONG and Mirror Descent in Section
A Whitened Neural Layer
The building block of WNN is the following neural layer
hi
fi Vi Ui
hi
ci
di
Compared to we have introduced an explicit centering parameter ci RNi equal to
which ensures that the input to the dot product has zero mean in expectation This is analogous to the centering reparametrization for Deep Boltzmann Machines The weight matrix
Ui RNi Ni is a per-layer PCA-whitening matrix whose rows are obtained from an eigendecomposition of
diag
iT Ui diag
iT
The hyper-parameter is a regularization term controlling the maximal multiplier on the learning
rate or equivalently the size of the trust region The parameters Vi RNi Ni and di RNi are
analogous to the canonical parameters of a neural network as introduced in though operate
in the space of whitened unit activations Ui hi ci This layer can be stacked to form a deep
neural network having layers with model parameters d1 VL dL and whitening
coefficients c0 UL cL as depicted in
Though the above layer might appear over-parametrized at first glance we crucially do not learn
the whitening coefficients via loss minimization but instead estimate them directly from the model
statistics These coefficients are thus constants from the point of view of the optimizer and simply
serve to improve conditioning of the Fisher with respect to the parameters denoted Indeed
using the same derivation
that led
to we can see that the block-diagonal terms of now
involve terms Ui hi hi which equals identity by construction
Updating the Whitening Coefficients
As the whitened model parameters evolve during training so do the statistics and For our
model to remain well conditioned the whitening coefficients must be updated at regular intervals
Algorithm Projected Natural Gradient Descent
Input training set initial parameters
Hyper-parameters reparam frequency number of samples Ns regularization term
Ui
I ci
repeat
if mod(t then
amortize cost of lines
for all layers do
Compute canonical parameters Wi Vi Ui bi di Wi ci proj
Estimate and using Ns samples from D.
Update ci from and Ui from eigen decomp of
update
Update parameters Vi
Wi Ui di
bi
proj
end for
end if
Perform SGD update wrt using samples from D.
until convergence
while taking care not to interfere with the convergence properties of gradient descent This can be
achieved by coupling updates to with corresponding updates to such that the overall function
implemented by the MLP remains unchanged by preserving the product Vi Ui before and
after each update to the whitening coefficients with an analoguous constraint on the biases
Unfortunately while estimating the mean and diag(?i could be performed online over a minibatch of samples as in the recent Batch Normalization scheme estimating the full covariance
matrix will undoubtedly require a larger number of samples While statistics could be accumulated
online via an exponential moving average as in RMSprop or K-FAC the cost of the eigendecomposition required for computing the whitening matrix Ui remains cubic in the layer size
In the simplest instantiation of our method we exploit the smoothness of gradient descent by simply
amortizing the cost of these operations over consecutive updates SGD updates in the whitened
model will be closely aligned to NGD immediately following the reparametrization The quality
of this approximation will degrade over time until the subsequent reparametrization The resulting
algorithm is shown in the pseudo-code of Algorithm We can improve upon this basic amortization scheme by updating the whitened parameters using a per-batch diagonal natural gradient update whose statistics are computed online In our framework this can be implemented
via the reparametrization Wi Vi Di Ui where Di is a diagonal matrix updated such that
Di Ui hi for each minibatch Updates to Di can be compensated for exactly and
cheaply by scaling the rows of Ui and columns of Vi accordingly A simpler implementation of
this idea is to combine PRONG with batch-normalization which we denote as PRONG
Duality and Mirror Descent
There is an inherent duality between the parameters of our whitened neural layer and the parameters of a canonical model Indeed there exist linear projections and which map
from canonical parameters to whitened parameters and vice-versa corresponds to line
of Algorithm while corresponds to line This duality between and reveals a
close connection between PRONG and Mirror Descent
Mirror Descent is an online learning algorithm which generalizes the proximal form of gradient descent to the class of Bregman divergences where and is a
strictly convex and differentiable function Replacing the distance by mirror descent solves
the proximal problem of by applying first-order updates in a dual space and then projecting back onto the primal space Defining and with the complex
conjugate of the mirror descent updates are given by
Figure Fisher matrix for a small MLP before and after the first reparametrization Best viewed in
colour Condition number of the FIM during training relative to the initial conditioning All models where
initialized such that the initial conditioning was the same and learning rate where adjusted such that they reach
roughly the same training error in the given time
It is well known that the natural gradient is a special case of MD where the distance
generating function is chosen to be
The mirror updates are somewhat unintuitive however Why is the gradient applied to the dual
space if it has been computed in the space of parameters
This is where PRONG relates to MD. It
is trivial to show that using the function instead of the previously defined
enables us to directly update the dual parameters using the gradient computed directly in the
dual space Indeed the resulting updates can be shown to implement the natural gradient and are
thus equivalent to the updates of with the appropriate choice of
and
correspond to the projections and used by PRONG
The operators
to map from the canonical neural parameters to those of the whitened layers As illustrated
in the advantage of this whitened form of MD is that one may amortize the cost of the
projections over several updates as gradients can be computed directly in the dual parameter space
Related Work
This work extends the recent contributions of in formalizing many commonly used heuristics
for training MLPs the importance of zero-mean activations and gradients as well as the
importance of normalized variances in the forward and backward passes More recently
Vatanen extended their previous work by introducing a multiplicative constant
to the centered non-linearity In contrast we introduce a full whitening matrix Ui and focus on
whitening the feedforward network activations instead of normalizing a geometric mean over units
and gradient variances
The recently introduced batch normalization scheme quite closely resembles a diagonal
version of PRONG the main difference being that BN normalizes the variance of activations before
the non-linearity as opposed to normalizing the latent activations by looking at the full covariance
Furthermore BN implements normalization by modifying the feed-forward computations thus requiring the method to backpropagate through the normalization operator A diagonal version of
PRONG also bares an interesting resemblance to RMSprop in that both normalization terms
involve the square root of the FIM. An important distinction however is that PRONG applies this
update in the whitened parameter space thus preserving the natural gradient interpretation
As the Fisher and thus
which we drop for clarity
depend on the parameters these should be indexed with a time superscript
Figure Optimizing a deep auto-encoder on MNIST Impact of eigenvalue regularization term
Impact of amortization period showing that initialization with the whitening reparametrization is important
for achieving faster learning and better error rate Training error vs number of updates Training error
vs cpu-time Plots show that PRONG achieves better error rate both in number of updates and wall clock
K-FAC is closely related to PRONG and was developed concurrently to our method It targets
the same layer-wise block-diagonal of the Fisher approximating each block as in Unlike
our method however KFAC does not approximate the covariance of backpropagated gradients as
the identity and further estimates the required statistics using exponential moving averages unlike our approach based on amortization Similar techniques can be found in the preconditioning
of the Kaldi speech recognition toolkit By modeling the Fisher matrix as the covariance of
a sparsely connected Gaussian graphical model FANG represents a general formalism for
exploiting model structure to efficiently compute the natural gradient One application to neural
networks is in decorrelating gradients across neighbouring layers
A similar algorithm to PRONG was later found in where it appeared simply as a thought
experiment but with no amortization or recourse for efficiently computing
Experiments
We begin with a set of diagnostic experiments which highlight the effectiveness of our method at
improving conditioning We also illustrate the impact of the hyper-parameters and controlling
the frequency of the reparametrization and the size of the trust region Section evaluates PRONG
on unsupervised learning problems where models are both deep and fully connected Section
then moves onto large convolutional models for image classification Experimental details such as
model architecture or hyper-parameter configurations can be found in the supplemental material
Introspective Experiments
Conditioning To provide a better understanding of the approximation made by PRONG we train
a small 3-layer MLP with tanh non-linearities on a downsampled version of MNIST
The model size was chosen in order for the full Fisher to be tractable shows the FIM
of the middle hidden layers before and after whitening the model activations we took the absolute
value of the entries to improve visibility 2c depicts the evolution of the condition number
of the FIM during training measured as a percentage of its initial value before the first whitening
reparametrization in the case of PRONG We present such curves for SGD RMSprop batch normalization and PRONG The results clearly show that the reparametrization performed by PRONG
improves conditioning reduction of more than These observations confirm our initial assumption namely that we can improve conditioning of the block diagonal Fisher by whitening
activations alone
Sensitivity of Hyper-Parameters Figures 3b highlight the effect of the eigenvalue regularization term and the reparametrization interval The experiments were performed on the best
Figure Classification error on CIFAR-10 and ImageNet On CIFAR-10 PRONG achieves better
test error and converges faster On ImageNet PRONG achieves comparable validation error while maintaining a faster covergence rate
performing auto-encoder of Section on the MNIST dataset Figures 3b plot the reconstruction
error on the training set for various values of and As determines a maximum multiplier on the
learning rate learning becomes extremely sensitive when this learning rate is high2 For smaller step
sizes however lowering can yield significant speedups often converging faster than simply using a
larger learning rate This confirms the importance of the manifold curvature for optimization lower
allows for different directions to be scaled drastically different according to their corresponding
curvature Fig 3b compares the impact of for models having a proper whitened initialization
solid lines to models being initialized with a standard fan-in initialization dashed lines
These results are quite surprising in showing the effectiveness of the whitening reparametrization
as a simple initialization scheme That being said performance can degrade due to ill conditioning
when becomes excessively large
Unsupervised Learning
Following Martens we compare PRONG on the task of minimizing reconstruction error of a
dense 8-layer auto-encoder on the MNIST dataset Reconstruction error with respect to updates and
wallclock time are shown in We can see that PRONG significantly outperforms the
baseline methods by up to an order of magnitude in number of updates With respect to wallclock
our method significantly outperforms the baselines in terms of time taken to reach a certain error
threshold despite the fact that the runtime per epoch for PRONG was that of SGD compared
to batch normalization SGD and RMSprop SGD Note that these timing numbers reflect
performance under the optimal choice of hyper-parameters which in the case of batch normalization
yielded a batch size of compared to for all other methods Further breaking down the
performance of the runtime of PRONG was spent performing the whitening reparametrization
compared to for estimating the per layer means and covariances This confirms that amortization
is paramount to the success of our method.3
Supervised Learning
We now evaluate our method for training deep supervised convolutional networks for object recognition Following we perform whitening across feature maps only that is we treat pixels in a
given feature map as independent samples This allows us to implement the whitened neural layer
as a sequence of two convolutions where the first is by a whitening filter PRONG is compared
to SGD RMSprop and batch normalization with each algorithm being accelerated via momentum
Results are presented on CIFAR-10 and the ImageNet Challenge ILSVRC12 datasets In
both cases learning rates were decreased using a waterfall annealing schedule which divided the
learning rate by when the validation error failed to improve after a set number of evaluations
Unstable combinations of learning rates and are omitted for clarity
We note that our whitening implementation is not optimized as it does not take advantage of GPU acceleration Runtime is therefore expected to improve as we move the eigen-decompositions to GPU.
CIFAR-10 We now evaluate PRONG on CIFAR-10 using a deep convolutional model inspired
by the VGG architecture The model was trained on 24 24 random crops with random
horizontal reflections Model selection was performed on a held-out validation set of 5k examples
Results are shown in With respect to training error PRONG and BN seem to offer similar
speedups compared to SGD with momentum Our hypothesis is that the benefits of PRONG are more
pronounced for densely connected networks where the number of units per layer is typically larger
than the number of maps used in convolutional networks Interestingly PRONG generalized better
achieving test error for batch normalization This reflects the findings of
which showed how NGD can leverage unlabeled data for better generalization the unlabeled data
here comes from the extra crops and reflections observed when estimating the whitening matrices
ImageNet Challenge Dataset Our final set of experiments aims to show the scalability of our
method We applied our natural gradient algorithm to the large-scale ILSVRC12 dataset images labelled into categories using the Inception architecture In order to scale to problems
of this size we parallelized our training loop so as to split the processing of a single minibatch of
size across multiple GPUs Note that PRONG can scale well in this setting as the estimation
of the mean and covariance parameters of each layer is also embarassingly parallel Eight GPUs
were used for computing gradients and estimating model statistics though the eigen decomposition
required for whitening was itself not parallelized in the current implementation Given the difficulty
of the task we employed the enhanced version of the algorithm PRONG as simple periodic
whitening of the model proved to be unstable Figure shows that batch normalisation and
PRONG converge to approximately the same top-1 validation error vs respectively
for similar cpu-time In comparison SGD achieved a validation error of PRONG however
exhibits much faster convergence initially after updates it obtains around error compared
to for BN alone We stress that the ImageNet results are somewhat preliminary While our
top-1 error is higher than reported in we used a much less extensive data augmentation
pipeline We are only beginning to explore what natural gradient methods may achieve on these
large scale optimization problems and are encouraged by these initial findings
Discussion
We began this paper by asking whether convergence speed could be improved by simple model
reparametrizations driven by the structure of the Fisher matrix From a theoretical and experimental
perspective we have shown that Whitened Neural Networks can achieve this via a simple scalable
and efficient whitening reparametrization They are however one of several possible instantiations
of the concept of Natural Neural Networks In a previous incarnation of the idea we exploited a
similar reparametrization to include whitening of backpropagated gradients4 We favor the simpler
approach presented in this paper as we generally found the alternative less stable for deep networks
This may be due to the difficulty in estimating gradient covariances in lower layers a problem which
seems to mirror the famous vanishing gradient problem
Maintaining whitened activations may also offer additional benefits from the point of view of model
compression and generalization By virtue of whitening the projection Ui hi forms an ordered representation having least and most significant bits The sharp roll-off in the eigenspectrum of
may explain why deep networks are ammenable to compression Similarly one could envision
spectral versions of Dropout where the dropout probability is a function of the eigenvalues
Alternative ways of orthogonalizing the representation at each layer should also be explored via alternate decompositions of or perhaps by exploiting the connection between linear auto-encoders
and PCA. We also plan on pursuing the connection with Mirror Descent and further bridging the
gap between deep learning and methods from online convex optimization
Acknowledgments
We are extremely grateful to Shakir Mohamed for invaluable discussions and feedback in the preparation of
this manuscript We also thank Philip Thomas Volodymyr Mnih Raia Hadsell Sergey Ioffe and Shane Legg
for feedback on the paper
The weight matrix can be parametrized as Wi RiT Vi Ui
with Ri the whitening matrix for

<<----------------------------------------------------------------------------------------------------------------------->>

title: 6270-stochastic-multiple-choice-learning-for-training-diverse-deep-ensembles.pdf

Stochastic Multiple Choice Learning for
Training Diverse Deep Ensembles
Stefan Lee
Virginia Tech
steflee@vt.edu
Senthil Purushwalkam
Carnegie Mellon University
spurushw@andrew.cmu.edu
David Crandall
Indiana University
djcran@indiana.edu
Michael Cogswell
Virginia Tech
cogswell@vt.edu
Viresh Ranjan
Virginia Tech
rviresh@vt.edu
Dhruv Batra
Virginia Tech
dbatra@vt.edu
Abstract
Many practical perception systems exist within larger processes that include interactions with users or additional components capable of evaluating the quality of
predicted solutions In these contexts it is beneficial to provide these oracle mechanisms with multiple highly likely hypotheses rather than a single prediction In this
work we pose the task of producing multiple outputs as a learning problem over an
ensemble of deep networks introducing a novel stochastic gradient descent based
approach to minimize the loss with respect to an oracle Our method is simple
to implement agnostic to both architecture and loss function and parameter-free
Our approach achieves lower oracle error compared to existing methods on a wide
range of tasks and deep architectures We also show qualitatively that the diverse
solutions produced often provide interpretable representations of task ambiguity
Introduction
Perception problems rarely exist in a vacuum Typically problems in Computer Vision Natural
Language Processing and other AI subfields are embedded in larger applications and contexts For
instance the task of recognizing and segmenting objects in an image semantic segmentation
might be embedded in an autonomous vehicle while the task of describing an image with a
sentence image captioning might be part of a system to assist visually-impaired users
In these scenarios the goal of perception is often not to generate a single output but a set of plausible
hypotheses for a downstream process such as a verification component or a human operator These
downstream mechanisms may be abstracted as oracles that have the capability to pick the correct
solution from this set Such a learning setting is called Multiple Choice Learning MCL where
the goal for the learner is to minimize oracle loss achieved by a set of solutions More formally
given a dataset of input-output pairs the goal of classical supervised
learning is to search for a mapping that minimizes a task-dependent loss
capturing the error between the actual labeling and predicted labeling y?i In this setting the learned
function makes a single prediction for each input and pays a penalty for that prediction In contrast
Multiple Choice Learning seeks to learn a mapping that produces solutions
Y?i
yi1 y?iM such that oracle loss minm y?im is minimized
In this work we fix the form of this mapping to be the union of outputs from an ensemble of
predictors such that f2 fM and address the task of training ensemble
members f1 fM such that minimizes oracle loss Under our formulation different ensemble
members are free to specialize on subsets of the data distribution so that collectively they produce a
set of outputs which covers the space of high probability predictions well
Conference on Neural Information Processing Systems NIPS Barcelona Spain
Horse
Cow
A
couple
of
birds
that
are
standing
in
the
grass
A
bird
perched
on
top
of
a
tree
branch
A
bird
perched
on
a
tree
branch
in
the
sky
Figure Single-prediction based models often produce solutions with low expected loss in the face of ambiguity
however these solutions are often unrealistic or do not reflect the image content well row Instead we train
ensembles under a unified loss which allows each member to produce different outputs reflecting multi-modal
beliefs row We evaluate our method on image classification segmentation and captioning tasks
Diverse solution sets are especially useful for structured prediction problems with multiple reasonable
interpretations only one of which is correct Situations that often arise in practical systems include
Implicit class confusion The label space of many classification problems is often an arbitrary
quantization of a continuous space For example a vision system may be expected to classify
between tables and desks despite many real-world objects arguably belonging to both classes By
making multiple predictions this implicit confusion can be viewed explicitly in system outputs
Ambiguous evidence Often there is simply not enough information to make a definitive prediction
For example even a human expert may not be able to identify a fine-grained class particular
breed of dog given an occluded or distant view but they likely can produce a small set of reasonable
guesses In such cases the task of producing a diverse set of possibilities is more clearly defined
than producing one correct answer
Bias towards the mode Many models have a tendency to exhibit mode-seeking behaviors as a
way to reduce expected loss over a dataset a conversation model frequently producing I
don?t know By making multiple predictions a system can improve coverage of lower density
areas of the solution space without sacrificing performance on the majority of examples
In other words by optimizing for the oracle loss a multiple-prediction learner can respond to
ambiguity much like a human does by making multiple guesses that capture multi-modal beliefs
In contrast a single-prediction learner is forced to produce a solution with low expected loss in
the face of ambiguity Figure illustrates how this can produce solutions that are not useful in
practice In semantic segmentation for example this problem often causes objects to be predicted
as a mixture of multiple classes like the horse-cow shown in the figure In image captioning
minimizing expected loss encourages generic sentences that are safe with respect to expected error
but not very informative For example Figure shows two pairs of images each having different
image content but very similar generic captions the model knows it is safe to assume that birds are
on branches and that cakes are eaten with forks
In this paper we generalize the Multiple Choice Learning paradigm to jointly learn ensembles
of deep networks that minimize the oracle loss directly We are the first to adapt these ideas to deep
networks and we present a novel training algorithm that avoids costly retraining and learning
difficulty of past methods Our primary technical contribution is the formulation of a stochastic
block gradient descent optimization approach well-suited to minimizing the oracle loss in ensembles
of deep networks which we call Stochastic Multiple Choice Learning sMCL Our formulation
is applicable to any model trained with stochastic gradient descent is agnostic to the form of the task
dependent loss is parameter-free and is time efficient training all ensemble members concurrently
We demonstrate the broad applicability and efficacy of sMCL for training diverse deep ensembles
with interpretable emergent expertise on a wide range of problem domains and network architectures
including Convolutional Neural Network CNN ensembles for image classification FullyConvolutional Network FCN ensembles for semantic segmentation and combined CNN
and Recurrent Neural Network RNN ensembles for image captioning We provide detailed
analysis of the training and output behaviors of the resulting ensembles demonstrating how ensemble
member specialization and expertise emerge automatically when trained using sMCL Our method
outperforms existing baselines and produces sets of outputs with high oracle performance
Related Work
Ensemble Learning Much of the existing work on training ensembles focuses on diversity between
member models as a means to improve performance by decreasing error correlation This is often
accomplished by resampling existing training data for each member model or by producing
artificial data that encourages new models to be decorrelated with the existing ensemble Other
approaches train or combine ensemble members under a joint loss More recently work of
Hinton and Ahmed explores using generalist network performance statistics to
inform the design of ensemble-of-expert architectures for classification In contrast sMCL discovers
specialization as a consequence of minimizing oracle loss Importantly most existing methods do
not generalize to structured output labels while sMCL seamlessly adapts discovering different
task-dependent specializations automatically
Generating Multiple Solutions There is a large body of work on the topic of extracting multiple
diverse solutions from a single model 23 however these approaches are designed for
probabilistic structured-output models and are not directly applicable to general deep architectures
Most related to our approach is the work of Guzman-Rivera which explicitly minimizes
oracle loss over the outputs of an ensemble formalizing this setting as the Multiple Choice Learning
MCL paradigm They introduce a general alternating block coordinate descent training approach
which requires retraining models multiple times More recently Dey reformulated this
problem as a submodular optimization task in which ensemble members are learned sequentially
in a boosting-like manner to maximize marginal gain in oracle performance Both these methods
require either costly retraining or sequential training making them poorly suited to modern deep
architectures that can take weeks to train To address this serious shortcoming and to provide the first
practical algorithm for training diverse deep ensembles we introduce a stochastic gradient descent
SGD based algorithm to train ensemble members concurrently
Multiple-Choice Learning as Stochastic Block Gradient Descent
We consider the task of training an ensemble of differentiable learners that together produce a set of
solutions with minimal loss with respect to an oracle that selects only the lowest-error prediction
Notation We use to denote the set Given a training set of input-output pairs
our goal is to learn a function which maps
each input to outputs We fix the form of to be an ensemble of learners such that
fM For some task-dependent loss which measures the error
between true and predicted outputs and we define the oracle loss of over the dataset as
LO
min fm
Minimizing Oracle Loss with Multiple Choice Learning In order to directly minimize the oracle
loss for an ensemble of learners Guzman-Rivera present an objective which forms a
potentially tight upper-bound This objective replaces the min in the oracle loss with indicator
variables pi,m
where pi,m is if predictor has the lowest error on example
argmin
fm pm,i
pi,m fm
pi,m
pi,m
The resulting minimization is a constrained joint optimization over ensemble parameters and datapoint assignments The authors propose an alternating block algorithm shown in Algorithm to
approximately minimize this objective Similar to K-Means or hard-EM this approach alternates
between assigning examples to their min-loss predictors and training models to convergence on the
partition of examples assigned to them Note that this approach is not feasible with training deep
networks since modern architectures can take weeks or months to train a single model once
Stochastic Multiple Choice Learning To overcome this shortcoming we propose a stochastic
algorithm for differentiable learners which interleaves the assignment step with batch updates in
Figure The MCL approach of Alg requires costly retraining while our sMCL method Alg works
within standard SGD solvers training all ensemble members under a joint loss
stochastic gradient descent Consider the partial derivative of the objective in with respect to
the output of the mth individual learner on example
LO
fm
pi,m
fm
fm
Notice that if fm is the minimum error predictor for example then pi,m and the gradient
term is the same as if training a single model otherwise the gradient is zero This behavior lends
itself to a straightforward optimization strategy for learners trained by SGD based solvers For each
batch we pass the examples through the learners calculating losses from each ensemble member for
each example During the backward pass the gradient of the loss for each example is backpropagated
only to the lowest error predictor on that example with ties broken arbitrarily
This approach which we call Stochastic Multiple Choice Learning sMCL is shown in Algorithm
sMCL is generalizable to any learner trained by stochastic gradient descent and is thus applicable to
an extensive range of modern deep networks Unlike the iterative training schedule of MCL sMCL
ensembles need only be trained to convergence once in parallel sMCL is also agnostic to the exact
form of loss function such that it can be applied without additional effort on a variety of problems
Experiments
In this section we present results for sMCL ensembles trained for the tasks and deep architectures
shown in Figure These include CNN ensembles for image classification FCN ensembles for
semantic segmentation and a CNN+RNN ensembles for image caption generation
Baselines Many existing general techniques for inducing diversity are not directly applicable to deep
networks We compare our proposed method against
Classical ensembles in which each model is trained under an independent loss with differing
random initializations We will refer to these as Indp ensembles in figures
MCL that alternates between training models to convergence on assigned examples and
allocating examples to their lowest error model We repeat this process for meta-iterations and
initialize ensembles with different random weights We find MCL performs similarly to sMCL
on small classification tasks however MCL performance drops substantially on segmentation and
captioning tasks Unlike sMCL which can effectively reassign an example once per epoch MCL
only does this after convergence limiting its capacity to specialize compared to sMCL We also
note that sMCL is 5x faster than MCL where the factor is the result of choosing meta-iterations
other applications may require more further increasing the gap
Dey train models sequentially in a boosting-like fashion each time reweighting examples
to maximize marginal increase of the evaluation metric We find these models saturate quickly as
the ensemble size grows As performance increases the marginal gain and therefore the weights
approach zero With low weights the average gradient backpropagated for stochastic learners drops
substantially reducing the rate and effectiveness of learning without careful tuning To compute
Convolutional classification
model of for CIFAR10
Fully-convolutional segmentation model of Long
CNN+RNN based captioning
model of Karpathy
Figure We experiment with three problem domains using the various architectures shown above
weights requires an error measure bounded above by accuracy for classification and IoU
for segmentation satisfy this the CIDEr-D score divided by guarantees this for captioning
Oracle Evaluation We present results as oracle versions of the task-dependent performance metrics
These oracle metrics report the highest score over all outputs for a given input For example in
classification tasks oracle accuracy is exactly the top-k criteria of ImageNet whether at
least one of the outputs is the correct label Likewise the oracle intersection over union IoU is the
highest IoU between the ground truth segmentation and any one of the outputs Oracle metrics allow
the evaluation of multiple-prediction systems separately from downstream re-ranking or selection
systems and have been extensively used in previous work 23
Our experiments convincingly demonstrate the broad applicability and efficacy of sMCL for training
diverse deep ensembles In all three experiments sMCL significantly outperforms classical ensembles
Dey typical improvements of and MCL while providing a 5x speedup over MCL
Our analysis shows that the exact same algorithm sMCL leads to the automatic emergence of
different interpretable notions of specializations among ensemble members
Image Classification
Model We begin our experiments with sMCL on the CIFAR10 dataset using the small convolutional neural network CIFAR10-Quick provided with the Caffe deep learning framework
CIFAR10 is a ten way classification task with small images For these experiments the

<<----------------------------------------------------------------------------------------------------------------------->>

title: 5059-compete-to-compute.pdf

Compete to Compute
Rupesh Kumar Srivastava Jonathan Masci Sohrob Kazerounian
Faustino Gomez J?rgen Schmidhuber
IDSIA USI-SUPSI
Manno?Lugano Switzerland
rupesh jonathan sohrob tino juergen}@idsia.ch
Abstract
Local competition among neighboring neurons is common in biological neural networks In this paper we apply the concept to gradient-based
backprop-trained artificial multilayer NNs. NNs with competing linear
units tend to outperform those with non-competing nonlinear units and
avoid catastrophic forgetting when training sets change over time
Introduction
Although it is often useful for machine learning methods to consider how nature has arrived
at a particular solution it is perhaps more instructive to first understand the functional
role of such biological constraints Indeed artificial neural networks which now represent
the state-of-the-art in many pattern recognition tasks not only resemble the brain in a
superficial sense but also draw on many of its computational and functional properties
One of the long-studied properties of biological neural circuits which has yet to fully impact
the machine learning community is the nature of local competition That is a common
finding across brain regions is that neurons exhibit on-center off-surround organization
and this organization has been argued to give rise to a number of interesting
properties across networks of neurons such as winner-take-all dynamics automatic gain
control and noise suppression
In this paper we propose a biologically inspired mechanism for artificial neural networks
that is based on local competition and ultimately relies on local winner-take-all LWTA
behavior We demonstrate the benefit of LWTA across a number of different networks and
pattern recognition tasks by showing that LWTA not only enables performance comparable
to the state-of-the-art but moreover helps to prevent catastrophic forgetting common
to artificial neural networks when they are first trained on a particular task then abruptly
trained on a new task This property is desirable in continual learning wherein learning
regimes are not clearly delineated Our experiments also show evidence that a type of
modularity emerges in LWTA networks trained in a supervised setting such that different
modules subnetworks respond to different inputs This is beneficial when learning from
multimodal data distributions as compared to learning a monolithic model
In the following we first discuss some of the relevant neuroscience background motivating
local competition then show how we incorporate it into artificial neural networks and
how LWTA as implemented here compares to alternative methods We then show how
LWTA networks perform on a variety of tasks and how it helps buffer against catastrophic
forgetting
Neuroscience Background
Competitive interactions between neurons and neural circuits have long played an important
role in biological models of brain processes This is largely due to early studies showing that
many cortical and sub-cortical hippocampal and cerebellar regions of the
brain exhibit a recurrent on-center off-surround anatomy where cells provide excitatory
feedback to nearby cells while scattering inhibitory signals over a broader range Biological
modeling has since tried to uncover the functional properties of this sort of organization
and its role in the behavioral success of animals
The earliest models to describe the emergence of winner-take-all WTA behavior from local
competition were based on Grossberg?s shunting short-term memory equations which
showed that a center-surround structure not only enables WTA dynamics but also contrast
enhancement and normalization Analysis of their dynamics showed that networks with
slower-than-linear signal functions uniformize input patterns linear signal functions preserve
and normalize input patterns and faster-than-linear signal functions enable WTA dynamics
Sigmoidal signal functions which contain slower-than-linear linear and faster-than-linear
regions enable the supression of noise in input patterns while contrast-enhancing normalizing and storing the relevant portions of an input pattern form of soft WTA The
functional properties of competitive interactions have been further studied to show among
other things the effects of distance-dependent kernels inhibitory time lags development of self-organizing maps and the role of WTA networks in attention
Biological models have also been extended to show how competitive interactions in spiking
neural networks give rise to soft WTA dynamics as well as how they may be efficiently
constructed in VLSI
Although competitive interactions and WTA dynamics have been studied extensively in the
biological literature it is only more recently that they have been considered from computational or machine learning perspectives For example Maas showed that feedforward
neural networks with WTA dynamics as the only non-linearity are as computationally powerful as networks with threshold or sigmoidal gates and networks employing only soft
WTA competition are universal function approximators Moreover these results hold even
when the network weights are strictly positive?a finding which has ramifications for our
understanding of biological neural circuits as well as the development of neural networks
for pattern recognition The large body of evidence supporting the advantages of locally
competitive interactions makes it noteworthy that this simple mechanism has not provoked
more study by the machine learning community Nonetheless networks employing local
competition have existed since the late and along with serve as a primary
inspiration for the present work More recently maxout networks have leveraged locally
competitive interactions in combination with a technique known as dropout to obtain
the best results on certain benchmark problems
Networks with local winner-take-all blocks
This section describes the general network architecture with locally competing neurons
The network consists of blocks which are organized into layers Figure Each block
bi contains computational units neurons and produces an output vector
determined by the local interactions between the individual neuron activations in the block
yij g(h1i h2i hni
where is the competition/interaction function encoding the effect of local interactions
in each block and hji is the activation of the j-th neuron in block computed by
hi wij
where is the input vector from neurons in the previous layer wij is the weight vector of
neuron in block and is a generally non-linear activation function The output
activations are passed as inputs to the next layer In this paper we use the winner-take-all
interaction function inspired by studies in computational neuroscience In particular we
use the hard winner-take-all function
hi if hji hki
yij
otherwise
In the case of multiple winners ties are broken by index precedence In order to investigate the capabilities of the hard winner-take-all interaction function in isolation
Figure A Local Winner-Take-All LWTA network with blocks of size two showing the
winning neuron in each block shaded for a given input example Activations flow forward
only through the winning neurons errors are backpropagated through the active neurons
Greyed out connections do not propagate activations The active neurons form a subnetwork
of the full network which changes depending on the inputs
identity is used for the activation function in equation The difference between this
Local Winner Take All LWTA network and a standard multilayer perceptron is that no
non-linear activation functions are used and during the forward propagation of inputs local
competition between the neurons in each block turns off the activation of all neurons except
the one with the highest activation During training the error signal is only backpropagated
through the winning neurons
In a LWTA layer there are as many neurons as there are blocks active at any one time for
a given input pattern1 We denote a layer with blocks of size as LWTA-n For each input
pattern presented to a network only a subgraph of the full network is active the highlighted neurons and synapses in figure Training on a dataset consists of simultaneously
training an exponential number of models that share parameters as well as learning which
model should be active for each pattern Unlike networks with sigmoidal units where all of
the free parameters need to be set properly for all input patterns only a subset is used for
any given input so that patterns coming from very different sub-distributions can potentially be modelled more efficiently through specialization This modular property is similar
to that of networks with rectified linear units ReLU which have recently been shown to
be very good at several learning tasks links with ReLU are discussed in section
Comparison with related methods
Max-pooling
Neural networks with max-pooling layers have been found to be very useful especially
for image classification tasks where they have achieved state-of-the-art performance
These layers are usually used in convolutional neural networks to subsample the representation obtained after convolving the input with a learned filter by dividing the representation
into pools and selecting the maximum in each one Max-pooling lowers the computational
burden by reducing the number of connections in subsequent convolutional layers and adds
translational/rotational invariance
However there is always the possibility that the winning neuron in a block has an activation
of exactly zero so that the block has no output
before
after
before
after
max-pooling
LWTA
Figure Max-pooling LWTA In max-pooling each group of neurons in a layer
has a single set of output weights that transmits the winning unit?s activation in this
case to the next layer the layer activations are subsampled In an LWTA block
there is no subsampling The activations flow into subsequent units via a different set of
connections depending on the winning unit
At first glance the max-pooling seems very similar to a WTA operation however the two
differ substantially there is no downsampling in a WTA operation and thus the number of
features is not reduced instead the representation is sparsified figure
Dropout
Dropout can be interpreted as a model-averaging technique that jointly trains several
models sharing subsets of parameters and input dimensions or as data augmentation when
applied to the input layer This is achieved by probabilistically omitting dropping units from a network for each example during training so that those neurons do not
participate in forward/backward propagation Consider hypothetically training an LWTA
network with blocks of size two and selecting the winner in each block at random This
is similar to training a neural network with a dropout probability of Nonetheless the
two are fundamentally different Dropout is a regularization technique while in LWTA the
interaction between neurons in a block replaces the per-neuron non-linear activation
Dropout is believed to improve generalization performance since it forces the units to learn
independent features without relying on other units being active During testing when
propagating an input through the network all units in a layer trained with dropout are
used with their output weights suitably scaled In an LWTA network no output scaling is
required A fraction of the units will be inactive for each input pattern depending on their
total inputs Viewed this way WTA is restrictive in that only a fraction of the parameters
are utilized for each input pattern However we hypothesize that the freedom to use different
subsets of parameters for different inputs allows the architecture to learn from multimodal
data distributions more accurately
Rectified Linear units
Rectified Linear Units ReLU are simply linear neurons that clamp negative activations to
zero if otherwise ReLU networks were shown to be useful for
Restricted Boltzmann Machines outperformed sigmoidal activation functions in deep
neural networks and have been used to obtain the best results on several benchmark
problems across multiple domains
Consider an LWTA block with two neurons compared to two ReLU neurons where and
are the weighted sum of the inputs to each neuron Table shows the outputs y1 and
y2 in all combinations of positive and negative and for ReLU and LWTA neurons
For both ReLU and LWTA neurons and are passed through as output in half of the
possible cases The difference is that in LWTA both neurons are never active or inactive at
the same time and the activations and errors flow through exactly one neuron in the block
For ReLU neurons being inactive saturation is a potential drawback since neurons that
Table Comparison of rectified linear activation and LWTA-2
Positive
Positive
Negative
Positive
Negative
Negative
Positive
Negative
Negative
Positive
Positive
Negative
ReLU neurons
y1
y2
LWTA neurons
y1
y2
do not get activated will not get trained leading to wasted capacity However previous
work suggests that there is no negative impact on optimization leading to the hypothesis
that such hard saturation helps in credit assignment and as long as errors flow through
certain paths optimization is not affected adversely Continued research along these
lines validates this hypothesis but it is expected that it is possible to train ReLU
networks better
While many of the above arguments for and against ReLU networks apply to LWTA networks there is a notable difference During training of an LWTA network inactive neurons
can become active due to training of the other neurons in the same block This suggests
that LWTA nets may be less sensitive to weight initialization and a greater portion of the
network?s capacity may be utilized
Experiments
In the following experiments LWTA networks were tested on various supervised learning
datasets demonstrating their ability to learn useful internal representations without utilizing
any other non-linearities In order to clearly assess the utility of local competition no special
strategies such as augmenting data with transformations noise or dropout were used We
also did not encourage sparse representations in the hidden layers by adding activation
penalties to the objective function a common technique also for ReLU units Thus our
objective is to evaluate the value of using LWTA rather than achieving the absolute best
testing scores Blocks of size two are used in all the experiments.2
All networks were trained using stochastic gradient descent with mini-batches learning rate
lt and momentum mt at epoch given by
if min
otherwise
min
Tt mf if
mt
pf
if
where is the learning rate annealing factor min is the lower learning rate limit and
momentum is scaled from mi to mf over epochs after which it remains constant at
mf weight decay was used for the convolutional network section and max-norm
normalization for other experiments This setup is similar to that of
Permutation Invariant MNIST
The MNIST handwritten digit recognition task consists of images
training test of the digits centered by their center of mass In the permutation
invariant setting of this task we attempted to classify the digits without utilizing the 2D
structure of the images every digit is a vector of pixels The last examples in the
training set were used for hyperparameter tuning The model with the best hyperparameter
setting was trained until convergence on the full training set Mini-batches of size were
To speed up our experiments the Gnumpy and CUDAMat libraries were used
Table Test set errors on the permutation invariant MNIST dataset for methods without
data augmentation or unsupervised pre-training
Activation
Sigmoid
ReLU
ReLU dropout in hidden layers
LWTA-2
Test Error
Table Test set errors on MNIST dataset for convolutional architectures with no data
augmentation Results marked with an asterisk use layer-wise unsupervised feature learning
to pre-train the network and global fine tuning
Architecture
2-layer CNN layer MLP
2-layer ReLU CNN layer LWTA-2
3-layer ReLU CNN
2-layer CNN layer MLP
3-layer ReLU CNN stochastic pooling
3-layer maxout dropout
Test Error
used the pixel values were rescaled to no further preprocessing The best model
obtained which gave a test set error of consisted of three LWTA layers of
blocks followed by a softmax layer To our knowledge this is the best reported
error without utilizing implicit/explicit model averaging for this setting which does not use
deformations/noise to enhance the dataset or unsupervised pretraining Table compares
our results with other methods which do not use unsupervised pre-training The performance
of LWTA is comparable to that of a ReLU network with dropout in the hidden layers Using
dropout in input layers as well lower error rates of using ReLU and using
maxout have been obtained
Convolutional Network on MNIST
For this experiment a convolutional network CNN was used consisting of filters in
the first layer followed by a second layer of with and 32 maps respectively and
ReLU activation Every convolutional layer is followed by a max-pooling operation
We then use two LWTA-2 layers each with 64 blocks and finally a softmax output
layer A weight decay of was found to be beneficial to improve generalization The
results are summarized in Table along with other state-of-the-art approaches which do not
use data augmentation for details of convolutional architectures see
Amazon Sentiment Analysis
LWTA networks were tested on the Amazon sentiment analysis dataset since ReLU units
have been shown to perform well in this domain We used the balanced subset of the
dataset consisting of reviews of four categories of products Books DVDs Electronics and
Kitchen appliances The task is to classify the reviews as positive or negative The dataset
consists of positive and negative reviews in each category The text of each review
was converted into a binary feature vector encoding the presence or absence of unigrams
and bigrams Following the most frequent vocabulary entries were retained as
features for classification We then divided the data into equal balanced folds and
tested our network with cross-validation reporting the mean test error over all folds ReLU
activation was used on this dataset in the context of unsupervised learning with denoising
autoencoders to obtain sparse feature representations which were used for classification We
trained an LWTA-2 network with three layers of blocks each in a supervised setting to
directly classify each review as positive or negative using a 2-way softmax output layer We
obtained mean accuracies of Books DVDs Electronics and Kitchen
giving a mean accuracy of compared to reported in for denoising
autoencoders using ReLU and unsupervised pre-training to find a good initialization
Table LWTA networks outperform sigmoid and ReLU activation in remembering dataset
P1 after training on dataset P2.
Testing error on P1
After training on P1
After training on P2
LWTA
Sigmoid
ReLU
Implicit long term memory
This section examines the effect of the LWTA architecture on catastrophic forgetting That
is does the fact that the network implements multiple models allow it to retain information
about dataset A even after being trained on a different dataset To test for this implicit
long term memory the MNIST training and test sets were each divided into two parts P1
containing only digits and P2 consisting of the remaining digits
Three different network architectures were compared three LWTA layers each with
blocks of size three layers each with sigmoidal neurons and three layers each
of ReLU neurons All networks have a 5-way softmax output layer representing the
probability of an example belonging to each of the five classes All networks were initialized
with the same parameters and trained with a fixed learning rate and momentum
Each network was first trained to reach a log-likelihood error on the P1 training set
This value was chosen heuristically to produce low test set errors in reasonable time for
all three network types The weights for the output layer corresponding to the softmax
classifier were then stored and the network was trained further starting with new initial
random output layer weights to reach the same log-likelihood value on P2. Finally the
output layer weights saved from P1 were restored and the network was evaluated on the
P1 test set The experiment was repeated for different initializations
Table shows that the LWTA network remembers what was learned from P1 much better
than sigmoid and ReLU networks though it is notable that the sigmoid network performs
much worse than both LWTA and ReLU While the test error values depend on the learning
rate and momentum used LWTA networks tended to remember better than the ReLU
network by about a factor of two in most cases and sigmoid networks always performed
much worse Although standard network architectures are known to suffer from catastrophic
forgetting we not only show here for the first time that ReLU networks are actually quite
good in this regard and moreover that they are outperformed by LWTA We expect this
behavior to manifest itself in competitive models in general and to become more pronounced
with increasingly complex datasets The neurons encoding specific features in one dataset
are not affected much during training on another dataset whereas neurons encoding common
features can be reused Thus LWTA may be a step forward towards models that do not
forget easily
Analysis of subnetworks
A network with a single LWTA-m of blocks consists of mN subnetworks which can be
selected and trained for individual examples while training over a dataset After training
we expect the subnetworks consisting of active neurons for examples from the same class to
have more neurons in common compared to subnetworks being activated for different classes
In the case of relatively simple datasets like MNIST it is possible to examine the number
of common neurons between mean subnetworks which are used for each class To do this
which neurons were active in the layer for each example in a subset of examples were
recorded For each class the subnetwork consisting of neurons active for at least of the
examples was designated the representative mean subnetwork which was then compared to
all other class subnetworks by counting the number of neurons in common
Figure 3a shows the fraction of neurons in common between the mean subnetworks of each
pair of digits Digits that are morphologically similar such as and have subnetworks
with more neurons in common than the subnetworks for digits and or and
which are intuitively less similar To verify that this subnetwork specialization is a result
of training we looked at the fraction of common neurons between all pairs of digits for the
untrained
trained
Digits
Fraction of neurons in common
Digits
MNIST digit pairs
Figure Each entry in the matrix denotes the fraction of neurons that a pair of MNIST
digits has in common on average in the subnetworks that are most active for each of the
two digit classes The fraction of neurons in common in the subnetworks of each of the
55 possible digit pairs before and after training
same examples both before and after training Figure Clearly the subnetworks
were much more similar prior to training and the full network has learned to partition its
parameters to reflect the structure of the data
Conclusion and future research directions
Our LWTA networks automatically self-modularize into multiple parameter-sharing subnetworks responding to different input representations Without significant degradation of
state-of-the-art results on digit recognition and sentiment analysis LWTA networks also
avoid catastrophic forgetting thus retaining useful representations of one set of inputs even
after being trained to classify another This has implications for continual learning agents
that should not forget representations of parts of their environment when being exposed to
other parts We hope to explore many promising applications of these ideas in the future
Acknowledgments
This research was funded by EU projects WAY NeuralDynamics and NASCENCE additional funding from ArcelorMittal

<<----------------------------------------------------------------------------------------------------------------------->>

title: 1541-active-noise-canceling-using-analog-neuro-chip-with-on-chip-learning-capability.pdf

Active Noise Canceling using Analog NeuroChip with On-Chip Learning Capability
Jung-Wook Cho and Soo-Young Lee
Computation and Neural Systems Laboratory
Department of Electrical Engineering
Korea Advanced Institute of Science and Technology
Kusong-dong Yusong-gu Taejon Korea
sylee@ee.kaist.ac.kr
Abstract
A modular analogue neuro-chip set with on-chip learning capability is
developed for active noise canceling The analogue neuro-chip set
incorporates the error backpropagation learning rule for practical
applications and allows pin-to-pin interconnections for multi-chip
boards The developed neuro-board demonstrated active noise
canceling without any digital signal processor Multi-path fading of
acoustic channels random noise and nonlinear distortion of the loud
speaker are compensated by the adaptive learning circuits of the
neuro-chips Experimental results are reported for cancellation of car
noise in real time
INTRODUCTION
Both analog and digital implementations of neural networks have been reported
Digital neuro-chips can be designed and fabricated with the help of well-established
CAD tools and digital VLSI fabrication technology Although analogue neurochips have potential advantages on integration density and speed over digital chips[2
they suffer from non-ideal characteristics of the fabricated chips such as offset and
nonlinearity and the fabricated chips are not flexible enough to be used for many
different applications Also much careful design is required and the fabricated chip
characteristics are fairly dependent upon fabrication processes
For the implementation of analog neuro-chips there exist two different approaches
with and without on-chip learning capability Currently the majority of analog
neuro-chips does not have learning capability while many practical applications
require on-line adaptation to continuously changing environments and must have online adaptation learning capability Therefore neuro-chips with on-chip learning
capability are essential for such practical applications Modular architecture is also
Active Noise Canceling with Analog On-Chip Learning Neuro-Chip
advantageous to provide flexibility of implementing many large complex systems from
same chips
Although many applications have been studied for analog neuro-chips it is very
important to find proper problems where analog neuro-chips may have potential
advantages over popular DSPs We believe applications with analog input/output
signals and high computational requirements are those good problems For example
active noise controls and adaptive equalizers are good applications for analog
neuro-chips
In this paper we report a demonstration of the active noise canceling which may have
many applications in real world A modular analog neuro-chip set is developed with
on-chip learning capability and a neuro-board is fabricated from multiple chips with
PC interfaces for input and output measurements Unlike our previous implementations
for adaptive equalizers with binary outputs both input and output values are
analogue in this noise canceling
xl
Figure Block diagram of a synapse cell
Figure Block diagram of a neuron cell
ANALOG NEURO-CHIP WITH ON-CHIP LEARNING
We had developed analog neuro-chips with error backpropagation learning capability
With the modular architecture the developed analog neuro-chip set consists of a
synapse chip and a neuron The basic cell of the synapse chip is shown in
Figure Each synapse cell receives two inputs pre-synaptic neural activation
and error correction term and generates two outputs feed-forward signal wx and
back-propagated error Also it updates a stored weight by the amount of
Therefore a synapse cell consists of three multiplier circuits and one analogue storage
for the synaptic weight Figure shows the basic cell in the neuron chip which collects
signals from synapses in the previous layer and distributes to synapses in the following
layer Each neuron body receives two inputs post-synaptic neural activation and
back-propagated error from the following layer and generates two outputs
Sigmoid-squashed neural activation and a new backpropagated error multiplied by
a bell-shaped Sigmoid-derivative The backpropagated error may be input to the
synapse cells in the previous layer
To provide easy connectivity with other chips the two inputs of the synapse cell are
represented as voltage while the two outputs are as currents for simple current
summation On the other hand the inputs and outputs of the neuron cell are represented
as currents and voltages respectively For simple pin-to-pin connections between chips
one package pin is maintained to each input and output of the chip No time
Cho and Lee
multiplexing is introduced and no other control is required for multi-chip and multilayer systems However it makes the number of package pins the main limiting factor
for the number of synapse and neuron cells in the developed chip sets
Although many simplified multipliers had been reported for high-density integration
their performance is limited in linearity resolution and speed For on-chip learning it
is desirable to have high precision and a faithful implementation of the 4-quadranr
Gilbert multipliers is used Especially the mUltiplier for weight updates in the synapse
cell requires high precision.[9 The synaptic weight is stored on a capacitor and an
MaS switch is used to allow current flow from the multiplier to the capacitor during a
short time interval for weight adaptation For applications like active noise controls
and telecommunications tapped analog delay lines are also designed and
integrated in the synapse chip To reduce offset accumulation a parallel analog delay
line is adopted Same offset voltage is introduced for operational amplifiers at all
nodes Diffusion capacitors with pF are used for the storage of the tapped
analog delay line
In a synapse chip synapse cells are integrated in a array with a
analog delay line Inputs may be applied either from the analog delay line or from
external pins in parallel To select a capacitor in the cell for refresh decoders are
placed in columns and rows The actual size of the synapse cell is
and the size of the synapse chip is The chip is fabricated in a
single-poly CMOS process On the other hand the neuron chip has a very
simple structure which consists of neuron cells without additional circuits The
Sigmoid circuit in the neuron cell uses a differential pair and the slope and
amplitude are controlled by a voltage-controlled resistor Sigmoid-derivative
circuit is also using differential pair with min-select circuit The size of the neuron cell
is
Synapse
Chip
PC
Neuron
PC
Chip
Target
I
I
I
DSP
I
Output
I
Input
ANN Board
Figure Block diagram of the analog neuro-board
GDAB
tv.c
ArIC
D1 16bitll
DO 48bitll
Active Noise Canceling with Analog On-Chip Learning Neuro-Chip
Using these chip sets an analog neuro-system is constructed Figure shows a brief
block diagram of the analog neuro-system where an analogue neuro-board is
interfaced to a host computer through a GDAB General Data Acquisition Board The
GDAB board is specially designed for the data interface with the analogue neuro-chips
The neuro-board has synapse chips and neuron chips with the 2-layer Perceptron
architecture For test and development purposes a DSP ADC and DAC are installed
on the neuro-board to refresh and adjust weights
Forward propagation time of the layers Perceptron is measured as about f..lsec
Therefore the computation speed of the neuro-board is about MCPS Mega
Connections Per Second for recall and about MCUPS Mega Connections
Updates Per Second for error backpropagation learning To achieve this speed with a
DSP about MIPS is required for recall and at least MIPS for error-back
propagation learning
Channel
Error
Signal
Noise
Source
Adaptive Filter
or
Multilayer Perceptron
Figure Structure of a feedforward active noise canceling
ACTIVE NOISE CANCELING USING NEURO-CHIP
Basic architecture of the feed forward active noise canceling is shown in Figure An
area near the microphone is called quiet zone which actually means noise should be
small in this area Noise propagates from a source to the quiet zone through a
dispersive medium of which characteristics are modeled as a finite impulse response
FIR filter with additional random noise An active noise canceller should generate
electric signals for a loud speaker which creates acoustic signals to cancel the noise at
the quiet zone In general the electric-to-acoustic signal transfer characteristics of the
loud speaker is nonlinear and the overall active noise canceling ANC system also
becomes nonlinear Therefore multilayer Perceptron has a potential advantage over
popular transversal adaptive filters based on linear-mean.-square LMS error
minimization
Experiments had been conducted for car noise canceling The

<<----------------------------------------------------------------------------------------------------------------------->>

title: 6221-memory-efficient-backpropagation-through-time.pdf

Memory-Efficient Backpropagation Through Time
Audrunas
Gruslys
Google DeepMind
audrunas@google.com
R?mi Munos
Google DeepMind
munos@google.com
Marc Lanctot
Google DeepMind
lanctot@google.com
Ivo Danihelka
Google DeepMind
danihelka@google.com
Alex Graves
Google DeepMind
gravesa@google.com
Abstract
We propose a novel approach to reduce memory consumption of the backpropagation through time BPTT algorithm when training recurrent neural networks
RNNs Our approach uses dynamic programming to balance a trade-off between
caching of intermediate results and recomputation The algorithm is capable of
tightly fitting within almost any user-set memory budget while finding an optimal
execution policy minimizing the computational cost Computational devices have
limited memory capacity and maximizing a computational performance given a
fixed memory budget is a practical use-case We provide asymptotic computational
upper bounds for various regimes The algorithm is particularly effective for long
sequences For sequences of length our algorithm saves of memory
usage while using only one third more time per iteration than the standard BPTT
Introduction
Recurrent neural networks RNNs are artificial neural networks where connections between units
can form cycles They are often used for sequence mapping problems as they can propagate hidden
state information from early parts of the sequence back to later points LSTM in particular
is an RNN architecture that has excelled in sequence generation 13 speech recognition
and reinforcement learning settings Other successful RNN architectures include the
differentiable neural computer DNC DRAW network and Neural Transducers
Backpropagation Through Time algorithm BPTT is typically used to obtain gradients
during training One important problem is the large memory consumption required by the BPTT
This is especially troublesome when using Graphics Processing Units GPUs due to the limitations
of GPU memory
Memory budget is typically known in advance Our algorithm balances the tradeoff between memorization and recomputation by finding an optimal memory usage policy which minimizes the total
computational cost for any fixed memory budget The algorithm exploits the fact that the same
memory slots may be reused multiple times The idea to use dynamic programming to find a provably
optimal policy is the main contribution of this paper
Our approach is largely architecture agnostic and works with most recurrent neural networks Being
able to fit within limited memory devices such as GPUs will typically compensate for any increase in
computational cost
Background and related work
In this section we describe the key terms and relevant previous work for memory-saving in RNNs
Conference on Neural Information Processing Systems NIPS Barcelona Spain
Definition An RNN core is a feed-forward neural network which is cloned unfolded in time
repeatedly where each clone represents a particular time point in the recurrence
For example if an RNN has a single hidden layer whose outputs feed back into the same hidden
layer then for a sequence length of the unfolded network is feed-forward and contains RNN cores
Definition The hidden state of the recurrent network is the part of the output of the RNN core
which is passed into the next RNN core as an input
In addition to the initial hidden state there exists a single hidden state per time step once the network
is unfolded
Definition The internal state of the RNN core for a given time-point is all the necessary information required to backpropagate gradients over that time step once an input vector a gradient with
respect to the output vector and a gradient with respect to the output hidden state is supplied We
define it to also include an output hidden state
An internal state can be re)evaluated by executing a single forward operation taking the previous
hidden state and the respective entry of an input sequence as an input For most network architectures
the internal state of the RNN core will include a hidden input state as this is normally required to
evaluate gradients This particular choice of the definition will be useful later in the paper
Definition A memory slot is a unit of memory which is capable of storing a single hidden state
or a single internal state depending on the context
Backpropagation through Time
Backpropagation through Time BPTT is one of the commonly used techniques to train
recurrent networks BPTT unfolds the neural network in time by creating several copies of the
recurrent units which can then be treated like a deep feed-forward network with tied weights Once
this is done a standard forward-propagation technique can be used to evaluate network fitness over
the whole sequence of inputs while a standard backpropagation algorithm can be used to evaluate
partial derivatives of the loss criteria with respect to all network parameters This approach while
being computationally efficient is also fairly intensive in memory usage This is because the standard
version of the algorithm effectively requires storing internal states of the unfolded network core at
every time-step in order to be able to evaluate correct partial derivatives
Trading memory for computation time
The general idea of trading computation time and memory consumption in general computation
graphs has been investigated in the automatic differentiation community Recently the rise of
deep architectures and recurrent networks has increased interest in a less general case where the
graph of forward computation is a chain and gradients have to be chained in a reverse order This
simplification leads to relatively simple memory-saving strategies and heuristics In the context of
BPTT instead of storing hidden network states some of the intermediate results can be recomputed
on demand by executing an extra forward operation
Chen proposed subdividing the sequence of size into equal parts and memorizing only
hidden
states between the subsequences and all internal states within each segment This uses
memory at the cost of making an additional forward pass on average as once the errors are
backpropagated through the right-side of the sequence the second-last subsequence
has to be restored
by repeating a number of forward operations We refer to this as Chen?s algorithm
The authors also suggest applying the same technique recursively several times by sub-dividing the
sequence into equal parts and terminating the recursion once the subsequence length becomes less
than The authors have established that this would lead to memory consumption of O(k logk+1
and computational complexity of O(t logk This algorithm has a minimum possible memory
usage of log2 in the case when We refer to this as Chen?s recursive algorithm
Memory-efficient backpropagation through time
We first discuss two simple examples when memory is very scarce and when it is somewhat limited
When memory is very scarce it is straightforward to design a simple but computationally inefficient
algorithm for backpropagation of errors on RNNs which only uses a constant amount of memory
Every time when the state of the network at time has to be restored the algorithm would simply
re-evaluate the state by forward-propagating inputs starting from the beginning until time As
backpropagation happens in the reverse temporal order results from the previous forward steps can
not be reused as there is no memory to store them This would require repeating forward steps
before backpropagating gradients one step backwards we only remember inputs and the initial state
This would produce an algorithm requiring t(t forward passes to backpropagate errors over
time steps The algorithm would be in space and in time
When the memory is somewhat limited but not very scarce we may store only hidden RNN states
at all time points When errors have to be backpropagated from time to an internal RNN
core state can be re-evaluated by executing another forward operation taking the previous hidden
state as an input The backward operation can follow immediately This approach can lead to fairly
significant memory savings as typically the recurrent network hidden state is much smaller than an
internal state of the network core itself On the other hand this leads to another forward operation
being executed during the backpropagation stage
Backpropagation though time with selective hidden state memorization BPTT-HSM
The idea behind the proposed algorithm is to compromise between two previous extremes Suppose
that we want to forward and backpropagate a sequence of length but we are only able to store
hidden states in memory at any given time We may reuse the same memory slots to store different
hidden states during backpropagation Also suppose that we have a single RNN core available for
the purposes of intermediate calculations which is able to store a single internal state Define
as a computational cost of backpropagation measured in terms of how many forward-operations one
has to make in total during forward and backpropagation steps combined when following an optimal
memory usage policy minimizing the computational cost One can easily set the boundary conditions
t(t is the cost of the minimal memory approach while 2t for all
when memory is plentiful as shown in Our approach is illustrated in Figure Once
we start forward-propagating steps at time t0 at any given point t0 we can choose to put the
current hidden state into memory step This step has the cost of forward operations States will
be read in the reverse order in which they were written this allows the algorithm to store states in a
stack Once the state is put into memory at time we can reduce the problem into two
parts by using a divide-and-conquer approach running the same algorithm on the side of the
sequence while using of the remaining memory slots at the cost of C(t step
and then reusing memory slots when backpropagating on the side at the cost of
step We use a full size memory capacity when performing step because we could release the
hidden state immediately after finishing step
Legend
Step cost
Hidden state is propagated
Gradients get back-propagated
Hidden state stored in memory
Internal state of RNN core at time
A single forward operation
Step cost
A single backward operation
Recursive application
of the algorithm
Hidden state is read from memory
Step cost
Hidden state is saved in memory
Hidden state is removed from memory
Figure The proposed divide-and-conquer approach
The base case for the recurrent algorithm is simply a sequence of length when forward and
backward propagation may be done trivially on a single available RNN network core This step has
the cost
Theoretical computational cost
measured in number of forward operations per time step
Measured computational cost in
miliseconds
Figure Computational cost per time-step when the algorithm is allowed to remember
green blue violet cyan hidden states The grey line shows the performance
of standard BPTT without memory constraints also includes a large constant value caused by a
single backwards step per time step which was excluded from the theoretical computation which
value makes a relative performance loss much less severe in practice than in theory
Having established the protocol we may find an optimal policy Define the cost of choosing
the first state to be pushed at position and later following the optimal policy as
C(t
time
argmin
CPTS
time
CPTS
Hidden state stored in memory
Forward computation
Backward computation
Order of execution
CPTS cost per time step in the number
of forward operations
Figure Illustration of the optimal policy for and and Logical sequence
time goes from left to right while execution happens from top to the bottom
Equations can be solved exactly by using dynamic programming subject to the boundary conditions
established previously as in Figure will determine the optimal policy to follow
Pseudocode is given in the supplementary material Figure illustrates an optimal policy found for
two simple cases
Backpropagation though time with selective internal state memorization BPTT-ISM
Saving internal RNN core states instead of hidden RNN states would allow us to save a single forward
operation during backpropagation in every divide-and-conquer step but at a higher memory cost
Suppose we have a memory capacity capable of saving exactly internal RNN states First we need
to modify the boundary conditions t(t is a cost reflecting the minimal memory
approach while for all when memory is plentiful equivalent to standard BPTT
As previously is defined to be the computational cost for combined forward and backward
propagations over a sequence of length with memory allowance while following an optimal
memory usage policy As before the cost is measured in terms of the amount of total forward steps
made because the number of backwards steps is constant Similarly to BPTT-HSM the process
can be divided into parts using divide-and-conquer approach Fig For any values of and
position of the first memorization is evaluated forward operations are executed and
an internal RNN core state is placed into memory This step has the cost of forward operations
Step in Figure As the internal state also contains an output hidden state the same algorithm can
be recurrently run on the high-time right side of the sequence while having one less memory slot
available Step in Figure This step has the cost of C(t forward operations Once
gradients are backpropagated through the right side of the sequence backpropagation can be done
over the stored RNN core Step in Figure This step has no additional cost as it involves no more
forward operations The memory slot can now be released leaving memory available Finally the
same algorithm is run on the left-side of the sequence Step in Figure This final step has the cost
of C(y forward operations Summing the costs gives us the following equation
C(y C(t
Recursion has a single base case backpropagation over an empty sequence is a nil operation which
has no computational cost making
Legend
Step cost
Hidden state gets propagated
Internal stat of RNN core at time
Internal RNN core state stored in
memory incl output hidden state
Step
cost
Hidden state is read from memory
Internal state is saved
Internal state is removed
Step
cost
A single backwards
operation no forward
operations involved
Recursive application
of the algorithm
Gradients get back-propagated
A single initial RNN hidden state
Step
cost
A single forward operation
A single backward operation
Figure Illustration of the divide-and-conquer approach used by BPTT-ISM
Compared to the previous section stays the same while is minimized over instead
of This is because it is meaningful to remember the last internal state while there was
no reason to remember the last hidden state A numerical solution of for several different
memory capacities is shown in Figure
argmin
As seen in Figure our methodology saves of memory for sequences of excluding input
vectors while using only more time per training-iteration than the standard BPTT assuming a
single backward step being twice as expensive as a forward step
Backpropagation though time with mixed state memorization BPTT-MSM
It is possible to derive an even more general model by combining both approaches as described in
Sections and Suppose we have a total memory capacity measured in terms of how much a
single hidden states can be remembered Also suppose that storing an internal RNN core state takes
times more memory where is some integer number We will choose between saving a single
hidden state while using a single memory unit and storing an internal RNN core state by using
times more memory The benefit of storing an internal RNN core state is that we will be able to save
a single forward operation during backpropagation
Define as a computational cost in terms of a total amount of forward operations when
running an optimal strategy We use the following boundary conditions t(t as a
BPTT-ISM section
BPTT-MSM section
Figure Comparison of two backpropagation algorithms in terms of theoretical costs Different
lines show the number of forward operations per time-step when the memory capacity is limited to
green blue violet cyan internal RNN core states Please note that
the units of memory measurement are different than in Figure size of an internal state vs size of
a hidden state It was assumed that the size of an internal core state is times larger than the
size of a hidden state The value of influences only the right plot All costs shown on the right plot
should be less than the respective costs shown on the left plot for any value of
cost reflecting the minimal memory approach while for all when memory is
plentiful and C(t for all and for notational convenience We use
a similar divide-and-conquer approach to the one used in previous sections
Define Q1 as the computational cost if we choose to firstly remember a hidden state at
position and thereafter follow an optimal policy identical to
Q1 C(t
Similarly define Q2 as the computational cost if we choose to firstly remember an internal
state at position and thereafter follow an optimal policy similar to except that now the internal
state takes memory units
Q2 C(y C(t
Define D1 as an optimal position of the next push assuming that the next state to be pushed is a
hidden state and define D2 as an optimal position if the next push is an internal core state Note that
D2 has a different range over which it is minimized for the same reasons as in equation
D1 argmin Q1
D2 argmin Q2
Also define Ci Qi and finally
min Ci
argmin Ci
We can solve the above equations by using simple dynamic programming will indicate
whether the next state to be pushed into memory in a hidden state or an internal state while the
respective values if D1 and D2 will indicate the position of the next push
Removing double hidden-state memorization
Definition of internal RNN core state would typically require for a hidden input state to be included
for each memorization This may lead to the duplication of information For example when an
optimal strategy is to remember a few internal RNN core states in sequence a memorized hidden
output of one would be equal to a memorized hidden input for the other one Definition
Every time we want to push an internal RNN core state onto the stack and a previous internal state is
already there we may omit pushing the input hidden state Recall that an internal core RNN state
when an input hidden state is otherwise not known is times larger than a hidden state Define
as the space required to memorize the internal core state when an input hidden state is known A
relationship between and is application-specific but in many circumstances We only
have to modify to reflect this optimization
Q2 C(y C(t
is an indicator function Equations for Di and are identical to and
Analytical upper bound for BPTT-HSM
We have established a theoretical upper bound for BPTT-HSM algorithm as As
the bound is not tight for short sequences it was also numerically verified that for
and or less than 3t
if the initial forward pass is excluded In addition to that
we have established a different bound in the regime where mm For any integer value a and for
a
all ma the computational cost is bounded by The proofs are given in the
supplementary material Please refer to supplementary material for discussion on the upper bounds
for BPTT-MSM and BPTT-ISM
Comparison of the three different strategies
Using memory
Using memory
Figure Comparison of three strategies in the case when a size of an internal RNN core state is
times larger than that of the hidden state and the total memory capacity allows us remember
either internal RNN states or hidden states or any arbitrary mixture of those in the left plot
and respectively in the right plot The red curve illustrates BPTT-HSM the green curve
BPTT-ISM and the blue curve BPTT-MSM Please note that for large sequence lengths the red
curve out-performs the green one and the blue curve outperforms the other two
Computational costs for each previously described strategy and the results are shown in Figure
BPTT-MSM outperforms both BPTT-ISM and BPTT-HSM This is unsurprising because the search
space in that case is a superset of both strategy spaces while the algorothm finds an optimal strategy
within that space Also for a fixed memory capacity the strategy memorizing only hidden states
outperforms a strategy memorizing internal RNN core states for long sequences while the latter
outperforms the former for relatively short sequences
Discussion
We used an LSTM mapping inputs to with a batch size of 64 and measured execution time for
a single gradient descent step forward and backward operation combined as a function of sequence
length Figure Please note that measured computational time also includes the time taken by
backward operations at each time-step which dynamic programming equations did not take into the
account A single backward operation is usually twice as expensive than a forward operation because
it involves evaluating gradients both with respect to input data and internal parameters Still as the
number of backward operations is constant it has no impact on the optimal strategy
Optimality
The dynamic program finds the optimal computational strategy by construction subject to memory
constraints and a fairly general model that we impose As both strategies proposed by are
consistent with all the assumptions that we have made in section when applied to RNNs BPTTMSM is guaranteed to perform at least as well under any memory budget and any sequence length
This is because strategies proposed by can be expressed by providing a potentially suboptimal
policy Di subject to the same equations for Qi
Numerical comparison with Chen?s
algorithm
Chen?s algorithm requires to remember hidden states and internal RNN states excluding
input hidden states while the recursive approach requires to remember at least log2 hidden states
In other words the model does not allow for a fine-grained control over memory usage and rather
saves some memory In the meantime our proposed BPTT-MSM can fit within almost arbitrary
constant memory constraints and this is the main advantage of our algorithm
Figure Left memory consumption divided by for a fixed computational
cost
Right computational cost per time-step for a fixed memory consumption of Red green
and blue curves correspond to respectively
The non-recursive Chen?s approach does not allow to match any particular memory budget
making a like-for-like comparison difficult Instead of fixing the memory budge it?is possible to fix
computational cost at forwards iterations on average to match the cost of the
algorithm and
observe how much memory
would
our
approach
use
Memory
usage
by
the
algorithm would
be equivalent to saving hidden states and internal core states Lets suppose that the internal
RNN core state is times larger than hidden states In this case the size of the internal RNN core
state excluding
state is This would
give a memory?usage of Chen?s
the input hidden
algorithm as as it needs to remember hidden states and internal states
where input hidden states?can be omitted to avoid duplication Figure illustrates memory usage by
our algorithm divided by for a fixed execution speed of as a function of sequence length
and for different values of parameter Values lower than indicate memory savings As it is seen
we can save a significant amount of memory for the same computational cost
Another experiment is to measure computational cost for a fixed memory consumption
of
The results are shown in Figure Computational cost of corresponds to Chen?s algorithm This
illustrates that our approach
does not perform significantly faster although it does not do any worse
This is because Chen?s strategy is actually near optimal for this particular memory budget Still
as seen from the previous paragraph this memory budget is already in the regime of diminishing
returns and further memory reductions are possible for almost the same computational cost
Conclusion
In this paper we proposed a novel approach for finding optimal backpropagation strategies for
recurrent neural networks for a fixed user-defined memory budget We have demonstrated that the
most general of the algorithms is at least as good as many other used common heuristics The main
advantage of our approach is the ability to tightly fit to almost any user-specified memory constraints
gaining maximal computational performance

<<----------------------------------------------------------------------------------------------------------------------->>

title: 5666-variational-dropout-and-the-local-reparameterization-trick.pdf

Variational Dropout and
the Local Reparameterization Trick
Diederik P. Kingma Tim Salimans and Max Welling
Machine Learning Group University of Amsterdam
Algoritmica
University of California Irvine and the Canadian Institute for Advanced Research CIFAR
D.P.Kingma@uva.nl salimans.tim@gmail.com M.Welling@uva.nl
Abstract
We investigate a local reparameterizaton technique for greatly reducing the variance of stochastic gradients for variational Bayesian inference SGVB of a posterior over model parameters while retaining parallelizability This local reparameterization translates uncertainty about global parameters into local noise that
is independent across datapoints in the minibatch Such parameterizations can be
trivially parallelized and have variance that is inversely proportional to the minibatch size generally leading to much faster convergence Additionally we explore
a connection with dropout Gaussian dropout objectives correspond to SGVB with
local reparameterization a scale-invariant prior and proportionally fixed posterior
variance Our method allows inference of more flexibly parameterized posteriors
specifically we propose variational dropout a generalization of Gaussian dropout
where the dropout rates are learned often leading to better models The method
is demonstrated through several experiments
Introduction
Deep neural networks are a flexible family of models that easily scale to millions of parameters and
datapoints but are still tractable to optimize using minibatch-based stochastic gradient ascent Due
to their high flexibility neural networks have the capacity to fit a wide diversity of nonlinear patterns
in the data This flexbility often leads to overfitting when left unchecked spurious patterns are found
that happen to fit well to the training data but are not predictive for new data Various regularization
techniques for controlling this overfitting are used in practice a currently popular and empirically
effective technique being dropout In it was shown that regular binary dropout has a
Gaussian approximation called Gaussian dropout with virtually identical regularization performance
but much faster convergence In section of it is shown that Gaussian dropout optimizes a lower
bound on the marginal likelihood of the data In this paper we show that a relationship between
dropout and Bayesian inference can be extended and exploited to greatly improve the efficiency of
variational Bayesian inference on the model parameters This work has a direct interpretation as a
generalization of Gaussian dropout with the same fast convergence but now with the freedom to
specify more flexibly parameterized posterior distributions
Bayesian posterior inference over the neural network parameters is a theoretically attractive method
for controlling overfitting exact inference is computationally intractable but efficient approximate
schemes can be designed Markov Chain Monte Carlo MCMC is a class of approximate inference
methods with asymptotic guarantees pioneered by for the application of regularizing neural
networks Later useful refinements include and
An alternative to MCMC is variational inference or the equivalent minimum description length
MDL framework Modern variants of stochastic variational inference have been applied to neural
networks with some succes but have been limited by high variance in the gradients Despite
their theoretical attractiveness Bayesian methods for inferring a posterior distribution over neural
network weights have not yet been shown to outperform simpler methods such as dropout Even a
new crop of efficient variational inference algorithms based on stochastic gradients with minibatches
of data 17 have not yet been shown to significantly improve upon simpler dropout-based
regularization
In section we explore an as yet unexploited trick for improving the efficiency of stochastic gradientbased variational inference with minibatches of data by translating uncertainty about global parameters into local noise that is independent across datapoints in the minibatch The resulting method
has an optimization speed on the same level as fast dropout and indeed has the original Gaussian dropout method as a special case An advantage of our method is that it allows for full Bayesian
analysis of the model and that it?s significantly more flexible than standard dropout The approach
presented here is closely related to several popular methods in the literature that regularize by adding
random noise these relationships are discussed in section
Efficient and Practical Bayesian Inference
We consider Bayesian analysis of a dataset containing a set of observations of tuples
where the goal is to learn a model with parameters or weights of the conditional probability standard classification or regression)1 Bayesian inference in such a model consists
of updating some initial belief over parameters in the form of a prior distribution after
observing data into an updated belief over these parameters in the form of an approximation
to the posterior distribution Computing the true posterior distribution through Bayes rule
involves computationally intractable integrals so good approximations are necessary In variational inference inference is cast as an optimization problem where we
optimize the parameters of some parameterized model such that is a close approximation to as measured by the Kullback-Leibler divergence DKL This
divergence of our posterior to the true posterior is minimized in practice by maximizing the
so-called variational lower bound of the marginal likelihood of the data
where LD
DKL LD
Eq log
We?ll call LD the expected log-likelihood
The bound plus DKL equals
the conditional marginal log-likelihood log Since this marginal log-likelihood
is constant maximizing the bound will minimize DKL
Stochastic Gradient Variational Bayes SGVB
Various algorithms for gradient-based optimization of the variational bound with differentiable and exist See section for an overview A recently proposed efficient method for
minibatch-based optimization with differentiable models is the stochastic gradient variational Bayes
SGVB method introduced in especially appendix and The basic trick in SGVB is
to parameterize the random parameters as where is a differentiable function and is a random noise variable In this new parameterisation an unbiased
differentiable minibatch-based Monte Carlo estimator of the expected log-likelihood can be formed
LD
LSGVB
log p(yi
where
is a minibatch of data with random datapoints and is a noise
vector drawn from the noise distribution We?ll assume that the remaining term in the variational lower bound DKL can be computed deterministically but otherwise it may
be approximated similarly The estimator is differentiable and unbiased so its gradient
Note that the described method is not limited to classification or regression and is straightforward to apply
to other modeling settings like unsupervised models and temporal models
is also unbiased LD LSGVB
We can proceed with variational Bayesian inference
by randomly initializing and performing stochastic gradient ascent on
Variance of the SGVB estimator
The theory of stochastic approximation tells us that stochastic gradient ascent using will asymptotically converge to a local optimum for an appropriately declining step size and sufficient weight
updates but in practice the performance of stochastic gradient ascent crucially depends on
the variance of the gradients If this variance is too large stochastic gradient descent will fail
to make much progress in any reasonable amount of time Our objective function consists of an
expected log likelihood term that we approximate using Monte Carlo and a KL divergence term
DKL that we assume can be calculated analytically and otherwise be approximated
with Monte Carlo with similar reparameterization
Assume that we draw minibatches of datapoints with replacement see appendix for a similar
analysis for minibatches without replacement Using Li as shorthand for log p(yi
the contribution to the likelihood for the i-th datapoint in the minibatch the Monte Carlo estimator
PM
may be rewritten as LSGVB
Li whose variance is given by
N2
Var LSGVB
Var
Cov
Var Li
Cov Li Lj
where the variances and
the data distribution and distribution
covariances are both
Var Li Var?,xi log p(yi with drawn from the empirical distribution defined by the training set As can be seen from the total contribution to the variance by
Var Li is inversely proportional to the minibatch size However the total contribution by the
covariances does not decrease with In practice this means that the variance of LSGVB
can be
dominated by the covariances for even moderately large
Local Reparameterization Trick
We therefore propose an alternative estimator for which we have Cov Li Lj so that the variance of our stochastic gradients scales as We then make this new estimator computationally
efficient by not sampling directly but only sampling the intermediate variables through which
influences LSGVB
By doing so the global uncertainty in the weights is translated into a form
of local uncertainty that is independent across examples and easier to sample We refer to such a
reparameterization from global noise to local noise as the local reparameterization trick Whenever
a source of global noise can be translated to local noise in the intermediate states of computation
a local reparameterization can be applied to yield a computationally and statistically
efficient gradient estimator
Such local reparameterization applies to a fairly large family of models but is best explained through
a simple example Consider a standard fully connected neural network containing a hidden layer
consisting of neurons This layer receives an input feature matrix A from the layer
below which is multiplied by a weight matrix before a nonlinearity is applied
AW. We then specify the posterior approximation on the weights to be a fully factor2
ized Gaussian wi,j i,j
8wi,j which means the weights are sampled as
wi,j i,j with In this case we could make sure that Cov Li Lj
by sampling a separate weight matrix for each example in the minibatch but this is not computationally efficient we would need to sample million random numbers for just a single layer
of the neural network Even if this could be done efficiently the computation following this step
would become much harder Where we originally performed a simple matrix-matrix product of the
form AW this now turns into separate local vector-matrix products The theoretical complexity of this computation is higher but more importantly such a computation can usually not be
performed in parallel using fast device-optimized BLAS Basic Linear Algebra Subprograms This
also happens with other neural network architectures such as convolutional neural networks where
optimized libraries for convolution cannot deal with separate filter matrices per example
Fortunately the weights and therefore only influence the expected log likelihood through the
neuron activations which are of much lower dimension If we can therefore sample the random
activations directly without sampling or we may obtain an efficient Monte Carlo estimator
at a much lower cost For a factorized Gaussian posterior on the weights the posterior for the
activations conditional on the input A is also factorized Gaussian
wi,j i,j
8wi,j bm,j m,j m,j with
m,j
am,i and
m,j
a2m,i
i,j
Rather than sampling the Gaussian weights and then computing the resulting activations we may
thus
sample the activations from their implied Gaussian distribution directly using bm,j m,j
m,j with Here is an matrix so we only need to sample
thousand random variables instead of million a thousand fold savings
In addition to yielding a gradient estimator that is more computationally efficient than drawing separate weight matrices for each training example the local reparameterization trick also leads to an
estimator that has lower variance To see why consider the stochastic gradient estimate with respect
to the posterior parameter i,j
for a minibatch of size Drawing random weights we get
@LSGVB
@LSGVB
am,i
@ i,j
@bm,j i,j
If on the other hand we form the same gradient using the local reparameterization trick we get
a2m,i
@LSGVB
@LSGVB
@ i,j
@bm,j m,j
Here there are two stochastic terms The first is the backpropagated gradient @LSGVB
@bm,j and
the second is the sampled random noise or Estimating the gradient with respect to i,j
then basically comes down to estimating the covariance between these two terms This is much
easier to do for as there are much fewer of these individually they have higher correlation
with the backpropagated gradient @LSGVB
@bm,j so the covariance is easier to estimate In other
words measuring the effect of on @LSGVB
@bm,j is easy as is the only random variable
directly influencing this gradient via bm,j On the other hand when sampling random weights
there are a thousand influencing each gradient term so their individual effects get lost in the
noise In appendix we make this argument more rigorous and in section we show that it holds
experimentally
Variational Dropout
Dropout is a technique for regularization of neural network parameters which works by adding
multiplicative noise to the input of each layer of the neural network during optimization Using the
notation of section for a fully connected neural network dropout corresponds to
A with
where A is the matrix of input features for the current minibatch is a weight matrix and is the output matrix for the current layer before a nonlinearity is applied The
symbol denotes the elementwise Hadamard product of the input matrix with a matrix
of independent noise variables By adding noise to the input during training the weight parameters are less likely to overfit to the training data as shown empirically by previous publications
Originally proposed drawing the elements of from a Bernoulli distribution with probability
with the dropout rate Later it was shown that using a continuous distribution with the same
relative mean and variance such as a Gaussian with works as well or better
Here we re-interpret dropout with continuous noise as a variational method and propose a generalization that we call variational dropout In developing variational dropout we provide a firm
Bayesian justification for dropout training by deriving its implicit prior distribution and variational
objective This new interpretation allows us to propose several useful extensions to dropout such as
a principled way of making the normally fixed dropout rates adaptive to the data
Variational dropout with independent weight noise
If the elements of the noise matrix are drawn independently from a Gaussian the marginal
distributions of the activations bm,j are Gaussian as well
bm,j
m,j m,j
with
m,j
am,i and
m,j
a2m,i
Making use of this fact proposed Gaussian dropout a regularization method where instead
of applying the activations are directly drawn from their approximate or exact marginal distributions as given by argued that these marginal distributions are exact for Gaussian noise
and for Bernoulli noise still approximately Gaussian because of the central limit theorem This
ignores the dependencies between the different elements of as present using but report
good results nonetheless
As noted by and explained in appendix this Gaussian dropout noise can also be interpreted
as arising from a Bayesian treatment of a neural network with weights that multiply the input to
give AW where the posterior distribution of the weights is given by a factorized Gaussian with
wi,j
From this perspective the marginal distributions then arise through
the application of the local reparameterization trick as introduced in section The variational
objective corresponding to this interpretation is discussed in section
Variational dropout with correlated weight noise
Instead of ignoring the dependencies of the activation noise as in section we may retain the
dependencies by interpreting dropout as a form of correlated weight noise
A bm am with
wK
and si with si
where am is a row of the input matrix and bm a row of the output The are the rows of the
weight matrix each of which is constructed by multiplying a non-stochastic parameter vector by
a stochastic scale variable si The distribution on these scale variables we interpret as a Bayesian
posterior distribution The weight parameters and the biases are estimated using maximum
likelihood The original Gaussian dropout sampling procedure can then be interpreted as arising
from a local reparameterization of our posterior on the weights W.
Dropout?s scale-invariant prior and variational objective
The posterior distributions proposed in sections and have in common that they can
be decomposed into a parameter vector that captures the mean and a multiplicative noise term
determined by parameters Any posterior distribution on for which the noise enters this multiplicative way we will call a dropout posterior Note that many common distributions such as
univariate Gaussians with nonzero mean can be reparameterized to meet this requirement
During dropout training is adapted to maximize the expected log likelihood Eq LD For this
to be consistent with the optimization of a variational lower bound of the form in the prior on
the weights has to be such that DKL does not depend on In appendix we
show that the only prior that meets this requirement is the scale invariant log-uniform prior
p(log(|wi,j
a prior that is uniform on the log-scale of the weights the weight-scales si for section As
explained in appendix A this prior has an interesting connection with the floating point format for
storing numbers From an MDL perspective the floating point format is optimal for communicating
numbers drawn from this prior Conversely the KL divergence DKL with this prior
has a natural interpretation as regularizing the number of significant digits our posterior stores
for the weights wi,j in the floating-point format
Putting the expected log likelihood and KL-divergence penalty together we see that dropout training
maximizes the following variatonal lower bound
Eq LD
DKL
where we have made the dependence on the and parameters explicit The noise parameters
the dropout rates are commonly treated as hyperparameters that are kept fixed during training
For the log-uniform prior this then corresponds to a fixed limit on the number of significant digits
we can learn for each of the weights wi,j In section we discuss the possibility of making this
limit adaptive by also maximizing the lower bound with respect to
For the choice of a factorized Gaussian approximate posterior with wi,j
as
discussed in section the lower bound is analyzed in detail in appendix C. There it is shown
that for this particular choice of posterior the negative KL-divergence DKL is not
analytically tractable but can be approximated extremely accurately using
DKL constant c1 c2 c3
with
c1
c2
c3
The same expression may be used to calculate the corresponding term
posterior approximation of section
DKL for the
Adaptive regularization through optimizing the dropout rate
The noise parameters used in dropout training the dropout rates are usually treated as fixed
hyperparameters but now that we have derived dropout?s variational objective making these
parameters adaptive is trivial simply maximize the variational lower bound with respect to We
can use this to learn a separate dropout rate per layer per neuron of even per separate weight In
section we look at the predictive performance obtained by making adaptive
We found that very large values of correspond to local optima from which it is hard to escape due
to large-variance gradients To avoid such local optima we found it beneficial to set a constraint
during training we maximize the posterior variance at the square of the posterior mean
which corresponds to a dropout rate of
Related Work
Pioneering work in practical variational inference for neural networks was done in where a
biased variational lower bound estimator was introduced with good results on recurrent neural network models In later work it was shown that even more practical estimators can be formed
for most types of continuous latent variables or parameters using a non-local reparameterization
trick leading to efficient and unbiased stochastic gradient-based variational inference These works
focused on an application to latent-variable inference extensive empirical results on inference of
global model parameters were reported in including succesful application to reinforcement
learning These earlier works used the relatively high-variance estimator upon which we improve Variable reparameterizations have a long history in the statistics literature but have only
recently found use for efficient gradient-based machine learning and inference 13 Related
is also probabilistic backpropagation an algorithm for inferring marginal posterior probabilities
however it requires certain tractabilities in the network making it insuitable for the type of models
under consideration in this paper
As we show here regularization by dropout can be interpreted as variational inference
DropConnect is similar to dropout but with binary noise on the weights rather than hidden units
DropConnect thus has a similar interpretation as variational inference with a uniform prior over the
weights and a mixture of two Dirac peaks as posterior In standout was introduced a variation
of dropout where a binary belief network is learned for producing dropout rates Recently
proposed another Bayesian perspective on dropout In recent work a similar reparameterization
is described and used for variational inference their focus is on closed-form approximations of the
variational bound rather than unbiased Monte Carlo estimators and also investigate a
Bayesian perspective on dropout but focus on the binary variant reports various encouraging
results on the utility of dropout?s implied prediction uncertainty
Experiments
We compare our method to standard binary dropout and two popular versions of Gaussian dropout
which we?ll denote with type A and type B. With Gaussian dropout type A we denote the pre-linear
Gaussian dropout from type denotes the post-linear Gaussian dropout from This way
the method names correspond to the matrix names in section A or where noise is injected
Models were implemented in Theano and optimization was performed using Adam with
default hyper-parameters and temporal averaging
Two types of variational dropout were included Type A is correlated weight noise as introduced
in section an adaptive version of Gaussian dropout type A. Variational dropout type has
independent weight uncertainty as introduced in section and corresponds to Gaussian dropout
type B.
A de facto standard benchmark for regularization methods is the task of MNIST hand-written digit
classification We choose the same architecture as a fully connected neural network with
hidden layers and rectified linear units ReLUs We follow the dropout hyper-parameter recommendations from these earlier publications which is a dropout rate of for the hidden layers
and for the input layer We used early stopping with all methods where the amount of
epochs to run was determined based on performance on a validation set
Variance We start out by empirically comparing the variance of the different available stochastic
estimators of the gradient of our variational objective To do this we train the neural network described above for either epochs test error or epochs test error using variational
dropout with independent weight noise After training we calculate the gradients for the weights of
the top and bottom level of our network on the full training set and compare against the gradient
estimates per batch of training examples Appendix contains the same analysis for the
case of variational dropout with correlated weight noise
Table shows that the local reparameterization trick yields the lowest variance among all variational
dropout estimators for all conditions although it is still substantially higher compared to not having any dropout regularization The variance scaling achieved by our estimator is especially
important early on in the optimization when it makes the largest difference compare weight sample
per minibatch and weight sample per data point The additional variance reduction obtained by our
estimator through drawing fewer random numbers section is about a factor of and this remains relatively stable as training progresses compare local reparameterization and weight sample
per data point
stochastic gradient estimator
local reparameterization ours
weight sample per data point slow
weight sample per minibatch standard
no dropout noise minimal var
top layer
epochs
top layer
epochs
bottom layer
epochs
bottom layer
epochs
Table Average empirical variance of minibatch stochastic gradient estimates examples for
a fully connected neural network regularized by variational dropout with independent weight noise
Speed We compared the regular SGVB estimator with separate weight samples per datapoint
with the efficient estimator based on local reparameterization in terms of wall-clock time efficiency
With our implementation on a modern GPU optimization with the na??ve estimator took seconds per epoch while the efficient estimator took seconds an over fold speedup
Classification error Figure shows test-set classification error for the tested regularization methods for various choices of number of hidden units Our adaptive variational versions of Gaussian
dropout perform equal or better than their non-adaptive counterparts and standard dropout under all
tested conditions The difference is especially noticable for the smaller networks In these smaller
networks we observe that variational dropout infers dropout rates that are on average far lower than
the dropout rates for larger networks This adaptivity comes at negligable computational cost
Classification error on the MNIST dataset
Classification error on the CIFAR-10 dataset
Figure Best viewed in color Comparison of various dropout methods when applied to fullyconnected neural networks for classification on the MNIST dataset Shown is the classification
error of networks with hidden layers averaged over runs he variational versions of Gaussian
dropout perform equal or better than their non-adaptive counterparts the difference is especially
large with smaller models where regular dropout often results in severe underfitting Comparison of dropout methods when applied to convolutional net a trained on the CIFAR-10 dataset for
different settings of network size The network has two convolutional layers with each and
feature maps respectively each with stride and followed by a softplus nonlinearity This is
followed by two fully connected layers with each hidden units
We found that slightly downscaling the KL divergence part of the variational objective can be beneficial Variational in figure denotes performance of type A variational dropout but with a
KL-divergence downscaled with a factor of this small modification seems to prevent underfitting
and beats all other dropout methods in the tested models
Conclusion
Efficiency of posterior inference using stochastic gradient-based variational Bayes SGVB can often
be significantly improved through a local reparameterization where global parameter uncertainty is
translated into local uncertainty per datapoint By injecting noise locally instead of globally at the
model parameters we obtain an efficient estimator that has low computational complexity can be
trivially parallelized and has low variance We show how dropout is a special case of SGVB with
local reparameterization and suggest variational dropout a straightforward extension of regular
dropout where optimal dropout rates are inferred from the data rather than fixed in advance We
report encouraging empirical results
Acknowledgments
We thank the reviewers and Yarin Gal for valuable feedback Diederik Kingma is supported by the
Google European Fellowship in Deep Learning Max Welling is supported by research grants from
Google and Facebook and the NWO project in Natural AI

<<----------------------------------------------------------------------------------------------------------------------->>

