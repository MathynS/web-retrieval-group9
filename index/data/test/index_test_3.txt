query sentence: tokenization of artificially-generated natural-language
---------------------------------------------------------------------
title: 3690-learning-from-multiple-partially-observed-views-an-application-to-multilingual-text-categorization.pdf

Learning from Multiple Partially Observed Views
an Application to Multilingual Text Categorization
Massih R. Amini
Interactive Language Technologies Group
National Research Council Canada
Nicolas Usunier
Laboratoire d?Informatique de Paris
Universit?e Pierre Marie Curie France
Massih-Reza.Amini@cnrc-nrc.gc.ca
Nicolas.Usunier@lip6.fr
Cyril Goutte
Interactive Language Technologies Group
National Research Council Canada
Cyril.Goutte@cnrc-nrc.gc.ca
Abstract
We address the problem of learning classifiers when observations have multiple
views some of which may not be observed for all examples We assume the
existence of view generating functions which may complete the missing views
in an approximate way This situation corresponds for example to learning text
classifiers from multilingual collections where documents are not available in all
languages In that case Machine Translation systems may be used to translate each document in the missing languages We derive a generalization error
bound for classifiers learned on examples with multiple artificially created views
Our result uncovers a trade-off between the size of the training set the number
of views and the quality of the view generating functions As a consequence
we identify situations where it is more interesting to use multiple views for learning instead of classical single view learning An extension of this framework is
a natural way to leverage unlabeled multi-view data in semi-supervised learning
Experimental results on a subset of the Reuters RCV1/RCV2 collections support
our findings by showing that additional views obtained from MT may significantly
improve the classification performance in the cases identified by our trade-off
Introduction
We study the learning ability of classifiers trained on examples generated from different sources
but where some observations are partially missing This problem occurs for example in non-parallel
multilingual document collections where documents may be available in different languages but
each document in a given language may not be translated in all any of the other languages
Our framework assumes the existence of view generating functions which may approximate missing examples using the observed ones In the case of multilingual corpora these view generating
functions may be Machine Translation systems which for each document in one language produce
its translations in all other languages Compared to other multi-source learning techniques
we address a different problem here by transforming our initial problem of learning from partially
observed examples obtained from multiple sources into the classical multi-view learning The contributions of this paper are twofold We first introduce a supervised learning framework in which
we define different multi-view learning tasks Our main result is a generalization error bound for
classifiers trained over multi-view observations From this result we induce a trade-off between the
number of training examples the number of views and the ability of view generating functions to
produce accurate additional views This trade-off helps us identify situations in which artificially
generated views may lead to substantial performance gains We then show how the agreement of
classifiers over their class predictions on unlabeled training data may lead to a much tighter trade-off
Experiments are carried out on a large part of the Reuters RCV1/RCV2 collections freely available
from Reuters using well-represented languages for text classification Our results show that our
approach yields improved classification performance in both the supervised and semi-supervised
settings
In the following two sections we first define our framework then the learning tasks we address
Section describes our trade-off bound in the Empirical Risk Minimization ERM setting and
shows how and when the additional artificially generated views may yield a better generalization
performance in a supervised setting Section shows how to exploit these results when additional
unlabeled training data are available in order to obtain a more accurate trade-off Finally section
describes experimental results that support this approach
Framework and Definitions
In this section we introduce basic definitions and the learning objectives that we address in our
setting of artificially generated representations
Observed and Generated Views
def
A multi-view observation is a sequence
xV where different views xv provide a representation of the same object in different sets Xv A typical example is given in where each
Web-page is represented either by its textual content first view or by the anchor texts which point
to it second view In the setting of multilingual classification each view is the textual representation of a document written in a given language English German French
We consider binary classification problems where given a multi-view observation some of the
views are not observed we obviously require that at least one view is observed This happens for instance when documents may be available in different languages yet a given document
may only be available in a single language Formally our observations belong to the input set
def
XV where xv means that the v-th view is not observed
def
In binary classification we assume that examples are pairs with
drawn
according to a fixed but unknown distribution over such that xv
at least one view is available In multilingual text classification a parallel corpus is a dataset where
all views are always observed xv while a comparable corpus is a
dataset where only one view is available for each example xv
For a given observation the views such that xv will be called the observed views The
originality of our setting is that we assume that we have view generating functions Xv
Xv which take as input a given view xv and output an element of Xv that we assume is close
to what xv would be if it was observed In our multilingual text classification example the view
generating functions are Machine Translation systems These generating functions can then be used
to create surrogate observations such that all views are available For a given partially observed
the completed observation is obtained as
xv
xv
xv
if xv
otherwise where is such that xv
In this paper we focus on the case where only one view is observed for each example This setting
corresponds to the problem of learning from comparable corpora which will be the focus of our
experiments Our study extends to the situation where two or more views may be observed in a
straightforward manner Our setting differs from previous multi-view learning studies mainly on
the straightforward generalization to more than two views and the use of view generating functions
to induce the missing views from the observed ones
Learning objective
The learning task we address is to find in some predefined classifier set the stochastic classifier
that minimizes the classification error on multi-view examples with potentially unobserved views
drawn according to some distribution as described above Following the standard multi-view
framework in which all views are observed we assume that we are given deterministic
classifier sets Hv each working on one specific view1 That is for each view Hv is a set
of functions hv Xv The final set of classifiers contains stochastic classifiers whose
output only depends on the outputs of the view-specific classifiers That is associated to a set of
classifiers there is a function Hv such that
hV hv Hv
For simplicity in the rest of the paper when the context is clear the function hV
will be denoted by ch1 The overall objective of learning is therefore to find with low
generalization error defined as
where is a pointwise error for instance the loss
In the following sections we address this learning task in our framework in terms of supervised and
semi-supervised learning
Supervised Learning Tasks
We first focus on the supervised learning case We assume that we have a training set of
examples drawn according to a distribution as presented in the previous section Depending
on how the generated views are used at both training and test stages we consider the following
learning scenarios
Baseline This setting corresponds to the case where each view-specific classifier is trained using
the corresponding observed view on the training set and prediction for a test example is
done using the view-specific classifier corresponding to the observed view
hv arg min
xv
h?Hv
In this case we pose cbh1 hv xv where is the observed view for Notice
that this is the most basic way of learning a text classifier from a comparable corpus
Generated Views as Additional Training Data The most natural way to use the generated
views for learning is to use them as additional training material for the view-specific classifiers
hv arg min
xv
h?Hv
with defined by eq Prediction is still done using the view-specific classifiers corresponding to the observed view cbh1 hv xv Although the test set
distribution is a subdomain of the training set distribution this mismatch is hopefully
compensated by the addition of new examples
Multi-view Gibbs Classifier In order to avoid the potential bias introduced by the use of generated views only during training we consider them also during testing This becomes a standard multi-view setting where generated views are used exactly as if they were observed
The view-specific classifiers are trained exactly as above but the prediction is carried out with respect to the probability distribution of classes by estimating the probability
of class membership in class from the mean prediction of each view-specific classifier
cmg
h1
hv xv
We assume deterministic view-specific classifiers for simplicity and with no loss of generality
Multi-view Majority Voting With view generating functions involved in training and test a natural way to obtain a generally deterministic classifier with improved performance is to
take the majority vote associated with the Gibbs classifier The view-specific classifiers are
again trained as in eq but the final prediction is done using a majority vote
PV
if hv xv V2
mv
ch1
I
otherwise
hv
Where is the indicator function The classifier outputs either the majority voted class
or either one of the classes with probability in case of a tie
The trade-offs with the ERM principle
We now analyze how the generated views can improve generalization performance Essentially
the trade-off is that generated views offer additional training material therefore potentially helping
learning but can also be of lower quality which may degrade learning
The following theorem sheds light on this trade-off by providing bounds on the baseline multiview strategies Note that such trade-offs have already been studied in the literature although in
different settings Our first result is the following theorem The notion of function class capacity used here is the empirical Rademacher complexity Proof is given in the
supplementary material
Theorem Let be a distribution over satisfying xv
Let be a dataset of examples drawn according to D. Let be the
loss and let Hv be the view-specific deterministic classifier sets For each view denote
def
xv Hv and denote for any sequence Xv of
Hv
Hv the empirical Rademacher complexity of Hv on Then we have
size mv
Baseline setting for all with probability at least over
inf
hv Hv
mv
Rmv Hv
2m
def
xvi and xvi mv and hv Hv is the
where for all
classifier minimizing the empirical risk on
Multi-view Gibbs classification setting for all with probability at least over
mg
inf
Rm Hv
hv Hv
2m
def
where for all
xvi hv Hv is the classifier minimizing the
empirical risk on and
inf cmg
inf
hv Hv
hv Hv
This theorem gives us a rule for whether it is preferable to learn only with the observed views
the baseline setting or preferable to use the view-generating functions in the multi-view Gibbs
Hv
classification setting we should use the former when mmv
Hv and the latter otherwise
Let us first explain the role of The difference between the two settings is in the train
and test distributions for the view-specific
classifiers
compares the best achievable error for each
of the distribution inf h?v Hv is the best achievable error in the baseline setting
without generated
views
with the automatically generated views the best achievable error becomes
inf h?v Hv cmg
Therefore measures the loss incurred by using the view generating functions In a favorable
situation the quality of the generating functions will be sufficient to make small
The terms depending on the complexity of the class of functions may be better explained using
orders of magnitude Typically the Rademacher complexity for a sample of size is usually of
order
Assuming for simplicity
that all empirical Rademacher complexities in Theorem are approxi
mately equal to where is the size of the sample on which they are computed and assuming
that mv m/V for all The trade-off becomes
Choose the Multi-view Gibbs classification setting when
This means that we expect important performance gains when the number of examples is small the
generated views of sufficiently high quality for the given classification task and/or there are many
views available Note that our theoretical framework does not take the quality of the MT system in a
standard way in our setup a good translation system is roughly one which generates bag-of-words
representations that allow to correctly discriminate between classes
Majority voting One advantage of the multi-view setting at prediction time is that we can use a
majority voting scheme as described in Section In such a case we expect that cmv
mg
if the view-specific classifiers are not correlated in their errors It can not be guaranteed
mg
in general though since in general we can not prove any better than cmv
Agreement-Based Semi-Supervised Learning
One advantage of the multi-view settings described in the previous section is that unlabeled training
examples may naturally be taken into account in a semi?supervised learning scheme using existing
approaches for multi-view learning
In this section we describe how under the framework of the supervised learning trade-off
presented above can be improved using extra unlabeled examples This framework is based on
the notion of disagreement between the various view-specific classifiers defined as the expected
variance of their outputs
def
hV
hv xv
hv xv
The overall idea is that a set of good view-specific classifiers should agree on their predictions
making the expected variance small This notion of disagreement has two key advantages First it
does not depend on the true class labels making its estimation easy over a large unlabeled training
set The second advantage is that if during training it turns out that the view-specific classifiers
have a disagreement of at most on the unlabeled set the set of possible view-specific classifiers
that needs be considered in the supervised learning stage is reduced to
def
Hv
Hv Hv h?V
Thus the more the various view-specific classifiers tend to agree the smaller the possible set of
functions will be This suggests a simple way to do semi-supervised learning the unlabeled data
can be used to choose among the classifiers minimizing the empirical risk on the labeled training
set those with best generalization performance by choosing the classifiers with highest agreement
on the unlabeled set This is particularly interesting when the number of labeled examples is small
as the train error is usually close to
Theorem of provides a theoretical value for the minimum number of unlabeled examples required to estimate with precision and probability this bound depends on
Hv The following result gives a tighter bound of the generalization error of the multi-view
Gibbs classifier when unlabeled data are available The proof is similar to Theorem in
Proposition Let and Under the conditions and notations of Theorem
assume furthermore that we have access to unlabeled examples drawn
according to the marginal distribution of on
Then with
probability at least if the empirical risk minimizers hv
arg minh?Hv xv xv have a disagreement less than on the unlabeled
set we have
cmg
h1
Rm Hv
inf
hv Hv
2m
We can now rewrite the trade-off between the baseline setting and the multi-view Gibbs classifier
taking semi-supervised learning into account Using orders of magnitude and assuming that for
Hv is O(du with the proportional factor du the trade-off
each view
becomes
Choose the mutli-view Gibbs classification setting when du
Thus the improvement is even more important than in the supervised setting Also note that the
more views we have the greater the reduction in classifier set complexity should be
Notice that this semi-supervised learning principle enforces agreement between the view specific
classifiers In the extreme case where they almost always give the same output majority voting is
then nearly equivalent to the Gibbs classifier when all voters agree any vote is equal to the majority
vote We therefore expect the majority vote and the Gibbs classifier to yield similar performance in
the semi-supervised setting
Experimental Results
In our experiments we address the problem of learning document classifiers from a comparable
corpus We build the comparable corpus by sampling parts of the Reuters RCV1 and RCV2 collections We used newswire articles written in languages English French German
Italian and Spanish We focused on relatively populous classes CCAT ECAT
GCAT M11.
For each language and each class we sampled up to documents from the RCV1 for English
or RCV2 for other languages Documents belonging to more than one of our classes were assigned the label of their smallest class This resulted in documents per language and
documents per class Table In addition we reserved a test split containing of the documents respecting class and language proportions for testing For each document we indexed
the text appearing in the title headline tag and the body body tags of each article As preprocessing we lowercased mapped digits to a single digit token and removed non alphanumeric
tokens We also filtered out function words using a stop-list as well as tokens occurring in less than
documents
Documents were then represented as a bag of words using a TFIDF-based weighting scheme The
final vocabulary size for each language is given in table The artificial views were produced using
Table Distribution of documents over languages and classes in the comparable corpus
Language
English
French
German
Italian
Spanish
Total
docs
18
26
29
24
tokens
24
34
Class
CCAT
ECAT
GCAT
Size all lang
18
13
19
19
19
PORTAGE a statistical machine translation system developed at NRC Each document from
the comparable corpus was thus translated to the other languages.2
For each class we set up a binary classification task by using all documents from that class as
positive examples and all others as negative We first present experimental results obtained in
supervised learning using various amounts of labeled examples We rely on linear SVM models as
base classifiers using the SVM-Perf package For comparisons we employed the four learning
strategies described in section the single-view baseline svb generated views as
additional training data gvb multi-view Gibbs mvg and multi-view majority
voting mvm Recall that the second setting gvb is the most straightforward way to train and
test classifiers when additional examples are available generated from different sources It can
thus be seen as a baseline approach as opposed to the last two strategies mvg and mvm where
view-specific classifiers are both trained and tested over both original and translated documents
Note also that in our case views additional training examples obtained from machine
translation represent times as many labeled examples as the original texts used to train the baseline
svb All test results were averaged over randomly sampled training sets
Table Test classification accuracy and F1 in the supervised setting for both baselines svb gvb
Gibbs mvg and majority voting mvw strategies averaged over random sets of labeled
examples per view indicates statistically significantly worse performance that the best result
according to a Wilcoxon rank sum test
CCAT
ECAT
GCAT
Strategy
Acc.
F1
Acc. F1
Acc.
F1
Acc. F1
Acc. F1
Acc.
F1
svb
gvb
mvg
mvm
Results obtained in a supervised setting with only labeled documents per language for training are
summarized in table All learning strategies using the generated views during training outperform
the single-view baseline This shows that although imperfect artificial views do bring additional
information that compensates the lack of labeled data Although the multi-view Gibbs classifier
predicts based on a translation rather than the original in of cases it produces almost identical
performance to the gvb run which only predicts using the original text These results indicate that
the translation produced by our MT system is of sufficient quality for indexing and classification
purposes Multi-view majority voting reaches the best performance yielding a improvement in accuracy over the baseline A similar increase in performance is observed using F1 which
suggests that the multi-view SVM appropriately handles unbalanced classes
Figure shows the learning curves obtained on classes ECAT and M11. These figures show
that when there are enough labeled examples around for these classes the artificial views do
not provide any additional useful information over the original-language examples These empirical
results illustrate the trade-off discussed at the previous section When there are sufficient original
labeled examples additional generated views do not provide more useful information for learning
than what view-specific classifiers have available already
We now investigate the use of unlabeled training examples for learning the view-specific classifiers
Our overall aim is to illustrate our findings from section Recall that in the case where view-specific
classifiers are in agreement over the class labels of a large number of unlabeled examples the multiview Gibbs and majority vote strategies should have the same performance In order to enforce
agreement between classifiers on the unlabeled set we use a variant of the iterative co-training
algorithm Given the view-specific classifiers trained on an initial set of labeled examples we
iteratively assign pseudo-labels to the unlabeled examples for which all classifier predictions agree
We then train new view-specific classifiers on the joint set of the original labeled examples and those
unanimously pseudo-)labeled ones Key differences between this algorithm and co-training are the
number of views used for learning instead of and the use of unanimous and simultaneous
labeling
The dataset is available from http://multilingreuters.iit.nrc.ca/ReutersMultiLingualMultiView.htm
ECAT
F1
F1
F1
mvm
mvg
svb
mvm
mvg
svb
Labeled training size
mvm
mvg
svb
Labeled training size
Labeled training size
Figure F1 size of the labeled training set for classes ECAT and M11.
We call this iterative process self-learning multiple-view algorithm as it also bears a similarity with
the self-training paradigm Prediction from the multi-view SVM models obtained from this
self-learning multiple-view algorithm is done either using Gibbs mvgs or majority voting mvm
These results are shown in table For comparison we also trained a TSVM model on each view
separately a semi-supervised equivalent to the single-view baseline strategy Note that the TSVM
model mostly out-performs the supervised baseline svb although the F1 suffers on some classes
This suggests that the TSVM has trouble handling unbalanced classes in this setting
Table Test classification accuracy and F1 in the semi-supervised setting for single-view TSVM
averaged over
and multi-view self-learning using either Gibbs mvgs or majority voting mvm
random sets using labeled examples per view to start For comparison we provide the single-view
baseline and multi-view majority voting performance for supervised learning
Strategy
svb
mvm
TSVM
mvgs
mvm
Acc.
F1
CCAT
Acc.
F1
Acc.
F1
ECAT
Acc. F1
GCAT
Acc.
F1
Acc.
F1
The multi-view self-learning algorithm achieves the best classification performance in both accuracy
and F1 and significantly outperforms both the TSVM and the supervised multi-view strategy in all
classes As expected the performance of both mvgs and mvm
strategies are similar
Conclusion
The contributions of this paper are twofold First we proposed a bound on the risk of the Gibbs
classifier trained over artificially completed multi-view observations which directly corresponds to
our target application of learning text classifiers from a comparable corpus We showed that our
bound may lead to a trade-off between the size of the training set the number of views and the
quality of the view generating functions Our result identifies in which case it is advantageous to
learn with additional artificial views as opposed to sticking with the baseline setting in which a classifier is trained over single view observations This result leads to our second contribution which is
a natural way of using unlabeled data in semi-supervised multi-view learning We showed that in the
case where view-specific classifiers agree over the class labels of additional unlabeled training data
the previous trade-off becomes even much tighter Empirical results on a comparable multilingual
corpus support our findings by showing that additional views obtained using a Machine Translation
system may significantly increase classification performance in the most interesting situation when
there are few labeled data available for training
Acknowlegdements This work was supported in part by the IST Program of the European Community under the PASCAL2 Network of Excellence

----------------------------------------------------------------

title: 2787-subsequence-kernels-for-relation-extraction.pdf

Subsequence Kernels for Relation Extraction
Razvan C. Bunescu
Department of Computer Sciences
University of Texas at Austin
University Station
Austin TX
razvan@cs.utexas.edu
Raymond J. Mooney
Department of Computer Sciences
University of Texas at Austin
University Station
Austin TX
mooney@cs.utexas.edu
Abstract
We present a new kernel method for extracting semantic relations between entities in natural language text based on a generalization of subsequence kernels This kernel uses three types of subsequence patterns
that are typically employed in natural language to assert relationships
between two entities Experiments on extracting protein interactions
from biomedical corpora and top-level relations from newspaper corpora
demonstrate the advantages of this approach
Introduction
Information Extraction is an important task in natural language processing with many
practical applications It involves the analysis of text documents with the aim of identifying
particular types of entities and relations among them Reliably extracting relations between
entities in natural-language documents is still a difficult unsolved problem Its inherent
difficulty is compounded by the emergence of new application domains with new types
of narrative that challenge systems developed for other well-studied domains Traditionally IE systems have been trained to recognize names of people organizations locations
and relations between them MUC ACE For example in the sentence protesters
seized several pumping stations the task is to identify a OCATED AT relationship between protesters ERSON entity and stations OCATION entity Recently substantial resources have been allocated for automatically extracting information from biomedical
corpora and consequently much effort is currently spent on automatically identifying biologically relevant entities as well as on extracting useful biological relationships such as
protein interactions or subcellular localizations For example the sentence specifically binds Fas ligand asserts an interaction relationship between the two proteins TR6
and Fas ligand As in the case of the more traditional applications of IE systems based on
manually developed extraction rules were soon superseded by information extractors
learned through training on supervised corpora One challenge posed by the biological domain is that current systems for doing part-of-speech POS tagging or parsing do
not perform as well on the biomedical narrative as on the newspaper corpora on which they
were originally trained Consequently IE systems developed for biological corpora need
to be robust to POS or parsing errors or to give reasonable performance using shallower
but more reliable information such as chunking instead of parsing
Motivated by the task of extracting protein-protein interactions from biomedical corpora
we present a generalization of the subsequence kernel from that works with sequences
containing combinations of words and word classes This generalized kernel is further
tailored for the task of relation extraction Experimental results show that the new relation
kernel outperforms two previous rule-based methods for interaction extraction With a
small modification the same kernel is used for extracting top-level relations from ACE
corpora providing better results than a recent approach based on dependency tree kernels
Background
One of the first approaches to extracting protein interactions is that of Blaschke described in Their system is based on a set of manually developed rules where each
rule frame is a sequence of words POS tags and two protein-name tokens Between every two adjacent words is a number indicating the maximum number of intervening words allowed when matching the rule to a sentence An example rule is interaction
of with where is used to denote a protein name A sentence matches the rule if and only if it satisfies the word constraints in the given order and
respects the respective word gaps
In the authors described a new method ELCS Extraction using Longest Common Subsequences that automatically learns such rules ELCS rule representation is similar to
that in except that it currently does not use POS tags but allows disjunctions of
words An example rule learned by this system is interaction between of
Words in square brackets separated by indicate disjunctive
lexical constraints one of the given words must match the sentence at that position
The numbers in parentheses between adjacent constraints indicate the maximum number
of unconstrained words allowed between the two
Extraction using a Relation Kernel
Both Blaschke and ELCS do interaction extraction based on a limited set of matching
rules where a rule is simply a sparse gappy subsequence of words or POS tags anchored
on the two protein-name tokens Therefore the two methods share a common limitation
either through manual selection Blaschke or as a result of the greedy learning procedure
ELCS they end up using only a subset of all possible anchored sparse subsequences
Ideally we would want to use all such anchored sparse subsequences as features with
weights reflecting their relative accuracy However explicitly creating for each sentence a
vector with a position for each such feature is infeasible due to the high dimensionality
of the feature space Here we can exploit dual learning algorithms that process examples
only via computing their dot-products such as the Support Vector Machines SVMs
Computing the dot-product between two such vectors amounts to calculating the number
of common anchored subsequences between the two sentences This can be done very
efficiently by modifying the dynamic programming algorithm used in the string kernel
from to account only for common sparse subsequences constrained to contain the two
protein-name tokens We further prune down the feature space by utilizing the following
property of natural language statements when a sentence asserts a relationship between
two entity mentions it generally does this using one of the following three patterns
Fore?Between words before and between the two entity mentions are simultaneously used to express the relationship Examples interaction of hP1 with hP2 activation of hP1 by hP2
Between only words between the two entities are essential for asserting the relationship Examples interacts with hP2 is activated by hP2
Between?After words between and after the two entity mentions are simultaneously used to express the relationship Examples hP2 complex and hP2
interact
Another observation is that all these patterns use at most words to express the relationship
not counting the two entity names Consequently when computing the relation kernel
we restrict the counting of common anchored subsequences only to those having one of
the three types described above with a maximum word-length of This type of feature
selection leads not only to a faster kernel computation but also to less overfitting which
results in increased accuracy Section for comparative experiments
The patterns enumerated above are completely lexicalized and consequently their performance is limited by data sparsity This can be alleviated by categorizing words into classes
with varying degrees of generality and then allowing patterns to use both words and their
classes Examples of word classes are POS tags and generalizations over POS tags such as
Noun Active Verb or Passive Verb The entity type can also be used if the word is part of
a known named entity as well as the type of the chunk containing the word when chunking information is available Content words such as nouns and verbs can also be related to
their synsets via WordNet Patterns then will consist of sparse subsequences of words POS
tags general POS GPOS tags entity and chunk types or WordNet synsets For example
Noun of hP1 by hP2 is an FB pattern based on words and general POS tags
Subsequence Kernels for Relation Extraction
We are going to show how to compute the relation kernel described in the previous section
in two steps First in Section we present a generalization of the subsequence kernel
from This new kernel works with patterns construed as mixtures of words and word
classes Based on this generalized subsequence kernel in Section we formally define
and show the efficient computation of the relation kernel used in our experiments
A Generalized Subsequence Kernel
Let be some disjoint feature spaces Following the example in Section
could be the set of words the set of POS tags etc Let
be the set of all possible feature vectors where a feature vector would be associated with
each position in a sentence Given two feature vectors let denote the
number of common features between and The next notation follows that introduced
in Thus let be two sequences over the finite set and let denote the length
of s1 The sequence s[i is the contiguous subsequence si of Let
be a sequence of indices in in ascending order We define the length
of the index sequence in as i1 Similarly is a sequence of indices in
Let be the set of all possible features We say that the sequence
is a sparse subsequence of if there is a sequence of indices such that
uk sik for all Equivalently we write as a shorthand for the
component-wise relationship between and
Finally let Kn Equation be the number of weighted sparse subsequences of
length common to and where the weight of is for
some
Kn
Because for two fixed index sequences
and both of length the size of the set
Qn
is c(sik tjk then we can rewrite Kn as in
Equation
c(sik tjk
Kn
We use as a decaying factor that penalizes longer subsequences For sparse subsequences
this means that wider gaps will be penalized more which is exactly the desired behavior
for our patterns Through them we try to capture head-modifier dependencies that are
important for relation extraction for lack of reliable dependency information the larger
the word gap is between two words the less confident we are in the existence of a headmodifier relationship between them
To enable an efficient computation of Kn we use the auxiliary function Kn with a similar
definition as Kn the only difference being that it counts the length from the beginning of
the particular subsequence to the end of the strings and as illustrated in Equation
Kn
An equivalent formula for Kn is obtained by changing the exponent of from
Equation to i1 j1
Based on all definitions above Kn can be computed in time by modifying the
recursive computation from with the new factor as shown in Figure In this
figure the sequence sx is the result of appending to with ty defined in a similar way
To avoid clutter the parameter is not shown in the argument list of and unless it
is instantiated to a specific constant
K0
Ki ty
Ki
Kn
or all
Ki
Ki Ki
Kn
Figure Computation of subsequence kernel
Computing the Relation Kernel
As described in Section the input consists of a set of sentences where each sentence contains exactly two entities protein names in the case of interaction extraction In Figure
we show the segments that will be used for computing the relation kernel between two example sentences and In sentence for instance and are the two entities sf is the
sentence segment before sb is the segment between and and sa is the sentence
segment after For convenience we also include the auxiliary segment sb sb
whose span is computed as l(sb l(sb all length computations we consider
and as contributing one unit only
sb
sf
sa
s?b
tf
tb
y1
ta
y2
t?b
Figure Sentence segments
The relation kernel computes the number of common patterns between two sentences and
where the set of patterns is restricted to the three types introduced in Section Therefore
the kernel rK(s is expressed as the sum of three sub-kernels bK(s counting the
rK(s
bK(s bK(s baK(s
bKi
Ki sb tb y1 y2 l(sb
bK(s
bKi Kj sf tf
fbmax
i,j
bK(s
bKi
bmax
baK(s
bKi Kj
a ta
bamax
i,j
Figure Computation of relation kernel
number of common fore?between patterns bK(s for between patterns and baK(s
for between?after patterns as in Figure
All three sub-kernels include in their computation the counting of common subsequences
between sb and tb In order to speed up the computation all these common counts can be
calculated separately in bKi which is defined as the number of common subsequences of
length between sb and tb anchored at and y1 respectively constrained to
start at and to end at Then bK simply counts the number of subsequences
that match positions before the first entity and positions between the entities constrained
to have length less than a constant bmax To obtain a similar formula for baK we simply
use the reversed mirror version of segments sa and ta
a and ta In Section
we observed that all three subsequence patterns use at most words to express a relation
therefore we set constants bmax bmax and bamax to Kernels and are computed
using the procedure described in Section
Experimental Results
The relation kernel ERK is evaluated on the task of extracting relations from two corpora with different types of narrative which are described in more detail in the following
sections In both cases we assume that the entities and their labels are known All preprocessing steps sentence segmentation tokenization POS tagging and chunking were
performed using the OpenNLP1 package If a sentence contains entities it
is replicated into n2 sentences each containing only two entities If the two entities are
known to be in a relationship then the replicated sentence is added to the set of corresponding positive sentences otherwise it is added to the set of negative
sentences During testing
a sentence having entities is again replicated into n2 sentences in a similar way
The relation kernel is used in conjunction with SVM learning in order to find a decision
hyperplane that best separates the positive examples from negative examples We modified the LibSVM2 package by plugging in the kernel described above In all experiments
the decay factor is set to The performance is measured using precision percentage of correctly extracted relations out of total extracted and recall percentage of correctly extracted relations out of total number of relations annotated in the corpus When
PR curves are reported the precision and recall are computed using output from 10-fold
cross-validation The graph points are obtained by varying a threshold on the minimum
acceptable extraction confidence based on the probability estimates from LibSVM
URL http://opennlp.sourceforge.net
URL:http://www.csie.ntu.edu.tw/?cjlin/libsvm
Interaction Extraction from AImed
We did comparative experiments on the AImed corpus which has been previously used
for training the protein interaction extraction systems in It consists of Medline
abstracts of which are known to describe interactions between human proteins while
the other do not refer to any interaction There are protein

----------------------------------------------------------------

title: 6295-can-active-memory-replace-attention.pdf

Can Active Memory Replace Attention
ukasz Kaiser
Google Brain
lukaszkaiser@google.com
Samy Bengio
Google Brain
bengio@google.com
Abstract
Several mechanisms to focus attention of a neural network on selected parts of its
input or memory have been used successfully in deep learning models in recent
years Attention has improved image classification image captioning speech
recognition generative models and learning algorithmic tasks but it had probably
the largest impact on neural machine translation
Recently similar improvements have been obtained using alternative mechanisms
that do not focus on a single part of a memory but operate on all of it in parallel
in a uniform way Such mechanism which we call active memory improved over
attention in algorithmic tasks image processing and in generative modelling
So far however active memory has not improved over attention for most natural
language processing tasks in particular for machine translation We analyze this
shortcoming in this paper and propose an extended model of active memory that
matches existing attention models on neural machine translation and generalizes
better to longer sentences We investigate this model and explain why previous
active memory models did not succeed Finally we discuss when active memory
brings most benefits and where attention can be a better choice
Introduction
Recent successes of deep neural networks have spanned many domains from computer vision to
speech recognition and many other tasks In particular sequence-to-sequence recurrent neural
networks RNNs with long short-term memory LSTM cells have proven especially successful
at natural language processing NLP tasks including machine translation
The basic sequence-to-sequence architecture for machine translation is composed of an RNN encoder
which reads the source sentence one token at a time and transforms it into a fixed-sized state vector
This is followed by an RNN decoder which generates the target sentence one token at a time from
the state vector While a pure sequence-to-sequence recurrent neural network can already obtain good
translation results it suffers from the fact that the whole sentence to be translated needs to be
encoded into a single fixed-size vector This clearly manifests itself in the degradation of translation
quality on longer sentences Figure and hurts even more when there is less training data
In a successful mechanism to overcome this problem was presented a neural model of attention
In a sequence-to-sequence model with attention one retains the outputs of all steps of the encoder
and concatenates them to a memory tensor At each step of the decoder a probability distribution
over this memory is computed and used to estimate a weighted average encoder representation to be
used as input to the next decoder step The decoder can hence focus on different parts of the encoder
representation while producing tokens Figure illustrates a single step of this process
The attention mechanism has proven useful well beyond the machine translation task Image models
can benefit from attention too for instance image captioning models can focus on the relevant parts
of the image when describing it generative models for images yield especially good results with
attention as was demonstrated by the DRAW model where the network focuses on a part of the
Conference on Neural Information Processing Systems NIPS Barcelona Spain
new memory memory
new state
mask over memory
memory
state
Figure Attention model The state vector is used to compute a probability distribution over memory
Weighted average of memory elements with focus on one of them is used to compute the new state
image to produce at a given time Another interesting use-case for the attention mechanism is the
Neural Turing Machine which can learn basic algorithms and generalize beyond the length of
the training instances
While the attention mechanism is very successful one important limitation is built into its definition
Since the attention mask is computed using a Softmax it by definition tries to focus on a single
element of the memory it is attending to In the extreme case also known as hard attention one of
the memory elements is selected and the selection is trained using the REINFORCE algorithm since
this is not differentiable It is easy to demonstrate that this restriction can make some tasks
almost unlearnable for an attention model For example consider the task of adding two decimal
numbers presented one after another like this
Input
Output
A recurrent neural network can have the carry-over in its state and could learn to shift its attention to
subsequent digits But that is only possible if there are two attention heads attending to the first and
to the second number If only a single attention mechanism is present the model will have a hard
time learning this task and will not generalize properly as was demonstrated in
A solution to this problem already proposed in the recent literature for instance the Neural GPU
from is to allow the model to access and change all its memory at each decoding step We
will call this mechanism an active memory While it might seem more expensive than attention
models it is actually not since the attention mechanism needs to compute an attention score for all
its memory as well in order to focus on the most appropriate part The approximate complexity of an
attention mechanism is therefore the same as the complexity of the active memory In practice we
get step-times around second for an active memory model the Extended Neural GPU introduced
below and second for a comparable model with an attention mechanism But active memory can
potentially make parallel computations on the whole memory as depicted in Figure
new memory
memory
Figure Active memory model The whole memory takes part in the computation at every step
Each element of memory is active and changes in a uniform way using a convolution
Active memory is a natural choice for image models as they usually operate on a canvas And indeed
recent works have shown that actively updating the canvas that will be used to produce the final results
can be beneficial Residual networks the currently best performing model on the ImageNet task
falls into this category In it was shown that the weights of different layers of a residual network
can be tied so it becomes recurrent without degrading performance Other models that operate on
the whole canvas at each step were presented in Both of these models are generative and
show very good performance yielding better results than the original DRAW model Thus the active
memory approach seems to be a better choice for image models
But what about non-image models The Neural GPUs demonstrated that active memory yields
superior results on algorithmic tasks But can it be applied to real-world problems In particular
the original attention model brought a great success to natural language processing esp to neural
machine translation Can active memory be applied to this task on a large scale
We answer this question positively by presenting an extension of the Neural GPU model that yields
good results for neural machine translation This model allows us to investigate in depth a number of
questions about the relationship between attention and active memory We clarify why the previous
active memory model did not succeed on machine translation by showing how it is related to the
inherent dependencies in the target distributions and we study a few variants of the model that show
how a recurrent structure on the output side is necessary to obtain good results
Active Memory Models
In the previous section we used the term active memory broadly referring to any model where every
part of the memory undergoes active change at every step This is in contrast to attention models
where only a small part of the memory changes at every step or where the memory remains constant
The exact implementation of an active change of the memory might vary from model to model In
the present paper we will focus on the most common ways this change is implemented that all rely
on the convolution operator
The convolution acts on a kernel bank and a 3-dimensional tensor Our kernel banks are 4-dimensional
tensors of shape kw kh they contain kw kh m2 parameters where kw and kh are
kernel width and height A kernel bank can be convolved with a 3-dimensional tensor of shape
which results in the tensor of the same shape as defined by
bkw
bkh
s[x
u=b?kw v=b?kh
In the equation above the index might sometimes be negative or larger than the size of and
in such cases we assume the value is This corresponds to the standard convolution operator used
in many deep learning toolkits with zero padding on both sides and stride Using the standard
operator has the advantage that it is heavily optimized and can directly benefit from any new work
on optimizing convolutions
Given a memory tensor an active memory model will produce the next memory s0 by using a
number of convolutions on and combining them In the most basic setting a residual active memory
model will be defined as
s0
it will only add to an already existing state
While residual models have been successful in image analysis and generation they might
suffer from the vanishing gradient problem in the same way as recurrent neural networks do Therefore in the same spirit as LSTM gates and GRU gates improve over pure RNNs one can
introduce convolutional LSTM and GRU operators Let us focus on the convolutional GRU which
we define in the same way as in namely
CGRU(s tanh(U where
and
As a baseline for our investigation of active memory models we will use the Neural GPU model from
depicted in Figure and defined as follows The given sequence in of discrete
o1
i1
CGRU1
CGRU2
CGRU1
CGRU2
on
in
s0
s1
sn
Figure Neural GPU with layers and width unfolded in time
symbols from I is first embedded into the tensor s0 by concatenating the vectors obtained
from an embedding lookup of the input symbols into its first column More precisely we create the
starting tensor s0 of shape by using an embedding matrix of shape and setting
s0 E[ik python notation for all here i1 in is the input All other
elements of s0 are set to Then we apply different CGRU gates in turn for steps to produce the
final tensor sfin
CGRUl CGRUl?1 CGRU1 st and sfin sn
The result of a Neural GPU is produced by multiplying each item in the first column of sfin by an
output matrix to obtain the logits lk Osfin and then selecting the largest one ok
argmax(lk During training we use the standard loss function we compute a Softmax over the
logits lk and use the negative log probability of the target as the loss
The Markovian Neural GPU
The baseline Neural GPU model yields very poor results on neural machine translation its per-word
perplexity on WMT1 does not go below good models on this task go below and its BLEU
scores are also very bad below while good models are higher than Which part of the model is
responsible for such bad results
It turns out that the main culprit is the output generator As one can see in Figure above every
output symbol is generated independently of all other output symbols conditionally only on the state
sfin This is fine for learning purely deterministic functions like the toy tasks the Neural GPU was
designed for But it does not work for harder real-world problems where there could be multiple
possible outputs for each input
The most basic way to mitigate this problem is to make every output symbol depend on the previous
output This only changes the output generation not the state so the definition of the model is the
same as above until sfin The result is then obtained by multiplying by an output matrix each item
from the first column of sfin concatenated with the embedding of the previous output generated by
another embedding matrix
lk concat(sfin
For we use a special symbol GO and to get the output we select ok argmax(lk
During training we use the standard loss function we compute a Softmax over the logits lk and
use the negative log probability of the target as the loss Also as is standard in recurrent networks
we use teacher forcing during training we provide the true output label as instead of using
the previous output generated by the model This means that the loss incurred from generating ok
does not directly influence the value of We depict this model in Figure
The Extended Neural GPU
The Markovian Neural GPU yields much better results on neural machine translation than the baseline
model its per-word perplexity reaches about and its BLEU scores improve a bit But these results
are still far from those achieved by models with attention
See Section for more details on the experimental setting
o1
o2
o3
i1
CGRU1
CGRU2
CGRU1
CGRU2
on
in
s0
s1
sn
Figure Markovian Neural GPU. Each output ok is conditionally dependent on the final tensor
sfin sn and the previous output symbol
p0
o1
p1
o2
p2
on
i1
CGRU CGRU
CGRU
CGRUd
CGRUd CGRUd
CGRUd
in
s0
s1
sn d0
d1
d2
dn
Figure Extended Neural GPU with active memory decoder See the text below for definition
Could it be that the Markovian dependence of the outputs is too weak for this problem that a full
recurrent dependence of the state is needed for good performance We test this by extending the
baseline model with an active memory decoder as depicted in Figure
The definition of the Extended Neural GPU follows the baseline model until sfin sn We consider
sn as the starting point for the active memory decoder we set d0 sn In the active memory
decoder we will also use a separate output tape tensor of the same shape as d0 is of shape
We start with p0 set to all and define the decoder states by
CGRUdl CGRUdl?1 CGRUd1 st pt pt pt
where CGRUd is defined just like CGRU in Equation but with additional input as highlighted
below in bold
CGRUd tanh(U where
and
We generate the k-th output by multiplying the k-th vector in the first column of dk by the output
matrix lk dk We then select ok argmax(lk The symbol ok is then embedded
back into a dense representation using another embedding matrix and we put it into the k-th place
on the output tape we define
pk
pk ok
with
In this way we accumulate embedded outputs step-by-step on the output tape Each step pt has
access to all outputs produced in all steps before
Again it is important to note that during training we use teacher forcing we provide the true
output labels for ok instead of using the outputs generated by the model
Related Models
A convolutional architecture has already been used to obtain good results in word-level neural
machine translation in and more recently in These model use a standard RNN on top of
the convolution to generate the output and avoid the output dependence problem in this way But
the state of this RNN has a fixed size and in the first one the sentence representation generated by
the convolutional network is also a fixed-size vector Therefore while superficially similar to active
memory these models are more similar to fixed-size memory models The first one suffers from all
the limitations of sequence-to-sequence models without attention that we discussed before
Another recently introduced model the Grid LSTM might look less related to active memory
as it does not use convolutions at all But in fact it is to a large extend an active memory model the
memory is on the diagonal of the grid of the running LSTM cells The Reencoder architecture for
neural machine translation introduced in that paper is therefore related to the Extended Neural GPU.
But it differs in a number of ways For one the input is provided step-wise so the network cannot
start processing the whole input in parallel as in our model The diagonal memory changes in size
and the model is a 3-dimensional grid which might not be necessary for language processing The
Reencoder also does not use convolutions and this is crucial for performance The experiments from
are only performed on a very small dataset of short sentences This is almost times
smaller than the dataset we are experimenting with and makes is unclear whether Grid LSTMs can be
applied to large-scale real-world tasks
In image processing in addition to the captioning and generative models that we
mentioned before there are several other active memory models They use convolutional LSTMs an
architecture similar to CGRU and have recently been used for weather prediction and image
compression in both cases surpassing the state-of-the-art
Experiments
Since all components of our models defined above are differentiable we can train them using any
stochastic gradient descent optimizer For the results presented in this paper we used the Adam
optimizer with and gradients norm clipped to The number of layers was set to
the width of the state tensors was constant at the number of maps was and
the convolution kernels width and height was always kw kh
As our main test we train the models discussed above and a baseline attention model on the
English-French translation task This is the same task that was used to introduce attention but
to avoid the problem with the UNK token we spell-out each word that is not in the vocabulary More
precisely we use a vocabulary that includes all characters and the most common words and
every word that is not in the vocabulary is spelled-out letter-by-letter We also include a special SPACE
symbol which is used to mark spaces between characters we assume spaces between words We
train without any data filtering on the corpus and test on the test set newstest?14
As a baseline we use a GRU model with attention that is almost identical to the original one from
except that it has layers of GRU cells each with units Tokens from the vocabulary are
embedded into vectors of size and attention is put on the top layer This model is identical as the
one in except that is uses GRU cells instead of LSTM cells It has about parameters while
our Extended Neural GPU model has about parameters Better results have been reported on
this task with attention models with more parameters but we aim at a baseline similar in size to the
active memory model we are using
When decoding from the Extendend Neural GPU model one has to provide the expected size of the
output as it determines the size of the memory We test all sizes between input size and double the
input size using a greedy decoder and pick the result with smallest log-perplexity highest likelihood
This is expensive so we only use a very basic beam-search with beam of size and no length
normalization It is possible to reduce the cost by predicting the output length we tried a basic
estimator based just on input sentence length and it decreased the BLEU score by Better training
and decoding could remove the need to predict output length but we leave this for future work
Our model was implemented using TensorFlow Its code will be available as open-source at https
github.com/tensorflow/models/tree/master/neural_gpu/extended
Model
Neural GPU
Markovian Neural GPU
Extended Neural GPU
GRU+Attention
Perplexity log
BLEU
Table Results on the WMT English->French translation task We provide the average per-word
perplexity and its logarithm in parenthesis and the BLEU score Perplexity is computed on the test
set with the ground truth provided so it do not depend on the decoder
For the baseline model we use a full beam-search decoder with beam of size length normalization
and an attention coverage penalty in the decoder This is a basic penalty that pushes the decoder to
attend to all words in the source sentence We experimented with more elaborate methods following
but it did not improve our results The parameters for length normalization and coverage penalty
are tuned on the development set newstest?13 The final BLEU scores and per-word perplexities for
these different models are presented in Table Worse models have higher variance of their BLEU
scores so we only write for these models
One can see from Table that an active memory model can indeed match an attention model on
the machine translation task even with slightly fewer parameters It is interesting to note that the
active memory model does not need the length normalization that is necessary for the attention model
esp when rare words are spelled We conjecture that active memory inherently generalizes better
from shorter examples and makes decoding easier a welcome news since tuning decoders is a large
problem in sequence-to-sequence models
In addition to the summary results from Table we analyzed the performance of the models on
sentences of different lengths This was the key problem solved by the attention mechanism so it is
worth asking if active memory solves it as well In Figure we plot the BLEU scores on the test set
for sentences in each length bucket bucketing by for lengths and so on We
plot the curves for the Extended Neural GPU model the long baseline GRU model with attention
and for comparison we add the numbers for a non-attention model from Figure of Note
that these numbers are for a model that uses different tokenization so they are not fully comparable
but still provide a context
As can be seen our active memory model is less sensitive to sentence length than the attention
baseline It indeed solves the problem that the attention mechanism was designed to solve
Parsing In addition to the main large-scale translation task we tested the Extended Neural GPU
on English constituency parsing the same task as in We only used the standard WSJ dataset for
training It is small by neural network standards as it contains only sentences We trained the
Extended Neural GPU with the same settings as above only with instead of
and dropout of in each step During decoding we selected well-bracketed outputs with the right
number of POS-tags from all lengths considered Evaluated with the standard EVALB tool on the
standard WSJ 23 test set we got F1 score This is lower than reported in but we didn?t
use any of their optimizations no early stopping no POS-tag substitution no special tuning Since a
pure sequence-to-sequence model has F1 score well below 70 this shows that the Extended Neural
GPU is versatile and can learn and generalize well even on small data-sets
Discussion
To better understand the main shortcoming of previous active memory models let us look at the
average log-perplexities of different attention models in Table A pure Neural GPU model yields
a Markovian one yields and only a model with full dependence trained with teacher forcing
achieves The recurrent dependence in generating the output distribution turns out to be the key
to achieving good performance
We find it illuminating that the issue of dependencies in the output distribution can be disentangled
from the particularities of the model or model class In earlier works such dependence and training
with teacher forcing was always used in LSTM and GRU models but very rarely in other kinds
27
24
BLEU score
18
Extended Neural GPU
GRU+Attention
No Attention
Sentence length
Figure BLEU score the higher the better vs source sentence length
models We show that it can be beneficial to consider this issue separately from the model architecture
It allows us to create the Extended Neural GPU and this way of thinking might also prove fruitful for
other classes of models
When the issue of recurrent output dependencies is addressed as we do in the Extended Neural GPU
an active memory model can indeed match or exceed attention models on a large-scale real-world
task Does this mean we can always replace attention by active memory
The answer could be yes for the case of soft attention Its cost is approximately the same as active
memory it performs much worse on some tasks like learning algorithms and with the introduction
of the Extended Neural GPU we do not know of a task where it performs clearly better
Still an attention mask is a very natural concept and it is probable that some tasks can benefit from
a selector that focuses on single items by definition This is especially obvious for hard attention
it can be used over large memories with potentially much less computational cost than an active
memory so it might be indispensable for devising long-term memory mechanisms Luckily active
memory and attention are not exclusive and we look forward to investigating models that combine
these mechanisms

----------------------------------------------------------------

title: 3171-mining-internet-scale-software-repositories.pdf

Mining Internet-Scale Software Repositories
Erik Linstead Paul Rigor Sushil Bajracharya Cristina Lopes and Pierre Baldi
Donald Bren School of Information and Computer Science
University of California Irvine
Irvine CA
elinstea,prigor,sbajrach,lopes,pfbaldi}@ics.uci.edu
Abstract
Large repositories of source code create new challenges and opportunities for statistical machine learning Here we first develop Sourcerer an infrastructure for
the automated crawling parsing and database storage of open source software
Sourcerer allows us to gather Internet-scale source code For instance in one experiment we gather java projects from SourceForge and Apache totaling
over 38 million lines of code from developers Simple statistical analyses
of the data first reveal robust power-law behavior for package SLOC and lexical
containment distributions We then develop and apply unsupervised author-topic
probabilistic models to automatically discover the topics embedded in the code
and extract topic-word and author-topic distributions In addition to serving as
a convenient summary for program function and developer activities these and
other related distributions provide a statistical and information-theoretic basis for
quantifying and analyzing developer similarity and competence topic scattering
and document tangling with direct applications to software engineering Finally
by combining software textual content with structural information captured by our
CodeRank approach we are able to significantly improve software retrieval performance increasing the AUC metric to roughly better than previous approaches based on text alone Supplementary material may be found at
http://sourcerer.ics.uci.edu/nips2007/nips07.html
Introduction
Large repositories of private or public software source code such as the open source projects available on the Internet create considerable new opportunities and challenges for statistical machine
learning information retrieval and software engineering Mining such repositories is important for
instance to understand software structure function complexity and evolution as well as to improve
software information retrieval systems and identify relationships between humans and the software
they produce Tools to mine source code for functionality structural organization team structure
and developer contributions are also of interest to private industry where these tools can be applied
to such problems as in-house code reuse and project staffing While some progress has been made
in the application of statistics and machine learning techniques to mine software corpora empirical
studies have typically been limited to small collections of projects often on the order of one hundred
projects or less several orders of magnitude smaller than publicly available repositories(eg
Mining large software repositories requires leveraging both the textual and structural aspects of software data as well as any relevant meta data Here we develop Sourcerer a large-scale infrastructure
to explore such aspects We first identify a number of robust power-law behaviors by simple statistical analyses We then develop and apply unsupervised author-topic probabilistic models to discover
the topics embedded in the code and extract topic-word and author-topic distributions Finally we
leverage the dual textual and graphical nature of software to improve code search and retrieval
Infrastructure and Data
To allow for the Internet-scale analysis of source code we have built Sourcerer an extensive infrastructure designed for the automated crawling downloading parsing organization and storage of
large software repositories in a relational database A highly configurable crawler allows us to specify the number and types of projects desired as well as the host databases that should be targeted
and to proceed with incremental updates in an automated fashion Once target projects are downloaded a depackaging module uncompresses archive files while saving useful metadata project
name version etc While the infrastructure is general we apply it here to a sample of projects
in Java Specifically for the results reported we download projects from Sourceforge and
Apache and filter out distributions packaged without source code binaries only The end result is
a repository consisting of projects containing source files with million lines
of code written by developers For the software author-topic modeling approach we also
employ the Eclipse source code as a baseline Though only a single project Eclipse is a large
active open source effort that has been widely studied In this case we consider source files
associated with about lines of code a vocabulary of words and 59 programmers
Methods for extracting and assigning words and programmers to documents are described in the
next sections A complete list of all the projects contained in our repository is available from the
supplementary materials web pages
Statistical Analysis
During the parsing process our system performs a static analysis on project source code files to
extract code entities and their relationships storing them in a relational database For java these entities consist of packages classes interfaces methods and fields as well as more specific constructs
such as constructors and static initializers Relations capture method calls inheritance and encapsulation to name a few The populated database represents a substantial foundation on which to
base statistical analysis of source code Parsing the multi-project repository described above yields
a repository of over million entities organized into 48 thousand packages thousand classes
and million methods participating in over million relations By leveraging the query capabilities of the underlying database we can investigate other interesting statistics For example table
contains the frequencies of Java keywords across all projects Upon examining this data we
can see that the default keyword occurs about percent less frequently than the switch keyword
despite the fact that best practice typically mandates all switch statements contain a default block
Moreover the for loop is about twice as pervasive as the while loop suggesting that the bound on
the number of iterations is more likely to be known or based on the size of a known data structure
Table Frequency of java keyword occurrence
Keyword
public
if
new
return
import
int
null
void
private
static
final
else
throws
Percentage
Keyword
boolean
false
case
true
class
protected
catch
for
try
throw
package
byte
extends
Percentage
Keyword
this
break
while
super
instanceof
double
long
implements
char
float
abstract
synchronized
short
Percentage
Keyword
switch
interface
continue
finally
default
native
transient
do
assert
enum
volatile
strictfp
Percentage
Finally statistical analyses of distributions also identify several power-law distributions We have
observed power-law distributions governing package SLOC and inside relation lexical contain
ment counts For instance Figure shows the log-log plots for the number of packages across
projects Similar graphs for other distributions are available from the supplemental materials page
Distribution of Packages over Projects
Number of Packages
Rank
Figure Approximate power-law distribution for packages over projects
Topic and Author-Topic Probabilistic Modeling of Source Code
Automated topic and author-topic modeling have been successfully used in text mining and information retrieval where they have been applied for instance to the problem of summarizing large
text corpora Recent techniques include Latent Dirichlet Allocation which probabilistically
models text documents as mixtures of latent topics where topics correspond to key concepts presented in the corpus also Author-Topic modeling is an extension of topic modeling
that captures the relationship of authors to topics in addition to extracting the topics themselves An
extension of LDA to probabilistic AT modeling has been developed in In the literature
these more recent approaches have been found to produce better results than more traditional methods such as latent semantic analysis LSA Despite previous work in classifying code based
on concepts applications of LDA and AT models have been limited to traditional text corpora
such as academic publications news reports corporate emails and historical documents At
the most basic level however a code repository can be viewed as a text corpus where source files
are analogous to documents and developers to authors Though vocabulary syntax and conventions
differentiate a programming language from a natural language the tokens present in a source file
are still indicative of its function its topics Thus here we develop and apply probabilistic AT
models to software data
In AT models for text the data consists of a set of documents The authors of each documents are
known and each document is treated as a bag of words We let A be the total number of authors
the total number of distinct words vocabulary size and the total number of topics present in the
documents While non-parametric Bayesian and other methods exist to try to infer from
the data here we assume that is fixed though we explore different values
As in our model assumes that each topic is associated with a multinomial distribution over
words and each author a is associated with a multinomial distribution over topics More
precisely the parameters are given by two matrices a A matrix of author-topic
distributions and a matrix of topic-word distributions Given a document
containing Nd words with known authors in generative mode each word is assigned to one of the
authors a of the document uniformly then the corresponding is sampled to derive a topic and
finally the corresponding is sampled to derive a word A fully Bayesian model is derived by
putting symmetric Dirichlet priors with hyperparameters and over the distributions and
So for instance the prior on is given by
ta
and similarly for If A is the set of authors of the corpus and document has Ad authors it is
easy to see that under these assumptions the likelihood of a document is given by
A
Nd
XX
ta
A
a
which can be integrated over and and their Dirichlet distributions to get A). The
posterior can be sampled efficiently using Markov Chain Monte Carlo Methods Gibbs sampling
and for instance the and parameter matrices can be estimated by MAP or MPE methods
Once the data is obtained applying this basic AT model to software requires the development of
several tools to facilitate the processing and modeling of source code In addition to the crawling
infrastructure described above the primary functions of the remaining tools are to extract and resolve
author names from source code as well as convert the source code to the bag-of-words format
Information Extraction from Source Code
Author-Document The author-document matrix is produced from the output of our author extraction tool It is a binary matrix where entry if author contributed to document and
otherwise Extracting author information is ultimately a matter of tokenizing the code and associating developer names with file document names when this information is available This process is
further simplified for java software due to the prevalence of javadoc tags which present this metadata
in the form of attribute-value pairs
Exploratory analysis of the Eclipse code base however shows that most source files are credited
to The IBM Corporation rather than specific developers Thus to generate a list of authors for
specific source files we parsed the Eclipse bug data available in After pruning files not
associated with any author this input dataset consists of Java source files comprising
lines of code from a total of 59 developers
While leveraging bug data is convenient and necessary to generate the developer list for Eclipse
it is also desirable to develop a more flexible approach that uses only the source code itself
and not other data sources Thus to extract author names from source code we also develop a
lightweight parser that examines the code for javadoc @author tags as well as free form labels such
as author and developer Occurrences of these labels are used to isolate and identify developer
names Ultimately author identifiers may come in the form of full names email addresses url?s
or CVS account names This multitude of formats combined with the fact that author names are
typically labeled in the code header is key to our decision to extract developer names using our own
parsing utilities rather than part-of-speech taggers leveraged in other text mining projects
A further complication for author name extraction is the fact that the same developer may write
his name in several different ways For example John Q. Developer alternates between John
Developer Q. Developer or simply Developer To account for this effect we implement
also a two-tiered approach to name resolution using the q-gram algorithm When an individual
project is parsed a list of contributing developers and the files they modified is created A pairwise
comparison of author-names is then performed using q-gram similarity and pairs of names whose
similarity is greater than a threshold t1 are merged This process continues until all pairwise similarities are below the threshold and the project list is then added to a global list of authors When
parsing is complete for all projects the global author list is resolved using the same process but
with a new threshold such that t2 This approach effectively implements more conservative name resolution across projects in light of the observation that the scope of most developer
activities is limited to a relatively small number in many cases of open source efforts In practice we set t1 65 and t2 75 Running our parser on the multi-project repository yields
distinct authors respectively
Word-Document To produce the word-document matrix for our input data we have developed a
comprehensive tokenization tool tuned to the Java programming language This tokenizer includes
language-specific heuristics that follow the commonly practiced naming conventions For example
the Java class name QuickSort will generate the words quick and sort All punctuation is
ignored As an important step in processing source files our tool removes commonly occurring stop
words We augment a standard list of stop words used for the English language and the but
etc to include the names of all classes from the Java SDK ArrayList HashMap etc This is
done to specifically avoid extracting common topics relating to the Java collections framework.We
run the LDA-based AT algorithm on the input matrices and set the total number of topics
and the number of iterations by experimentation For instance the number of iterations to run the
algorithm is determined empirically by analyzing results for ranging from to several thousands
The results presented in the next section are derived using iterations which were found to
produce interpretable topics in a reasonable amount of time week or Because the algorithm
contains a stochastic component we also verified the stability of the results across multiple runs
Topic and Author-Topic Modeling Results
A representative subset of topics extracted via Author-Topic modeling on the selected source
files from Eclipse is given in Table Each topic is described by several words associated with
the topic concept To the right of each topic is a list of the most likely authors for each topic with
their probabilities Examining the topic column of the table it is clear that various functions of the
Eclipse framework are represented For example topic clearly corresponds to unit testing topic
to debugging topic to building projects and topic to automated code completion Remaining
topics range from package browsing to compiler options
Table Representative topics and authors from Eclipse
Topic
junit
run
listener
item
suite
target
source
debug
breakpoint
location
ast
button
cplist
entries
astnode
Author Probabilities
egamma
wmelhem
darin
krbarnes
kkolosow
jaburns
darin
lbourlier
darins
jburns
maeschli
mkeller
othomann
tmaeder
teicher
Topic
nls-1
ant
manager
listener
classpath
type
length
names
match
methods
token
completion
current
identifier
assist
Author Probabilities
darins
dmegert
nick
kkolosow
maeschli
kjohnson
jlanneluc
darin
johna
pmulet
daudel
teicher
jlanneluc
twatson
dmegert
Table presents representative author-topic assignments from the multi-project repository This
dataset yields a substantial increase in topic diversity Topics representing major sub-domains of
software development are clearly represented with the first topic corresponding to web applications the second to databases the third to network applications and the fourth to file processing
Topics and are especially interesting as they correspond to common examples of crosscutting
concerns from aspect-oriented programming namely security and logging Topic is also
demonstrative of the inherent difficulty of resolving author names and the shortcomings of the qgram algorithm as the developer gert van ham and the developer hamgert are most likely the
same person documenting their name in different ways
Several trends reveal themselves when all results are considered Though the majority of topics
can be intuitively mapped to their corresponding domains some topics are too noisy to be able to
associate any functional description to them For example one topic extracted from our repository
consists of Spanish words unrelated to software engineering which seem to represent the subset
of source files with comments in Spanish Other topics appear to be very project specific and
while they may indeed describe a function of code they are not easily understood by those who
are only casually familiar with the software artifacts in the codebase This is especially true with
Eclipse which is limited in both the number and diversity of source files In general noise appears to
diminish as repository size grows Noise can be controlled to some degree with tuning the number
of topics to be extracted but of course can not be eliminated completely
Examining the author assignments and probabilities for the various topics provides a simple means
by which to discover developer contributions and infer their competencies It should come as no
surprise that the most probable developer assigned to the JUnit framework topic is egamma or
Erich Gamma In this case there is a chance that any source file in our dataset assigned
to this topic will have him as a contributor Based on this rather high probability we can also
infer that he is likely to have extensive knowledge of this topic This is of course a particularly
Table Representative topics and authors from the multi-project repository
Topic
servlet
session
response
request
http
sql
column
jdbc
type
result
packet
type
session
snmpwalkmv
address
Author Probabilities
craig mcclanahan
remy maucherat
peter rossbach
greg wilkins
amy roh
mark matthews
ames
mike bowler
manuel laflamme
gavin king
brian weaver
apache directory project
opennms
matt whitlock
trustin lee
Topic
file
path
dir
directory
stream
token
key
security
param
cert
service
str
log
config
result
Author Probabilities
adam murdoch
peter donald
ludovic claude
matthew hawthorne
lk
werner dittmann
apache software foundation
gert van ham
hamgert
jcetaglib.sourceforge.net
wayne osse
dirk mascher
david irwin
linke
jason
attractive example because Erich Gamma is widely known for being a founder of the JUnit project
a fact which lends credibility to the ability of the topic modeling algorithm to assign developers to
reasonable topics One can interpret the remaining author-topic assignments along similar lines For
example developer daudel is assigned to the topic corresponding to automatic code completion
with probability 99 Referring back to the Eclipse bug data it is clear that the overwhelming majority
of bug fixes for the codeassist framework were made by this developer One can infer that this is
likely to be an area of expertise of the developer
In addition to determining developer contributions one may also be curious to know the scope
of a developer?s involvement Does a developer work across application areas or are his contributions highly focused How does the breadth of one developer compare to another These are
natural questions that arise in the software development process To answer these questions within
the framework
of author-topic models we can measure the breadth of an author a by the entropy
ta log ta of the corresponding distribution over topics Applying the measure to
our multi-project dataset we find that the average measure is bits The developer with the lowest entropy is thierry danard with bits The developer with the highest entropy is wdi
with bits with bits being the maximum possible score for topics While the entropy
twatson
jeff
rchaves
dj
wmelhem
prapicau
johna
jfogell
kkolosow
jaburns
dejan
ikhelifi schan
dpollock cwong
dbirsan
nick
jburns
darins
khorne
mrennie
sxenos
bbokowski
sarsenau
mfaraj
darin
sfranklin
dwilson
tod
mvanmeek
cmarti
kent
krbarnes
lbourlier
egamma
kjohnson
jdesrivieres
jeromel
aweinand
kmaetzel
daudel
bbiggs
ffusier
jeem
bbaumgart
akiezun
othomann jlanneluc
oliviert
dmegert
twidmer
pmulet
maeschlimann
tmaeder
teicher
dbaeumer
mkeller
ptff
maeschli
Figure All 59 Eclipse authors clustered by KL divergence
of the distribution of an author over topics measures the author?s breadth the similarity between two
authors can be measured by comparing their respective distributions over topics Several metrics
are possible for this purpose but one of the most natural measures is provided by the symmetrized
Kullback-Leibler divergence Multidimensional scaling MDS is employed to further visual
ize author similarities resulting in Figure for the Eclipse project The boxes represent individual
developers and are arranged such that developers with similar topic distributions are nearest one another A similar figure displaying only a subset of the SourceForge and Apache authors due to
space and legibility constraints is available in the supplementary materials This information is especially useful when considering how to form a development team choosing suitable programmers
to perform code updates or deciding to whom to direct technical questions Two other important
distributions that can be retrieved from the AT modeling approach are the distribution of topics
across documents and the distribution of documents across topics not shown The corresponding
entropies provide an automated and novel way to precisely formalize and measure topic scattering
and document tangling two fundamental concepts of software design which are important to
software architects when performing activities such as code refactoring
Code Search and Retrieval
Sourcerer relies on a deep analysis of code to extract pertinent textual and structural features that can
be used to improve the quality and performance of source code search as well as augment the ways
in which code can be searched By combining standard text information retrieval techniques with
source-specific heuristics and a relational representation of code we have available a comprehensive
platform for searching software components While there has been progress in developing sourcecode-specific search engines in recent years Koders Krugle and Google?s CodeSearch these
systems continue to focus strictly on text information retrieval and do not appear to leverage the
copious relations that can be extracted and analyzed from code
Programs are best modeled as graphs with code entities comprising the nodes and various relations
the edges As such it is worth exploring possible ranking methods that leverage the underlying
graphs A natural starting point is Google?s PageRank which considers hyperlinks to formulate
a notion of popularity among web pages This can be applied to source as well as it is likely that a
code entity referenced by many other entities are more robust than those with few

----------------------------------------------------------------

title: 986-grammar-learning-by-a-self-organizing-network.pdf

Grammar Learning by a Self-Organizing
Network
Michiro Negishi
Dept of Cognitive and Neural Systems Boston University
Cummington Street
Boston MA email negishi@cns.bu.edu
Abstract
This paper presents the design and simulation results of a selforganizing neural network which induces a grammar from example sentences Input sentences are generated from a simple phrase
structure grammar including number agreement verb transitivity and recursive noun phrase construction rules The network
induces a grammar explicitly in the form of symbol categorization
rules and phrase structure rules
Purpose and related works
The purpose of this research is to show that a self-organizing network with a certain
structure can acquire syntactic knowledge from only positive grammatical
data without requiring any initial knowledge or external teachers that correct
errors
There has been research on supervised neural network models of language acquisition tasks Elman Miikkulainen and Dyer John and McClelland
Unlike these supervised models the current model self-organizes word and phrasal
categories and phrase construction rules through mere exposure to input sentences
without any artificially defined task goals There also have been self-organizing
models of language acquisition tasks Ritter and Kohonen Scholtes
Compared to these models the current model acquires phrase structure rules in
more explicit forms and it learns wider and more structured contexts as will be
explained below
Network Structure and Algorithm
The design of the current network is motivated by the observation that humans
have the ability to handle a frequently occurring sequence of symbols chunk
as an unit of information Grossberg Mannes The network consists
of two parts classification networks and production networks Figure The
classification networks categorize words and phrases and the production networks
28
Michiro Negishi
evaluate how it is likely for a pair of categories to form a phrase A pair of combined
categories is given its own symbol and fed back to the classifiers
After weights are formed the network parses a sentence as follows Input words
are incrementally added to the neural sequence memory called the Gradient Field
Grossberg GF hereafter The top most recent two symbols and the
lookahead token are classified by three classification networks Here a symbol
is either a word or a phrase and the lookahead token is the word which will be
read in next Then the lookahead token and the top symbol in the GF are sent to
the right production network and the top and the second ones are sent to the left
production network If the latter pair is judged to be more likely to form a phrase
the symbol pair reduces to a phrase and the phrase is fed back to the GF after
removing the top two symbols Otherwise the lookahead token is added to the
sequence memory causing a shift in the sequence memory If the input sentence is
grammatical the repetition of this process reduces the whole sentence to a single
sentence symbol The sequence of shifts and reductions annoted with the
resultant symbols amounts to a parse of the sentence
During learning the operations stated above are carried out as weights are gradually formed In classification networks the weights record a distribution pattern
with respect to each symbol That is the weights record the co-occurrence of
up to three adjacent symbols in the corpus An symbol is classified in terms of
this distribution in the classification networks The production networks keep
track of the categories of adjacent symbols If the occurrence of one category reliably predicts the next or the previous one the pair of categories forms a phrase
and is given the status of an symbol which is treated just like a word in the
sentence Because the symbols include phrases the learned context is wider
and more structured than the mere bigram as well as the contexts utilized in
Ritter and Kohonen Scholtes
Simulation
The Simulation Task
The grammar used to generate input sentences Table is identical to that used
in except that it does not include optionally transitive verbs and
proper nouns Lengths of the input sentences are limited to words To determine the completion of learning after accepting consecutive sentences with
learning learning is suppressed and other sentences are processed to see if all
are accepted In addition the network was tested for 44 ungrammatical sentences
to see that they are correctly rejected Ungrammatical sentences are derived by
hand from randomly generated grammatical sentences Parameters used in the
simulation are number of symbol nodes words phrases number
of category nodes ISO
and
Grammar Learning by a Self-Organizing NeMork
29
Acquired Syntax Rules
Learning was completed after learning grammatical sentences Tables and
show the acquired syntax rules extracted from the connection weights Note that
category names such as Ns VPp are not given a priori but assigned by the author
for the exposition Only rules that eventually may reach the S"(sentence node
are shown There were a small number of uninterpretable rules which are marked
These rules might disturb normal parsing for some sentences but they were
not activated while testing for sentences after learning
Discussion
Recursive noun phrase structures should be learned by finding equivalences of
distribution between noun phrases and nouns However nouns and noun phrases
have the same contextual features only when they are in certain contexts An
examination of the acquired grammar reveals that the network finds equivalence
of features not of and RC where RC is a relative clause but of and
RC when RC is subjective or and IV NRC when RC
is objective As an example let us examine the parsing of the sentence
below The rule used to reduce FEEDS CATS WHO UVE NRC is PO which
is classified as category which includes where are the singular
forms of transitive verbs and also includes the where are singular forms of
intransitive verbs Thus GIRL WHO FEEDS CATS WHO UVE is reduced to GIRL
WHO VPsingle
I
I
I
I
I
I
I
I
I
I
I
I
I
I
BOYS CHASE GIRL WHO FEEDS CATS WHO LIVE
Accepted
Top symbol was 77
Conclusion and Future Direction
In this paper a self-organizing neural network model of grammar learning was
presented A basic principle of the network is that all words and phrases are
categorized by the contexts in which they appear and that familiar sequence of
categories are chunked
As it stands the scope of the grammar used in the simulation is extremely limited
Also considering the poverty of the actual learning environment the learning
of syntax should also be guided by the cognitive competence to comprehend the
utterance situations and conversational contexts However being a self-organizing
network the current model offers a plausible model of natural language acquisition
through mere exposures to only grammatical sentences not requiring any external
teacher or an explicit goal
Miclziro Negislzi
Table Acquired categorization rules
C4
a
NPs VPs I
enr~vpp
LIVES I ALKS I
POrvrsNpRC I
rvrsNs I
rvrsNs I
rvrsNpo
GIRL I DOG I
CAT I BOY
CHASE I FEED
WHO
CHASES I FEEDS
BOYS I CATS I
DOGS I GIRLS
Ns RC VPs I
rNs VPs
P2 vrp NP\.iPp I
vrp
I
WALK I LIVE I
PI r~NpRC I
pNpo I
P88rvrpNs I
vrp
rvrs
rNsR
rNpR
rVps
rvrp
rNPsvps
wnere
RCs
RCp
NPp
NPs
Ns vrs I
Np vrVi I
Ns RC so I
rNpRCvrp
rNs VPs
rNsRNvr
rNpRVpp
I
rNpRNvr
rNpVPp I
rNpRCVpp
PIO rvrs NPs VPs I
vrs NPp VPp
rNvr
rNsRCs
rNpRCp
rNPpVPp
vrsNvr
rNsR vrsNvr
VPs VPp's
RVPs I RNvr
RVPp I RNvr
Np I NpRCp
Ns I NsRCs
rvpp
Table Acquired production rules
PO
P1
P2
PI0
rvrs
rvrp
rvrp
rvrs
rNs
rvrs
rNp
Np
rNs
rvrp
rNsRCs
rvrs
rVTp
rNsRCs
rvrp
vrs
rVTp
NsR vrsNVT
rNs
rNp
rNp RCs
rvrs
rvrs
Ns
rNp RCs
Np
Ns
rNpR
rNpRCp
C74rNpRCp
en NPpVpp
rNPs VPs
rvrso
C77/"NPpVPp
rNs RCs
Ns RCs
C4 VPs
rNs
VPp
C4rVps
rvrp
C4 rVPs?1
Vpp
Vpp
rNvrol
vrs Np RCp
rvrpNpRCp
rvrpNPp VPp
rvrsNPs VPs
rNsvrs
vrsNPpVPp
Npvrp
rNpR
rNsR
NsRCs vrs
vrs Ns RCs
rvrpNsRCs
Ns RCs VPs
rvrpNvr
vrs
rvrpNs
rNsvps
rNp RCs vrp
rvrsNvr
rvrsNp
NsRVPs
Np RCs VPp
NpRVPp
NsRNvr
Ns vrs vr
RNvr
Grammar Learning by a Self-Organizing Network
Acknowledgements
The author wishes to thank Prof Dan Bullock Prof Cathy Harris Prof Mike
Cohen and Chris Myers of Boston University for valuable discussions
This work was supported in part by the Air Force Office of Scientific Research
AFOSR

----------------------------------------------------------------

title: 5502-altitude-training-strong-bounds-for-single-layer-dropout.pdf

Altitude Training
Strong Bounds for Single-Layer Dropout
Stefan Wager William Fithian Sida Wang and Percy Liang
Departments of Statistics and Computer Science
Stanford University Stanford USA
swager wfithian}@stanford.edu sidaw pliang}@cs.stanford.edu
Abstract
Dropout training originally designed for deep neural networks has been successful on high-dimensional single-layer natural language tasks This paper proposes
a theoretical explanation for this phenomenon we show that under a generative
Poisson topic model with long documents dropout training improves the exponent
in the generalization bound for empirical risk minimization Dropout achieves this
gain much like a marathon runner who practices at altitude once a classifier learns
to perform reasonably well on training examples that have been artificially corrupted by dropout it will do very well on the uncorrupted test set We also show
that under similar conditions dropout preserves the Bayes decision boundary and
should therefore induce minimal bias in high dimensions
Introduction
Dropout training is an increasingly popular method for regularizing learning algorithms Dropout
is most commonly used for regularizing deep neural networks but it has also been found
to improve the performance of logistic regression and other single-layer models for natural language
tasks such as document classification and named entity recognition For single-layer linear
models learning with dropout is equivalent to using blankout noise
The goal of this paper is to gain a better theoretical understanding of why dropout regularization
works well for natural language tasks We focus on the task of document classification using linear
classifiers where data comes from a generative Poisson topic model In this setting dropout effectively deletes random words from a document during training this corruption makes the training
examples harder A classifier that is able to fit the training data will therefore receive an accuracy
boost at test time on the much easier uncorrupted examples An apt analogy is altitude training
where athletes practice in more difficult situations than they compete in Importantly our analysis
does not rely on dropout merely creating more pseudo-examples for training but rather on dropout
creating more challenging training examples Somewhat paradoxically we show that removing information from training examples can induce a classifier that performs better at test time
Main Result Consider training the zero-one loss empirical risk minimizer ERM using dropout
where each word is independently removed with probability For a class of Poisson
generative topic models we show that dropout gives rise to what we call the altitude training phenomenon dropout improves the excess risk of the ERM by multiplying the exponent
in its decay
rate by
This improvement comes at the cost of an additive term of where
be the expected and
is the average number of words per document More formally let and
S. Wager and W. Fithian are supported by a B.C. and E.J. Eaves Stanford Graduate Fellowship and NSF
VIGRE grant respectively
be the corresponding quantities for dropout
empirical risk minimizers respectively let and
training Let Err(h denote the error rate on test examples of In Section we show that
eP Err
Err
Err
Err
@
A
dropout excess risk
ERM excess risk
eP is a variant of big-O in probability notation that suppresses logarithmic factors If is
where
large we are classifying long documents rather than short snippets of text dropout considerably
accelerates the decay rate of excess risk The bound holds for fixed choices of The constants
in the bound worsen as approaches and so we cannot get zero excess risk by sending to
Our result is modular in that it converts upper bounds on the ERM excess risk to upper bounds on
the dropout
excess risk For example recall from classic VC theory that the ERM excess risk is
eP where is the number of features vocabulary size and is the number of training
examples With
dropout our result directly implies that the dropout excess risk is
eP
The intuition behind the proof of is as follows when we essentially train on half
documents and test on whole documents By conditional independence properties of the generative
topic model the classification score is roughly Gaussian under a Berry-Esseen bound and the error
rate is governed by the tails of the Gaussian Compared to half documents the coefficient
of variation
of the classification score on whole documents at test time is scaled down by
compared to
half documents
at training time resulting in an exponential reduction in error The additive penalty
of
stems from the Berry-Esseen approximation
Note that the bound only controls the dropout excess risk Even if dropout reduces the excess
risk it may introduce a bias Err(h Err(h and thus is useful only when this bias is small In
Section we will show that the optimal Bayes decision boundary is not affected by dropout under
the Poisson topic model Bias is thus negligible when the Bayes boundary is close to linear
It is instructive to compare our generalization bound to that of Ng and Jordan who showed that
the naive Bayes classifier exploits a strong generative assumption?conditional
independence of the
features given the label?to achieve an excess risk of OP log However if the generative
assumption is incorrect then naive Bayes can have a large bias Dropout enables us to cut excess risk
without incurring as much bias In fact naive Bayes is closely related to logistic regression trained
using an extreme form of dropout with Training logistic regression with dropout rates from
the range thus gives a family of classifiers between unregularized logistic regression and
naive Bayes allowing us to tune the bias-variance tradeoff
Other perspectives on dropout In the general setting dropout only improves generalization by
a multiplicative factor McAllester used the PAC-Bayes framework to prove a generalization
bound for dropout that decays as
Moreover provided that is not too close to dropout
behaves similarly to an adaptive regularizer with parameter and at least in linear
regression such regularization improves generalization error by a constant factor In contrast by
leveraging the conditional independence assumptions of the topic model we are able to improve the
exponent in the rate of convergence of the empirical risk minimizer
It is also possible to analyze dropout as an adaptive regularizer in comparison with
regularization dropout favors the use of rare features and encourages confident predictions If we
believe that good document classification should produce confident predictions by understanding
rare words with Poisson-like occurrence patterns then the work on dropout as adaptive regularization and our generalization-based analysis are two complementary explanations for the success of
dropout in natural language tasks
Dropout Training for Topic Models
In this section we introduce binomial dropout a form of dropout suitable for topic models and the
Poisson topic model on which all our analyses will be based
Binomial Dropout Suppose that we have a binary classification problem1 with count features
and labels For example is the number of times the j-th
word in our dictionary appears in the i-th document and is the label of the document Our goal
is to train a weight vector
that classifies new examples with features via a linear decision rule
I{w
We start with the usual empirical risk minimizer
def
b0 argminw2Rd
for some loss function we will analyze the zero-one loss but use logistic loss in experiments
Binomial dropout trains on perturbed features
instead of the original features
def
argminw
where
Binom
In other words during training we randomly thin the j-th feature with binomial noise If
counts the number of times the j-th word appears in the document then replacing with
is
equivalent to independently deleting each occurrence of word with probability Because we
are only interested in the decision boundary we do not scale down the weight vector obtained by
dropout by a factor
as is often done
Binomial dropout differs slightly from the usual definition of blankout dropout which alters the
feature vector by setting random coordinates to The reason we chose to study
binomial rather than blankout dropout is that Poisson random variables remain Poisson even after
binomial thinning this fact lets us streamline our analysis For rare words that appear once in the
document the two types of dropout are equivalent
A Generative Poisson Topic Model Throughout our analysis we assume that the data is drawn
from a Poisson topic model depicted in Figure 1a and defined as follows Each document is assigned a label according to some Bernoulli distribution Then given the label the document
gets a topic from a distribution Given the topic for every word in the vocabu(i
lary we generate its frequency according to Poisson
where
is the expected number of times word appears under topic Note that k1 is the average
def
length of a document with topic Define min k1 to be the shortest average document length across topics If contains only two topics?one for each class?we get the naive
Bayes model If is the 1)-dimensional simplex where is a mixture over basis
vectors we get the K-topic latent Dirichlet allocation
Note that although our generalization result relies on a generative model the actual learning algorithm is agnostic to it Our analysis shows that dropout can take advantage of a generative structure
while remaining a discriminative procedure If we believed that a certain topic model held exactly
and we knew the number of topics we could try to fit the full generative model by EM. This
however could make us vulnerable to model misspecification In contrast dropout benefits from
generative assumptions while remaining more robust to misspecification
Altitude Training Linking the Dropout and Data-Generating Measures
trained using dropout During dropout
Our goal is to understand the behavior of a classifier
the error of any classifier is characterized by two measures In the end we are interested in the
usual generalization error expected risk of where is drawn from the underlying data-generating
measure
def
Err
Dropout training is known to work well in practice for multi-class problems For simplicity however
we will restrict our theoretical analysis to a two-class setup
In topic modeling the vertices of the simplex are topics and is a mixture of topics whereas we call
itself a topic
However since dropout training works on the corrupted data
in the limit of infinite data
the dropout estimator will converge to the minimizer of the generalization error with respect to the
dropout measure over
def
Err
The main difficulty in analyzing the generalization of dropout is that classical theory tells us that
the generalization error with respect to the dropout measure will decrease as but we are
interested in the original measure Thus we need to bound Err in terms of Err In this section we
show that the error on the original measure is actually much smaller than the error on the dropout
measure we call this the altitude training phenomenon
Under our generative model the count features are conditionally independent given the topic
We thus focus on a single fixed topic and establish the following theorem which provides a
per-topic analogue of Section will then use this theorem to obtain our main result
Theorem Let be a binary linear classifier with weights and suppose that our features are
drawn from the Poisson generative model given topic Let be the more likely label given
def
arg max
Let be the sub-optimal prediction rate in the dropout measure
def
I
where
is an example thinned by binomial dropout and is taken over the data-generating
process Let be the sub-optimal prediction rate in the original measure
def
I
Then
where
Pd
maxj wj2
wj
and the constants in the bound depend only on
Theorem only provides us with a useful bound
when the term is
small Whenever the largest
wj2 is not much larger than the average wj2 then
scales
as
where is the average
document length Thus the bound is most useful for long documents
A Heuristic Proof of Theorem The proof of Theorem is provided in the technical appendix
Here we provide a heuristic argument for intuition
Given a fixed
topic suppose that it is optimal
to predict so our test error is For long enough documents by
def
the central limit theorem the score will be roughly Gaussian where
Pd
Pd
wj and wj2 This implies that where is the
def
cumulative distribution function of the Gaussian Now let
be the score on a dropout
sample Clearly
and Var
because the variance of a Poisson
random variable scales with its mean Thus
Figure 1b illustrates the relationship betweenpthe two Gaussians This explains the first term on the
right-hand side of The extra error term
arises from a Berry-Esseen bound that approximates Poisson mixtures by Gaussian random variables
A Generalization Bound for Dropout
By setting up a bridge between the dropout measure and the original data-generating measure Theorem provides a foundation for our analysis It remains to translate this result into a statement
about the generalization error of dropout For this we need to make a few assumptions
density
Original
Dropout
score
I
Graphical representation of the Poisson topic model
Given a document with label we draw a document
topic from the multinomial distribution with probabilities Then we draw the words from the topic?s
Poisson distribution with mean Boxes indicate repeated observations and greyed-out nodes are observed
during training
For a fixed classifier the probabilities of error on an example drawn from the original and
dropout measures are governed by the tails of two
Gaussians shaded The dropout Gaussian has a
larger coefficient of variation which means the error on the original measure test is much smaller
than the error on the dropout measure train
In this example and
Figure Graphical model The altitude training phenomenon
Our first assumption is fundamental if the classification signal is concentrated among just a few
features then we cannot expect dropout training to do well The second and third assumptions
which are more technical guarantee that a classifier can only do well overall if it does well on every
topic this lets us apply Theorem A more general analysis that relaxes Assumptions and may
be an interesting avenue for future work
Assumption well-balanced weights First we need to assume that all the signal is not concentrated in a few features To make this intuition formal we say a linear classifier with weights is
well-balanced if the following holds for each topic
Pd
maxj wj2
for some
Pd
wj
For example suppose each word was either useful or not wj then is the inverse
expected fraction of words in a document that are useful In Theorem we restrict the ERM to
well-balanced classifiers and assume that the expected risk minimizer over all linear rules is also
well-balanced
Assumption discrete topics Second we assume that there are a finite number of topics and
that the available topics are not too rare or ambiguous the minimal probability of observing any
topic is bounded below by
pmin
and that each topic-conditional probability is bounded away from random guessing
for all topics This assumption substantially simplifies our arguments allowing us
to apply Theorem to each topic separately without technical overhead
Assumption distinct topics Finally as an extension of Assumption we require that the topics
be well separated First define Errmin P[y where is the most likely label given
topic this is the error rate of the optimal decision rule that sees topic We assume that the
best linear rule satisfying is almost as good as always guessing the best label under the
dropout measure
Err Errmin
where as usual is a lower bound on the average document length If the dimension is larger
than the number of topics this assumption is fairly weak the condition holds whenever the
matrix of topic centers has full rank and the minimum singular value of is not too small
Proposition in the Appendix for details This assumption is satisfied if the different topics can be
separated from each other with a large margin
Under Assumptions we can turn Theorem into a statement about generalization error
Theorem Suppose that our features are drawn from the Poisson generative model Figure
on the dropout and
and Assumptions hold Define the excess risks of the dropout classifier
data-generating measures respectively
def
def
Err
Err and Err
Err
Then the altitude training phenomenon applies
The above bound scales linearly in pmin1 and
the full dependence on is shown in the appendix
In a sense Theorem is a meta-generalization bound that allows us to transform generalization
bounds with respect to the dropout measure
into ones on the data-generating
measure in a
eP bound which
modular way As a simple example standard VC theory provides an
together with Theorem yields
achieves the folCorollary Under the same conditions as Theorem the dropout classifier
lowing excess risk
eP @
A.
Err
Err
More generally we can often check that upper bounds for Err(h
Err(h also work as upper
bounds for Err Err this gives us the heuristic result from
The Bias of Dropout
In the previous section we showed that under the Poisson topic model in Figure dropout can
Err(h But to complete our picture of dropout?s
achieve a substantial cut in excess risk Err(h
performance we must address the bias of dropout Err(h Err(h
Dropout can be viewed as importing hints from a generative assumption about the data Each observed pair each labeled document gives us information not only about the conditional class
probability at but also about the conditional class probabilities at numerous other hypothetical
values
representing shorter documents of the same class that did not occur Intuitively if these
are actually good representatives of that class the bias of dropout should be mild
For our key result in this section we will take the Poisson generative model from Figure but
further assume that document length is independent of the topic Under this assumption we will
show that dropout preserves the Bayes decision boundary in the following sense
Proposition Let be distributed according to the Poisson topic model of Figure Assume
that document length is independent of topic k1 for all topics Let
be a binomial
dropout sample of with some dropout probability Then for every feature vector Rd
we have
x=v
If we had an infinite
corrupted under dropout we would predict according to
amount of data
I{P
The significance of Proposition is that this decision rule is identical to
the true Bayes decision boundary without dropout Therefore the empirical risk minimizer of a
sufficiently rich hypothesis class trained with dropout would incur very small bias
Test Error Rate
X2
LogReg Boundary
Dropout Boundary
Bayes Boundary
Long Documents
Short Dropout Documents
LR
NB
X1
Dropout with For long documents circles in the upper-right logistic regression focuses on capturing the small red cluster the
large red cluster has almost no influence Dropout
dots in the lower-left distributes influence more
equally between the two red clusters
Learning curves for the synthetic experiment
Each axis is plotted on a log scale Here the dropout
rate ranges from logistic regression to naive
Bayes for multiple values of training set sizes
As increases less dropout is preferable as the
bias-variance tradeoff shifts
Figure Behavior of binomial dropout in simulations In the left panel the circles are the original
data while the dots are dropout-thinned examples The Monte Carlo error is negligible
However Proposition does not guarantee that dropout incurs no bias when we fit a linear classifier
In general the best linear approximation for classifying shorter documents is not necessarily the
best for classifying longer documents As a linear classifier trained on pairs will
eventually outperform one trained on
pairs
Dropout for Logistic Regression To gain some more intuition about how dropout affects linear
classifiers we consider logistic regression A similar phenomenon should also hold for the ERM
but discussing this solution is more difficult since the ERM solution does not have have a simple
characterization The relationship between the loss and convexP
surrogates has been studied
by The score criterion for logistic regression is p?i where
p?i w?x
are the fitted probabilities Note that easily-classified examples where p?i is
close to play almost no role in driving the fit Dropout turns easy examples into hard examples
giving more examples a chance to participate in learning a good classification rule
Figure 2a illustrates dropout?s tendency to spread influence more democratically for a simple classification problem with The red class is a mixture over two topics one of which is much
less common but harder to classify than the other There is only one topic for the blue class For
long documents open circles in the top right the infrequent hard-to-classify red cluster dominates
the fit while the frequent easy-to-classify red cluster is essentially ignored For dropout documents
with small dots lower left both red clusters are relatively hard to classify so the infrequent one plays a less disproportionate role in driving the fit As a result the fit based on dropout is
more stable but misses the finer structure near the decision boundary Note that the solid gray curve
the Bayes boundary is unaffected by dropout per Proposition But because it is nonlinear we
obtain a different linear approximation under dropout
Experiments and Discussion
Synthetic Experiment Consider the following
instance
of the Poisson topic model We choose
the document label uniformly at random Given label we choose topic
deterministically given label we choose a real-valued topic The per-topic Poisson
intensities are defined as follows
if
otherwise
ej
The first block of independent words are indicative of label the second block of correlated
words are indicative of label and the remaining words are indicative of neither
Log.Reg
Naive Bayes
Dropout?0.8
Dropout?0.5
Dropout?0.2
Log.Reg
Naive Bayes
Dropout?0.8
Dropout?0.5
Dropout?0.2
Error rate
Error rate
Fraction of data used
Polarity dataset
Fraction of data used
IMDB dataset
Figure Experiments on sentiment classification More dropout is better relative to logistic regression for small datasets and gradually worsens with more training data
We train a model on training sets of various size and evaluate the resulting classifiers error rates
on a large test set For dropout we recalibrate the intercept on the training set Figure 2b shows
the results There is a clear bias-variance tradeoff with logistic regression and naive Bayes
on the two ends of the spectrum For moderate values of dropout improves performance
with resulting in roughly 50-word documents appearing nearly optimal for this example
Sentiment Classification We also examined the performance of dropout as a function of training
set size on a document classification task Figure 3a shows results on the Polarity task
where the goal is to classify positive versus negative movie reviews on IMDB We divided the
dataset into a training set of size and a test set of size and trained a bag-of-words logistic
regression model with features This example exhibits the same behavior as our simulation
Using a larger results in a classifier that converges faster at first but then plateaus We also
ran experiments on a larger IMDB dataset with training and test sets of size each and
approximately features As Figure 3b shows the results are similar although the training
set is not large enough for the learning curves to cross When using the full training set all but three
pairwise comparisons in Figure are statistically significant for McNemar?s test
Dropout and Generative Modeling Naive Bayes and empirical risk minimization represent two
divergent approaches to the classification problem ERM is guaranteed to find the best model as
but can have suboptimal generalization error when is not large relative to Conversely naive
Bayes has very low generalization error but suffers from asymptotic bias In this paper we showed
that dropout behaves as a link between ERM and naive Bayes and can sometimes achieve a more
favorable bias-variance tradeoff By training on randomly generated sub-documents rather than on
whole documents dropout implicitly codifies a generative assumption about the data namely that
excerpts from a long document should have the same label as the original document Proposition
Logistic regression with dropout appears to have an intriguing connection to the naive Bayes SVM
NBSVM which is a way of using naive Bayes generative assumptions to strengthen an SVM.
In a recent survey of bag-of-words classifiers for document classification NBSVM and dropout often
obtain state-of-the-art accuracies This suggests that a good way to learn linear models for
document classification is to use discriminative models that borrow strength from an approximate
generative assumption to cut their generalization error Our analysis presents an interesting contrast
to other work that directly combine generative and discriminative modeling by optimizing a hybrid
likelihood 22 23 24 Our approach is more guarded in that we only let the generative
assumption speak through pseudo-examples
Conclusion We have presented a theoretical analysis that explains how dropout training can be
very helpful under a Poisson topic model assumption Specifically by making training examples
artificially difficult dropout improves the exponent in the generalization bound for ERM. We believe
that this work is just the first step in understanding the benefits of training with artificially corrupted
features and we hope the tools we have developed can be extended to analyze other training schemes
under weaker data-generating assumptions

----------------------------------------------------------------

title: 5776-expressing-an-image-stream-with-a-sequence-of-natural-sentences.pdf

Expressing an Image Stream with a Sequence of
Natural Sentences
Cesc Chunseong Park
Gunhee Kim
Seoul National University Seoul Korea
park.chunseong,gunhee}@snu.ac.kr
https://github.com/cesc-park/CRCN
Abstract
We propose an approach for retrieving a sequence of natural sentences for an
image stream Since general users often take a series of pictures on their special
moments it would better take into consideration of the whole image stream to produce natural language descriptions While almost all previous studies have dealt
with the relation between a single image and a single natural sentence our work
extends both input and output dimension to a sequence of images and a sequence
of sentences To this end we design a multimodal architecture called coherent
recurrent convolutional network CRCN which consists of convolutional neural
networks bidirectional recurrent neural networks and an entity-based local coherence model Our approach directly learns from vast user-generated resource of
blog posts as text-image parallel training data We demonstrate that our approach
outperforms other state-of-the-art candidate methods using both quantitative measures BLEU and top-K recall and user studies via Amazon Mechanical Turk
Introduction
Recently there has been a hike of interest in automatically generating natural language descriptions
for images in the research of computer vision natural language processing and machine learning
26 While most of existing work aims at discovering the relation
between a single image and a single natural sentence we extend both input and output dimension to
a sequence of images and a sequence of sentences which may be an obvious next step toward joint
understanding of the visual content of images and language descriptions albeit under-addressed in
current literature Our problem setup is motivated by that general users often take a series of pictures
on their memorable moments For example many people who visit New York City NYC would
capture their experiences with large image streams and thus it would better take the whole photo
stream into consideration for the translation to a natural language description
Figure An intuition of our problem statement with a New York City example We aim at expressing an image
stream with a sequence of natural sentences We leverage natural blog posts to learn the relation between
image streams and sentence sequences We propose coherent recurrent convolutional networks CRCN
that integrate convolutional networks bidirectional recurrent networks and the entity-based coherence model
Fig.1 illustrates an intuition of our problem statement with an example of visiting NYC. Our objective
is given a photo stream to automatically produce a sequence of natural language sentences that
best describe the essence of the input image set We propose a novel multimodal architecture named
coherent recurrent convolutional networks CRCN that integrate convolutional neural networks for
image description bidirectional recurrent neural networks for the language model and the
local coherence model for a smooth flow of multiple sentences Since our problem deals with
learning the semantic relations between long streams of images and text it is more challenging to
obtain appropriate text-image parallel corpus than previous research of single sentence generation
Our idea to this issue is to directly leverage online natural blog posts as text-image parallel training
data because usually a blog consists of a sequence of informative text and multiple representative
images that are carefully selected by authors in a way of storytelling See an example in
We evaluate our approach with the blog datasets of the NYC and Disneyland consisting of more than
blog posts with associated images Although we focus on the tourism topics in our experiments our approach is completely unsupervised and thus applicable to any domain that has a large
set of blog posts with images We demonstrate the superior performance of our approach by comparing with other state-of-the-art alternatives including We evaluate with quantitative
measures BLEU and Top-K recall and user studies via Amazon Mechanical Turk
Related work Due to a recent surge of volume of literature on this subject of generating natural language descriptions for image data here we discuss a representative selection of ideas that are closely
related to our work One of the most popular approaches is to pose the text generation as a retrieval
problem that learns ranking and embedding in which the caption of a test image is transferred from
the sentences of its most similar training images Our approach partly involves the
text retrieval because we search for candidate sentences for each image of a query sequence from
training database However we then create a final paragraph by considering both compatibilities
between individual images and text and the coherence that captures text relatedness at the level of
sentence-to-sentence transitions There have been also video-sentence works our key
novelty is that we explicitly include the coherence model Unlike videos consecutive images in the
streams may show sharp changes of visual content which cause the abrupt discontinuity between
consecutive sentences Thus the coherence model is more demanded to make output passages fluent
Many recent works have exploited multimodal networks that combine deep convolutional neural networks CNN and recurrent neural network RNN Notable architectures in this category
integrate the CNN with bidirectional RNNs long-term recurrent convolutional nets longshort term memory nets deep Boltzmann machines dependency-tree RNN and other
variants of multimodal RNNs Although our method partly take advantage of such recent
progress of multimodal neural networks our major novelty is that we integrate it with the coherence
model as a unified end-to-end architecture to retrieve fluent sequential multiple sentences
In the following we compare more previous work that bears a particular resemblance to ours
Among multimodal neural network models the long-term recurrent convolutional net is related
to our objective because their framework explicitly models the relations between sequential inputs
and outputs However the model is applied to a video description task of creating a sentence for a
given short video clip and does not address the generation of multiple sequential sentences Hence
unlike ours there is no mechanism for the coherence between sentences The work of addresses
the retrieval of image sequences for a query paragraph which is the opposite direction of our problem They propose a latent structural SVM framework to learn the semantic relevance relations from
text to image sequences However their model is specialized only for the image sequence retrieval
and thus not applicable to the natural sentence generation
Contributions We highlight main contributions of this paper as follows To the best of our
knowledge this work is the first to address the problem of expressing image streams with sentence
sequences We extend both input and output to more elaborate forms with respect to a whole body
of existing methods image streams instead of individual images and sentence sequences instead of
individual sentences We develop a multimodal architecture of coherent recurrent convolutional
networks CRCN which integrates convolutional networks for image representation recurrent networks for sentence modeling and the local coherence model for fluent transitions of sentences
We evaluate our method with large datasets of unstructured blog posts consisting of blog posts
with associated images With both quantitative evaluation and user studies we show that our
approach is more successful than other state-of-the-art alternatives in verbalizing an image stream
Text-Image Parallel Dataset from Blog Posts
We discuss how to transform blog posts to a training set of image-text parallel data streams each
of which is a sequence of image-sentence pairs T1l IN
TN B. The training
set size is denoted by shows the summary of pre-processing steps for blog posts
Blog Pre-processing
We assume that blog authors augment their text with multiple images in a semantically meaningful
manner In order to decompose each blog into a sequence of images and associated text we first
perform text segmentation and then text summarization The purpose of text segmentation is to
divide the input blog text into a set of text segments each of which is associated with a single
image Thus the number of segments is identical to the number of images in the blog The objective
of text summarization is to reduce each text segment into a single key sentence As a result of these
two processes we can transform each blog into a form of T1l IN
TN
Text segmentation We first divide the blog passage into text blocks according to paragraphs We
apply a standard paragraph tokenizer of NLTK that uses rule-based regular expressions to detect
paragraph divisions We then use the heuristics based on the image-to-text block distances proposed
in Simply we assign each text block to the image that has the minimum index distance where
each text block and image is counted as a single index distance in the blog
Text summarization We summarize each text segment into a single key sentence We apply the
Latent Semantic Analysis LSA)-based summarization method which uses the singular value
decomposition to obtain the concept dimension of sentences and then recursively finds the most
representative sentences that maximize the inter-sentence similarity for each topic in a text segment
Data augmentation The data augmentation is a well-known technique for convolutional neural
networks to improve image classification accuracies Its basic idea is to artificially increase
the number of training examples by applying transformations horizontal reflection or adding noise
to training images We empirically observe that this idea leads better performance in our problem
as well For each image-sentence sequence T1l IN
TN we augment each
sentence Tn with multiple sentences for training That is when we perform the LSA-based text
summarization we select top highest ranked summary sentences among which the top-ranked
one becomes the summary sentence for the associated image and all the top ones are used for
training in our model With a slight abuse of notation we let Tnl to denote both the single summary
sentence and augmented sentences We choose after thorough empirical tests
Text Description
Once we represent each text segment with sentences we extract the paragraph vector to represent the content of text The paragraph vector is a neural-network based unsupervised algorithm
that learns fixed-length feature representation from variable-length pieces of passage We learn 300dimensional dense vector representation separately from the two classes of the blog dataset using
the gensim doc2vec code We use pn to denote the paragraph vector representation for text Tn
We then extract a parsed tree for each Tn to identify coreferent entities and grammatical roles of the
words We use the Stanford core NLP library The parse trees are used for the local coherence
model which will be discussed in section
Our Architecture
Many existing sentence generation models combine words or phrases from training
data to generate a sentence for a novel image Our approach is one level higher we use sentences
from training database to author a sequence of sentences for a novel image stream Although our
model can be easily extended to use words or phrases as basic building blocks such granularity
makes sequences too long to train the language model which may cause several difficulties for
learning the RNN models For example the vanishing gradient effect is a well-known hardship to
backpropagate an error signal through a long-range temporal interval Therefore we design our
approach that retrieves individual candidate sentences for each query image from training database
and crafts a best sentence sequence considering both the fitness of individual image-to-sentence
pairs and coherence between consecutive sentences
Figure Illustration of pre-processing steps of blog posts and the proposed CRCN architecture
illustrates the structure of our CRCN It consists of three main components which are
convolutional neural networks CNN for image representation bidirectional recurrent neural
networks BRNN for sentence sequence modeling and the local coherence model for a
smooth flow of multiple sentences Each data stream is a variable-length sequence denoted by
T1 IN TN We use to denote a position of a sentence/image in a
sequence We define the CNN and BRNN model for each position separately and the coherent
model for a whole data stream For the CNN component our choice is the VGGNet that
represents images as 4,096-dimensional vectors We discuss the details of our BRNN and coherence
model in section and section respectively and finally present how to combine the output of
the three components to create a single compatibility score in section
The BRNN Model
The role of BRNN model is to represent a content flow of text sequences In our problem the BRNN
is more suitable than the normal RNN because the BRNN can simultaneously model forward and
backward streams which allow us to consider both previous and next sentences for each sentence to
make the content of a whole sequence interact with one another As shown in our BRNN
has five layers input layer forward/backward layer output layer and ReLU activation layer which
are finally merged with that of the coherent model into two fully connected layers Note that each
text is represented by 300-dimensional paragraph vector pt as discussed in section The exact
form of our BRNN is as follows See together for better understanding
xft Wif pt bfi
hft
xft
Wf hft?1
xbt Wib pt bbi
bf hbt xbt Wb hbt+1 bb ot
Wo hft
hbt bo
The BRNN takes a sequence of text vectors pt as input We then compute xft and xbt which are the
activations of input units to forward and backward units Unlike other BRNN models we separate
the input activation into forward and backward ones with different sets of parameters Wif and Wib
which empirically leads a better performance We set the activation function to the Rectified
Linear Unit ReLU Then we create two independent forward and backward
hidden units denoted by hft and hbt The final activation of the BRNN ot can be regarded as a
description for the content of the sentence at location which also implicitly encodes the flow of
the sentence and its surrounding context in the sequence The parameter sets to learn include weights
Wif Wib Wf Wb Wo and biases bfi bbi bf bb bo
The Local Coherence Model
The BRNN model can capture the flow of text content but it lacks learning the coherence of passage
that reflects distributional syntactic and referential information between discourse entities Thus
we explicitly include a local coherence model based on the work of which focuses on resolving
the patterns of local transitions of discourse entities coreferent noun phrases in the whole
text As shown in we first extract parse trees for every summarized text denoted by Zt
and then concatenate all sequenced parse trees into one large one from which we make an entity
grid for the whole sequence The entity grid is a table where each row corresponds to a discourse
entity and each column represents a sentence Grammatical role are expressed by three categories
and one for absent not referenced in the sentence subjects objects other than
subject or object and absent After making the entity grid we enumerate the transitions of the
grammatical roles of entities in the whole text We set the history parameter to three which means
we can obtain 43 64 transition descriptions SO or OOX By computing the ratio of
the occurrence frequency of each transition we finally create a 64-dimensional representation that
captures the coherence of a sequence Finally we make this descriptor to a 300-dimensional vector
by zero-padding and forward it to ReLU layer as done for the BRNN output
Combination of CNN RNN and Coherence Model
After the ReLU activation layers of the RNN and the coherence model their output ot
and
goes through two fully connected layers whose role is to decide a proper combination of the
BRNN language factors and the coherence factors We drop the bias terms for the fully-connected
layers and the dimensions of variables are Wf Wf ot
st and
Wf Wf
We use the shared parameters for and so that the output mixes well the interaction between the
content flows and coherency In our tests joint learning outperforms learning the two terms with
separate parameters Note that the multiplication Wf Wf of the last two FC layers does not reduce
to a single linear mapping thanks to dropout We assign and dropout rates to the two layers
Empirically it improves generalization performance much over a single FC layer with dropout
Training the CRCN
To train our CRCN model we first define the compatibility score between an image stream and a
paragraph sequence While our score function is inspired by Karpathy there are two major
differences First the score function of deals between sentence fragments and image fragments
and thus the algorithm considers all combinations between them to find out the best matching On
the other hand we define the score by an ordered and paired compatibility between a sentence
sequence and an image sequence Second we also add the term that measures the relevance relation
of coherency between an image sequence and a text sequence Finally the score Skl for a sentence
sequence and an image stream is defined by
Skl
skt vtl vtl
where vtl denotes the CNN feature vector for t-th image of stream We then define the cost function
to train our CRCN model as follows
XhX
Skl Skk
Slk Skk
where Skk denotes the score between a training pair of corresponding image and sentence sequence
The objective based on the max-margin structured loss encourages aligned image-sentence sequence pairs to have a higher score by a margin than misaligned pairs For each positive training
example we randomly sample ne examples from the training set Since each contrastive example has a random length and is sampled from the dataset of a wide range of content it is extremely
unlikely that the negative examples have the same length and the same content order of sentences
with positive examples
Optimization We use the backpropagation through time BPTT algorithm to train our model
We apply the stochastic gradient descent SGD with mini-batches of data streams Among
many SGD techniques we select RMSprop optimizer which leads the best performance in
our experiments We initialize the weights of our CRCN model using the method of He
which is robust in deep rectified models We observe that it is better than a simple Gaussian random
initialization although our model is not extremely deep We use dropout regularization in all layers
except the BRNN with dropout for the last FC layer and for the other remaining layers
Retrieval of Sentence Sequences
At test time the objective is to retrieve a best sentence sequence for a given query image stream
IqN First we select K-nearest images for each query image from training database using the distance on the CNN VGGNet fc7 features In our experiments is successful
We then generate a set of sentence sequence candidates by concatenating the sentences associated
with the K-nearest images at each location Finally we use our learned CRCN model to compute
the compatibility score between the query image stream and each sequence candidate according to
which we rank the candidates
However one major difficulty of this scenario is that there are exponentially many candidates
To resolve this issue we use an approximate divide-and-conquer strategy we recursively halve the problem into subproblems until the size of the subproblem is manageable For
example if we halve the search candidate length times then the search space of each subproblem
becomes Using the beam search idea we first find the top-M best sequence candidates in
the subproblem of the lowest level and recursively increase the candidate lengths while the maximum candidate size is limited to We set Though it is an approximate search our
experiments assure that it achieves almost optimal solutions with plausible combinatorial search
mainly because the local fluency and coherence is undoubtedly necessary for the global one That
is in order for a whole sentence sequence to be fluent and coherent its any subparts must be as well
Experiments
We compare the performance of our approach with other state-of-the-art candidate methods via
quantitative measures and user studies using Amazon Mechanical Turk Please refer to the
supplementary material for more results and the details of implementation and experimental setting
Experimental Setting
Dataset We collect blog datasets of the two topics NYC and Disneyland We reuse the blog data
of Disneyland from the dataset of and newly collect the data of NYC using the same crawling
method with in which we first crawl blog posts and their associated pictures from two popular
blog publishing sites BLOGSPOT and WORDPRESS by changing query terms from Google search
Then we manually select the travelogue posts that describe stories and events with multiple images
Finally the dataset includes unique blog posts and images for NYC and blog
posts and images for Disneyland
Task For quantitative evaluation we randomly split our dataset into as a training set as
a validation and the others as a test set For each test post we use the image sequence as a query
Iq and the sequence of summarized sentences as groundtruth TG Each algorithm retrieves the best
sequences from training database for a query image sequence and ideally the retrieved sequences
match well with TG Since the training and test data are disjoint each algorithm can only retrieve
similar but not identical sentences at best
For quantitative measures we exploit two types of metrics of language similarity BLEU
CIDEr and METEOR scores and retrieval accuracies top-K recall and median rank
which are popularly used in text generation literature 19 The top-K recall R@K is
the recall rate of a groundtruth retrieval given top candidates and the median rank indicates the
median ranking value of the first retrieved groundtruth A better performance is indicated by higher
BLEU CIDEr METEOR R@K scores and lower median rank values
Baselines Since the sentence sequence generation from image streams has not been addressed yet
in previous research we instead extend several state-of-the-art single-sentence models that have
publicly available codes as baselines including the log-bilinear multimodal models by Kiros
and recurrent convolutional models by Karpathy and Vinyals For
we use the three variants introduced in the paper which are the standard log-bilinear model
and two multi-modal extensions modality-based LBL MLBL-B and factored three-way
LBL MLBL-F We use the NeuralTalk package authored by Karpathy for the baseline
of denoted by CNN+RNN and denoted by CNN+LSTM As the simplest baseline we
also compare with the global matching GloMatch in For all the baselines we create final
sentence sequences by concatenating the sentences generated for each image in the query stream
CNN+LSTM
CNN+RNN
MLBL-F
MLBL-B
LBL
GloMatch
RCN
CRCN
CNN+LSTM
CNN+RNN
MLBL-F
MLBL-B
LBL
GloMatch
RCN
CRCN
Language metrics
Retrieval metrics
CIDEr METEOR R@1 R@5 MedRank
New York City
Disneyland
Table Evaluation of sentence generation for the two datasets New York City and Disneyland with language
similarity metrics BLEU and retrieval metrics R@K median Rank A better performance is indicated by
higher BLEU CIDEr METEOR R@K scores and lower median rank values
We also compare between different variants of our method to validate the contributions of key components of our method We test the K-nearest search without the RNN part as the simplest
variant for each image in a test query we find its most similar training images and simply
concatenate their associated sentences The second variant is the BRNN-only method denoted by
RCN that excludes the entity-based coherence model from our approach Our complete method is
denoted by CRCN and this comparison quantifies the improvement by the coherence model To be
fair we use the same VGGNet fc7 feature for all the algorithms
Quantitative Results
Table shows the quantitative results of experiments using both language and retrieval metrics
Our approach CRCN and RCN outperform with large margins other state-of-the-art baselines
which generate passages without consideration of sentence-to-sentence transitions unlike ours The
MLBL-F shows the best performance among the three models of albeit with a small margin
partly because they share the same word dictionary in training Among mRNN-based models the
CNN+LSTM significantly outperforms the CNN+RNN because the LSTM units help learn models
from irregular and lengthy data of natural blogs more robustly
We also observe that CRCN outperforms and especially with the retrieval metrics
It shows that the integration of two key components the BRNN and the coherence model indeed
contributes the performance improvement The CRCN is only slightly better than the RCN in language metrics but significantly better in retrieval metrics It means that RCN is fine with retrieving
fairly good solutions but not good at ranking the only correct solution high compared to CRCN
The small margins in language metrics are also attributed by their inherent limitation for example
the BLEU focuses on counting the matches of n-gram words and thus is not good at comparing
between sentences even worse between paragraphs for fully evaluating their fluency and coherency
Fig.3 illustrates several examples of sentence sequence retrieval In each set we show a query
image stream and text results created by our method and baselines Except we show parts
of sequences because they are rather long for illustration These qualitative examples demonstrate
that our approach is more successful to verbalize image sequences that include a variety of content
User Studies via Amazon Mechanical Turk
We perform user studies using AMT to observe general users preferences between text sequences
by different algorithms Since our evaluation involves multiple images and long passages of text we
design our AMT task to be sufficiently simple for general turkers with no background knowledge
Figure Examples of sentence sequence retrieval for NYC top and Disneyland bottom In each set we
present a part of a query image stream and its corresponding text output by our method and a baseline
Baselines
GloMatch
CNN+LSTM
MLBL-B
RCN
RCN
NYC
Disneyland
Table The results of AMT pairwise preference tests We present the percentages of responses that turkers
vote for our CRCN over baselines The length of query streams is except the last column which has
We first randomly sample test streams from the two datasets We first set the maximum number
of images per query to If a query is longer than that we uniformly sample it to In an AMT
test we show a query image stream Iq and a pair of passages generated by our method CRCN and
one baseline in a random order We ask turkers to choose more agreed text sequence with Iq We
design the test as a pairwise comparison instead of a multiple-choice question to make answering
and analysis easier The questions look very similar to the examples of We obtain answers
from three different turkers for each query We compare with four baselines we choose MLBL-B
among the three variants of and CNN+LSTM among mRNN-based methods We also select
GloMatch and RCN as the variants of our method
Table shows the results of AMT tests which validate that AMT annotators prefer our results to
those of baselines The GloMatch is the worst because it uses too weak image representation
GIST and Tiny images The differences between CRCN and RCN 4th column of Table
are not as significant as previous quantitative measures mainly because our query image stream
is sampled to relatively short The coherence becomes more critical as the passage is longer To
justify this argument we run another set of AMT tests in which we use images per query As
shown in the last column of Table the performance margins between CRCN and RCN become
larger as the lengths of query image streams increase This result assures that as passages are longer
the coherence becomes more important and thus CRCN)?s output is more preferred by turkers
Conclusion
We proposed an approach for retrieving sentence sequences for an image stream We developed
coherent recurrent convolutional network CRCN which consists of convolutional networks bidirectional recurrent networks and entity-based local coherence model With quantitative evaluation
and users studies using AMT on large collections of blog posts we demonstrated that our CRCN
approach outperformed other state-of-the-art candidate methods
Acknowledgements This research is partially supported by Hancom and Basic Science Research
Program through National Research Foundation of Korea

----------------------------------------------------------------

title: 5303-learning-a-concept-hierarchy-from-multi-labeled-documents.pdf

Learning a Concept Hierarchy from
Multi-labeled Documents
Viet-An Nguyen1 Jordan Boyd-Graber2 Philip Resnik1,3,4 Jonathan Chang5
Computer Science Linguistics UMIACS
Computer Science
Facebook
Univ of Maryland College Park MD
Univ of Colorado Boulder CO Menlo Park CA
vietan@cs.umd.edu
Jordan.Boyd.Graber jonchang@fb.com
resnik@umd.edu
@colorado.edu
Abstract
While topic models can discover patterns of word usage in large corpora it is
difficult to meld this unsupervised structure with noisy human-provided labels
especially when the label space is large In this paper we present a model?Label
to Hierarchy H)?that can induce a hierarchy of user-generated labels and the
topics associated with those labels from a set of multi-labeled documents The
model is robust enough to account for missing labels from untrained disparate
annotators and provide an interpretable summary of an otherwise unwieldy label
set We show empirically the effectiveness of in predicting held-out words
and labels for unseen documents
Understanding Large Text Corpora through Label Annotations
Probabilistic topic models discover the thematic structure of documents from news blogs and
web pages Typical unsupervised topic models such as latent Dirichlet allocation LDA uncover
topics from unannotated documents In many settings however documents are also associated
with additional data which provide a foundation for joint models of text with continuous response
variables 48 categorical labels 18 46 or link structure
This paper focuses on additional information in the form of multi-labeled data where each document
is tagged with a set of labels These data are ubiquitous Web pages are tagged with multiple
directories,1 books are labeled with different categories or political speeches are annotated with
multiple issues.2 Previous topic models on multi-labeled data focus on a small set of relatively
independent labels 36 Unfortunately in many real-world examples the number of labels
from hundreds to thousands?is incompatible with the independence assumptions of these models
In this paper we capture the dependence among the labels using a learned tree-structured hierarchy
Our proposed model H?Label to Hierarchy?learns from label co-occurrence and word usage to
discover a hierarchy of topics associated with user-generated labels We show empirically that
can improve over relevant baselines in predicting words or missing labels in two prediction tasks
is designed to explicitly capture the relationships among labels to discover a highly interpretable
hierarchy from multi-labeled data This interpretable hierarchy helps improve prediction performance
and also provides an effective way to search browse and understand multi-labeled data
Part of this work was done while the first author interned at Facebook
Open Directory Project http://www.dmoz.org
Policy Agenda Codebook http://policyagendas.org
Capturing Label Dependencies using a Tree-structured Hierarchy
Discovering a topical hierarchy from text has been the focus of much topic modeling research One
popular approach is to learn an unsupervised hierarchy of topics For example hLDA learns
an unbounded tree-structured hierarchy of topics from unannotated documents One drawback of
hLDA is that documents only are associated with a single path in the topic tree Recent work
relaxing this restriction include TSSB nHDP nCRF and LDA Going beyond
tree structure PAM captures the topic hierarchy using a pre-defined DAG inspiring more flexible
extensions However since only unannotated text is used to infer the hierarchical topics
it usually requires an additional step of topic labeling to make them interpretable This difficulty
motivates work leveraging existing taxonomies such as HSLDA and hLLDA
A second active area of research is constructing a taxonomy from multi-labeled data For example
Heymann and Garcia-Molina extract a tag hierarchy using the tag network centrality similar work has been applied to protein hierarchies Hierarchies of concepts have come from
seeded ontologies crowdsourcing and user-specified relations More sophisticated
approaches build domain-specific keyword taxonomies with adapting Bayesian Rose Trees
These approaches however concentrate on the tags ignoring the content the tags describe
In this paper we combine ideas from these two lines of research and introduce a hierarchical
topic model that discovers a tree-structured hierarchy of concepts from a collection of multi-labeled
documents takes as input a set of documents wd each tagged with a set of labels ld
The label set contains unique unstructured labels and the word vocabulary size is To
learn an interpretable taxonomy associates each label?a user-generated word/phrase?with a
topic?a multinomial distribution over the vocabulary?to form a concept and infers a tree-structured
hierarchy to capture the relationships among concepts Figure shows the plate diagram for
together with its generative process
Create label graph and draw a uniform spanning tree from
For each node in
If is the root draw background topic Dir(?u
Otherwise draw topic where is node k?s
parent
For each document having labels ld
Define L0d and L1d using and ld
Draw Dir(L0d and Dir(L1d
Draw a stochastic switching variable Beta(?0
For each token Nd
Draw set indicator xd,n Bern(?d
ii Draw topic indicator zd,n Mult(?d d,n
iii Draw word wd,n Mult(?zd,n
Figure Generative process and the plate diagram notation of H.
Generating a labeled topic hierarchy
We assume an underlying directed graph in which each node is a concept consisting of
a label?observable user-generated input and a topic?latent multinomial distribution over
words.3 The prior weight of a directed edge from node to node is the fraction of documents tagged
with label which are also tagged with label ti,k Di,k Dj We also assume an additional
Background node Edges to the Background node have prior zero weight and edges from the
Background node to node have prior weight troot,i Di maxk Dk Here Di is the number of
documents tagged with label and Di,k is the number of documents tagged with both labels and
The tree is a spanning tree generated from G. The probability
of a tree given the graph is thus
the product of all its edge prior weights p(T e?E te To capture the intuition that child
nodes in the hierarchy specialize the concepts of their parents we model the topic at each node
In this paper we use node when emphasizing the structure discovered by the model Each node corresponds
to a concept which consists of a label and a topic
using a Dirichlet distribution whose mean is centered at the topic of node k?s parent
The topic at the root node is drawn from a symmetric Dirichlet root
where is a uniform distribution over the vocabulary This is similar to the idea of backoff in
language models where more specific contexts inherit the ideas expressed in more general contexts
if we talk about pedagogy in education there?s a high likelihood we?ll also talk about it in
university education
Generating documents
As in LDA each word in a document is generated by one of the latent topics however also
uses the labels and topic hierarchy to restrict the topics a document uses The document?s label set ld
identifies which nodes are more likely to be used Restricting tokens of a document in this way?to
be generated only from a subset of the topics depending the document?s labels?creates specific
focused labeled topics Labeled LDA
Unfortunately ld is unlikely to be an exhaustive enumeration particularly when the label set is large
users often forget or overlook relevant labels We therefore depend on the learned topology of the
hierarchy to fill in the gaps of what users forget by expanding ld into a broader set L1d which is the
union of nodes on the paths from the root node to any of the document?s label nodes We call this
the document?s candidate set The candidate set also induces a complementary set L0d L1d
illustrated in Figure Previous approaches such as LPAM and Tree labeled LDA also
leverage the label hierarchy to expand the original label set However these previous models require
that the label hierarchy is given rather than inferred as in our H.
Figure Illustration of the candidate label set given a document
having labels ld double-circled nodes the candidate label
set of consists of nodes on all the paths from the root node to node
and node L1d and L0d This allows
an imperfect label set to induce topics that the document should be
associated with even if they weren?t explicitly enumerated
replaces Labeled LDA?s absolute restriction to specific topics to a soft preference To achieve this
each document has a switching variable drawn from Beta(?0 which effectively decides
how likely tokens in are to be generated from L1d versus L0d Token in document is generated by
first flipping the biased coin to choose the set indicator xd,n Given xd,n the label zd,n is drawn
from the corresponding label distribution d,n and the token is generated from the corresponding
topic wd,n Mult(?zd,n
Posterior Inference
Given a set of documents with observed words wd and labels ld inference finds the posterior
distribution over the latent variables We use a Markov chain Monte Carlo MCMC algorithm to
perform posterior inference in which each iteration after initialization consists of the following
steps sample a set indicator xd,n and topic assignment zd,n for each token sample a word
distribution for each node in the tree and update the structure of the label tree
Initialization With the large number of labels the space of hierarchical structures that MCMC
needs to explore is huge Initializing the tree-structure hierarchy is crucial to help the sampler focus
on more important regions of the search space and help the sampler converge We initialize the
hierarchy with the maximum a priori probability tree by running Chu-Liu/Edmonds algorithm to
find the maximum spanning tree on the graph starting at the background node
Sampling assignments xd,n and zd,n For each token we need to sample whether it was generated
from the label set or not xd,n We choose label set with probability
node in the chosen set with probability
Nd,k
Cd,i
Cd,i
and we sample a
k,wd,n Here Cd,i is the number of times
tokens in document are assigned to label set Nd,k is the number of times tokens in document
are assigned to node Marginal counts are denoted by and denotes the counts excluding the
assignment of token wd,n
After we have the label set we can sample the topic assignment This is more efficient than sampling
jointly as most tokens are in the label set and there are a limited number of topics in the label set
The probability of assigning node to zd,n is
p(xd,n zd,n x?d,n Lid
Cd,i
Nd,k
Cd,i
Lid
k,wd,n
Sampling topics As discussed in Section topics on each path in the hierarchy form a
cascaded Dirichlet-multinomial chain where the multinomial at node is drawn from a Dirichlet
distribution with the mean vector being the topic at the parent node Given assignments
of tokens to nodes we need to determine the conditional probability of a word given the token This
can be done efficiently in two steps bottom-up smoothing and top-down sampling
k,v of node propagated from its children
Bottom-up smoothing This step estimates the counts
This can be approximated efficiently by using the minimal/maximal path assumption For
k,v if Mk0
the minimal path assumption each child node of propagates a value of to
k,v
For the maximal path assumption each child node of propagates the full count Mk0 to
k,v for each node from leaf to root we sample the word
Top-down sampling After estimating
distributions top-down using its actual counts mk its children?s propagated counts
and its
parent?s word distribution as Dir(mk
Updating tree structure We update the tree structure by looping through each non-root
node proposing a new parent node and either accepting or rejecting the proposed parent using
the Metropolis-Hastings algorithm More specifically given a non-root node with current parent
we propose a new parent node by sampling from all incoming nodes of in graph with probability
proportional to the corresponding edge weights If the proposed parent node is a descendant of
we reject the proposal
to avoid a cycle
If it is not a descendant we accept the proposed move
with probability min where and denote the proposal distribution and the
model?s joint distribution respectively and denotes the case where is the parent of
Since we sample the proposed parent using the edge weights the proposal probability ratio is
Q(i
ti,k
Q(j
tj,k
The joint probability of H?s observed and latent variables is
e?E
p(e
p(xd xd ld zd
p(?root
When node changes its parent from node to the candidate set L1d changes for any document
that is tagged with any label in the subtree rooted at Let 4k denote the subtree rooted at and
D4k 4k ld the set of documents whose candidate set might change when k?s
parent changes Canceling unchanged quantities the ratio of the joint probabilities is
tj,k
ti,k
p(zd p(xd p(wd
p(zd p(xd p(wd
d?D4k
We now expand each factor in Equation The probability of node assignments zd for document is
computed by integrating out the document-topic multinomials and for the candidate set and its
inverse
p(zd xd L0d L1d
l?Ld
Similarly we compute the probability of xd for each document integrating out
p(xd
Since we explicitly sample the topic at each node we need to re-sample all topics for the case that
QK
is the parent of to compute the ratio
Given the sampled the word likelihood is
QNd
p(wd zd
zd,n wd,n
However re-sampling the topics for the whole hierarchy for every node proposal is inefficient To
avoid that we keep all fixed and approximate the ratio as
p(wd
d?k
p(mk
d?k
p(wd
p(mk
d?D4k
is the word counts propagated from children of
where mk is the word counts at node and
Since is fixed and the node assignments are unchanged the word likelihoods cancel out except
for tokens assigned at or any of its children The integration in Equation is
d?k
p(mk
k,v
Using Equations and we can compute the Metropolis-Hastings acceptance probability
Experiments Analyzing Political Agendas in U.S. Congresses
In our experiments we focus on studying political attention in the legislative process of interest to
both computer scientists and political scientists GovTrack provides bill text from
the US Congress each of which is assigned multiple political issues by the Congressional Research
Service Examples of Congressional issues include Education Higher Education Health Medicare etc To evaluate the effectiveness of we evaluate on two computational tasks document
modeling?measuring perplexity on a held-out set of documents?and multi-label classification We
also discuss qualitative results based on the label hierarchy learned by our model
Data We use the text and labels from GovTrack for the through Congresses
For both quantitative tasks we perform 5-fold cross-validation For each fold we perform
standard pre-processing steps on the training set including tokenization removing stopwords stemming adding bigrams and filtering using TF IDF to obtain a vocabulary of words final
statistics in Figure After building the vocabulary from training documents we discard all
out-of-vocabulary words in the test documents We ignore labels associated with fewer than bills
Document modeling
In the first quantitative experiment we focus on the task of predicting the words in held-out test
documents given their labels This is measured by perplexity a widely-used evaluation metric
To compute perplexity we follow the estimating method described in Wallach Sec.
and split each test document into wdTE1 and wdTE2 During training we estimate all topics
During test first we run Gibbs sampling using the learned topics
distributions over the vocabulary
TE
on wd to estimate the topic proportions dTEfor each test document Then we compute the
TE
log(p(wd ld
perplexity on the held-out words wdTE2 as exp
where TE2 is the total
TE2
number of tokens in wdTE2
We find bigram candidates that occur at least ten times in the training set and use a test to filter out those
having a value less than We then treat selected bigrams as single word types in the vocabulary
Setup We compare our proposed model with the following methods
LDA unsupervised topic model with a flat topic structure In our experiments we set the
number of topics of LDA equal to the number of labels in each dataset
LDA associates each topic with a label and a document is generated using the topics
associated with the document?s labels only
Label to Flat structure a simplified version of with a fixed flat topic structure The
major difference between and LDA is that allows tokens to be drawn from topics that are
not in the document?s label set via the use of the switching variable Section Improvements
of over show the importance of the hierarchical structure
For all models the number of topics is the number of labels in the dataset We run for iterations
on the training data with a burn-in period of iterations After the burn-in period we store ten
sets of estimated parameters one after every fifty iterations During test time we run ten chains
using these ten learned models on the test data and compute the perplexity after iterations The
perplexity of each fold is the average value over the ten chains
Number of bills
Congress
Figure Dataset statistics
Perplexity
Number of labels
LDA
L?LDA
L2F
L2H
Figure Perplexity on held-out documents averaged over folds
Results Figure shows the perplexity of the four models averaged over five folds on the four
datasets LDA outperforms the other models with labels since it can freely optimize the likelihood
without additional constraints LDA and are comparable However significantly outperforms both LDA and F. Thus if incorporating labels into a model learning an additional topic
hierarchy improves predictive power and generalizability of LDA.
Multi-label Classification
Multi-label classification is predicting a set of labels for a test document given its text 23
The prediction is from a set of pre-defined labels and each document can be tagged with any
of the 2K possible subsets In this experiment we use L?an efficient max-margin multi-label
classifier study how features extracted from our improve classification
We use F1 as the evaluation metric The F1 score is first computed for each document as F1
where and are the precision and recall for document After F1 is
computed for all documents the overall performance can be summarized by micro-averaging and
macro-averaging to obtain Micro-F1 and Macro-F1 respectively In macro-averaging F1 is first
computed for each document using its own confusion matrix and then averaged In micro-averaging
on the other hand only a single confusion matrix is computed for all documents and the F1 score is
computed based on this single confusion matrix
Setup We use the following sets of features
TF Each document is represented by a vector of term frequency of all word types in the vocabulary
TF IDF Each document is represented by a vector dTFIDF of TF IDF of all word types
LDA TF IDF Ramage combine LDA features and TF IDF features to improve the
performance on recommendation tasks Likewise we extract a K-dimensional vector LDA and
combine with TF IDF vector dTFIDF to form the feature vector of LDA TF IDF.5
We run LDA on train for iterations and ten models after burn-in iterations For each model we
sample assignments for all tokens using iterations and average over chains to estimate LDA
TF IDF Similarly we combine TF IDF with the features extracted using
same MCMC setup as LDA
One complication for is the candidate label set L1d which is not observed during test time Thus
during test time we estimate L1d using TF IDF. Let Dl be the set of documents tagged with label
For each we compute a TF IDF vector lTFIDF avgd?Dl dTFIDF Then for each document we
generate the nearest labels using cosine similarity and add them to the candidate label set L1d of
Finally we expand this initial set by adding all labels on the paths from the root of the learned
hierarchy to any of the nearest labels Figure We explored different values of
with similar results the results in this section are reported with
Micro F1
Macro F1
TF
L?LDA TFIDF
L2H TFIDF
TFIDF
Figure Multi-label classification results The results are averaged over folds
Results Figure shows classification results For both Macro-F and Micro-F TF IDF LDA TF IDF and TF IDF significantly outperform TF Also LDA TF IDF performs better
than TF IDF which is consistent with Ramage
TF IDF performs better than LDA TF IDF which in turn performs better than TF IDF. This
shows that features extracted from are more predictive than those extracted from LDA and
both improve classification The improvements of TF IDF and LDA TF IDF over TF IDF are
clearer for Macro-F compared with Micro-F Thus features from both topic models help improve
prediction regardless of the frequencies of their tagged labels
Learned label hierarchy A taxonomy of Congressional issues
Terrorism
Military operations and
strategy
intellig intellig_commun
afghanistan nation_intellig
guantanamo_bai qaeda
central_intellig detent pakistan
interrog defens_intellig detaine
armi air_forc none navi
addit_amount control_act
emerg_deficit fund_appropri
balanc_budget terror_pursuant
transfer_author,marin_corp
International affairs
libya unit_nation intern_religi
bahrain religi_freedom
religi_minor freedom_act africa
violenc secur_council
benghazi privileg_resolut hostil
Foreign aid and
international relief
International law and
treaties
Human rights
fund_appropri foreign_assist
remain_avail regular_notif
intern_develop relat_program
unit_nation pakistan
foreign_oper usaid prior_act
foreign_assist intern_develop
vessel foreign_countri sanit
appropri_congression
develop_countri violenc girl
defens_articl export
traffick russian_feder
traffick_victim prison alien
visa nation_act victim detent
human_traffick corrupt russian
foreign_labor sex_traffick
Military personnel and
dependents
Department of Defense
Asia
Int'l organizations
cooperation
Middle East
coast_guard vessel command
special_select sexual_violenc
academi sexual_harass navi
former_offic gulf_coast haze
port marin marin_debri
air_forc militari_construct
author_act armi nation_defens
navi militari_depart aircraft
congression_defens command
sexual_assault activ_duti
china vietnam taiwan republ
chines sea north_korea
tibetan north_korean refuge
south_china intern_religi tibet
enterpris religi_freedom
export arm_export control_act
foreign_assist cuba
defens_articl foreign_countri
foreign_servic export_administr
author_act munit_list
syria israel iran enterpris_fund
unit_nation egypt palestinian
cypru tunisia hezbollah
lebanon republ hama syrian
violenc weapon
Armed forces and
national security
Department of
Homeland Security
Europe
Latin America
Sanctions
Religion
cemeteri nation_guard dog
service_memb
homeless_veteran funer
medic_center militari_servic
arlington_nation armi guard
cybersecur inform_secur
inform_system cover_critic
critic_infrastructur
inform_infrastructur
cybersecur_threat
republ belaru turkei nato
holocaust_survivor north_atlant
holocaust european_union
albania jew china macedonia
treati_organ albanian greec
border_protect haiti
merchandis evas tariff_act
cover_merchandis export
custom_territori custom_enforc
countervail_duti intern_trade
iran sanction syria
comprehens_iran north_korea
financi_institut presid_determin
islam_republ foreign_person
weapon iran_sanction
unit_nation israel iaea harass
syria iran peacekeep_oper
regular_budget unrwa
palestinian refuge durban bulli
secur_council
Figure A subtree in the hierarchy learned by H. The subtree root International Affairs is a child
node of the Background root node
To qualitatively analyze the hierarchy learned by our model Figure shows a subtree whose root
is about International Affairs obtained by running on bills in the U.S. Congress The
learned topic at International Affairs shows the focus of Congress on the Arab Spring?a
revolutionary wave of demonstrations and protests in Arab countries like Libya Bahrain etc The
concept is then split into two distinctive aspects of international affairs Military and Diplomacy
We are working with domain experts to formally evaluate the learned concept hierarchy A political
scientist personal communication comments
The international affairs topic does an excellent job of capturing the key distinction
between military/defense and diplomacy/aid Even more impressive is that it then
also captures the major policy areas within each of these issues the distinction
between traditional military issues and terrorism-related issues and the distinction
between thematic policy human rights and geographic/regional policy
Conclusion
We have presented a model that discovers not just the interaction between overt labels and
the latent topics used in a corpus but also how they fit together in a hierarchy Hierarchies are a
natural way to organize information and combining labels with a hierarchy provides a mechanism
for integrating user knowledge and data-driven summaries in a single consistent structure Our
experiments show that yields interpretable label/topic structures that it can substantially improve
model perplexity compared to baseline approaches and that it improves performance on a multi-label
prediction task
Acknowledgments
We thank Kristina Miler Ke Zhai Leo Claudino and He He for helpful discussions and thank
the anonymous reviewers for insightful comments This research was supported in part by NSF
under grant Resnik and Boyd-Graber and Resnik Any opinions findings
conclusions or recommendations expressed here are those of the authors and do not necessarily
reflect the view of the sponsor

----------------------------------------------------------------

title: 5204-devise-a-deep-visual-semantic-embedding-model.pdf

DeViSE A Deep Visual-Semantic Embedding Model
Andrea Frome Greg S. Corrado Jonathon Shlens Samy Bengio
Jeffrey Dean Marc?Aurelio Ranzato Tomas Mikolov
These authors contributed equally
afrome gcorrado shlens bengio jeff ranzato tmikolov}@google.com
Google Inc.
Mountain View CA USA
Abstract
Modern visual recognition systems are often limited in their ability to scale to
large numbers of object categories This limitation is in part due to the increasing
difficulty of acquiring sufficient training data in the form of labeled images as the
number of object categories grows One remedy is to leverage data from other
sources such as text data both to train visual models and to constrain their predictions In this paper we present a new deep visual-semantic embedding model
trained to identify visual objects using both labeled image data as well as semantic information gleaned from unannotated text We demonstrate that this model
matches state-of-the-art performance on the ImageNet object recognition challenge while making more semantically reasonable errors and also show
that the semantic information can be exploited to make predictions about tens
of thousands of image labels not observed during training Semantic knowledge
improves such zero-shot predictions achieving hit rates of up to across thousands of novel labels never seen by the visual model
Introduction
The visual world is populated with a vast number of objects the most appropriate labeling of which
is often ambiguous task specific or admits multiple equally correct answers Yet state-of-theart vision systems attempt to solve recognition tasks by artificially assigning images to a small
number of rigidly defined classes This has led to building labeled image data sets according to
these artificial categories and in turn to building visual recognition systems based on N-way discrete
classifiers While growing the number of labels and labeled images has improved the utility of
visual recognition systems scaling such systems beyond a limited number of discrete categories
remains an unsolved problem This problem is exacerbated by the fact that N-way discrete classifiers
treat all labels as disconnected and unrelated resulting in visual recognition systems that cannot
transfer semantic information about learned labels to unseen words or phrases One way of dealing
with this issue is to respect the natural continuity of visual space instead of artificially partitioning
it into disjoint categories
We propose an approach that addresses these shortcomings by training a visual recognition model
with both labeled images and a comparatively large and independent dataset semantic information
from unannotated text data This deep visual-semantic embedding model DeViSE leverages textual
data to learn semantic relationships between labels and explicitly maps images into a rich semantic
embedding space We show that this model performs comparably to state-of-the-art visual object
classifiers when trained and evaluated on flat metrics while simultaneously making fewer
semantically unreasonable mistakes along the way Furthermore we show that the model leverages
Current affiliation Facebook Inc.
visual and semantic similarity to correctly predict object category labels for unseen categories
zero-shot classification even when the number of unseen visual categories is for a model
trained on just categories
Previous Work
The current state-of-the-art approach to image classification is a deep convolutional neural network
trained with a softmax output layer multinomial logistic regression that has as many units
as the number of classes see for instance However as the number of classes grows the
distinction between classes blurs and it becomes increasingly difficult to obtain sufficient numbers
of training images for rare concepts
One solution to this problem termed WSABIE is to train a joint embedding model of both images and labels by employing an online learning-to-rank algorithm The proposed model contained
two sets of parameters a linear mapping from image features to the joint embedding space and
an embedding vector for each possible label Compared to the proposed approach WSABIE
only explored linear mappings from image features to the embedding space and the available labels
were only those provided in the image training set It could thus not generalize to new classes
More recently Socher al presented a model for zero-shot learning where a deep neural
network was first trained in an unsupervised manner from many images in order to obtain a rich
image representation in parallel a neural network language model was trained in order to
obtain embedding representations for thousands of common terms The authors trained a linear
mapping between the image representations and the word embeddings representing classes for
which they had labeled images thus linking the image representation space to the embedding space
This last step was performed using a mean-squared error criterion They also trained a simple model
to determine if a given image was from any of the original classes or not an outlier detector
When the model determined an image to be in the set of classes a separately trained softmax
model was used to perform the 8-way classification otherwise the model predicted the nearest class
in the embedding space their setting only outlier classes were considered Their model differs
from our proposed approach in several ways first and foremost the scale as our model considers
known classes for the image model and up to unknown classes instead of respectively
and second in there is an inherent trade-off between the quality of predictions for trained
and outlier classes third by using a different visual model different language model and different
training objective we were able to train a single unified model that uses only embeddings
There has been other recent work showing impressive zero-shot performance on visual recognition
tasks 17 however all of these rely on a curated source of semantic information for the
labels the WordNet hierarchy is used in and and uses a knowledge base containing
descriptive properties for each class By contrast our approach learns its semantic representation
directly from unannotated data
Proposed Approach
Our objective is to leverage semantic knowledge learned in the text domain and transfer it to a model
trained for visual object recognition We begin by pre-training a simple neural language model wellsuited for learning semantically-meaningful dense vector representations of words In parallel
we pre-train a state-of-the-art deep neural network for visual object recognition complete with
a traditional softmax output layer We then construct a deep visual-semantic model by taking the
lower layers of the pre-trained visual object recognition network and re-training them to predict the
vector representation of the image label text as learned by the language model These three training
phases are detailed below
Language Model Pre-training
The skip-gram text modeling architecture introduced by Mikolov al has been shown to
efficiently learn semantically-meaningful floating point representations of terms from unannotated
text The model learns to represent each term as a fixed length embedding vector by predicting
adjacent terms in the document Figure right We call these vector representations embedding
A
Traditional
Visual Model
Deep Visual Semantic
Embedding Model
Skip-gram
Language Model
label
similarity metric
nearby word
softmax layer
transformation
core
visual
model
core
visual
model
embedding
vector
lookup table
image
label
image
parameter
initialization
softmax layer
parameter
initialization
embedding
vector
lookup table
source word
reptiles
birds
insects
food
musical instruments
clothing
dogs
aquatic life
animals
transportation
Figure Left a visual object categorization network with a softmax output layer Right a skip-gram
language model Center our joint model which is initialized with parameters pre-trained at the lower layers
of the other two models t-SNE visualization of a subset of the ILSVRC 1K label embeddings
learned using skip-gram
vectors Because synonyms tend to appear in similar contexts this simple objective function drives
the model to learn similar embedding vectors for semantically related words
We trained a skip-gram text model on a corpus of million documents billion words extracted
from wikipedia.org The text of the web pages was tokenized into a lexicon of roughly
single and multi-word terms consisting of common English words and phrases as well as terms from
commonly used visual object recognition datasets Our skip-gram model used a hierarchical
softmax layer for predicting adjacent terms and was trained using a 20-word window with a single
pass through the corpus For more details and a pointer to open-source code see
We trained skip-gram models of varying hidden dimensions ranging from to and
found and embeddings to be a good compromise between training speed semantic
quality and the ultimate performance of the DeViSE model described below The semantic quality
of the embedding representations learned by these models is impressive.1 A visualization of the language embedding space over a subset of ImageNet labels indicates that the language model learned
a rich semantic structure that could be exploited in vision tasks Figure
Visual Model Pre-training
The visual model architecture we employ is based on the winning model for the ImageNet Large Scale Visual Recognition Challenge ILSVRC The deep neural network
model consists of several convolutional filtering local contrast normalization and max-pooling layers followed by several fully connected neural network layers trained using the dropout regularization technique We trained this model with a softmax output layer as described in to
predict one of object categories from the ILSVRC 1K dataset and were able to reproduce their results This trained model serves both as our benchmark for performance comparisons
as well as the initialization for our joint model
Deep Visual-Semantic Embedding Model
Our deep visual-semantic embedding model DeViSE is initialized from these two pre-trained neural network models Figure The embedding vectors learned by the language model are unit
normed and used to map label terms into target vector representations2
The core visual model with its softmax prediction layer now removed is trained to predict these
vectors for each image by means of a projection layer and a similarity metric The projection layer
is a linear transformation that maps the representation at the top of our core visual model
into the or representation native to our language model
For example the nearest terms to tiger shark using cosine distance are bull shark blacktip shark shark
oceanic whitetip shark sandbar shark dusky shark blue shark requiem shark and great white shark The
nearest terms to car are cars muscle car sports car compact car automobile racing car pickup truck
dealership and sedans
In which introduced the skip-gram model for text cosine similarity between vectors is used for
measuring semantic similarity Unit-norming the vectors and using dot product similarity is an equivalent
similarity measurement
The choice of loss function proved to be important We used a combination of dot-product similarity
and hinge rank loss similar to such that the model was trained to produce a higher dot-product
similarity between the visual model output and the vector representation of the correct label than between the visual output and other randomly chosen text terms We defined the per training example
hinge rank loss
loss(image label
margin tlabel M~v image tj M~v image
j6=label
where image is a column vector denoting the output of the top layer of our core visual network
for the given image is the matrix of trainable parameters in the linear transformation layer
tlabel is a row vector denoting learned embedding vector for the provided text label and tj are
the embeddings of other text terms In practice we found that it was expedient to randomize the
algorithm both by restricting the set of false text terms to possible image labels and truncating
the sum after the first margin-violating false term was encountered The vectors were constrained
to be unit norm and a fixed margin of was used in all experiments3 We also experimented
with an loss between visual and label embeddings as suggested by Socher but that
consistently yielded about half the accuracy of the rank loss model We believe this is because the
nearest neighbor evaluation is fundamentally a ranking problem and is best solved with a ranking
loss whereas the loss only aims to make the vectors close to one another but remains agnostic to
incorrect labels that are closer to the target image
The DeViSE model was trained by asynchronous stochastic gradient descent on a distributed computing platform described in As above the model was presented only with images drawn from
the ILSVRC 1K training set but now trained to predict the term strings as text4 The parameters of the projection layer were first trained while holding both the core visual model and the
text representation fixed In the later stages of training the derivative of the loss function was backpropagated into the core visual model to fine-tune its output5 which typically improved accuracy
by absolute Adagrad per-parameter dynamic learning rates were utilized to keep gradients
well scaled at the different layers of the network
At test time when a new image arrives one first computes its vector representation using the visual
model and the transformation layer then one needs to look for the nearest labels in the embedding
space This last step can be done efficiently using either a tree or a hashing technique in order to
be faster than the naive linear search approach for instance The nearest labels are then
mapped back to ImageNet synsets for scoring Supplementary Materials for details
Results
The goals of this work are to develop a vision model that makes semantically relevant predictions
even when it makes errors and that generalizes to classes outside of its labeled training set zeroshot learning We compare DeViSE to two models that employ the same high-quality core vision
model but lack the semantic structure imparted by our language model a softmax baseline
model a state-of-the-art vision model which employs a softmax classifier a
random embedding model a version of our model that uses random unit-norm embedding vectors
in place of those learned by the language model Both use the trained visual model described in
Section
In order to demonstrate parity with the softmax baseline on the most commonly-reported metric we
compute flat hit@k metrics the percentage of test images for which the model returns the one
true label in its top predictions To measure the semantic quality of predictions beyond the true
label we employ a hierarchical precision@k metric based on the label hierarchy provided with the
The margin was chosen to be a fraction of the norm of the vectors which is A wide range of values
would likely work well
ImageNet image labels are synsets a set of synonymous terms where each term is a word or phrase We
found training the model to predict the first term in each synset to be sufficient but sampling from the synset
terms might work equally well
In principle the gradients can also be back-propagated into the vector representations of the text labels In
this case the language model should continue to train simultaneously in order to maintain the global semantic
structure over all terms in the vocabulary
Model type
Softmax baseline
DeViSE
Random embeddings
Chance
dim
N/A
N/A
Flat hit@k
Hierarchical precision@k
Table Comparison of model performance on our test set taken from the ImageNet ILSVRC 1K
validation set Note that hierarchical precision@1 is equivalent to flat hit@1 See text for details
ImageNet image repository In particular for each true label and value of we generate a ground
truth list from the semantic hierarchy and compute a per-example precision equal to the fraction of
the model?s predictions that overlap with the ground truth list We report mean precision across
the test set Detailed descriptions of the generation of the ground truth lists the hierarchical scoring
metric and train/validation/test dataset splits are provided in the Supplementary Materials
ImageNet ILSVRC 1K Results
This section presents flat and hierarchical results on the ILSVRC 1K dataset where the classes
of the examples presented at test time are the same as those used for training Table shows results
for the DeViSE model for and 1000-dimensional skip-gram models compared to the random
embedding and softmax baseline models on both the flat and hierarchical metrics.6
On the flat metric the softmax baseline shows higher accuracy for At the
DeViSE model has reached parity and at not shown it performs slightly better
We expected the softmax model to be the best performing model on the flat metric given that its
cross-entropy training objective is most well matched to the evaluation metric and are surprised that
the performance of DeViSE is so close to softmax performance
On the hierarchical metric the DeViSE models show better semantic generalization than the softmax baseline especially for larger At the DeViSE model shows a relative
improvement over the softmax baseline and at almost a relative improvement This is a
surprisingly large gain considering that the softmax baseline is a reproduction of the best published
model on these data The gap that exists between the DeViSE model and softmax baseline on the
hierarchical metric reflects the benefit of semantic information above and beyond visual similarity The gap between the DeViSE model and the random embeddings model establishes that the
source of the gain is the well-structured embeddings learned by the language model not some other
property of our architecture
Generalization and Zero-Shot Learning
A distinct advantage of our model is its ability to make reasonable inferences about candidate labels
it has never visually observed For example a DeViSE model trained on images labeled tiger shark
bull shark and blue shark but never with images labeled shark would likely have the ability to
generalize to this more coarse-grained descriptor because the language model has learned a representation of the general concept of shark which is similar to all of the specific sharks Similarly
if tested on images of highly specific classes which the model has never seen before for example
a photo of an oceanic whitecap shark and asked whether the correct label is more likely oceanic
whitecap shark or some other unfamiliar label say nuclear submarine our model stands a fighting chance of guessing correctly because the language model ensures that representation of oceanic
whitecap shark is closer to the representation of sharks the model has seen while the representation
of nuclear submarine is closer to those of other sea vessels
Note that our softmax baseline results differ from the results in due to a simplification in the evaluation
procedure creates several distorted versions of each test image and aggregates the results for a final label
whereas in our experiments we evaluate using only the original test image Our softmax baseline is able to
reproduce the performance of the model in when evaluated with the same procedure
Our model
A
Softmax over ImageNet 1K
eyepiece ocular
Polaroid
compound lens
telephoto lens zoom lens
rangefinder range finder
typewriter keyboard
tape player
reflex camera
CD player
space bar
oboe hautboy hautbois
bassoon
English horn cor anglais
hook and eye
hand
reel
punching bag punch bag
whistle
bassoon
letter opener paper knife
barbet
patas hussar monkey
babbler cackler
titmouse tit
bowerbird catbird
patas hussar monkey
proboscis monkey Nasalis
macaque
titi titi monkey
guenon guenon monkey
Our model
Softmax over ImageNet 1K
fruit
pineapple ananas
pineapple
coral fungus
pineapple plant Ananas artichoke globe artichoke
sweet orange
sea anemone anemone
sweet orange tree
cardoon
comestible edible
dressing salad dressing
Sicilian pizza
vegetable veggie veg
fruit
pot flowerpot
cauliflower
guacamole
cucumber cuke
broccoli
dune buggy beach buggy
searcher beetle
seeker searcher quester
Tragelaphus eurycerus
bongo bongo drum
warplane military plane
missile
projectile missile
sports car sport car
submarine pigboat sub
Figure For each image the top zero-shot predictions of DeViSE+1K from the label set and the
softmax baseline model both trained on ILSVRC Predictions ordered by decreasing score with correct predictions in bold Ground truth telephoto lens zoom lens English horn cor anglais babbler
cackler pineapple pineapple plant Ananas comosus salad bar spacecraft ballistic capsule space
vehicle
Flat hit@k
Data Set
2-hop
3-hop
ImageNet
Model
DeViSE-0
DeViSE+1K
DeViSE-0
DeViSE+1K
DeViSE-0
DeViSE+1K
Candidate
Labels
Table Flat hit@k performance of DeViSE on ImageNet-based zero-shot datasets of increasing difficulty
from top to bottom DeViSE-0 and DeViSE+1K are the same trained model but DeViSE-0 is restricted to only
predict zero-shot classes whereas DeViSE+1K predicts both the zero-shot and the 1K training labels For all
zero-shot classes did not occur in the image training set
To test this hypothesis we extracted images from the ImageNet dataset with labels that
were not included in the ILSVRC 1K dataset on which DeViSE was trained These are zeroshot data sets in the sense that our model has no visual knowledge of these labels though embeddings for the labels were learned by the language model The softmax baseline is only able to predict
labels from ILSVRC The zero-shot experiments were performed with the same trained
DeViSE model used for results in Section but it is evaluated in two ways DeViSE-0
only predicts the zero-shot labels and DeViSE+1K predicts zero-shot labels and the ILSVRC
1K training labels
Figure shows label predictions for a handful of selected examples from this dataset to qualitatively
illustrate model behavior Note that DeViSE successfully predicts a wide range of labels outside
its training set and furthermore the incorrect predictions are generally semantically close to the
desired label Figure and show cases where our model makes significantly better
top-5 predictions than the softmax-based model For example in Figure the DeViSE model
is able to predict a number of lens-related labels even though it was not trained on images in any
of the predicted categories Figure illustrates a case where the top softmax prediction is quite
good but where it is unable to generalize to new labels and its remaining predictions are off the
mark while our model?s predictions are more plausible Figure highlights a case where neither
model gets the exact true label but both models are giving plausible labels Figure shows a
case where the softmax model emits more nearly correct labels than the DeViSE model
To quantify the performance of the model on zero-shot data we constructed from our ImageNet
zero-shot data three test data sets of increasing difficulty based on the image labels
tree distance from the training ILSVRC 1K labels in the ImageNet label hierarchy The
easiest dataset is comprised of the labels that are within two tree hops of the training
labels making them visually and semantically similar to the training set A more difficult
dataset was constructed in the same manner Finally we built a third particularly challenging dataset
consisting of all the labels in ImageNet that are not in ILSVRC
Data Set
2-hop
3-hop
ImageNet
Model
DeViSE-0
DeViSE+1K
Softmax baseline
DeViSE-0
DeViSE+1K
Softmax baseline
DeViSE-0
DeViSE+1K
Softmax baseline
Hierarchical precision@k
Table Hierarchical precision@k results on zero-shot classification Performance of DeViSE compared to
the softmax baseline model across the same datasets as in Table Note that the softmax model can never
directly predict the correct label so its precision@1 is
Model
DeViSE
Mensink
Rohrbach
labels
labels
Table Flat hit@5 accuracy on the zero-shot task from DeViSE experiments were performed with a
model The model uses a curated hierarchy over labels for zero-shot classification but without using
this information our model is close in performance on the zero-shot class label task When the models can
predict any of the labels we achieve better accuracy indicating DeViSE has less of a bias toward training
classes than As in we include a result on a similar task from though their work used a different
set of zero-shot classes
We again calculated the flat hit@k measure to determine how frequently DeViSE-0 and DeViSE+1K
predicted the correct label for each of these data sets Table DeViSE-0?s top prediction was the
correct label of the time across novel labels and the rate increases with to within
the top predictions As the zero-shot data sets become more difficult the accuracy decreases in
absolute terms though it is better relative to chance not shown Since a traditional softmax visual
model can never produce the correct label on zero-shot data its performance would be for all
The DeViSE+1K model performed uniformly worse than the plain DeViSE-0 model by a margin
that indicates it has a bias toward training classes
To provide a stronger baseline for comparison we compared the performance of our model and
the softmax model on the hierarchical metric we employed above Although the softmax baseline
model can never predict exactly the correct label the hierarchical metric will give the model credit
for predicting labels that are in the neighborhood of the correct label in the ImageNet hierarchy
for Visual similarity is strongly correlated with semantic similarity for nearby object
categories and the softmax model does leverage visual similarity between zero-shot and training
images to make predictions that will be scored favorably Figure
The easiest dataset contains object categories that are as visually and semantically similar
to the training set as possible For this dataset the softmax model outperforms the DeViSE model for
hierarchical precision@2 demonstrating just how large a role visual similarity plays in predicting
semantically nearby labels Table However for our model produces superior
predictions relative to the ImageNet hierarchy even on this easiest dataset For the two more difficult datasets where there are more novel categories and the novel categories are less closely related
to those in the training data set DeViSE outperforms the softmax model at all measured hierarchical precisions The quantitative gains can be quite large as much as relative improvement
over softmax performance and qualitatively the softmax model?s predictions can be surprisingly
unreasonable in some cases Figure The random embeddings model we described above
performed substantially worse than either of the real models These results indicate that our architecture succeeds in leveraging the semantic knowledge captured by the language model to make
reasonable predictions even as test images become increasingly dissimilar from those used in the
training set
To provide a comparison with other work in zero-shot learning we also directly compare to the
zero-shot results from These were performed on a particular split of the classes
from ImageNet training and model tuning is performed using the classes and test images
are drawn from the remaining classes Results are shown in Table
Taken together these zero-shot experiments indicate that the DeViSE model can exploit both visual
and semantic information to predict novel classes never before observed Furthermore the presence
of semantic information in the model substantially improves the quality of its predictions
Conclusion
In contrast to previous attempts in this area we have shown that our joint visual-semantic embedding model can be trained to give performance comparable to a state-of-the-art softmax based
model on a flat object classification metric while simultaneously making more semantically reasonable errors as indicated by its improved performance on a hierarchical label metric We have
also shown that this model is able to make correct predictions across thousands of previously unseen
classes by leveraging semantic knowledge elicited only from unannotated text
The advantages of this architecture however extend beyond the experiments presented here
First we believe that our model?s unusual compatibility with larger less manicured data sets will
prove to be a major strength moving forward In particular the skip-gram language model we
constructed included only a modestly sized vocabulary and was exposed only to the text of a single
online encyclopedia we believe that the gains available to models with larger vocabularies and
trained on vastly larger text corpora will be significant and easily outstrip methods which rely on
manually constructed semantic hierarchies Perhaps more importantly though here we
trained on a curated academic image dataset our model?s architecture naturally lends itself to being
trained on all available images that can be annotated with any text term contained in the larger
vocabulary We believe that training massive open image datasets of this form will dramatically
improve the quality of visual object categorization systems
Second we believe that the and nearly balanced visual object classification problem is
soon to be outmoded by practical visual object categorization systems that can handle very large
numbers of labels and the re-definition of valid label sets at test time For example our model
can be trained once on all available data and simultaneously used in one application requiring
only coarse object categorization house car pedestrian and another application requiring
fine categorization in a very specialized subset Honda Civic Ferrari Tesla Model-S
Moreover because test time computation can be sub-linear in the number of labels contained in the
training set our model can be used in exactly such systems with much larger numbers of labels
including overlapping or never-observed categories
Moving forward we are experimenting with techniques which more directly leverage the structure
inherent in the learned language embedding greatly reducing training costs of the joint model and
allowing even greater scaling
Acknowledgments
Special thanks to those who lent their insight and technical support for this work including Matthieu
Devin Alex Krizhevsky Quoc Le Rajat Monga Ilya Sutskever and Wojciech Zaremba

----------------------------------------------------------------

title: 6066-f-gan-training-generative-neural-samplers-using-variational-divergence-minimization.pdf

GAN Training Generative Neural Samplers using
Variational Divergence Minimization
Sebastian Nowozin Botond Cseke Ryota Tomioka
Machine Intelligence and Perception Group
Microsoft Research
Sebastian.Nowozin Botond.Cseke ryoto}@microsoft.com
Abstract
Generative neural samplers are probabilistic models that implement sampling using
feedforward neural networks they take a random input vector and produce a sample
from a probability distribution defined by the network weights These models
are expressive and allow efficient computation of samples and derivatives but
cannot be used for computing likelihoods or for marginalization The generativeadversarial training method allows to train such models through the use of an
auxiliary discriminative neural network We show that the generative-adversarial
approach is a special case of an existing more general variational divergence
estimation approach We show that any divergence can be used for training
generative neural samplers We discuss the benefits of various choices of divergence
functions on training complexity and the quality of the obtained generative models
Introduction
Probabilistic generative models describe a probability distribution over a given domain for example
a distribution over natural language sentences natural images or recorded waveforms
Given a generative model from a class of possible models we are generally interested in
performing one or multiple of the following operations
Sampling Produce a sample from Q. By inspecting samples or calculating a function on
a set of samples we can obtain important insight into the distribution or solve decision
problems
Estimation Given a set of iid samples from an unknown true distribution
find that best describes the true distribution
Point-wise likelihood evaluation Given a sample evaluate the likelihood
Generative-adversarial networks GAN in the form proposed by are an expressive class of
generative models that allow exact sampling and approximate estimation The model used in GAN is
simply a feedforward neural network which receives as input a vector of random numbers sampled
for example from a uniform distribution This random input is passed through each layer in the
network and the final layer produces the desired output for example an image Clearly sampling
from a GAN model is efficient because only one forward pass through the network is needed to
produce one exact sample
Such probabilistic feedforward neural network models were first considered in and here we
call these models generative neural samplers GAN is also of this type as is the decoder model of
a variational autoencoder
In the original GAN paper the authors show that it is possible to estimate neural samplers by
approximate minimization of the symmetric Jensen-Shannon divergence
DJS kQ DKL DKL Qk
where DKL denotes the Kullback-Leibler divergence The key technique used in the GAN training
is that of introducing a second discriminator neural networks which is optimized simultaneously
Conference on Neural Information Processing Systems NIPS Barcelona Spain
Because DJS kQ is a proper divergence measure between distributions this implies that the true
distribution can be approximated well in case there are sufficient training samples and the model
class is rich enough to represent
In this work we show that the principle of GANs is more general and we can extend the variational
divergence estimation framework proposed by Nguyen to recover the GAN training
objective and generalize it to arbitrary divergences
More concretely we make the following contributions over the state-of-the-art
We derive the GAN training objectives for all divergences and provide as example
additional divergence functions including the Kullback-Leibler and Pearson divergences
We simplify the saddle-point optimization procedure of Goodfellow and provide
a theoretical justification
We provide experimental insight into which divergence function is suitable for estimating
generative neural samplers for natural images
Method
We first review the divergence estimation framework of Nguyen which is based on
divergences We then extend this framework from divergence estimation to model estimation
The f-divergence Family
Statistical divergences such as the well-known Kullback-Leibler divergence measure the difference
between two given probability distributions A large class of different divergences are the so called
divergences also known as the Ali-Silvey distances Given two distributions and
that possess respectively an absolutely continuous density function and with respect to a base
measure dx defined on the domain we define the divergence
dx
Df kQ
q(x)f
where the generator function is a convex lower-semicontinuous function satisfying
Different choices of recover popular divergences as special cases in We illustrate
common choices in Table See supplementary material for more divergences and plots
Variational Estimation of divergences
Nguyen derive a general variational method to estimate divergences given only samples
from and Q. An equivalent result has also been derived by Reid and Williamson We will
extend these results from merely estimating a divergence for a fixed model to estimating model
parameters We call this new method variational divergence minimization VDM and show that
generative-adversarial training is a special case of our VDM framework
For completeness we first provide a self-contained derivation of Nguyen al?s divergence estimation
procedure Every convex lower-semicontinuous function has a convex conjugate function also
known as Fenchel conjugate This function is defined as
sup ut
u?domf
The function is again convex and lower-semicontinuous and the pair is dual to another
in the sense that Therefore we can also represent as supt?domf tu
Nguyen leverage the above variational representation of in the definition of the divergence
to obtain a lower bound on the divergence
Df kQ sup
dx
t?domf
supT
dx
dx
sup Ex?P Ex?Q
Jensen-Shannon
Df kQ
log
dx
log
dx
dx
dx
log
log
GAN
Name
Kullback-Leibler
Reverse KL
Pearson
Squared Hellinger
log
log
Generator
log
qq(x
log
log
log
dx
dx
log
log
log log(u
log
Table List of divergences Df kQ together with generator functions Part of the list of divergences and
their generators is based on For all divergences we have domf where is convex and
lower-semicontinuous Also we have which ensures that Df kP for any distribution As
shown by GAN is related to the Jensen-Shannon divergence through DGAN 2DJS
where is an arbitrary class of functions R. The above derivation yields a lower bound
because the class of functions may contain only a subset of all possible functions By taking the
variation of the lower bound in we find that under mild conditions on the bound
is tight for
where denotes the first order derivative of This condition can serve as a guiding principle for
choosing and designing the class of functions For example the popular reverse Kullback-Leibler
divergence corresponds to log(u resulting in see Table
We list common divergences in Table and provide their Fenchel conjugates and the domains domf in Table We provide plots of the generator functions and their conjugates in the
supplementary materials
Variational Divergence Minimization VDM
We now use the variational lower bound on the divergence Df kQ in order to estimate a
generative model given a true distribution
To this end we follow the generative-adversarial approach and use two neural networks and
is our generative model taking as input a random vector and outputting a sample of interest
We parametrize through a vector and write is our variational function taking as input a
sample and returning a scalar We parametrize using a vector and write
We can train a generative model by finding a saddle-point of the following GAN objective
function where we minimize with respect to and maximize with respect to
Ex?P Ex?Q
To optimize on a given finite training data set we approximate the expectations using minibatch
samples To approximate Ex?P we sample instances without replacement from the training set
To approximate Ex?Q we sample instances from the current generative model
Representation for the Variational Function
To apply the variational objective for different divergences we need to respect the domain
domf of the conjugate functions To this end we assume that variational function is
represented in the form gf and rewrite the saddle objective as follows
Ex?P gf Ex?Q gf
where without any range constraints on the output and gf domf is an output
activation function specific to the divergence used In Table we propose suitable output activation
functions for the various conjugate functions and their domains.1 Although the choice of gf is
somewhat arbitrary we choose all of them to be monotone increasing functions so that a large output
Note that for numerical implementation we recommend directly implementing the scalar function gf
robustly instead of evaluating the two functions in sequence see Figure
Name
Output activation gf
domf
Conjugate
Kullback-Leibler
Reverse KL
Pearson
Squared Hellinger
Jensen-Shannon
GAN
exp(?v
exp(?v
log(1
log(1
exp(t
log(?t
4t
log(2 exp(t
log(1 exp(t
Table Recommended final layer activation functions and critical variational function level defined by
The critical value can be interpreted as a classification threshold applied to to distinguish between
true and generated samples
gf
gf
KL
Reverse KL
Pearson
Squared Hellinger
Jensen-Shannon
GAN
KL
Reverse KL
Pearson
Squared Hellinger
Jensen-Shannon
GAN
Figure The two terms in the saddle objective are plotted as a function of the variational function
corresponds to the belief of the variational function that the sample comes from the data
distribution as in the GAN case see Figure It is also instructive to look at the second term
gf in the saddle objective This term is typically except for the Pearson divergence
a decreasing function of the output favoring variational functions that output negative numbers
for samples from the generator
We can see the GAN objective
Ex?P log Ex?Q
as a special instance of by identifying each terms in the expectations of and In particular
choosing the last nonlinearity in the discriminator as the sigmoid
corresponds to output activation function is gf log(1 e?v see Table
Example Univariate Mixture of Gaussians
To demonstrate the properties of the different divergences and to validate the variational divergence
estimation framework we perform an experiment similar to the one of
Setup We approximate a mixture of Gaussians by learning a Gaussian distribution We represent our
model using a linear function which receives a random and outputs
where are the two scalar parameters to be learned For the variational function we use
a neural network with two hidden layers having 64 units each and tanh activations We optimize the
objective by using the single-step gradient method presented in Section In each step we
sample batches of size from and and we use a step-size of for updating
both and We compare the results to the best fit provided by the exact optimization of Df kQ
which is feasible in this case by solving the required integrals in numerically We use
learned and best fit to distinguish the parameters sets used in these two approaches
Results The left side of Table shows the optimal divergence and objective values Df
as well as the corresponding optimal means and standard deviations Note that the
and
There is a good
results are in line with the lower bound property having Df
correspondence between the gap in objectives and the difference between the fitted means and
standard deviations The right side of Table shows the results of the following experiment we
train and using a particular divergence then we estimate the divergence and re-train
while keeping fixed As expected performs best on the divergence it was trained with We
present further details and plots of the fitted Gaussians and variational functions in the supplementary
materials
KL
KL-rev
JS
Jeffrey
Pearson
Df
train test
KL
KL-rev
JS
Jeffrey
Pearson
KL
KL-rev
JS
Jeffrey
Pearson
Table Gaussian approximation of a mixture of Gaussians Left optimal objectives and the learned mean
learned and best fit Right objective values to the true
and the standard deviation
distribution for each trained model For each divergence the lowest objective function value is achieved by the
model that was trained for this divergence
In summary our results demonstrate that when the generative model is misspecified the divergence
function used for estimation has a strong influence on which model is learned
Algorithms for Variational Divergence Minimization VDM
We now discuss numerical methods to find saddle points of the objective To this end we
distinguish two methods first the alternating method originally proposed by Goodfellow
and second a more direct single-step optimization procedure
In our variational framework the alternating gradient method can be described as a double-loop
method the internal loop tightens the lower bound on the divergence whereas the outer loop improves
the generator model While the motivation for this method is plausible in practice a popular choice is
taking a single step in the inner loop requiring two backpropagation passes for one outer iteration
Goodfellow provide a local convergence guarantee
Single-Step Gradient Method
Motivated by the success of the alternating gradient method with a single inner step we propose an
even simpler algorithm shown in Algorithm The algorithm differs from the original one in that there
is no inner loop and the gradients with respect to and are computed in a single back-propagation
Algorithm Single-Step Gradient Method
function INGLE TEP RADIENT I TERATION(P
Sample XP xB and XQ x0B from and Q?t respectively
Update
Update
end function
Analysis Here we show that Algorithm geometrically converges to a saddle point if
there is a neighborhood around the saddle point in which is strongly convex in and strongly
concave in These assumptions are similar to those made in Formally we assume
for in the neighborhood of Note that although there could be many saddle points that
arise from the structure of deep networks they would not qualify as the solution of our variational
framework under these assumptions
For convenience let?s define Now the convergence of Algorithm can be stated as
follows the proof is given in the supplementary material
Theorem Suppose that there is a saddle point with a neighborhood that satisfies
conditions Moreover we define k?F and assume that in the above neighborhood
is sufficiently smooth so that there is a constant such that Lk
for any in the neighborhood of Then using the step-size in Algorithm we have
That is the squared norm of the gradient decreases geometrically
Practical Considerations
Here we discuss principled extensions of the heuristic proposed in and real/fake statistics
discussed by Larsen and S?nderby2 Furthermore we discuss practical advice that slightly deviate
from the principled viewpoint
Goodfellow noticed that training GAN can be significantly sped up by maximizing
Ex?Q log instead of minimizing Ex?Q log for updating the generator In
the more general GAN Algorithm this means that we replace line with the update
Ex?Q?t gf
thereby maximizing the variational function output on the generated samples We can show that this
transformation preserves the stationary point as follows which is a generalization of the argument in
note that the only difference between the original direction line and is the scalar factor
which is the derivative of the conjugate function Since is the inverse of
Cor. Chapter if using we can see that this factor would be the density
ratio which would be one at the stationary point We found this transformation useful
also for other divergences We found Adam and gradient clipping to be useful especially in the
large scale experiment on the LSUN dataset
The original implementation of GANs3 and also Larsen and S?nderby monitor certain real and
fake statistics which are defined as the true positive and true negative rates of the variational function
viewing it as a binary classifier Since our output activation gf are all monotone we can derive similar
statistics for any divergence by only changing the decision threshold Due to the link between the
density ratio and the variational function the threshold lies at Table That is we
can interpret the output of the variational function as classifying the input as a true sample if the
variational function is larger than and classifying it as a generator sample otherwise
Experiments
We now train generative neural samplers based on VDM on the MNIST and LSUN datasets
MNIST Digits We use the MNIST training data set samples pixel images to
train the generator and variational function model proposed in for various divergences With
Uniform100 as input the generator model has two linear layers each followed by batch
normalization and ReLU activation and a final linear layer followed by the sigmoid function The
variational function has three linear layers with exponential linear unit in between The
final activation is specific to each divergence and listed in Table As in we use Adam with a
learning rate of and update weight We use a batchsize of sampled from
the training set without replacement and train each model for one hour We also compare against
variational autoencoders with latent dimensions
Results and Discussion We evaluate the performance using the kernel density estimation Parzen
window approach used in To this end we sample images from the model and estimate
a Parzen window estimator using an isotropic Gaussian kernel bandwidth using three fold cross
validation The final density model is used to evaluate the average log-likelihood on the MNIST test
set samples We show the results in Table and some samples from our models in Figure
The use of the KDE approach to log-likelihood estimation has known deficiencies In particular
for the dimensionality used in MNIST the number of model samples required to obtain
accurate log-likelihood estimates is infeasibly large We found a large variability up to nats
between multiple repetitions As such the results are not entirely conclusive We also trained the
same KDE estimator on the MNIST training set achieving a significantly higher holdout likelihood
However it is reassuring to see that the model trained for the Kullback-Leibler divergence indeed
achieves a high holdout likelihood compared to the GAN model
http://torch.ch/blog/2015/11/13/gan.html
Available at https://github.com/goodfeli/adversarial
Training divergence
Kullback-Leibler
Reverse Kullback-Leibler
Pearson
Neyman
Squared Hellinger
Jeffrey
Jensen-Shannon
GAN
KDE hLLi nats
SEM
Variational Autoencoder
KDE MNIST train
Table Kernel Density Estimation evaluation on the MNIST test data set Each
KDE model is build from samples from the learned generative model
We report the mean log-likelihood on the MNIST test set and the
standard error of the mean The KDE MNIST result is using MNIST
training images to fit a single KDE model
Figure MNIST model
samples trained using KL
reverse KL Hellinger
Jensen from top to bottom
LSUN Natural Images Through the DCGAN work the generative-adversarial approach has
shown real promise in generating natural looking images Here we use the same architecture as as
in and replace the GAN objective with our more general GAN objective
We use the large scale LSUN database of natural images of different categories To illustrate
the different behaviors of different divergences we train the same model on the classroom category
of images containing images of classroom environments rescaled and center-cropped to
pixels
Setup We use the generator architecture and training settings proposed in DCGAN The model
receives Uniformdrand and feeds it through one linear layer and three deconvolution
layers with batch normalization and ReLU activation in between The variational function is the same
as the discriminator architecture in and follows the structure of a convolutional neural network
with batch normalization exponential linear units and one final linear layer
Results Figure shows random samples from neural samplers trained using GAN KL and
squared Hellinger divergences All three divergences produce equally realistic samples Note that the
difference in the learned distribution arise only when the generator model is not rich enough
GAN
KL
Squared Hellinger
Figure Samples from three different divergences
Related Work
We now discuss how our approach relates to existing work Building generative models of real world
distributions is a fundamental goal of machine learning and much related work exists We only
discuss work that applies to neural network models
Mixture density networks are neural networks which directly regress the parameters of a finite
parametric mixture model When combined with a recurrent neural network this yields impressive
generative models of handwritten text
NADE and RNADE perform a factorization of the output using a predefined and somewhat
arbitrary ordering of output dimensions The resulting model samples one variable at a time conditioning on the entire history of past variables These models provide tractable likelihood evaluations
and compelling results but it is unclear how to select the factorization order in many applications
Diffusion probabilistic models define a target distribution as a result of a learned diffusion
process which starts at a trivial known distribution The learned model provides exact samples and
approximate log-likelihood evaluations
Noise contrastive estimation NCE is a method that estimates the parameters of unnormalized
probabilistic models by performing non-linear logistic regression to discriminate the data from
artificially generated noise NCE can be viewed as a special case of GAN where the discriminator
is constrained to a specific form that depends on the model logistic regression classifier and the
generator kept fixed is providing the artificially generated noise supplementary material
The generative neural sampler models of and did not provide satisfactory learning methods
used importance sampling and expectation maximization The main difference to GAN and
to our work really is in the learning objective which is effective and computationally inexpensive
Variational auto-encoders VAE are pairs of probabilistic encoder and decoder models
which map a sample to a latent representation and back trained using a variational Bayesian learning
objective The advantage of VAEs is in the encoder model which allows efficient inference from
observation to latent representation and overall they are a compelling alternative to GANs and
recent work has studied combinations of the two approaches
As an alternative to the GAN training objective the work and independently considered the
use of the kernel maximum mean discrepancy MMD as a training objective for probabilistic
models This objective is simpler to train compared to GAN models because there is no explicitly
represented variational function However it requires the choice of a kernel function and the reported
results so far seem slightly inferior compared to GAN. MMD is a particular instance of a larger class of
probability metrics which all take the form supT Ex?P Ex?Q
where the function class is chosen in a manner specific to the divergence Beyond MMD other
popular metrics of this form are the total variation metric also an divergence the Wasserstein
distance and the Kolmogorov distance
A previous attempt to enable minimization of the KL-divergence in deep generative models is due to
Goodfellow where an approximation to the gradient of the KL divergence is derived
In another generalization of the GAN objective is proposed by using an alternative JensenShannon divergence that interpolates between the KL and the reverse KL divergence and has JensenShannon as its mid-point We discuss this work in more detail in the supplementary materials
Discussion
Generative neural samplers offer a powerful way to represent complex distributions without limiting
factorizing assumptions However while the purely generative neural samplers as used in this paper
are interesting their use is limited because after training they cannot be conditioned on observed data
and thus are unable to provide inferences
We believe that in the future the true benefits of neural samplers for representing uncertainty will be
found in discriminative models and our presented methods extend readily to this case by providing
additional inputs to both the generator and variational function as in the conditional GAN model
We hope that the practical difficulties of training with saddle point objectives are not an underlying
feature of the model but instead can be overcome with novel optimization algorithms Further
investigations such as are needed to investigate and hopefully overcome these difficulties
Acknowledgements We thank Ferenc Husz?ar for discussions on the generative-adversarial approach

----------------------------------------------------------------

