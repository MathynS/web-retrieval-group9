query sentence: deployment of fault tolerant clouds
<<------------------------------------------------------------------------------------------------------------------------------------------->>
title: 2508-parameterized-novelty-detectors-for-environmental-sensor-monitoring.pdf

Parameterized Novelty Detection for
Environmental Sensor Monitoring
Cynthia Archer Todd K. Leen Antonio Baptista
OGI School of Science Engineering
Oregon Health Science University
N. W. Walker Road
Beaverton OR
archer@cse.ogi.edu tleen@cse.ogi.edu baptista@ccalmr.ogi.edu
Abstract
As part of an environmental observation and forecasting system
sensors deployed in the Columbia RIver Estuary CORIE gather
information on physical dynamics and changes in estuary habitat Of these salinity sensors are particularly susceptible to biofouling which gradually degrades sensor response and corrupts critical data Automatic fault detectors have the capability to identify
bio-fouling early and minimize data loss Complicating the development of discriminatory classifiers is the scarcity of bio-fouling
onset examples and the variability of the bio-fouling signature To
solve these problems we take a novelty detection approach that
incorporates a parameterized bio-fouling model These detectors
identify the occurrence of bio-fouling and its onset time as reliably
as human experts Real-time detectors installed during the summer of produced no false alarms yet detected all episodes of
sensor degradation before the field staff scheduled these sensors for
cleaning From this initial deployment through February our
bio-fouling detectors have essentially doubled the amount of useful
data coming from the CORIE sensors
Introduction
Environmental observation and forecasting systems EOFS gather process and
deliver environmental information to facilitate sustainable development of natural resources Our work is part of a pilot EOFS system being developed for the
Columbia River Estuary CORIE This system uses data from sensors deployed throughout the estuary Figure to calibrate and verify numerical models
of circulation and material transport CORIE scientists use these models to predict
and evaluate the effects of development on the estuary environment
CORIE salinity sensors deployed in the estuary lose several months of data every
year due to sensor degradation Corrupted and missing field measurements compromise model calibration and verification which can lead to invalid environmental
forecasts The most common form of salinity sensor degradation is bio-fouling a
reduction of the sensor response due to growth of biological material on the sensor
Prior the deployment of the technology described here on a yearly basis CORIE
salinity sensors suffered a data loss due to bio-fouling Although bio-fouling
degradation is a common problem for environmental sensors there is apparently no
previous work that develops automatic detectors of such degradation
Figure Map of Columbia River estuary marked with locations of CORIE sensors
Early bio-fouling detection is made difficult by the normal variability of salinity
measurements Tides cause the measurements to vary from near river salinity to
near ocean salinity twice a day The temporal pattern of salinity penetration varies
spatially in the estuary In addition upriver sites such as show substantial
variability with the and 28 day spring-neap tidal cycle Changes in weather
winds precipitation and ocean conditions cause additional variations in salinity
To complicate bio-fouling detection further the bio-fouling signature also varies
from episode to episode The time from onset to complete bio-fouling can take
anywhere from weeks to months depending on the season and type of growth
We observe two types of bio-fouling in the estuary hard growth barnacles
characterized by quick linear degradation and soft growth plant material
characterized by slow linear degradation with occasional interruptions in the downtrend
Figure illustrates tidal variations in salinity and the effect that bio-fouling has on
these measurements It contains salinity time series in practical salinity units psu
from two sensors mounted at the Red26 station Figure The upper trace from
sensor contains only clean measurements The lower trace from sensor
contains both clean and bio-fouled measurements The first half of the two
time series are similar but beginning on September the salinity measurements
diverge The sensor exhibits typical hard-growth bio-fouling degradation
The primary challenge to our work is to detect the degradation quickly ideally
within several diurnal cycles Early detection will limit the use of corrupted data in
on-line applications and provide a basis to rapidly replace degrading sensors and
thus drastically reduce data loss
Although the CORIE data archives contain many months of bio-fouled data there
are relatively few examples of the onset of degradation for most of the sensors
salinity
salinity
Date month/day
Figure Clean and bio-fouled salinity time series examples from Red26 station The
upper time series is from clean instrument The lower time series from instrument
shows degradation beginning on September 28 On removal was
found to be bio-fouled
deployed in the estuary and it is this onset that we must detect The dearth of
onset examples and the observed variability of the bio-fouling signature spatially
seasonally and weekly according to the spring/neap tidal cycle prevents use of
classical discriminatory fault detectors Instead we develop a parameterized novelty
detector to detect bio-fouling This detector incorporates a parameterized model
of bio-fouling behavior The parameters in the model of bio-fouled sensor behavior
are fit on-line by maximum-likelihood estimation A model of the clean sensor
behavior is fit to archival data These models are used in a sequential likelihood
test to provide detection of bio-fouling and an estimation of the time at which the
degradation began
Evaluations show that our detectors identify the onset of bio-fouling as reliably as
human experts and frequently within fewer tidal cycles of the onset Our deployment of sensors throughout the estuary has resulted in an actual reduction of the
error loss from to However this figure does not adequately reflect the
efficacy of the detectors Were it economical to replace sensors immediately upon
detection of degradation the data loss would have been reduced to
Salinity and Temperature
Our detectors monitor maximum diurnal salinity defined as the maximum
salinity near one of the two diurnal tidal floods When the sensor is clean the md
salinity stays close to some mean value with occasional dips of several psu caused
by variations in the intrusion of salt water into the estuary When the sensor biofouls the md salinity gradually decreases to typically less than half its normal mean
value as seen in the Figure example
Detectors that monitor salinity alone can not distinguish between normal decreases
in salinity and early bio-fouling This results in a high false alarm rate Natural
salinity decreases can be recognized by monitoring a correlated source of information
that is not corrupted by bio-fouling
Salinity and temperature at a station are products of the same mixing process of
ocean and river waters so we expect these values will be correlated Assuming
linear mixing of ocean and river waters measured salinity Sm and temperature Tm
are linear functions of ocean So To and river Sr Tr values
Sm
Tm
where is the mixing coefficient at time River salinity Sr is close to zero
Consequently the estimated mixing coefficient
Tr
Tr
should be well correlated with salinity Sm So The river temperature is measured at far upstream stations Elliot or Woody The ocean temperature is estimated from measurements at Sand Island the outermost sensor station
Bio-fouling Detection
Our early experiments with single-measurement detection suggested that we develop
detectors that accrue information over time similar to the standard sequential
likelihood methods in classical pattern recognition The is a natural framework for
detecting degradation that grows with time
Assume a sequence of measurements salinity and temperature yn
where is the current time We construct probability densities for such sequences
for both clean sensors yN and for biofouled sensors yN
With these distributions we construct a likelihood ratio test
yN
yN
where the threshold is chosen high enough to provide a specified false alarm rate
Neyman-Pearson test
We assume that the probability density for the measurement sequence for fouled
detectors is parameterized by a vector of unknown parameters The model is
constructed such that at the density for the sequence assuming a fouled
detector is equal to the density of the sequence assuming a clean detector
yN yN
Next we suppose that a given sequence contains a bio-fouling event that is initiated
at the unknown time Under our density models below consecutive measurements in the sequence are independent conditioned on the state of the detector
Equivalently if the alarm threshold is increased to maintain a low false alarm rate
the rate of proper detections is decreased
Consequently the likelihood ratio for the sequence reduces to
yN
yN
yN
yN
p(yn
p(yn
Finally we fit the fouling model parameters and the onset time by maximizing
the log-likelihood yN with respect to and Since the clean
detector model is independent of and this is equivalent to maximizing the
log-likelihood ratio in Hence we replace the latter with
max
p(yn
p(yn
If the sequence is coming from a clean sensor the fit should give and hence
cf and we will detect no event assuming This construction is a
variant of the type of signal change detection discussed by Basseville
Bio-fouling Fault Model
By parameterizing the bio-fouling model we are able to develop detectors using
only clean example data In this parameterized novelty detector the bio-fouled
parameters are fit on-line to the data under test To develop our classifier we
first define models of the clean and bio-fouled data We model the true salinity
and temperature-based mixing coefficient as jointly Gaussian
where
and
This provides a regression of the salinity on The probability of md salinity measurement conditioned on temperature when the sensor is clean is Gaussian
with conditional mean
and conditional variance
When bio-fouling occurs the salinity measurement is suppressed relative to the
true value We model this suppression as a linear downtrend with unknown rate
slope that begins at unknown time The model of the measured md salinity
value for a fouled detector is
g(n)sn
where the suppression factor is
m(n
and is the bio-fouling rate Using this suppression factor the
probability of the salinity measurement conditioned on temperature is
p(xn
Note that since the temperature sensor is not susceptible to bio-fouling we need
not consider the case of both sensors degrading at the same time
The discriminant function in depends on the parameters of the clean model
and which are estimated from historical data It also depends on the slope
parameter of the fouling model and the onset time which are fit online as
per
Applying our Gaussian models in and to gives us
m(n
max
m(n
m(n
When is above our chosen threshold the detector signals a biofouled sensor The
threshold is set to provide a maximum false alarm rate on historical data
Model Fitting
We find maximum likelihood estimates for and from clean archival timeP
series
data For yn sn and training values the mean is given by N1 yn
and the covariance matrix by N1 yn All other classifier
parameter values such as or can be extracted or calculated from and
At each time step we determine the maximum likelihood estimate of onset time
and bio-fouling rate from the data under test We find the maximum likelihood
estimate of bio-fouling rate for some onset time by setting the first derivative
of with respect to equal to zero This operation yields the relation
xk
xk
where m(k and is the current time Note that appears both at
the beginning of and in the definition of so we do not have a closed form
solution for However the values act as weights that increase the importance
of most recent measurements This weighting accounts for the expected decrease in
measurement variance as bio-fouling progresses To estimate we take an iterative
approach First initialize to its minimum mean-squared error value given by
PN
PN
Second repeatedly solve for with calculated using the previous value
The estimated rate value stops changing when reaches a maximum
If we set the window length to maximize the log likelihood ratio the best
estimate of onset time is To determine the onset time estimate we search
over over all past time for the value of that maximizes For each possible
window length that is we determine the maximum likelihood estimate
for and then calculate the corresponding discriminant The estimated onset
time is the window length that gives the largest value of If this is
above our threshold the current measurement is classified as bio-fouled
On-line Bio-fouling Detectors
To see how well our classifiers worked in practice we implemented versions that operated on real-time salinity and temperature measurements For all four instances
35
35
Max Sal
Max Sal
of sensor degradation three bio-fouling incidents and one instrument failure that
mimicked bio-fouling that occurred in the summer test period our classifiers
correctly indicated a sensor problem before the field staff was aware of it In addition the real-time classifiers produced no false alarms during the summer test
period More in-depth discussion of the detector suite is given by Archer al in
Date
SLR
SLR
Date
Red26
Tansy Point
Figure Bio-fouling Indicators Red26 and Tansy Point Top plots show maximum
diurnal salinity Dotted lines indicate historical no false alarm lower and false
alarm rate upper Field staff schedule sensors for cleaning when the maximum salinity
drops too low roughly the no false alarm level Bottom plots show the sequential
likelihood discriminant for forty days of salinity and temperature measurements Dotted
lines indicate historical no false alarm upper and false alarm rate lower The
indicates the estimated bio-fouling onset time
The on-line monitor displays a bio-fouling indicator for the previous forty days of
data Figure shows the on-line bio-fouling monitor during incidents at the Red26
sensor and the Tansy Point sensor Since we had another sensor
mounted at the Red26 site that did not bio-foul Figure we were able to estimate
the bio-fouling time as September Our detector discriminant passed the no
false alarm threshold five days after onset and roughly three days before the field
staff decided the instrument needed cleaning This reduction in time to detection
corresponds to reduced data loss of over In addition the onset time estimate
of September was within a day of the true onset time
The Tansy Point sensor began to bio-foul a few days after the Red26
sensor Our detector indicated that the Tansy Point sensor was bio-fouling
on October 9th Since neighboring sensor Red26 was being replaced on October
the field staff decided to retrieve the Tansy Point sensor as well On removal
this sensor was found to be in the early stages of bio-fouling In this case indications
from our classifier permitted the sensor to be replaced before the field staff would
normally have scheduled it for retrieval Experience with our on-line bio-fouling
indicators demonstrates that these methods substantially reduce time from biofouling onset to detection
In addition to the events described above we have fairly extensive experience with
the online detectors since their initial deployment in the Spring of At this
writing we have bio-fouling detectors at all observing stations in the estuary and
experience with events throughout the year Near the end of October we
experienced a false alarm in a sensor near the surface in the lower estuary In
this case a steady downward trend in surface salinity caused by several days of
rain triggered a detector response Following cessation of the precipitation the
discriminant function returned back to sub-threshold levels
In a recent February study of five sensor stations in the estuary we compared
data loss prior to the deployment of bio-fouling detectors with data loss postdeployment The pre-deployment period included approximately four years of data
from through the summer of The post-deployment period ran from
spring/summer of through February
Neglecting seasonal variation prior to the deployment of our detectors of all
the sensor data was corrupted by bio-fouling Following deployment the rate of
data loss due to bio-fouling dropped to This is the actual data loss and
includes delay in responding to the event detection Were it economical to replace
the sensors immediately upon detection of bio-fouling the data loss rate would have
been dropped farther to Even with the delay in responding to event detection
the detectors have more than doubled the amount of reliable data collected from the
estuary
Discussion
CORIE salinity sensors lose several months of data every year due to sensor biofouling Developing discriminatory fault detectors for these sensors is hampered by
the variability of the bio-fouling time-signature and the dearth of bio-fouling onset
example data for training To solve this problem we built parameterized novelty
detectors Clean sensor models were developed based on archive data while biofouled sensor models are given a simple parametric form that is fit online On-line
bio-fouling detectors deployed during the summer of detected all episodes
of sensor degradation several days before the field staff without generating any
false alarms Expanded installation of a suite of detectors throughout the estuary
continue to successfully detect bio-fouling with minimal false alarm intrusion The
detector deployment has effectively doubled the amount of clean data available from
the estuary salinity sensors
Acknowledgements
We thank members of the CORIE team Arun Chawla and Charles Seaton for
their help in acquiring appropriate sensor data Michael Wilkin for his assistance
in labeling the sensor data and Haiming Zheng for carrying forward the sensor
development and deployment and providing the comparison of data loss rates before
and after the detector deployment This work was supported by the National
Science Foundation under grants and

<<----------------------------------------------------------------------------------------------------------------------->>

title: 228-synergy-of-clustering-multiple-back-propagation-networks.pdf

Lincoln and Skrzypek
Synergy Of Clustering Multiple Back Propagation Networks
William P. Lincoln and Josef Skrzypekt
UCLA Machine Perception Laboratory
Computer Science Department
Los Angeles CA
ABSTRACT
The properties of a cluster of multiple back-propagation networks
are examined and compared to the performance of a single BP network The underlying idea is that a synergistic effect within the cluster
improves the perfonnance and fault tolerance Five networks were initially trained to perfonn the same input-output mapping Following
training a cluster was created by computing an average of the outputs
generated by the individual networks The output of the cluster can be
used as the desired output during training by feeding it back to the individual networks In comparison to a single BP network a cluster of
multiple BP's generalization and significant fault tolerance It appear
that cluster advantage follows from simple maxim you can fool some
of the single BP's in a cluster all of the time but you cannot fool all of
them all of the time Lincoln
INTRODUCTION
Shortcomings of back-propagation in supervised learning has been well documented in the past Soulie Bernasconi Often a network of a finite size
does not learn a particular mapping completely or it generalizes poorly Increasing the
size and number of hidden layers most often does not lead to any improvements Soulie
also with Hughes Aircraft Company
to whom the correspondence should be addressed
Synergy of Clustering Multiple Back Propagation Networks
The central question that this paper addresses is whether a synergy of clustering
multiple back-prop nets improves the properties of the clustered system over a comparably complex non-clustered system We use the formulation of back-prop given in
Rumelhart A cluster is shown in figure We start with five three-layered
back propagation networks that learn to perform the same input-output mapping Initially the nets are given different starting weights Thus after learning the individual nets
are expected to have different internal representations An input to the cluster is routed to
each of the nets Each net computes its output and the judge uses these outputs Yk to
form the cluster output There are many ways of forming but for the sake of simplicity in this paper we consider the following two rules
simple average:y
Yk
K=l
convex combination:y
WkYk
K=l
Cluster function adds an extra level of fault tolerance by giving the judge the ability
to bias the outputs based on the past reliability of the nets The Wk are adjusted to take
into account the recent reliability of the net One weight adjustment rule is
where
ek is the gain of adjustment and
ek
k=l
ek I IY Yk I I is the network deviation from the cluster output Also in the absence
Wk
of an initial training period with a perfect teacher the cluster can collectively selforganize The cluster in this case is performing an averaging of the mappings that the
individual networks perform based on their initial distribution of weights Simulations
have been done to verify that self organization does in fact occur In all the simulations
convergence occurred before passes
Besides improved learning and generalization our clustered network displays other desirable characteristics such as fault tolerance and self-organization Feeding back the
cluster's output to the individual networks as the desired output in training endows the
cluster with fault tolerance in the absence of a teacher Feeding back also makes the cluster continuously adaptable to changing conditions This aspects of clustering is similar to
the tracking capabilities of adaptive equalizers After the initial training period it is usually assumed that no teacher is present or that a teacher is present only at relatively infrequent intervals However if the failure rate is large enough the perfonnance of a single
non-clustered net will degrade during the periods when no teacher is present
CLUSTERING WITH FEEDBACK TO INCREASE FAULT
TOLERANCE IN THE ABSENCE OF A PERFECT TEACHER
When a teacher is not present can be used as the desired output and used to continuously train the individual nets In general the correc error that should backpropagated dk Y-Yk will differ from the actual error dk Yk If dk and dk differ
significantly the error of the individual nets and thus the cluster as a whole can increase
Lincoln and Skrzypek
over time This phenomenon is called drift Because of drift retraining using as the
desired output may seem disadvantageous when no faults exist within the nets The possibility of drift is decreased by training the nets to a sufficiently small error In fact under
these circumstance with sufficiently small error it is possible to see the error to decrease
even further
It is when we assume that faults exist that retraining becomes more advantageous If the
failure rate of a network node is sufficiently low the injured net can be retrained using
the judge's output By having many nets in the cluster the effect of the injured net's output on the cluster output can be minimized Retraining using adds fault tolerance but
causes drift if the nets did not complete learning when the teacher was removed
cluster
Figure A cluster of back-prop nets
EXPERIMENT AL METHODS
To test the ideas outlined in this paper an abstract learning problem was chosen This
abstract problem was used because many neural network problems require similar
separation and classification of a group of topologically equivalent sets in the process of
learning Lippman For instance images categorized according to their characteristics The input is a 3-dimensional point The problem is to categorize
the point into one of eight sets The sets are the spheres of radius centered at
The input layer consists of three continuous nodes The size of
the output layer was with each node trained to be an indicator function for its associated sphere One hidden layer was used with full connectivity between layers Five nets
with the above specifications were used to form a cluster Generalization was tested using
points outside the spheres
Synergy of Clustering Multiple Back Propagation Networks
CLUSTER ADVANTAGE
The performance of a single net is compared to performance of a five net cluster when
the nets are not retrained using The networks in the cluster have the same structure and
size as the single network Average errors of the two systems are compared A useful
measure of the cluster advantage is obtained by taking the ratio of an individual net's
error to the cluster error This ratio will be smaller or larger than depending on the relative magnitudes of the cluster and individual net's errors Figures 2a and 2b show the
cluster advantage plotted versus individual net error for and training passes
respectively It is seen that when the individual nets either learn the task completely or
don't learn at all there is not a cluster advantage However when the task is learned even
marginally there is a cluster advantage
at
I
Pass
at
I
I
A Pass
Error
Error
Figure Cluster Advantage versus Error
Data points from more than one learning task are shown
A After training passes Mter training passes
The cluster's increased learning is based on the synergy between the individual networks
and not on larger size of a cluster compared to an individual network An individual net's
error is dependent on the size of the hidden layer and the length of the training period
However in general the error is not a decreasing function of the size of the hidden layer
throughout its domain increasing the size of the hidden layer does not always result
in a decrease in the error This may be due to the more direct credit assignment with the
smaller number of nodes Figures 4a and 4b show an individual net's error versus hidden
layer size for different training passes The point to this pedagogics is to counter the anticipated argument a cluster should have a lower error based on the fact that it has more
nodes
Lincoln and Skrzypek
A Pass
Pass
Number of Hidden Unit
Number of Hidden Unit
Figure Error of a single BP network is a nonlinear funtion
of the number of hidden nodes
A After training passes After training passes
FAULT TOLERANCE
the judge's output as the desired output and retraining the individual networks fault
tolerance is added The fault tolerant capabilities of a cluster of were studied The size
of the hidden layer is After the nets were trained a failure rate of link in the cluster
per inputs was introduced This failure rate in terms of a single unclustered net is
link per inputs The link that is chosen to fail in the cluster was randomly
selected from the links of all the networks in the cluster When a link failed its weight
was set to The links from the nets to the judge are considered immune from faults in
this comparison A pass consisted of presentation of a random point from each of the
spheres Figure shows the fault tolerant capabilities of a cluster By knowing the
behavior of the single net in the presence of faults the fault tolerant behavior of any conventional configuration comparison and spares of single nets can be determined so
that this form of fault tolerance can be compared with conventional fault tolerant
schemes
Synergy of Clustering Multiple Back Propagation Networks
Numb.r of training
Figure Fault tolerance of a cluster using feedback
from the judge as a desired training output
Error as a function of time of training passes without link
failures solid circles and with link failures open cirles
Link failure rate cluster link per inputs
or single net link per inputs
CONCLUSIONS
Clustering multiple back-prop nets has been shown to increase the performance and fault
tolerance over a single network Clustering has exhibited very interesting self organization Preliminary investigations are restricted to a few simple examples Nevertheless
there are some interesting results that appear to be rather general and which can thus be
expected to remain valid for much larger and complex systems The clustering ideas
presented in this paper are not specific to back-prop but can apply to any nets trained
with a supervised learning rule The results of this paper can be viewed in an enlightening way Given a set of weights the cluster performs a mapping There is empirical evidence of local minimum in this mapping space The initial point in the mapping space
is taken to be when the cluster output begins to be fed back Each time a new cluster output is fed back the point in the mapping space moves The step size is related to the step
size of the back prop algorithm Each task is conjectured to have a local minimum in the
mapping space If the point moves away from the desired local minimum drift occurs A
fault moves the point away from the local minimum Feedback moves the point closer to
the local minimum Self organization can be viewed as finding the local minimum of the
valley that the point is initially placed based on the initial distribution of weights
Lincoln and Skrzypek
Numb.r of trllnlng
Figure Cluster can continue to learn in the absence of a
teacher if the feedback from the judge is used as a
desired training output No link failures
INTERPRETAnON OF RESULTS
The results of the previous section can be interpreted from the viewpoint of the model
described in this section This model attempts to describe how the state of the nets change
due to possibly incorrect error terms being back-propagated and how in turn the state of
the net determines its performance The state of a net could be defined by its weight
string Given its weight string there is a duality between the mapping that the net is performing and its error When a net is being trained towards a particular mapping its
current weight string determines the error of the net The back-propagation algorithm is
used to change the weight string so that the error decreases The duality is that at any
time a net is performing some mapping it may not be the desired mapping it is perfonning that mapping with no error This duality has significance in connection with selforganization which can be viewed as taking an average of the mappings
Synergy of Clustering Multiple Back Propagation Networks
While the state of a net could be defined by its weight string a state transition due to a
backward error propagation is not obvious A more useful definition of the state of a net
is its error The error can be estimated by taking a representative sample of input vectors
and propagating them through the net and computing the average error of the outputs
Having defined the state a description of the state transition rules can now be given
output of net state of net input
state of net state of net output of net output of net(N
delta error error at error at
cluster mistake I correct output cluster output I
This model says that for positive constants A and
delta error A cluster mistake
This equation has the property that the error increase or decrease is proportional to the
size of the cluster mistake The equilibrium is when the mistake equals B. An assumption
is made that an individual net's mistake is a guassian random variable Zj with mean and
variance equal to its error For the purposes of this analysis the judge uses a convex
combination of the net outputs to form the cluster output Using the assumptions of this
I1VJdel it can be shown that a strategy of increasing the relative weight in the convex
combination of a net that has a relatively small error and conversely decreasing the relative weight for poorly performing nets is an example weight adjustment rule This
rule has the effect of increasing the weight of a network that produced a network deviation that was smaller than average The opposite effect is seen for a network that produced a network deviation that was larger than average

<<----------------------------------------------------------------------------------------------------------------------->>

title: 344-neural-network-application-to-diagnostics-and-control-of-vehicle-control-systems.pdf

Neural Network Application to Diagnostics and
Control of Vehicle Control Systems
Kenneth A. Marko
Research Staff
Ford Motor Company
Dearborn Michigan
ABSTRACT
Diagnosis of faults in complex real-time control systems is a
complicated task that has resisted solution by traditional methods We
have shown that neural networks can be successfully employed to
diagnose faults in digitally controlled powertrain systems This paper
discusses the means we use to develop the appropriate databases for
training and testing in order to select the optimum network architectures
and to provide reasonable estimates of the classification accuracy of
these networks on new samples of data Recent work applying neural
nets to adaptive control of an active suspension system is presented
INTRODUCTION
This paper reports on work performed on the application of artificial neural systems
ANS techniques to the diagnosis and control of vehicle systems Specifically we have
examined the diagnosis of common faults in powertrain systems and investigated the
problem of developing an adaptive controller for an active suspension system
In our diagnostic investigations we utilize neural networks routinely to establish the
standards for diagnostic accuracy we can expect from analysis of vehicle data Previously
we have examined the use of various ANS paradigms to diagnosis of a wide range of
faults in a carefully collected data set from a vehicle operated in a narrow range of speed
and load Subsequently we have explored the classification of a data set with a more
restricted set of faults drawn from a much broader range of operating conditions This
step was taken as concern about needs for specific real-time continuous diagnostics
superseded the need to develop well-controlled on-demand diagnostic testing The
Marko
impetus arises from recently enacted legislation which dictates that such real-time
diagnosis of powertrain systems be req uired on cars sold in the U.S. by the
The difference between the two applications is simple in the former studies
it was presumed that an independent agent has identified that a fault is present the root
cause needs only to be identified In the real-time problem the diagnostic task is to detect
and identify the fault as soon as it occurs Consequently the real-time application is
more demanding In analyzing this more difficult task we explore some of the
complications that arise in developing successful classification schemes for the virtually
semi-infinite data streams that are prcxJuced in continuous operation of a vehicle fleet
The obstacles to realized applications of neural nets in this area often stem from the
sophistication required of the classifier and the complexity of the problems addressed The
limited computational resources on-board vehicles will determine the scope of the
diagnostic task and how implementations such as ANS methods will operate
Finally we briefly examine an extension of the ANS work to developing trainable
controllers for non-linear dynamic systems such as active suspension systems
Preliminary work in this area indicates that effecti ve controllers for non-linear plants can
be developed effiCiently despite the exclusion of an accurate plant model from the training
process Although our studies were carried out in simulation and accurate plant models
were therefore available the capability to develop controllers in the absence of such
models is a significant step forward Such controllers can be developed for existing unmodeled hardware and thereby reduce both the efforts required to develop control
algorithms by conventional means and the time to program the real-time controllers
NEURAL NET
DIAG~OSTICS
OF CONTROL SYSTEMS
Our interest in neural networks for diagnosis of faults in control systems stemmed from
work on model-based diagnosis of faults in such systems typically called plants In the
model-based approach a model of the system under control is developed and used to
predict the dynamic behavior of the system With the system in operation the plant
performance is observed The expected behavior and the observed behavior are compared
and if no differences are found the plant is deemed to be operating normally If deviations
are found the differences indicate that a fault of some sort is present failure detection
and an analysis of the differences is used in an attempt to identify the cause fault
identification Successful implememations Liubakka al Rizzoni
aI of fault detection and identification in complex systems linearized about
selected operating points were put together utilizing mathematical constructs called failure
detection filters These filters are simply matrices which transform a set of observations
which become an input vector to the filter of a plant into another vector space the
output vector or classification space The form of these filters suggested to us that
neural networks could be used to learn similar transforms and thereby avoid the tedious
process of model development and validation and a priori identification of the detection
filter matrix elements We showed previously that complex signal patterns from
operating internal combustion engines could be examined on a cycle by cycle basis two
revolutions of the common four-stroke engine cycle and used to correctly identify faults
present in the engine Marko el
Typical data collected from an operating engine has been shown elsewhere Marko
This demonstration was focussed on a production engine limited to a small
Neural Network Application to Diagnostics
operating range One might suppose that a linear model-based diagnostic system could
be constructed for such a task if one wished to expend the time and effort and therefore
this exercise was not a strenuous test of the neural network approach Additionally our
expert diagnostician could examine the data traces and accurately identify the faults
However we demonstrated that this problem which had eluded automated solution by
other means up to that time could easily be handled by neural network classifiers and
encouraged us to proceed to more difficult problems for which efficient rigorous
procedures did not exist We were prepared to tolerate developing empirical solutions to
our more difficult problems since we did not expect that a thorough analytic
understanding would precede a demonstrated solution The process outlined here utilized
neural network analysis almost exclusively predominantly back-propagation on these
problems The understanding of the relationship of neural networks the structure of the
data and the training and testing of the classifiers emerged after acceptable solutions using
the neural networks methods were obtained
Consequently the next problem addressed was that of identifying similar faults by
observing the system through the multiplex serial communication link resident on the
engine control computer The serial link provides a simple hook-up procedure to the
vehicle without severing any links between plant and microcontroller However the chief
drawback of this approach is that it greatly complicates the recognition task The
complication arises because the data from the plant is sampled too infrequently is
contaminated by some processing in the controller and delivered asynchronously to the
serial link with respect to events in the plant the data output process is not permitted to
interrupt the real-time control requirements In this case a test sample of a smaller
number of faults was drawn from a vehicle operated in a similar limited range to the fIrst
example and an attempt to detect and identify the faults was made using a variety of
networks Unlike the previous case it was impossible for any experienced technicians to
identify the faults Again neural network classifIers were found to develop satisfactory
solutions over these limited data sets which were later verified by a number of careful
statistical tests Marko el aI This more complex problem also produced a wider
range of performance among the various neural ne.t paradigms studied as shown in Figure
I. where the error rates for various classifiers on these data sets are shown in the graph
These results suggested that not only would data quality and quantity need to be controlled
and improved but that the problem itself would implicitly direct us to the choice of the
classifier paradigm These issues are more thoroughly discussed elsewhere Marko al
Weiss but the conclusion was that a complete acceptable solution to
the real scope of this problem could not be developed with our group's resources for data
collection data verification and classifier validation
With these two experiences in mind we could see that the fIrst approach was an effective
means of handling the failure detection and identification FDI problem while the latter
although attractive from the standpoint of easy link-up to a vehicle was for our
numerical analysis a very difficult task It seemed that the appropriate course was to
obtain reliable data by observing the plant directly and to perfonn the classification on
that data An effective scheme to accomplish this goal is to perfonn the classifIcation task
in the control microprocessor which has access to the dire{:t data Adopting this strategy
we move the diagnostics from an off-board processor to the on-board processor and
create a new set of possibilities for diagnostics
Marko
With diagnostics contained in the controlling processor diagnostics can be shifted from
an on-demand activity undertaken at predetermined intervals or when the vehicle operator
has detected a problem to a continuous real-time activity This change implies that me
diagnostic algorithms will for the most part be evaluating a properly operating system
and only infrequently be required to detect a failure and identify the cause Additionally
the diagnostic algorithms will have to be very compact since the current control
microprocessors have very limited time and memory for calculation compared to a
off-board PC. Furthermore the classification task will need to be learned from a sample
of data which is minuscule compared to the data sets that the deployed diagnostics will
have to classify This fact imposes on the training data set the requirement that it be an
accurate statistical sample of the much more voluminous real-world data This situation
must prevail because we cannot anticipate the deployment of a classifier mal is
undergoing continuous training A classifier capable of continuous adaptation would
require more computational capability and quite likely a supervised learning environment
The fact is even for relatively simple diagnostics of operating engines assembling a
large accurate training data set off-line is a considerable task This last issue is explored
in the next paragraph but it seems to rule out early deployment of anything other than
pretrained classifiers until some experience with much larger data sets from deployed
diagnostic systems is obtained
ERROR RATE
PIN DATA
DCLDATA
N.N.
RCESINGLE
RCEMULT
BACK
PROP
TREEHYPPL
TREE
C.R.
CLASSIFIER
Figure I. Comparison of the performance of various neural network paradigms on two
static data sets by leave~ne~ut testing from measurements performed on vehicles in a
service bay The network paradigms tested arc nearest neighbor Restricted Coulomb
Energy CRCE Single Unit RCE Multiple Units Backpropagation Tree Classifier using
hyperplane separation Tree Classifier using Center-Radius decision surface The 6O-Pin
data is the data obtained directly from the engine the DCL Data Communication Link
data comes through the control microprocessor on a multiplexed two-wire link Note that
RCE-Multiple requires a priori knowledge about the problem which was unavailable for
the DCL data and thal the complete statistical testing of backpropagation was impractical
due to the length of time required to train each network
Neural Network Application to Diagnostics
We have examined this issue of real-time diagnostics as it applies to engine misfire
detection and identification Data from nonnal and misfiring engines was required from a
wide range of conditions a task which consumes hours of test track driving The set of
measurements taken is extensive in order to be cenain that the infonnation obtained is a
superset of the minimum set of informaLion required Additionally great care needed to be
exercised in eSLablishing the accuracy of a training set for supervised learning
Specifically we needed to be certain that the only samples of misfires included were those
intentionally created and not those which occurred spontaneously and were presumably
mislabeled as normal because no intentional fault was being introduced at that time In
order to accomplish this purification of the training set one must either have an
independent detector of misfires none exists for a production engine operating in a
vehicle or go through an iterative process to remove all the data vectors misclassified as
misfire from the data set after the network has completed training Since the independent
assessment of misfire cannot be obtained we must accept the latter method which is not
altogether satisfactory The problem with the iterative method is that one must initially
exclude from the training set exactly the type of event that the system is being trained to
classify We have to stan with the assumption that any additional misfires beyond the
number we introduce are classification errors We then reserve the right to amend this
judgment in light of further experience as we build up confidence in the classifier The
results of our initial studies is shown in Here we can see that a backpropagation
neural network can classify a broad range of engine operation correctly and thal the
network does quite well when we broaden the operating range almost to the perfonnance
limits of the engine The classification errors indicated in the more exhaustive study are
misfires detected when no misfire was introduced At this stage of our investigation we
cannot be certain that these are real errors they may very well be misfires occurring
spontaneously or appearing as a result of an additional unintentional induced misfrre in
an engine cycle following the one in which the fault was introduced
The results shown in therefore represent a conservative estimate of the
classification errors thaL can be expected from tests of our engine data The
backpropagation network we constructed demonstrated that misfire detection and
identification is attainable if adequate computation resources are available and appropriate
LIMITED OPERA nON
EXTENDED OPERA nON
NORMAL
NORMAL
13
MISFIRE
MISFIRE
a
ANSCLASS
NORMAL MISFIRE
NORMAL MISFIRE
Figure Classification accuracy of a backpropagation neural network trained on
misfire data tabulated as confusion matrices Data similar to that shown in is
collected over a modest range of dynamic conditions and then over a very wide range of
conditions potholed roads severe accelerations and braking etc to estimate the
performance limits of classifiers on such data These misclassification rates are indicators
of the best possible perfonnance obtainable from such data and therefore they are not
reasonable estimates of what practical implementations of classifiers should produce
Marko
care in obtaining a suitable training set is exercised However in order to make a neural
net a practical means of performing this diagnosis aboard vehicles we need to eHminate
information from the input vector which has no effect on the classification accuracy
otherwise the computational task is hopelessly beyond the capability of the engine's
microcontroller This work is currently underway using a combination of a priori
knowledge about the sensor information and principal component analysis of the data
sets Nonetheless the neural network analysis has once again established that a solution
exists and set standards for classification accuracy that we can hope to emulate with more
compact forms of classifiers
NEURAL NET CONTROL OF ACTIVE SUSPENSION
The empirical approach to developing solutions for diagnostic problems suggested that a
similar tactic might be employed effectively to control problems for which developing
acceptable controllers for non-linear dynamic systems by conventional means was a
daunting task We wished to explore the application of feed-forward networks to the
problem of learning to control a model of a non-linear active suspension system This
problem was of interest because considerable effort had gone into designing controllers by
conventional means and a performance comparison could readily be made In addition
since active suspension systems are being investigated by a number of companies we
wished to examine the possibility of developing model-independent controllers for such
systems since effective hardware systems are usually available before thoroughly
validated system models appear The initial results of this investigation outlined below
are quite encouraging
A backpropagation network was trained to emulate an existing controller for an active
suspension as a first exercise to establish some feel for the complexity of the network
required to perform such a task A complete description of the work can be found
elsewhere Hampo but briefly a network with seve.ral hidden nodes was trained to
provide perfonnance equivalent to the conventional controller Since this exercise simply
replicated an existing controller the next step was to develop a controller in the absence
of any conventional controller Therefore a system model with a novel non-linearity was
developed and utilized to train a neural network to control such a plant The architecture
for this control system is similar to that used by Nygen and Widrow Ngyen al
and is described in detail elsewhere.(Hampo Once again a backpropagation
network with only hidden nodes was trained to provide an satisfactory performance in
controlling the suspension system simulation running on a workstation This small
network learned the task with less than WOO training vectors the equivalent of less than
feet of bumpy road
Finally we examined the performance of the neural network on the same plant but
without explicit use of the plant model in the control architecture In this scheme the
output error is derived from the difference between the observed performance and the
desired performance produced by a cost function based upon conventional measures of
suspension performance In this Cost Function architecture networks of similar size
were readily trained to control non-linear plants and attain performance equivalent to
conventional controllers hand-tuned for such plants Controllers developed in this
manner provide a flexible means of approaching the problem of investigating tradeoffs
between the conflicting demands made on such suspension systems These demands
Neural Network Application to Diagnostics
include ride quality vehicle control and energy management This control architecture is
being applied both to simulations of new systems and to actual un-modeled hardware rigs
to expedite prototype development
CONCLUSIONS
This brief summary of our investigations has shown that neural networks play an
important role in the development both of classification systems for diagnosis of faults in
control systems and of controllers for practical non-linear plants In these tasks neural
networks must compete with conventional methods Conventional methods although
endowed with a more thorough analytic understanding have usually failed to provide
acceptable solutions to the problems we encountered as readily as have the neural network
methods Therefore the ANS methods have a crucial role in developing solutions
Although neural networks provide these solutions expeditiously we are just beginning to
understand how these solutions arise The growth of this understanding will detennine
the role neural networks play in the deployed implementations of these solutions

<<----------------------------------------------------------------------------------------------------------------------->>

title: 1377-bidirectional-retrieval-from-associative-memory.pdf

Bidirectional Retrieval from Associative
Memory
Friedrich T. Sommer and Gunther Palm
Department of Neural Information Processing
University of Ulm Ulm Germany
sommer,palm}~informatik.uni-ulm.de
Abstract
Similarity based fault tolerant retrieval in neural associative memories AM has not lead to wiedespread applications A drawback of the efficient Willshaw model for sparse patterns
is that the high asymptotic information capacity is of
little practical use because of high cross talk noise arising in the
retrieval for finite sizes Here a new bidirectional iterative retrieval
method for the Willshaw model is presented called crosswise bidirectional retrieval providing enhanced performance We discuss its asymptotic capacity limit analyze the first step and compare it in experiments with the Willshaw model Applying the very
efficient CB memory model either in information retrieval systems
or as a functional model for reciprocal cortico-cortical pathways
requires more than robustness against random noise in the input
Our experiments show also the segmentation ability of CB-retrieval
with addresses containing the superposition of pattens provided
even at high memory load
INTRODUCTION
From a technical point of view neural associative memories AM provide data
storage and retrieval Neural models naturally imply parallel implementation of
storage and retrieval algorithms by the correspondence to synaptic modification
and neural activation With distributed coding of the data the recall in AM
models is fault tolerant It is robust against noise or superposition in the addresses
and against local damage in the synaptic weight matrix As biological models AM
F. T. Sommer and G. Palm
have been proposed as general working schemes of networks of pyramidal cells in
many places of the cortex
An important property of a NAM model is its information capacity measuring
how efficient the synaptic weights are used In the early sixties Steinbuch realized
under the name Lernmatrix a memory model with binary synapses which is now
known as Wills haw model Ste6I The great variety of NAM models
proposed since then many triggered by Hopfield's work do not reach the
high asymptotic information capacity of the Willshaw model
For finite network size the Willshaw model does not optimally retrieve the
stored information since the inner product between matrix colum and input
pattern determines the activity for each output neuron independently For autoassociative pattern completion iterative retrieval can reduce cross talk noise
A simple bidirectional iteration as in bidirectional
associative memory BAM can however not improve heteroassociative
pattern mapping For this task we propose CB-retrieval where each retrieval step
forms the resulting activity pattern in an autoassociative process that uses the connectivity matrix twice before thresholding thereby exploiting the stored information
more efficiently
WILLSHAW MODEL AND CB EXTENSION
Here pattern mapping tasks XV yV are considered for a set of memory patterns
XV O,I}n,yv I The number of I-components
in a pattern is called pattern activity The Willshaw model works efficiently if
the memories are sparse if the memory patterns have the same activities
Ixvi a,lyvl Yi
with a nand During
learning the set of memory patterns is transformed to the weight matrix by
Cij min(I
xiv supxiy'j
For a given initial pattern XJ1. the retrieval yields the output pattern by forming
in each neuron the dendritic sum Ci/if and by calculating the activity
value by threshold comparison
yj xJ1.j Vj
with the global threshold value and denoting the Heaviside function
For finite sizes and with high memory load PI Prob[Cij
the Willshaw model provides no tolerance with respect to errors in the address see
and A bidirectional iteration of standard simple retrieval as proposed in
BAM models can therefore be ruled out for further retrieval error reduction
In the energy function of the Willshaw BAM
LCijXiYj
ij
LXi
LYj
we now indroduce a factor accounting for the magnitudes of dendritic potentials at
acti vated neurons
Bidirectional Retrieval from Associative Memory
Differentiating the energy function yields the gradient descent equations
yr
LCi CikXi Yk
CxU
Wjk
X~ew
CT
LPiiCljYi Xl I
As new terms in and sums over pattern components weighted with the
quantities wjk and wft occur wjk is the overlap between the matrix columns and
conditioned by the pattern which we call a conditioned link between y-units
Restriction on the conditioned link terms yields a new iterative retrieval scheme
which we denote as crosswise bidirectional retrieval
I)i
Cij[CT
Cij[Cx(r-I))j
iEx(r
iEy(r
For pattern has to be replaced by for Boolean
ANDing with results from timestep can be applied which has been shown to
improve iterative retrieval in the Willshaw model for autoassociation
MODEL EVALUATION
Two possible retrieval error types can be distinguished a miss error converts a
I-entry in to and a add error does the opposite
35
simple add error
add error CB-r miss error
Figure Mean retrieval error rates for a
corresponding to a memory load of The x-axes display the address
activity lilLl corresponds to a errorfree learning pattern lower activities are
due to miss errors higher activities due to add errors Left Theory Add errors
for simple retrieval eq upper curve and lower bound for the first step of
CB-retrieval eq Right Simulations Errors for simple and CB retrieval
The analysis of simple retrieval from the address yields with optimal threshold
setting the add error rate the expectation of spurious ones
b)Prob
F. T. Sommer and G. Palm
with the binomial random variable Prob[r=l B(Lit'I,Pt}I where
a denotes the add error rate and lit'l a the number of
correct in the address
For the first step of CB-retrieval a lower bound of the add error rate can be
derived by the analysis of CB-retrieval with fixed address iIJ and the perfect
learning pattern ylJ as starting patterns in the y-Iayer In this case the add error
rate is
where the random variables rl and r2 have the distributions
Prob rl lib and Prob B(ab PI Thus
PdsBS PI
where BS is the binomial sum
In the analytic results for the first step and can be compared with
simulations left versus right diagram In the experiments simple retrieval is performed with threshold CB-retrieval is iterated in the y-Iayer with fixed
address starting with three randomly chosen from the simple retrieval result
The iteration is stopped if a stable pattern at threshold bk is reached
The memory capacity can be calculated per pattern component under the assumption that in the memory patterns each component is independent the probabilities for a are a/n or b/m respectively and the probabilities of an add
and a miss error are simply the renormalized rates denoted by and for
x-patterns and for y-patterns The information about the stored pattern
contained in noisy initial or retrieved patterns is then given by the transinformation where is the Shannon information and
the conditional information The heteroassociative mapping is evaluated
by the output capacity Mm units bit/synapse It
depends on the initial noise since the performance drops with growing initial errors
and assumes the maximum if no fault tolerance is provided that is with noiseless
initial patterns see Autoassociative completion of a distorted x-pattern is
evaluated by the completion capacity Mn(t(p
A BAM maps and completes at the same time and should be therefore evaluated
by the search capacity A.
The asymptotic capacity of the Willshaw model is strikingly high The completion
capacity for autoassociation is the mapping capacity for heteroassociation with input noise is A bit/syn leading to a value for
the search capacity of bit/syn To estimate for general retrieval
procedures one can consider a recognition process of stored patterns in the whole
space of sparse initial patterns an initial pattern is recognized if it is invariant under a bidirectional retrieval cycle The so-called recognition capacity of this
process is an upper bound of the completion capacity and it had been determined
as In see This is achieved again with parameters providing
A yielding bit/syn as upper bound of the asymptotic search capacity In summary we know about the asymptotic search capacity of the CB-model
bit/syn For experimental results see
Bidirectional Retrieval from Associative Memory
EXPERIMENTAL RESULTS
The CB model has been tested in simulations and compared with the Willshaw
model simple retrieval for addresses with random noise and for addresses
composed by two learning patterns In the widely enlarged range of
high qualtity retrieval in the CB-model is demonstrated for different system sizes
output miss errors
simple
CB?r
output add errors
simple
transinformation in output pattern bit
45
35
imple
Retrieval from addresses with random
noise The x-axis labeling is as in Small
system with 35 left system size
as in two trials right Output activities
adjusted near Iyl by threshold setting
Retrieval from addresses
composed by two learning patterns Parameters as in right column of explanation of left
and right column see text
In the address contains one learning pattern and I-components of a second
learning pattern successively added with increasing abscissae On the right end
of each diagram both patterns are completely superimposed Diagrams in the left
column show errors and transinformation if retrieval results are compared with
the learning pattern which is for li I dominantly addressed Simple retrieval
errors behave similiar as for random noise in the address while the error
level of CB-retrieval raises faster if more than adds from the second pattern are
present Diagrams in the right column show the same quantities if the retrieval
result is compared with the closest of the two learning patterns It can be observed
that a learning pattern is retrieved even if the address is a complete superposition and ii if the second pattern is almost complete in the address the retrieved
pattern corresponds in some cases to the second pattern However in all cases CBretrieval yields one of the learning pattern pairs and it could be used to generate
a good address for further retrieval of the other by deletion of the corresponding
I-components in the original address
F. T. Sommer and G. Palm
output
searchc
18
Output and search capacity of CB retrieval in
bit/syn with x-axis labeling as in for
a The difference between both curves
is the contribution due to x-pattern completion the completion capacity C. It is zero for if the initial
pattern is errorfree
The search capacity of the CB model in is close to the theoretical expectations
from Sect increasing with input noise due to the address completion
SPARSE CODING
To apply the proposed AM model for instance in information retrieval a coding
of the data to be accessed into sparse binary patterns is required A useful extraction
of sparse features should take account of statistical data properties and the way the
user is acting on them There is evidence from cognitive psychology that such a
coding is typically quite easy to find The feature encoding where a person is
extracting feature sets to characterize complex situations by a few present features
is one of the three basic classes of cognitive processes defined by Sternberg
Similarities in the data are represented by feature patterns having a large number
of present features in common that is a high overlap L:i XiX'i For text
retrieval word fragments used in existing indexing techniques can be directly taken
as sparse binary features For image processing sparse coding strategies
and neural models for sparse feature extraction by anti-Hebbian learning
have been proposed Sparse patterns extracted from different data channels
in heterogeneous data can simply be concatenated and processed simultaneously in
AM. If parts of the original data should be held in a conventional memory also
these addresses have to be represented by distributed and sparse patterns in order
to exploit the high performance of the proposed NAM.
CONCLUSION
A new bidirectional retrieval method CB-retrieval has been presented for the Willshaw neural associative memory model Our analysis of the first CB-retrieval step
indicates a high potential for error reduction and increased input fault tolerance
The asymptotic capacity for bidirectional retrieval in the binary Willshaw matrix
has been determined between and bit/syn In experiments CB-retrieval
showed significantly increased input fault tolerance with respect to the standard
model leading to a practical information capacity in the order of the theoretical
expectations bit/syn Also the segmentation ability of CB-retrieval with ambiguous addresses has been shown Even at high memory load such input patterns can be decomposed and corresponding memory entries returned individually
The model improvement does not require sophisticated individual threshold setting
strategies proposed for BAM like more complex learning procedures or
dummy augmentation in the pattern coding
The demonstrated performance of the CB-model encourages applications as massively parallel search strategies in Information Retrieval The sparse coding requirement has been briefly discussed regarding technical strategies and psychological plausibility Biologically plausible variants of CB-retrieval contribute to more
Bidirectional Retrieval from Associative Memory
refined cell assembly theories see
Acknowledgement One of the authors was supported by grant
of the Deutsche Forschungsgemeinschaft

<<----------------------------------------------------------------------------------------------------------------------->>

title: 3721-noisy-generalized-binary-search.pdf

Noisy Generalized Binary Search
Robert Nowak
University of Wisconsin-Madison
Engineering Drive Madison WI
nowak@ece.wisc.edu
Abstract
This paper addresses the problem of noisy Generalized Binary Search
GBS is a well-known greedy algorithm for determining a binary-valued hypothesis through a sequence of strategically selected queries At each step a query is
selected that most evenly splits the hypotheses under consideration into two disjoint subsets a natural generalization of the idea underlying classic binary search
GBS is used in many applications including fault testing machine diagnostics
disease diagnosis job scheduling image processing computer vision and active
learning In most of these cases the responses to queries can be noisy Past work
has provided a partial characterization of GBS but existing noise-tolerant versions of GBS are suboptimal in terms of query complexity This paper presents
an optimal algorithm for noisy GBS and demonstrates its application to learning
multidimensional threshold functions
Introduction
This paper studies learning problems of the following form Consider a finite but potentially very
large collection of binary-valued functions defined on a domain In this paper will be called
the hypothesis space and will be called the query space Each is a mapping from to
Assume that the functions in are unique and that one function produces the
correct binary labeling The goal is to determine through as few queries from as possible For
each query the value corrupted with independently distributed binary noise is observed If the queries were noiseless then they are usually called membership queries to distinguish
them from other types of queries here we will simply refer to them as queries Problems
of this nature arise in many applications including channel coding experimental design
disease diagnosis fault-tolerant computing job scheduling
image processing computer vision computational geometry AMM and
active learning
Past work has provided a partial characterization of this problem If the responses to queries are
noiseless then selecting the optimal sequence of queries from is equivalent to determining an
optimal binary decision tree where a sequence of queries defines a path from the root of the tree
corresponding to to a leaf corresponding to a single element of H). In general the determination of the optimal tree is NP-complete However there exists a greedy procedure
that yields query sequences that are within an O(log factor of the optimal search tree depth
AMM 98 where denotes the cardinality of H. The greedy
procedure is referred to as Generalized Binary Search GBS or the splitting algorithm and it reduces to classic binary search in special cases
The GBS algorithm is outlined in Figure At each step GBS selects a query that results in
the most even split of the hypotheses under consideration into two subsets responding and
respectively to the query The correct response to the query eliminates one of these two subsets
from further consideration Since the hypotheses are assumed to be distinct it is clear that GBS
terminates in at most queries since it is always possible to find query that eliminates at least
Noisy Generalized Binary Search NGBS
initialize p0 uniform over H.
for
arg minx?X h?H pi
Obtain noisy response
Bayes update pi Eqn.
Generalized Binary Search GBS
initialize H0 H.
while Hi
Select arg minx?X h?Hi
Obtain response
Set Hi h(xi
hypothesis selected at each step
hi arg maxh?H pi
Figure Generalized binary search GBS algorithm and a noise-tolerant variant NGBS
one hypothesis at each step In fact there are simple examples demonstrating that this is the best
one can hope to do in general However it is also true that
in many cases the performance of GBS can be much better AMM 98 In general the
number of queries required can be bounded in terms of a combinatorial parameter of called the
extended teaching dimension also see for related work Alternatively
there exists a geometric relation between the pair called the neighborly condition that is
sufficient to bound the number of queries needed
The focus of this paper is noisy GBS. In many if not most applications it is unrealistic to assume
that the responses to queries are without error Noise-tolerant versions of classic binary search have
been well-studied The classic binary search problem is equivalent to learning a one-dimensional
binary-valued threshold function by selecting point evaluations of the function according to a bisection procedure A noisy version of classic binary search was studied first in the context of channel
coding with feedback Horstein?s probabilistic bisection procedure was shown to
be optimal optimal decay of the error probability also
One straightforward approach to noisy GBS was explored in The idea is to follow the GBS
algorithm but to repeat the query at each step multiple times in order to decide whether the response
is more probably or The strategy of repeating queries has been suggested as a general
approach for devising noise-tolerant learning algorithms This simple approach has been
studied in the context of noisy versions of classic binary search and shown to be suboptimal
Since classic binary search is a special case of the general problem it follows immediately that the
approach proposed in is suboptimal This paper addresses the open problem of determining
an optimal strategy for noisy GBS. An optimal noise-tolerant version of GBS is developed here The
number of queries an algorithm requires to confidently identify is called the query complexity of
the algorithm The query complexity of the new algorithm is optimal and we are not aware of any
other algorithm with this capability
It is also shown that optimal convergence rate and query complexity is achieved for a broad class
of geometrical hypotheses arising in image recovery and binary classification Edges in images and
decision boundaries in classification problems are naturally viewed as curves in the plane or surfaces embedded in higher-dimensional spaces and can be associated with multidimensional threshold functions valued and on either side of the curve/surface Thus one important setting for
GBS is when is a subset of dimensional Euclidean space and the set consists of multidimensional threshold functions We show that our algorithm achieves the optimal query complexity for
actively learning multidimensional threshold functions in noisy conditions
The paper is organized as follows Section describes the Bayesian algorithm for noisy GBS and
presents the main results Section examines the proposed method for learning multidimensional
threshold functions Section discusses an agnostic algorithm that performs well even if is not
in the hypothesis space H. Proofs are given in Section
A Bayesian Algorithm for Noisy GBS
In noisy GBS one must cope with erroneous responses Specifically assume that the binary response
to each query is an independent realization of the random variable satisfying
P(Y P(Y where is fixed but unknown In other words the
response is only probably correct If a query is repeated more than once then each response is
an independent realization of Define the noise-level for the query as P(Y
Throughout the paper we will let supx?X and assume that
A Bayesian approach to noisy GBS is investigated
in this paper Let p0 be a known probability meaP
sure over H. That is p0 and h?H p0 The measure p0 can be viewed as an
initial weighting over the hypothesis class expressing the fact that all hypothesis are equally reasonable prior to making queries After each query and response the distribution
is updated according to
pi
where zi h(xiP
is any constant satisfying and is
normalized to satisfy h?H The update can be viewed as an application of Bayes rule
and its effect is simple the probability masses of hypotheses that agree with the label are boosted
relative to those that disagree The parameter controls the size of the boost The hypothesis with
the largest weight is selected at each step
hi arg maxh?H pi If the maximizer is not unique
one of the maximizers is selected at random The goal of noisy GBS is to drive the error P(b
hi
to zero as quickly as possible by strategically selecting the queries A similar procedure has been
shown to be optimal for noisy classic binary search problem The crucial distinction
here is that GBS calls for a fundamentally different approach to query selection
The query selection at eachPstep must be informative with respect to the distribution pi For example
if the weighted prediction h?H pi is close to zero for a certain then a label at that point is
informative due to the large disagreement among the hypotheses This suggests the following noisetolerant variant of GBS outlined in Figure This paper shows that a slight variation of the query
selection in the NGBS algorithm in Figure yields an algorithm with optimal query complexity
It is shown that as long as is larger than the noise-level of each query then the NGBS produces
a sequence of hypotheses
h0
h1 such that P(b
hn is bounded above by a monotonically
decreasing sequence Theorem The main interest of this paper is an algorithm that drives the
error to zero exponentially fast and this requires the query selection criterion to be modified slightly
To see why this is necessary suppose that at some step of the NGBS algorithm a single hypothesis
has the majority of the probability mass Then the weighted prediction will be almost
equal to the prediction of that hypothesis close to or for all queries and therefore the
responses to all queries are relatively certain and non-informative Thus the convergence of the
algorithm could become quite slow in such conditions A similar effect is true in the case of noisy
classic binary search To address this issue the query selection criterion is modified
via randomization so that the response to the selected query is always highly uncertain
In order to state the modified selection procedure and the main results observe that the query space
can be partitioned into equivalence subsets such that every is constant
for all queries in
each such subset Let A denote the smallest such partition Note that A?A A. For every
A A and the value of is constant either or for all A denote this value
by As first noted in A can play an important role in GBS. In particular observe that
the query selection step in NGBS is equivalent to an optimization over A rather that itself The
randomization of the query selection step is based on the notion of neighboring sets in A.
Definition Two sets A A0 A are said to be neighbors if only a single hypothesis and its
complement if it also belongs to outputs a different value on A and A0
The modified NGBS algorithm is outlined in Figure Note that the query selection step is identical
to that of the original NGBS algorithm unless there exist two neighboring sets with strongly bipolar
weighted responses In the latter case a query is randomly selected from one of these two sets with
equal probability which guarantees a highly uncertain response
Theorem Let denotes the underlying probability measure governing noises and algorithm randomization If then both the NGBS and modified NGBS algorithms in Figure and
Figure respectively generate a sequence of hypotheses such that P(b
hn an where
an is a monotonically decreasing sequence
The condition ensures that the update is not overly aggressive We now turn to the
matter of sufficient conditions guaranteeing that P(b
hn exponentially fast with The
Modified NGBS
initialize p0 uniform over H.
for
Let
minA?A h?H pi
neighboring sets A and A0
If there exists
with h?H pi and h?H pi then select from
A or A0 with
probability each Otherwise select from the set Amin
arg minA?A h?H pi In the case that the sets above are non-unique
choose at random any one satisfying the requirements
Obtain noisy response
Bayes update pi Eqn.
hypothesis selected at each step
hi arg maxh?H pi
Figure Modified NGBS algorithm
exponential convergence rate of classic binary search hinges on the fact that the hypotheses can be
ordered with respect to In general situations the hypothesis space cannot be ordered in such a
fashion but the neighborhood graph of A provides a similar local structure
Definition The pair is said to be neighborly if the neighborhood graph of A is connected
for every pair of sets in A there exists a sequence of neighboring sets that begins at one of the
pair and ends with the other
In essence the neighborly condition simply means that each hypothesis is locally distinguishable
from all others By local we mean in the vicinity of points where the output of the hypothesis
changes from to The neighborly condition was first introduced in in the analysis
of GBS. It is shown in Section that the neighborly condition holds for the important case of
hypothesis spaces consisting of multidimensional threshold functions If is neighborly then
the modified NGBS algorithm guarantees that P(b
hi exponentially fast
Theorem Let denotes the underlying probability measure governing noises and algorithm randomization If and is neighborly then the modified NGBS algorithm in Figure
generates a sequence of hypotheses satisfying
P(b
hn
with exponential constant min
where
min max dP
h?H
The exponential convergence rate1 is governed by the key parameter The minimizer in
exists because the minimization can be computed over the space of finite-dimensional probability
mass functions over the elements of A. As long as no hypothesis is constant over the whole of
the value of is typically a small constant much less than that is independent of the size
of and the next section for concrete examples In such situations the
convergence rate of modified NGBS is optimal up to constant factors No other algorithm can solve
the noisy GBS problem with a lower query complexity The query complexity of the modified NGBS
algorithm can be derived as follows Let be a prespecified confidence parameter The number
of queries required to ensure that P(b
hn is log
O(log which is the
optimal query complexity Intuitively O(log bits are required to encode each hypothesis More
formally the classic noisy binary search problem satisfies the assumptions of Theorem
Note that the factor
in the exponential rate parameter is a positive constant
strictly less than For a noise level this factor is maximized by a value which tends to
as tends to
and hence it is a special case of the general problem It is known that the optimal query complexity
for noisy classic binary search is O(log
We contrast this with the simple noise-tolerant GBS algorithm based on repeating each query in the
standard GBS algorithm of Figure multiple times to control the noise 06 for
related derivations It follows from Chernoff?s bound that the query complexity of determining the
correct label for a single query with confidence at least is
Suppose that GBS
requires n0 queries in the noiseless situation Then using the union bound we require log(n
queries at each step to guarantee that the labels determined for all n0 queries are correct with probability If is neighborly then GBS requires n0 O(log queries in noiseless
conditions Therefore under the conditions of Theorem the query complexity of the
simple noise-tolerant GBS algorithm is O(log log log?|H a logarithmic factor worse than the
optimal query complexity
Noisy GBS for Learning Multidimensional Thresholds
We now apply the theory and modified NGBS algorithm to the problem of learning multidimensional
threshold functions from point evaluations a problem that arises commonly in computer vision
AMM image processing and active learning
In this case the hypotheses are determined by possibly nonlinear decision surfaces in
d-dimensional Euclidean space is a subset of Rd and the queries are points in Rd It
suffices to consider linear decision surfaces of the form ha,b sign(ha where a Rd
kak2 for some constant and ha denotes the inner product in Rd
Note that hypotheses of this form can be used to represent nonlinear decision surfaces by applying
a nonlinear mapping to the query space
Theorem Let be a finite collection of hypotheses of form sign(ha for some constant
Then the hypotheses selected by the modified NGBS algorithm with satisfy
P(b
hn
Moreover
hn can be computed in time polynomial in
with 41
Based on the discussion at the end of the previous section we conclude that the query complexity
of the modified NGBS algorithm is O(log this is the optimal up to constant factors The only
other algorithm with this capability that we are aware of was analyzed in and it is based
on a quite different approach tailored specifically to linear threshold problem
Agnostic Algorithms
We also mention the possibility of agnostic algorithms guaranteed to find the best hypothesis in
even if the optimal hypothesis is not in and/or the assumptions of Theorem or do not hold
The best hypothesis in is the one that minimizes the error with respect to a given probability measure on denoted by PX The following theorem proved in demonstrates an agnostic
algorithm that performs almost as well as empirical risk minimization ERM in general and has
the optimal O(log query complexity when the conditions of Theorem hold
Theorem Let PX denote a probability distribution on and suppose we have a query budget
of Let h1 denote the hypothesis selected by modified NGBS using of the queries and let h2
denote the hypothesis selected by ERM from queries drawn independently from PX Draw the
remaining queries independently from the restriction of PX to the set on which h1
and
denote the average number of errors made by h1 and
and h2 disagree and let
Then in general
h2 on these queries Select
arg min{R
E[R(b
min{E[R(h1
where denotes the probability of error of with respect to PX and denotes the
expectation with respect to all random quantities Furthermore if the assumptions of Theorem
hold with noise bound then
P(b
Appendix Proofs
Proof of Theorem
Let denote expectation with respect to and define Cn pn Note that
Cn reflects the amount of mass that pn places on the suboptimal hypotheses First note
that
P(b
hn P(pn P(Cn E[Cn by Markov?s inequality
Next observe that
E[Cn
E[(Cn
E[(Cn max E[(Cn
C0
max max Ci
pi
Note that because p0 is assumed to be uniform C0 A similar conditioning technique is employed for interval estimation in The rest of the proof entails showing that
Ci which proofs the result and requires a very different approach than
The precise form of p1 p2 is derived as follows Let pi zi the
weighted proportion of hypotheses that agree with
The factor that normalizes the updated dis(1?zi
tribution
in
is
related
to
as
follows
Note
that
pi
h:zi pi
h:zi pi Thus
pi
Denote the reciprocal of the update factor for by
where zi and observe that pi Thus
pi pi
pi
Ci
pi pi
pi
Now to bound maxpi Ci pi we will show that maxpi pi To accomplish
this we will assume that pi is arbitrary
For every
A A and every let denote the value of on the set A. Define A
pi the proportion of hypotheses that take the value on A. Note that for
every A we have A
since at least one hypothesis has the value on A and for
all H. Let Ai denote that set that is selected from and consider the four possible situations
A
A
To bound pi it is helpful to condition on Ai Define qi Px,y|Ai If Ai
then
pi Ai
A
A
A
qi Ai
qi
qi qi
A
A
Define Ai A
A
pi Ai
qi
Similarly if Ai then
qi qi
A
Ai
Ai
By assumption qi and since the factor
qi
Define
to obtain the bounds
Since both
Ai
and
Ai
A
A
Ai
A
Ai
A
are less than it follows that pi
Proof of Theorem
The proof amounts to obtaining upper bounds for Ai and Ai defined above in and
For every A
A and any probability measure on the weighted prediction on A is defined
to be A h?H where is the constant value of for every A. The
following lemma plays a crucial role in the analysis of the modified NGBS algorithm
Lemma If is neighborly then for every probability measure on there either exists a set
A A such that or a pair of neighboring sets A A0 A such that A
and A0
Proof of Lemma Suppose that minA?A Then there must exist A A0 A
such that A and A0 otherwise cannot be the minimax moment
of H. To see this suppose for
that A for all A A. Then for every
instance
distribution
on we have h?H p(h)h(x)dPR This contradicts Rthe definition of
since h?H p(h)h(x)dP h?H dP maxh?H dP
The neighborly condition guarantees that there exists a sequence of neighboring sets beginning at A
and ending at A0 Since on every set and the sign of must change at some
point in the sequence it follows that there exist neighboring sets satisfying the claim
Now consider two distinct situations Define bi minA?A pi A)|. First suppose that there do
not exist neighboring sets A and A0 with pi A bi and pi A0 bi Then by Lemma
this implies that bi and according the query selection step of the modified NGBS algorithm
Ai arg minA pi A)|. Note that because pi Ai A
Hence both Ai and Ai are bounded above by
Now suppose that there exist neighboring sets A and A0 with pi A bi and pi A0 bi
Recall that in this case Ai is randomly chosen to be A or A0 with equal probability Note that
A
bi and A
bi If A then applying results in
bi
bi
bi
since bi Similarly if then yields pi Ai A0
If on A and then applying on A and on A0 yields
pi Ai A0
A A
A
A
A
A
A
A
A
A
pi Ai A0
since A
A
The final possibility is that and A Apply on
A and on A to obtain
pi Ai A0
A A
A
A
A
A
Next use the fact that because A and A0 are neighbors A
A
pi pi if does
not belong to then pi Hence
pi Ai A0
A
A
A
pi pi pi pi
pi pi pi
since the bound is maximized when pi Now bound pi by the maximum of the
conditional bounds above to obtain
pi max pi
and thus it is easy to see that
pi pi
min
pi
Ci
pi
Proof of Theorem
First we show that the pair Rd is neighborly Definition Each A A is a polytope in Rd
These polytopes are generated by intersections of the halfspaces corresponding to the hypotheses
Any two polytopes that share a common face are neighbors the hypothesis whose decision boundary
defines the face and its complement if it exists are the only ones that predict different values on
these two sets Since the polytopes tessellate Rd the neighborhood graph of A is connected
Next consider the final bound in the proof of Theorem above We next show that the value of
defined in is Since the offsets of the hypotheses are all less than in magnitude it follows
that the distance from the origin to the nearest point of the decision surface of every hypothesis is at
most Let Pr denote the uniform probability distribution on a ball of radius centered at the origin
in Rd Then for every of the form sign(ha
dP
and limr dPr and so
Lastly note that the modified NGBS algorithm involves computing h?H pi for all A A
at each step The computational complexity of each step is therefore proportional to the cardinality
of A which is equal to the number of polytopes generated by intersections of half-spaces It is
Pd
known that

<<----------------------------------------------------------------------------------------------------------------------->>

title: 1540-general-purpose-localization-of-textured-image-regions.pdf

General-purpose localization of textured
Image
regions
Rutb Rosenboltz
XeroxPARC
Coyote Hill Rd.
Palo Alto CA
Abstract
We suggest a working definition of texture Texture is stuff that is
more compactly represented by its statistics than by specifying the
configuration of its parts This definition suggests that to fmd
texture we look for outliers to the local statistics and label as
texture the regions with no outliers We present a method based
upon this idea for labeling points in natural scenes as belonging to
texture regions while simultaneously allowing us to label lowlevel bottom-up cues for visual attention This method is based
upon recent psychophysics results on processing of texture and
popout
WHAT IS TEXTURE AND WHY DO WE WANT TO
FIND IT
In a number of problems in computer VlSlon and image processing one must
distinguish between image regions that correspond to objects and those which
correspond to texture and perform different processing depending upon the type of
region Current computer vision algorithms assume one magically knows this
region labeling But what is texture We have the notion that texture involves a
pattern that is somehow homogeneous or in which signal changes are too
complex to describe so that aggregate properties must be used instead Saund
There is by no means a firm division between texture and objects rather the
characterization often depends upon the scale of interest Saund
Email rruth@parc.xerox.com
R. Rosenholtz
Ideally the defmition of texture should probably depend upon the application We
investigate a definition that we believe will be of fairly general utility Texture is
stuff that seems to belong to the local statistics We propose extracting several
texture features at several different scales and labeling as texture those regions
whose feature values are likely to have come from the local distribution
Outliers to the local statistics tend to draw our attention Rosenholtz
The phenomenon is often referred to as popout Thus while labeling locally
statistically homogeneous regions as texture we can simultaneously highlight
salient outliers to the local statistics Our revised defmition is that texture is the
absence of popout
In Section we discuss previous work in both human perception and in fmding
texture and regions of interest in an image In Section we describe our method
We present and discuss results on a number of real images in Section
PREVIOUS WORK
See Wolfe for a review of the visual search literature Popout is typically
studied using simple displays in which an experimental subject searches for the
unusual target item among the other distractor items One typically attempts to
judge the saliency or degree to which the target pops out by studying the
efficiency of search for that item Typically popout is modeled by a relatively lowlevel operator which operates independently on a number of basic features of the
image including orientation contrast/color depth and motion In this paper we
look only at the features of contrast and orientation
Within the image-processing field much of the work in fmding texture has defmed
as texture any region with a high luminance variance Vaisey Gersho
Unfortunately the luminance variance in a region containing an edge can be as high
as that in a textured region Won Park use model fitting to detect image
blocks containing an edge and then label blocks with high variance as containing
texture
Recently several computer vision researchers have also tackled this problem
Leung Malik found regions of completely deterministic texture Other
researchers have used the defmition that if the luminance goes up and then down
again vice versa it's texture Forsyth aI However this method will
treat lines as if they were texture Also with no notion of similarity within a texture
also lacking in the image-processing work one would mark a fault in a texture
as belonging to that texture This would be unacceptable for a texture synthesis
application in which a routine that tried to synthesize such a texture would most
likely fail to reproduce the highly visible fault More recently Shi and Malik
presented a method for segmenting images based upon texture features Their
method performs extremely well at the segmentation task dividing an image into
regions with internal similarity that is high compared to the similarity across
regions However it is difficult to compare with their results since they do not
explicitly label a subset of the resulting regions as texture Furthermore this
method may also tend to mark a fault in a texture as belonging to that texture
This is both because the method is biased against separating out small regions and
because the grouping of a patch with one region depends as much upon the
difference between that patch and other regions as it does upon the similarity
between the patch and the given region
Very little computer vision work has been done on attentional cues Milanese al
found salient image regions using both top-down information and a bottomup conspicuity operator which marks a local region as more salient the greater the
General-Purpose Localization o/Textured Image Regions
difference between a local feature value and the mean feature value in the
surrounding region However for the same difference in means a local region is
less salient when there is a greater variance in the feature values in the surrounding
region Duncan Humphreys Rosenholtz We use as our saliency
measure a test for outliers to the local distribution This captures in many cases the
dependence of saliency on difference between a given feature value and the local
mean relative to the local standard deviation We will discuss our saliency measure
in greater detail in the following section
FINDING TEXTURE AND REGIONS OF INTEREST
We compute multiresolution feature maps for orientation and contrast and then look
for outliers in the local orientation and contrast statistics We do this by fast
creating a 3-level Gaussian pyramid representation of the image To extract
contrast we filter the pyramid with a difference of circularly symmetric Gaussians
The response of these filters will oscillate even in a region with constant-contrast
texture a sinewave pattern We approximate a computation of the maximum
response of these filters over a small region by fast squaring the filter responses
and then filtering the contrast energy with an appropriate Gaussian Finally we
threshold the contrast to eliminate low-contrast regions flat texture These
thresholds one for each scale were set by examining the visibility of sinewave
patterns of various spatial frequencies
We compute orientation in a simple and biologically plausible way using Bergen
Landy's back pocket model for low-level computations
Filter the pyramid with horizontal vertical and oriented Gaussian second
derivatives
Compute opponent energy by squaring the filter outputs pooling them over a
region times the scale of the second derivative filters and subtracting the
vertical from the horizontal response and the 45 from the response
Normalize the opponent energy at each scale by dividing by the total energy in
the orientation energy bands at that scale
The result is two images at each scale of the pyramid To a good approximation in
regions which are strongly oriented these images represent and
where is the local orientation at that scale and is a value between and which
is related to the local orientation specificity Orientation estimates from points with
low specificity tend to be very noisy In images of white noise of the
estimates of fall below therefore with confidence an orientation
specificity of did not occur due to chance We use this value to threshold out
orientation estimates with low orientedness
We then estimate the local feature distribution for each feature and scale using
the method of Parzen windows The blurring of the distribution estimate by the
Parzen window mimics uncertainty in estimates of feature values by the visual
system We collect statistics over a local integration region For texture processing
the size of this region is ind.ependent of viewing distance and is roughly lOS in
diameter where is the support of the Gaussian 2nd derivative filters used to extract
the texture features Kingdom Keeble Kingdom
We next compute a non-parametric measure of saliency
saliency
P(v ID
maxP(x ID
R. Rosenholtz
Note that if were Gaussian this simplifies to
which should be compared to the standard parametric test for outliers which uses
the measure Our saliency measure is essentially a more general nonparametric form of this measure it does not assume a Gaussian distribution
Points with saliency less than are labeled as candidate texture points If were
Gaussian this would correspond to feature estimates within one standard deviation
of the mean Points with saliency greater than are labeled as candidates for
bottom-up attentional cues If were Gaussian this would correspond to feature
estimates more than from the mean a standard parametric test for outliers
One could of course keep the raw saliency values as a measure of the likelihood
that a region contained texture rather than setting a hard threshold We use a hard
threshold in our examples to better display the results Both the texture images and
the region of interest images are median-filtered to remove extraneous points
EXPERIMENTAL RESULTS
Figure shows several example images Figures and show texture found at
each scale of processing The striped and checkered patterns represent oriented and
homogeneous contrast texture respectively The absence of an image in any of
these figures means that no texture of the given type was found in that image at the
given scale Note that we perform no segmentation of one texture from another
For the building image the algorithm labeled bricks and window panes as fme-scale
texture and windows and shutters as coarser-scale texture The leopard skin and
low-frequency stripes in the lower right comer of the leopard image were correctly
labeled as texture In the desk image the wood texture was correctly identified
The regular pattern of windows were marked as texture in the hotel image In the
house image the wood siding trees and part of the grass were labeled as texture
much of the grass was low contrast and labeled as flat texture One of the
bushes is correctly identified as having coarser texture than the other has In the
lighthouse image the house sans window fence and tower were marked as well as
a low-frequency oriented pattern in the clouds
Figure shows the regions of interest that were found the striped and plaid patterns
here have no meaning but were chosen for maximum visibility Most complex
natural scenes had few interesting low-level attentional areas In the lighthouse
image the life preserver is marked In the hotel curved or unusual angular
windows are identified as attentional cues as well as the top of the building Both
of these results are in agreement with psychophysical results showing that observers
quickly identify curved or bent lines among straight lines reviewed in Wolfe
The simpler desk scene yields more intuitive results with each of the
objects labeled as well as the phone cord
Bottom-up attentional cues are outliers to the local distribution of features and we
have suggested that texture is the absence of such outliers This definition captures
some of the intuition that texture is homogeneous and statistical in nature We
presented a method for fmding contrast and orientation outliers and results both on
localizing texture and on finding popout in natural images For the simple desk
image the algorithm highlights salient regions that correspond to our notions of the
important objects in the scene On complicated natural scenes its results are less
intuitive suggesting that search in natural scenes makes use of higher-level
General-Purpose Localization o/Textured Image Regions
processing such as grouping into objects This result should not be terribly
surprising but serves as a useful check on simple low-level models of visual
attention The algorithm does a good job of identifying textured regions at a
number of different scales with the results perhaps more intuitive at finer scales
Acknowledgments
This work was partially supported by an NRC postdoctoral award at NASA Ames
Many thanks to David Marimont and Eric Saund for useful discussions

<<----------------------------------------------------------------------------------------------------------------------->>

title: 6145-a-multi-batch-l-bfgs-method-for-machine-learning.pdf

A Multi-Batch L-BFGS Method for Machine
Learning
Albert S. Berahas
Northwestern University
Evanston IL
albertberahas@u.northwestern.edu
Jorge Nocedal
Northwestern University
Evanston IL
j-nocedal@northwestern.edu
Martin Tak??c
Lehigh University
Bethlehem PA
takac.mt@gmail.com
Abstract
The question of how to parallelize the stochastic gradient descent SGD method
has received much attention in the literature In this paper we focus instead on batch
methods that use a sizeable fraction of the training set at each iteration to facilitate
parallelism and that employ second-order information In order to improve the
learning process we follow a multi-batch approach in which the batch changes
at each iteration This can cause difficulties because L-BFGS employs gradient
differences to update the Hessian approximations and when these gradients are
computed using different data points the process can be unstable This paper shows
how to perform stable quasi-Newton updating in the multi-batch setting illustrates
the behavior of the algorithm in a distributed computing platform and studies its
convergence properties for both the convex and nonconvex cases
Introduction
It is common in machine learning to encounter optimization problems involving millions of parameters
and very large datasets To deal with the computational demands imposed by such applications high
performance implementations of stochastic gradient and batch quasi-Newton methods have been
developed In this paper we study a batch approach based on the L-BFGS method that
strives to reach the right balance between efficient learning and productive parallelism
In supervised learning one seeks to minimize empirical risk
1X
def
fi
where denote the training examples and Rd is the composition of a
prediction function parametrized by and a loss function The training problem consists of finding
an optimal choice of the parameters Rd with respect to
min
w?Rd
1X
fi
At present the preferred optimization method is the stochastic gradient descent SGD method
and its variants 24 which are implemented either in an asynchronous manner when
Conference on Neural Information Processing Systems NIPS Barcelona Spain
using a parameter server in a distributed setting or following a synchronous mini-batch approach that
exploits parallelism in the gradient evaluation 22 A drawback of the asynchronous approach
is that it cannot use large batches as this would cause updates to become too dense and compromise
the stability and scalability of the method As a result the algorithm spends more time in
communication as compared to computation On the other hand using a synchronous mini-batch
approach one can achieve a near-linear decrease in the number of SGD iterations as the mini-batch
size is increased up to a certain point after which the increase in computation is not offset by the
faster convergence
An alternative to SGD is a batch method such as L-BFGS which is able to reach high training
accuracy and allows one to perform more computation per node so as to achieve a better balance
with communication costs Batch methods are however not as efficient learning algorithms as
SGD in a sequential setting To benefit from the strength of both methods some high performance
systems employ SGD at the start and later switch to a batch method
Multi-Batch Method In this paper we follow a different approach consisting of a single method
that selects a sizeable subset batch of the training data to compute a step and changes this batch at
each iteration to improve the learning abilities of the method We call this a multi-batch approach
to differentiate it from the mini-batch approach used in conjunction with SGD which employs a
very small subset of the training data When using large batches it is natural to employ a quasiNewton method as incorporating second-order information imposes little computational overhead
and improves the stability and speed of the method We focus here on the L-BFGS method which
employs gradient information to update an estimate of the Hessian and computes a step in flops
where is the number of variables The multi-batch approach can however cause difficulties to
L-BFGS because this method employs gradient differences to update Hessian approximations When
the gradients used in these differences are based on different data points the updating procedure can
be unstable Similar difficulties arise in a parallel implementation of the standard L-BFGS method if
some of the computational nodes devoted to the evaluation of the function and gradient are unable to
return results on time as this again amounts to using different data points to evaluate the function
and gradient at the beginning and the end of the iteration The goal of this paper is to show that stable
quasi-Newton updating can be achieved in both settings without incurring extra computational cost or
special synchronization The key is to perform quasi-Newton updating based on the overlap between
consecutive batches The only restriction is that this overlap should not be too small something that
can be achieved in most situations
Contributions We describe a novel implementation of the batch L-BFGS method that is robust in
the absence of sample consistency when different samples are used to evaluate the objective
function and its gradient at consecutive iterations The numerical experiments show that the method
proposed in this paper which we call the multi-batch L-BFGS method achieves a good balance
between computation and communication costs We also analyze the convergence properties of the
new method using a fixed step length strategy on both convex and nonconvex problems
The Multi-Batch Quasi-Newton Method
In a pure batch approach one applies a gradient based method such as L-BFGS to the
deterministic optimization problem When the number of training examples is large it is
natural to parallelize the evaluation of and by assigning the computation of the component
functions fi to different processors If this is done on a distributed platform it is possible for some
of the computational nodes to be slower than the rest In this case the contribution of the slow
unresponsive computational nodes could be ignored given the stochastic nature of the objective
function This leads however to an inconsistency in the objective function and gradient at the
beginning and at the end of the iteration which can be detrimental to quasi-Newton methods Thus
we seek to find a fault-tolerant variant of the batch L-BFGS method that is capable of dealing with
slow or unresponsive computational nodes
A similar challenge arises in a multi-batch implementation of the L-BFGS method in which the entire
training set is not employed at every iteration but rather a subset of the data is
used to compute the gradient Specifically we consider a method in which the dataset is randomly
divided into a number of batches say or and the minimization is performed with
respect to a different batch at every iteration At the k-th iteration the algorithm chooses a batch
Sk computes
fi wk
Sk wk
Sk
i?Sk
Sk wk gkSk
fi wk
Sk
i?Sk
and takes a step along the direction Hk gkSk where Hk is an approximation to wk Allowing the sample Sk to change freely at every iteration gives this approach flexibility of implementation
and is beneficial to the learning process as we show in Section We refer to Sk as the sample of
training points even though Sk only indexes those points
The case of unresponsive computational nodes and the multi-batch method are similar The main
difference is that node failures create unpredictable changes to the samples Sk whereas a multi-batch
method has control over sample generation In either case the algorithm employs a stochastic approximation to the gradient and can no longer be considered deterministic We must however distinguish
our setting from that of the classical SGD method which employs small mini-batches and noisy
gradient approximations Our algorithm operates with much larger batches so that distributing the
function evaluation is beneficial and the compute time of gkSk is not overwhelmed by communication
costs This gives rise to gradients with relatively small variance and justifies the use of a second-order
method such as L-BFGS
Robust Quasi-Newton Updating The difficulties created by the use of a different sample Sk at each
iteration can be circumvented if consecutive samples Sk and overlap so that Ok Sk
One can then perform stable quasi-Newton updating by computing gradient differences based on
this overlap by defining
Ok
gkOk
wk
in the notation given in The correction pair yk sk can then be used in the BFGS update
When the overlap set Ok is not too small yk is a useful approximation of the curvature of the objective
function along the most recent displacement and will lead to a productive quasi-Newton step This
observation is based on an important property of Newton-like methods namely that there is much
more freedom in choosing a Hessian approximation than in computing the gradient Thus a
smaller sample Ok can be employed for updating the inverse Hessian approximation Hk than for
computing the batch gradient gkSk in the search direction Hk gkSk In summary by ensuring that
unresponsive nodes do not constitute the vast majority of all working nodes in a fault-tolerant parallel
implementation or by exerting a small degree of control over the creation of the samples Sk in the
multi-batch method one can design a robust method that naturally builds upon the fundamental
properties of BFGS updating
We should mention in passing that a commonly used strategy for ensuring stability of quasi-Newton
updating in machine learning is to enforce gradient consistency to use the same sample
Sk to compute gradient evaluations at the beginning and the end of the iteration Another popular
remedy is to use the same batch Sk for multiple iterations alleviating the gradient inconsistency
problem at the price of slower convergence In this paper we assume that achieving such sample
consistency is not possible the fault-tolerant case or desirable a multi-batch framework and
wish to design a new variant of L-BFGS that imposes minimal restrictions in the sample changes
Specification of the Method
At the k-th iteration the multi-batch BFGS algorithm chooses a set Sk and computes a
new iterate
wk Hk gkSk
where is the step length gkSk is the batch gradient and Hk is the inverse BFGS Hessian
matrix approximation that is updated at every iteration by means of the formula
VkT Hk Vk sk sTk
Ts
yk
Vk I yk sTk
To compute the correction vectors sk yk we determine the overlap set Ok Sk consisting
of the samples that are common at the k-th and iterations We define
Ok wk
fi wk
Ok wk gkOk
fi wk
Ok
Ok
i?Ok
i?Ok
and compute the correction vectors as in In this paper we assume that is constant
In the limited memory version the matrix Hk is defined at each iteration as the result of applying
BFGS updates to a multiple of the identity matrix using a set of correction pairs si
kept in storage The memory parameter is typically in the range to When computing the
matrix-vector product in it is not necessary to form that matrix Hk since one can obtain this
product via the two-loop recursion using the most recent correction pairs si After the
step has been computed the oldest pair sj yj is discarded and the new curvature pair is stored
A pseudo-code of the proposed method is given below and depends on several parameters The
parameter denotes the fraction of samples in the dataset used to define the gradient
The parameter denotes the length of overlap between consecutive samples and is defined as a
fraction of the number of samples in a given batch
Algorithm Multi-Batch L-BFGS
Input w0 initial iterate for training set memory parameter
batch fraction of overlap fraction of batch iteration counter
Create initial batch S0
As shown in Firgure
for do
Calculate the search direction pk Hk gkSk
Using L-BFGS formula
Choose the step length
Compute wk pk
Create the next batch
Ok
Compute the curvature pairs wk and
gkOk
Replace the oldest pair si by
end for
Sample Generation
We now discuss how the sample is created at each iteration Line in Algorithm
Distributed Computing with Faults Consider a distributed implementation in which slave nodes
read the current iterate wk from the master node compute a local gradient on a subset of the
dataset and send it back to the master node for aggregation in the calculation Given a time
computational budget it is possible for some nodes to fail to return a result The schematic in
Figure 1a illustrates the gradient calculation across two iterations and in the presence of faults
Here Bi denote the batches of data that each slave node receives where Bi
is the gradient calculation using all nodes that responded within the preallocated time
and
MASTER
NODE
wk
S0
B1
B2
B1 wk
rf
B3
SLAVE
NODES
BB
B3 wk rf
BB wk
rf
wk
B1
B2
B3
B1
rf
MASTER
NODE
wk
rf
SHUFFLED DATA
O0
SHUFFLED DATA
S3
O3
S6
O6
BB
S1
O1
S4
O4
BB
rf
S2
O2
S5
O5
rf
Figure Sample and Overlap formation
Let Jk and be the set of indices of all nodes that returned a
gradient at the k-th and iterations respectively Using this notation Sk j?Jk Bj and
Bj and we define Ok j?Jk Bj The simplest implementation in this
setting preallocates the data on each compute node requiring minimal data communication only
one data transfer In this case the samples Sk will be independent if node failures occur randomly
On the other hand if the same set of nodes fail then sample creation will be biased which is harmful
both in theory and practice One way to ensure independent sampling is to shuffle and redistribute the
data to all nodes after a certain number of iterations
Multi-batch Sampling We propose two strategies for the multi-batch setting
Figure 1b illustrates the sample creation process in the first strategy The dataset is shuffled and
batches are generated by collecting subsets of the training set in order Every set except S0 is
of the form Sk Nk Ok where and Ok are the overlapping samples with batches
and respectively and Nk are the samples that are unique to batch Sk After each pass
through the dataset the samples are reshuffled and the procedure described above is repeated In our
implementation samples are drawn without replacement guaranteeing that after every pass epoch
all samples are used This strategy has the advantage that it requires no extra computation in the
Ok
evaluation of gkOk and
but the samples Sk are not independent
The second sampling strategy is simpler and requires less control At every iteration a batch Sk is
created by randomly selecting Sk elements from The overlapping set Ok is then formed
by randomly selecting Ok elements from Sk subsampling This strategy is slightly more expensive
Ok
since
requires extra computation but if the overlap is small this cost is not significant
Convergence Analysis
In this section we analyze the convergence properties of the multi-batch L-BFGS method Algorithm
when applied to the minimization of strongly convex and nonconvex objective functions using a
fixed step length strategy We assume that the goal is to minimize the empirical risk given in
but note that a similar analysis could be used to study the minimization of the expected risk
Strongly Convex case
Due to the stochastic nature of the multi-batch approach every iteration of Algorithm employs a
gradient that contains errors that do not converge to zero Therefore by using a fixed step length
strategy one cannot establish convergence to the optimal solution but only convergence to a
neighborhood of Nevertheless this result is of interest as it reflects the common practice of
using a fixed step length and decreasing it only if the desired testing error has not been achieved It
also illustrates the tradeoffs that arise between the size of the batch and the step length
In our analysis we make the following assumptions about the objective function and the algorithm
Assumptions A.
is twice continuously differentiable
and
I
such that I
for all Rd and all
There exist positive constants
sets
There is a constant such that ES k?F for all Rd and all sets
The samples are drawn independently and is an unbiased estimator of the true
gradient for all Rd ES
Note that Assumption implies that the entire Hessian also satisfies
I
Rd
for some constants Assuming that every sub-sampled function is strongly convex
is not unreasonable as a regularization term is commonly added in practice when that is not the case
We begin by showing that the inverse Hessian approximations Hk generated by the multi-batch
L-BFGS method have eigenvalues that are uniformly bounded above and away from zero The proof
technique used is an adaptation of that in
Lemma If Assumptions above hold there exist constants such that the
Hessian approximations Hk generated by Algorithm satisfy
I Hk I
for
Utilizing Lemma we show that the multi-batch L-BFGS method with a constant step length
converges to a neighborhood of the optimal solution
Theorem Suppose that Assumptions hold and let where is the
minimizer of Let wk be the iterates generated by Algorithm with starting
from w0 Then for all
E[F wk
The bound provided by this theorem has two components a term decaying linearly to zero and
a term identifying the neighborhood of convergence Note that a larger step length yields a
more favorable constant in the linearly decaying term at the cost of an increase in the size of the
neighborhood of convergence We will consider again these tradeoffs in Section where we also
note that larger batches increase the opportunities for parallelism and improve the limiting accuracy
in the solution but slow down the learning abilities of the algorithm
One can establish convergence of the multi-batch L-BFGS method to the optimal solution by
employing a sequence of step lengths that converge to zero according to the schedule proposed
by Robbins and Monro However that provides only a sublinear rate of convergence which is of
little interest in our context where large batches are employed and some type of linear convergence is
expected In this light Theorem is more relevant to practice
Nonconvex case
The BFGS method is known to fail on noconvex problems Even for L-BFGS which
makes only a finite number of updates at each iteration one cannot guarantee that the Hessian
approximations have eigenvalues that are uniformly bounded above and away from zero To establish
convergence of the BFGS method in the nonconvex case cautious updating procedures have been
proposed Here we employ a cautious strategy that is well suited to our particular algorithm we
skip the update set Hk if the curvature condition
ykT sk ksk
is not satisfied where is a predetermined constant Using said mechanism we show that the
eigenvalues of the Hessian matrix approximations generated by the multi-batch L-BFGS method are
bounded above and away from zero Lemma The analysis presented in this section is based on
the following assumptions
Assumptions B.
is twice continuously differentiable
The gradients of are Lipschitz continuous and the gradients of are Lipschitz
continuous for all Rd and all sets
The function is bounded below by a scalar Fb
There exist constants and such that ES k?F for all
Rd and all sets
The samples are drawn independently and is an unbiased estimator of the true
gradient for all Rd
Lemma Suppose that Assumptions hold and let be given Let Hk be the
Hessian approximations generated by Algorithm with the modification that Hk whenever
is not satisfied Then there exist constants such that
I Hk I
for
We can now follow the analysis in Chapter to establish the following result about the behavior
of the gradient norm for the multi-batch L-BFGS method with a cautious update strategy
Theorem Suppose that Assumptions above hold and let be given Let wk be
the iterates generated by Algorithm with
starting from w0 and with the
modification that Hk whenever is not satisfied Then
Fb
k?F wk
This result bounds the average norm of the gradient of after the first iterations and shows
that the iterates spend increasingly more time in regions where the objective function has a small
gradient
Numerical Results
In this Section we present numerical results that evaluate the proposed robust multi-batch L-BFGS
scheme Algorithm on logistic regression problems Figure shows the performance on the
webspam dataset1 where we compare it against three methods multi-batch L-BFGS without
enforcing sample consistency L-BFGS where gradient differences are computed using different
samples yk
gkSk multi-batch gradient descent Gradient Descent which is
obtained by setting Hk I in Algorithm and iii serial SGD where at every iteration one
sample is used to compute the gradient We run each method with different random seeds and
where applicable report results for different batch and overlap sizes The proposed method
is more stable than the standard L-BFGS method this is especially noticeable when is small On
the other hand serial SGD achieves similar accuracy as the robust L-BFGS method and at a similar
rate at the cost of communications per epochs versus
communications per
epoch Figure also indicates that the robust L-BFGS method is not too sensitive to the size of
overlap Similar behavior was observed on other datasets in regimes where was not too small
We mention in passing that the L-BFGS step was computed using the a vector-free implementation
proposed in
webspam
Epochs
webspam
Epochs
webspam
Robust L?BFGS
L?BFGS
Gradient Descent
SGD
Epochs
webspam
Robust L?BFGS
L?BFGS
Gradient Descent
SGD
Robust L?BFGS
L?BFGS
Gradient Descent
SGD
k?F
k?F
k?F
Robust L?BFGS
L?BFGS
Gradient Descent
SGD
webspam
Robust L?BFGS
L?BFGS
Gradient Descent
SGD
k?F
k?F
webspam
Robust L?BFGS
L?BFGS
Gradient Descent
SGD
k?F
Epochs
Epochs
Epochs
Figure webspam dataset Comparison of Robust L-BFGS L-BFGS multi-batch L-BFGS without
enforcing sample consistency Gradient Descent multi-batch Gradient method and SGD for various
batch and overlap sizes Solid lines show average performance and dashed lines show worst
and best performance over runs per algorithm MPI processes
We also explore the performance of the robust multi-batch L-BFGS method in the presence of node
failures faults and compare it to the multi-batch variant that does not enforce sample consistency
L-BFGS Figure illustrates the performance of the methods on the webspam dataset for various
LIBSVM https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html
probabilities of node failures and suggests that the robust L-BFGS variant is
more stable
webspam
webspam
Robust L?BFGS
L?BFGS
webspam
Robust L?BFGS
L?BFGS
Robust L?BFGS
L?BFGS
k?F
k?F
k?F
Iterations/Epochs
Iterations/Epochs
Iterations/Epochs
Figure webspam dataset Comparison of Robust L-BFGS and L-BFGS multi-batch L-BFGS
without enforcing sample consistency for various node failure probabilities Solid lines show
average performance and dashed lines show worst and best performance over runs per algorithm
MPI processes
Lastly we study the strong and weak scaling properties of the robust L-BFGS method on artificial
data Figure We measure the time needed to compute a gradient Gradient and the associated
communication Gradient+C as well as the time needed to compute the L-BFGS direction LBFGS and the associated communication L-BFGS+C for various batch sizes The figure
on the left shows strong scaling of multi-batch LBFGS on a dimensional problem with
samples The size of input data is and we vary the number of MPI processes
The time it takes to compute the gradient decreases with however for small
values of the communication time exceeds the compute time The figure on the right shows weak
scaling on a problem of similar size but with varying sparsity Each sample has non-zero
elements thus for any the size of local problem is roughly for size of data
We observe almost constant time for the gradient computation while the cost of computing
the L-BFGS direction decreases with however if communication is considered the overall time
needed to compute the L-BFGS direction increases slightly
Strong Scaling
Elapsed Time
Gradient
Gradient+C
L?BFGS
L?BFGS+C
Elapsed Time
Number of MPI processes
Weak Scaling Fix problem dimensions
Gradient
Gradient+C
L?BFGS
L?BFGS+C
Number of MPI processes
Figure Strong and weak scaling of multi-batch L-BFGS method
Conclusion
This paper describes a novel variant of the L-BFGS method that is robust and efficient in two settings
The first occurs in the presence of node failures in a distributed computing implementation the second
arises when one wishes to employ a different batch at each iteration in order to accelerate learning
The proposed method avoids the pitfalls of using inconsistent gradient differences by performing
quasi-Newton updating based on the overlap between consecutive samples Numerical results show
that the method is efficient in practice and a convergence analysis illustrates its theoretical properties
Acknowledgements
The first two authors were supported by the Office of Naval Research award the
Department of Energy grant and the National Science Foundation grant
Martin Tak??c was supported by National Science Foundation grant

<<----------------------------------------------------------------------------------------------------------------------->>

title: 473-constructing-proofs-in-symmetric-networks.pdf

Constructing Proofs in Symmetric Networks
Gadi Pinkas
Computer Science Department
Washington University
Campus Box
St. Louis MO
Abstract
This paper considers the problem of expressing predicate calculus in connectionist networks that are based on energy minimization Given a firstorder-logic knowledge base and a bound a symmetric network is constructed like a Boltzman machine or a Hopfield network that searches
for a proof for a given query If a resolution-based proof of length no
longer than exists then the global minima of the energy function that
is associated with the network represent such proofs The network that
is generated is of size cubic in the bound and linear in the knowledge
size There are no restrictions on the type of logic formulas that can be
represented The network is inherently fault tolerant and can cope with
inconsistency and nonmonotonicity
Introduction
The ability to reason from acquired knowledge is undoubtedly one of the basic and
most important components of human intelligence Among the major tools for
reasoning in the area of AI are deductive proof techniques However traditional
methods are plagued by intractability inability to learn and adjust as well as by
inability to cope with noise and inconsistency A connectionist approach may be
the missing link fine grain massively parallel architecture may give us real-time
approximation networks are potentially trainable and adjustable and they may be
made tolerant to noise as a result of their collective computation
Most connectionist reasoning systems that implement parts of first-order logic
for examples Holldobler Shastri use the spreading activation
paradigm and usually trade expressiveness with time efficiency In contrast this
Pinkas
paper uses the energy minimization paradigm like Derthick Ballard and
Pinkas representing an intractable problem but trading time with correctness as more time is given the probability of converging to a correct answer
increases
Symmetric connectionist networks used for constraint satisfaction are the
target platform Hopfield Hinton Sejnowski peterson Hartman
Smolensky They are characterized by a quadratic energy function that should
be minimized Some of the models in the family may be seen as performing a search
for a global minimum of their energy function The task is therefore to represent
logic deduction that is bound by a finite proof length as energy minimization without a bound on the proof length the problem is undecidable When a query is
clamped the network should search for a proof that supports the query If a proof
to the query exists then every global minimum of the energy function associated
with the network represents a proof If no proof exists the global minima represent
the lack of a proof
The paper elaborates the propositional case however due to space limitations the
first-order FOL case is only sketched For more details and full treatment of FOL
see Pinkas
Representing proofs of propositional logic
I'll start by assuming that the knowledge base is propositional
The proof area
A proof is a list of clauses ending with the query such that every clause used is
either an original clause a copy weakening of a clause that appears earlier in
the proof or a result of a resolution step of the two clauses that appeared just
earlier The proof emerges as an activation pattern on special unit structures called
the proof area and is represented in reverse to the common practice the query
appears first For example given a knowledge base of the following clauses
A
vC
we would like to prove the query by generating the following list of clauses
A
AVCv
vC
obtained by resolution of clauses and by canceling A).
original clause no
obtained by resolution of clauses and by canceling C).
original clause no
obtained by resolution of clauses and by canceling B).
original clause no
original clause no
Each clause in the proof is either an original clause a copy of a clause from earlier
in the proof or a resolution step
The matrix in figure functions as a clause list This list represents an ordered
set of clauses that form the proof The query clauses are clamped onto this area
Constructing Proofs in Symmetric Networks
and activate hard constraints that force the rest of the units of the matrix to form
a valid proof if it exists
IN
A
AvD
CvD
AvCvD
Query
JJvD
AvBvC
A
@
@
RES KB O'Y
0@
l"igure The proof area for a propositional case
Variable binding is performed by dynamic allocation of instances using a technique
similar to Anand an and Barnden In this technique if two symbols
need to be bound together an instance is allocated from a pool of general purpose
instances and is connected to both symbols An instance can be connected to a
literal in a clause to a predicate type to a constant to a function or to a slot
of another instance for example a constant that is bound to the first slot of a
predicate
The clauses that participate in the proof are represented using a 3-dimensional
matrix and a 2-dimensional matrix as illustrated in figure The
rows of represent clauses of the proof while the rows of represent atomic
Pinkas
propositions The columns of both matrices represent the pool of instances used for
binding propositions to clauses
A clause is a list of negative and positive instances that represent literals The
instance thus behaves as a two-way pointer that binds composite structures like
clauses with their constituents the atomic propositions A row in the matrix
represents a clause which is composed of pairs of instances If the unit is
set then the matrix represents a positive literal in clause If PA,i is also set then
represents a positive literal of clause that is bound to the atomic proposition
A. Similarly represents a negative literal
The first row of matrix in the figure is the query clause D. It contains only one
positive literal that is bound to atomic proposition via instance For another
example consider the third row of the which represents a clause of two literals a
positive one that is bound to via instance and a negative one bound to A via
instance it is the clause generated as a result of a resolution step
Participation in the proof The vector IN represents whether clauses in
participate in the proof In our example all the clauses are in the proof however
in the general case some of the rows of may be meaningless When IN. is on it
means that the clause is in the proof and must be proved as well Every clause that
participates in the proof is either a result of a resolution step RES is set a copy
of a some clause CPYi is set or it is an original clause from the knowledge base
B. is set The second clause of in figure for example is an original clause
of the knowledge base If a clause is copied it must be in the proof itself and
therefore I Nj is set Similarly if clause is a result of a resolution step then the two
resolved clauses must also be in the proof I Ni+l,i and I and therefore must
be themselves resolvents copies or originals This chain of constraints continues
until all constraints are satisfied and a valid proof is generated
Posting a query The user posts a query clamping its clauses onto the first rows
of and setting the appropriate IN units This indicates that the query clauses
participate in the proof and should be proved by either a resolution step a copy
step or by an original clause Figure represents the complete proof for the query
We start by allocating an instance for in the matrix and clamping a
positive literal in the first row of the rest of the first row's units are
clamped zero The unit INl is biased to have the value of one indicating that
the query is in the proof this cause a chain of constraints to be activated that are
satisfied only by a valid proof If no proof exists the I Nl unit will become zero
the global minima is obtained by setting I Nl to zero despite the bias
Representing resolutions steps The vector RES is a structure of units that
indicates which are the clauses in that are obtained by a resolution step If RES
is set then the ith row is obtained by resolving row of with row
Thus the unit RESl in figure indicates that the clause of the first row of
is a resolvent of the second and the third rows of representing and
A respectfully Two literals cancel each other if they have opposite signs and are
represented by the same instance In figure literal A of the third row of and
literal of the second row cancel each other generating the clause of the first
row
The rows of matrix represent literals canceled by resolution steps If row of
Constructing Proofs in Symmetric Networks
is the result of a resolution step there must be one and only one instance such
that both clause and clause include it with opposite signs For example
figure clause in the first row of is the result of resolving clause A with
clause..,A which are in the second and third rows of respectfully Instance
representing atomic proposition A is the one that is canceled RI,I is set therefore
indicating that clause is obtained by a resolution step that cancels the literals of
instance
Copied and original clauses The matrix indicates which clauses are copied
to other clauses in the proof area Setting Di,i means that clause is obtained by
copying weakening clause into clause the example does not use copy steps
The matrix indicates which original knowledge-base clauses participate in the
proof The unit Ki,J indicates that a clause in the proof area is an original clause
and the syntax of the j-th clause in the knowledge base must be imposed on the
units of clause In figure for example clause in the proof the second row in
assumes the identity of clause number in the knowledge base and therefore
is set
Constraints
We are now ready to specify the constraints that must be satisfied by the units so
that a proof is found The constraints are specified as well formed logic formulas
For example the formula A imposes a constraint over the units
such that the only possible valid assignments to those units are
A general method to implement an arbitrary logical constraint on connectionist
networks is shown in Pinkas Most of the constraints specified in this section
are hard constraints must be satisfied for a valid proof to emerge Towards the
end of this section some soft constraints are presented
In-proof constraints If a clause participates in the proof it must be either a
result of a resolution step a copy step or an original clause In logic the constraints
may be expressed as Vi INi RESi CP'Yi Bi. The three units per clause
consist a winner takes all subnetwork This means that only one of the
three units is actually set The WTA constraints may be expressed as
RESi-..,CP'Yi Bi
CP'Yi--,RESi Bi
Bi--,RESi
The WTA property may be enforced by inhibitory connections between every pair
of the three units
Copy constraints If CPYi is set then clause must be a copy of another clause
in the proof This can be expressed as Vi P'Yi V.(Di,i I Ni The rows of
Dare WTAs allowing-i to be a copy of only one In addition if clause is copied
or weakened into clause then every unit set in clause must also be set in clause
This may be specified as Vi,j,l
Resolution constraints If a clause is a result of resolving the two clauses
and then there must be one and only one instance that is canceled
represented by and C. is obtained by copying both the instances of CHI and
without the instance These constraints may be expressed as
Pinkas
Vi RE Si Vi Ri,i
Vi
Vi RESi-INi+l
Vi RE
Vi RESi
at least one instance is canceled
only one instance is canceled WT.t
cancel literals with opposite signs
the two resolvents are also in proof
copy positive literals
copy negative literals
Clause-instance constraints The sign of an instance in a clause should be
unique therefore any instance pair in the matrix Cis WTA Vi
The columns of matrix are WTAs since an instance is allowed to represent only
one atomic prop06ition VA A The rows of may be also
WTAs VA,i,j:f PA,i-"'PA,j this constraint is not imposed in the FOL case
Knowledge base constraints If a clause is an original knowledge base clause
then there must be a clause out of the original clauses whose syntax is forced
upon the units of the i-th row of matrix C. This constraint can be expressed as
Vi Bi Vj Ki,i The rows of are WTA networks so that only one original
clause is forced on the units of clause Vi
The only hard constraints that are left are those that force the syntax of a particular
clause from the knowledge base Assume for example that is set meaning that
clause in must have the syntax of the fourth clause in the knowledge base of our
example D). Instances and must be allocated to the atomic propositions
and respectfully and must appear also in clause as the literals C-,iJ and
The following constraints capture the syntax of
Vi iJ PC,i
Vi
there exists a negative literal that is bound to
there exists a positive literal that is bound to D.
FOL extension
In first-order predicate logic FOL instead of atomic propositions we must deal
with predicates pinkas for details As in the propositional case a literal
in a clause is represented by a positive or negative instance however the instance
must be allocated now to a predicate name and may have slots to be filled by other
instances representing functions and constants To accommodate such complexity
a new matrix NEST is added and the role of matrix is revised
The matrix must accommodate now function names predicate names and constant names instead of just atomic propositions Each row of represents a name
and the columns represent instances that are allocated to those names The rows
of that are associated with predicates and functions may contain several different instances of the same predicate or function thus they are not WTA anymore
In order to represent compound terms and predicates instances may be bound to
slots of other instances The new matrix ESn,i,p is capable of representing
such bindings If ESn,i,p is set then instance is bound to the slot of instance
The columns of NEST are WTA allowing only one instance to be bound to
a certain slot of another instance When a clause is forced to have the syntax
of some original clause I syntactic constraints are triggered so that the literals of
clause become instantiated by the relevant predicates functions constants and
variables imposed by clause I.
Constructing Proofs in Symmetric Networks
Unification is implicitly obtained if two predicates are representing by the same
instance while still satisfying all the constraints imposed by the syntax of the
two clauses When a resolution step is needed the network tries to allocate the
same instance to the two literals that need to cancel each other If the syntactic
constraints on the literals permit such sharing of an instance then the attempt
to share the instance is successful and a unification occurs occur check is done
implicitly since the matrix NEST allows the only finite trees to be represented
Minimizing the violation of soft constraints Among the valid proofs some
are preferable to others By means of soft constraints and optimization it is possible
to encourage the network to search for preferred proofs Theorem-proving thus is
viewed as a constraint optimization problem A weight may be assigned to each
of the constraints Pinkas and the network tries to minimize the weighted sum
of the violated constraints so that the set of the optimized solutions is exactly the
set of the preferred proofs For example preference of proofs with most general
unification is obtained by assignment of small penalties negative bias to every
binding of a function to a position of another instance NEST Using similar
techniques the network can be made to prefer shorter more parsimonious or more
reliable proofs low-cost plans or even more specific arguments as in nonmonotonic
reasonmg
Summary
Given a finite set of clauses where is the number of different predicates
functions and constants and given also a bound over the proof length we can
generate a network that searches for a proof with length not longer then for
a clamped query Q. If a global minimum is found then an answer is given as to
whether there exists such a proof and the proof with MGU's may be extracted
from the state of the visible units Among the possible valid proofs the system
prefers some better proofs by minimizing the violation of soft constraints The
concept of better proofs may apply to applications like planning minimize the
cost abduction parsimony and nonmonotonic reasoning specificity
In the propositional case the generated network is of km kn units and
km kn connections For predica;te logic there are km kn units
and connections and we need to add Pm connections and hidden units where
is the complexity-level of the syntactic constraints Pinkas
The results improve an earlier approach Ballard There are no restrictions on
the rules allowed every proof no longer than the bound is allowed the network
is compact and the representation of bindings unifications is efficient nesting of
functions and multiple uses of rules are allowed only one relaxation phase is needed
inconsistency is allowed in the knowledge base and the query does not need to be
negated and pre-wired it can be clamped during query time
The architecture discussed has a natural fault-tolerance capability When a unit
becomes faulty it simply cannot assume a role in the proof and other units are
allocated instead
Acknowledgment I wish to thank Dana Ballard Bill Ball Rina Dechter
Peter Had dawy Dan Kimura Stan Kwasny Ron Loui and Dave Touretzky for
Pinkas
helpful conunents

<<----------------------------------------------------------------------------------------------------------------------->>

title: 237-a-reconfigurable-analog-vlsi-neural-network-chip.pdf

Satyanarayana Tsividis and Graf
A Reconfigurable Analog VLSI Neural Network
Chip
Srinagesh Satyanarayana and Yannis Tsividis
Department of Electrical Engineering
and
Center for Telecommunications Research
Columbia University New York NY USA
Hans Peter Graf
AT&T
Bell Laboratories
Holmdel NJ
USA
ABSTRACT
distributed-neuron synapses have been integrated in an active
area of using a double-metal single-poly
n-well CMOS technology The distributed-neuron synapses are arranged in blocks of which we call tiles Switch matrices
are interleaved between each of these tiles to provide programmability of interconnections With a small area overhead the
units of the network can be rearranged in various configurations Some of the possible configurations are a network
a network two networks etc the numbers separated by dashes indicate the number of units per layer including
the input layer Weights are stored in analog form on MaS capacitors The synaptic weights are usable to a resolution of of
their full scale value The limitation arises due to charge injection
from the access switch and charge leakage Other parameters like
gain and shape of nonlinearity are also programmable
Introduction
A wide variety of ptoblems can be solved by using the neural network framework
However each of these problems requires a different topology and weight set
At a much lower system level the performance of the network can be improved
by selecting suitable neuron gains and saturation levels Hardware realizations of
A Reconfigurable Analog VLSI Neural Network Chip
hidcMn
NtUYOftS
inputs
inputs
Inputs
Figure Reconfigurability
neural networks provide a fast means of solving the problem We have chosen
analog circuits to implement neural networks because they provide high synapse
density and high computational speed In order to provide a general purpose hardware for solving a wide variety of problems that can be mapped into the neural
network framework it is necessary to make the topology weights and other neurosynaptic parameters programmable Weight programmability has been extensively
dealt in several implementations However features like programmable topology neuron gains and saturation levels have not been addressed extensively We
have designed fabricated and tested an analog VLSI neural network in which the
topology weights and neuron gains and saturations levels are all programmable
Since the process of design fabrication and testing is time-consuming and expensive
redesigning the hardware for each application is inefficient Since the field of neural
networks is still in its infancy new solutions to problems are being searched for
everyday These involve modifying the topology and finding the best weight
set In such an environment a computational tool that is fully programmable is
very desirable
The Concept of Reconfigurability
We define reconfigurabilityas the ability to alter the topology the number oflayers
number of neurons per layer interconnections from layer to layer and interconnections within a layer of the network The topology of a network does not describe
the value of each synaptic weight It only specifies the presence or absence of a
synapse between two neurons However in the special case of binary weight
defining the topology specifies the weight The ability to alter the synaptic weight
can be defined as weight programmability Figure illustrates reconfigurability
whereas Figure shows how the weight value is realized in our implementation
The Voltage Vw across the capacitor represents the synaptic weight Altering this
voltage makes weight programmability possible
Why is On-Chip Reconfigurability Important
Synapses neurons and interconnections occupy real estate on a chip Chip sizes
are limited due to various factors like yield and cost Hence only a limited number
Satyanarayana Tsividis and Graf
Figure Weight programmability
of synapses can be integrated in a given chip area Currently the most compact
realizations considering more than bits of synaptic accuracy permit us to integrate only a few thousand synapses per cm In such a situation every zero-valued
inactive synapse represents wasted area and decreases the computational ability
per unit area of the chip If a fixed topology network is used for different problems
it will be underutilized as long as some synapses are set to zero value On the
other hand if the network is reconfigurable the limited resources on-chip can be
reallocated to build networks with different topologies more efficiently For example
the network with topology-2 of Figure requires synapses If the network was
reconfigurable we could utilize these synapses to build a two-layer network with
synapses in the first layer and in the second layer In a similar fashion we could
also build the network with topology-3 which is a network with localized receptive
fields
The Distributed-Neuron Concept
In order to provide reconfigurability on-chip we have developed a new cell called the
distributed-neuron synapse In addition to making reconfiguration easy it has
other advantages like being modular hence making design easy provides automatic
gain scaling avoids large current build-up at any point and makes possible a fault
tolerant system
Figure shows a lumped neuron with synaptic inputs We call it lumped
because the circuit that provides the nonlinear function is lumped into one block
Yout
Figure A lumped neuron with synaptic inputs
A Recontigurable Analog VLSI Neural Network Chip
The synapses are assumed to be voltage-to-current transconductor cells and the
neuron is assumed to be a current-to-voltage cell Summation is achieved through
addition of the synapse output currents in the parallel connection
Figure shows the equivalent distributed-neuron with synaptic inputs It is
called distributed because the circuit that functions as the neuron is split into
parts One of these parts is integrated with each synapse This new block that
contains a a synapse and a fraction of the neuron is called the distributed-neuron
synapse Details of the distributed-neuron concept are described in It has to
be noted that the splitting of the neuron to form the distributed-neuron synapse
is done at the summation point where the computation is linear Hence the two
realizations of the neuron are computationally equivalent However the distributedneuron implementation offers a number of advantages as is now explained
Yout
Distribut.d
N.uron
Disiribui.d-n.uron
s~nllps
Figure A distributed-neuron with synaptic inputs
Modularity of the design
As is obvious from Figure the task of building a complete network involves designing one single distributed-neuron synapse module and interconnecting several of
them to form the whole system Though at a circuit level a fraction of the neuron
has to be integrated with each synapse the system level design is simplified due to
the modularity
Automatic gain normalization
In the distributed-neuron each unit of the neuron serves as a load to the output
of a synapse As the number of synapses at the input of a neuron increases the
number of neuron elements also increases by the same number The neuron output
is given by
Yj
WijXi
Where Yj is the output of the ph neuron Wij is the weight from the ith synaptic
input and 8j is the threshold implemented by connecting in parallel an appropriate number of distributed-neuron synapses with fixed inputs Assume for the
Satyanarayana Tsividis and Graf
Distri buted neuron
synapse
Figure Switches used for reconfiguration in the distributed-neuron implementation
moment that all the inputs are at a maximum possible value Then it is easily
seen that Yj is independent of This is the manifestation of the automatic gain
normalization that is inherent to the idea of distributed-neuron synapses
Ease of reconfiguration
In a distributed-neuron implementation reconfiguration involves interconnecting a
set of distributed-neuron synapse modules Figure A neuron of the right size
gets formed when the outputs of the required number of synapses are connected
In a lumped neuron implementation reconfiguration involves interconnecting a set
of synapses with a set of neurons This involves more wiring switches and logic
control blocks
A voiding large current build-up in the neuron
In our implementation the synaptic outputs are currents These currents are summed
by Kirchoffs current law and sent to the neuron Since the neuron is distributed the
total current is divided into equal parts where is the number of distributedneuron synapses One of these part flows through each unit of the distributed neuron
as illustrated in Figure This obviates the need for large current summation wires
and avoids other problems associated with large currents at any single point
Fault tolerance
On a VLSI chip defects are commonly seen Some of these defects can short wires
hence corrupting the signals that are carried on them Defects can also render some
synapses and neurons defective In our implementation we have integrated switches
in-between groups of distributed-neuron synapses which we call tiles to make the
chip reconfigurable Figure This makes each tile of the chip externally testable
The defective sections of the chip can be isolated and the remaining synapses can
thus be reconfigured into another topology as shown in Figure
Circuit Description of the Distributed-Neuron Synapse
Figure shows a distributed-neuron synapse constructed around a differential-input
differential-output transconductance multiplier A weight converter is used to con
A Reconfigurable Analog VLSI Neural Network Chip
Figure Improved fault tolerance in the distributed-neuron system
Figure The distributed-neuron synapse circuit
vert the single-ended weight controlling voltage Vw into a set of differential currents
that serve as the bias currents of the multiplier The weight is stored on aMOS
capacitor
The differential nature of the circuit offers several advantages like improved rejection
of power supply noise and linearity of multiplication Common-mode feedback is
provided at the output of the synapse An amplitude limiter that is operational
only when the weighted sum exceeds a certain range serves as the distributed-neuron
part The saturation levels of the neuron can be programmed by adjusting VN1 and
VN Gains can be set by adjusting the bias current IB and/or a load not shown
The measured synapse characteristics are shown in Figure
Satyanarayana Tsividis and Graf
If
a
III
Difterential Input
wt I FS wt FS wt O.OIFS
wt FS wt FS
Individual curVElS are for different
eIght values I FS Full Scale I
Figure Measured characteristics of the distributed-neuron synapse
Distributed-neuron synapse
output
wlr
i$a$l:a:ml:ttn
SYMBOLIC
DIAGRAM
ACTUAL ON-CHIP
WIRING OF A
DODD
DODO
TILE
horizontal
swi tch matri
DODD
DODD
DODO
DODD
DODO
DODO
DODO
DODO
DODO
SYNAPSES
IN GROUPS OF
Figure Organization of the distributed-neurons and switches on chip
A Reconfigurable Analog VLSI Neural Network Chip
Organization of the Chip
Figure shows how the distributed-neuron synapses are arranged on-chip
distributed-neuron synapses have been arranged in a crossbar fashion to
form a 4-input-4-output network We call this a tile Input and output
wires are available on all four sides of the tile This makes interconnections to adjacent blocks easy Vertical and horizontal switch matrices are interleaved in-between
the tiles to select one of the various possible modes of interconnections These
modes can be configured by setting the bits of memory in each switch matrix
distributed-neuron synapses have been integrated in an active area of
using a double-metal single-poly n-well CMOS technology
The Weight Update/Refresh Scheme
Weights are stored in analog form on a MOS capacitor A semi-serial-parallel weight
update scheme has been built pins of the chip are used to distribute the weights
to the capacitors on the chip Each pin can refresh capacitors contained
in a row of tiles The capacitors in each tile-row are selected one at a time by a
decoder The maximum refresh speed depends on the time needed to charge up the
weight storage capacitor and the parasitic capacitances One complete refresh of all
weights on the chip is possible in about J.l seconds However one could refresh
at a much slower rate the lower limit of which is decided by the charge leakage For
a 7-bit precision in the weight at room temperature a refresh rate in the order of
milliseconds should be adequate Charge injection due to the parasitic capacitances
has been kept low by using very small switches In the first version of the chip
only the distributed-neuron synapses the switches used for reconfiguration and
the topology memory have been integrated Weights are stored outside the chip in
digital form in a 1K RAM. The contents of the RAM are continuously read and
converted into analog form using a bank of off-chip A converters An advantage
of our scheme is that the forward-pass operation is not interrupted by the weight
refresh mechanism A fast weight update scheme of the type used here is very
desirable while executing learning algorithms at a high speed The complete block
diagram of the weight refresh/update and testing scheme is shown in Figure
Configuration Examples
In Figure we show some of the network topologies that can be configured with
the resources available on the chip The left-hand side of the figure shows the
actual wiring on the chip and the right-hand side shows the symbolic diagram of
the network configuration The darkened tiles have been used for implementing
the thresholds Several other topologies like feedback networks and networks with
localized receptive fields can be configured with this chip
The complete system
Figure shows how the neural network chip fits into a complete system that is
necessary for its use and testing The Config-EPROM stores the bit pattern corre
Satyanarayana Tsividis and Graf
SIngle-ended to dUferenltl1
conYerter
WeIght
RAn
Neural
Networll
Conflg
EPRon
Figure Block diagram of the system for reconfiguration weight update/refresh
and testing
sponding to the desired topology This bit pattern is down-loaded into the memory
cells of the switch matrices before the start of computation Input vectors are read
out from the Data memory and converted into analog form by D/A converters
The outputs of the A converters are further transformed into differential signals and then fed into the chip The chip delivers differential outputs which are
converted into digital form using an A/D converter and stored in a computer for
further analysis
The delay in processing one layer with inputs driving another layer with an
equal number of inputs is typically 1J.lsec Hence a network should take
about 6J.lsecs for one forward-pass operation However external loads can slow down
the computation considerably This problem can be solved by increasing the bias
currents or/and using pad buffers Each block on the chip has been tested and has
been found to function as expected Tests of the complete chip in a variety of neural
network configurations are being planned
Conclusions
We have designed a reconfigurable array of distributed-neuron synapses that
can be configured into several different types of neural networks The distributedneuron concept that is integral to this chip offers advantages in terms of modularity
and automatic gain normalization The chip can be cascaded with several other
chips of the same type to build larger systems

<<----------------------------------------------------------------------------------------------------------------------->>

title: 516-neural-network-routing-for-random-multistage-interconnection-networks.pdf

Neural Network Routing for Random Multistage
Interconnection Networks
Mark W. Goudreau
Princeton University
and
NEe Research Institute Inc.
Independence Way
Princeton NJ
Lee Giles
NEC Research Institute Inc.
Independence Way
Princeton NJ
Abstract
A routing scheme that uses a neural network has been developed that can
aid in establishing point-to-point communication routes through multistage interconnection networks MINs The neural network is a network
of the type that was examined by Hopfield Hopfield and
In this work the problem of establishing routes through random MINs
RMINs in a shared-memory distributed computing system is addressed
The performance of the neural network routing scheme is compared to two
more traditional approaches exhaustive search routing and greedy routing The results suggest that a neural network router may be competitive
for certain RMIN
INTRODUCTION
A neural network has been developed that can aid in establishing point-topoint communication routes through multistage interconnection networks MINs
Goudreau and Giles Such interconnection networks have been widely studied Huang Siegel The routing problem is of great interest due to
its broad applicability Although the neural network routing scheme can accommodate many types of communication systems this work concentrates on its use in a
shared-memory distributed computing system
Neural networks have sometimes been used to solve certain interconnection network
Neural Network Routing for Random Multistage Interconnection Networks
Input
Ports
Output
Ports
Interconnection
Network
Control Bits
Logic1
I
Neural
Network
Interconnection
Logic2
Network
I
Controller
I
Externa Control
Figure The communication system with a neural network router The input
ports processors are on the left while the output ports memory modules are on
the right
problems such as finding legal routes Brown Hakim and Meadows
and increasing the throughput of an interconnection network Brown and Liu
Marrakchi and Troudet The neural network router that is the subject of
this work however differs significantly from these other routers and is specially
designed to handle parallel processing systems that have MINs with random interstage connections Such random MINs are called RMINs RMINs tend to have
greater fault-tolerance than regular MINs
The problem is to allow a set of processors to access a set of memory modules
through the RMIN A picture of the communication system with the neural network
router is shown in Figure The are processors and memory modules The
system is assumed to be synchronous At the beginning of a message cycle some
set of processors may desire to access some set of memory modules It is the
job of the router to establish as many of these desired connections as possible in
a non-conflicting manner Obtaining the optimal solution is not critical Stymied
processors may attempt communication again during the subsequent message cycle
It is the combination of speed and the quality of the solution that is important
The object of this work was to discover if the neural network router could be competitive with other types of routers in terms of quality of solution speed and resource
Goudreau and Giles
RMIN2
RMINI
RMIN3
Figure Three random multistage interconnection networks The blocks that are
shown are crossbar switches for which each input may be connected to each output
utilization To this end the neural
other schemes for routing in RMINs
routing So far the results of this
router may indeed be a practicable
too large
network routing scheme was compared to two
namely exhaustive search routing and greedy
investigation suggest that the neural network
alternative for routing in RMINs that are not
EXHAUSTIVE SEARCH ROUTING
The exhaustive search routing method is optimal in terms of the ability of the router
to find the best solution There are many ways to implement such a router One
approach is described here
For a given interconnection network every route from each input to each output
was stored in a database The RMIN that were used as test cases in this paper
always had at least one route from each processor to each memory module When
a new message cycle began and a new message set was presented to the router
the router would search through the database for a combination of routes for the
message set that had no conflicts A conflict was said to occur if more than one
route in the set of routes used a single bus in the interconnection network In the
case where every combination of routes for the message set had a conflict the router
would find a combination of routes that could establish the largest possible number
of desired connections
If there are possible routes for each message this algorithm needs a memory of
size mnk and in the worst case takes exponential time with respect to the size
Neural Network Routing for Random Multistage Interconnection Networks
of the message set Consequently it is an impractical approach for most RMINs
but it provides a convenient upper bound for the performance of other routers
GREEDY ROUTING
When greedy routing is applied message connections are established one at a time
Once a route is established in a given message cycle it may not be removed Greedy
routing does not always provide the optimal routing solution
The greedy routing algorithm that was used required the same route database as
the exhaustive search router did However it selects a combination of routes in
the following manner When a new message set is present the router chooses one
desired message and looks at the first route on that message's list of routes The
router then establishes that route Next the router examines a second message
assuming a second desired message was requested and sees if one of the routes
in the second message's route list can be established without conflicting with the
already established first message If such a route does exist the router establishes
that route and moves on to the next desired message
In the worst case the speed of the greedy router is quadratic with respect to the
size of the message set
NEURAL NETWORK ROUTING
The focal point of the neural network router is a neural network of the type that
was examined by Hopfield Hopfield and The problem of establishing
a set of non-conflicting routes can be reduced to a constraint satisfaction problem
The structure of the neural network router is completely determined by the RMIN
When a new set of routes is desired only certain bias currents in the network change
The neural network routing scheme also has certain fault-tolerant properties that
will not be described here
The neural network calculates the routes by converging to a legal routing array A
legal routing array is 3-dimensional Therefore each element of the routing array
will have three indices If element ai,i,k is equal to then message is routed
through output port of stage We say and are in the same row if
I and They are in the same column if I and Finally they are
in the same rod if and
A legal routing array will satisfy the following three constraints
one and only one element in each column is equal to
the elements in successive columns that are equal to represent output ports
that can be connected in the interconnection network
no more than one element in each rod is equal to
The first restriction ensures that each message will be routed through one and
only one output port at each stage of the interconnection network The second
restriction ensures that each message will be routed through a legal path in the
Goudreau and Giles
interconnection network The third restriction ensures that any resource contention
in the interconnection network is resolved In other words only one message can
use a certain output port at a certain stage in the interconnection network When
all three of these constraints are met the routing array will provide a legal route
for each message in the message set
Like the routing array the neural network router will naturally have a 3-dimensional
structure Each ai,j,k of a routing array is represented by the output voltage of a
neuron At the beginning of a message cycle the neurons have a random
output voltage If the neural network settles in one of the global minima the
problem will have been solved
A continuous time mode network was chosen It was simulated digitally The neural
network has neurons The input to neuron is Ui its input bias current is Ii and
its output is Vi. The input Ui is converted to the output Vi by a sigmoid function
Neuron influences neuron by a connection represented by Similarly
neuron affects neuron through connection Iij. In order for the Liapunov function
Equation to be constructed Iij must equal7ji We further assume that Iii
For the synchronous updating model there is also a time constant denoted by T.
The equations which describe the output of a neuron are
duo
LN T...
dt
T=RC
g(Uj
e-X
The equations above force the neural net into stable states that are the local minima
of this approximate energy equation
iNN
2L
Iij Vi V'i Ii
i=l
For the neural network the weights Iii's are set as are the bias currents
It is the output voltages that vary to to minimize E.
Let be the number of messages in a message set let be the number of stages
in the RMIN and let be the number of ports per stage may be a function
of the stage number Below are the energy functions that implement the three
constraints discussed above
A
E1
E2
p=l
Vm,I,p
Neural Network Routing for Random Multistage Interconnection Networks
Ea
S-l
m=l
p=l
tt
i=l
Vm",i
JIm j)Vm,IJ Pm Vm,S IJ
A and are arbitrary positive constants El and Ea handle the first
constraint in the routing array E4 deals with the second constraint E2 ensures the
third From the equation for the function d(sl,pl,p2 represents the distance
between output port pI from stage sl and output port p2 from stage If pI
can connect to p2 through stage sl then this distance may be set to zero If pI
and p2 are not connected through stage sl then the distance may be set to one
Also am is the source address of message while f3m is the destination address
of message
The entire energy function is
Solving for the connection and bias current values as shown in Equation results
in the following equations
Oml,m2
1m am,p
is a Kronecker delta when and otherwise
Essentially this approach is promising because the neural network is acting as a
parallel computer The hope is that the neural network will generate solutions much
faster than conventional approaches for routing in RMINs
The neural network that is used here has the standard problem namely a global
minimum is not always reached But this is not a serious difficulty Typically
when the globally minimal energy is not reached by the neural network some of
the desired routes will have been calculated while others will not have Even a
locally minimal solution may partially solve the routing problem Consequently
this would seem to be a particularly encouraging type of application for this type
of neural network For this application the traditional problem of not reaching
the global minimum may not hurt the system's performance very much while the
expected speed of the neural network in calculating the solution will be a great
asset
IFor the simulations A
and were chosen empirically
and
These values for A
Goudreau and Giles
Table Routing results for the RMINs shown in Figure The
calculated due to their computational complexity
RMIN1
RMIN2
entries were not
RMIN3
Eel
Egr
Enn
Eel
Egr
Enn
Eel
Egr
Enn
The neural network router uses a large number of neurons If there are input
ports and output ports for each stage of the RMIN an upper bound on the
number of neurons needed is S. Often however the number of neurons actually
required is much smaller than this upper bound
It has been shown empirically that neural networks of the type used here can con
verge to a solution in essentially constant time For example this claim is made for
the neural network described in Takefuji and Lee which is a slight variation
of the model used here
SIMULATION RESULTS
Figure shows three RMINs that were examined The routing results for the three
routing schemes are shown in Table Eel represents the expected number of
messages to be routed using exhaustive search routing Egr is for greedy routing
while Enn is for neural network routing These values are functions of the size
of the message set M. Only message sets that did not have obvious conflicts
were examined For example no message set could have two processors trying to
communicate to the same memory module The table shows that for at least these
three RMINs the three routing schemes produce solutions that are of similar virtue
In some cases the neural network router appears to outperform the supposedly
optimal exhaustive search router That is because the Eel and Egr values were
calculated by testing every message set of size while Enn was calculated by
testing randomly generated message sets of size M. For the neural network
router to appear to perform best it must have gotten message sets that were easier
to route than average
In general the performance of the neural network router degenerates as the size of
the RMIN increases It is felt that the neural network router in its present form will
not scale well for large RMINs This is because other work has shown that large
neural networks of the type used here have difficulty converging to a valid solution
Hopfield
Neural Network Routing for Random Multistage Interconnection Networks
CONCLUSIONS
The results show that there is not much difference in terms of quality of solution for
the three routing methodologies working on these relatively small sample RMINs
The exhaustive search approach is clearly not a practical approach since it is too
time consuming But when considering the asymptotic analyses for these three
methodologies one should keep in mind the performance degradation of the greedy
router and the neural network router as the size of the RMIN increases
Greedy routing and neural network routing would appear to be valid approaches
for RMINs of moderate size But since asymptotic analysis has a very limited
significance here the best way to compare the speeds of these two routing schemes
would be to build actual implementations
Since the neural network router essentially calculates the routes in parallel it can
reasonably be hoped that a fast analog implementation for the neural network
router may find solutions faster than the exhaustive search router and even the
greedy router Thus the neural network router may be a viable alternative for
RMIN that are not too large

<<----------------------------------------------------------------------------------------------------------------------->>

