query sentence: backpropagated error in neural-network
---------------------------------------------------------------------
title: 5865-efficient-exact-gradient-update-for-training-deep-networks-with-very-large-sparse-targets.pdf

Efficient Exact Gradient Update for training Deep
Networks with Very Large Sparse Targets
Pascal Vincent? , Alexandre de Br?bisson, Xavier Bouthillier
D?partement d?Informatique et de Recherche Op?rationnelle
Universit? de Montr?al, Montr?al, Qu?bec, CANADA
?
and CIFAR

Abstract
An important class of problems involves training deep neural networks with sparse
prediction targets of very high dimension D. These occur naturally in e.g. neural
language models or the learning of word-embeddings, often posed as predicting
the probability of next words among a vocabulary of size D (e.g. 200 000). Computing the equally large, but typically non-sparse D-dimensional output vector
from a last hidden layer of reasonable dimension d (e.g. 500) incurs a prohibitive
O(Dd) computational cost for each example, as does updating the D ? d output
weight matrix and computing the gradient needed for backpropagation to previous
layers. While efficient handling of large sparse network inputs is trivial, the case
of large sparse targets is not, and has thus so far been sidestepped with approximate alternatives such as hierarchical softmax or sampling-based approximations
during training. In this work we develop an original algorithmic approach which,
for a family of loss functions that includes squared error and spherical softmax,
can compute the exact loss, gradient update for the output weights, and gradient for backpropagation, all in O(d2 ) per example instead of O(Dd), remarkably
without ever computing the D-dimensional output. The proposed algorithm yields
D
a speedup of 4d
, i.e. two orders of magnitude for typical sizes, for that critical part
of the computations that often dominates the training time in this kind of network
architecture.

1

Introduction

Many modern applications of neural networks have to deal with data represented, or representable,
as very large sparse vectors. Such representations arise in natural language related tasks, where
the dimension D of that vector is typically (a multiple of) the size of the vocabulary, and also in
the sparse user-item matrices of collaborative-filtering applications. It is trivial to handle very large
sparse inputs to a neural network in a computationally efficient manner: the forward propagation
and update to the input weight matrix after backpropagation are correspondingly sparse. By contrast, training with very large sparse prediction targets is problematic: even if the target is sparse, the
computation of the equally large network output and the corresponding gradient update to the huge
output weight matrix are not sparse and thus computationally prohibitive. This has been a practical
problem ever since Bengio et al. [1] first proposed using a neural network for learning a language
model, in which case the computed output vector represents the probability of the next word and
is the size of the considered vocabulary, which is becoming increasingly large in modern applications [2]. Several approaches have been proposed to attempt to address this difficulty essentially by
sidestepping it. They fall in two categories:
? Sampling or selection based approximations consider and compute only a tiny fraction of the
output?s dimensions sampled at random or heuristically chosen. The reconstruction sampling of
Dauphin et al. [3], the efficient use of biased importance sampling in Jean et al. [4], the use of
1

Noise Contrastive Estimation [5] in Mnih and Kavukcuoglu [6] and Mikolov et al. [7] all fall
under this category. As does the more recent use of approximate Maximum Inner Product Search
based on Locality Sensitive Hashing techniques[8, 9] to select a good candidate subset.
? Hierarchical softmax [10, 7] imposes a heuristically defined hierarchical tree structure for the
computation of the normalized probability of the target class.
Compared to the initial problem of considering all D output dimensions, both kinds of approaches
are crude approximations. In the present work, we will instead investigate a way to actually perform
the exact gradient update that corresponds to considering all D outputs, but do so implicitly, in a
computationally efficient manner, without actually computing the D outputs. This approach works
for a relatively restricted class of loss functions, the simplest of which is linear output with squared
error (a natural choice for sparse real-valued regression targets). The most common choice for
multiclass classification, the softmax loss is not part of that family, but we may use an alternative
spherical softmax, which will also yield normalized class probabilities. For simplicity and clarity,
our presentation will focus on squared error and on an online setting. We will briefly discuss its
extension to minibatches and to the class of possible loss functions in sections 3.5 and 3.6.

2
2.1

The problem
Problem definition and setup

We are concerned with gradient-descent based training of a deep feed-forward neural network with
target vectors of very high dimension D (e.g. D = 200 000) but that are sparse, i.e. a comparatively
small number, at most K  D, of the elements of the target vector are non-zero. Such a Ksparse vector will typically be stored and represented compactly as 2K numbers corresponding
to pairs (index, value). A network to be trained with such targets will naturally have an equally
large output layer of dimension D. We can also optionally allow the input to the network to be a
similarly high dimensional sparse vector of dimension Din . Between the large sparse target, output,
and (optionally large sparse) input, we suppose the network?s intermediate hidden layers to be of
smaller, more typically manageable, dimension d  D (e.g. d = 500)1 .
Mathematical notation: Vectors are denoted using lower-case letters, e.g. h, and are considered
column-vectors; corresponding row vectors are denoted with a transpose, e.g. hT . Matrices are
denoted using upper-case letters, e.g. W , with W T the transpose of W . The ith column of W is
T
denoted Wi , and its ith row W:i (both viewed as a column vector). U ?T = U ?1 denotes the
transpose of the inverse of a square matrix. Id is the d ? d identity matrix.

Network architecture: We consider a standard feed forward neural network architecture as depicted in Figure 1. An input vector x ? RDin is linearly transformed into a linear activation
a(1) = W (1)T x + b(1) through a Din ? d input weight matrix W (1) (and an optional bias vector
b(1) ? Rd ). This is typically followed by a non-linear transformation s to yield the representation of
the first hidden layer h(1) = s(a(1) ). This first hidden layer representation is then similarly transformed through a number of subsequent non-linear layers (that can be of any usual kind amenable to
backpropagation) e.g. h(k) = s(a(k) ) with a(k) = W (k)T h(k?1) + b(k) until we obtain last hidden
layer representation h = h(m) . We then obtain the final D-dimensional network output as o = W h
where W is a D ? d output weight matrix, which will be our main focus in this work. Finally, the
network?s D-dimensional output o is compared to the D-dimensional target vector y associated with
input x using squared error, yielding loss L = ko ? yk2 .

Training procedure: This architecture is a typical (possibly deep) multi-layer feed forward neural
network architecture with a linear output layer and squared error loss. Its parameters (weight matrices and bias vectors) will be trained by gradient descent, using gradient backpropagation [11, 12, 13]
to efficiently compute the gradients. The procedure is shown in Figure 1. Given an example from
the training set as an (input,target) pair (x, y), a pass of forward propagation proceeds as outlined above, computing the hidden representation of each hidden layer in turn based on the previous one, and finally the network?s predicted output o and associated loss L. A pass of gradient
backpropagation then works in the opposite direction, starting from ?o = ?L
?o = 2(o ? y) and

1
Our approach does not impose any restriction on the architecture nor size of the hidden layers, as long as
they are amenable to usual gradient backpropagation.

2

? Training deep neural networks with very large sparse targets is an important problem
? Arises e.g. in Neural Language Models [1] with large vocabulary size (e.g. D = 500 000 one-hot target).
? Efficient handling of large sparse inputs is trivial.
? But backprop training with large sparse targets is prohibitively expensive.
? Focus on output layer: maps last hidden representation h of reasonable dimension d (e.g. 500)
to very large output o of dimension D (e.g. 500 000) with a Dxd parameter matrix W:

Problem: expensive computation
Output o

o = Wh

O(Dd)

Backpropagation

(small d)

O(Kd)

Ex: d = 500

W(1)
(D d)
x

Input x

b) W ? W + 2?yh

We will now see how
only U and V respe
date
versions
Q=
5.3
E?cient
with
Q
= W T Wof gra
hSupposing
in Equations
we ????
hav
The for
gradient
the
squ
updateof
b)
).will
2
d?d matrix
(we

h?y?
?L
W? is ?W
= ??W?W
h = Qh
haswould
a comp
update
to
W
b
Solution:
representation
ofSeco
y,co
learning
rate. Again,
vector,
so that
a) Unew
=U
?comp
2?(
computational
complex
O(K).
So the all
overal
then
to
update
the
b) Vnew
=
V
+
2?y
O(Kd
+ d2 ) = To
O((K
N
will
be sparse).
ove
of magnitude
cheape
(a
Proof:

If we define inter
computation of L caW

Vnew Un

...
W(2)(d d)
x

?

Forward propagation

O(Dd)

!h = W T !o

O(d2)

hidden 1

we suppose K << d << D

Prohibitivley expensive!

O(d2) O(d2)

hidden 2
(small d)

O(D)
!o = 2(o-y)

...

h

(small d)

O(D)

large D, not sparse

O(Dd)
W "W- ! !o hT

last hidden

Loss L = ?o ? y?2

a) W ? W ? 2?W

Altogether: O(3Dd )

Note that we can d

Prohibitive!

O(d2) O(d2)

Boo

W(1) "W (1)- ! x !aT
O(Kd) cheap!

...

??

(large D, but K-sparse)

Ex: D = 500 000, K=5

?

Us
com

!

?

...

??

Target y (large D, but K-sparse)

?

Current workarounds are approximations:
Figure 1: ?The
computational
problem
posed
very
large
sparse
Dealing
sparse inSampling
based approximations
compute
only by
a tiny
fraction
of the
output?stargets.
dimensions
sampled atwith
random.
put efficiently
is trivial,sampling
with both
the
and
backward
propagation
achieved in
Reconstruction
[2] and
theforward
use of Noise
Contrastive
Estimation
[3] in [4, 5]phases
fall undereasily
this category.
O(Kd). However
this is not the case with large sparse targets. They incur a prohibitive compu? Hierarchical softmax [6, 4] imposes a heuristically defined hierarchical tree structure for the computation of the
tational costnormalized
of O(Dd)
at the
output
layer as forward propagation, gradient backpropagation and
probability
of the
target class.
weight update each require accessing all D ? d elements of the large output weight matrix.

! Ne
" ac
SHORT FORMU
LINE CASE: !
Ne
" ac

Qn

a) First update
implicitly by updat

Proof:
[1] Bengio, Y., Ducharme, R., and Vincent, P. (2001). A neural probabilistic language model. NIPS 2000.
?L Y. (2011). Large-scale?L
Dauphin,
Y., Glorot,
learning
of embeddings with reconstruction sampling. ICML 2011.
back[2]the
gradients
?hX.,
= Bengio,
(k) and
(k) and ?a(k) =
(k) upstream through the network.

References:

propagating
?h
?a
The corresponding gradient contributions on parameters (weights and biases), collected along the
way, are straightforward once we have the associated ?a(k) . Specifically they are ?b(k) = ?a(k)
and ?W (k) = h(k?1) (?a(k) )T . Similarly for the input layer ?W (1) = x(?a(1) )T , and for the
output layer ?W = (o ? y)hT . Parameters are then updated through a gradient descent step
W (k) ? W (k) ? ??W (k) and b(k) ? b(k) ? ??b(k) , where ? is a positive learning-rate. Similarly
for the output layer which will be our main focus here: W ? W ? ??W .
2.2

The easy part: input layer forward propagation and weight update

It is easy and straightforward to efficiently compute the forward propagation, and the backpropagation and weight update part for the input layer when we have a very large Din -dimensional but
K?sparse input vector x with appropriate sparse representation. Specifically we suppose that x is
represented as a pair of vectors u, v of length (at most) K, where u contains integer indexes and v
the associated real values of the elements of x such that xi = 0 if i ?
/ u, and xuk = vk .

? Forward propagation through the input layer: The sparse representation of x as the positions
of K elements together with their value makes it cheap to compute W (1)T x. Even though W (1)
may be a huge full Din ? d matrix, only K of its rows (those corresponding to the non-zero
entries of x) need to be visited and summed to compute W (1)T x. Precisely, with our (u, v) sparse
PK
(1)
(1)
representation of x this operation can be written as W (1)T x = k=1 vk W:uk where each W:uk
is a d-dimensional vector, making this an O(Kd) operation rather than O(Dd).
? Gradient and update through input layer: Let us for now suppose that we were able to get
gradients (through backpropagation) up to the first hidden layer activations a(1) ? Rd in the form
of gradient vector ?a(1) = ?a?L
(1) . The corresponding gradient-based update to input layer weights
W (1) is simply W (1) ? W (1) ? ?x(?a(1) )T . This is a rank-one update to W (1) . Here again, we
see that only the K rows of W (1) associated to the (at most) K non-zero entries of x need to be
(1)
(1)
modified. Precisely this operation can be written as: W:uk ? W:uk ??vk ?a(1) ?k ? {1, . . . , K}
making this again a O(Kd) operation rather than O(Dd).
3

[3] G
[4] M

2.3

The hard part: output layer propagation and weight update

Given some network input x, we suppose we can compute without difficulty through forward propagation the associated last hidden layer representation h ? Rd . From then on:

? Computing the final output o = W h incurs a prohibitive computational cost of O(Dd) since W
is a full D ? d matrix. Note that there is a-priori no reason for representation h to be sparse (e.g.
with a sigmoid non-linearity) but even if it was, this would not fundamentally change the problem
since it is D that is extremely large, and we supposed d reasonably sized already. Computing the
residual (o ? y) and associated squared error loss ko ? yk2 incurs an additional O(D) cost.
T
? The gradient on h that we need to backpropagate to lower layers is ?h = ?L
?h = 2W (o ? y)
which is another O(Dd) matrix-vector product.
? Finally, when performing the corresponding output weight update W ? W ? ?(o ? y)hT we see
that it is a rank-one update that updates all D ? d elements of W , which again incurs a prohibitive
O(Dd) computational cost.

For very large D, all these three O(Dd) operations are prohibitive, and the fact that y is sparse, seen
from this perspective, doesn?t help, since neither o nor o ? y will be sparse.

3

A computationally efficient algorithm for performing the exact online
gradient update

Previously proposed workarounds are approximate or use stochastic sampling. We propose a different approach that results in the exact same, yet efficient gradient update, remarkably without ever
having to compute large output o.
3.1

Computing the squared error loss L and the gradient with respect to h efficiently

Suppose that, we have, for a network input example x, computed the last hidden representation
h ? Rd through forward propagation. The network?s D dimensional output o = W h is then in
principle compared to the high dimensional target y ? RD . The corresponding squared error loss
2
is L = kW h ? yk . As we saw in Section 2.3, computing it in the direct naive way would have
a prohibitive computational complexity of O(Dd + D) = O(Dd) because computing output W h
with a full D ? d matrix W and a typically non-sparse h is O(Dd). Similarly, to backpropagate
the gradient through the network, we need to compute the gradient of loss L with respect to last
?kW h?yk2
hidden layer representation h. This is ?h = ?L
= 2W T (W h ? y). So again, if
?h =
?h
we were to compute it directly in this manner, the computational complexity would be a prohibitive
O(Dd). Provided we have maintained an up-to-date matrix Q = W T W , which is of reasonable
size d ? d and can be cheaply maintained as we will see in Section 3.3, we can rewrite these two
operations so as to perform them in O(d2 ):
Loss computation:

Gradient on h:

O(Dd)

L =
=

z}|{
k W h ?yk2

?h =

T

(W h ? y) (W h ? y)

= hT W T W h ? y T W h ? hT W T y + y T y
= hT Qh ? 2hT (W T y) + y T y
= hT ( Qh ?2 W T y ) + y T y
|{z}
| {z }
|{z}
O(d2 )

O(Kd)

(1)

?L
?h

=
=
=
=

?kW h ? yk2
?h
2W T (W h ? y)

2 WTWh ? WTy
2( Qh ? W T y )
|{z} | {z }
O(d2 )

O(K)



(2)

O(Kd)

The terms in O(Kd) and O(K) are due to leveraging the K-sparse representation of target vector
y. With K  D and d  D, we get altogether a computational cost of O(d2 ) which can be several
orders of magnitude cheaper than the prohibitive O(Dd) of the direct approach.
4

3.2

Efficient gradient update of W

The gradient of the squared error loss with respect to output layer weight matrix W is
?kW h?yk2
?W

?L
?W

=

= 2(W h ? y)h . And the corresponding gradient descent update to W would be
Wnew ? W ? 2?(W h ? y)hT , where ? is a positive learning rate. Again, computed in this manner,
this induces a prohibitive O(Dd) computational complexity, both to compute output and residual
W h ? y, and then to update all the Dd elements of W (since generally neither W h ? y nor h will
be sparse). All D ? d elements of W must be accessed during this update. On the surface this seems
hopeless. But we will now see how we can achieve the exact same update on W in O(d2 ). The trick
is to represent W implicitly as the factorization |{z}
W = |{z}
V |{z}
U and update U and V instead
T

D?d

a) Unew

b) Vnew

D?d d?d

= U ? 2?(U h)hT
= V +

(3)

?T
2?y(Unew
h)T

(4)

This results in implicitly updating W as we did explicitly in the naive approach as we now prove:

Vnew Unew

?T
(V + 2?y(Unew
h)T ) Unew

=

?T
= V Unew + 2?y(Unew
h)T Unew
?1
= V Unew + 2?yhT Unew
Unew
?1
= V (U ? 2?(U h)hT ) + 2?yhT (Unew
Unew )

= V U ? 2?V U hhT + 2?yhT
= V U ? 2?(V U h ? y)hT
= W ? 2?(W h ? y)T hT
= Wnew

We see that the update of U in Eq. 3 is a simple O(d2 ) operation. Following this simple rank-one
update to U , we can use the Sherman-Morrison formula to derive the corresponding rank-one update
to U ?T which will also be O(d2 ):
?T
Unew

=

U ?T +

2?

2 (U

1 ? 2? khk

?T

h)hT

(5)

?T
h, an O(d2 ) operation needed in Eq. 4. The ensuing rank-one
It is then easy to compute the Unew
update of V in Eq 4, thanks to the K-sparsity of y is only O(Kd): only the K rows V associated
to non-zero elements in y are accessed and updated, instead of all D rows of W we had to modify
in the naive update! Note that with the factored representation of W as V U , we only have W
implicitly, so the W T y terms that entered in the computation of L and ?h in the previous paragraph
need to be adapted slightly as y? = W T y = U T (V T y), which becomes O(d2 + Kd) rather than
O(Kd) in computational complexity. But this doesn?t change the overall O(d2 ) complexity of these
computations.

3.3

Bookkeeping: keeping an up-to-date Q and U ?T

We have already seen, in Eq. 5, how we can cheaply maintain an up-to-date U ?T following our
update of U . Similarly, following our updates to U and V , we need to keep an up-to-date Q =
W T W which is needed to efficiently compute the loss L (Eq. 1) and gradient ?h (Eq. 2). We have
shown that updates to U and V in equations 3 and 4 are equivalent to implicitly updating W as
Wnew ? W ? 2?(W h ? y)hT , and this translates into the following update to Q = W T W :
z? =
Qnew

=

Qh ? U T (V T y)


Q ? 2? h?
z T + z?hT + (4? 2 L)hhT

(6)

The proof is straightforward but due to space constraints we put it in supplementary material. One
can see that this last bookkeeping operation also has a O(d2 ) computational complexity.
5

3.4

Putting it all together: detailed algorithm and expected benefits

We have seen that we can efficiently compute cost L, gradient with respect to h (to be later backpropagated further) as well as updating U and V and performing the bookkeeping for U ?T and
Q. Algorithm 1 describes the detailed algorithmic steps that we put together from the equations
derived above. Having K  d  D we see that the proposed algorithm requires O(d2 ) operations,
whereas the standard approach required O(Dd) operations. If we take K ? d , we may state more
precisely that the proposed algorithm, for computing the loss and the gradient updates will require
roughly 12d2 operations whereas the standard approach required roughly 3Dd operations. So overD
all the proposed algorithm change corresponds to a computational speedup by a factor of 4d
. For
D = 200 000 and d = 500 the expected speedup is thus 100. Note that the advantage is not only
in computational complexity, but also in memory access. For each example, the standard approach
needs to access and change all D ? d elements of matrix W , whereas the proposed approach only
accesses the much smaller number K ? d elements of V as well as the three d ? d matrices U , U ?T ,
and Q. So overall we have a substantially faster algorithm, which, while doing so implicitly, will
nevertheless perform the exact same gradient update as the standard approach. We want to emphasize here that our approach is completely different from simply chaining 2 linear layers U and V
and performing ordinary gradient descent updates on them: this would result in the same prohibitive
computational complexity as the standard approach, and such ordinary separate gradient updates to
U and V would not be equivalent to the ordinary gradient update to W = V U .
Algorithm 1 Efficient computation of cost L, gradient on h, and update to parameters U and V
Step Operation
Computational Number of
#
complexity
multiply-adds
? = Qh
1:
O(d2 )
d2
h
2:
y? = U T (V T y)
O(Kd + d2 )
Kd + d2
? ? y?
3:
z? = h
O(d)
d
4:
?h = 2?
z
O(d)
d
? ? 2hT y? + y T y
5:
L = hT h
O(2d + K)
2d + K + 1
6:
Unew = U ? 2?(U h)hT
O(d2 )
2d2 + d
?T
2
2
7:
Unew =
O(d )
2d + 2d + 3
2?
?T
U ?T + 1?2?khk
h)hT
2 (U
?T
8:
Vnew = V + 2?y(Unew
h)T
O(d2 + Kd)
d2 + K + Kd
2
9:
Qnew =
O(d )
4 + 2d + 3d2

Q ? 2? h?
z T + z?hT +
(4? 2 L)hhT
Altogether:
O(d2 )
? 12d2
provided
elementary
K<dD
operations
3.5

Controlling numerical stability and extension to the minibatch case

The update of U in Equation 3 may over time lead U to become ill-conditioned. To prevent this,
we regularly (every 100 updates) monitor its conditioning number. If either the smallest or largest
singular value moves outside an acceptable range2 , we bring it back to 1 by doing an appropriate
rank-1 update to V (which costs Dd operations, but is only done rarely). Our algorithm can also
be straightforwardly extended to the minibatch case (the derivations are given in the supplementary material section) and yields the same theoretical speedup factor with respect to the standard
naive approach. But one needs to be careful in order to keep the computation of U ?T h reasonably
efficient: depending on the size of the minibatch m, it may be more efficient to solve the corresponding linear equation for each minibatch from scratch rather than updating U ?T with the Woodbury
equation (which generalizes the Sherman-Morrison formula for m > 1).
2

More details on our numerical stabilization procedure can be found in the supplementary material

6

3.6

Generalization to a broader class of loss functions

The approach that we just detailed for linear output and squared error can be extended to a broader,
though restricted, family of loss functions. We call it the spherical family of loss functions because
it includes the spherical alternative to the softmax, thus named in [14]. Basically it contains any loss
2
function
P 2 that can be expressed as a function of only the oc associated to non-zero yc and of kok =
j oj the squared norm of the whole output vector, which we can compute cheaply, irrespective of
D, as we did above3 . This family does not include the standard softmax loss log
o2 +
P c 2
j (oj +)

Pexp(oc ) ,
j exp(oj )

but it

does include the spherical softmax4 : log
. Due to space constraints we will not detail this
extension here, only give a sketch of how it can be obtained. Deriving it may not appear obvious at
first, but it is relatively straightforward once we realize that: a) the gain in computing the squared
error loss comes from being able to very cheaply compute the sum of squared activations kok2 (a
scalar quantity), and will thus apply equally well to other losses that can be expressed based on that
quantity (like the spherical softmax). b) generalizing our gradient update trick to such losses follows
naturally from gradient backpropagation: the gradient is first backpropagated from the final loss to
the scalar sum of squared activations, and from there on follows the same path and update procedure
as for the squared error loss.

4

Experimental validation

We implemented both a CPU version using blas and a parallel GPU (Cuda) version using cublas
of the proposed algorithm5 . We evaluated the GPU and CPU implementations by training word
embeddings with simple neural language models, in which a probability map of the next word
given its preceding n-gram is learned by a neural network. We used a Nvidia Titan Black GPU
and a i7-4820K @ 3.70GHz CPU and ran experiments on the one billion word dataset[15], which
is composed of 0.8 billions words belonging to a vocabulary of 0.8 millions words. We evaluated
the resulting word embeddings with the recently introduced Simlex-999 score [16], which measures
the similarity between words. We also compared our approach to unfactorised versions and to a
two-layer hierarchical softmax. Figure 2 and 3 (left) illustrate the practical speedup of our approach
for the output layer only. Figure 3 (right) shows that out LST (Large Sparse Target) models are
much faster to train than the softmax models and converge to only slightly lower Simlex-999 scores.
Table 1 summarizes the speedups for the different output layers we tried, both on CPU and GPU.
We also empirically verified that our proposed factored algorithm learns the model weights (V U ) as
the corresponding naive unfactored algorithm?s W , as it theoretically should, and followed the same
learning curves (as a function of number of iterations, not time!).

5

Conclusion and future work

We introduced a new algorithmic approach to efficiently compute the exact gradient updates for
training deep networks with very large sparse targets. Remarkably the complexity of the algorithm
is independent of the target size, which allows tackling very large problems. Our CPU and GPU
implementations yield similar speedups to the theoretical one and can thus be used in practical
applications, which could be explored in further work. In particular, neural language models seem
good candidates. But it remains unclear how using a loss function other than the usual softmax might
affect the quality of the resulting word embeddings so further research needs to be carried out in this
direction. This includes empirically investigating natural extensions of the approach we described
to other possible losses in the spherical family such as the spherical-softmax.
Acknowledgements: We wish to thank Yves Grandvalet for stimulating discussions, ?a?glar
G?l?ehre for pointing us to [14], the developers of Theano [17, 18] and Blocks [19] for making
these libraries available to build on, and NSERC and Ubisoft for their financial support.
3

P
In addition loss functions in this family are also allowed to depend on sum(o) = j oj which we can also
P
P
compute cheaply without computing o, by tracking w
? = j W:j whereby sum(o) = j W:jT h = w
? T h.
4
where c is the correct class label, and  is a small positive constant that we added to the spherical interpretation in [14] for numerical stability: to guarantee we never divide by 0 nor take the log of 0.
5
Open source code is available at: https://github.com/pascal20100/factored_output_layer

7

Table 1: Speedups with respect to the baseline naive model on CPU, for a minibatch of 128 and the
whole vocabulary of D = 793471 words. This is a model with two hidden layers of d = 300 neurons.
Model
output layer only speedup whole model speedup
cpu unfactorised (naive)
1
1
gpu unfactorised (naive)
6.8
4.7
gpu hierarchical softmax
125.2
178.1
cpu factorised
763.3
501
gpu factorised
3257.3
1852.3

1

10

un-factorised CPU
un-factorised GPU
factorised GPU
factorised CPU
h_softmax GPU

0.008

Timing (sec) of a minibatch of size 128

Timing (sec) of a minibatch of size 128

0.010

0.006

0.004

0.002

un-factorised CPU
un-factorised GPU
factorised GPU
factorised CPU
h_softmax GPU

0

10

-1

10

-2

10

-3

0.000
0

2000

4000
6000
Size of the vocabulary D

8000

10

10000

1

2

10

10

4

3

10
10
Size of the vocabulary D

5

10

6

10

Figure 2: Timing of different algorithms. Time taken by forward and backward propagations in the
output layer, including weight update, on a minibatch of size 128 for different sizes of vocabulary
D on both CPU and GPU. The input size d is fixed to 300. The Timing of a 2 layer hierarchical
softmax efficient GPU implementation (h_softmax) is also provided for comparison. Right plot is
in log-log scale. As expected, the timings of factorized versions are independent of vocabulary size.

0.25

cpu_unfact / cpu_fact, experimental
gpu_unfact / gpu_fact, experimental
unfact / fact, theoretical
cpu_unfact / gpu_fact, experimental
cpu_unfact / gpu_unfact, experimental

1400

1200

0.15
SimLex-999

Speedup

1000

0.20

800

600

0.05

LST CPU
LST GPU
Softmax CPU
Softmax GPU
H-Softmax GPU

0.00

400

?0.05

200

0
0

0.10

100

200
300
400
500
600
700
Size of the vocabulary D (in thousands)

?0.10

800

-1

10

0

10

1

10
Training time (hours)

2

10

3

10

Figure 3: Left: Practical and theoretical speedups for different sizes of vocabulary D and fixed input
size d=300. The practical unfact / fact speedup is similar to the theoretical one. Right: Evolution
of the Simlex-999 score obtained with different models as a function of training time (CPU softmax
times were extrapolated from fewer iterations). Softmax models are zero hidden-layer models, while
our large sparse target (LST) models have two hidden layers. These were the best architectures
retained in both cases (surprisingly the softmax models with hidden layers performed no better on
this task). The extra non-linear layers in LST may help compensate for the lack of a softmax. LST
models converge to slightly lower scores at similar speed as the hierarchical softmax model but
significantly faster than softmax models.
8

References
[1] Y. Bengio, R. Ducharme, and P. Vincent. A neural probabilistic language model. In Advances in Neural
Information Processing Systems 13 (NIPS?00), pages 932?938, 2001.
[2] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493?2537, 2011.
[3] Y. Dauphin, X. Glorot, and Y. Bengio. Large-scale learning of embeddings with reconstruction sampling.
In Proceedings of the 28th International Conference on Machine learning, ICML ?11, 2011.
[4] S. Jean, K. Cho, R. Memisevic, and Y. Bengio. On using very large target vocabulary for neural machine
translation. In ACL-IJCNLP?2015, 2015. arXiv:1412.2007.
[5] M. Gutmann and A. Hyvarinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of The Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS?10), 2010.
[6] A. Mnih and K. Kavukcuoglu. Learning word embeddings efficiently with noise-contrastive estimation.
In Advances in Neural Information Processing Systems 26, pages 2265?2273. 2013.
[7] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed representations of words and
phrases and their compositionality. In NIPS?2013, pages 3111?3119. 2013.
[8] A. Shrivastava and P. Li. Asymmetric LSH (ALSH) for sublinear time maximum inner product search
(MIPS). In Advances in Neural Information Processing Systems 27, pages 2321?2329. 2014.
[9] S. Vijayanarasimhan, J. Shlens, R. Monga, and J. Yagnik. Deep networks with large output spaces.
arxiv:1412.7479, 2014.
[10] F. Morin and Y. Bengio. Hierarchical probabilistic neural network language model. In Proceedings of the
Tenth International Workshop on Artificial Intelligence and Statistics, pages 246?252, 2005.
[11] D. Rumelhart, G. Hinton, and R. Williams. Learning representations by back-propagating errors. Nature,
323:533?536, 1986.
[12] Y. LeCun. Une proc?dure d?apprentissage pour R?seau ? seuil assym?trique. In Cognitiva 85: A la
Fronti?re de l?Intelligence Artificielle, des Sciences de la Connaissance et des Neurosciences, pages 599?
604, 1985.
[13] Y. LeCun. Learning processes in an asymmetric threshold network. In Disordered Systems and Biological
Organization, pages 233?240. Les Houches 1985, 1986.
[14] Y. Ollivier. Riemannian metrics for neural networks. CoRR, abs/1303.0818, 2013.
[15] C. Chelba, T. Mikolov, M. Schuster, Q. Ge, T. Brants, P. Koehn, and T. Robinson. One billion word
benchmark for measuring progress in statistical language modeling. In INTERSPEECH 2014, 15th Annual
Conference of the International Speech Communication Association, Singapore, September 14-18, 2014,
pages 2635?2639, 2014.
[16] F. Hill, R. Reichart, and A. Korhonen. Simlex-999: Evaluating semantic models with (genuine) similarity
estimation. CoRR, abs/1408.3456, 2014.
[17] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins, J. Turian, D. Warde-Farley,
and Y. Bengio. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for
Scientific Computing Conference (SciPy), 2010. Oral Presentation.
[18] F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. J. Goodfellow, A. Bergeron, N. Bouchard, and Y. Bengio.
Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS
2012 Workshop, 2012.
[19] B. van Merri?nboer, D. Bahdanau, V. Dumoulin, D. Serdyuk, D. Warde-Farley, J. Chorowski, and Y. Bengio. Blocks and Fuel: Frameworks for deep learning. ArXiv e-prints, June 2015.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 459-kernel-regression-and-backpropagation-training-with-noise.pdf

Kernel Regression and
Backpropagation Training with Noise

Petri Koistinen and Lasse Holmstrom
Rolf Nevanlinna Institute, University of Helsinki
Teollisuuskatu 23, SF-0051O Helsinki, Finland

Abstract
One method proposed for improving the generalization capability of a feedforward network trained with the backpropagation algorithm is to use
artificial training vectors which are obtained by adding noise to the original training vectors. We discuss the connection of such backpropagation
training with noise to kernel density and kernel regression estimation. We
compare by simulated examples (1) backpropagation, (2) backpropagation
with noise, and (3) kernel regression in mapping estimation and pattern
classification contexts.

1

INTRODUCTION

Let X and Y be random vectors taking values in R d and RP, respectively. Suppose
that we want to estimate Y in terms of X using a feedforward network whose
input-output mapping we denote by y g(x, w). Here the vector w includes all the
weights and biases of the network. Backpropagation training using the quadratic
loss (or error) function can be interpreted as an attempt to minimize the expected
loss
'\(w) ElIg(X, w) _ Y1I2.
(1)
Suppose that EIIYW < 00. Then the regression function

=

=

(2)
m(x) = E[YIX = x].
minimizes the loss Ellb(X) - YI1 2 over all Borel measurable mappings b. Therefore,
backpropagation training can also be viewed as an attempt to estimate m with the
network g.
1033

1034

Koistinen and Holmstrom

In practice, one cannot minimize -' directly because one does not know enough
about the distribution of (X, Y). Instead one minimizes a sample estimate

(3)
in the hope that weight vectors w that are near optimal for ~n are also near optimal
for -'. In fact, under rather mild conditions the minimizer of ~n actually converges
towards the minimizing set of weights for -' as n -+ 00, with probability one (White,
1989). However, if n is small compared to the dimension of w, minimization of ~n
can easily lead to overfitting and poor generalization, i.e., weights that render ~n
small may produce a large expected error -'.
Many cures for overfitting have been suggested. One can divide the available samples into a training set and a validation set, perform iterative minimization using
the training set and stop minimization when network performance over the validation set begins to deteriorate (Holmstrom et al., 1990, Weigend et al., 1990). In
another approach, the minimization objective function is modified to include a term
which tries to discourage the network from becoming too complex (Weigend et al.,
1990). Network pruning (see, e.g., Sietsma and Dow, 1991) has similar motivation.
Here we consider the approach of generating artificial training vectors by adding
noise to the original samples. We have recently analyzed such an approach and
proved its asymptotic consistency under certain technical conditions (Holmstrom
and Koistinen, 1990).

2

ADDITIVE NOISE AND KERNEL REGRESSION

Suppose that we have n original training vectors (Xi, Yi) and want to generate
artificial training vectors using additive noise. If the distributions of both X and Y
are continuous it is natural to add noise to both X and Y components of the sample.
However, if the distribution of X is continuous and that of Y is discrete (e.g., in
pattern classification), it feels more natural to add noiRe to the X components only.
In Figure 1 we present sampling procedures for both ca~es. In the x-only case the
additive noise is generated from a random vector Sx with density Kx whereas in the
x-and-y case the noise is generated from a random vector SXy with density Kxy.
Notice that we control the magnitude of noise with a scalar smoothing parameter
h > O.
In both cases the sampling procedures can be thought of as generating random
samples from new random vectors Xk n ) and y~n) . Using the same argument as
in the Introduction we see that a network trained with the artificial samples tends
to approximate the regression function E[y~n) IXkn)]. Generate I uniformly on
{1, ... , n} and denote by I and I( .11 = i) the density and conditional density of
Xk n ). Then in the x-only case we get
n

m~n)(Xkn)) := E[y~n)IXkn)] = LYiP(I
i=l

= iIXkn))

Kernel Regression and Backpropagation Training with Noise

Procedure 2.
(Add noise to both x and y)

Procedure 1.
(Add noise to x only)

1. Select i E {I, ... , n} with equal
probability for each index.
2. Draw a sample (sx, Sy) from
density /{Xy on Rd+p.

1. Select i E {I, ... , n} with equal

probability for each index.
2. Draw a sample Sx from density
I<x on Rd.

x~n)

3. Set

(n)

Yh

Xi

+ hsx

3. Set

x~n)

+ hsx
Yi + h Sy.
Xi

Yh(n)

Yi?

Figure 1: Two Procedures for Generating Artificial Training Vectors.

tt

f(X~n)II = i)P(I

n

=
Denoting

Yi

/{x

= i)
=

f(Xh n ))

tt
n

h-d/{x?Xkn ) - xi)/h). n- 1
Yi 2:7=1 n- 1 h- d/{x?Xkn ) - xi)/h)?

by k we obtain
(n)( ) _

mh

X

-

2:~=1 k?x - xi)/h)Yi
",0
.
L..,j=1 k?x - xi)/h)

(4)

We result in the same expression also in the x-and-y case provided that
fY/{Xy(x,y)dy
0 and that we take k(x)
fI<Xy(x,y)dy (Watson, 1964).
The expression (4) is known as the (N adaraya-Watson) kernel regression estimator
(Nadaraya, 1964, Watson, 1964, Devroye and Wagner, 1980).

=

=

A common way to train a p-class neural network classifier is to train the network
to associate a vector x from class j with the j'th unit vector (0, ... ,0,1,0, ... ,0).
It is easy to see that then the kernel regression estimator components estimate the
class a posteriori probabilities using (Parzen-Rosenblatt) kernel density estimators
for the class conditional densities. Specht (1990) argues that such a classifier can
be considered a neural network. Analogously, a kernel regression estimator can be
considered a neural network though such a network would need units proportional
to the number of training samples. Recently Specht (1991) has advocated using
kernel regression and has also presented a clustering variant requiring only a fixed
amount of units. Notice also the resemblance of kernel regression to certain radial
basis function schemes (Moody and Darken, 1989, Stokbro et al., 1990).
An often used method for choosing h is to minimize the cross-validated error (HardIe
and Marron, 1985, Friedman and Silverman, 1989)
(5)
Another possibility is to use a method suggested by kernel density estimation theory
(Duin, 1976, Habbema et al., 1974) whereby one chooses that h maximizing a crossvalidated (pseudo) likelihood function
o

Lxy (h)

= II f:,L(Xi, yd,
i=1

o

Lx(h)

= IT f:'h,i(Xi),
i=1

(6)

1035

1036

Koistinen and Holmstrom

I;r

where
i (I; h i) is a kernel density estimate with kernel Kxy (Kx) and smoothing para~~ter h hut with the i'th sample point left out.

3

EXPERIMENTS

In the first experiment we try to estimate a mapping go from noisy data (x, y),

go(X)+Ny =asinX+b+Ny ,
UNI( -71",71"),
Ny '" N(O, (72),

Y

a=0.4,b=0.5

=

X '"
(7
O.l.
Here UNI and N denote the uniform and the normal distribution. We experimented with backpropagation, backpropagation with noise and kernel regression.
Backpropagation loss function was minimized using Marquardt's method. The network architecture was FN-1-13-1 with 40 adaptable weights (a feedforward network
with one input, 13 hidden nodes, one output, and logistic activation functions in
the hidden and output layers). We started the local optimizations from 3 different
random initial weights and kept the weights giving the least value for ~n. Backpropagation training with noise was similar except that instead of the original n vectors
we used IOn artificial vectors generated with Procedure 2 using SXy '" N(O, 12 ).
Magnitude of noise was chosen with the criterion Lxy (which, for backpropagation,
gave better results than M). In the kernel regression experiments SXy was kept the
same. Table 1 characterizes the distribution of J, the expected squared distance of
the estimator 9 (g(., w) or m~n) from go,
J = E[g(X) - gO(X)]2.
Table 2 characterizes the distribution of h chosen according to the criteria Lxy and
M and Figure 2 shows the estimators in one instance. Notice that, on the average,
kernel regression is better than backpropagation with noise which is better than
plain backpropagation. The success of backpropagation with noise is partly due to
the fact that (7 and n have here been picked favorably. Notice too that in kernel
regression the results with the two cross-validation methods are similar although
the h values they suggest are clearly different .

In the second experiment we trained classifiers for a four-dimensional two-class
problem with equal a priori probabilities and class-conditional densities N(J.ll, C 1 )
and N(J.l2' C2),

J.ll

= 2.32[1 0 0 O]T, C = 14;
1

J.l2

= 0, C2 = 414.

An FN-4-6-2 with 44 adaptable weights was trained to associate vectors from class 1
with [0.9 O.l]T and vectors from class 2 with [0.1 0.9jT. We generated n/2 original
vectors from each class and a total of IOn artificial vectors using Procedure 1 with
Sx '" N(O, 14). We chose the smoothing parameters, hI and h2' separately for
the two classes using the criterion Lx: hi was chosen by evaluating Lx on class
i samples only. We formed separate kernel regression estimators for each class;
the i'th estimator was trained to output 1 for class i vectors and 0 for the other
sample vectors. The M criterion then produces equal values for hI and h2. The
classification rule was to classify x to class i if the output corresponding to the
i'th class was the maximum output. The error rates are given in Table 3. (The
error rate of the Bayesian classifier is 0.116 in this task.) Table 4 summarizes the
distribution of hI and h2 as selected by Lx and M .

Kernel Regression and Backpropagation Training with Noise

Table 1: Results for Mapping Estimation. Mean value (left) and standard deviation
(right) of J based on 100 repetitions are given for each method.
BP

BP+noise,

Lxy

n

40
80

.0218
.00764

.016
.0048

.0104
.00526

.0079
.0018

Kernel regression
M
Lxy
.00446 .0022
.00365 .0019
.00250 .00078 .00191 .00077

Table 2: Values of h Suggested by the Two Cross-validation Methods in the Mapping Estimation Experiment. Mean value and standard deviation based on 100
repetitions are given.

Lxy

n

40
80

0.149
0.114

M

0.020
0.011

0.276
0.241

0.086
0.062

Table 3: Error Rates for the Different Classifiers. Mean value and standard deviation based on 25 repetitions are given for each method.
BP+noise,

BP

Lx

n

44
88
176

Kernel regression

.281
.264
.210

.054
.028
.023

.189
.163
.145

Lx
.018
.011
.0lD

.201
.182
.164

M

.022
.010
.0089

.207
.184
.164

.027
.013
.011

Table 4: Values of hl and h2 Suggested by the Two Cross-validation Methods in
the Classification Experiment. Mean value and standard deviation based on 25
repetitions are given.

Lx
n

44
88
176

M

hl
.818
.738
.668

h2
.078
.056
.048

1.61
1.48
1.35

.14
.11
.090

hl = h2
1.14 .27
1.01 .19
.868 .lD

1037

1038

Koistinen and Holmstrom

4

CONCLUSIONS

Additive noise can improve the generalization capability of a feedforward network
trained with the backpropagation approach. The magnitude of the noise cannot
be selected blindly, though. Cross-validation-type procedures seem to suit well for
the selection of noise magnitude. Kernel regression, however, seems to perform well
whenever backpropagation with noise performs well. If the kernel is fixed in kernel
regression, we only have to choose the smoothing parameter h, and the method is
not overly sensitive to its selection.
References

[Devroye and Wagner, 1980] Devroye, 1. and Wagner, T. (1980). Distribution-free
consistency results in non parametric discrimination and regression function estimation. The Annals of Statistics, 8(2):231-239.
[Duin, 1976] Duin, R. P. W. (1976). On the choice of smoothing parameters for
Parzen estimators of probability density functions. IEEE Transactions on Computers, C-25:1175-1179.
[Friedman and Silverman, 1989] Friedman, J. and Silverman, B. (1989). Flexible
parsimonious smoothing and additive modeling. Technometrics, 31(1):3-2l.
[Habbema et al., 1974] Habbema, J. D. F., Hermans, J., and van den Broek, K.
(1974). A stepwise discriminant analysis program using density estimation. In
Bruckmann, G., editor, COMPSTAT 1974, pages 101-110, Wien. Physica Verlag.
[HardIe and Marron, 1985] HardIe, W. and Marron, J. (1985). Optimal bandwidth
selection in nonparametric regression function estimation. The Annals of Statistics, 13(4):1465-148l.
[Holmstrom and Koistinen, 1990] Holmstrom, 1. and Koistinen, P. (1990). Using
additive noise in back-propagation training. Research Reports A3, Rolf Nevanlinna Institute. To appear in IEEE Trans. Neural Networks.
[Holmstrom et al., 1990] Holmstrom, L., Koistinen, P., and Ilmoniemi, R. J. (1990).
Classification of un averaged evoked cortical magnetic fields. In Proc. IJCNN-90WASH DC, pages II: 359-362. Lawrence Erlbaum Associates.
[Moody and Darken, 1989] Moody, J. and Darken, C. (1989). Fast learning in networks of locally-tuned processing units. Neural Computation, 1:281-294.
[N adaraya, 1964] Nadaraya, E. (1964). On estimating regression. Theor. Probability
Appl., 9:141-142.
[Sietsma and Dow, 1991] Sietsma, J. and Dow, R. J. F. (1991). Creating artificial
neural networks that generalize. Neural Networks, 4:67-79.
[Specht, 1991] Specht, D. (1991). A general regression neural network. IEEE Transactions on Neural Networks, 2(6):568-576.
[Specht, 1990] Specht, D. F. (1990). Probabilistic neural networks. Neural Networks, 3(1):109-118.
[Stokbro et al., 1990] Stokbro, K., Umberger, D., and Hertz, J. (1990). Exploiting
neurons with localized receptive fields to learn chaos. NORDITA preprint.
[Watson, 1964] Watson, G. (1964). Smooth regression analysis. Sankhyii Ser. A,
26:359-372.
[Weigend et al., 1990] Weigend, A., Huberman, B., and Rumelhart, D. (1990). Predicting the future: A connectionist approach. International Journal of Neural
Systems, 1(3):193-209.

Kernel Regression and Backpropagation Training with Noise

[White, 1989] White, H. (1989). Learning in artificial neural networks: A statistical
perspective. Neural Computation, 1:425-464.
1.5.....--.....----.---.....--....----.---.....--....----,

o

1

0

0.5

0

0

-0.54

--- true
-3

kernel

0

-2

3

2

4

1.5

?

-

..

':

..

? 0

1

0.5

0

..

.
..
-0.54

-3

-2

-

..

'

-I

0

1

----

BP

-

BP+noise
2

3

4

Figure 2: Results From a Mapping Estimation Experiment. Shown are the n = 40
original vectors (o's), the artificial vectors (dots), the true function asinx + band
the fitting results using kernel regression, backpropagation and backpropagation
with noise. Here h = 0.16 was chosen with Lxy. Values of J are 0.0075 (kernel
regression), 0.014 (backpropagation with noise) and 0.038 (backpropagation) .

1039


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 1100-tempering-backpropagation-networks-not-all-weights-are-created-equal.pdf

Tempering Backpropagation Networks:
Not All Weights are Created Equal

Nicol N. Schraudolph
EVOTEC BioSystems GmbH

Grandweg 64
22529 Hamburg, Germany
nici@evotec.de

Terrence J. Sejnowski
Computational Neurobiology Lab
The Salk Institute for BioI. Studies
San Diego, CA 92186-5800, USA
terry@salk.edu

Abstract
Backpropagation learning algorithms typically collapse the network's
structure into a single vector of weight parameters to be optimized. We
suggest that their performance may be improved by utilizing the structural information instead of discarding it, and introduce a framework for
''tempering'' each weight accordingly.
In the tempering model, activation and error signals are treated as approximately independent random variables. The characteristic scale of weight
changes is then matched to that ofthe residuals, allowing structural properties such as a node's fan-in and fan-out to affect the local learning rate
and backpropagated error. The model also permits calculation of an upper
bound on the global learning rate for batch updates, which in turn leads
to different update rules for bias vs. non-bias weights.
This approach yields hitherto unparalleled performance on the family relations benchmark, a deep multi-layer network: for both batch learning
with momentum and the delta-bar-delta algorithm, convergence at the
optimal learning rate is sped up by more than an order of magnitude.

1 Introduction
Although neural networks are structured graphs, learning algorithms typically view them
as a single vector of parameters to be optimized. All information about a network's architecture is thus discarded in favor of the presumption of an isotropic weight space - the
notion that a priori all weights in the network are created equal. This serves to decouple
the learning process from network design and makes a large body of function optimization
techniques directly applicable to backpropagation learning.
But what if the discarded structural information holds valuable clues for efficient weight
optimization? Adaptive step size and second-order gradient techniques (Battiti, 1992) may

N. N. SCHRAUDOLPH. T. J. SEJNOWSKI

564

recover some of it, at considerable computational expense. Ad hoc attempts to incorporate
structural information such as the fan-in (Plaut et aI., 1986) into local learning rates have become a familiar part of backpropagation lore; here we deri ve a more comprehensi ve framework - which we call tempering - and demonstrate its effectiveness.
Tempering is based on modeling the acti vities and error signals in a backpropagation network as independent random variables. This allows us to calculate activity- and weightinvariant upper bounds on the effect of synchronous weight updates on a node's activity.
We then derive appropriate local step size parameters by relating this maximal change in a
node's acti vi ty to the characteristic scale of its residual through a global learning rate.
Our subsequent derivation of an upper bound on the global learning rate for batch learning
suggests that the d.c. component of the error signal be given special treatment. Our experiments show that the resulting method of error shunting allows the global learning rate to
approach its predicted maximum, for highly efficient learning performance.

2 Local Learning Rates
Consider a neural network with feedforward activation given by
x j = /j (Yj)

,

Yj

=

L

(1)

Xi Wij ,

iEAj

where Aj denotes the set of anterior nodes feeding directly into node j, and /j is a nonlinear
(typically sigmoid) activation function. We imply that nodes are activated in the appropriate
sequence, and that some have their values clamped so as to represent external inputs.
With a local learning rate of'1j for node j, gradient descent in an objective function E produces the weight update

(2)
Linearizing

Ij

around

Yj

approximates the resultant change in activation

Xj

as
(3)

iEAj

iEAj

Our goal is to put the scale of ~Xj in relation to that of the error signal tSj . Specifically, when
averaged over many training samples, we want the change in output activity of each node
in response to each pattern limited to a certain proportion - given by the global learning
rate '1 - of its residual. We achieve this by relating the variation of ~X j over the training
set to that of the error signal:
(4)

where (.) denotes averaging over training samples. Formally, this approach may be interpreted as a diagonal approximation of the inverse Fischer information matrix (Amari, 1995).
We implement (4) by deriving an upper bound for the left-hand side which is then equated
with the right-hand side. Replacing the acti vity-dependent slope of Ij by its maximum value

s(/j) == maxl/j(u)1
u

(5)

and assuming that there are no correlations! between inputs Xi and error tSj ' we obtain
(~x}):::; '1} s(/j)2 (tS})f.j
1 Note

that such correlations are minimized by the local weight update.

(6)

Tempering Backpropagation Networks: Not All Weights Are Created Equal

565

from (3), provided that

ej ~ e;

==

([,Lxlf) ,

(7)

lEA]

We can now satisfy (4) by setting the local learning rate to
TJ' =
J -

8 (fj

TJ

(8)

).j[j .

ej

There are several approaches to computing an upper bound on the total squared input
power
One option would be to calculate the latter empirically during training, though
this raises sampling and stability issues. For external inputs we may precompute orderive
an upper bound based on prior knowledge of the training data. For inputs from other nodes
in the network we assume independence and derive
from the range of their activation
functions:
=
p(fd 2 , where p(fd == ffiuax/i(u)2.
(9)

e;.

e;

ej

ej

L

iEAj

Note that when all nodes use the same activation function

I,

we obtain the well-known

Vfan-in heuristic (Plaut et al., 1986) as a special case of (8).

3 Error Backpropagation
In deriving local learning rates above we have tacitly used the error signal as a stand-in for
the residual proper, i.e. the distance to the target. For output nodes we can scale the error to
never exceed the residual:
(10)

Note that for the conventional quadratic error this simplifies to <Pj = s(/j) . What about
the remainder of the network? Unlike (Krogh et aI., 1990), we do not wish to prescribe
definite targets (and hence residuals) for hidden nodes. Instead we shall use our bounds
and independence arguments to scale backpropagated error signals to roughly appropriate
magnitude. For this purpose we introduce an attenuation coefficient aj into the error backpropagation equation:

c5j

= II (Yi)
aj

L

Wjj

c5j

,

(11)

jEP,

where Pi denotes the set of posterior nodes fed directly from node i. We posit that the appropriate variation for c5i be no more than the weighted average of the variation of backpropagated errors:
(12)

whereas, assuming independence between the c5j and replacing the slope of Ii by its maximum value, (11) gives us

(c5?) ~

a? 8(f;)2 L

wi /

(c5/) .

(13)

jEP,

Again we equate the right-hand sides of both inequalities to satisfy (12), yielding
ai

==

1

8(fdJiP;T .

(14)

566

N. N. SCHRAUDOLPH, T. J. SEJNOWSKI

Note that the incorporation ofthe weights into (12) is ad hoc, as we have no a priori reason
to scale a node's step size in proportion to the size of its vector of outgoing weights. We
have chosen (12) simply because it produces a weight-invariant value for the attenuation
coefficient. The scale of the backpropagated error could be controlled more rigorously, at
the expense of having to recalculate ai after each weight update.

4

Global Learning Rate

We now derive the appropriate global learning rate for the batch weight update
LiWij

==

1]j

L dj (t)

Xi

(15)

(t)

tET

over a non-redundant training sample T. Assuming independent and zero-mean residuals,
we then have
(16)

by virtue of (4). Under these conditions we can ensure
~

2 ~ (dj 2)

/).Xj

(17)

,

i.e. that the variation of the batch weight update does not exceed that of the residual, by
using a global learning rate of
1]

~

1]*

== l/JiTf.

(18)

Even when redundancy in the training set forces us to use a lower rate, knowing the upper
bound 1]* effectively allows an educated guess at 1], saving considerable time in practice.

5 Error Shunting
It remains to deal with the assumption made above that the residuals be zero-mean, i.e. that
(dj)
O. Any d.c. component in the error requires a learning rate inversely proportional to
the batch size - far below 1]* , the rate permissible for zero-mean residuals. This suggests
handling the d.c. component of error signals separately. This is the proper job of the bias
weight, so we update it accordingly :

=

(19)
In order to allow learning at rates close to
then centered by subtracting the mean:

1]*

for all other weights, their error signals are
(20)

tET
T/j

(L

tET

dj (t)

X i (t)

- (Xi)

L

dj (t))

(21)

tET

Note that both sums in (21) must be collected in batch implementations of backpropagation
anyway - the only additional statistic required is the average input activity (Xi) ' Indeed
for batch update centering errors is equivalent to centering inputs, which is known to assist
learning by removing a large eigenvalue of the Hessian (LeCun et al., 1991). We expect
online implementations to perform best when both input and error signals are centered so
as to improve the stochastic approximation.

567

Tempering Backpropagation Networks: Not All Weights Are Created Equal

person

2 OO~O~OOOOOOO

000000000000

TJeff ~

'""<Lt tr,: j 1*'j:'i~

1.5 TJ

000000

A "~t{(d$?.?.?:?. ?.?.d+; BI?.

000000000000
~?;;?dt!i """" "'
~El~
000000
000000
~if;i ? ' :r:.? . ; ..0;:....",
..<1!. (i+1 ~ S?!? .? . ; ~
person 1 OOOOO~OOOOOO

000000000000

.25 TJ
.10TJ

.05 TJ

OOOOOOOOOO~O

relationship

Figure 1: Backpropagation network for learning family relations (Hinton, 1986).

6 Experimental Setup
We tested these ideas on the family relations task (Hinton, 1986): a backpropagation network is given examples of a family member and relationship as input, and must indicate
on its output which family members fit the relational description according to an underlying family tree. Its architecture (Figure 1) consists of a central association layer of hidden
units surrounded by three encoding layers that act as informational bottlenecks, forcing the
network to make the deep structure of the data explicit.
The input is presented to the network in a canonical local encoding: for any given training
example, exactly one input in each of the two input layers is active. On account of the always
active bias input, the squared input power for tempering at these layers is thus C = 4. Since
the output uses the same local code, only one or two targets at a time will be active; we
therefore do not attenuate error signals in the immediately preceding layer. We use crossentropy error and the logistic squashing function (1 + e- Y)-l at the output (giving ?> = 1)
but prefer the hyperbolic tangent for hidden units, with p(tanh) = s(tanh) = 1.
To illustrate the impact of tempering on this architecture we translate the combined effect
of local learning rate and error attenuation into an effective learning rate 2 for each layer,
shown on the right in Figure 1. We observe that effective learning rates are largest near the
output and decrease towards the input due to error attenuation. Contrary to textbook opinion
(LeCun, 1993; Haykin, 1994, page 162) we find that such unequal step sizes are in fact the
key to efficient learning here. We suspect that the logistic squashing function may owe its
popUlarity largely to the error attenuation side-effect inherent in its maximum slope of 114We expect tempering to be applicable to a variety of backpropagation learning algorithms;
here we present first results for batch learning with momentum and the delta-bar-delta
rule (Jacobs, 1988). Both algorithms were tested under three conditions: conventional,
tempered (as described in Sections 2 and 3), and tempered with error shunting. All experiments were performed with a customized simulator based on Xerion 3.1.3
For each condition the global learning rate TJ was empirically optimized (to single-digit precision) for fastest reliable learning performance, as measured by the sum of empirical mean
and standard deviation of epochs required to reach a given low value of the cost function.
All other parameters were held in variant across experiments; their values (shown in Table 1)
were chosen in advance so as not to bias the results.
2This is possible only for strictly layered networks, i.e. those with no shortcut (or "skip-through")
connections between topologically non-adjacent layers.
3 At the time of writing, the Xerion neural network simulator and its successor UTS are available
by anonymous file transfer from ai.toronto.edu, directory pub/xerion.

568

N. N. SCHRAUDOLPH. T. 1. SEJNOWSKI

Parameter

Val ue

training set size (= epoch)
momentum parameter
uniform initial weight range
weight decay rate per epoch

100
0.9
?0.3
10- 4

II

I Value I

Parameter

0.2
1.0
0.1
0.9

zero-error radius around target
acceptable error & weight cost
delta-bar-delta gain increment
delta-bar-delta gain decrement

Table 1: Invariant parameter settings for our experim~nts.

7

Experimental Results

Table 2 lists the empirical mean and standard deviation (over ten restarts) of the number
of epochs required to learn the family relations task under each condition, and the optimal
learning rate that produced this performance. Training times for conventional backpropagation are quite long; this is typical for deep multi-layer networks. For comparison, Hinton
reports around 1,500 epochs on this problem when both learning rate and momentum have
been optimized (personal communication). Much faster convergence - though to a far
looser criterion - has recently been observed for online algorithms (O'Reilly, 1996).
Tempering, on the other hand, is seen here to speed up two batch learning methods by almost an order of magnitude. It reduces not only the average training time but also its coefficient of variation, indicating a more reliable optimization process. Note that tempering
makes simple batch learning with momentum run about twice as fast as the delta-bar-delta
algorithm. This is remarkable since delta-bar-delta uses online measurements to continually adapt the learning rate for each individual weight, whereas tempering merely prescales
it based on the network's architecture. We take this as evidence that tempering establishes
appropriate local step sizes upfront that delta-bar-delta must discover empirically.
This suggests that by using tempering to set the initial (equilibrium) learning rates for deltabar-delta, it may be possible to reap the benefits of both prescaling and adaptive step size
control. Indeed Table 2 confirms that the respective speedups due to tempering and deltabar-delta multiply when the two approaches are combined in this fashion. Finally, the addition of error shunting increases learning speed yet further by allowing the global learning
rate to be brought close to the maximum of 7]*
0.1 that we would predict from (18).

=

8 Discussion
In our experiments we have found tempering to dramatically improve speed and reliability
of learning. More network architectures, data sets and learning algorithms will have to be
"tempered" to explore the general applicability and limitations of this approach; we also
hope to extend it to recurrent networks and online learning. Error shunting has proven useful
in facilitating of near-maximal global learning rates for rapid optimization.

Algorithm
Condition
conventional
with tempering
tempering & shunting

batch & momentum
7]=

mean

st.d.

3.10- 3 2438 ? 1153
1.10- 2 339 ? 95.0
4.10- 2 142?27.1

delta-bar-delta
7]=

mean

st.d.

3.10- 4 696? 218
3.10- 2 89.6 ? 11 .8
9.10- 2 61.7?8.1

Table 2: Epochs required to learn the family relations task.

Tempering Backpropagation Networks: Not All Weights Are Created Equal

569

Although other schemes may speed up backpropagation by comparable amounts, our approach has some unique advantages. It is computationally cheap to implement: local learning and error attenuation rates are invariant with respect to network weights and activities
and thus need to be recalculated only when the network architecture is changed.
More importantly, even advanced gradient descent methods typically retain the isotropic
weight space assumption that we improve upon; one would therefore expect them to benefit from tempering as much as delta-bar-delta did in the experiments reported here. For
instance, tempering could be used to set non-isotropic model-trust regions for conjugate and
second-order gradient descent algorithms.
Finally, by restricting ourselves to fixed learning rates and attenuation factors for now we
have arrived at a simplified method that is likely to leave room for further improvement.
Possible refinements include taking weight vector size into account when attenuating error
signals, or measuring quantities such as (6 2 ) online instead of relying on invariant upper
bounds. How such adaptive tempering schemes will compare to and interact with existing
techniques for efficient backpropagation learning remains to be explored.

Acknowledgements
We would like to thank Peter Dayan, Rich Zemel and Jenny Orr for being instrumental in
discussions that helped shape this work. Geoff Hinton not only offered invaluable comments, but is the source of both our simulator and benchmark problem. N. Schraudolph
received financial support from the McDonnell-Pew Center for Cognitive Neuroscience in
San Diego, and the Robert Bosch Stiftung GmbH.

References
Amari, S.-1. (1995). Learning and statistical inference. In Arbib, M. A., editor, The Handbook of Brain Theory and Neural Networks, pages 522-526. MIT Press, Cambridge.
Battiti, T. (1992). First- and second-order methods for learning: Between steepest descent
and Newton's method. Neural Computation,4(2):141-166.
Haykin, S. (1994). Neural Networks: A Comprehensive Foundation. Macmillan, New York.
Hinton, G. (1986). Learning distributed representations of concepts. In Proceedings of
the Eighth Annual Conference of the Cognitive Science Society, pages 1-12, Amherst
1986. Lawrence Erlbaum, Hillsdale.
Jacobs, R. (1988). Increased rates of convergence through learning rate adaptation. Neural
Networks,1:295-307.
Krogh, A., Thorbergsson, G., and Hertz, J. A. (1990). A cost function for internal representations. In Touretzky, D. S., editor,Advances in Neural Information Processing Systems, volume 2, pages 733-740, Denver, CO, 1989. Morgan Kaufmann, San Mateo.
LeCun, Y. (1993). Efficient learning & second-order methods. Tutorial given at the NIPS
Conference, Denver, CO.
LeCun, Y., Kanter, I., and Solla, S. A. (1991). Second order properties of error surfaces:
Learning time and generalization. In Lippmann, R. P., Moody, J. E., and Touretzky,
D. S., editors, Advances in Neural Information Processing Systems, volume 3, pages
918-924, Denver, CO, 1990. Morgan Kaufmann, San Mateo.
O'Reilly, R. C. (1996). Biologically plausible error-driven learning using local activation
differences: The generalized recirculation algorithm. Neural Computation, 8.
Plaut, D., Nowlan, S., and Hinton, G. (1986). Experiments on learning by back propagation. Technical Report CMU-CS-86-126, Department of Computer Science, Carnegie
Mellon University, Pittsburgh, PA.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 786-solvable-models-of-artificial-neural-networks.pdf

Solvable Models of Artificial Neural
Networks
Sumio Watanabe
Information and Communication R&D Center
Ricoh Co., Ltd.
3-2-3, Shin-Yokohama, Kohoku-ku, Yokohama, 222 Japan
sumio@ipe.rdc.ricoh.co.jp

Abstract
Solvable models of nonlinear learning machines are proposed, and
learning in artificial neural networks is studied based on the theory
of ordinary differential equations. A learning algorithm is constructed, by which the optimal parameter can be found without
any recursive procedure. The solvable models enable us to analyze
the reason why experimental results by the error backpropagation
often contradict the statistical learning theory.

1

INTRODUCTION

Recent studies have shown that learning in artificial neural networks can be understood as statistical parametric estimation using t.he maximum likelihood method
[1], and that their generalization abilities can be estimated using the statistical
asymptotic theory [2]. However, as is often reported, even when the number of
parameters is too large, the error for the test.ing sample is not so large as the theory
predicts. The reason for such inconsistency has not yet been clarified, because it is
difficult for the artificial neural network t.o find the global optimal parameter.
On the other hand, in order to analyze the nonlinear phenomena, exactly solvable
models have been playing a central role in mathematical physics, for example, the
K-dV equation, the Toda lattice, and some statistical models that satisfy the Yang-

423

424

Watanabe

Baxter equation[3].
This paper proposes the first solvable models in the nonlinear learning problem. We
consider simple three-layered neural networks, and show that the parameters from
the inputs to the hidden units determine the function space that is characterized
by a differential equation. This fact means that optimization of the parameters
is equivalent to optimization of the differential equation. Based on this property,
we construct a learning algorithm by which the optimal parameters can be found
without any recursive procedure. Experimental result using the proposed algorithm
shows that the maximum likelihood estimator is not always obtained by the error
backpropagation, and that the conventional statistical learning theory leaves much
to be improved.

2

The Basic Structure of Solvable Models

Let us consider a function fc,w( x) given by a simple neural network with 1 input
unit, H hidden units, and 1 output unit,
H

fc,w(x) =

L CiIPw;{X),

(I)

i=1

where both C = {Ci} and w = {Wi} are parameters to be optimized, IPw;{x) is the
output of the i-th hidden unit.
We assume that {IPi(X) = IPw, (x)} is a set of independent functions in C H-class.
The following theorem is the start point of this paper.
Theorem 1 The H -th order differential equation whose fundamental system of solution is {IPi( x)} and whose H -th order coefficient is 1 is uniquely given by

(Dwg)(x)

=(_l)H H!H+l(g,1P1,1P2,
.. ?,IPH) = 0,
lVH(IP1, IP2, .. ?,IPH)

(2)

where ltVH is the H -th order Wronskian,
IPH
( 1)
IPH
(2)

'PH
(H-l)

'PI

(H-l)

'P2

(H -1)

IPH

For proof, see [4]. From this theorem, we have the following corollary.
Corollary 1 Let g(x) be a C H-class function. Then the following conditions for
g(x) and w = {wd are equivalent.

(1) There exists a set
(2) (Dwg)(x) = O.

C

= {cd

such that g{x)

= E~l CjIPw;(x).

Solvable Models of Artificial Neural Networks

Example 1 Let us consider a case, !Pw;(x)

= exp(WiX).

H

L Ci exp(WiX)

g(x) =

i=l

is equivalent to {DH + P1D H- 1 + P2DH-2 + ... + PH }g(x)
and a set {Pi} is determined from {Wi} by the relation,

= 0, where D = (d/dx)

H

zH

+ Plz H- 1 + P2zH-2 + ... + PIl

= II(z - Wi)

('Vz E C).

i=l

Example 2

(RBF)

A function g(x) is given by radial basis functions,
11

g(x)

=L

Ci exp{ -(x - Wi)2},

i=l

if and only if e- z2 {DIl + P1DIl-l + P2DIl-2 + ... + PIl }(e Z2 g(x)) = 0, where a set
{Pi} is determined from {Wi} by the relation,
11

zll + Plz ll - 1 + P2zll-2

+ ... + PII = II(z -

2Wi) ('Vz E C).

i=l

Figure 1 shows a learning algorithm for the solvable models. When a target function
g( x) is given, let us consider the following function approximation problem.
11

g(x) =

L Ci!Pw;(X) + E(X).

(3)

i=l

Learning in the neural network is optimizing both {cd and {wd such that E( x) is
minimized for some error function. From the definition of D w , eq. (3) is equivalent
to (Dwg)(x) = (Dw?)(x), where the term (Dwg)(x) is independent of Cj. Therefore,
if we adopt IIDwEIl as the error function to be minimized, {wd is optimized by
minimizing IIDwgll, independently of {Cj}, where 111112 = J II(x)1 2dx. After IIDwgll
is minimized, we have (Dw.g)(x) ~ 0, where w* is the optimized parameter. From
the corollary 1, there exists a set {cn such that g(x) ~ L:ci!Pw~(x), where {en
can be found using the ordinary least square method.

3

Solvable Models

For a general function !Pw, the differential operator Dw does not always have such
a simple form as the above examples. In this section, we consider a linear operator
L such that the differential equation of L!pw has a simple form.

Definition A neural network L: Cj!PWi (x) is called solvable ifthere exist functions
a, b, and a linear operator L such that

(L!pwJ(x) = exp{a{wj)x + b(wi)).
The following theorem shows that the optimal parameter of the solvable models can
be found using the same algorithm as Figure 1.

425

426

Watanabe

H

g(X) =

L Ci ~ (x) +E(X)
i=l

equiv.

to

It is difficult
optimize wi
independently ?f ci

t
There exits C i s.t.

=L
H

g(x)

Dw g(x) = Dw E(X)

i

i=l

Ci

<P

.(x)

wi

I
Least Square Method

=L <<P

~

II Dwg II :minimited - - W: optimized

. -.-----1
.
eqmv.

q,* g(x)

0

I

c i : optimized

H

g(x)

i=l

.(x)

wi

Figure 1: St.ructure of Solvable Models

Theorem 2 For a solvable model of a neuml network, the following conditions are
equivalent when Wi "# Wj (i "# j).

= E:!:l Ci<t'w;(X).
that {DH + P1D H- 1 + P2DH-2 + ... + PH }(Lg)(x) =

(1) There exist both {cd and {wd such that g(x)
(2) There exists {Pi} such

O.

(3) For arbitmry Q > 0, we define a sequence {Yn} by Yn = (Lg)(nQ). Then, there
exists {qd such that Yn + qlYn-l + q2Yn-2 + ... + qHYn-H = o.
Note that IIDwLgl12 is a quadratic form for {pd, which is easily minimized by the
least square method. En IYn + qlYn-l + ... + QHYn_HI 2 is also a quadratic form for

{Qd?
Theorem 3 The sequences { wd, {pd, and {qd in the theorem 2 have the following
relations.
H

z H+ PIZ H-l+ P2 ZH-2+ ... + PH

IT(z - a(wi)) ('Vz E C),
i=l

H

zH

+ qlzH-l + q2zH-2 + ... + qH

=

IT(z - exp(a(Wi)Q)) ('Vz E C).
i=l

For proofs of the above theorems, see [5]. These theorems show that, if {Pi} or

Solvable Models of Artificial Neural Networks

{qd is optimized for a given function g( x), then {a( wd} can be found as a set of
solutions of the algebraic equation.
Suppose that a target function g( x) is given. Then, from the above theorems,
the globally optimal parameter w* = {wi} can be found by minimizing IIDwLgll
independently of {cd. Moreover, if the function a(w) is a one-to-one mapping, then
there exists w* uniquely without permutation of {wi}, if and only if the quadratic
form II{DH + P1 DH-1 + ... + PH }g1l2 is not degenerate[4]. (Remark that, if it is
degenerate, we can use another neural network with the smaller number of hidden
units.)
Example 3

A neural network without scaling
H

fb,c(X) =

L CiU(X + bi),

(4)

i=1

is solvable when (F u)( x) I- 0 (a.e.), where F denotes the Fourier transform. Define
(Fg)(x)/(Fu)(x), then, it follows that
a linear operator L by (Lg)(x)

=

H

(Lfb,c)(X)

=L

Ci

exp( -vCi bi x).

(5)

i=l

By the Theorem 2, the optimal {bd can be obtained by using the differential
sequential equation.
Example 4 (MLP)

01'

the

A three-layered perceptron
H

~

fb,c(X) = L

Ci

tan

-1

(

X

i=1

+
bi
a . ),

(6)

z

is solvable. Define a linear operator L by (Lg)( x) = x . (F g)( x), then, it follows
that
H

(Lfb,c)(X)

=L

Ci exp( -(a.i

+ yCi bdx + Q(ai, bd) (x

~ 0).

(7)

i=1

where Q( ai, bi ) is some function of ai and bj. Since the function tan -1 (x) is monotone increasing and bounded, we can expect that a neural network given by eq.
(6) has the same ability in the function approximation problem as the ordinary
three-layered perceptron using the sigmoid function, tanh{x).
Example 5 (Finite Wavelet Decomposition)
H

fb,c(X)

=L

Cju(

x

= (d/dx)n(1/(l + x 2 ?

),

(8)

a.j

i=l

is solvable when u(x)

+ bj

A finite wavelet decomposition

(n

~ 1). Define a lineal' operator

L by

(Lg)(x) = x- n . (Fg)(x) then, it follows that
H

(Lfb,c)(X)

=L
i=1

Ci

exp( -(a.j

+ vCi bi)x + P(a.j, bi?

(x ~ 0).

(9)

427

428

Watanabe

where f3(ai, bi) is some function of ai and bi. Note that O"(x) is an analyzing wavelet,
and that this example shows a method how to optimize parameters for the finite
wavelet decomposition.

4

Learning Algorithm

We construct a learning algorithm for solvable models, as shown in Figure 1-

< <Learning Algorithm> >
(0) A target function g(x) is given.
(1) {Ym} is calculated by Ym = (Lg)(mQ).
(2) {qi} is optimized by minimizing L:m IYm + Q1Ym-l + Q2Ym-2 + ... + QHYm_HI 2.
(3) {Zi} is calculated by solving zH + q1zH-1 + Q2zH-2 + ... + QH = 0.
(4) {wd is determined by a( wd = (l/Q) log Zi.
(5) {cd is optimized by minimizing L:j(g(Xj) - L:i Cj<;?w;(Xj?2.
Strictly speaking, g(x) should be given for arbitrary x. However, in the practical
applicat.ion, if the number of training samples is sufficiently large so that (Lg)( x)
can be almost precisely approximated, this algorithm is available. In the third
procedure, to solve the algebraic equation, t.he DKA method is applied, for example.

5
5.1

Experimental Results and Discussion
The backpropagation and the proposed method

For experiments, we used a probabilit.y density fUllction and a regression function
given by

Q(Ylx)

h(x)

1

((y - h(X?2)
exp -

J27r0"2

1

-3" tan

20"2
-1

X - 1/3
1
-1 X - 2/3
( 0.04 ) + 6" tan ( 0.02 )

where 0" = 0.2. One hundred input samples were set at the same interval in [0,1),
and output samples were taken from the above condit.ional distribution.
Table 1 shows the relation between the number of hidden units, training errors,
and regression errors. In the table, the t.raining errol' in the back propagation shows
the square error obtained after 100,000 training cycles. The traiuing error in the
proposed method shows the square errol' by the above algorithm. And the regression error shows the square error between the true regression curve h( x) and the
estimated curve.
Figure 2 shows the true and estimated regression lines: (0) the true regression
line and sanlple points, (1) the estimated regression line with 2 hidden units, by
the BP (the error backpropagation) after 100,000 training cycles, (2) the estimated
regression line with 12 hidden units, by the BP after 100,000 training cycles, (3) the

Solvable Models of Artificial Neural Networks

Table 1: Training errors and regression errors
Hidden
Units
2
4
6
8
10
12

Backpropagation
Training Regression
0.7698
4.1652
3.3464
0.4152
0.4227
3.3343
0.4189
3.3267
3.3284
0.4260
3.3170
0.4312

Proposed Method
Training Regression
4.0889
0.3301
3.8755
0.2653
3.5368
0.3730
3.2237
0.4297
3.2547
0.4413
3.1988
0.5810

estimated line with 2 hidden units by the proposed method, and (4) the estimated
line with 12 hidden units by the proposed method.

5.2

Discussion

When the number of hidden units was small, the training errors by the BP were
smaller, but the regression errors were larger. Vlhen the number of hidden units
was taken to be larger, the training error by the BP didn't decrease so much as the
proposed method, and the regression error didn't increase so mnch as the proposed
method.
By the error back propagation , parameters dichl 't reach the maximum likelihood
estimator, or they fell into local minima. However, when t.he number of hidden
units was large, the neural network wit.hout. t.he maximum likelihood estimator
attained the bett.er generalization. It seems that paramet.ers in the local minima
were closer to the true parameter than the maximum likelihood estimator.
Theoretically, in the case of the layered neural networks, the maximum likelihood
estimator may not be subject to asymptotically normal distribution because the
Fisher informat.ion matrix may be degenerate. This can be one reason why the
experimental results contradict the ordinary st.atistical theory. Adding such a problem, the above experimental results show that the local minimum causes a strange
problem. In order to construct the more precise learning t.heory for the backpropagation neural network, and to choose the better parameter for generalization, we
maybe need a method to analyze lea1'1ling and inference with a local minimum.

6

Conclusion

We have proposed solvable models of artificial neural networks, and studied their
learning structure. It has been shown by the experimental results that the proposed
method is useful in analysis of the neural network generalizat.ion problem.

429

430

Watanabe

........'..'
: ..

~--------.

'.

H : the number of hidden units

....

.'"

".'

...
'

..

t.he t.raining error
E"eg : the regression error
Etrain :

"0

..

(0) True Curve and Samples.
Sample error sum = 3.6874

.
.,
......
.
.

"0

e"

~
....... ~
..: ...........:......::::.. .
"0,

:

..: .... "... ".

e" e " '

..

'.

...

'

.' . '..
(1) BP after 100,000 cycles
H = 2, Etrain = 4.1652, E"eg = 0.7698

. . . ...

....." :

. .
,'.

..

..

'.'

.

.....

?

?

'

.

0"

(2) TIP aft.er 100,000 cycles
H = 12, E Ir?a;" = 3.3170, E"eg = 0.4312

.

,".

..

'

'

(3) Proposed Method
H = 2, Etrain = 4.0889, Ereg

= 0.3301

?

......

. .:'{:

..

(4) Proposed Met.hod
H = 12, E'm;" = 3.1988, Ereg = 0.5810

Figure 2: Experimental Results

References
[I] H. White. (1989) Learning in artificial neural networks: a statistical perspective.
Neural Computation, 1, 425-464.
[2] N.Murata, S.Yoshizawa, and S.-I.Amari.(1992) Learning Curves, Model Selection
and Complexity of Neural Networks. Adlla:nces in Neural Information Processing
Systems 5, San Mateo, Morgan Kaufman, pp.607-614.
[3] R. J. Baxter. (1982) Exactly Solved Models in Statistical Mechanics, Academic
Press.
[4] E. A. Coddington. (1955) Th.eory of ordinary differential equations, the McGrawHill Book Company, New York.
[5] S. Watanabe. (1993) Function approximation by neural networks and solution
spaces of differential equations. Submitted to Neural Networks.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 6221-memory-efficient-backpropagation-through-time.pdf

Memory-Efficient Backpropagation Through Time

?
Audrunas
Gruslys
Google DeepMind
audrunas@google.com

R?mi Munos
Google DeepMind
munos@google.com

Marc Lanctot
Google DeepMind
lanctot@google.com

Ivo Danihelka
Google DeepMind
danihelka@google.com

Alex Graves
Google DeepMind
gravesa@google.com

Abstract
We propose a novel approach to reduce memory consumption of the backpropagation through time (BPTT) algorithm when training recurrent neural networks
(RNNs). Our approach uses dynamic programming to balance a trade-off between
caching of intermediate results and recomputation. The algorithm is capable of
tightly fitting within almost any user-set memory budget while finding an optimal
execution policy minimizing the computational cost. Computational devices have
limited memory capacity and maximizing a computational performance given a
fixed memory budget is a practical use-case. We provide asymptotic computational
upper bounds for various regimes. The algorithm is particularly effective for long
sequences. For sequences of length 1000, our algorithm saves 95% of memory
usage while using only one third more time per iteration than the standard BPTT.

1

Introduction

Recurrent neural networks (RNNs) are artificial neural networks where connections between units
can form cycles. They are often used for sequence mapping problems, as they can propagate hidden
state information from early parts of the sequence back to later points. LSTM [9] in particular
is an RNN architecture that has excelled in sequence generation [3, 13, 4], speech recognition
[5] and reinforcement learning [12, 10] settings. Other successful RNN architectures include the
differentiable neural computer (DNC) [6], DRAW network [8], and Neural Transducers [7].
Backpropagation Through Time algorithm (BPTT) [11, 14] is typically used to obtain gradients
during training. One important problem is the large memory consumption required by the BPTT.
This is especially troublesome when using Graphics Processing Units (GPUs) due to the limitations
of GPU memory.
Memory budget is typically known in advance. Our algorithm balances the tradeoff between memorization and recomputation by finding an optimal memory usage policy which minimizes the total
computational cost for any fixed memory budget. The algorithm exploits the fact that the same
memory slots may be reused multiple times. The idea to use dynamic programming to find a provably
optimal policy is the main contribution of this paper.
Our approach is largely architecture agnostic and works with most recurrent neural networks. Being
able to fit within limited memory devices such as GPUs will typically compensate for any increase in
computational cost.

2

Background and related work

In this section, we describe the key terms and relevant previous work for memory-saving in RNNs.
30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

Definition 1. An RNN core is a feed-forward neural network which is cloned (unfolded in time)
repeatedly, where each clone represents a particular time point in the recurrence.
For example, if an RNN has a single hidden layer whose outputs feed back into the same hidden
layer, then for a sequence length of t the unfolded network is feed-forward and contains t RNN cores.
Definition 2. The hidden state of the recurrent network is the part of the output of the RNN core
which is passed into the next RNN core as an input.
In addition to the initial hidden state, there exists a single hidden state per time step once the network
is unfolded.
Definition 3. The internal state of the RNN core for a given time-point is all the necessary information required to backpropagate gradients over that time step once an input vector, a gradient with
respect to the output vector, and a gradient with respect to the output hidden state is supplied. We
define it to also include an output hidden state.
An internal state can be (re)evaluated by executing a single forward operation taking the previous
hidden state and the respective entry of an input sequence as an input. For most network architectures,
the internal state of the RNN core will include a hidden input state, as this is normally required to
evaluate gradients. This particular choice of the definition will be useful later in the paper.
Definition 4. A memory slot is a unit of memory which is capable of storing a single hidden state
or a single internal state (depending on the context).
2.1

Backpropagation through Time

Backpropagation through Time (BPTT) [11, 14] is one of the commonly used techniques to train
recurrent networks. BPTT ?unfolds? the neural network in time by creating several copies of the
recurrent units which can then be treated like a (deep) feed-forward network with tied weights. Once
this is done, a standard forward-propagation technique can be used to evaluate network fitness over
the whole sequence of inputs, while a standard backpropagation algorithm can be used to evaluate
partial derivatives of the loss criteria with respect to all network parameters. This approach, while
being computationally efficient is also fairly intensive in memory usage. This is because the standard
version of the algorithm effectively requires storing internal states of the unfolded network core at
every time-step in order to be able to evaluate correct partial derivatives.
2.2

Trading memory for computation time

The general idea of trading computation time and memory consumption in general computation
graphs has been investigated in the automatic differentiation community [2]. Recently, the rise of
deep architectures and recurrent networks has increased interest in a less general case where the
graph of forward computation is a chain and gradients have to be chained in a reverse order. This
simplification leads to relatively simple memory-saving strategies and heuristics. In the context of
BPTT, instead of storing hidden network states, some of the intermediate results can be recomputed
on demand by executing an extra forward operation.
?
Chen et. al. proposed subdividing the sequence of size t into t equal parts and memorizing only
hidden
? states between the subsequences and all internal states within each segment [1]. This uses
O( t) memory at the cost of making an additional forward pass on average, as once the errors are
backpropagated through the right-side of the sequence, the second-last subsequence
has to be restored
?
by repeating a number of forward operations. We refer to this as Chen?s t algorithm.
The authors also suggest applying the same technique recursively several times by sub-dividing the
sequence into k equal parts and terminating the recursion once the subsequence length becomes less
than k. The authors have established that this would lead to memory consumption of O(k logk+1 (t))
and computational complexity of O(t logk (t)). This algorithm has a minimum possible memory
usage of log2 (t) in the case when k = 1. We refer to this as Chen?s recursive algorithm.

3

Memory-efficient backpropagation through time

We first discuss two simple examples: when memory is very scarce, and when it is somewhat limited.
2

When memory is very scarce, it is straightforward to design a simple but computationally inefficient
algorithm for backpropagation of errors on RNNs which only uses a constant amount of memory.
Every time when the state of the network at time t has to be restored, the algorithm would simply
re-evaluate the state by forward-propagating inputs starting from the beginning until time t. As
backpropagation happens in the reverse temporal order, results from the previous forward steps can
not be reused (as there is no memory to store them). This would require repeating t forward steps
before backpropagating gradients one step backwards (we only remember inputs and the initial state).
This would produce an algorithm requiring t(t + 1)/2 forward passes to backpropagate errors over t
time steps. The algorithm would be O(1) in space and O(t2 ) in time.
When the memory is somewhat limited (but not very scarce) we may store only hidden RNN states
at all time points. When errors have to be backpropagated from time t to t ? 1, an internal RNN
core state can be re-evaluated by executing another forward operation taking the previous hidden
state as an input. The backward operation can follow immediately. This approach can lead to fairly
significant memory savings, as typically the recurrent network hidden state is much smaller than an
internal state of the network core itself. On the other hand this leads to another forward operation
being executed during the backpropagation stage.
3.1

Backpropagation though time with selective hidden state memorization (BPTT-HSM)

The idea behind the proposed algorithm is to compromise between two previous extremes. Suppose
that we want to forward and backpropagate a sequence of length t, but we are only able to store m
hidden states in memory at any given time. We may reuse the same memory slots to store different
hidden states during backpropagation. Also, suppose that we have a single RNN core available for
the purposes of intermediate calculations which is able to store a single internal state. Define C(t, m)
as a computational cost of backpropagation measured in terms of how many forward-operations one
has to make in total during forward and backpropagation steps combined when following an optimal
memory usage policy minimizing the computational cost. One can easily set the boundary conditions:
C(t, 1) = 12 t(t + 1) is the cost of the minimal memory approach, while C(t, m) = 2t ? 1 for all
m ? t when memory is plentiful (as shown in Fig. 3 a). Our approach is illustrated in Figure 1. Once
we start forward-propagating steps at time t = t0 , at any given point y > t0 we can choose to put the
current hidden state into memory (step 1). This step has the cost of y forward operations. States will
be read in the reverse order in which they were written: this allows the algorithm to store states in a
stack. Once the state is put into memory at time y = D(t, m), we can reduce the problem into two
parts by using a divide-and-conquer approach: running the same algorithm on the t > y side of the
sequence while using m ? 1 of the remaining memory slots at the cost of C(t ? y, m ? 1) (step 2),
and then reusing m memory slots when backpropagating on the t ? y side at the cost of C(y, m)
(step 3). We use a full size m memory capacity when performing step 3 because we could release the
hidden state y immediately after finishing step 2.
Legend

Step 1: cost = y

1

2

...

y

y+1

...

Hidden state is propagated
Gradients get back-propagated
Hidden state stored in memory

t
t

Internal state of RNN core at time t
A single forward operation

Step 2: cost = C(t-y, m-1)

y+1

...

t

A single backward operation
Recursive application
of the algorithm

1

2

...

y

Hidden state is read from memory

Step 3: cost = C(y, m)

Hidden state is saved in memory
Hidden state is removed from memory

Figure 1: The proposed divide-and-conquer approach.
The base case for the recurrent algorithm is simply a sequence of length t = 1 when forward and
backward propagation may be done trivially on a single available RNN network core. This step has
the cost C(1, m) = 1.
3

(a) Theoretical computational cost
measured in number of forward operations per time step.

(b) Measured computational cost in
miliseconds.

Figure 2: Computational cost per time-step when the algorithm is allowed to remember 10 (red), 50
(green), 100 (blue), 500 (violet), 1000 (cyan) hidden states. The grey line shows the performance
of standard BPTT without memory constraints; (b) also includes a large constant value caused by a
single backwards step per time step which was excluded from the theoretical computation, which
value makes a relative performance loss much less severe in practice than in theory.
Having established the protocol we may find an optimal policy D(t, m). Define the cost of choosing
the first state to be pushed at position y and later following the optimal policy as:
Q(t, m, y) = y + C(t ? y, m ? 1) + C(y, m)

C(t, m) = Q(t, m, D(t, m))

1

1

time

D(t, m) = argmin Q(t, m, y)

(2)

1

(3)

1?y<t

t

1

1
11

CPTS ? 2
1

(1)

1

1

time

CPTS ? 3
Hidden state stored in memory

1

Forward computation
Backward computation

a)

Order of execution

b)

CPTS cost per time step in the number
of forward operations

Figure 3: Illustration of the optimal policy for m = 4 and a) t = 4 and b) t = 10. Logical sequence
time goes from left to right, while execution happens from top to the bottom.
Equations can be solved exactly by using dynamic programming subject to the boundary conditions
established previously (e.g. as in Figure 2(a)). D(t, m) will determine the optimal policy to follow.
Pseudocode is given in the supplementary material. Figure 3 illustrates an optimal policy found for
two simple cases.
3.2

Backpropagation though time with selective internal state memorization (BPTT-ISM)

Saving internal RNN core states instead of hidden RNN states would allow us to save a single forward
operation during backpropagation in every divide-and-conquer step, but at a higher memory cost.
Suppose we have a memory capacity capable of saving exactly m internal RNN states. First, we need
to modify the boundary conditions: C(t, 1) = 12 t(t + 1) is a cost reflecting the minimal memory
approach, while C(t, m) = t for all m ? t when memory is plentiful (equivalent to standard BPTT).
4

As previously, C(t, m) is defined to be the computational cost for combined forward and backward
propagations over a sequence of length t with memory allowance m while following an optimal
memory usage policy. As before, the cost is measured in terms of the amount of total forward steps
made, because the number of backwards steps is constant. Similarly to BPTT-HSM, the process
can be divided into parts using divide-and-conquer approach (Fig 4). For any values of t and m
position of the first memorization y = D(t, m) is evaluated. y forward operations are executed and
an internal RNN core state is placed into memory. This step has the cost of y forward operations
(Step 1 in Figure 4). As the internal state also contains an output hidden state, the same algorithm can
be recurrently run on the high-time (right) side of the sequence while having one less memory slot
available (Step 2 in Figure 4). This step has the cost of C(t ? y, m ? 1) forward operations. Once
gradients are backpropagated through the right side of the sequence, backpropagation can be done
over the stored RNN core (Step 3 in Figure 4). This step has no additional cost as it involves no more
forward operations. The memory slot can now be released leaving m memory available. Finally, the
same algorithm is run on the left-side of the sequence (Step 4 in Figure 4). This final step has the cost
of C(y ? 1, m) forward operations. Summing the costs gives us the following equation:
Q(t, m, y) = y + C(y ? 1, m) + C(t ? y, m ? 1)

(4)

Recursion has a single base case: backpropagation over an empty sequence is a nil operation which
has no computational cost making C(0, m) = 0.
Legend

Step 1: cost = y

1

...

y-1

y

y+1

...

t

Hidden state gets propagated

t

Internal stat of RNN core at time t
Internal RNN core state stored in
memory (incl. output hidden state)

Step 2:
cost = C(t-y, m-1)

y

y+1

...

t

Hidden state is read from memory
Internal state is saved
Internal state is removed

Step 3:
cost = 0

y

A single backwards
operation, no forward
operations involved.

Recursive application
of the algorithm
Gradients get back-propagated
A single initial RNN hidden state

1

...

y-1

y

Step 4:
cost = C(y-1, m)

A single forward operation
A single backward operation

Figure 4: Illustration of the divide-and-conquer approach used by BPTT-ISM.
Compared to the previous section (20) stays the same while (19) is minimized over 1 ? y ? t instead
of 1 ? y < t. This is because it is meaningful to remember the last internal state while there was
no reason to remember the last hidden state. A numerical solution of C(t, m) for several different
memory capacities is shown in Figure 5(a).
D(t, m) = argmin Q(t, m, y)

(5)

1?y?t

As seen in Figure 5(a), our methodology saves 95% of memory for sequences of 1000 (excluding input
vectors) while using only 33% more time per training-iteration than the standard BPTT (assuming a
single backward step being twice as expensive as a forward step).
3.3

Backpropagation though time with mixed state memorization (BPTT-MSM)

It is possible to derive an even more general model by combining both approaches as described in
Sections 3.1 and 3.2. Suppose we have a total memory capacity m measured in terms of how much a
single hidden states can be remembered. Also suppose that storing an internal RNN core state takes
? times more memory where ? ? 2 is some integer number. We will choose between saving a single
hidden state while using a single memory unit and storing an internal RNN core state by using ?
times more memory. The benefit of storing an internal RNN core state is that we will be able to save
a single forward operation during backpropagation.
Define C(t, m) as a computational cost in terms of a total amount of forward operations when
running an optimal strategy. We use the following boundary conditions: C(t, 1) = 12 t(t + 1) as a
5

(a) BPTT-ISM (section 3.2).

(b) BPTT-MSM (section 3.3).

Figure 5: Comparison of two backpropagation algorithms in terms of theoretical costs. Different
lines show the number of forward operations per time-step when the memory capacity is limited to
10 (red), 50 (green), 100 (blue), 500 (violet), 1000 (cyan) internal RNN core states. Please note that
the units of memory measurement are different than in Figure 2(a) (size of an internal state vs size of
a hidden state). It was assumed that the size of an internal core state is ? = 5 times larger than the
size of a hidden state. The value of ? influences only the right plot. All costs shown on the right plot
should be less than the respective costs shown on the left plot for any value of ?.
cost reflecting the minimal memory approach, while C(t, m) = t for all m ? ?t when memory is
plentiful and C(t ? y, m) = ? for all m ? 0 and C(0, m) = 0 for notational convenience. We use
a similar divide-and-conquer approach to the one used in previous sections.
Define Q1 (t, m, y) as the computational cost if we choose to firstly remember a hidden state at
position y and thereafter follow an optimal policy (identical to ( 18)):
Q1 (t, m, y) = y + C(y, m) + C(t ? y, m ? 1)

(6)

Similarly, define Q2 (t, m, y) as the computational cost if we choose to firstly remember an internal
state at position y and thereafter follow an optimal policy (similar to ( 4) except that now the internal
state takes ? memory units):
Q2 (t, m, y) = y + C(y ? 1, m) + C(t ? y, m ? ?)

(7)

Define D1 as an optimal position of the next push assuming that the next state to be pushed is a
hidden state and define D2 as an optimal position if the next push is an internal core state. Note that
D2 has a different range over which it is minimized, for the same reasons as in equation 5:
D1 (t, m) = argmin Q1 (t, m, y)

D2 (t, m) = argmin Q2 (t, m, y)

1?y<t

1?y?t

(8)

Also define Ci (t, m) = Qi (t, m, D(t, m)) and finally:
C(t, m) = min Ci (t, m)
i

H(t, m) = argmin Ci (t, m)

(9)

i

We can solve the above equations by using simple dynamic programming. H(t, m) will indicate
whether the next state to be pushed into memory in a hidden state or an internal state, while the
respective values if D1 (t, m) and D2 (t, m) will indicate the position of the next push.
3.4

Removing double hidden-state memorization

Definition 3 of internal RNN core state would typically require for a hidden input state to be included
for each memorization. This may lead to the duplication of information. For example, when an
optimal strategy is to remember a few internal RNN core states in sequence, a memorized hidden
output of one would be equal to a memorized hidden input for the other one (see Definition 3).
Every time we want to push an internal RNN core state onto the stack and a previous internal state is
already there, we may omit pushing the input hidden state. Recall that an internal core RNN state
when an input hidden state is otherwise not known is ? times larger than a hidden state. Define ? ? ?
as the space required to memorize the internal core state when an input hidden state is known. A
6

relationship between ? and ? is application-specific, but in many circumstances ? = ? + 1. We only
have to modify (7) to reflect this optimization:
Q2 (t, m, y) = y + C(y ? 1, m) + C(t ? y, m ? 1y>1 ? ? 1y=1 ?)

(10)

1 is an indicator function. Equations for H(t, m), Di (t, m) and C(t, m) are identical to (8) and (9).
3.5

Analytical upper bound for BPTT-HSM
1

We have established a theoretical upper bound for BPTT-HSM algorithm as C(t, m) ? mt1+ m . As
1
the bound is not tight for short sequences, it was also numerically verified that C(t, m) < 4t1+ m for
1
5
3
1+ m
t < 10 and m < 10 , or less than 3t
if the initial forward pass is excluded. In addition to that,
m
we have established a different bound in the regime where t < mm! . For any integer value a and for
a
all t < ma! the computational cost is bounded by C(t, m) ? (a + 1)t. The proofs are given in the
supplementary material. Please refer to supplementary material for discussion on the upper bounds
for BPTT-MSM and BPTT-ISM.
3.6

Comparison of the three different strategies

(a) Using 10? memory

(b) Using 20? memory

Figure 6: Comparison of three strategies in the case when a size of an internal RNN core state is
? = 5 times larger than that of the hidden state, and the total memory capacity allows us remember
either 10 internal RNN states, or 50 hidden states or any arbitrary mixture of those in the left plot
and (20, 100) respectively in the right plot. The red curve illustrates BPTT-HSM, the green curve
- BPTT-ISM and the blue curve - BPTT-MSM. Please note that for large sequence lengths the red
curve out-performs the green one, and the blue curve outperforms the other two.
Computational costs for each previously described strategy and the results are shown in Figure 6.
BPTT-MSM outperforms both BPTT-ISM and BPTT-HSM. This is unsurprising, because the search
space in that case is a superset of both strategy spaces, while the algorothm finds an optimal strategy
within that space. Also, for a fixed memory capacity, the strategy memorizing only hidden states
outperforms a strategy memorizing internal RNN core states for long sequences, while the latter
outperforms the former for relatively short sequences.

4

Discussion

We used an LSTM mapping 256 inputs to 256 with a batch size of 64 and measured execution time for
a single gradient descent step (forward and backward operation combined) as a function of sequence
length (Figure 2(b)). Please note that measured computational time also includes the time taken by
backward operations at each time-step which dynamic programming equations did not take into the
account. A single backward operation is usually twice as expensive than a forward operation, because
it involves evaluating gradients both with respect to input data and internal parameters. Still, as the
number of backward operations is constant it has no impact on the optimal strategy.
4.1

Optimality

The dynamic program finds the optimal computational strategy by construction, subject to memory
constraints and a fairly general model that we impose. As both strategies proposed by [1] are
7

consistent with all the assumptions that we have made in section 3.4 when applied to RNNs, BPTTMSM is guaranteed to perform at least as well under any memory budget and any sequence length.
This is because strategies proposed by [1] can be expressed by providing a (potentially suboptimal)
policy Di (t, m), H(t, m) subject to the same equations for Qi (t, m).
4.2

Numerical comparison with Chen?s

?

t algorithm

?
?
?
Chen?s t algorithm requires to remember t hidden states and t internal RNN states (excluding
input hidden states), while the recursive approach requires to remember at least log2 t hidden states.
In other words, the model does not allow for a fine-grained control over memory usage and rather
saves some memory. In the meantime our proposed BPTT-MSM can fit within almost arbitrary
constant memory constraints, and this is the main advantage of our algorithm.

?
Figure 7: Left: memory consumption divided by t(1 + ?) for a fixed computational
cost C = 2.
?
Right: computational cost per time-step for a fixed memory consumption of t(1 + ?). Red, green
and blue curves correspond to ? = 2, 5, 10 respectively.
?
The non-recursive Chen?s t approach does not allow to match any particular memory budget
making a like-for-like comparison difficult. Instead of fixing the memory budge, it?is possible to fix
computational cost at 2 forwards iterations on average to match the cost of the
? t algorithm and
observe how much memory
would
our
approach
use.
Memory
usage
by
the
t algorithm would
?
?
be equivalent to saving t hidden states and t internal core states. Lets suppose that the internal
RNN core state is ? times larger than hidden states. In this case the size of the internal RNN core
state excluding
state is ? = ? ? 1. This would
give a memory?usage of Chen?s
?
? the input hidden
?
algorithm as t(1 + ?) = t(?), as it needs to remember t hidden states and t internal states
where input hidden states?can be omitted to avoid duplication. Figure 7 illustrates memory usage by
our algorithm divided by t(1 + ?) for a fixed execution speed of 2 as a function of sequence length
and for different values of parameter ?. Values lower than 1 indicate memory savings. As it is seen,
we can save a significant amount of memory for the same computational cost.
?
Another experiment is to measure computational cost for a fixed memory consumption
? of t(1 + ?).
The results are shown in Figure 7. Computational cost of 2 corresponds to Chen?s t algorithm. This
illustrates that our approach
? does not perform significantly faster (although it does not do any worse).
This is because Chen?s t strategy is actually near optimal for this particular memory budget. Still,
as seen from the previous paragraph, this memory budget is already in the regime of diminishing
returns and further memory reductions are possible for almost the same computational cost.

5

Conclusion

In this paper, we proposed a novel approach for finding optimal backpropagation strategies for
recurrent neural networks for a fixed user-defined memory budget. We have demonstrated that the
most general of the algorithms is at least as good as many other used common heuristics. The main
advantage of our approach is the ability to tightly fit to almost any user-specified memory constraints
gaining maximal computational performance.
8

References
[1] Tianqi Chen, Bing Xu, Zhiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear
memory cost. arXiv preprint arXiv:1604.06174, 2016.
[2] Benjamin Dauvergne and Laurent Hasco?t. The data-flow equations of checkpointing in reverse
automatic differentiation. In Computational Science?ICCS 2006, pages 566?573. Springer,
2006.
[3] Douglas Eck and Juergen Schmidhuber. A first look at music composition using LSTM recurrent
neural networks. Istituto Dalle Molle Di Studi Sull Intelligenza Artificiale, 103, 2002.
[4] Alex Graves. Supervised Sequence Labelling with Recurrent Neural Networks. Studies in
Computational Intelligence. Springer, 2012.
[5] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep
recurrent neural networks. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE
International Conference on, pages 6645?6649. IEEE, 2013.
[6] Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka GrabskaBarwi?nska, Sergio G?mez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou,
Adri? Puigdom?nech Badia, Karl Moritz Hermann, Yori Zwols, Georg Ostrovski, Adam Cain,
Helen King, Christopher Summerfield, Phil Blunsom, Koray Kavukcuoglu, and Demis Hassabis.
Hybrid computing using a neural network with dynamic external memory. Nature, advance
online publication, October 2016.
[7] Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, and Phil Blunsom. Learning to
transduce with unbounded memory. In Advances in Neural Information Processing Systems,
pages 1819?1827, 2015.
[8] Karol Gregor, Ivo Danihelka, Alex Graves, and Daan Wierstra. DRAW: A recurrent neural
network for image generation. arXiv preprint arXiv:1502.04623, 2015.
[9] Sepp Hochreiter and J?rgen Schmidhuber. Long short-term memory. Neural computation,
9(8):1735?1780, 1997.
[10] Volodymyr Mnih, Adri? Puigdom?nech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap,
Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Proceedings of the 33rd International Conference on Machine Learning
(ICML), pages 1928?1937, 2016.
[11] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning internal representations by error propagation. Technical report, DTIC Document, 1985.
[12] Ivan Sorokin, Alexey Seleznev, Mikhail Pavlov, Aleksandr Fedorov, and Anastasiia Ignateva.
Deep attention recurrent Q-network. arXiv preprint arXiv:1512.01693, 2015.
[13] Ilya Sutskever, James Martens, and Geoffrey E Hinton. Generating text with recurrent neural
networks. In Proceedings of the 28th International Conference on Machine Learning (ICML-11),
pages 1017?1024, 2011.
[14] Paul J Werbos. Backpropagation through time: what it does and how to do it. Proceedings of
the IEEE, 78(10):1550?1560, 1990.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 815-hoo-optimality-criteria-for-lms-and-backpropagation.pdf

Hoo Optimality Criteria for LMS and
Backpropagation

Babak Hassibi
Information Systems Laboratory
Stanford University
Stanford, CA 94305

Ali H. Sayed
Dept. of Elec. and Compo Engr.
University of California Santa Barbara
Santa Barbara, CA 93106

Thomas Kailath
Information Systems Laboratory
Stanford University
Stanford, CA 94305

Abstract
We have recently shown that the widely known LMS algorithm is
an H OO optimal estimator. The H OO criterion has been introduced,
initially in the control theory literature, as a means to ensure robust performance in the face of model uncertainties and lack of
statistical information on the exogenous signals. We extend here
our analysis to the nonlinear setting often encountered in neural
networks, and show that the backpropagation algorithm is locally
H OO optimal. This fact provides a theoretical justification of the
widely observed excellent robustness properties of the LMS and
backpropagation algorithms. We further discuss some implications
of these results.

1

Introduction

The LMS algorithm was originally conceived as an approximate recursive procedure
that solves the following problem (Widrow and Hoff, 1960): given a sequence of n x 1
input column vectors {hd, and a corresponding sequence of desired scalar responses
{di }, find an estimate of an n x 1 column vector of weights w such that the sum
of squared errors, L:~o Idi w1 2 , is minimized. The LMS solution recursively

hi

351

352

Hassibi. Sayed. and Kailath

updates estimates of the weight vector along the direction of the instantaneous gradient of the squared error. It has long been known that LMS is an approximate
minimizing solution to the above least-squares (or H2) minimization problem. Likewise, the celebrated backpropagation algorithm (Rumelhart and McClelland, 1986)
is an extension of the gradient-type approach to nonlinear cost functions of the form
2:~o Id i - hi (W ) 12 , where hi ( .) are known nonlinear functions (e. g., sigmoids). It
also updates the weight vector estimates along the direction of the instantaneous
gradients.
We have recently shown (Hassibi, Sayed and Kailath, 1993a) that the LMS algorithm is an H<Xl-optimal filter, where the H<Xl norm has recently been introduced
as a robust criterion for problems in estimation and control (Zames, 1981). In general terms, this means that the LMS algorithm, which has long been regarded as
an approximate least-mean squares solution, is in fact a minimizer of the H<Xl error
norm and not of the JI2 norm. This statement will be made more precise in the
next few sections. In this paper, we extend our results to a nonlinear setting that
often arises in the study of neural networks, and show that the backpropagation
algorithm is a locally H<Xl-optimal filter. These facts readily provide a theoretical
justification for the widely observed excellent robustness and tracking properties of
the LMS and backpropagation algorithms, as compared to, for example, exact least
squares methods such as RLS (Haykin, 1991).
In this paper we attempt to introduce the main concepts, motivate the results, and
discuss the various implications. \Ve shall, however, omit the proofs for reasons of
space. The reader is refered to (Hassibi et al. 1993a), and the expanded version of
this paper for the necessary details.

2

Linear HOO Adaptive Filtering

\Ve shall begin with the definition of the H<Xl norm of a transfer operator. As
will presently become apparent, the motivation for introducing the H<Xl norm is to
capture the worst case behaviour of a system.
Let h2 denote the vector space of square-summable complex-valued causal sequences
{fk, 0 :::; k < oo}, viz.,
<Xl
h2 = {set of sequences {fk} such that
f; fk < oo}
k=O

L

=

with inner product < {Ik}, {gd >
2:~=o f; gk ,where * denotes complex
conjugation. Let T be a transfer operator that maps an input sequence {ud to an
output sequence {yd. Then the H<Xl norm of T is equal to
IITII<Xl =

sup
utO,uEh 2

IIyl12
II u l1 2

where the notation 111/.112 denotes the h 2 -norm of the causal sequence

{ttd,

viz.,

2 ~<Xl *
Ilull:? = L...Jk=o ttkUk
The H<Xl norm may thus be regarded as the maximum energy gain from the input
u to the output y.

Hoc Optimality Criteria for LMS and Backpropagation

Suppose we observe an output sequence {dd that obeys the following model:
di

= hT W + Vi

(1)

where hT = [hi1 hi2
hin ] is a known input vector, W is an unknown weight
vector, and {Vi} is an unknown disturbance, which may also include modeling errors.
We shall not make any assumptions on the noise sequence {vd (such as whiteness,
normally distributed, etc.).
Let Wi = F(d o, di, ... , di) denote the estimate of the weight vector W given the
observations {dj} from time 0 up to and including time i. The objective is to
determine the functional F, and consequently the estimate Wi, so as to minimize a
certain norm defined in terms of the prediction error
ei = hT W - hT Wi-1

which is the difference between the true (uncorrupted) output hT wand the predicted output hT Wi -1. Let T denote the transfer operator that maps the unknowns
{w - W_1, {vd} (where W-1 denotes an initial guess of w) to the prediction errors
{ed. The HOO estimation problem can now be formulated as follows.

Problem 1 (Optimal HOC Adaptive Problem) Find an Hoc -optimal estimation strategy Wi
F(d o, d 1, ... , d i ) that minimizes IITlloc' and obtain the resulting

=

!~

= inf IITII!:, = inf
:F

:F

(2)

sup
w,vEh 2

=

where Iw - w_11 2
(w - w-1f (w - W-1), and J1- is a positive constant that reflects
apriori knowledge as to how close w is to the initial guess W-1 .

Note that the infimum in (2) is taken over all causal estimators F. The above
problem formulation shows that HOC optimal estimators guarantee the smallest
prediction error energy over all possible disturbances offixed energy. Hoc estimators
are thus over conservative, which reflects in a more robust behaviour to disturbance
variation.
Before stating our first result we shall define the input vectors {hd exciting if, and
only if,
N

lim

N-+oc

L hT hi =

00

i=O

Theoreln 1 (LMS Algorithm) Consider the model (1), and suppose we wish to
minimize the Hoc norm of the transfer operator from the unknowns w - W-1 and
Vi to the prediction errors ei. If the input vectors hi are exciting and

o < J1- < i~f h:h.

(3)

tit

then the minimum H oo norm is !Opt = 1. In this case an optimal Hoo estimator is
given by the LA-IS alg01'ithm with learning rate J1-, viz.

(4)

353

354

Hassibi, Sayed, and Kailath

In other words, the result states that the LMS algorithm is an H oo -optimal filter.
Moreover, Theorem 1 also gives an upper bound on the learning rate J-t that ensures
the H oo optimality of LMS. This is in accordance with the well-known fact that
LMS behaves poorly if the learning rate is too large.
Intuitively it is not hard to convince oneself that "'{opt cannot be less than one. To
this end suppose that the estimator has chosen some initial guess W-l. Then one
may conceive of a disturbance that yields an observation that coincides with the
output expected from W-l, i.e.
hT W-l = hT W + Vi = di
In this case one expects that the estimator will not change its estimate of w, so that
Wi
W-l for all i. Thus the prediction error is

=

ei

= hTw -

hTwi-l

= hTw -

hTw-l

= -Vi

and the ratio in (2) can be made arbitrarily close to one.
The surprising fact though is that "'{opt is one and that the LMS algorithm achieves
it. What this means is that LMS guarantees that the energy of the prediction
error will never exceed the energy of the disturbances. This is not true for other
estimators. For example, in the case of the recursive least-squares (RLS) algorithm,
one can come up with a disturbance of arbitrarily small energy that will yield a
prediction error of large energy.
To demonstrate this, we consider a special case of model (1) where hi is now a
scalar that randomly takes on the values + 1 or -1. For this model J-t must be less
than 1 and we chose the value J-t
.9. We compute the Hoo norm of the transfer
operator from the disturbances to the prediction errors for both RLS and LMS. We
also compute the worst case RLS disturbance, and show the resulting prediction
errors. The results are illustrated in Fig. 1. As can be seen, the H OO norm in
the RLS case increases with the number of observations, whereas in the LMS case
it remains constant at one. Using the worst case RLS disturbance, the prediction
error due to the LMS algorithm goes to zero, whereas the prediction error due to
the RLS algorithm does not. The form of the worst case RLS disturbance is also
interesting; it competes with the true output early on, and then goes to zero.

=

We should mention that the LMS algorithm is only one of a family of HOO optimal
estimators. However, LMS corresponds to what is called the central solution, and
has the additional properties of being the maximum entropy solution and the risksensitive optimal solution (Whittle 1990, Glover and Mustafa 1989, Hassibi et al.
1993b).
If there is no disturbance in (1) we have the following
Corollary 1 If in addition to the assumptions of Theorem 1 there is no disturbance
in {1J, then LMS guarantees II e II~:::; J-t-1Iw - w_11 2 , meaning that the prediction
error converges to zero.

Note that the above Corollary suggests that the larger J-t is (provided (3) is satisfied)
the faster the convergence will be.
Before closing this section we should mention that if instead of the prediction error
one were to consider the filtered error ej,i = hjw - hjwj, then the HOO optimal
estimator is the so-called normalized LMS algorithm (Hassibi et al. 1993a).

Hoo Optimality Criteria for LMS and Backpropagation

2.5 . - - - - - - - - - - ' a' - = - - - - - - - - ,

1

0.98
0.96
0.94
0.92

0.5L-------------J

o

50

0.9

0

50

0.5 r - - - - - - > -(d)
=--------,

(e)

0.5
\,

o

1"'-"

"

-0.5
-l~---------~

o

50

-1L-------------------~

o

50

Figure 1: Hoo norm of transfer operator as a function of the number of observations
for (a) RLS, and (b) LMS. The true output and the worst case disturbance signal
(dotted curve) for RLS are given in (c). The predicted errors for the RLS (dashed)
and LMS (dotted) algorithms corresponding to this disturbance are given in (d).
The LMS predicted error goes to zero while the RLS predicted error does not.

3

Nonlinear HOO Adaptive Filtering

In this section we suppose that the observed sequence {dd obeys the following
nonlinear model

(5)
where hi (.) is a known nonlinear function (with bounded first and second order
derivatives), W is an unknown weight vector, and {vd is an unknown disturbance
sequence that includes noise and/or modelling errors. In a neural network context
the index i in hi (.) will correspond to the nonlinear function that maps the weight
vector to the output when the ith input pattern is presented, i.e., hi(W) h(x(i), w)
where x(i) is the ith input pattern. As before we shall denote by Wi = :F(do, ... , di)
the estimate of the weight vector using measurements up to and including time i,
and the prediction error by

=

I

ei

= hi(w) -

hi(Wi-1)

Let T
{ W -

denote the transfer operator that maps the unknowns/disurbances
W -1 , { vd} to the prediction errors {e;}.

Problem 2 (Optimal Nonlinear HOO Adaptive Problem) Find
an Hoo-optimal estimation strategy Wi = :F(d o, d 1 , . .. , d i ) that minimizes

IITllooI

355

356

Hassibi, Sayed, and Kailath

and obtain the resulting

i'~

= inf
:F

IITII~

= inf
:F

(6)

sup
w,vEh2

Currently there is no general solution to the above problem, and the class of nonlinear functions hi(.) for which the above problem has a solution is not known (Ball
and Helton, 1992).
To make some headway, though, note that by using the mean value theorem (5)
may be rewritten as
di

= hi(wi-d + ~~ T (wi_d.(w -

Wi-I)

+ Vi

(7)

where wi-l is a point on the line connecting wand Wi-I. Theorem 1 applied to (7)
shows that the recursion
(8)

=

will yield i'
1. The problem with the above algorithm is that the wi's are
not known. But it suggests that the i'opt in Problem 2 (if it exists) cannot be
less than one. Moreover, it can be seen that the backpropagation algorithm is an
approximation to (8) where wi is replaced by Wi. To pursue this point further we
use again the mean value theorem to write (5) in the alternative form
ohi T
) 1
T 02hi(_
di = hi(wi-d+ ow (wi-d?(w-Wi-l +2(W-Wi-d . ow 2 wi-d?(w-Wi-d+Vi
(9)
where once more Wi-l lies on the line connecting Wi-l and w. Using (9) and
Theorem 1 we have the following result.

Theorem 2 (Backpropagation Algorithm) Consider the model (5) and the
backpropagation algorithm
Wi

= Wi-l + J.L ohi
Ow (wi-d(di -

(10)

hi(wi-d)

then if the ~~i (Wi- d are exciting, and

. f - - : : T =1- - - - - - o < J.L < In
i

(11)

ill!..
) ill!..(
ow (Wi-I?
ow wi-l )

then for all nonzero w, v E h 2:

II ~~i
w_112+ II Vi + !(w -

II~

T (wi-d(w - wi-d
-----------~~=-~--~~--~~---------------

J.L-11w where

wi_d T ~:::J (wi-d?(w - Wi-I) II~

<

-

1

Hoo Optimality Criteria for LMS and Backpropagation

v;

The above result means that if one considers a new disturbance
= Vi + ~ (w Wi_I)T ~::J (Wi-I).(W - Wi-I), whose second term indicates how far hi(w) is from a
first order approximation at point Wi-I, then backpropagation guarantees that the
energy of the linearized prediction error ~~ T (wi-d(w - Wi-I) does not exceed the
energy of the new disturbances W - W-l and

v:.

It seems plausible that if W-I is close enough to w then the second term in v~ should

be small and the true and linearized prediction errors should be close, so that we
should be able to bound the ratio in (6). Thus the following result is expected,
where we have defined the vectors {hd persistently exciting if, and only if, for all
a E

nn

Theorem 3 (Local Hoc Optimality) Consider the model (5) and the backpropagation algorithm (10). Suppose that the ~':: (Wi-I) are persistently exciting, and
that (11) is satisfied. Then for each ( > 0, there exist cSt, ch > 0 such that for all
Iw - w-ti < cSt and all v E h2 with IVil < 82, we have

, 12
II I 2
Il-Ilw - w_112+ II v
ej

II~

< 1+(

-

The above Theorem indicates that the backpropagation algorithm is locally HOC
optimal. In other words for W-l sufficiently close to w, and for sufficiently small
disturbance, the ratio in (6) can be made arbitrarily close to one. Note that the
conditions on wand Vi are reasonable, since if for example W is too far from W-l,
or if some Vi is too large, then it is well known that backpropagation may get stuck
in a local minimum, in which case the ratio in (6) may get arbitrarily large.
As before (11) gives an upper bound on the learning rate Il, and indicates why
backpropagation behaves poorly if the learning rate is too large.
If there is no disturbance in (5) we have the following
Corollary 2 If in addition to the assumptions in Theorem 3 there is no disturbance
in (5), then for every ( > 0 there exists a 8 > 0 such that for all Iw - w-il < 8,
the backpropagation algorithm will yield II e' II~:::; 1l- 18(1 + (), meaning that the
prediction error converges to zero. Moreover Wi will converge to w.

Again provided (11) is satisfied, the larger Il is the faster the convergence will be.

4

Discussion and Conclusion

The results presented in this paper give some new insights into the behaviour of
instantaneous gradient-based adaptive algorithms. We showed that ifthe underlying
observation model is linear then LMS is an HOC optimal estimator, whereas if the
underlying observation model is nonlinear then the backpropagation algorithm is
locally HOC optimal. The HOC optimality of these algorithms explains their inherent
robustness to unknown disturbances and modelling errors, as opposed to other
estimation algorithms for which such bounds are not guaranteed.

357

358

Hassibi, Sayed, and Kailath

Note that if one considers the transfer operator from the disturbances to the prediction errors, then LMS (backpropagation) is H OO optimal (locally), over all causal
estimators. This indicates that our result is most applicable in situations where
one is confronted with real-time data and there is no possiblity of storing the training patterns. Such cases arise when one uses adaptive filters or adaptive neural
networks for adaptive noise cancellation, channel equalization, real-time control,
and undoubtedly many other situations. This is as opposed to pattern recognition,
where one has a set of training patterns and repeatedly retrains the network until
a desired performance is reached.
Moreover, we also showed that the H oo optimality result leads to convergence proofs
for the LMS and backpropagation algorithms in the absence of disturbances. We
can pursue this line of thought further and argue why choosing large learning rates
increases the resistance of backpropagation to local minima, but we shall not do so
due to lack of space.
In conclusion these results give a new interpretation of the LMS and backpropagation algorithms, which we believe should be worthy of further scrutiny.
Acknowledgements

This work was supported in part by the Air Force Office of Scientific Research, Air
Force Systems Command under Contract AFOSR91-0060 and in part by a grant
from Rockwell International Inc.
References

J. A. Ball and J. W. Helton. (1992) Nonlinear H oo control theory for stable plants.
Math. Control Signals Systems, 5:233-261.
K. Glover and D. Mustafa. (1989) Derivation of the maximum entropy H oo controller and a state space formula for its entropy. Int. 1. Control, 50:899-916.
B. Hassibi, A. H. Sayed, and T. Kailath. (1993a) LMS is HOO Optimal. IEEE Conf.
on Decision and Control, 74-80, San Antonio, Texas.
B. Hassibi, A. H. Sayed, and T. Kailath. (1993b) Recursive linear estimation in
Krein spaces - part II: Applications. IEEE Conf. on Decision and Control, 34953501, San Antonio, Texas.
S. Haykin. (1991) Adaptive Filter Theory. Prentice Hall, Englewood Cliffs, NJ.
D. E. Rumelhart, J. L. McClelland and the PDP Research Group. (1986) Parallel
distributed processing: explorations in the microstructure of cognition. Cambridge,
Mass. : MIT Press.
P. Whittle. (1990) Risk Sensitive Optimal Control. John Wiley and Sons, New
York.
B. Widrow and M. E. Hoff, Jr. (1960) Adaptive switching circuits. IRE WESCON
Conv. Rec., Pt.4:96-104.
G. Zames. (1981) Feedback optimal sensitivity: model preference transformation,
multiplicative seminorms and approximate inverses. IEEE Trans. on Automatic
Control, AC-26:301-320.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 821-backpropagation-convergence-via-deterministic-nonmonotone-perturbed-minimization.pdf

Backpropagation Convergence Via
Deterministic Nonmonotone Perturbed
Minimization
o.

L. Mangasarian & M. v. Solodov
Computer Sciences Department
University of Wisconsin
Madison, WI 53706
Email: olvi@cs.wisc.edu, solodov@cs.wisc.edu

Abstract
The fundamental backpropagation (BP) algorithm for training artificial neural networks is cast as a deterministic nonmonotone perturbed gradient method. Under certain natural assumptions, such
as the series of learning rates diverging while the series of their
squares converging, it is established that every accumulation point
of the online BP iterates is a stationary point of the BP error function. The results presented cover serial and parallel online BP,
modified BP with a momentum term, and BP with weight decay.

1

INTRODUCTION

We regard training artificial neural networks as an unconstrained minimization
problem
N

min f(x) := ~ h(x)

xERn

~

(1)

j=l

where h : ~n --+ ~, j = 1, ... , N are continuously differentiable functions from the
n-dimensional real space ~n to the real numbers~. Each function Ii represents the
error associated with the j-th training example, and N is the number of examples
in the training set. The n-dimensional variable space here is that of the weights
associated with the arcs of the neural network and the thresholds of the hidden and

383

384

Mangasarian and Solodov

output units. For an explicit description of f(x) see (Mangasarian, 1993). We note
that our convergence results are equally applicable to any other form of the error
function, provided that it is smooth.
BP (Rumelhart,Hinton & Williams, 1986; Khanna, 1989) has long been successfully
used by the artificial intelligence community for training artificial neural networks.
Curiously, there seems to be no published deterministic convergence results for this
method. The primary reason for this is the nonmonotonic nature of the process.
Every iteration of online BP is a step in the direction of negative gradient of a partial
error function associated with a single training example, e.g. Ii (x) in (1). It is clear
that there is no guarantee that such a step will decrease the full objective function
f( x), which is the sum of the errors for all the training examples . Therefore a single
iteration of BP may, in fact, increase rather than decrease the objective function
f( x) we are trying to minimize. This difficulty makes convergence analysis of BP
a challenging problem that has currently attracted interest of many researchers
(Mangasarian & Solodov, 1994; Gaivoronski, 1994; Grippo, 1994; Luo & Tseng,
1994; White, 1989) .
By using stochastic approximation ideas (Kashyap,Blaydon & Fu, 1970; Ermoliev &
Wets, 1988), White (White, 1989) has shown that, under certain stochastic assumptions, the sequence of weights generated by BP either diverges or converges almost
surely to a point that is a stationary point of the error function. More recently,
Gaivoronski obtained stronger stochastic results (Gaivoronski, 1994). It is worth
noting that even if the data is assumed to be deterministic, the best that stochastic
analysis can do is to establish convergence of certain sequences with probability
one. This means that convergence is not guaranteed. Indeed, there may exist some
noise patterns for which the algorithm diverges, even though this event is claimed
to be unlikely.
By contrast, our approach is purely deterministic. In particular, we show that
online BP can be viewed as an ordinary perturbed nonmonotone gradient-type
algorithm for unconstrained optimization (Section 3) . We note in the passing, that
the term gradient descent which is widely used in the backpropagation and neural
networks literature is incorrect. From an optimization point of view, online BP
is not a descent method, because there is no guaranteed decrease in the objective
function at each step. We thus prefer to refer to it as a nonmonotone perturbed
gradient algorithm.
We give a convergence result for a serial (Algorithm 2.1), a parallel (Algorithm 2.2)
BP, a modified BP with a momentum term, and BP with weight decay. To the best
of our knowledge, there is no published convergence analysis, either stochastic or
deterministic, for the latter three versions of BP. The proposed parallel algorithm is
an attempt to accelerate convergence of BP which is generally known to be relatively
slow.

2

CONVERGENCE OF THE BACKPROPAGATION
ALGORITHM AND ITS MODIFICATIONS

We now turn our attention to the classical BP algorithm for training feedforward
artificial neural networks with one layer of hidden units (Rumelhart,Hinton &

Backpropagation Convergence via Deterministic Nonmonotone Perturbed Minimization

Williams, 1986; Khanna, 1989). Throughout our analysis the number of hidden
units is assumed to be fixed. The choice of network topology is a separate issue
that is not addressed in this work. For some methods for choosing the number of
hidden units see (Courrien, 1993; Arai, 1993).
We now summarize our notation.
N : Nonnegative integer denoting number of examples in the training set.

i = 1,2, ... : Index number of major iterations (epochs) of BP. Each major iteration consists of going through the entire set of error functions !1(x), ... , fN(X).

=

j
1, ... ,N : Index of minor iterations. Each minor iteration j consists of a step
in the direction of the negative gradient - \7 fmU)(zi,j) and a momentum step . Here
m(j) is an element of the permuted set {I, ... , N}, and zi,j is defined immediately
below. Note that if the training set is randomly permuted after every epoch, the
map m(?) depends on the index i. For simplicity, we skip this dependence in our
notation.
xi :

Iterate in ~n of major iteration (epoch) i = 1,2, ....

zi,; : Iterate in ~n of minor iteration j = 1, ... , N, within major iteration i
1,2, .... Iterates zi,j can be thought of as elements of a matrix with N columns and
infinite number of rows, with row i corresponding to the i-th epoch of BP.
By 1}i we shall denote the learning rate (the coefficient multiplying the gradient),
and by (ki the momentum rate (the coefficient multiplying the momentum term).
For simplicity we shall assume that the learning and momentum rates remain fixed
within each major iteration. In a manner similar to that of conjugate gradients
(Polyak, 1987) we reset the momentum term to zero periodically.

Algorithm 2.1. Serial Online BP with a Momentum Term.
Start with any xO E ~n. Having xi, stop if \7 f(x i )
0, else compute xi+l as

=

follows:

(2)
zi,j+l

= zi,j

- TJi \7 fmu)(i,j)
xi+l

+ aif1zi,j,

j

= 1, ... , N

= zi,N+l

(3)

(4)

where
if j = 1
otherwise

(5)

Remark 2.1. Note that the stopping criterion of this algorithm is typically that
used in first order optimization methods, and is not explicitly related to the ability of the neural network to generalize. However, since we are concerned with
convergence properties of BP as a numerical algorithm, this stopping criterion is

385

386

Mangasarian and Solodov

justified. Another point related to the issue of generalization versus convergence is
the following. Our analysis allows the use of a weight decay term in the objective
function (Hinton, 1986; Weigend,Huberman & Rumelhart, 1990) which often yields
a network with better generalization properties. In the latter case the minimization
problem becomes
N

min I(x) := L~ hex)

xElRn

+ All x l1 2

(6)

i=l

where A is a small positive scaling factor.

= 0 reduces

Remark 2.2. The choice of C?i
without a momentum term.

Algorithm 2.1 to the original BP

Remark 2.3. We can easily handle the "mini-batch" methods (M!2l11er, 1992) by
merely redefining the meaning of the partial error function Ii to represent the error
associated with a subset of training examples. Thus "mini-batch" methods also fall
within our framework.
We next present a parallel modification of BP. Suppose we have k parallel processors, k 2: 1. We consider a partition of the set {l, ... , N} into the subsets
J" 1 1, ... ,k, so that each example is assigned to at least one processor. Let
N, be the cardinality of the corresponding set J,. In the parallel BP each processor
performs one (or more) cycles of serial BP on its set of training examples. Then a
synchronization step is performed that consists of averaging the iterates computed
by all the k processors. From the mathematical point of view this is equivalent to
each processor I E {I, ... , k} handling the partial error function I' (x) associated
with the corresponding set of training examples J , . In this setting we have

=

k

J'(x)=~Ii(x),

f(x)=~f'(x)

iEJ I

1=1

We note that in training a neural network it might be advantageous to assign
some training examples to more than one parallel processor. We thus allow for the
possibility of overlapping sets J,.
The notation for Algorithm 2.2 is similar to that for Algorithm 2.1, except for the
index 1 that is used to label the partial error function and minor iterates associated
with the l-th parallel processor. We now state the parallel BP with a momentum
term.

Algorithm 2.2. Parallel Online BP with a Momentum Term.
Start with any xO E ~n. Having xi, stop if x i+l = xi, else compute x i +l as follows

(i) Parallelization. For each parallel processor I E {I, ... , k} do
i,l

z,
z,i,i+l _- z,i,i where

'~f'

7], v m(j)

~zlili = {

0

= xi

(iIi)
z,

(7)

+ c?,uz"
. i,i

z;,i - z;,i-

A

=

l

J.

= 1, ... , N I

if j
1
otherwise

(8)

(9)

Backpropagation Convergence via Deterministic Nonmonotone Perturbed Minimization

o < TJi < 1,

O:s a i < 1

(ii) Synchronization
k

Xi+l =

~ L z;,Nr+l

(10)

1=1

We give below in Table 1 a flowchart of this algorithm.

/
i 1

Z 1'

..
-

x'.

Major iteration i :

..... ~

xi

.~

i1
.
z'I '.- x'

i1
.
z'k .'- x'

~
Serial BP on
examples in Jl

~

Serial BP on
examples in J,

Serial BP on
examples in Jk

~

~
i,Nr+l
z,

J
? ?IteratIOn
. z. + 1 : x ,'+1
M aJor

i,N,,+I
zk

/
i Nr+ 1
= k1 "k
L.....I=1 z,'

Table 1. Flowchart of the Parallel BP

Remark 2.4. It is well known that ordinary backpropagation is a relatively slow
algorithm. One appealing remedy is parallelization (Zhang,Mckenna,Mesirov &
Waltz, 1990). The proposed Algorithm 2.2 is a possible step in that direction.
Note that in Algorithm 2.2 all processors typically use the same program for their
computations. Thus load balancing is easily achieved.
Remark 2.5. We wish to point out that synchronization strategies other than
(10) are possible. For example, one may choose among the k sets of weights and
thresholds the one that best classifies the training data.
To the best of our knowledge there are no published deterministic convergence

387

388

Mangasarian and Solodov

proofs for either of Algorithms 2.1,2.2. Using new convergence analysis for a class of
nonmonotone optimization methods with perturbations (Mangasarian & Solodov,
1994), we are able to derive deterministic convergence properties for online BP
and its modifications. Once again we emphasize the equivalence of either of those
methods to a deterministic nonmonotone perturbed gradient-type algorithm.
We now state our main convergence theorem. An important result used in the proof
is given in the Mathematical Appendix. We refer interested readers to (Mangasarian
& Solodov, 1994) for more details.
Theorem 2.1. If the learning and momentum rates are chosen such that
00

L =
l7i

i=O

00

00,

L 171 <
i=O

00

00,

L

O:'il7i

< 00,

(11)

i=O

then for any sequence {xi} generated by any of the Algorithms 2.1 or 2.2, it follows
that {/(xiH converges, {\7 !(xi)} - 0, and for each accumulation point x of the
sequence {x'}, \7 I( x) = O.

Remark 2.6. We note that conditions (11) imply that both the learning and
momentum rates asymptotically tend to zero. These conditions are similar to those
used in (White, 1989; Luo & Tseng, 1994) and seem to be the inevitable price paid
for rigorous convergence. For practical purposes the learning rate can be fixed or
adjusted to some small but finite number to obtain an approximate solution to the
minimization problem. For state-of-the-art techniques of computing the learning
rate see (Ie Cun, Simard & Pearlmutter, 1993).
Remark 2.7. We wish to point out that Theorem 2.1 covers BP with momentum
and/or decay terms for which there is no published convergence analysis of any
kind.
Remark 2.8. We note that the approach of perturbed minimization provides
theoretical justification to the well known properties of robustness and recovery
from damage for neural networks (Sejnowski & Rosenberg, 1987). In particular, this
approach shows that the net should recover from any reasonably small perturbation.
Remark 2.9. Establishing convergence to a stationary point seems to be the
best one can do for a first-order minimization method without any additional restrictive assumptions on the objective function. There have been some attempts
to achieve global descent in training, see for example, (Cetin,Burdick & Barhen,
1993). However, convergence to global minima was not proven rigorously in the
multidimensional case.

3

MATHEMATICAL APPENDIX: CONVERGENCE OF
ALGORITHMS WITH PERTURBATIONS

In this section we state a new result that enables us to establish convergence properties of BP. The full proof is nontrivial and is given in (Mangasarian & Solodov,
1994).

Backpropagation Convergence via Deterministic Nonmonotone Perturbed Minimization

Theorem 3.1. General Nonmonotonic Perturbed Gradient Convergence
(subsumes BP convergence).
Suppose that f(x) is bou?,!-ded below and that \1 f(x) is bounded and Lipschitz continuous on the sequence {x'} defined below. Consider the following perturbed gradient
algorithm. Start with any x O E ~n. Having xi, stop if \1 f(x i ) 0, else compute

=

(12)

where
di = -\1f(x i )

for some ei E ~n, TJi E~, TJi
00

L TJi =
;=0

L TJl <
i=O

(13)

> 0 and such that for some I > 0

00

00,

+ ei

00

00,

L TJdleili <

00,

Ileill ~ I

Vi

(14)

i=O

It follows that {f(x i)} converges, {\1 f(x i )} -+ 0, and for each accumulation point
x of the sequence {x'}, V' f(x) = O. If, in addition, the number of stationary points
of f(x) is finite, then the sequence {xi} converges to a stationary point of f(x).
Remark 3.1. The error function of BP is nonnegative, and thus the boundedness
condition on f(x) is satisfied automatically. There are a number of ways to ensure
that f(x) has Lipschitz continuous and bounded gradient on {xi} . In (Luo & Tseng,
1994) a simple projection onto a box is introduced which ensures that the iterates
remain in the box. In (Grippo, 1994) a regularization term as in (6) is added to the
error function so that the modified objective function has bounded level sets. We
note that the latter provides a mathematical justification for weight decay (Hinton,
1986; Weigend,Huberman & Rumelhart, 1990). In either case the iterates remain
in some compact set, and since f( x) is an infinitely smooth function, its gradient is
bounded and Lipschitz continuous on this set as desired.
Acknowledgements
This material is based on research supported by Air Force Office of Scientific
Research Grant F49620-94-1-0036 and National Science Foundation Grant CCR9101801.
References
M. Arai. (1993) Bounds on the number of hidden units in binary-valued three-layer
neural networks. Neural Networks, 6:855-860.

B. C. Cetin, J. W. Burdick, and J. Barhen. (1993) Global descent replaces gradient
descent to avoid local minima problem in learning with artificial neural networks.
In IEEE International Conference on Neural Networks, (San Francisco), volume 2,
836-842.
P. Courrien.(1993) Convergent generator of neural networks.
6:835-844.

Neural Networks,

Yu. Ermoliev and R.J.-B. Wets (editors). (1988) Numerical Techniques for Stochastic Optimization Problems. Springer-Verlag, Berlin.

389

390

Mangasarian and Solodov

A.A. Gaivoronski. (1994) Convergence properties of backpropagation for neural
networks via theory of stochastic gradient methods. Part 1. Optimization Methods
and Software, 1994, to appear.
1. Grippo. (1994) A class of unconstrained minimization methods for neural network training. Optimization Methods and Software, 1994, to appear.
G. E. Hinton. (1986) Learning distributed representations of concepts. In Proceedings of the Eighth Annual Conference of the Cognitive Science Society, 1-12,
Hillsdale. Erlbaum.
R. 1. Kashyap, C. C. Blaydon and K. S. Fu. (1970) Applications of stochastic
approximation methods. In J .M.Mendel and K.S. Fu, editors, Adaptive, Learning,
and Pattern Recognition Systems. Academic Press.
T. Khanna. (1989) Foundations of neural networks. Addison-Wesley, New Jersey.
Y. Ie Cun, P.Y. Simard, and B. Pearlmutter. (1993) Automatic learning rate
maximization by on-line estimation of the Hessian's eigenvectors. In C.1.Giles
S.J .Hanson, J .D.Cowan, editor, Advances in Neural Information Processing Systems 5, 156-163, San Mateo, California, Morgan Kaufmann.
Z.-Q. Luo and P. Tseng. (1994) Analysis of an approximate gradient projection
method with applications to the backpropagation algorithm. Optimization Methods
and Software, 1994, to appear.
0.1. Mangasarian. (1993) Mathematical programming in neural networks. ORSA
Journal on Computing, 5(4), 349-360.
0.1. Mangasarian and M.V. Solodov. (1994) Serial and parallel backpropagation
convergence via nonmonotone perturbed minimization. Optimization Methods and
Software, 1994, to appear. Proceedings of Symposium on Parallel Optimization 3,
Madison July 7-9, 1993.
M.F. M!2Sller. (1992) Supervised learning on large redundant training sets. In Neural
Networks for Signal Processing 2. IEEE Press.
B.T. Polyak. (1987) Introduction to Optimization. Optimization Software, Inc.,
Publications Division, New York.
D.E. Rumelhart, G.E. Hinton, and R.J. Williams. (1986) Learning internal representations by error propagation. In D.E. Rumelhart and J.1. McClelland, editors,
Parallel Distributed Processing, 318-362, Cambridge, Massachusetts. MIT Press.
T.J. Sejnowski and C.R. Rosenberg. (1987) Parallel networks that learn to pronounce english text. Complex Systems, 1:145-168.
A.S. Weigend, B.A. Huberman, and D.E. Rumelhart. (1990) Predicting the future:a
connectionist approach. International Journal of Neural Systems, 1 :193-209.
H. White. (1989) Some asymptotic results for learning in single hidden-layer
feedforward network models. Journal of the American Statistical Association,
84( 408): 1003-1013.
X. Zhang, M. Mckenna, J. P. Mesirov, and D. 1. Waltz. (1990) The backpropagation
algorithm on grid and hypercube architectures. Parallel Computing, 14:317-327.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 514-hierarchies-of-adaptive-experts.pdf

Hierarchies of adaptive experts

Robert A. Jacobs
Michael I. Jordan
Department of Brain and Cognitive Sciences
Massachusetts Institute of Technology
Cambridge, MA 02139

Abstract
In this paper we present a neural network architecture that discovers a
recursive decomposition of its input space. Based on a generalization of the
modular architecture of Jacobs, Jordan, Nowlan, and Hinton (1991), the
architecture uses competition among networks to recursively split the input
space into nested regions and to learn separate associative mappings within
each region. The learning algorithm is shown to perform gradient ascent
in a log likelihood function that captures the architecture's hierarchical
structure.

1

INTRODUCTION

Neural network learning architectures such as the multilayer perceptron and adaptive radial basis function (RBF) networks are a natural nonlinear generalization
of classical statistical techniques such as linear regression, logistic regression and
additive modeling. Another class of nonlinear algorithms, exemplified by CART
(Breiman, Friedman, Olshen, & Stone, 1984) and MARS (Friedman, 1990), generalizes classical techniques by partitioning the training data into non-overlapping
regions and fitting separate models in each of the regions. These two classes of algorithms extend linear techniques in essentially independent directions, thus it seems
worthwhile to investigate algorithms that incorporate aspects of both approaches
to model estimation. Such algorithms would be related to CART and MARS as
multilayer neural networks are related to linear statistical techniques. In this paper we present a candidate for such an algorithm. The algorithm that we present
partitions its training data in the manner of CART or MARS, but it does so in a
parallel, on-line manner that can be described as the stochastic optimization of an
appropriate cost functional.

985

986

Jordan and Jacobs

Why is it sensible to partition the training data and to fit separate models within
each of the partitions? Essentially this approach enhances the flexibility of the
learner and allows the data to influence the choice between local and global representations. For example, if the data suggest a discontinuity in the function being
approximated, then it may be more sensible to fit separate models on both sides of
the discontinuity than to adapt a global model across the discontinuity. Similarly,
if the data suggest a simple functional form in some region, then it may be more
sensible to fit a global model in that region than to approximate the function locally
with a large number of local models. Although global algorithms such as backpropagation and local algorithms such as adaptive RBF networks have some degree of
flexibility in the tradeoff that they realize between global and local representation,
they do not have the flexibility of adaptive partitioning schemes such as CART and
MARS.
In a previous paper we presented a modular neural network architecture in which
a number of "expert networks" compete to learn a set of training data (Jacobs,
Jordan, Nowlan & Hinton, 1991). As a result of the competition, the architecture
adaptively splits the input space into regions, and learns separate associative mappings within each region. The architecture that we discuss here is a generalization
of the earlier work and arises from considering what would be an appropriate internal structure for the expert networks in the competing experts architecture. In our
earlier work, the expert networks were multilayer perceptrons or radial basis function networks. If the arguments in support of data partitioning are valid, however,
then they apply equally well to a region in the input space as they do to the entire input space, and therefore each expert should itself be composed of competing
sub-experts. Thus we are led to consider recursively-defined hierarchies of adaptive
experts.

2

THE ARCHITECTURE

Figure 1 shows two hierarchical levels of the architecture. (We restrict ourselves to
two levels throughout the paper to simplify the exposition; the algorithm that we
develop, however, generalizes readily to trees of arbitrary depth). The architecture
has a number of expert networks that map from the input vector x to output
vectors Yij. There are also a number of gating networks that define the hierarchical
structure of the architecture. There is a gating network for each cluster of expert
networks and a gating network that serves to combine the outputs of the clusters.
The output of the ith cluster is given by
Yi

=L

gjliYij

(1)

j

where gjli is the activation of the ph output unit of the gating network in the
cluster. The output of the architecture as a whole is given by

ith

(2)
where gi is the activation of the

ith

output unit of the top-level gating network.

Hierarchies of adaptive experts

Gating
Network

Expert
Network

Gating
Network

Expert
Network

Gating
Network
9 1/2

9 2/2

Expert
Network

Expert
Network

Y22

Figure 1: Two hierarchical levels of adaptive experts. All of the expert networks
and all of the gating networks have the same input vector.

We assume that the outputs of the gating networks are given by the normalizing
softmax function (Bridle) 1989):
eS ,

gi

= 'Ii:""""

~j

e

(3)

S

J

and
gjli = Lk

eSkl.

(4)

where Si and Sjli are the weighted sums arriving at the output units of the corresponding gating networks.
The gating networks in the architecture are essentially classifiers that are responsible for partitioning the input space. Their choice of partition is based on the ability

987

988

Jordan and Jacobs

of the expert networks to model the input-output functions within their respective regions (as quantified by their posterior probabilities; see below). The nested
arrangement of gating networks in the architecture (cf. Figure 1) yields a nested
partitioning much like that found in CART or MARS. The architecture is a more
general mathematical object than a CART or MARS tree, however, given that the
gating networks have non-binary outputs and given that they may form nonlinear
decision surfaces.

3

THE LEARNING ALGORITHM

We derive a learning algorithm for our architecture by developing a probabilistic
model of a tree-structured estimation problem. The environment is assumed to be
characterized by a finite number of stochastic processes that map input vectors x
into output vectors y*. These processes are partitioned into nested collections of
processes that have commonalities in their input-output parameterizations. Data
are assumed to be generated by the model in the following way. For any given x,
collection i is chosen with probability 9i, and a particular process j is then chosen
with conditional probability 9jli. The selected process produces an output vector
y* according to the probability density f(y* I x; Yij), where Yij is a vector of
parameters. The total probability of generating y* is:
P(y* I x) =

L 9i L 9jld(Y* I x;

Yij),

(5)

j

where 9i, 9jli, and Yij are unknown nonlinear functions of x.
Treating the probability P(y* Ix) as a likelihood function in the unknown parameters 9i, 9j Ii, and Yij, we obtain a learning algorithm by using gradient ascent to
maximize the log likelihood. Let us assume that the probability density associated
with the residual vector (y* - Yij) is the multivariate normal density, where Yij is
the mean of the ph process of the ith cluster (or the (i, j)th expert network) and
Eij is its covariance matrix. Ignoring the constant terms in the normal density, the
log likelihoo d is:

In L = In

L 9i L 9jlil E ij I-~ e-!(Y*-Y'J)Tl:;-;l(Y*-Y'J).

(6)

j

(7)
which is the posterior probability that a process in the ith cluster generates a particular target vector y*. We also define the conditional posterior probability:

h. . Jlz -

"1- ~ e - ~(Y* - Y'J )Tl;;-;l (y* -Y'J)

9J?I?IE
i
iJ

L j 9J'I' I~ ZJ I- ~ e _'!'(Y*-Y
i

~'.

2

2

)Tl:-l(y*_y 'J )'

(8)

'J'J

which is the conditional posterior probability that the ph expert in the ith cluster
generates a particular target vector y*. Differentiating 6, and using Equations 3, 4,

Hierarchies of adaptive experts

7, and 8, we obtain the partial derivative of the log likelihood with respect to the
output of the (i,j)th expert network:
f)

In L

-?)-

UYij

= hi hjli (y

of<

- Yij).

(9)

This partial derivative is a supervised error term modulated by the appropriate
posterior probabilities . Similarly, the partial derivatives of the log likelihood with
respect to the weighted sums at the output units of the gating networks are given
by:
(10)

and

f) In L
- h?
'I' - g'I
')'
?) - ! (h J!
J!

(ll)

USjli

These derivatives move the prior probabilities associated with the gating networks
toward the corresponding posterior probabilities.

It is interesting to note that the posterior probability hi appears in the gradient for
the experts in the ith cluster (Equation 9) and in the gradient for the gating network
in the ith cluster (Equation 11). This ties experts within a cluster to each other and
implies that experts within a cluster tend to learn similar mappings early in the
training process. They differentiate later in training as the probabilities associated
with the cluster to which they belong become larger. Thus the architecture tends
to acquire coarse structure before acquiring fine structure, This feature of the
architecture is significant because it implies a natural robustness to problems with
overfitting in deep hierarchies.
We have also found it useful in practice to obtain an additional degree of control over
the coarse-to-fine development of the algorithm. This is achieved with a heuristic
that adjusts the learning rate at a given level of the tree as a function of the timeaverage entropy of the gating network at the next higher level of the tree:
j.l ' li(t

+ 1) = G:j.l'li(f) + f3(Mi + L

gili In gjli)

j

where Mi is the maximum possible entropy at level i of the tree. This equation
has the effect that the networks at level i + 1 are less inclined to diversify if the
superordinate cluster at level i has yet to diversify (where diversification is quantified
by the entropy of the gating network).

4

SIMULATIONS

We present simulation results from an unsupervised learning task and two supervised learning tasks.
In the unsupervised learning task, the problem was to extract regularities from a set
of measurements of leaf morphology. Two hundred examples of maple, poplar, oak,
and birch leaves were generated from the data shown in Table 1. The architecture
that we used had two hierarchical levels, two clusters of experts, and two experts

989

990

Jordan and Jacobs

Length
Width
Flare
Lobes
Margin
Apex
Base
Color

Maple
3,4,5,6
3,4,5
0
5
Entire
Acute
Truncate
Light

Poplar
1,2,3
1,2
0,1
1
Crenate, Serrate
Acute
Rounded
Yellow

Oak
5,6,7,8,9
2,3,4,5
0
7,9
Entire
Rounded
Cumeate
Light

Birch
2,3,4,5
1,2,3
1
1
Doubly-Serrate
Acute
Rounded
Dark

Table 1: Data used to generate examples of leaves from four types of trees. The
columns correspond to the type of tree; the rows correspond to the features of a
tree 's leaf. The table's entries give the possible values for each feature for each type
of leaf. See Preston (1976).
within each cluster. Each expert network was an auto-associator that maps fortyeight input units into forty-eight output units through a bottleneck of two hidden
units. Within the experts, backpropagation was used to convert the derivatives
in Equation 9 into changes to the weights. The gating networks at both levels
were affine. We found that the hierarchical architecture consistently discovers the
decomposition ,o f the data that preserves the natural classes of tree species (cf.
Preston, 1976). That is, within one cluster of expert networks, one expert learns
the maple training patterns and the other expert learns the oak patterns. Within the
other cluster, one expert learns the poplar patterns and the other expert learns the
birch patterns . Moreover , due to the use of the autoassociator experts, the hidden
unit representations within each expert are principal component decompositions
that are specific to a particular species of leaf.
We have also studied a supervised learning problem in which the learner must
predict the grayscale pixel values in noisy images of human faces based on values of
the pixels in surrounding 5x5 masks. There were 5000 masks in the training set. We
used a four-level binary tree, with affine experts (each expert mapped from twentyfive input units to a single output unit) and affine gating networks. We compared
the performance of the hierarchical architecture to CART and to backpropagation. 1
In the case of backpropagation and the hierarchical architecture, we utilized crossvalidation (using a test set of 5000 masks) to stop the iterative training procedure.
As shown in Figure 2, the performance of the hierarchical architecture is comparable
to backpropagation and better than CART.
Finally we also studied a system identification problem involving learning the simulated forward dynamics of a four-joint, three-dimensional robot arm. The task
was to predict the joint accelerations from the joint positions, sines and cosines of
joint positions, joint velocities, and torques. There were 6000 data items in the
training set. We used a four-level tree with trinary splits at the top two levels,
and binary splits at lower levels. The tree had affine experts (each expert mapped
1 Fifty hidden units were used in the backpropagation network, making the number of parameters in the backpropagation network and the hierarchical network roughly
comparable.

Hierarchies of adaptive experts

0.1 -

0.08 -

Relative 0.06Error
0.04 -

0.02 -

CART

BP

Hier4

Figure 2: The results on the image restoration task. The dependent measure is
relative error on the test set. (cf. Breiman , et al., 1984) .

from twenty input units to four output units) and affine gating networks. We once
again compared the performance of the hierarchical architecture to CART and to
back propagation . In the case of back propagation and the hierarchical architecture,
we utilized a conjugate gradient technique, and halted the training process after
1000 iterations. In the case of CART, we ran the algorithm four separate times on
the four output variables. Two of these runs produced 100 percent relative error,
a third produced 75 percent relative error , and the fourth (the most proximal joint
acceleration) yielded 46 percent relative error, which is the value we report in Figure 3. As shown in the figure, the hierarchical architecture and backpropagation
achieve comparable levels of performance.

5

DISCUSSION

In this paper we have presented a neural network learning algorithm that captures
aspects of the recursive approach to function approximation exemplified by algorithms such as CART and MARS . The results obtained thus far suggest that the
algorithm is computationally viable, comparing favorably to backpropagation in
terms of generalization performance on a set of small and medium-sized tasks. The
algorithm also has a number of appealing theoretical properties when compared to
backpropagation: In the affine case, it is possible to show that (1) no backward
propagation of error terms is required to adjust parameters in multi-level trees (cf.
the activation-dependence of the multiplicative terms in Equat.ions 9 and 11), (2)
all of the parameters in the tree are ma..ximum likelihood estimators . The latter
property suggests that the affine architecture may be a particularly suitable architecture in which to explore the effects of priors on the parameter space (cf. Nowlan

991

992

Jordan and Jacobs
0.6 -

0.4 Relative
Error

0.2 -

0.0-'---CART

BP

Hier4

Figure 3: The results on the system identification task.
& Hinton, this volume).
Acknowledgements

This project was supported by grant IRI-9013991 awarded by the National Science
Foundation, by a grant from Siemens Corporation, by a grant from ATR Auditory
and Visual Perception Research Laboratories, by a grant from the Human Frontier
Science Program, and by an NSF Presidential Young Investigator Award to the first
author.
References

Breiman, L., Friedman, J .H., Olshen, R.A., & Stone, C.J. (1984) Classification and
Regression Trees. Belmont, CA: Wadsworth International Group.
Bridle, J. (1989) Probabilistic interpretation of feedforward classification network
outputs, with relationships to statistical pattern recognition. In F. Fogelman-Soulie
& J. Herault (Eds.), Neuro-computing: Algorithms, Architectures, and Applications.
New York: Springer-Verlag.
Friedman, J .H. (1990) Multivariate adaptive regression splines.
Statistics, 19,1-141.

The Annals of

Jacobs, R.A, Jordan, M.L, Nowlan, S.J., & Hinton, G.E. (1991) Adaptive mixtures
of local experts. Neural Computation, 3, 79-87.
Preston, R.J. (1976) North American Trees (Third Edition). Ames, IA: Iowa State
University Press.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 958-a-lagrangian-formulation-for-optical-backpropagation-training-in-kerr-type-optical-networks.pdf

A Lagrangian Formulation For
Optical Backpropagation Training In
Kerr-Type Optical Networks

James E. Steck
Mechanical Engineering
Wichita State University
Wichita, KS 67260-0035

Steven R. Skinner
Electrical Engineering
Wichita State University
Wichita, KS 67260-0044

Alvaro A. Cruz-Cabrara
Electrical Engineering
Wichita State University
Wichita, KS 67260-0044

Elizabeth C. Behrman
Physics Department
Wichita State University
Wichita, KS 67260-0032

Abstract
A training method based on a form of continuous spatially distributed
optical error back-propagation is presented for an all optical network
composed of nondiscrete neurons and weighted interconnections. The all
optical network is feed-forward and is composed of thin layers of a Kerrtype self focusing/defocusing nonlinear optical material. The training
method is derived from a Lagrangian formulation of the constrained
minimization of the network error at the output. This leads to a
formulation that describes training as a calculation of the distributed error
of the optical signal at the output which is then reflected back through the
device to assign a spatially distributed error to the internal layers. This
error is then used to modify the internal weighting values. Results from
several computer simulations of the training are presented, and a simple
optical table demonstration of the network is discussed.

Elizabeth C. Behrman

772

1 KERR TYPE MATERIALS
Kerr-type optical networks utilize thin layers of Kerr-type nonlinear materials, in which the
index of refraction can vary within the material and depends on the amount of light striking
the material at a given location. The material index of refraction can be described by:
n(x)=no+nzI(x), where 110 is the linear index of refraction, ~ is the nonlinear coefficient, and
I(x) is the irradiance of a applied optical field as a function of position x across the material
layer (Armstrong, 1962). This means that a beam of light (a signal beam carrying
information perhaps) passing through a layer of Kerr-type material can be steered or
controlled by another beam of light which applies a spatially varying pattern of intensity
onto the Kerr-type material. Steering of light with a glass lens (having constance index of
refraction) is done by varying the thickness of the lens (the amount of material present) as
a function of position. Thus the Kerr effect can be loosely thought of as a glass lens whose
geometry and therefore focusing ability could be dynamically controlled as a function of
position across the lens. Steering in the Kerr material is accomplished by a gradient or
change in the material index of refraction which is created by a gradient in applied light
intensity. This is illustrated by the simple experiment in Figure 1 where a small weak probe
beam is steered away from a straight path by the intensity gradient of a more powerful pump
beam.

lex)
Pump

I~

> /-.. .

x

Figure 1: Light Steering In Kerr Materials

2

OPTICAL NETWORKS USING KERR MATERIALS

The Kerr optical network, shown in Figure 2, is made up of thin layers of the Kerr- type
nonlinear medium separated by thick layers of a linear medium (free space) (Skinner, 1995).
The signal beam to be processed propagates optically in a direction z perpendicular to the
layers, from an input layer through several alternating linear and nonlinear layers to an output
layer. The Kerr material layers perform the nonlinear processing and the linear layers serve
as connection layers. The input (l(x)) and the weights (W\(x),W2(x) ... Wn(x)) are irradiance
fields applied to the Kerr type layers, as functions of lateral position x, thus varying the

A Lagrangian Formulation for Optical Backpropasation

773

refractive index profile of the nonlinear medium. Basically, the applied weight irradiences
steer the signal beam via the Kerr effect discussed above to produce the correct output. The
advantage of this type of optical network is that both neuron processing and weighted
connections are achieved by uniform layers of the Kerr material. The all optical nature
eliminates the need to physically construct neurons and connections on an individual basis.

O(x,y)

?
Plane Wave (Eo)

Figure 2: Kerr Optical Neural Network Architecture

If E;(ex) is the light entering the itlt nonlinear layer at lateral position ex, then the effect of the
nonlinear layer is given by
(1)

where W;( ex) is the applied weight field. Transmission of light at lateral location ex at the
beginning of the itlt linear layer to location p just before the i+ 1tit nonlinear layer is given by
where

c

3

ko

=--

I

2!:lLI

(2)

OPTICAL BACK-PROPAGATION TRAINING

Traditional feed-forward artificial neural networks composed of a finite number of discrete
neurons and weighted connections can be trained by many techniques. Some of the most
successful techniques are based upon the well known training method called backpropagation which results from minimizing the network output error, with respect to the
network weights by a gradient descent algorithm. The optical network is trained using a
form of continuous optical back-propagation which is developed for a nondiscrete network.
Gradient descent is applied to minimize the error over the entire output region of the optical
network. This error is a continuous distribution of error calculated over the output region.

774

Elizabeth C. Behrman

Optical back-propagation is a specific technique by which this error distribution is optically
propagated backward through the linear and nonlinear optical layers to produce error signals
by which the light applied to the nonlinear layers is modified. Recall that this applied light
Wi controls what serves as connection "weights" in the optical network. Optical backpropagation minimizes the error Lo over an output region 0 0 > a subdomain of the fmal or nth
layer of the network,

~

where

=

'Y

r

O(u'fJ '(uliu
)c o

(3)

subject to the constraint that the propagated light, Ei( ex), satisfies the equations of forward
propagation (1) and (2). O(P) = En+I(P) and is the network output, y is a scaling factor on
the output intensity. Lo then is the squared error between the desired output value D and the
average intensity 10 of the output distribution O( P).
This constrained minimization problem is posed in a Lagrange formulation similar to the
work of (Ie Cunn, 1988) for conventional feedforward networks and (pineda, 1987) for
conventional recurrent networks; the difference being that for the optical network of this
paper the Electric field E and the Lagrange multiplier are complex and also continuous in the
spatial variable thus requiring the Lagrangian below. A Lagrangian is defmed as;
L =

4,

+

+

:t

fA; t u ) [ EI+I(U) - fFI~)~ Ie -jctP ... )z
0.
0.

JA/+~U) [Ei+~U)

it.
- 0.

-

fF~~)~/e-jC~13-?)Z

dP ] ax
(4)

dP

r ax

0.

Taking the variation ofL with respect to E i, the Lagrange multipliers Ai, and using gradient
descent to minimize L with respect to the applied weight fields Wi gives a set of equations
that amount to calculating the error at the output and propagating the error optically
backwards through the network. The pertinent results are given below. The distributed
assigrunent of error on the output field is calculated by
A1f+1(R.)
=
...

~0
'Y

' (R.)
[ D - 10 ]
...

(5)

This error is then propagated back through the nth or final linear optical layer by the equation

?c

6 (~) = ~
"

1t

r

z

A + (u) e -jC,/..13-u) dx

) Co " 1

(6)

which is used to update the "weight" light applied to the nth nonlinear layer. Optical backpropagation, through the ith nonlinear layer (giving AlP? followed by the linear layer
(giving ~i-I(P? is performed according to the equations

A Lagrangian Formulation for Optical Backpropagation

775

(7)

This gives the error signal ~j'I(P) used to update the "weight" light distribution Wj.I(P)
applied to the i-I nonlinear layer. The "weights" are updated based upon these errors
according to the gradient descent rule
Wi-(~)

=

w/,/d(P)

+l'lt~)ktPNLin2W/"t~) 2

IM[

~(~) 6,(~) e-~ANL.nZ<lw,ClId(p)f.IE.(I\)I'>]

(8)

where ,,;CP) is a learning rate which can be, but usually is not a function of layer number i
and spatial position p. Figure 3 shows the optical network (thick linear layers and thin
nonlinear layers) with the unifonn plane wave Eo, the input signal distribution I, forward
propagation signals EI E2 .. . En' the weighting light distributions at the nonlinear layers WI
W 2 .. , W n. Also shown are the error signal An+1 at the output and the back-propagated error
signals ~n ... ~2 ~I for updating the nonlinear layers. Common nonlinear materials exist for
which the material constants are such that the second term in the first of Equations 7
becomes small. Ignoring this second term gives an approximate fonn of optical backpropagation which amounts to calculating the error at the output of the network and then
reversing its direction to optically propagate this error backward through the device.
This can be easily seen by comparing Equations 6 and 7 (with the second tenn dropped) for
optical back-propagation of the output error An with Equations I and 2 for the forward
propagation of the signal E j. This means that the optical back-propagation training
calculations potentially can be implemented in the same physical device as the forward
network calculations. Equation (8) then becomes;
Wi-(~)
+

=

Wio/d(~)

(2t'l,(~'>kot:.NL"2) w//d(~) [ (Et~) At~? - ~t~) ~(~)r]

(9)

which may be able to be implemented optically.

4

SIMULATION RESULTS

To prove feasibility, the network was then trained and tested on several benchmark
classification problems, two of which are discussed here. More details on these and other
simulations of the optical network can be found in (Skinner, 1995). In the first (Using
Nworks, 1991), iris species were classified into one of three categories: Setosa,
Versicolor or Virginica. Classification was based upon length and width of the sepals and

Elizabeth C. Behrman

776

petals. The network consisted of an input self-defocusing layer with an applied irradiance
field which was divided into 4 separate Gaussian distributed input regions 25 microns in
width followed by a linear layer. This pattern is repeated for 4 more groups composed
of a nonlinear layer (with applied weights) followed by a linear layer. The final linear
layer has three separate output regions 10 microns wide for binary classification as to
species. The nonlinear layers were all 20 microns thick with n2=-.05 and the linear
layers were 100 microns thick. The wavelength of applied light was 1 micron and the
width of the network was 512 microns discretized into 512 pixels. This network was
trained on a set of 50 training pairs to produce correct classification of all 50 training
pairs. The network was then used to classify 50 additional pairs of test data which were
not used in the training phase. The network classified 46 of these correctly for a 92 %
accuracy level which is
comparable
to
a
standard feedforward
network with discrete
output region
sigmoidal neurons.

-\

\..

In the second problem,

we
tested
the
performance of the
network on a set of
data from a dolphin
sonar discrimination
experiment (Roitblat,
1991). In this study a
dolphin was presented
with one of three
of
different
types
objects (a tube, a
sphere, and a cone),
allowed to echolocate,
and
rewarded
for
choosing the correct
one from a comparison
array.
The Fourier
transforms of his click
echoes, in the form of
average amplitudes in
each of 30 frequency
bins, were then used as
inputs for a neural
network.
Nine
nonlinear layers were
used along with 30
input regions and 3

\1l'Y). ? ?

output plane

tttttttttttT
ALn
ttl

O(x,y)

?n(x,y)

?

?

?

?

?

?

?

?

?

I .ili'Ln

Wn(x,y)
En(x,y)

t t t t. t t t. t t t t
?

01 (X,y)

?

?

?

?

???

?

t?. t t t

W1 (x,y)
El(X,y)

ttttttttttt

I .ili'L1

T
ALo

...----_
_----,1
~
~1
____________I(_X_,y_)____________

Eo

.ili'Lo

ttttttttttt
Plane Wave

Figure 3: Optical Network Forward Data and Backward Error
DataFlow

A Lagrangian Formulatio1l for Optical Backpropagation

777

output regions, the remainder of the network physical parameters were the same as above
for the iris classification. Half the data (13 sets of clicks) was used to train the network,
with the other half of the data (14 sets) used to test the training. After training,
classification of the test data set was 100% correct.

5 EXPERIMENTAL RESULTS
As a proof of the concept, the optical neural network was constructed in the laboratory to be
trained to perform various logic functions. Two thermal self-defocusing layers were used,
one for the input and the other for a single layer of weighting. The nonlinear coefficient of
the index of refraction (nJ was measured to be -3xlO'" cm21W. The nonlinear layers had a
thickness (~NLo and ~NL.) of 630llm and were separated by a distance (~Lo) of 15cm. The
output region was 100llm wide and placed 15cm (~L.) behind the weighting layer. The
experiment used HeNe laser light to provide the input plane wave and the input and
weighting irradiances. The spatial profiles of the input and weighting layers were realized
by imaging a LCD spatial light modulator onto the respective nonlinear layers. The inputs
were two bright or dark regions on a Gaussian input beam producing the intensity profile:

whereIo= 12.5 mW/cm2, leo = 900llm, Xo = 600Ilffi, K. = 400Ilffi, and Qo and Q. are the logic
inputs taking on a value of zero or one. The weight profile W.(x) = Ioexp[-(xIKo)2][1 +w.(x)]
where w.(x) can range from zero to one and is found through training using an algorithm
which probed the weighting mask in order update the training weights. Table 1 shows the
experimental results for three different logic gates. Given is the normalized output before
and after training. The network was trained to recognize a logic zero for a normalized output
~ 0.9 and a logic one or a normalized output ~ 1.1. An output value greater than I is
considered a logic one and an output value less than one is a logic zero. RME is the root
mean error.

6

CONCLUSIONS

Work is in progress to improve the logic gate results by increasing the power of
propagating signal beam as well as both the input and weighting beams. This will
effectively increase the nonlinear processing capability of the network since a higher power
produces more nonlinear effect. Also, more power will allow expansion of all of the beams
thereby increasing the effective resolution of the thermal materials. Thisreduces the effect
of heat transfer within the material which tends to wash out or diffuse benificial steep
gradients in temperature which are what produce the gradients in the index of refraction. In
addition, the use of photorefractive crystals for optical weight storage shows promise for
being able to optically phase conjugate and backpropagate the output errror as well as
implement the weight update rule for all optical network training. This appears to be simpler
than optical networks using volume hologram weight storage because the Kerr network
requires only planar hologram storage.

778

Elizabeth C. Behrman

Inputs
Start
Finish
NOR
Change
Output

AND

Start
Finish
Change
Output
Start

XNOR Finish
Change
Output

0 0
1.001
1.110
.109

0 1
.802
.884

.998

.082
0
1.092

.757
-.241

.855
-.237

0

0

.998
1.084
.086

.880
.933

1

1

.053
0

1 0
.698

1 1

Rl\1E

.807
.896

7.3%

.089
0
1.440

-7.3%

1.124

0

-.254
0
.893

-.316

-16.4%

1
.994

7.3%

.928
.035

1.073
.079

2.7%
-4.6%

0

1

.772
.074
0
1.148
.894

0

16.4%

Table 1: Preliminary Experimental Logic Gate Results

References
Armstrong, J.A., Bloembergen, N., Ducuing, J., and Pershan, P.S., (1962) "Interactions
Between Light Waves in a Nonlinear Dielectric", Physical Review, Vol. 127, pp. 1918-1939.
Ie Cun, Yann, (1988) itA Theoretical Framework for Back-Propagation", Proceedings of the
1988 Connectionist Models Summer School, Morgan Kaufmann, pp. 21-28.
Pineda, F.J., (1987) "Generalization of backpropagation to recurrent and higher order neural
networks", Proceedings of IEEE Conference on Neural information Processing Systems,
November 1987, IEEE Press.
Roitblat., Moore, Nachtigall, and Penner, (1991) "Natural dolphin echo recognition
using an integrator gateway network," in Advances in Neural Processing Systems 3
Morgan Kaufmann, San Mateo, CA, 273-281.
Skinner, S.R, Steck, J.E., Behnnan, E .C., (1995) "An Optical Neural Network Using Kerr
Type Nonlinear Materials", To Appear in Applied Optics.
"Using Nworks, (1991) An Extended Tutorial for NeuralWorks Professional /lIPlus and
NeuralWorks Explorer, NeuralWare, Inc. Pittsburgh, PA, pg. UN-18.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 5862-backpropagation-for-energy-efficient-neuromorphic-computing.pdf

Backpropagation for
Energy-Efficient Neuromorphic Computing

Steve K. Esser
IBM Research?Almaden
650 Harry Road, San Jose, CA 95120
sesser@us.ibm.com

Rathinakumar Appuswamy
IBM Research?Almaden
650 Harry Road, San Jose, CA 95120
rappusw@us.ibm.com

Paul A. Merolla
IBM Research?Almaden
650 Harry Road, San Jose, CA 95120
pameroll@us.ibm.com

John V. Arthur
IBM Research?Almaden
650 Harry Road, San Jose, CA 95120
arthurjo@us.ibm.com

Dharmendra S. Modha
IBM Research?Almaden
650 Harry Road, San Jose, CA 95120
dmodha@us.ibm.com

Abstract
Solving real world problems with embedded neural networks requires both training algorithms that achieve high performance and compatible hardware that runs
in real time while remaining energy efficient. For the former, deep learning using
backpropagation has recently achieved a string of successes across many domains
and datasets. For the latter, neuromorphic chips that run spiking neural networks
have recently achieved unprecedented energy efficiency. To bring these two advances together, we must first resolve the incompatibility between backpropagation, which uses continuous-output neurons and synaptic weights, and neuromorphic designs, which employ spiking neurons and discrete synapses. Our approach
is to treat spikes and discrete synapses as continuous probabilities, which allows
training the network using standard backpropagation. The trained network naturally maps to neuromorphic hardware by sampling the probabilities to create one
or more networks, which are merged using ensemble averaging. To demonstrate,
we trained a sparsely connected network that runs on the TrueNorth chip using the
MNIST dataset. With a high performance network (ensemble of 64), we achieve
99.42% accuracy at 108 ?J per image, and with a high efficiency network (ensemble of 1) we achieve 92.7% accuracy at 0.268 ?J per image.

1

Introduction

Neural networks today are achieving state-of-the-art performance in competitions across a range of
fields [1][2][3]. Such success raises hope that we can now begin to move these networks out of
the lab and into embedded systems that can tackle real world problems. This necessitates a shift
DARPA: Approved for Public Release, Distribution Unlimited

1

in thinking to system design, where both neural network and hardware substrate must collectively
meet performance, power, space, and speed requirements.
On a neuron-for-neuron basis, the most efficient substrates for neural network operation today are
dedicated neuromorphic designs [4][5][6][7]. To achieve high efficiency, neuromorphic architectures can use spikes to provide event based computation and communication that consumes energy
only when necessary, can use low precision synapses to colocate memory with computation keeping
data movement local and allowing for parallel distributed operation, and can use constrained connectivity to implement neuron fan-out efficiently thus dramatically reducing network traffic on-chip.
However, such design choices introduce an apparent incompatibility with the backpropagation algorithm [8] used for training today?s most successful deep networks, which uses continuous-output
neurons and high-precision synapses, and typically operates with no limits on the number of inputs
per neuron. How then can we build systems that take advantage of algorithmic insights from deep
learning, and the operational efficiency of neuromorphic hardware?
As our main contribution here, we demonstrate a learning rule and a network topology that reconciles
the apparent incompatibility between backpropagation and neuromorphic hardware. The essence
of the learning rule is to train a network offline with hardware supported connectivity, as well as
continuous valued input, neuron output, and synaptic weights, but values constrained to the range
[0, 1]. We further impose that such constrained values represent probabilities, either of a spike
occurring or of a particular synapse being on. Such a network can be trained using backpropagation,
but also has a direct representation in the spiking, low synaptic precision deployment system, thereby
bridging these two worlds. The network topology uses a progressive mixing approach, where each
neuron has access to a limited set of inputs from the previous layer, but sources are chosen such that
neurons in successive layers have access to progressively more network input.
Previous efforts have shown success with subsets of the elements we bring together here. Backpropagation has been used to train networks with spiking neurons but with high-precision weights
[9][10][11][12], and the converse, networks with trinary synapses but with continuous output neurons [13]. Other probabilistic backpropagation approaches have been demonstrated for networks
with binary neurons and binary or trinary synapses but full inter-layer connectivity [14][15].
The work presented here is novel in that i) we demonstrate for the first time an offline training
methodology using backpropagation to create a network that employs spiking neurons, synapses requiring less bits of precision than even trinary weights, and constrained connectivity, ii) we achieve
the best accuracy to date on MNIST (99.42%) when compared to networks that use spiking neurons,
even with high precision synapses (99.12%) [12], as well as networks that use binary synapses and
neurons (97.88%) [15], and iii) we demonstrate the network running in real-time on the TrueNorth
chip [7], achieving by far the best published power efficiency for digit recognition (4 ?J per classification at 95% accuracy running 1000 images per second) compared to other low power approaches
(6 mJ per classification at 95% accuracy running 50 images per second) [16].

2

Deployment Hardware

We use the TrueNorth neurosynaptic chip [7] as our example deployment system, though the approach here could be generalized to other neuromorphic hardware [4][5][6]. The TrueNorth chip
consists of 4096 cores, with each core containing 256 axons (inputs), a 256 ? 256 synapse crossbar, and 256 spiking neurons. Information flows via spikes from a neuron to one axon between
any two cores, and from the axon to potentially all neurons on the core, gated by binary synapses
in the crossbar. Neurons can be considered to take on a variety of dynamics [17], including those
described below. Each axon is assigned 1 of 4 axon types, which is used as an index into a lookup
table of s-values, unique to each neuron, that provides a signed 9-bit integer synaptic strength to the
corresponding synapse. This approach requires only 1 bit per synapse for the on/off state and an
additional 0.15 bits per synapse for the lookup table scheme.

3

Network Training

In our approach, we employ two types of multilayer networks. The deployment network runs on a
platform supporting spiking neurons, discrete synapses with low precision, and limited connectivity.
2

The training network is used to learn binary synaptic connectivity states and biases. This network
shares the same topology as the deployment network, but represents input data, neuron outputs,
and synaptic connections using continuous values constrained to the range [0, 1] (an overview is
provided in Figure 1 and Table 1). These values correspond to probabilities of a spike occurring
or of a synapse being ?on?, providing a means of mapping the training network to the deployment
network, while providing a continuous and differentiable space for backpropagation. Below, we
describe the deployment network, our training methodology, and our procedure for mapping the
training network to the deployment network.
3.1

Deployment network

Deployment

Our deployment network follows a feedforward methodology where neurons are sequentially updated from the first to the last
layer. Input to the network is represented using stochastically generated spikes, where the
value of each input unit is 0 or 1 with some
probability. We write this as P (xi = 1) ? x
?i ,
where xi is the spike state of input unit i and
x
?i is a continuous value in the range [0, 1] derived by re-scaling the input data (pixels). This
scheme allows representation of data using binary spikes, while preserving data precision in
the expectation.

Training

Input

Input

Neuron

Connected
synapses

.2

.9

.5

.2

.7

.7
Synaptic
connection
probabilities

Neuron

Spikes
Synapse
strength
-1
1

.7
Spike
probabilities

Figure 1: Diagram showing input, synapses, and
output for one neuron in the deployment and training network. For simplicity, only three synapses
are depicted.

Summed neuron input is computed as
X
Ij =
xi cij sij + bj ,

(1)

i

where j is the target neuron index, cij is a binary indicator variable representing whether a
synapse is on, sij is the synaptic strength, and bj is the bias term. This is identical to common practice in neural networks, except that we have factored the synaptic weight into cij and sij , such that
we can focus our learning efforts on the former for reasons described below. The neuron activation
function follows a history-free thresholding equation

nj =

1
0

if Ij > 0,
otherwise.

These dynamics are implemented in TrueNorth by setting each neuron?s leak equal to the learned
bias term (dropping any fractional portion), its threshold to 0, its membrane potential floor to 0, and
setting its synapse parameters using the scheme described below.
We represent each class label using multiple output neurons in the last layer of the network, which
we found improves prediction performance. The network prediction for a class is simply the average
of the output of all neurons assigned to that class.

Table 1: Network components

Network input
Synaptic connection
Synaptic strength
Neuron output

Deployment Network
Variable
Values
x
{0, 1}
c
{0, 1}
s
{?1, 1}
n
{0, 1}

3

Correspondance
P (x = 1) ? x
?
P (c = 1) ? c?
s?s
P (n = 1) ? n
?

Training Network
Variable Values
x
?
[0, 1]
c?
[0, 1]
s
{?1, 1}
n
?
[0, 1]

3.2

Training network

Training follows the backpropagation methodology by iteratively i) running a forward pass from
the first layer to the last layer, ii) comparing the network output to desired output using a loss
function, iii) propagating the loss backwards through the network to determine the loss gradient
at each synapse and bias term, and iv) using this gradient to update the network parameters. The
training network forward pass is a probabilistic representation of the deployment network forward
pass.
Synaptic connections are represented as probabilities using c?ij , where P (cij = 1) ? c?ij , while
synaptic strength is represented using sij as in the deployment network. It is assumed that sij
can be drawn from a limited set of values and we consider the additional constraint that it is set
in ?blocks? such that multiple synapses share the same value, as done in TrueNorth for efficiency.
While it is conceivable to learn optimal values for sij under such conditions, this requires stepwise
changes between allowed values and optimization that is not local to each synapse. We take a simpler
approach here, which is to learn biases and synapse connection probabilities, and to intelligently fix
the synapse strengths using an approach described in the Network Initialization section.
Input to the training network is represented using x
?i , which is the probability of an input spike
occurring in the deployment network. For neurons, we note that Equation 1 is a summation of
weighted Bernoulli variables plus a bias term. If we assume independence of these inputs and have
sufficient numbers, then we can approximate the probability distribution of this summation as a
Gaussian with mean
X
?j = bj +
x
?i c?ij sij
i

and variance
?j2 =

X

x
?i c?ij (1 ? x
?i c?ij )s2ij .

(2)

i

We can then derive the probability of such a neuron firing using the complementary cumulative
distribution function of a Gaussian:
?
?
??
1
? ? ?j ??
n
? j = 1 ? ?1 + erf ? q
,
(3)
2
2? 2
j

where erf is the error function, ? = 0 and P (nj = 1) ? n
? j . For layers after the first, x
?i is replaced
by the input from the previous layer, n
? i , which represents the probability that a neuron produces a
spike.
A variety of loss functions are suitable for our approach, but we found that training converged the
fastest when using log loss,
X
E=?
[yk log(pk ) + (1 ? yk ) log(1 ? pk )] ,
k

where for each class k, yk is a binary class label that is 1 if the class is present and 0 otherwise, and
pk is the probability that the the average spike count for the class is greater than 0.5. Conveniently,
we can use the Gaussian approximation in Equation 3 for this, with ? = 0.5 and the mean and
variance terms set by the averaging process.
The training network backward pass is an adaptation of backpropagation using the neuron and
synapse equations above. To get the gradient at each synapse, we use the chain rule to compute
?E ? n
?j
?E
=
.
??
cij
?n
? j ??
cij
For the bias, a similar computation is made by replacing c?ij in the above equation with bj .
We can then differentiate Equation 3 to produce


?n
?j
x
?i sij ?
= ? e
??
cij
?j 2?

(???j )2
2? 2
j





x
?i s2ij ? x
?2i c?ij s2ij ?
?
? (? ? ?j )
e
?j3 2?
4

(???j )2
2? 2
j



.

(4)

As described below, we will assume that the synapse strengths to each neuron are balanced between
positive and negative values and that each neuron receives 256 inputs, so we can expect ? to be
close to zero, and ?, n
? i and c?ij to be much less than ?. Therefore, the right term of Equation 4
containing the denominator ?j3 , can be expected to be much smaller than the left term containing the
denominator ?j . Under these conditions, for computational efficiency we can approximate Equation
4 by dropping the right term and factoring out the remainder as
?n
?j
?n
? j ??j
?
,
??
cij
??j ??
cij
where

?
?n
?j
1
= ? e
??j
?j 2?

(???j )2
2? 2
j



,

and
??j
=x
?i sij .
??
cij
A similar treatment can be used to show that corresponding gradient with respect to the bias term
equals one.
The network is updated using the loss gradient at each synapse and bias term. For each iteration,
synaptic connection probability changes according to
??
cij = ??

?E
,
??
cij

where ? is the learning rate. Any synaptic connection probabilities that fall outside of the range
[0, 1] as a result of the update rule are ?snapped? to the nearest valid value. Changes to the bias
term are handled in a similar fashion, with values clipped to fall in the range [?255, 255], the largest
values supported using TrueNorth neuron parameters.
The training procedure described here is amenable to methods and heuristics applied in standard
backpropagation. For the results shown below, we used mini batch size 100, momentum 0.9, dropout
0.5 [18], learning rate decay on a fixed schedule across training iterations starting at 0.1 and multiplying by 0.1 every 250 epochs, and transformations of the training data for each iteration with
rotation up to ?15? , shift up to ?5 pixels and rescale up to ?15%.
3.3

Mapping training network to deployment network

Training is performed offline, and the resulting network is mapped to the deployment network for
hardware operation. For deployment, depending on system requirements, we can utilize an ensemble
of one or more samplings of the training network to increase overall output performance. Unlike
other ensemble methods, we train only once then sample the training network for each member. The
system output for each class is determined by averaging across all neurons in all member networks
assigned to the class. Synaptic connection states are set on or off according to P (cij = 1) ?
c?ij , using independent random number draws for each synapse in each ensemble member. Data is
converted into a spiking representation for input using P (xi = 1) ? x
?i , using independent random
number draws for each input to each member of the ensemble.
3.4

Network initialization

The approach for network initialization described here allows us to optimize for efficient neuromorphic hardware that employs less than 2 bits per synapse. In our approach, each synaptic connection
probability is initialized from a uniform random distribution over the range [0, 1]. To initialize
synapse strength values, we begin from the principle that each core should maximize information
transfer by maximizing information per neuron and minimizing redundancy between neurons. Such
methods have been explored in detail in approaches such as infomax [19]. While the first of these
goals is data dependent, we can pursue the second at initialization time by tuning the space of possible weights for a core, represented by the matrix of synapse strength values, S.
5

In our approach, we wish to minimize redundancy
between neurons on a core by attempting to induce a
product distribution on the outputs for every pair of
neurons. To simplify the problem, we note that the
summed weighted inputs to a pair of neurons is wellapproximated by a bi-variate Gaussian distribution.
Thus, forcing the covariance between the summed
weighted inputs to zero guarantees that the inputs are
independent. Furthermore, since functions of pairwise independent random variables remain pair-wise
independent, the neuron outputs are guaranteed to be
independent.

Axons 1-64
Type 1

Axons 65-128
Type 2

Axons 129-192
Type 3

Axons 193-256
Type 4
Synapse
strength

Neurons 246-256

Neurons 12-22
Neurons 1-11

The summed weighted input to j-th neuron is given
-1
...
1
by Equation 1. It is desirable for the purposes of
maintaining balance in neuron dynamics to configure its weights using a mix of positive and negative
values that sum to zero. Thus for all j,
Figure 2:
Synapse strength values deX
sij = 0,
(5) picted as axons (rows) ? neurons (columns)
array. The learning procedure fixes these
i
values when the network is initialized and
which implies that E[Ij ] ? 0 assuming inputs and learns the probability that each synapse is
synaptic connection states are both decorrelated and in a transmitting state. The blocky appearthe bias term is near 0. This simplifies the covariance ance of the strength matrix is the result of
between the inputs to any two neurons on a core to
the shared synaptic strength approach used
?
?
by TrueNorth to reduce memory footprint.
X
E[Ij Ir ] = E ?
xi cij sij xq cqr sqr ? .
i,q

Rearranging terms, we get
E[Ij Ir ] =

X

cij sij cqr sqr E[x2i ] +

i

X
i

cij sij

X

cqr sqr E[xi xq ].

(6)

q6=i

Next, we note from the equation for covariance that E[xi xq ] = ?(xi , xq ) + E[xi ]E[xq ]. Under
the assumption that inputs have equal mean and variance, then for any i, E[x2i ] = ?, where ? =
?(xi , xq ) + E[xi ]E[xq ] is a constant. Further assuming that covariance between xi and xq where
i 6= q is the same for all inputs, then E[xi xq ] = ?, where ? = ?(xi , xq ) + E[xi ]E[xq ] is a constant.
Using this and equation (5), Equation 6 becomes
X
E[Ij Ir ] = ? hcj sj , cr sr i + ?
cij sij (?cir sir )
i

= ? hcj sj , cr sr i ? ? hcj sj , cr sr i
= (? ? ?) hcj sj , cr sr i .
So minimizing the absolute value of the inner product between columns of W forces Ij and Ir to be
maximally uncorrelated under the constraints.
Inspired by this observation, we apriori (i.e., without any knowledge of the input data) choose the
strength values such that the absolute value of the inner product between columns of the effective
weight matrix is minimized, and the sum of effective weights to each neuron is zero. Practically,
this is achieved by assigning half of each neuron?s s-values to ?1 and the other half to 1, balancing
the possible permutations of such assignments so they occur as equally as possible across neurons
on a core, and evenly distributing the four possible axon types amongst the axons on a core. The
resulting matrix of synaptic strength values can be seen in Figure 2. This configuration thus provides
an optimal weight subspace, given the constraints, in which backpropagation can operate in a datadriven fashion to find desirable synaptic on/off states.
6

B

A

1 cm

one core
(256 neurons)

input window

C
stride

256 neurons
layer
input

12

input

3

16

16
5 Core Network

30 Core Network

Figure 3: A) Two network configurations used for the results described here, a 5 core network
designed to minimize core count and a 30 core network designed to maximize accuracy. B) Board
with a socketed TrueNorth chip used to run the deployment networks. The chip is 4.3 cm2 , runs
in real time (1 ms neuron updates), and consumes 63 mW running a benchmark network that uses
all of its 1 million neuron [7]. C) Measured accuracy and measured energy for the two network
configurations running on the chip. Ensemble size is shown to the right of each data point.

4

Network topology

The network topology is designed to support neurons with responses to local, regional or global
features while respecting the ?core-to-core? connectivity of the TrueNorth architecture ? namely
that all neurons on a core share access to the same set of inputs, and that the number of such inputs
is limited. The network uses a multilayer feedforward scheme, where the first layer consists of input
elements in a rows ? columns ? channels array, such as an image, and the remaining layers consist
of TrueNorth cores. Connections between layers are made using a sliding window approach.
Input to each core in a layer is drawn from an R ? R ? F input window (Figure 3A), where R
represents the row and column dimensions, and F represents the feature dimension. For input from
the first layer, rows and columns are in units of input elements and features are input channels,
while for input from the remaining layers rows and columns are in units of cores and features are
neurons. The first core in a given target layer locates its input window in the upper left corner of its
source layer, and the next core in the target layer shifts its input window to the right by a stride of
S. Successive cores slide the window over by S until the edge of the source layer is reached, then
the window is returned to the left, and shifted down by S and the process is repeated. Features are
sub-selected randomly, with the constraint that each neuron can only be selected by one target core.
We allow input elements to be selected multiple times. This scheme is similar in some respects to
that used by a convolution network, but we employ independent synapses for each location. The
specific networks employed here, and associated parameters, are shown in Figure 3A.

5

Results

We applied the training method described above to the MNIST dataset [20], examining accuracy vs.
energy tradeoffs using two networks running on the TrueNorth chip (Figure 3B). The first network is
the smallest multilayer TrueNorth network possible for the number of pixels present in the dataset,
consisting of 5 cores distributed in 2 layers, corresponding to 512 neurons. The second network
was built with a primary goal of maximizing accuracy, and is composed of 30 cores distributed in
4 layers (Figure 3A), corresponding to 3840 neurons. Networks are configured with a first layer
using R = 16 and F = 1 in both networks, and S = 12 in the 5 core network and S = 4 in the 30
core network, while all subsequent layers in both networks use R = 2, F = 64, and S = 1. These
parameters result in a ?pyramid? shape, where all cores from layer 2 to the final layer draw input
7

from 4 source cores and 64 neurons in each of those sources. Each core employs 64 neurons per
core it targets, up to a maximum of 256 neurons.
We tested each network in an ensemble of 1, 4, 16, or 64 members running on a TrueNorth chip in
real-time. Each image was encoded using a single time step (1 ms), with a different spike sampling
used for each input line targeted by a pixel. The instrumentation available measures active power
for the network in operation and leakage power for the entire chip, which consists of 4096 cores.
We report energy numbers as active power plus the fraction of leakage power for the cores in use.
The highest overall performance we observed of 99.42% was achieved with a 30 core trained network using a 64 member ensemble, for a total of 1920 cores, that was measured using 108 ?J per
classification. The lowest energy was achieved by the 5 core network operating in an ensemble of
1, that was measured using 0.268 ?J per classification while achieving 92.70% accuracy. Results
are plotted showing accuracy vs. energy in Figure 3C. Both networks classified 1000 images per
second.

6

Discussion

Our results show that backpropagation operating in a probabilistic domain can be used to train
networks that naturally map to neuromorphic hardware with spiking neurons and extremely lowprecision synapses. Our approach can be succinctly summarized as constrain-then-train, where
we first constrain our network to provide a direct representation of our deployment system and
then train within those constraints. This can be contrasted with a train-then-constrain approach,
where a network agnostic to the final deployment system is first trained, and following training is
constrained through normalization and discretization methods to provide a spiking representation or
low precision weights. While requiring a customized training rule, the constrain-then-train approach
offers the advantage that a decrease in training error has a direct correspondence to a decrease in
error for the deployment network. Conversely, the train-then-constrain approach allows use of off
the shelf training methods, but unconstrained training is not guaranteed to produce a reduction in
error after hardware constraints are applied.
Looking forward, we see several avenues for expanding this approach to more complex datasets.
First, deep convolution networks [20] have seen a great deal of success by using backpropagation
to learn the weights of convolutional filters. The learning method introduced here is independent of
the specific network structure beyond the given sparsity constraint, and could certainly be adapted
for use in convolution networks. Second, biology provides a number of examples, such as the retina
or cochlea, for mapping high-precision sensory data into a binary spiking representation. Drawing
inspiration from such approaches may improve performance beyond the linear mapping scheme used
in this work. Third, this approach may also be adaptable to other gradient based learning methods,
or to methods with existing probabilistic components such as contrastive divergence [21]. Further,
while we describe the use of this approach with TrueNorth to provide a concrete use case, we see
no reason why this training approach cannot be used with other spiking neuromorphic hardware
[4][5][6].
We believe this work is particularly timely, as in recent years backpropagation has achieved a high
level of performance on a number tasks reflecting real world tasks, including object detection in
complex scenes [1], pedestrian detection [2], and speech recognition [3]. A wide range of sensors
are found in mobile devices ranging from phones to automobiles, and platforms like TrueNorth
provide a low power substrate for processing that sensory data. By bridging backpropagation and
energy efficient neuromorphic computing, we hope that the work here provides an important step
towards building low-power, scalable brain-inspired systems with real world applicability.
Acknowledgments
This research was sponsored by the Defense Advanced Research Projects Agency under contracts
No. HR0011- 09-C-0002 and No. FA9453-15-C-0055. The views, opinions, and/or findings contained in this paper are those of the authors and should not be interpreted as representing the official
views or policies of the Department of Defense or the U.S. Government.
8

References
[1] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla,
M. Bernstein, A. C. Berg, and L. Fei-Fei, ?ImageNet Large Scale Visual Recognition Challenge,? International Journal of Computer Vision, 2015.
[2] W. Ouyang and X. Wang, ?Joint deep learning for pedestrian detection,? in International Conference on
Computer Vision, pp. 2056?2063, 2013.
[3] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen, R. Prenger, S. Satheesh, S. Sengupta, A. Coates, et al., ?Deepspeech: Scaling up end-to-end speech recognition,? arXiv preprint
arXiv:1412.5567, 2014.
[4] B. Benjamin, P. Gao, E. McQuinn, S. Choudhary, A. Chandrasekaran, J.-M. Bussat, R. Alvarez-Icaza,
J. Arthur, P. Merolla, and K. Boahen, ?Neurogrid: A mixed-analog-digital multichip system for largescale neural simulations,? Proceedings of the IEEE, vol. 102, no. 5, pp. 699?716, 2014.
[5] E. Painkras, L. Plana, J. Garside, S. Temple, F. Galluppi, C. Patterson, D. Lester, A. Brown, and S. Furber,
?SpiNNaker: A 1-W 18-core system-on-chip for massively-parallel neural network simulation,? IEEE
Journal of Solid-State Circuits, vol. 48, no. 8, pp. 1943?1953, 2013.
[6] T. Pfeil, A. Gr?ubl, S. Jeltsch, E. M?uller, P. M?uller, M. A. Petrovici, M. Schmuker, D. Br?uderle, J. Schemmel, and K. Meier, ?Six networks on a universal neuromorphic computing substrate,? Frontiers in neuroscience, vol. 7, 2013.
[7] P. A. Merolla, J. V. Arthur, R. Alvarez-Icaza, A. S. Cassidy, J. Sawada, F. Akopyan, B. L. Jackson,
N. Imam, C. Guo, Y. Nakamura, et al., ?A million spiking-neuron integrated circuit with a scalable communication network and interface,? Science, vol. 345, no. 6197, pp. 668?673, 2014.
[8] D. Rumelhart, G. Hinton, and R. Williams, ?Learning representations by back-propagating errors,? Nature, vol. 323, no. 6088, pp. 533?536, 1986.
[9] P. Moerland and E. Fiesler, ?Neural network adaptations to hardware implementations,? in Handbook of
neural computation (E. Fiesler and R. Beale, eds.), New York: Institute of Physics Publishing and Oxford
University Publishing, 1997.
[10] E. Fiesler, A. Choudry, and H. J. Caulfield, ?Weight discretization paradigm for optical neural networks,?
in The Hague?90, 12-16 April, pp. 164?173, International Society for Optics and Photonics, 1990.
[11] Y. Cao, Y. Chen, and D. Khosla, ?Spiking deep convolutional neural networks for energy-efficient object
recognition,? International Journal of Computer Vision, vol. 113, no. 1, pp. 54?66, 2015.
[12] P. U. Diehl, D. Neil, J. Binas, M. Cook, S.-C. Liu, and M. Pfeiffer, ?Fast-classifying, high-accuracy
spiking deep networks through weight and threshold balancing,? in International Joint Conference on
Neural Networks, 2015, in press.
[13] L. K. Muller and G. Indiveri, ?Rounding methods for neural networks with low resolution synaptic
weights,? arXiv preprint arXiv:1504.05767, 2015.
[14] J. Zhao, J. Shawe-Taylor, and M. van Daalen, ?Learning in stochastic bit stream neural networks,? Neural
Networks, vol. 9, no. 6, pp. 991 ? 998, 1996.
[15] Z. Cheng, D. Soudry, Z. Mao, and Z. Lan, ?Training binary multilayer neural networks for image classification using expectation backpropgation,? arXiv preprint arXiv:1503.03562, 2015.
[16] E. Stromatias, D. Neil, F. Galluppi, M. Pfeiffer, S. Liu, and S. Furber, ?Scalable energy-efficient, lowlatency implementations of spiking deep belief networks on spinnaker,? in International Joint Conference
on Neural Networks, IEEE, 2015, in press.
[17] A. S. Cassidy, P. Merolla, J. V. Arthur, S. Esser, B. Jackson, R. Alvarez-Icaza, P. Datta, J. Sawada, T. M.
Wong, V. Feldman, A. Amir, D. Rubin, F. Akopyan, E. McQuinn, W. Risk, and D. S. Modha, ?Cognitive
computing building block: A versatile and efficient digital neuron model for neurosynaptic cores,? in
International Joint Conference on Neural Networks, 2013.
[18] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov, ?Improving neural
networks by preventing co-adaptation of feature detectors,? arXiv preprint arXiv:1207.0580, 2012.
[19] A. J. Bell and T. J. Sejnowski, ?An information-maximization approach to blind separation and blind
deconvolution,? Neural computation, vol. 7, no. 6, pp. 1129?1159, 1995.
[20] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, ?Gradient-based learning applied to document recognition,? Proceedings of the IEEE, vol. 86, no. 11, pp. 2278?2324, 1998.
[21] G. E. Hinton and R. R. Salakhutdinov, ?Reducing the dimensionality of data with neural networks,?
Science, vol. 313, no. 5786, pp. 504?507, 2006.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

