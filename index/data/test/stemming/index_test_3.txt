query sentence: backpropagated error in neural-network
---------------------------------------------------------------------
title: 1100-tempering-backpropagation-networks-not-all-weights-are-created-equal.pdf

Tempering Backpropagation Networks:
Not All Weights are Created Equal

Nicol N. Schraudolph
EVOTEC BioSystems GmbH

Grandweg 64
22529 Hamburg, Germany
nici@evotec.de

Terrence J. Sejnowski
Computational Neurobiology Lab
The Salk Institute for BioI. Studies
San Diego, CA 92186-5800, USA
terry@salk.edu

Abstract
Backpropagation learning algorithms typically collapse the network's
structure into a single vector of weight parameters to be optimized. We
suggest that their performance may be improved by utilizing the structural information instead of discarding it, and introduce a framework for
''tempering'' each weight accordingly.
In the tempering model, activation and error signals are treated as approximately independent random variables. The characteristic scale of weight
changes is then matched to that ofthe residuals, allowing structural properties such as a node's fan-in and fan-out to affect the local learning rate
and backpropagated error. The model also permits calculation of an upper
bound on the global learning rate for batch updates, which in turn leads
to different update rules for bias vs. non-bias weights.
This approach yields hitherto unparalleled performance on the family relations benchmark, a deep multi-layer network: for both batch learning
with momentum and the delta-bar-delta algorithm, convergence at the
optimal learning rate is sped up by more than an order of magnitude.

1 Introduction
Although neural networks are structured graphs, learning algorithms typically view them
as a single vector of parameters to be optimized. All information about a network's architecture is thus discarded in favor of the presumption of an isotropic weight space - the
notion that a priori all weights in the network are created equal. This serves to decouple
the learning process from network design and makes a large body of function optimization
techniques directly applicable to backpropagation learning.
But what if the discarded structural information holds valuable clues for efficient weight
optimization? Adaptive step size and second-order gradient techniques (Battiti, 1992) may

N. N. SCHRAUDOLPH. T. J. SEJNOWSKI

564

recover some of it, at considerable computational expense. Ad hoc attempts to incorporate
structural information such as the fan-in (Plaut et aI., 1986) into local learning rates have become a familiar part of backpropagation lore; here we deri ve a more comprehensi ve framework - which we call tempering - and demonstrate its effectiveness.
Tempering is based on modeling the acti vities and error signals in a backpropagation network as independent random variables. This allows us to calculate activity- and weightinvariant upper bounds on the effect of synchronous weight updates on a node's activity.
We then derive appropriate local step size parameters by relating this maximal change in a
node's acti vi ty to the characteristic scale of its residual through a global learning rate.
Our subsequent derivation of an upper bound on the global learning rate for batch learning
suggests that the d.c. component of the error signal be given special treatment. Our experiments show that the resulting method of error shunting allows the global learning rate to
approach its predicted maximum, for highly efficient learning performance.

2 Local Learning Rates
Consider a neural network with feedforward activation given by
x j = /j (Yj)

,

Yj

=

L

(1)

Xi Wij ,

iEAj

where Aj denotes the set of anterior nodes feeding directly into node j, and /j is a nonlinear
(typically sigmoid) activation function. We imply that nodes are activated in the appropriate
sequence, and that some have their values clamped so as to represent external inputs.
With a local learning rate of'1j for node j, gradient descent in an objective function E produces the weight update

(2)
Linearizing

Ij

around

Yj

approximates the resultant change in activation

Xj

as
(3)

iEAj

iEAj

Our goal is to put the scale of ~Xj in relation to that of the error signal tSj . Specifically, when
averaged over many training samples, we want the change in output activity of each node
in response to each pattern limited to a certain proportion - given by the global learning
rate '1 - of its residual. We achieve this by relating the variation of ~X j over the training
set to that of the error signal:
(4)

where (.) denotes averaging over training samples. Formally, this approach may be interpreted as a diagonal approximation of the inverse Fischer information matrix (Amari, 1995).
We implement (4) by deriving an upper bound for the left-hand side which is then equated
with the right-hand side. Replacing the acti vity-dependent slope of Ij by its maximum value

s(/j) == maxl/j(u)1
u

(5)

and assuming that there are no correlations! between inputs Xi and error tSj ' we obtain
(~x}):::; '1} s(/j)2 (tS})f.j
1 Note

that such correlations are minimized by the local weight update.

(6)

Tempering Backpropagation Networks: Not All Weights Are Created Equal

565

from (3), provided that

ej ~ e;

==

([,Lxlf) ,

(7)

lEA]

We can now satisfy (4) by setting the local learning rate to
TJ' =
J -

8 (fj

TJ

(8)

).j[j .

ej

There are several approaches to computing an upper bound on the total squared input
power
One option would be to calculate the latter empirically during training, though
this raises sampling and stability issues. For external inputs we may precompute orderive
an upper bound based on prior knowledge of the training data. For inputs from other nodes
in the network we assume independence and derive
from the range of their activation
functions:
=
p(fd 2 , where p(fd == ffiuax/i(u)2.
(9)

e;.

e;

ej

ej

L

iEAj

Note that when all nodes use the same activation function

I,

we obtain the well-known

Vfan-in heuristic (Plaut et al., 1986) as a special case of (8).

3 Error Backpropagation
In deriving local learning rates above we have tacitly used the error signal as a stand-in for
the residual proper, i.e. the distance to the target. For output nodes we can scale the error to
never exceed the residual:
(10)

Note that for the conventional quadratic error this simplifies to <Pj = s(/j) . What about
the remainder of the network? Unlike (Krogh et aI., 1990), we do not wish to prescribe
definite targets (and hence residuals) for hidden nodes. Instead we shall use our bounds
and independence arguments to scale backpropagated error signals to roughly appropriate
magnitude. For this purpose we introduce an attenuation coefficient aj into the error backpropagation equation:

c5j

= II (Yi)
aj

L

Wjj

c5j

,

(11)

jEP,

where Pi denotes the set of posterior nodes fed directly from node i. We posit that the appropriate variation for c5i be no more than the weighted average of the variation of backpropagated errors:
(12)

whereas, assuming independence between the c5j and replacing the slope of Ii by its maximum value, (11) gives us

(c5?) ~

a? 8(f;)2 L

wi /

(c5/) .

(13)

jEP,

Again we equate the right-hand sides of both inequalities to satisfy (12), yielding
ai

==

1

8(fdJiP;T .

(14)

566

N. N. SCHRAUDOLPH, T. J. SEJNOWSKI

Note that the incorporation ofthe weights into (12) is ad hoc, as we have no a priori reason
to scale a node's step size in proportion to the size of its vector of outgoing weights. We
have chosen (12) simply because it produces a weight-invariant value for the attenuation
coefficient. The scale of the backpropagated error could be controlled more rigorously, at
the expense of having to recalculate ai after each weight update.

4

Global Learning Rate

We now derive the appropriate global learning rate for the batch weight update
LiWij

==

1]j

L dj (t)

Xi

(15)

(t)

tET

over a non-redundant training sample T. Assuming independent and zero-mean residuals,
we then have
(16)

by virtue of (4). Under these conditions we can ensure
~

2 ~ (dj 2)

/).Xj

(17)

,

i.e. that the variation of the batch weight update does not exceed that of the residual, by
using a global learning rate of
1]

~

1]*

== l/JiTf.

(18)

Even when redundancy in the training set forces us to use a lower rate, knowing the upper
bound 1]* effectively allows an educated guess at 1], saving considerable time in practice.

5 Error Shunting
It remains to deal with the assumption made above that the residuals be zero-mean, i.e. that
(dj)
O. Any d.c. component in the error requires a learning rate inversely proportional to
the batch size - far below 1]* , the rate permissible for zero-mean residuals. This suggests
handling the d.c. component of error signals separately. This is the proper job of the bias
weight, so we update it accordingly :

=

(19)
In order to allow learning at rates close to
then centered by subtracting the mean:

1]*

for all other weights, their error signals are
(20)

tET
T/j

(L

tET

dj (t)

X i (t)

- (Xi)

L

dj (t))

(21)

tET

Note that both sums in (21) must be collected in batch implementations of backpropagation
anyway - the only additional statistic required is the average input activity (Xi) ' Indeed
for batch update centering errors is equivalent to centering inputs, which is known to assist
learning by removing a large eigenvalue of the Hessian (LeCun et al., 1991). We expect
online implementations to perform best when both input and error signals are centered so
as to improve the stochastic approximation.

567

Tempering Backpropagation Networks: Not All Weights Are Created Equal

person

2 OO~O~OOOOOOO

000000000000

TJeff ~

'""<Lt tr,: j 1*'j:'i~

1.5 TJ

000000

A "~t{(d$?.?.?:?. ?.?.d+; BI?.

000000000000
~?;;?dt!i """" "'
~El~
000000
000000
~if;i ? ' :r:.? . ; ..0;:....",
..<1!. (i+1 ~ S?!? .? . ; ~
person 1 OOOOO~OOOOOO

000000000000

.25 TJ
.10TJ

.05 TJ

OOOOOOOOOO~O

relationship

Figure 1: Backpropagation network for learning family relations (Hinton, 1986).

6 Experimental Setup
We tested these ideas on the family relations task (Hinton, 1986): a backpropagation network is given examples of a family member and relationship as input, and must indicate
on its output which family members fit the relational description according to an underlying family tree. Its architecture (Figure 1) consists of a central association layer of hidden
units surrounded by three encoding layers that act as informational bottlenecks, forcing the
network to make the deep structure of the data explicit.
The input is presented to the network in a canonical local encoding: for any given training
example, exactly one input in each of the two input layers is active. On account of the always
active bias input, the squared input power for tempering at these layers is thus C = 4. Since
the output uses the same local code, only one or two targets at a time will be active; we
therefore do not attenuate error signals in the immediately preceding layer. We use crossentropy error and the logistic squashing function (1 + e- Y)-l at the output (giving ?> = 1)
but prefer the hyperbolic tangent for hidden units, with p(tanh) = s(tanh) = 1.
To illustrate the impact of tempering on this architecture we translate the combined effect
of local learning rate and error attenuation into an effective learning rate 2 for each layer,
shown on the right in Figure 1. We observe that effective learning rates are largest near the
output and decrease towards the input due to error attenuation. Contrary to textbook opinion
(LeCun, 1993; Haykin, 1994, page 162) we find that such unequal step sizes are in fact the
key to efficient learning here. We suspect that the logistic squashing function may owe its
popUlarity largely to the error attenuation side-effect inherent in its maximum slope of 114We expect tempering to be applicable to a variety of backpropagation learning algorithms;
here we present first results for batch learning with momentum and the delta-bar-delta
rule (Jacobs, 1988). Both algorithms were tested under three conditions: conventional,
tempered (as described in Sections 2 and 3), and tempered with error shunting. All experiments were performed with a customized simulator based on Xerion 3.1.3
For each condition the global learning rate TJ was empirically optimized (to single-digit precision) for fastest reliable learning performance, as measured by the sum of empirical mean
and standard deviation of epochs required to reach a given low value of the cost function.
All other parameters were held in variant across experiments; their values (shown in Table 1)
were chosen in advance so as not to bias the results.
2This is possible only for strictly layered networks, i.e. those with no shortcut (or "skip-through")
connections between topologically non-adjacent layers.
3 At the time of writing, the Xerion neural network simulator and its successor UTS are available
by anonymous file transfer from ai.toronto.edu, directory pub/xerion.

568

N. N. SCHRAUDOLPH. T. 1. SEJNOWSKI

Parameter

Val ue

training set size (= epoch)
momentum parameter
uniform initial weight range
weight decay rate per epoch

100
0.9
?0.3
10- 4

II

I Value I

Parameter

0.2
1.0
0.1
0.9

zero-error radius around target
acceptable error & weight cost
delta-bar-delta gain increment
delta-bar-delta gain decrement

Table 1: Invariant parameter settings for our experim~nts.

7

Experimental Results

Table 2 lists the empirical mean and standard deviation (over ten restarts) of the number
of epochs required to learn the family relations task under each condition, and the optimal
learning rate that produced this performance. Training times for conventional backpropagation are quite long; this is typical for deep multi-layer networks. For comparison, Hinton
reports around 1,500 epochs on this problem when both learning rate and momentum have
been optimized (personal communication). Much faster convergence - though to a far
looser criterion - has recently been observed for online algorithms (O'Reilly, 1996).
Tempering, on the other hand, is seen here to speed up two batch learning methods by almost an order of magnitude. It reduces not only the average training time but also its coefficient of variation, indicating a more reliable optimization process. Note that tempering
makes simple batch learning with momentum run about twice as fast as the delta-bar-delta
algorithm. This is remarkable since delta-bar-delta uses online measurements to continually adapt the learning rate for each individual weight, whereas tempering merely prescales
it based on the network's architecture. We take this as evidence that tempering establishes
appropriate local step sizes upfront that delta-bar-delta must discover empirically.
This suggests that by using tempering to set the initial (equilibrium) learning rates for deltabar-delta, it may be possible to reap the benefits of both prescaling and adaptive step size
control. Indeed Table 2 confirms that the respective speedups due to tempering and deltabar-delta multiply when the two approaches are combined in this fashion. Finally, the addition of error shunting increases learning speed yet further by allowing the global learning
rate to be brought close to the maximum of 7]*
0.1 that we would predict from (18).

=

8 Discussion
In our experiments we have found tempering to dramatically improve speed and reliability
of learning. More network architectures, data sets and learning algorithms will have to be
"tempered" to explore the general applicability and limitations of this approach; we also
hope to extend it to recurrent networks and online learning. Error shunting has proven useful
in facilitating of near-maximal global learning rates for rapid optimization.

Algorithm
Condition
conventional
with tempering
tempering & shunting

batch & momentum
7]=

mean

st.d.

3.10- 3 2438 ? 1153
1.10- 2 339 ? 95.0
4.10- 2 142?27.1

delta-bar-delta
7]=

mean

st.d.

3.10- 4 696? 218
3.10- 2 89.6 ? 11 .8
9.10- 2 61.7?8.1

Table 2: Epochs required to learn the family relations task.

Tempering Backpropagation Networks: Not All Weights Are Created Equal

569

Although other schemes may speed up backpropagation by comparable amounts, our approach has some unique advantages. It is computationally cheap to implement: local learning and error attenuation rates are invariant with respect to network weights and activities
and thus need to be recalculated only when the network architecture is changed.
More importantly, even advanced gradient descent methods typically retain the isotropic
weight space assumption that we improve upon; one would therefore expect them to benefit from tempering as much as delta-bar-delta did in the experiments reported here. For
instance, tempering could be used to set non-isotropic model-trust regions for conjugate and
second-order gradient descent algorithms.
Finally, by restricting ourselves to fixed learning rates and attenuation factors for now we
have arrived at a simplified method that is likely to leave room for further improvement.
Possible refinements include taking weight vector size into account when attenuating error
signals, or measuring quantities such as (6 2 ) online instead of relying on invariant upper
bounds. How such adaptive tempering schemes will compare to and interact with existing
techniques for efficient backpropagation learning remains to be explored.

Acknowledgements
We would like to thank Peter Dayan, Rich Zemel and Jenny Orr for being instrumental in
discussions that helped shape this work. Geoff Hinton not only offered invaluable comments, but is the source of both our simulator and benchmark problem. N. Schraudolph
received financial support from the McDonnell-Pew Center for Cognitive Neuroscience in
San Diego, and the Robert Bosch Stiftung GmbH.

References
Amari, S.-1. (1995). Learning and statistical inference. In Arbib, M. A., editor, The Handbook of Brain Theory and Neural Networks, pages 522-526. MIT Press, Cambridge.
Battiti, T. (1992). First- and second-order methods for learning: Between steepest descent
and Newton's method. Neural Computation,4(2):141-166.
Haykin, S. (1994). Neural Networks: A Comprehensive Foundation. Macmillan, New York.
Hinton, G. (1986). Learning distributed representations of concepts. In Proceedings of
the Eighth Annual Conference of the Cognitive Science Society, pages 1-12, Amherst
1986. Lawrence Erlbaum, Hillsdale.
Jacobs, R. (1988). Increased rates of convergence through learning rate adaptation. Neural
Networks,1:295-307.
Krogh, A., Thorbergsson, G., and Hertz, J. A. (1990). A cost function for internal representations. In Touretzky, D. S., editor,Advances in Neural Information Processing Systems, volume 2, pages 733-740, Denver, CO, 1989. Morgan Kaufmann, San Mateo.
LeCun, Y. (1993). Efficient learning & second-order methods. Tutorial given at the NIPS
Conference, Denver, CO.
LeCun, Y., Kanter, I., and Solla, S. A. (1991). Second order properties of error surfaces:
Learning time and generalization. In Lippmann, R. P., Moody, J. E., and Touretzky,
D. S., editors, Advances in Neural Information Processing Systems, volume 3, pages
918-924, Denver, CO, 1990. Morgan Kaufmann, San Mateo.
O'Reilly, R. C. (1996). Biologically plausible error-driven learning using local activation
differences: The generalized recirculation algorithm. Neural Computation, 8.
Plaut, D., Nowlan, S., and Hinton, G. (1986). Experiments on learning by back propagation. Technical Report CMU-CS-86-126, Department of Computer Science, Carnegie
Mellon University, Pittsburgh, PA.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 358-continuous-speech-recognition-by-linked-predictive-neural-networks.pdf

Continuous Speech Recognition by
Linked Predictive Neural Networks

Joe Tebelskis, Alex Waibel, Bojan Petek, and Otto Schmidbauer
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213

Abstract
We present a large vocabulary, continuous speech recognition system based
on Linked Predictive Neural Networks (LPNN's). The system uses neural networks as predictors of speech frames, yielding distortion measures
which are used by the One Stage DTW algorithm to perform continuous
speech recognition. The system, already deployed in a Speech to Speech
Translation system, currently achieves 95%, 58%, and 39% word accuracy
on tasks with perplexity 5, 111, and 402 respectively, outperforming several simple HMMs that we tested. We also found that the accuracy and
speed of the LPNN can be slightly improved by the judicious use of hidden
control inputs. We conclude by discussing the strengths and weaknesses
of the predictive approach.

1

INTRODUCTION

Neural networks are proving to be useful for difficult tasks such as speech recognition, because they can easily be trained to compute smooth, nonlinear, nonparametric functions from any input space to output space. In speech recognition, the
function most often computed by networks is classification, in which spectral frames
are mapped into a finite set of classes, such as phonemes. In theory, classification
networks approximate the optimal Bayesian discriminant function [1], and in practice they have yielded very high accuracy [2, 3, 4]. However, integrating a phoneme
classifier into a speech recognition system is nontrivial, since classification decisions
tend to be binary, and binary phoneme-level errors tend to confound word-level
hypotheses. To circumvent this problem, neural network training must be carefully
integrated into word level training [1, 5]. An alternative function which can be com-

199

200

Tebelskis, Waibel, Petek, and Schmidbauer
puted by networks is prediction, where spectral frames are mapped into predicted
spectral frames. This provides a simple way to get non-binary distortion measures,
with straightforward integration into a speech recognition system. Predictive networks have been used successfully for small vocabulary [6, 7] and large vocabulary
[8, 9] speech recognition systems. In this paper we describe our prediction-based
LPNN system [9], which performs large vocabulary continuous speech recognition,
and which has already been deployed within a Speech to Speech Translation system
[10]. We present our experimental results, and discuss the strengths and weaknesses
of the predictive approach.

2

LINKED PREDICTIVE NEURAL NETWORKS

The LPNN system is based on canonical phoneme models, which can be logically
concatenated in any order (using a "linkage pattern") to create templates for different words; this makes the LPNN suitable for large vocabulary recognition.
Each canonical phoneme is modeled by a short sequence of neural networks. The
number of nets in the sequence, N >= 1, corresponds to the granularity of the
phoneme model. These phone modeling networks are nonlinear, multilayered, feedforward, and "predictive" in the sense that, given a short section of speech, the
networks are required to extrapolate the raw speech signal, rather than to classify
it. Thus, each predictive network produces a time-varying model of the speech
signal which will be accurate in regions corresponding to the phoneme for which
that network has been trained, but inaccurate in other regions (which are better
modeled by other networks). Phonemes are thus "recognized" indirectly, by virtue
of the relative accuracies of the different predictive networks in various sections of
speech. Note, however, that phonemes are not classified at the frame level. Instead,
continuous scores (prediction errors) are accumulated for various word candidates,
and a decision is made only at the word level, where it is finally appropriate.

2.1

TRAINING AND TESTING ALGORITHMS

The purpose of the training procedure is both (a) to train the networks to become
better predictors, and (b) to cause the networks to specialize on different phonemes.
Given a known training utterance, the training procedure consists of three steps:
1. Forward Pass: All the networks make their predictions across the speech sample, and we compute the Euclidean distance matrix of prediction errors between
predicted and actual speech frames. (See Figure 1.)
2. Alignment Step: We compute the optimal time-alignment path between the
input speech and corresponding predictor nets, using Dynamic Time Warping.
3. Backward Pass: Prediction error is backpropagated into the networks according
to the segmentation given by the alignment path. (See Figure 2.)
Hence backpropagation causes the nets to become better predictors, and the alignment path induces specialization of the networks for different phonemes.
Testing is performed using the One Stage algorithm [11], which is a classical extension of the Dynamic Time Warping algorithm for continuous speech.

Continuous Speech Recognition by Linked ftedictive Neural Networks

?

?

???

?

- - - - prediction errors

..--+--+-----if---~

......?................ . ...

+--+---+-+--+-----4:?......?....................
t--t---t--+--t---t--~

.... . .?......... . . . ........

CtJ

?
phoneme "a"
predictors

phoneme "b"
predictors

Figure 1: The forward pass during training. Canonical phonemes are modeled by
sequences of N predictive networks, shown as triangles (here N=3). Words are
represented by "linkage patterns" over these canonical phoneme models (shown in
the area above the triangles), according to the phonetic spelling of the words. Here
we are training on the word "ABA". In the forward pass, prediction errors (shown
as black circles) are computed for all predictors, for each frame of the input speech.
As these prediction errors are routed through the linkage pattern, they fill a distance
matrix (upper right).

?

+--+---1--1---1--+---1-: : :: : :A(igr)~~~(p.~th?

? ....

CtJ

? \~~-\tI------+-

~;:;:1tt:;:;:;:;:::;::;:;::;::;::;:;:;:;:;::;:::;::;J;;tt

Figure 2: The backward pass during training. After the DTW alignment path has
been computed, error is backpropagated into the various predictors responsible for
each point along the alignment path. The back propagated error signal at each such
point is the vector difference between the predicted and actual frame. This teaches
the networks to become better predictors, and also causes the networks to specialize
on different phonemes.

201

202

Tebelskis, Waibel, Petek, and Schmidbauer

3

RECOGNITION EXPERIMENTS

We have evaluated the LPNN system on a database of continuous speech recorded
at CMU. The database consists of 204 English sentences using a vocabulary of 402
words, comprising 12 dialogs in the domain of conference registration. Training
and testing versions of this database were recorded in a quiet office by multiple
speakers for speaker-dependent experiments. Recordings were digitized at a sampling rate of 16 KHz. A Hamming window and an FFT were computed, to produce
16 melscale spectral coefficients every 10 msec. In our experiments we used 40
context-independent phoneme models (including one for silence), each of which had
a 6-state phoneme topology similar to the one used in the SPICOS system [12].

f~ lIE
(1/
lit Z

tV>o tt..1s; I?UD IS THIS Il? IFfll?
"*,,,theOl ..d p/l<QM.;

,

iii EH l

ClII'EREKI

111 IH 5

(seero

111 1ft

= 17. 61
III f 1ft 5

KRAHfRIftNS

f ER

,

I'

(1/

,

. , ,; n .....

.. 11 ? ? , 11 . . . . . " " , ?? , . , . . . . . . ,., . . . . . . . .. . . . . . . . . , . . . . . , . . . . . . . . .

:

. . . " ? ? ? 1 . 11' . . .. . -. " . . . . 1111 . .. .. " . .. " . . . . 11 . . . . . . . . . " ????????

''':::::::::~:: 1I11 ............ lh ?? , . . . . . lllIltlll ? ? ??? .. III ........ ?' .. . . . . . . ..

, . ? ? , llll . . . . . . . . . . . . . . ,IIIIIIIIU .. ' ? ? "..

: ::~ml;i!E::!~:l;::~:!i;~: I""
U "." U lu, ........ ,III.,IIII.III . . . . . . . . . . . . . . . . . . . . . . . . . . . ? . ?...
"ll ........... " .. .. ... ,11 . . . . . . " " ?? ? ? h' U ??? ,I.'??, ,111""
.... , ?? U .... , . . . . I.II ............ UI ..... , ................ 111 . . . . . . . . . . . ' .,.111 .. 11 ?? 11. ,? . 111"'?? 111_1111,,0, ?? 1111111'1'" '1" " " "
" " " ' " , .? '".IIIII .... IIII ... IIIIUIIIlIlIl"I . . . . . . . . . . ." ?? ? ?? IU ... 'IIUIIlIlIlIl ......II'h .. . ' , ..................... IIIU' ..... , ? ? ,II'." I.

,It.......... ,?," "III"hl. . ? "."""
" 1' "
'11." ??
', ......
'11'
......
1111
????
,.'.'10?,
.????
1......11 ..... . """'11'" " " .? 00.""
., ..... 11 ........ ".0 , ............ ,.

''';;:;;;:;:~:::: ,1I1I1I1I11t1l11l"11I.1I""","1I1O ? ???
!!~

' " .... UIlIlI ...... "

' ' ' .... 'I . . . . ? . . . . . . . . . . . . . . . . ,., " , .. II . . . . . ,? " ..
. ' " ???? 1111111 .. . 11"11 . . . . . . . . . . . , . . . . . . . . . . . . . ' . .. .. '.1' . . . . . . . , ?

'

.

IUIlIl ................? .,

""''' ' 1

I

.

I

... , ??? ,.

'01' '',,,
"'''."
"""11

Figure 3: Actual and predicted spectrograms.
Figure 3 shows the result of testing the LPNN system on a typical sentence. The
top portion is the actual spectrogram for this utterance; the bottom portion shows
the frame-by-frame predictions made by the networks specified by each point along
the optimal alignment path. The similarity of these two spectrograms indicates that
the hypothesis forms a good acoustic model of the unknown utterance (in fact the
hypothesis was correct in this case). In our speaker-dependent experiments using
two males speakers, our system averaged 95%, 58%, and 39% word accuracy on
tasks with perplexity 5, 111, and 402 respectively.
In order to confirm that the predictive networks were making a positive contribution to the overall system, we performed a set of comparisons between the LPNN
and several pure HMM systems. When we replaced each predictive network by a
univariate Gaussian whose mean and variance were determined analytically from
the labeled training data, the resulting HMM achieved 44% word accuracy, compared to 60% achieved by the LPNN under the same conditions (single speaker,
perplexity 111). When we also provided the HMM with delta coefficients (which
were not directly available to the LPNN), it achieved 55%. Thus the LPNN was
outperforming each of these simple HMMs.

Continuous Speech Recognition by Linked R-edictive Neural Networks

4

HIDDEN CONTROL EXPERIMENTS

In another series of experiments, we varied the LPNN architecture by introducing
hidden control inputs, as proposed by Levin [7]. The idea, illustrated in Figure 4,
is that a sequence of independent networks is replaced by a single network which is
modulated by an equivalent number of "hidden control" input bits that distinguish
the state.

Sequence of
Predictive Networks

Hidden Control
Network

Figure 4: A sequence of networks corresponds to a single Hidden Control network.
A theoretical advantage of hidden control architectures is that they reduce the
number offree parameters in the system. As the number of networks is reduced, each
one is exposed to more training data, and - up to a certain point - generalization
may improve. The system can also run faster, since partial results of redundant
forward pass computations can be saved. (Notice, however, that the total number
of forward passes is unchanged.) Finally, the savings in memory can be significant.
In our experiments, we found that by replacing 2-state phoneme models by equivalent Hidden Control networks, recognition accuracy improved slightly and the system ran much faster. On the other hand, when we replaced all of the phonemic
networks in the entire system by a single Hidden Control network (whose hidden
control inputs represented the phoneme as well as its state), recognition accuracy
degraded significantly. Hence, hidden control may be useful, but only if it is used
judiciously.

5

CURRENT LIMITATIONS OF PREDICTIVE NETS

While the LPNN system is good at modeling the acoustics of speech, it presently
tends to suffer from poor discrimination. In other words, for a given segment
of speech, all of the phoneme models tend to make similarly good predictions,
rendering all phoneme models fairly confusable. For example, Figure 5 shows an
actual spectrogram and the frame-by-frame predictions made by the /eh/ model
and the /z/ model. Disappointingly, both models are fairly accurate predictors for
the entire utterance.
This problem arises because each predictor receives training in only a small region
of input acoustic space (i.e., those frames corresponding to that phoneme). Consequently, when a predictor is shown any other input frames, it will compute an

203

204

Tebelskis, Waibel, Petek, and Schmidbauer

"1

., 0,

1~";";":7';l' lti1"rl
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . " .. . . .. ' ?? " . . . . . . . . . . . . . . . . . . . . . . 11 . . . . . . . . . . . . . . . . . . . . . . 1' . . . . . . . . . . ? . . . . 1111" ?? ? . . . . . . . . . . ? . . . " ,. ? . . . . . . . . . " . . . . . . . . . .. . . . . . . " ?? ? ? " , . . . . . . . . . . ..
.... II .. ' .... ' ? ? " . . ......... "nu ..... . .. . ... IIII . . . . . 'U .. . , ' . . . . . .UI ... I I U l l l l l l ' ' ' IIU ............ . ... , . . . u ' I , ...... II ... .. ,II . .... U. l lh . . ... ' .... " ..... .... . . .. . . ... . 1 .... ..

/eh/

, ? ??? 1111 ... "1111 ??? ,1.111111 .. " ... , . . ... ................. 111 ? ? 111" ... 11 . 111 ??? , ..... . ..... .. ... 111 .............. 1' .. '1111111' .. .... " .111I111".,t" ? .... , . . . . .. . ?.' ." ......... ..
, ? ?? 1111 . . . . 11"1 . . . . . . . . . 1,.'1' . ... .... ..... . .... "111 ? ? " . . . . . . . . . ' ... ' " " . . . . . . . . . . . '11 ?? ? ,'1 ? ? " " ... 111 ...... . ... '1 11' ... 1" . .. . . ... 11 11 . . . . . , . '? .. .. ? . .. '1 ?? 1. '? . . ...... ... .. 1
? ? ? 1 .............................. 1 ?? 1........ 1. . . . 1111. . . . . . . . . . 1..... , . . . . . . . . . . . . .111 ...... 11 . . . . . . . . . . . 111111 . . . . . . ,1 ?? , . . . . .'11 . . . . . . . . 1.' ???????? ' .1 ' ?? ? .... 1. . . . . ..
....... , ..... ul,." ???? 'I III'II . ....... ", ..... " ........... ...
1.. 1.11' ... 111??? " .... , ..... 1... . .. ' .1 1' . . . . " ....... ' ? ??? " " ?? ? ' ...... , ?? , ,, ? ?

II'".II '",., .........." .......

?? '111 . . . . . .. ... 111' ..'1 .. " ' .. " 1 ? ? " ' ..... 1'?" .. ? ' ' ' .. ' .. ' ' 1 ... 1 .. 111111 . . . . 1 . . . . . . . . . 11' .... 11 ?? ' ' ' ??' . ' ' '' ''"1'1 ..... ' . .. 1111' . . . . . . . . . . . . . . " " . ' . " ............ . . , ? ? ? , , " " , .
11' . . . . . . . . . . . 1111'111 ?? ' .. " .... 11' .... 11111111111 . . . . . . . . . . . . . . . . . . . 111 . . . . " . . . . . . '. 1.11' ... 1'''. . . . ' .... 1111 11 11 111111 ...... 11 . . . . . . . . . . . . . . .......... , ..... . . ... 1 . ' ... ..

? 1 . . . . . .. . . . . . . . . . . . . . . . . . . 111 11 .. 11111 ... 11.' ? ??? , . . . ." ?? ? "1 .... 1 . . . . . . . IIH ?? ? ? ' .. , ............. IIII'I' . . . . . . . . , ???? ? "., . . . . . I1 ......... . , . . . . . . . . . . ... .. '. 1.1 ??? "
................................... , ..... ," ?? "'1111 ?? 11111"111 ..... 1 .. '".11". . . . . . . 1..... 1 ..... 1.. 111'"11 ?? "1 . . . . . 1 ??? " ?? " . . . . . . . ., ....... , " "',I., IIf,.,. " .I."" ? ??

?

:: :~~-::::IC:=:~~::~~!:::::::::::::::::::::~::::.~!::::~::~:::~~~:~ .:=::::::::::::~~:::::~::::.-:::~~::.:::::::::~~:;u::::: ::::: ::~:~::::: : :: :::!:: : :~:

'."N"'"

.

..... _ . - - . . .................................. , .......... _ . , ...... , ....... ,.,? . . . . . . . 1.....? .......- . . . .. ?? ......... ' ..... ,111 ..... ..
........ 11 ........ , .........................,1111 ?? " ...... , ........................ , ...... , .......... , ................... " . . . . 11.11 ??????
??????? , ...........................................................1......................................................, .......... , ......... I'II ??.!

r"TT~!"."""'""'"TmmTmTmTnm""'TnT""'IJII1""'rrm~!!!'I1TT!".""rrmrrm""'...,.,nT11""';nnIlPl_rm""'m;1,m,mn'""rrmTm?nnTTi1TtTTT11TTTm .mml

,It."., '''' ,., ?.

I1 .. ' ??
?? ??111" ............

/z/

?? t.'1 ?

I . . . . . . . . , ?? ? ???? ,,, . . . . . . . . . . . . . . , . . . . . . . . . . 1.1 ............. . '1111, ...... . ...... , ., ??? , .. ......... . .. , .....
u . , .... , ".' .. '11' . . . , . . . . . . . . . . . . . . . . 11.111
.. . 1 .... 11 ..... .. " " ,?" ? ? .. 1.. ............ .. '.11 . . . . . . . '",, . . . . . . . '.11'".11 ?? " .. , .. ,,' ?? 1 ... 1111' ... 111.1111.1111" ....... , .... , . ............ '00 . .. .. . .. , . , ... , ........... , . . . . . . ,
??? I .. ' .. . .
I.II ....
"III III I ? ? HIIIIII' ., 111 ?? ' 11 11111011' ....." ...
1111."".' .... ' . ....... " ........ 1.11111 .......... ".1 ?? 1 . . . . . . . .
111 1 11.11111" ...... . , ... . ... ,. ? .. .. .. , . . . . ..
? , , ......... "1 '1. 11111111 1' ..... . .. . , "" .. , ' .... U ' .. I IIII " ....... , 11 1 ' .. 111 . . . . . '11 .. " .... . , , . .. ... , .. , ........ , . .. .. '"11 ' ?? ? ? ? , " .1111111'1 11'" " ' " ... .. , ????? ' . """111"
. , . IUff'.""I".'.IIIIIIIIIII'" . " , . " ? ? 11, .... , ... ..... ' .. 1'1 ?? ' .. . 11".' ............ , ... ? ' ? ? 1 ? ? ?? 11 .... ",,, ,, ,,,,," ... ,11' "' , . . . . . . . . . . . , ... . " .. ....... , .. ,
.. " '1 . . . . . . . 111111111111 11 .' ..... ' . ' ... .. ,.11 ' ? ? ? ? ' " ' ... ' .1.1I1t1l.1 ..... ' .. ' " ' . . . . . . . . . 1 ?? , ? ? " ".tt'"IIII , I" .. . ... 111111 .. . .. '
??
II I H ?? . ?
.. ? 1 . . . . . . . . . . . . . . . . . , . . . . . . ' ?? , .. ,? , .. ... , 11 ... .. , ???? 'II . . . . . . . ' ?? , .. ... . . . . . . . . . . . . . . . . . . 1.. " . . . . . . . ..... .. 1" .... "1 .. 1'
.... . . . . . . 1..... .. '" , ,11 " .... . .
,." . . . . . . . . . . . . . . . . . . . . . ,," ....... 11 1''''' ......... 11 ......'1' . . ..... ............... ' . . ... 1 . . . . . . . 11' ..... ' .......... " , ... , ............ .... , .11101 ...... ,

. . . . . . . . 1 ?? ""

I..... ' ........... ,,' ......

1........ 1'"'''.' .... ", .,?? .' .??111 ''.' .' ... .10 ... .. . '. ,.... . , ............. .
11"' ??? ?11' ".1, ........... , ........ , ..... ?????

'.I ....

?. , " " . . . . . . . . . . . . . . . I . . . . U IIl' ... . .. ... .. , .. . ... .. , .. . , . . . 1' . 1. ' ?? , . .. .. ' " ' . . . . . . . . . 11" . . .. I .............. . .. . .
' " ? ? " . . . . . . . . . . . . . . . . . . . . . f ... ? ' . ...... .

11 ....... ' " '1".', '11 "" ?? , .. I II' .......... "" . . ????? 11'1"

" .'11., ?? 1 ? ? . , ? ? ? , . . . . . . . ... .. , ,"

" " 01 ' 11" "

"" " " ??? 1 1."110 ?? '? ? . ? . . . . . . . . 111 .... . . ? ? ? . . . . . ...... . , ., ... . , ... .

? , ? ? , . . . . . . . . . . . . . . . . . . . . . . . . 1"" ? ? ? , .. .... .. .. 1 ?? , . . . . . . . 11" ?? ,0, ? ? , I................. . ', .. ,' ...... "00 , . .. .. ....... 1" ... "., . . ............... . . ,. , , ". 11 tu ? ? , . . . . ... ' ' " ??

? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . "1 . . . . . . . 11.1." . . . . . . . . . 1. , ... ..................... . . 0011 ......... 11' ?? , . . . . . . . . . 1 ..... ' .' . . . . . . . . . . . " ...... . , , "III.'h, ... . . " ? ? 00 ? ?
, . . . . . . . . . . . . . I.H ?? ? ??? III . . . . . . . . . . . . I ....... " . . . . . . II ... 'I ?? ? ?? ,' . . . . . . . . . . . . . . . . . 1 .. . ........... 1" " '" ? ?? 1 . . . . . . . . . . . . .. . 1............ 11 ??? , '.' " ........ 1'.'. ', " ' "
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 . . ." ........ 111 ........... , ........ ' "' .............. 11' ???? , . . . . . . . .. . ,' . . . . . . " . . . . ,.11 ?? I.,.I .. '~~h ??? ~.!~ .. ,

Figure 5: Actual spectrogram, and corresponding predictions by the /eh/ and /z/
phoneme models.

undefined output, which may overlap with the outputs of other predictors. In other
words, the predictors are currently only trained on positive instances, because it is
not obvious what predictive output target is meaningful for negative instances; and
this leads to problematic "undefined regions" for the predictors. Clearly some type
of discriminatory training technique should be introduced, to yield better performance in prediction based recognizers.

6

CONCLUSION

We have studied the performance of Linked Predictive Neural Networks for large vocabulary, continuous speech recognition. Using a 6-state phoneme topology, without
duration modeling or other optimizations, the LPNN achieved an average of 95%,
58%, and 39% accuracy on tasks with perplexity 5, 111, and 402, respectively. This
was better than the performance of several simple HMMs that we tested . Further
experiments revealed that the accuracy and speed of the LPNN can be slightly
improved by the judicious use of hidden control inputs.
The main advantages of predictive networks are that they produce non-binary distortion measures in a simple and elegant way, and that by virtue of their nonlinearity
they can model the dynamic properties of speech (e.g., curvature) better than linear predictive models [13]. Their main current weakness is that they have poor
discrimination, since their strictly positive training causes them all to make confusably accurate predictions in any context. Future research should concentrate
on improving the discriminatory power of the LPNN, by such techniques as corrective training, explicit context dependent phoneme modeling, and function word
modeling.

Continuous Speech Recognition by Linked ltedictive Neural Networks

Acknowledgements

The authors gratefully acknowledge the support of DARPA, the National Science
Foundation, ATR Interpreting Telephony Research Laboratories, and NEC Corporation. B. Petek also acknowledges support from the University of Ljubljana and
the Research Council of Slovenia. O. Schmidbauer acknowledges support from his
employer, Siemens AG, Germany.

References
[1] H. Bourlard and C. J. Wellekens. Links Between Markov Models and Multilayer
Perceptrons. Pattern Analysis and Machine Intelligence, 12:12, December 1990.
[2] A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. Lang. Phoneme Recognition
Using Time-Delay Neural Networks. IEEE Transactions on Acoustics, Speech, and
Signal Processing, March 1989.
[3] M. Miyatake, H. Sawai, and K. Shikano. Integrated Training for Spotting Japanese
Phonemes Using Large Phonemic Time-Delay Neural Networks. In Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing, April 1990.
[4] E. McDermott and S. Katagiri. Shift-Invariant, Multi-Category Phoneme Recognition
using Kohonen's LVQ2. In Proc. IEEE International Conference on Acoustics, Speech,
and Signal Processing, May 1989.

[5] P. Haffner, M. Franzini, and A. Waibel. Integrating Time Alignment and Connectionist Networks for High Performance Continuous Speech Recognition. In Proc. IEEE
International Conference on Acoustics, Speech, and Signal Processing, May 1991.

[6] K. Iso and T. Watanabe. Speaker-Independent Word Recognition Using a Neural
Prediction Model. In Proc. IEEE International Conference on Acoustics, Speech, and
Signal Processing, April 1990.

[7] E. Levin. Speech Recognition Using Hidden Control Neural Network Architecture.
In Proc. IEEE International Conference on Acoustics, Speech and Signal Processing,
April 1990.
[8] J. Tebelskis and A. Waibel. Large Vocabulary Recognition Using Linked Predictive
Neural Networks. In Proc. IEEE International Conference on Acoustics, Speech, and
Signal Processing, April 1990.
[9] J. Tebelskis, A. Waibel, B. Petek, and O. Schmidbauer. Continuous Speech Recognition Using Linked Predictive Neural Networks. In Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing, May 1991.
[10] A. Waibel, A. Jain, A. McNair, H. Saito, A. Hauptmann, and J. Tebelskis. A Speechto-Speech Translation System Using Connectionist and Symbolic Processing Strategies. In Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing, May 1991.
[11] H. Ney. The Use of a One-Stage Dynamic Programming Algorithm for Connected
Word Recognition. IEEE Transactions on Acoustics, Speech, and Signal Processing,
32:2, April 1984.
[12] H. Ney, A. Noll. Phoneme Modeling Using Continuous Mixture Densities. In Proc.
IEEE International Conference on Acoustics, Speech, and Signal Processing, April
1988.
[13] N. Tishby. A Dynamic Systems Approach to Speech Processing. In Proc. IEEE
International Conference on Acoustics, Speech, and Signal Processing, April 1990.

205


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 2137-relative-density-nets-a-new-way-to-combine-backpropagation-with-hmms.pdf

Relative Density Nets: A New Way to
Combine Backpropagation with HMM's

Andrew D. Brown
Department of Computer Science
University of Toronto
Toronto, Canada M5S 3G4
andy@cs.utoronto.ca

Geoffrey E. Hinton
Gatsby Unit, UCL
London, UK WCIN 3AR
hinton@gatsby.ucl.ac.uk

Abstract
Logistic units in the first hidden layer of a feedforward neural network compute the relative probability of a data point under two
Gaussians. This leads us to consider substituting other density
models. We present an architecture for performing discriminative
learning of Hidden Markov Models using a network of many small
HMM's. Experiments on speech data show it to be superior to the
standard method of discriminatively training HMM's.

1

Introduction

A standard way of performing classification using a generative model is to divide the
training cases into their respective classes and t hen train a set of class conditional
models. This unsupervised approach to classification is appealing for two reasons. It
is possible to reduce overfitting, because t he model learns the class-conditional input
densities P(xlc) rather t han the input -conditional class probabilities P(clx). Also,
provided that the model density is a good match to the underlying data density
then the decision provided by a probabilistic model is Bayes optimal. The problem
with this unsupervised approach to using probabilistic models for classification is
that, for reasons of computational efficiency and analytical convenience, very simple
generative models are typically used and the optimality of the procedure no longer
holds. For this reason it is usually advantageous to train a classifier discriminatively.
In this paper we will look specifically at the problem of learning HMM 's for classifying speech sequences. It is an application area where the assumption that the HMM
is the correct generative model for the data is inaccurate and discriminative methods
of training have been successful. The first section will give an overview of current
methods of discriminatively training HMM classifiers. We will then introduce a new
type of multi-layer backpropagation network which takes better advantage of the
HMM 's for discrimination. Finally, we present some simulations comparing the two
methods.

19 ' S1

c1

1

1

1

="

[tn] [tn][t n] HMM 's

\V
Sequence

Figure 1: An Alphanet with one HMM per class. Each computes a score for the
sequence and this feeds into a softmax output layer.

2

Alphanets and Discriminative Learning

The unsupervised way of using an HMM for classifying a collection of sequences is to
use the Baum-Welch algorithm [1] to fit one HMM per class. Then new sequences
are classified by computing the probability of a sequence under each model and
assigning it to the one with the highest probability. Speech recognition is one of the
commonest applications of HMM 's, but unfortunately an HMM is a poor model of
the speech production process. For this reason speech researchers have looked at the
possibility of improving the performance of an HMM classifier by using information
from negative examples - examples drawn from classes other than the one which
the HMM was meant to model. One way of doing this is to compute the mutual
information between the class label and the data under the HMM density, and
maximize that objective function [2].
It was later shown that this procedure could be viewed as a type of neural network
(see Figure 1) in which the inputs to the network are the log-probability scores
C(Xl:TIH) of the sequence under hidden Markov model H [3]. In such a model
there is one HMM per class, and the output is a softmax non-linearity:

(1)
Training this model by maximizing the log probability of correct classification leads
to a classifier which will perform better than an equivalent HMM model trained
solely in a unsupervised manner. Such an architecture has been termed an "AIphanet" because it may be implemented as a recurrent neural network which mimics
the forward pass of the forward-backward algorithm.l

3

Backpropagation Networks as Density Comparators

A multi-layer feedforward network is usually thought of as a flexible non-linear
regression model, but if it uses the logistic function non-linearity in the hidden
layer, there is an interesting interpretation of the operation performed by each
hidden unit. Given a mixture of two Gaussians where we know the component
priors P(9) and the component densities P(xl9) then the posterior probability that
Gaussian, 90 , generated an observation x , is a logistic function whose argument is
the negative log-odds of the two classes [4] . This can clearly be seen by rearranging
lThe results of the forward pass are the probabilities of the hidden states conditioned
on the past observations, or "alphas" in standard HMM terminology.

the expression for the posterior:

P(xI9o)P(Qo)
P(xI9o)P(Qo) + P(xI9d P (Qd

P(Qolx)

1

1 + exp {-log

P(x IQo) P(x lQd

log

(2)
P(Qo) }
P(Ql)

If the class conditional densities in question are multivariate Gaussians

P(xI9k) =

121f~1-~ exp {-~(x -

with equal covariance matrices,
written in this familiar form:

~,

Pk)T ~-l(X - Pk)}

(3)

then the posterior class probability may be
1

P(Qo Ix) = -l-+-e-xp-{-=---(:-x=Tw-+-b---:-)

(4)

where,
w

(5)

b

(6)

Thus, the multi-layer perceptron can be viewed as computing pairwise posteriors
between Gaussians in the input space, and then combining these in the output layer
to compute a decision.

4

A New Kind of Discriminative Net

This view of a feedforward network suggests variations in which other kinds of
density models are used in place of Gaussians in the input space. In particular,
instead of performing pairwise comparisons between Gaussians, the units in the
first hidden layer can perform pairwise comparisons between the densities of an
input sequence under M different HMM's. For a given sequence the log-probability
of a sequence under each HMM is computed and the difference in log-probability
is used as input to the logistic hidden unit. 2 This is equivalent to computing the
posterior responsibilities of a mixture of two HMM's with equal prior probabilities.
In order to maximally leverage the information captured by the HMM's we use (~)
hidden units so that all possible pairs are included. The output of a hidden unit h
is given by

(7)
where we have used (mn) as an index over the set, (~) , of all unordered pairs of
the HMM's. The results of this hidden layer computation are then combined using
a fully connected layer of free weights, W, and finally passed through a soft max
function to make the final decision.
ak

=

L

W(m ,n)kh(mn)

(8)

(mn) E (~)

(9)
2We take the time averaged log-probability so that the scale of the inputs is independent
of the length of the sequence.

Density
Comparator
Units

Figure 2: A multi-layer density net with HMM's in the input layer. The hidden
layer units perform all pairwise comparisons between the HMM 's.
where we have used u(?) as shorthand for the logistic function, and Pk is the value
of the kth output unit. The resulting architecture is shown in figure 2. Because
each unit in the hidden layer takes as input the difference in log-probability of two
HMM 's, this can be thought of as a fixed layer of weights connecting each hidden
unit to a pair of HMM's with weights of ?l.
In contrast to the Alphanet , which allocates one HMM to model each class, this network does not require a one-to-one alignment between models and classes and it gets
maximum discriminative benefit from the HMM's by comparing all pairs. Another
benefit of this architecture is that it allows us to use more HMM's than there are
classes. The unsupervised approach to training HMM classifiers is problematic because it depends on the assumption that a single HMM is a good model of the data
and, in the case of speech, this is a poor assumption. Training the classifier discriminatively alleviated this drawback and the multi-layer classifier goes even further in
this direction by allowing many HMM's to be used to learn the decision boundaries
between the classes. The intuition here is that many small HMM's can be a far
more efficient way to characterize sequences than one big HMM. When many small
HMM's cooperate to generate sequences, the mutual information between different
parts of generated sequences scales linearly with the number of HMM's and only
logarithmically with the number of hidden nodes in each HMM [5].

5

Derivative Updates for a Relative Density Network

The learning algorithm for an RDN is just the backpropagation algorithm applied
to the network architecture as defined in equations 7,8 and 9. The output layer is
a distribution over class memberships of data point Xl:T, and this is parameterized
as a softmax function. We minimize the cross-entropy loss function:
K

f =

2: tk logpk

(10)

k= l

where Pk is the value of the kth output unit and tk is an indicator variable which is
equal to 1 if k is the true class. Taking derivatives of this expression with respect
to the inputs of the output units yields

of

- = t k - Pk
oak

(11)

O?
o?
Oak
(12)
- , - - - - = (tk - Pk)h(mn)
OW(mn) ,k
oak OW(mn) ,k
The derivative of the output of the (mn)th hidden unit with respect to the output
of ith HMM, ?i, is
oh(mn)
(13)
~ = U(?m - ?n)(l - U(?m - ?n))(bim - bin)
where (bim - bin) is an indicator which equals +1 if i = m, -1 if i = n and zero
otherwise. This derivative can be chained with the the derivatives backpropagated
from the output to the hidden layer.
For the final step of the backpropagation procedure we need the derivative of the
log-likelihood of each HMM with respect to its parameters. In the experiments we
use HMM 's with a single, axis-aligned, Gaussian output density per state. We use
the following notation for the parameters:

?
?
?
?
?

A: aij is the transition probability from state i to state j
II: 7ri is the initial state prior
f./,i: mean vector for state i
Vi: vector of variances for state i
1-l: set of HMM parameters {A , II, f./" v}

We also use the variable St to represent the state of the HMM at time t. We make
use of the property of all latent variable density models that the derivative of the
log-likelihood is equal to the expected derivative of the joint log-likelihood under
the posterior distribution. For an HMM this means that:

O?(Xl:TI1-l)
'"
0
o1-l i
= ~ P(Sl:Tlxl:T' 1-l) o1-l i log P(Xl:T' Sl:TI1-l)

(14)

Sl:T

The joint likelihood of an HMM is:
(logP(Xl:T ' Sl:TI1-l)) =
T

L(b81 ,i)log 7ri

+ LL(b "jb
8

8

,_1 ,i)log aij

+

i,j

t=2

~ ~(b8" i) [-~ ~IOgVi'd ~ ~(Xt'd -

f./,i,d) 2 /Vi,d]

-

+ canst

(15)

where (-) denotes expectations under the posterior distribution and (b 8 , ,i) and
(b 8 , ,jb8 '_1 ,i) are the expected state occupancies and transitions under this distribution. All the necessary expectations are computed by the forward backward algorithm. We could take derivatives with respect to this functional directly, but that would require doing constrained gradient descent on the probInstead, we reparameterize the model using a
abilities and the variances.
softmax basis for probability vectors and an exponential basis for the variance parameters.
This choice of basis allows us to do unconstrained optimization in the new basis.
The new parameters are defined as follows:
. _

a' J -

exp(e;; ?)
(e (a? ) ,

2:

JI

exp

1JI

. _

7r, -

exp(e; ~?)

2:

if

exp

(e i(~?)'

.

_

(v)

V"d - exp(Oi,d )

This results in the following derivatives:

O?(Xl :T 11-l)
oO(a)
'J

T

L
t= 2

[(b 8 , ,jb 8 '_1 ,i) - (b 8 '_1 ,i)aij ]

(16)

8?(Xl:T 11?)
80(7r)

?

8?(Xl:T 11?)
8f..li,d
8?(Xl:T 11?)
80(v)

.,d

(8

S1

,i) -

(17)

1fi

T

l)8 st ,i)(Xt,d -

(18)

f..li ,d)/Vi ,d

t= l

1 T

2"l)8st ,i)

[(Xt ,d - f..li ,d)2/Vi ,d -

IJ

(19)

t= l

When chained with the error signal backpropagated from the output, these derivatives give us the direction in which to move the parameters of each HMM in order
to increase the log probability of the correct classification of the sequence.

6

Experiments

To evaluate the relative merits of the RDN, we compared it against an Alphanet
on a speaker identification task. The data was taken from the CSLU 'Speaker
Recognition' corpus. It consisted of 12 speakers uttering phrases consisting of 6
different sequences of connected digits recorded multiple times (48) over the course
of 12 recording sessions. The data was pre-emphasized and Fourier transformed
in 32ms frames at a frame rate of lOms. It was then filtered using 24 bandpass,
mel-frequency scaled filters. The log magnitude filter response was then used as the
feature vector for the HMM's. This pre-processing reduced the data dimensionality
while retaining its spectral structure.
While mel-cepstral coefficients are typically recommended for use with axis-aligned
Gaussians, they destroy the spectral structure of the data, and we would like to
allow for the possibility that of the many HMM's some of them will specialize on
particular sub-bands of the frequency domain. They can do this by treating the
variance as a measure of the importance of a particular frequency band - using
large variances for unimportant bands, and small ones for bands to which they pay
particular attention.
We compared the RDN with an Alphanet and three other models which were implemented as controls. The first of these was a network with a similar architecture
to the RDN (as shown in figure 2), except that instead of fixed connections of ?1,
the hidden units have a set of adaptable weights to all M of the HMM's. We refer
to this network as a comparative density net (CDN). A second control experiment
used an architecture similar to a CDN without the hidden layer, i.e. there is a single
layer of adaptable weights directly connecting the HMM's with the softmax output
units. We label this architecture a CDN-l. The CDN-l differs from the Alphanet
in that each softmax output unit has adaptable connections to the HMM's and we
can vary the number of HMM's, whereas the Alphanet has just one HMM per class
directly connected to each softmax output unit. Finally, we implemented a version
of a network similar to an Alphanet, but using a mixture of Gaussians as the input density model. The point of this comparison was to see if the HMM actually
achieves a benefit from modelling the temporal aspects of the speaker recognition
task.
In each experiment an RDN constructed out of a set of, M, 4-state HMM's was
compared to the four other networks all matched to have the same number of free
parameters, except for the MoGnet. In the case of the MoGnet, we used the same
number of Gaussian mixture models as HMM's in the Alphanet, each with the
same number of hidden states. Thus, it has fewer parameters, because it is lacking
the transition probabilities of the HMM. We ran the experiment four times with

a)
0.95

~
~

0.9

b)

~

0.95

e

E=:l
~

0.9

e =

0.85

0.85
0.8

0.8

0.75

0.75

8

0.7
0.65

0

0.7

0.6

0.6
0.55
Alphanet

MaGnet

CDN

EJ

0.65

0.55
RDN

B

CDN-1

RDN

Alphanet

Architecture

C)

$

D

~

d)

e

~

0.9

8

*
c
0

CDN-1

~

*0.8

?in

gj
Ci

gj
CiO.5

B
MeG net

Architecture

CDN

U

gO.7
~
~0.6
?in

Alphanet

~

a:

~

~O.8

RDN

CDN

~

a:

0.6

MaGnet

Architecture

CDN-1

8

0.4
0.3
RDN

Alphanet

MeGnet

CDN

CDN-1

Architecture

Figure 3: Results of the experiments for an RDN with (a) 12, (b) 16, (c) 20 and
(d) 24 HMM's.
values of M of 12, 16, 20 and 24. For the Alphanet and MoGnet we varied the
number of states in the HMM's and the Gaussian mixtures, respectively. For the
CDN model we used the same number of 4-state HMM's as the RDN and varied
the number of units in the hidden layer of the network. Since the CDN-1 network
has no hidden units, we used the same number of HMM's as the RDN and varied
the number of states in the HMM. The experiments were repeated 10 times with
different training-test set splits. All the models were trained using 90 iterations of
a conjugate gradient optimization procedure [6] .

7

Results

The boxplot in figure 3 shows the results of the classification performance on the
10 runs in each of the 4 experiments. Comparing the Alphanet and the RDN we
see that the RDN consistently outperforms the Alphanet. In all four experiments
the difference in their performance under a paired t-test was significant at the level
p < 0.01. This indicates that given a classification network with a fixed number of
parameters, there is an advantage to using many small HMM 's and using all the
pairwise information about an observed sequence, as opposed to using a network
with a single large HMM per class.
In the third experiment involving the MoGnet we see that its performance is comparable to that of the Alphanet. This suggests that the HMM's ability to model the
temporal structure of the data is not really necessary for the speaker classification
task as we have set it Up.3 Nevertheless, the performance of both the Alphanet and
3If we had done text-dependent speaker identification, instead of multiple digit phrases

the MoGnet is less than the RDN.
Unfortunately the CDN and CDN-l networks perform much worse than we expected. While we expected these models to perform similarly to the RDN, it seems
that the optimization procedure takes much longer with these models. This is probably because the small initial weights from the HMM's to the next layer severely
attenuate the backpropagated error derivatives that are used to train the HMM's.
As a result the CDN networks do not converge properly in the time allowed.

8

Conclusions

We have introduced relative density networks, and shown that this method of discriminatively learning many small density models in place of a single density model
per class has benefits in classification performance. In addition, there may be a
small speed benefit to using many smaller HMM 's compared to a few big ones.
Computing the probability of a sequence under an HMM is order O(TK 2 ), where T
is the length of the sequence and K is the number of hidden states in the network.
Thus, smaller HMM 's can be evaluated faster. However, this is somewhat counterbalanced by the quadratic growth in the size of the hidden layer as M increases.
Acknowledgments
We would like to thank John Bridle, Chris Williams, Radford Neal, Sam Roweis ,
Zoubin Ghahramani, and the anonymous reviewers for helpful comments.

References
[1] L. E. Baum, T. Petrie, G. Soules, and N. Weiss, "A maximization technique
occurring in the statistical analysis of probabilistic functions of Markov chains,"
The Annals of Mathematical Statistics, vol. 41, no. 1, pp. 164-171, 1970.

[2] 1. R. Bahl, P. F. Brown, P. V. de Souza, and R. 1. Mercer, "Maximum mutual information of hidden Markov model parameters for speech recognition,"
in Proceeding of the IEEE International Conference on Acoustics, Speech and
Signal Processing, pp. 49- 53, 1986.
[3] J. Bridle, "Training stochastic model recognition algorithms as networks can
lead to maximum mutual information estimation of parameters," in Advances in
Neural Information Processing Systems (D. Touretzky, ed.), vol. 2, (San Mateo,
CA), pp. 211- 217, Morgan Kaufmann, 1990.
[4] M. I. Jordan, "Why the logistic function? A tutorial discussion on probabilities
and neural networks," Tech. Rep. Computational Cognitive Science, Technical
Report 9503, Massachusetts Institute of Technology, August 1995.
[5] A. D. Brown and G. E. Hinton, "Products of hidden Markov models," in Proceedings of Artificial Intelligence and Statistics 2001 (T. Jaakkola and T. Richardson, eds.), pp. 3- 11, Morgan Kaufmann, 2001.
[6] C. E. Rasmussen, Evaluation of Gaussian Processes and other Methods for NonLinear Regression. PhD thesis, University of Toronto, 1996. Matlab conjugate
gradient code available from http ://www .gatsby.ucl.ac.uk/~edward/code/.

then this might have made a difference.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 1820-combining-ica-and-top-down-attention-for-robust-speech-recognition.pdf

Combining ICA and top-down attention
for robust speech recognition

Un-Min Bae and Soo-Young Lee

Department of Electrical Engineering and Computer Science
and Brain Science Research Center
Korea Advanced Institute of Science and Technology
373-1 Kusong-dong, Yusong-gu, Taejon, 305-701, Korea
bum@neuron.kaist.ac.kr, sylee@ee.kaist.ac.kr

Abstract
We present an algorithm which compensates for the mismatches
between characteristics of real-world problems and assumptions of
independent component analysis algorithm. To provide additional
information to the ICA network, we incorporate top-down selective attention. An MLP classifier is added to the separated signal
channel and the error of the classifier is backpropagated to the
ICA network. This backpropagation process results in estimation
of expected ICA output signal for the top-down attention. Then,
the unmixing matrix is retrained according to a new cost function
representing the backpropagated error as well as independence. It
modifies the density of recovered signals to the density appropriate
for classification. For noisy speech signal recorded in real environments, the algorithm improved the recognition performance and
showed robustness against parametric changes.

1

Introduction

Independent Component Analysis (ICA) is a method for blind signal separation.
ICA linearly transforms data to be statistically as independent from each other as
possible [1,2,5]. ICA depends on several assumptions such as linear mixing and
source independence which may not be satisfied in many real-world applications.
In order to apply ICA to most real-world problems, it is necessary either to release
of all assumptions or to compensate for the mismatches with another method.
In this paper, we present a complementary approach to compensate for the mismatches. The top-down selective attention from a classifier to the ICA network
provides additional information of the signal-mixing environment. A new cost function is defined to retrain the unmixing matrix of the ICA network considering the
propagated information. Under a stationary mixing environment, the averaged
adaptation by iterative feedback operations can adjust the feature space to be more
helpful to classification performance. This process can be regarded as a selective
attention model in which input patterns are adapted according to top-down infor-

mation. The proposed algorithm was applied to noisy speech recognition in real
environments and showed the effectiveness of the feedback operations.

2
2.1

The proposed algorithm
Feedback operations based on selective attention

As previously mentioned, ICA supposes several assumptions. For example, one
assumption is a linearly mixing condition, but in general, there is inevitable nonlinearity of microphones to record input signals. Such mismatches between the
assumptions of ICA and real mixing conditions cause unsuccessful separation of
sources. To overcome this problem, a method to supply valuable information to
the rcA network was proposed. In the learning phase of ICA, the unmixing matrix
is subject to the signal-mixing matrix, not the input patterns. Under stationary
mixing environment where the mixing matrix is fixed, iteratively providing additional information of the mixing matrix can contribute to improving blind signal
separation performance. The algorithm performs feedback operations from a classifier to the ICA network in the test phase, which adapts the unmixing matrices
of ICA according to a newly defined measure considering both independence and
classification error. This can result in adaptation of input space of the classifier and
so improve recognition performance. This process is inspired from the selective attention model [9,10] which calculates expected input signals according to top-down
information.
In the test phase, as shown in Figure 1, ICA separates signal and noise, and Melfrequency cepstral coefficients (MFCCs) extracted as a feature vector are delivered
to a classifier, multi-layer perceptron (MLP). After classification, the error function
of the classifier is defined as
E m1p

= 2"1~
L...,.(tmIP,i -

2

(1)

Ymlp,i) ,

i

where tmlp,i is target value of the output neuron Ymlp,i. In general, the target
values are not known and should be determined from the outputs Ymlp. Only the
target value of the highest output is set to 1, and the others are set to -1 when the
nonlinear function of the classifier is the bipolar sigmoid function. The algorithm
performs gradient-descent calculation by error backpropagation. To reduce the
error, it computes the required changes of the input values of the classifier and
finally those of the unmixed signals of the ICA network. Then, the leaning rule
of the ICA algorithm should be changed considering these variations. The newly
defined cost function of the ICA network includes the error backpropagated term
as well as the joint entropy H (Yica) of the outputs Yica.
Eica

=

1

-H(Yica) + 'Y. 2" (Utarget - u)(Utarget - u)
1

H

-H(Yica) + 'Y. 2"~u~u ,

H

(2)

where u are the estimate recovered sources and 'Y is a coefficient which represents the
relative importance of two terms. The learning rule derived using gradient descent
on the cost function in Eq.(2) is
~w ex: [I - <p(u)uH]W

+ 'Y. x~u,

(3)

where x are the input signals of the rcA network. The first term in Eq.(3) is the
learning rule of ICA which is applicable to complex-valued data in the frequency

Input Speech

Block into Frames

Hamming Window

Fourier Transform

ICA

Retraining ICA
~u

u

Linear-to-Mel Freq.
Filter Bank Conversion

Mel-to-Linear Freq.
Filter Bank Conversion

Frame Normalization

Frame Re-normalization

MLP Classification

MLP Backpropagation

Figure 1: Real-world speech recognition with feedback operations from a classifier
to the ICA network
domain [8,11]. In real environments where substantial time delays occur, the observed input signals are convolved mixtures of sources, not linear mixtures and the
mixing model no longer is a matrix. In this case, blind signal separation using ICA
can be achieved in the frequency domain. The complex score function is

'P(z)

= tanh(Re{z}) + j. tanh(Im{z}).

(4)

The procedure in the test phase is summarized as follows.
1. For a test input, perform the forward operation and classify the pattern.

2. Define the error function of the classifier in Eq. (1) and perform error backpropagation to find the required changes of unmixed signals of ICA.
3. Define the cost function of the ICA network in Eq.(2) and update unmixing
matrix with the learning rule in Eq.(3). Then, go to step 1.
The newly defined error function ofthe classifier in Eq.(l) does not cause overfitting
problems because it is used for updating the unmixing matrix of ICA only once.
If classification performance is good, the averaged changes of the unmixing matrix
over the total input patterns can contribute to improving recognition performance.
2.2

Considering the assumptions of ICA

The assumptions of ICA [3,4,5] are summarized as follows.

Figure 2: a nonlinear mixing model due to the distortions of microphones
1. The sources are linearly mixed.
2. The sources are mutually independent.

3. At most, one source is normally distributed.
4. The number of sensors is equal to or greater than the number of sources.
5. No sensor noise or only low additive noise signals are permitted.
The assumptions 4 and 5 can be released if there are enough sensors. The assumption 3 is also negligible because the source distribution is usually approximated as
super-Gaussian or Laplacian distributions in the speech recognition problem.
As to speech recognition in real mixing environments, the nonlinearity of microphones is an inevitable problem. Figure 2 shows a nonlinear mixing model, the
nonlinear functions g(.) and h(?) denote the distortions of microphones. s are original sources, x are observed signals, and u are the estimates of the recovered sources.
If the sources 81 and 82 are mutually independent, the random variables 8r and 8 2
are still independent each other, and so are Voo and VlO. The density of Zl = VOO+VlO
equals the convolution of the densities of Voo and VlO [7].

=
=

f

Pvoo(Zl - VlO)PVIO(VlO)dvlO,

P(Zl)
h~

(5)

.

After all, the observed signal Xl is not a linear mixture of two independent components due to the nonlinear distortion h(?). The assumption of source independence
is violated. In this situation, it is hard to expect what would be the leA solution
and to assert the solution is reliable. Even if Xl has two independent components,
which is the case of linear distortion of microphones, there is a conflict between independence and source density approximation because the densities of independent
components of observed signals are different from those of original sources by g(.)
and h(?), and may be far from the density approximated by f(?).
The proposed algorithm can be a solution to this problem. In the training phase, a
classifier learns noiseless data and the density of Xl used for the learning is
p(xd

=

p(81)

aoo h~g~ .

(6)

The second backpropagated term in the cost function Eq.(2) changes the unmixing
matrix W to adapt the density of unmixed signals to the density that the classifier

Table 1: The recognition rates of noisy speech recorded with F-16 fighter noise (%)
SNR
MLP
leA
The proposed
algorithm

lJlean
99.9
99.7
99.9

Training data
15dB lOdB
93.3
73.5
97.0
91.9
99.3

94.5

5dB
42.8
78.7

lJlean
96.1
93.9

Test data
15dB 10dB
84.8
63.0
90.6
85.6

5dB
36.7
68.9

80.6

96.1

93.5

71.1

86.3

learned. This can be a clue to what should be the leA solution. Iterative operations
over the total data induce that the averaged change of the unmixing matrix becomes
roughly a function of the nonlinearity g(.) and h(?), not a certain density P(Sl)
subject to every pattern.

3

Noisy Speech Recognition in Real Environments

The proposed algorithm was applied to isolated-word speech recognition. The input
data are convolved mixtures of speech and noise recorded in real environments. The
speech data set consists of 75 Korean words of 48 speakers, and F-16 fighter noise
and speech babbling noise were used as noise sources. Each leA network has two
inputs and two outputs for the signal and noise sources. Tables 1 and 2 show the
recognition results for the three methods: MLP only, MLP with standard leA,
and the proposed algorithm. 'Training data' mean the data used for learning of
the classifier, and 'Test data' are the rest. leA improves classification performance
compared to MLP only in the heavy-noise cases, but in the cases of clean data,
leA does not contribute to recognition and the recognition rates are lower than
those of MLP only. The proposed algorithm shows better recognition performance
than standard leA for both training and test data. Especially, for the clean data,
the proposed algorithm improves the recognition rates to be the same as those of
MLP only in most cases. The algorithm reduces the false recognition rates by about
30% to 80% in comparison with standard leA when signal to noise ratios (SNRs)
are 15dB or higher. With such low noise, the classification performance of MLP is
relatively reliable, and MLP can provide the leA network for helpful information.
However, with heavy noise, the recognition rates of MLP sharply decrease, and the
error backpropagation can hardly provide valuable information to the leA network.
The overall improvement for the training data is higher than that for the test data.
This is because the the recognition performance of MLP is better for the training
data.
As shown in Figure 3, iterative feedback operations decrease the false recognition
rates, and the variation of the unknown parameter '"Y in Eq.(2) doesn't affect the
final recognition performance. The variation of the learning rate for updating the
unmixing matrix also doesn't affect the final performance, and it only influences on
the converging time to reach the final recognition rates. The learning rate was fixed
regardless of SNR in all of the experiments.

4

Discussion

The proposed algorithm is an approach to complement leA by providing additional
information based on top-down selective attention with a pre-trained MLP classifier. The error backpropagation operations adapt the density of recovered signals

Table 2: The recognition rates of noisy speech recorded with speech babbling noise

(%)
SNR
MLP
ICA
The proposed
algorithm

lJlean
99.7
98.5
99.7

Training data
15dtl lOdtl
88.6
61.5
95.2
91.9
97.7

92.5

5dtl
32.6
76.5

lJlean
96.8
91.7

Test data
15dtl 10dtl
82.9
64.5
88.6
85.1

5dtl
38.5
73.2

76.7

97.2

93.1

73.4

87.4

according to the new cost function of ICA. This can help ICA find the solution
proper for classification under the nonlinear and independence violations, but this
needs the stationary condition. For nonstationary environments, a mixture model
like the ICA mixture model [6] can be considered. The ICA mixture model can
assign class membership to each environment category and separate independent
sources in each class. To completely settle the nonlinearity problem in real environment, it is necessary to introduce a scheme which models the nonlinearity such
as the distortions of microphones. Multi-layered ICA can be an approach to model
nonlinearity.
In the noisy recognition problem, the proposed algorithm improved recognition performance compared to ICA alone. Especially in moderate noise cases, the algorithm
remarkably reduced the false recognition rates. This is due to the high classification performance of the pre-trained MLP. In the case of heavy noise the expected
ICA output estimated from the top-down attention may not be accurate, and the
selective attention does not help much. It is natural that we only put attention to
familiar subjects. Therefore more robust classifiers may be needed for signals with
heavy noise.

Acknowledgments
This work was supported as a Brain Science & Engineering Research Program
sponsored by Korean Ministry of Science and Technology.

References
[1] Amari, S., Cichocki, A., and Yang, H. (1996) A new learning algorithm for
blind signal separation, In Advances in Neural Information Processing Systems
8, pp. 757-763.
[2] Bell, A. J. and Sejnowski, T. J. (1995) An information-maximization approach
to blind separation and blind deconvolution, Neural Computation, 7:1129-1159.
[3] Cardoso, J.-F. and Laheld, B. (1996) Equivariant adaptive source separation,
IEEE Trans. on S.P., 45(2):434-444.
[4] Comon, P. (1994) Independent component analysis - a new concept?, Signal
Processing, 36(3):287-314.
[5] Lee, T.-W. (1998) Independent component analysis - theory and applications,
Kluwer Academic Publishers, Boston.
[6] Lee, T.-W., Lewicki, M. S., and Sejnowski, T. J. (1999) ICA mixture models
for unsupervised classification of non-Gaussian sources and automatic context

2.5

~~~~~~~~~~~~~~~~~~~~~~~~~~~j ~ ~:~ ~-

~

~

2

1ll

r--------~---_____,

~~~~~~~:::::::::~~:::~~::~~j ~ ~:~ ~-

~

~

6

1ll

a:

a:

c:: 1.5

c:: 4.5

o

o

:;:;

:;:;

'E

'E

ell

8

7.5

r-~------~---_____'

ell

8

1

Q)

3

Q)

a:

a:

Q)

Q)

!!) 0.5

'"

U.

!!)

K----- --------------- ----- ----- ---------O

'"

1.5

U.

O

'------..~~~~~~~~~~~--"-----'

o

5

10

15

'------~-~-~-~--------'

o

Iteration of total data set

5

(a)

~12

~~~~~~~~~~~~~~~~~~~~~~~~~~~j ~ ~~:~ ~-

1ll
o

c:: 21

'';::;

'E

ell

6

-------~---~
- ~~~~

Q)

- - - - - - - - - - - - - - - ---~
. ~~~

ell

8

19

Q)

a:

a:

Q)

'"

1:-

o

:;:;

!!)

:~~~~~:::~~~~::~~~~~::::~~:::i : ;~:~

~
~23

a:
9

'E

8

,----~---~---__,

1ll

a:

c::

15

(b)
25

~

10

Iteration of total data set

Q)

!!) 17

3

'"

U.

U.

5

10

Iteration of total data set

(c)

15

15

'------~-~-~-~---~

o

5

10

15

Iteration of total data set

(d)

Figure 3: The false recognition rates by iteration of total data and the value of the
'Y parameter. (a) Clean speech; (b) SNR=15 dB; (c) SNR=lO dB; (d) SNR=5 dB

[7]
[8]

[9]
[10]
[11]

switching in blind signal separation, IEEE Trans . on Pattern Analysis and
Machine Intelligence, in press.
Papoulis, A. (1991) Probability, random variables, and stochastic processes,
McGraw-Hill, Inc.
Park, H.-M., Jung, H.-Y., Lee, T.-W., and Lee, S.-Y. (1999) Subbandbased blind signal separation for noisy speech recognition, Electronics Letters,
35(23) :2011-2012.
Park, K.-Y. and Lee, S.-Y. (1999) Selective attention for robust speech recognition in noisy environments, In Proc. of IJCNN, paper no. 829.
Park, K.-Y. and Lee, S.-Y. (2000) Out-of-vocabulary rejection based on selective attention model, Neural Processing Letters, 12:41-48.
Smaragdis, P. (1997) Information theoretic approaches to source separation,
Masters Thesis, MIT Media Arts and Science Dept.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 5953-natural-neural-networks.pdf

Natural Neural Networks
Guillaume Desjardins, Karen Simonyan, Razvan Pascanu, Koray Kavukcuoglu
{gdesjardins,simonyan,razp,korayk}@google.com
Google DeepMind, London

Abstract
We introduce Natural Neural Networks, a novel family of algorithms that speed up
convergence by adapting their internal representation during training to improve
conditioning of the Fisher matrix. In particular, we show a specific example that
employs a simple and efficient reparametrization of the neural network weights by
implicitly whitening the representation obtained at each layer, while preserving
the feed-forward computation of the network. Such networks can be trained efficiently via the proposed Projected Natural Gradient Descent algorithm (PRONG),
which amortizes the cost of these reparametrizations over many parameter updates and is closely related to the Mirror Descent online learning algorithm. We
highlight the benefits of our method on both unsupervised and supervised learning tasks, and showcase its scalability by training on the large-scale ImageNet
Challenge dataset.

1

Introduction

Deep networks have proven extremely successful across a broad range of applications. While their
deep and complex structure affords them a rich modeling capacity, it also creates complex dependencies between the parameters which can make learning difficult via first order stochastic gradient
descent (SGD). As long as SGD remains the workhorse of deep learning, our ability to extract highlevel representations from data may be hindered by difficult optimization, as evidenced by the boost
in performance offered by batch normalization (BN) [7] on the Inception architecture [25].
Though its adoption remains limited, the natural gradient [1] appears ideally suited to these difficult
optimization issues. By following the direction of steepest descent on the probabilistic manifold,
the natural gradient can make constant progress over the course of optimization, as measured by the
Kullback-Leibler (KL) divergence between consecutive iterates. Utilizing the proper distance measure ensures that the natural gradient is invariant to the parametrization of the model. Unfortunately,
its application has been limited due to its high computational cost. Natural gradient descent (NGD)
typically requires an estimate of the Fisher Information Matrix (FIM) which is square in the number
of parameters, and worse, it requires computing its inverse. Truncated Newton methods can avoid
explicitly forming the FIM in memory [12, 15], but they require an expensive iterative procedure to
compute the inverse. Such computations can be wasteful as they do not take into account the highly
structured nature of deep models.
Inspired by recent work on model reparametrizations [17, 13], our approach starts with a simple question: can we devise a neural network architecture whose Fisher is constrained to be
identity? This is an important question, as SGD and NGD would be equivalent in the resulting
model. The main contribution of this paper is in providing a simple, theoretically justified network
reparametrization which approximates via first-order gradient descent, a block-diagonal natural gradient update over layers. Our method is computationally efficient due to the local nature of the
reparametrization, based on whitening, and the amortized nature of the algorithm. Our second contribution is in unifying many heuristics commonly used for training neural networks, under the roof
of the natural gradient, while highlighting an important connection between model reparametrizations and Mirror Descent [3]. Finally, we showcase the efficiency and the scalability of our method
1

across a broad-range of experiments, scaling our method from standard deep auto-encoders to large
convolutional models on ImageNet[20], trained across multiple GPUs. This is to our knowledge the
first-time a (non-diagonal) natural gradient algorithm is scaled to problems of this magnitude.

2

The Natural Gradient

This section provides the necessary background and derives a particular form of the FIM whose
structure will be key to our efficient approximation. While we tailor the development of our method
to the classification setting, our approach generalizes to regression and density estimation.
2.1

Overview

We consider the problem of fitting the parameters ? 2 RN of a model p(y | x; ?) to an empirical
distribution ?(x, y) under the log-loss. We denote by x 2 X the observation vector and y 2 Y its
associated label. Concretely, this stochastic optimization problem aims to solve:
??

2

argmin? E(x,y)?? [ log p(y | x, ?)] .

(1)

Defining the per-example loss as `(x, y), Stochastic Gradient Descent (SGD) performs the above
minimization by iteratively following the direction of steepest descent, given by the column vector
r = E? [d`/d?]. Parameters are updated using the rule ?(t+1)
?(t) ?(t) r(t) , where ? is a
learning rate. An equivalent proximal form of gradient descent [4] reveals the precise nature of ?:
?
2
1
?(t+1) = argmin? h?, ri + (t) ? ?(t)
(2)
2
2?

Namely, each iterate ?(t+1) is the solution to an auxiliary optimization problem, where ? controls
the distance between consecutive iterates, using an L2 distance. In contrast, the natural gradient
relies on the KL-divergence between iterates, a more appropriate distance measure for probability
distributions. Its metric is determined by the Fisher Information matrix,
(
"?
??
?T #)
@ log p
@ log p
F? = Ex?? Ey?p(y|x,?)
,
(3)
@?
@?
i.e. the covariance of the gradients of the model log-probabilities wrt. its parameters. The natural
gradient direction is then obtained as rN = F? 1 r. See [15, 14] for a recent overview of the topic.
2.2

Fisher Information Matrix for MLPs

We start by deriving the precise form of the Fisher for a canonical multi-layer perceptron (MLP)
composed of L layers. We consider the following deep network for binary classification, though our
approach generalizes to an arbitrary number of output classes.
p(y = 1 | x) ? hL
???
h1

=

fL (WL hL

=

f1 (W1 x + b1 )

1

+ bL )

(4)

The parameters of the MLP, denoted ? = {W1 , b1 , ? ? ? , WL , bL }, are the weights Wi 2 RNi ?Ni
connecting layers i and i 1, and the biases bi 2 RNi . fi is an element-wise non-linear function.

1

Let us define i to be the backpropagated gradient through the i-th non-linearity. We ignore the
off block-diagonal components of the Fisher matrix and focus on the block FWi , corresponding to
interactions between parameters of layer i. This block takes the form:
h
i
T
FWi = Ex?? vec i hTi 1 vec i hTi t
,
y?p

where vec(X) is the vectorization function yielding a column vector from the rows of matrix X.
Assuming that

i

and activations hi

1

are independent random variables, we can write:

FWi (km, ln) ? Ex?? [ i (k) i (l)] E? [hi
y?p

2

1 (m)hi 1 (n)] ,

(5)

?t

?t+T

1

F (?t) 2

F (?t)
?t+1

?t

1
2

?t+T

Figure 1: (a) A 2-layer natural neural network. (b) Illustration of the projections involved in PRONG.

where X(i, j) is the element at row i and column j of matrix X and x(i) is the i-th element of vector
x. FWi (km, ln) is the entry in the Fisher capturing interactions between parameters Wi (k, m)
and Wj (l, n). Our hypothesis, verified experimentally
?
? in Sec. 4.1, is that we can greatly improve
conditioning of the Fisher by enforcing that E? hi hTi = I, for all layers of the network, despite
ignoring possible correlations in the ?s and off block diagonal terms of the Fisher.

3

Projected Natural Gradient Descent

This section introduces Whitened Neural Networks (WNN), which perform approximate whitening
of their internal representations. We begin by presenting a novel whitened neural layer, with the
assumption that the network statistics ?i (?) = E[hi ] and ?i (?) = E[hi hTi ] are fixed. We then show
how these layers can be adapted to efficiently track population statistics over the course of training.
The resulting learning algorithm is referred to as Projected Natural Gradient Descent (PRONG). We
highlight an interesting connection between PRONG and Mirror Descent in Section 3.3.
3.1

A Whitened Neural Layer

The building block of WNN is the following neural layer,
hi

=

fi (Vi Ui

1

(hi

1

ci

1)

(6)

+ di ) .

Compared to Eq. 4, we have introduced an explicit centering parameter ci 1 2 RNi 1 , equal to
?i 1 , which ensures that the input to the dot product has zero mean in expectation. This is analogous to the centering reparametrization for Deep Boltzmann Machines [13]. The weight matrix
Ui 1 2 RNi 1 ?Ni 1 is a per-layer PCA-whitening matrix whose rows are obtained from an eigendecomposition of ?i 1 :
?i ? diag ( i ) ? U
?iT =) Ui = diag (
?i = U

i

+ ?)

1
2

?iT .
?U

(7)

The hyper-parameter ? is a regularization term controlling the maximal multiplier on the learning
rate, or equivalently the size of the trust region. The parameters Vi 2 RNi ?Ni 1 and di 2 RNi are
analogous to the canonical parameters of a neural network as introduced in Eq. 4, though operate
in the space of whitened unit activations Ui (hi ci ). This layer can be stacked to form a deep
neural network having L layers, with model parameters ? = {V1 , d1 , ? ? ? VL , dL } and whitening
coefficients = {U0 , c0 , ? ? ? , UL 1 , cL 1 }, as depicted in Fig. 1a.

Though the above layer might appear over-parametrized at first glance, we crucially do not learn
the whitening coefficients via loss minimization, but instead estimate them directly from the model
statistics. These coefficients are thus constants from the point of view of the optimizer and simply
serve to improve conditioning of the Fisher with respect to the parameters ?, denoted F? . Indeed,
using the same derivation
that led
?
? to Eq. 5, we can see that the block-diagonal terms of F? now
involve terms E (Ui hi )(Ui hi )T , which equals identity by construction.
3.2

Updating the Whitening Coefficients

As the whitened model parameters ? evolve during training, so do the statistics ?i and ?i . For our
model to remain well conditioned, the whitening coefficients must be updated at regular intervals,
3

Algorithm 1 Projected Natural Gradient Descent
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:

Input: training set D, initial parameters ?.
Hyper-parameters: reparam. frequency T , number of samples Ns , regularization term ?.
Ui
I; ci
0; t
0
repeat
if mod(t, T ) = 0 then
. amortize cost of lines [6-11]
for all layers i do
Compute canonical parameters Wi = Vi Ui 1 ; bi = di Wi ci 1 . . proj. P 1 (?)
Estimate ?i and ?i , using Ns samples from D.
Update ci from ?i and Ui from eigen decomp. of ?i + ?I.
. update
Update parameters Vi
Wi Ui 11 ; di
bi + V i U i 1 c i 1 .
. proj. P (?)
end for
end if
Perform SGD update wrt. ? using samples from D.
t
t+1
until convergence

while taking care not to interfere with the convergence properties of gradient descent. This can be
achieved by coupling updates to with corresponding updates to ? such that the overall function
implemented by the MLP remains unchanged, e.g. by preserving the product Vi Ui 1 before and
after each update to the whitening coefficients (with an analoguous constraint on the biases).
Unfortunately, while estimating the mean ?i and diag(?i ) could be performed online over a minibatch of samples as in the recent Batch Normalization scheme [7], estimating the full covariance
matrix will undoubtedly require a larger number of samples. While statistics could be accumulated
online via an exponential moving average as in RMSprop [27] or K-FAC [8], the cost of the eigendecomposition required for computing the whitening matrix Ui remains cubic in the layer size.
In the simplest instantiation of our method, we exploit the smoothness of gradient descent by simply
amortizing the cost of these operations over T consecutive updates. SGD updates in the whitened
model will be closely aligned to NGD immediately following the reparametrization. The quality
of this approximation will degrade over time, until the subsequent reparametrization. The resulting
algorithm is shown in the pseudo-code of Algorithm 1. We can improve upon this basic amortization scheme by updating the whitened parameters ? using a per-batch diagonal natural gradient update, whose statistics are computed online. In our framework, this can be implemented
via the reparametrization Wi = Vi Di 1 Ui 1 , where Di 1 is a diagonal matrix updated such that
V [Di 1 Ui 1 hi 1 ] = 1, for each minibatch. Updates to Di 1 can be compensated for exactly and
cheaply by scaling the rows of Ui 1 and columns of Vi accordingly. A simpler implementation of
this idea is to combine PRONG with batch-normalization, which we denote as PRONG+ .
3.3

Duality and Mirror Descent

There is an inherent duality between the parameters ? of our whitened neural layer and the parameters ? of a canonical model. Indeed, there exist linear projections P (?) and P 1 (?), which map
from canonical parameters ? to whitened parameters ?, and vice-versa. P (?) corresponds to line
10 of Algorithm 1, while P 1 (?) corresponds to line 7. This duality between ? and ? reveals a
close connection between PRONG and Mirror Descent [3].
Mirror Descent (MD) is an online learning algorithm which generalizes the proximal form of gradient descent to the class of Bregman divergences B (q, p), where q, p 2 and : ! R is a
strictly convex and differentiable function. Replacing the L2 distance by B , mirror descent solves
the proximal problem of Eq. 2 by applying first-order updates in a dual space and then projecting back onto the primal space. Defining ? = r? (?) and ? = r?? (?), with ? the complex
conjugate of , the mirror descent updates are given by:
?(t+1)

=

r?

?(t+1)

=

r?

?

?
?(t)
?(t) r?
?
?
?
?(t+1)

4

(8)
(9)

(a)

(b)

(c)

Figure 2: Fisher matrix for a small MLP (a) before and (b) after the first reparametrization. Best viewed in
colour. (c) Condition number of the FIM during training, relative to the initial conditioning. All models where
initialized such that the initial conditioning was the same, and learning rate where adjusted such that they reach
roughly the same training error in the given time.

It is well known [26, 18] that the natural gradient is a special case of MD, where the distance
generating function 1 is chosen to be (?) = 12 ?T F ?.
The mirror updates are somewhat unintuitive however. Why is the gradient r? applied to the dual
space if it has been computed in the space of parameters
p ? ? This is where PRONG relates to MD. It
is trivial to show that using the function ?(?) = 12 ?T F ?, instead of the previously defined (?),
enables us to directly update the dual parameters using r? , the gradient computed directly in the
dual space. Indeed, the resulting updates can be shown to implement the natural gradient and are
thus equivalent to the updates of Eq. 9 with the appropriate choice of (?):
?
? ?
1
1
d`
(t+1)
(t)
(t)
(t)
(t)
?
?
?
= r?
?
? r? = F 2 ?
? E?
F 2
d?
?
?
?
d`
? (t+1) = ?(t) ?(t) F 1 E?
??(t+1) = r? ?? ?
(10)
d?
? and r
? ? correspond to the projections P (?) and P 1 (?) used by PRONG
The operators r
to map from the canonical neural parameters ? to those of the whitened layers ?. As illustrated
in Fig. 1b, the advantage of this whitened form of MD is that one may amortize the cost of the
projections over several updates, as gradients can be computed directly in the dual parameter space.
3.4

Related Work

This work extends the recent contributions of [17] in formalizing many commonly used heuristics
for training MLPs: the importance of zero-mean activations and gradients [10, 21], as well as the
importance of normalized variances in the forward and backward passes [10, 21, 6]. More recently,
Vatanen et al. [28] extended their previous work [17] by introducing a multiplicative constant i
to the centered non-linearity. In contrast, we introduce a full whitening matrix Ui and focus on
whitening the feedforward network activations, instead of normalizing a geometric mean over units
and gradient variances.
The recently introduced batch normalization (BN) scheme [7] quite closely resembles a diagonal
version of PRONG, the main difference being that BN normalizes the variance of activations before
the non-linearity, as opposed to normalizing the latent activations by looking at the full covariance.
Furthermore, BN implements normalization by modifying the feed-forward computations thus requiring the method to backpropagate through the normalization operator. A diagonal version of
PRONG also bares an interesting resemblance to RMSprop [27, 5], in that both normalization terms
involve the square root of the FIM. An important distinction however is that PRONG applies this
update in the whitened parameter space, thus preserving the natural gradient interpretation.
1
As the Fisher and thus
which we drop for clarity.

?

depend on the parameters ?(t) , these should be indexed with a time superscript,

5

(a)

(b)

(c)

(d)

Figure 3: Optimizing a deep auto-encoder on MNIST. (a) Impact of eigenvalue regularization term ?. (b)
Impact of amortization period T showing that initialization with the whitening reparametrization is important
for achieving faster learning and better error rate. (c) Training error vs number of updates. (d) Training error
vs cpu-time. Plots (c-d) show that PRONG achieves better error rate both in number of updates and wall clock.

K-FAC [8] is closely related to PRONG and was developed concurrently to our method. It targets
the same layer-wise block-diagonal of the Fisher, approximating each block as in Eq. 5. Unlike
our method however, KFAC does not approximate the covariance of backpropagated gradients as
the identity, and further estimates the required statistics using exponential moving averages (unlike our approach based on amortization). Similar techniques can be found in the preconditioning
of the Kaldi speech recognition toolkit [16]. By modeling the Fisher matrix as the covariance of
a sparsely connected Gaussian graphical model, FANG [19] represents a general formalism for
exploiting model structure to efficiently compute the natural gradient. One application to neural
networks [8] is in decorrelating gradients across neighbouring layers.
A similar algorithm to PRONG was later found in [23], where it appeared simply as a thought
experiment, but with no amortization or recourse for efficiently computing F .

4

Experiments

We begin with a set of diagnostic experiments which highlight the effectiveness of our method at
improving conditioning. We also illustrate the impact of the hyper-parameters T and ?, controlling
the frequency of the reparametrization and the size of the trust region. Section 4.2 evaluates PRONG
on unsupervised learning problems, where models are both deep and fully connected. Section 4.3
then moves onto large convolutional models for image classification. Experimental details such as
model architecture or hyper-parameter configurations can be found in the supplemental material.
4.1

Introspective Experiments

Conditioning. To provide a better understanding of the approximation made by PRONG, we train
a small 3-layer MLP with tanh non-linearities, on a downsampled version of MNIST (10x10) [11].
The model size was chosen in order for the full Fisher to be tractable. Fig. 2(a-b) shows the FIM
of the middle hidden layers before and after whitening the model activations (we took the absolute
value of the entries to improve visibility). Fig. 2c depicts the evolution of the condition number
of the FIM during training, measured as a percentage of its initial value (before the first whitening
reparametrization in the case of PRONG). We present such curves for SGD, RMSprop, batch normalization and PRONG. The results clearly show that the reparametrization performed by PRONG
improves conditioning (reduction of more than 95%). These observations confirm our initial assumption, namely that we can improve conditioning of the block diagonal Fisher by whitening
activations alone.
Sensitivity of Hyper-Parameters. Figures 3a- 3b highlight the effect of the eigenvalue regularization term ? and the reparametrization interval T . The experiments were performed on the best
6

(a)

(b)

(c)

(d)

Figure 4: Classification error on CIFAR-10 (a-b) and ImageNet (c-d). On CIFAR-10, PRONG achieves better
test error and converges faster. On ImageNet, PRONG+ achieves comparable validation error while maintaining a faster covergence rate.

performing auto-encoder of Section 4.2 on the MNIST dataset. Figures 3a- 3b plot the reconstruction
error on the training set for various values of ? and T . As ? determines a maximum multiplier on the
learning rate, learning becomes extremely sensitive when this learning rate is high2 . For smaller step
sizes however, lowering ? can yield significant speedups often converging faster than simply using a
larger learning rate. This confirms the importance of the manifold curvature for optimization (lower
? allows for different directions to be scaled drastically different according to their corresponding
curvature). Fig 3b compares the impact of T for models having a proper whitened initialization
(solid lines), to models being initialized with a standard ?fan-in? initialization (dashed lines) [10].
These results are quite surprising in showing the effectiveness of the whitening reparametrization
as a simple initialization scheme. That being said, performance can degrade due to ill conditioning
when T becomes excessively large (T = 105 ).
4.2

Unsupervised Learning

Following Martens [12], we compare PRONG on the task of minimizing reconstruction error of a
dense 8-layer auto-encoder on the MNIST dataset. Reconstruction error with respect to updates and
wallclock time are shown in Fig. 3 (c,d). We can see that PRONG significantly outperforms the
baseline methods, by up to an order of magnitude in number of updates. With respect to wallclock,
our method significantly outperforms the baselines in terms of time taken to reach a certain error
threshold, despite the fact that the runtime per epoch for PRONG was 3.2x that of SGD, compared
to batch normalization (2.3x SGD) and RMSprop (9x SGD). Note that these timing numbers reflect
performance under the optimal choice of hyper-parameters, which in the case of batch normalization
yielded a batch size of 256, compared to 128 for all other methods. Further breaking down the
performance, 34% of the runtime of PRONG was spent performing the whitening reparametrization,
compared to 4% for estimating the per layer means and covariances. This confirms that amortization
is paramount to the success of our method.3
4.3

Supervised Learning

We now evaluate our method for training deep supervised convolutional networks for object recognition. Following [7], we perform whitening across feature maps only: that is we treat pixels in a
given feature map as independent samples. This allows us to implement the whitened neural layer
as a sequence of two convolutions, where the first is by a 1x1 whitening filter. PRONG is compared
to SGD, RMSprop and batch normalization, with each algorithm being accelerated via momentum.
Results are presented on CIFAR-10 [9] and the ImageNet Challenge (ILSVRC12) datasets [20]. In
both cases, learning rates were decreased using a ?waterfall? annealing schedule, which divided the
learning rate by 10 when the validation error failed to improve after a set number of evaluations.
2

Unstable combinations of learning rates and ? are omitted for clarity.
We note that our whitening implementation is not optimized, as it does not take advantage of GPU acceleration. Runtime is therefore expected to improve as we move the eigen-decompositions to GPU.
3

7

CIFAR-10 We now evaluate PRONG on CIFAR-10, using a deep convolutional model inspired
by the VGG architecture [22]. The model was trained on 24 ? 24 random crops with random
horizontal reflections. Model selection was performed on a held-out validation set of 5k examples.
Results are shown in Fig. 4. With respect to training error, PRONG and BN seem to offer similar
speedups compared to SGD with momentum. Our hypothesis is that the benefits of PRONG are more
pronounced for densely connected networks, where the number of units per layer is typically larger
than the number of maps used in convolutional networks. Interestingly, PRONG generalized better,
achieving 7.32% test error vs. 8.22% for batch normalization. This reflects the findings of [15],
which showed how NGD can leverage unlabeled data for better generalization: the ?unlabeled? data
here comes from the extra crops and reflections observed when estimating the whitening matrices.
ImageNet Challenge Dataset Our final set of experiments aims to show the scalability of our
method. We applied our natural gradient algorithm to the large-scale ILSVRC12 dataset (1.3M images labelled into 1000 categories) using the Inception architecture [7]. In order to scale to problems
of this size, we parallelized our training loop so as to split the processing of a single minibatch (of
size 256) across multiple GPUs. Note that PRONG can scale well in this setting, as the estimation
of the mean and covariance parameters of each layer is also embarassingly parallel. Eight GPUs
were used for computing gradients and estimating model statistics, though the eigen decomposition
required for whitening was itself not parallelized in the current implementation. Given the difficulty
of the task, we employed the enhanced version of the algorithm (PRONG+), as simple periodic
whitening of the model proved to be unstable. Figure 4 (c-d) shows that batch normalisation and
PRONG+ converge to approximately the same top-1 validation error (28.6% vs 28.9% respectively)
for similar cpu-time. In comparison, SGD achieved a validation error of 32.1%. PRONG+ however
exhibits much faster convergence initially: after 105 updates it obtains around 36% error compared
to 46% for BN alone. We stress that the ImageNet results are somewhat preliminary. While our
top-1 error is higher than reported in [7] (25.2%), we used a much less extensive data augmentation
pipeline. We are only beginning to explore what natural gradient methods may achieve on these
large scale optimization problems and are encouraged by these initial findings.

5

Discussion

We began this paper by asking whether convergence speed could be improved by simple model
reparametrizations, driven by the structure of the Fisher matrix. From a theoretical and experimental
perspective, we have shown that Whitened Neural Networks can achieve this via a simple, scalable
and efficient whitening reparametrization. They are however one of several possible instantiations
of the concept of Natural Neural Networks. In a previous incarnation of the idea, we exploited a
similar reparametrization to include whitening of backpropagated gradients4 . We favor the simpler
approach presented in this paper, as we generally found the alternative less stable for deep networks.
This may be due to the difficulty in estimating gradient covariances in lower layers, a problem which
seems to mirror the famous vanishing gradient problem. [17].
Maintaining whitened activations may also offer additional benefits from the point of view of model
compression and generalization. By virtue of whitening, the projection Ui hi forms an ordered representation, having least and most significant bits. The sharp roll-off in the eigenspectrum of ?i
may explain why deep networks are ammenable to compression [2]. Similarly, one could envision
spectral versions of Dropout [24] where the dropout probability is a function of the eigenvalues.
Alternative ways of orthogonalizing the representation at each layer should also be explored, via alternate decompositions of ?i , or perhaps by exploiting the connection between linear auto-encoders
and PCA. We also plan on pursuing the connection with Mirror Descent and further bridging the
gap between deep learning and methods from online convex optimization.
Acknowledgments
We are extremely grateful to Shakir Mohamed for invaluable discussions and feedback in the preparation of
this manuscript. We also thank Philip Thomas, Volodymyr Mnih, Raia Hadsell, Sergey Ioffe and Shane Legg
for feedback on the paper.
4

The weight matrix can be parametrized as Wi = RiT Vi Ui

8

1,

with Ri the whitening matrix for

i.

References
[1] Shun-ichi Amari. Natural gradient works efficiently in learning. Neural Computation, 1998.
[2] Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? In NIPS. 2014.
[3] Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for convex
optimization. Oper. Res. Lett., 2003.
[4] P. L. Combettes and J.-C. Pesquet. Proximal Splitting Methods in Signal Processing. ArXiv e-prints,
December 2009.
[5] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. In JMLR. 2011.
[6] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In AISTATS, May 2010.
[7] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. ICML, 2015.
[8] Roger Grosse James Martens. Optimizing neural networks with kronecker-factored approximate curvature. In ICML, June 2015.
[9] Alex Krizhevsky. Learning multiple layers of features from tiny images. Master?s thesis, University of
Toronto, 2009.
[10] Yann LeCun, L?eon Bottou, Genevieve B. Orr, and Klaus-Robert M?uller. Efficient backprop. In Neural
Networks, Tricks of the Trade, Lecture Notes in Computer Science LNCS 1524. Springer Verlag, 1998.
[11] Yann Lecun, Lon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. In Proceedings of the IEEE, pages 2278?2324, 1998.
[12] James Martens. Deep learning via Hessian-free optimization. In ICML, June 2010.
[13] K.-R. M?uller and G. Montavon. Deep boltzmann machines and the centering trick. In K.-R. M?uller,
G. Montavon, and G. B. Orr, editors, Neural Networks: Tricks of the Trade. Springer, 2013.
[14] Yann Ollivier. Riemannian metrics for neural networks. arXiv, abs/1303.0818, 2013.
[15] Razvan Pascanu and Yoshua Bengio. Revisiting natural gradient for deep networks. In ICLR, 2014.
[16] Daniel Povey, Xiaohui Zhang, and Sanjeev Khudanpur. Parallel training of deep neural networks with
natural gradient and parameter averaging. ICLR workshop, 2015.
[17] T. Raiko, H. Valpola, and Y. LeCun. Deep learning made easier by linear transformations in perceptrons.
In AISTATS, 2012.
[18] G. Raskutti and S. Mukherjee. The Information Geometry of Mirror Descent. arXiv, October 2013.
[19] Ruslan Salakhutdinov Roger B. Grosse. Scaling up natural gradient by sparsely factorizing the inverse
fisher matrix. In ICML, June 2015.
[20] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large
Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 2015.
[21] Nicol N. Schraudolph. Accelerated gradient descent by factor-centering decomposition. Technical Report
IDSIA-33-98, Istituto Dalle Molle di Studi sull?Intelligenza Artificiale, 1998.
[22] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In
International Conference on Learning Representations, 2015.
[23] Jascha Sohl-Dickstein. The natural gradient by analogy to signal whitening, and recipes and tricks for its
use. arXiv, 2012.
[24] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout:
A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 2014.
[25] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru
Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. arXiv, 2014.
[26] Philip S Thomas, William C Dabney, Stephen Giguere, and Sridhar Mahadevan. Projected natural actorcritic. In Advances in Neural Information Processing Systems 26. 2013.
[27] Tijmen Tieleman and Geoffrey Hinton. Rmsprop: Divide the gradient by a running average of its recent
magnitude. coursera: Neural networks for machine learning. 2012.
[28] Tommi Vatanen, Tapani Raiko, Harri Valpola, and Yann LeCun. Pushing stochastic gradient towards
second-order methods ? backpropagation learning with transformations in nonlinearities. ICONIP, 2013.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 6270-stochastic-multiple-choice-learning-for-training-diverse-deep-ensembles.pdf

Stochastic Multiple Choice Learning for
Training Diverse Deep Ensembles

Stefan Lee
Virginia Tech
steflee@vt.edu

Senthil Purushwalkam
Carnegie Mellon University
spurushw@andrew.cmu.edu
David Crandall
Indiana University
djcran@indiana.edu

Michael Cogswell
Virginia Tech
cogswell@vt.edu

Viresh Ranjan
Virginia Tech
rviresh@vt.edu

Dhruv Batra
Virginia Tech
dbatra@vt.edu

Abstract
Many practical perception systems exist within larger processes that include interactions with users or additional components capable of evaluating the quality of
predicted solutions. In these contexts, it is beneficial to provide these oracle mechanisms with multiple highly likely hypotheses rather than a single prediction. In this
work, we pose the task of producing multiple outputs as a learning problem over an
ensemble of deep networks ? introducing a novel stochastic gradient descent based
approach to minimize the loss with respect to an oracle. Our method is simple
to implement, agnostic to both architecture and loss function, and parameter-free.
Our approach achieves lower oracle error compared to existing methods on a wide
range of tasks and deep architectures. We also show qualitatively that the diverse
solutions produced often provide interpretable representations of task ambiguity.

1

Introduction

Perception problems rarely exist in a vacuum. Typically, problems in Computer Vision, Natural
Language Processing, and other AI subfields are embedded in larger applications and contexts. For
instance, the task of recognizing and segmenting objects in an image (semantic segmentation [6])
might be embedded in an autonomous vehicle [7], while the task of describing an image with a
sentence (image captioning [18]) might be part of a system to assist visually-impaired users [22, 29].
In these scenarios, the goal of perception is often not to generate a single output but a set of plausible
hypotheses for a ?downstream? process, such as a verification component or a human operator. These
downstream mechanisms may be abstracted as oracles that have the capability to pick the correct
solution from this set. Such a learning setting is called Multiple Choice Learning (MCL) [8], where
the goal for the learner is to minimize oracle loss achieved by a set of M solutions. More formally,
given a dataset of input-output pairs {(xi , yi ) | xi ? X , yi ? Y}, the goal of classical supervised
learning is to search for a mapping F : X ? Y that minimizes a task-dependent loss ` : Y ?Y ? R+
capturing the error between the actual labeling yi and predicted labeling y?i . In this setting, the learned
function f makes a single prediction for each input and pays a penalty for that prediction. In contrast,
Multiple Choice Learning seeks to learn a mapping g : X ? Y M that produces M solutions
Y?i = (?
yi1 , . . . , y?iM ) such that oracle loss minm ` (yi , y?im ) is minimized.
In this work, we fix the form of this mapping g to be the union of outputs from an ensemble of
predictors such that g(x) = {f1 (x), f2 (x), . . . , fM (x)}, and address the task of training ensemble
members f1 , . . . , fM such that g minimizes oracle loss. Under our formulation, different ensemble
members are free to specialize on subsets of the data distribution, so that collectively they produce a
set of outputs which covers the space of high probability predictions well.
30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

Horse

Cow

A	
 ?couple	
 ? of	
 ?birds	
 ? that	
 ?are	
 ?standing	
 ?in	
 ?the	
 ?grass.
A	
 ?bird	
 ?perched	
 ?on	
 ?top	
 ?of	
 ?a	
 ?tree	
 ?branch.
A	
 ?bird	
 ?perched	
 ?on	
 ?a	
 ?tree	
 ?branch	
 ?in	
 ?the	
 ?sky.

Figure 1: Single-prediction based models often produce solutions with low expected loss in the face of ambiguity;
however, these solutions are often unrealistic or do not reflect the image content well (row 1). Instead, we train
ensembles under a unified loss which allows each member to produce different outputs reflecting multi-modal
beliefs (row 2). We evaluate our method on image classification, segmentation, and captioning tasks.

Diverse solution sets are especially useful for structured prediction problems with multiple reasonable
interpretations, only one of which is correct. Situations that often arise in practical systems include:
? Implicit class confusion. The label space of many classification problems is often an arbitrary
quantization of a continuous space. For example, a vision system may be expected to classify
between tables and desks, despite many real-world objects arguably belonging to both classes. By
making multiple predictions, this implicit confusion can be viewed explicitly in system outputs.
? Ambiguous evidence. Often there is simply not enough information to make a definitive prediction.
For example, even a human expert may not be able to identify a fine-grained class (e.g., particular
breed of dog) given an occluded or distant view, but they likely can produce a small set of reasonable
guesses. In such cases, the task of producing a diverse set of possibilities is more clearly defined
than producing one correct answer.
? Bias towards the mode. Many models have a tendency to exhibit mode-seeking behaviors as a
way to reduce expected loss over a dataset (e.g., a conversation model frequently producing ?I
don?t know?). By making multiple predictions, a system can improve coverage of lower density
areas of the solution space, without sacrificing performance on the majority of examples.
In other words, by optimizing for the oracle loss, a multiple-prediction learner can respond to
ambiguity much like a human does, by making multiple guesses that capture multi-modal beliefs.
In contrast, a single-prediction learner is forced to produce a solution with low expected loss in
the face of ambiguity. Figure 1 illustrates how this can produce solutions that are not useful in
practice. In semantic segmentation, for example, this problem often causes objects to be predicted
as a mixture of multiple classes (like the horse-cow shown in the figure). In image captioning,
minimizing expected loss encourages generic sentences that are ?safe? with respect to expected error
but not very informative. For example, Figure 1 shows two pairs of images each having different
image content but very similar, generic captions ? the model knows it is safe to assume that birds are
on branches and that cakes are eaten with forks.
In this paper, we generalize the Multiple Choice Learning paradigm [8, 9] to jointly learn ensembles
of deep networks that minimize the oracle loss directly. We are the first to adapt these ideas to deep
networks and we present a novel training algorithm that avoids costly retraining [8] and learning
difficulty [5] of past methods. Our primary technical contribution is the formulation of a stochastic
block gradient descent optimization approach well-suited to minimizing the oracle loss in ensembles
of deep networks, which we call Stochastic Multiple Choice Learning (sMCL). Our formulation
is applicable to any model trained with stochastic gradient descent, is agnostic to the form of the task
dependent loss, is parameter-free, and is time efficient, training all ensemble members concurrently.
We demonstrate the broad applicability and efficacy of sMCL for training diverse deep ensembles
with interpretable emergent expertise on a wide range of problem domains and network architectures,
including Convolutional Neural Network (CNN) [1] ensembles for image classification [17], FullyConvolutional Network (FCN) [20] ensembles for semantic segmentation [6], and combined CNN
and Recurrent Neural Network (RNN) ensembles [14] for image captioning [18]. We provide detailed
analysis of the training and output behaviors of the resulting ensembles, demonstrating how ensemble
member specialization and expertise emerge automatically when trained using sMCL. Our method
outperforms existing baselines and produces sets of outputs with high oracle performance.
2

2

Related Work

Ensemble Learning. Much of the existing work on training ensembles focuses on diversity between
member models as a means to improve performance by decreasing error correlation. This is often
accomplished by resampling existing training data for each member model [27] or by producing
artificial data that encourages new models to be decorrelated with the existing ensemble [21]. Other
approaches train or combine ensemble members under a joint loss [19, 26]. More recently, work of
Hinton et al. [12] and Ahmed et al. [2] explores using ?generalist? network performance statistics to
inform the design of ensemble-of-expert architectures for classification. In contrast, sMCL discovers
specialization as a consequence of minimizing oracle loss. Importantly, most existing methods do
not generalize to structured output labels, while sMCL seamlessly adapts, discovering different
task-dependent specializations automatically.
Generating Multiple Solutions. There is a large body of work on the topic of extracting multiple
diverse solutions from a single model [3, 15, 16, 23, 24]; however, these approaches are designed for
probabilistic structured-output models and are not directly applicable to general deep architectures.
Most related to our approach is the work of Guzman-Rivera et al. [8, 9] which explicitly minimizes
oracle loss over the outputs of an ensemble, formalizing this setting as the Multiple Choice Learning
(MCL) paradigm. They introduce a general alternating block coordinate descent training approach
which requires retraining models multiple times. More recently, Dey et al. [5] reformulated this
problem as a submodular optimization task in which ensemble members are learned sequentially
in a boosting-like manner to maximize marginal gain in oracle performance. Both these methods
require either costly retraining or sequential training, making them poorly suited to modern deep
architectures that can take weeks to train. To address this serious shortcoming and to provide the first
practical algorithm for training diverse deep ensembles, we introduce a stochastic gradient descent
(SGD) based algorithm to train ensemble members concurrently.

3

Multiple-Choice Learning as Stochastic Block Gradient Descent

We consider the task of training an ensemble of differentiable learners that together produce a set of
solutions with minimal loss with respect to an oracle that selects only the lowest-error prediction.
Notation. We use [n] to denote the set {1, 2, . . . , n}. Given a training set of input-output pairs
D = {(xi , yi ) | xi ? X , yi ? Y}, our goal is to learn a function g : X ? Y M which maps
each input to M outputs. We fix the form of g to be an ensemble of M learners f such that
g(x) = (f1 (x), . . . , fM (x)). For some task-dependent loss `(y, y?), which measures the error
between true and predicted outputs y and y?, we define the oracle loss of g over the dataset D as
LO (D) =

n
X
i=1

min ` (yi , fm (xi )) .

m?[M ]

Minimizing Oracle Loss with Multiple Choice Learning. In order to directly minimize the oracle
loss for an ensemble of learners, Guzman-Rivera et al. [8] present an objective which forms a
(potentially tight) upper-bound. This objective replaces the min in the oracle loss with indicator
variables (pi,m )M
m=1 where pi,m is 1 if predictor m has the lowest error on example i,
argmin
fm ,pm,i

s.t.

n X
M
X

pi,m ` (yi , fm (xi ))

(1)

i=1 m=1
M
X

pi,m = 1,

pi,m ? {0, 1}.

The resulting minimization is a constrained joint optimization over ensemble parameters and datapoint assignments. The authors propose an alternating block algorithm, shown in Algorithm 1, to
approximately minimize this objective. Similar to K-Means or ?hard-EM,? this approach alternates
between assigning examples to their min-loss predictors and training models to convergence on the
partition of examples assigned to them. Note that this approach is not feasible with training deep
networks, since modern architectures [11] can take weeks or months to train a single model once.
Stochastic Multiple Choice Learning. To overcome this shortcoming, we propose a stochastic
algorithm for differentiable learners which interleaves the assignment step with batch updates in
3

Figure 2: The MCL approach of [8] (Alg. 1) requires costly retraining while our sMCL method (Alg. 2) works
within standard SGD solvers, training all ensemble members under a joint loss.

stochastic gradient descent. Consider the partial derivative of the objective in Eq. 1 with respect to
the output of the mth individual learner on example xi ,
?LO
?`(yi , fm (xi ))
= pi,m
.
?fm (xi )
?fm (xi )

(2)

Notice that if fm is the minimum error predictor for example xi , then pi,m = 1, and the gradient
term is the same as if training a single model; otherwise, the gradient is zero. This behavior lends
itself to a straightforward optimization strategy for learners trained by SGD based solvers. For each
batch, we pass the examples through the learners, calculating losses from each ensemble member for
each example. During the backward pass, the gradient of the loss for each example is backpropagated
only to the lowest error predictor on that example (with ties broken arbitrarily).
This approach, which we call Stochastic Multiple Choice Learning (sMCL), is shown in Algorithm 2.
sMCL is generalizable to any learner trained by stochastic gradient descent and is thus applicable to
an extensive range of modern deep networks. Unlike the iterative training schedule of MCL, sMCL
ensembles need only be trained to convergence once in parallel. sMCL is also agnostic to the exact
form of loss function ` such that it can be applied without additional effort on a variety of problems.

4

Experiments

In this section, we present results for sMCL ensembles trained for the tasks and deep architectures
shown in Figure 3. These include CNN ensembles for image classification, FCN ensembles for
semantic segmentation, and a CNN+RNN ensembles for image caption generation.
Baselines. Many existing general techniques for inducing diversity are not directly applicable to deep
networks. We compare our proposed method against:
- Classical ensembles in which each model is trained under an independent loss with differing
random initializations. We will refer to these as Indp. ensembles in figures.
- MCL [8] that alternates between training models to convergence on assigned examples and
allocating examples to their lowest error model. We repeat this process for 5 meta-iterations and
initialize ensembles with (different) random weights. We find MCL performs similarly to sMCL
on small classification tasks; however, MCL performance drops substantially on segmentation and
captioning tasks. Unlike sMCL which can effectively reassign an example once per epoch, MCL
only does this after convergence, limiting its capacity to specialize compared to sMCL. We also
note that sMCL is 5x faster than MCL, where the factor 5 is the result of choosing 5 meta-iterations
(other applications may require more, further increasing the gap.)
- Dey et al. [5] train models sequentially in a boosting-like fashion, each time reweighting examples
to maximize marginal increase of the evaluation metric. We find these models saturate quickly as
the ensemble size grows. As performance increases, the marginal gain and therefore the weights
approach zero. With low weights, the average gradient backpropagated for stochastic learners drops
substantially, reducing the rate and effectiveness of learning without careful tuning. To compute
4

(a) Convolutional classification
model of [1] for CIFAR10 [17]

(b) Fully-convolutional segmentation model of Long et al. [20]

(c) CNN+RNN based captioning
model of Karpathy et al. [14]

Figure 3: We experiment with three problem domains using the various architectures shown above.

weights, [5] requires an error measure bounded above by 1: accuracy (for classification) and IoU
(for segmentation) satisfy this; the CIDEr-D score [28] divided by 10 guarantees this for captioning.
Oracle Evaluation. We present results as oracle versions of the task-dependent performance metrics.
These oracle metrics report the highest score over all outputs for a given input. For example, in
classification tasks, oracle accuracy is exactly the top-k criteria of ImageNet [25], i.e. whether at
least one of the outputs is the correct label. Likewise, the oracle intersection over union (IoU) is the
highest IoU between the ground truth segmentation and any one of the outputs. Oracle metrics allow
the evaluation of multiple-prediction systems separately from downstream re-ranking or selection
systems, and have been extensively used in previous work [3, 5, 8, 9, 15, 16, 23, 24].
Our experiments convincingly demonstrate the broad applicability and efficacy of sMCL for training
diverse deep ensembles. In all three experiments, sMCL significantly outperforms classical ensembles,
Dey et al. [5] (typical improvements of 6-10%), and MCL (while providing a 5x speedup over MCL).
Our analysis shows that the exact same algorithm (sMCL) leads to the automatic emergence of
different interpretable notions of specializations among ensemble members.
4.1

Image Classification

Model. We begin our experiments with sMCL on the CIFAR10 [17] dataset using the small convolutional neural network ?CIFAR10-Quick? provided with the Caffe deep learning framework [13].
CIFAR10 is a ten way classification task with small 32?32 images. For these experiments, the
reference model is trained using a batch size of 350 for 5,000 iterations with a momentum of 0.9,
weight decay of 0.004, and an initial learning rate of 0.001 which drops to 0.0001 after 4000 iterations.
Results. Oracle accuracy for sMCL and baseline ensembles of size 1 to 6 are shown in Figure
4a. The sMCL trained ensembles result in higher oracle accuracy than the baseline methods, and
are comparable to MCL while being 5x faster. The method of Dey et al. [5] performs worse than
independent ensembles as ensemble size grows. Figure 4b shows the oracle loss during training for
sMCL and regular ensembles. The sMCL trained models optimize for the oracle cross-entropy loss
directly, not only arriving at lower loss solutions but also reducing error more quickly.
Interpretable Expertise: sMCL Induces Label-Space Clustering. Figure 4c shows the class-wise
distribution of the assignment of test datapoints to the oracle or ?winning? predictor for an M = 4
sMCL ensemble. The level of class division is striking ? most predictors become specialists for
certain classes. Note that these divisions emerge from training under the oracle loss and are not
hand-designed or pre-initialized in any way. In contrast, Figure 4f show that the oracle assignments
for a standard ensemble are nearly uniform. To explore the space between these two extremes, we
loosen the constraints of Eq. 1 such that the lowest k error predictors are penalized. By varying k
between 1 and the number of ensemble members M , the models transition from minimizing oracle
loss at k = 1 to a traditional ensemble at k = M . Figures 4d and 4e show these results. We find
a direct correlation between the degree of specialization and oracle accuracy, with k = 1 netting
highest oracle accuracy.
4.2

Semantic Segmentation

We now present our results for the semantic segmentation task on the Pascal VOC dataset [6].
Model. We use the fully convolutional network (FCN) architecture presented by Long et al. [20]
as our base model. Like [20], we train on the Pascal VOC 2011 training set augmented with extra
segmentations provided in [10] and we test on a subset of the VOC 2011 validation set. We initialize
5

Oracle Loss

Oracle Accuracy

95
90
85
80

sMCL

MCL

Dey [5]

Indp.

sMCL

4

Indp.

2

0
1

2

3

4

5

6

0

2,500

Ensemble Size M

(b) Oracle Loss During Training (M = 4)

(a) Effect of Ensemble Size
airplaine

0.10%

99.60%

0.10%

0.20%

70.60%

automobile

0.20%

0.00%

99.80%

0.00%

bird

99.50%

0.10%

0.30%

0.10%

cat

0.10%

99.90%

0.00%

deer

37.60%

0.00%

dog

0.10%

frog

5,000

Iterations

0.10%

29.20%

0.10%

28.50%

39.90%

31.60%

0.00%

22.60%

33.20%

25.20%

19.00%

0.00%

0.00%

22.20%

77.80%

36.20%

0.00%

38.10%

25.70%

30.30%

20.30%

26.10%

23.30%

0.00%

78.80%

19.30%

1.90%

27.90%

47.60%

0.00%

24.50%

19.70%

27.70%

26.30%

26.30%

0.00%

0.00%

0.00%

62.90%

37.10%

0.00%

71.30%

20.50%

8.20%

26.30%

26.40%

24.30%

23.00%

62.40%

0.00%

55.80%

0.00%

44.20%

0.00%

24.90%

57.40%

0.00%

17.70%

20.00%

23.60%

31.70%

24.70%

0.00%

0.00%

99.90%

63.30%

0.10%

0.00%

36.60%

61.20%

0.00%

24.00%

14.80%

29.30%

21.40%

27.90%

21.40%

99.90%

0.10%

0.00%

0.00%

27.70%

72.30%

0.00%

0.00%

50.40%

0.00%

33.30%

16.30%

17.30%

18.30%

32.50%

31.90%

horse

0.00%

99.90%

0.00%

0.10%

38.90%

0.00%

60.10%

1.00%

23.20%

0.00%

49.40%

27.40%

26.30%

26.80%

22.60%

24.30%

ship

0.00%

0.00%

100.00%

0.00%

0.00%

80.00%

0.00%

20.00%

0.10%

57.60%

11.60%

30.70%

25.30%

22.70%

24.40%

27.60%

truck

0.00%

0.00%

0.20%

99.80%

68.40%

0.00%

31.40%

0.20%

35.40%

0.00%

28.00%

36.60%

23.80%

20.60%

27.10%

28.50%

0

1

2

3

0

1

2

3

0

1

2

3

0

1

2

3

(c) k=1
(d) k=2
(e) k=3
(f) k=M=4
Figure 4: sMCL trained ensembles produce higher oracle accuracies than baselines (a) by directly optimizing
the oracle loss (b). By varying the number of predictors k each example can be assigned to, we can interpolate
between sMCL and standard ensembles, and (c-f) show the percentage of test examples of each class assigned
to each ensemble member by the oracle for various k. These divisions are not preselected and show how
specialization is an emergent property of sMCL training.

our sMCL models from a standard ensemble trained for 50 epochs at a learning rate of 10?3 . The
sMCL ensemble is then fine-tuned for another 15 epochs at a reduced learning rate of 10?5 .
Results. Figure 5a shows oracle accuracy (class-averaged IoU) for all methods with ensemble sizes
ranging from 1 to 6. Again, sMCL significantly outperforms all baselines (~7% relative improvement
over classical ensembles). In this more complex setting, we see the method of Dey et al. [5] saturates
more quickly ? resulting in performance worse than classical ensembles as ensemble size grows.
Though we expect MCL to achieve similar results as sMCL, retraining the MCL ensembles a sufficient
number of times proved infeasible so results after five meta-iterations are shown.

Oracle Mean IoU

Interpretable Expertise: sMCL as Segmentation Specialists. In Figure 5b, we analyze the class
distribution of the predictions using an sMCL ensemble with 4 members. For each test sample, the
oracle picks the prediction which corresponds to the ensemble member with the highest accuracy
for that sample. We find the specialization with respect to classes is much less evident than in the
classification experiments. As segmentation presents challenges other than simply selecting the
correct class, specialization can occur in terms of shape and frequency of predicted segments in
addition to class divisions; however, we do still see some class biases ? network 2 captures cows,
tables, and sofas well and network 4 has become an expert on sheep and horses.
Figure 6 shows qualitative results from a four member sMCL ensemble. We can clearly observe
the diversity in the segmentations predicted by different members. In the first row, we see the
majority of the ensemble members produce dining tables of various completeness in response to the
visual uncertainty caused by the clutter. Networks 2 and 3 capture this ambiguity well, producing
segmentations with the dining table completely present or absent. Row 2 demonstrates the capacity
of sMCL ensembles to provide multiple high quality solutions. The models are confused whether the

75

sMCL

MCL

Dey [5]

Indp.

Net ?1
Net ?2

70

Net ?3

65
Net ?4

60
1

2

3

4

5

6

Ensemble Size M

(a) Effect of Ensemble Size
(b) Oracle Assignment Distributions by Class
Figure 5: a) sMCL trained ensembles consistently result in improved oracle mean IoU over baselines on PASCAL
VOC 2011. b) Distribution of examples from each category assigned by the oracle for an sMCL ensemble.

6

Independent
Ensemble Oracle

sMCL Ensemble Predictions

IoU 82.64

IoU 77.11

IoU 88.12

IoU 58.70

IoU 52.78

IoU 54.26

IoU 56.45

IoU 62.03

IoU 47.68

IoU 37.73

IoU 20.31

IoU 21.34

IoU 14.17

IoU 94.55

IoU 19.18

Net 1

Net 2

Net 3

Net 4

Input

Figure 6: Samples images and corresponding predictions obtained by each member of the sMCL ensemble as
well as the top output of a classical ensemble. The output with minimum loss on each example is outlined in red.
Notice that sMCL ensembles vary in the shape, class, and frequency of predicted segments.

animal is a horse or a cow ? models 1 and 3 produce typical ?safe? responses while models 2 and 4
attempt to give cohesive responses. Finally, row 3 shows how the models can learn biases about the
frequency of segments with model 3 presenting only the sheep.
4.3

Image Captioning

In this section, we show that sMCL trained ensembles can produce sets of high quality and diverse
sentences, which is essential to improving recall and capturing ambiguities in language and perception.
Model. We adopt the model and training procedure of Karpathy et al. [14], utilizing their publicly
available implementation neuraltalk2. The model consists of an VGG16 network [4] which encodes
the input image as a fixed-length representation for a Long Short-Term Memory (LSTM) language
model. We train and test on the MSCOCO dataset [18], using the same splits as [14]. We perform two
experimental setups by either freezing or finetuning the CNN. In the first, we freeze the parameters
of the CNN and train multiple LSTM models using the CNN as a static feature generator. In the
second, we aggregate and back-propagate the gradients from each LSTM model through the CNN in
a tree-like model structure. This is largely a construct of memory restrictions as our hardware could
not accommodate multiple VGG16 networks. We train each ensemble for 70k iterations with the
parameters of the CNN fixed. For the fine-tuning experiments, we perform another 70k iterations of
training to fine-tune the CNN. We generate sentences for testing by performing beam search with a
beam width of two (following [14]).
Results. Table 1 presents the oracle CIDEr-D [28] scores for all methods on the validation set. We
additionally compare with all outputs of a beam search over a single CNN+LSTM model with beam
width ranging from 1 to 5. sMCL significantly outperforms the baseline ensemble learning methods
(shown in the upper section of the table), increasing both oracle performance and the number of
unique n-grams. For M = 5, beam search from a single model achieves greater oracle but produces
Oracle CIDEr-D for Ensemble of Size

# Unique n-Grams (M=5)

M=1

2

3

4

5

n=1

2

3

4

Avg.
Length

sMCL
MCL [8]
Dey [5]
Indp.

0.684

0.822
0.752
0.798
0.757

0.862
0.81
0.850
0.784

0.911
0.823
0.887
0.809

0.922
0.852
0.910
0.831

713
384
584
540

2902
1565
2266
2003

6464
3586
4969
4312

15427
9551
12208
10297

10.21
9.87
10.26
10.24

sMCL (fine-tuned CNN)
Indp. (fine-tuned CNN)

0.912

1.064
1.001

1.130
1.05

1.179
1.073

1.184
1.095

1135
921

6028
4335

15184
10534

35518
23811

10.43
10.33

Beam Search

0.654

0.754

0.833

0.888

0.943

580

2272

4920

12920

10.62

Table 1: sMCL base methods outperform other ensemble methods a captioning, improve both oracle performance
and the number of distinct n-grams. For low M, sMCL also performs better than multiple-output decoders.

7

Input

Independently Trained Networks

sMCL Ensemble

A man riding a wave on top of a surfboard.
A man riding a wave on top of a surfboard.
A man riding a wave on top of a surfboard.
A man riding a wave on top of a surfboard.

A man riding a wave on top of a surfboard.
A person on a surfboard in the water.
A surfer is riding a wave in the ocean.
A surfer riding a wave in the ocean.

A group of people standing on a sidewalk.
A man is standing in the middle of the street.
A group of people standing around a fire hydrant.
A group of people standing around a fire hydrant

A man is walking down the street with an umbrell.
A group of people sitting at a table with umbrellas.
A group of people standing around a large plane.
A group of people standing in front of a building

A kitchen with a stove and a microwave.
A white refrigerator freezer sitting inside of a kitchen.
A white refrigerator sitting next to a window.
A white refrigerator freezer sitting in a kitchen

A cat sitting on a chair in a living room.
A kitchen with a stove and a sink.
A cat is sitting on top of a refrigerator.
A cat sitting on top of a wooden table

A bird is sitting on a tree branch.
A bird is perched on a branch in a tree.
A bird is perched on a branch in a tree.
A bird is sitting on a tree branch

A small bird perched on top of a tree branch.
A couple of birds that are standing in the grass.
A bird perched on top of a branch.
A bird perched on a tree branch in the sky

Figure 7: Comparison of sentences generated by members of a standard independently trained ensemble and an
sMCL based ensemble of size four.

significantly fewer unique n-grams. We note that beam search is an inference method and increased
beam width could provide similar benefits for sMCL ensembles.
Intepretable Expertise: sMCL as N-Gram Specialists. Figure 7 shows example images and generated captions from standard and sMCL ensembles of size four (results from beam search over a
single model are similar). It is evident that the independently trained models tend to predict similar
sentences independent of initialization, perhaps owing to the highly structured nature of the output
space and the mode bias of the underlying language model. On the other hand, the sMCL based
ensemble generates diverse sentences which capture ambiguity both in language and perception. The
first row shows an extreme case in which all of the members of the standard ensemble predict identical
sentences. In contrast, the sMCL ensemble produces sentences that describe the scene with many
different structures. In row three, both models are confused about the content of the image, mistaking
the pile of suitcases as kitchen appliances. However, the sMCL ensemble widens the scope of some
sentences to include the cat clearly depicted in the image. The fourth row is an example of regression
towards the mode, with the standard model producing multiple similar sentences describing birds on
branches. In the sMCL ensemble, we also see this tendency; however, one model breaks away and
captures the true content of the image.

5

Conclusion

To summarize, we propose Stochastic Multiple Choice Learning (sMCL), an SGD-based technique
for training diverse deep ensembles that follows a ?winner-take-gradient? training strategy. Our
experiments demonstrate the broad applicability and efficacy of sMCL for training diverse deep
ensembles. In all experimental settings, sMCL significantly outperforms classical ensembles and
other strong baselines including the 5x slower MCL procedure. Our analysis shows that exactly the
same algorithm (sMCL) automatically generates specializations among ensemble members along
different task-specific dimensions. sMCL is simple to implement, agnostic to both architecture and
loss function, parameter free, and simply involves introducing one new sMCL layer into existing
ensemble architectures.
Acknowledgments
This work was supported in part by a National Science Foundation CAREER award, an Army Research Office YIP
award, ICTAS Junior Faculty award, Office of Naval Research grant N00014-14-1-0679, Google Faculty Research
award, AWS in Education Research grant, and NVIDIA GPU donation, all awarded to DB, and by an NSF CAREER
award (IIS-1253549), the Intelligence Advanced Research Projects Activity (IARPA) via Air Force Research Laboratory
contract FA8650-12-C-7212, a Google Faculty Research award, and an NVIDIA GPU donation, all awarded to DC.
Computing resources used by this work are supported in part by NSF (ACI-0910812 and CNS-0521433), the Lily
Endowment, Inc., and the Indiana METACyt Initiative. The U.S. Government is authorized to reproduce and distribute

8

reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and
conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the
official policies or endorsements, either expressed or implied, of IARPA, AFRL, NSF, or the U.S. Government.

References
[1] CIFAR-10 Quick Network Tutorial. http://caffe.berkeleyvision.org/gathered/examples/cifar10.html, 2016.
[2] K. Ahmed, M. H. Baig, and L. Torresani. Network of experts for large-scale image categorization. In
arXiv preprint arXiv:1604.06119, 2016.
[3] D. Batra, P. Yadollahpour, A. Guzman-Rivera, and G. Shakhnarovich. Diverse M-Best Solutions in Markov
Random Fields. In Proceedings of European Conference on Computer Vision (ECCV), 2012.
[4] K. Chatfield, K. Simonyan, A. Vedaldi, and A. Zisserman. Return of the devil in the details: Delving deep
into convolutional nets. arXiv preprint arXiv:1405.3531, 2014.
[5] D. Dey, V. Ramakrishna, M. Hebert, and J. Andrew Bagnell. Predicting multiple structured visual
interpretations. In Proceedings of IEEE International Conference on Computer Vision (ICCV), 2015.
[6] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman.
The
PASCAL Visual Object Classes Challenge 2011 (VOC2011) Results.
http://www.pascalnetwork.org/challenges/VOC/voc2011/workshop/index.html.
[7] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets Robotics: The KITTI Dataset. International
Journal of Robotics Research (IJRR), 2013.
[8] A. Guzman-Rivera, D. Batra, and P. Kohli. Multiple Choice Learning: Learning to Produce Multiple
Structured Outputs. In Advances in Neural Information Processing Systems (NIPS), 2012.
[9] A. Guzman-Rivera, P. Kohli, D. Batra, and R. Rutenbar. Efficiently enforcing diversity in multi-output
structured prediction. In Proceedings of the International Conference on Artificial Intelligence and
Statistics (AISTATS), 2014.
[10] B. Hariharan, P. Arbelaez, L. Bourdev, S. Maji, and J. Malik. Semantic contours from inverse detectors. In
Proceedings of IEEE International Conference on Computer Vision (ICCV), 2011.
[11] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. arXiv preprint
arXiv:1512.03385, 2015.
[12] G. E. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. In Advances in Neural
Information Processing Systems (NIPS) - Deep Learning Workshop, 2014.
[13] Y. Jia. Caffe: An open source convolutional architecture for fast feature embedding. http://caffe.
berkeleyvision.org/, 2013.
[14] A. Karpathy and L. Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In
Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.
[15] A. Kirillov, B. Savchynskyy, D. Schlesinger, D. Vetrov, and C. Rother. Inferring m-best diverse solutions
in a single one. In Proceedings of IEEE International Conference on Computer Vision (ICCV), 2015.
[16] A. Kirillov, D. Schlesinger, D. Vetrov, C. Rother, and B. Savchynskyy. M-best-diverse labelings for
submodular energies and beyond. In Advances in Neural Information Processing Systems (NIPS), 2015.
[17] A. Krizhevsky. Learning multiple layers of features from tiny images, 2009.
[18] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll?r, and C. L. Zitnick. Microsoft
COCO: Common objects in context, 2014.
[19] Y. Liu and X. Yao. Ensemble learning via negative correlation. Neural Networks, 12(10):1399?1404, 1999.
[20] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In
Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.
[21] P. Melville and R. J. Mooney. Creating diversity in ensembles using artificial data. Information Fusion,
6(1):99?111, 2005.
[22] Microsoft. Decades of computer vision research, one ?Swiss Army knife?. blogs.microsoft.com
/next/2016/03/30/decades-of-computer-vision-research-one-swiss-army-knife/, 2016.
[23] D. Park and D. Ramanan. N-best maximal decoders for part models. In Proceedings of IEEE International
Conference on Computer Vision (ICCV), pages 2627?2634, 2011.
[24] A. Prasad, S. Jegelka, and D. Batra. Submodular meets structured: Finding diverse subsets in exponentiallylarge structured item sets. In Advances in Neural Information Processing Systems (NIPS), 2014.
[25] O. Russakovsky, J. Deng, J. Krause, A. Berg, and L. Fei-Fei. The ImageNet Large Scale Visual Recognition
Challenge 2012 (ILSVRC2012). http://www.image-net.org/challenges/LSVRC/2012/.
[26] A. Strehl and J. Ghosh. Cluster ensembles?a knowledge reuse framework for combining multiple
partitions. The Journal of Machine Learning Research, 3:583?617, 2003.
[27] K. Tumer and J. Ghosh. Error correlation and error reduction in ensemble classifiers. Connection Science,
8(3-4):385?404, 1996.
[28] R. Vedantam, C. Lawrence Zitnick, and D. Parikh. Cider: Consensus-based image description evaluation.
In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015.
[29] WIRED. Facebook?s AI Can Caption Photos for the Blind on Its Own. wired.com/2015/10/facebookartificial-intelligence-describes-photo-captions-for-blind-people/, 2015.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 5059-compete-to-compute.pdf

Compete to Compute

Rupesh Kumar Srivastava, Jonathan Masci, Sohrob Kazerounian,
Faustino Gomez, J?rgen Schmidhuber
IDSIA, USI-SUPSI
Manno?Lugano, Switzerland
{rupesh, jonathan, sohrob, tino, juergen}@idsia.ch

Abstract
Local competition among neighboring neurons is common in biological neural networks (NNs). In this paper, we apply the concept to gradient-based,
backprop-trained artificial multilayer NNs. NNs with competing linear
units tend to outperform those with non-competing nonlinear units, and
avoid catastrophic forgetting when training sets change over time.

1

Introduction

Although it is often useful for machine learning methods to consider how nature has arrived
at a particular solution, it is perhaps more instructive to first understand the functional
role of such biological constraints. Indeed, artificial neural networks, which now represent
the state-of-the-art in many pattern recognition tasks, not only resemble the brain in a
superficial sense, but also draw on many of its computational and functional properties.
One of the long-studied properties of biological neural circuits which has yet to fully impact
the machine learning community is the nature of local competition. That is, a common
finding across brain regions is that neurons exhibit on-center, off-surround organization
[1, 2, 3], and this organization has been argued to give rise to a number of interesting
properties across networks of neurons, such as winner-take-all dynamics, automatic gain
control, and noise suppression [4].
In this paper, we propose a biologically inspired mechanism for artificial neural networks
that is based on local competition, and ultimately relies on local winner-take-all (LWTA)
behavior. We demonstrate the benefit of LWTA across a number of different networks and
pattern recognition tasks by showing that LWTA not only enables performance comparable
to the state-of-the-art, but moreover, helps to prevent catastrophic forgetting [5, 6] common
to artificial neural networks when they are first trained on a particular task, then abruptly
trained on a new task. This property is desirable in continual learning wherein learning
regimes are not clearly delineated [7]. Our experiments also show evidence that a type of
modularity emerges in LWTA networks trained in a supervised setting, such that different
modules (subnetworks) respond to different inputs. This is beneficial when learning from
multimodal data distributions as compared to learning a monolithic model.
In the following, we first discuss some of the relevant neuroscience background motivating
local competition, then show how we incorporate it into artificial neural networks, and
how LWTA, as implemented here, compares to alternative methods. We then show how
LWTA networks perform on a variety of tasks, and how it helps buffer against catastrophic
forgetting.

2

Neuroscience Background

Competitive interactions between neurons and neural circuits have long played an important
role in biological models of brain processes. This is largely due to early studies showing that
1

many cortical [3] and sub-cortical (e.g., hippocampal [1] and cerebellar [2]) regions of the
brain exhibit a recurrent on-center, off-surround anatomy, where cells provide excitatory
feedback to nearby cells, while scattering inhibitory signals over a broader range. Biological
modeling has since tried to uncover the functional properties of this sort of organization,
and its role in the behavioral success of animals.
The earliest models to describe the emergence of winner-take-all (WTA) behavior from local
competition were based on Grossberg?s shunting short-term memory equations [4], which
showed that a center-surround structure not only enables WTA dynamics, but also contrast
enhancement, and normalization. Analysis of their dynamics showed that networks with
slower-than-linear signal functions uniformize input patterns; linear signal functions preserve
and normalize input patterns; and faster-than-linear signal functions enable WTA dynamics.
Sigmoidal signal functions which contain slower-than-linear, linear, and faster-than-linear
regions enable the supression of noise in input patterns, while contrast-enhancing, normalizing and storing the relevant portions of an input pattern (a form of soft WTA). The
functional properties of competitive interactions have been further studied to show, among
other things, the effects of distance-dependent kernels [8], inhibitory time lags [8, 9], development of self-organizing maps [10, 11, 12], and the role of WTA networks in attention [13].
Biological models have also been extended to show how competitive interactions in spiking
neural networks give rise to (soft) WTA dynamics [14], as well as how they may be efficiently
constructed in VLSI [15, 16].
Although competitive interactions, and WTA dynamics have been studied extensively in the
biological literature, it is only more recently that they have been considered from computational or machine learning perspectives. For example, Maas [17, 18] showed that feedforward
neural networks with WTA dynamics as the only non-linearity are as computationally powerful as networks with threshold or sigmoidal gates; and, networks employing only soft
WTA competition are universal function approximators. Moreover, these results hold, even
when the network weights are strictly positive?a finding which has ramifications for our
understanding of biological neural circuits, as well as the development of neural networks
for pattern recognition. The large body of evidence supporting the advantages of locally
competitive interactions makes it noteworthy that this simple mechanism has not provoked
more study by the machine learning community. Nonetheless, networks employing local
competition have existed since the late 80s [21], and, along with [22], serve as a primary
inspiration for the present work. More recently, maxout networks [19] have leveraged locally
competitive interactions in combination with a technique known as dropout [20] to obtain
the best results on certain benchmark problems.

3

Networks with local winner-take-all blocks

This section describes the general network architecture with locally competing neurons.
The network consists of B blocks which are organized into layers (Figure 1). Each block,
bi , i = 1..B, contains n computational units (neurons), and produces an output vector yi ,
determined by the local interactions between the individual neuron activations in the block:
yij = g(h1i , h2i ..., hni ),

(1)

where g(?) is the competition/interaction function, encoding the effect of local interactions
in each block, and hji , j = 1..n, is the activation of the j-th neuron in block i computed by:
T
hi = f (wij
x),

(2)

where x is the input vector from neurons in the previous layer, wij is the weight vector of
neuron j in block i, and f (?) is a (generally non-linear) activation function. The output
activations y are passed as inputs to the next layer. In this paper we use the winner-take-all
interaction function, inspired by studies in computational neuroscience. In particular, we
use the hard winner-take-all function:
 j
hi if hji ? hki , ?k = 1..n
yij =
0 otherwise.
In the case of multiple winners, ties are broken by index precedence. In order to investigate the capabilities of the hard winner-take-all interaction function in isolation, f (x) = x
2

Figure 1: A Local Winner-Take-All (LWTA) network with blocks of size two showing the
winning neuron in each block (shaded) for a given input example. Activations flow forward
only through the winning neurons, errors are backpropagated through the active neurons.
Greyed out connections do not propagate activations. The active neurons form a subnetwork
of the full network which changes depending on the inputs.

(identity) is used for the activation function in equation (2). The difference between this
Local Winner Take All (LWTA) network and a standard multilayer perceptron is that no
non-linear activation functions are used, and during the forward propagation of inputs, local
competition between the neurons in each block turns off the activation of all neurons except
the one with the highest activation. During training the error signal is only backpropagated
through the winning neurons.
In a LWTA layer, there are as many neurons as there are blocks active at any one time for
a given input pattern1 . We denote a layer with blocks of size n as LWTA-n. For each input
pattern presented to a network, only a subgraph of the full network is active, e.g. the highlighted neurons and synapses in figure 1. Training on a dataset consists of simultaneously
training an exponential number of models that share parameters, as well as learning which
model should be active for each pattern. Unlike networks with sigmoidal units, where all of
the free parameters need to be set properly for all input patterns, only a subset is used for
any given input, so that patterns coming from very different sub-distributions can potentially be modelled more efficiently through specialization. This modular property is similar
to that of networks with rectified linear units (ReLU) which have recently been shown to
be very good at several learning tasks (links with ReLU are discussed in section 4.3).

4
4.1

Comparison with related methods
Max-pooling

Neural networks with max-pooling layers [23] have been found to be very useful, especially
for image classification tasks where they have achieved state-of-the-art performance [24, 25].
These layers are usually used in convolutional neural networks to subsample the representation obtained after convolving the input with a learned filter, by dividing the representation
into pools and selecting the maximum in each one. Max-pooling lowers the computational
burden by reducing the number of connections in subsequent convolutional layers, and adds
translational/rotational invariance.
1
However, there is always the possibility that the winning neuron in a block has an activation
of exactly zero, so that the block has no output.

3

0.5

0.5

0

0.8

0.8

before

after

0.8
0.8

before

after

(a) max-pooling

(b) LWTA

Figure 2: Max-pooling vs. LWTA. (a) In max-pooling, each group of neurons in a layer
has a single set of output weights that transmits the winning unit?s activation (0.8 in this
case) to the next layer, i.e. the layer activations are subsampled. (b) In an LWTA block,
there is no subsampling. The activations flow into subsequent units via a different set of
connections depending on the winning unit.

At first glance, the max-pooling seems very similar to a WTA operation, however, the two
differ substantially: there is no downsampling in a WTA operation and thus the number of
features is not reduced, instead the representation is "sparsified" (see figure 2).
4.2

Dropout

Dropout [20] can be interpreted as a model-averaging technique that jointly trains several
models sharing subsets of parameters and input dimensions, or as data augmentation when
applied to the input layer [19, 20]. This is achieved by probabilistically omitting (?dropping?) units from a network for each example during training, so that those neurons do not
participate in forward/backward propagation. Consider, hypothetically, training an LWTA
network with blocks of size two, and selecting the winner in each block at random. This
is similar to training a neural network with a dropout probability of 0.5. Nonetheless, the
two are fundamentally different. Dropout is a regularization technique while in LWTA the
interaction between neurons in a block replaces the per-neuron non-linear activation.
Dropout is believed to improve generalization performance since it forces the units to learn
independent features, without relying on other units being active. During testing, when
propagating an input through the network, all units in a layer trained with dropout are
used with their output weights suitably scaled. In an LWTA network, no output scaling is
required. A fraction of the units will be inactive for each input pattern depending on their
total inputs. Viewed this way, WTA is restrictive in that only a fraction of the parameters
are utilized for each input pattern. However, we hypothesize that the freedom to use different
subsets of parameters for different inputs allows the architecture to learn from multimodal
data distributions more accurately.
4.3

Rectified Linear units

Rectified Linear Units (ReLU) are simply linear neurons that clamp negative activations to
zero (f (x) = x if x > 0, f (x) = 0 otherwise). ReLU networks were shown to be useful for
Restricted Boltzmann Machines [26], outperformed sigmoidal activation functions in deep
neural networks [27], and have been used to obtain the best results on several benchmark
problems across multiple domains [24, 28].
Consider an LWTA block with two neurons compared to two ReLU neurons, where x1 and
x2 are the weighted sum of the inputs to each neuron. Table 1 shows the outputs y1 and
y2 in all combinations of positive and negative x1 and x2 , for ReLU and LWTA neurons.
For both ReLU and LWTA neurons, x1 and x2 are passed through as output in half of the
possible cases. The difference is that in LWTA both neurons are never active or inactive at
the same time, and the activations and errors flow through exactly one neuron in the block.
For ReLU neurons, being inactive (saturation) is a potential drawback since neurons that
4

Table 1: Comparison of rectified linear activation and LWTA-2.
x1

x2

Positive
Positive
Negative

Positive
Negative
Negative

Positive
Negative
Negative

Positive
Positive
Negative

ReLU neurons
y1
y2
x1 > x2
x1
x2
x1
0
0
0
x2 > x1
x1
x2
0
x2
0
0

LWTA neurons
y1
y2
x1
x1
x1

0
0
0

0
0
0

x2
x2
x2

do not get activated will not get trained, leading to wasted capacity. However, previous
work suggests that there is no negative impact on optimization, leading to the hypothesis
that such hard saturation helps in credit assignment, and, as long as errors flow through
certain paths, optimization is not affected adversely [27]. Continued research along these
lines validates this hypothesis [29], but it is expected that it is possible to train ReLU
networks better.
While many of the above arguments for and against ReLU networks apply to LWTA networks, there is a notable difference. During training of an LWTA network, inactive neurons
can become active due to training of the other neurons in the same block. This suggests
that LWTA nets may be less sensitive to weight initialization, and a greater portion of the
network?s capacity may be utilized.

5

Experiments

In the following experiments, LWTA networks were tested on various supervised learning
datasets, demonstrating their ability to learn useful internal representations without utilizing
any other non-linearities. In order to clearly assess the utility of local competition, no special
strategies such as augmenting data with transformations, noise or dropout were used. We
also did not encourage sparse representations in the hidden layers by adding activation
penalties to the objective function, a common technique also for ReLU units. Thus, our
objective is to evaluate the value of using LWTA rather than achieving the absolute best
testing scores. Blocks of size two are used in all the experiments.2
All networks were trained using stochastic gradient descent with mini-batches, learning rate
lt and momentum mt at epoch t given by

?0 ?t if ?t > ?min
?t =
?
otherwise
 t min
m
+
(1 ? Tt )mf if t < T
i
T
mt =
pf
if t ? T
where ? is the learning rate annealing factor, ?min is the lower learning rate limit, and
momentum is scaled from mi to mf over T epochs after which it remains constant at
mf . L2 weight decay was used for the convolutional network (section 5.2), and max-norm
normalization for other experiments. This setup is similar to that of [20].
5.1

Permutation Invariant MNIST

The MNIST handwritten digit recognition task consists of 70,000 28x28 images (60,000
training, 10,000 test) of the 10 digits centered by their center of mass [33]. In the permutation
invariant setting of this task, we attempted to classify the digits without utilizing the 2D
structure of the images, e.g. every digit is a vector of pixels. The last 10,000 examples in the
training set were used for hyperparameter tuning. The model with the best hyperparameter
setting was trained until convergence on the full training set. Mini-batches of size 20 were
2

To speed up our experiments, the Gnumpy [30] and CUDAMat [31] libraries were used.

5

Table 2: Test set errors on the permutation invariant MNIST dataset for methods without
data augmentation or unsupervised pre-training
Activation
Sigmoid [32]
ReLU [27]
ReLU + dropout in hidden layers [20]
LWTA-2

Test Error
1.60%
1.43%
1.30%
1.28%

Table 3: Test set errors on MNIST dataset for convolutional architectures with no data
augmentation. Results marked with an asterisk use layer-wise unsupervised feature learning
to pre-train the network and global fine tuning.
Architecture
2-layer CNN + 2 layer MLP [34] *
2-layer ReLU CNN + 2 layer LWTA-2
3-layer ReLU CNN [35]
2-layer CNN + 2 layer MLP [36] *
3-layer ReLU CNN + stochastic pooling [33]
3-layer maxout + dropout [19]

Test Error
0.60%
0.57%
0.55%
0.53%
0.47%
0.45%

used, the pixel values were rescaled to [0, 1] (no further preprocessing). The best model
obtained, which gave a test set error of 1.28%, consisted of three LWTA layers of 500
blocks followed by a 10-way softmax layer. To our knowledge, this is the best reported
error, without utilizing implicit/explicit model averaging, for this setting which does not use
deformations/noise to enhance the dataset or unsupervised pretraining. Table 2 compares
our results with other methods which do not use unsupervised pre-training. The performance
of LWTA is comparable to that of a ReLU network with dropout in the hidden layers. Using
dropout in input layers as well, lower error rates of 1.1% using ReLU [20] and 0.94% using
maxout [19] have been obtained.
5.2

Convolutional Network on MNIST

For this experiment, a convolutional network (CNN) was used consisting of 7 ? 7 filters in
the first layer followed by a second layer of 6 ? 6, with 16 and 32 maps respectively, and
ReLU activation. Every convolutional layer is followed by a 2 ? 2 max-pooling operation.
We then use two LWTA-2 layers each with 64 blocks and finally a 10-way softmax output
layer. A weight decay of 0.05 was found to be beneficial to improve generalization. The
results are summarized in Table 3 along with other state-of-the-art approaches which do not
use data augmentation (for details of convolutional architectures, see [33]).
5.3

Amazon Sentiment Analysis

LWTA networks were tested on the Amazon sentiment analysis dataset [37] since ReLU units
have been shown to perform well in this domain [27, 38]. We used the balanced subset of the
dataset consisting of reviews of four categories of products: Books, DVDs, Electronics and
Kitchen appliances. The task is to classify the reviews as positive or negative. The dataset
consists of 1000 positive and 1000 negative reviews in each category. The text of each review
was converted into a binary feature vector encoding the presence or absence of unigrams
and bigrams. Following [27], the 5000 most frequent vocabulary entries were retained as
features for classification. We then divided the data into 10 equal balanced folds, and
tested our network with cross-validation, reporting the mean test error over all folds. ReLU
activation was used on this dataset in the context of unsupervised learning with denoising
autoencoders to obtain sparse feature representations which were used for classification. We
trained an LWTA-2 network with three layers of 500 blocks each in a supervised setting to
directly classify each review as positive or negative using a 2-way softmax output layer. We
obtained mean accuracies of Books: 80%, DVDs: 81.05%, Electronics: 84.45% and Kitchen:
85.8%, giving a mean accuracy of 82.82%, compared to 78.95% reported in [27] for denoising
autoencoders using ReLU and unsupervised pre-training to find a good initialization.
6

Table 4: LWTA networks outperform sigmoid and ReLU activation in remembering dataset
P1 after training on dataset P2.
Testing error on P1
After training on P1
After training on P2

6

LWTA
1.55 ? 0.20%
6.12 ? 3.39%

Sigmoid
1.38 ? 0.06%
57.84 ? 1.13%

ReLU
1.30 ? 0.13%
16.63 ? 6.07%

Implicit long term memory

This section examines the effect of the LWTA architecture on catastrophic forgetting. That
is, does the fact that the network implements multiple models allow it to retain information
about dataset A, even after being trained on a different dataset B? To test for this implicit
long term memory, the MNIST training and test sets were each divided into two parts, P1
containing only digits {0, 1, 2, 3, 4}, and P2 consisting of the remaining digits {5, 6, 7, 8, 9}.
Three different network architectures were compared: (1) three LWTA layers each with 500
blocks of size 2, (2) three layers each with 1000 sigmoidal neurons, and (3) three layers each
of 1000 ReLU neurons. All networks have a 5-way softmax output layer representing the
probability of an example belonging to each of the five classes. All networks were initialized
with the same parameters, and trained with a fixed learning rate and momentum.
Each network was first trained to reach a 0.03 log-likelihood error on the P1 training set.
This value was chosen heuristically to produce low test set errors in reasonable time for
all three network types. The weights for the output layer (corresponding to the softmax
classifier) were then stored, and the network was trained further, starting with new initial
random output layer weights, to reach the same log-likelihood value on P2. Finally, the
output layer weights saved from P1 were restored, and the network was evaluated on the
P1 test set. The experiment was repeated for 10 different initializations.
Table 4 shows that the LWTA network remembers what was learned from P1 much better
than sigmoid and ReLU networks, though it is notable that the sigmoid network performs
much worse than both LWTA and ReLU. While the test error values depend on the learning
rate and momentum used, LWTA networks tended to remember better than the ReLU
network by about a factor of two in most cases, and sigmoid networks always performed
much worse. Although standard network architectures are known to suffer from catastrophic
forgetting, we not only show here, for the first time, that ReLU networks are actually quite
good in this regard, and moreover, that they are outperformed by LWTA. We expect this
behavior to manifest itself in competitive models in general, and to become more pronounced
with increasingly complex datasets. The neurons encoding specific features in one dataset
are not affected much during training on another dataset, whereas neurons encoding common
features can be reused. Thus, LWTA may be a step forward towards models that do not
forget easily.

7

Analysis of subnetworks

A network with a single LWTA-m of N blocks consists of mN subnetworks which can be
selected and trained for individual examples while training over a dataset. After training,
we expect the subnetworks consisting of active neurons for examples from the same class to
have more neurons in common compared to subnetworks being activated for different classes.
In the case of relatively simple datasets like MNIST, it is possible to examine the number
of common neurons between mean subnetworks which are used for each class. To do this,
which neurons were active in the layer for each example in a subset of 10,000 examples were
recorded. For each class, the subnetwork consisting of neurons active for at least 90% of the
examples was designated the representative mean subnetwork, which was then compared to
all other class subnetworks by counting the number of neurons in common.
Figure 3a shows the fraction of neurons in common between the mean subnetworks of each
pair of digits. Digits that are morphologically similar such as ?3? and ?8? have subnetworks
with more neurons in common than the subnetworks for digits ?1? and ?2? or ?1? and ?5?
which are intuitively less similar. To verify that this subnetwork specialization is a result
of training, we looked at the fraction of common neurons between all pairs of digits for the
7

untrained
trained

0.4

0.7
0.6
0.5
0.4

0.3

0.3

0.2
0

1

2

3

4 5 6
Digits

7

8

9

0.2
0

10

20

30

40

50

Fraction of neurons in common

Digits

0
1
2
3
4
5
6
7
8
9

0.1

MNIST digit pairs
(b)

(a)

Figure 3: (a) Each entry in the matrix denotes the fraction of neurons that a pair of MNIST
digits has in common, on average, in the subnetworks that are most active for each of the
two digit classes. (b) The fraction of neurons in common in the subnetworks of each of the
55 possible digit pairs, before and after training.
same 10000 examples both before and after training (Figure 3b). Clearly, the subnetworks
were much more similar prior to training, and the full network has learned to partition its
parameters to reflect the structure of the data.

8

Conclusion and future research directions

Our LWTA networks automatically self-modularize into multiple parameter-sharing subnetworks responding to different input representations. Without significant degradation of
state-of-the-art results on digit recognition and sentiment analysis, LWTA networks also
avoid catastrophic forgetting, thus retaining useful representations of one set of inputs even
after being trained to classify another. This has implications for continual learning agents
that should not forget representations of parts of their environment when being exposed to
other parts. We hope to explore many promising applications of these ideas in the future.
Acknowledgments
This research was funded by EU projects WAY (FP7-ICT-288551), NeuralDynamics (FP7ICT-270247), and NASCENCE (FP7-ICT-317662); additional funding from ArcelorMittal.

References
[1] Per Anderson, Gary N. Gross, Terje L?mo, and Ola Sveen. Participation of inhibitory and
excitatory interneurones in the control of hippocampal cortical output. In Mary A.B. Brazier,
editor, The Interneuron, volume 11. University of California Press, Los Angeles, 1969.
[2] John Carew Eccles, Masao Ito, and J?nos Szent?gothai. The cerebellum as a neuronal machine.
Springer-Verlag New York, 1967.
[3] Costas Stefanis. Interneuronal mechanisms in the cortex. In Mary A.B. Brazier, editor, The
Interneuron, volume 11. University of California Press, Los Angeles, 1969.
[4] Stephen Grossberg. Contour enhancement, short-term memory, and constancies in reverberating neural networks. Studies in Applied Mathematics, 52:213?257, 1973.
[5] Michael McCloskey and Neal J. Cohen. Catastrophic interference in connectionist networks:
The sequential learning problem. The Psychology of Learning and Motivation, 24:109?164,
1989.
[6] Gail A. Carpenter and Stephen Grossberg. The art of adaptive pattern recognition by a
self-organising neural network. Computer, 21(3):77?88, 1988.
[7] Mark B. Ring. Continual Learning in Reinforcement Environments. PhD thesis, Department
of Computer Sciences, The University of Texas at Austin, Austin, Texas 78712, August 1994.
[8] Samuel A. Ellias and Stephen Grossberg. Pattern formation, contrast control, and oscillations
in the short term memory of shunting on-center off-surround networks. Bio. Cybernetics, 1975.
[9] Brad Ermentrout. Complex dynamics in winner-take-all neural nets with slow inhibition.
Neural Networks, 5(1):415?431, 1992.

8

[10] Christoph von der Malsburg. Self-organization of orientation sensitive cells in the striate cortex.
Kybernetik, 14(2):85?100, December 1973.
[11] Teuvo Kohonen. Self-organized formation of topologically correct feature maps. Biological
cybernetics, 43(1):59?69, 1982.
[12] Risto Mikkulainen, James A. Bednar, Yoonsuck Choe, and Joseph Sirosh. Computational maps
in the visual cortex. Springer Science+ Business Media, 2005.
[13] Dale K. Lee, Laurent Itti, Christof Koch, and Jochen Braun. Attention activates winner-takeall competition among visual filters. Nature Neuroscience, 2(4):375?81, April 1999.
[14] Matthias Oster and Shih-Chii Liu. Spiking inputs to a winner-take-all network. In Proceedings
of NIPS, volume 18. MIT; 1998, 2006.
[15] John P. Lazzaro, Sylvie Ryckebusch, Misha Anne Mahowald, and Caver A. Mead. Winnertake-all networks of O(n) complexity. Technical report, 1988.
[16] Giacomo Indiveri. Modeling selective attention using a neuromorphic analog VLSI device.
Neural Computation, 12(12):2857?2880, 2000.
[17] Wolfgang Maass. Neural computation with winner-take-all as the only nonlinear operation. In
Proceedings of NIPS, volume 12, 1999.
[18] Wolfgang Maass. On the computational power of winner-take-all. Neural Computation,
12:2519?2535, 2000.
[19] Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio.
Maxout networks. In Proceedings of the ICML, 2013.
[20] Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R.
Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors,
2012. arXiv:1207.0580.
[21] Juergen Schmidhuber. A local learning algorithm for dynamic feedforward and recurrent
networks. Connection Science, 1(4):403?412, 1989.
[22] Rupesh K. Srivastava, Bas R. Steunebrink, and Juergen Schmidhuber. First experiments with
powerplay. Neural Networks, 2013.
[23] Maximillian Riesenhuber and Tomaso Poggio. Hierarchical models of object recognition in
cortex. Nature Neuroscience, 2(11), 1999.
[24] Alex Krizhevsky, Ilya Sutskever, and Goeffrey E. Hinton. Imagenet classification with deep
convolutional neural networks. In Proceedings of NIPS, pages 1?9, 2012.
[25] Dan Ciresan, Ueli Meier, and J?rgen Schmidhuber. Multi-column deep neural networks for
image classification. Proceeedings of the CVPR, 2012.
[26] Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the ICML, number 3, 2010.
[27] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier networks. In AISTATS, volume 15, pages 315?323, 2011.
[28] George E. Dahl, Tara N. Sainath, and Geoffrey E. Hinton. Improving Deep Neural Networks
for LVCSR using Rectified Linear Units and Dropout. In Proceedings of ICASSP, 2013.
[29] Andrew L. Maas, Awni Y. Hannun, and Andrew Y. Ng. Rectifier nonlinearities improve neural
network acoustic models. In Proceedings of the ICML, 2013.
[30] Tijmen Tieleman. Gnumpy: an easy way to use GPU boards in Python. Department of
Computer Science, University of Toronto, 2010.
[31] Volodymyr Mnih. CUDAMat: a CUDA-based matrix class for Python. Department of Computer Science, University of Toronto, Tech. Rep. UTML TR, 4, 2009.
[32] Patrice Y. Simard, Dave Steinkraus, and John C. Platt. Best practices for convolutional
neural networks applied to visual document analysis. In International Conference on Document
Analysis and Recognition (ICDAR), 2003.
[33] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning
applied to document recognition. Proceedings of the IEEE, 1998.
[34] Marc?Aurelio Ranzato, Christopher Poultney, Sumit Chopra, and Yann LeCun. Efficient learning of sparse representations with an energy-based model. In Proceedings of NIPS, 2007.
[35] Matthew D. Zeiler and Rob Fergus. Stochastic pooling for regularization of deep convolutional
neural networks. In Proceedings of the ICLR, 2013.
[36] Kevin Jarrett, Koray Kavukcuoglu, Marc?Aurelio Ranzato, and Yann LeCun. What is the best
multi-stage architecture for object recognition? In Proc. of the ICCV, pages 2146?2153, 2009.
[37] John Blitzer, Mark Dredze, and Fernando Pereira. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classification. Annual Meeting-ACL, 2007.
[38] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Domain adaptation for large-scale sentiment classification: A deep learning approach. In Proceedings of the ICML, number 1, 2011.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 1541-active-noise-canceling-using-analog-neuro-chip-with-on-chip-learning-capability.pdf

Active Noise Canceling using Analog NeuroChip with On-Chip Learning Capability
Jung-Wook Cho and Soo-Young Lee
Computation and Neural Systems Laboratory
Department of Electrical Engineering
Korea Advanced Institute of Science and Technology
373-1 Kusong-dong, Yusong-gu, Taejon 305-701, Korea
sylee@ee.kaist.ac.kr

Abstract
A modular analogue neuro-chip set with on-chip learning capability is
developed for active noise canceling. The analogue neuro-chip set
incorporates the error backpropagation learning rule for practical
applications, and allows pin-to-pin interconnections for multi-chip
boards. The developed neuro-board demonstrated active noise
canceling without any digital signal processor. Multi-path fading of
acoustic channels, random noise, and nonlinear distortion of the loud
speaker are compensated by the adaptive learning circuits of the
neuro-chips. Experimental results are reported for cancellation of car
noise in real time.

1

INTRODUCTION

Both analog and digital implementations of neural networks have been reported.
Digital neuro-chips can be designed and fabricated with the help of well-established
CAD tools and digital VLSI fabrication technology [1]. Although analogue neurochips have potential advantages on integration density and speed over digital chips[2],
they suffer from non-ideal characteristics of the fabricated chips such as offset and
nonlinearity, and the fabricated chips are not flexible enough to be used for many
different applications. Also, much careful design is required, and the fabricated chip
characteristics are fairly dependent upon fabrication processes.
For the implementation of analog neuro-chips, there exist two different approaches, i.e.,
with and without on-chip learning capability [3,4], Currently the majority of analog
neuro-chips does not have learning capability, while many practical applications
require on-line adaptation to continuously changing environments, and must have online adaptation learning capability. Therefore neuro-chips with on-chip learning
capability are essential for such practical applications. Modular architecture is also

665

Active Noise Canceling with Analog On-Chip Learning Neuro-Chip

advantageous to provide flexibility of implementing many large complex systems from
same chips.
Although many applications have been studied for analog neuro-chips, it is very
important to find proper problems where analog neuro-chips may have potential
advantages over popular DSPs. We believe applications with analog input/output
signals and high computational requirements are those good problems. For example,
active noise controls [5] and adaptive equalizers [6,7] are good applications for analog
neuro-chips.
In this paper we report a demonstration of the active noise canceling, which may have
many applications in real world. A modular analog neuro-chip set is developed with
on-chip learning capability, and a neuro-board is fabricated from multiple chips with
PC interfaces for input and output measurements. Unlike our previous implementations
for adaptive equalizers with binary outputs [7], both input and output values are
analogue in this noise canceling.

..-.---1-1.... 0

II'

.

xl, ~t---(~.-~-.lI'"'"'"iI--- (~

Figure 1. Block diagram of a synapse cell

2

~

._._.- i')

Figure 2. Block diagram of a neuron cell

ANALOG NEURO-CHIP WITH ON-CHIP LEARNING

We had developed analog neuro-chips with error backpropagation learning capability.
With the modular architecture the developed analog neuro-chip set consists of a
synapse chip and a neuron chip.[8] The basic cell of the synapse chip is shown in
Figure 1. Each synapse cell receives two inputs, i.e., pre-synaptic neural activation x
and error correction term 8, and generates two outputs, i.e., feed-forward signal wx and
back-propagated error w8. Also it updates a stored weight w by the amount of x8.
Therefore, a synapse cell consists of three multiplier circuits and one analogue storage
for the synaptic weight. Figure 2 shows the basic cell in the neuron chip, which collects
signals from synapses in the previous layer and distributes to synapses in the following
layer. Each neuron body receives two inputs, i.e., post-synaptic neural activation 0 and
back-propagated error 8 from the following layer, and generates two outputs, i.e.,
Sigmoid-squashed neural activation 0 and a new backpropagated error 8 multiplied by
a bell-shaped Sigmoid-derivative. The backpropagated error may be input to the
synapse cells in the previous layer.
To provide easy connectivity with other chips, the two inputs of the synapse cell are
represented as voltage, while the two outputs are as currents for simple current
summation. On the other hand the inputs and outputs of the neuron cell are represented
as currents and voltages, respectively. For simple pin-to-pin connections between chips,
one package pin is maintained to each input and output of the chip. No time-

J.-W Cho and s.-Y. Lee

666

multiplexing is introduced, and no other control is required for multi-chip and multilayer systems. However, it makes the number of package pins the main limiting factor
for the number of synapse and neuron cells in the developed chip sets.
Although many simplified multipliers had been reported for high-density integration,
their performance is limited in linearity, resolution, and speed. For on-chip learning, it
is desirable to have high precision, and a faithful implementation of the 4-quadranr
Gilbert multipliers is used. Especially, the mUltiplier for weight updates in the synapse
cell requires high precision.[9] The synaptic weight is stored on a capacitor, and an
MaS switch is used to allow current flow from the multiplier to the capacitor during a
short time interval for weight adaptation. For applications like active noise controls [5]
and telecommunications [6,7], tapped analog delay lines are also designed and
integrated in the synapse chip. To reduce offset accumulation, a parallel analog delay
line is adopted. Same offset voltage is introduced for operational amplifiers at all
nodes [10] . Diffusion capacitors with 2.2 pF are used for the storage of the tapped
analog delay line.
In a synapse chip 250 synapse cells are integrated in a 25xl0 array with a 25-tap
analog delay line. Inputs may be applied either from the analog delay line or from
external pins in parallel. To select a capacitor in the cell for refresh, decoders are
placed in columns and rows. The actual size of the synapse cell is 14111m x 17911m,
and the size of the synapse chip is 5.05mm x 5.05mm. The chip is fabricated in a
0.811m single-poly CMOS process. On the other hand, the neuron chip has a very
simple structure, which consists of 20 neuron cells without additional circuits. The
Sigmoid circuit [3] in the neuron cell uses a differential pair, and the slope and
amplitude are controlled by a voltage-controlled resistor [II]. Sigmoid-derivative
circuit is also using differential pair with min-select circuit. The size of the neuron cell
is 177.2I1m x 62.4l1m.

Synapse
Chip

PC

Neuron

PC

Chip

,

~ _ ' Target
I

N t-.,.' ----t~
I

I

DSP
TMS320C51

N

I

Output

:

-:-r!1"iJ-[B~h:---'-:_._.-.____;--..; r:1-r:"1'
L!..r~
..._~Q_-I_""',_ _
I

Input

- L -_ _ _--I

ANN Board
Figure 3: Block diagram of the analog neuro-board

GDAB
tv.c : 32ch
ArIC : 48<:h
D1 : 16bitll
DO : 48bitll

Active Noise Canceling with Analog On-Chip Learning Neuro-Chip

667

Using these chip sets, an analog neuro-system is constructed. Figure 3 shows a brief
block diagram of the analog neuro-system, where an analogue neuro-board is
interfaced to a host computer through a GDAB (General Data Acquisition Board). The
GDAB board is specially designed for the data interface with the analogue neuro-chips.
The neuro-board has 6 synapse chips and 2 neuron chips with the 2-layer Perceptron
architecture. For test and development purposes, a DSP, ADC and DAC are installed
on the neuro-board to refresh and adjust weights.
Forward propagation time of the 2 layers Perceptron is measured as about 30 f..lsec.
Therefore the computation speed of the neuro-board is about 266 MCPS (Mega
Connections Per Second) for recall and about 200 MCUPS (Mega Connections
Updates Per Second) for error backpropagation learning. To achieve this speed with a
DSP, about 400 MIPS is required for recall and at least 600 MIPS for error-back
propagation learning.

C 1 (z)
Channel

Error
Signal

Noise
Source

Adaptive Filter

or
Multilayer Perceptron

Figure 4: Structure of a feedforward active noise canceling

3 ACTIVE NOISE CANCELING USING NEURO-CHIP
Basic architecture of the feed forward active noise canceling is shown in Figure 4. An
area near the microphone is called "quiet zone," which actually means noise should be
small in this area. Noise propagates from a source to the quiet zone through a
dispersive medium, of which characteristics are modeled as a finite impulse response
(FIR) filter with additional random noise. An active noise canceller should generate
electric signals for a loud speaker, which creates acoustic signals to cancel the noise at
the quiet zone. In general the electric-to-acoustic signal transfer characteristics of the
loud speaker is nonlinear, and the overall active noise canceling (ANC) system also
becomes nonlinear. Therefore, multilayer Perceptron has a potential advantage over
popular transversal adaptive filters based on linear-mean.-square (LMS) error
minimization.
Experiments had been conducted for car noise canceling. The reference signal for the
noise source was extracted from an engine room, while a compact car was running at
60 kmlhour. The difference of the two acoustic channels, i.e., H(z) = C1 (z) / C2 ( z) ,
addition noise n, and nonlinear characteristics of the loud speaker need be
compensated. Two different acoustic channels are used for the experiments. The first
channel Hl (z) = 0.894 + 0.447z- 1 is a minimum phase channel, while the second non-

J.-W Cho and S-Y. Lee

668
minimum

phase

channel

H2 (z)

= 0.174 + 0.6z -I + 0.6z -2 + 0.174z -3

characterizes

frequency-selective multipath fading with a deep spectral amplitude null. A simple
cubic distortion model was used for the characteristics of the loud speaker.[12] To
compare performance of the neuro-chip with digital processors, computer simulation
was first conducted with error backpropagation algorithm for a single hidden-layer
Perceptron as well as the LMS algorithm for a transversal adaptive filter. Then, the
same experimental data were provided to the developed neuro-board by a personal
computer through the GDAB.

o

ro
a::

-5

c

o

:.;:;

g -10 .

"0
CI.>

0:::
CI.>

.~

o -15
z
- 20 ~-----~------~----~------~--=-~

o

5

10

15

20

25

Signal-to-Distortion Ratio
(a)

o

15 -5
0:::

c
o

.......

~ -10

"0
CI.>

0:::
CI.>

.~

o

-15

z

-20~----~----~----~----~----~

o

5

10
15
20
Signal-to-Distortion Ratio

25

(b)
Figure 5: Noise Reduction Ratio (dB) versus Signal-to-Distortion Ratio (dB) for (a) a
simple acoustic channel HI (z) and (b) a multi-path fading acoustic channel H2 (z) _
Here, '+', '*', 'x', and '0' denote results ofLMS algorithm, neural networks simulation,
neural network simulation with 8-bit input quantization, and neuro-chips, respectively_

Active Noise Canceling with Analog On-Chip Learning Neuro-Chip

669

Results for the channels HI ( z) and H2 (z) are shown in Figures 5(a) and 5(b),
respectively. Each point in these figures denotes the result of one experiment with
different parameters. The horizontal axes represent Signal-to-Distortion Ratio (SDR)
of the speaker nonlinear characteristics. The vertical axes represent Noise Reduction
Ratio (NRR) of the active noise canceling systems. As expected, severe nonlinear
distortion of the loud speaker resulted in poor noise canceling for the LMS canceller.
However, the performance degradation was greatly reduced by neural network
canceller. With the neuro-chips the performance was worse than that of computer
simulation. Although the neuro-chip demonstrated active noise canceling and worked
better than LMS cancellers for very small SDRs, i.e. , very high nonlinear distortions,
its performance became saturated to -8 dB and -5 dB NRRs, respectively. The
performance saturation was more severe for the harder problem with the complicated
H 2 (z ) channel.
The performance degradation with neuro-chips may come from inherent limitations of
analogue chips such as limited dynamic ranges of synaptic weights and signals,
unwanted offsets and nonlinearity, and limited resolution of the learning rate and
sigmoid slope. [9] However, other side effects of the GDAB board, i.e., fixed resolution
of AID converters and D/A converters for data 110, also contributed to the performance
degradation. The input and output resolutions of the GDAB were J6 bit and 8 bit,
respectively. Unlike actual real-world systems the input values of the experimental
analogue neuro-chips are these 8-bit quantized values. As shown in Figures 5, results
of the computer simulation with 8-bit quantized target values showed much degraded
performance compared to the floating-point simulations. Therefore, a significant
portion of the poor performance in the experimental analogue system may be
contributed from the AID converters, and the analogue system may work better in real
world systems.
Actual acoustic signals are plotted in Figure 6. The top, middle, and bottom signals
denote noise , negated speaker signal, and residual noise at the quiet zone, respectively.

Figure 6: Examples of noise, negated loud-speaker canceling signal, and residual error

J.-w. Cho and s.-Y. Lee

670

4

CONCLUSION

In this paper we report an experimental results of active noise canceling using analogue
neuro-chips with on-chip learning capability. Although the its performance is limited
due to nonideal characteristics of analogue chip itself and also peripheral devices, it
clearly demonstrates feasibility of analogue chips for real world applications.
Acknowledgements
This research was supported by Korean Ministry of Information and Telecommunications.
References
[1] T. Watanabe, K. Kimura, M. Aold, T. Sakata & K. Ito (1993) A Single 1.5-V
Digital Chip for a 106 Synapse Neural Network, IEEE Trans. Neural Network,
VolA, No.3, pp.387-393.
[2J T. Morie and Y. Amemiya (1994) An All-Analog Expandable Neural Network
LSI with On-Chip Backpropagation Learning, IEEE Journal of Solid State
Circuits, vo1.29, No.9, pp.1086-1093.
[3J J.-W. Cho, Y. K. Choi, S.-Y. Lee (1996) Modular Neuro-Chip with On-Chip
Learning and Adjustable Learning Parameters, Neural Processing Letters, VolA,
No.1.
[4J J. Alspector, A. Jayakumar, S. Luna (1992) Experimental evaluation of learning in
neural microsystem, Advances in Neural Information Processing Systems 4, pp.
871-878 .
[5 J B. Widrow, et al. (1975) Adative Noise Cancelling: Principles and Applications,
Proceeding of IEEE, Vo1.63, No.12, pp.1692-1716.
[6] J. Choi, S.H. Bang, BJ. Sheu (1993) A Programmable Analog VLSI Neural
Network Processor for Communication Receivers, IEEE Transaction on Neural
Network, VolA, No.3, ppA84-495.
[7J J.-W. Cho and S.-Y. Lee (1998) Analog neuro-chips with on-chip learning
capability for adaptive nonlinear equalizer, Proc. lJCNN, pp. 581-586, May 4-9,
Anchorage, USA.
[8J J. Van der Spiegel, C. Donham, R. Etienne-Cummings, S. Fernando (1994) Large
scale analog neural computer with programmable architecture and programmable
time constants for temporal pattern analysis, Proc. ICNN, pp. 1830-1835.
[9J Y.K. Choi, K.H. Ahn, and S.Y. Lee (1996) Effects of multiplier offsets on onchip learning for analog neuro-chip, Neural Processing Letters, vol. 4, No.1, 1-8.
[1OJ T. Enomoto, T. Ishihara and M. Yasumoto (1982) Integrated tapped MaS
analogue delay line using switched-capacitor technique, Electronics Lertters,
Vo1.l8, pp.193-194.
[11 J P.B. Allen, D.R. Holberg (1987) CMOS Analog Circuit Design, Holt, Douglas
Rinehart and Winston.
[12J F. Gao and W.M. Snelgrove (1991) Adaptive linearization of a loudspeaker,
Proc. International Conference on Acoustics, Speech and Signal processing, pp.
3589-3592.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 6221-memory-efficient-backpropagation-through-time.pdf

Memory-Efficient Backpropagation Through Time

?
Audrunas
Gruslys
Google DeepMind
audrunas@google.com

R?mi Munos
Google DeepMind
munos@google.com

Marc Lanctot
Google DeepMind
lanctot@google.com

Ivo Danihelka
Google DeepMind
danihelka@google.com

Alex Graves
Google DeepMind
gravesa@google.com

Abstract
We propose a novel approach to reduce memory consumption of the backpropagation through time (BPTT) algorithm when training recurrent neural networks
(RNNs). Our approach uses dynamic programming to balance a trade-off between
caching of intermediate results and recomputation. The algorithm is capable of
tightly fitting within almost any user-set memory budget while finding an optimal
execution policy minimizing the computational cost. Computational devices have
limited memory capacity and maximizing a computational performance given a
fixed memory budget is a practical use-case. We provide asymptotic computational
upper bounds for various regimes. The algorithm is particularly effective for long
sequences. For sequences of length 1000, our algorithm saves 95% of memory
usage while using only one third more time per iteration than the standard BPTT.

1

Introduction

Recurrent neural networks (RNNs) are artificial neural networks where connections between units
can form cycles. They are often used for sequence mapping problems, as they can propagate hidden
state information from early parts of the sequence back to later points. LSTM [9] in particular
is an RNN architecture that has excelled in sequence generation [3, 13, 4], speech recognition
[5] and reinforcement learning [12, 10] settings. Other successful RNN architectures include the
differentiable neural computer (DNC) [6], DRAW network [8], and Neural Transducers [7].
Backpropagation Through Time algorithm (BPTT) [11, 14] is typically used to obtain gradients
during training. One important problem is the large memory consumption required by the BPTT.
This is especially troublesome when using Graphics Processing Units (GPUs) due to the limitations
of GPU memory.
Memory budget is typically known in advance. Our algorithm balances the tradeoff between memorization and recomputation by finding an optimal memory usage policy which minimizes the total
computational cost for any fixed memory budget. The algorithm exploits the fact that the same
memory slots may be reused multiple times. The idea to use dynamic programming to find a provably
optimal policy is the main contribution of this paper.
Our approach is largely architecture agnostic and works with most recurrent neural networks. Being
able to fit within limited memory devices such as GPUs will typically compensate for any increase in
computational cost.

2

Background and related work

In this section, we describe the key terms and relevant previous work for memory-saving in RNNs.
30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

Definition 1. An RNN core is a feed-forward neural network which is cloned (unfolded in time)
repeatedly, where each clone represents a particular time point in the recurrence.
For example, if an RNN has a single hidden layer whose outputs feed back into the same hidden
layer, then for a sequence length of t the unfolded network is feed-forward and contains t RNN cores.
Definition 2. The hidden state of the recurrent network is the part of the output of the RNN core
which is passed into the next RNN core as an input.
In addition to the initial hidden state, there exists a single hidden state per time step once the network
is unfolded.
Definition 3. The internal state of the RNN core for a given time-point is all the necessary information required to backpropagate gradients over that time step once an input vector, a gradient with
respect to the output vector, and a gradient with respect to the output hidden state is supplied. We
define it to also include an output hidden state.
An internal state can be (re)evaluated by executing a single forward operation taking the previous
hidden state and the respective entry of an input sequence as an input. For most network architectures,
the internal state of the RNN core will include a hidden input state, as this is normally required to
evaluate gradients. This particular choice of the definition will be useful later in the paper.
Definition 4. A memory slot is a unit of memory which is capable of storing a single hidden state
or a single internal state (depending on the context).
2.1

Backpropagation through Time

Backpropagation through Time (BPTT) [11, 14] is one of the commonly used techniques to train
recurrent networks. BPTT ?unfolds? the neural network in time by creating several copies of the
recurrent units which can then be treated like a (deep) feed-forward network with tied weights. Once
this is done, a standard forward-propagation technique can be used to evaluate network fitness over
the whole sequence of inputs, while a standard backpropagation algorithm can be used to evaluate
partial derivatives of the loss criteria with respect to all network parameters. This approach, while
being computationally efficient is also fairly intensive in memory usage. This is because the standard
version of the algorithm effectively requires storing internal states of the unfolded network core at
every time-step in order to be able to evaluate correct partial derivatives.
2.2

Trading memory for computation time

The general idea of trading computation time and memory consumption in general computation
graphs has been investigated in the automatic differentiation community [2]. Recently, the rise of
deep architectures and recurrent networks has increased interest in a less general case where the
graph of forward computation is a chain and gradients have to be chained in a reverse order. This
simplification leads to relatively simple memory-saving strategies and heuristics. In the context of
BPTT, instead of storing hidden network states, some of the intermediate results can be recomputed
on demand by executing an extra forward operation.
?
Chen et. al. proposed subdividing the sequence of size t into t equal parts and memorizing only
hidden
? states between the subsequences and all internal states within each segment [1]. This uses
O( t) memory at the cost of making an additional forward pass on average, as once the errors are
backpropagated through the right-side of the sequence, the second-last subsequence
has to be restored
?
by repeating a number of forward operations. We refer to this as Chen?s t algorithm.
The authors also suggest applying the same technique recursively several times by sub-dividing the
sequence into k equal parts and terminating the recursion once the subsequence length becomes less
than k. The authors have established that this would lead to memory consumption of O(k logk+1 (t))
and computational complexity of O(t logk (t)). This algorithm has a minimum possible memory
usage of log2 (t) in the case when k = 1. We refer to this as Chen?s recursive algorithm.

3

Memory-efficient backpropagation through time

We first discuss two simple examples: when memory is very scarce, and when it is somewhat limited.
2

When memory is very scarce, it is straightforward to design a simple but computationally inefficient
algorithm for backpropagation of errors on RNNs which only uses a constant amount of memory.
Every time when the state of the network at time t has to be restored, the algorithm would simply
re-evaluate the state by forward-propagating inputs starting from the beginning until time t. As
backpropagation happens in the reverse temporal order, results from the previous forward steps can
not be reused (as there is no memory to store them). This would require repeating t forward steps
before backpropagating gradients one step backwards (we only remember inputs and the initial state).
This would produce an algorithm requiring t(t + 1)/2 forward passes to backpropagate errors over t
time steps. The algorithm would be O(1) in space and O(t2 ) in time.
When the memory is somewhat limited (but not very scarce) we may store only hidden RNN states
at all time points. When errors have to be backpropagated from time t to t ? 1, an internal RNN
core state can be re-evaluated by executing another forward operation taking the previous hidden
state as an input. The backward operation can follow immediately. This approach can lead to fairly
significant memory savings, as typically the recurrent network hidden state is much smaller than an
internal state of the network core itself. On the other hand this leads to another forward operation
being executed during the backpropagation stage.
3.1

Backpropagation though time with selective hidden state memorization (BPTT-HSM)

The idea behind the proposed algorithm is to compromise between two previous extremes. Suppose
that we want to forward and backpropagate a sequence of length t, but we are only able to store m
hidden states in memory at any given time. We may reuse the same memory slots to store different
hidden states during backpropagation. Also, suppose that we have a single RNN core available for
the purposes of intermediate calculations which is able to store a single internal state. Define C(t, m)
as a computational cost of backpropagation measured in terms of how many forward-operations one
has to make in total during forward and backpropagation steps combined when following an optimal
memory usage policy minimizing the computational cost. One can easily set the boundary conditions:
C(t, 1) = 12 t(t + 1) is the cost of the minimal memory approach, while C(t, m) = 2t ? 1 for all
m ? t when memory is plentiful (as shown in Fig. 3 a). Our approach is illustrated in Figure 1. Once
we start forward-propagating steps at time t = t0 , at any given point y > t0 we can choose to put the
current hidden state into memory (step 1). This step has the cost of y forward operations. States will
be read in the reverse order in which they were written: this allows the algorithm to store states in a
stack. Once the state is put into memory at time y = D(t, m), we can reduce the problem into two
parts by using a divide-and-conquer approach: running the same algorithm on the t > y side of the
sequence while using m ? 1 of the remaining memory slots at the cost of C(t ? y, m ? 1) (step 2),
and then reusing m memory slots when backpropagating on the t ? y side at the cost of C(y, m)
(step 3). We use a full size m memory capacity when performing step 3 because we could release the
hidden state y immediately after finishing step 2.
Legend

Step 1: cost = y

1

2

...

y

y+1

...

Hidden state is propagated
Gradients get back-propagated
Hidden state stored in memory

t
t

Internal state of RNN core at time t
A single forward operation

Step 2: cost = C(t-y, m-1)

y+1

...

t

A single backward operation
Recursive application
of the algorithm

1

2

...

y

Hidden state is read from memory

Step 3: cost = C(y, m)

Hidden state is saved in memory
Hidden state is removed from memory

Figure 1: The proposed divide-and-conquer approach.
The base case for the recurrent algorithm is simply a sequence of length t = 1 when forward and
backward propagation may be done trivially on a single available RNN network core. This step has
the cost C(1, m) = 1.
3

(a) Theoretical computational cost
measured in number of forward operations per time step.

(b) Measured computational cost in
miliseconds.

Figure 2: Computational cost per time-step when the algorithm is allowed to remember 10 (red), 50
(green), 100 (blue), 500 (violet), 1000 (cyan) hidden states. The grey line shows the performance
of standard BPTT without memory constraints; (b) also includes a large constant value caused by a
single backwards step per time step which was excluded from the theoretical computation, which
value makes a relative performance loss much less severe in practice than in theory.
Having established the protocol we may find an optimal policy D(t, m). Define the cost of choosing
the first state to be pushed at position y and later following the optimal policy as:
Q(t, m, y) = y + C(t ? y, m ? 1) + C(y, m)

C(t, m) = Q(t, m, D(t, m))

1

1

time

D(t, m) = argmin Q(t, m, y)

(2)

1

(3)

1?y<t

t

1

1
11

CPTS ? 2
1

(1)

1

1

time

CPTS ? 3
Hidden state stored in memory

1

Forward computation
Backward computation

a)

Order of execution

b)

CPTS cost per time step in the number
of forward operations

Figure 3: Illustration of the optimal policy for m = 4 and a) t = 4 and b) t = 10. Logical sequence
time goes from left to right, while execution happens from top to the bottom.
Equations can be solved exactly by using dynamic programming subject to the boundary conditions
established previously (e.g. as in Figure 2(a)). D(t, m) will determine the optimal policy to follow.
Pseudocode is given in the supplementary material. Figure 3 illustrates an optimal policy found for
two simple cases.
3.2

Backpropagation though time with selective internal state memorization (BPTT-ISM)

Saving internal RNN core states instead of hidden RNN states would allow us to save a single forward
operation during backpropagation in every divide-and-conquer step, but at a higher memory cost.
Suppose we have a memory capacity capable of saving exactly m internal RNN states. First, we need
to modify the boundary conditions: C(t, 1) = 12 t(t + 1) is a cost reflecting the minimal memory
approach, while C(t, m) = t for all m ? t when memory is plentiful (equivalent to standard BPTT).
4

As previously, C(t, m) is defined to be the computational cost for combined forward and backward
propagations over a sequence of length t with memory allowance m while following an optimal
memory usage policy. As before, the cost is measured in terms of the amount of total forward steps
made, because the number of backwards steps is constant. Similarly to BPTT-HSM, the process
can be divided into parts using divide-and-conquer approach (Fig 4). For any values of t and m
position of the first memorization y = D(t, m) is evaluated. y forward operations are executed and
an internal RNN core state is placed into memory. This step has the cost of y forward operations
(Step 1 in Figure 4). As the internal state also contains an output hidden state, the same algorithm can
be recurrently run on the high-time (right) side of the sequence while having one less memory slot
available (Step 2 in Figure 4). This step has the cost of C(t ? y, m ? 1) forward operations. Once
gradients are backpropagated through the right side of the sequence, backpropagation can be done
over the stored RNN core (Step 3 in Figure 4). This step has no additional cost as it involves no more
forward operations. The memory slot can now be released leaving m memory available. Finally, the
same algorithm is run on the left-side of the sequence (Step 4 in Figure 4). This final step has the cost
of C(y ? 1, m) forward operations. Summing the costs gives us the following equation:
Q(t, m, y) = y + C(y ? 1, m) + C(t ? y, m ? 1)

(4)

Recursion has a single base case: backpropagation over an empty sequence is a nil operation which
has no computational cost making C(0, m) = 0.
Legend

Step 1: cost = y

1

...

y-1

y

y+1

...

t

Hidden state gets propagated

t

Internal stat of RNN core at time t
Internal RNN core state stored in
memory (incl. output hidden state)

Step 2:
cost = C(t-y, m-1)

y

y+1

...

t

Hidden state is read from memory
Internal state is saved
Internal state is removed

Step 3:
cost = 0

y

A single backwards
operation, no forward
operations involved.

Recursive application
of the algorithm
Gradients get back-propagated
A single initial RNN hidden state

1

...

y-1

y

Step 4:
cost = C(y-1, m)

A single forward operation
A single backward operation

Figure 4: Illustration of the divide-and-conquer approach used by BPTT-ISM.
Compared to the previous section (20) stays the same while (19) is minimized over 1 ? y ? t instead
of 1 ? y < t. This is because it is meaningful to remember the last internal state while there was
no reason to remember the last hidden state. A numerical solution of C(t, m) for several different
memory capacities is shown in Figure 5(a).
D(t, m) = argmin Q(t, m, y)

(5)

1?y?t

As seen in Figure 5(a), our methodology saves 95% of memory for sequences of 1000 (excluding input
vectors) while using only 33% more time per training-iteration than the standard BPTT (assuming a
single backward step being twice as expensive as a forward step).
3.3

Backpropagation though time with mixed state memorization (BPTT-MSM)

It is possible to derive an even more general model by combining both approaches as described in
Sections 3.1 and 3.2. Suppose we have a total memory capacity m measured in terms of how much a
single hidden states can be remembered. Also suppose that storing an internal RNN core state takes
? times more memory where ? ? 2 is some integer number. We will choose between saving a single
hidden state while using a single memory unit and storing an internal RNN core state by using ?
times more memory. The benefit of storing an internal RNN core state is that we will be able to save
a single forward operation during backpropagation.
Define C(t, m) as a computational cost in terms of a total amount of forward operations when
running an optimal strategy. We use the following boundary conditions: C(t, 1) = 12 t(t + 1) as a
5

(a) BPTT-ISM (section 3.2).

(b) BPTT-MSM (section 3.3).

Figure 5: Comparison of two backpropagation algorithms in terms of theoretical costs. Different
lines show the number of forward operations per time-step when the memory capacity is limited to
10 (red), 50 (green), 100 (blue), 500 (violet), 1000 (cyan) internal RNN core states. Please note that
the units of memory measurement are different than in Figure 2(a) (size of an internal state vs size of
a hidden state). It was assumed that the size of an internal core state is ? = 5 times larger than the
size of a hidden state. The value of ? influences only the right plot. All costs shown on the right plot
should be less than the respective costs shown on the left plot for any value of ?.
cost reflecting the minimal memory approach, while C(t, m) = t for all m ? ?t when memory is
plentiful and C(t ? y, m) = ? for all m ? 0 and C(0, m) = 0 for notational convenience. We use
a similar divide-and-conquer approach to the one used in previous sections.
Define Q1 (t, m, y) as the computational cost if we choose to firstly remember a hidden state at
position y and thereafter follow an optimal policy (identical to ( 18)):
Q1 (t, m, y) = y + C(y, m) + C(t ? y, m ? 1)

(6)

Similarly, define Q2 (t, m, y) as the computational cost if we choose to firstly remember an internal
state at position y and thereafter follow an optimal policy (similar to ( 4) except that now the internal
state takes ? memory units):
Q2 (t, m, y) = y + C(y ? 1, m) + C(t ? y, m ? ?)

(7)

Define D1 as an optimal position of the next push assuming that the next state to be pushed is a
hidden state and define D2 as an optimal position if the next push is an internal core state. Note that
D2 has a different range over which it is minimized, for the same reasons as in equation 5:
D1 (t, m) = argmin Q1 (t, m, y)

D2 (t, m) = argmin Q2 (t, m, y)

1?y<t

1?y?t

(8)

Also define Ci (t, m) = Qi (t, m, D(t, m)) and finally:
C(t, m) = min Ci (t, m)
i

H(t, m) = argmin Ci (t, m)

(9)

i

We can solve the above equations by using simple dynamic programming. H(t, m) will indicate
whether the next state to be pushed into memory in a hidden state or an internal state, while the
respective values if D1 (t, m) and D2 (t, m) will indicate the position of the next push.
3.4

Removing double hidden-state memorization

Definition 3 of internal RNN core state would typically require for a hidden input state to be included
for each memorization. This may lead to the duplication of information. For example, when an
optimal strategy is to remember a few internal RNN core states in sequence, a memorized hidden
output of one would be equal to a memorized hidden input for the other one (see Definition 3).
Every time we want to push an internal RNN core state onto the stack and a previous internal state is
already there, we may omit pushing the input hidden state. Recall that an internal core RNN state
when an input hidden state is otherwise not known is ? times larger than a hidden state. Define ? ? ?
as the space required to memorize the internal core state when an input hidden state is known. A
6

relationship between ? and ? is application-specific, but in many circumstances ? = ? + 1. We only
have to modify (7) to reflect this optimization:
Q2 (t, m, y) = y + C(y ? 1, m) + C(t ? y, m ? 1y>1 ? ? 1y=1 ?)

(10)

1 is an indicator function. Equations for H(t, m), Di (t, m) and C(t, m) are identical to (8) and (9).
3.5

Analytical upper bound for BPTT-HSM
1

We have established a theoretical upper bound for BPTT-HSM algorithm as C(t, m) ? mt1+ m . As
1
the bound is not tight for short sequences, it was also numerically verified that C(t, m) < 4t1+ m for
1
5
3
1+ m
t < 10 and m < 10 , or less than 3t
if the initial forward pass is excluded. In addition to that,
m
we have established a different bound in the regime where t < mm! . For any integer value a and for
a
all t < ma! the computational cost is bounded by C(t, m) ? (a + 1)t. The proofs are given in the
supplementary material. Please refer to supplementary material for discussion on the upper bounds
for BPTT-MSM and BPTT-ISM.
3.6

Comparison of the three different strategies

(a) Using 10? memory

(b) Using 20? memory

Figure 6: Comparison of three strategies in the case when a size of an internal RNN core state is
? = 5 times larger than that of the hidden state, and the total memory capacity allows us remember
either 10 internal RNN states, or 50 hidden states or any arbitrary mixture of those in the left plot
and (20, 100) respectively in the right plot. The red curve illustrates BPTT-HSM, the green curve
- BPTT-ISM and the blue curve - BPTT-MSM. Please note that for large sequence lengths the red
curve out-performs the green one, and the blue curve outperforms the other two.
Computational costs for each previously described strategy and the results are shown in Figure 6.
BPTT-MSM outperforms both BPTT-ISM and BPTT-HSM. This is unsurprising, because the search
space in that case is a superset of both strategy spaces, while the algorothm finds an optimal strategy
within that space. Also, for a fixed memory capacity, the strategy memorizing only hidden states
outperforms a strategy memorizing internal RNN core states for long sequences, while the latter
outperforms the former for relatively short sequences.

4

Discussion

We used an LSTM mapping 256 inputs to 256 with a batch size of 64 and measured execution time for
a single gradient descent step (forward and backward operation combined) as a function of sequence
length (Figure 2(b)). Please note that measured computational time also includes the time taken by
backward operations at each time-step which dynamic programming equations did not take into the
account. A single backward operation is usually twice as expensive than a forward operation, because
it involves evaluating gradients both with respect to input data and internal parameters. Still, as the
number of backward operations is constant it has no impact on the optimal strategy.
4.1

Optimality

The dynamic program finds the optimal computational strategy by construction, subject to memory
constraints and a fairly general model that we impose. As both strategies proposed by [1] are
7

consistent with all the assumptions that we have made in section 3.4 when applied to RNNs, BPTTMSM is guaranteed to perform at least as well under any memory budget and any sequence length.
This is because strategies proposed by [1] can be expressed by providing a (potentially suboptimal)
policy Di (t, m), H(t, m) subject to the same equations for Qi (t, m).
4.2

Numerical comparison with Chen?s

?

t algorithm

?
?
?
Chen?s t algorithm requires to remember t hidden states and t internal RNN states (excluding
input hidden states), while the recursive approach requires to remember at least log2 t hidden states.
In other words, the model does not allow for a fine-grained control over memory usage and rather
saves some memory. In the meantime our proposed BPTT-MSM can fit within almost arbitrary
constant memory constraints, and this is the main advantage of our algorithm.

?
Figure 7: Left: memory consumption divided by t(1 + ?) for a fixed computational
cost C = 2.
?
Right: computational cost per time-step for a fixed memory consumption of t(1 + ?). Red, green
and blue curves correspond to ? = 2, 5, 10 respectively.
?
The non-recursive Chen?s t approach does not allow to match any particular memory budget
making a like-for-like comparison difficult. Instead of fixing the memory budge, it?is possible to fix
computational cost at 2 forwards iterations on average to match the cost of the
? t algorithm and
observe how much memory
would
our
approach
use.
Memory
usage
by
the
t algorithm would
?
?
be equivalent to saving t hidden states and t internal core states. Lets suppose that the internal
RNN core state is ? times larger than hidden states. In this case the size of the internal RNN core
state excluding
state is ? = ? ? 1. This would
give a memory?usage of Chen?s
?
? the input hidden
?
algorithm as t(1 + ?) = t(?), as it needs to remember t hidden states and t internal states
where input hidden states?can be omitted to avoid duplication. Figure 7 illustrates memory usage by
our algorithm divided by t(1 + ?) for a fixed execution speed of 2 as a function of sequence length
and for different values of parameter ?. Values lower than 1 indicate memory savings. As it is seen,
we can save a significant amount of memory for the same computational cost.
?
Another experiment is to measure computational cost for a fixed memory consumption
? of t(1 + ?).
The results are shown in Figure 7. Computational cost of 2 corresponds to Chen?s t algorithm. This
illustrates that our approach
? does not perform significantly faster (although it does not do any worse).
This is because Chen?s t strategy is actually near optimal for this particular memory budget. Still,
as seen from the previous paragraph, this memory budget is already in the regime of diminishing
returns and further memory reductions are possible for almost the same computational cost.

5

Conclusion

In this paper, we proposed a novel approach for finding optimal backpropagation strategies for
recurrent neural networks for a fixed user-defined memory budget. We have demonstrated that the
most general of the algorithms is at least as good as many other used common heuristics. The main
advantage of our approach is the ability to tightly fit to almost any user-specified memory constraints
gaining maximal computational performance.
8

References
[1] Tianqi Chen, Bing Xu, Zhiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear
memory cost. arXiv preprint arXiv:1604.06174, 2016.
[2] Benjamin Dauvergne and Laurent Hasco?t. The data-flow equations of checkpointing in reverse
automatic differentiation. In Computational Science?ICCS 2006, pages 566?573. Springer,
2006.
[3] Douglas Eck and Juergen Schmidhuber. A first look at music composition using LSTM recurrent
neural networks. Istituto Dalle Molle Di Studi Sull Intelligenza Artificiale, 103, 2002.
[4] Alex Graves. Supervised Sequence Labelling with Recurrent Neural Networks. Studies in
Computational Intelligence. Springer, 2012.
[5] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep
recurrent neural networks. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE
International Conference on, pages 6645?6649. IEEE, 2013.
[6] Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka GrabskaBarwi?nska, Sergio G?mez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou,
Adri? Puigdom?nech Badia, Karl Moritz Hermann, Yori Zwols, Georg Ostrovski, Adam Cain,
Helen King, Christopher Summerfield, Phil Blunsom, Koray Kavukcuoglu, and Demis Hassabis.
Hybrid computing using a neural network with dynamic external memory. Nature, advance
online publication, October 2016.
[7] Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, and Phil Blunsom. Learning to
transduce with unbounded memory. In Advances in Neural Information Processing Systems,
pages 1819?1827, 2015.
[8] Karol Gregor, Ivo Danihelka, Alex Graves, and Daan Wierstra. DRAW: A recurrent neural
network for image generation. arXiv preprint arXiv:1502.04623, 2015.
[9] Sepp Hochreiter and J?rgen Schmidhuber. Long short-term memory. Neural computation,
9(8):1735?1780, 1997.
[10] Volodymyr Mnih, Adri? Puigdom?nech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap,
Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Proceedings of the 33rd International Conference on Machine Learning
(ICML), pages 1928?1937, 2016.
[11] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning internal representations by error propagation. Technical report, DTIC Document, 1985.
[12] Ivan Sorokin, Alexey Seleznev, Mikhail Pavlov, Aleksandr Fedorov, and Anastasiia Ignateva.
Deep attention recurrent Q-network. arXiv preprint arXiv:1512.01693, 2015.
[13] Ilya Sutskever, James Martens, and Geoffrey E Hinton. Generating text with recurrent neural
networks. In Proceedings of the 28th International Conference on Machine Learning (ICML-11),
pages 1017?1024, 2011.
[14] Paul J Werbos. Backpropagation through time: what it does and how to do it. Proceedings of
the IEEE, 78(10):1550?1560, 1990.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 5666-variational-dropout-and-the-local-reparameterization-trick.pdf

Variational Dropout and
the Local Reparameterization Trick

?

Diederik P. Kingma? , Tim Salimans? and Max Welling??
?
Machine Learning Group, University of Amsterdam
?
Algoritmica
University of California, Irvine, and the Canadian Institute for Advanced Research (CIFAR)
D.P.Kingma@uva.nl, salimans.tim@gmail.com, M.Welling@uva.nl

Abstract
We investigate a local reparameterizaton technique for greatly reducing the variance of stochastic gradients for variational Bayesian inference (SGVB) of a posterior over model parameters, while retaining parallelizability. This local reparameterization translates uncertainty about global parameters into local noise that
is independent across datapoints in the minibatch. Such parameterizations can be
trivially parallelized and have variance that is inversely proportional to the minibatch size, generally leading to much faster convergence. Additionally, we explore
a connection with dropout: Gaussian dropout objectives correspond to SGVB with
local reparameterization, a scale-invariant prior and proportionally fixed posterior
variance. Our method allows inference of more flexibly parameterized posteriors;
specifically, we propose variational dropout, a generalization of Gaussian dropout
where the dropout rates are learned, often leading to better models. The method
is demonstrated through several experiments.

1

Introduction

Deep neural networks are a flexible family of models that easily scale to millions of parameters and
datapoints, but are still tractable to optimize using minibatch-based stochastic gradient ascent. Due
to their high flexibility, neural networks have the capacity to fit a wide diversity of nonlinear patterns
in the data. This flexbility often leads to overfitting when left unchecked: spurious patterns are found
that happen to fit well to the training data, but are not predictive for new data. Various regularization
techniques for controlling this overfitting are used in practice; a currently popular and empirically
effective technique being dropout [10]. In [22] it was shown that regular (binary) dropout has a
Gaussian approximation called Gaussian dropout with virtually identical regularization performance
but much faster convergence. In section 5 of [22] it is shown that Gaussian dropout optimizes a lower
bound on the marginal likelihood of the data. In this paper we show that a relationship between
dropout and Bayesian inference can be extended and exploited to greatly improve the efficiency of
variational Bayesian inference on the model parameters. This work has a direct interpretation as a
generalization of Gaussian dropout, with the same fast convergence but now with the freedom to
specify more flexibly parameterized posterior distributions.
Bayesian posterior inference over the neural network parameters is a theoretically attractive method
for controlling overfitting; exact inference is computationally intractable, but efficient approximate
schemes can be designed. Markov Chain Monte Carlo (MCMC) is a class of approximate inference
methods with asymptotic guarantees, pioneered by [16] for the application of regularizing neural
networks. Later useful refinements include [23] and [1].
An alternative to MCMC is variational inference [11] or the equivalent minimum description length
(MDL) framework. Modern variants of stochastic variational inference have been applied to neural
1

networks with some succes [8], but have been limited by high variance in the gradients. Despite
their theoretical attractiveness, Bayesian methods for inferring a posterior distribution over neural
network weights have not yet been shown to outperform simpler methods such as dropout. Even a
new crop of efficient variational inference algorithms based on stochastic gradients with minibatches
of data [14, 17, 19] have not yet been shown to significantly improve upon simpler dropout-based
regularization.
In section 2 we explore an as yet unexploited trick for improving the efficiency of stochastic gradientbased variational inference with minibatches of data, by translating uncertainty about global parameters into local noise that is independent across datapoints in the minibatch. The resulting method
has an optimization speed on the same level as fast dropout [22], and indeed has the original Gaussian dropout method as a special case. An advantage of our method is that it allows for full Bayesian
analysis of the model, and that it?s significantly more flexible than standard dropout. The approach
presented here is closely related to several popular methods in the literature that regularize by adding
random noise; these relationships are discussed in section 4.

2

Efficient and Practical Bayesian Inference

We consider Bayesian analysis of a dataset D, containing a set of N i.i.d. observations of tuples
(x, y), where the goal is to learn a model with parameters or weights w of the conditional probability p(y|x, w) (standard classification or regression)1 . Bayesian inference in such a model consists
of updating some initial belief over parameters w in the form of a prior distribution p(w), after
observing data D, into an updated belief over these parameters in the form of (an approximation
to) the posterior distribution p(w|D). Computing the true posterior distribution through Bayes? rule
p(w|D) = p(w)p(D|w)/p(D) involves computationally intractable integrals, so good approximations are necessary. In variational inference, inference is cast as an optimization problem where we
optimize the parameters of some parameterized model q (w) such that q (w) is a close approximation to p(w|D) as measured by the Kullback-Leibler divergence DKL (q (w)||p(w|D)). This
divergence of our posterior q (w) to the true posterior is minimized in practice by maximizing the
so-called variational lower bound L( ) of the marginal likelihood of the data:
L( ) =

where LD ( ) =

DKL (q (w)||p(w)) + LD ( )
X
Eq (w) [log p(y|x, w)]

(1)
(2)

(x,y)2D

We?ll call LD ( ) the expected log-likelihood.
P The bound L( ) plus DKL (q (w)||p(w|D)) equals
the (conditional) marginal log-likelihood (x,y)2D log p(y|x). Since this marginal log-likelihood
is constant w.r.t. , maximizing the bound w.r.t. will minimize DKL (q (w)||p(w|D)).
2.1

Stochastic Gradient Variational Bayes (SGVB)

Various algorithms for gradient-based optimization of the variational bound (eq. (1)) with differentiable q and p exist. See section 4 for an overview. A recently proposed efficient method for
minibatch-based optimization with differentiable models is the stochastic gradient variational Bayes
(SGVB) method introduced in [14] (especially appendix F) and [17]. The basic trick in SGVB is
to parameterize the random parameters w ? q (w) as: w = f (?, ) where f (.) is a differentiable function and ? ? p(?) is a random noise variable. In this new parameterisation, an unbiased
differentiable minibatch-based Monte Carlo estimator of the expected log-likelihood can be formed:
LD ( ) '

LSGVB
(
D

M
N X
)=
log p(yi |xi , w = f (?, )),
M i=1

(3)

i
i
where (xi , yi )M
i=1 is a minibatch of data with M random datapoints (x , y ) ? D, and ? is a noise
vector drawn from the noise distribution p(?). We?ll assume that the remaining term in the variational lower bound, DKL (q (w)||p(w)), can be computed deterministically, but otherwise it may
be approximated similarly. The estimator (3) is differentiable w.r.t. and unbiased, so its gradient
1
Note that the described method is not limited to classification or regression and is straightforward to apply
to other modeling settings like unsupervised models and temporal models.

2

is also unbiased: r LD ( ) ' r LSGVB
( ). We can proceed with variational Bayesian inference
D
by randomly initializing and performing stochastic gradient ascent on L( ) (1).
2.2

Variance of the SGVB estimator

The theory of stochastic approximation tells us that stochastic gradient ascent using (3) will asymptotically converge to a local optimum for an appropriately declining step size and sufficient weight
updates [18], but in practice the performance of stochastic gradient ascent crucially depends on
the variance of the gradients. If this variance is too large, stochastic gradient descent will fail
to make much progress in any reasonable amount of time. Our objective function consists of an
expected log likelihood term that we approximate using Monte Carlo, and a KL divergence term
DKL (q (w)||p(w)) that we assume can be calculated analytically and otherwise be approximated
with Monte Carlo with similar reparameterization.
Assume that we draw minibatches of datapoints with replacement; see appendix F for a similar
analysis for minibatches without replacement. Using Li as shorthand for log p(yi |xi , w = f (?i , )),
the contribution to the likelihood for the i-th datapoint in the minibatch, the Monte Carlo estimator
PM
N
(3) may be rewritten as LSGVB
( )= M
D
i=1 Li , whose variance is given by
M
M X
M
?
X
?
? N2 ? X
Var LSGVB
(
)
=
Var
[L
]
+
2
Cov
[L
,
L
]
(4)
i
i
j
D
M 2 i=1
i=1 j=i+1
? 1
?
M 1
=N 2
Var [Li ] +
Cov [Li , Lj ] ,
(5)
M
M
where the variances and
the data distribution and ? distribution, i.e.
? covariances are w.r.t. both
?
Var [Li ] = Var?,xi ,yi log p(yi |xi , w = f (?, )) , with xi , yi drawn from the empirical distribution defined by the training set. As can be seen from (5), the total contribution to the variance by
Var [Li ] is inversely proportional to the minibatch size M . However, the total contribution by the
covariances does not decrease with M . In practice, this means that the variance of LSGVB
( ) can be
D
dominated by the covariances for even moderately large M .

2.3

Local Reparameterization Trick

We therefore propose an alternative estimator for which we have Cov [Li , Lj ] = 0, so that the variance of our stochastic gradients scales as 1/M . We then make this new estimator computationally
efficient by not sampling ? directly, but only sampling the intermediate variables f (?) through which
? influences LSGVB
( ). By doing so, the global uncertainty in the weights is translated into a form
D
of local uncertainty that is independent across examples and easier to sample. We refer to such a
reparameterization from global noise to local noise as the local reparameterization trick. Whenever
a source of global noise can be translated to local noise in the intermediate states of computation
(? ! f (?)), a local reparameterization can be applied to yield a computationally and statistically
efficient gradient estimator.
Such local reparameterization applies to a fairly large family of models, but is best explained through
a simple example: Consider a standard fully connected neural network containing a hidden layer
consisting of 1000 neurons. This layer receives an M ? 1000 input feature matrix A from the layer
below, which is multiplied by a 1000 ? 1000 weight matrix W, before a nonlinearity is applied,
i.e. B = AW. We then specify the posterior approximation on the weights to be a fully factor2
ized Gaussian, i.e. q (wi,j ) = N (?i,j , i,j
) 8wi,j 2 W, which means the weights are sampled as
wi,j = ?i,j + i,j ?i,j , with ?i,j ? N (0, 1). In this case we could make sure that Cov [Li , Lj ] = 0
by sampling a separate weight matrix W for each example in the minibatch, but this is not computationally efficient: we would need to sample M million random numbers for just a single layer
of the neural network. Even if this could be done efficiently, the computation following this step
would become much harder: Where we originally performed a simple matrix-matrix product of the
form B = AW, this now turns into M separate local vector-matrix products. The theoretical complexity of this computation is higher, but, more importantly, such a computation can usually not be
performed in parallel using fast device-optimized BLAS (Basic Linear Algebra Subprograms). This
also happens with other neural network architectures such as convolutional neural networks, where
optimized libraries for convolution cannot deal with separate filter matrices per example.
3

Fortunately, the weights (and therefore ?) only influence the expected log likelihood through the
neuron activations B, which are of much lower dimension. If we can therefore sample the random
activations B directly, without sampling W or ?, we may obtain an efficient Monte Carlo estimator
at a much lower cost. For a factorized Gaussian posterior on the weights, the posterior for the
activations (conditional on the input A) is also factorized Gaussian:
2
q (wi,j ) = N (?i,j , i,j
) 8wi,j 2 W =) q (bm,j |A) = N ( m,j , m,j ), with
m,j

=

1000
X

am,i ?i,j , and

i=1

m,j

=

1000
X

a2m,i

2
i,j .

(6)

i=1

Rather than sampling the Gaussian weights and then computing the resulting activations, we may
thus
p sample the activations from their implied Gaussian distribution directly, using bm,j = m,j +
m,j ?m,j , with ?m,j ? N (0, 1). Here, ? is an M ? 1000 matrix, so we only need to sample M
thousand random variables instead of M million: a thousand fold savings.

In addition to yielding a gradient estimator that is more computationally efficient than drawing separate weight matrices for each training example, the local reparameterization trick also leads to an
estimator that has lower variance. To see why, consider the stochastic gradient estimate with respect
2
to the posterior parameter i,j
for a minibatch of size M = 1. Drawing random weights W, we get
@LSGVB
@LSGVB
?i,j am,i
D
D
=
.
2
@ i,j
@bm,j 2 i,j

(7)

If, on the other hand, we form the same gradient using the local reparameterization trick, we get
?m,j a2m,i
@LSGVB
@LSGVB
D
D
p
=
.
(8)
2
@ i,j
@bm,j 2 m,j

Here, there are two stochastic terms: The first is the backpropagated gradient @LSGVB
/@bm,j , and
D
2
the second is the sampled random noise (?i,j or ?m,j ). Estimating the gradient with respect to i,j
then basically comes down to estimating the covariance between these two terms. This is much
easier to do for ?m,j as there are much fewer of these: individually they have higher correlation
with the backpropagated gradient @LSGVB
/@bm,j , so the covariance is easier to estimate. In other
D
words, measuring the effect of ?m,j on @LSGVB
/@bm,j is easy as ?m,j is the only random variable
D
directly influencing this gradient via bm,j . On the other hand, when sampling random weights,
there are a thousand ?i,j influencing each gradient term, so their individual effects get lost in the
noise. In appendix D we make this argument more rigorous, and in section 5 we show that it holds
experimentally.

3

Variational Dropout

Dropout is a technique for regularization of neural network parameters, which works by adding
multiplicative noise to the input of each layer of the neural network during optimization. Using the
notation of section 2.3, for a fully connected neural network dropout corresponds to:
B = (A ?)?, with ?i,j ? p(?i,j )
(9)
where A is the M ? K matrix of input features for the current minibatch, ? is a K ? L weight matrix, and B is the M ? L output matrix for the current layer (before a nonlinearity is applied). The
symbol denotes the elementwise (Hadamard) product of the input matrix with a M ? K matrix
of independent noise variables ?. By adding noise to the input during training, the weight parameters ? are less likely to overfit to the training data, as shown empirically by previous publications.
Originally, [10] proposed drawing the elements of ? from a Bernoulli distribution with probability
1 p, with p the dropout rate. Later it was shown that using a continuous distribution with the same
relative mean and variance, such as a Gaussian N (1, ?) with ? = p/(1 p), works as well or better
[20].
Here, we re-interpret dropout with continuous noise as a variational method, and propose a generalization that we call variational dropout. In developing variational dropout we provide a firm
Bayesian justification for dropout training by deriving its implicit prior distribution and variational
objective. This new interpretation allows us to propose several useful extensions to dropout, such as
a principled way of making the normally fixed dropout rates p adaptive to the data.
4

3.1

Variational dropout with independent weight noise

If the elements of the noise matrix ? are drawn independently from a Gaussian N (1, ?), the marginal
distributions of the activations bm,j 2 B are Gaussian as well:
q (bm,j |A) = N (

m,j , m,j ),

with

m,j

=

K
X

am,i ?i,j , and

i=1

m,j

=?

K
X

2
a2m,i ?i,j
.

(10)

i=1

Making use of this fact, [22] proposed Gaussian dropout, a regularization method where, instead
of applying (9), the activations are directly drawn from their (approximate or exact) marginal distributions as given by (10). [22] argued that these marginal distributions are exact for Gaussian noise
?, and for Bernoulli noise still approximately Gaussian because of the central limit theorem. This
ignores the dependencies between the different elements of B, as present using (9), but [22] report
good results nonetheless.
As noted by [22], and explained in appendix B, this Gaussian dropout noise can also be interpreted
as arising from a Bayesian treatment of a neural network with weights W that multiply the input to
give B = AW, where the posterior distribution of the weights is given by a factorized Gaussian with
2
q (wi,j ) = N (?i,j , ??i,j
). From this perspective, the marginal distributions (10) then arise through
the application of the local reparameterization trick, as introduced in section 2.3. The variational
objective corresponding to this interpretation is discussed in section 3.3.
3.2

Variational dropout with correlated weight noise

Instead of ignoring the dependencies of the activation noise, as in section 3.1, we may retain the
dependencies by interpreting dropout (9) as a form of correlated weight noise:
B = (A ?)?, ?i,j ? N (1, ?) () bm = am W, with

0 0
W = (w10 , w20 , . . . , wK
) , and wi = si ?i , with q (si ) = N (1, ?),

(11)

where am is a row of the input matrix and bm a row of the output. The wi are the rows of the
weight matrix, each of which is constructed by multiplying a non-stochastic parameter vector ?i by
a stochastic scale variable si . The distribution on these scale variables we interpret as a Bayesian
posterior distribution. The weight parameters ?i (and the biases) are estimated using maximum
likelihood. The original Gaussian dropout sampling procedure (9) can then be interpreted as arising
from a local reparameterization of our posterior on the weights W.
3.3

Dropout?s scale-invariant prior and variational objective

The posterior distributions q (W) proposed in sections 3.1 and 3.2 have in common that they can
be decomposed into a parameter vector ? that captures the mean, and a multiplicative noise term
determined by parameters ?. Any posterior distribution on W for which the noise enters this multiplicative way, we will call a dropout posterior. Note that many common distributions, such as
univariate Gaussians (with nonzero mean), can be reparameterized to meet this requirement.
During dropout training, ? is adapted to maximize the expected log likelihood Eq? [LD (?)]. For this
to be consistent with the optimization of a variational lower bound of the form in (2), the prior on
the weights p(w) has to be such that DKL (q (w)||p(w)) does not depend on ?. In appendix C we
show that the only prior that meets this requirement is the scale invariant log-uniform prior:
p(log(|wi,j |)) / c,

i.e. a prior that is uniform on the log-scale of the weights (or the weight-scales si for section 3.2). As
explained in appendix A, this prior has an interesting connection with the floating point format for
storing numbers: From an MDL perspective, the floating point format is optimal for communicating
numbers drawn from this prior. Conversely, the KL divergence DKL (q (w)||p(w)) with this prior
has a natural interpretation as regularizing the number of significant digits our posterior q stores
for the weights wi,j in the floating-point format.
Putting the expected log likelihood and KL-divergence penalty together, we see that dropout training
maximizes the following variatonal lower bound w.r.t. ?:
Eq? [LD (?)]

DKL (q? (w)||p(w)),
5

(12)

where we have made the dependence on the ? and ? parameters explicit. The noise parameters ?
(e.g. the dropout rates) are commonly treated as hyperparameters that are kept fixed during training.
For the log-uniform prior this then corresponds to a fixed limit on the number of significant digits
we can learn for each of the weights wi,j . In section 3.4 we discuss the possibility of making this
limit adaptive by also maximizing the lower bound with respect to ?.
2
For the choice of a factorized Gaussian approximate posterior with q (wi,j ) = N (?i,j , ??i,j
), as
discussed in section 3.1, the lower bound (12) is analyzed in detail in appendix C. There, it is shown
that for this particular choice of posterior the negative KL-divergence DKL (q? (w)||p(w)) is not
analytically tractable, but can be approximated extremely accurately using

DKL [q (wi )|p(wi )] ? constant + 0.5 log(?) + c1 ? + c2 ?2 + c3 ?3 ,
with
c1 = 1.16145124,

c2 =

1.50204118,

c3 = 0.58629921.

The same expression may be used to calculate the corresponding term
posterior approximation of section 3.2.
3.4

DKL (q? (s)||p(s)) for the

Adaptive regularization through optimizing the dropout rate

The noise parameters ? used in dropout training (e.g. the dropout rates) are usually treated as fixed
hyperparameters, but now that we have derived dropout?s variational objective (12), making these
parameters adaptive is trivial: simply maximize the variational lower bound with respect to ?. We
can use this to learn a separate dropout rate per layer, per neuron, of even per separate weight. In
section 5 we look at the predictive performance obtained by making ? adaptive.
We found that very large values of ? correspond to local optima from which it is hard to escape due
to large-variance gradients. To avoid such local optima, we found it beneficial to set a constraint
? ? 1 during training, i.e. we maximize the posterior variance at the square of the posterior mean,
which corresponds to a dropout rate of 0.5.

4

Related Work

Pioneering work in practical variational inference for neural networks was done in [8], where a
(biased) variational lower bound estimator was introduced with good results on recurrent neural network models. In later work [14, 17] it was shown that even more practical estimators can be formed
for most types of continuous latent variables or parameters using a (non-local) reparameterization
trick, leading to efficient and unbiased stochastic gradient-based variational inference. These works
focused on an application to latent-variable inference; extensive empirical results on inference of
global model parameters were reported in [6], including succesful application to reinforcement
learning. These earlier works used the relatively high-variance estimator (3), upon which we improve. Variable reparameterizations have a long history in the statistics literature, but have only
recently found use for efficient gradient-based machine learning and inference [4, 13, 19]. Related
is also probabilistic backpropagation [9], an algorithm for inferring marginal posterior probabilities;
however, it requires certain tractabilities in the network making it insuitable for the type of models
under consideration in this paper.
As we show here, regularization by dropout [20, 22] can be interpreted as variational inference.
DropConnect [21] is similar to dropout, but with binary noise on the weights rather than hidden units.
DropConnect thus has a similar interpretation as variational inference, with a uniform prior over the
weights, and a mixture of two Dirac peaks as posterior. In [2], standout was introduced, a variation
of dropout where a binary belief network is learned for producing dropout rates. Recently, [15]
proposed another Bayesian perspective on dropout. In recent work [3], a similar reparameterization
is described and used for variational inference; their focus is on closed-form approximations of the
variational bound, rather than unbiased Monte Carlo estimators. [15] and [7] also investigate a
Bayesian perspective on dropout, but focus on the binary variant. [7] reports various encouraging
results on the utility of dropout?s implied prediction uncertainty.
6

5

Experiments

We compare our method to standard binary dropout and two popular versions of Gaussian dropout,
which we?ll denote with type A and type B. With Gaussian dropout type A we denote the pre-linear
Gaussian dropout from [20]; type B denotes the post-linear Gaussian dropout from [22]. This way,
the method names correspond to the matrix names in section 2 (A or B) where noise is injected.
Models were implemented in Theano [5], and optimization was performed using Adam [12] with
default hyper-parameters and temporal averaging.
Two types of variational dropout were included. Type A is correlated weight noise as introduced
in section 3.2: an adaptive version of Gaussian dropout type A. Variational dropout type B has
independent weight uncertainty as introduced in section 3.1, and corresponds to Gaussian dropout
type B.
A de facto standard benchmark for regularization methods is the task of MNIST hand-written digit
classification. We choose the same architecture as [20]: a fully connected neural network with 3
hidden layers and rectified linear units (ReLUs). We follow the dropout hyper-parameter recommendations from these earlier publications, which is a dropout rate of p = 0.5 for the hidden layers
and p = 0.2 for the input layer. We used early stopping with all methods, where the amount of
epochs to run was determined based on performance on a validation set.
Variance. We start out by empirically comparing the variance of the different available stochastic
estimators of the gradient of our variational objective. To do this we train the neural network described above for either 10 epochs (test error 3%) or 100 epochs (test error 1.3%), using variational
dropout with independent weight noise. After training, we calculate the gradients for the weights of
the top and bottom level of our network on the full training set, and compare against the gradient
estimates per batch of M = 1000 training examples. Appendix E contains the same analysis for the
case of variational dropout with correlated weight noise.
Table 1 shows that the local reparameterization trick yields the lowest variance among all variational
dropout estimators for all conditions, although it is still substantially higher compared to not having any dropout regularization. The 1/M variance scaling achieved by our estimator is especially
important early on in the optimization when it makes the largest difference (compare weight sample
per minibatch and weight sample per data point). The additional variance reduction obtained by our
estimator through drawing fewer random numbers (section 2.3) is about a factor of 2, and this remains relatively stable as training progresses (compare local reparameterization and weight sample
per data point).
stochastic gradient estimator
local reparameterization (ours)
weight sample per data point (slow)
weight sample per minibatch (standard)
no dropout noise (minimal var.)

top layer
10 epochs
7.8 ? 103
1.4 ? 104
4.9 ? 104
2.8 ? 103

top layer
100 epochs
1.2 ? 103
2.6 ? 103
4.3 ? 103
5.9 ? 101

bottom layer
10 epochs
1.9 ? 102
4.3 ? 102
8.5 ? 102
1.3 ? 102

bottom layer
100 epochs
1.1 ? 102
2.5 ? 102
3.3 ? 102
9.0 ? 100

Table 1: Average empirical variance of minibatch stochastic gradient estimates (1000 examples) for
a fully connected neural network, regularized by variational dropout with independent weight noise.
Speed. We compared the regular SGVB estimator, with separate weight samples per datapoint
with the efficient estimator based on local reparameterization, in terms of wall-clock time efficiency.
With our implementation on a modern GPU, optimization with the na??ve estimator took 1635 seconds per epoch, while the efficient estimator took 7.4 seconds: an over 200 fold speedup.
Classification error. Figure 1 shows test-set classification error for the tested regularization methods, for various choices of number of hidden units. Our adaptive variational versions of Gaussian
dropout perform equal or better than their non-adaptive counterparts and standard dropout under all
tested conditions. The difference is especially noticable for the smaller networks. In these smaller
networks, we observe that variational dropout infers dropout rates that are on average far lower than
the dropout rates for larger networks. This adaptivity comes at negligable computational cost.
7

(a) Classification error on the MNIST dataset

(b) Classification error on the CIFAR-10 dataset

Figure 1: Best viewed in color. (a) Comparison of various dropout methods, when applied to fullyconnected neural networks for classification on the MNIST dataset. Shown is the classification
error of networks with 3 hidden layers, averaged over 5 runs. he variational versions of Gaussian
dropout perform equal or better than their non-adaptive counterparts; the difference is especially
large with smaller models, where regular dropout often results in severe underfitting. (b) Comparison of dropout methods when applied to convolutional net a trained on the CIFAR-10 dataset, for
different settings of network size k. The network has two convolutional layers with each 32k and
64k feature maps, respectively, each with stride 2 and followed by a softplus nonlinearity. This is
followed by two fully connected layers with each 128k hidden units.
We found that slightly downscaling the KL divergence part of the variational objective can be beneficial. Variational (A2) in figure 1 denotes performance of type A variational dropout but with a
KL-divergence downscaled with a factor of 3; this small modification seems to prevent underfitting,
and beats all other dropout methods in the tested models.

6

Conclusion

Efficiency of posterior inference using stochastic gradient-based variational Bayes (SGVB) can often
be significantly improved through a local reparameterization where global parameter uncertainty is
translated into local uncertainty per datapoint. By injecting noise locally, instead of globally at the
model parameters, we obtain an efficient estimator that has low computational complexity, can be
trivially parallelized and has low variance. We show how dropout is a special case of SGVB with
local reparameterization, and suggest variational dropout, a straightforward extension of regular
dropout where optimal dropout rates are inferred from the data, rather than fixed in advance. We
report encouraging empirical results.
Acknowledgments
We thank the reviewers and Yarin Gal for valuable feedback. Diederik Kingma is supported by the
Google European Fellowship in Deep Learning, Max Welling is supported by research grants from
Google and Facebook, and the NWO project in Natural AI (NAI.14.108).

References
[1] Ahn, S., Korattikara, A., and Welling, M. (2012). Bayesian posterior sampling via stochastic gradient
Fisher scoring. arXiv preprint arXiv:1206.6380.
[2] Ba, J. and Frey, B. (2013). Adaptive dropout for training deep neural networks. In Advances in Neural
Information Processing Systems, pages 3084?3092.
[3] Bayer, J., Karol, M., Korhammer, D., and Van der Smagt, P. (2015). Fast adaptive weight noise. arXiv
preprint arXiv:1507.05331.
[4] Bengio, Y. (2013). Estimating or propagating gradients through stochastic neurons. arXiv preprint
arXiv:1305.2982.

8

[5] Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde-Farley,
D., and Bengio, Y. (2010). Theano: a CPU and GPU math expression compiler. In Proceedings of the
Python for Scientific Computing Conference (SciPy), volume 4.
[6] Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra, D. (2015). Weight uncertainty in neural networks. arXiv preprint arXiv:1505.05424.
[7] Gal, Y. and Ghahramani, Z. (2015). Dropout as a Bayesian approximation: Representing model uncertainty
in deep learning. arXiv preprint arXiv:1506.02142.
[8] Graves, A. (2011). Practical variational inference for neural networks. In Advances in Neural Information
Processing Systems, pages 2348?2356.
[9] Hern?andez-Lobato, J. M. and Adams, R. P. (2015). Probabilistic backpropagation for scalable learning of
Bayesian neural networks. arXiv preprint arXiv:1502.05336.
[10] Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. R. (2012). Improving
neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580.
[11] Hinton, G. E. and Van Camp, D. (1993). Keeping the neural networks simple by minimizing the description length of the weights. In Proceedings of the sixth annual conference on Computational learning theory,
pages 5?13. ACM.
[12] Kingma, D. and Ba, J. (2015). Adam: A method for stochastic optimization. Proceedings of the International Conference on Learning Representations 2015.
[13] Kingma, D. P. (2013). Fast gradient-based inference with continuous latent variable models in auxiliary
form. arXiv preprint arXiv:1306.0733.
[14] Kingma, D. P. and Welling, M. (2014). Auto-encoding variational Bayes. Proceedings of the 2nd International Conference on Learning Representations.
[15] Maeda, S.-i. (2014). A Bayesian encourages dropout. arXiv preprint arXiv:1412.7003.
[16] Neal, R. M. (1995). Bayesian learning for neural networks. PhD thesis, University of Toronto.
[17] Rezende, D. J., Mohamed, S., and Wierstra, D. (2014). Stochastic backpropagation and approximate
inference in deep generative models. In Proceedings of the 31st International Conference on Machine
Learning (ICML-14), pages 1278?1286.
[18] Robbins, H. and Monro, S. (1951). A stochastic approximation method. The Annals of Mathematical
Statistics, 22(3):400?407.
[19] Salimans, T. and Knowles, D. A. (2013). Fixed-form variational posterior approximation through stochastic linear regression. Bayesian Analysis, 8(4).
[20] Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014). Dropout: A simple
way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929?
1958.
[21] Wan, L., Zeiler, M., Zhang, S., Cun, Y. L., and Fergus, R. (2013). Regularization of neural networks using
dropconnect. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), pages
1058?1066.
[22] Wang, S. and Manning, C. (2013). Fast dropout training. In Proceedings of the 30th International
Conference on Machine Learning (ICML-13), pages 118?126.
[23] Welling, M. and Teh, Y. W. (2011). Bayesian learning via stochastic gradient Langevin dynamics. In
Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 681?688.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

