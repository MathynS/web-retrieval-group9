query sentence: backpropagated error in neural-network
---------------------------------------------------------------------
title: 1100-tempering-backpropagation-networks-not-all-weights-are-created-equal.pdf

temper backpropag network weight creat equal nicol n. schraudolph evotec biosystem gmbh grandweg hamburg germani nici evotec.d terrenc j. sejnowski comput neurobiolog lab salk institut bioi studi san diego ca usa terri salk.edu abstract backpropag learn algorithm typic collaps network structur singl vector weight paramet optim suggest perform may improv util structur inform instead discard introduc framework temper weight accord temper model activ error signal treat approxim independ random variabl characterist scale weight chang match ofth residu allow structur properti node 's fan-in fan-out affect local learn rate backpropag error model also permit calcul upper bound global learn rate batch updat turn lead differ updat rule bias non-bia weight approach yield hitherto unparallel perform famili relat benchmark deep multi-lay network batch learn momentum delta-bar-delta algorithm converg optim learn rate sped order magnitud introduct although neural network structur graph learn algorithm typic view singl vector paramet optim inform network 's architectur thus discard favor presumpt isotrop weight space notion priori weight network creat equal serv decoupl learn process network design make larg bodi function optim techniqu direct applic backpropag learn discard structur inform hold valuabl clue effici weight optim adapt step size second-ord gradient techniqu battiti may n. n. schraudolph t. j. sejnowski recov consider comput expens ad hoc attempt incorpor structur inform fan-in plaut local learn rate becom familiar part backpropag lore deri comprehensi framework call temper demonstr effect temper base model acti viti error signal backpropag network independ random variabl allow us calcul activ weightinvari upper bound effect synchron weight updat node 's activ deriv appropri local step size paramet relat maxim chang node 's acti vi ty characterist scale residu global learn rate subsequ deriv upper bound global learn rate batch learn suggest compon error signal given special treatment experi show result method error shunt allow global learn rate approach predict maximum high effici learn perform local learn rate consid neural network feedforward activ given yj wij ieaj aj denot set anterior node feed direct node nonlinear typic sigmoid activ function impli node activ appropri sequenc valu clamp repres extern input local learn rate of'1j node gradient descent object function produc weight updat linear ij around yj approxim result chang activ xj ieaj ieaj goal put scale xj relat error signal tsj specif averag mani train sampl want chang output activ node respons pattern limit certain proport given global learn rate residu achiev relat variat train set error signal denot averag train sampl formal approach may interpret diagon approxim invers fischer inform matrix amari implement deriv upper bound left-hand side equat right-hand side replac acti vity-depend slope ij maximum valu maxl/j u assum correl input error tsj obtain note correl minim local weight updat temper backpropag network all weight creat equal provid ej lea satisfi set local learn rate tj fj tj ej there sever approach comput upper bound total squar input power one option would calcul latter empir train though rais sampl stabil issu extern input may precomput order upper bound base prior knowledg train data input node network assum independ deriv rang activ function p fd p fd ffiuax/i u ej ej ieaj note all node use activ function obtain well-known vfan-in heurist plaut special case error backpropag deriv local learn rate tacit use error signal stand-in residu proper distanc target output node scale error never exceed residu note convent quadrat error simplifi pj what remaind network unlik krogh wish prescrib definit target henc residu hidden node instead shall use bound independ argument scale backpropag error signal rough appropri magnitud purpos introduc attenu coeffici aj error backpropag equat c5j aj wjj c5j jep pi denot set posterior node fed direct node posit appropri variat c5i weight averag variat backpropag error wherea assum independ c5j replac slope ii maximum valu give us jep again equat right-hand side inequ satisfi yield fdjip t n. n. schraudolph t. j. sejnowski note incorpor ofth weight ad hoc priori reason scale node 's step size proport size vector outgo weight chosen simpli produc weight-invari valu attenu coeffici scale backpropag error could control rigor expens recalcul weight updat global learn rate deriv appropri global learn rate batch weight updat liwij dj tet non-redund train sampl t. assum independ zero-mean residu virtu condit ensur dj variat batch weight updat exceed residu use global learn rate l/jitf even redund train set forc us use lower rate know upper bound effect allow educ guess save consider time practic error shunt remain deal assumpt made residu zero-mean compon error requir learn rate invers proport batch size far rate permiss zero-mean residu suggest handl compon error signal separ proper job bias weight updat accord order allow learn rate close center subtract mean all weight error signal tet t/j tet dj dj tet note sum must collect batch implement backpropag anyway addit statist requir averag input activ inde batch updat center error equival center input known assist learn remov larg eigenvalu hessian lecun we expect onlin implement perform best input error signal center improv stochast approxim temper backpropag network all weight creat equal person oo~o~ooooooo tjeff tj bi i person ooooo~oooooo tj tj oooooooooo~o relationship figur backpropag network learn famili relat hinton experiment setup we test idea famili relat task hinton backpropag network given exampl famili member relationship input must indic output famili member fit relat descript accord under famili tree architectur figur consist central associ layer hidden unit surround three encod layer act inform bottleneck forc network make deep structur data explicit input present network canon local encod ani given train exampl exact one input two input layer activ account alway activ bias input squar input power temper layer thus sinc output use local code one two target time activ we therefor attenu error signal immedi preced layer we use crossentropi error logist squash function output give prefer hyperbol tangent hidden unit p tanh tanh illustr impact temper architectur we translat combin effect local learn rate error attenu effect learn rate layer shown right figur we observ effect learn rate largest near output decreas toward input due error attenu contrari textbook opinion lecun haykin page we find unequ step size fact key effici learn we suspect logist squash function may owe popular larg error attenu side-effect inher maximum slope expect temper applic varieti backpropag learn algorithm we present first result batch learn momentum delta-bar-delta rule jacob algorithm test three condit convent temper describ section temper error shunt all experi perform custom simul base xerion condit global learn rate tj empir optim single-digit precis fastest reliabl learn perform measur sum empir mean standard deviat epoch requir reach given low valu cost function all paramet held variant across experi valu shown tabl chosen advanc not bias result 2this possibl strict layer network shortcut skip-through connect topolog non-adjac layer time write xerion neural network simul successor ut avail anonym file transfer ai.toronto.edu directori pub/xerion n. n. schraudolph t. sejnowski paramet val ue train set size epoch momentum paramet uniform initi weight rang weight decay rate per epoch i valu i paramet zero-error radius around target accept error weight cost delta-bar-delta gain increment delta-bar-delta gain decrement tabl invari paramet set experim~nt experiment result tabl list empir mean standard deviat ten restart number epoch requir learn famili relat task under condit optim learn rate produc perform train time convent backpropag quit long typic deep multi-lay network comparison hinton report around epoch this problem learn rate momentum optim person communic much faster converg though far looser criterion recent observ onlin algorithm o'reilli temper hand seen speed two batch learn method almost order magnitud reduc not averag train time also it coeffici variat indic reliabl optim process note temper make simpl batch learn momentum run twice fast delta-bar-delta algorithm this remark sinc delta-bar-delta use onlin measur continu adapt learn rate individu weight wherea temper mere prescal base network 's architectur we take this evid temper establish appropri local step size upfront delta-bar-delta must discov empir this suggest use temper set initi equilibrium learn rate deltabar-delta it may possibl reap benefit prescal adapt step size control inde tabl confirm respect speedup due temper deltabar-delta multipli two approach are combin this fashion final addit error shunt increas learn speed yet allow global learn rate brought close maximum we would predict discuss our experi we found temper dramat improv speed reliabl learn more network architectur data set learn algorithm temper explor general applic limit this approach we also hope extend it recurr network onlin learn error shunt proven use in facilit near-maxim global learn rate rapid optim algorithm condit convent temper temper shunt batch momentum mean st.d delta-bar-delta mean st.d tabl epoch requir learn famili relat task temper backpropag network not all weight are creat equal although scheme may speed backpropag compar amount our approach uniqu advantag it comput cheap implement local learn error attenu rate are invari respect network weight activ thus need recalcul network architectur chang more import even advanc gradient descent method typic retain the isotrop weight space assumpt we improv upon one would therefor expect benefit temper much delta-bar-delta in the experi report for instanc temper could use set non-isotrop model-trust region for conjug second-ord gradient descent algorithm final restrict fix learn rate attenu factor for we arriv at a simplifi method like leav room for improv possibl refin includ take weight vector size account attenu error signal measur quantiti onlin instead reli on invari upper bound how adapt temper scheme compar interact with exist techniqu for effici backpropag learn remain explor acknowledg we would like to thank peter dayan rich zemel jenni orr for instrument in discuss help shape this work geoff hinton not offer invalu comment but the sourc both our simul benchmark problem n. schraudolph receiv financi support the mcdonnell-pew center for cognit neurosci in san diego the robert bosch stiftung gmbh
----------------------------------------------------------------

title: 358-continuous-speech-recognition-by-linked-predictive-neural-networks.pdf

continu speech recognit link predict neural network joe tebelski alex waibel bojan petek otto schmidbauer school comput scienc carnegi mellon univers pittsburgh pa abstract present larg vocabulari continu speech recognit system base link predict neural network lpnn 's system use neural network predictor speech frame yield distort measur use one stage dtw algorithm perform continu speech recognit system alreadi deploy speech speech translat system current achiev word accuraci task perplex respect outperform sever simpl hmms test also found accuraci speed lpnn slight improv judici use hidden control input conclud discuss strength weak predict approach introduct neural network prove use difficult task speech recognit easili train comput smooth nonlinear nonparametr function input space output space speech recognit function often comput network classif spectral frame map finit set class phonem theori classif network approxim optim bayesian discrimin function practic yield high accuraci howev integr phonem classifi speech recognit system nontrivi sinc classif decis tend binari binari phoneme-level error tend confound word-level hypothes circumv problem neural network train must care integr word level train altern function com tebelski waibel petek schmidbauer pute network predict spectral frame map predict spectral frame provid simpl way get non-binari distort measur straightforward integr speech recognit system predict network use success small vocabulari larg vocabulari speech recognit system paper describ prediction-bas lpnn system perform larg vocabulari continu speech recognit alreadi deploy within speech speech translat system present experiment result discuss strength weak predict approach link predict neural network lpnn system base canon phonem model logic concaten order use linkag pattern creat templat differ word make lpnn suitabl larg vocabulari recognit canon phonem model short sequenc neural network number net sequenc correspond granular phonem model phone model network nonlinear multilay feedforward predict sens given short section speech network requir extrapol raw speech signal rather classifi thus predict network produc time-vari model speech signal accur region correspond phonem network train inaccur region better model network phonem thus recogn indirect virtu relat accuraci differ predict network various section speech note howev phonem classifi frame level instead continu score predict error accumul various word candid decis made word level final appropri train test algorithm purpos train procedur train network becom better predictor caus network special differ phonem given known train utter train procedur consist three step forward pass network make predict across speech sampl comput euclidean distanc matrix predict error predict actual speech frame see figur align step comput optim time-align path input speech correspond predictor net use dynam time warp backward pass predict error backpropag network accord segment given align path see figur henc backpropag caus net becom better predictor align path induc special network differ phonem test perform use one stage algorithm classic extens dynam time warp algorithm continu speech continu speech recognit link ftedict neural network predict error ctj phonem predictor phonem predictor figur forward pass train canon phonem model sequenc predict network shown triangl word repres linkag pattern canon phonem model shown area triangl accord phonet spell word here train word forward pass predict error shown black circl comput predictor frame input speech predict error rout linkag pattern fill distanc matrix upper right ctj figur backward pass train after dtw align path comput error backpropag various predictor respons point along align path back propag error signal point vector differ predict actual frame teach network becom better predictor also caus network special differ phonem tebelski waibel petek schmidbauer recognit experi evalu lpnn system databas continu speech record cmu databas consist english sentenc use vocabulari word compris dialog domain confer registr train test version databas record quiet offic multipl speaker speaker-depend experi record digit sampl rate khz ham window fft comput produc melscal spectral coeffici everi msec experi use context-independ phonem model includ one silenc 6-state phonem topolog similar one use spico system lie lit tv i ud il iffll iii eh clii'ereki ih seero 1ft iii 1ft krahfriftn er i lh llliltlll iii llll iiiiiiiiu lu iii. iiii.iii i.ii ui iiiii iiii iiiiuiiililil i iu iiuiilililil iiiu i. iii hl uilili i iuilil i i figur actual predict spectrogram figur show result test lpnn system typic sentenc top portion actual spectrogram this utter bottom portion show frame-by-fram predict made network specifi point along optim align path similar two spectrogram indic hypothesi form good acoust model unknown utter fact hypothesi correct this case speaker-depend experi use two male speaker system averag word accuraci task perplex respect order confirm predict network make posit contribut overal system perform set comparison lpnn sever pure hmm system we replac predict network univari gaussian whose mean varianc determin analyt label train data result hmm achiev word accuraci compar achiev lpnn condit singl speaker perplex we also provid hmm delta coeffici direct avail lpnn achiev thus lpnn outperform these simpl hmms continu speech recognit link r-edict neural network hidden control experi anoth seri experi we vari lpnn architectur introduc hidden control input propos levin idea illustr in figur sequenc independ network replac singl network modul equival number hidden control input bit distinguish state sequenc predict network hidden control network figur sequenc network correspond singl hidden control network theoret advantag hidden control architectur reduc number offre paramet in system number network reduc one expos train data certain point general may improv system also run faster sinc partial result redund forward pass comput save notic howev total number forward pass unchang final save in memori signific in experi we found replac 2-state phonem model equival hidden control network recognit accuraci improv slight system ran much faster hand when we replac phonem network in entir system singl hidden control network whose hidden control input repres phonem well as state recognit accuraci degrad signific henc hidden control may use use judici current limit predict net while lpnn system good model acoust speech present tend suffer poor discrimin in word given segment speech phonem model tend make similar good predict render phonem model fair confus exampl figur show actual spectrogram frame-by-fram predict made model model disappoint model fair accur predictor entir utter this problem aris each predictor receiv train in small region input acoust space frame correspond phonem consequ when a predictor shown input frame comput tebelski waibel petek schmidbauer lti1 rl nu iiii ui i i iiu i u. lh i iii'ii ii .ii iih iiii'i i1 i'ii r tt~ tmmtmtmtnm tnt ijii1 rrm~ i1tt rrmrrm nt11 nnilpl_rm 'm mn rrmtm nntti1ttttt11tttm mml i1 i i i.ii iii iii i hiiiiii i iiii iuff'. i .iiiiiiiiiii tt iiii i i i iil i i tu i.h iii i i figur actual spectrogram correspond predict phonem model undefin output may overlap output predictor in word predictor current train posit instanc obvious predict output target meaning negat instanc this lead problemat undefin region predictor clear type discriminatori train techniqu introduc yield better perform in predict base recogn conclus we studi the perform link predict neural network for larg vocabulari continu speech recognit use a 6-state phonem topolog without durat model optim the lpnn achiev an averag accuraci task perplex respect this better the perform sever simpl hmms we test further experi reveal the accuraci speed the lpnn slight improv the judici use hidden control input the main advantag predict network produc non-binari distort measur in a simpl eleg way virtu nonlinear model the dynam properti speech curvatur better linear predict model main current weak is poor discrimin sinc their strict posit train caus all to make confus accur predict in context futur research concentr on improv the discriminatori power the lpnn techniqu as correct train explicit context depend phonem model function word model continu speech recognit link ltedict neural network acknowledg the author grate acknowledg the support darpa the nation scienc foundat atr interpret telephoni research laboratori nec corpor b. petek also acknowledg support the univers ljubljana and the research council of slovenia schmidbauer acknowledg support employ siemen ag germani
----------------------------------------------------------------

title: 2137-relative-density-nets-a-new-way-to-combine-backpropagation-with-hmms.pdf

relat densiti net new way combin backpropag hmm andrew d. brown depart comput scienc univers toronto toronto canada m5s andi cs.utoronto.ca geoffrey e. hinton gatsbi unit ucl london uk wcin 3ar hinton gatsby.ucl.ac.uk abstract logist unit first hidden layer feedforward neural network comput relat probabl data point two gaussian lead us consid substitut densiti model present architectur perform discrimin learn hidden markov model use network mani small hmm 's experi speech data show superior standard method discrimin train hmm introduct standard way perform classif use generat model divid train case respect class hen train set class condit model unsupervis approach classif appeal two reason possibl reduc overfit model learn class-condit input densiti p xlc rather han input condit class probabl p clx also provid model densiti good match under data densiti decis provid probabilist model bay optim problem unsupervis approach use probabilist model classif reason comput effici analyt conveni simpl generat model typic use optim procedur longer hold reason usual advantag train classifi discrimin paper look specif problem learn hmm classifi speech sequenc applic area assumpt hmm correct generat model data inaccur discrimin method train success first section give overview current method discrimin train hmm classifi introduc new type multi-lay backpropag network take better advantag hmm discrimin final present simul compar two method s1 c1 hmm sequenc figur alphanet one hmm per class comput score sequenc feed softmax output layer alphanet discrimin learn unsupervis way use hmm classifi collect sequenc use baum-welch algorithm fit one hmm per class new sequenc classifi comput probabl sequenc model assign one highest probabl speech recognit one commonest applic hmm unfortun hmm poor model speech product process reason speech research look possibl improv perform hmm classifi use inform negat exampl exampl drawn class one hmm meant model one way comput mutual inform class label data hmm densiti maxim object function later shown procedur could view type neural network figur input network log-prob score c xl tih sequenc hidden markov model model one hmm per class output softmax non-linear train model maxim log probabl correct classif lead classifi perform better equival hmm model train sole unsupervis manner such architectur term aiphanet may implement recurr neural network mimic forward pass forward-backward algorithm.l backpropag network densiti compar multi-lay feedforward network usual thought flexibl non-linear regress model use logist function non-linear hidden layer interest interpret oper perform hidden unit given mixtur two gaussian know compon prior compon densiti posterior probabl gaussian generat observ logist function whose argument negat log-odd two class clear seen rearrang lthe result forward pass probabl hidden state condit past observ alpha standard hmm terminolog express posterior p xi9o p qo p xi9o p qo p xi9d qd p qolx exp log p x iqo p x lqd log p qo p ql class condit densiti question multivari gaussian p xi9k exp equal covari matric written familiar form pk t posterior class probabl may p qo ix thus multi-lay perceptron view comput pairwis posterior gaussian input space combin output layer comput decis new kind discrimin net view feedforward network suggest variat kind densiti model use place gaussian input space particular instead perform pairwis comparison gaussian unit first hidden layer perform pairwis comparison densiti input sequenc differ hmm 's given sequenc log-prob sequenc hmm comput differ log-prob use input logist hidden unit equival comput posterior respons mixtur two hmm 's equal prior probabl order maxim leverag inform captur hmm 's use hidden unit possibl pair includ output hidden unit given use index set unord pair hmm 's result hidden layer comput combin use fulli connect layer free weight final pass soft max function make final decis ak w n kh mn 2we take time averag log-prob scale input independ length sequenc densiti compar unit figur multi-lay densiti net hmm 's input layer hidden layer unit perform pairwis comparison hmm use shorthand logist function pk valu kth output unit result architectur shown figur unit hidden layer take input differ log-prob two hmm thought fix layer weight connect hidden unit pair hmm 's weight contrast alphanet alloc one hmm model class network requir one-to-on align model class get maximum discrimin benefit hmm 's compar pair anoth benefit architectur allow us use hmm 's class unsupervis approach train hmm classifi problemat depend assumpt singl hmm good model data case speech poor assumpt train classifi discrimin allevi drawback multi-lay classifi goe even direct allow mani hmm 's use learn decis boundari class intuit mani small hmm 's far effici way character sequenc one big hmm when mani small hmm 's cooper generat sequenc mutual inform differ part generat sequenc scale linear number hmm 's logarithm number hidden node hmm deriv updat relat densiti network learn algorithm rdn backpropag algorithm appli network architectur defin equat output layer distribut class membership data point xl t parameter softmax function minim cross-entropi loss function tk logpk pk valu kth output unit tk indic variabl equal true class take deriv express respect input output unit yield pk oak oak tk pk h mn ow mn oak ow mn deriv output mn th hidden unit respect output ith hmm oh mn bin bim bin indic equal zero otherwis deriv chain deriv backpropag output hidden layer final step backpropag procedur need deriv log-likelihood hmm respect paramet experi use hmm singl axis-align gaussian output densiti per state use follow notat paramet aij transit probabl state state 7ri initi state prior mean vector state vi vector varianc state set hmm paramet also use variabl st repres state hmm time make use properti latent variabl densiti model deriv log-likelihood equal expect deriv joint log-likelihood posterior distribut hmm mean p sl tlxl t log p xl t sl ti1-l sl t joint likelihood hmm logp xl t log 7ri ll b jb log aij j iogvi'd canst denot expect posterior distribut expect state occup transit distribut necessari expect comput forward backward algorithm could take deriv respect function direct would requir constrain gradient descent probinstead reparameter model use abil varianc softmax basi probabl vector exponenti basi varianc paramet this choic basi allow us unconstrain optim new basi new paramet defin follow exp e ji exp 1ji exp e exp v exp oi this result follow deriv o xl oo jb aij s1 1fi st f..li f..li ij when chain error signal backpropag output deriv give us direct move paramet hmm order increas log probabl correct classif sequenc experi evalu relat merit rdn compar alphanet speaker identif task data taken cslu speaker recognit corpus consist speaker utter phrase consist differ sequenc connect digit record multipl time cours record session data pre-emphas fourier transform frame frame rate lom it filter use bandpass mel-frequ scale filter log magnitud filter respons then use featur vector hmm 's this pre-process reduc data dimension retain spectral structur while mel-cepstr coeffici typic recommend use axis-align gaussian destroy spectral structur data would like allow possibl mani hmm 's special particular sub-band frequenc domain this treat varianc measur import particular frequenc band use larg varianc unimport band small one for band they pay particular attent compar rdn alphanet three model implement control first network similar architectur rdn shown figur except instead fix connect hidden unit set adapt weight hmm 's refer this network compar densiti net second control experi use architectur similar cdn without hidden layer singl layer adapt weight direct connect hmm 's softmax output unit label this architectur cdn-l cdn-l differ alphanet softmax output unit adapt connect hmm 's vari number hmm 's wherea alphanet one hmm per class direct connect softmax output unit final implement version network similar alphanet use mixtur gaussian input densiti model point this comparison see if hmm actual achiev benefit model tempor aspect speaker recognit task experi rdn construct set 4-state hmm 's compar four network match number free paramet except for mognet case mognet we use number gaussian mixtur model hmm 's alphanet number hidden state thus it fewer paramet it lack transit probabl hmm we ran experi four time alphanet magnet cdn ej rdn cdn-1 rdn alphanet architectur cdn-1 gj ci gj cio.5 meg net architectur cdn alphanet rdn cdn magnet architectur cdn-1 rdn alphanet megnet cdn cdn-1 architectur figur result experi for rdn hmm valu for alphanet mognet we vari number state hmm 's gaussian mixtur respect for cdn model we use number 4-state hmm 's rdn vari number unit hidden layer network sinc cdn-1 network hidden unit we use number hmm 's rdn vari number state hmm experi repeat time differ training-test set split model train use iter conjug gradient optim procedur result boxplot in figur show result classif perform run in each experi compar alphanet rdn we see rdn consist outperform alphanet in all four experi differ in perform pair t-test signific level this indic given classif network a fix number paramet advantag use mani small hmm use all pairwis inform observ sequenc oppos use a network a singl larg hmm per class in third experi involv mognet we see perform compar alphanet this suggest hmm 's abil model tempor structur data realli necessari for speaker classif task we set it nevertheless perform the alphanet 3if we done text-depend speaker identif instead multipl digit phrase the mognet less the rdn unfortun the cdn cdn-l network perform much wors we expect while we expect model perform similar the rdn it seem the optim procedur take much longer model this probabl becaus the small initi weight the hmm 's the next layer sever attenu the backpropag error deriv use train the hmm a result the cdn network converg proper in the time allow conclus we introduc relat densiti network shown this method discrimin learn mani small densiti model in place a singl densiti model per class benefit in classif perform in addit may a small speed benefit use mani smaller hmm compar a big one comput the probabl a sequenc an hmm order o tk the length the sequenc the number hidden state in the network thus smaller hmm evalu faster howev this somewhat counterbalanc the quadrat growth in the size the hidden layer as increas acknowledg we would like to thank john bridl chris william radford neal sam rowei zoubin ghahramani the anonym review for help comment
----------------------------------------------------------------

title: 1820-combining-ica-and-top-down-attention-for-robust-speech-recognition.pdf

combin ica top-down attent robust speech recognit un-min bae soo-young lee depart electr engin comput scienc brain scienc research center korea advanc institut scienc technolog kusong-dong yusong-gu taejon korea bum neuron.kaist.ac.kr syle ee.kaist.ac.kr abstract present algorithm compens mismatch characterist real-world problem assumpt independ compon analysi algorithm provid addit inform ica network incorpor top-down select attent mlp classifi ad separ signal channel error classifi backpropag ica network backpropag process result estim expect ica output signal top-down attent then unmix matrix retrain accord new cost function repres backpropag error well independ modifi densiti recov signal densiti appropri classif noisi speech signal record real environ algorithm improv recognit perform show robust parametr chang introduct independ compon analysi ica method blind signal separ ica linear transform data statist independ possibl ica depend sever assumpt linear mix sourc independ may satisfi mani real-world applic order appli ica real-world problem necessari either releas assumpt compens mismatch anoth method paper present complementari approach compens mismatch top-down select attent classifi ica network provid addit inform signal-mix environ new cost function defin retrain unmix matrix ica network consid propag inform stationari mix environ averag adapt iter feedback oper adjust featur space help classif perform process regard select attent model input pattern adapt accord top-down infor mation propos algorithm appli noisi speech recognit real environ show effect feedback oper propos algorithm feedback oper base select attent previous mention ica suppos sever assumpt exampl one assumpt linear mix condit general inevit nonlinear microphon record input signal mismatch assumpt ica real mix condit caus unsuccess separ sourc overcom problem method suppli valuabl inform rca network propos learn phase ica unmix matrix subject signal-mix matrix input pattern under stationari mix environ mix matrix fix iter provid addit inform mix matrix contribut improv blind signal separ perform algorithm perform feedback oper classifi ica network test phase adapt unmix matric ica accord newli defin measur consid independ classif error result adapt input space classifi improv recognit perform process inspir select attent model calcul expect input signal accord top-down inform test phase shown figur ica separ signal nois melfrequ cepstral coeffici mfccs extract featur vector deliv classifi multi-lay perceptron after classif error function classifi defin m1p ymlp tmlp target valu output neuron ymlp general target valu known determin output ymlp target valu highest output set other set nonlinear function classifi bipolar sigmoid function algorithm perform gradient-desc calcul error backpropag reduc error comput requir chang input valu classifi final unmix signal ica network then lean rule ica algorithm chang consid variat newli defin cost function ica network includ error backpropag term well joint entropi yica output yica eica h yica utarget u utarget h yica estim recov sourc coeffici repres relat import two term learn rule deriv use gradient descent cost function ex i p u uh w input signal rca network first term learn rule ica applic complex-valu data frequenc input speech block frame ham window fourier transform ica retrain ica linear-to-mel freq filter bank convers mel-to-linear freq filter bank convers frame normal frame re-norm mlp classif mlp backpropag figur real-world speech recognit feedback oper classifi ica network domain real environ substanti time delay occur observ input signal convolv mixtur sourc linear mixtur mix model longer matrix this case blind signal separ use ica achiev frequenc domain complex score function tanh re z tanh im z procedur test phase summar follow test input perform forward oper classifi pattern defin error function classifi perform error backpropag find requir chang unmix signal ica defin cost function ica network updat unmix matrix learn rule then go step newli defin error function ofth classifi caus overfit problem use updat unmix matrix ica classif perform good averag chang unmix matrix total input pattern contribut improv recognit perform consid assumpt ica assumpt ica summar follow figur nonlinear mix model due distort microphon sourc linear mix sourc mutual independ at one sourc normal distribut number sensor equal greater number sourc no sensor nois low addit nois signal permit assumpt releas enough sensor assumpt also neglig sourc distribut usual approxim super-gaussian laplacian distribut speech recognit problem speech recognit real mix environ nonlinear microphon inevit problem figur show nonlinear mix model nonlinear function denot distort microphon origin sourc observ signal estim recov sourc if sourc mutual independ random variabl 8r still independ voo vlo densiti zl voo+vlo equal convolut densiti voo vlo pvoo zl vlo pvio vlo dvlo p zl after observ signal xl linear mixtur two independ compon due nonlinear distort assumpt sourc independ violat this situat hard expect would lea solut assert solut reliabl even if xl two independ compon case linear distort microphon conflict independ sourc densiti approxim densiti independ compon observ signal differ origin sourc may far densiti approxim propos algorithm solut this problem train phase classifi learn noiseless data densiti xl use learn p xd aoo second backpropag term cost function chang unmix matrix adapt densiti unmix signal densiti classifi tabl recognit rate noisi speech record fighter nois snr mlp lea propos algorithm ljlean train data lodb 5db ljlean test data 5db learn this clue lea solut iter oper total data induc averag chang unmix matrix becom rough function nonlinear certain densiti p sl subject everi pattern noisi speech recognit real environ propos algorithm appli isolated-word speech recognit input data convolv mixtur speech nois record real environ speech data set consist korean word speaker fighter nois speech babbl nois use nois sourc lea network two input two output signal nois sourc tabl show recognit result three method mlp mlp standard lea propos algorithm train data mean data use learn classifi test data rest lea improv classif perform compar mlp heavy-nois case in case clean data lea contribut recognit recognit rate lower mlp propos algorithm show better recognit perform standard lea train test data especi clean data propos algorithm improv recognit rate mlp in case algorithm reduc fals recognit rate in comparison standard lea signal nois ratio snrs higher low nois classif perform mlp relat reliabl mlp provid lea network help inform howev heavi nois recognit rate mlp sharpli decreas error backpropag hard provid valuabl inform lea network overal improv train data higher test data this recognit perform mlp better train data as shown in figur iter feedback oper decreas the fals recognit rate the variat the unknown paramet in n't affect the final recognit perform the variat the learn rate for updat the unmix matrix also n't affect the final perform influenc the converg time reach the final recognit rate the learn rate fix regardless snr in the experi discuss the propos algorithm approach complement lea provid addit inform base top-down select attent pre-train mlp classifi the error backpropag oper adapt the densiti recov signal tabl the recognit rate noisi speech record speech babbl nois snr mlp ica the propos algorithm ljlean train data 15dtl lodtl 5dtl ljlean test data 15dtl 10dtl 5dtl accord the new cost function ica this help ica find the solut proper for classif under the nonlinear independ violat this need the stationari condit for nonstationari environ mixtur model like the ica mixtur model consid the ica mixtur model assign class membership environ categori separ independ sourc in each class complet settl the nonlinear problem in real environ it necessari introduc scheme model the nonlinear such as the distort microphon multi-lay ica an approach to model nonlinear in the noisi recognit problem the propos algorithm improv recognit perform compar to ica alon especi in moder nois case the algorithm remark reduc the fals recognit rate this due to the high classif perform the pre-train mlp in the case heavi nois the expect ica output estim the top-down attent may accur the select attent help much it natur we onli put attent to familiar subject therefor robust classifi may need for signal with heavi nois acknowledg this work support as a brain scienc engin research program sponsor korean ministri scienc technolog
----------------------------------------------------------------

title: 5953-natural-neural-networks.pdf

natur neural network guillaum desjardin karen simonyan razvan pascanu koray kavukcuoglu gdesjardin simonyan razp korayk google.com googl deepmind london abstract introduc natur neural network novel famili algorithm speed converg adapt intern represent train improv condit fisher matrix particular show specif exampl employ simpl effici reparametr neural network weight implicit whiten represent obtain layer preserv feed-forward comput network network train effici via propos project natur gradient descent algorithm prong amort cost reparametr mani paramet updat close relat mirror descent onlin learn algorithm highlight benefit method unsupervis supervis learn task showcas scalabl train large-scal imagenet challeng dataset introduct deep network proven extrem success across broad rang applic deep complex structur afford rich model capac also creat complex depend paramet make learn difficult via first order stochast gradient descent long sgd remain workhors deep learn abil extract highlevel represent data may hinder difficult optim evidenc boost perform offer batch normal incept architectur though adopt remain limit natur gradient appear ideal suit difficult optim issu follow direct steepest descent probabilist manifold natur gradient make constant progress cours optim measur kullback-leibl diverg consecut iter util proper distanc measur ensur natur gradient invari parametr model unfortun applic limit due high comput cost natur gradient descent ngd typic requir estim fisher inform matrix fim squar number paramet wors requir comput invers truncat newton method avoid explicit form fim memori requir expens iter procedur comput invers comput wast take account high structur natur deep model inspir recent work model reparametr approach start simpl question devis neural network architectur whose fisher constrain ident import question sgd ngd would equival result model main contribut paper provid simpl theoret justifi network reparametr approxim via first-ord gradient descent block-diagon natur gradient updat layer method comput effici due local natur reparametr base whiten amort natur algorithm second contribut unifi mani heurist common use train neural network roof natur gradient highlight import connect model reparametr mirror descent final showcas effici scalabl method across broad-rang experi scale method standard deep auto-encod larg convolut model imagenet train across multipl gpus knowledg first-tim non-diagon natur gradient algorithm scale problem magnitud natur gradient section provid necessari background deriv particular form fim whose structur key effici approxim tailor develop method classif set approach general regress densiti estim overview consid problem fit paramet rn model p empir distribut log-loss denot observ vector associ label concret stochast optim problem aim solv argmin log p defin per-exampl loss stochast gradient descent sgd perform minim iter follow direct steepest descent given column vector paramet updat use rule learn rate equival proxim form gradient descent reveal precis natur argmin ri name iter solut auxiliari optim problem control distanc consecut iter use distanc contrast natur gradient reli kl-diverg iter appropri distanc measur probabl distribut metric determin fisher inform matrix log log covari gradient model log-prob wrt paramet natur gradient direct obtain rn see recent overview topic fisher inform matrix mlps start deriv precis form fisher canon multi-lay perceptron mlp compos layer consid follow deep network binari classif though approach general arbitrari number output class p hl h1 fl wl hl f1 b1 bl paramet mlp denot b1 wl bl weight wi rni ni connect layer bias bi rni fi element-wis non-linear function let us defin backpropag gradient i-th non-linear ignor block-diagon compon fisher matrix focus block fwi correspond interact paramet layer block take form fwi vec hti vec hti p vec x vector function yield column vector row matrix assum activ hi independ random variabl write fwi hi p hi figur 2-layer natur neural network illustr project involv prong element row column matrix i-th element vector fwi entri fisher captur interact paramet wi wj hypothesi verifi experiment sec great improv condit fisher enforc hi hti layer network despit ignor possibl correl block diagon term fisher project natur gradient descent section introduc whiten neural network perform approxim whiten intern represent begin present novel whiten neural layer assumpt network statist e hi e hi hti fix show layer adapt effici track popul statist cours train result learn algorithm refer project natur gradient descent prong highlight interest connect prong mirror descent section whiten neural layer build block wnn follow neural layer hi fi vi ui hi ci di compar introduc explicit center paramet ci rni equal ensur input dot product zero mean expect analog center reparametr deep boltzmann machin weight matrix ui rni ni per-lay pca-whiten matrix whose row obtain eigendecomposit diag ui diag hyper-paramet regular term control maxim multipli learn rate equival size trust region paramet vi rni ni di rni analog canon paramet neural network introduc though oper space whiten unit activ ui hi ci layer stack form deep neural network layer model paramet d1 vl dl whiten coeffici c0 ul cl depict though layer might appear over-parametr first glanc crucial learn whiten coeffici via loss minim instead estim direct model statist coeffici thus constant point view optim simpli serv improv condit fisher respect paramet denot inde use deriv led see block-diagon term involv term ui hi hi equal ident construct updat whiten coeffici whiten model paramet evolv train statist model remain well condit whiten coeffici must updat regular interv algorithm project natur gradient descent input train set initi paramet hyper-paramet reparam frequenc number sampl ns regular term ui i ci repeat mod amort cost line layer comput canon paramet wi vi ui bi di wi ci proj estim use ns sampl d. updat ci ui eigen decomp updat updat paramet vi wi ui di bi proj end end perform sgd updat wrt use sampl d. converg take care interfer converg properti gradient descent achiev coupl updat correspond updat overal function implement mlp remain unchang preserv product vi ui updat whiten coeffici analogu constraint bias unfortun estim mean diag i could perform onlin minibatch sampl recent batch normal scheme estim full covari matrix undoubt requir larger number sampl while statist could accumul onlin via exponenti move averag rmsprop k-fac cost eigendecomposit requir comput whiten matrix ui remain cubic layer size simplest instanti method exploit smooth gradient descent simpli amort cost oper consecut updat sgd updat whiten model close align ngd immedi follow reparametr qualiti approxim degrad time subsequ reparametr result algorithm shown pseudo-cod algorithm improv upon basic amort scheme updat whiten paramet use per-batch diagon natur gradient updat whose statist comput onlin framework implement via reparametr wi vi di ui di diagon matrix updat di ui hi minibatch updat di compens exact cheapli scale row ui column vi accord simpler implement idea combin prong batch-norm denot prong dualiti mirror descent inher dualiti paramet whiten neural layer paramet canon model inde there exist linear project map canon paramet whiten paramet vice-versa correspond line algorithm while correspond line dualiti reveal close connect prong mirror descent mirror descent onlin learn algorithm general proxim form gradient descent class bregman diverg strict convex differenti function replac distanc mirror descent solv proxim problem appli first-ord updat dual space project back onto primal space defin complex conjug mirror descent updat given figur fisher matrix small mlp first reparametr best view colour condit number fim train relat initi condit model initi initi condit learn rate adjust reach rough train error given time well known natur gradient special case md distanc generat function chosen mirror updat somewhat unintuit howev gradient appli dual space comput space paramet this prong relat md it trivial show use function instead previous defin enabl us direct updat dual paramet use gradient comput direct dual space inde result updat shown implement natur gradient thus equival updat appropri choic correspond project use prong oper map canon neural paramet whiten layer illustr advantag this whiten form md one may amort cost project sever updat gradient comput direct dual paramet space relat work this work extend recent contribut formal mani common use heurist train mlps import zero-mean activ gradient well import normal varianc forward backward pass recent vatanen extend previous work introduc multipl constant center non-linear contrast introduc full whiten matrix ui focus whiten feedforward network activ instead normal geometr mean unit gradient varianc recent introduc batch normal scheme quit close resembl diagon version prong main differ bn normal varianc activ non-linear oppos normal latent activ look full covari furthermor bn implement normal modifi feed-forward comput thus requir method backpropag normal oper diagon version prong also bare interest resembl rmsprop normal term involv squar root fim import distinct howev prong appli this updat whiten paramet space thus preserv natur gradient interpret fisher thus drop clariti depend paramet index time superscript figur optim deep auto-encod mnist impact eigenvalu regular term impact amort period show initi whiten reparametr import achiev faster learn better error rate train error vs number updat train error vs cpu-tim plot show prong achiev better error rate number updat wall clock k-fac close relat prong develop concurr method it target layer-wis block-diagon fisher approxim block unlik method howev kfac approxim covari backpropag gradient ident estim requir statist use exponenti move averag unlik approach base amort similar techniqu found precondit kaldi speech recognit toolkit model fisher matrix covari spars connect gaussian graphic model fang repres general formal exploit model structur effici comput natur gradient one applic neural network decorrel gradient across neighbour layer similar algorithm prong later found it appear simpli thought experi amort recours effici comput experi begin set diagnost experi highlight effect method improv condit also illustr impact hyper-paramet control frequenc reparametr size trust region section evalu prong unsupervis learn problem model deep fulli connect section move onto larg convolut model imag classif experiment detail such model architectur hyper-paramet configur found supplement materi introspect experi condit provid better understand approxim made prong we train small 3-layer mlp tanh non-linear downsampl version mnist model size chosen order full fisher tractabl show fim middl hidden layer whiten model activ we took absolut valu entri improv visibl 2c depict evolut condit number fim train measur percentag it initi valu first whiten reparametr case prong we present such curv sgd rmsprop batch normal prong result clear show reparametr perform prong improv condit reduct observ confirm initi assumpt name we improv condit block diagon fisher whiten activ alon sensit hyper-paramet figur 3b highlight effect eigenvalu regular term reparametr interv experi perform best figur classif error cifar-10 imagenet cifar-10 prong achiev better test error converg faster imagenet prong achiev compar valid error while maintain faster coverg rate perform auto-encod section mnist dataset figur 3b plot reconstruct error train set various valu determin maximum multipli learn rate learn becom extrem sensit this learn rate high2 smaller step size howev lower yield signific speedup often converg faster simpli use larger learn rate this confirm import manifold curvatur optim lower allow differ direct scale drastic differ accord correspond curvatur fig 3b compar impact model proper whiten initi solid line model initi standard fan-in initi dash line result quit surpris show effect whiten reparametr simpl initi scheme said perform degrad due ill condit becom excess larg unsupervis learn follow marten we compar prong task minim reconstruct error dens 8-layer auto-encod mnist dataset reconstruct error respect updat wallclock time shown we see prong signific outperform baselin method an order magnitud number updat respect wallclock method signific outperform baselin term time taken reach certain error threshold despit fact runtim per epoch prong sgd compar batch normal sgd rmsprop sgd note these time number reflect perform optim choic hyper-paramet case batch normal yield batch size compar all method break perform runtim prong spent perform whiten reparametr compar estim per layer mean covari this confirm amort paramount success method.3 supervis learn we evalu method train deep supervis convolut network object recognit follow we perform whiten across featur map we treat pixel given featur map independ sampl this allow us implement whiten neural layer sequenc two convolut first whiten filter prong compar sgd rmsprop batch normal algorithm acceler via momentum result present cifar-10 imagenet challeng ilsvrc12 dataset case learn rate decreas use waterfal anneal schedul divid learn rate valid error fail improv set number evalu unstabl combin learn rate omit clariti we note whiten implement optim it take advantag gpu acceler runtim therefor expect improv we move eigen-decomposit gpu cifar-10 we evalu prong cifar-10 use deep convolut model inspir vgg architectur model train random crop random horizont reflect model select perform held-out valid set 5k exampl result shown respect train error prong bn seem offer similar speedup compar sgd momentum hypothesi benefit prong more pronounc dens connect network number unit per layer typic larger number map use convolut network interest prong general better achiev test error batch normal this reflect find show ngd leverag unlabel data better general unlabel data come extra crop reflect observ estim whiten matric imagenet challeng dataset our final set experi aim show scalabl our method we appli our natur gradient algorithm large-scal ilsvrc12 dataset imag label categori use incept architectur order scale problem this size we parallel our train loop as split process singl minibatch size across multipl gpus note prong scale well this set as estim mean covari paramet layer also embarass parallel eight gpus use comput gradient estim model statist though eigen decomposit requir whiten parallel in current implement given difficulti task we employ enhanc version algorithm prong as simpl period whiten model prove unstabl figur show batch normalis prong converg approxim top-1 valid error vs respect similar cpu-tim in comparison sgd achiev valid error prong howev exhibit much faster converg initi updat it obtain around error compar bn alon we stress imagenet result somewhat preliminari while our top-1 error higher report in we use much less extens data augment pipelin we begin explor natur gradient method may achiev these larg scale optim problem encourag these initi find discuss we began this paper ask whether converg speed could improv simpl model reparametr driven by structur fisher matrix theoret experiment perspect we shown that whiten neural network achiev this via a simpl scalabl effici whiten reparametr they howev one sever possibl instanti the concept natur neural network in a previous incarn the idea we exploit a similar reparametr includ whiten backpropag gradients4 we favor the simpler approach present in this paper as we general found the altern less stabl deep network this may due the difficulti in estim gradient covari in lower layer a problem seem mirror the famous vanish gradient problem maintain whiten activ may also offer addit benefit the point view model compress general by virtu whiten the project ui hi form an order represent least signific bit the sharp roll-off in the eigenspectrum may explain whi deep network ammen compress similar one could envis spectral version dropout the dropout probabl a function the eigenvalu altern way orthogon the represent layer also explor via altern decomposit perhap by exploit the connect linear auto-encod pca we also plan on pursu the connect with mirror descent further bridg the gap deep learn method from onlin convex optim acknowledg we extrem grate to shakir moham invalu discuss feedback in the prepar this manuscript we also thank philip thoma volodymyr mnih raia hadsel sergey ioff shane legg for feedback on the paper the weight matrix parametr as wi rit vi ui with ri the whiten matrix for
----------------------------------------------------------------

title: 6270-stochastic-multiple-choice-learning-for-training-diverse-deep-ensembles.pdf

stochast multipl choic learn train divers deep ensembl stefan lee virginia tech stefle vt.edu senthil purushwalkam carnegi mellon univers spurushw andrew.cmu.edu david crandal indiana univers djcran indiana.edu michael cogswel virginia tech cogswel vt.edu viresh ranjan virginia tech rviresh vt.edu dhruv batra virginia tech dbatra vt.edu abstract mani practic percept system exist within larger process includ interact user addit compon capabl evalu qualiti predict solut context benefici provid oracl mechan multipl high like hypothes rather singl predict work pose task produc multipl output learn problem ensembl deep network introduc novel stochast gradient descent base approach minim loss respect oracl method simpl implement agnost architectur loss function parameter-fre approach achiev lower oracl error compar exist method wide rang task deep architectur also show qualit divers solut produc often provid interpret represent task ambigu introduct percept problem rare exist vacuum typic problem comput vision natur languag process ai subfield embed larger applic context instanc task recogn segment object imag semant segment might embed autonom vehicl task describ imag sentenc imag caption might part system assist visually-impair user scenario goal percept often generat singl output set plausibl hypothes downstream process verif compon human oper downstream mechan may abstract oracl capabl pick correct solut set learn set call multipl choic learn mcl goal learner minim oracl loss achiev set solut formal given dataset input-output pair goal classic supervis learn search map minim task-depend loss captur error actual label predict label set learn function make singl predict input pay penalti predict contrast multipl choic learn seek learn map produc solut yi1 im oracl loss minm y im minim work fix form map union output ensembl predictor f2 fm address task train ensembl member f1 fm minim oracl loss formul differ ensembl member free special subset data distribut collect produc set output cover space high probabl predict well confer neural inform process system nip barcelona spain hors cow coupl bird stand grass bird perch top tree branch bird perch tree branch sky figur single-predict base model often produc solut low expect loss face ambigu howev solut often unrealist reflect imag content well row instead train ensembl unifi loss allow member produc differ output reflect multi-mod belief row evalu method imag classif segment caption task divers solut set especi use structur predict problem multipl reason interpret one correct situat often aris practic system includ implicit class confus label space mani classif problem often arbitrari quantize continu space exampl vision system may expect classifi tabl desk despit mani real-world object arguabl belong class make multipl predict implicit confus view explicit system output ambigu evid often simpli enough inform make definit predict exampl even human expert may abl identifi fine-grain class particular breed dog given occlud distant view like produc small set reason guess case task produc divers set possibl clear defin produc one correct answer bias toward mode mani model tendenc exhibit mode-seek behavior way reduc expect loss dataset convers model frequent produc know make multipl predict system improv coverag lower densiti area solut space without sacrif perform major exampl word optim oracl loss multiple-predict learner respond ambigu much like human make multipl guess captur multi-mod belief contrast single-predict learner forc produc solut low expect loss face ambigu figur illustr produc solut use practic semant segment exampl problem often caus object predict mixtur multipl class like horse-cow shown figur imag caption minim expect loss encourag generic sentenc safe respect expect error inform exampl figur show two pair imag differ imag content similar generic caption model know safe assum bird branch cake eaten fork paper general multipl choic learn paradigm joint learn ensembl deep network minim oracl loss direct first adapt idea deep network present novel train algorithm avoid cost retrain learn difficulti past method primari technic contribut formul stochast block gradient descent optim approach well-suit minim oracl loss ensembl deep network call stochast multipl choic learn smcl formul applic model train stochast gradient descent agnost form task depend loss parameter-fre time effici train ensembl member concurr demonstr broad applic efficaci smcl train divers deep ensembl interpret emerg expertis wide rang problem domain network architectur includ convolut neural network cnn ensembl imag classif fullyconvolut network fcn ensembl semant segment combin cnn recurr neural network rnn ensembl imag caption provid detail analysi train output behavior result ensembl demonstr ensembl member special expertis emerg automat train use smcl method outperform exist baselin produc set output high oracl perform relat work ensembl learn much exist work train ensembl focus divers member model mean improv perform decreas error correl often accomplish resampl exist train data member model produc artifici data encourag new model decorrel exist ensembl approach train combin ensembl member joint loss more recent work hinton ahm explor use generalist network perform statist inform design ensemble-of-expert architectur classif contrast smcl discov special consequ minim oracl loss import exist method general structur output label smcl seamless adapt discov differ task-depend special automat generat multipl solut there larg bodi work topic extract multipl divers solut singl model howev approach design probabilist structured-output model direct applic general deep architectur most relat our approach work guzman-rivera explicit minim oracl loss output ensembl formal set multipl choic learn mcl paradigm they introduc general altern block coordin descent train approach requir retrain model multipl time more recent dey reformul problem submodular optim task ensembl member learn sequenti boosting-lik manner maxim margin gain in oracl perform both method requir either cost retrain sequenti train make poor suit modern deep architectur take week train address serious shortcom provid first practic algorithm train divers deep ensembl introduc stochast gradient descent sgd base algorithm train ensembl member concurr multiple-choic learn stochast block gradient descent we consid task train ensembl differenti learner togeth produc set solut minim loss respect oracl select lowest-error predict notat we use denot set given train set input-output pair our goal learn function map input output we fix form ensembl learner fm task-depend loss measur error true predict output we defin oracl loss dataset lo min fm minim oracl loss multipl choic learn in order direct minim oracl loss ensembl learner guzman-rivera present object form potenti tight upper-bound object replac min in oracl loss indic variabl pi pi predictor lowest error exampl argmin fm pm i pi fm pi pi result minim constrain joint optim ensembl paramet datapoint assign author propos altern block algorithm shown in algorithm approxim minim object similar k-mean hard-em approach altern assign exampl min-loss predictor train model converg partit exampl assign note approach feasibl train deep network sinc modern architectur take week month train singl model stochast multipl choic learn overcom shortcom we propos stochast algorithm differenti learner interleav assign step batch updat in figur mcl approach alg requir cost retrain our smcl method alg work within standard sgd solver train ensembl member joint loss stochast gradient descent consid partial deriv object in respect output mth individu learner exampl lo fm pi fm fm notic fm minimum error predictor exampl pi gradient term train singl model otherwis gradient zero behavior lend straightforward optim strategi learner train sgd base solver batch we pass exampl learner calcul loss ensembl member exampl dure backward pass gradient loss exampl backpropag lowest error predictor exampl tie broken arbitrarili approach we call stochast multipl choic learn smcl shown in algorithm smcl generaliz learner train stochast gradient descent thus applic extens rang modern deep network unlik iter train schedul mcl smcl ensembl need train converg in parallel smcl also agnost exact form loss function such appli without addit effort varieti problem experi in section we present result smcl ensembl train task deep architectur shown in figur includ cnn ensembl imag classif fcn ensembl semant segment cnn+rnn ensembl imag caption generat baselin mani exist general techniqu induc divers direct applic deep network we compar our propos method classic ensembl in model train under independ loss differ random initi we refer indp ensembl in figur mcl altern train model converg assign exampl alloc exampl lowest error model we repeat this process meta-iter initi ensembl differ random weight we find mcl perform similar smcl small classif task howev mcl perform drop substanti segment caption task unlik smcl effect reassign exampl per epoch mcl this converg limit capac special compar smcl we also note smcl 5x faster mcl factor result choos meta-iter other applic may requir more increas gap dey train model sequenti in boosting-lik fashion time reweight exampl to maxim margin increas evalu metric we find these model satur quick ensembl size grow perform increas margin gain therefor weight approach zero low weight averag gradient backpropag stochast learner drop substanti reduc rate effect learn without care tune to comput convolut classif model for cifar10 fully-convolut segment model long cnn+rnn base caption model karpathi figur we experi three problem domain use various architectur shown weight requir error measur bound by accuraci for classif iou for segment satisfi this cider-d score divid by guarante this for caption oracl evalu we present result as oracl version task-depend perform metric these oracl metric report highest score output for a given input for exampl in classif task oracl accuraci exact top-k criteria imagenet whether least one output correct label likewis oracl intersect union iou highest iou ground truth segment one output oracl metric allow evalu multiple-predict system separ downstream re-rank select system extens use in previous work our experi convinc demonstr broad applic efficaci smcl for train divers deep ensembl in three experi smcl signific outperform classic ensembl dey typic improv mcl provid a 5x speedup mcl our analysi show exact algorithm smcl lead to automat emerg differ interpret notion special among ensembl member imag classif model we begin our experi smcl the cifar10 dataset use the small convolut neural network cifar10-quick provid the caff deep learn framework cifar10 a ten way classif task with small imag for these experi the
----------------------------------------------------------------

title: 5059-compete-to-compute.pdf

compet comput rupesh kumar srivastava jonathan masci sohrob kazerounian faustino gomez j rgen schmidhub idsia usi-supsi manno lugano switzerland rupesh jonathan sohrob tino juergen idsia.ch abstract local competit among neighbor neuron common biolog neural network paper appli concept gradient-bas backprop-train artifici multilay nns nns compet linear unit tend outperform non-compet nonlinear unit avoid catastroph forget train set chang time introduct although often use machin learn method consid natur arriv particular solut perhap instruct first understand function role biolog constraint inde artifici neural network repres state-of-the-art mani pattern recognit task resembl brain superfici sens also draw mani comput function properti one long-studi properti biolog neural circuit yet fulli impact machin learn communiti natur local competit common find across brain region neuron exhibit on-cent off-surround organ organ argu give rise number interest properti across network neuron winner-take-al dynam automat gain control nois suppress paper propos biolog inspir mechan artifici neural network base local competit ultim reli local winner-take-al lwta behavior demonstr benefit lwta across number differ network pattern recognit task show lwta enabl perform compar state-of-the-art moreov help prevent catastroph forget common artifici neural network first train particular task abrupt train new task properti desir continu learn wherein learn regim clear delin experi also show evid type modular emerg lwta network train supervis set differ modul subnetwork respond differ input benefici learn multimod data distribut compar learn monolith model follow first discuss relev neurosci background motiv local competit show incorpor artifici neural network lwta implement compar altern method show lwta network perform varieti task help buffer catastroph forget neurosci background competit interact neuron neural circuit long play import role biolog model brain process larg due earli studi show mani cortic sub-cort hippocamp cerebellar region brain exhibit recurr on-cent off-surround anatomi cell provid excitatori feedback nearbi cell scatter inhibitori signal broader rang biolog model sinc tri uncov function properti sort organ role behavior success anim earliest model describ emerg winner-take-al wta behavior local competit base grossberg shunt short-term memori equat show center-surround structur enabl wta dynam also contrast enhanc normal analysi dynam show network slower-than-linear signal function uniform input pattern linear signal function preserv normal input pattern faster-than-linear signal function enabl wta dynam sigmoid signal function contain slower-than-linear linear faster-than-linear region enabl supress nois input pattern contrast-enhanc normal store relev portion input pattern form soft wta function properti competit interact studi show among thing effect distance-depend kernel inhibitori time lag develop self-organ map role wta network attent biolog model also extend show competit interact spike neural network give rise soft wta dynam well may effici construct vlsi although competit interact wta dynam studi extens biolog literatur recent consid comput machin learn perspect exampl maa show feedforward neural network wta dynam non-linear comput power network threshold sigmoid gate network employ soft wta competit univers function approxim moreov result hold even network weight strict posit find ramif understand biolog neural circuit well develop neural network pattern recognit larg bodi evid support advantag local competit interact make noteworthi simpl mechan provok studi machin learn communiti nonetheless network employ local competit exist sinc late along serv primari inspir present work recent maxout network leverag local competit interact combin techniqu known dropout obtain best result certain benchmark problem network local winner-take-al block section describ general network architectur local compet neuron network consist block organ layer figur block bi contain comput unit neuron produc output vector determin local interact individu neuron activ block yij g h1i h2i hni competition/interact function encod effect local interact block hji activ j-th neuron block comput hi wij input vector neuron previous layer wij weight vector neuron block general non-linear activ function output activ pass input next layer paper use winner-take-al interact function inspir studi comput neurosci particular use hard winner-take-al function hi hji hki yij otherwis case multipl winner tie broken index preced order investig capabl hard winner-take-al interact function isol figur local winner-take-al lwta network block size two show win neuron block shade given input exampl activ flow forward win neuron error backpropag activ neuron grey connect propag activ activ neuron form subnetwork full network chang depend input ident use activ function equat differ local winner take lwta network standard multilay perceptron non-linear activ function use forward propag input local competit neuron block turn activ neuron except one highest activ train error signal backpropag win neuron lwta layer mani neuron block activ one time given input pattern1 denot layer block size lwta-n input pattern present network subgraph full network activ highlight neuron synaps figur train dataset consist simultan train exponenti number model share paramet well learn model activ pattern unlik network sigmoid unit free paramet need set proper input pattern subset use given input pattern come differ sub-distribut potenti model effici special modular properti similar network rectifi linear unit relu recent shown good sever learn task link relu discuss section comparison relat method max-pool neural network max-pool layer found use especi imag classif task achiev state-of-the-art perform layer usual use convolut neural network subsampl represent obtain convolv input learn filter divid represent pool select maximum one max-pool lower comput burden reduc number connect subsequ convolut layer add translational/rot invari howev alway possibl win neuron block activ exact zero block output max-pool lwta figur max-pool lwta max-pool group neuron layer singl set output weight transmit win unit activ case next layer layer activ subsampl lwta block subsampl activ flow subsequ unit via differ set connect depend win unit first glanc max-pool seem similar wta oper howev two differ substanti downsampl wta oper thus number featur reduc instead represent sparsifi figur dropout dropout interpret model-averag techniqu joint train sever model share subset paramet input dimens data augment appli input layer achiev probabilist omit drop unit network exampl train neuron particip forward/backward propag consid hypothet train lwta network block size two select winner block random similar train neural network dropout probabl nonetheless two fundament differ dropout regular techniqu lwta interact neuron block replac per-neuron non-linear activ dropout believ improv general perform sinc forc unit learn independ featur without reli unit activ dure test propag input network unit layer train dropout use output weight suitabl scale lwta network output scale requir fraction unit inact input pattern depend total input view way wta restrict fraction paramet util input pattern howev hypothes freedom use differ subset paramet differ input allow architectur learn multimod data distribut accur rectifi linear unit rectifi linear unit relu simpli linear neuron clamp negat activ zero otherwis relu network shown use restrict boltzmann machin outperform sigmoid activ function deep neural network use obtain best result sever benchmark problem across multipl domain consid lwta block two neuron compar two relu neuron weight sum input neuron tabl show output y1 y2 combin posit negat relu lwta neuron relu lwta neuron pass output half possibl case differ lwta neuron never activ inact time activ error flow exact one neuron block relu neuron inact satur potenti drawback sinc neuron tabl comparison rectifi linear activ lwta-2 posit posit negat posit negat negat posit negat negat posit posit negat relu neuron y1 y2 lwta neuron y1 y2 get activ get train lead wast capac howev previous work suggest negat impact optim lead hypothesi hard satur help credit assign long error flow certain path optim affect advers continu research along line valid hypothesi expect possibl train relu network better mani argument relu network appli lwta network notabl differ dure train lwta network inact neuron becom activ due train neuron block suggest lwta net may less sensit weight initi greater portion network capac may util experi follow experi lwta network test various supervis learn dataset demonstr abil learn use intern represent without util non-linear order clear assess util local competit special strategi augment data transform nois dropout use also encourag spars represent hidden layer ad activ penalti object function common techniqu also relu unit thus object evalu valu use lwta rather achiev absolut best test score block size two use experiments.2 network train use stochast gradient descent mini-batch learn rate lt momentum mt epoch given min otherwis min tt mf mt pf learn rate anneal factor min lower learn rate limit momentum scale mi mf epoch remain constant mf weight decay use convolut network section max-norm normal experi setup similar permut invari mnist mnist handwritten digit recognit task consist imag train test digit center center mass permut invari set task we attempt classifi digit without util 2d structur imag everi digit vector pixel last exampl train set use hyperparamet tune model best hyperparamet set train converg full train set mini-batch size speed experi gnumpi cudamat librari use tabl test set error permut invari mnist dataset method without data augment unsupervis pre-train activ sigmoid relu relu dropout hidden layer lwta-2 test error tabl test set error mnist dataset convolut architectur data augment result mark asterisk use layer-wis unsupervis featur learn pre-train network global fine tune architectur 2-layer cnn layer mlp 2-layer relu cnn layer lwta-2 3-layer relu cnn 2-layer cnn layer mlp 3-layer relu cnn stochast pool 3-layer maxout dropout test error use pixel valu rescal preprocess best model obtain gave test set error consist three lwta layer block follow softmax layer knowledg best report error without util implicit/explicit model averag this set use deformations/nois enhanc dataset unsupervis pretrain tabl compar result method use unsupervis pre-train perform lwta compar relu network dropout hidden layer use dropout input layer well lower error rate use relu use maxout obtain convolut network mnist this experi convolut network cnn use consist filter first layer follow second layer map respect relu activ everi convolut layer follow max-pool oper we use two lwta-2 layer block final softmax output layer weight decay found benefici improv general result summar tabl along state-of-the-art approach use data augment detail convolut architectur see amazon sentiment analysi lwta network test amazon sentiment analysi dataset sinc relu unit shown perform well this domain we use balanc subset dataset consist review four categori product book dvds electron kitchen applianc task classifi review posit negat dataset consist posit negat review categori text review convert binari featur vector encod presenc absenc unigram bigram follow frequent vocabulari entri retain featur classif we divid data equal balanc fold test our network cross-valid report mean test error fold relu activ use this dataset context unsupervis learn denois autoencod obtain spars featur represent use classif we train lwta-2 network three layer block supervis set direct classifi review posit negat use 2-way softmax output layer we obtain mean accuraci book dvds electron kitchen give mean accuraci compar report denois autoencod use relu unsupervis pre-train find good initi tabl lwta network outperform sigmoid relu activ rememb dataset p1 train dataset p2 test error p1 train p1 train p2 lwta sigmoid relu implicit long term memori this section examin effect lwta architectur catastroph forget fact network implement multipl model allow retain inform dataset even train differ dataset test this implicit long term memori mnist train test set divid two part p1 contain digit p2 consist remain digit three differ network architectur compar three lwta layer block size three layer sigmoid neuron three layer relu neuron network 5-way softmax output layer repres probabl exampl belong five class all network initi paramet train fix learn rate momentum network first train reach log-likelihood error p1 train set this valu chosen heurist produc low test set error reason time all three network type weight output layer correspond softmax classifi store network train start new initi random output layer weight reach log-likelihood valu p2 final output layer weight save p1 restor network evalu p1 test set experi repeat differ initi tabl show lwta network rememb learn p1 much better sigmoid relu network though notabl sigmoid network perform much wors lwta relu while test error valu depend learn rate momentum use lwta network tend rememb better relu network factor two case sigmoid network alway perform much wors although standard network architectur known suffer catastroph forget we show first time relu network actual quit good this regard moreov outperform lwta we expect this behavior manifest competit model general becom pronounc increas complex dataset neuron encod specif featur in one dataset affect much dure train anoth dataset wherea neuron encod common featur reus thus lwta may step forward toward model forget easili analysi subnetwork network a singl lwta-m block consist mn subnetwork select train individu exampl while train a dataset train we expect subnetwork consist activ neuron exampl class neuron in common compar subnetwork activ differ class in case relat simpl dataset like mnist possibl examin number common neuron mean subnetwork use class this neuron activ in the layer exampl in a subset exampl record for class the subnetwork consist neuron activ for least the exampl design the repres mean subnetwork compar all class subnetwork count the number neuron in common figur 3a show the fraction neuron in common the mean subnetwork pair digit digit morpholog similar subnetwork neuron in common the subnetwork for digit intuit less similar verifi this subnetwork special a result train we look at the fraction common neuron all pair digit for the untrain train digit fraction neuron in common digit mnist digit pair figur each entri in the matrix denot the fraction neuron a pair mnist digit in common averag in the subnetwork that activ for each the two digit class the fraction neuron in common in the subnetwork each the possibl digit pair after train exampl after train figur clear the subnetwork much more similar prior train the full network learn partit paramet to reflect the structur the data conclus futur research direct our lwta network automat self-modular multipl parameter-shar subnetwork respond to differ input represent without signific degrad state-of-the-art result digit recognit sentiment analysi lwta network also avoid catastroph forget thus retain use represent one set input even after train to classifi anoth this implic for continu learn agent that forget represent part environ expos to part we hope to explor mani promis applic these idea in the futur acknowledg this research fund eu project way neuraldynam nascenc addit fund arcelormitt
----------------------------------------------------------------

title: 1541-active-noise-canceling-using-analog-neuro-chip-with-on-chip-learning-capability.pdf

activ nois cancel use analog neurochip on-chip learn capabl jung-wook cho soo-young lee comput neural system laboratori depart electr engin korea advanc institut scienc technolog kusong-dong yusong-gu taejon korea syle ee.kaist.ac.kr abstract modular analogu neuro-chip set on-chip learn capabl develop activ nois cancel analogu neuro-chip set incorpor error backpropag learn rule practic applic allow pin-to-pin interconnect multi-chip board develop neuro-board demonstr activ nois cancel without digit signal processor multi-path fade acoust channel random nois nonlinear distort loud speaker compens adapt learn circuit neuro-chip experiment result report cancel car nois real time introduct analog digit implement neural network report digit neuro-chip design fabric help well-establish cad tool digit vlsi fabric technolog although analogu neurochip potenti advantag integr densiti speed digit chip suffer non-id characterist fabric chip offset nonlinear fabric chip flexibl enough use mani differ applic also much care design requir fabric chip characterist fair depend upon fabric process implement analog neuro-chip exist two differ approach without on-chip learn capabl current major analog neuro-chip learn capabl mani practic applic requir on-lin adapt continu chang environ must onlin adapt learn capabl therefor neuro-chip on-chip learn capabl essenti practic applic modular architectur also activ nois cancel analog on-chip learn neuro-chip advantag provid flexibl implement mani larg complex system chip although mani applic studi analog neuro-chip import find proper problem analog neuro-chip may potenti advantag popular dsps believ applic analog input/output signal high comput requir good problem exampl activ nois control adapt equal good applic analog neuro-chip paper we report demonstr activ nois cancel may mani applic real world modular analog neuro-chip set develop on-chip learn capabl neuro-board fabric multipl chip pc interfac input output measur unlik previous implement adapt equal binari output both input output valu analogu nois cancel xl figur block diagram synaps cell figur block diagram neuron cell analog neuro-chip on-chip learn we develop analog neuro-chip error backpropag learn capabl modular architectur develop analog neuro-chip set consist synaps chip neuron basic cell synaps chip shown figur synaps cell receiv two input pre-synapt neural activ error correct term generat two output feed-forward signal wx back-propag error also updat store weight amount therefor synaps cell consist three multipli circuit one analogu storag synapt weight figur show basic cell neuron chip collect signal synaps previous layer distribut synaps follow layer each neuron bodi receiv two input post-synapt neural activ back-propag error follow layer generat two output sigmoid-squash neural activ new backpropag error multipli bell-shap sigmoid-deriv backpropag error may input synaps cell previous layer provid easi connect chip two input synaps cell repres voltag two output current simpl current summat hand input output neuron cell repres current voltag respect simpl pin-to-pin connect chip one packag pin maintain each input output chip time cho lee multiplex introduc no control requir multi-chip multilay system howev make number packag pin main limit factor number synaps neuron cell develop chip set although mani simplifi multipli report high-dens integr perform limit linear resolut speed on-chip learn desir high precis faith implement 4-quadranr gilbert multipli use especi multipli weight updat synaps cell requir high precis synapt weight store capacitor mas switch use allow current flow multipli capacitor short time interv weight adapt applic like activ nois control telecommun tap analog delay line also design integr synaps chip reduc offset accumul parallel analog delay line adopt same offset voltag introduc oper amplifi node diffus capacitor pf use storag tap analog delay line synaps chip synaps cell integr array analog delay line input may appli either analog delay line extern pin parallel select capacitor cell refresh decod place column row actual size synaps cell size synaps chip chip fabric single-poli cmos process hand neuron chip simpl structur consist neuron cell without addit circuit sigmoid circuit neuron cell use differenti pair slope amplitud control voltage-control resistor sigmoid-deriv circuit also use differenti pair min-select circuit size neuron cell synaps chip pc neuron pc chip target i i i dsp i output i input ann board figur block diagram analog neuro-board gdab tv.c aric d1 16bitll do 48bitll activ nois cancel analog on-chip learn neuro-chip use chip set analog neuro-system construct figur show brief block diagram analog neuro-system analogu neuro-board interfac host comput gdab general data acquisit board gdab board special design data interfac analogu neuro-chip neuro-board synaps chip neuron chip 2-layer perceptron architectur test develop purpos dsp adc dac instal on neuro-board to refresh adjust weight forward propag time the layer perceptron measur f..lsec therefor the comput speed the neuro-board mcps mega connect per second for recal mcup mega connect updat per second for error backpropag learn to achiev speed with dsp mip requir for recal least mip for error-back propag learn channel error signal nois sourc adapt filter multilay perceptron figur structur feedforward activ nois cancel activ nois cancel use neuro-chip basic architectur the feed forward activ nois cancel shown in figur an area near the microphon call quiet zone actual mean nois small in area nois propag sourc to the quiet zone dispers medium characterist model finit impuls respons fir filter with addit random nois an activ nois cancel generat electr signal for a loud speaker creat acoust signal to cancel the nois the quiet zone in general the electric-to-acoust signal transfer characterist the loud speaker nonlinear the overal activ nois cancel anc system also becom nonlinear therefor multilay perceptron a potenti advantag popular transvers adapt filter base on linear-mean.-squar lms error minim experi conduct for car nois cancel the
----------------------------------------------------------------

title: 6221-memory-efficient-backpropagation-through-time.pdf

memory-effici backpropag time audruna grusli googl deepmind audruna google.com r mi muno googl deepmind muno google.com marc lanctot googl deepmind lanctot google.com ivo danihelka googl deepmind danihelka google.com alex grave googl deepmind gravesa google.com abstract propos novel approach reduc memori consumpt backpropag time bptt algorithm train recurr neural network rnns approach use dynam program balanc trade-off cach intermedi result recomput algorithm capabl tight fit within almost user-set memori budget find optim execut polici minim comput cost comput devic limit memori capac maxim comput perform given fix memori budget practic use-cas provid asymptot comput upper bound various regim algorithm particular effect long sequenc sequenc length algorithm save memori usag use one third time per iter standard bptt introduct recurr neural network rnns artifici neural network connect unit form cycl often use sequenc map problem propag hidden state inform earli part sequenc back later point lstm particular rnn architectur excel sequenc generat speech recognit reinforc learn set success rnn architectur includ differenti neural comput dnc draw network neural transduc backpropag time algorithm bptt typic use obtain gradient train one import problem larg memori consumpt requir bptt especi troublesom use graphic process unit gpus due limit gpu memori memori budget typic known advanc algorithm balanc tradeoff memor recomput find optim memori usag polici minim total comput cost fix memori budget algorithm exploit fact memori slot may reus multipl time idea use dynam program find provabl optim polici main contribut paper approach larg architectur agnost work recurr neural network abl fit within limit memori devic gpus typic compens increas comput cost background relat work section describ key term relev previous work memory-sav rnns confer neural inform process system nip barcelona spain definit rnn core feed-forward neural network clone unfold time repeat clone repres particular time point recurr exampl rnn singl hidden layer whose output feed back hidden layer sequenc length unfold network feed-forward contain rnn core definit hidden state recurr network part output rnn core pass next rnn core input addit initi hidden state exist singl hidden state per time step network unfold definit intern state rnn core given time-point necessari inform requir backpropag gradient time step input vector gradient respect output vector gradient respect output hidden state suppli defin also includ output hidden state intern state evalu execut singl forward oper take previous hidden state respect entri input sequenc input network architectur intern state rnn core includ hidden input state normal requir evalu gradient particular choic definit use later paper definit memori slot unit memori capabl store singl hidden state singl intern state depend context backpropag time backpropag time bptt one common use techniqu train recurr network bptt unfold neural network time creat sever copi recurr unit treat like deep feed-forward network tie weight done standard forward-propag techniqu use evalu network fit whole sequenc input standard backpropag algorithm use evalu partial deriv loss criteria respect network paramet approach comput effici also fair intens memori usag standard version algorithm effect requir store intern state unfold network core everi time-step order abl evalu correct partial deriv trade memori comput time general idea trade comput time memori consumpt general comput graph investig automat differenti communiti recent rise deep architectur recurr network increas interest less general case graph forward comput chain gradient chain revers order simplif lead relat simpl memory-sav strategi heurist context bptt instead store hidden network state intermedi result recomput demand execut extra forward oper chen propos subdivid sequenc size equal part memor hidden state subsequ intern state within segment use memori cost make addit forward pass averag error backpropag right-sid sequenc second-last subsequ restor repeat number forward oper refer chen algorithm author also suggest appli techniqu recurs sever time sub-divid sequenc equal part termin recurs onc subsequ length becom less author establish would lead memori consumpt o k logk+1 comput complex o logk algorithm minimum possibl memori usag log2 case refer chen recurs algorithm memory-effici backpropag through time first discuss two simpl exampl memori scarc somewhat limit memori scarc straightforward design simpl comput ineffici algorithm backpropag error rnns use constant amount memori everi time state network time restor algorithm would simpli re-evalu state forward-propag input start begin time backpropag happen revers tempor order result previous forward step reus memori store would requir repeat forward step backpropag gradient one step backward rememb input initi state this would produc algorithm requir forward pass backpropag error time step algorithm would space time memori somewhat limit scarc may store hidden rnn state time point error backpropag time intern rnn core state re-evalu execut anoth forward oper take previous hidden state input backward oper follow immedi this approach lead fair signific memori save typic recurr network hidden state much smaller intern state network core hand this lead anoth forward oper execut backpropag stage backpropag though time select hidden state memor bptt-hsm idea behind propos algorithm compromis two previous extrem suppos want forward backpropag sequenc length abl store hidden state memori given time may reus memori slot store differ hidden state backpropag also suppos singl rnn core avail purpos intermedi calcul abl store singl intern state defin comput cost backpropag measur term mani forward-oper one make total forward backpropag step combin follow optim memori usag polici minim comput cost one easili set boundari condit cost minim memori approach 2t memori plenti shown approach illustr figur onc start forward-propag step time t0 given point t0 choos put current hidden state memori step this step cost forward oper state read revers order they written this allow algorithm store state stack onc state put memori time reduc problem two part use divide-and-conqu approach run algorithm side sequenc use remain memori slot cost c step reus memori slot backpropag side cost step use full size memori capac perform step could releas hidden state immedi finish step legend step cost hidden state propag gradient get back-propag hidden state store memori intern state rnn core time singl forward oper step cost singl backward oper recurs applic algorithm hidden state read memori step cost hidden state save memori hidden state remov memori figur propos divide-and-conqu approach base case recurr algorithm simpli sequenc length forward backward propag may done trivial singl avail rnn network core this step cost theoret comput cost measur number forward oper per time step measur comput cost milisecond figur comput cost per time-step algorithm allow rememb green blue violet cyan hidden state grey line show perform standard bptt without memori constraint also includ larg constant valu caus singl backward step per time step exclud theoret comput valu make relat perform loss much less sever practic theori establish protocol may find optim polici defin cost choos first state push posit later follow optim polici c time argmin cpts time cpts hidden state store memori forward comput backward comput order execut cpts cost per time step number forward oper figur illustr optim polici logic sequenc time goe left right execut happen top bottom equat solv exact use dynam program subject boundari condit establish previous figur determin optim polici follow pseudocod given supplementari materi figur illustr optim polici found two simpl case backpropag though time select intern state memor bptt-ism save intern rnn core state instead hidden rnn state would allow us save singl forward oper backpropag everi divide-and-conqu step higher memori cost suppos memori capac capabl save exact intern rnn state first need modifi boundari condit cost reflect minim memori approach memori plenti equival standard bptt previous defin comput cost combin forward backward propag sequenc length memori allow follow optim memori usag polici cost measur term amount total forward step made number backward step constant similar bptt-hsm process divid part use divide-and-conqu approach fig valu posit first memor evalu forward oper execut intern rnn core state place memori this step cost forward oper step figur intern state also contain output hidden state algorithm recurr run high-tim right side sequenc have one less memori slot avail step figur this step cost c forward oper onc gradient backpropag through right side sequenc backpropag done store rnn core step figur this step addit cost involv forward oper memori slot releas leav memori avail final algorithm run left-sid sequenc step figur this final step cost c forward oper sum cost give us follow equat c c recurs singl base case backpropag empti sequenc nil oper comput cost make legend step cost hidden state get propag intern stat rnn core time intern rnn core state store memori incl output hidden state step cost hidden state read memori intern state save intern state remov step cost singl backward oper forward oper involv recurs applic algorithm gradient get back-propag singl initi rnn hidden state step cost singl forward oper singl backward oper figur illustr divide-and-conqu approach use bptt-ism compar previous section stay minim instead this meaning rememb last intern state reason rememb last hidden state numer solut sever differ memori capac shown figur argmin seen figur methodolog save memori sequenc exclud input vector use time per training-iter standard bptt assum singl backward step be twice expens forward step backpropag though time mix state memor bptt-msm possibl deriv even general model combin approach describ section suppos total memori capac measur term much singl hidden state rememb also suppos store intern rnn core state take time memori integ number choos save singl hidden state use singl memori unit store intern rnn core state use time memori benefit store intern rnn core state abl save singl forward oper backpropag defin comput cost term total amount forward oper run optim strategi we use follow boundari condit bptt-ism section bptt-msm section figur comparison two backpropag algorithm term theoret cost differ line show number forward oper per time-step memori capac limit green blue violet cyan intern rnn core state pleas note unit memori measur differ figur size intern state vs size hidden state assum size intern core state time larger size hidden state valu influenc right plot cost shown right plot less respect cost shown left plot valu cost reflect minim memori approach memori plenti c notat conveni we use similar divide-and-conqu approach one use previous section defin q1 comput cost we choos first rememb hidden state posit thereaft follow optim polici ident q1 c similar defin q2 comput cost we choos first rememb intern state posit thereaft follow optim polici similar except intern state take memori unit q2 c c defin d1 optim posit next push assum next state push hidden state defin d2 optim posit next push intern core state note d2 differ rang minim reason equat d1 argmin q1 d2 argmin q2 also defin ci qi final min ci argmin ci we solv equat use simpl dynam program indic whether next state push memori hidden state intern state respect valu d1 d2 indic posit next push remov doubl hidden-st memor definit intern rnn core state would typic requir hidden input state includ memor this may lead duplic inform exampl optim strategi rememb intern rnn core state sequenc memor hidden output one would equal memor hidden input one definit everi time we want push intern rnn core state onto stack previous intern state alreadi we may omit push input hidden state recal intern core rnn state input hidden state otherwis known time larger hidden state defin space requir memor intern core state when input hidden state known relationship application-specif mani circumst we modifi reflect this optim q2 c c indic function equat di ident analyt upper bound bptt-hsm we establish theoret upper bound bptt-hsm algorithm bound tight short sequenc also numer verifi less 3t initi forward pass exclud addit we establish differ bound regim mm integ valu comput cost bound proof given supplementari materi pleas refer supplementari materi discuss upper bound bptt-msm bptt-ism comparison three differ strategi use memori use memori figur comparison three strategi case when size intern rnn core state time larger hidden state total memori capac allow us rememb either intern rnn state hidden state arbitrari mixtur left plot respect right plot red curv illustr bptt-hsm green curv bptt-ism blue curv bptt-msm pleas note larg sequenc length red curv out-perform green one blue curv outperform two comput cost previous describ strategi result shown figur bptt-msm outperform bptt-ism bptt-hsm this unsurpris search space in case superset strategi space algorothm find an optim strategi within space also fix memori capac strategi memor hidden state outperform strategi memor intern rnn core state long sequenc latter outperform former relat short sequenc discuss we use an lstm map input batch size measur execut time singl gradient descent step forward backward oper combin function sequenc length figur pleas note measur comput time also includ time taken backward oper time-step dynam program equat take account singl backward oper usual twice expens forward oper involv evalu gradient respect input data intern paramet still number backward oper constant impact optim strategi optim dynam program find optim comput strategi construct subject memori constraint fair general model we impos strategi propos consist all the assumpt we made in section when appli rnns bpttmsm guarante perform least as well memori budget sequenc length this strategi propos express provid potenti suboptim polici di subject the equat qi numer comparison chen algorithm chen algorithm requir rememb hidden state intern rnn state exclud input hidden state the recurs approach requir rememb least log2 hidden state in word the model allow fine-grain control memori usag rather save memori in the meantim propos bptt-msm fit within almost arbitrari constant memori constraint this the main advantag algorithm figur left memori consumpt divid a fix comput cost right comput cost per time-step a fix memori consumpt red green blue curv correspond respect the non-recurs chen approach allow match particular memori budget make a like-for-lik comparison difficult instead fix the memori budg possibl fix comput cost forward iter on averag match the cost the algorithm observ much memori would our approach use memori usag the algorithm would equival save hidden state intern core state let suppos the intern rnn core state time larger hidden state in this case the size the intern rnn core state exclud state this would give a memori usag chen the input hidden algorithm as as need rememb hidden state intern state input hidden state omit avoid duplic figur illustr memori usag our algorithm divid a fix execut speed as a function sequenc length differ valu paramet valu lower indic memori save as it seen we save a signific amount memori the comput cost anoth experi measur comput cost for a fix memori consumpt the result shown in figur comput cost correspond chen algorithm this illustr our approach perform signific faster although it wors this chen strategi actual near optim for this particular memori budget still as seen the previous paragraph this memori budget alreadi in the regim diminish return memori reduct possibl for almost the comput cost conclus in this paper we propos a novel approach for find optim backpropag strategi for recurr neural network for a fix user-defin memori budget we demonstr the general the algorithm least as good as mani other use common heurist the main advantag our approach the abil tight fit almost user-specifi memori constraint gain maxim comput perform
----------------------------------------------------------------

title: 5666-variational-dropout-and-the-local-reparameterization-trick.pdf

variat dropout local reparameter trick diederik p. kingma tim saliman max well machin learn group univers amsterdam algoritmica univers california irvin canadian institut advanc research cifar d.p.kingma uva.nl salimans.tim gmail.com m.well uva.nl abstract investig local reparameterizaton techniqu great reduc varianc stochast gradient variat bayesian infer sgvb posterior model paramet retain paralleliz local reparameter translat uncertainti global paramet local nois independ across datapoint minibatch parameter trivial parallel varianc invers proport minibatch size general lead much faster converg addit explor connect dropout gaussian dropout object correspond sgvb local reparameter scale-invari prior proport fix posterior varianc method allow infer flexibl parameter posterior specif propos variat dropout general gaussian dropout dropout rate learn often lead better model method demonstr sever experi introduct deep neural network flexibl famili model easili scale million paramet datapoint still tractabl optim use minibatch-bas stochast gradient ascent due high flexibl neural network capac fit wide divers nonlinear pattern data flexbl often lead overfit left uncheck spurious pattern found happen fit well train data predict new data various regular techniqu control overfit use practic current popular empir effect techniqu dropout shown regular binari dropout gaussian approxim call gaussian dropout virtual ident regular perform much faster converg section shown gaussian dropout optim lower bound margin likelihood data paper show relationship dropout bayesian infer extend exploit great improv effici variat bayesian infer model paramet work direct interpret general gaussian dropout fast converg freedom specifi flexibl parameter posterior distribut bayesian posterior infer neural network paramet theoret attract method control overfit exact infer comput intract effici approxim scheme design markov chain mont carlo mcmc class approxim infer method asymptot guarante pioneer applic regular neural network later use refin includ altern mcmc variat infer equival minimum descript length mdl framework modern variant stochast variat infer appli neural network succ limit high varianc gradient despit theoret attract bayesian method infer posterior distribut neural network weight yet shown outperform simpler method dropout even new crop effici variat infer algorithm base stochast gradient minibatch data yet shown signific improv upon simpler dropout-bas regular section explor yet unexploit trick improv effici stochast gradientbas variat infer minibatch data translat uncertainti global paramet local nois independ across datapoint minibatch result method optim speed level fast dropout inde origin gaussian dropout method special case advantag method allow full bayesian analysi model signific flexibl standard dropout approach present close relat sever popular method literatur regular ad random nois relationship discuss section effici practic bayesian infer consid bayesian analysi dataset contain set observ tupl goal learn model paramet weight condit probabl standard classif regress bayesian infer model consist updat initi belief paramet form prior distribut observ data updat belief paramet form approxim posterior distribut comput true posterior distribut bay rule involv comput intract integr good approxim necessari variat infer infer cast optim problem optim paramet parameter model close approxim measur kullback-leibl diverg dkl diverg posterior true posterior minim practic maxim so-cal variat lower bound margin likelihood data ld dkl ld eq log call ld expect log-likelihood bound plus dkl equal condit margin log-likelihood log sinc margin log-likelihood constant maxim bound minim dkl stochast gradient variat bay sgvb various algorithm gradient-bas optim variat bound differenti exist see section overview recent propos effici method minibatch-bas optim differenti model stochast gradient variat bay sgvb method introduc especi appendix basic trick sgvb parameter random paramet differenti function random nois variabl new parameteris unbias differenti minibatch-bas mont carlo estim expect log-likelihood form ld lsgvb log p yi minibatch data random datapoint nois vector drawn nois distribut assum remain term variat lower bound dkl comput determinist otherwis may approxim similar estim differenti unbias gradient note describ method limit classif regress straightforward appli model set like unsupervis model tempor model also unbias ld lsgvb proceed variat bayesian infer random initi perform stochast gradient ascent varianc sgvb estim theori stochast approxim tell us stochast gradient ascent use asymptot converg local optimum appropri declin step size suffici weight updat practic perform stochast gradient ascent crucial depend varianc gradient varianc larg stochast gradient descent fail make much progress reason amount time object function consist expect log likelihood term approxim use mont carlo kl diverg term dkl assum calcul analyt otherwis approxim mont carlo similar reparameter assum draw minibatch datapoint replac see appendix similar analysi minibatch without replac use li shorthand log p yi contribut likelihood i-th datapoint minibatch mont carlo estim pm may rewritten lsgvb li whose varianc given n2 var lsgvb var cov var li cov li lj varianc data distribut distribut covari var li var xi log p yi drawn empir distribut defin train set seen total contribut varianc var li invers proport minibatch size howev total contribut covari decreas practic mean varianc lsgvb domin covari even moder larg local reparameter trick therefor propos altern estim cov li lj varianc stochast gradient scale make new estim comput effici sampl direct sampl intermedi variabl influenc lsgvb global uncertainti weight translat form local uncertainti independ across exampl easier sampl refer reparameter global nois local nois local reparameter trick whenev sourc global nois translat local nois intermedi state comput local reparameter appli yield comput statist effici gradient estim local reparameter appli fair larg famili model best explain simpl exampl consid standard fulli connect neural network contain hidden layer consist neuron layer receiv input featur matrix layer multipli weight matrix nonlinear appli aw specifi posterior approxim weight fulli factor2 ize gaussian wi j j 8wi j mean weight sampl wi j j case could make sure cov li lj sampl separ weight matrix exampl minibatch comput effici would need sampl million random number singl layer neural network even if could done effici comput follow step would becom much harder origin perform simpl matrix-matrix product form aw turn separ local vector-matrix product theoret complex comput higher import comput usual perform parallel use fast device-optim blas basic linear algebra subprogram also happen neural network architectur convolut neural network optim librari convolut deal separ filter matric per exampl fortun weight therefor influenc expect log likelihood neuron activ much lower dimens if therefor sampl random activ direct without sampl may obtain effici mont carlo estim much lower cost factor gaussian posterior weight posterior activ condit input also factor gaussian wi j j 8wi j bm j j j j j a2m j rather sampl gaussian weight comput result activ may thus sampl activ impli gaussian distribut direct use bm j j j matrix need sampl thousand random variabl instead million thousand fold save addit yield gradient estim comput effici draw separ weight matric train exampl local reparameter trick also lead estim lower varianc see consid stochast gradient estim respect posterior paramet j minibatch size draw random weight get lsgvb lsgvb j bm j j if hand form gradient use local reparameter trick get a2m lsgvb lsgvb j bm j j here two stochast term first backpropag gradient lsgvb bm j second sampl random nois estim gradient respect j basic come estim covari two term much easier much fewer individu higher correl backpropag gradient lsgvb bm j covari easier estim word measur effect lsgvb bm j easi random variabl direct influenc gradient via bm j hand sampl random weight thousand influenc gradient term individu effect get lost nois appendix make argument rigor section show hold experiment variat dropout dropout techniqu regular neural network paramet work ad multipl nois input layer neural network optim use notat section fulli connect neural network dropout correspond matrix input featur current minibatch weight matrix output matrix current layer nonlinear appli symbol denot elementwis hadamard product input matrix matrix independ nois variabl ad nois input train weight paramet less like overfit train data shown empir previous public origin propos draw element bernoulli distribut probabl dropout rate later shown use continu distribut relat mean varianc gaussian work well better here re-interpret dropout continu nois variat method propos general call variat dropout develop variat dropout provid firm bayesian justif dropout train deriv implicit prior distribut variat object new interpret allow us propos sever use extens dropout principl way make normal fix dropout rate adapt data variat dropout independ weight nois if element nois matrix drawn independ gaussian margin distribut activ bm j gaussian well bm j j j j j a2m make use fact propos gaussian dropout regular method instead appli activ direct drawn approxim exact margin distribut given argu margin distribut exact gaussian nois bernoulli nois still approxim gaussian central limit theorem ignor depend differ element present use report good result nonetheless note explain appendix gaussian dropout nois also interpret aris bayesian treatment neural network weight multipli input give aw posterior distribut weight given factor gaussian wi j perspect margin distribut aris applic local reparameter trick introduc section variat object correspond interpret discuss section variat dropout correl weight nois instead ignor depend activ nois section may retain depend interpret dropout form correl weight nois bm wk si si row input matrix bm row output row weight matrix construct multipli non-stochast paramet vector stochast scale variabl si distribut scale variabl interpret bayesian posterior distribut weight paramet bias estim use maximum likelihood origin gaussian dropout sampl procedur interpret aris local reparameter posterior weight w. dropout scale-invari prior variat object posterior distribut propos section common decompos paramet vector captur mean multipl nois term determin paramet posterior distribut nois enter multipl way call dropout posterior note mani common distribut univari gaussian nonzero mean reparameter meet requir dropout train adapt maxim expect log likelihood eq ld consist optim variat lower bound form prior weight such dkl depend appendix show prior meet requir scale invari log-uniform prior p log |wi j prior uniform log-scal weight weight-scal si section explain appendix prior interest connect float point format store number mdl perspect float point format optim communic number drawn prior convers kl diverg dkl this prior natur interpret regular number signific digit posterior store weight wi j floating-point format put expect log likelihood kl-diverg penalti togeth see dropout train maxim follow variaton lower bound eq ld dkl made depend paramet explicit nois paramet dropout rate common treat hyperparamet kept fix train log-uniform prior this correspond fix limit number signific digit learn weight wi j section discuss possibl make this limit adapt also maxim lower bound respect choic factor gaussian approxim posterior wi j discuss section lower bound analyz detail appendix c. there shown this particular choic posterior negat kl-diverg dkl analyt tractabl approxim extrem accur use dkl constant c1 c2 c3 c1 c2 c3 express may use calcul correspond term posterior approxim section dkl adapt regular optim dropout rate nois paramet use dropout train dropout rate usual treat fix hyperparamet deriv dropout variat object make paramet adapt trivial simpli maxim variat lower bound respect use this learn separ dropout rate per layer per neuron even per separ weight section look predict perform obtain make adapt we found larg valu correspond local optima hard escap due large-vari gradient avoid such local optima we found benefici set constraint dure train we maxim posterior varianc squar posterior mean correspond dropout rate relat work pioneer work practic variat infer neural network done bias variat lower bound estim introduc good result recurr neural network model later work shown even practic estim form type continu latent variabl paramet use non-loc reparameter trick lead effici unbias stochast gradient-bas variat infer work focus applic latent-vari infer extens empir result infer global model paramet report includ succes applic reinforc learn earlier work use relat high-vari estim upon we improv variabl reparameter long histori in statist literatur recent found use effici gradient-bas machin learn infer relat also probabilist backpropag algorithm infer margin posterior probabl howev requir certain tractabl in network make insuit type model consider in this paper we show here regular dropout interpret variat infer dropconnect similar dropout binari nois weight rather hidden unit dropconnect thus similar interpret variat infer uniform prior weight mixtur two dirac peak posterior in standout introduc variat dropout binari belief network learn produc dropout rate recent propos anoth bayesian perspect dropout in recent work similar reparameter describ use variat infer focus closed-form approxim variat bound rather unbias mont carlo estim also investig bayesian perspect dropout focus binari variant report various encourag result util dropout impli predict uncertainti experi we compar method standard binari dropout two popular version gaussian dropout we denot type type b gaussian dropout type we denot pre-linear gaussian dropout type denot post-linear gaussian dropout this way method name correspond matrix name in section nois inject model implement in theano optim perform use adam default hyper-paramet tempor averag two type variat dropout includ type correl weight nois as introduc in section adapt version gaussian dropout type a. variat dropout type independ weight uncertainti as introduc in section correspond gaussian dropout type b de facto standard benchmark regular method task mnist hand-written digit classif we choos architectur as fulli connect neural network hidden layer rectifi linear unit relus we follow dropout hyper-paramet recommend these earlier public a dropout rate hidden layer input layer we use earli stop method amount epoch run determin base perform a valid set varianc we start empir compar varianc differ avail stochast estim gradient variat object this we train neural network describ either epoch test error epoch test error use variat dropout independ weight nois after train we calcul gradient weight top bottom level network full train set compar gradient estim per batch train exampl appendix contain analysi case variat dropout correl weight nois tabl show local reparameter trick yield lowest varianc among variat dropout estim condit although still substanti higher compar ani dropout regular varianc scale achiev estim especi import earli in optim make largest differ compar weight sampl per minibatch weight sampl per data point addit varianc reduct obtain our estim draw fewer random number section a factor this remain relat stabl as train progress compar local reparameter weight sampl per data point stochast gradient estim local reparameter weight sampl per data point slow weight sampl per minibatch standard dropout nois minim var top layer epoch top layer epoch bottom layer epoch bottom layer epoch tabl averag empir varianc minibatch stochast gradient estim exampl a fulli connect neural network regular variat dropout independ weight nois speed we compar the regular sgvb estim separ weight sampl per datapoint the effici estim base local reparameter in term wall-clock time effici our implement a modern gpu optim the na estim took second per epoch the effici estim took second an fold speedup classif error figur show test-set classif error the test regular method various choic number hidden unit our adapt variat version gaussian dropout perform equal better non-adapt counterpart standard dropout test condit the differ especi notic the smaller network in these smaller network we observ variat dropout infer dropout rate averag far lower the dropout rate for larger network this adapt come neglig comput cost classif error the mnist dataset classif error the cifar-10 dataset figur best view in color comparison various dropout method appli to fullyconnect neural network for classif the mnist dataset shown the classif error network hidden layer averag run variat version gaussian dropout perform equal better non-adapt counterpart the differ especi larg smaller model regular dropout often result in sever underfit comparison dropout method appli to convolut net a train on the cifar-10 dataset for differ set network size the network two convolut layer featur map respect stride follow a softplus nonlinear this follow two fulli connect layer hidden unit we found slight downscal the kl diverg part the variat object benefici variat in figur denot perform type a variat dropout a kl-diverg downscal with a factor this small modif seem to prevent underfit beat dropout method in the test model conclus effici posterior infer use stochast gradient-bas variat bay sgvb often signific improv a local reparameter global paramet uncertainti translat local uncertainti per datapoint by inject nois local instead global the model paramet we obtain an effici estim low comput complex trivial parallel low varianc we show dropout a special case sgvb with local reparameter suggest variat dropout a straightforward extens regular dropout where optim dropout rate infer from the data rather fix in advanc we report encourag empir result acknowledg we thank the review yarin gal for valuabl feedback diederik kingma support by the googl european fellowship in deep learn max well support by research grant from googl facebook the nwo project in natur ai
----------------------------------------------------------------

