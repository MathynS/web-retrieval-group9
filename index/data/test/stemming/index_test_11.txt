query sentence: Bayesian framework
---------------------------------------------------------------------
title: 5537-a-statistical-decision-theoretic-framework-for-social-choice.pdf

A Statistical Decision-Theoretic Framework for
Social Choice
Hossein Azari Soufiani?

David C. Parkes ?

Lirong Xia?

Abstract
In this paper, we take a statistical decision-theoretic viewpoint on social choice,
putting a focus on the decision to be made on behalf of a system of agents. In
our framework, we are given a statistical ranking model, a decision space, and a
loss function defined on (parameter, decision) pairs, and formulate social choice
mechanisms as decision rules that minimize expected loss. This suggests a general
framework for the design and analysis of new social choice mechanisms. We
compare Bayesian estimators, which minimize Bayesian expected loss, for the
Mallows model and the Condorcet model respectively, and the Kemeny rule. We
consider various normative properties, in addition to computational complexity
and asymptotic behavior. In particular, we show that the Bayesian estimator for the
Condorcet model satisfies some desired properties such as anonymity, neutrality,
and monotonicity, can be computed in polynomial time, and is asymptotically
different from the other two rules when the data are generated from the Condorcet
model for some ground truth parameter.

1

Introduction

Social choice studies the design and evaluation of voting rules (or rank aggregation rules). There
have been two main perspectives: reach a compromise among subjective preferences of agents, or
make an objectively correct decision. The former has been extensively studied in classical social
choice in the context of political elections, while the latter is relatively less developed, even though
it can be dated back to the Condorcet Jury Theorem in the 18th century [9].
In many multi-agent and social choice scenarios the main consideration is to achieve the second
objective, and make an objectively correct decision. Meanwhile, we also want to respect agents?
preferences and opinions, and require the voting rule to satisfy well-established normative properties in social choice. For example, when a group of friends vote to choose a restaurant for dinner,
perhaps the most important goal is to find an objectively good restaurant, but it is also important
to use a good voting rule in the social choice sense. Even for applications with less societal context, e.g. using voting rules to aggregate rankings in meta-search engines [12], recommender systems [15], crowdsourcing [23], semantic webs [27], some social choice normative properties are still
desired. For example, monotonicity may be desired, which requires that raising the position of an
alternative in any vote does not hurt the alternative in the outcome of the voting rule. In addition,
we require voting rules to be efficiently computable.
Such scenarios propose the following new challenge: How can we design new voting rules with
good statistical properties as well as social choice normative properties?
To tackle this challenge, we develop a general framework that adopts statistical decision theory [3].
Our approach couples a statistical ranking model with an explicit decision space and loss function.
?
azari@google.com, Google Research, New York, NY 10011, USA. The work was done when the author
was at Harvard University.
?
parkes@eecs.harvard.edu, Harvard University, Cambridge, MA 02138, USA.
?
xial@cs.rpi.edu, Rensselaer Polytechnic Institute, Troy, NY 12180, USA.

1

Anonymity, neutrality Majority,
Consistency
Monotonicity
Condorcet

Kemeny
Bayesian est. of
M1? (uni. prior)
Bayesian est. of
M2? (uni. prior)

Y

Y

N

Y

N

N

Y

N

N

Complexity

NP-hard,
NP-hard,

PNP
|| -hard
PNP
|| -hard

(Theorem 3)

P (Theorem 4)

Min. Bayesian risk

N
Y
Y

Table 1: Kemeny for winners vs. Bayesian estimators of M1? and M2? to choose winners.
Given these, we can adopt Bayesian estimators as social choice mechanisms, which make decisions
to minimize the expected loss w.r.t. the posterior distribution on the parameters (called the Bayesian
risk). This provides a principled methodology for the design and analysis of new voting rules.
To show the viability of the framework, we focus on selecting multiple alternatives (the alternatives
that can be thought of as being ?tied? for the first place) under a natural extension of the 0-1 loss
function for two models: let M1? denote the Mallows model with fixed dispersion [22], and let M2?
denote the Condorcet model proposed by Condorcet in the 18th century [9, 34]. In both models the
dispersion parameter, denoted ?, is taken as a fixed parameter. The difference is that in the Mallows
model the parameter space is composed of all linear orders over alternatives, while in the Condorcet
model the parameter space is composed of all possibly cyclic rankings over alternatives (irreflexive,
antisymmetric, and total binary relations). M2? is a natural model that captures real-world scenarios
where the ground truth may contain cycles, or agents? preferences are cyclic, but they have to report
a linear order due to the protocol. More importantly, as we will show later, a Bayesian estimator on
M2? is superior from a computational viewpoint.
Through this approach, we obtain two voting rules as Bayesian estimators and then evaluate them
with respect to various normative properties, including anonymity, neutrality, monotonicity, the majority criterion, the Condorcet criterion and consistency. Both rules satisfy anonymity, neutrality,
and monotonicity, but fail the majority criterion, Condorcet criterion,1 and consistency. Admittedly,
the two rules do not enjoy outstanding normative properties, but they are not bad either. We also
investigate the computational complexity of the two rules. Strikingly, despite the similarity of the
two models, the Bayesian estimator for M2? can be computed in polynomial time, while computing
the Bayesian estimator for M1? is PNP
|| -hard, which means that it is at least NP-hard. Our results are
summarized in Table 1.
We also compare the asymptotic outcomes of the two rules with the Kemeny rule for winners,
which is a natural extension of the maximum likelihood estimator of M1? proposed by Fishburn
[14]. It turns out that when n votes are generated under M1? , all three rules select the same winner
asymptotically almost surely (a.a.s.) as n ? ?. When the votes are generated according to M2? ,
the rule for M1? still selects the same winner as Kemeny a.a.s.; however, for some parameters, the
winner selected by the rule for M2? is different with non-negligible probability. These are confirmed
by experiments on synthetic datasets.
Related work. Along the second perspective in social choice (to make an objectively correct decision), in addition to Condorcet?s statistical approach to social choice [9, 34], most previous work
in economics, political science, and statistics focused on extending the theorem to heterogeneous,
correlated, or strategic agents for two alternatives, see [25, 1] among many others. Recent work in
computer science views agents? votes as i.i.d. samples from a statistical model, and computes the
MLE to estimate the parameters that maximize the likelihood [10, 11, 33, 32, 2, 29, 7]. A limitation
of these approaches is that they estimate the parameters of the model, but may not directly inform
the right decision to make in the multi-agent context. The main approach has been to return the
modal rank order implied by the estimated parameters, or the alternative with the highest, predicted
marginal probability of being ranked in the top position.
There have also been some proposals to go beyond MLE in social choice. In fact, Young [34]
proposed to select a winning alternative that is ?most likely to be the best (i.e., top-ranked in the true
ranking)? and provided formulas to compute it for three alternatives. This idea has been formalized
and extended by Procaccia et al. [29] to choose a given number of alternatives with highest marginal
1

?
The new voting rule for M1? fails them for all ? < 1/ 2.

2

probability under the Mallows model. More recently, independent to our work, Elkind and Shah
[13] investigated a similar question for choosing multiple winners under the Condorcet model. We
will see that these are special cases of our proposed framework in Example 2. Pivato [26] conducted
a similar study to Conitzer and Sandholm [10], examining voting rules that can be interpreted as
expect-utility maximizers.
We are not aware of previous work that frames the problem of social choice from the viewpoint
of statistical decision theory, which is our main conceptual contribution. Technically, the approach
taken in this paper advocates a general paradigm of ?design by statistics, evaluation by social choice
and computer science?. We are not aware of a previous work following this paradigm to design
and evaluate new rules. Moreover, the normative properties for the two voting rules investigated in
this paper are novel, even though these rules are not really novel. Our result on the computational
complexity of the first rule strengthens the NP-hardness result by Procaccia et al. [29], and the
complexity for the second rule (Theorem 5) was independently discovered by Elkind and Shah [13].
The statistical decision-theoretic framework is quite general, allowing considerations such as estimators that minimize the maximum expected loss, or the maximum expected regret [3]. In a different
context, focused on uncertainty about the availability of alternatives, Lu and Boutilier [20] adopt a
decision-theoretic view of the design of an optimal voting rule. Caragiannis et al. [8] studied the
robustness of social choice mechanisms w.r.t. model uncertainty, and characterized a unique social
choice mechanism that is consistent w.r.t. a large class of ranking models.
A number of recent papers in computational social choice take utilitarian and decision-theoretical
approaches towards social choice [28, 6, 4, 5]. Most of them evaluate the joint decision w.r.t. agents?
subjective preferences, for example the sum of agents? subjective utilities (i.e. the social welfare).
We don?t view this as fitting into the classical approach to statistical decision theory as formulated
by Wald [30]. In our framework, the joint decision is evaluated objectively w.r.t. the ground truth in
the statistical model. Several papers in machine learning developed algorithms to compute MLE or
Bayesian estimators for popular ranking models [18, 19, 21], but without considering the normative
properties of the estimators.

2

Preliminaries

In social choice, we have a set of m alternatives C = {c1 , . . . , cm } and a set of n agents. Let
L(C) denote the set of all linear orders over C. For any alternative c, let Lc (C) denote the set
of linear orders over C where c is ranked at the top. Agent j uses a linear order Vj ? L(C) to
represent her preferences, called her vote. The collection of agents votes is called a profile, denoted
by P = {V1 , . . . , Vn }. A (irresolute) voting rule r : L(C)n ? (2C \ ?) selects a set of winners that
are ?tied? for the first place for every profile of n votes.
For any pair of linear orders V, W , let Kendall(V, W ) denote the Kendall-tau distance between
V and W , that is, the number of different pairwise comparisons in V and W . The Kemeny rule
(a.k.a. Kemeny-Young method) [17, 35] selects all linear orders with the minimum Kendall-tau distance from the preference profile P , that is, Kemeny(P ) = arg minW Kendall(P, W ). The most
well-known variant of Kemeny to select winning alternatives, denoted by KemenyC , is due to Fishburn [14], who defined it as a voting rule that selects all alternatives that are ranked in the top
position of some winning linear orders under the Kemeny rule. That is, KemenyC (P ) = {top(V ) :
V ? Kemeny(P )}, where top(V ) is the top-ranked alternative in V .
Voting rules are often evaluated by the following normative properties. An irresolute rule r satisfies:
? anonymity, if r is insensitive to permutations over agents;
? neutrality, if r is insensitive to permutations over alternatives;
? monotonicity, if for any P , c ? r(P ), and any P 0 that is obtained from P by only raising the
positions of c in one or multiple votes, then c ? r(P 0 );
? Condorcet criterion, if for any profile P where a Condorcet winner exists, it must be the unique
winner. A Condorcet winner is the alternative that beats every other alternative in pair-wise elections.
? majority criterion, if for any profile P where an alternative c is ranked in the top positions for more
than half of the votes, then r(P ) = {c}. If r satisfies Condorcet criterion then it also satisfies the
majority criterion.
? consistency, if for any pair of profiles P1 , P2 with r(P1 )?r(P2 ) 6= ?, r(P1 ?P2 ) = r(P1 )?r(P2 ).
3

For any profile P , its weighted majority graph (WMG), denoted by WMG(P ), is a weighted directed
graph whose vertices are C, and there is an edge between any pair of alternatives (a, b) with weight
wP (a, b) = #{V ? P : a V b} ? #{V ? P : b V a}.
A parametric model M = (?, S, Pr) is composed of three parts: a parameter space ?, a sample
space S composing of all datasets, and a set of probability distributions over S indexed by elements
of ?: for each ? ? ?, the distribution indexed by ? is denoted by Pr(?|?).2
Given a parametric model M, a maximum likelihood estimator (MLE) is a function fMLE : S ? ?
such that for any data P ? S, fMLE (P ) is a parameter that maximizes the likelihood of the data.
That is, fMLE (P ) ? arg max??? Pr(P |?).
In this paper we focus on parametric ranking models. Given C, a parametric ranking model MC =
(?, Pr) is composed of a parameter space ? and a distribution Pr(?|?) over L(C) for each ? ?
?, such that for any number of voters n, the sample space is Sn = L(C)n , where each vote is
generated
i.i.d. from Pr(?|?). Hence, for any profile P ? Sn and any ? ? ?, we have Pr(P |?) =
Q
V ?P Pr(V |?). We omit the sample space because it is determined by C and n.
Definition 1 In the Mallows model [22], a parameter is composed of a linear order W ? L(C)
and
parameter ? with 0 < ? < 1. For any profile P and
?), Pr(P |?) =
Q a dispersion
P ? = (W,
1 Kendall(V,W )
Kendall(V,W )
?
,
where
Z
is
the
normalization
factor
with
Z
=
?
.
V ?P Z
V ?L(C)
Statistical decision theory [30, 3] studies scenarios where the decision maker must make a decision
d ? D based on the data P generated from a parametric model, generally M = (?, S, Pr). The
quality of the decision is evaluated by a loss function L : ??D ? R, which takes the true parameter
and the decision as inputs.
In this paper, we focus on the Bayesian principle of statistical decision theory to design social
choice mechanisms as choice functions that minimize the Bayesian risk under a prior distribution
over ?. More precisely, the Bayesian risk, RB (P, d), is the expected loss of the decision d when
the parameter is generated according to the posterior distribution given data P . That is, RB (P, d) =
E?|P L(?, d). Given a parametric model M, a loss function L, and a prior distribution over ?, a
(deterministic) Bayesian estimator fB is a decision rule that makes a deterministic decision in D
to minimize the Bayesian risk, that is, for any P ? S, fB (P ) ? arg mind RB (P, d). We focus on
deterministic estimators in this work and leave randomized estimators for future research.
Example 1 When ? is discrete, an MLE of a parametric model M is a Bayesian estimator of the
statistical decision problem (M, D = ?, L0-1 ) under the uniform prior distribution, where L0-1 is
the 0-1 loss function such that L0-1 (?, d) = 0 if ? = d, otherwise L0-1 (?, d) = 1.
In this sense, all previous MLE approaches in social choice can be viewed as the Bayesian estimators
of a statistical decision-theoretic framework for social choice where D = ?, a 0-1 loss function, and
the uniform prior.

3

Our Framework

Our framework is quite general and flexible because we can choose any parametric ranking model,
any decision space, any loss function, and any prior to use the Bayesian estimators social choice
mechanisms. Common choices of both ? and D are L(C), C, and (2C \ ?).
Definition 2 A statistical decision-theoretic framework for social choice is a tuple F =
(MC , D, L), where C is the set of alternatives, MC = (?, Pr) is a parametric ranking model,
D is the decision space, and L : ? ? D ? R is a loss function.
Let B(C) denote the set of all irreflexive, antisymmetric, and total binary relations over C. For
any c ? C, let Bc (C) denote the relations in B(C) where c  a for all a ? C ? {c}. It follows
that L(C) ? B(C), and moreover, the Kendall-tau distance can be defined to count the number of
pairwise disagreements between elements of B(C).
In the rest of the paper, we focus on the following two parametric ranking models, where the dispersion is a fixed parameter.
2
This notation should not be taken to mean a conditional distribution over S unless we are taking a Bayesian
point of view.

4

Definition 3 (Mallows model with fixed dispersion, and the Condorcet model) Let M1? denote
the Mallows model with fixed dispersion, where the parameter space is ? = L(C) and given any
W ? ?, Pr(?|W ) is Pr(?|(W, ?)) in the Mallows model, where ? is fixed.
In the Condorcet model, M2? , the parameter space is ? = B(C). For any W ? ? and any profile

Q
P , we have Pr(P |W ) = V ?P Z1 ?Kendall(V,W ) , where Z is the normalization factor such that
P
Z = V ?B(C) ?Kendall(V,W ) , and parameter ? is fixed.3
M1? and M2? degenerate to the Condorcet model for two alternatives [9]. The Kemeny rule that
selects a linear order is an MLE of M1? for any ?.
We now formally define two statistical decision-theoretic frameworks associated with M1? and M2? ,
which are the focus of the rest of our paper.
Definition 4 For ? = L(C) or B(C), any ? ? ?, and any c ? C, we define a loss function Ltop (?, c)
such that Ltop (?, c) = 0 if for all b ? C, c  b in ?; otherwise Ltop (?, c) = 1.
Let F?1 = (M1? , 2C \ ?, Ltop ) and F?2 = (M2? , 2C \ ?, Ltop ), where for any C ? C, Ltop (?, C) =
P
1
2
1
c?C Ltop (?, c)/|C|. Let fB (respectively, fB ) denote the Bayesian estimators of F? (respectively,
2
F? ) under the uniform prior.
We note that Ltop in the above definition takes a parameter and a decision in 2C \ ? as inputs, which
makes it different from the 0-1 loss function L0-1 that takes a pair of parameters as inputs, as the
one in Example 1. Hence, fB1 and fB2 are not the MLEs of their respective models, as was the
case in Example 1. We focus on voting rules obtained by our framework with Ltop . Certainly our
framework is not limited to this loss function.
Example 2 Bayesian estimators fB1 and fB2 coincide with Young [34]?s idea of selecting the alternative that is ?most likely to be the best (i.e., top-ranked in the true ranking)?, under F?1 and
F?2 respectively. This gives a theoretical justification of Young?s idea and other followups under
our framework. Specifically, fB1 is similar to rule studied by Procaccia et al. [29] and fB2 was
independently studied by Elkind and Shah [13].

4

Normative Properties of Bayesian Estimators

All omitted proofs can be found in the full version on arXiv.
Theorem 1 For any ?, fB1 satisfies anonymity, neutrality, and monotonicity. fB1 does not satisfy
majority or the Condorcet criterion for any ? < ?12 ,4 and it does not satisfy consistency.
Proof sketch: Anonymity and neutrality are obviously satisfied.
Monotonicity. Monotonicity follows from the following lemma.
Lemma 1 For any c ? C, let P 0 denote a profile obtained from P by raising the position of c in
one vote. For any W ? Lc (C), Pr(P 0 |W ) = Pr(P |W )/?; for any b ? C and any V ? Lb (C),
Pr(P 0 |V ) ? Pr(P |V )/?.
Majority and the Condorcet criterion. Let C = {c, b, c3 , . . . , cm }. We construct a profile P ?
where c is ranked in the top positions for more than half of the votes, but c 6? fB1 (P ? ).
For any k, let P ? denote a profile composed of k copies of [c  b  c3  ? ? ?  cm ], 1 of
[c  b  cm  ? ? ?  c3 ] and k ? 1 copies of [b  cm  ? ? ?  c3  c]. It is not hard to verify that
the WMG of P ? is as in Figure 1 (a).
P
Then, we prove that for any ? <

?1 ,
2

we can find m and k so that

P V ?Lc (C)
W ?Lb (C)

Pr(P |V )
Pr(P |W )

1+?2k +???+?2k(m?2)
1+?2 +???+?2(m?2)

=

? ?2 < 1. It follows that c is the Condorcet winner in P ? but it does not
minimize the Bayesian risk under M1? , which means that it is not the winner under fB1 .
3
4

In the Condorcet model the sample space is B(C)n [31]. We study a variant with sample space L(C)n .
1
Characterizing majority and Condorcet criterion of fB
for ? ? ?12 is an open question.

5

c
2	


c3	


2	


c4	


2k	


c

b

2	

2	


2k	


?

2k	


2k	


4k	


c3	


cm	


(a) The WMG of P ? .

c

b
2k	


2k	


c4	

 c3	


b	


b
2k	


4k	


c4	


(b) The WMGs of P1 (left) and P2 (right).

4	


6	

6	


c	


2	


a

6	


WMG of 6P	


6	

6	


(c) The WMG of P 0 (Thm. 3).

Figure 1: WMGs of the profiles for proofs: (a) for majority and Condorcet (Thm. 1); (b) for consistency
(Thm. 1); (c) for computational complexity (Thm. 3).

Consistency. We construct an example to show that fB1 does not satisfy consistency. In our construction m and n are even, and C = {c, b, c3 , c4 }. Let P1 and P2 denote profiles whose WMGs are
as shown in Figure 1 (b), respectively. We have the following lemma.
Lemma 2 Let P ? {P1 , P2 },

P
Pr(P |V )
P V ?Lc (C)
W ?L (C) Pr(P |W )

=

b

3(1+?4k )
.
2(1+?2k +?4k )

4k

3(1+? )
1
1
For any 0 < ? < 1, 2(1+?
2k +?4k ) > 1 for all k. It is not hard to verify that fB (P1 ) = fB (P2 ) = {c}
1
1
and fB (P1 ? P2 ) = {c, b}, which means that fB is not consistent.


Similarly, we can prove the following theorem for fB2 .
Theorem 2 For any ?, fB2 satisfies anonymity, neutrality, and monotonicity. It does not satisfy
majority, the Condorcet criterion, or consistency.
By Theorem 1 and 2, fB1 and fB2 do not satisfy as many desired normative properties as the Kemeny
rule for winners. On the other hand, they minimize Bayesian risk under F?1 and F?2 , respectively,
for which Kemeny does neither. In addition, neither fB1 nor fB2 satisfy consistency, which means
that they are not positional scoring rules.

5

Computational Complexity

We consider the following two types of decision problems.
Definition 5 In the BETTER BAYESIAN DECISION problem for a statistical decision-theoretic
framework (MC , D, L) under a prior distribution, we are given d1 , d2 ? D, and a profile P . We are
asked whether RB (P, d1 ) ? RB (P, d2 ).
We are also interested in checking whether a given alternative is the optimal decision.
Definition 6 In the OPTIMAL BAYESIAN DECISION problem for a statistical decision-theoretic
framework (MC , D, L) under a prior distribution, we are given d ? D and a profile P . We are
asked whether d minimizes the Bayesian risk RB (P, ?).
PNP
|| is the class of decision problems that can be computed by a P oracle machine with polynomial
NP
number of parallel calls to an NP oracle. A decision problem A is PNP
|| -hard, if for any P|| problem
B, there exists a polynomial-time many-one reduction from B to A. It is known that PNP
|| -hard
problems are NP-hard.
Theorem 3 For any ?, BETTER BAYESIAN DECISION and OPTIMAL BAYESIAN DECISION for F?1
under uniform prior are PNP
|| -hard.
Proof: The hardness of both problems is proved by a unified reduction from the K EMENY WINNER
problem, which is PNP
|| -complete [16]. In a K EMENY WINNER problem, we are given a profile P and
an alternative c, and we are asked if c is ranked in the top of at least one V ? L(C) that minimizes
Kendall(P, V ).
For any alternative c, the Kemeny score of c under M1? is the smallest distance between the profile
1
P and any linear order where c is ranked in the top. We next prove that when ? < m!
, the Bayesian
risk of c is largely determined by the Kemeny score of c.
1
, any c, b ? C, and any profile P , if the Kemeny score of c is strictly
Lemma 3 For any ? < m!
smaller than the Kemeny score of b in P , then RB (P, c) < RB (P, b) for M1? .
6

1
Let t be any natural number such that ?t < m!
. For any K EMENY WINNER instance (P, c) for
0
alternatives C , we add two more alternatives {a, b} and define a profile P 0 whose WMG is as
shown in Figure 3(c) using McGarvey?s trick [24]. The WMG of P 0 contains the WMG(P ) as a
subgraph, where the weights are 6 times the weights in WMG(P ).

Then, we let P ? = tP 0 , which is t copies of P 0 . It follows that for any V ? L(C), Pr(P ? |V, ?) =
Pr(P 0 |V, ?t ). By Lemma 3, if an alternative e has the strictly lowest Kemeny score for profile P 0 ,
then it the unique alternative that minimizes the Bayesian risk for P 0 and dispersion parameter ?t ,
which means that e minimizes the Bayesian risk for P ? and dispersion parameter ?.
Let O denote the set of linear orders over C 0 that minimizes the Kendall tau distance from P and let
k denote this minimum distance. Choose an arbitrary V 0 ? O. Let V = [b  a  V 0 ]. It follows
that Kendall(P 0 , V ) = 4 + 6k. If there exists W 0 ? O where c is ranked in the top position, then
we let W = [a  c  b  (V 0 ? {c})]. We have Kendall(P 0 , W ) = 2 + 6k. If c is not a Kemeny
winner in P , then for any W where d is not ranked in the top position, Kendall(P 0 , W ) ? 6 + 6k.
Therefore, a minimizes the Bayesian risk if and only if c is a Kemeny winner in P , and if c does not
minimize the Bayesian risk, then b does. Hence BETTER DECISION (checking if a is better than b)
and OPTIMAL BAYESIAN DECISION (checking if a is the optimal alternative) are PNP

|| -hard.
We note that OPTIMAL BAYESIAN DECISION in Theorem 3 is equivalent to checking whether a
given alternative c is in fB1 (P ). We do not know whether these problems are PNP
|| -complete. In
1
2
sharp contrast to fB , the next theorem states that fB under uniform prior is in P.
Theorem 4 For any rational number5 ?, BETTER BAYESIAN DECISION and OPTIMAL BAYESIAN
2
under uniform prior are in P.
DECISION for F?
The theorem is a corollary of the following stronger theorem that provides a closed-form formula
for Bayesian loss for F?2 .6 We recall that for any profile P and any pair of alternatives c, b, that
wP (c, b) is the weight on c ? b in the weighted majority graph of P .
Theorem 5 For F?2 under uniform prior, for any c ? C and any profile P , RB (P, c) = 1 ?
Q
1
.
b6=c
1 + ?wP (c,b)
The comparisons of Kemeny, fB1 , and fB2 are summarized in Table 1. According to the criteria we
consider, none of the three outperforms the others. Kemeny does well in normative properties, but
does not minimize Bayesian risk under either F?1 or F?2 , and is hard to compute. fB1 minimizes the
Bayesian risk under F?1 , but is hard to compute. We would like to highlight fB2 , which minimizes
the Bayesian risk under F?2 , and more importantly, can be computed in polynomial time despite the
similarity between F?1 and F?2 .

6

Asymptotic Comparisons

In this section, we ask the following question: as the number of voters, n ? ?, what is the
probability that Kemeny, fB1 , and fB2 choose different winners? We show that when the data is
generated from M1? , all three methods are equal asymptotically almost surely (a.a.s.), that is, they
are equal with probability 1 as n ? ?.
Theorem 6 Let Pn denote a profile of n votes generated i.i.d. from M1? given W ? Lc (C). Then,
Prn?? (Kemeny(Pn ) = fB1 (Pn ) = fB2 (Pn ) = c) = 1.
However, when the data are generated from M2? , we have a different story.
Theorem 7 For any W ? B(C) and any ?, fB1 (Pn ) = Kemeny(Pn ) a.a.s. as n ? ? and votes in
Pn are generated i.i.d. from M2? given W .
For any m ? 5, there exists W ? B(C) such that for any ?, there exists  > 0 such that with
probability at least , fB1 (Pn ) 6= fB2 (Pn ) and Kemeny(Pn ) 6= fB2 (Pn ) as n ? ? and votes in Pn
are generated i.i.d. from M2? given W .
5

We require ? to be rational to avoid representational issues.
The formula resembles Young?s calculation for three alternatives [34], where it was not clear whether the
calculation was done for F?2 . Recently it was clarified by Xia [31] that this is indeed the case.
6

7

c1	

c5	


c2	


c4	


c3	


(a) W ? B(C) for m = 5.

(b) Probability that g is different from Kemeny under M2? .

Figure 2: The ground truth W and asymptotic comparisons between Kemeny and g in Definition 7.
Proof sketch: The first part of Theorem 7 is proved by the Central Limit Theorem. For the second
part, the proof for m = 5 uses an acyclic W ? B(C) illustrated in Figure 2 (a).

Theorem 6 suggests that, when n is large and the votes are generated from M1? , it does not matter
much which of fB1 , fB2 , and Kemeny we use. A similar observation has been made for other voting
rules by Caragiannis et al. [7]. On the other hand, Theorem 7 states that when the votes are generated
from M2? , interestingly, for some ground truth parameter, fB2 is different from the other two with
non-negligible probability, and as we will see in the experiments, this probability can be quite large.
6.1

Experiments

We focus on the comparison between rule fB2 and Kemeny using synthetic data generated from M2?
given the binary relation W illustrated in Figure 2 (a). By Theorem 5, the computation involves
computing ??(n) , which is exponentially small for large n since ? < 1. Hence, we need a special
data structure to handle the computation of fB2 , because a straightforward implementation easily
loses precision. In our experiments, we use the following approximation for fB2 .
P
Definition 7 For any c ? C and profile P , let s(c, P ) = b:wP (b,c)>0 wP (b, c). Let g be the voting
rule such that for any profile P , g(P ) = arg minc s(c, P ).
In words, g selects the alternative c with the minimum total weight on the incoming edges in the
WMG. By Theorem 5, the Bayesian risk is largely determined by ??s(c,P ) . Therefore, g is a good
approximation of fB2 with reasonably large n. Formally, this is stated in the following theorem.
Theorem 8 For any W ? B(C) and any ?, fB2 (Pn ) = g(Pn ) a.a.s. as n ? ? and votes in Pn are
generated i.i.d. from M2? given W .
In our experiments, data are generated by M2? given W in Figure 2 (a) for m = 5, n ?
{100, 200, . . . , 2000}, and ? ? {0.1, 0.5, 0.9}. For each setting we generate 3000 profiles, and
calculate the fraction of trials in which g and Kemeny are different. The results are shown in Figuire 2 (b). We observe that for ? = 0.1 and 0.5, the probability for g(Pn ) 6= Kemeny(Pn ) is about
30% for most n in our experiments; when ? = 0.9, the probability is about 10%. In light of Theorem 8, these results confirm Theorem 7. We have also conducted similar experiments for M1? , and
found that the g winner is the same as the Kemeny winner in all 10000 randomly generated profiles
with m = 5, n = 100. This provides a check for Theorem 6.

7

Acknowledgments

We thank Shivani Agarwal, Craig Boutilier, Yiling Chen, Vincent Conitzer, Edith Elkind, Ariel
Procaccia, and anonymous reviewers of AAAI-14 and NIPS-14 for helpful suggestions and discussions. Azari Soufiani acknowledges Siebel foundation for the scholarship in his last year of PhD
studies. Parkes was supported in part by NSF grant CCF #1301976 and the SEAS TomKat fund.
Xia acknowledges an RPI startup fund for support.
8

References
[1] David Austen-Smith and Jeffrey S. Banks. Information Aggregation, Rationality, and the Condorcet Jury
Theorem. The American Political Science Review, 90(1):34?45, 1996.
[2] Hossein Azari Soufiani, David C. Parkes, and Lirong Xia. Random utility theory for social choice. In
Proc. NIPS, pages 126?134, 2012.
[3] James O. Berger. Statistical Decision Theory and Bayesian Analysis. Springer, 2nd edition, 1985.
[4] Craig Boutilier and Tyler Lu. Probabilistic and Utility-theoretic Models in Social Choice: Challenges for
Learning, Elicitation, and Manipulation. In IJCAI-11 Workshop on Social Choice and AI, 2011.
[5] Craig Boutilier, Ioannis Caragiannis, Simi Haber, Tyler Lu, Ariel D. Procaccia, and Or Sheffet. Optimal
social choice functions: A utilitarian view. In Proc. EC, pages 197?214, 2012.
[6] Ioannis Caragiannis and Ariel D. Procaccia. Voting Almost Maximizes Social Welfare Despite Limited
Communication. Artificial Intelligence, 175(9?10):1655?1671, 2011.
[7] Ioannis Caragiannis, Ariel Procaccia, and Nisarg Shah. When do noisy votes reveal the truth? In Proc. EC,
2013.
[8] Ioannis Caragiannis, Ariel D. Procaccia, and Nisarg Shah. Modal Ranking: A Uniquely Robust Voting
Rule. In Proc. AAAI, 2014.
[9] Marquis de Condorcet. Essai sur l?application de l?analyse a` la probabilit?e des d?ecisions rendues a` la
pluralit?e des voix. Paris: L?Imprimerie Royale, 1785.
[10] Vincent Conitzer and Tuomas Sandholm. Common voting rules as maximum likelihood estimators. In
Proc. UAI, pages 145?152, Edinburgh, UK, 2005.
[11] Vincent Conitzer, Matthew Rognlie, and Lirong Xia. Preference functions that score rankings and maximum likelihood estimation. In Proc. IJCAI, pages 109?115, 2009.
[12] Cynthia Dwork, Ravi Kumar, Moni Naor, and D. Sivakumar. Rank aggregation methods for the web. In
Proc. WWW, pages 613?622, 2001.
[13] Edith Elkind and Nisarg Shah. How to Pick the Best Alternative Given Noisy Cyclic Preferences? In
Proc. UAI, 2014.
[14] Peter C. Fishburn. Condorcet social choice functions. SIAM Journal on Applied Mathematics, 33(3):
469?489, 1977.
[15] Sumit Ghosh, Manisha Mundhe, Karina Hernandez, and Sandip Sen. Voting for movies: the anatomy of
a recommender system. In Proc. AAMAS, pages 434?435, 1999.
[16] Edith Hemaspaandra, Holger Spakowski, and J?org Vogel. The complexity of Kemeny elections. Theoretical Computer Science, 349(3):382?391, December 2005.
[17] John Kemeny. Mathematics without numbers. Daedalus, 88:575?591, 1959.
[18] Jen-Wei Kuo, Pu-Jen Cheng, and Hsin-Min Wang. Learning to Rank from Bayesian Decision Inference.
In Proc. CIKM, pages 827?836, 2009.
[19] Bo Long, Olivier Chapelle, Ya Zhang, Yi Chang, Zhaohui Zheng, and Belle Tseng. Active Learning for
Ranking Through Expected Loss Optimization. In Proc. SIGIR, pages 267?274, 2010.
[20] Tyler Lu and Craig Boutilier. The Unavailable Candidate Model: A Decision-theoretic View of Social
Choice. In Proc. EC, pages 263?274, 2010.
[21] Tyler Lu and Craig Boutilier. Learning mallows models with pairwise preferences. In Proc. ICML, pages
145?152, 2011.
[22] Colin L. Mallows. Non-null ranking model. Biometrika, 44(1/2):114?130, 1957.
[23] Andrew Mao, Ariel D. Procaccia, and Yiling Chen. Better human computation through principled voting.
In Proc. AAAI, 2013.
[24] David C. McGarvey. A theorem on the construction of voting paradoxes. Econometrica, 21(4):608?610,
1953.
[25] Shmuel Nitzan and Jacob Paroush. The significance of independent decisions in uncertain dichotomous
choice situations. Theory and Decision, 17(1):47?60, 1984.
[26] Marcus Pivato. Voting rules as statistical estimators. Social Choice and Welfare, 40(2):581?630, 2013.
[27] Daniele Porello and Ulle Endriss. Ontology Merging as Social Choice: Judgment Aggregation under the
Open World Assumption. Journal of Logic and Computation, 2013.
[28] Ariel D. Procaccia and Jeffrey S. Rosenschein. The Distortion of Cardinal Preferences in Voting. In
Proc. CIA, volume 4149 of LNAI, pages 317?331. 2006.
[29] Ariel D. Procaccia, Sashank J. Reddi, and Nisarg Shah. A maximum likelihood approach for selecting
sets of alternatives. In Proc. UAI, 2012.
[30] Abraham Wald. Statistical Decision Function. New York: Wiley, 1950.
[31] Lirong Xia. Deciphering young?s interpretation of condorcet?s model. ArXiv, 2014.
[32] Lirong Xia and Vincent Conitzer. A maximum likelihood approach towards aggregating partial orders. In
Proc. IJCAI, pages 446?451, Barcelona, Catalonia, Spain, 2011.
[33] Lirong Xia, Vincent Conitzer, and J?er?ome Lang. Aggregating preferences in multi-issue domains by using
maximum likelihood estimators. In Proc. AAMAS, pages 399?406, 2010.
[34] H. Peyton Young. Condorcet?s theory of voting. American Political Science Review, 82:1231?1244, 1988.
[35] H. Peyton Young and Arthur Levenglick. A consistent extension of Condorcet?s election principle. SIAM
Journal of Applied Mathematics, 35(2):285?300, 1978.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 2935-variational-bayesian-stochastic-complexity-of-mixture-models.pdf

Variational Bayesian Stochastic
Complexity of Mixture Models
Kazuho Watanabe?
Department of Computational Intelligence
and Systems Science
Tokyo Institute of Technology
Mail Box:R2-5, 4259 Nagatsuta,
Midori-ku, Yokohama, 226-8503, Japan
kazuho23@pi.titech.ac.jp

Sumio Watanabe
P& I Lab.
Tokyo Institute of Technology
swatanab@pi.titech.ac.jp

Abstract
The Variational Bayesian framework has been widely used to approximate the Bayesian learning. In various applications, it has
provided computational tractability and good generalization performance. In this paper, we discuss the Variational Bayesian learning of the mixture of exponential families and provide some additional theoretical support by deriving the asymptotic form of
the stochastic complexity. The stochastic complexity, which corresponds to the minimum free energy and a lower bound of the
marginal likelihood, is a key quantity for model selection. It also
enables us to discuss the e?ect of hyperparameters and the accuracy of the Variational Bayesian approach as an approximation of
the true Bayesian learning.

1

Introduction

The Variational Bayesian (VB) framework has been widely used as an approximation of the Bayesian learning for models involving hidden (latent) variables such as
mixture models[2][4]. This framework provides computationally tractable posterior
distributions with only modest computational costs in contrast to Markov chain
Monte Carlo (MCMC) methods. In many applications, it has performed better
generalization compared to the maximum likelihood estimation.
In spite of its tractability and its wide range of applications, little has been done
to investigate the theoretical properties of the Variational Bayesian learning itself.
For example, questions like how accurately it approximates the true one remained
unanswered until quite recently. To address these issues, the stochastic complexity
in the Variational Bayesian learning of gaussian mixture models was clari?ed and
the accuracy of the Variational Bayesian learning was discussed[10].
?
This work was supported by the Ministry of Education, Science, Sports and Culture,
Grant-in-Aid for JSPS Fellows 4637 and for Scientific Research 15500130, 2005.

In this paper, we focus on the Variational Bayesian learning of more general mixture models, namely the mixtures of exponential families which include mixtures of
distributions such as gaussian, binomial and gamma. Mixture models are known to
be non-regular statistical models due to the non-identi?ability of parameters caused
by their hidden variables[7]. In some recent studies, the Bayesian stochastic complexities of non-regular models have been clari?ed and it has been proven that they
become smaller than those of regular models[12][13]. This indicates an advantage
of the Bayesian learning when it is applied to non-regular models.
As our main results, the asymptotic upper and lower bounds are obtained for the
stochastic complexity or the free energy in the Variational Bayesian learning of the
mixture of exponential families. The stochastic complexity is important quantity
for model selection and giving the asymptotic form of it also contributes to the
following two issues. One is the accuracy of the Variational Bayesian learning as
an approximation method since the stochastic complexity shows the distance from
the variational posterior distribution to the true Bayesian posterior distribution in
the sense of Kullback information. Indeed, we give the asymptotic form of the
stochastic complexity as F (n)  ? log n where n is the sample size, by comparing
the coe?cient ? with that of the true Bayesian learning, we discuss the accuracy of
the VB approach. Another is the in?uence of the hyperparameter on the learning
process. Since the Variational Bayesian algorithm is a procedure of minimizing the
functional that ?nally gives the stochastic complexity, the derived bounds indicate
how the hyperparameters in?uence the process of the learning. Our results have
an implication for how to determine the hyperparameter values before the learning
process.
We consider the case in which the true distribution is contained in the learner model.
Analyzing the stochastic complexity in this case is most valuable for comparing the
Variational Bayesian learning with the true Bayesian learning. This is because the
advantage of the Bayesian learning is typical in this case[12]. Furthermore, this
analysis is necessary and essential for addressing the model selection problem and
hypothesis testing.
The paper is organized as follows. In Section 2, we introduce the mixture of exponential family model. In Section 3, we describe the Bayesian learning. In Section
4, the Variational Bayesian framework is described and the variational posterior
distribution for the mixture of exponential family model is derived. In Section 5,
we present our main result. Discussion and conclusion follow in Section 6.

2

Mixture of Exponential Family

Denote by c(x|b) a density function of the input x ? RN given an M -dimensional
parameter vector b = (b(1) , b(2) , ? ? ? , b(M ))T ? B where B is a subset of RM . The
general mixture model p(x|?) with a parameter vector ? is de?ned by
p(x|?) =

K


ak c(x|bk ),

k=1


where integer K is the number of components and {ak |ak ? 0, K
k=1 ak = 1} is the
set of mixing proportions. The model parameter ? is {ak , bk }K
k=1 .
A mixture model is called a mixture of exponential family (MEF) model or exponential family mixture model if the probability distribution c(x|b) for each component
is given by the following form,
c(x|b) = exp{b ? f(x) + f0 (x) ? g(b)},
(1)

where b ? B is called the natural parameter, b ? f(x) is its inner product with the
vector f(x) = (f1 (x), ? ? ? , fM (x))T , f0 (x) and g(b) are real-valued functions of the
input x and the parameter b, respectively[3]. Suppose functions f1 , ? ? ? , fM and a
constant function are linearly independent, which means the e?ective number of
parameters in a single component distribution c(x|b) is M .
The conjugate prior distribution ?(?) for the MEF model is given by the product
K
of the following two distributions on a = {ak }K
k=1 and b = {bk }k=1 ,
?(a) =

K
?(K?0 )  ?0 ?1
ak
,
?(?0 )K

(2)

k=1

?(b) =

K


?(bk ) =

k=1

K

exp{?0 (bk ? ?0 ? g(bk ))}
,
C(?0 , ?0)

(3)

k=1

where ?0 > 0, ?0 ? RM and ?0 > 0 are constants called hyperparameters and

C(?, ?) = exp{?(? ? b ? g(b))}db

(4)

is a function of ? ? R and ? ? RM .
The mixture model can be rewritten as follows by using a hidden variable y =
(y1 , ? ? ? , yK ) ? {(1, 0, ? ? ?, 0), (0, 1, ? ? ? , 0), ? ? ? , (0, 0, ? ? ?, 1)},
p(x, y|?) =

K 


yk
ak c(x|bk ) .

k=1

If and only if the datum x is generated from the kth component, yk = 1.

3

The Bayesian Learning

Suppose n training samples X n = {x1 , ? ? ? , xn } are independently and identically
taken from the true distribution p0 (x). In the Bayesian learning of a model p(x|?)
whose parameter is ?, ?rst, the prior distribution ?(?) on the parameter ? is set.
Then the posterior distribution p(?|X n ) is computed from the given dataset and
the prior by
1
exp(?nHn (?))?(?),
p(?|X n ) =
(5)
Z(X n )
where Hn (?) is the empirical Kullback information,
n

Hn (?) =

1
p0 (xi )
,
log
n i=1
p(xi |?)

(6)

and Z(X n ) is the normalization constant that is also known as the marginal likelihood or the evidence of the dataset X n [6]. The Bayesian predictive distribution
p(x|X n ) is given by averaging the model over the posterior distribution as follows,

p(x|X n ) = p(x|?)p(?|X n )d?.
(7)
The stochastic complexity F (X n ) is de?ned by
F (X n ) = ? log Z(X n ),

(8)

which is also called the free energy and is important in most data modelling problems. Practically, it is used as a criterion by which the model is selected and the
hyperparameters in the prior are optimized[1][9].
De?ne the average stochastic complexity F (n) by


F (n) = EX n F (X n ) ,

(9)

where EX n [?] denotes the expectation value over all sets of training samples. Recently, it was proved that F (n) has the following asymptotic form[12],
F (n)  ? log n ? (m ? 1) log log n + O(1),

(10)

where ? and m are the rational number and the natural number respectively which
are determined by the singularities of the set of true parameters. In regular statistical models, 2? is equal to the number of parameters and m = 1, whereas in
non-regular models such as mixture models, 2? is not larger than the number of
parameters and m ? 1. This means an advantage of the Bayesian learning.
However, in the Bayesian learning, one computes the stochastic complexity or the
predictive distribution by integrating over the posterior distribution, which typically
cannot be performed analytically. As an approximation, the VB framework was
proposed[2][4].

4

The Variational Bayesian Learning

4.1

The Variational Bayesian Framework

In the VB framework, the Bayesian posterior p(Y n , ?|X n ) of the hidden variables
and the parameters is approximated by the variational posterior q(Y n , ?|X n ), which
factorizes as
q(Y n , ?|X n ) = Q(Y n |X n )r(?|X n ),
(11)
n
n
where Q(Y |X ) and r(?|X n ) are posteriors on the hidden variables and the parameters respectively. The variational posterior q(Y n , ?|X n ) is chosen to minimize
the functional F [q] de?ned by

q(Y n , ?|X n )p0 (X n )
d?,
(12)
q(Y n , ?|X n ) log
F [q] =
p(X n , Y n , ?)
n
Y

= F (X n ) + K(q(Y n , ?|X n )||p(Y n , ?|X n )),
n

n

n

(13)

n

where K(q(Y , ?|X )||p(Y , ?|X )) is the Kullback information between the true
Bayesian posterior p(Y n , ?|X n ) and the variational posterior q(Y n , ?|X n ) 1 . This
leads to the following theorem. The proof is well known[8].
Theorem 1 If the functional F [q] is minimized under the constraint (11) then the
variational posteriors, r(?|X n ) and Q(Y n |X n ), satisfy
	


1
r(?|X n ) =
?(?) exp log p(X n , Y n |?) Q(Y n |X n ) ,
(14)
Cr
	


1
Q(Y n |X n ) =
exp log p(X n , Y n |?) r(?|X n ) ,
(15)
CQ
1

K(q(x)||p(x)) denotes the Kullback information from a distribution q(x) to a distribution p(x), that is,

q(x)
K(q(x)||p(x)) =
dx.
q(x) log
p(x)

where Cr and CQ are the normalization constants2 .
We de?ne the stochastic complexity in the VB learning F (X n ) by the minimum
value of the functional F [q] , that is ,
F (X n ) = min F [q],
r,Q

which shows the accuracy of the VB approach as an approximation of the Bayesian
learning. F (X n ) is also used for model selection since it gives an upper bound of
the true Bayesian stochastic complexity F (X n ).
4.2

Variational Posterior for MEF Model

In this subsection, we derive the variational posterior r(?|X n ) for the MEF model
based on (14) and then de?ne the variational parameter for this model.
Using the complete data {X n , Y n } = {(x1 , y1 ), ? ? ? , (xn , yn )}, we put
n
n

1  k
y ki = yik Q(Y n ) , nk =
y ki , and ?k =
y f(xi ),
nk i=1 i
i=1
where yik = 1 if and only if the ith datum xi is from the kth component. The
variable nk is the expected number of the data that are estimated to be from the
kth component. From (14) and the respective prior (2) and (3), the variational
posterior r(?) is obtained as the product of the following two distributions3 ,
K
?(n + K?0 )  nk +?0 ?1
r(a) = K
ak
,
k=1 ?(nk + ?0 ) k=1

r(b) =

K


r(bk ) =

k=1
nk ?k +?0 ?0
and
nk +?0

where ?k =

K

k=1

1
exp{?k (?k ? bk ? g(bk ))},
C(?k , ?k )

(16)

(17)

?k = nk + ?0 . Let

nk + ? 0
,
(18)
n + K?0
1 ? log C(?k , ?k )
bk = bk r(bk ) =
,
(19)
?k
??k
and de?ne the variational parameter ? by ? = ?r(?) = {ak , bk }K
k=1 . Then it is
noted that the variational posterior r(?) and CQ in (15) are parameterized by the
variational parameter ?. Therefore, we denote them as r(?|?) and CQ (?) henceforth.
We de?ne the variational estimator ?vb by the variational parameter ? that attains
the minimum value of the stochastic complexity F (X n ). Then, putting (15) into
(12), we obtain
ak = ak r(a) =

F (X n )
n

where S(X ) = ?

=

min{K(r(?|?)||?(?)) ? (log CQ(?) + S(X n ))},
?

K(r(?|? vb )||?(?)) ? (log CQ (? vb ) + S(X n )),
i=1 log p0 (x).

=
n

(20)
(21)

Therefore, our aim is to evaluate the minimum value of (20) as a function of the
variational parameter ?.
2

?p(x) denotes the expectation over p(x).
Hereafter, we omit the condition X n of the variational posteriors, and abbreviate them
to q(Y n , ?), Q(Y n ) and r(?).
3

5

Main Result

The average stochastic complexity F (n) in the VB learning is de?ned by
F (n) = EX n [F (X n )].

(22)

We assume the following conditions.
(i) The true distribution p0 (x) is an MEF model p(x|?0 ) which has K0 com0
ponents and the parameter ?0 = {a?k , b?k }K
k=1 ,
p(x|?0 ) =

K0


a?k exp{b?k ? f(x) + f0 (x) ? g(b?k )},

k=1

b?k

M

? R and
where
has K components,

b?k

p(x|?) =

= b?j (k = j). And suppose that the model p(x|?)
K


ak exp{bk ? f(x) + f0 (x) ? g(bk )},

k=1

and K ? K0 holds.
(ii) The prior distribution of the parameters is ?(?) = ?(a)?(b) given by (2)
and (3) with ?(b) bounded.
(iii) Regarding the distribution c(x|b) of each component, the Fisher information
2
g(b)
matrix I(b) = ??b?b
satis?es 0 < |I(b)| < +?, for arbitrary b ? B 4 . The
function ? ? b ? g(b) has a stationary point at ?b in the interior of B for each
? ? { ?g(b)
?b |b ? B}.
Under these conditions, we prove the following.
Theorem 2 (Main Result) Assume the conditions (i),(ii) and (iii). Then the
average stochastic complexity F (n) defined by (22) satisfies


? log n + EX n nHn (? vb ) + C1 ? F (n) ? ? log n + C2 ,
(23)
for an arbitrary natural number n, where C1 , C2 are constants independent of n and


0 ?1
,
(?0 ? M2+1 ),
(K ? 1)?0 + M
(K ? K0 )?0 + M K0 +K
2
2
?
=
?=
(24)
M K+K?1
M K+K?1
,
(?0 > M2+1 ).
2
2
This theorem shows the asymptotic form of the average stochastic complexity in
the Variational Bayesian learning. The coe?cients ?, ? of the leading terms are
identi?ed by K,K0 , that are the numbers of components of the learner and the true
distribution, the number of parameters M of each component and the hyperparameter ?0 of the conjugate prior given by (2).


In this theorem, nHn (?vb ) = ? ni=1 log p(xi |?vb )?S(X n ), and ? ni=1 log p(xi |? vb )
is a  training error which is computable during the learning. If the term
EX n nHn (? vb ) is a bounded function of n, then it immediately follows from this
theorem that
? log n + O(1) ? F 0 (n) ? ? log n + O(1),
4 ? 2 g(b)
?b?b

denotes the matrix whose ijth entry is
of a matrix.

? 2 g(b)
?b(i) ?b(j)

and |?| denotes the determinant

where O(1) is a bounded function of n. In certain cases, such as binomial mixtures
and mixtures of von-Mises distributions, it is actually a bounded function of n. In
the case of gaussian mixtures, if B = RN , it is conjectured that the minus likelihood
ratio min? nHn (?), a lower bound of nHn (? vb ), is at most of the order of log log n[5].
Since the dimension of the parameter ? is M K + K ? 1, the average stochastic
complexity of regular statistical models, which coincides with the Bayesian information criterion (BIC)[9] is given by ?BIC log n where ?BIC = M K+K?1
. Theorem
2
2 claims that the coe?cient ? of log n is smaller than ?BIC when ?0 ? (M + 1)/2.
This implies that the advantage of non-regular models in the Bayesian learning still
remains in the VB learning.
(Outline of the proof of Theorem 2)
From the condition (iii), calculating C(?k , ?k ) in (17) by the saddle point approximation, K(r(?|?)||?(?)) in (20) is evaluated as follows 5 ,
K(r(?|?)||?(?)) = G(a) ?

K


log ?(bk ) + Op (1),

(25)

k=1

where the function G(a) of a = {ak }K
k=1 is given by
K

G(a) =

MK + K ? 1
1 
M
log ak .
log n + {
? (?0 ? )}
2
2
2

(26)

k=1

Then log CQ (?) in (20) is evaluated as follows.
nHn (?) + Op(1) ? ?(log CQ (?) + S(X n )) ? nH n (?) + Op (1)
where

(27)

n

H n (?) =

1
p(xi |?0 )
log K

?
n i=1
k=1 ak c(xi |bk ) exp ?

C
nk +min{?0 ,?0 }

,

and C  is a constant. Thus, from (20), evaluating the right-hand sides of (25) and
(27) at speci?c points near the true parameter ?0 , we obtain the upper bound in
(23). The lower bound in (23) is obtained from (25) and (27) by Jensen?s inequality
K
and the constraint k=1 ak = 1. (Q.E.D)

6

Discussion and Conclusion

In this paper, we showed the upper and lower bounds of the stochastic complexity
for the mixture of exponential family models in the VB learning.
Firstly, we compare the stochastic complexity shown in Theorem 2 with the one
in the true Bayesian learning. On the mixture models with M parameters in each
component, the following upper bound for the coe?cient of F (n) in (10) is known
[13],

(K + K0 ? 1)/2 (M = 1),
??
(28)
(K ? K0 ) + (M K0 + K0 ? 1)/2 (M ? 2).
By the certain conditions about the prior distribution under which the above bound
was derived, we can compare the stochastic complexity when ?0 = 1. Putting ?0 = 1
in (24), we have
? = K ? K0 + (M K0 + K0 ? 1)/2.
(29)
5

Op (1) denotes a random variable bounded in probability.

Since we obtain F (n)  ? log n+O(1) under certain assumptions[11], let us compare
? of the VB learning to ? in (28) of the true Bayesian learning. When M = 1, that
is, each component has one parameter, ? ? ? holds since K0 ? K. This means
that the more redundant components the model has, the more the VB learning
di?ers from the true Bayesian learning. In this case, 2? is equal to the number
of the parameters of the model. Hence the BIC[9] corresponds to ? log n when
M = 1. If M ? 2, the upper bound of ? is equal to ?. This implies that the
variational posterior is close to the true Bayesian posterior when M ? 2. More
precise discussion about the accuracy of the approximation can be done for models
on which tighter bounds or exact values of the coe?cient ? in (10) are given[10].
Secondly, we point out that Theorem 2 shows how the hyperparameter ?0 in?uence the process of the VB learning. The coe?cient ? in (24) indicates that only
when ?0 ? (M + 1)/2, the prior distribution (2) works to eliminate the redundant
components that the model has and otherwise it works to use all the components.
And lastly, let us give examples of how to use the theoretical bounds in (23). One
can examine experimentally whether the actual iterative algorithm converges to the
optimal variational posterior instead of local minima by comparing the stochastic
complexity with our theoretical result. The theoretical bounds would also enable us
to compare the accuracy of the VB learning with that of the Laplace approximation
or the MCMC method. As mentioned in Section 4, our result will be important for
developing e?ective model selection methods using F (X n ) in the future work.

References
[1] H.Akaike, ?Likelihood and Bayes procedure,? Bayesian Statistics, (Bernald J.M. eds.)
University Press, Valencia, Spain, pp.143-166, 1980.
[2] H.Attias, ?Inferring parameters and structure of latent variable models by variational
bayes,? Proc. of UAI, 1999.
[3] L.D.Brown, ?Fundamentals of statistical exponential families,? IMS Lecture NotesMonograph Series, 1986.
[4] Z.Ghahramani, M.J.Beal, ?Graphical models and variational methods,? Advanced
Mean Field Methods , MIT Press, 2000.
[5] J.A.Hartigan, ?A Failure of likelihood asymptotics for normal mixtures,? Proc. of the
Berkeley Conference in Honor of J.Neyman and J.Kiefer, Vol.2, 807-810, 1985.
[6] D.J. Mackay, ?Bayesian interpolation,? Neural Computation, 4(2), pp.415-447, 1992.
[7] G.McLachlan, D.Peel,?Finite mixture models,? Wiley, 2000.
[8] M.Sato, ?Online model selection based on the variational bayes,? Neural Computation,
13(7), pp.1649-1681, 2001.
[9] G.Schwarz, ?Estimating the dimension of a model,? Annals of Statistics, 6(2), pp.461464, 1978.
[10] K.Watanabe, S.Watanabe, ?Lower bounds of stochastic complexities in variational
bayes learning of gaussian mixture models,? Proc. of IEEE CIS04, pp.99-104, 2004.
[11] K.Watanabe, S.Watanabe, ?Stochastic complexity for mixture of exponential families
in variational bayes,? Proc. of ALT05, pp.107-121, 2005.
[12] S.Watanabe,?Algebraic analysis for non-identifiable learning machines,? Neural Computation, 13(4), pp.899-933, 2001.
[13] K.Yamazaki, S.Watanabe, ?Singularities in mixture models and upper bounds of
stochastic complexity,? Neural Networks, 16, pp.1029-1038, 2003.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 1726-a-variational-baysian-framework-for-graphical-models.pdf

A Variational Bayesian Framework for
Graphical Models

Hagai Attias
hagai@gatsby.ucl.ac.uk
Gatsby Unit, University College London
17 Queen Square
London WC1N 3AR, U.K.

Abstract
This paper presents a novel practical framework for Bayesian model
averaging and model selection in probabilistic graphical models.
Our approach approximates full posterior distributions over model
parameters and structures, as well as latent variables, in an analytical manner. These posteriors fall out of a free-form optimization
procedure, which naturally incorporates conjugate priors. Unlike
in large sample approximations, the posteriors are generally nonGaussian and no Hessian needs to be computed. Predictive quantities are obtained analytically. The resulting algorithm generalizes
the standard Expectation Maximization algorithm, and its convergence is guaranteed. We demonstrate that this approach can be
applied to a large class of models in several domains, including
mixture models and source separation.

1

Introduction

A standard method to learn a graphical model 1 from data is maximum likelihood
(ML). Given a training dataset, ML estimates a single optimal value for the model
parameters within a fixed graph structure. However, ML is well known for its tendency to overfit the data. Overfitting becomes more severe for complex models
involving high-dimensional real-world data such as images, speech, and text. Another problem is that ML prefers complex models, since they have more parameters
and fit the data better. Hence, ML cannot optimize model structure.
The Bayesian framework provides, in principle, a solution to these problems. Rather
than focusing on a single model, a Bayesian considers a whole (finite or infinite) class
of models. For each model, its posterior probability given the dataset is computed.
Predictions for test data are made by averaging the predictions of all the individual models, weighted by their posteriors. Thus, the Bayesian framework avoids
overfitting by integrating out the parameters. In addition, complex models are
automatically penalized by being assigned a lower posterior probability, therefore
optimal structures can be identified.
Unfortunately, computations in the Bayesian framework are intractable even for
lWe use the term 'model' to refer collectively to parameters and structure.

H. Attias

210

very simple cases (e.g. factor analysis; see [2]). Most existing approximation methods fall into two classes [3]: Markov chain Monte Carlo methods and large sample
methods (e.g., Laplace approximation). MCMC methods attempt to achieve exact
results but typically require vast computational resources, and become impractical
for complex models in high data dimensions. Large sample methods are tractable,
but typically make a drastic approximation by modeling the 'posteriors over all
parameters as Normal, even for parameters that are not positive definite (e.g., covariance matrices). In addition, they require the computation ofthe Hessian, which
may become quite intensive.
In this paper I present Variational Bayes (VB), a practical framework for Bayesian
computations in graphical models. VB draws together variational ideas from intractable latent variables models [8] and from Bayesian inference [4,5,9], which, in
turn, draw on the work of [6]. This framework facilitates analytical calculations of
posterior distributions over the hidden variables, parameters and structures. The
posteriors fall out of a free-form optimization procedure which naturally incorporates conjugate priors, and emerge in standard forms, only one of which is Normal.
They are computed via an iterative algorithm that is closely related to Expectation
Maximization (EM) and whose convergence is guaranteed. No Hessian needs to
be computed. In addition, averaging over models to compute predictive quantities
can be performed analytically. Model selection is done using the posterior over
structure; in particular, the BIC/MDL criteria emerge as a limiting case.

2

General Framework

We restrict our attention in this paper to directed acyclic graphs (DAGs, a.k.a.
Bayesian networks). Let Y = {y., ... ,YN} denote the visible (data) nodes, where
n = 1, ... , N runs over the data instances, and let X = {Xl, ... , XN} denote the
hidden nodes. Let e denote the parameters, which are simply additional hidden
nodes with their own distributions. A model with a fixed structure m is fully defined
by the joint distribution p(Y, X, elm). In a DAG, this joint factorizes over the
nodes, i.e. p(Y,X I e,m) = TIiP(Ui I pai,Oi,m), where Ui E YUX, pai is the set
of parents of Ui, and Oi E e parametrize the edges directed toward Ui. In addition,
we usually assume independent instances, p(Y, X Ie, m) = TIn p(y n, Xn Ie, m).
We shall also consider a set of structures m E M, where m controls the number
of hidden nodes and the functional forms of the dependencies p( Ui I pai , 0i, m),
including the range of values assumed by each node (e.g., the number of components
in a mixture model). Associated with the set of structures is a structure prior p(m).
Marginal likelihood and posterior over parameters. For a fixed structure m,
we are interested in two quantities. The first is the parameter posterior distribution
p(e I Y,m). The second is the marginal likelihood p(Y I m), also known as the
evidence assigned to structure m by the data. In the following, the reference to m is
usually omitted but is always implied. Both quantities are obtained from the joint
p(Y, X, elm). For models with no hidden nodes the required computations can
often be performed analytically. However, in the presence of hidden nodes, these
quantities become computationally intractable. We shall approximate them using
a variational approach as follows.
Consider the joint posterior p(X, elY) over hidden nodes and parameters. Since
it is intractable, consider a variational posterior q(X, elY), which is restricted to
the factorized form
q(X, elY) = q(X I Y)q(e I Y) ,

(1)

wher"e given the data, the parameters and hidden nodes are independent. This

A Variational Baysian Frameworkfor Graphical Models

211

restriction is the key: It makes q approximate but tractable. Notice that we do
not require complete factorization, as the parameters and hidden nodes may still
be correlated amongst themselves.
We compute q by optimizing a cost function Fm[q] defined by

Fm[q]

=

!

dE> q(X)q(E? log ~~i~(:j

~ logp(Y I m) ,

(2)

where the inequality holds for an arbitrary q and follows from Jensen's inequality
(see [6]); it becomes an equality when q is the true posterior. Note that q is always
understood to include conditioning on Y as in (1). Since Fm is bounded from above
by the marginal likelihood, we can obtain the optimal posteriors by maximizing it
w.r.t. q. This can be shown to be equivalent to minimizing the KL distance between
q and the true posterior. Thus, optimizing Fm produces the best approximation to
the true posterior within the space of distributions satisfying (1), as well as the
tightest lower bound on the true marginal likelihood.
Penalizing complex models. To see that the VB objective function Fm penalizes
complexity, it is useful to rewrite it as

Fm = (log

p(Y, X

I E?

q(X)

)X,9 - KL[q(E?

II p(E?]

,

(3)

where the average in the first term on the r.h.s. is taken w.r.t. q(X, E?. The first
term corresponds to the (averaged) likelihood. The second term is the KL distance
between the prior and posterior over the parameters. As the number of parameters
increases, the KL distance follows and consequently reduces Fm.
This penalized likelihood interpretation becomes transparent in the large sample
limit N -7 00, where the parameter posterior is sharply peaked about the most
probable value E> = E>o. It can then be shown that the KL penalty reduces to
(I E>o 1/2) log N, which is linear in the number of parameters I E>o I of structure m.
Fm then corresponds precisely the Bayesian information criterion (BIC) and the
minimum description length criterion (MDL) (see [3]). Thus, these popular model
selection criteria follow as a limiting case of the VB framework.
Free-form optimization and an EM-like algorithm. Rather than assuming a
specific parametric form for the posteriors, we let them fall out of free-form optimization of the VB objective function. This results in an iterative algorithm directly
analogous to ordinary EM. In the E-step, we compute the posterior over the hidden
nodes by solving 8Fm/8q(X) = 0 to get

q(X) ex

e(log p(Y,XI9?e ,

(4)

where the average is taken w.r.t. q(E?.
In the M-step, rather than the 'optimal' parameters, we compute the posterior
distribution over the parameters by solving 8Fm/8q(E? = 0 to get

q(E? ex

e(IOgp(y,X I9?x p (E?

,

(5)

where the average is taken w.r.t. q(X).
This is where the concept of conjugate priors becomes useful. Denoting the exponential term on the r.h.s. of (5) by f(E?, we choose the prior p(E? from a family of
distributions such that q(E? ex f(E?p(E? belongs to that same family. p(E? is then
said to be conjugate to f(E?. This procedure allows us to select a prior from a fairly
large family of distributions (which includes non-informative ones as limiting cases)

H. Attias

212

and thus not compromise generality, while facilitating mathematical simplicity and
elegance. In particular, learning in the VB framework simply amounts to updating the hyperparameters, i.e., transforming the prior parameters to the posterior
parameters. We point out that, while the use of conjugate priors is widespread in
statistics, so far they could only be applied to models where all nodes were visible.
Structure posterior.
To compute q(m) we exploit Jensen's inequality once again to define a more general objective function, .1'[q] l:mEM q(m) [.1'm + logp(m)jq(m)] ~ 10gp(Y), where now q = q(X I m, Y)q(8 I
m, Y)q( m I Y) . After computing .1'm for each m EM, the structure posterior is
obtained by free-form optimization of .1':

q(m) ex e:Frnp(m) .

(6)

Hence, prior assumptions about the likelihood of different structures, encoded by
the prior p(m), affect the selection of optimal model structures performed according
to q( m), as they should.
Predictive quantities. The ultimate goal of Bayesian inference is to estimate
predictive quantities, such as a density or regression function. Generally, these
quantities are computed by averaging over all models, weighting each model by
its posterior. In the VB framework, exact model averaging is approximated by
replacing the true posterior p(8 I Y) by the variational q(8 I Y). In density
estimation, for example, the density assigned to a new data point Y is given by
p(y I Y) = J d8 p(y I 8) q(8 I Y) .

In some situations (e.g. source separation), an estimate of hidden node values x
from new data y may be required. The relevant quantity here is the conditional
p(x I y, Y), from which the most likely value of hidden nodes is extracted. VB
approximates it by p(x I y, Y) ex J d8 p(y, x I 8) q(8 I Y).

3

Variational Bayes Mixture Models

Mixture models have been investigated and analyzed extensively over many years.
However, the well known problems of regularizing against likelihood divergences and
of determining the required number of mixture components are still open. Whereas
in theory the Bayesian approach provides a solution, no satisfactory practical algorithm has emerged from the application of involved sampling techniques (e.g.,
[7]) and approximation methods [3] to this problem. We now present the solution
provided by VB.
We consider models of the form
m

P(Yn I 8,m) = LP(Yn

I Sn =

s,8) p(sn = s I 8),

(7)

s=1

where Yn denotes the nth observed data vector, and Sn denotes the hidden component that generated it. The components are labeled by s = 1, ... , m, with the structure parameter m denoting the number of components. Whereas our approach can
be applied to arbitrary models, for simplicity we consider here Normal component
distributions, P(Yn I Sn = s, 8) = N(JJ. s' r 8), where I-Ls is the mean and r 8 the precision (inverse covariance) matrix. The mixing proportions are P(Sn = S I 8) = 'Tr s'
In hindsight, we use conjugate priors on the parameters 8 = {'Trs, I-Ls' rs}. The
mixing proportions are jointly Dirichlet, p( {'Trs}) = V(..~O), the means (conditioned
on the preCisions) are Normal, p(l-Ls Irs) = N(pO, f3 0 r s), and the precisions are
Wishart, p(r s) = W(v O, ~O). We find that the parameter posterior for a fixed m

A Variational Baysian Framework/or Graphical Models

213

factorizes into q(8) = q({1I"s})flsq(J.?s,rs). The posteriors are obtained by the
following iterative algorithm, termed VB-MOG.
E-step. Compute the responsibilities for instance n using (4):

= S I Yn) ex ffs r!/2 e-(Yn-P,)Tr,(Yn-P,)/2 e- d / 2f3? ,
(8)
here X = S and q(S) = TIn q(Sn). This expression resembles the re-

,: == q(sn

noting that
sponsibilities in ordinary MLj the differences stem from integrating out the parameters. The special quantities in (8) are logffs == (log1l"s) = 1/1()..s) -1/1CLJs' )..s,),
d
logrs == (log I rs I) = Li=l1/1(lIs + 1 - i)/2) - log 1 ~s 1 +dlog2, and
i\ == (r s) = IIs~;l, where 1/1(x) = dlog r(x)/dx is the digamma function, and
the averages (-} are taken w.r.t. q(8). The other parameters are described below.
M-step. Compute the parameter posterior in two stages. First, compute the
quantities
1I"s

1 N
" n ,
= N 'L..J's

J.?s

=

1 N
" n Yn ,
N 'L..J's

~
Lis

N

=

" n enS '
N1 'L..J's

(9)

n=l
where C~ = (Yn - f.ts)(Yn - f.ts)T and fls = N7r s. This stage is identical to the
M-step in ordinary EM where it produces the new parameters. In VB, however, the
quantities in (9) only help characterize the new parameter posteriors. These posteriors are functionally identical to the priors but have different parameter values. The
mixing proportions are jointly Dirichlet, q( {11" s}) = D( {)..s}), the means are Normal,
q(J..t s Irs) = N(ps' /3srs), and the precisions are Wishart, p(rs) = W(lI s , ~s). The
posterior parameters are updated in the second stage, using the simple rules
)..s
lis

fls +)..0,

=

-

Ns +

0
II,

s n=1

/3s = fls + /30 ,
(10)
aT
a
~s = NsEs + N s/3 (J.?s - P )(I's - P ) /(Ns + f3 ) + ~ .
Ps

= (flsf.ts + /3opO)/(Ns +~) ,

s n=l

-

-

-

0 -

0-

?

The final values of the posterior parameters form the output of the VB-MOG. We
remark that (a) Whereas no specific assumptions have been made about them, the
parameter posteriors emerge in suitable, non-trivial (and generally non-Normal)
functional forms. (b) The computational overhead of the VB-MOG compared to
EM is minimal. (c) The covariance of the parameter posterior is O(l/N), and VBMOG reduces to EM (regularized by the priors) as N ~ 00. (d) VB-MOG has no
divergence problems. (e) Stability is guaranteed by the existence of an objective
function. (f) Finally, the approximate marginal likelihood F m , required to optimize
the number of components via (6), can also be obtained in closed form (omitted).
Predictive Density. Using our posteriors, we can integrate out the parameters
and show that the density assigned by the model to a new data vector Y is a mixture
of Student-t distributions,
m

(11)
s=1
where component S has Ws = lis + 1 - d d.o.f., mean Ps' covariance As = ?/3s +
1)//3sws)~s, and proportion 7rs = )..s/ Ls' )..s" (11) reduces to a MOG as N ~ 00.
Nonlinear Regression. We may divide each data vector into input and output parts, Y = (yi,y o ), and use the model to estimate the regression function
yO = f(yi) and error spheres. These may be extracted from the conditional
p(yO I yt, Y) = L:n=l Ws tw~ (yO I p~, A~), which also turns out to be a mixture
of Student-t distributions, with means p~ being linear, and covariances A~ and mixing proportions Ws nonlinear, in yi, and given in terms of the posterior parameters.

H Attias

214
Buffalo post offIce digits

Misclasslflcation rate histogram

1 , - - - - -- -- - - - - -- ,
0.8
0.6
0 .4
0 .2
0

0

(

,,
,,
-,
-

_1 -

0 .05

~

,

,,

,,
0 .1

Figure 1: VB-MOG applied to handwritten digit recognition.
VB-MOG was applied to the Boston housing dataset (UCI machine learning repository), where 13 inputs are used to predict the single output, a house's price. 100
random divisions of the N = 506 dataset into 481 training and 25 test points were
used, resulting in an average MSE of 11.9. Whereas ours is not a discriminative
method, it was nevertheless competitive with Breiman's (1994) bagging technique
using regression trees (MSE=11.7). For comparison, EM achieved MSE=14.6.
Classification. Here, a separate parameter posterior is computed for each class c
from a training dataset yc. Test data vector y is then classified according to the
conditional p(c I y, {yC}), which has a form identical to (11) (with c-dependent
parameters) multiplied by the relative size of yc.
VB-MOG was applied to the Buffalo post office dataset, which contains 1100 examples for each digit 0 - 9. Each digit is a gray-level 8 x 8 pixel array (see examples
in Fig. 1 (left)). We used 10 random 500-digit batches for training, and a separate
batch of 200 for testing. An average misclassification rate of .018 was obtained
using m = 30 components; EM achieved .025. The misclassification histograms
(VB=solid, EM=dashed) are shown in Fig. 1 (right).

4

VB and Intractable Models: a Blind Separation Example

The discussion so far assumed that a free-form optimization of the VB objective
function is feasible. Unfortunately, for many interesting models, in particular models where ordinary ML is intractable, this is not the case. For such models, we
modify the VB procedure as follows: (a) Specify a parametric functional form for
the posterior over the hidden nodes q(X) , and optimize w.r.t. its parameters, in the
spirit of [8J. (b) Let the parameter posterior q(8) fall out of free-form optimization,
as before.
We illustrate this approach in the context of the blind source separation (BSS)
problem (see, e.g., [1]). This problem is described by Yn = HX n + Un , where Xn is
an unobserved m-dim source vector at instance n, H is an unknown mixing matrix,
and the noise Un is Normally distributed with an unknown precision >'1. The task is
to construct a source estimate xn from the observed d-dim data y. The sources are
independent and non-Normally distributed. Here we assume the high-kurtosis distribution p(xi) ex: cosh-\xf /2) , which is appropriate for modeling speech sources.
One important but heretofore unresolved problem in BSS is determining the number m of sources from data. Another is to avoid overfitting the mixing matrix. Both
problems, typical to ML algorithms, can be remedied using VB.
It is the non-Normal nature ofthe sources that renders the source posterior p(X I Y)
intractable even before a Bayesian treatment. We use a Normal variational posterior
q(X) = TIn N(x n lPn' r n) with instance-dependent mean and precision. The
mixing matrix posterior q(H) then emerges as Normal. For simplicity, >. is optimized
rather than integrated out. The reSUlting VB-BSS algorithm runs as follows:

A Variational Baysian Framework for Graphical Models
log PrIm)

o

source reconstruction errOr

o '

-1000

-5

-2000

-10

-3000
-4000

215

-15
2

4

6

8
m

10

12

-roO~--~5~--~1-0--~
15
SNR(dB)

Figure 2: Application of VB to blind source separation algorithm (see text).
E-step. Optimize the variational mean Pn by iterating to convergence, for each n,
the fixed-point equation XfI:T(Yn - HPn) - tanhpn /2 = C- I Pn , where C is the
source covariance conditioned on the data. The variational precision matrix turns
out to be n-independent: r n = A. T AA. + 1/2 + C- I .
M-step. Update the mean and precision of the posterior q(H) (rules omitted).
This algorithm was applied to ll-dim data generated by linearly mixing 5 lOOmseclong speech and music signals obtained from commercial CDs. Gaussian noise were
added at different SNR levels. A uniform structure prior p( m) = 1/ K for m ~ K
was used. The resulting posterior over the number of sources (Fig. 2 (left)) is
peaked at the correct value m = 5. The sources were then reconstructed from test
data via p(x I y, Y). The log reconstruction error is plotted vs. SNR in Fig. 2
(right, solid). The ML error (which includes no model averaging) is also shown
(dashed) and is larger, reflecting overfitting.

5

Conclusion

The VB framework is applicable to a large class of graphical models. In fact, it may
be integrated with the junction tree algorithm to produce general inference engines
with minimal overhead compared to ML ones. Dirichlet, Normal and Wishart
posteriors are not special to models treated here but emerge as a general feature.
Current research efforts include applications to multinomial models and to learning
the structure of complex dynamic probabilistic networks.
Acknowledgements
I thank Matt Beal, Peter Dayan, David Mackay, Carl Rasmussen, and especially Zoubin
Ghahramani, for important discussions.
References
[1) Attias, H. (1999). Independent Factor Analysis. Neural Computation 11, 803-85l.
[2) Bishop, C.M. (1999). Variational Principal Component Analysis. Proc. 9th ICANN.
[3) Chickering, D.M. & Heckerman, D. (1997) . Efficient approximations for the marginal
likelihood of Bayesian networks with hidden variables. Machine Learning 29, 181-212.
[4) Hinton, G.E. & Van Camp, D. (1993). Keeping neural networks simple by minimizing
the description length of the weights. Proc. 6th COLT, 5-13.
[5) Jaakkola, T. & Jordan, M.L (1997). Bayesian logistic regression: A variational approach. Statistics and Artificial Intelligence 6 (Smyth, P. & Madigan, D., Eds).
[6) Neal, R.M. & Hinton, G.E. (1998). A view of the EM algorithm that justifies incremental, sparse, and other variants. Learning in Graphical Models, 355-368 (Jordan, M.L,
Ed). Kluwer Academic Press, Norwell, MA.
[7) Richardson, S. & Green, P.J. (1997). On Bayesian analysis of mixtures with an unknown number of components. Journal of the Royal Statistical Society B, 59, 731-792.
[8) Saul, L.K., Jaakkola, T., & Jordan, M.I. (1996). Mean field theory of sigmoid belief
networks. Journal of Artificial Intelligence Research 4, 61-76.
[9) Waterhouse, S., Mackay, D., & Robinson, T. (1996). Bayesian methods for mixture of
experts. NIPS-8 (Touretzky, D.S. et aI, Eds). MIT Press.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 4752-bayesian-hierarchical-reinforcement-learning.pdf

Bayesian Hierarchical Reinforcement Learning

Soumya Ray
Department of EECS
Case Western Reserve University
Cleveland, OH 44106
sray@case.edu

Feng Cao
Department of EECS
Case Western Reserve University
Cleveland, OH 44106
fxc100@case.edu

Abstract
We describe an approach to incorporating Bayesian priors in the MAXQ framework
for hierarchical reinforcement learning (HRL). We define priors on the primitive
environment model and on task pseudo-rewards. Since models for composite tasks
can be complex, we use a mixed model-based/model-free learning approach to
find an optimal hierarchical policy. We show empirically that (i) our approach
results in improved convergence over non-Bayesian baselines, (ii) using both task
hierarchies and Bayesian priors is better than either alone, (iii) taking advantage
of the task hierarchy reduces the computational cost of Bayesian reinforcement
learning and (iv) in this framework, task pseudo-rewards can be learned instead of
being manually specified, leading to hierarchically optimal rather than recursively
optimal policies.

1

Introduction

Reinforcement learning (RL) is a well known framework that formalizes decision making in unknown, uncertain environments. RL agents learn policies that map environment states to available
actions while optimizing some measure of long-term utility. While various algorithms have been developed for RL [1], and applied successfully to a variety of tasks [2], the standard RL setting suffers
from at least two drawbacks. First, it is difficult to scale standard RL approaches to large state spaces
with many factors (the well-known ?curse of dimensionality?). Second, vanilla RL approaches do
not incorporate prior knowledge about the environment and good policies.
Hierarchical reinforcement learning (HRL) [3] attempts to address the scaling problem by simplifying the overall decision making problem in different ways. For example, one approach introduces
macro-operators for sequences of primitive actions. Planning at the level of these operators may
result in simpler policies [4]. Another idea is to decompose the task?s overall value function, for
example by defining task hierarchies [5] or partial programs with choice points [6]. The structure
of the decomposition provides several benefits: first, for the ?higher level? subtasks, policies are
defined by calling ?lower level? subtasks (which may themselves be quite complex); as a result
policies for higher level subtasks may be expressed compactly. Second, a task hierarchy or partial
program can impose constraints on the space of policies by encoding knowledge about the structure
of good policies and thereby reduce the search space. Third, learning within subtasks allows state
abstraction, that is, some state variables can be ignored because they do not affect the policy within
that subtask. This also simplifies the learning problem.
While HRL attempts to address the scalability issue, it does not take into account probabilistic prior
knowledge the agent may have about the task. For example, the agent may have some idea about
where high/low utility states may be located and what their utilities may be, or some idea about the
approximate shape of the value function or policy. Bayesian reinforcement learning addresses this
issue by incorporating priors on models [7], value functions [8, 9] or policies [10]. Specifying good
1

priors leads to many benefits, including initial good policies, directed exploration towards regions
of uncertainty, and faster convergence to the optimal policy.
In this paper, we propose an approach that incorporates Bayesian priors in hierarchical reinforcement
learning. We use the MAXQ framework [5], that decomposes the overall task into subtasks so that
value functions of the individual subtasks can be combined to recover the value function of the
overall task. We extend this framework by incorporating priors on the primitive environment model
and on task pseudo-rewards. In order to avoid building models for composite tasks (which can
be very complex), we adopt a mixed model-based/model-free learning approach. We empirically
evaluate our algorithm to understand the effect of the priors in addition to the task hierarchy. Our
experiments indicate that: (i) taking advantage of probabilistic prior knowledge can lead to faster
convergence, even for HRL, (ii) task hierarchies and Bayesian priors can be complementary sources
of information, and using both sources is better than either alone, (iii) taking advantage of the task
hierarchy can reduce the computational cost of Bayesian RL, which generally tends to be very
high, and (iv) task pseudo-rewards can be learned instead of being manually specified, leading to
automatic learning of hierarchically optimal rather than recursively optimal policies. In this way
Bayesian RL and HRL are synergistic: Bayesian RL improves convergence of HRL and can learn
hierarchy parameters, while HRL can reduce the significant computational cost of Bayesian RL.
Our work assumes the probabilistic priors to be given in advance and focuses on learning with
them. Other work has addressed the issue of obtaining these priors. For example, one source of
prior information is multi-task reinforcement learning [11, 12], where an agent uses the solutions of
previous RL tasks to build priors over models or policies for future tasks. We also assume the task
hierarchy is given. Other work has explored learning MAXQ hierarchies in different settings [13].

2

Background and Related Work

In the MAXQ framework, each composite subtask Ti defines a semi-Markov decision process with
parameters hSi , Xi , Ci , Gi i. Si defines the set of ?non-terminal? states for Ti , where Ti may be
called by its parent. Gi defines a set of ?goal? states for Ti . The actions available within Ti are
described by the set of ?child tasks? Ci . Finally, Xi denotes the set of ?relevant state variables? for
Ti . Often, we unify the non-Si states and Gi into a single ?termination? predicate, Pi . An (s, a, s0 )
triple where Pi (s) is false, Pi (s0 ) is true, a ? Ci , and the transition probability P (s0 |s, a) > 0
? a) can be defined over exits to
is called an exit of the subtask Ti . A pseudo-reward function R(s,
express preferences over the possible exits of a subtask.
A hierarchical policy ? for the overall task is an assignment of a local policy to each SMDP Ti .
A hierarchically optimal policy is a hierarchical policy that has the maximum expected reward. A
hierarchical policy is said to be recursively optimal if the local policy for each subtask is optimal
given that all its subtask policies are optimal. Given a task graph, model-free [5] or model-based [14]
methods can be used to learn value functions for each task-subtask pair. In the model-free method,
a policy is produced by maintaining a value and a completion function for each subtask. For a task
i, the value V (a, s) denotes the expected value of calling child task a in state s. This is (recursively)
estimated as the expected reward obtained while executing a. The completion function C(i, s, a)
denotes the expected reward obtained while completing i after having called a in s. The central idea
behind MAXQ is that the value of i, V (i, s), can be (recursively) decomposed in terms of V (a, s)
and C(i, s, a). The model-based RMAXQ [14] algorithm extends RMAX [15] to MAXQ by learning
models for all primitive and composite tasks. Value iteration is used with these models to learn a
policy for each subtask. An optimistic exploration strategy is used together with a parameter m that
determines how often a transition or reward needs to be seen to be usable in the planning step.
In the MAXQ framework, pseudo-rewards must be manually specified to learn hierarchically optimal
policies. Recent work has attempted to directly learn hierarchically optimal policies for ALisp
partial programs, that generalize MAXQ task hierarchies [6, 16], using a model-free approach. Here,
along with task value and completion functions, an ?external? Q function QE is maintained for each
subtask. This function stores the reward obtained after the parent of a subtask exits. A problem here
is that this hurts state abstraction, since QE is no longer ?local? to a subtask. In later work [16],
this is addressed by recursively representing QE in terms of task value and completion functions,
linked by conditional probabilities of parent exits given child exits. The conditional probabilities
and recursive decomposition are used to compute QE as needed to select actions.
2

Bayesian reinforcement learning methods incorporate probabilistic prior knowledge on models [7],
value functions [8, 9], policies [10] or combinations [17]. One Bayesian model-based RL algorithm
proceeds as follows. At each step, a distribution over model parameters is maintained. At each
step, a model is sampled from this distribution (Thompson sampling [18, 19]). This model is then
solved and actions are taken according to the policy obtained. This yields observations that are used
to update the parameters of the current distribution to create a posterior distribution over models.
This procedure is then iterated to convergence. Variations of this idea have been investigated; for
example, some work converts the distribution over models to an empirical distribution over Qfunctions, and produces policies by sampling from this distribution instead [7].
Relatively little work exists that attempts to incorporate probabilistic priors into HRL. We have
found one preliminary attempt [20] that builds on the RMAX + MAXQ [14] method. This approach
adds priors to each subtask model and performs (separate) Bayesian model-based learning for each
subtask. 1 In our approach, we do not construct models for subtasks, which can be very complex
in general. Instead, we only maintain distributions over primitive actions, and use a mixed modelbased/model-free learning algorithm that is naturally integrated with the standard MAXQ learning
algorithm. Further, we show how to learn pseudo-rewards for MAXQ in the Bayesian framework.

3

Bayesian MAXQ Algorithm

In this section, we describe our approach to incorporating probabilistic priors into MAXQ. We use
priors over primitive models and pseudo-rewards. As we explain below, pseudo-rewards are value
functions; thus our approach uses priors both on models and value functions. While such an integration may not be needed for standard Bayesian RL, it appears naturally in our setting.
We first describe our approach to incorporating priors on environment models alone (assuming
pseudo-rewards are fixed). We do this following the Bayesian model-based RL framework. At
each step we have a distribution over environment models (initially the prior). The algorithm has
two main subroutines: the main BAYESIAN MAXQ routine (Algorithm 1) and an auxiliary R ECOM PUTE VALUE routine (Algorithm 2). In this description, the value V and completion C functions
are assumed to be global. At the start of each episode, the BAYESIAN MAXQ routine is called with
the Root task and the initial state for the current episode. The MAXQ execution protocol is then
followed, where each task chooses an action based on its current value function (initially random).
When a primitive action is reached and executed, it updates the posterior over model parameters
(Line 3) and its own value estimate (which is just the reward function for primitive actions). When
a task exits and returns to its parent, the parent subsequently updates its completion function based
on the current estimates of the value of the exit state (Lines 14 and 15). Note that in MAXQ, the
value function of a composite task can be (recursively) computed using the completion functions of
subtasks and the rewards obtained by executing primitive actions, so we do not need to separately
store or update the value functions (except for the primitive actions where the value function is the
reward). Finally, each primitive action maintains a count of how many times it has been executed
and each composite task maintains a count of how many child actions have been taken.
When k (an algorithm parameter) steps have been executed in a composite task, BAYESIAN MAXQ
calls R ECOMPUTE VALUE to re-estimate the value and completion functions (the check on k is
shown in R ECOMPUTE VALUE, Line 2). When activated, this function recursively re-estimates the
value/completion functions for all subtasks of the current task. At the level of a primitive action,
this simply involves resampling the reward and transition parameters from the current posterior
over models. For a composite task, we use the MAXQ - Q algorithm (Table 4 in [5]). We run this
algorithm for Sim episodes, starting with the current subtask as the root, with the current pseudoreward estimates (we explain below how these are obtained). This algorithm recursively updates the
completion function of the task graph below the current task. Note that in this step, the subtasks
with primitive actions use model-based updates. That is, when a primitive action is ?executed? in
such tasks, the currently sampled transition function (part of ? in Line 5) is used to find the next
state, and then the associated reward is used to update the completion function. This is similar to
Lines 12, 14 and 15 in BAYESIAN MAXQ, except that it uses the sampled model ? instead of the
1
While we believe this description is accurate, unfortunately, due to language issues and some missing
technical and experimental details in the cited article, we have been unable to replicate this work.

3

Algorithm 1 BAYESIAN MAXQ
Input: Task i, State s, Update Interval k, Simulation Episodes Sim
Output: Next state s0 , steps taken N , cumulative reward CR
1: if i is primitive then
2:
Execute i, observe r, s0
3:
Update current posterior parameters ? using (s, i, r, s0 )
4:
Update current value estimate: V (i, s) ? (1 ? ?) ? V (i, s) + ? ? r
5:
Count(i) ? Count(i) + 1
6:
return (s0 , 1, r)
7: else
8:
N ? 0, CR ? 0, taskStack ? Stack(){i is composite}
9:
while i is not terminated do
10:
R ECOMPUTE VALUE(i, k, Sim)
11:
a ? -greedy action from V (i, s)
12:
hs0 , Na , cri ? BAYESIAN MAXQ(a, s)
0
13:
taskStack.push(ha,
a , cri)
 s ,N

?
? s0 , a0 ) + V (a0 , s0 )
14:
as0 ? arg maxa0 C(i,


15:
C(i, s, a) ? (1 ? ?) ? C(i, s, a) + ? ? ? Na C(i, s0 , a?s0 ) + V (a?s0 , s0 )


? s0 ) + C(i,
? s0 , a?0 ) + V (a?0 , s0 )
? s, a) ? (1 ? ?) ? C(i,
? s, a) + ? ? ? Na R(i,
16:
C(i,
s
s
17:
s ? s0 , CR ? CR + ? N ? cr, N ? N + Na , Count(i) ? Count(i) + 1
18:
end while
? s0 ))
19:
U PDATE PSEUDO REWARD(taskStack, R(i,
20:
return (s0 , N, CR)
21: end if
Algorithm 2 R ECOMPUTE VALUE
Input: Task i, Update Interval k, Simulation Episodes Sim
Output: Recomputed value and completion functions for the task graph below and including i
1: if Count(i) < k then
2:
return
3: end if
4: if i is primitive then
5:
Sample new transition and reward parameters ? from current posterior ?
6: else
7:
for all child tasks a of i do
8:
R ECOMPUTE VALUE(a, k, Sim)
9:
end for
10:
for Sim episodes do
11:
s ? random nonterminal state of i
?
12:
Run MAXQ - Q(i, s, ?, R)
13:
end for
14: end if
15: Count(i) ? 0

real environment. After R ECOMPUTE VALUE terminates, a new set of value/completion functions
are available for BAYESIAN MAXQ to use to select actions.
Next we discuss task pseudo-rewards (PRs). A PR is a value associated with a subtask exit that
defines how ?good? that exit is for that subtask. The ideal PR for an exit is the expected reward under
the hierarchically optimal policy after exiting the subtask, until the global task (Root) ends; thus the
PR is a value function. This PR would enable the subtask to choose the ?right? exit in the context
of what the rest of the task hierarchy is doing. In standard MAXQ, these have to be set manually.
This is problematic because it presupposes (quite detailed) knowledge of the hierarchically optimal
policy. Further, setting the wrong PRs can result in non-convergence or highly suboptimal policies.
Sometimes this problem is sidestepped simply by setting all PRs to zero, resulting in recursively
optimal policies. However, it is easy to construct examples where a recursively optimal policy
4

Algorithm 3 U PDATE PSEUDO REWARD
?p
Input: taskStack, Parent?s pseudo reward R
0
?
0
1: tempCR ? Rp , Na ? 0, cr ? 0
2: while taskStack is not empty do
3:
ha, s, Na , cri ? taskStack.pop()
0
4:
tempCR ? ? Na ? tempCR + cr0
? s) using (a, s, tempCR)
5:
Update pseudo-reward posterior ? for R(a,
? s) from ?
6:
Resample R(a,
7:
Na0 ? Na , cr0 ? cr
8: end while

is arbitrarily worse than the hierarchically optimal policy. For all these reasons, PRs are major
?nuisance parameters? in the MAXQ framework.
What makes learning PRs tricky is that they are not only value functions, but also function as parameters of MAXQ. That is, setting different PRs essentially results in a new learning problem. For
this reason, simply trying to learn PRs in a standard temporal difference (TD) way fails (as we show
in our experiments). Fortunately, Bayesian RL allows us to address both these issues. First, we
can treat value functions as probabilistic unknown parameters. Second, and more importantly, a key
idea in Bayesian RL is the ?lifting? of exploration to the space of task parameters. That is, instead
of exploration through action selection, Bayesian RL can perform exploration by sampling task parameters. Thus treating a PR as an unknown Bayesian parameter also leads to exploration over the
value of this parameter, until an optimal value is found. In this way, hierarchically optimal policies
can be learned from scratch?a major advantage over the standard MAXQ setting.
To learn PRs, we again maintain a distribution over all such parameters, ?, initially a prior. For
simplicity, we only focus on tasks with multiple exits, since otherwise, a PR has no effect on the
policy (though the value function changes). When a composite task executes, we keep track of each
child task?s execution in a stack. When the parent itself exits, we obtain a new observation of the
PRs of each child by computing the discounted cumulative reward received after it exited, added to
the current estimate of the parent?s PR (Algorithm 3). This observation is used to update the current
posterior over the child?s PR. Since this is a value function estimate, early in the learning process,
the estimates are noisy. Following prior work [8], we use a window containing the most recent
observations. When a new observation arrives, the oldest observation is removed, the new one is
added and a new posterior estimate is computed. After updating the posterior, it is sampled to obtain
a new PR estimate for the associated exit. This estimate is used where needed (in Algorithms 1 and
2) until the next posterior update. Combined with the model-based priors above, we hypothesize
that this procedure, iterated till convergence, will produce a hierarchically optimal policy.

4

Empirical Evaluation

In this section, we evaluate our approach and test four hypotheses: First, does incorporating modelbased priors help speed up the convergence of MAXQ to the optimal policy? Second, does the
task hierarchy still matter if very good priors are available for primitive actions? Third, how does
Bayesian MAXQ compare to standard (flat) Bayesian RL? Does Bayesian RL perform better (in
terms of computational time) if a task hierarchy is available? Finally, can our approach effectively
learn PRs and policies that are hierarchically optimal?
We first focus on evaluating the first three hypotheses using domains where a zero PR results in
hierarchical optimality. To evaluate these hypotheses, we use two domains: the fickle version of
Taxi-World [5] (625 states) and Resource-collection [13] (8265 states). 2 In Taxi-World, the
agent controls a taxi in a grid-world that has to pick up a passenger from a source location and drop
them off at their destination. The state variables consist of the location of the taxi and the source
and destination of the passenger. The actions available to the agent consist of navigation actions and
actions to pickup and putdown the passenger. The agent gets a reward of +20 upon completing the
task, a constant ?1 reward for every action and a ?10 penalty for an erroneous action. Further, each
2

Task hierarchies for all domains are available in the supplementary material.

5

Average Cumulative Reward Per Episode

0

0

-200

-200

-400

-400

-600

-600

B-MaxQ Uninformed
B-MaxQ Good
B-MB-Q Uninformed
B-MB-Q Good
B-MB-Q Good Comparable Simulations

-800

-1000

Average Cumulative Reward Per Episode

0

0

100

200

300

400

-800

-1000
500

0

B-MaxQ Uninformed
B-MaxQ Good
B-MB-Q Uninformed
B-MB-Q Good

-200

B-MaxQ Uninformed
R-MaxQ
MaxQ
FlatQ
0

100

200

-200

-400

-400

-600

-600

-800

-800

-1000

300

400

500

B-MaxQ Uninformed
MaxQ
R-MaxQ
FlatQ

-1000
0

200

400

600

800

1000

0

Episode

200

400

600

800

1000

Episode

Figure 1: Performance on Taxi-World (top row) and Resource-collection (bottom). The x-axis
shows episodes. The prefix ?B-? denotes Bayesian, ?Uninformed/Good? denotes the prior and ?MB?
denotes model-based. Left column: Bayesian methods, right: non-Bayesian methods, with Bayesian
MAXQ for reference.
navigation action has a 15% chance of moving in each direction orthogonal to the intended move. In
the Resource-collection domain, the agent collects resources (gold and wood) from a grid world
map. Here the state variables consist of the location of the agent, what the agent is carrying, whether
a goldmine or forest is adjacent to its current location and whether a desired gold or wood quota has
been met. The actions available to the agent are to move to a specific location, chop gold or harvest
wood, and to deposit the item it is carrying (if any). For each navigation action, the agent has a 30%
chance of moving to a random location. In our experiments, the map contains two goldmines and
two forests, each containing two units of gold and two units of wood, and the gold and wood quota
is set to three each. The agent gets a +50 reward when it meets the gold/wood quota, a constant ?1
reward for every action and an additional ?1 for erroneous actions (such as trying to deposit when
it is not carrying anything).
For the Bayesian methods, we use Dirichlet priors for the transition function parameters and NormalGamma priors for the reward function parameters. We use two priors: an uninformed prior, set
to approximate a uniform distribution, and a ?good? prior where a previously computed model
posterior is used as the ?prior.? The prior distributions we use are conjugate to the likelihood, so
we can compute the posterior distributions in closed form. In general, this is not necessary; more
complex priors could be used as long as we can sample from the posterior distribution.
The methods we evaluate are: (i) Flat Q, the standard Q-learning algorithm, (ii) MAXQ -0, the standard, Q-learning algorithm for MAXQ with no PR, (iii) Bayesian model-based Q-learning with an
uninformed prior and (iv) a ?good? prior, (v) Bayesian MAXQ (our proposed approach) with an uninformed prior and (vi) a ?good? prior, and (vii) R MAXQ [14]. In our implementation, the Bayesian
model-based Q-learning uses the same code as the Bayesian MAXQ algorithm, with a ?trivial? hierarchy consisting of the Root task with only the primitive actions as children. For the Bayesian
methods, the update frequency k was set to 50 for Taxi-World and 25 for Resource-collection.
Sim was set to 200 for Bayesian MAXQ for Taxi-World and 1000 for Bayesian model-based Q, and
to 1000 for both for Resource collection. For R MAXQ, the threshold sample size m was set to
5 following prior work [14]. The value iteration was terminated either after 300 loops or when the
successive difference between iterations was less than 0.001. The theoretical version of R MAXQ
requires updating and re-solving the model every step. In practice for the larger problems, this is too
6

time-consuming, so we re-solve the models every 10 steps. This is similar to the update frequency
k for Bayesian MAXQ. The results are shown in Figure 1 (episodes on x-axis).
From these results, comparing the Bayesian versions of MAXQ to standard MAXQ, we observe that
for Taxi-World, the Bayesian version converges faster to the optimal policy even with the uninformed prior, while for Resource-collection, the convergence rates are similar. When a good prior
is available, convergence is very fast (almost immediate) in both domains. Thus, the availability
of model priors can help speed up convergence in many cases for HRL. We further observe that
R MAXQ converges more slowly than MAXQ or Bayesian MAXQ, though it is much better than Flat
Q. This is different from prior work [14]. This may be because our domains are more stochastic than
the Taxi-world on which prior results [14] were obtained. We conjecture that, as the environment
becomes more stochastic, errors in primitive model estimates may propagate into subtask models
and hurt the performance of this algorithm. In their analysis [14], the authors noted that the error in
the transition function for a composite task is a function of the total number of terminal states in the
subtask. The error is also compounded as we move up the task hierarchy. This could be countered by
increasing m, the sample size used to estimate model parameters. This would improve the accuracy
of the primitive model, but would further hurt the convergence rate of the algorithm.
Next, we compare the Bayesian MAXQ approach to ?flat? Bayesian model-based Q learning. We
note that in Taxi-World, with uninformed priors, though the ?flat? method initially does worse,
it soon catches up to standard MAXQ and then to Bayesian MAXQ. This is probably because in
this domain, the primitive models are relatively easy to acquire, and the task hierarchy provides no
additional leverage. For Resource-collection, however, even with a good prior, ?flat? Bayesian
model-based Q does not converge. The difference is that in this case, the task hierarchy encodes
extra information that cannot be deduced just from the models. In particular, the task hierarchy
tells the agent that good policies consist of gold/wood collection moves followed by deposit moves.
Since the reward structure in this domain is very sparse, it is difficult to deduce this even if very
good models are available. Taken together, these results show that task hierarchies and model priors
can be complementary: in general, Bayesian MAXQ outperforms both flat Bayesian RL and MAXQ
(in speed of convergence, since here MAXQ can learn the hierarchically optimal policy).
Table 1: Time for 500 episodes, Taxi-World.
Next, we compare the time taken by the difMethod
Time (s)
ferent approaches in our experiments in TaxiBayesian MaxQ, Uninformed Prior
205
World (Table 1). As expected, the Bayesian
Bayesian Model-based Q, Uninformed 4684
RL approaches are significantly slower than
Prior
the non-Bayesian approaches. Further, among
Bayesian MaxQ, Good Prior
96
non-Bayesian approaches, the hierarchical apBayesian Model-based Q, Good Prior
3089
proaches (MAXQ and R MAXQ) are slower than
Bayesian Model-based Q, Good Prior 4006
the non-hierarchical flat Q. Out of the Bayesian
& Comparable Simulations
methods, however, the Bayesian MAXQ apR MAXQ
229
MAXQ
2.06
proaches are significantly faster than the flat
Flat Q
1.77
Bayesian model-based approaches. This is because for the flat case, during the simulation in R ECOMPUTE VALUE, a much larger task needs to be
solved, while the Bayesian MAXQ approach is able to take into account the structure of the hierarchy
to only simulate subtasks as needed, which ends up being much more efficient. However, we note
that we allowed the flat Bayesian model-based approach 1000 episodes of simulation as opposed to
200 for Bayesian MAXQ. Clearly this increases the time taken for the flat cases. But at the same
time, this is necessary: the ?Comparable Simulations? row (and curve in Figure 1 top left) shows
that, if the simulations are reduced to 250 episodes for this approach, the resulting values are no
longer reliable and the performance of the Bayesian flat approach drops sharply. Notice that while
Flat Q runs faster than MAXQ (because of the additional ?bookkeeping? overhead due to the task
hierarchy), Bayesian MAXQ runs much faster than Bayesian model-based Q. Thus, taking advantage
of the hierarchical task decomposition helps reduce the computational cost of Bayesian RL.
Finally we evaluate how well our approach estimates PRs. Here we use two domains: a ModifiedTaxi-World and a Hallway domain [5, 21] (4320 states). In Modified-Taxi-World, we allow
dropoffs at any one of the four locations and do not provide a reward for task termination. Thus
the Navigate subtask needs a PR (corresponding to the correct dropoff location) to learn a good
policy. The Hallway domain consists of a maze with a large scale structure of hallways and intersections. The agent has stochastic movement actions. For these experiments, we use uninformed
priors on the environment model. The PR Gaussian-Gamma priors are set to prefer each exit from
7

Average Cumulative Reward Per Episode

0

0

-200

-200

-400

-400

-600

-600

-800

-800
B-MaxQ Bayes PR
B-MaxQ Manual PR
B-MaxQ No PR

-1000

Average Cumulative Reward Per Episode

-200

0

100

200

300

400

B-MaxQ Bayes PR
MaxQ Non-Bayes PR
MaxQ Manual PR
MaxQ No PR
ALispQ
FlatQ

-1000
500

-200

-400

-400

-600

-600

-800

-800

-1000

-1000

-1200

-1200

-1400

-1400

-1600

0

100

200

300

400

500

0

1000

2000
3000
Episode

4000

5000

-1600
B-MaxQ Bayes PR
B-MaxQ Manual PR
B-MaxQ No PR

-1800
-2000
0

1000

2000
3000
Episode

4000

-1800
-2000
5000

Figure 2: Performance on Modified-Taxi-World (top row) and Hallway (bottom). ?B-?: Bayesian,
?PR?: Pseudo Reward. Left: Bayesian methods, right: non-Bayesian methods, with Bayesian MAXQ
as reference. The x-axis is episodes. The bottom right figure has the same legend as the top right.
a subtask equally. The baselines we use are: (i) Bayesian MAXQ and MAXQ with fixed zero PR, (ii)
Bayesian MAXQ and MAXQ with fixed manually set PR, (iii) flat Q, (iv) ALISPQ [6] and (v) MAXQ
with a non-Bayesian PR update. This last method tracks PR just as our approach; however, instead
of a Bayesian update, it updates the PR using a temporal difference update, treating it as a simple
value function. The results are shown in Figure 2 (episodes on x-axis).
From these results, we first observe that the methods with zero PR always do worse than those with
?proper? PR, indicating that in these cases the recursively optimal policy is not the hierarchically
optimal policy. When a PR is manually set, in both domain, MAXQ converges to better policies. We
observe that in each case, the Bayesian MAXQ approach is able to learn a policy that is as good, starting with no pseudo rewards; further, its convergence rates are often better. Further, we observe that
the simple TD update strategy (MAXQ Non-Bayes PR in Figure 2) fails in both cases?in ModifiedTaxi-World, it is able to learn a policy that is approximately as good as a recursively optimal policy,
but in the Hallway domain, it fails to converge completely, indicating that this strategy cannot generally learn PRs. Finally, we observe that the tripartite Q-decomposition of ALISPQ is also able to
correctly learn hierarchically optimal policies, however, it converges slowly compared to Bayesian
MAXQ or MAXQ with manual PRs. This is especially visible in the Hallway domain, where there are
not many opportunities for state abstraction. We believe this is likely because it is estimating entire
Q-functions rather than just the PRs. In a sense, it is doing more work than is needed to capture
the hierarchically optimal policy, because an exact Q-function may not be needed to capture the
preference for the best exit, rather, a value that assigns it a sufficiently high reward compared to the
other exits would suffice. Taken together, these results indicate that incorporating Bayesian priors
into MAXQ can successfully learn PRs from scratch and produce hierarchically optimal policies.

5

Conclusion

In this paper, we have proposed an approach to incorporating probabilistic priors on environment
models and task pseudo-rewards into HRL by extending the MAXQ framework. Our experiments
indicate that several synergies exist between HRL and Bayesian RL, and combining them is fruitful.
In future work, we plan to investigate approximate model and value representations, as well as
multi-task RL to learn the priors.
8

References
[1] R.S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press, 1998.
[2] Leslie Pack Kaelbling, Michael L. Littman, and Andrew W. Moore. Reinforcement learning: A survey.
Journal of Artificial Intelligence Research, 4:237?285, 1996.
[3] Andrew G. Barto and Sridhar Mahadevan. Recent advances in hierarchical reinforcement learning. Discrete Event Dynamic Systems, 13(4):341?379, 2003.
[4] Martin Stolle and Doina Precup. Learning Options in reinforcement Learning, volume 2371/2002 of
Lecture Notes in Computer Science, pages 212?223. Springer, 2002.
[5] Thomas G. Dietterich. Hierarchical reinforcement learning with the maxq value function decomposition.
Journal of Artificial Intelligence Research, 13:227?303, 2000.
[6] D. Andre and S. Russell. State Abstraction for Programmable Reinforcement Learning Agents. In Proceedings of the Eighteenth National Conference on Artificial Intelligence (AAAI), 2002.
[7] R. Dearden, N. Friedman, and D. Andre. Model based bayesian exploration. In Proceedings of Fifteenth
Conference on Uncertainty in Artificial Intelligence. Morgan Kaufmann, 1999.
[8] R. Dearden, N. Friedman, and S. Russell. Bayesian Q-learning. In Proceedings of the Fifteenth National
Conference on Artificial Intelligence, 1998.
[9] Y. Engel, S. Mannor, and R. Meir. Bayes meets Bellman:the Gaussian process approach to temporal
difference learning. In Proceedings of the Twentieth Internationl Conference on Machine Learning, 2003.
[10] Mohammad Ghavamzadeh and Yaakov Engel. Bayesian policy gradient algorithms. In Advances in
Neural Information Processing Systems 19. MIT Press, 2007.
[11] Alessandro Lazaric and Mohammad Ghavamzadeh. Bayesian multi-task reinforcement learning. In Proceedings of the 27th International Conference on Machine Learning, 2010.
[12] Aaron Wilson, Alan Fern, Soumya Ray, and Prasad Tadepalli. Multi-task reinforcement learning: a
hierarchical bayesian approach. In Proceedings of the 24th international conference on Machine learning,
pages 1015?1022, New York, NY, USA, 2007. ACM.
[13] N. Mehta, S. Ray, P. Tadepalli, and T. Dietterich. Automatic discovery and transfer of MAXQ hierarchies.
In Andrew McCallum and Sam Roweis, editors, Proceedings of the 25th International Conference on
Machine Learning, pages 648?655. Omnipress, 2008.
[14] Nicholas K. Jong and Peter Stone. Hierarchical model-based reinforcement learning: R-MAX + MAXQ.
In Proceedings of the 25th International Conference on Machine Learning, 2008.
[15] Ronen I. Brafman, Moshe Tennenholtz, and Pack Kaelbling. R-MAX - a general polynomial time algorithm for near-optimal reinforcement learning. Journal of Machine Learning Research, 2001.
[16] B. Marthi, S. Russell, and D. Andre. A compact, hierarchically optimal q-function decomposition. In
22nd Conference on Uncertainty in Artificial Intelligence, 2006.
[17] M. Ghavamzadeh and Y. Engel. Bayesian actor-critic algorithms. In Zoubin Ghahramani, editor, Proceedings of the 24th Annual International Conference on Machine Learning, pages 297?304. Omnipress,
2007.
[18] W. R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence
of two samples. Biometrika, 25:285?294, 1933.
[19] M. J. A. Strens. A Bayesian framework for reinforcement learning. In Proceeding of the 17th International
Conference on Machine Learning, 2000.
[20] Zhaohui Dai, Xin Chen, Weihua Cao, and Min Wu. Model-based learning with bayesian and maxq value
function decomposition for hierarchical task. In Proceedings of the 8th World Congress on Intelligent
Control and Automation, 2010.
[21] Ronald Edward Parr. Hierarchical Control and Learning for Markov Decision Processes. PhD thesis,
1998.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 2570-constraining-a-bayesian-model-of-human-visual-speed-perception.pdf

Constraining a Bayesian Model of Human Visual
Speed Perception

Alan A. Stocker and Eero P. Simoncelli
Howard Hughes Medical Institute,
Center for Neural Science, and Courant Institute of Mathematical Sciences
New York University, U.S.A.

Abstract
It has been demonstrated that basic aspects of human visual motion perception are qualitatively consistent with a Bayesian estimation framework, where the prior probability distribution on velocity favors slow
speeds. Here, we present a refined probabilistic model that can account
for the typical trial-to-trial variabilities observed in psychophysical speed
perception experiments. We also show that data from such experiments
can be used to constrain both the likelihood and prior functions of the
model. Specifically, we measured matching speeds and thresholds in a
two-alternative forced choice speed discrimination task. Parametric fits
to the data reveal that the likelihood function is well approximated by
a LogNormal distribution with a characteristic contrast-dependent variance, and that the prior distribution on velocity exhibits significantly
heavier tails than a Gaussian, and approximately follows a power-law
function.
Humans do not perceive visual motion veridically. Various psychophysical experiments
have shown that the perceived speed of visual stimuli is affected by stimulus contrast,
with low contrast stimuli being perceived to move slower than high contrast ones [1, 2].
Computational models have been suggested that can qualitatively explain these perceptual
effects. Commonly, they assume the perception of visual motion to be optimal either within
a deterministic framework with a regularization constraint that biases the solution toward
zero motion [3, 4], or within a probabilistic framework of Bayesian estimation with a prior
that favors slow velocities [5, 6].
The solutions resulting from these two frameworks are similar (and in some cases identical), but the probabilistic framework provides a more principled formulation of the problem
in terms of meaningful probabilistic components. Specifically, Bayesian approaches rely
on a likelihood function that expresses the relationship between the noisy measurements
and the quantity to be estimated, and a prior distribution that expresses the probability of
encountering any particular value of that quantity. A probabilistic model can also provide a
richer description, by defining a full probability density over the set of possible ?percepts?,
rather than just a single value. Numerous analyses of psychophysical experiments have
made use of such distributions within the framework of signal detection theory in order to
model perceptual behavior [7].
Previous work has shown that an ideal Bayesian observer model based on Gaussian forms

?

posterior

low contrast
probability density

probability density

high contrast

likelihood

prior

a

posterior
likelihood

prior

v?

v?
visual speed

?

b

visual speed

Figure 1: Bayesian model of visual speed perception. a) For a high contrast stimulus, the
likelihood has a narrow width (a high signal-to-noise ratio) and the prior induces only a
small shift ? of the mean v? of the posterior. b) For a low contrast stimuli, the measurement
is noisy, leading to a wider likelihood. The shift ? is much larger and the perceived speed
lower than under condition (a).
for both likelihood and prior is sufficient to capture the basic qualitative features of global
translational motion perception [5, 6]. But the behavior of the resulting model deviates
systematically from human perceptual data, most importantly with regard to trial-to-trial
variability and the precise form of interaction between contrast and perceived speed. A
recent article achieved better fits for the model under the assumption that human contrast
perception saturates [8]. In order to advance the theory of Bayesian perception and provide
significant constraints on models of neural implementation, it seems essential to constrain
quantitatively both the likelihood function and the prior probability distribution. In previous
work, the proposed likelihood functions were derived from the brightness constancy constraint [5, 6] or other generative principles [9]. Also, previous approaches defined the prior
distribution based on general assumptions and computational convenience, typically choosing a Gaussian with zero mean, although a Laplacian prior has also been suggested [4]. In
this paper, we develop a more general form of Bayesian model for speed perception that
can account for trial-to-trial variability. We use psychophysical speed discrimination data
in order to constrain both the likelihood and the prior function.

1
1.1

Probabilistic Model of Visual Speed Perception
Ideal Bayesian Observer

Assume that an observer wants to obtain an estimate for a variable v based on a measurement m that she/he performs. A Bayesian observer ?knows? that the measurement device
is not ideal and therefore, the measurement m is affected by noise. Hence, this observer
combines the information gained by the measurement m with a priori knowledge about v.
Doing so (and assuming that the prior knowledge is valid), the observer will ? on average ?
perform better in estimating v than just trusting the measurements m. According to Bayes?
rule
1
p(v|m) = p(m|v)p(v)
(1)
?
the probability of perceiving v given m (posterior) is the product of the likelihood of v for
a particular measurements m and the a priori knowledge about the estimated variable v
(prior). ? is a normalization constant independent of v that ensures that the posterior is a
proper probability distribution.

P(v^ 2 > v^1)

1

+

Pcum=0.5

0

a

b

Pcum=0.875

vmatch vthres

v2

Figure 2: 2AFC speed discrimination experiment. a) Two patches of drifting gratings were
displayed simultaneously (motion without movement). The subject was asked to fixate
the center cross and decide after the presentation which of the two gratings was moving
faster. b) A typical psychometric curve obtained under such paradigm. The dots represent
the empirical probability that the subject perceived stimulus2 moving faster than stimulus1.
The speed of stimulus1 was fixed while v2 is varied. The point of subjective equality, vmatch ,
is the value of v2 for which Pcum = 0.5. The threshold velocity vthresh is the velocity for
which Pcum = 0.875.
It is important to note that the measurement m is an internal variable of the observer and
is not necessarily represented in the same space as v. The likelihood embodies both the
mapping from v to m and the noise in this mapping. So far, we assume that there is a
monotonic function f (v) : v ? vm that maps v into the same space as m (m-space).
Doing so allows us to analytically treat m and vm in the same space. We will later propose
a suitable form of the mapping function f (v).
An ideal Bayesian observer selects the estimate that minimizes the expected loss, given the
posterior and a loss function. We assume a least-squares loss function. Then, the optimal
estimate v? is the mean of the posterior in Equation (1). It is easy to see why this model
of a Bayesian observer is consistent with the fact that perceived speed decreases with contrast. The width of the likelihood varies inversely with the accuracy of the measurements
performed by the observer, which presumably decreases with decreasing contrast due to
a decreasing signal-to-noise ratio. As illustrated in Figure 1, the shift in perceived speed
towards slow velocities grows with the width of the likelihood, and thus a Bayesian model
can qualitatively explain the psychophysical results [1].
1.2

Two Alternative Forced Choice Experiment

We would like to examine perceived speeds under a wide range of conditions in order to
constrain a Bayesian model. Unfortunately, perceived speed is an internal variable, and it is
not obvious how to design an experiment that would allow subjects to express it directly 1 .
Perceived speed can only be accessed indirectly by asking the subject to compare the speed
of two stimuli. For a given trial, an ideal Bayesian observer in such a two-alternative forced
choice (2AFC) experimental paradigm simply decides on the basis of the two trial estimates
v?1 (stimulus1) and v?2 (stimulus2) which stimulus moves faster. Each estimate v? is based
on a particular measurement m. For a given stimulus with speed v, an ideal Bayesian
observer will produce a distribution of estimates p(?
v |v) because m is noisy. Over trials,
the observers behavior can be described by classical signal detection theory based on the
distributions of the estimates, hence e.g. the probability of perceiving stimulus2 moving
1
Although see [10] for an example of determining and even changing the prior of a Bayesian
model for a sensorimotor task, where the estimates are more directly accessible.

faster than stimulus1 is given as the cumulative probability
 ?
 v?2
Pcum (?
v2 > v?1 ) =
p(?
v2 |v2 )
p(?
v1 |v1 ) d?
v1 d?
v2
0

(2)

0

Pcum describes the full psychometric curve. Figure 2b illustrates the measured psychometric curve and its fit from such an experimental situation.

2

Experimental Methods

We measured matching speeds (Pcum = 0.5) and thresholds (Pcum = 0.875) in a 2AFC
speed discrimination task. Subjects were presented simultaneously with two circular
patches of horizontally drifting sine-wave gratings for the duration of one second (Figure 2a). Patches were 3deg in diameter, and were displayed at 6deg eccentricity to either
side of a fixation cross. The stimuli had an identical spatial frequency of 1.5 cycle/deg. One
stimulus was considered to be the reference stimulus having one of two different contrast
values (c1 =[0.075 0.5]) and one of five different speed values (u1 =[1 2 4 8 12] deg/sec)
while the second stimulus (test) had one of five different contrast values (c2 =[0.05 0.1 0.2
0.4 0.8]) and a varying speed that was determined by an interleaved staircase procedure.
For each condition there were 96 trials. Conditions were randomly interleaved, including
a random choice of stimulus identity (test vs. reference) and motion direction (right vs.
left). Subjects were asked to fixate during stimulus presentation and select the faster moving stimulus. The threshold experiment differed only in that auditory feedback was given
to indicate the correctness of their decision. This did not change the outcome of the experiment but increased significantly the quality of the data and thus reduced the number of
trials needed.

3

Analysis

With the data from the speed discrimination experiments we could in principal apply a
parametric fit using Equation (2) to derive the prior and the likelihood, but the optimization
is difficult, and the fit might not be well constrained given the amount of data we have obtained. The problem becomes much more tractable given the following weak assumptions:
? We consider the prior to be relatively smooth.
? We assume that the measurement m is corrupted by additive Gaussian noise with
a variance whose dependence on stimulus speed and contrast is separable.
? We assume that there is a mapping function f (v) : v ? vm that maps v into the
space of m (m-space). In that space, the likelihood is convolutional i.e. the noise
in the measurement directly defines the width of the likelihood.
These assumptions allow us to relate the psychophysical data to our probabilistic model in
a simple way. The following analysis is in the m-space. The point of subjective equality
(Pcum = 0.5) is defined as where the expected values of the speed estimates are equal. We
write
E?
vm,1 
vm,1 ? E?1 

= E?
vm,2 
= vm,2 ? E?2 

(3)

where E? is the expected shift of the perceived speed compared to the veridical speed.
For the discrimination threshold experiment, above assumptions imply that the variance
var?
vm  of the speed estimates v?m is equal for both stimuli. Then, (2) predicts that the
discrimination threshold is proportional to the standard deviation, thus

vm,2 ? vm,1 = ? var?
vm 
(4)

likelihood

a
b

prior
vm

Figure 3: Piece-wise approximation We perform a parametric fit by assuming the prior to
be piece-wise linear and the likelihood to be LogNormal (Gaussian in the m-space).
where ? is a constant that depends on the threshold criterion Pcum and the exact shape of
p(?
vm |vm ).
3.1

Estimating the prior and likelihood

In order to extract the prior and the likelihood of our model from the data, we have to find
a generic local form of the prior and the likelihood and relate them to the mean and the
variance of the speed estimates. As illustrated in Figure 3, we assume that the likelihood is
Gaussian with a standard deviation ?(c, vm ). Furthermore, the prior is assumed to be wellapproximated by a first-order Taylor series expansion over the velocity ranges covered by
the likelihood. We parameterize this linear expansion of the prior as p(vm ) = avm + b.
We now can derive a posterior for this local approximation of likelihood and prior and then
define the perceived speed shift ?(m). The posterior can be written as
2
vm
1
1
p(m|vm )p(vm ) = [exp(?
)(avm + b)]
?
?
2?(c, vm )2
where ? is the normalization constant
 ?
b
p(m|vm )p(vm )dvm =
?2?(c, vm )2
?=
2
??

p(vm |m) =

(5)

(6)

We can compute ?(m) as the first order moment of the posterior for a given m. Exploiting
the symmetries around the origin, we find
 ?
a(m)
?(m) =
?(c, vm )2
vp(vm |m)dvm ?
(7)
b(m)
??
The expected value of ?(m) is equal to the value of ? at the expected value of the measurement m (which is the stimulus velocity vm ), thus
a(vm )
?(c, vm )2
E? = ?(m)|m=vm =
(8)
b(vm )
Similarly, we derive var?
vm . Because the estimator is deterministic, the variance of the
estimate only depends on the variance of the measurement m. For a given stimulus, the
variance of the estimate can be well approximated by
??
vm (m)
var?
vm  = varm(
|m=vm )2
(9)
?m
??(m)
|m=vm )2 ? varm
= varm(1 ?
?m

Under the assumption of a locally smooth prior, the perceived velocity shift remains locally
constant. The variance of the perceived speed v?m becomes equal to the variance of the
measurement m, which is the variance of the likelihood (in the m-space), thus
var?
vm  = ?(c, vm )2
(10)
With (3) and (4), above derivations provide a simple dependency of the psychophysical
data to the local parameters of the likelihood and the prior.
3.2

Choosing a Logarithmic speed representation

We now want to choose the appropriate mapping function f (v) that maps v to the m-space.
We define the m-space as the space in which the likelihood is Gaussian with a speedindependent width. We have shown that discrimination threshold is proportional to the
width of the likelihood (4), (10). Also, we know from the psychophysics literature that
visual speed discrimination approximately follows a Weber-Fechner law [11, 12], thus that
the discrimination threshold increases roughly proportional with speed and so would the
likelihood. A logarithmic speed representation would be compatible with the data and our
choice of the likelihood. Hence, we transform the linear speed-domain v into a normalized
logarithmic domain according to
v + v0
vm = f (v) = ln(
)
(11)
v0
where v0 is a small normalization constant. The normalization is chosen to account for
the expected deviation of equal variance behavior at the low end. Surprisingly, it has been
found that neurons in the Medial Temporal area (Area MT) of macaque monkeys have
speed-tuning curves that are very well approximated by Gaussians of constant width in
above normalized logarithmic space [13]. These neurons are known to play a central role
in the representation of motion. It seems natural to assume that they are strongly involved
in tasks such as our performed psychophysical experiments.

4

Results

Figure 4 shows the contrast dependent shift of speed perception and the speed discrimination threshold data for two subjects. Data points connected with a dashed line represent
the relative matching speed (v2 /v1 ) for a particular contrast value c2 of the test stimulus
as a function of the speed of the reference stimulus. Error bars are the empirical standard deviation of fits to bootstrapped samples of the data. Clearly, low contrast stimuli
are perceived to move slower. The effect, however, varies across the tested speed range
and tends to become smaller for higher speeds. The relative discrimination thresholds for
two different contrasts as a function of speed show that the Weber-Fechner law holds only
approximately. The data are in good agreement with other data from the psychophysics
literature [1, 11, 8].
For each subject, data from both experiments were used to compute a parametric leastsquares fit according to (3), (4), (7), and (10). In order to test the assumption of a LogNormal likelihood we allowed the standard deviation to be dependent on contrast and speed,
thus ?(c, vm ) = g(c)h(vm ). We split the speed range into six bins (subject2: five) and
parameterized h(vm ) and the ratio a/b accordingly. Similarly, we parameterized g(c) for
the seven contrast values. The resulting fits are superimposed as bold lines in Figure 4.
Figure 5 shows the fitted parametric values for g(c) and h(v) (plotted in the linear domain),
and the reconstructed prior distribution p(v) transformed back to the linear domain. The
approximately constant values for h(v) provide evidence that a LogNormal distribution
is an appropriate functional description of the likelihood. The resulting values for g(c)
suggest for the likelihood width a roughly exponential decaying dependency on contrast
with strong saturation for higher contrasts.

discrimination threshold (relative)

reference stimulus contrast c1:
0.075
0.5

subject 1
normalized matching speed

1.5

contrast c2

1

0.5

1

10

0.075
0.5

0.79

0.5
0.4
0.3
0.2
0.1
0

10

1

contrast:

1

10

discrimination threshold (relative)

normalized matching speed

subject 2

1.5

contrast c2

1

0.5

10

1

a

0.5
0.4
0.3
0.2
0.1

10

1

1

b

speed of reference stimulus [deg/sec]

10

stimulus speed [deg/sec]

Figure 4: Speed discrimination data for two subjects. a) The relative matching speed of
a test stimulus with different contrast levels (c2 =[0.05 0.1 0.2 0.4 0.8]) to achieve subjective equality with a reference stimulus (two different contrast values c1 ). b) The relative
discrimination threshold for two stimuli with equal contrast (c1,2 =[0.075 0.5]).
reconstructed prior

subject 1
p(v) [unnormalized]

1

Gaussian
Power-Law

g(c)

1

h(v)

2

0.9
1.5

0.8
0.1

n=-1.41

0.7
1
0.6

0.01

0.5

0.5

0.4
0.3
1

p(v) [unnormalized]

subject 2

10

0.1

1

1

1

1

10

1

10

2

0.9

n=-1.35

0.1

1.5

0.8
0.7

1
0.6
0.01

0.5

0.5

0.4
1

speed [deg/sec]

10

0.3

0
0.1

1

contrast

speed [deg/sec]

Figure 5: Reconstructed prior distribution and parameters of the likelihood function. The
reconstructed prior for both subjects show much heavier tails than a Gaussian (dashed fit),
approximately following a power-law function with exponent n ? ?1.4 (bold line).

5

Conclusions

We have proposed a probabilistic framework based on a Bayesian ideal observer and standard signal detection theory. We have derived a likelihood function and prior distribution
for the estimator, with a fairly conservative set of assumptions, constrained by psychophysical measurements of speed discrimination and matching. The width of the resulting likelihood is nearly constant in the logarithmic speed domain, and decreases approximately
exponentially with contrast. The prior expresses a preference for slower speeds, and approximately follows a power-law distribution, thus has much heavier tails than a Gaussian.
It would be interesting to compare the here derived prior distributions with measured true
distributions of local image velocities that impinge on the retina. Although a number of
authors have measured the spatio-temporal structure of natural images [14, e.g. ], it is
clearly difficult to extract therefrom the true prior distribution because of the feedback loop
formed through movements of the body, head and eyes.
Acknowledgments
The authors thank all subjects for their participation in the psychophysical experiments.

References
[1] P. Thompson. Perceived rate of movement depends on contrast. Vision Research, 22:377?380,
1982.
[2] L.S. Stone and P. Thompson. Human speed perception is contrast dependent. Vision Research,
32(8):1535?1549, 1992.
[3] A. Yuille and N. Grzywacz. A computational theory for the perception of coherent visual
motion. Nature, 333(5):71?74, May 1988.
[4] Alan Stocker. Constraint Optimization Networks for Visual Motion Perception - Analysis and
Synthesis. PhD thesis, Dept. of Physics, Swiss Federal Institute of Technology, Z?urich, Switzerland, March 2002.
[5] Eero Simoncelli. Distributed analysis and representation of visual motion. PhD thesis, MIT,
Dept. of Electrical Engineering, Cambridge, MA, 1993.
[6] Y. Weiss, E. Simoncelli, and E. Adelson. Motion illusions as optimal percept. Nature Neuroscience, 5(6):598?604, June 2002.
[7] D.M. Green and J.A. Swets. Signal Detection Theory and Psychophysics. Wiley, New York,
1966.
[8] F. H?urlimann, D. Kiper, and M. Carandini. Testing the Bayesian model of perceived speed.
Vision Research, 2002.
[9] Y. Weiss and D.J. Fleet. Probabilistic Models of the Brain, chapter Velocity Likelihoods in
Biological and Machine Vision, pages 77?96. Bradford, 2002.
[10] K. Koerding and D. Wolpert. Bayesian integration in sensorimotor learning.
427(15):244?247, January 2004.

Nature,

[11] Leslie Welch. The perception of moving plaids reveals two motion-processing stages. Nature,
337:734?736, 1989.
[12] S. McKee, G. Silvermann, and K. Nakayama. Precise velocity discrimintation despite random
variations in temporal frequency and contrast. Vision Research, 26(4):609?619, 1986.
[13] C.H. Anderson, H. Nover, and G.C. DeAngelis. Modeling the velocity tuning of macaque MT
neurons. Journal of Vision/VSS abstract, 2003.
[14] D.W. Dong and J.J. Atick. Statistics of natural time-varying images. Network: Computation in
Neural Systems, 6:345?358, 1995.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 2912-sensory-adaptation-within-a-bayesian-framework-for-perception.pdf

Sensory Adaptation within a Bayesian
Framework for Perception

Alan A. Stocker? and Eero P. Simoncelli
Howard Hughes Medical Institute and
Center for Neural Science
New York University

Abstract
We extend a previously developed Bayesian framework for perception
to account for sensory adaptation. We first note that the perceptual effects of adaptation seems inconsistent with an adjustment of the internally represented prior distribution. Instead, we postulate that adaptation
increases the signal-to-noise ratio of the measurements by adapting the
operational range of the measurement stage to the input range. We show
that this changes the likelihood function in such a way that the Bayesian
estimator model can account for reported perceptual behavior. In particular, we compare the model?s predictions to human motion discrimination
data and demonstrate that the model accounts for the commonly observed
perceptual adaptation effects of repulsion and enhanced discriminability.

1

Motivation

A growing number of studies support the notion that humans are nearly optimal when performing perceptual estimation tasks that require the combination of sensory observations
with a priori knowledge. The Bayesian formulation of these problems defines the optimal
strategy, and provides a principled yet simple computational framework for perception that
can account for a large number of known perceptual effects and illusions, as demonstrated
in sensorimotor learning [1], cue combination [2], or visual motion perception [3], just to
name a few of the many examples.
Adaptation is a fundamental phenomenon in sensory perception that seems to occur at all
processing levels and modalities. A variety of computational principles have been suggested as explanations for adaptation. Many of these are based on the concept of maximizing the sensory information an observer can obtain about a stimulus despite limited sensory
resources [4, 5, 6]. More mechanistically, adaptation can be interpreted as the attempt of
the sensory system to adjusts its (limited) dynamic range such that it is maximally informative with respect to the statistics of the stimulus. A typical example is observed in the
retina, which manages to encode light intensities that vary over nine orders of magnitude
using ganglion cells whose dynamic range covers only two orders of magnitude. This is
achieved by adapting to the local mean as well as higher order statistics of the visual input
over short time-scales [7].
?

corresponding author.

If a Bayesian framework is to provide a valid computational explanation of perceptual
processes, then it needs to account for the behavior of a perceptual system, regardless of
its adaptation state. In general, adaptation in a sensory estimation task seems to have two
fundamental effects on subsequent perception:
? Repulsion: The estimate of parameters of subsequent stimuli are repelled by
those of the adaptor stimulus, i.e. the perceived values for the stimulus variable
that is subject to the estimation task are more distant from the adaptor value after
adaptation. This repulsive effect has been reported for perception of visual speed
(e.g. [8, 9]), direction-of-motion [10], and orientation [11].
? Increased sensitivity: Adaptation increases the observer?s discrimination ability
around the adaptor (e.g. for visual speed [12, 13]), however it also seems to decrease it further away from the adaptor as shown in the case of direction-of-motion
discrimination [14].
In this paper, we show that these two perceptual effects can be explained within a Bayesian
estimation framework of perception. Note that our description is at an abstract functional
level - we do not attempt to provide a computational model for the underlying mechanisms
responsible for adaptation, and this clearly separates this paper from other work which
might seem at first glance similar [e.g., 15].

2

Adaptive Bayesian estimator framework

Suppose that an observer wants to estimate a property of a stimulus denoted by the variable
?, based on a measurement m. In general, the measurement can be vector-valued, and
is corrupted by both internal and external noise. Hence, combining the noisy information
gained by the measurement m with a priori knowledge about ? is advantageous. According
to Bayes? rule
1
p(?|m) = p(m|?)p(?) .
(1)
?
That is, the probability of stimulus value ? given m (posterior) is the product of the likelihood p(m|?) of the particular measurement and the prior p(?). The normalization constant
? serves to ensure that the posterior is a proper probability distribution. Under the assump?
tion of a squared-error loss function, the optimal estimate ?(m)
is the mean of the posterior,
thus
Z ?
?
?(m)
=
? p(?|m) d? .
(2)
0

?
Note that ?(m)
describes an estimate for a single measurement m. As discussed in [16],
the measurement will vary stochastically over the course of many exposures to the same
stimulus, and thus the estimator will also vary. We return to this issue in Section 3.2.
Figure 1a illustrates a Bayesian estimator, in which the shape of the (arbitrary) prior distribution leads on average to a shift of the estimate toward a lower value of ? than the true
stimulus value ?stim . The likelihood and the prior are the fundamental constituents of the
Bayesian estimator model. Our goal is to describe how adaptation alters these constituents
so as to account for the perceptual effects of repulsion and increased sensitivity.
Adaptation does not change the prior ...
An intuitively sensible hypothesis is that adaptation changes the prior distribution. Since
the prior is meant to reflect the knowledge the observer has about the distribution of occurrences of the variable ? in the world, repeated viewing of stimuli with the same parameter

a

b

probability

probability

attraction !

posterior
likelihood
prior

modified prior

??

?

?? '

?
?adapt

Figure 1: Hypothetical model in which adaptation alters the prior distribution. a) Unadapted Bayesian estimation configuration in which the prior leads to a shift of the estimate
? relative to the stimulus parameter ?stim . Both the likelihood function and the prior distri?,
bution contribute to the exact value of the estimate ?? (mean of the posterior). b) Adaptation
acts by increasing the prior distribution around the value, ?adapt , of the adapting stimulus
parameter. Consequently, an subsequent estimate ??0 of the same stimulus parameter value
?stim is attracted toward the adaptor. This is the opposite of observed perceptual effects,
and we thus conclude that adjustments of the prior in a Bayesian model do not account for
adaptation.

value ?adapt should presumably increase the prior probability in the vicinity of ?adapt . Figure 1b schematically illustrates the effect of such a change in the prior distribution. The
estimated (perceived) value of the parameter under the adapted condition is attracted to the
adapting parameter value. In order to account for observed perceptual repulsion effects,
the prior would have to decrease at the location of the adapting parameter, a behavior that
seems fundamentally inconsistent with the notion of a prior distribution.
... but increases the reliability of the measurements
Since a change in the prior distribution is not consistent with repulsion, we are led to the
conclusion that adaptation must change the likelihood function. But why, and how should
this occur?
In order to answer this question, we reconsider the functional purpose of adaptation. We assume that adaptation acts to allocate more resources to the representation of the parameter
values in the vicinity of the adaptor [4], resulting in a local increase in the signal-to-noise
ratio (SNR). This can be accomplished, for example, by dynamically adjusting the operational range to the statistics of the input. This kind of increased operational gain around
the adaptor has been effectively demonstrated in the process of retinal adaptation [17]. In
the context of our Bayesian estimator framework, and restricting to the simple case of a
scalar-valued measurement, adaptation results in a narrower conditional probability density p(m|?) in the immediate vicinity of the adaptor, thus an increase in the reliability of
the measurement m. This is offset by a broadening of the conditional probability density p(m|?) in the region beyond the adaptor vicinity (we assume that total resources are
conserved, and thus an increase around the adaptor must necessarily lead to a decrease
elsewhere).
Figure 2 illustrates the effect of this local increase in signal-to-noise ratio on the likeli-

unadapted

adapted

?adapt

p(m2| ? )'

1/SNR
?

?
?1

?2

?1

p(m2|?)

?2

?

m2

p(m1| ? )'

m1
m

m

p(m1|?)
?

?

?

?adapt
p(m| ?2)'

p(m|?2)

likelihoods

p(m|?1)
p(m| ?1)'
p(m|?adapt )'

conditionals

Figure 2: Measurement noise, conditionals and likelihoods. The two-dimensional conditional density, p(m|?), is shown as a grayscale image for both the unadapted and adapted
cases. We assume here that adaptation increases the reliability (SNR) of the measurement
around the parameter value of the adaptor. This is balanced by a decrease in SNR of the
measurement further away from the adaptor. Because the likelihood is a function of ? (horizontal slices, shown plotted at right), this results in an asymmetric change in the likelihood
that is in agreement with a repulsive effect on the estimate.

a

b

^

??

^

?? [deg]

+

0

60

30

0

-30

-

?
? adapt

-60
-180

-90

90

?adapt

180

? [deg]

Figure 3: Repulsion: Model predictions vs. human psychophysics. a) Difference in perceived direction in the pre- and post-adaptation condition, as predicted by the model. Postadaptive percepts of motion direction are repelled away from the direction of the adaptor.
b) Typical human subject data show a qualitatively similar repulsive effect. Data (and fit)
are replotted from [10].
hood function. The two gray-scale images represent the conditional probability densities,
p(m|?), in the unadapted and the adapted state. They are formed by assuming additive
noise on the measurement m of constant variance (unadapted) or with a variance that
decreases symmetrically in the vicinity of the adaptor parameter value ?adapt , and grows
slightly in the region beyond. In the unadapted state, the likelihood is convolutional and
the shape and variance are equivalent to the distribution of measurement noise. However,
in the adapted state, because the likelihood is a function of ? (horizontal slice through the
conditional surface) it is no longer convolutional around the adaptor. As a result, the mean
is pushed away from the adaptor, as illustrated in the two graphs on the right. Assuming
that the prior distribution is fairly smooth, this repulsion effect is transferred to the posterior
distribution, and thus to the estimate.

3

Simulation Results

We have qualitatively demonstrated that an increase in the measurement reliability around
the adaptor is consistent with the repulsive effects commonly seen as a result of perceptual adaptation. In this section, we simulate an adapted Bayesian observer by assuming a
simple model for the changes in signal-to-noise ratio due to adaptation. We address both
repulsion and changes in discrimination threshold. In particular, we compare our model
predictions with previously published data from psychophysical experiments examining
human perception of motion direction.
3.1

Repulsion

In the unadapted state, we assume the measurement noise to be additive and normally
distributed, and constant over the whole measurement space. Thus, assuming that m and
? live in the same space, the likelihood is a Gaussian of constant width. In the adapted
state, we assume a simple functional description for the variance of the measurement noise
around the adapter. Specifically, we use a constant plus a difference of two Gaussians,

a

b
relative discrimination threshold

relative discrimination threshold

1.8

1

?

?adapt

1.6
1.4
1.2
1
0.8
-40

-20

? adapt

20

40

? [deg]

Figure 4: Discrimination thresholds: Model predictions vs. human psychophysics. a) The
model predicts that thresholds for direction discrimination are reduced at the adaptor. It
also predicts two side-lobes of increased threshold at further distance from the adaptor.
b) Data of human psychophysics are in qualitative agreement with the model. Data are
replotted from [14] (see also [11]).
each having equal area, with one twice as broad as the other (see Fig. 2). Finally, for
simplicity, we assume a flat prior, but any reasonable smooth prior would lead to results
that are qualitatively similar. Then, according to (2) we compute the predicted estimate of
motion direction in both the unadapted and the adapted case.
Figure 3a shows the predicted difference between the pre- and post-adaptive average estimate of direction, as a function of the stimulus direction, ?stim . The adaptor is indicated with
an arrow. The repulsive effect is clearly visible. For comparison, Figure 3b shows human
subject data replotted from [10]. The perceived motion direction of a grating was estimated,
under both adapted and unadapted conditions, using a two-alternative-forced-choice experimental paradigm. The plot shows the change in perceived direction as a function of test
stimulus direction relative to that of the adaptor. Comparison of the two panels of Figure 3
indicate that despite the highly simplified construction of the model, the prediction is quite
good, and even includes the small but consistent repulsive effects observed 180 degrees
from the adaptor.
3.2

Changes in discrimination threshold

Adaptation also changes the ability of human observers to discriminate between the direction of two different moving stimuli. In order to model discrimination thresholds, we
need to consider a Bayesian framework that can account not only for the mean of the estimate but also its variability. We have recently developed such a framework, and used
it to quantitatively constrain the likelihood and the prior from psychophysical data [16].
This framework accounts for the effect of the measurement noise on the variability of the
? Specifically, it provides a characterization of the distribution p(?|?
? stim ) of the
estimate ?.
estimate for a given stimulus direction in terms of its expected value and its variance as a
function of the measurement noise. As in [16] we write
?
? stim i = varhmi( ? ?(m) )2 |m=? .
varh?|?
(3)
stim
?m
Assuming that discrimination threshold is proportional to the standard deviation,

q

? stim i, we can now predict how discrimination thresholds should change after adapvarh?|?
tation. Figure 4a shows the predicted change in discrimination thresholds relative to the unadapted condition for the same model parameters as in the repulsion example (Figure 3a).
Thresholds are slightly reduced at the adaptor, but increase symmetrically for directions
further away from the adaptor. For comparison, Figure 4b shows the relative change in discrimination thresholds for a typical human subject [14]. Again, the behavior of the human
observer is qualitatively well predicted.

4

Discussion

We have shown that adaptation can be incorporated into a Bayesian estimation framework
for human sensory perception. Adaptation seems unlikely to manifest itself as a change
in the internal representation of prior distributions, as this would lead to perceptual bias
effects that are opposite to those observed in human subjects. Instead, we argue that adaptation leads to an increase in reliability of the measurement in the vicinity of the adapting
stimulus parameter. We show that this change in the measurement reliability results in
changes of the likelihood function, and that an estimator that utilizes this likelihood function will exhibit the commonly-observed adaptation effects of repulsion and changes in
discrimination threshold. We further confirm our model by making quantitative predictions
and comparing them with known psychophysical data in the case of human perception of
motion direction.
Many open questions remain. The results demonstrated here indicate that a resource allocation explanation is consistent with the functional effects of adaptation, but it seems unlikely
that theory alone can lead to a unique quantitative prediction of the detailed form of these
effects. Specifically, the constraints imposed by biological implementation are likely to
play a role in determining the changes in measurement noise as a function of adaptor parameter value, and it will be important to characterize and interpret neural response changes
in the context of our framework. Also, although we have argued that changes in the prior
seem inconsistent with adaptation effects, it may be that such changes do occur but are
offset by the likelihood effect, or occur only on much longer timescales.
Last, if one considers sensory perception as the result of a cascade of successive processing
stages (with both feedforward and feedback connections), it becomes necessary to expand
the Bayesian description to describe this cascade [e.g., 18, 19]. For example, it may be
possible to interpret this cascade as a sequence of Bayesian estimators, in which the measurement of each stage consists of the estimate computed at the previous stage. Adaptation
could potentially occur in each of these processing stages, and it is of fundamental interest
to understand how such a cascade can perform useful stable computations despite the fact
that each of its elements is constantly readjusting its response properties.

References
[1] K. K?ording and D. Wolpert. Bayesian integration in sensorimotor learning.
427(15):244?247, January 2004.

Nature,

[2] D C Knill and W Richards, editors. Perception as Bayesian Inference. Cambridge University
Press, 1996.
[3] Y. Weiss, E. Simoncelli, and E. Adelson. Motion illusions as optimal percept. Nature Neuroscience, 5(6):598?604, June 2002.
[4] H.B. Barlow. Vision: Coding and Efficiency, chapter A theory about the functional role and
synaptic mechanism of visual after-effects, pages 363?375. Cambridge University Press., 1990.
[5] M.J. Wainwright. Visual adaptation as optimal information transmission. Vision Research,
39:3960?3974, 1999.

[6] N. Brenner, W. Bialek, and R. de Ruyter van Steveninck. Adaptive rescaling maximizes information transmission. Neuron, 26:695?702, June 2000.
[7] S.M. Smirnakis, M.J. Berry, D.K. Warland, W. Bialek, and M. Meister. Adaptation of retinal
processing to image contrast and spatial scale. Nature, 386:69?73, March 1997.
[8] P. Thompson. Velocity after-effects: the effects of adaptation to moving stimuli on the perception of subsequently seen moving stimuli. Vision Research, 21:337?345, 1980.
[9] A.T. Smith. Velocity coding: evidence from perceived velocity shifts. Vision Research,
25(12):1969?1976, 1985.
[10] P. Schrater and E. Simoncelli. Local velocity representation: evidence from motion adaptation.
Vision Research, 38:3899?3912, 1998.
[11] C.W. Clifford. Perceptual adaptation: motion parallels orientation. Trends in Cognitive Sciences, 6(3):136?143, March 2002.
[12] C. Clifford and P. Wenderoth. Adaptation to temporal modulaton can enhance differential speed
sensitivity. Vision Research, 39:4324?4332, 1999.
[13] A. Kristjansson. Increased sensitivity to speed changes during adaptation to first-order, but not
to second-order motion. Vision Research, 41:1825?1832, 2001.
[14] R.E. Phinney, C. Bowd, and R. Patterson. Direction-selective coding of stereoscopic (cyclopean) motion. Vision Research, 37(7):865?869, 1997.
[15] N.M. Grzywacz and R.M. Balboa. A Bayesian framework for sensory adaptation. Neural
Computation, 14:543?559, 2002.
[16] A.A. Stocker and E.P. Simoncelli. Constraining a Bayesian model of human visual speed perception. In Lawrence K. Saul, Yair Weiss, and L?eon Bottou, editors, Advances in Neural Information Processing Systems NIPS 17, pages 1361?1368, Cambridge, MA, 2005. MIT Press.
[17] D. Tranchina, J. Gordon, and R.M. Shapley. Retinal light adaptation ? evidence for a feedback
mechanism. Nature, 310:314?316, July 1984.
[18] S. Deneve. Bayesian inference in spiking neurons. In Lawrence K. Saul, Yair Weiss, and L e? on
Bottou, editors, Adv. Neural Information Processing Systems (NIPS*04), vol 17, Cambridge,
MA, 2005. MIT Press.
[19] R. Rao. Hierarchical Bayesian inference in networks of spiking neurons. In Lawrence K. Saul,
Yair Weiss, and L?eon Bottou, editors, Adv. Neural Information Processing Systems (NIPS*04),
vol 17, Cambridge, MA, 2005. MIT Press.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 2565-instance-specific-bayesian-model-averaging-for-classification.pdf

Instance-Specific Bayesian Model
Averaging f or Classification
Shyam Visweswaran
Center for Biomedical Informatics
Intelligent Systems Program
Pittsburgh, PA 15213
shyam@cbmi.pitt.edu

Gregory F. Cooper
Center for Biomedical Informatics
Intelligent Systems Program
Pittsburgh, PA 15213
gfc@cbmi.pitt.edu

Abstract
Classification algorithms typically induce population-wide models
that are trained to perform well on average on expected future
instances. We introduce a Bayesian framework for learning
instance-specific models from data that are optimized to predict
well for a particular instance. Based on this framework, we present
a lazy instance-specific algorithm called ISA that performs
selective model averaging over a restricted class of Bayesian
networks. On experimental evaluation, this algorithm shows
superior performance over model selection. We intend to apply
such instance-specific algorithms to improve the performance of
patient-specific predictive models induced from medical data.

1

In t ro d u c t i o n

Commonly used classification algorithms, such as neural networks, decision trees,
Bayesian networks and support vector machines, typically induce a single model
from a training set of instances, with the intent of applying it to all future instances.
We call such a model a population-wide model because it is intended to be applied
to an entire population of future instances. A population-wide model is optimized to
predict well on average when applied to expected future instances. In contrast, an
instance-specific model is one that is constructed specifically for a particular
instance. The structure and parameters of an instance-specific model are specialized
to the particular features of an instance, so that it is optimized to predict especially
well for that instance.
Usually, methods that induce population-wide models employ eager learning in
which the model is induced from the training data before the test instance is
encountered. In contrast, lazy learning defers most or all processing until a response
to a test instance is required. Learners that induce instance-specific models are
necessarily lazy in nature since they take advantage of the information in the test
instance. An example of a lazy instance-specific method is the lazy Bayesian rule
(LBR) learner, implemented by Zheng and Webb [1], which induces rules in a lazy
fashion from examples in the neighborhood of the test instance. A rule generated by
LBR consists of a conjunction of the attribute-value pairs present in the test instance

as the antecedent and a local simple (na?ve) Bayes classifier as the consequent. The
structure of the local simple Bayes classifier consists of the attribute of interest as
the parent of all other attributes that do not appear in the antecedent, and the
parameters of the classifier are estimated from the subset of training instances that
satisfy the antecedent. A greedy step-forward search selects the optimal LBR rule
for a test instance to be classified. When evaluated on 29 UCI datasets, LBR had the
lowest average error rate when compared to several eager learning methods [1].
Typically, both eager and lazy algorithms select a single model from some model
space, ignoring the uncertainty in model selection. Bayesian model averaging is a
coherent approach to dealing with the uncertainty in model selection, and it has
been shown to improve the predictive performance of classifiers [2]. However, since
the number of models in practically useful model spaces is enormous, exact model
averaging over the entire model space is usually not feasible. In this paper, we
describe a lazy instance-specific averaging (ISA) algorithm for classification that
approximates Bayesian model averaging in an instance-sensitive manner. ISA
extends LBR by adding Bayesian model averaging to an instance-specific model
selection algorithm.
While the ISA algorithm is currently able to directly handle only discrete variables
and is computationally more intensive than comparable eager algorithms, the results
in this paper show that it performs well. In medicine, such lazy instance-specific
algorithms can be applied to patient-specific modeling for improving the accuracy
of diagnosis, prognosis and risk assessment.
The rest of this paper is structured as follows. Section 2 introduces a Bayesian
framework for instance-specific learning. Section 3 describes the implementation of
ISA. In Section 4, we evaluate ISA and compare its performance to that of LBR.
Finally, in Section 5 we discuss the results of the comparison.

2 Deci si on Th eo ret i c F rame wo rk
We use the following notation. Capital letters like X, Z, denote random variables
and corresponding lower case letters, x, z, denote specific values assigned to them.
Thus, X = x denotes that variable X is assigned the value x. Bold upper case letters,
such as X, Z, represent sets of variables or random vectors and their realization is
denoted by the corresponding bold lower case letters, x, z. Hence, X = x denotes that
the variables in X have the states given by x. In addition, Z denotes the target
variable being predicted, X denotes the set of attribute variables, M denotes a model,
D denotes the training dataset, and <Xt , Zt> denotes a generic test instance that is
not in D.
We now characterize population-wide and instance-specific model selection in
decision theoretic terms. Given training data D and a separate generic test instance
<Xt, Zt>, the Bayes optimal prediction for Zt is obtained by combining the
predictions of all models weighted by their posterior probabilities, as follows:
P (Z t | X t , D ) = ? P( Z t | X t , M ) P ( M | D )dM .

(1)

M

The optimal population-wide model for predicting Zt is as follows:
?
?
max?? U P( Z t | X t , D), P (Z t | X t , M ) P ( X | D)? ,
M
? Xt
?

[

]

(2)

where the function U gives the utility of approximating the Bayes optimal estimate
P(Zt | Xt , D), with the estimate P(Zt | Xt , M) obtained from model M. The term
P(X | D) is given by:
P ( X | D) = ? P ( X | M ) P ( M | D)dM .

(3)

M

The optimal instance-specific model for predicting Zt is as follows:

{ [

]}

max U P ( Z t | X t = x t , D), P (Z t | X t = x t , M ) ,
M

(4)

where xt are the values of the attributes of the test instance Xt for which we want to
predict Zt. The Bayes optimal estimate P(Zt | Xt = xt, D), in Equation 4 is derived
using Equation 1, for the special case in which Xt = xt .
The difference between the population-wide and the instance-specific models can be
noted by comparing Equations 2 and 4. Equation 2 for the population-wide model
selects the model that on average will have the greatest utility. Equation 4 for the
instance-specific model, however, selects the model that will have the greatest
expected utility for the specific instance Xt = xt . For predicting Zt in a given instance
Xt = xt, the model selected using Equation 2 can never have an expected utility
greater than the model selected using Equation 4. This observation provides support
for developing instance-specific models.
Equations 2 and 4 represent theoretical ideals for population-wide and instancespecific model selection, respectively; we are not suggesting they are practical to
compute. The current paper focuses on model averaging, rather than model
selection. Ideal Bayesian model averaging is given by Equation 1. Model averaging
has previously been applied using population-wide models. Studies have shown that
approximate Bayesian model averaging using population-wide models can improve
predictive performance over population-wide model selection [2]. The current paper
concentrates on investigating the predictive performance of approximate Bayesian
model averaging using instance-specific models.

3 In st an ce- S p eci fi c Algo ri t h m
We present the implementation of the lazy instance-specific algorithm based on the
above framework. ISA searches the space of a restricted class of Bayesian networks
to select a subset of the models over which to derive a weighted (averaged)
posterior of the target variable Zt . A key characteristic of the search is the use of a
heuristic to select models that will have a significant influence on the weighted
posterior. We introduce Bayesian networks briefly and then describe ISA in detail.
3.1

B ay e s i a n N e t w or k s

A Bayesian network is a probabilistic model that combines a graphical
representation (the Bayesian network structure) with quantitative information (the
parameters of the Bayesian network) to represent the joint probability distribution
over a set of random variables [3]. Specifically, a Bayesian network M representing
the set of variables X consists of a pair (G, ?G ). G is a directed acyclic graph that
contains a node for every variable in X and an arc between every pair of nodes if the
corresponding variables are directly probabilistically dependent. Conversely, the
absence of an arc between a pair of nodes denotes probabilistic independence
between the corresponding variables. ?G represents the parameterization of the
model.

In a Bayesian network M, the immediate predecessors of a node X i in X are called
the parents of X i and the successors, both immediate and remote, of Xi in X are
called the descendants of X i . The immediate successors of X i are called the children
of X i . For each node Xi there is a local probability distribution (that may be discrete
or continuous) on that node given the state of its parents. The complete joint
probability distribution over X, represented by the parameterization ?G, can be
factored into a product of local probability distributions defined on each node in the
network. This factorization is determined by the independences captured by the
structure of the Bayesian network and is formalized in the Bayesian network
Markov condition: A node (representing a variable) is independent of its nondescendants given just its parents. According to this Markov condition, the joint
probability distribution on model variables X = (X1 , X 2, ?, X n ) can be factored as
follows:
n

P ( X 1 , X 2 , ..., X n ) = ? P ( X i | parents( X i )) ,

(5)

i =1

where parents(Xi ) denotes the set of nodes that are the parents of X i . If Xi has no
parents, then the set parents(Xi ) is empty and P(Xi | parents(X i)) is just P(Xi ).
3.2

I S A M od e l s

The LBR models of Zheng and Webb [1] can be represented as members of a
restricted class of Bayesian networks (see Figure 1). We use the same class of
Bayesian networks for the ISA models, to facilitate comparison between the two
algorithms. In Figure 1, all nodes represent attributes that are discrete. Each node in
X has either an outgoing arc into target node, Z, or receives an arc from Z. That is,
each node is either a parent or a child of Z. Thus, X is partitioned into two sets: the
first containing nodes (X 1 , ?, X j in Figure 1) each of which is a parent of Z and
every node in the second set, and the second containing nodes (X j+1 , ?, X k in Figure
1) that have as parents the node Z and every node in the first set. The nodes in the
first set are instantiated to the corresponding values in the test instance for which Zt
is to be predicted. Thus, the first set of nodes represents the antecedent of the LBR
rule and the second set of nodes represents the consequent.

...

X1= x1

Xi = xi

Z

Xi+1

...

Xk

Figure 1: An example of a Bayesian network LBR model with target
node Z and k attribute nodes of which X1 , ?, X j are instantiated to
values x 1 , ?, x j in xt . X 1, ?, X j are present in the antecedent of the LBR
rule and Z, X j+1 , ?, X k (that form the local simple Bayes classifier) are
present in the consequent. The indices need not be ordered as shown,
but are presented in this example for convenience of exposition.

3.3

M od e l A ve r ag i n g

For Bayesian networks, Equation 1 can be evaluated as follows:
P ( Z t | x t , D ) = ? P ( Z t | x t , M ) P( M | D ) ,

(6)

M

with M being a Bayesian network comprised of structure G and parameters ?G. The
probability distribution of interest is a weighted average of the posterior distribution
over all possible Bayesian networks where the weight is the probability of the
Bayesian network given the data. Since exhaustive enumeration of all possible
models is not feasible, even for this class of simple Bayesian networks, we
approximate exact model averaging with selective model averaging. Let R be the set
of models selected by the search procedure from all possible models in the model
space, as described in the next section. Then, with selective model averaging,
P(Zt | xt, D) is estimated as:
P( Z t | x t , M ) P ( M | D )
?
P (Z t | x t , D) ? M ?R
.
P (M | D)
?
M ?R

(7)

Assuming uniform prior belief over all possible models, the model posterior
P(M | D) in Equation 7 can be replaced by the marginal likelihood P(D | M), to
obtain the following equation:
P ( Z | x , D) ?
t

t

? P ( Z t | x t , M ) P( D | M )
.
P( D | M )
?
M ?R

M ?R

(8)

The (unconditional) marginal likelihood P(D | M) in Equation 8, is a measure of the
goodness of fit of the model to the data and is also known as the model score. While
this score is suitable for assessing the model?s fit to the joint probability
distribution, it is not necessarily appropriate for assessing the goodness of fit to a
conditional probability distribution which is the focus in prediction and
classification tasks, as is the case here. A more suitable score in this situation is a
conditional model score that is computed from training data D of d instances as:
d

score( D, M ) = ? P ( z p | x1 ,..., x p ,z 1 ,...,z p ?1 ,M ) .

(9)

p =1

This score is computed in a predictive and sequential fashion: for the pth training
instance the probability of predicting the observed value zp for the target variable is
computed based on the values of all the variables in the preceding p-1 training
instances and the values xp of the attributes in the pth instance. One limitation of this
score is that its value depends on the ordering of the data. Despite this limitation, it
has been shown to be an effective scoring criterion for classification models [4].
The parameters of the Bayesian network M, used in the above computations, are
defined as follows:
P ( X i = k | parents ( X i ) = j ) ? ? ijk =

N ijk + ? ijk
N ij + ? ij

,

(10)

where (i) Nijk is the number of instances in the training dataset D where variable Xi
has value k and the parents of X i are in state j, (ii) N ij = ?k N ijk , (iii) ?ijk is a

parameter prior that can be interpreted as the belief equivalent of having previously
observed ?ijk instances in which variable Xi has value k and the parents of X i are in
state j, and (iv) ? ij = ?k ? ijk .
3.4

M od e l Se a r c h

We use a two-phase best-first heuristic search to sample the model space. The first
phase ignores the evidence xt in the test instance while searching for models that
have high scores as given by Equation 9. This is followed by the second phase that
searches for models having the greatest impact on the prediction of Zt for the test
instance, which we formalize below.
The first phase searches for models that predict Z in the training data very well;
these are the models that have high conditional model scores. The initial model is
the simple Bayes network that includes all the attributes in X as children of Z. A
succeeding model is derived from a current model by reversing the arc of a child
node in the current model, adding new outgoing arcs from it to Z and the remaining
children, and instantiating this node to the value in the test instance. This process is
performed for each child in the current model. An incoming arc of a child node is
considered for reversal only if the node?s value is not missing in the test instance.
The newly derived models are added to a priority queue, Q. During each iteration of
the search, the model with the highest score (given by Equation 9) is removed from
Q and placed in a set R, following which new models are generated as described just
above, scored and added to Q. The first phase terminates after a user-specified
number of models have accumulated in R.
The second phase searches for models that change the current model-averaged
estimate of P(Zt | xt , D) the most. The idea here is to find viable competing models
for making this posterior probability prediction. When no competitive models can
be found, the prediction becomes stable. During each iteration of the search, the
highest ranked model M* is removed from Q and added to R. The ranking is based
on how much the model changes the current estimate of P(Zt | xt , D). More change is
better. In particular, M* is the model in Q that maximizes the following function:
f ( R, M *) = g ( R) ? g ( R U {M *}) ,

(11)

where for a set of models S, the function g(S) computes the approximate model
averaged prediction for Zt, as follows:
g (S ) =

? P(Z

M ?S

t

| x t , M ) score( D, M )

?? score( D, M )

.

(12)

M S

The second phase terminates when no new model can be found that has a value (as
given by Equation 11) that is greater than a user-specified minimum threshold T.
The final distribution of Zt is then computed from the models in R using Equation 8.

4

Ev a lu a t i o n

We evaluated ISA on the 29 UCI datasets that Zheng and Webb used for the
evaluation of LBR. On the same datasets, we also evaluated a simple Bayes
classifier (SB) and LBR. For SB and LBR, we used the Weka implementations
(Weka v3.3.6, http://www.cs.waikato.ac.nz/ml/weka/) with default settings [5]. We
implemented the ISA algorithm as a standalone application in Java. The following

settings were used for ISA: a maximum of 100 phase-1 models, a threshold T of
0.001 in phase-2, and an upper limit of 500 models in R. For the parameter priors in
Equation 10, all ?ijk were set to 1.
All error rates were obtained by averaging the results from two stratified 10-fold
cross-validation (20 trials total) similar to that used by Zheng and Webb. Since,
both LBR and ISA can handle only discrete attributes, all numeric attributes were
discretized in a pre-processing step using the entropy based discretization method
described in [6]. For each pair of training and test folds, the discretization intervals
were first estimated from the training fold and then applied to both folds. The error
rates of two algorithms on a dataset were compared with a paired t-test carried out at
the 5% significance level on the error rate statistics obtained from the 20 trials.
The results are shown in Table 1. Compared to SB, ISA has significantly fewer
errors on 9 datasets and significantly more errors on one dataset. Compared to LBR,
ISA has significantly fewer errors on 7 datasets and significantly more errors on two
datasets. On two datasets, chess and tic-tac-toe, ISA shows considerable
improvement in performance over both SB and LBR. With respect to computation
Table 1: Percent error rates of simple Bayes (SB), Lazy Bayesian Rule (LBR)
and Instance-Specific Averaging (ISA). A - indicates that the ISA error rate is
statistically significantly lower than the marked SB or LBR error rate. A +
indicates that the ISA error rate is statistically significantly higher.
Dataset

Size

Annealing
Audiology
Breast (W)
Chess (KR-KP)
Credit (A)
Echocardiogram
Glass
Heart (C)
Hepatitis
Horse colic
House votes 84
Hypothyroid
Iris
Labor
LED 24
Liver disorders
Lung cancer
Lymphography
Pima
Postoperative
Primary tumor
Promoters
Solar flare
Sonar
Soybean
Splice junction
Tic-Tac-Toe
Wine
Zoo

898
226
699
3169
690
131
214
303
155
368
435
3163
150
57
200
345
32
148
768
90
339
106
1389
208
683
3177
958
178
101

No. of
classes
6
24
2
2
2
2
6
2
2
2
2
2
3
2
10
2
3
4
2
3
22
2
2
2
19
3
2
3
7

Num.
Attrib.
6
0
9
0
6
6
9
13
6
7
0
7
4
8
0
6
0
0
8
1
0
0
0
60
0
0
0
13
0

Nom.
Attrib.
32
69
0
36
9
1
0
0
13
15
16
18
0
8
24
0
56
18
0
7
17
57
10
0
35
60
9
0
16

Percent error rate
SB
LBR
ISA
1.9
3.5 2.7 29.6
29.4
30.9
3.7
2.9 +
2.8 +
1.1
12.1 3.0 13.8
14.0
13.9
33.2
34.0
35.9
26.9
27.8
29.0
16.2
16.2
17.5
14.2 - 14.2 - 11.3
20.2
16.0
17.8
5.1
10.1 7.0 0.9
0.9
1.4 6.0
6.0
5.3
8.8
6.1
7.0
40.5
40.5
40.3
36.8
36.8
36.8
56.3
56.3
56.3
15.5 - 15.5 - 13.2
21.8
22.0
22.3
33.3
33.3
33.3
54.4
53.5
54.2
7.5
7.5
7.5
20.2
18.3 + 19.4
15.4
15.6
15.9
7.1
7.2
7.9 4.7
4.3
4.4
30.3 - 13.7 - 10.3
1.1
1.1
1.1
6.4
8.4 8.4 -

times, ISA took 6 times longer to run than LBR on average for a single test instance
on a desktop computer with a 2 GHz Pentium 4 processor and 3 GB of RAM.

5

C o n c lu si o n s a n d Fu t u re R e s ea rc h

We have introduced a Bayesian framework for instance-specific model averaging
and presented ISA as one example of a classification algorithm based on this
framework. An instance-specific algorithm like LBR that does model selection has
been shown by Zheng and Webb to perform classification better than several eager
algorithms [1]. Our results show that ISA, which extends LBR by adding Bayesian
model averaging, improves overall on LBR, which provides support that we can
obtain additional prediction improvement by performing instance-specific model
averaging rather than just instance-specific model selection.
In future work, we plan to explore further the behavior of ISA with respect to the
number of models being averaged and the effect of the number of models selected in
each of the two phases of the search. We will also investigate methods to improve
the computational efficiency of ISA. In addition, we plan to examine other
heuristics for model search as well as more general model spaces such as
unrestricted Bayesian networks.
The instance-specific framework is not restricted to the Bayesian network models
that we have used in this investigation. In the future, we plan to explore other
models using this framework. Our ultimate interest is to apply these instancespecific algorithms to improve patient-specific predictions (for diagnosis, therapy
selection, and prognosis) and thereby to improve patient care.
A c k n ow l e d g me n t s
This work was supported by the grant T15-LM/DE07059 from the National Library
of Medicine (NLM) to the University of Pittsburgh?s Biomedical Informatics
Training Program. We would like to thank the three anonymous reviewers for their
helpful comments.
References
[1] Zheng, Z. and Webb, G.I. (2000). Lazy Learning of Bayesian Rules. Machine Learning,
41(1):53-84.
[2] Hoeting, J.A., Madigan, D., Raftery, A.E. and Volinsky, C.T. (1999). Bayesian Model
Averaging: A Tutorial. Statistical Science, 14:382-417.
[3] Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann, San
Mateo, CA.
[4] Kontkanen, P., Myllymaki, P., Silander, T., and Tirri, H. (1999). On Supervised Selection
of Bayesian Networks. In Proceedings of the 15th International Conference on Uncertainty
in Artificial Intelligence, pages 334-342, Stockholm, Sweden. Morgan Kaufmann.
[5] Witten, I.H. and Frank, E. (2000). Data Mining: Practical Machine Learning Tools with
Java Implementations. Morgan Kaufmann, San Francisco, CA.
[6] Fayyad, U.M., and Irani, K.B. (1993). Multi-Interval Discretization of ContinuousValued Attributes for Classification Learning. In Proceedings of the Thirteenth International
Joint Conference on Artificial Intelligence, pages 1022-1027, San Mateo, CA. Morgan
Kaufmann.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 4489-efficient-coding-provides-a-direct-link-between-prior-and-likelihood-in-perceptual-bayesian-inference.pdf

Efficient coding provides a direct link between prior
and likelihood in perceptual Bayesian inference

Xue-Xin Wei and Alan A. Stocker?
Departments of Psychology and
Electrical and Systems Engineering
University of Pennsylvania
Philadelphia, PA-19104, U.S.A.

Abstract
A common challenge for Bayesian models of perception is the fact that the two
fundamental Bayesian components, the prior distribution and the likelihood function, are formally unconstrained. Here we argue that a neural system that emulates
Bayesian inference is naturally constrained by the way it represents sensory information in populations of neurons. More specifically, we show that an efficient
coding principle creates a direct link between prior and likelihood based on the
underlying stimulus distribution. The resulting Bayesian estimates can show biases away from the peaks of the prior distribution, a behavior seemingly at odds
with the traditional view of Bayesian estimation, yet one that has been reported
in human perception. We demonstrate that our framework correctly accounts for
the repulsive biases previously reported for the perception of visual orientation,
and show that the predicted tuning characteristics of the model neurons match
the reported orientation tuning properties of neurons in primary visual cortex.
Our results suggest that efficient coding is a promising hypothesis in constraining Bayesian models of perceptual inference.

1

Motivation

Human perception is not perfect. Biases have been observed in a large number of perceptual tasks
and modalities, of which the most salient ones constitute many well-known perceptual illusions. It
has been suggested, however, that these biases do not reflect a failure of perception but rather an observer?s attempt to optimally combine the inherently noisy and ambiguous sensory information with
appropriate prior knowledge about the world [13, 4, 14]. This hypothesis, which we will refer to as
the Bayesian hypothesis, has indeed proven quite successful in providing a normative explanation of
perception at a qualitative and, more recently, quantitative level (see e.g. [15]). A major challenge in
forming models based on the Bayesian hypothesis is the correct selection of two main components:
the prior distribution (belief) and the likelihood function. This has encouraged some to criticize the
Bayesian hypothesis altogether, claiming that arbitrary choices for these components always allow
for unjustified post-hoc explanations of the data [1].
We do not share this criticism, referring to a number of successful attempts to constrain prior beliefs
and likelihood functions based on principled grounds. For example, prior beliefs have been defined
as the relative distribution of the sensory variable in the environment in cases where these statistics
are relatively easy to measure (e.g. local visual orientations [16]), or where it can be assumed that
subjects have learned them over the course of the experiment (e.g. time perception [17]). Other
studies have constrained the likelihood function according to known noise characteristics of neurons
that are crucially involved in the specific perceptual process (e.g motion tuned neurons in visual cor?

http://www.sas.upenn.edu/ astocker/lab

1

world

neural representation
efficient
encoding

percept
Bayesian
decoding

Figure 1: Encoding-decoding framework. A stimulus representing a sensory variable ? elicits a firing
rate response R = {r1 , r2 , ..., rN } in a population of N neurons. The perceptual task is to generate a
?
good estimate ?(R)
of the presented value of the sensory variable based on this population response.
Our framework assumes that encoding is efficient, and decoding is Bayesian based on the likelihood
p(R|?), the prior p(?), and a squared-error loss function.
tex [18]). However, we agree that finding appropriate constraints is generally difficult and that prior
beliefs and likelihood functions have been often selected on the basis of mathematical convenience.
Here, we propose that the efficient coding hypothesis [19] offers a joint constraint on the prior and
likelihood function in neural implementations of Bayesian inference. Efficient coding provides a
normative description of how neurons encode sensory information, and suggests a direct link between measured perceptual discriminability, neural tuning characteristics, and environmental statistics [11]. We show how this link can be extended to a full Bayesian account of perception that
includes perceptual biases. We validate our model framework against behavioral as well as neural
data characterizing the perception of visual orientation. We demonstrate that we can account not
only for the reported perceptual biases away from the cardinal orientations, but also for the specific response characteristics of orientation-tuned neurons in primary visual cortex. Our work is a
novel proposal of how two important normative hypotheses in perception science, namely efficient
(en)coding and Bayesian decoding, might be linked.

2

Encoding-decoding framework

We consider perception as an inference process that takes place along the simplified neural encodingdecoding cascade illustrated in Fig. 11 .
2.1

Efficient encoding

Efficient encoding proposes that the tuning characteristics of a neural population are adapted to
the prior distribution p(?) of the sensory variable such that the population optimally represents the
sensory variable [19]. Different definitions of ?optimally? are possible, and may lead to different
results. Here, we assume an efficient representation that maximizes the mutual information between
the sensory variable and the population response. With this definition and an upper limit on the total
firing activity, the square-root of the Fisher Information must be proportional to the prior distribution [12, 21].
In order to constrain the tuning curves of individual neurons in the population we also impose a
homogeneity constraint, requiring that there exists a one-to-one mapping F (?) that transforms the
physical space with units ? to a homogeneous space with units ?? = F (?) in which the stimulus
distribution becomes uniform. This defines the mapping as
Z ?
F (?) =
p(?)d? ,
(1)
??

which is the cumulative of the prior distribution p(?). We then assume a neural population with identical tuning curves that evenly tiles the stimulus range in this homogeneous space. The population
provides an efficient representation of the sensory variable ? according to the above constraints [11].
? Fig. 2
The tuning curves in the physical space are obtained by applying the inverse mapping F ?1 (?).
1

In the context of this paper, we consider ?inferring?, ?decoding?, and ?estimating? as synonymous.

2

stimulus distribution

d

samples #

a

Fisher information

discriminability

and

average firing rates

and

b

firing rate [ Hz]

efficient encoding

F

likelihood function

F -1

likelihood

c

symmetric

asymmetric

homogeneous space

physical space

Figure 2: Efficient encoding constrains the likelihood function. a) Prior distribution p(?) derived
from stimulus statistics. b) Efficient coding defines the shape of the tuning curves in the physical
space by transforming a set of homogeneous neurons using a mapping F ?1 that is the inverse of
the cumulative of the prior p(?) (see Eq. (1)). c) As a result, the likelihood shape is constrained by
the prior distribution showing heavier tails on the side of lower prior density. d) Fisher information,
discrimination threshold, and average firing rates are all uniform in the homogeneous space.
illustrates the applied efficient encoding scheme, the mapping, and the concept of the homogeneous
space for the example of a symmetric, exponentially decaying prior distribution p(?). The key idea
here is that by assuming efficient encoding, the prior (i.e. the stimulus distribution in the world)
directly constrains the likelihood function. In particular, the shape of the likelihood is determined
by the cumulative distribution of the prior. As a result, the likelihood is generally asymmetric, as
shown in Fig. 2, exhibiting heavier tails on the side of the prior with lower density.
2.2

Bayesian decoding

Let us consider a population of N sensory neurons that efficiently represents a stimulus variable ?
as described above. A stimulus ?0 elicits a specific population response that is characterized by the
vector R = [r1 , r2 , ..., rN ] where ri is the spike-count of the ith neuron over a given time-window
? . Under the assumption that the variability in the individual firing rates is governed by a Poisson
process, we can write the likelihood function over ? as
p(R|?) =

N
Y
(? fi (?))ri

ri !

i=1

e?? fi (?) ,

(2)

with fi (?) describing the tuning curve of neuron i. We then define a Bayesian decoder ??LSE as
the estimator that minimizes the expected squared-error between the estimate and the true stimulus
value, thus
R
?p(R|?)p(?)d?
?
?LSE (R) = R
,
(3)
p(R|?)p(?)d?
where we use Bayes? rule to appropriately combine the sensory evidence with the stimulus prior
p(?).

3

Bayesian estimates can be biased away from prior peaks

Bayesian models of perception typically predict perceptual biases toward the peaks of the prior density, a characteristic often considered a hallmark of Bayesian inference. This originates from the
3

a

b

prior attraction

prior

prior attraction likelihood repulsion!

likelihood

c
prior

prior

repulsive bias

likelihood

likelihood mean

posterior mean

posterior mean

Figure 3: Bayesian estimates biased away from the prior. a) If the likelihood function is symmetric,
then the estimate (posterior mean) is, on average, shifted away from the actual value of the sensory
variable ?0 towards the prior peak. b) Efficient encoding typically leads to an asymmetric likelihood
function whose normalized mean is away from the peak of the prior (relative to ?0 ). The estimate
is determined by a combination of prior attraction and shifted likelihood mean, and can exhibit an
0
overall repulsive bias. c) If p(?0 )0 < 0 and the likelihood is relatively narrow, then (1/p(?)2 ) > 0
(blue line) and the estimate is biased away from the prior peak (see Eq. (6)).
common approach of choosing a parametric description of the likelihood function that is computationally convenient (e.g. Gaussian). As a consequence, likelihood functions are typically assumed to
be symmetric (but see [23, 24]), leaving the bias of the Bayesian estimator to be mainly determined
by the shape of the prior density, i.e. leading to biases toward the peak of the prior (Fig. 3a).
In our model framework, the shape of the likelihood function is constrained by the stimulus prior
via efficient neural encoding, and is generally not symmetric for non-flat priors. It has a heavier tail
on the side with lower prior density (Fig. 3b). The intuition is that due to the efficient allocation
of neural resources, the side with smaller prior density will be encoded less accurately, leading to a
broader likelihood function on that side. The likelihood asymmetry pulls the Bayes? least-squares
estimate away from the peak of the prior while at the same time the prior pulls it toward its peak.
Thus, the resulting estimation bias is the combination of these two counter-acting forces - and both
are determined by the prior!
3.1

General derivation of the estimation bias

In the following, we will formally derive the mean estimation bias b(?) of the proposed encodingdecoding framework. Specifically, we will study the conditions for which the bias is repulsive i.e.
away from the peak of the prior density.
We first re-write the estimator ??LSE (3) by replacing ? with the inverse of its mapping to the homo? The motivation for this is that the likelihood in the homogeneous
geneous space, i.e., ? = F ?1 (?).
space is symmetric (Fig. 2). Given a value ?0 and the elicited population response R, we can write
the estimator as
R
R ?1
?1 ?
?1 ?
?
?
?p(R|?)p(?)d?
F (?)p(R|F
(?))p(F ?1 (?))dF
(?)
?
R
?LSE (R) =
=
.
R
?1
?1
?1
?
?
?
p(R|?)p(?)d?
p(R|F (?))p(F (?))dF (?)
Calculating the derivative of the inverse function and noting that F is the cumulative of the prior
density, we get
1
?
? = (F ?1 (?))
? 0 d?? = 1 d?? = 1 d?? =
d?.
dF ?1 (?)
?1
?
F (?)0
p(?)
p(F (?))
Hence, we can simplify ??LSE (R) as
??LSE (R) =

R

?1 ?
?
F ?1 (?)p(R|F
(?))d??
.
R
? ??
p(R|F ?1 (?))d

With
? =R
K(R, ?)

?
p(R|F ?1 (?))
? ??
p(R|F ?1 (?))d
4

we can further simplify the notation and get
??LSE (R) =

Z

?
? ?? .
F ?1 (?)K(R,
?)d

(4)

? we marginalize (4) over the population
In order to get the expected value of the estimate, ??LSE (?),
response space S,
Z Z
? =
?
? ?dR
?
??LSE (?)
p(R)F ?1 (?)K(R,
?)d
S

Z
=

F

?1

?
(?)(

Z

?
p(R)K(R, ?)dR)d
?? =

Z

? ?)d
? ?,
?
F ?1 (?)L(

S

where we define
? =
L(?)

Z

?
p(R)K(R, ?)dR.

S

R
? ?? = 1. Due to the symmetry in this space, it can be shown that L(?)
? is
It follows that L(?)d
?
?
symmetric around the true stimulus value ?0 . Intuitively, L(?) can be thought as the normalized
average likelihood in the homogeneous space. We can then compute the expected bias at ?0 as
Z
? ?)d
? ?? ? F ?1 (??0 )
b(?0 ) = F ?1 (?)L(
(5)
? is defined as the inverse of the cumulative of an arbitrary
This is expression is general where F ?1 (?)
? is determined by the internal noise level.
prior density p(?) (see Eq. (1)) and the dispersion of L(?)
Assuming the prior density to be smooth, we expand F ?1 in a neighborhood (??0 ? h, ??0 + h) that
is larger than the support of the likelihood function. Using Taylor?s theorem with mean-value forms
of the remainder, we get
? = F ?1 (??0 ) + F ?1 (??0 )0 (?? ? ??0 ) + 1 F ?1 (??x )00 (?? ? ??0 )2 ,
F ?1 (?)
2
?
?
?
with ?x lying between ?0 and ?. By applying this expression to (5), we find
??0 +h

Z
b(?0 ) =

=

1
2

??0 ?h

Z

1 ?1 ? 00 ? ? 2 ? ? 1
F (?x )?? (? ? ?0 ) L(?)d? =
2
2

??0 +h

?(
??0 ?h

p(?x )0? ? ? 2 ? ? 1
)(? ? ?0 ) L(?)d? =
p(?x )3
4

??0 +h

Z

(

1
? ??
)0??(?? ? ??0 )2 L(?)d
?1
?
p(F (?x ))

(

1
? ?.
?
)0 (?? ? ??0 )2 L(?)d
p(?x )2 ?

??0 ?h

Z

??0 +h

??0 ?h

In general, there is no simple rule to judge the sign of b(?0 ). However, if the prior is monotonic
on the interval F ?1 ((??0 ? h, ??0 + h)), then the sign of ( p(?1x )2 )0 is always the same as the sign of
( p(?10 )2 )0 . Also, if the likelihood is sufficiently narrow we can approximate ( p(?1x )2 )0 by ( p(?10 )2 )0 ,
and therefore approximate the bias as
b(?0 ) ? C(

1
)0 ,
p(?0 )2

(6)

where C is a positive constant.
The result is quite surprising because it states that as long as the prior is monotonic over the support
of the likelihood function, the expected estimation bias is always away from the peaks of the prior!
3.2

Internal (neural) versus external (stimulus) noise

The above derivation of estimation bias is based on the assumption that all uncertainty about the
sensory variable is caused by neural response variability. This level of internal noise depends on the
response magnitude, and thus can be modulated e.g. by changing stimulus contrast. This contrastcontrolled noise modulation is commonly exploited in perceptual studies (e.g. [18]). Internal noise
will always lead to repulsive biases in our framework if the prior is monotonic. If internal noise is
low, the likelihood is narrow and thus the bias is small. Increasing internal noise leads to increasingly
5

larger biases up to the point where the likelihood becomes wide enough such that monotonicity of
the prior over the support of the likelihood is potentially violated.
Stimulus noise is another way to modulate the noise level in perception (e.g. random-dot motion
stimuli). Such external noise, however, has a different effect on the shape of the likelihood function
as compared to internal noise. It modifies the likelihood function (2) by convolving it with the noise
kernel. External noise is frequently chosen as additive and symmetric (e.g. zero-mean Gaussian). It
is straightforward to prove that such symmetric external noise does not lead to a change in the mean
of the likelihood, and thus does not alter the repulsive effect induced by its asymmetry. However, by
increasing the overall width of the likelihood, the attractive influence of the prior increases, resulting
in an estimate that is closer to the prior peak than without external noise2 .

4

Perception of visual orientation

We tested our framework by modelling the perception of visual orientation. Our choice was based
on the fact that i) we have pretty good estimates of the prior distribution of local orientations in
natural images, ii) tuning characteristics of orientation selective neurons in visual cortex are wellstudied (monkey/cat), and iii) biases in perceived stimulus orientation have been well characterized.
We start by creating an efficient neural population based on measured prior distributions of local
visual orientation, and then compare the resulting tuning characteristics of the population and the
predicted perceptual biases with reported data in the literature.
4.1

Efficient neural model population for visual orientation

Previous studies measured the statistics of the local orientation in large sets of natural images and
consistently found that the orientation distribution is multimodal, peaking at the two cardinal orientations as shown in Fig. 4a [16, 20]. We assumed that the visual system?s prior belief over orientation
p(?) follows this distribution and approximate it formally as
p(?) ? 2 ? | sin(?)| (black line in Fig. 4b) .

(7)

Based on this prior distribution we defined an efficient neural representation for orientation. We
assumed a population of model neurons (N = 30) with tuning curves that follow a von-Mises
distribution in the homogeneous space on top of a constant spontaneous firing rate (5 Hz). We then
? to all these tuning curves to get the corresponding tuning
applied the inverse transformation F ?1 (?)
curves in the physical space (Fig. 4b - red curves), where F (?) is the cumulative of the prior (7). The
concentration parameter for the von-Mises tuning curves was set to ? ? 1.6 in the homogeneous
space in order to match the measured average tuning width (? 32 deg) of neurons in area V1 of the
macaque [9].
4.2

Predicted tuning characteristics of neurons in primary visual cortex

The orientation tuning characteristics of our model population well match neurophysiological data
of neurons in primary visual cortex (V1). Efficient encoding predicts that the distribution of neurons?
preferred orientation follows the prior, with more neurons tuned to cardinal than oblique orientations
by a factor of approximately 1.5. A similar ratio has been found for neurons in area V1 of monkey/cat [9, 10]. Also, the tuning widths of the model neurons vary between 25-42 deg depending
on their preferred tuning (see Fig. 4c), matching the measured tuning width ratio of 0.6 between
neurons tuned to the cardinal versus oblique orientations [9].
An important prediction of our model is that most of the tuning curves should be asymmetric. Such
asymmetries have indeed been reported for the orientation tuning of neurons in area V1 [6, 7, 8].
We computed the asymmetry index for our model population as defined in previous studies [6, 7],
and plotted it as a function of the preferred tuning of each neuron (Fig. 4d). The overall asymmetry
index in our model population is 1.24 ? 0.11, which approximately matches the measured values for
neurons in area V1 of the cat (1.26 ? 0.06) [6]. It also predicts that neurons tuned to the cardinal and
oblique orientations should show less symmetry than those tuned to orientations in between. Finally,
2

Note, that these predictions are likely to change if the external noise is not symmetric.

6

a

b

25

firing rate(Hz)

0

orientation(deg)

asymmetry vs. tuning width

1.0

2.0

90

2.0

e

asymmetry

1.0

0

asymmetry index

50
30

width (deg)

10

90

preferred tuning(deg)

-90

0

d

0

0

90

asymmetry index

0

orientation(deg)

tuning width

-90

0

0

probability

0
-90

c

efficient representation
0.01

0.01

image statistics

-90

0

90

preferred tuning(deg)

25

30

35

40

tuning width (deg)

Figure 4: Tuning characteristics of model neurons. a) Distribution of local orientations in natural
images, replotted from [16]. b) Prior used in the model (black) and predicted tuning curves according
to efficient coding (red). c) Tuning width as a function of preferred orientation. d) Tuning curves
of cardinal and oblique neurons are more symmetric than those tuned to orientations in between. e)
Both narrowly and broadly tuned neurons neurons show less asymmetry than neurons with tuning
widths in between.
neurons with tuning widths at the lower and upper end of the range are predicted to exhibit less
asymmetry than those neurons whose widths lie in between these extremes (illustrated in Fig. 4e).
These last two predictions have not been tested yet.
4.3

Predicted perceptual biases

Our model framework also provides specific predictions for the expected perceptual biases. Humans
show systematic biases in perceived orientation of visual stimuli such as e.g. arrays of Gabor patches
(Fig. 5a,d). Two types of biases can be distinguished: First, perceived orientations show an absolute
bias away from the cardinal orientations, thus away from the peaks of the orientation prior [2, 3].
We refer to these biases as absolute because they are typically measured by adjusting a noise-free
reference until it matched the orientation of the test stimulus. Interestingly, these repulsive absolute
biases are the larger the smaller the external stimulus noise is (see Fig. 5b). Second, the relative bias
between the perceived overall orientations of a high-noise and a low-noise stimulus is toward the
cardinal orientations as shown in Fig. 5c, and thus toward the peak of the prior distribution [3, 16].
The predicted perceptual biases of our model are shown Fig. 5e,f. We computed the likelihood
function according to (2) and used the prior in (7). External noise was modeled by convolving
the stimulus likelihood function with a Gaussian (different widths for different noise levels). The
predictions well match both, the reported absolute bias away as well as the relative biases toward
the cardinal orientations. Note, that our model framework correctly accounts for the fact that less
external noise leads to larger absolute biases (see also discussion in section 3.2).

5

Discussion

We have presented a modeling framework for perception that combines efficient (en)coding and
Bayesian decoding. Efficient coding imposes constraints on the tuning characteristics of a population of neurons according to the stimulus distribution (prior). It thus establishes a direct link
between prior and likelihood, and provides clear constraints on the latter for a Bayesian observer
model of perception. We have shown that the resulting likelihoods are in general asymmetric, with
7

absolute bias (data)

b

c

relative bias (data)

-4

0

bias(deg)

4

a

low-noise stimulus

-90

e

90

absolute bias (model)

low external noise
high external noise

3

high-noise stimulus

-90

f

0

90

relative bias (model)

0

bias(deg)

d

0

attraction

-3

repulsion

-90

0

orientation (deg)

90

-90

0

orientation (deg)

90

Figure 5: Biases in perceived orientation: Human data vs. Model prediction. a,d) Low- and highnoise orientation stimuli of the type used in [3, 16]. b) Humans show absolute biases in perceived
orientation that are away from the cardinal orientations. Data replotted from [2] (pink squares)
and [3] (green (black) triangles: bias for low (high) external noise). c) Relative bias between stimuli
with different external noise level (high minus low). Data replotted from [3] (blue triangles) and [16]
(red circles). e,f) Model predictions for absolute and relative bias.

heavier tails away from the prior peaks. We demonstrated that such asymmetric likelihoods can lead
to the counter-intuitive prediction that a Bayesian estimator is biased away from the peaks of the
prior distribution. Interestingly, such repulsive biases have been reported for human perception of
visual orientation, yet a principled and consistent explanation of their existence has been missing so
far. Here, we suggest that these counter-intuitive biases directly follow from the asymmetries in the
likelihood function induced by efficient neural encoding of the stimulus. The good match between
our model predictions and the measured perceptual biases and orientation tuning characteristics of
neurons in primary visual cortex provides further support of our framework.
Previous work has suggested that there might be a link between stimulus statistics, neuronal tuning characteristics, and perceptual behavior based on efficient coding principles, yet none of these
studies has recognized the importance of the resulting likelihood asymmetries [16, 11]. We have
demonstrated here that such asymmetries can be crucial in explaining perceptual data, even though
the resulting estimates appear ?anti-Bayesian? at first sight (see also models of sensory adaptation [23]).
Note, that we do not provide a neural implementation of the Bayesian inference step. However,
we and others have proposed various neural decoding schemes that can approximate Bayes? leastsquares estimation using efficient coding [26, 25, 22]. It is also worth pointing out that our estimator
is set to minimize total squared-error, and that other choices of the loss function (e.g. MAP estimator) could lead to different predictions. Our framework is general and should be directly applicable
to other modalities. In particular, it might provide a new explanation for perceptual biases that are
hard to reconcile with traditional Bayesian approaches [5].
Acknowledgments
We thank M. Jogan and A. Tank for helpful comments on the manuscript. This work was partially
supported by grant ONR N000141110744.
8

References
[1] M. Jones, and B. C. Love. Bayesian fundamentalism or enlightenment? On the explanatory status and theoretical contributions of Bayesian models of cognition. Behavioral and Brain Sciences, 34, 169?231,2011.
[2] D. P. Andrews. Perception of contours in the central fovea. Nature, 205:1218- 1220, 1965.
[3] A. Tomassini, M. J.Morgam. and J. A. Solomon. Orientation uncertainty reduces perceived obliquity.
Vision Res, 50, 541?547, 2010.
[4] W. S. Geisler, D. Kersten. Illusions, perception and Bayes. Nature Neuroscience, 5(6):508- 510, 2002.
[5] M. O. Ernst Perceptual learning: inverting the size-weight illusion. Current Biology, 19:R23- R25, 2009.
[6] G. H. Henry, B. Dreher, P. O. Bishop. Orientation specificity of cells in cat striate cortex. J Neurophysiol,
37(6):1394-409,1974.
[7] D. Rose, C. Blakemore An analysis of orientation selectivity in the cat?s visual cortex. Exp Brain Res.,
Apr 30;20(1):1-17, 1974.
[8] N. V. Swindale. Orientation tuning curves: empirical description and estimation of parameters. Biol
Cybern., 78(1):45-56, 1998.
[9] R. L. De Valois, E. W. Yund, N. Hepler. The orientation and direction selectivity of cells in macaque visual
cortex. Vision Res.,22, 531544,1982.
[10] B. Li, M. R. Peterson, R. D. Freeman. The oblique effect: a neural basis in the visual cortex. J. Neurophysiol., 90, 204217, 2003.
[11] D. Ganguli and E.P. Simoncelli. Implicit encoding of prior probabilities in optimal neural populations. In
Adv. Neural Information Processing Systems NIPS 23, vol. 23:658?666, 2011.
[12] M. D. McDonnell, N. G. Stocks. Maximally Informative Stimuli and Tuning Curves for Sigmoidal RateCoding Neurons and Populations. Phys Rev Lett., 101(5):058103, 2008.
[13] H Helmholtz. Treatise on Physiological Optics (transl.). Thoemmes Press, Bristol, U.K., 2000. Original
publication 1867.
[14] Y. Weiss, E. Simoncelli, and E. Adelson. Motion illusions as optimal percept. Nature Neuroscience,
5(6):598?604, June 2002.
[15] D.C. Knill and W. Richards, editors. Perception as Bayesian Inference. Cambridge University Press,
1996.
[16] A R Girshick, M S Landy, and E P Simoncelli. Cardinal rules: visual orientation perception reflects
knowledge of environmental statistics. Nat Neurosci, 14(7):926?932, Jul 2011.
[17] M. Jazayeri and M.N. Shadlen. Temporal context calibrates interval timing. Nature Neuroscience,
13(8):914?916, 2010.
[18] A.A. Stocker and E.P. Simoncelli. Noise characteristics and prior expectations in human visual speed
perception. Nature Neuroscience, pages 578?585, April 2006.
[19] H.B. Barlow. Possible principles underlying the transformation of sensory messages. In W.A. Rosenblith,
editor, Sensory Communication, pages 217?234. MIT Press, Cambridge, MA, 1961.
[20] D.M. Coppola, H.R. Purves, A.N. McCoy, and D. Purves The distribution of oriented contours in the real
world. Proc Natl Acad Sci U S A., 95(7): 4002?4006, 1998.
[21] N. Brunel and J.-P. Nadal. Mutual information, Fisher information and population coding. Neural Computation, 10, 7, 1731?1757, 1998.
[22] X-X. Wei and A.A. Stocker. Bayesian inference with efficient neural population codes. In Lecture Notes
in Computer Science, Artificial Neural Networks and Machine Learning - ICANN 2012, Lausanne, Switzerland, volume 7552, pages 523?530, 2012.
[23] A.A. Stocker and E.P. Simoncelli. Sensory adaptation within a Bayesian framework for perception. In
Y. Weiss, B. Sch?olkopf, and J. Platt, editors, Advances in Neural Information Processing Systems 18, pages
1291?1298. MIT Press, Cambridge, MA, 2006. Oral presentation.
[24] D.C. Knill. Robust cue integration: A Bayesian model and evidence from cue-conflict studies with stereoscopic and figure cues to slant. Journal of Vision, 7(7):1?24, 2007.
[25] Deep Ganguli. Efficient coding and Bayesian inference with neural populations. PhD thesis, Center for
Neural Science, New York University, New York, NY, September 2012.
[26] B. Fischer. Bayesian estimates from heterogeneous population codes. In Proc. IEEE Intl. Joint Conf. on
Neural Networks. IEEE, 2010.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 2267-data-dependent-bounds-for-bayesian-mixture-methods.pdf

Data-Dependent Bounds for Bayesian
Mixture Methods
Ron Meir
Department of Electrical Engineering
Technion, Haifa 32000, Israel
rmeir@ee.technion.ac.il

Tong Zhang
IBM T.J. Watson Research Center
Yorktown Heights, NY 10598, USA
tzhang@watson.ibm.com

Abstract
We consider Bayesian mixture approaches, where a predictor is
constructed by forming a weighted average of hypotheses from some
space of functions. While such procedures are known to lead to
optimal predictors in several cases, where sufficiently accurate prior
information is available, it has not been clear how they perform
when some of the prior assumptions are violated. In this paper we
establish data-dependent bounds for such procedures, extending
previous randomized approaches such as the Gibbs algorithm to
a fully Bayesian setting. The finite-sample guarantees established
in this work enable the utilization of Bayesian mixture approaches
in agnostic settings, where the usual assumptions of the Bayesian
paradigm fail to hold. Moreover, the bounds derived can be directly
applied to non-Bayesian mixture approaches such as Bagging and
Boosting.

1

Introduction and Motivation

The standard approach to Computational Learning Theory is usually formulated
within the so-called frequentist approach to Statistics. Within this paradigm one is
interested in constructing an estimator, based on a finite sample, which possesses a
small loss (generalization error). While many algorithms have been constructed and
analyzed within this context, it is not clear how these approaches relate to standard
optimality criteria within the frequentist framework. Two classic optimality criteria
within the latter approach are the minimax and admissibility criteria, which characterize optimality of estimators in a rigorous and precise fashion [9]. Except in some
special cases [12], it is not known whether any of the approaches used within the
Learning community lead to optimality in either of the above senses of the word.
On the other hand, it is known that under certain regularity conditions, Bayesian
estimators lead to either minimax or admissible estimators, and thus to well-defined
optimality in the classical (frequentist) sense. In fact, it can be shown that Bayes
estimators are essentially the only estimators which can achieve optimality in the
above senses [9]. This optimality feature provides strong motivation for the study
of Bayesian approaches in a frequentist setting.
While Bayesian approaches have been widely studied, there have not been generally

applicable bounds in the frequentist framework. Recently, several approaches have
attempted to address this problem. In this paper we establish finite sample datadependent bounds for Bayesian mixture methods, which together with the above
optimality properties suggest that these approaches should become more widely
used.
Consider the problem of supervised learning where we attempt to construct an estimator based on a finite sample of pairs of examples S = {(x1 , y1 ), . . . , (xn , yn )},
each drawn independently according to an unknown distribution ?(x, y). Let A be
a learning algorithm which, based on the sample S, constructs a hypothesis (estimator) h from some set of hypotheses H. Denoting by `(y, h(x)) the instantaneous
loss of the hypothesis h, we wish to assess the true loss L(h) = E? `(y, h(x)) where
the expectation is taken with respect to ?. In particular, the objective is to provide
data-dependent bounds of the following form. For any h ? H and ? ? (0, 1), with
probability at least 1 ? ?,
L(h) ? ?(h, S) + ?(h, S, ?),

(1)

where ?(h, S) is some empirical assessment of the true loss, and ?(h, S, ?) is a complexity term. For example, inPthe classic Vapnik-Chervonenkis framework, ?(h, S)
n
is the empirical error (1/n) i=1 `(yi , h(xi )) and ?(h, S, ?) depends on the VCdimension of H but is independent of both the hypothesis h and the sample S. By
algorithm and data-dependent bounds we mean bounds where the complexity term
depends on both the hypothesis (chosen by the algorithm A) and the sample S.

2

A Decision Theoretic Bayesian Framework

Consider a decision theoretic setting where we define the sample dependent loss of
an algorithm A by R(?, A, S) = E? `(y, A(x, S)). Let ?? be the optimal predictor
for y, namely the function minimizing E? {`(y, ?(x))} over ?. It is clear that the
best algorithm A (Bayes algorithm) is the one that always return ?? , assuming ?
is known. We are interested in the expected loss of an algorithm averaged over
samples S:
Z
R(?, A) = ES R(?, A, S) = R(?, A, S)d?(S),
where the expectation is taken with respect to the sample S drawn i.i.d. from the
probability measure ?. If we consider a family of measures ?, which possesses some
underlying prior distribution ?(?), then we can construct the averaged risk function
with respect to the prior as,
Z
Z
r(?, A) = E? R(?, A) = d?(S)d?(?) R(?, A, S)d?(?|S),
where d?(?|S) =

R d?(S)d?(?)
d?(S)d?(?)
?

is the posterior distribution on the ? family, which

induces a posterior distribution on the sample space as ?S = E?(?|S) ?. An algorithm
minimizing the Bayes risk r(?, A) is referred to as a Bayes algorithm. In fact, for a
given prior, and a given sample S, the optimal algorithm should return the Bayes
optimal predictor with respect to the posterior measure ?S .
For many important practical problems, the optimal Bayes predictor is a linear
functional of the underlying probability measure. For example, if the loss function is
quadratic, namely `(y, A(x)) = (y ?A(x))2 , then the optimal Bayes predictor ?? (x)
is the conditional mean of y, namely E? [y|x]. For binary classification problems, we
can let the predictor be the conditional probability ?? (x) = ?(y = 1|x) (the optimal
classification decision rule then corresponds to a test of whether ?? (x) > 0.5), which

is also a linear functional of ?. Clearly if the Bayes predictor is a linear functional
of the probability measure, then the optimal Bayes algorithm with respect to the
prior ? is given by
R
Z
? (x)d?(S)d?(?)
? ?
R
.
(2)
AB (x, S) =
?? (x)d?(?|S) =
d?(S)d?(?)
?
?
In this case, an optimal Bayesian algorithm can be regarded as the predictor constructed by averaging over all predictors with respect to a data-dependent posterior
?(?|S). We refer to such methods as Bayesian mixture methods. While the Bayes
estimator AB (x, S) is optimal with respect to the Bayes risk r(?, A), it can be
shown, that under appropriate conditions (and an appropriate prior) it is also a
minimax and admissible estimator [9].

In general, ?? is unknown. Rather we may have some prior information about
possible models for ?? . In view of (2) we consider a hypothesis space H, and an
algorithm based on a mixture of hypotheses h ? H. This should be contrasted
with classical approaches where an algorithm selects a single hypothesis h form
H. For simplicity, we consider a countable hypothesis space H = {h1 , h2 , . . .}; the
general case will be deferredPto the full paper. Let q = {qj }?
j=1 be a probability
vector, namely qj ? 0 and j qj = 1, and construct the composite predictor by
P
fq (x) =
j qj hj (x). Observe that in general fq (x) may be a great deal more
complex that any single hypothesis hj . For example, if hj (x) are non-polynomial
ridge functions, the composite predictor f corresponds to a two-layer neural network
with universal approximation
power. We denote by Q the probability distribution
P
defined by q, namely j qj hj = Eh?Q h.

A main feature of this work is the establishment of data-dependent bounds on
L(Eh?Q h), the loss of the Bayes mixture algorithm. There has been a flurry of
recent activity concerning data-dependent bounds (a non-exhaustive list includes
[2, 3, 5, 11, 13]). In a related vein, McAllester [7] provided a data-dependent bound
for the so-called Gibbs algorithm, which selects a hypothesis at random from H
based on the posterior distribution ?(h|S). Essentially, this result provides a bound
on the average error Eh?Q L(h) rather than a bound on the error of the averaged
hypothesis. Later, Langford et al. [6] extended this result to a mixture of classifiers
using a margin-based loss function. A more general result can also be obtained using
the covering number approach described in [14]. Finally, Herbrich and Graepel
[4] showed that under certain conditions the bounds for the Gibbs classifier can
be extended to a Bayesian mixture classifier. However, their bound contained an
explicit dependence on the dimension (see Thm. 3 in [4]).
Although the approach pioneered by McAllester came to be known as PAC-Bayes,
this term is somewhat misleading since an optimal Bayesian method (in the decision
theoretic framework outline above) does not average over loss functions but rather
over hypotheses. In this regard, the learning behavior of a true Bayesian method is
not addressed in the PAC-Bayes analysis. In this paper, we would like to narrow the
discrepancy by analyzing Bayesian mixture methods, where we consider a predictor
that is the average of a family of predictors with respect to a data-dependent posterior distribution. Bayesian mixtures can often be regarded as a good approximation
to a true optimal Bayesian method. In fact, we have shown above that they are
equivalent for many important practical problems.
Therefore the main contribution of the present work is the extension of the above
mentioned results in PAC-Bayes analysis to a rather unified setting for Bayesian
mixture methods, where different regularization criteria may be incorporated, and
their effect on the performance easily assessed. Furthermore, it is also essential that

the bounds obtained are dimension-independent, since otherwise they yield useless
results when applied to kernel-based methods, which often map the input space into
a space of very high dimensionality. Similar results can also be obtained using the
covering number analysis in [14]. However the approach presented in the current
paper, which relies on the direct computation of the Rademacher complexity, is more
direct and gives better bounds. The analysis is also easier to generalize than the
corresponding covering number approach. Moreover, our analysis applies directly
to other non-Bayesian mixture approaches such as Bagging and Boosting.
Before moving to the derivation of our bounds, we formalize our approach. Consider
a countable hypothesis space H = {hj }?
, and a probability distribution {qj } over
P?j=1
H. Introduce the vector notation k=1 qk hk (x) = q> h(x). A learning algorithm
within the Bayesian mixture framework uses the sample S to select a distribution
Q over H and then constructs a mixture hypothesis fq (x) = q> h(x). In order to
constrain the class of mixtures used in constructing the mixture q> h we impose
constraints on the mixture vector q. Let g(q) be a non-negative convex function of
q and define for any positive A,
?
?
?A = {q ? S : g(q) ? A} ; FA = fq : fq (x) = q> h(x) : q ? ?A ,
(3)
where S denotes the probability simplex. In subsequent sections we will consider
different choices for g(q), which essentially acts as a regularization term. Finally,
for any mixture q> h we define the loss by L(q> h) = E? `(y, (q> h)(x)) and the
? > h) = (1/n) Pn `(yi , (q> h)(xi )).
empirical loss incurred on the sample by L(q
i=1

3

A Mixture Algorithm with an Entropic Constraint

In this section we consider an entropic constraint, which penalizes weights deviating significantly from some prior probability distribution ? = {?j }?
j=1 , which may
incorporate our prior information about he problem. The weights q themselves are
chosen by the algorithm based on the data. In particular, in this section we set g(q)
to be the Kullback-Leibler divergence of q from ?,
X
qj log(qj /?j ).
g(q) = D(qk?) ; D(qk?) =
j

Let F be a class of real-valued functions, and denote by ?i independent Bernoulli
random variables assuming the values ?1 with equal probability. We define the
data-dependent Rademacher complexity of F as
"
#
n
X
1
? n (F) = E? sup
R
?i f (xi ) |S .
f ?F n i=1

? n (F) with respect to S will be denoted by Rn (F). We note
The expectation of R
?
that Rn (F) is concentrated around its mean value Rn (F) (e.g., Thm. 8 in [1]). We
quote a slightly adapted result from [5].
Theorem 1 (Adapted from Theorem 1 in [5])
Let {x1 , x2 , . . . , xn } ? X be a sequence of points generated independently at random
according to a probability distribution P , and let F be a class of measurable functions
from X to R. Furthermore, let ? be a non-negative Lipschitz function with Lipschitz
constant ?, such that ??f is uniformly bounded by a constant M . Then for all f ? F
with probability at least 1 ? ?
r
n
1X
log(1/?)
E?(f (x)) ?
?(f (xi )) ? 4?Rn (F) + M
.
n i=1
2n

An immediate consequence of Theorem 1 is the following.
Lemma 3.1 Let the loss function ` be bounded by M , and assume that it is Lipschitz with constant ?. Then for all q ? ?A with probability at least 1 ? ?
r
log(1/?)
>
>
?
.
L(q h) ? L(q h) + 4?Rn (FA ) + M
2n
Next, we bound the empirical Rademacher average of FA using g(q) = D(qk?).
Lemma 3.2 The empirical Rademacher complexity of FA is upper bounded as follows:
v
?r !
u n
u1 X
2A
?
Rn (FA ) ?
sup t
hj (xi )2 .
n
n i=1
j

Proof: We first recall a few facts from the theory of convex duality ?[10]. Let p(u)
?
>
be a convex function over a domain U , and set its dual s(z) = supP
u?U u z ? p(u) .
It is known that s(z) is also convex. Setting u = q and p(q) = j qj log(qj /?j ) we
P
find that s(v) = log j ?j ezj . From the definition of s(z) it follows that for any
q ? S,
X
X
q> z ?
qj log(qj /?j ) + log
? j ez j .
j

j

P

Since z is arbitrary, we set z = (?/n) i ?i h(xi ) and conclude that for q ? ?A and
any ? > 0
?
( n
"
)
#?
?
?
X
X
X
1
?
1
sup
A + log
?j exp
?i q> h(xi ) ?
?i hj (xi )
.
?
n i=1
??
n i
q??A
j
Taking the
to ?, and using the Chernoff bound
?
P expectation ?with
P 2respect
E? {exp ( i ?i ai )} ? exp
a
/2
,
we
have
that
i i
?
"
#?
?
?
X
X
?
1
? n (FA ) ?
?j exp
A + E? log
?i hj (xi )
R
?
??
n i
j
(
"
#)
1
?X
?
A + sup log E? exp
?i hj (xi )
(Jensen)
?
n i
j
(
"
#)
1
?2 X hj (xi )2
?
A + sup log exp 2
(Chernoff)
?
n i
2
j
X
A
?
= + 2 sup
hj (xi )2 .
?
2n j i
Minimizing the r.h.s. with respect to ?, we obtain the desired result.

?

Combining Lemmas 3.1 and 3.2 yields our basic bound, where ? and M are defined
in Lemma 3.1.
Theorem 2 Let S = {(x1 , y1 ), . . . , (xn , yn )} be a sample of i.i.d. points each
drawn according to a distribution ?(x, y). Let H be a countable hypothesis class,
and set FA to be the class defined in (3) with g(q) = D(qk?). Set ?H =

?

(1/n)E? supj
1??

Pn

hj (xi )2

?1/2

. Then for any q ? ?A with probability at least
r
r
2A
log(1/?)
>
>
?
+M
.
L(q h) ? L(q h) + 4??H
n
2n
i=1

Note that if hj are uniformly bounded, hj ? c, then ?H ? c. Theorem 2 holds for a
fixed value of A. Using the so-called multiple testing Lemma (e.g. [11]) we obtain:
Corollary 3.1 Let the assumptions
of Theorem 2 hold, and let {Ai , pi } be a set of
P
positive numbers such that i pi = 1. Then for all Ai and q ? ?Ai with probability
at least 1 ? ?,
r
r
2Ai
log(1/pi ?)
>
>
?
L(q h) ? L(q h) + 4??H
+M
.
n
2n
Note that the only distinction with Theorem 2 is the extra factor of log pi which is
the price paid for the uniformity of the bound.
Finally, we present a data-dependent bound of the form (1).
Theorem 3 Let the assumptions of Theorem 2 hold. Then for all q ? S with
probability at least 1 ? ?,
r
130D(qk?) + log(1/?)
>
>
?
L(q h) ? L(q h) + max(??H , M ) ?
.
(4)
n
P
Proof sketch Pick Ai = 2i and pi = 1/i(i + 1), i = 1, 2, . . . (note that i pi = 1).
For each q, let i(q) be the smallest index for which Ai(q) ? D(qk?) implying that
log(1/pi(q) ) ? 2 log log2 (4D(qk?)). A few lines of algebra, to be presented in the
full paper, yield the desired result.
?
The results of Theorem 3 can be compared to those derived by McAllester [8] for
the randomized Gibbs procedure. In the latter case, the first term on the r.h.s. is
?
Eh?Q L(h),
namely the average empirical error of the base classifiers h. In our case
? h?Q h), namely the empirical error of the average
the corresponding term is L(E
hypothesis. Since Eh?Q h is potentially much more complex than any single h ? H,
we expect that the empirical term in (4) is much smaller than the corresponding
term in [8]. Moreover, the complexity term we obtain is in fact tighter than the
corresponding term in [8] by a logarithmic factor in n (although the logarithmic
factor in [8] could probably be eliminated). We thus expect that Bayesian mixture
approach advocated here leads to better performance guarantees.
Finally, we comment that Theorem 3 can be used to obtain so-called oracle inequalities. In particular, let q? be the optimal distribution minimizing L(q> h), which
can only be computed if the underlying distribution ?(x, y) is known. Consider an
? by minimizing
algorithm which, based only on the data, selects a distribution q
the r.h.s. of (4), with the implicit constants appropriately specified. Then, using
standard approaches (e.g. [2]) we can obtain a bound on L(?
q> h) ? L(q?> h). For
lack of space, we defer the derivation of the precise bound to the full paper.

4

General Data-Dependent Bounds for Bayesian Mixtures

The Kullback-Leibler divergence is but one way to incorporate prior information.
In this section we extend the results to general convex regularization functions

g(q). Some possible choices for g(q) besides the Kullback-Leibler divergence are
the standard Lp norms kqkp .
In order to proceed along the lines of Section 3, we let
? s(z) be the? convex function associated with g(q), namely s(z) = supq??A q> z ? g(q) . Repeating
Pn
the arguments of Section 3 we have for any ? > 0 that n1 i=1 ?i q> h(xi ) ?
??
?
?? P
1
i ?i h(xi ) , which implies that
? A+s n
(
?
!)
1
?X
?
Rn (FA ) ? inf
A + E? s
?i h(xi )
.
(5)
??0 ?
n i
Pn
Assume that s(z) is second order differentiable, and that for any h = i=1 ?i h(xi )
1
2 (s(h + ?h) + s(h ? ?h)) ? s(h) ? u(?h). Then, assuming that s(0) = 0, it is
easy to show by induction that
n
?
? X
Xn
E? s (?/n)
?i h(xi ) ?
u((?/n)h(xi )).
i=1

(6)

i=1

In the remainder of the section we focus on the the case of regularization based on
the Lp norm. Consider p and q such that 1/q + 1/p = 1, p ? (1, ?), and let p0 =
max(p, 2) and q 0 = min(q, 2). Note that if p ? 2 then q ? 2, q 0 = p0 = 2 and if p > 2
0
then q < 2, q 0 = q, p0 = p. Consider p-norm regularization g(q) = p10 kqkpp , in which
0

case s(z) = q10 kzkqq . The Rademacher averaging result for p-norm regularization
is known in the Geometric theory of Banach spaces (type structure of the Banach
space), and it also follows from Khinchtine?s inequality. We show that it can be
easily obtained in our framework.
In this case, it is easy to see that s(z) =
Substituting in (5) we have
? n (FA ) ? inf 1
R
??0 ?

(

q?1
A+
q0

where Cq = ((q ? 1)/q 0 )

1
q0
q 0 kzkq

implies u(h(x)) ?

q?1
q0
q 0 kh(x)kq .

)
!1/q0
?
? ?q0 X
n
n
Cq
?
1X
q0
q0
1/p0
kh(xi )kq = 1/p0 A
kh(xi )kq
n
n i=1
n
i=1

1/q 0

.

Combining this result with the methods described in Section 3, we establish a bound
for regularization based on the Lp norm. Assume that kh(xi )kq is finite for all i,
? n
o?1/q0
Pn
0
and set ?H,q = E (1/n) i=1 kh(xi )kqq
.

Theorem 4 Let the conditions of Theorem 3 hold and set g(q) =
(1, ?). Then for all q ? S, with probability at least 1 ? ?,
? > h) + max(??H,q , M ) ? O
L(q h) ? L(q
>

?

kqkp
+
n1/p0

r

1
p0
p0 kqkp ,

log log(kqkp + 3) + log(1/?)
n

p ?

!

where O(?) hides a universal constant that depends only on p.

5

Discussion

We have introduced and analyzed a class of regularized Bayesian mixture approaches, which construct complex composite estimators by combining hypotheses

from some underlying hypothesis class using data-dependent weights. Such weighted
averaging approaches have been used extensively within the Bayesian framework,
as well as in more recent approaches such as Bagging and Boosting. While Bayesian
methods are known, under favorable conditions, to lead to optimal estimators in a
frequentist setting, their performance in agnostic settings, where no reliable assumptions can be made concerning the data generating mechanism, has not been well
understood. Our data-dependent bounds allow the utilization of Bayesian mixture
models in general settings, while at the same time taking advantage of the benefits
of the Bayesian approach in terms of incorporation of prior knowledge. The bounds
established, being independent of the cardinality of the underlying hypothesis space,
can be directly applied to kernel based methods.
Acknowledgments We thank Shimon Benjo for helpful discussions. The research
of R.M. is partially supported by the fund for promotion of research at the Technion
and by the Ollendorff foundation of the Electrical Engineering department at the
Technion.

References
[1] P. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: risk bounds
and structural results. In Proceedings of the Fourteenth Annual Conference on Computational Learning Theory, pages 224?240, 2001.
[2] P.L. Bartlett, S. Boucheron, and G. Lugosi. Model selection and error estimation.
Machine Learning, 48:85?113, 2002.
[3] O. Bousquet and A. Chapelle. Stability and generalization. J. Machine Learning
Research, 2:499?526, 2002.
[4] R. Herbrich and T. Graepel. A pac-bayesian margin bound for linear classifiers; why
svms work. In Advances in Neural Information Processing Systems 13, pages 224?230,
Cambridge, MA, 2001. MIT Press.
[5] V. Koltchinksii and D. Panchenko. Empirical margin distributions and bounding the
generalization error of combined classifiers. Ann. Statis., 30(1), 2002.
[6] J. Langford, M. Seeger, and N. Megiddo. An improved predictive accuracy bound
for averaging classifiers. In Proceeding of the Eighteenth International Conference on
Machine Learning, pages 290?297, 2001.
[7] D. A. McAllester. Some pac-bayesian theorems. In Proceedings of the eleventh Annual
conference on Computational learning theory, pages 230?234, New York, 1998. ACM
Press.
[8] D. A. McAllester. PAC-bayesian model averaging. In Proceedings of the twelfth
Annual conference on Computational learning theory, New York, 1999. ACM Press.
[9] C. P. Robert. The Bayesian Choice: A Decision Theoretic Motivation. Springer
Verlag, New York, 1994.
[10] R.T. Rockafellar. Convex Analysis. Princeton University Press, Princeton, N.J., 1970.
[11] J. Shawe-Taylor, P. Bartlett, R.C. Williamson, and M. Anthony. Structural risk
minimization over data-dependent hierarchies. IEEE trans. Inf. Theory, 44:1926?
1940, 1998.
[12] Y. Yang. Minimax nonparametric classification - part I: rates of convergence. IEEE
Trans. Inf. Theory, 45(7):2271?2284, 1999.
[13] T. Zhang. Generalization performance of some learning problems in hilbert functional
space. In Advances in Neural Information Processing Systems 15, Cambridge, MA,
2001. MIT Press.
[14] T. Zhang. Covering number bounds of certain regularized linear function classes.
Journal of Machine Learning Research, 2:527?550, 2002.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 2819-a-bayesian-spatial-scan-statistic.pdf

A Bayesian Spatial Scan Statistic

Daniel B. Neill
Andrew W. Moore
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213
{neill,awm}@cs.cmu.edu

Gregory F. Cooper
Center for Biomedical Informatics
University of Pittsburgh
Pittsburgh, PA 15213
gfc@cbmi.pitt.edu

Abstract
We propose a new Bayesian method for spatial cluster detection, the
?Bayesian spatial scan statistic,? and compare this method to the standard
(frequentist) scan statistic approach. We demonstrate that the Bayesian
statistic has several advantages over the frequentist approach, including
increased power to detect clusters and (since randomization testing is
unnecessary) much faster runtime. We evaluate the Bayesian and frequentist methods on the task of prospective disease surveillance: detecting spatial clusters of disease cases resulting from emerging disease outbreaks. We demonstrate that our Bayesian methods are successful in
rapidly detecting outbreaks while keeping number of false positives low.

1

Introduction

Here we focus on the task of spatial cluster detection: finding spatial regions where some
quantity is significantly higher than expected. For example, our goal may be to detect
clusters of disease cases, which may be indicative of a naturally occurring epidemic (e.g.
influenza), a bioterrorist attack (e.g. anthrax release), or an environmental hazard (e.g. radiation leak). [1] discusses many other applications of cluster detection, including mining
astronomical data, medical imaging, and military surveillance. In all of these applications,
we have two main goals: to identify the locations, shapes, and sizes of potential clusters,
and to determine whether each potential cluster is more likely to be a ?true? cluster or simply a chance occurrence. Thus we compare the null hypothesis H0 of no clusters against
some set of alternative hypotheses H1 (S), each representing a cluster in some region or
regions S. In the standard frequentist setting, we do this by significance testing, computing
the p-values of potential clusters by randomization; here we propose a Bayesian framework, in which we compute posterior probabilities of each potential cluster.
Our primary motivating application is prospective disease surveillance: detecting spatial
clusters of disease cases resulting from a disease outbreak. In this application, we perform
surveillance on a daily basis, with the goal of finding emerging epidemics as quickly as
possible. For this task, we are given the number of cases of some given syndrome type
(e.g. respiratory) in each spatial location (e.g. zip code) on each day. More precisely, we
typically cannot measure the actual number of cases, and instead rely on related observable
quantities such as the number of Emergency Department visits or over-the-counter drug
sales. We must then detect those increases which are indicative of emerging outbreaks,
as close to the start of the outbreak as possible, while keeping the number of false positives low. In biosurveillance of disease, every hour of earlier detection can translate into
thousands of lives saved by more timely administration of antibiotics, and this has led to
widespread interest in systems for the rapid and automatic detection of outbreaks.

In this spatial surveillance setting, each day we have data collected for a set of discrete
spatial locations si . For each location si , we have a count ci (e.g. number of disease cases),
and an underlying baseline bi . The baseline may correspond to the underlying population
at risk, or may be an estimate of the expected value of the count (e.g. derived from the
time series of previous count data). Our goal, then, is to find if there is any spatial region
S (set of locations si ) for which the counts are significantly higher than expected, given the
baselines. For simplicity, we assume here (as in [2]) that the locations si are aggregated to a
uniform, two-dimensional, N ? N grid G, and we search over the set of rectangular regions
S ? G. This allows us to search both compact and elongated regions, allowing detection of
elongated disease clusters resulting from dispersal of pathogens by wind or water.
1.1 The frequentist scan statistic
One of the most important statistical tools for cluster detection is Kulldorff?s spatial scan
statistic [3-4]. This method searches over a given set of spatial regions, finding those regions which maximize a likelihood ratio statistic and thus are most likely to be generated
under the alternative hypothesis of clustering rather than the null hypothesis of no clustering. Randomization testing is used to compute the p-value of each detected region,
correctly adjusting for multiple hypothesis testing, and thus we can both identify potential
clusters and determine whether they are significant. Kulldorff?s framework assumes that
counts ci are Poisson distributed with ci ? Po(qbi ), where bi represents the (known) census population of cell si and q is the (unknown) underlying disease rate. Then the goal of
the scan statistic is to find regions where the disease rate is higher inside the region than
| H1 (S))
outside. The statistic used for this is the likelihood ratio F(S) = P(Data
P(Data | H0 ) , where the
null hypothesis H0 assumes a uniform disease rate q = qall . Under H1 (S), we assume that
q = qin for all si ? S, and q = qout for all si ? G ? S, for some constants qin > qout . From
this, we can derive an expression for F(S) using maximum likelihood estimates of qin ,
qout , and qall : F(S) = ( CBinin )Cin ( CBout
)Cout ( CBall
)?Call , if CBinin > CBout
, and F(S) = 1 otherwise.
out
out
all
In this expression, we have Cin = ?S ci , Cout = ?G?S ci , Call = ?G ci , and similarly for the
baselines Bin = ?S bi , Bout = ?G?S bi , and Ball = ?G bi .
Once we have found the highest scoring region S? = arg maxS F(S) of grid G, and its score
F ? = F(S? ), we must still determine the statistical significance of this region by randomization testing. To do so, we randomly create a large number R of replica grids by sampling
under the null hypothesis ci ? Po(qall bi ), and find the highest scoring region and its score
+1
for each replica grid. Then the p-value of S? is Rbeat
R+1 , where Rbeat is the number of repli0
?
cas G with F higher than the original grid. If this p-value is less than some threshold (e.g.
0.05), we can conclude that the discovered region is unlikely to have occurred by chance,
and is thus a significant spatial cluster; otherwise, no significant clusters exist.
The frequentist scan statistic is a useful tool for cluster detection, and is commonly used in
the public health community for detection of disease outbreaks. However, there are three
main disadvantages to this approach. First, it is difficult to make use of any prior information that we may have, for example, our prior beliefs about the size of a potential outbreak
and its impact on disease rate. Second, the accuracy of this technique is highly dependent
on the correctness of our maximum likelihood parameter estimates. As a result, the model
is prone to parameter overfitting, and may lose detection power in practice because of
model misspecification. Finally, the frequentist scan statistic is very time consuming, and
may be computationally infeasible for large datasets. A naive approach requires searching
over all rectangular regions, both for the original grid and for each replica grid. Since there
are O(N 4 ) rectangles to search for an N ? N grid, the total computation time is O(RN 4 ),
where R = 1000 is a typical number of replications. In past work [5, 2, 6], we have shown
how to reduce this computation time by a factor of 20-2000x through use of the ?fast spatial
scan? algorithm; nevertheless, we must still perform this faster search both for the original
grid and for each replica.

We propose to remedy these problems through the use of a Bayesian spatial scan statistic.
First, our Bayesian model makes use of prior information about the likelihood, size, and
impact of an outbreak. If these priors are chosen well, we should achieve better detection power than the frequentist approach. Second, the Bayesian method uses a marginal
likelihood approach, averaging over possible values of the model parameters qin , qout , and
qall , rather than relying on maximum likelihood estimates of these parameters. This makes
the model more flexible and less prone to overfitting, and reduces the potential impact of
model misspecification. Finally, under the Bayesian model there is no need for randomization testing, and (since we need only to search the original grid) even a naive search can be
performed relatively quickly. We now present the Bayesian spatial scan statistic, and then
compare it to the frequentist approach on the task of detecting simulated disease epidemics.

2

The Bayesian scan statistic

Here we consider the natural Bayesian extension of Kulldorff?s scan statistic, moving from
a Poisson to a conjugate Gamma-Poisson model. Bayesian Gamma-Poisson models are
a common representation for count data in epidemiology, and have been used in disease
mapping by Clayton and Kaldor [7], Molli?e [8], and others. In disease mapping, the effect
of the Gamma prior is to produce a spatially smoothed map of disease rates; here we instead
focus on computing the posterior probabilities, allowing us to determine the likelihood that
an outbreak has occurred, and to estimate the location and size of potential outbreaks.
For the Bayesian spatial scan, as in the frequentist approach, we wish to compare the null
hypothesis H0 of no clusters to the set of alternative hypotheses H1 (S), each representing
a cluster in some region S. As before, we assume Poisson likelihoods, ci ? Po(qbi ). The
difference is that we assume a hierarchical Bayesian model where the disease rates qin , qout ,
and qall are themselves drawn from Gamma distributions. Thus, under the null hypothesis
H0 , we have q = qall for all si ? G, where qall ? Ga(?all , ?all ). Under the alternative hypothesis H1 (S), we have q = qin for all si ? S and q = qout for all si ? G ? S, where we independently draw qin ? Ga(?in , ?in ) and qout ? Ga(?out , ?out ). We discuss how the ? and ? priors
are chosen below. From this model, we can compute the posterior probabilities P(H1 (S)|D)
of an outbreak in each region S, and the probability P(H0 | D) that no outbreak has ocH0 )P(H0 )
(S))P(H1 (S))
curred, given dataset D: P(H0 | D) = P(D |P(D)
and P(H1 (S) | D) = P(D | H1 P(D)
,
where P(D) = P(D | H0 )P(H0 ) + ?S P(D | H1 (S))P(H1 (S)). We discuss the choice of prior
probabilities P(H0 ) and P(H1 (S)) below. To compute the marginal likelihood of the data
given each hypothesis, we must integrate over all possible values of the parameters (qin ,
qout , qall ) weighted by their respective probabilities. Since we have chosen a conjugate
prior, we can easily obtain a closed-form solution for these likelihoods:
P(D | H0 ) =

Z

P(qall ? Ga(?all , ?all ))

P(D | H1 (S)) =
?

Z

? P(ci ? Po(qall bi )) dqall

si ?G

Z

P(qin ? Ga(?in , ?in )) ? P(ci ? Po(qin bi )) dqin

P(qout ? Ga(?out , ?out ))

si ?S

?

P(ci ? Po(qout bi )) dqout

si ?G?S

Now, computing the integral, and letting C = ? ci and B = ? bi , we obtain:
?? ??1 ??q
(qbi )ci e?qbi
q
e
dq ?
?
?(?)
(ci )!
si
si
Z
Z
?
?
?
?
?
? ?(? +C)
q??1 e??q q? ci e?q ? bi dq =
q?+C?1 e?(?+B)q dq =
?(?)
?(?)
(? + B)?+C ?(?)
Z

P(q ? Ga(?, ?)) ? P(ci ? Po(qbi )) dq =

Z

Thus we have the following expressions for the marginal likelihoods: P(D | H0 ) ?
?
(?all )?all ?(?all +Call )
)?out ?(?out +Cout )
in ) in ?(?in +Cin )
, and P(D | H1 (S)) ? (?(?+B
? (? (?out
.
?all +Call
+B )?out +Cout ?(? )
)?in +Cin ?(? )
(?all +Ball )

?(?all )

in

in

in

out

out

out

The Bayesian spatial scan statistic can be computed simply by first calculating the score
P(D | H1 (S))P(H1 (S)) for each spatial region S, maintaining a list of regions ordered by
score. We then calculate P(D | H0 )P(H0 ), and add this to the sum of all region scores, obtaining the probability of the data P(D). Finally, we can compute the posterior probability
H0 )P(H0 )
(S))P(H1 (S))
for each region, as well as P(H0 | D) = P(D |P(D)
. Then
P(H1 (S) | D) = P(D | H1 P(D)
we can return all regions with non-negligible posterior probabilities, the posterior probability of each, and the overall probability of an outbreak. Note that no randomization testing
is necessary, and thus overall complexity is proportional to number of regions searched,
e.g. O(N 4 ) for searching over axis-aligned rectangles in an N ? N grid.
2.1

Choosing priors

One of the most challenging tasks in any Bayesian analysis is the choice of priors. For
any region S that we examine, we must have values of the parameter priors ?in (S), ?in (S),
?out (S), and ?out (S), as well as the region prior probability P(H1 (S)). We must also choose
the global parameter priors ?all and ?all , as well as the ?no outbreak? prior P(H0 ).
Here we consider the simple case of a uniform region prior, with a known prior probability
of an outbreak P1 . In other words, if there is an outbreak, it is assumed to be equally
1
likely to occur in any spatial region. Thus we have P(H0 ) = 1 ? P1 , and P(H1 (S)) = NPreg
,
where Nreg is the total number of regions searched. The parameter P1 can be obtained from
historical data, estimated by human experts, or can simply be used to tune the sensitivity
and specificity of the algorithm. The model can also be easily adapted to a non-uniform
region prior, taking into account our prior beliefs about the size and shape of outbreaks.
For the parameter priors, we assume that we have access to a large number of days of past
data, during which no outbreaks are known to have occurred. We can then obtain estimated
values of the parameter priors under the null hypothesis by matching the moments of each
Gamma distribution to their historical values. In other words, we set the expectation and
variance of the Gamma distribution Ga(?allh, ?alli) to the sample expectation
h i and variance
Call
?all
?all
Call
of Ball observed in past data: ? = Esample Ball , and ?2 = Varsample CBall
. Solving for
all
all

?all and ?all , we obtain ?all



h

i2

all

h

i

Esample CBall
Esample CBall
hall i and ?all =
h all i .
=
Varsample CBall
Varsample CBall
all
all

The calculation of priors ?in (S), ?in (S), ?out (S), and ?out (S) is identical except for two differences: first, we must condition on the region S, and second, we must assume the alternative hypothesis H1 (S) rather than the null hypothesis H0 . Repeating the above derivation for
i2
i
h

h
(S)
(S)
Esample CBout
Esample CBout
out (S) i
out (S) i
h
h
the ?out? parameters, we obtain ?out (S) =
and ?out (S) =
(S)
(S) ,
Varsample CBout
Varsample CBout
out (S)
out (S)
where Cout (S) and Bout (S) are respectively the total count ?G?S ci and total baseline ?G?S bi
outside the region. Note that an outbreak in some region S does not affect the disease rate
outside region S. Thus we can use the same values of ?out (S) and ?out (S) whether we are
assuming the null hypothesis H0 or the alternative hypothesis H1 (S).
On the other hand, the effect of an outbreak inside region S must be taken into account when
computing ?in (S) and ?in (S); since we assume that no outbreak has occurred in the past
data, we cannot just use the sample mean and variance, but must consider what we expect
these quantities to be in the event of an outbreak. We assume that the outbreak will increase
qin by a multiplicative factor m, thus multiplying the mean and variance of CBinin by m. To
account for this in the Gamma distribution Ga(?in , ?in ), we multiply ?in by m while leaving

h
i2
h
i
Esample CBinin (S)
Esample CBinin (S)
(S)
(S)
h
i and ?in (S) =
h
i,
?in unchanged. Thus we have ?in (S) = m
Varsample CBinin (S)
Varsample CBinin (S)
(S)
(S)

where Cin (S) = ?S ci and Bin (S) = ?S bi . Since we typically do not know the exact value of
m, here we use a discretized uniform distribution for m, ranging from m = 1 . . . 3 at intervals
of 0.2. Then scores can be calculated by averaging likelihoods over the distribution of m.
Finally, we consider how to deal with the case where the past values of the counts and
baselines are not given. In this ?blind Bayesian? (BBayes) case, we assume that counts
are randomly generated under the null hypothesis ci ? Po(q0 bi ), where q0 is the expected
ratio of count to baseline under the null (for example, q0 = 1 if baselines are obtained
by estimating the expected value of the count). Under this simple assumption, we can
easily compute the expectation and variance of the ratio of count to baseline under the null
  Var[Po(q0 B)] q0 B q0
  E[Po(q0 B)] q0 B
= B = q0 , and Var CB =
= B2 = B . Thus
hypothesis: E CB =
B
B2
we have ? = q0 B and ? = B under the null hypothesis. This gives us ?all = q0 Ball , ?all =
Ball , ?out (S) = q0 Bout (S), ?out (S) = Bout (S), ?in (S) = mq0 Bin (S), and ?in (S) = Bin (S). We
can use a uniform distribution for m as before. In our empirical evaluation below, we
consider both the Bayes and BBayes methods of generating parameter priors.

3

Results: detection power

We evaluated the Bayesian and frequentist methods on two types of simulated respiratory
outbreaks, injected into real Emergency Department and over-the-counter drug sales data
for Allegheny County, Pennsylvania. All data were aggregated to the zip code level to
ensure anonymity, giving the daily counts of respiratory ED cases and sales of OTC cough
and cold medication in each of 88 zip codes for one year. The baseline (expected count)
for each zip code was estimated using the mean count of the previous 28 days. Zip code
centroids were mapped to a 16 ? 16 grid, and all rectangles up to 8 ? 8 were examined. We
first considered simulated aerosol releases of inhalational anthrax (e.g. from a bioterrorist
attack), generated by the Bayesian Aerosol Release Detector, or BARD [9]. The BARD
simulator uses a Bayesian network model to determine the number of spores inhaled by
individuals in affected areas, the resulting number and severity of anthrax cases, and the
resulting number of respiratory ED cases on each day of the outbreak in each affected zip
code. Our second type of outbreak was a simulated ?Fictional Linear Onset Outbreak?
(or ?FLOO?), as in [10]. A FLOO(?, T ) outbreak is a simple simulated outbreak with
duration T , which generates t? cases in each affected zip code on day t of the outbreak
(0 < t ? T /2), then generates T ?/2 cases per day for the remainder of the outbreak. Thus
we have an outbreak where the number of cases ramps up linearly and then levels off.
While this is clearly a less realistic outbreak than the BARD-simulated anthrax attack, it
does have several advantages: most importantly, it allows us to precisely control the slope
of the outbreak curve and examine how this affects our methods? detection ability.
To test detection power, a semi-synthetic testing framework similar to [10] was used: we
first run our spatial scan statistic for each day of the last nine months of the year (the first
three months are used only to estimate baselines and priors), and obtain the score F ? for
each day. Then for each outbreak we wish to test, we inject that outbreak into the data, and
obtain the score F ? (t) for each day t of the outbreak. By finding the proportion of baseline
days with scores higher than F ? (t), we can determine the proportion of false positives we
would have to accept to detect the outbreak on day t. This allows us to compute, for any
given level of false positives, what proportion of outbreaks can be detected, and the mean
number of days to detection. We compare three methods of computing the score F ? : the frequentist method (F ? is the maximum likelihood ratio F(S) over all regions S), the Bayesian
maximum method (F ? is the maximum posterior probability P(H1 (S) | D) over all regions
S), and the Bayesian total method (F ? is the sum of posterior probabilities P(H1 (S)|D) over
all regions S, i.e. total posterior probability of an outbreak). For the two Bayesian methods,
we consider both Bayes and BBayes methods for calculating priors, thus giving us a total
of five methods to compare (frequentist, Bayes max, BBayes max, Bayes tot, BBayes tot).
In Table 1, we compare these methods with respect to proportion of outbreaks detected and

Table 1: Days to detect and proportion of outbreaks detected, 1 false positive/month
method
frequentist
Bayes max
BBayes max
Bayes tot
BBayes tot

FLOO ED
(4,14)
1.859
(100%)
1.740
(100%)
1.683
(100%)
1.882
(100%)
1.840
(100%)

FLOO ED
(2,20)
3.324
(100%)
2.875
(100%)
2.848
(100%)
3.195
(100%)
3.180
(100%)

FLOO ED
(1,20)
6.122
(96%)
5.043
(100%)
4.984
(100%)
5.777
(100%)
5.672
(100%)

BARD ED
(.125)
1.733
(100%)
1.600
(100%)
1.600
(100%)
1.633
(100%)
1.617
(100%)

BARD ED
(.016)
3.925
(88%)
3.755
(88%)
3.698
(88%)
3.811
(88%)
3.792
(88%)

FLOO OTC
(40,14)
3.582
(100%)
5.455
(63%)
5.164
(65%)
3.475
(100%)
4.380
(100%)

FLOO OTC
(25,20)
5.393
(100%)
7.588
(79%)
7.035
(77%)
5.195
(100%)
6.929
(99%)

mean number of days to detect, at a false positive rate of 1/month. Methods were evaluated
on seven types of simulated outbreaks: three FLOO outbreaks on ED data, two FLOO outbreaks on OTC data, and two BARD outbreaks (with different amounts of anthrax release)
on ED data. For each outbreak type, each method?s performance was averaged over 100 or
250 simulated outbreaks for BARD or FLOO respectively.
In Table 1, we observe very different results for the ED and OTC datasets. For the five runs
on ED data, all four Bayesian methods consistently detected outbreaks faster than the frequentist method. This difference was most evident for the more slowly growing (harder to
detect) outbreaks, especially FLOO(1,20). Across all ED outbreaks, the Bayesian methods showed an average improvement of between 0.13 days (Bayes tot) and 0.43 days
(BBayes max) as compared to the frequentist approach; ?max? methods performed substantially better than ?tot? methods, and ?BBayes? methods performed slightly better than
?Bayes? methods. For the two runs on OTC data, on the other hand, most of the Bayesian
methods performed much worse (over 1 day slower) than the frequentist method. The exception was the Bayes tot method, which again outperformed the frequentist method by an
average of 0.15 days. We believe that the main reason for these differing results is that the
OTC data is much noisier than the ED data, and exhibits much stronger seasonal trends.
As a result, our baseline estimates (using mean of the previous 28 days) are reasonably accurate for ED, but for OTC the baseline estimates will lag behind the seasonal trends (and
thus, underestimate the expected counts for increasing trends and overestimate for decreasing trends). The BBayes methods, which assume E[C/B] = 1 and thus rely heavily on the
accuracy of baseline estimates, are not reasonable for OTC. On the other hand, the Bayes
methods (which instead learn the priors from previous counts and baselines) can adjust for
consistent misestimation of baselines and thus more accurately account for these seasonal
trends. The ?max? methods perform badly on the OTC data because a large number of
baseline days have posterior probabilities close to 1; in this case, the maximum region posterior varies wildly from day to day, depending on how much of the total probability is
assigned to a single region, and is not a reliable measure of whether an outbreak has occurred. The total posterior probability of an outbreak, on the other hand, will still be higher
for outbreak than non-outbreak days, so the ?tot? methods can perform well on OTC as
well as ED data. Thus, our main result is that the Bayes tot method, which infers baselines
from past counts and uses total posterior probability of an outbreak to decide when to sound
the alarm, consistently outperforms the frequentist method for both ED and OTC datasets.

4

Results: computation time

As noted above, the Bayesian spatial scan must search over all rectangular regions for the
original grid only, while the frequentist scan (in order to calculate statistical significance by
randomization) must also search over all rectangular regions for a large number (typically
R = 1000) of replica grids. Thus, as long as the search time per region is comparable for the
Bayesian and frequentist methods, we expect the Bayesian approach to be approximately
1000x faster. In Table 2, we compare the run times of the Bayes, BBayes, and frequen-

Table 2: Comparison of run times for varying grid size N
method
Bayes (naive)
BBayes (naive)
frequentist (naive)
frequentist (fast)

N = 16
0.7 sec
0.6 sec
12 min
20 sec

N = 32
10.8 sec
9.3 sec
2.9 hrs
1.8 min

N = 64
2.8 min
2.4 min
49 hrs
10.7 min

N = 128
44 min
37 min
?31 days
77 min

N = 256
12 hrs
10 hrs
?500 days
10 hrs

tist methods for searching a single grid and calculating significance (p-values or posterior
probabilities for the frequentist and Bayesian methods respectively), as a function of the
grid size N. All rectangles up to size N/2 were searched, and for the frequentist method
R = 1000 replications were performed. The results confirm our intuition: the Bayesian
methods are 900-1200x faster than the frequentist approach, for all values of N tested.
However, the frequentist approach can be accelerated dramatically using our ?fast spatial
scan? algorithm [2], a multiresolution search method which can find the highest scoring
region of a grid while searching only a small subset of regions. Comparing the fast spatial
scan to the Bayesian approach, we see that the fast spatial scan is slower than the Bayesian
method for grid sizes up to N = 128, but slightly faster for N = 256. Thus we now have two
options for making the spatial scan statistic computationally feasible for large grid sizes:
to use the fast spatial scan to speed up the frequentist scan statistic, or to use the Bayesian
scan statistics framework (in which case the naive algorithm is typically fast enough). For
even larger grid sizes, it may be possible to extend the fast spatial scan to the Bayesian
approach: this would give us the best of both worlds, searching only one grid, and using a
fast algorithm to do so. We are currently investigating this potentially useful synthesis.

5

Discussion

We have presented a Bayesian spatial scan statistic, and demonstrated several ways in
which this method is preferable to the standard (frequentist) scan statistics approach. In
Section 3, we demonstrated that the Bayesian method, with a relatively non-informative
prior distribution, consistently outperforms the frequentist method with respect to detection power. Since the Bayesian framework allows us to easily incorporate prior information about size, shape, and impact of an outbreak, it is likely that we can achieve even
better detection performance using more informative priors, e.g. obtained from experts in
the domain. In Section 4, we demonstrated that the Bayesian spatial scan can be computed
in much less time than the frequentist method, since randomization testing is unnecessary.
This allows us to search large grid sizes using a naive search algorithm, and even larger
grids might be searched by extending the fast spatial scan to the Bayesian framework.
We now consider three other arguments for use of the Bayesian spatial scan. First, the
Bayesian method has easily interpretable results: it outputs the posterior probability that
an outbreak has occurred, and the distribution of this probability over possible outbreak
regions. This makes it easy for a user (e.g. public health official) to decide whether to
investigate each potential outbreak based on the costs of false positives and false negatives;
this type of decision analysis cannot be done easily in the frequentist framework. Another
useful result of the Bayesian method is that we can compute a ?map? of the posterior probabilities of an outbreak in each grid cell, by summing the posterior probabilities P(H1 (S) | D)
of all regions containing that cell. This technique allows us to deal with the case where the
posterior probability mass is spread among many regions, by observing cells which are
common to most or all of these regions. We give an example of such a map below:
Figure 1: Output of Bayesian spatial scan on baseline OTC data, 1/30/05.
Cell shading is based on posterior probability of an outbreak in that cell,
ranging from white (0%) to black (100%). The bold rectangle represents
the most likely region (posterior probability 12.27%) and the darkest cell
is the most likely cell (total posterior probability 86.57%). Total posterior
probability of an outbreak is 86.61%.

Second, calibration of the Bayesian statistic is easier than calibration of the frequentist
statistic. As noted above, it is simple to adjust the sensitivity and specificity of the Bayesian
method by setting the prior probability of an outbreak P1 , and then we can ?sound the
alarm? whenever posterior probability of an outbreak exceeds some threshold. In the frequentist method, on the other hand, many regions in the baseline data have sufficiently
high likelihood ratios that no replicas beat the original grid; thus we cannot distinguish the
p-values of outbreak and non-outbreak days. While one alternative is to ?sound the alarm?
when the likelihood ratio is above some threshold (rather than when p-value is below some
threshold), this is technically incorrect: because the baselines for each day of data are different, the distribution of region scores under the null hypothesis will also differ from day
to day, and thus days with higher likelihood ratios do not necessarily have lower p-values.
Third, we argue that it is easier to combine evidence from multiple detectors within the
Bayesian framework, i.e. by modeling the joint probability distribution. We are in the process of examining Bayesian detectors which look simultaneously at the day?s Emergency
Department records and over-the-counter drug sales in order to detect emerging clusters,
and we believe that combination of detectors is an important area for future research.
In conclusion, we note that, though both Bayesian modeling [7-8] and (frequentist) spatial scanning [3-4] are common in the spatial statistics literature, this is (to the best of our
knowledge) the first model which combines the two techniques into a single framework.
In fact, very little work exists on Bayesian methods for spatial cluster detection. One notable exception is the literature on spatial cluster modeling [11-12], which attempts to infer
the location of cluster centers by inferring parameters of a Bayesian process model. Our
work differs from these methods both in its computational tractability (their models typically have no closed form solution, so computationally expensive MCMC approximations
are used) and its easy interpretability (their models give no indication as to statistical significance or posterior probability of clusters found). Thus we believe that this is the first
Bayesian spatial cluster detection method which is powerful and useful, yet computationally tractable. We are currently running the Bayesian and frequentist scan statistics on
daily OTC sales data from over 10000 stores, searching for emerging disease outbreaks on
a daily basis nationwide. Additionally, we are working to extend the Bayesian statistic to
fMRI data, with the goal of discovering regions of brain activity corresponding to given
cognitive tasks [13, 6]. We believe that the Bayesian approach has the potential to improve
both speed and detection power of the spatial scan in this domain as well.
References
[1] M. Kulldorff. 1999. Spatial scan statistics: models, calculations, and applications. In J. Glaz and M. Balakrishnan, eds., Scan
Statistics and Applications, Birkhauser, 303-322.
[2] D. B. Neill and A. W. Moore. 2004. Rapid detection of significant spatial clusters. In Proc. 10th ACM SIGKDD Intl. Conf.
on Knowledge Discovery and Data Mining, 256-265.
[3] M. Kulldorff and N. Nagarwalla. 1995. Spatial disease clusters: detection and inference. Statistics in Medicine 14, 799-810.
[4] M. Kulldorff. 1997. A spatial scan statistic. Communications in Statistics: Theory and Methods 26(6), 1481-1496.
[5] D. B. Neill and A. W. Moore. 2004. A fast multi-resolution method for detection of significant spatial disease clusters. In
Advances in Neural Information Processing Systems 16, 651-658.
[6] D. B. Neill, A. W. Moore, F. Pereira, and T. Mitchell. 2005. Detecting significant multidimensional spatial clusters. In
Advances in Neural Information Processing Systems 17, 969-976.
[7] D. G. Clayton and J. Kaldor. 1987. Empirical Bayes estimates of age-standardized relative risks for use in disease mapping.
Biometrics 43, 671-681.
[8] A. Molli?e. 1999. Bayesian and empirical Bayes approaches to disease mapping. In A. B. Lawson, et al., eds. Disease Mapping
and Risk Assessment for Public Health. Wiley, Chichester.
[9] W. Hogan, G. Cooper, M. Wagner, and G. Wallstrom. 2004. A Bayesian anthrax aerosol release detector. Technical Report,
RODS Laboratory, University of Pittsburgh.
[10] D. B. Neill, A. W. Moore, M. Sabhnani, and K. Daniel. 2005. Detection of emerging space-time clusters. In Proc. 11th ACM
SIGKDD Intl. Conf. on Knowledge Discovery and Data Mining.
[11] R. E. Gangnon and M. K. Clayton. 2000. Bayesian detection and modeling of spatial disease clustering. Biometrics 56,
922-935.
[12] A. B. Lawson and D. G. T. Denison, eds. 2002. Spatial Cluster Modelling. Chapman & Hall/CRC, Boca Raton, FL.
[13] X. Wang, R. Hutchinson, and T. Mitchell. 2004. Training fMRI classifiers to detect cognitive states across multiple human
subjects. In Advances in Neural Information Processing Systems 16, 709-716.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

