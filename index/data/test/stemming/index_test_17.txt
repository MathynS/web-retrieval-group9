query sentence: Neural networks
---------------------------------------------------------------------
title: 232-analog-neural-networks-of-limited-precision-i-computing-with-multilinear-threshold-functions.pdf

702

Obradovic and Pclrberry

Analog Neural Networks of Limited Precision I:
Computing with Multilinear Threshold Functions
(Preliminary Version)

Zoran Obradovic and Ian Parberry
Department of Computer Science.
Penn State University.
University Park. Pa. 16802.

ABSTRACT
Experimental evidence has shown analog neural networks to be ex~mely fault-tolerant; in particular. their performance does not appear to be significantly impaired when precision is limited. Analog
neurons with limited precision essentially compute k-ary weighted
multilinear threshold functions. which divide R" into k regions with
k-l hyperplanes. The behaviour of k-ary neural networks is investigated. There is no canonical set of threshold values for k>3.
although they exist for binary and ternary neural networks. The
weights can be made integers of only 0 ?z +k ) log (z +k ? bits. where
z is the number of processors. without increasing hardware or running time. The weights can be made ?1 while increasing running
time by a constant multiple and hardware by a small polynomial in z
and k. Binary neurons can be used if the running time is allowed to
increase by a larger constant multiple and the hardware is allowed to
increase by a slightly larger polynomial in z and k. Any symmetric
k-ary function can be computed in constant depth and size
(n k - 1/(k-2)!). and any k-ary function can be computed in constant
depth and size 0 (nk"). The alternating neural networks of Olafsson
and Abu-Mostafa. and the quantized neural networks of Fleisher are
closely related to this model.

o

Analog Neural Networks of Limited Precision I

1 INTRODUCTION
Neural networks are typically circuits constructed from processing units which compute simple functions of the form f(Wl> ... ,wlI):RII-+S where SeR, wieR for 1~~,
and
II

f (Wl> ... ,WII)(Xl, .?. ,xlI)=g (LWi X;)
i=1

for some output function g :R-+S. There are two choices for the set S which are
currently popular in the literature. The first is the discrete model, with S=B (where B
denotes the Boolean set (0,1)). In this case, g is typically a linear threshold function
g (x)= 1 iff x~. and f is called a weighted linear threshold function. The second is
the analog model, with S=[O,I] (where [0,1] denotes (re RI~~I}). In this case. g
is typically a monotone increasing function, such as the sigmoid function
g (x)=(1 +c -% 1 for some constant c e R. The analog neural network model is popular
because it is easy to construct processors with the required characteristics using a few
transistors. The digital model is popular because its behaviour is easy to analyze.

r

Experimental evidence indicates that analog neural networks can produce accurate
computations when the precision of their components is limited. Consider what actually happens to the analog model when the precision is limited. Suppose the neurons
can take on k distinct excitation values (for example, by restricting the number of digits in their binary or decimal expansions). Then S is isomorphic to Zk={O, ... ,k-l}.
We will show that g is essentially the multilinear threshold function
g (hloh2 ....,hk-l):R-+Zk defined by

Here and throughout this paper, we will assume that hl~h2~ ... ~hk-1> and for convenience define ho=-oo and h/c=oo. We will call f a k-ary weighted multilinear threshold
function when g is a multilinear threshold function.
We will study neural networks constructed from k-ary multilinear threshold functions.
We will call these k-ary neural networks, in order to distinguish them from the standard 2-ary or binary neural network. We are particularly concerned with the resources
of time, size (number of processors), and weight (sum of all the weights) of k-ary
neural networks when used in accordance with the classical computational paradigm.
The reader is referred to (parberry, 1990) for similar results on binary neural networks.
A companion paper (Obradovic & Parberry, 1989b) deals with learning on k-ary neural networks. A more detailed version of this paper appears in (Obradovic & Parberry,
1989a).

2 A K-ARY NEURAL NETWORK MODEL
A k-ary neural network is a weighted graph M =(V ,E ,W ,h), where V is a set of processors and E cVxV is a set of connections between processors. Function
w:VxV -+R assign weights to interconnections and h:V -+Rk - assign a set of k-l
thresholds to each of the processors. We assume that if (u ,v) eE, W (u ,v )=0. The
size of M is defined to be the number of processors, and the weight of M is

703

704

Obradovic and Parberry

The processors of a k-ary neural network are relatively limited in computing power.
A k-ary function is a function f :Z:~Z". Let F; denote the set of all n-input k-ary
functions. Define e::R,,+Ir;-l~F; by e:(w l .....w".h It .???h''_l):R;~Z,,. where

.

e;(w It ???? w" .h h???.h,,-l)(X 1o... ,%.. )=i iff hi ~~Wi xi <h; +1?
i=1

The set of k-ary weighted multilinear threshold functions is the union. over all n e N.
of the range of e;. Each processor of a k-ary neural network can compute a k-ary
weighted multilinear threshold function of its inputs.
Each processor can be in one of k states, 0 through k-l. Initially. the input processors of M are placed into states which encode the input If processor v was updated
during interval t, its state at time t -1 was i and output was j. then at time t its state
will be j. A k-ary neural network computes by having the processors change state until a stable configuration is reached. The output of M are the states of the output processors after a stable state has been reached. A neural network M 2 is said to be f (t )equivalent to M 1 iff for all inputs x. for every computation of M 1 on input x which
terminates in time t there is a computation of M 2 on input x which terminates in time
f (t) with the same output. A neural network M 2 is said to be equivalent to M 1 iff it
is t -equivalent to it.

3 ANALOG NEURAL NETWORKS
Let f be a function with range [0.1]. Any limited-precision device which purports to
compute f must actually compute some function with range the k rational values

R"={ilk-llieZ,,,~<k} (for some keN). This is sufficient for all practical purposes
provided k is large enough. Since R" is isomorphic to Z". we will formally define
the limited precision variant of f to be the function f" :X ~Z" defined by
f,,(x)=round(j (x).(k-l?, where round:R~N is the natural rounding function defined
by round(x)=n iff n-o.5~<n-tO.5.

Theorem 3.1 : Letf(Wlo ... ,w.. ):R"~[O,I] where WieR for

1~~.

be defined by

.
f (w1O.?.,W,,)(X 10 .?? ,x.. )=g (LWiXi)
i=l

where g:R~[O,I] is monotone increasing and invertible. Then f(Wlo ... ,W.. )":R"~Z,,
is a k-ary weighted multilinear threshold function.
Proof: It is easy to verify that f(Wlo ...?W")"=S;(Wl' ... ,w",hl, ...?h,,_l)' where
hi =g-1?2i-l)/2(k-l?. 0
Thus we see that analog neural networks with limited precision are essentially k-ary
neural networks.

Analog Neural Networks of Limited Precision I

4 CANONICAL THRESHOLDS
Binary neural networks have the advantage that all thresholds can be taken equal to
zero (see. for example. Theorem 4.3.1 of Parberry, 1990). A similar result holds for
ternary neural networks.
Theorem 4.1 : For every n-input ternary weighted multilinear threshold function there
is an equivalent (n +I)-input ternary weighted multilinear threshold function with
threshold values equal to zero and one.
Proof: Suppose W=(W1o ??? ,WII )E R", hloh2E R. Without loss of generality assume
h l<h 2.
Define W=(Wl ?...?wlI+l)e RII+I by wj=wjl(hrh 1) for I~!0t, and
wlI +I=-h I/(h2-h 1). It can be demonstrated by a simple case analysis that for all
x =(x 1, ??? ,xll)e

Z;.

8;(w,h l,hz)(x )=8;+I(W ,0,I)(x l,... ,xll ,1).

o
The choice of threshold values in Theorem 4.1 was arbitrary. Unfortunately there is
no canonical set of thresholds for k >3.
Theorem 4.2 : For every k>3, n~2, m~. h 1o ??? ,hk - 1E R. there exists an n-input k-ary
weighted multilinear threshold function

such that for all (n +m )-input k-ary weighted multilinear threshold functions

8 k"+m("
WI.???

)?zm+1I
.WII+m. h 10???. hk-l'
k
~Z k
A

Proof (Sketch): Suppose that t I ?.. . .tk-l e R is a canonical set of thresholds. and w.t.o.g.
assume n =2. Let h =(h 1o ??? ,hk - 1), where h l=h z=2. h j=4, hi =5 for 4Si <k. and
f=8i(1,I.h).
By hypothesis there exist wlo ????wm+2 and y=(ylo ...?ym)eRm such that for all xeZi,

f (x )=8r+2(w 1.? .. ,Wm+2,t 1, ??? ,tk-l)(X ,y).
m

Let S= I:Wi+2Yi. Since f (1.0)=0. f (0.1)=0, f (2,1)=2, f (1,2)=2. it follows that
;=1

2(Wl+Wz+S )<tl+t 3.
Since f (2,0)=2, f (1.1 )=2. and f (0.2)=2, it follows that

(1)

70S

706

Obradovic and Pdrberry

Wl+W2+S~2?

(2)

2t2<ll+13.

(3)

Inequalities (1) and (2) imply that

By similar arguments from g=S;(1,l,l.3.3.4 ?...?4) we can conclude that
(4)

But (4) contradicts (3). 0

S NETWORKS OF BOUNDED WEIGHT
Although our model allows each weight to take on an infinite number of possible
values. there are only a finite number of threshold functions (since there are only a
finite number of k-ary functions) with a fixed number of inputs. Thus the number of
n -input threshold functions is bounded above by some function in n and k. In fact.
something stronger can be shown. All weights can be made integral. and
o ((n +k) log (n +k? bits are sufficient to describe each one.
Theorem 5.1 : For every k-ary neural network M 1 of size z there exists an equivalent
k-ary neural network M2 of size z and weight ((k_l)/2)Z(z+I)(z+k)'2+0(1) with integer
weights.
Proof (Sketch): It is sufficient to prove that for every weighted threshold function
f:(Wlt ...?wll.hh ...?h"-I):Z:~Z,, for some neN. there is an equivalent we1f.hted threshold function g:(w~ ?...? w:.hi ?...? h;-d such that Iwtl~((k-l)/2)I(n+l)'" )12+0(1) for
l~i~. By extending the techniques used by Muroga. Toda and Takasu (1961) in the
binary case. we see that the weights are bounded above by the maximum determinant
of a matrix of dimension n +k -lover Z". 0
Thus if k is bounded above by a polynomial in n. we are guaranteed of being able to
describe the weights using a polynomial number of bits.

6 THRESHOLD CIRCUITS
A k-ary neural network with weights drawn from {?1} is said to have unit weights. A
unit-weight directed acyclic k-ary neural network is called a k-ary threshold circuit.
A k-ary threshold circuit can be divided into layers. with each layer receiving inputs
only from the layers above it. The depth of a k-ary threshold circuit is defined to be
the number of layers. The weight is equal to the number of edges. which is bounded
above by the square of the size. Despite the apparent handicap of limited weights. kary threshold circuits are surprisingly powerful.
Much interest has focussed on the computation of symmetric functions by neural networks. motivated by the fact that the visual system appears to be able to recognize objects regardless of their position on the retina A function f :Z:~Z" is called symmetric if its output remains the same no matter how the input is permuted.

Analog Neural Networks of Limited Precision I

Theorem 6.1 : Any symmetric k-ary function on n inputs can be computed by a k-ary
threshold circuit of depth 6 and size (n+1)k-l/(k-2)!+ o (kn).
Proof: Omitted. 0
It has been noted many times that neural networks can compute any Boolean function
in constant depth. The same is true of k-ary neural networks, although both results
appear to require exponential size for many interesting functions.

Theorem 6.2 : Any k-ary function of n inputs can be computed by a k-ary threshold
circuit with size (2n+1)k"+k+1 and depth 4.
Proof: Similar to that for k=2 (see Chandra et. al., 1984; Parberry, 1990). 0
The interesting problem remaining is to determine which functions require exponential
size to achieve constant depth, and which can be computed in polynomial size and
constant depth. We will now consider the problem of adding integers represented in
k-ary notation.

Theorem 6.3 : The sum of two k-ary integers of size n can be computed by a k-ary
threshold circuit with size 0 (n 2) and depth 5.
Proof: First compute the carry of x and y in 'luadratic size and depth 3 using the standard elementary school algorithm. Then the it position of the result can be computed
from the i tit position of the operands and a carry propagated in that position in constant size and depth 2. 0
Theorem 6.4 : The sum of n k-~ integers of size n can be computed by a k-ary
threshold circuit with size 0 (n 3+kn ) and constant depth.
Proof: Similar to the proof for k=2 using Theorem 6.3 (see Chandra et. al., 1984; Parberry, 1990). 0
Theorem 6.S : For every k-ary neural network M 1 of size z there exists an 0 (t)equivalent unit-weight k-ary neural network M2 of size o ((z+k)410g3(z+k?.
Proof: By Theorem 5.1 we can bound all weights to have size 0 ((z+k)log(z+k? in
binary notation. By Theorem 6.4 we can replace every processor with non-unit
weights by a threshold circuit of size o ((z+k)310g3(z+k? and constant depth. 0
Theorem 6.5 implies that we can assume unit weights by increasing the size by a polynomial and the running time by only a constant multiple provided the number of
logic levels is bounded above by a polynomial in the size of the network. The
number of thresholds can also be reduced to one if the size is increased by a larger
polynomial:

Theorem 6.6 : For every k-ary neural network M 1 of size z there exists an 0 (t )equivalent unit-weight binary neural network M 2 of size 0 (z 4k 4)(log z + log k)3
which outputs the binary encoding of the required result
Proof: Similar to the proof of Theorem 6.5. 0
This result is primarily of theoretical interest. Binary neural networks appear simpler,
and hence more desirable than analog neural networks. However, analog neural networks are actually more desirable since they are easier to build. With this in mind,
Theorem 6.6 simply serves as a limit to the functions that an analog neural network

707

708

Obradovic and Parberry

can be expected to compute efficiently. We are more concerned with constructing a
model of the computational abilities of neural networks, rather than a model of their
implementation details.

7 NONMONOTONE MULTILINEAR NEURAL NETWORKS
Olafsson and Abu-Mostafa (1988) study
f(Wlt ... ,wl):R"-+B for w;ER, 1~~, where

f

information

capacity

of functions

II
(Wlt.. ??WII)(X1 ?... , xlI)=g (~W;X;)
;=1

and g is the alternating threshold function g (h loh2.....hk-1):R-+B for some monotone
increasing h;ER, 1~<k, defined by g(x)=O if h2i~<h2i+1 for some ~5:nI2. We
will call f an alternating weighted multilinear threshold function, and a neural network constructed from functions of this form alternating multilinear neural networks.
Alternating multilinear neural networks are closely related to k-ary neural networks:
Theorem 7.1 : For every k-ary neural network of size z and weight w there is an
equivalent alternating multilinear neural network of size z log k and weight
(k -l)w log (k -1) which produces the output of the former in binary notation.
Proof (Sketch): Each k-ary gate is replaced by log k gates which together essentially
perform a "binary search" to determine each bit of the k-ary gate. Weights which increase exponentially are used to provide the correct output value. 0
Theorem 7.2 : For every alternating multilinear neural network of size z and weight
w there is a 3t-equivalent k-ary neural network of size 4z and weight w+4z.
Proof (Sketch): Without loss of generality. assume k is odd. Each alternating gate is
replaced by a k-ary gate with identical weights and thresholds. The output of this gate
goes with weight one to a k-ary gate with thresholds 1,3,S ?... ,k-1 and with weight
minus one to a k-ary gate with thresholds -(k-1), ... ,-3,-1. The output of these gates
goes to a binary gate with threshold k. 0
Both k-ary and alternating multilinear neural networks are a special case of nonmonotone multilinear neural networks, where g :R-+R is the defined by g (x )=Ci iff
hi~<h;+lt for some monotone increasing h;ER, 1~<k, and co, ... ,Ck-1EZk. Nonmonotone neural networks correspond to analog neural networks whose output function is not necessarily monotone nondecreasing. Many of the result of this paper, including Theorems 5.1, 6.5, and 6.6, also apply to nonmonotone neural networks. The
size, weight and running time of many of the upper-bounds can also be improved by a
small amount by using nonmonotone neural networks instead of k-ary ones. The details are left to the interested reader.

8 MUL TILINEAR HOPFIELD NETWORKS
A multilinear version of the Hopfield network called the quantized neural network has
been studied by Fleisher (1987). Using the terminology of (parberry, 1990), a quantized neural network is a simple symmetric k-ary neural network (that is, its interconnection pattern is an undirected graph without self-loops) with the additional property
that all processors have an identical set of thresholds. Although the latter assumption

Analog Neural Networks of Limited Precision I

is reasonable for binary neural networks (see, for example, Theorem 4.3.1 of Parberry,
1990), and ternary neural networks (Theorem 4.1), it is not necessarily so for k-ary
neural networks with k>3 (Theorem 4.2). However, it is easy to extend Fleisher's
main result to give the following:
Theorem 8.1 : Any productive sequential computation of a simple symmetric k-ary
neural network will converge.

9 CONCLUSION
It has been shown that analog neural networks with limited precision are essentially
k-ary neural networks. If k is limited to a polynomial, then polynomial size, constant
depth k-ary neural networks are equivalent to polynomial size, constant depth binary
neural networks. Nonetheless, the savings in time (at most a constant multiple) and
hardware (at most a polynomial) arising from using k-ary neural networks rather than
binary ones can be quite significant. We do not suggest that one should actually construct binary or k-ary neural networks. Analog neural networks can be constructed by
exploiting the analog behaviour of transistors, rather than using extra hardware to inhibit it Rather, we suggest that k-ary neural networks are a tool for reasoning about the
behaviour of analog neural networks.
Acknowledgements
The financial support of the Air Force Office of Scientific Research, Air Force S ysterns Command, DSAF, under grant numbers AFOSR 87-0400 and AFOSR 89-0168
and NSF grant CCR-8801659 to Ian Parberry is gratefully acknowledged.
References
Chandra A. K., Stockmeyer L. J. and Vishkin D., (1984) "Constant depth reducibility,"
SIAM 1. Comput., vol. 13, no. 2, pp. 423-439.
Fleisher M., (1987) "The Hopfield model with multi-level neurons," Proc. IEEE
Conference on Neural Information Processing Systems, pp. 278-289, Denver, CO.
Muroga S., Toda 1. and Takasu S., (1961) "Theory of majority decision elements," 1.
Franklin Inst., vol. 271., pp. 376-418.
Obradovic Z. and Parberry 1., (1989a) "Analog neural networks of limited precision I:
Computing with multilinear threshold functions (preliminary version)," Technical Report CS-89-14, Dept of Computer Science, Penn. State Dniv.
Obradovic Z. and Parberry I., (1989b) "Analog neural networks of limited precision II:
Learning with multilinear threshold functions (preliminary version)," Technical Report
CS-89-15, Dept. of Computer Science, Penn. State Dniv.
Olafsson S. and Abu-Mostafa Y. S., (1988) "The capacity of multilevel threshold functions," IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 10, no. 2, pp.
277-281.
Parberry I., (To Appear in 1990) "A Primer on the Complexity Theory of Neural Networks," in A Sourcebook of Formal Methods in Artificial Intelligence, ed. R. Banerji,
North-Holland.

709


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 2111-computing-time-lower-bounds-for-recurrent-sigmoidal-neural-networks.pdf

Computing Time Lower Bounds for
Recurrent Sigmoidal Neural Networks

Michael Schmitt
Lehrstuhl Mathematik und Informatik, Fakultat fUr Mathematik
Ruhr-Universitat Bochum, D- 44780 Bochum, Germany
mschmitt@lmi.ruhr-uni-bochum.de

Abstract
Recurrent neural networks of analog units are computers for realvalued functions. We study the time complexity of real computation in general recurrent neural networks. These have sigmoidal,
linear, and product units of unlimited order as nodes and no restrictions on the weights. For networks operating in discrete time,
we exhibit a family of functions with arbitrarily high complexity,
and we derive almost tight bounds on the time required to compute
these functions. Thus, evidence is given of the computational limitations that time-bounded analog recurrent neural networks are
subject to.

1

Introduction

Analog recurrent neural networks are known to have computational capabilities that
exceed those of classical Turing machines (see, e.g., Siegelmann and Sontag, 1995;
Kilian and Siegelmann, 1996; Siegelmann, 1999). Very little, however, is known
about their limitations. Among the rare results in this direction, for instance,
is the one of Sima and Orponen (2001) showing that continuous-time Hopfield
networks may require exponential time before converging to a stable state. This
bound, however, is expressed in terms of the size of the network and, hence, does
not apply to fixed-size networks with a given number of nodes. Other bounds
on the computational power of analog recurrent networks have been established by
Maass and Orponen (1998) and Maass and Sontag (1999). They show that discretetime recurrent neural networks recognize only a subset of the regular languages in
the presence of noise. This model of computation in recurrent networks, however,
receives its inputs as sequences. Therefore, computing time is not an issue since
the network halts when the input sequence terminates. Analog recurrent neural
networks, however, can also be run as "real" computers that get as input a vector
of real numbers and, after computing for a while, yield a real output value. No
results are available thus far regarding the time complexity of analog recurrent
neural networks with given size.
We investigate here the time complexity of discrete-time recurrent neural networks
that compute functions over the reals. As network nodes we allow sigmoidal units,
linear units, and product units- that is, monomials where the exponents are ad-

justable weights (Durbin and Rumelhart, 1989) . We study the complexity of real
computation in the sense of Blum et aI. (1998). That means, we consider real numbers as entities that are represented exactly and processed without restricting their
precision. Moreover, we do not assume that the information content of the network
weights is bounded (as done, e.g., in the works of Balcazar et aI. , 1997; Gavalda and
Siegelmann, 1999). With such a general type of network, the question arises which
functions can be computed with a given number of nodes and a limited amount of
time. In the following , we exhibit a family of real-valued functions ft, l 2: 1, in one
variable that is computed by some fixed size network in time O(l). Our main result
is, then, showing that every recurrent neural network computing the functions ft
requires at least time nW /4). Thus, we obtain almost tight time bounds for real
computation in recurrent neural networks.

2

Analog Computation in Recurrent Neural Networks

We study a very comprehensive type of discrete-time recurrent neural network that
we call general recurrent neural network (see Figure 1). For every k, n E N there is
a recurrent neural architecture consisting of k computation nodes YI , . . . , Yk and n
input nodes Xl , ... , x n . The size of a network is defined to be the number ofits computation nodes. The computation nodes form a fully connected recurrent network.
Every computation node also receives connections from every input node. The input
nodes play the role of the input variables of the system. All connections are parameterized by real-valued adjustable weights. There are three types of computation
nodes: product units, sigmoidal units, and linear units. Assume that computation
node i has connections from computation nodes weighted by Wil, ... ,Wi k and from
input nodes weighted by ViI, .. . ,Vi n. Let YI (t) , . . . ,Yk (t) and Xl (t), ... ,X n (t) be the
values of the computation nodes and input nodes at time t, respectively. If node i
is a product unit, it computes at time t + 1 the value

(1)
that is, after weighting them exponentially, the incoming values are multiplied.
Sigmoidal and linear units have an additional parameter associated with them, the
threshold or bias ()i . A sigmoidal unit computes the value

where (J is the standard sigmoid (J( z ) = 1/ (1
simply outputs the weighted sum

+ e- Z ).

If node i is a linear unit, it

We allow the networks to be heterogeneous, that is, they may contain all three types
of computation nodes simultaneously. Thus, this model encompasses a wide class of
network types considered in research and applications. For instance, architectures
have been proposed that include a second layer of linear computation nodes which
have no recurrent connections to computation nodes but serve as output nodes (see,
e.g. , Koiran and Sontag, 1998; Haykin, 1999; Siegelmann, 1999). It is clear that in
the definition given here, the linear units can function as these output nodes if the
weights of the outgoing connections are set to O. Also very common is the use
of sigmoidal units with higher-order as computation nodes in recurrent networks
(see, e.g., Omlin and Giles, 1996; Gavalda and Siegelmann, 1999; Carrasco et aI.,
2000). Obviously, the model here includes these higher-order networks as a special
case since the computation of a higher-order sigmoidal unit can be simulated by
first computing the higher-order terms using product units and then passing their

.

I

I

sigmoidal, product, and linear units

computation
nodes

.

Yl

Yk

t
input nodes

Xl

Xn

I

Figure 1: A general recurrent neural network of size k. Any computation node may
serve as output node.

outputs to a sigmoidal unit. Product units , however, are even more powerful than
higher-order terms since they allow to perform division operations using negative
weights. Moreover, if a negative input value is weighted by a non-integer weight,
the output of a product unit may be a complex number. We shall ensure here that
all computations are real-valued. Since we are mainly interested in lower bounds,
however, these bounds obviously remain valid if the computations of the networks
are extended to the complex domain.
We now define what it means that a recurrent neural network N computes a function
f : ~n --+ llt Assume that N has n input nodes and let x E ~n. Given tE N,
we say that N computes f(x) in t steps if after initializing at time 0 the input
nodes with x and the computation nodes with some fixed values, and performing t
computation steps as defined in Equations (1) , (2) , and (3) , one of the computation
nodes yields the value f(x). We assume that the input nodes remain unchanged
during the computation. We further say that N computes f in time t if for every
x E ~n , network N computes f in at most t steps. Note that t may depend
on f but must be independent of the input vector. We emphasize that this is
a very general definition of analog computation in recurrent neural networks. In
particular, we do not specify any definite output node but allow the output to occur
at any node. Moreover, it is not even required that the network reaches a stable
state, as with attractor or Hopfield networks. It is sufficient that the output value
appears at some point of the trajectory the network performs. A similar view of
computation in recurrent networks is captured in a model proposed by Maass et al.
(2001). Clearly, the lower bounds remain valid for more restrictive definitions of
analog computation that require output nodes or stable states. Moreover, they
hold for architectures that have no input nodes but receive their inputs as initial
values of the computation nodes. Thus, the bounds serve as lower bounds also for
the transition times between real-valued states of discrete-time dynamical systems
comprising the networks considered here.
Our main tool of investigation is the Vapnik-Chervonenkis dimension of neural
networks. It is defined as follows (see also Anthony and Bartlett, 1999): A dichotomy
of a set S ~ ~n is a partition of S into two disjoint subsets (So , Sd satisfying
So U S1 = S. A class :F of functions mapping ~n to {O, I} is said to shatter S if
for every dichotomy (So , Sd of S there is some f E :F that satisfies f(So) ~ {O}
and f(S1) ~ {I}. The Vapnik-Chervonenkis (VC) dimension of :F is defined as

4"'+4",IL
'I

-1---Y-2----Y-5~1

S~

output

Y5

Y4
Figure 2: A recurrent neural network computing the functions fl in time 2l

+ 1.

the largest number m such that there is a set of m elements shattered by F. A
neural network given in terms of an architecture represents a class of functions
obtained by assigning real numbers to all its adjustable parameters, that is, weights
and thresholds or a subset thereof. The output of the network is assumed to be
thresholded at some fixed constant so that the output values are binary. The VC
dimension of a neural network is then defined as the VC dimension of the class of
functions computed by this network.

In deriving lower bounds in the next section, we make use of the following result
on networks with product and sigmoidal units that has been previously established
(Schmitt, 2002). We emphasize that the only constraint on the parameters of the
product units is that they yield real-valued, that is, not complex-valued, functions.
This means further that the statement holds for networks of arbitrary order, that is,
it does not impose any restrictions on the magnitude of the weights of the product
units.
Proposition 1. (Schmitt, 2002, Theorem 2) Suppose N is a feedforward neural
network consisting of sigmoidal, product, and linear units. Let k be its size and W
the number of adjustable weights. The VC dimension of N restricted to real-valued
functions is at most 4(Wk)2 + 20Wk log(36Wk).

3

Bounds on Computing Time

We establish bounds on the time required by recurrent neural networks for computing a family of functions fl : JR -+ JR, l 2:: 1, where l can be considered as a measure
of the complexity of fl. Specifically, fl is defined in terms of a dynamical system as
the lth iterate of the logistic map ?>(x) = 4x(1 - x), that is,
fl(X)

{

= 1,

?>(x)

l

?>(fl- l (x))

l > 2.

We observe that there is a single recurrent network capable of computing every fl
in time O(l).
Lemma 2. There is a general recurrent neural network that computes fl in time
2l + 1 for every l.
Proof. The network is shown in Figure 2. It consists of linear and second-order
units. All computation nodes are initialized with 0, except Yl, which starts with 1
and outputs 0 during all following steps. The purpose of Yl is to let the input x

output

Figure 3: Network Nt.

enter node Y2 at time 1 and keep it away at later times. Clearly, the value fl (x)
results at node Y5 after 2l + 1 steps.
D
The network used for computing fl requires only linear and second-order units. The
following result shows that the established upper bound is asymptotically almost
tight, with a gap only of order four . Moreover, the lower bound holds for networks
of unrestricted order and with sigmoidal units.
Theorem 3. Every general recurrent neural network of size k requires at least time
cl l / 4 j k to compute function fl' where c> 0 is some constant.

Proof. The idea is to construct higher-order networks Nt of small size that have
comparatively large VC dimension. Such a network will consist of linear and product
units and hypothetical units that compute functions fJ for certain values of j. We
shall derive a lower bound on the VC dimension of these networks. Assuming that
the hypothetical units can be replaced by time-bounded general recurrent networks,
we determine an upper bound on the VC dimension of the resulting networks in
terms of size and computing time using an idea from Koiran and Sontag (1998) and
Proposition 1. The comparison of the lower and upper VC dimension bounds will
give an estimate of the time required for computing k
Network Nt, shown in Figure 3, is a feedforward network composed of three networks
? r(1) , JVI
? r(2) , JVI
.r(3) . E ach networ k JVI
? r(/1) ,J.L = 1, 2, 3 , h as l ?lnput no d es Xl'
(/1) .. . , x I(/1)
JVI
and 2l + 2 computation nodes yb/1), ... , Y~r~l (see Figure 4). There is only one
adjustable parameter in Nt, denoted w, all other weights are fixed. The computation
nodes are defined as follows (omitting time parameter t):
for J.L

= 3,

for J.L = 1,2,

y~/1)

fll'--1 (Y~~)l) for i = 1, ... ,l and J.L = 1,2,3,

y}~{

y~/1) . x~/1), for i = 1, .. . ,l and

(/1)
Y21+l

(/1)
YIH

+ ... + Y21(/1)

J.L = 1,2,3,

c
- 1 2 3
lor
J.L , , ?

The nodes Yb/1) can be considered as additional input nodes for N//1), where N;(3)
gets this input from w, and N;(/1) from N;(/1+l) for J.L = 1,2. Node Y~r~l is the
output node of N;(/1), and node Y~~~l is also the output node of Nt. Thus, the entire
network has 3l + 6 nodes that are linear or product units and 3l nodes that compute
functions h, fl' or f12.

output

8

r - - - - - - - - - - - - '.....L - - - - - - - - - - - ,

I

I

B

B

t

t

I x~p)1

~

----t
input:

w

or

output of N;(P+1)

Figure 4: Network N;(p).

We show that Ni shatters some set of cardinality [3, in particular, the set S = ({ ei :
i = 1, . .. , [})3, where ei E {O, 1}1 is the unit vector with a 1 in position i and
elsewhere. Every dichotomy of S can be programmed into the network parameter
w using the following fact about the logistic function ? (see Koiran and Sontag,
1998, Lemma 2): For every binary vector b E {O, l}m, b = b1 .?. bm , there is some
real number w E [0,1] such that for i = 1, ... , m

?

E

{

[0,1 /2)

if bi = 0,

(1/2,1]

if bi = 1.

Hence, for every dichotomy (So, Sd of S the parameter w can be chosen such that
every (ei1' ei2 , ei3) E S satisfies
1/2 if (eillei2,eis) E So,
1/2 if (eillei2,eiJ E S1.
Since h +i2 H i 3 .12 (w) = ?i1 (?i2'1 (?i3 .1 2(w))), this is the value computed by Ni on
input (eill ei2' ei3), where ei" is the input given to network N;(p). (Input ei" selects
the function li"'I,,-1 in N;(p).) Hence, S is shattered by Ni, implying that Ni has
VC dimension at least [3.

Assume now that Ii can be computed by a general recurrent neural network of size
at most kj in time tj. Using an idea of Koiran and Sontag (1998), we unfold the
network to obtain a feedforward network of size at most kjtj computing fj. Thus we
can replace the nodes computing ft, ft, fl2 in Nz by networks of size k1t1, kltl, k12t12,
respectively, such that we have a feedforward network
consisting of sigmoidal,
product, and linear units. Since there are 3l units in Nl computing ft, ft, or fl2
and at most 3l + 6 product and linear units, the size of Nt is at most c1lkl2tl2
for some constant C1 > O. Using that Nt has one adjustable weight, we get from
Proposition 1 that its VC dimension is at most c2l2kr2tr2 for some constant C2 > o.
On the other hand, since Nz and Nt both shatter S, the VC dimension of Nt is at
least l3. Hence, l3 ~ C2l2 kr2 tr2 holds, which implies that tl2 2: cl 1/ 2/ kl2 for some
c > 0, and hence tl 2: cl 1/ 4/ kl .
D

'!J

Lemma 2 shows that a single recurrent network is capable of computing every
function fl in time O(l). The following consequence of Theorem 3 establishes that
this bound cannot be much improved.
Corollary 4. Every general recurrent neural network requires at least time 0(ll /4 )

to compute the functions fl.

4

Conclusions and Perspectives

We have established bounds on the computing time of analog recurrent neural
networks. The result shows that for every network of given size there are functions
of arbitrarily high time complexity. This fact does not rely on a bound on the
magnitude of weights. We have derived upper and lower bounds that are rather
tight- with a polynomial gap of order four- and hold for the computation of a
specific family of real-valued functions in one variable. Interestingly, the upper
bound is shown using second-order networks without sigmoidal units, whereas the
lower bound is valid even for networks with sigmoidal units and arbitrary product
units. This indicates that adding these units might decrease the computing time
only marginally. The derivation made use of an upper bound on the VC dimension
of higher-order sigmoidal networks. This bound is not known to be optimal. Any
future improvement will therefore lead to a better lower bound on the computing
time.
We have focussed on product and sigmoidal units as nonlinear computing elements.
However, the construction presented here is generic. Thus, it is possible to derive
similar results for radial basis function units, models of spiking neurons, and other
unit types that are known to yield networks with bounded VC dimension. The
questions whether such results can be obtained for continuous-time networks and for
networks operating in the domain of complex numbers, are challenging. A further
assumption made here is that the networks compute the functions exactly. By a
more detailed analysis and using the fact that the shattering of sets requires the
outputs only to lie below or above some threshold, similar results can be obtained
for networks that approximate the functions more or less closely and for networks
that are subject to noise.
Acknowledgment

The author gratefully acknowledges funding from the Deutsche Forschungsgemeinschaft (DFG). This work was also supported in part by the ESPRIT Working Group
in Neural and Computational Learning II, NeuroCOLT2, No. 27150.

References
Anthony, M. and Bartlett, P. L. (1999). Neural Network Learning: Theoretical
Foundations. Cambridge University Press, Cambridge.
Balcazar, J. , Gavalda, R., and Siegelmann, H. T. (1997). Computational power of
neural networks: A characterization in terms of Kolmogorov complexity. IEEE
Transcations on Information Theory, 43: 1175- 1183.
Blum, L., Cucker, F. , Shub, M. , and Smale, S. (1998) . Complexity and Real Computation. Springer-Verlag, New York.
Carrasco, R. C., Forcada, M. L., Valdes-Munoz, M. A. , and Neco, R. P. (2000).
Stable encoding of finite state machines in discrete-time recurrent neural nets
with sigmoid units. Neural Computation, 12:2129- 2174.
Durbin, R. and Rumelhart, D. (1989). Product units: A computationally powerful and biologically plausible extension to backpropagation networks. Neural
Computation, 1:133- 142.
Gavalda, R. and Siegelmann, H. T . (1999) . Discontinuities in recurrent neural
networks. Neural Computation, 11:715- 745.
Haykin, S. (1999). Neural Networks : A Comprehensive Foundation. Prentice Hall,
Upper Saddle River, NJ , second edition.
Kilian, J. and Siegelmann, H. T. (1996). The dynamic universality of sigmoidal
neural networks. Information and Computation, 128:48- 56.
Koiran, P. and Sontag, E . D. (1998). Vapnik-Chervonenkis dimension of recurrent
neural networks. Discrete Applied Mathematics, 86:63- 79.
Maass, W., NatschUiger, T., and Markram, H. (2001). Real-time computing without
stable states: A new framework for neural computation based on perturbations.
Preprint.
Maass, W. and Orponen, P. (1998). On the effect of analog noise in discrete-time
analog computations. Neural Computation, 10:1071- 1095.
Maass, W. and Sontag, E . D. (1999). Analog neural nets with Gaussian or other
common noise distributions cannot recognize arbitrary regular languages. Neural
Computation, 11:771- 782.
amlin, C. W. and Giles, C. L. (1996). Constructing deterministic finite-state automata in recurrent neural networks. Journal of the Association for Computing
Machinery, 43:937- 972.
Schmitt, M. (2002). On the complexity of computing and learning with multiplicative neural networks. Neural Computation, 14. In press.
Siegelmann, H. T . (1999). Neural Networks and Analog Computation: Beyond the
Turing Limit. Progress in Theoretical Computer Science. Birkhiiuser, Boston.
Siegelmann, H. T. and Sontag, E. D. (1995). On the computational power of neural
nets. Journal of Computer and System Sciences, 50:132- 150.
Sima, J. and Orponen, P. (2001). Exponential transients in continuous-time
symmetric Hopfield nets. In Dorffner, G., Bischof, H. , and Hornik, K. , editors , Proceedings of the International Conference on Artificial Neural Networks
ICANN 2001, volume 2130 of Lecture Notes in Computer Science, pages 806- 813,
Springer, Berlin.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 211-a-large-scale-neural-network-which-recognizes-handwritten-kanji-characters.pdf

A Large-Scale Neural Network

A LARGE-SCALE NEURAL NETWORK
WHICH RECOGNIZES HANDWRITTEN
KANJI CHARACTERS
Yoshihiro Mori
Kazuki Joe
ATR Auditory and Visual Perception Research Laboratories
Sanpeidani Inuidani Seika-cho Soraku-gun Kyoto 619-02 Japan

ABSTRACT
We propose a new way to construct a large-scale neural network for
3.000 handwritten Kanji characters recognition. This neural network
consists of 3 parts: a collection of small-scale networks which are
trained individually on a small number of Kanji characters; a network
which integrates the output from the small-scale networks, and a
process to facilitate the integration of these neworks. The recognition
rate of the total system is comparable with those of the small-scale
networks. Our results indicate that the proposed method is effective for
constructing a large-scale network without loss of recognition
performance.

1 INTRODUCTION
Neural networks have been applied to recognition tasks in many fields. with good results
[Denker, 1988][Mori,1988][Weideman, 1989]. They have performed better than
conventional methods. However these networks currently operate with only a few
categories, about 20 to 30. The Japanese writing system at present is composed of about
3,000 characters. For a network to recognize this many characters, it must be given a
large number of categories while maintaining its level of performance.
To train small-scale neural networks is not a difficult task. Therefore. exploring methods
for integrating these small-scale neural networks is important to construct a large-scale
network. If such methods could integrate small-scale networks without loss of the
performance, the scale of neural networks would be extended dramatically. In this paper,
we propose such a method for constructing a large-scale network whose object is to
recognize 3,000 handwritten Kanji characters, and report the result of a part of this
network. This method is not limited to systems for character recognition, and can be
applied to any system which recognizes many categories.

2 STRATEGIES FOR A LARGE-SCALE NETWORK
Knowing the current recognition and generalization capacity of a neural network. we
realized that constructing a large-scale monolithic network would not be efficient or

415

416

Mori and Joe
effective. Instead, from the start we decided on a building blocks approach
[Mori,1988] [Waibel,1988]. There are two strategies to mix many small-scale networks.

2.1 Selective Neural Network (SNN)
In this strategy, a large-scale neural network is made from many small-scale networks
which are trained individually on a small number of categories, and a network (SNN)
which selects the appropriate small-scale network (Fig. I). The advantage of this strategy
is that the information passed to a selected small-scale networks is always appropriate for
that network. Therefore, training these small-scale networks is very easy. But on the
other hand, increasing the number of categories will substantially increase the training
time of the SNN, and may make it harder for the SNN to retain high perfonnance.
Furthennore, the error rate of the SNN will limit the perfonnance of the whole system.

2.2 Integrative Neural Network (INN)
In this strategy, a large-scale neural network is made from many small-scale networks
which are trained individually on a small number of categories. and a network (INN)
which integrates the output from these small-scale networks(Fig. 2). The advantage of
this strategy is that every small-scale network gets information and contributes to finding
the right answer. Therefore, it is possible to use the knowledge distributed among each
small-scale network. But in some respects. various devices are needed to make the
integration easier.
The common advantage with both strategies just mentioned is that the size of each neural
network is relatively small, and it does not take a long time to train these networks. Each
small-scale networks is considered an independent part of the whole system. Therefore,
retraining these networks (to improve the performance of the whole system) will not take
too long.
~__....
O,utput

Sub
Net
1

? ?
Neural Network
(Selection Type)

':1U:U/W:::::::/:::/E::::::::.

: : Suspending
/ Network

(:::::{::::::::::::::::.:::::::: ~

Fig. 1 SNN Strategy

A Large-Scale Neural Network

Output
Neural Network
(Integration Type)

? ?
Fig. 2 INN Strategy

3 STRUCTURE OF LARGE-SCALE NETWORK
The whole system is constructed using three kinds of neural networks. The ftrst one,
called a SubNet, is an ordinary three layered feed forward type neural network trained
using the Back Propagation learning algorithm. The second kind of network is called a
SuperNet. This neural network makes its decision by integrating the outputs from all the
SubNets. This network is also a 3-layered feed-forward net, but is larger than the Subnets.
The last network, which we call an OtherFilter, is devised to improve the integration of
the S uperNet. This OtherFilter network was designed using the L VQ algorithm
[Khonen,1988]. There are also some changes made in the BP learning algorithm
especially for pattern recognition [Joe,1989].
We decided that, based on the time it takes for learning, there should be 9 categories in
each small-scale network. The 3,000 characters are separated into these small groups
through the K-means clustering method, which allows similar characters to be grouped
together. The separation occurs in two stages. First, 11 groups of 270 characters each are
formed, then each group is separated into 30 smaller units. In this way, 330 groups of 9
characters each are obtained. We choose the INN strategy to use distributed knowledge to
full advantage. The 9-character units are SubNets, which are integrated in 2 stages. First
30 SubNets are integrated by a higher level network SuperNet. Altogether, 11 SuperNets
are needed to recognize all 3,000 characters. SuperNets are in turn integrated by a higher
level network, the HyperNet. More precisely, the role and structure of these kinds of
networks are as follows:

3.1 SubNet
A feature vector extracted from handwritten patterns is used as the input (described in
Section 4.1). The number of units in the output layer is the same as the number of
categories to be recognized by the SubNet. In short, the role of a SubNet is to output the
similarity between the input pattern and the categories allotted to the SubNet. (Fig. 3)

3.2 SuperNet
The outputs from each SubNet fIltered by the OtherFilter network are used as the input to

417

418

Mori and Joe

the SuperNet. The number of units in an output layer is the same as the number of
SubNets belonging to a SuperNet. In shortt the role of SuperNet is to select the SubNet
which covers the category corresponding to the input patterns. (Fig. 5)

Output

Horizontal

+45?diagonal

Vertical

Original Pattern

Fig. 3 S ubNet

3.3 OtherFIIter
45(9x5) reference vectors are assigned to each SubNet. LVQ is used to adapt these
reference vectors t so that each input vector has a reference of the correct SubNet as its
closest reference vector. The
OtherFilter method is to frrst measure
the distance between all the reference
d
vectors and one input vector. The
mean distance and normal deviation of
distance are calculated. The distance
between a S ubNet and an input vector
is defmed to be the smallest distance
of that SubNet's reference vectors to
? References
the input vector .

?

?

XInput Vector

Fig4. Shape of OtherFilter

?

f(xo}=l 1(1+ e (x n-M+2d)/Cd) (1)
Xn : The Distance of Nth SubNet
M : The Mean of Xn
d : The Variance of Xn
C : Constant

A Large-Scale Neural Network

This distance modified by equation (1) is multiplied by the outputs of the SubNet. and fed
into the SuperNet. The outputs of SubNets whose distance is greater than the mean
distance are suppressed. and the outputs of SubNets whose distance is smaller than the
mean distance are amplified. In this way. the outputs of SubNets are modified to improve
the integration of the higher level SuperNet. (Fig. 5)

HyperNet 1
SuperNet 11
SubNet 330
OtherFilter 12

Other-Filter

FigS. Outline of the Whole System

4 RECOGNITION EXPERIMENT
4.1 TRAINING PATTERN
The training samples for this network were chosen from a database of about 3000 Kanji
characters [Saito 1985]. For each character. there are 200 handwritten samples from
different writers. 100 are used as training samples. and the remaining 100 are used to test
recognition accuracy of the trained network. All samples in the database consist of 64 by
63 dots.

419

420

Mori and Joe

JlQ
~
~~

~

.J-~

~~lJ
~~

~ ~
~

,~

~~

~
~~
-V'#f)

~

~

.orfffi

~~

J..~

~i2

O~

~

DI~J

o/N{

Fig. 6 Examples of training pattern

4.2 LDCD FEATURE
If we were to use this pattern as the input to our neural net, the number of units required
in the input layer would be too large for the computational abilities of current computers.
Therefore, a feature vector extracted from the handwritten patterns is used as the input. In
the "LDCD feature" [Hagita 1983], there are 256 dimensions computing a line segment
length along four directions: horizontal, vertical, and two diagonals in the 8 by 8 squares
into which the handwritten samples are divided.

t"
:61

o

horizontal
component

Fig 7. LDCD Feature
4.3 RECOGNITION RESULTS
In the work reported here, one SuperNet, 30 SubNets and one OtherFilter were
constructed for recognition experiments. SubNets were trained until the recognition of
training samples reaches at least 99%. With these SubNets, the mean recognition rate of
test patterns was 92%. This recognition rate is higher than that of conventional methods.
A SuperNet which integrates the output modified by OtherFilter from 30 trained SubNets

A Large-Scale Neural Network

was then constructed. The number of units in the input layer of the SuperNet was 270.
This SuperNet was trained until the performance of training samples becomes at least
93%. With this SuperNet, the recognition rate of test patterns was 74%, though that of
OtherFilter was 72%. The recognition rate of a system without the OtherFilter of test
patterns was 55%.

5 CONCLUSION
We have here proposed a new way of constructing a large-scale neural network for the
recognition of 3,000 handwritten Kanji characters. With this method, a system
recognizing 270 Kanji characters was constructed. This system will become a part of a
system recognizing 3,000 Kanji characters. Only a modest training time was necessary
owing to the modular nature of the system. Moreover, this modularity means that only a
modest re-training time is necessary for retraining an erroneous neural network in the
whole system. The overall system performance can be improved by retraining just that
neural network, and there is no need to retrain the whole system. However, the
performance of the OtherFilter is not satisfactory. We intend to improve the OtherFilter,
and build a large-scale network for the recognition of 3,000 handwritten Kanji characters
by the method reported here.

Acknowledgments
We are grateful to Dr. Yodogawa for his support and encouragement. Special thanks to
Dr. Sei Miyake for the ideas he provided in our many discussions. The authors would like
to acknowledge, with thanks, the help of Erik McDermott for his valuable assistance in
writing this paper in English.

References
[Denier, 1988]
l.S.Denker, W.R.Gardner, H.P. Graf, D.Henderson, R.E. Howard,
W.Hubbard, L.DJackel. H.S.Baird, I.Guyon : "Neural Network Recognizer for HandWritten ZIP Code Digits", NEURAL INFORMATION PROCESSING SYSTEMS 1.
pp.323-331, Morgan Kaufmann. 1988
[Mori,1988]
Y.Mori. K.Yokosawa : "Neural Networks that Learn to Discriminate
Similar Kanji Characters". NEURAL INFORMATION PROCESSING SYSTEMS 1,
pp.332-339, Morgan Kaufmann. 1988
[Weideman.1989]W.E.Weideman. M.T.Manry. H.C.Yau ; tI A COMPARISON OF A
NEAREST NEIGHBOR CLASSIFIER AND A NEURAL NETWORK FOR NUMERIC
HANDPRINT CHARACTER RECOGNITION". UCNN89(Washington), VoLl, pp.117120, June 1989

421

422

Mori and Joe

Alex Waibel, "Consonant Recognition by Modular Construction of
[Waibel, 1988]
Large Phonemic Time-Delay Neural Networks", NEURAL INFORMATION
PROCESSING SYSTEMS 1, pp.215-223, Morgan Kaufmann, 1988
[Joo,1989]
KJoo, Y.Mori, S.Miyake : "Simulation of a Large-Scale Neural
Networks on a Parallel Computer", 4th Hypercube Concurrent Computers,1989
[Khonen,1988] T.Kohonen, G.Barna, R.Chrisley : "Statistical Pattern Recognition
with Neural Networks", IEEE, Proc.of ICNN, YoU, pp.61-68, July 1988
[Saito,1985]
T.Saito, H.Yamada, K.Yamamoto : "On the Data Base ETL9 of
Handprinted Characters in 1IS Chinese Characters and Its Analysis", J68-D, 4, 757-764,
1985
[Hagita,1983]
N.Hagita, S.Naito, I.Masuda : "Recognition of Handprinted Chinese
Characters by Global and Local Direction Contributivity Density-Feature", J66-D, 6,
722-729,1983


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 1175-generating-accurate-and-diverse-members-of-a-neural-network-ensemble.pdf

Generating Accurate and Diverse
Members of a Neural-Network Ensemble
David w. Opitz
Computer Science Department
University of Minnesota
Duluth, MN 55812
opitz@d.umn.edu

Jude W. Shavlik
Computer Sciences Department
University of Wisconsin
Madison, WI 53706
shavlik@cs.wisc.edu

Abstract
Neural-network ensembles have been shown to be very accurate
classification techniques. Previous work has shown that an effective ensemble should consist of networks that are not only highly
correct, but ones that make their errors on different parts of the
input space as well. Most existing techniques, however, only indirectly address the problem of creating such a set of networks.
In this paper we present a technique called ADDEMUP that uses
genetic algorithms to directly search for an accurate and diverse
set of trained networks. ADDEMUP works by first creating an initial population, then uses genetic operators to continually create
new networks, keeping the set of networks that are as accurate as
possible while disagreeing with each other as much as possible. Experiments on three DNA problems show that ADDEMUP is able to
generate a set of trained networks that is more accurate than several existing approaches. Experiments also show that ADDEMUP
is able to effectively incorporate prior knowledge, if available, to
improve the quality of its ensemble.

1

Introduction

Many researchers have shown that simply combining the output of many classifiers
can generate more accurate predictions than that of any of the individual classifiers (Clemen, 1989; Wolpert, 1992). In particular, combining separately trained
neural networks (commonly referred to as a neural-network ensemble) has been
demonstrated to be particularly successful (Alpaydin, 1993; Drucker et al., 1994;
Hansen and Salamon, 1990; Hashem et al., 1994; Krogh and Vedelsby, 1995;
Maclin and Shavlik, 1995; Perrone, 1992). Both theoretical (Hansen and Salamon, 1990; Krogh and Vedelsby, 1995) and empirical (Hashem et al., 1994;

D. W. OPITZ, J. W. SHA VLIK

536

Maclin and Shavlik, 1995) work has shown that a good ensemble is one where
the individual networks are both accurate and make their errors on different parts
of the input space; however, most previous work has either focussed on combining
the output of multiple trained networks or only indirectly addressed how we should
generate a good set of networks. We present an algorithm, ADDEMUP (Accurate
anD Diverse Ensemble-Maker giving United Predictions), that uses genetic algorithms to generate a population of neural networks that are highly accurate, while
at the same time having minimal overlap on where they make their error.
Thaditional ensemble techniques generate their networks by randomly trying different topologies, initial weight settings, parameters settings, or use only a part of the
training set in the hopes of producing networks that disagree on where they make
their errors (we henceforth refer to diversity as the measure of this disagreement).
We propose instead to actively search for a good set of networks. The key idea behind our approach is to consider many networks and keep a subset of the networks
that minimizes our objective function consisting of both an accuracy and a diversity
term. In many domains we care more about generalization performance than we
do about generating a solution quickly. This, coupled with the fact that computing
power is rapidly growing, motivates us to effectively utilize available CPU cycles by
continually considering networks to possibly place in our ensemble.
proceeds by first creating an initial set of networks, then continually
produces new individuals by using the genetic operators of crossover and mutation.
It defines the overall fitness of an individual to be a combination of accuracy and
diversity. Thus ADDEMUP keeps as its population a set of highly fit individuals that
will be highly accurate, while making their mistakes in a different part of the input
space. Also, it actively tries to generate good candidates by emphasizing the current
population's erroneous examples during backpropagation training. Experiments
reported herein demonstrate that ADDEMUP is able to generate an effective set of
networks for an ensemble.
ADDEMUP

2

The Importance of an Accurate and Diverse Ensemble

Figure 1 illustrates the basic framework of a neural-network ensemble. Each network
in the ensemble (network 1 through network N in this case) is first trained using
the training instances. Then, for each example, the predicted output of each of
these networks (Oi in Figure 1) is combined to produce the output of the ensemble
(0 in Figure 1). Many researchers (Alpaydin, 1993; Hashem et al., 1994; Krogh
and Vedelsby, 1995; Mani, 1991) have demonstrated the effectiveness of combining
schemes that are simply the weighted average of the networks (Le., 0 = L:iEN Wi ?Oi
and L:iEN Wi = 1), and this is the type of ensemble we focus on in this paper.
Hansen and Salamon (1990) proved that for a neural-network ensemble, if the average error rate for a pattern is less than 50% and the networks in the ensemble are
independent in the production of their errors, the expected error for that pattern
can be reduced to zero as the number of networks combined goes to infinity; however, such assumptions rarely hold in practice. Krogh and Vedelsby (1995) later
proved that if diversity! Di of network i is measured by:
Di = I)Oi(X) -

o(xW,

(1)

x

then the ensemble generalization error

CE) consists of two distinct portions:

E = E - D,
1

Krogh and Vedelsby referred to this term as ambiguity.

(2)

Generating Accurate and Diverse Members of a Neural-network Ensemble

537

"o
?? ensemble output

InW$lllnW$21-lnW$NI

n

~

1j

??? input
Figure 1: A neural-network ensemble.
where [) = Li Wi? Di and E = Li Wi? Ei (Ei is the error rate of network i and the
Wi'S sum to 1). What the equation shows then, is that we want our ensemble to
consist of highly correct networks that disagree as much as possible. Creating such
a set of networks is the focus of this paper.

3

The ADDEMUP Algorithm

Table 1 summarizes our new algorithm, ADDEMUP, that uses genetic algorithms
to generate a set of neural networks that are accurate and diverse in their classifications. (Although ADDEMUP currently uses neural networks, it could be easily
extended to incorporate other types of learning algorithms as well.) ADDEMUP
starts by creating and training its initial population of networks. It then creates
new networks by using standard genetic operators, such as crossover and mutation.
ADDEMUP trains these new individuals, emphasizing examples that are misclassified
by the current population, as explained below. ADDEMUP adds these new networks
to the population then scores each population members with the fitness function :
Fitnessi = AccuracYi + A DiversitYi = (1 - E i ) + A D i ,

(3)

where A defines the tradeoff between accuracy and diversity. Finally, ADDEMUP
prunes the population to the N most-fit members, which it defines to be its current
ensemble, then repeats this process.
We define our accuracy term, 1 - E i , to be network i's validation-set accuracy (or
training-set accuracy if a validation set is not used), and we use Equation lover
this validation set to calculate our diversity term Di . We then separately normalize
each term so that the values range from 0 to 1. Normalizing both terms allows A to
have the same meaning across domains. Since it is not always clear at what value
one should set A, we have therefore developed some rules for automatically setting
A. First, we never change A if the ensemble error E is decreasing while we consider
new networks; otl.!erwise we change A if one of following two things happen: (1)
population error E is not increasing and the population diversity D is decreasing;
diversity seems to be under-emphasized and we increase A, or (2) E is increasing
and [) is not decreasing; diversity seems to be over-emphasized and we decrease A.
(We started A at 0.1 for the results in this paper.)
A useful network to add to an ensemble is one that correctly classifies as many
examples as possible while making its mistakes primarily on examples that most

D. W. OPITZ. 1. W. SHA VLIK

538

Table 1: The

ADDEMUP

algorithm.

GOAL: Genetically create an accurate and diverse ensemble of networks.
1. Create and train the initial population of networks.
2. Until a stopping criterion is reached:
(a) Use genetic operators to create new networks.
(b) Thain the new networks using Equation 4 and add them to the population.
(c) Measure the diversity of each network with respect to the current population (see Equation 1).
(d) Normalize the accuracy scores and the diversity scores of the individual
networks.
(e) Calculate the fitness of each population member (see Equation 3).
(f) Prune the population to the N fittest networks.
(g) Adjust oX (see the text for an explanation).
(h) Report the current population of networks as the ensemble. Combine
the output of the networks according to Equation 5.

of the current population members correctly classify. We address this during backpropagation training by multiplying the usual cost function by a term that measures
the combined population error on that example:
Cost =

L
kET

..2.-

It(k)

~O(k)I>-'+l

[t(k) -a(kW,

(4)

E

where t(k) is the target and a(k) is the network activation for example k in the
training set T. Notice that since our network is not yet a member of the ensemble,
o(k) and E are not dependent on our network; our new term is thus a constant when
calculating the derivatives during backpropagation. We normalize t(k) -o(k) by the
ensemble error E so that the average value of our new term is around 1 regardless of
the correctness of the ensemble. This is especially important with highly accurate
populations, since tk - o(k) will be close to 0 for most examples, and the network
would only get trained on a few examples. The exponent A~l represents the ratio
of importance of the diversity term in the fitness function. For instance, if oX is close
to 0, diversity is not considered important and the network is trained with the usual
cost function; however, if oX is large, diversity is considered important and our new
term in the cost function takes on more importance.
We combine the predictions of the networks by taking a weighted sum of the output
of each network, where each weight is based on the validation-set accuracy of the
network. Thus we define our weights for combining the networks as follows:

(5)
While simply averaging the outputs generates a good composite model (Clemen,
1989), we include the predicted accuracy in our weights since one should believe
accurate models more than inaccurate ones.

Generating Accurate and Diverse Members of a Neural-network Ensemble

4

539

Experimental Study

The genetic algorithm we use for generating new network topologies is the REGENT algorithm (Opitz and Shavlik, 1994). REGENT uses genetic algorithms
to search through the space of knowledge-based neural network (KNN) topologies. KNNs are networks whose topologies are determined as a result of the
direct mapping of a set of background rules that represent what we currently
know about our task. KBANN (Towell and Shavlik, 1994), for instance, translates a set of propositional rules into a neural network, then refines the resulting network's weights using backpropagation. Thained KNNs, such as KBANN'S
networks, have been shown to frequently generalize better than many other
inductive-learning techniques such as standard neural networks (Opitz, 1995;
Towell and Shavlik, 1994). Using KNNs allows us to have highly correct networks
in our ensemble; however, since each network in our ensemble is initialized with the
same set of domain-specific rules, we do not expect there to be much disagreement
among the networks. An alternative we consider in our experiments is to randomly
generate our initial population of network topologies, since domain-specific rules
are sometimes not available.
We ran ADDEMUP on NYNEX's MAX problem set and on three problems from the
Human Genome Project that aid in locating genes in DNA sequences (recognizing
promoters, splice-junctions, and ribosome-binding sites - RBS). Each of these domains is accompanied by a set of approximately correct rules describing what is
currently known about the task (see Opitz, 1995 or Opitz and Shavlik, 1994 for
more details). Our experiments measure the test-set error of ADDEMUP on these
tasks. Each ensemble consists of 20 networks, and the REGENT and ADDEMUP
algorithms considered 250 networks during their genetic search.
Table 2a presents the results from the case where the learners randomly create
the topology of their networks (Le., they do not use the domain-specific knowledge). Table 2a's first row, best-network, results from a single-layer neural network where, for each fold, we trained 20 networks containing between 0 and 100
(uniformly) hidden nodes and used a validation set to choose the best network. The
next row, bagging, contains the results of running Breiman's (1994) bagging algorithm on standard, single-hidden-Iayer networks, where the number of hidden nodes
is randomly set between 0 and 100 for each network. 2 Bagging is a "bootstrap"
ensemble method that trains each network in the ensemble with a different partition
of the training set. It generates each partition by randomly drawing, with replacement, N examples from the training set, where N is the size of the training set.
Breiman (1994) showed that bagging is effective on "unstable" learning algorithms,
such as neural networks, where small changes in the training set result in large
changes in predictions. The bottom row of Table 2a, AOOEMUP, contains the results
of a run of ADDEMUP where its initial population (of size 20) is randomly generated.
The results show that on these domains combining the output of mUltiple trained
networks generalizes better than trying to pick the single-best network.
While the top table shows the power of neural-network ensembles, Table 2b demonstrates ADDEMUP'S ability to utilize prior knowledge. The first row of Table 2b
contains the generalization results of the KBANN algorithm, while the next row,
KBANN-bagging, contains the results of the ensemble where each individual network in the ensemble is the KBANN network trained on a different partition of the
training set. Even though each of these networks start with the same topology and
2We also tried other ensemble approaches, such as randomly creating varying multilayer network topologies and initial weight settings, but bagging did significantly better
on all datasets (by 15-25% on all three DNA domains).

D. W. OPITZ. J. W. SHA VLlK

540

Table 2: Test-set error from a ten-fold cross validation. Table (a) shows the results
from running three learners without the domain-specific knowledge; Table (b) shows
the results of running three learners with this knowledge. Pairwise, one-tailed t-tests
indicate that AOOEMUP in Table (b) differs from the other algorithms in both tables
at the 95% confidence level, except with REGENT in the splice-junction domain.

I

Standard neural networks (no domain-specific knowledge used)
best-network
bagging
AOOEMUP

Promoters
6.6%
4.6%
4.6%

Splice Junction
7.8%
4.5%
4.9%

RBS
10.7%
9.5%
9.0%

I

MAX

37.0%
35.7%
34.9%

(a)
?Knowledge-based neural networks (domain-specific knowledge used)
KBANN
KBANN-bagging
REGENT-Combined
AOOEMUP

Promoters
6.2%
4.2%
3.9%
2.9%

Splice Junction
5.3%
4.5%
3.9%
3.6%

RBS
9.4%
8.5%
8.2%
7.5%

MAX

35.8%
35.6%
35.6%
34.7%

(b)

"large" initial weight settings (Le., the weights resulting from the domain-specific
knowledge), small changes in the training set still produce significant changes in
predictions. Also notice that on all datasets, KBANN-bagging is as good as or better
than running bagging on randomly generated networks (Le., bagging in Table 2a).
The next row, REGENT-Combined, contains the results of simply combining, using
Equation 5, the networks in REGENT'S final population. AOOEMUP, the final row of
Table 2b, mainly differs from REGENT-Combined in two ways: (a) its fitness function
(Le., Equation 3) takes into account diversity rather than just network accuracy, and
(b) it trains new networks by emphasizing the erroneous examples of the current
ensemble. Therefore, comparing AOOEMUP with REGENT-Combined helps directly
test ADDEMUP'S diversity-achieving heuristics, though additional results reported in
Opitz (1995) show ADDEMUP gets most of its improvement from its fitness function.
There are two main reasons why we think the results of ADDEMUP in Table 2b are
especially encouraging: (a) by comparing ADDEMUP with REGENT-Combined, we
explicitly test the quality of our heuristics and demonstrate their effectiveness, and
(b) ADDEMUP is able to effectively utilize background knowledge to decrease the
error of the individual networks in its ensemble, while still being able to create
enough diversity among them so as to improve the overall quality of the ensemble.

5

Conclusions

Previous work with neural-network ensembles have shown them to be an effective
technique if the classifiers in the ensemble are both highly correct and disagree
with each other as much as possible. Our new algorithm, ADDEMUP, uses genetic
algorithms to search for a correct and diverse population of neural networks to be
used in the ensemble. It does this by collecting the set of networks that best fits an
objective function that measures both the accuracy of the network and the disagreement of that network with respect to the other members of the set. ADDEMUP tries

Generating Accurate and Diverse Members of a Neural-network Ensemble

541

to actively generate quality networks during its search by emphasizing the current
ensemble's erroneous examples during backpropagation training.
Experiments demonstrate that our method is able to find an effective set of networks for our ensemble. Experiments also show that ADDEMUP is able to effectively
incorporate prior knowledge, if available, to improve the quality of this ensemble.
In fact, when using domain-specific rules, our algorithm showed statistically significant improvements over (a) the single best network seen during the search, (b) a
previously proposed ensemble method called bagging (Breiman, 1994), and (c) a
similar algorithm whose objective function is simply the validation-set correctness
of the network. In summary, ADDEMUP is successful in generating a set of neural
networks that work well together in producing an accurate prediction.
Acknowledgements
This work was supported by Office of Naval Research grant N00014-93-1-0998.

References
Alpaydin, E. (1993). Multiple networks for function learning. In Proceedings of the 1993
IEEE International Conference on Neural Networks, vol I, pages 27-32, San Fransisco.
Breiman, L. (1994). Bagging predictors. Technical Report 421, Department of Statistics,
University of California, Berkeley.
Clemen, R. (1989). Combining forecasts: A review and annotated bibliography. International Journal of Forecasting, 5:559-583.
Drucker, H., Cortes, C., Jackel, L., LeCun, Y., and Vapnik, V. (1994). Boosting and other
machine learning algorithms. In Proceedings of the Eleventh International Conference on
Machine Learning, pages 53-61, New Brunswick, NJ. Morgan Kaufmann.
Hansen, L. and Salamon, P. (1990). Neural network ensembles. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 12:993-100l.
Hashem, S., Schmeiser, B., and Yih, Y. (1994). Optimal linear combinations of neural
networks: An overview. In Proceedings of the 1994 IEEE International Conference on
Neural Networks, Orlando, FL.
Krogh, A. and Vedelsby, J. (1995). Neural network ensembles, cross validation, and
active learning. In Tesauro, G., Touretzky, D., and Leen, T., editors, Advances in Neural
Information Processing Systems, vol 7, Cambridge, MA. MIT Press.
Maclin, R. and Shavlik, J. (1995). Combining the predictions of multiple classifiers:
Using competitive learning to initialize neural networks. In Proceedings of the Fourteenth
International Joint Conference on Artificial Intelligence, Montreal, Canada.
Mani, G. (1991). Lowering variance of decisions by using artificial neural network portfolios. Neural Computation, 3:484-486.
Opitz, D. (1995). An Anytime Approach to Connectionist Theory Refinement: Refining
the Topologies of Knowledge-Based Neural Networks. PhD thesis, Computer Sciences
Department, University of Wisconsin, Madison, WI.
Opitz, D. and Shavlik, J. (1994). Using genetic search to refine knowledge-based neural
networks. In Proceedings of the Eleventh International Conference on Machine Learning,
pages 208-216, New Brunswick, NJ. Morgan Kaufmann.
Perrone, M. (1992). A soft-competitive splitting rule for adaptive tree-structured neural
networks. In Proceedings of the International Joint Conference on Neural Networks, pages
689-693, Baltimore, MD.
Towell, G. and Shavlik, J. (1994). Knowledge-based artificial neural networks. Artificial
Intelligence, 70(1,2):119- 165.
Wolpert, D. (1992). Stacked generalization. Neural Networks, 5:241- 259.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 1692-lower-bounds-on-the-complexity-of-approximating-continuous-functions-by-sigmoidal-neural-networks.pdf

Lower Bounds on the Complexity of
Approximating Continuous Functions by
Sigmoidal Neural Networks

Michael Schmitt
Lehrstuhl Mathematik und Informatik
FakuWit ftir Mathematik
Ruhr-Universitat Bochum
D-44780 Bochum, Germany
mschmitt@lmi.ruhr-uni-bochum.de

Abstract
We calculate lower bounds on the size of sigmoidal neural networks
that approximate continuous functions. In particular, we show
that for the approximation of polynomials the network size has
to grow as O((logk)1/4) where k is the degree of the polynomials.
This bound is valid for any input dimension, i.e. independently of
the number of variables. The result is obtained by introducing a
new method employing upper bounds on the Vapnik-Chervonenkis
dimension for proving lower bounds on the size of networks that
approximate continuous functions.

1

Introduction

Sigmoidal neural networks are known to be universal approximators. This is one of
the theoretical results most frequently cited to justify the use of sigmoidal neural
networks in applications. By this statement one refers to the fact that sigmoidal
neural networks have been shown to be able to approximate any continuous function
arbitrarily well. Numerous results in the literature have established variants of
this universal approximation property by considering distinct function classes to be
approximated by network architectures using different types of neural activation
functions with respect to various approximation criteria, see for instance [1, 2, 3, 5,
6, 11, 12, 14, 15]. (See in particular Scarselli and Tsoi [15] for a recent survey and
further references.)
All these results and many others not referenced here, some of them being constructive, some being merely existence proofs, provide upper bounds for the network size
asserting that good approximation is possible if there are sufficiently many network nodes available. This, however, is only a partial answer to the question that
mainly arises in practical applications: "Given some function, how many network
nodes are needed to approximate it?" Not much attention has been focused on
establishing lower bounds on the network size and, in particular, for the approximation of functions over the reals. As far as the computation of binary-valued

Complexity ofApproximating Continuous Functions by Neural Networks

329

functions by sigmoidal networks is concerned (where the output value of a network
is thresholded to yield 0 or 1) there are a few results in this direction. For a specific Boolean function Koiran [9] showed that networks using the standard sigmoid
u(y) = 1/(1 + e- Y ) as activation function must have size O(nl/4) where n is the
number of inputs. (When measuring network size we do not count the input nodes
here and in what follows.) Maass [13] established a larger lower bound by constructing a binary-valued function over IRn and showing that standard sigmoidal networks
require O(n) many network nodes for computing this function. The first work on
the complexity of sigmoidal networks for approximating continuous functions is due
to DasGupta and Schnitger [4]. They showed that the standard sigmoid in network
nodes can be replaced by other types of activation functions without increasing the
size of the network by more than a polynomial. This yields indirect lower bounds
for the size of sigmoidal networks in terms of other network types. DasGupta and
Schnitger [4] also claimed the size bound AO(I/d) for sigmoidal networks with d
layers approximating the function sin(Ax).
In this paper we consider the problem of using the standard sigmoid u(y) =
1/(1 + e- Y ) in neural networks for the approximation of polynomials. We show

that at least O?logk)1/4) network nodes are required to approximate polynomials
of degree k with small error in the loo norm. This bound is valid for arbitrary input
dimension, i.e., it does not depend on the number of variables. (Lower bounds can
also be obtained from the results on binary-valued functions mentioned above by
interpolating the corresponding functions by polynomials. This, however, requires
growing input dimension and does not yield a lower bound in terms of the degree.)
Further, the bound established here holds for networks of any number of layers. As
far as we know this is the first lower bound result for the approximation of polynomials. From the computational point of view this is a very simple class of functions;
they can be computed using the basic operations addition and multiplication only.
Polynomials also play an important role in approximation theory since they are
dense in the class of continuous functions and some approximation results for neural networks rely on the approximability of polynomials by sigmoidal networks (see,
e.g., [2, 15]).
We obtain the result by introducing a new method that employs upper bounds on
the Vapnik-Chervonenkis dimension of neural networks to establish lower bounds
on the network size. The first use of the Vapnik-Chervonenkis dimension to obtain
a lower bound is due to Koiran [9] who calculated the above-mentioned bound
on the size of sigmoidal networks for a Boolean function. Koiran's method was
further developed and extended by Maass [13] using a similar argument but another
combinatorial dimension. Both papers derived lower bounds for the computation
of binary-valued functions (Koiran [9] for inputs from {O, 1}n, Maass [13] for inputs
from IRn). Here, we present a new technique to show that and how lower bounds can
be obtained for networks that approximate continuous functions. It rests on two
fundamental results about the Vapnik-Chervonenkis dimension of neural networks.
On the one hand, we use constructions provided by Koiran and Sontag [10] to build
networks that have large Vapnik-Chervonenkis dimension and consist of gates that
compute certain arithmetic functions. On the other hand, we follow the lines of
reasoning of Karpinski and Macintyre [7] to derive an upper bound for the VapnikChervonenkis dimension of these networks from the estimates of Khovanskil [8] and
a result due to Warren [16].
In the following section we give the definitions of sigmoidal networks and the VapnikChervonenkis dimension. Then we present the lower bound result for function
approximation. Finally, we conclude with some discussion and open questions.

330

2

M Schmitt

Sigmoidal Neural Networks and VC Dimension

We briefly recall the definitions of a sigmoidal neural network and the VapnikChervonenkis dimension (see, e.g., [7, 10]). We consider /eed/orward neural networks
which have a certain number of input nodes and one output node. The nodes
which are not input nodes are called computation nodes and associated with each
of them is a real number t, the threshold. Further, each edge is labelled with a
real number W called weight. Computation in the network takes place as follows:
The input values are assigned to the input nodes. Each computation node applies
the standard sigmoid u(y) = 1/(1 + e- V ) to the sum W1Xl + ... + WrXr - t where
Xl, .?. ,X r are the values computed by the node's predecessors, WI, ??? ,W r are the
weights of the corresponding edges, and t is the threshold. The output value of the
network is defined to be the value computed by the output node. As it is common
for approximation results by means of neural networks, we assume that the output
node is a linear gate, i.e., it just outputs the sum WIXI + ... + WrXr - t. (Clearly,
for computing functions on finite sets with output range [0, 1] the output node
may apply the standard sigmoid as well.) Since u is the only sigmoidal function
that we consider here we will refer to such networks as sigmoidal neural networks.
(Sigmoidal functions in general need to satisfy much weaker assumptions than u
does.) The definition naturally generalizes to networks employing other types of
gates that we will make use of (e.g. linear, multiplication, and division gates).
The Vapnik-Chervonenkis dimension is a combinatorial dimension of a function class
and is defined as follows: A dichotomy of a set S ~ IRn is a partition of S into two
disjoint subsets (So, Sl) such that So U SI = S. Given a set F offunctions mapping
IRn to {O, I} and a dichotomy (So, Sd of S, we say that F induces the dichotomy
(So, Sd on S if there is some f E F such that /(So) ~ {O} and f(Sd ~ {I}.
We say further that F shatters S if F induces all dichotomies on S. The VapnikChervonenkis (VC) dimension of F, denoted VCdim(F), is defined as the largest
number m such that there is a set of m elements that is shattered by F. We refer
to the VC dimension of a neural network, which is given in terms of a "feedforward
architecture", i.e. a directed acyclic graph, as the VC dimension of the class of
functions obtained by assigning real numbers to all its programmable parameters,
which are in general the weights and thresholds of the network or a subset thereof.
Further, we assume that the output value of the network is thresholded at 1/2 to
obtain binary values.

3

Lower Bounds on Network Size

Before we present the lower bound on the size of sigmoidal networks required for
the approximation of polynomials we first give a brief outline of the proof idea.
We will define a sequence of univariate polynomials (Pn)n>l by means of which
we show how to construct neural architectures N n consistmg of various types of
gates such as linear, multiplication, and division gates, and, in particular, gates
that compute some of the polynomials. Further, this architecture has a single
weight as programmable parameter (all other weights and thresholds are fixed).
We then demonstrate that, assuming the gates computing the polynomials can be
approximated by sigmoidal neural networks sufficiently well, the architecture Nn
can shatter a certain set by assigning suitable values to its programmable weight.
The final step is to reason along the lines of Karpinski and Macintyre [7] to obtain
via Khovanskil's estimates [8] and Warren's result [16] an upper bound on the VC
dimension of N n in terms of the number of its computation nodes. (Note that we
cannot directly apply Theorem 7 of [7] since it does not deal with division gates.)
Comparing this bound with the cardinality of the shattered set we will then be able

331

Complexity ofApproximating Continuous Functions by Neural Networks

(3)

W

1

n

P3

(1)

(2)

W1

W1

(3)

Wi

(3)

W1

n

P2

(2)

Wj

(1)

Wk

(1)

(2)

Wn

Wn

n

P1

Wn

j --------------------------------~
k--------------------------------------------------~

Figure 1: The network N n with values k, j, i, 1 assigned to the input nodes
Xl, X2, X3, X4 respectively. The weight W is the only programmable parameter of
the network.

to conclude with a lower bound on the number of computation nodes in N n and
thus in the networks that approximate the polynomials.
Let the sequence (Pn)n2: l of polynomials over IR be inductively defined by
Pn(X) =

{ 4x(1 - x)

P(Pn-dx))

n = 1,
n 2:: 2 .

Clearly, this uniquely defines Pn for every n 2:: 1 and it can readily be seen that
Pn has degree 2n. The main lower bound result is made precise in the following
statement.
Theorem 1 Sigmoidal neural networks that approximate the polynomials (Pn)n >l
on the interval [0,1] with error at most O(2- n ) in the 100 norm must have at least
n(nl/4) computation nodes.
Proof. For each n a neural architecture N n can be constructed as follows: The
network has four input nodes Xl, X2, X3, X4. Figure 1 shows the network with input
values assigned to the input nodes in the order X4 = 1, X3 = i, X2 = j, Xl = k.
There is one weight which we consider as the (only) programmable parameter of
N n . It is associated with the edge outgoing from input node X4 and is denoted
by w. The computation nodes are partitioned into six levels as indicated by the
boxes in Figure 1. Each level is itself a network. Let us first assume, for the sake of
simplicity, that all computations over real numbers are exact. There are three levels
labeled with II, having n + 1 input nodes and one output node each, that compute
so-called projections 7r : IRnH -+ IR where 7r(YI,"" Yn, a) = Ya for a E {I, ... , n}.
The levels labeled P3 , P2 , PI have one input node and n output nodes each. Level
P3 receives the constant 1 as input and thus the value W which is the parameter of
the network. We define the output values of level P A for>. = 3,2, 1 by
(A)

wb

= Pbon"'-l ( v) ,

b= 1, ... ,n

where v denotes the input value to level P A. This value is equal to w for>. = 3 and
(A+l) , .?. , Wn()..+l) ,XA+l ) oth erWlse.
.
OUT
(A) can b
id
vve observe t h at wb+l
e calcu
ate f rom

7r (WI

332

M Schmitt

w~A) as Pn>'_l(W~A?). Therefore, the computations of level P A can be implemented
using n gates each of them computing the function Pn>.-l.
We show now that Nn can shatter a set of cardinality n 3 ? Let S = {I, ... ,n p. It
has been shown in Lemma 2 of [10] that for each (/31 , ... , /3r) E {O, 1Y there exists
some W E [0,1] such that for q = 1, ... ,T
pq(w) E [0,1/2)

if /3q

= 0,

and pq(w) E (1/2,1]

if /3q

= 1.

This implies that, for each dichotomy (So, Sd of S there is some
that for every (i, j, k) E S
Pk (pj.n (Pi.n 2(w)))
Pk(Pj.n(Pi.n2(w)))

< 1/2
> 1/2

if
if

W

E [0,1] such

(i, j, k) E So ,
(i,j,k)ES1'

Note that Pk(Pj.n(Pi.n2 (w))) is the value computed by N n given input values k, j, i, 1.
Therefore, choosing a suitable value for w, which is the parameter of Nn , the network
can induce any dichotomy on S. In other words, S is shattered by Nn .

An such that
for each E > weights can be chosen for An such that the function in,? computed
by this network satisfies lim?~o in,?(Yl, ... ,Yn, a) = Ya. Moreover, this architecture
consists of O(n) computation nodes, which are linear, multiplication, and division
gates. (Note that the size of An does not depend on E.) Therefore, choosing E
sufficiently small, we can implement the projections 1r in N n by networks of O(n)
computation nodes such that the resulting network N~ still shatters S. Now in N~
we have O(n) computation nodes for implementing the three levels labeled II and
we have in each level P A a number of O(n) computation nodes for computing Pn>.-l,
respectively. Assume now that the computation nodes for Pn>.-l can be replaced
by sigmoidal networks such that on inputs from S and with the parameter values
defined above the resulting network N:: computes the same functions as N~. (Note
that the computation nodes for Pn>.-l have no programmable parameters.)

?

It has been shown in Lemma 1 of [10] that there is an architecture

N::.

We estimate the size of
According to Theorem 7 of Karpinski and Macintyre
[7] a sigmoidal neural network with I programmable parameters and m computation
nodes has VC dimension O((ml)2). We have to generalize this result slightly before
being able to apply it. It can readily be seen from the proof of Theorem 7 in [7] that
the result also holds if the network additionally contains linear and multiplication
gates. For division gates we can derive the same bound taking into account that for
a gate computing division, say x/y, we can introduce a defining equality x = z . Y
where z is a new variable. (See [7] for how to proceed.) Thus, we have that a
network with I programmable parameters and m computation nodes, which are
linear, multiplication, division, and sigmoidal gates, has VC dimension O((ml)2).
In particular, if m is the number of computation nodes of N::, the VC dimension
can shatter a set
is O(m 2 ). On the other hand, as we have shown above,
of cardinality n 3 ? Since there are O(n) sigmoidal networks in
computing the
functions Pn>.-l, and since the number of linear, multiplication, and division gates
is bounded by O(n), for some value of A a single network computing Pn>.-l must
have size at least O(fo). This yields a lower bound of O(nl/4) for the size of a
sigmoidal network computing Pn.

N::
N::

Thus far, we have assumed that the polynomials Pn are computed exactly. Since
polynomials are continuous functions and since we require them to be calculated
only on a finite set of input values (those resulting from S and from the parameter
values chosen for w to shatter S) an approximation of these polynomials is sufficient.
A straightforward analysis, based on the fact that the output value of the network
has a "tolerance" close to 1/2, shows that if Pn is approximated with error O(2- n )

Complexity ofApproximating Continuous Functions by Neural Networks

333

in the loo norm, the resulting network still shatters the set S. This completes the
proof of the theorem.
D
The statement of the previous theorem is restricted to the approximation of polynomials on the input domain [0,1]. However, the result immediately generalizes to
any arbitrary interval in llt Moreover, it remains valid for multivariate polynomials
of arbitrary input dimension.
Corollary 2 The approximation of polynomials of degree k by sigmoidal neural
networks with approximation error O(ljk) in the 100 norm requires networks of size
O((log k)1/4). This holds for polynomials over any number of variables.

4

Conclusions and Open Questions

We have established lower bounds on the size of sigmoidal networks for the approximation of continuous functions. In particular, for a concrete class of polynomials
we have calculated a lower bound in terms of the degree of the polynomials. The
main result already holds for the approximation of univariate polynomials. Intuitively, approximation of multivariate polynomials seems to become harder when
the dimension increases. Therefore, it would be interesting to have lower bounds
both in terms of the degree and the input dimension.
Further, in our result the approximation error and the degree are coupled. Naturally,
one would expect that the number of nodes has to grow for each fixed function when
the error decreases. At present we do not know of any such lower bound.
We have not aimed at calculating the constants in the bounds. For practical applications such values are indispensable. Refining our method and using tighter results
it should be straightforward to obtain such numbers. Further, we expect that better
lower bounds can be obtained by considering networks of restricted depth.
To establish the result we have introduced a new method for deriving lower bounds
on network sizes. One of the main arguments is to use the functions to be approximated to construct networks with large VC dimension. The method seems suitable
to obtain bounds also for the approximation of other types of functions as long as
they are computationally powerful enough.
Moreover, the method could be adapted to obtain lower bounds also for networks
using other activation functions (e.g. more general sigmoidal functions, ridge functions, radial basis functions). This may lead to new separation results for the
approximation capabilities of different types of neural networks. In order for this
to be accomplished, however, an essential requirement is that small upper bounds
can be calculated for the VC dimension of such networks.
Acknowledgments
I thank Hans U. Simon for helpful discussions. This work was supported in part
by the ESPRIT Working Group in Neural and Computational Learning II, NeuroCOLT2, No. 27150.

References
[1] A. Barron. Universal approximation bounds for superposition of a sigmoidal
function. IEEE Transactions on Information Theory, 39:930--945, 1993.

334

M Schmitt

[2J C. K. Chui and X. Li. Approximation by ridge functions and neural networks
with one hidden layer. Journal of Approximation Theory, 70:131-141,1992.
[3J G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals, and Systems, 2:303-314, 1989.
[4J B. DasGupta and G. Schnitger. The power of approximating: A comparison
of activation functions. In C. L. Giles, S. J. Hanson, and J. D. Cowan, editors,
Advances in Neural Information Processing Systems 5, pages 615-622, Morgan
Kaufmann, San Mateo, CA, 1993.
[5] K. Hornik. Approximation capabilities of multilayer feedforward networks.
Neural Networks, 4:251-257, 1991.
[6] K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward networks
are universal approximators. Neural Networks, 2:359-366, 1989.
[7] M. Karpinski and A. Macintyre. Polynomial bounds for VC dimension of
sigmoidal and general Pfaffian neural networks. Journal of Computer and
System Sciences, 54:169-176, 1997.
[8] A. G. Khovanskil. Fewnomials, volume 88 of Translations of Mathematical
Monographs. American Mathematical Society, Providence, RI, 1991.
[9] P. Koiran. VC dimension in circuit complexity. In Proceedings of the 11th
Annual IEEE Conference on Computational Complexity CCC'96, pages 81-85,
IEEE Computer Society Press, Los Alamitos, CA, 1996.
[10] P. Koiran and E. D. Sontag. Neural networks with quadratic VC dimension.
Journal of Computer and System Sciences, 54:190-198, 1997.
[11] V. Y. Kreinovich. Arbitrary nonlinearity is sufficient to represent all functions
by neural networks: A theorem. Neural Networks, 4:381-383, 1991.
[12] M. Leshno, V. Y. Lin, A. Pinkus, and S. Schocken. Multilayer feedforward networks with a nonpolynomial activation function can approximate any function.
Neural Networks, 6:861-867, 1993.
[13] W. Maass. Noisy spiking neurons with temporal coding have more computational power than sigmoidal neurons. In M. Mozer, M. 1. Jordan, and
T. Petsche, editors, Advances in Neural Information Processing Systems 9,
pages 211-217. MIT Press, Cambridge, MA, 1997.
[14] H. Mhaskar. Neural networks for optimal approximation of smooth and analytic
functions. Neural Computation, 8:164-177, 1996.
[15J F. Scarselli and A. C. Tsoi. Universal approximation using feedforward neural
networks: A survey of some existing methods and some new results. Neural
Networks, 11:15-37, 1998.
[16] H. E. Warren. Lower bounds for approximation by nonlinear manifolds. Transactions of the American Mathematical Society, 133:167-178, 1968.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 1037-quadratic-type-lyapunov-functions-for-competitive-neural-networks-with-different-time-scales.pdf

Quadratic-Type Lyapunov Functions for
Competitive Neural Networks with
Different Time-Scales
Anke Meyer-Base
Institute of Technical Informatics
Technical University of Darmstadt
Darmstadt, Germany 64283

Abstract
The dynamics of complex neural networks modelling the selforganization process in cortical maps must include the aspects of
long and short-term memory. The behaviour of the network is such
characterized by an equation of neural activity as a fast phenomenon and an equation of synaptic modification as a slow part of the
neural system. We present a quadratic-type Lyapunov function for
the flow of a competitive neural system with fast and slow dynamic
variables. We also show the consequences of the stability analysis
on the neural net parameters.

1

INTRODUCTION

This paper investigates a special class of laterally inhibited neural networks. In
particular, we have examined the dynamics of a restricted class of laterally inhibited
neural networks from a rigorous analytic standpoint.
The network models for retinotopic and somatotopic cortical maps are usually composed of several layers of neurons from sensory receptors to cortical units, with
feedforward excitations between the layers and lateral (or recurrent) connection
within the layer. Standard techniques include (1) Hebbian rule and its variations
for modifying synaptic efficacies, (2) lateral inhibition for establishing topographical
organization of the cortex, and (3) adiabatic approximation in decoupling the dynamics of relaxation (which is on the fast time scale) and the dynamics of learning
(which is on the slow time scale) of the network . However, in most cases, only computer simulation results were obtained and therefore provided limited mathematical
understanding of the self-organizating neural response fields.
The networks under study model the dynamics of both the neural activity levels,

A. MEYER-BASE

338

the short-term memory (STM), and the dynamics of synaptic modifications, the
long-term memory (LTM). The actual network models under consideration may be
considered extensions of Grossberg's shunting network [Gr076] or Amari's model
for primitive neuronal competition [Ama82]. These earlier networks are considered
pools of mutually inhibitory neurons with fixed synaptic connections. Our results
extended these earlier studies to systems where the synapses can be modified by
external stimuli. The dynamics of competitive systems may be extremely complex,
exhibiting convergence to point attractors and periodic attractors. For networks
which model only the dynamic of the neural activity levels Cohen and Grossberg
[CG83] found a Lyapunov function as a necessary condition for the convergence
behavior to point attractors.
In this paper we apply the results of the theory of Lyapunov functions for singularly
perturbed systems on large-scale neural networks, which have two types of state
variables (LTM and STM) describing the slow and the fast dynamics of the system.
So we can find a Lyapunov function for the neural system with different time-scales
and give a design concept of storing desired pattern as stable equilibrium points.

2

THE CLASS OF NEURAL NETWORKS WITH
DIFFERENT TIME-SCALES

This section defines the network of differential equations characterizing laterally
inhibited neural networks. We consider a laterally inhibited network with a deterministic signal Hebbian learning law [Heb49] and is similar to the spatiotemporal
system of Amari [Ama83] .
The general neural network equations describe the temporal evolution of the STM
(activity modification) and LTM states (synaptic modification). For the jth neuron
of aN-neuron network these equations are:
N

Xj

= -ajxj + L

D i j!(Xi )

+ BjSj

(1)

i=l

(2)

where Xj is the current activity level, aj is the time constant of the neuron , Bj is
the contribution of the external stimulus term, !(Xi) is the neuron's output , D ij is
the .lateral inhibition term and Yi is the external stimulus. The dynamic variable
Sj represents the synaptic modification state and lyl21 is defined as lyl2 = yTy.
We will assume that the input stimuli are normalized vectors of unit magnitude
lyl2 = 1. These systems will be subject to our analysis considerations regarding the
stability of their equilibrium points.

3

ASYMPTOTIC STABILITY OF NEURAL
NETWORKS WITH DIFFERENT TIME-SCALES

We show in this section that it is possible to determine the asymptotic stability of
this class of neural networks interpreting them as nonlinear singularly perturbed
systems. While singular perturbation theory, a traditional tool of fluid dynamics
and nonlinear mechanics, embraces a wide variety of dynamic phenomena possesing
slow and fast modes, we show that singular perturbations are present in many

339

Quadratic-type Lyapunov Functions for Competitive Neural Networks

neurodynamical problems. In this sense we apply in this paper the results of this
valuable analysis tool on the dynamics of laterally inhibited networks.
In [SK84] is shown that a quadratic-type Lyapunov function for a singularly perturbed system is obtained as a weighted sum of quadratic-type Lyapunov functions
of two lower order systems: the so-called reduced and the boundary-layer systems.
Assuming that each of the two systems is asymptotically stable and has a Lyapunov
function, conditions are derived to guarantee that, for a sufficiently small perturbation parameter, asymptotic stability of the singularly perturbed system can be
established by means of a Lyapunov function which is composed as a weighted sum
of the Lyapunov functions of the reduced and boundary-layer systems.
Adopting the notations from [SK84] we will consider the singularly perturbed system 2

x = f(x, y)

x E Bx C R n

(3)
(4)

We assume that, in Bx and By, the origin (x = y = 0) is the unique equilibrium point
and (3) and (4) has a unique solution. A reduced system is defined by setting c = in (3)
and (4) to obtain

?

x = f(x,y)

(5)

O=g(x,y,O)

(6)

Assuming that in Bx and By, (6) has a unique root y = h(x), the reduced system is
rewritten as

x = f(x, h(x)) = fr(x)

(7)

A boundary-layer system is defined as

ay
aT

(8)

= g(X,y(T),O)

where T = tic is a stretching time scale. In (8) the vector x E R n is treated as a fixed
unknown parameter that takes values in Bx. The aim is to establish the stability properties
of the singularly perturbed system (3) and (4), for small c, from those of the reduced system
(7) and the boundary-layer system (8). The Lyapunov functions for system 7 and 8 are of
quadratic-type. In [SK84] it is shown that under mild assumptions, for sufficiently small
c, any weighted sum of the Lyapunov functions of the reduced and boundary-layer system
is a quadratic-type Lyapunov function for the singularly perturbed system (3) and (4).
The necessary assumptions are stated now [SK84]:
1. The reduced system (7) has a Lyapunov function V : R n

-+

R+ such that for all

xE Bx
(9)
where t/I(x) is a scalar-valued function of x that vanishes at x = 0 and is different
from zero for all other x E Bx. This condition guarantees that x = 0 is an
asymptotically stable equilibrium point of the reduced system (7).
2The symbol Bx indicates a closed sphere centered at x = OJ By is defined in the same
way.

A. MEYER-BASE

340

2. The boundary-layer system (8) has a Lyapunov function W(x, y) : R n x R m
R+ such that for all x E Bx and y E By

('\7yW(X,y)fg(X,y , O)::;-0:2??(y-h(x))

0:2>0

->

(10)

where ?>(y - h(x)) is a scalar-valued function (y - h(x)) E R m that vanishes
at y = h(x) and is different from zero for all other x E Bx and y E By. This
condition guarantees that y = h(x) is an asymptotically stable equilibrium point
of the boundary-layer system (8).

3. The following three inequalities hold "Ix E Bx and Vy E By:

a.)
('\7 ,..W(x, y)ff(x, y) ::; C1?>2(y - h(x)) + C21/J(X)?>(Y - h(x))

(11)

b.)
('\7,.. V(x)f[f(x, y) - f(x, h(x))] ::; /311/J(X)?>(y - h(x))

(12)

c.)

<

('\7yW(x,y)f[g(x,y,()-g(x,y,O)]

(K1?>2(y - h(x))

+

(K21/J(X)?>(Y - h(x)) (13)

The constants C1, C2, /31 , K1 and K2 are nonnegative. The inequalities above determine the
permissible interaction between the slow and fast variables. They are basically smoothness
requirements of f and g.

After these introductory remarks the stability criterion is now stated:
Theorem: Suppose that conditions 1-3 hold; let d be a positive number such that
0< d < 1, and let c*(d) be the positive number given by

(14)

where Ih = f{2 + G2, 'Y = f{l + Gl , then for all c < c*(d), the origin (x
is an asymptotically stable equilibrium point of (3) and (.0 and
v(x, y) = (1 - d)V(x)

+ dW(x, y)

= y = 0)
(15)

is a Lyapunov function of (3) and (4).

t

If we put c =
as a global neural time constant in equation (1) then we have
to determine two Lyapunov functions: one for the boundary-layer system and the
other for the reduced-order system.
In [CG83] is mentioned a global Lyapunov function for a competitive neural network
with only an activation dynamics.

(16)

under the constraints: mij = mji, ai(xi)

2: 0,

fj(xj)

2: O.

This Lyapunov-function can be t?aken as one for the boundary-layer system (STMequation) , if the LTM contribution Si is considered as a fixed unknown parameter:

Quadratic-type Lyapunov Functions for Competitive Neural Networks
N

W(x, S) = L
j=l

r

10

i

(Xi

N

aj((j)!;((j)d(j-L BjSj

0

10

j=l

341

1 N
f;((j)d(j-2 L Dij!i(Xj)!k(Xk)
j=l

0

(17)

For the reduced-order system (LTM- equation) we can take as a Lyapunov-function:
N

V(S)

= ~STS = L S;

(18)

i=l

The Lyapunov-function for the coupled STM and LTM dynamics is the sum of the
two Lyapunov-function:

vex, S)

4

= (1 -

d)V(S)

+ dW(x, S)

(19)

DESIGN OF STABLE COMPETITIVE NEURAL
NETWORKS

Competitive neural networks with learning rules have moving equilibria during the
learning process. The concept of asymptotic stability derived from matrix perturbation theory can capture this phenomenon.
We design in this section a competitive neural network that is able to store a desired
pattern as a stable equilibrium.
The theoretical implications are illustrated in an example of a two neuron network .
Example: Let N = 2, ai = A, B j = B, Dii = a > 0, Dij = -(3
nonlinearity be a linear function f(xj) = Xj in equations (1) and (2).

<

0 and the

We get for the boundary-layer system:
N

Xj

= -Axj + L

Dijf(xd + BSj

(20)

i=l

and for the reduced-order system:

.

B
lA-a

C
A-a

S? = S ? [ - - -1] - - J

(21)

Then we get for the Lyapunov-functions:
(22)
and
(23)

A. MEYER-BASE

342

-0.2
.

\
\

-0.4
[JJ

OJ
.IJ

lIS

.IJ
[JJ

-0.6

/

~
U)

\J

-0.8

-1

-1.2

~

o

__

~

__

~

1

2

____ __ __- L__
3
4
5
time in msec
~

~

~~

6

__

~

__- L__

7

~~~

8

9

10

Figure 1: Time histories of the neural network with the origin as an equilibrium
point: STM states.
For the nonnegative constants we get: al = 1 - A~a' a2
with B < 0 , and C2 = i3l = i32 = 1 and I<l = I<2 = O.

= (A -

a)2,

Cl

= 'Y = - B,

We get some interesting implications from the above results as: A-a> B , A-a> 0
and B < o.
The above impications can be interpreted as follows: To achieve a stable equilibrium
point (0,0) we should have a negative contribution of the external stimulus term
and the sum of the excitatory and inhibitory contribution of the neurons should
be less than the time constant of a neuron. An evolution of the trajectories of the
STM and LTM states for a two neuron system is shown in figure 1 and 2. The
STM states exhibit first an oscillation from the expected equilibrium point, while
the LTM states reach monotonically the equilibrium point. We can see from the
pictures that the equilibrium point (0,0) is reached after 5 msec by the STM- and
LTM-states.

= 55+ll.of
.
.d(1-d)
From the above formula we can see that f*(d) has a maximum at d = d* = 0.5.
Choosing B = -5, A

5

= 1 and a = 0.5 we obtain for

f*(d) : f*(d)

CONCLUSIONS

We presented in this paper a quadratic-type Lyapunov function for analyzing the
stability of equilibrium points of competitive neural networks with fast and slow
dynamics. This global stability analysis method is interpreting neural networks
as nonlinear singularly perturbed systems. The equilibrium point is constrained
to a neighborhood of (0,0). This technique supposes a monotonically increasing
non-linearity and a symmetric lateral inhibition matrix. The learning rule is a
deterministic Hebbian. This method gives an upper bound on the perturbation

343

Quadratic-type Lyapunov Functions for Competitive Neural Networks
0.6

~--~--~----~--~--~----~--'---~--~r---.

0.5

0.4
III

<II

.j.J

III

.j.J

III

0.3

~
~
0.2

0.1

o L-__
1
o

~~~~~~~

2

3

____~__~__~__- L_ _~

4

5

6

7

8

9

10

time in msec

Figure 2: Time histories of the neural network with the origin as an equilibrium
point: LTM states.
parameter and such an estimation of a maximal positive neural time-constant. The
practical implication ofthe theoretical problem is the design of a competitive neural
network that is able to store a desired pattern as a stable equilibrium.

References
[Ama82] S. Amari. Competitive and cooperative aspects in dynamics of neural excitation and self-organization. Competition and cooperation in neural networks, 20:1-28, 7 1982.
[Ama83] S. Amari. Field theory of self-organizing neural nets. IEEE Transactions
on systems, machines and communication, SMC-13:741-748, 7 1983.
A. M. Cohen und S. Grossberg. Absolute Stability of Global Pattern Formation and Parallel Memory Storage by Competitive Neural Networks.
IEEE Transactions on Systems, Man and Cybernetics, SMC-13:815-826,
9 1983.
[Gro76] S. Grossberg. Adaptive Pattern Classification and Universal Recording.
Biological Cybernetics, 23:121-134, 1 1976.

[CG83]

[Heb49] D. O. Hebb. The Organization of Behavior. J. Wiley Verlag, 1949.
[SK84] Ali Saberi und Hassan Khalil. Quadratic-Type Lyapunov Functions for
Singularly Perturbed Systems. IEEE Transactions on A utomatic Control,
pp. 542-550, June 1984.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 503-refining-pid-controllers-using-neural-networks.pdf

Refining PIn Controllers using Neural Networks

Gary M. Scott
Department of Chemical Engineering
1415 Johnson Drive
University of Wisconsin
Madison, WI 53706

Jude W. Shavlik
Department of Computer Sciences
1210 W. Dayton Street
University of Wisconsin
Madison, WI 53706

W. Harmon Ray
Department of Chemical Engineering
1415 Johnson Drive
University of Wisconsin
Madison, WI 53706

Abstract
The KBANN approach uses neural networks to refine knowledge that can
be written in the form of simple propositional rules. We extend this idea
further by presenting the MANNCON algorithm by which the mathematical
equations governing a PID controller determine the topology and initial
weights of a network, which is further trained using backpropagation. We
apply this method to the task of controlling the outflow and temperature
of a water tank, producing statistically-significant gains in accuracy over
both a standard neural network approach and a non-learning PID controller. Furthermore, using the PID knowledge to initialize the weights of
the network produces statistically less variation in testset accuracy when
compared to networks initialized with small random numbers.

1

INTRODUCTION

Research into the design of neural networks for process control has largely ignored
existing knowledge about the task at hand. One form this knowledge (often called
the "domain theory") can take is embodied in traditional controller paradigms. The

555

556

Scott, Shavlik, and Ray

recently-developed KBANN (Knowledge-Based Artificial Neural Networks) approach
(Towell et al., 1990) addresses this issue for tasks for which a domain theory (written
using simple propositional rules) is available. The basis of this approach is to use
the existing knowledge to determine an appropriate network topology and initial
weights, such that the network begins its learning process at a "good" starting
point.
This paper describes the MANNCON (Multivariable Artificial Neural Network Control) algorithm, a method of using a traditional controller paradigm to determine
the topology and initial weights of a network . The used of a PID controller in this
way eliminates network-design problems such as the choice of network topology
(i.e., the number of hidden units) and reduces the sensitivity of the network to the
initial values of the weights. Furthermore, the initial configuration of the network
is closer to its final state than it would normally be in a randomly-configured network. Thus, the MANNCON networks perform better and more consistently than
the standard, randomly-initialized three-layer approach.
The task we examine here is learning to control a Multiple-Input, Multiple-Output
(MIMO) system. There are a number of reasons to investigate this task using neural networks. One, it usually involves nonlinear input-output relationships, which
matches the nonlinear nature of neural networks. Two, there have been a number
of successful applications of neural networks to this task (Bhat & McAvoy, 1990;
Jordan & Jacobs, 1990; Miller et al., 1990). Finally, there are a number of existing
controller paradigms which can be used to determine the topology and the initial
weights of the network.

2

CONTROLLER NETWORKS

The MANNCON algorithm uses a Proportional-Integral-Derivative (PID) controller
(Stephanopoulos, 1984), one of the simplest of the traditional feedback controller
schemes, as the basis for the construction and initialization of a neural network controller. The basic idea of PID control is that the control action u (a vector) should
be proportional to the error, the integral of the error over time, and the temporal
derivative of the error. Several tuning parameters determine the contribution of
these various components. Figure 1 depicts the resulting network topology based
on the PID controller paradigm. The first layer of the network, that from Y$P (desired process output or setpoint) and Y(n-l) (actual process output of the past time
step), calculates the simple error (e). A simple vector difference,
e=Y$p-Y

accomplishes this. The second layer, that between e, e(n-l), and e, calculates the
actual error to be passed to the PID mechanism. In effect, this layer acts as a
steady-state pre-compensator (Ray, 1981), where

e = GIe
and produces the current error and the error signals at the past two time steps.
This compensator is a constant matrix, G I , with values such that interactions at a
steady state between the various control loops are eliminated. The final layer , that
between e and u(n) (controller output/plant input), calculates the controller action

Refining PID Controllers using Neural Networks

Fd
Td
den) Water

Tank

F
T
Yen)

WCO
WHO
WCI
WHI
WC2
WH2

Y(n-I)
t:(n-I)

Figure 1:

MANNCON network showing weights that are initialized using
Ziegler-Nichols tuning parameters.

based on the velocity form of the discrete PID controller:
UC(n)

= UC(n-l) + WCOCI(n) + WCICI(n-l) + WC2 CI(n-2)

where Wca, wCb and WC2 are constants determined by the tuning parameters of the
controller for that loop. A similar set of equations and constants (WHO, WHI, WH2)
exist for the other controller loop.
Figure 2 shows a schematic of the water tank (Ray, 1981) that the network controls. This figure also shows the controller variables (Fc and FH), the tank output
variables (F(h) and T), and the disturbance variables (Fd and Td). The controller
cannot measure the disturbances, which represent noise in the system.
MANN CON initializes the weights of Figure 1 's network with va.lues that mimic
the behavior of a PID controller tuned with Ziegler-Nichols (Z-N) parameters
(Stephanopoulos, 1984) at a particular operating condition. Using the KBANN
approach (Towell et al., 1990), it adds weights to the network such that all units
in a layer are connected to all units in all subsequent layers, and initializes these
weights to small random numbers several orders of magnitude smaller than the
weights determined by the PID parameters. We scaled the inputs and outputs of
the network to be in the range [0,1].

Initializing the weights of the network in the manner given above assumes that the
activation functions of the units in the network are linear, that is,

557

558

Scott, Shavlik, and Ray

Cold Stream
Fe
Hot Stream (at TH)

~
T
F

Dis t urban ce
Fd,Td
I-

= Temperature
= Flow Rate

h

II

l-

Output
F(h), T

I I

Figure 2: Stirred mixing tank requiring outflow and temperature control.

Table 1: Topology and initialization of networks.
Network
1. Standard neural network
2. MANNCON network I
3. MANNCON network II

Topology
3-layer (14 hidden units)
PID topology
PID topology

Weight Initialization
random
random
Z-N tuning

The strength of neural networks, however, lie in their having nonlinear (typically
sigmoidal) activation functions. For this reason, the MANNCON system initially sets
the weights (and the biases of the units) so that the linear response dictated by the
PID initialization is approximated by a sigmoid over the output range of the unit.
For units that have outputs in the range [-1,1]' the activation function becomes
2

1 + exp( -2.31 L
where

Wji

_ 1
WjiOi)

are the linear weights described above.

Once MANNCON configures and initializes the weights of the network, it uses a set
of training examples and backpropagation to improve the accuracy of the network.
The weights initialized with PID information, as well as those initialized with small
random numbers, change during backpropagation training.

3

EXPERIMENTAL DETAILS

We compared the performance of three networks that differed in their topology
and/or their method of initialization. Table 1 summarizes the network topology
and weight initialization method for each network. In this table, "PID topology"
is the network structure shown in Figure 1. "Random" weight initialization sets

Refining PID Controllers using Neural Networks

Table 2: Range and average duration of setpoints for experiments.
Experiment
1
2

3

Training Set
[0.1,0.9]
22 instances
[0.1,0.9]
22 instances
[0.4,0.6]
22 instances

Testing Set
[0.1,0.9]
22 instances
[0.1,0.9]
80 instances
[0.1,0.9]
80 instances

all weights to small random numbers centered around zero. We also compare these
networks to a (non-learning) PID controller.
We trained the networks using backpropagation over a randomly-determined schedule of setpoint YsP and disturbance d changes that did not repeat. The setpoints,
which represent the desired output values that the controller is to maintain, are the
temperature and outflow of the tank. The disturbances, which represent noise, are
the inflow rate and temperature of a disturbance stream. The magnitudes of the
setpoints and the disturbances formed a Gaussian distribution centered at 0.5. The
number of training examples between changes in the setpoints and disturbances
were exponentially distributed.
We performed three experiments in which the characteristics of the training and/or
testing set differed. Table 2 summarizes the range of the setpoints as well as their
average duration for each data set in the experiments. As can be seen, in Experiment
1, the training set and testing sets were qualitatively similar; in Experiment 2, the
test set was of longer duration setpoints; and in Experiment 3, the training set was
restricted to a subrange of the testing set. We periodically interrupted training and
tested the network . Results are averaged over 10 runs (Scott, 1991).
We used the error at the output of the tank (y in Figure 1) to determine the network
error (at u) by propagating the error backward through the plant (Psaltis et al.,
1988). In this method, the error signal at the input to the tank is given by

8u i

?Yi
= f '( netui ) ~
~ 8y j OUi
J

where 8yj represents the simple error at the output of the water tank and 8ui is the
error signal at the input of the tank . Since we used a model of the process and not a
real tank, we can calculate the partial derivatives from the process model equations.

4

RESULTS

Figure 3 compares the performance of the three networks for Experiment 1. As can
be seen, the MANNCON networks show an increase in correctness over the standard
neural network approach. Statistical analysis of the errors using a t-test show
that they differ significantly at the 99.5% confidence level. Furthermore, while the
difference in performance between MANNCON network I and MANNCON network II is

559

560

Scott, Shavlik, and Ray
l~---------------------------------------------,

1 = Standard neural network
2 = MANNCON network I
3 = MANN CON network II
4 = PID controller (non-learning)

10000

15000

20000

25000

30000

Training Instances
Figure 3: Mean square error of networks on the testset as a function of
the number of training instances presented for Experiment 1.
not significant, the difference in the variance of the testing error over different runs
is significant (99.5% confidence level). Finally, the MANNCON networks perform
significantly better (99.95% confidence level) than the non-learning PID controller.
The performance of the standard neural network represents the best of several trials
with a varying number of hidden units ranging from 2 to 20.
A second observation from Figure 3 is that the MANNCON networks learned much
more quickly than the standard neural-network approach. The MANNCON networks
required significantly fewer training instances to reach a performance level within
5% of its final error rate. For each of the experiments, Table 3 summarizes the
final mean error, as well as the number of training instances required to achieve a
performance within 5% of this value.

In Experiments 2 and 3 we again see a significant gain in correctness of the

MAN-

NCON networks over both the standard neural network approach (99.95% confidence
level) as well as the non-learning PID controller (99.95% confidence level). In these
experiments, the MANNCON network initialized with Z-N tuning also learned significantly quicker (99.95% confidence level) than the standard neural network.

5

FUTURE WORK

One question is whether the introduction of extra hidden units into the network
would improve the performance by giving the network "room" to learn concepts
that are outside the given domain theory. The addition of extra hidden units as
well as the removal of unneeded units is an area with much ongoing research.

Refining PID Controllers using Neural Networks

l.

2.
3.
4.
5.
l.

2.
3.
4.
5.
l.

2.
3.
4.
5.

Table 3: Comparison of network performance.
I Mean Square Error I Training Instances
Method
Experiment 1
25,200 ? 2, 260
Standard neural network
0.0103 ? 0.0004
5,000 ? 3,340
MANN CON network I
0.0090 ? 0.0006
MANN CON network II
640? 200
0.0086 ? 0.0001
PID control (Z-N tuning) 0.0109
0.0190
Fixed control action
Experiment 2
14,400 ? 3, 150
Standard neural network
0.0118 ? 0.00158
12 , 000 ? 3,690
MANN CON network I
0.0040 ? 0.00014
2,080? 300
0.0038 ? 0.00006
MANN CON network II
PID control (Z-N tuning) 0.0045
Fixed con trol action
0.0181
Experiment 3
0.0112 ? 0.00013
25,200 ? 2, 360
Standard neural network
25,000 ? 1, 550
MANN CON network I
0.0039 ? 0.00008
9,400 ? 1,180
MANN CON network II
0.0036 ? 0.00006
PID control (Z-N tuning) 0.0045
Fixed control action
0.0181

The "?" indicates that the true value lies within these bounds at a 95%
confidence level. The values given for fixed control action (5) represent
the errors resulting from fixing the control actions at a level that produces
outputs of [0.5,0.5) at steady state.

"Ringing" (rapid changes in controller actions) occurred in some of the trained
networks . A future enhancement of this approach would be to create a network
architecture that prevented this ringing, perhaps by limiting the changes in the
controller actions to some relatively small values.
Another important goal of this approach is the application of it to other real-world
processes. The water tank in this project, while illustrative of the approach , was
quite simple. Much more difficult problems (such as those containing significant
time delays) exist and should be explored.
There are several other controller paradigms that could be used as a basis for network construction and initialization. There are several different digital controllers,
such as Deadbeat or Dahlin's (Stephanopoulos, 1984), that could be used in place
of the digital PID controller used in this project. Dynamic Matrix Control (DMC)
(Pratt et al., 1980) and Internal Model Control (IMC) (Garcia & Morari, 1982) are
also candidates for consideration for this approach.
Finally, neural networks are generally considered to be "black boxes," in that their
inner workings are completely uninterpretable. Since the neural networks in this
approach are initialized with information, it may be possible to interpret the weights
of the network and extract useful information from the trained network.

561

562

Scott, Shavlik, and Ray

6

CONCLUSIONS

We have described the MANNCON algorithm, which uses the information from a
PID controller to determine a relevant network topology without resorting to trialand-error methods. In addition, the algorithm, through initialization of the weights
with prior knowledge, gives the backpropagtion algorithm an appropriate direction
in which to continue learning. Finally, we have shown that using the MANNCON
algorithm significantly improves the performance of the trained network in the following ways:
? Improved mean testset accuracy
? Less variability between runs
? Faster rate of learning
? Better generalization and extrapolation ability
Acknowledgements

This material based upon work partially supported under a National Science Foundation Graduate Fellowship (to Scott), Office of Naval Research Grant N00014-90J-1941, and National Science Foundation Grants IRI-9002413 and CPT-8715051.
References

Bhat, N. & McAvoy, T. J. (1990). Use of neural nets for dynamic modeling and
control of chemical process systems. Computers and Chemical Engineering, 14,
573-583.
Garcia, C. E. & Morari, M. (1982). Internal model control: 1. A unifying review
and some new results. I&EC Process Design & Development, 21, 308-323.
Jordan, M. I. & Jacobs, R. A. (1990). Learning to control an unstable system
with forward modeling. In Advances in Neural Information Processing Systems
(Vol. 2, pp. 325- 331). San Mateo, CA: Morgan Kaufmann.
Miller, W. T., Sutton, R. S., & Werbos, P. J. (Eds.)(1990). Neural networks for
control. Cambridge, MA : MIT Press.
Pratt, D. M., Ramaker, B. L., & Cutler, C. R. (1980) . Dynamic matrix control
method. Patent 4,349,869, Shell Oil Company.
Psaltis, D., Sideris, A., & Yamamura, A. A. (1988). A multilayered neural network
controller. IEEE Control Systems Magazine, 8, 17- 21.
Ray, W . H. (1981). Advanced process control. New York: McGraw-Hill, Inc.
Scott, G. M. (1991). Refining PID controllers using neural networks. Master's
project, University of Wisconsin, Department of Computer Sciences.
Stephanopoulos, G. (1984). Chemical process control: An introduction to theory
and practice. Englewood Cliffs, NJ: Prentice Hall, Inc.
Towell, G., Shavlik, J., & Noordewier, M. (1990). Refinement of approximate domain theories by knowledge-base neural networks. In Eighth National Conference on Aritificial Intelligence (pp. 861-866). Menlo Park, CA: AAAI Press .


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 657-optimal-depth-neural-networks-for-multiplication-and-related-problems.pdf

Optimal Depth Neural Networks for Multiplication
and Related Problems

Kai-Yeung Siu
Dept. of Electrical & Compo Engineering
University of California, Irvine
Irvine, CA 92717

Vwani Roychowdhury
School of Electrical Engineering
Purdue University
West Lafayette, IN 47907

Abstract
An artificial neural network (ANN) is commonly modeled by a threshold
circuit, a network of interconnected processing units called linear threshold
gates. The depth of a network represents the number of unit delays or the
time for parallel computation. The SIze of a circuit is the number of gates
and measures the amount of hardware . It was known that traditional logic
circuits consisting of only unbounded fan-in AND, OR, NOT gates would
require at least O(log n/log log n) depth to compute common arithmetic
functions such as the product or the quotient of two n-bit numbers, unless
we allow the size (and fan-in) to increase exponentially (in n). We show in
this paper that ANNs can be much more powerful than traditional logic
circuits. In particular, we prove that that iterated addition can be computed by depth-2 ANN, and multiplication and division can be computed
by depth-3 ANNs with polynomial size and polynomially bounded integer
weights, respectively. Moreover, it follows from known lower bound results that these ANNs are optimal in depth. We also indicate that these
techniques can be applied to construct polynomial-size depth-3 ANN for
powering, and depth-4 ANN for mUltiple product.

1

Introduction

Recent interest in the application of artificial neural networks [10, 11] has spurred
research interest in the theoretical study of such networks. In most models of neural networks, the basic processing unit is a Boolean gate that computes a linear
59

60

Siu and Roychowdhury
threshold function, or an analog element that computes a sigmoidal function. Artificial neural networks can be viewed as circuits of these processing units which are
massively interconnected together.
While neural networks have found wide application in many areas, the behavior
and the limitation of these networks are far from being understood. One common
model of a neural network is a threshold circuit. Incidentally, the study of threshold
circuits, motivated by some other complexity theoretic issues, has also gained much
interest in the area of computer science. Threshold circuits are Boolean circuits in
which each gate computes a linear threshold function, whereas in the classical model
of unbounded fan-in Boolean circuits only AND, OR, NOT gates are allowed. A
Boolean circuit is usually arranged in layers such that all gates in the same layer are
computed concurrently and the circuit is computed layer by layer in some increasing
depth order. We define the depth as the number of layers in the circuit. Thus each
layer represents a unit delay and the depth represents the overall delay in the
computation of the circuit .

2

Related Work

Theoretical computer scientists have used unbounded fan-in Boolean circuits as
a model to understand fundamental issues of parallel computation. To be more
specific, this computational model should be referred to as unbounded fan-in parallelism, since the number of inputs to each gate in the Boolean circuit is not bounded
by a constant. The theoretical study of unbounded fan-in parallelism may give us
insights into devising faster algorithms for various computational problems than
would be possible with bounded fan-in parallelism. In fact, any nondegenerate
Boolean function of n variables requires at least O(log n) depth to compute in a
bounded fan-in circuit. On the other hand, in some practical situations, (for example large fan-in circuits such as programmable logic arrays (PLAs) or multiple
processors simultaneously accessing a shared bus), unbounded fan-in parallelism
seems to be a natural model. For example, a PLA can be considered as a depth-2
AND/OR circuit.
In the Boolean circuit model, the amount of resources is usually measured by the
number of gates, and is considered to be 'reasonable' as long as it is bounded
by a polynomial (as opposed to exponential) in the number of the inputs. For
example, a Boolean circuit for computing the sum of two n-bit numbers with O(n 3 )
gates is 'reasonable', though circuit designers might consider the size of the circuit
impractical for moderately large n. One of the most important theoretical issues in
parallel computation is the following: Given that the number of gates in the Boolean
circuit is bounded by a polynomial in the size of inputs, what is the minimum depth
(i.e. number of layers) that is needed to compute certain functions?

A first step toward answering this important question was taken by Furst et al. [4]
and independently by Ajtai [2]. It follows from their results that for many basic
functions, such as the parity and the majority of n Boolean variables, or the multiplication of two n-bit numbers, any constant depth (i. e. independent of n) classical
Boolean circuit of unbounded fan-in AND/OR gates computing these functions
must have more than a polynomial (in n) number of gates. This lower bound on
the size was subsequently improved by Yao [18] and Hastad [7]; it was proved that

Optimal Depth Neural Networks for Multiplication and Related Problems

indeed an exponential number of AND/OR gates are needed. So functions such as
parity and majority are computationally 'hard' with respect to constant depth and
polynomial size classical Boolean circuits. Another way of interpreting these results
is that circuits of AND/OR gates computing these 'hard' functions which use polynomial amount of chip area must have unbounded delay (i. e. delay that increases
with n). In fact, the lower bound results imply that the minimum possible delay
for multipliers (with polynomial number of AND/OR gates) is O(logn/loglogn).
These results also give theoretical justification why it is impossible for circuit designers to implement fast parity circuit or multiplier in small chip area using AND,
OR gates as the basic building blocks.
One of the 'hard' functions mentioned above is the majority function, a special case
of a threshold function in which the weights or parameters are restricted. A natural
extension is to study Boolean circuits that contain majority gates. This type of
Boolean circuit is called a threshold circuit and is believed to capture some aspects
of the computation in our brain [12]. In the rest of the paper, the term 'neural
networks' refers to the threshold circuits model.
With the addition of majority gates, the resulting Boolean circuit model seems
much more powerful than the classical one. Indeed, it was first shown by Muroga
[13] three decades ago that any symmetric Boolean function (e.g. parity) can be
computed by a two-layer neural network with (n + 1) gates. Recently, Chandra
et al. [3] showed that multiplication of two n-bit numbers and sorting of n n-bit
numbers can be computed by neural networks with 'constant' depth and polynomial
size. These 'constants' have been significantly reduced by Siu and Bruck [14, 15] to
4 in both cases, whereas a lower bound of depth-3 was proved by Hajnal et al. [6]
in the case of multiplication. It is now known [8] that the size of the depth-4 neural
networks for multiplication can be reduced to O(n 2 ). However, the existence of
depth-3 and polynomial-size neural networks for multiplication was left as an open
problem [6, 5, 15] since the lower bound result in [6]. In [16], some depth-efficient
neural networks were constructed for division and related arithmetic problems; the
networks in [16] do not have optimal depth.
Our main contribution in this paper is to show that small constant depth neural
networks for multiplication, division and related problems can be constructed. For
the problems such as iterated addition, multiplication, and division, the neural networks constructed can be shown to have optimal depth. These results have the
following implication on their practical significance: Suppose we can use analog devices to build threshold gates with a cost (in terms of delay and chip area) that is
comparable to that of AND, OR, logic gates, then we can compute many basic functions much faster than using traditional circuits. Clearly, the particular weighting

of depth, fan-in, and size that gives a realistic measure of a network's cost and speed
depends on the technology used to build it. One case where circuit depth would
seem to be the most important parameter is when the circuit is implemented using
optical devices. We refer those who are interested in the optical implementation of
neural networks to [1].
Due to space limitations, we shall only state some of the important results; further
results and detailed proofs will appear in the journal version of this paper [17].

61

62

Siu and Roychowdhury

3

Main Results

Definition 1
Given n n-bit integers, Zi = Lj~; zi,i2i, i = 1, ... , n, zi,i E {O, I},
We define iterated addition to be the problem of computing the (n + log n )-bit sum
L~=l Zi of the n integers.

=

=

Definition 2
Given 2 n-bit integers, x
Lj==-~ xi2i and Y Lj==-~ Yi2i. We
define multiplication to be the problem of computing the (2n)-bit product of x and
y.

Using the notations of [15], let us denote the class of depth-d polynomial-size neural
networks where the (integer) weights are polynomially bounded by & d and the
corresponding class where the weights are unrestricted by LTd. It is easy to see that
if it~ated addition can be computed in &2, then multiplication can be computed
in LT 3 . We first prove the result on iterated addition. Our result hinges on a
recent striking result of Goldmann, Hcistad and Razborov [5]. The key observation
is that iterated addition can be computed as a sum of polynomially many linear
threshold (LTd functions (with exponential weights). Let us first state the result
of Goldmann, Hastad and Razborov [5].
Lemma 1
[5] Let LTd denote the class of depth-d polynomial-size neural networks where the weights at the output gate are polynomially bounded integers (with
no restriction on the weights of the other gates). Then LTd = & d for any fixed
integer d ~ 1.
The following lemma is a generalization of the result in [13]. Informally, the result
says that if a function is 1 when a weighted sum (possibly exponential) of its inputs
lies in one of polynomially many intervals, and is 0 otherwise, then the function can
be computed as a sum of polynomially many LTI functions.
Lemma 2
Let S = L7=1 WiXi and f(X) be a function such that f = 1 if S E
[Ii, ud for i = 1, ... , Nand f = 0 otherwise, where N is polynomially bounded.
The~ can be computed as a sum of polynomially many LTI functions and thus
f E LT2 ?
Combining the above two lemmas yields a depth-2 neural network for iterated addition.
.-

Theorem 1

Iterated addition is in LT2 ?

It is also easy to see that iterated addition cannot be computed in LTI

Simply
observe that the first bit of the sum is the parity function, which does not belong
to LT1 . Thus the above neural network for iterated addition has minimum possible
depth.
Theorem 2

Multiplication of 2 n-bit integers can be computed in

.

LT3.

It follows from the results in [6] that the depth-3 neural network for multiplication

stated in the above theorem has optimal depth.

Optimal Depth Neural Networks for Multiplication and Related Problems

We can further apply the results in [5] to construct small depth neural networks for
division, powering and multiple product. Let us give a formal definition of these
problems.

Definition 3
Let X be an input n-bit integer
2
n -bit representation of xn.

~

O. We define powering to be the

Definition 4
Given n n-bit integers Zi, i = 1, ... , n, We define multiple product
2
to be the n -bit representation of n~=l Zi.
Suppose we want to compute the quotient of two integers. Some quotient in binary representation might require infinitely many bits, however, a circuit can only
compute the most significant bits of the quotient. If a number has both finite and
infinite binary representation (for example 0.1 = 0.0111 ... ), we shall always express
the number in its finite binary representation. We are interested in computing the
truncated quotient, defined below:

=

Definition 5
Let X and Y ~ 1 be two input n bit integers. Let X /Y
L~;~oo zi 2i be the quotient of X divided by Y. We define DIVk(X/Y) to be
X/Y truncated to the (n + k)-bit number, i.e.

o
In particular, DIVo(X /Y) is l X /Y J, the greatest integer ~ X /Y.

Theorem 3
-.

1. Powering can be computed in LT3 .

2. DIVk(x/y) can be computed in

Lr3 .

3. Multiple Product can be computed in

LT4 .

It can be shown from the lower-bound results in [9] that the neural networks for
division are optimal in depth.

References
[1] Y. S. Abu-Mostafa and D. Psaltis. Optical Neural Computers. Scientific American
, 256(3) :88-95, 1987.

L~ -formulae on finite structures. Annals of Pure and Applied Logic,
24:1-48, 1983.

[2] M. Ajtai.

[3] A. K. Chandra, 1. Stockmeyer, and U. Vishkin. Constant depth reducibility. Siam
J. Comput., 13:423-439, 1984.
[4] M. Furst, J. B. Saxe, and M. Sipser. Parity, Circuits and the Polynomial-Time
Hierarchy. IEEE Symp. Found. Compo Sci., 22:260-270, 1981.
[5] M. Goldmann, J. Hastad, and A. Razborov. Majority Gates vs. General Weighted
Threshold Gates. preprint, 1991.

63

64

Siu and Roychowdhury
[6] A. Hajnal, W. Maass, P. Pudlak, M. Szegedy, and G. Turan. Threshold circuits of
bounded depth. IEEE Symp. Found. Compo Sci., 28:99-110, 1987.

[7] J. H1stad and M. Goldmann.

On the power of small-depth threshold circuits.
InProceedings of the 31st IEEE FOCS, pp. 610-618, 1990.

[8] T. Hofmeister, W. Hohberg and S. Kohling . Some notes on threshold circuits and
multiplication in depth 4. Information Processing Letters, 39:219-225, 1991.
[9] T. Hofmeister and P. PudIa.k, A proof that division is not in TC~. Forschungsbericht
Nr. 447, 1992, Uni Dortmund.
[10] J. J. Hopfield. Neural Networks and physical systems with emergent collective computational abilities. Proceedings of the National Academy of Sciences, 79:2554-2558,
1982.
[11] J. L. McClelland D. E. Rumelhart and the PDP Research Group. Parallel Distributed
Processing: Explorations in the Microstructure of Cognition, vol. 1. MIT Press, 1986.
[12] W. S. McCulloch and W. Pitts. A Logical Calculus of Ideas Immanent in Nervous
Activity. Bulletin of Mathematical Biophysics, 5:115-133, 1943.
[13] S. Muroga. The principle of majority decision logic elements and the complexity of
their circuits. Inti. Con/. on Information Processing, Paris, France, June 1959.
[14] K. Y. Siu and J. Bruck. Neural Computation of Arithmetic Functions. Proc. IEEE,
78, No. 10:1669-1675, October 1990. Special Issue on Neural Networks.
[15] K.-Y. Siu and J. Bruck. On the Power of Threshold Circuits with Small Weights.
SIAM J. Discrete Math., 4(3):423-435, August 1991.
[16] K.-Y. Siu, J. Bruck, T. Kailath, and T. Hofmeister. Depth-Efficient Neural Networks

for Division and Related Problems . to appear in IEEE Trans. Information Theory,
1993.
[17] K.- Y. Siu and V. Roychowdhury.

On Optimal Depth Threshold Circuits for Mulitplication and Related Problems. to appear in SIAM J. Discrete Math.

[18] A. Yao.

Separating the polynomial-time hierarchy by oracles. IEEE Symp. Found.
Compo Sci., pages 1-10, 1985.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 6427-toward-deeper-understanding-of-neural-networks-the-power-of-initialization-and-a-dual-view-on-expressivity.pdf

Toward Deeper Understanding of Neural Networks: The Power
of Initialization and a Dual View on Expressivity

Amit Daniely
Google Brain

Roy Frostig?
Google Brain

Yoram Singer
Google Brain

Abstract
We develop a general duality between neural networks and compositional kernel
Hilbert spaces. We introduce the notion of a computation skeleton, an acyclic
graph that succinctly describes both a family of neural networks and a kernel space.
Random neural networks are generated from a skeleton through node replication
followed by sampling from a normal distribution to assign weights. The kernel
space consists of functions that arise by compositions, averaging, and non-linear
transformations governed by the skeleton?s graph topology and activation functions.
We prove that random networks induce representations which approximate the
kernel space. In particular, it follows that random weight initialization often yields
a favorable starting point for optimization despite the worst-case intractability of
training neural networks.

1

Introduction

Neural network (NN) learning has underpinned state of the art empirical results in numerous applied
machine learning tasks, see for instance [25, 26]. Nonetheless, theoretical analyses of neural network
learning are still lacking in several regards. Notably, it remains unclear why training algorithms ?nd
good weights and how learning is impacted by network architecture and its activation functions.
This work analyzes the representation power of neural networks within the vicinity of random
initialization. We show that for regimes of practical interest, randomly initialized neural networks
well-approximate a rich family of hypotheses. Thus, despite worst-case intractability of training
neural networks, commonly used initialization procedures constitute a favorable starting point for
training.
Concretely, we de?ne a computation skeleton that is a succinct description of feed-forward networks.
A skeleton induces a family of network architectures as well as an hypothesis class H of functions
obtained by non-linear compositions mandated by the skeleton?s structure. We then analyze the set of
functions that can be expressed by varying the weights of the last layer, a simple region of the training
domain over which the objective is convex. We show that with high probability over the choice of
initial network weights, any function in H can be approximated by selecting the ?nal layer?s weights.
Before delving into technical detail, we position our results in the context of previous research.
Current theoretical understanding of NN learning. Standard results from complexity theory [22]
imply that all ef?ciently computable functions can be expressed by a network of moderate size.
Barron?s theorem [7] states that even two-layer networks can express a very rich set of functions. The
generalization ability of algorithms for training neural networks is also fairly well studied. Indeed,
both classical [3, 9, 10] and more recent [18, 33] results from statistical learning theory show that, as
the number of examples grows in comparison to the size of the network, the empirical risk approaches
the population risk. In contrast, it remains puzzling why and when ef?cient algorithms, such as
stochastic gradient methods, yield solutions that perform well. While learning algorithms succeed in
?

Most of this work performed while the author was at Stanford University.

30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

practice, theoretical analyses are overly pessimistic. For example, hardness results suggest that, in
the worst case, even very simple 2-layer networks are intractable to learn. Concretely, it is hard to
construct a hypothesis which predicts marginally better than random [15, 23, 24].
In the meantime, recent empirical successes of neural networks prompted a surge of theoretical
results on NN learning. For instance, we refer the reader to [1, 4, 12, 14, 16, 28, 32, 38, 42] and the
references therein.
Compositional kernels and connections to networks. The idea of composing kernels has repeatedly appeared in the machine learning literature. See for instance the early work by Grauman and
Darrell [17], Sch?lkopf et al. [41]. Inspired by deep networks? success, researchers considered deep
composition of kernels [11, 13, 29]. For fully connected two-layer networks, the correspondence
between kernels and neural networks with random weights has been examined in [31, 36, 37, 45].
Notably, Rahimi and Recht [37] proved a formal connection, in a similar sense to ours, for the
RBF kernel. Their work was extended to include polynomial kernels [21, 35] as well as other
kernels [5, 6]. Several authors have further explored ways to extend this line of research to deeper,
either fully-connected networks [13] or convolutional networks [2, 20, 29].
This work establishes a common foundation for the above research and expands the ideas therein. We
extend the scope from fully-connected and convolutional networks to a broad family of architectures.
In addition, we prove approximation guarantees between a network and its corresponding kernel in
our general setting. We thus generalize previous analyses which are only applicable to fully connected
two-layer networks.

2

Setting

Notation. We denote vectors by bold-face letters (e.g. x), and matrices by upper case Greek letters
(e.g. ?). The 2-norm of x ? Rd is denoted by ?x?. For functions ? : R ? R we let
?
??
?
x2
??? := EX?N (0,1) ? 2 (X) = ?12? ?? ? 2 (x)e? 2 dx .
Let G = (V, E) be a directed acyclic graph. The set of neighbors incoming to a vertex v is denoted
in(v) := {u ? V | uv ? E} .

The d ? 1 dimensional sphere is denoted Sd?1 = {x ? Rd | ?x? = 1}. We provide a brief overview
of reproducing kernel Hilbert spaces in the sequel and merely introduce notation here. In a Hilbert
space H, we use a slightly non-standard notation HB for the ball of radius B, {x ? H | ?x?H ? B}.
We use [x]+ to denote max(x, 0) and 1[b] to denote the indicator function of a binary variable b.
Input space. Throughout the paper we assume that each example is a sequence of n elements,
each of which? is represented
as a unit vector. Namely, we ?x n and take the input space to be
?n
X = Xn,d = Sd?1 . Each input example is denoted,
x = (x1 , . . . , xn ), where xi ? Sd?1 .

(1)

We refer to each vector xi as the input?s ith coordinate, and use xij to denote it jth scalar entry.
Though this notation is slightly non-standard, it uni?es input types seen in various domains. For
example, binary features can be encoded by taking d = 1, in which case X = {?1}n . Meanwhile,
images and audio signals are often represented as bounded and continuous numerical values; we can
assume in full generality that these values lie in [?1, 1]. To match the setup above, we embed [?1, 1]
into the circle S1 , e.g. through the map
? ?x ??
? ? ?x ?
, cos
.
x ?? sin
2
2
When each coordinate is categorical, taking one of d values, one can represent the category j ? [d]
by the unit vector ej ? Sd?1 . When d is very large or the basic units exhibit some structure?such as
when the input is a sequence of words?a more concise encoding may be useful, e.g. using unit vectors
?
in a low dimension space Sd where d? ? d (see for instance Levy and Goldberg [27], Mikolov et al.
[30]).
2

Supervised learning. The goal in supervised learning is to devise a mapping from the input space
X to an output space Y based on a sample S = {(x1 , y1 ), . . . , (xm , ym )}, where (xi , yi ) ? X ? Y,
drawn i.i.d. from a distribution D over X ? Y. A supervised learning problem is further speci?ed
by an output length k and a loss function ? : Rk ? Y ? [0, ?), and the goal is to ?nd a predictor
h : X ? Rk whose loss,
LD (h) := E ?(h(x), y)
(x,y)?D

is small. The empirical loss

m

LS (h) :=

1 ?
?(h(xi ), yi )
m i=1

is commonly used as a proxy for the loss LD . Regression problems correspond to Y = R and, for
instance, the squared loss ?(?
y , y) = (?
y ? y)2 . Binary classi?cation is captured by Y = {?1} and,
y , y) = 1[?
y y ? 0] or the hinge loss ?(?
y , y) = [1 ? y?y]+ , with standard
say, the zero-one loss ?(?
extensions to the multiclass case. A loss ? is L-Lipschitz if |?(y1 , y) ? ?(y2 , y)| ? L|y1 ? y2 | for all
y1 , y2 ? Rk , y ? Y, and it is convex if ?(?, y) is convex for every y ? Y.
Neural network learning. We de?ne a neural network N to be directed acyclic graph (DAG)
whose nodes are denoted V (N ) and edges E(N ). Each of its internal units, i.e. nodes with both
incoming and outgoing edges, is associated with an activation function ?v : R ? R. In this paper?s
context, an activation can be any function that is square integrable with respect to the Gaussian
measure on R. We say that ? is normalized if ??? = 1. The set of nodes having only incoming
edges are called the output nodes. To match the setup of a supervised learning problem, a network N
has nd input nodes and k output nodes, denoted o1 , . . . , ok . A network N together with a weight
vector w = {wuv | uv ? E} de?nes a predictor hN ,w : X ? Rk whose prediction is given by
?propagating? x forward through the network. Formally, we de?ne hv,w (?) to be the output of the
subgraph of the node v as follows: for an input node v, hv,w is the identity function, and for all other
nodes, we de?ne hv,w recursively as
?
??
hv,w (x) = ?v
w
h
(x)
.
uv
u,w
u?in(v)
Finally, we let hN ,w (x) = (ho1 ,w (x), . . . , hok ,w (x)). We also refer to internal nodes as hidden
units. The output layer of N is the sub-network consisting of all output neurons of N along with
their incoming edges. The representation induced by a network N is the network rep(N ) obtained
from N by removing the output layer. The representation function induced by the weights w is
RN ,w := hrep(N ),w . Given a sample S, a learning algorithm searches for weights w having small
?m
1
empirical loss LS (w) = m
i=1 ?(hN ,w (xi ), yi ). A popular approach is to randomly initialize the
weights and then use a variant of the stochastic gradient method to improve these weights in the
direction of lower empirical loss.

Kernel learning. A function ? : X ? X ? R is a reproducing kernel, or simply a kernel, if for
every x1 , . . . , xr ? X , the r ? r matrix ?i,j = {?(xi , xj )} is positive semi-de?nite. Each kernel
induces a Hilbert space H? of functions from X to R with a corresponding norm ???H? . A kernel and
its corresponding space are normalized if ?x ? X , ?(x, x) = 1. Given a convex loss function ?, a
k
sample S, and a kernel ?, a kernel
k ) ? H? whose
? learning algorithm ?nds a function f = (f1 , . . . , f?
1
empirical loss, LS (f ) = m i ?(f (xi ), yi ), is minimal among all functions with i ?fi ?2? ? R2
for some R > 0. Alternatively, kernel algorithms minimize the regularized loss,
LR
S (f ) =

k
m
1 ?
1 ?
?(f (xi ), yi ) + 2
?fi ?2? ,
m i=1
R i=1

a convex objective that often can be ef?ciently minimized.

3

Computation skeletons

In this section we de?ne a simple structure that we term a computation skeleton. The purpose of a
computational skeleton is to compactly describe feed-forward computation from an input to an output.
A single skeleton encompasses a family of neural networks that share the same skeletal structure.
Likewise, it de?nes a corresponding kernel space.
3

S1

S2

S3

S4

Figure 1: Examples of computation skeletons.
De?nition. A computation skeleton S is a DAG whose non-input nodes are labeled by activations.
Though the formal de?nition of neural networks and skeletons appear identical, we make a conceptual
distinction between them as their role in our analysis is rather different. Accompanied by a set of
weights, a neural network describes a concrete function, whereas the skeleton stands for a topology
common to several networks as well as for a kernel. To further underscore the differences we note
that skeletons are naturally more compact than networks. In particular, all examples of skeletons in
this paper are irreducible, meaning that for each two nodes v, u ? V (S), in(v) ?= in(u). We further
restrict our attention to skeletons with a single output node, showing later that single-output skeletons
can capture supervised problems with outputs in Rk . We denote by |S| the number of non-input
nodes of S.

Figure 1 shows four example skeletons, omitting the designation of the activation functions. The
skeleton S1 is rather basic as it aggregates all the inputs in a single step. Such topology can be
useful in the absence of any prior knowledge of how the output label may be computed from an input
example, and it is commonly used in natural language processing where the input is represented as a
bag-of-words [19]. The only structure in S1 is a single fully connected layer:
Terminology (Fully connected layer of a skeleton). An induced subgraph of a skeleton with r + 1
nodes, u1 , . . . , ur , v, is called a fully connected layer if its edges are u1 v, . . . , ur v.
The skeleton S2 is slightly more involved: it ?rst processes consecutive (overlapping) parts of the
input, and the next layer aggregates the partial results. Altogether, it corresponds to networks with a
single one-dimensional convolutional layer, followed by a fully connected layer. The two-dimensional
(and deeper) counterparts of such skeletons correspond to networks that are common in visual object
recognition.
Terminology (Convolution layer of a skeleton). Let s, w, q be positive integers and denote n =
s(q ? 1) + w. A subgraph of a skeleton is a one dimensional convolution layer of width w and stride s
if it has n + q nodes, u1 , . . . , un , v1 , . . . , vq , and qw edges, us(i?1)+j vi , for 1 ? i ? q, 1 ? j ? w.
The skeleton S3 is a somewhat more sophisticated version of S2 : the local computations are ?rst
aggregated, then reconsidered with the aggregate, and ?nally aggregated again. The last skeleton,
S4 , corresponds to the networks that arise in learning sequence-to-sequence mappings as used in
translation, speech recognition, and OCR tasks (see for example Sutskever et al. [44]).
3.1

From computation skeletons to neural networks

The following de?nition shows how a skeleton, accompanied with a replication parameter r ? 1 and
a number of output nodes k, induces a neural network architecture. Recall that inputs are ordered sets
of vectors in Sd?1 .
4

N (S, 5)

S

Figure 2: A 5-fold realizations of the computation skeleton S with d = 1.
De?nition (Realization of a skeleton). Let S be a computation skeleton and consider input coordinates in Sd?1 as in (1). For r, k ? 1 we de?ne the following neural network N = N (S, r, k). For
each input node in S, N has d corresponding input neurons. For each internal node v ? S labeled
by an activation ?, N has r neurons v 1 , . . . , v r , each with an activation ?. In addition, N has k
output neurons o1 , . . . , ok with the identity activation ?(x) = x. There is an edge v i uj ? E(N )
whenever uv ? E(S). For every output node v in S, each neuron v j is connected to all output
neurons o1 , . . . , ok . We term N the (r, k)-fold realization of S. We also de?ne the r-fold realization
of S as2 N (S, r) = rep (N (S, r, 1)).
Note that the notion of the replication parameter r corresponds, in the terminology of convolutional
networks, to the number of channels taken in a convolutional layer and to the number of hidden units
taken in a fully-connected layer.
Figure 2 illustrates a 5-realization of a skeleton with coordinate dimension d = 1. The realization is a
network with a single (one dimensional) convolutional layer having 5 channels, stride of 1, and width
of 2, followed by two fully-connected layers. The global replication parameter r in a realization
is used for brevity; it is straightforward to extend results when the different nodes in S are each
replicated to a different extent.
We next de?ne a scheme for random initialization of the weights of a neural network, that is similar
to what is often done in practice. We employ the de?nition throughout the paper whenever we refer
to random weights.
De?nition (Random weights). A random initialization of a neural network N is a multivariate
Gaussian w = (wuv )uv?E(N ) such that each
weight wuv
?
? is sampled independently from a normal
2
distribution with mean 0 and variance 1/ ??u ? |in(v)| .

Architectures such as convolutional nets have weights that are shared across different edges. Again, it
is straightforward to extend our results to these cases and for simplicity we assume no weight sharing.
3.2

From computation skeletons to reproducing kernels

In addition to networks? architectures, a computation skeleton S also de?nes a normalized kernel
?S : X ? X ? [?1, 1] and a corresponding norm ? ? ?S on functions f : X ? R. This norm has
the property that ?f ?S is small if and only if f can be obtained by certain simple compositions of
functions according to the structure of S. To de?ne the kernel, we introduce a dual activation and
dual kernel. For ? ? [?1, 1], we
by N? the multivariate Gaussian distribution on R2 with
? 1 ?denote
?
mean 0 and covariance matrix ? 1 .

De?nition (Dual activation and kernel). The dual activation of an activation ? is the function
?
? : [?1, 1] ? R de?ned as
?
? (?) =
E
?(X)?(Y ) .
(X,Y )?N?

The dual kernel w.r.t. to a Hilbert space H is the kernel ?? : H1 ? H1 ? R de?ned as
? (?x, y?H ) .
?? (x, y) = ?

2

Note that for every k, rep (N (S, r, 1)) = rep (N (S, r, k)).

5

Activation
Identity
2nd Hermite

x

ReLU

?

Step
Exponential

Dual Activation
?
?2

2
x?
?1
2

?
e

2 [x]+
2 1[x ? 0]

x?2

1
?
1
2
1
e

+
+
+

?
2
?
?
?
e

+
+
+

?

1??2 +(??cos?1 (?))?
?2
?4
2? + 24? + . . . =
?
??cos?1 (?)
?3
3?5
+
+
.
.
.
=
6?
40?
?
?2
?3
??1
+
+
.
.
.
=
e
2e
6e

Kernel
linear
poly

Ref

arccos1

[13]

arccos0
RBF

[13]
[29]

Table 1: Activation functions and their duals.
We show in the supplementary material that ?? is indeed a kernel for every activation ? that adheres
with the square-integrability requirement. In fact, any continuous ? : [?1, 1] ? R, such that
(x, y) ?? ?(?x, y?H ) is a kernel for all H, is the dual of some activation. Note that ?? is normalized
iff ? is normalized. We show in the supplementary material that dual activations are closely related
to Hermite polynomial expansions, and that these can be used to calculate the duals of activation
functions analytically. Table 1 lists a few examples of normalized activations and their corresponding
dual (corresponding derivations are in supplementary material). The following de?nition gives the
kernel corresponding to a skeleton having normalized activations.3
De?nition (Compositional kernels). Let S be a computation skeleton with normalized activations
and (single) output node o. For every node v, inductively de?ne a kernel ?v : X ? X ? R as follows.
For an input node v corresponding to the ith coordinate, de?ne ?v (x, y) = ?xi , yi ?. For a non-input
node v, de?ne
?
??
u?in(v) ?u (x, y)
.
?v
?v (x, y) = ?
|in(v)|
The ?nal kernel ?S is ?o , the kernel associated with the output node o. The resulting Hilbert space
and norm are denoted HS and ? ? ?S respectively, and Hv and ? ? ?v denote the space and norm
when formed at node v.

As we show later, ?S is indeed a (normalized) kernel for every skeleton S. To understand the
kernel in the context of learning, we need to examine which functions can be expressed as moderate
norm functions in HS . As we show in the supplementary material, these are the functions obtained
by certain simple compositions according to the feed-forward structure of S. For intuition, we
contrast two examples of two commonly used skeletons. For simplicity, we restrict to the case
X = Xn,1 = {?1}n , and omit the details of derivations.
Example 1 (Fully connected skeletons). Let S be a skeleton consisting of l fully connected
? layers,
?
?
where the i?th layer is associated with the activation ?i . We have ?S (x, x ) = ?
?l ? . . . ? ?
?1 ?x,y?
.
n
For such kernels, any moderate norm function in H is well approximated by a low degree polynomial.
? ?
For example, if ?f ?S ? n, then there is a second degree polynomial p such that ?f ? p?2 ? O ?1n .

We next argue that convolutional skeletons de?ne kernel spaces that are quite different from kernels
spaces
de?ned by fully connected skeletons. Concretely, suppose f : X ? R is of the form
?m
f = i=1 fi where each fi depends only on q adjacent coordinates. We call f a q-local function. In
Example 1 we stated that for fully-connected skeletons, any function of in the induced space of norm
less then n is well approximated by second degree polynomials. In contrast, the following example
underscores that for convolutional skeletons, we can ?nd functions that are more complex, provided
that they are local.
Example 2 (Convolutional skeletons). Let S be a skeleton consisting of a convolutional layer of
stride 1 and width q, followed by a single fully connected layer. (The skeleton S2 from Figure 1 is a
concrete example with q = 2 and n = 4.) To simplify the argument, we assume that all activations
are ?(x) = ex and q is a constant. For any q-local function f : X ? R we have
?
?f ?S ? C ? n ? ?f ?2 .
3
For a skeleton S with unnormalized activations, the corresponding kernel is the kernel of the skeleton S ?
obtained by normalizing the activations of S.

6

Here, C > 0 is a constant depending only on q. Hence, for example, any average of functions
from X to ?
[?1, 1], each of which depends on q adjacent coordinates, is in HS and has norm of
merely O ( n).

4

Main results

We review our main results. Proofs can be found in the supplementary material. Let us ?x a
compositional kernel S. There are a few upshots to underscore upfront. First, our analysis implies
that a representation generated by a random initialization of N = N (S, r, k) approximates the kernel
?S . The sense in which the result holds is twofold. First, with the proper rescaling we show that
?RN ,w (x), RN ,w (x? )? ? ?S (x, x? ). Then, we also show that the functions obtained by composing
bounded linear functions with RN ,w are approximately the bounded-norm functions in HS . In other
words, the functions expressed by N under varying the weights of the ?nal layer are approximately
bounded-norm functions in HS . For simplicity, we restrict the analysis to the case k = 1. We also
con?ne the analysis to either bounded activations, with bounded ?rst and second derivatives, or the
ReLU activation. Extending the results to a broader family of activations is left for future work.
Through this and remaining sections we use ? to hide universal constants.
De?nition. An activation ? : R ? R is C-bounded if it is twice continuously differentiable and
???? , ?? ? ?? , ?? ?? ?? ? ???C.
Note that many activations are C-bounded for some constant
? C > 0. In particular, most of the popular
sigmoid-like functions such as 1/(1 + e?x ), erf(x), x/ 1 + x2 , tanh(x), and tan?1 (x) satisfy the
boundedness requirements. We next introduce terminology that parallels the representation layer
of N with a kernel space. Concretely, let N be a network whose representation part has q output
neurons. Given weights w, the normalized representation
?w is obtained from the representation
?
RN ,w by dividing each output neuron v by ??v ? q. The empirical kernel corresponding to w is
de?ned as ?w (x, x? ) = ??w (x), ?w (x? )?. We also de?ne the empirical kernel space corresponding
to w as Hw = H?w . Concretely,
Hw = {hv (x) = ?v, ?w (x)? | v ? Rq } ,

and the norm of Hw is de?ned as ?h?w = inf{?v? | h = hv }. Our ?rst result shows that the
empirical kernel approximates the kernel kS .
Theorem 3. Let S be a skeleton with C-bounded activations. Let w be a random initialization of
N = N (S, r) with
(4C 4 )depth(S)+1 log (8|S|/?)
.
r?
?2
Then, for all x, x? ? X , with probability of at least 1 ? ?,
|kw (x, x? ) ? kS (x, x? )| ? ? .

We note that if we ?x the activation and assume that the depth of S is logarithmic, then the required
bound on r is polynomial. For the ReLU activation we get a stronger bound with only quadratic
dependence on the depth. However, it requires that ? ? 1/depth(S).
Theorem 4. Let S be a skeleton with ReLU activations. Let w be a random initialization of N (S, r)
with
depth2 (S) log (|S|/?)
r?
.
?2
?
Then, for all x, x ? X and ? ? 1/depth(S), with probability of at least 1 ? ?,
|?w (x, x? ) ? ?S (x, x? )| ? ? .

For the remaining theorems, we ?x a L-Lipschitz loss ? : R ? Y ? [0, ?). For a distribution D on
X ? Y we denote by ?D?0 the cardinality of the support of the distribution. We note that log (?D?0 )
is bounded by, for instance, the number of bits used to represent an element in X ? Y. We use the
following notion of approximation.
De?nition. Let D be a distribution on X ?Y. A space H1 ? RX ?-approximates the space H2 ? RX
w.r.t. D if for every h2 ? H2 there is h1 ? H1 such that LD (h1 ) ? LD (h2 ) + ?.
7

Theorem 5. Let S be a skeleton with C-bounded activations and let R > 0. Let w be a random
initialization of N (S, r) with
?
?
L4 R4 (4C 4 )depth(S)+1 log LRC|S|
??
.
r?
?4
Then,
with probability of at least 1?? ? over the choices of w we have that, for any data distribution,
?
R
Hw2R ?-approximates HSR and HS 2R ?-approximates Hw
.
Theorem 6. Let S be a skeleton with ReLU activations, ? ? 1/depth(C), and R > 0. Let w be a
random initialization of N (S, r) with
?
?
L4 R4 depth2 (S) log ?D??0 |S|
r?
.
?4
Then,
with probability of at least 1?? ? over the choices of w we have that, for any data distribution,
?
R
Hw2R ?-approximates HSR and HS 2R ?-approximates Hw
.
As in Theorems 3 and 4, for a ?xed C-bounded activation and logarithmically deep S, the required
bounds on r are polynomial. Analogously, for the ReLU activation the bound is polynomial even
without restricting the depth. However, the polynomial growth in Theorems 5 and 6 is rather large.
Improving the bounds, or proving their optimality, is left to future work.
Acknowledgments
We would like to thank Percy Liang and Ben Recht for fruitful discussions, comments, and suggestions.

References
[1] A. Andoni, R. Panigrahy, G. Valiant, and L. Zhang. Learning polynomials with neural networks. In
Proceedings of the 31st International Conference on Machine Learning, pages 1908?1916, 2014.
[2] F. Anselmi, L. Rosasco, C. Tan, and T. Poggio. Deep convolutional networks are hierarchical kernel
machines. arXiv:1508.01084, 2015.
[3] M. Anthony and P. Bartlet. Neural Network Learning: Theoretical Foundations. Cambridge University
Press, 1999.
[4] S. Arora, A. Bhaskara, R. Ge, and T. Ma. Provable bounds for learning some deep representations. In
Proceedings of The 31st International Conference on Machine Learning, pages 584?592, 2014.
[5] F. Bach. Breaking the curse of dimensionality with convex neural networks. arXiv:1412.8690, 2014.
[6] F. Bach. On the equivalence between kernel quadrature rules and random feature expansions. 2015.
[7] A.R. Barron. Universal approximation bounds for superposition of a sigmoidal function. IEEE Transactions
on Information Theory, 39(3):930?945, 1993.
[8] P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and structural
results. Journal of Machine Learning Research, 3:463?482, 2002.
[9] P.L. Bartlett. The sample complexity of pattern classi?cation with neural networks: the size of the weights
is more important than the size of the network. IEEE Transactions on Information Theory, 44(2):525?536,
March 1998.
[10] E.B. Baum and D. Haussler. What size net gives valid generalization? Neural Computation, 1(1):151?160,
1989.
[11] L. Bo, K. Lai, X. Ren, and D. Fox. Object recognition with hierarchical kernel descriptors. In Computer
Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 1729?1736. IEEE, 2011.
[12] J. Bruna and S. Mallat. Invariant scattering convolution networks. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 35(8):1872?1886, 2013.
[13] Y. Cho and L.K. Saul. Kernel methods for deep learning. In Advances in neural information processing
systems, pages 342?350, 2009.
[14] A. Choromanska, M. Henaff, M. Mathieu, G. Ben Arous, and Y. LeCun. The loss surfaces of multilayer
networks. In AISTATS, pages 192?204, 2015.
[15] A. Daniely and S. Shalev-Shwartz. Complexity theoretic limitations on learning DNFs. In COLT, 2016.
[16] R. Giryes, G. Sapiro, and A.M. Bronstein. Deep neural networks with random gaussian weights: A
universal classi?cation strategy? arXiv preprint arXiv:1504.08291, 2015.

8

[17] K. Grauman and T. Darrell. The pyramid match kernel: Discriminative classi?cation with sets of image
features. In Tenth IEEE International Conference on Computer Vision, volume 2, pages 1458?1465, 2005.
[18] M. Hardt, B. Recht, and Y. Singer. Train faster, generalize better: Stability of stochastic gradient descent.
arXiv:1509.01240, 2015.
[19] Z.S. Harris. Distributional structure. Word, 1954.
[20] T. Hazan and T. Jaakkola. Steps toward deep kernel methods from in?nite neural networks.
arXiv:1508.05133, 2015.
[21] P. Kar and H. Karnick. Random feature maps for dot product kernels. arXiv:1201.6530, 2012.
[22] R.M. Karp and R.J. Lipton. Some connections between nonuniform and uniform complexity classes. In
Proceedings of the twelfth annual ACM symposium on Theory of computing, pages 302?309. ACM, 1980.
[23] M. Kearns and L.G. Valiant. Cryptographic limitations on learning Boolean formulae and ?nite automata.
In STOC, pages 433?444, May 1989.
[24] A.R. Klivans and A.A. Sherstov. Cryptographic hardness for learning intersections of halfspaces. In FOCS,
2006.
[25] A. Krizhevsky, I. Sutskever, and G.E. Hinton. Imagenet classi?cation with deep convolutional neural
networks. In Advances in neural information processing systems, pages 1097?1105, 2012.
[26] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521(7553):436?444, 2015.
[27] O. Levy and Y. Goldberg. Neural word embedding as implicit matrix factorization. In Advances in Neural
Information Processing Systems, pages 2177?2185, 2014.
[28] R. Livni, S. Shalev-Shwartz, and O. Shamir. On the computational ef?ciency of training neural networks.
In Advances in Neural Information Processing Systems, pages 855?863, 2014.
[29] J. Mairal, P. Koniusz, Z. Harchaoui, and Cordelia Schmid. Convolutional kernel networks. In Advances in
Neural Information Processing Systems, pages 2627?2635, 2014.
[30] T. Mikolov, I. Sutskever, K. Chen, G.S. Corrado, and J. Dean. Distributed representations of words and
phrases and their compositionality. In NIPS, pages 3111?3119, 2013.
[31] R.M. Neal. Bayesian learning for neural networks, volume 118. Springer Science & Business Media,
2012.
[32] B. Neyshabur, R. R Salakhutdinov, and N. Srebro. Path-SGD: Path-normalized optimization in deep neural
networks. In Advances in Neural Information Processing Systems, pages 2413?2421, 2015.
[33] B. Neyshabur, N. Srebro, and R. Tomioka. Norm-based capacity control in neural networks. In COLT,
2015.
[34] R. O?Donnell. Analysis of boolean functions. Cambridge University Press, 2014.
[35] J. Pennington, F. Yu, and S. Kumar. Spherical random features for polynomial kernels. In Advances in
Neural Information Processing Systems, pages 1837?1845, 2015.
[36] A. Rahimi and B. Recht. Random features for large-scale kernel machines. In NIPS, pages 1177?1184,
2007.
[37] A. Rahimi and B. Recht. Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning. In Advances in neural information processing systems, pages 1313?1320, 2009.
[38] I. Safran and O. Shamir. On the quality of the initial basin in overspeci?ed neural networks.
arxiv:1511.04210, 2015.
[39] S. Saitoh. Theory of reproducing kernels and its applications. Longman Scienti?c & Technical England,
1988.
[40] I.J. Schoenberg et al. Positive de?nite functions on spheres. Duke Mathematical Journal, 9(1):96?108,
1942.
[41] B. Sch?lkopf, P. Simard, A. Smola, and V. Vapnik. Prior knowledge in support vector kernels. In Advances
in Neural Information Processing Systems 10, pages 640?646. MIT Press, 1998.
[42] H. Sedghi and A. Anandkumar. Provable methods for training neural networks with sparse connectivity.
arXiv:1412.2693, 2014.
[43] S. Shalev-Shwartz and S. Ben-David. Understanding Machine Learning: From Theory to Algorithms.
Cambridge University Press, 2014.
[44] I. Sutskever, O. Vinyals, and Q.V. Le. Sequence to sequence learning with neural networks. In Advances
in neural information processing systems, pages 3104?3112, 2014.
[45] C.K.I. Williams. Computation with in?nite neural networks. pages 295?301, 1997.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 5267-on-the-computational-efficiency-of-training-neural-networks.pdf

On the Computational Efficiency of Training Neural
Networks
Roi Livni
The Hebrew University
roi.livni@mail.huji.ac.il

Shai Shalev-Shwartz
The Hebrew University
shais@cs.huji.ac.il

Ohad Shamir
Weizmann Institute of Science
ohad.shamir@weizmann.ac.il

Abstract
It is well-known that neural networks are computationally hard to train. On the
other hand, in practice, modern day neural networks are trained efficiently using SGD and a variety of tricks that include different activation functions (e.g.
ReLU), over-specification (i.e., train networks which are larger than needed), and
regularization. In this paper we revisit the computational complexity of training
neural networks from a modern perspective. We provide both positive and negative results, some of them yield new provably efficient and practical algorithms
for training certain types of neural networks.

1

Introduction

One of the most significant recent developments in machine learning has been the resurgence of
?deep learning?, usually in the form of artificial neural networks. A combination of algorithmic
advancements, as well as increasing computational power and data size, has led to a breakthrough
in the effectiveness of neural networks, and they have been used to obtain very impressive practical
performance on a variety of domains (a few recent examples include [17, 16, 24, 10, 7]).
A neural network can be described by a (directed acyclic) graph, where each vertex in the graph corresponds to a neuron and each edge is associated with a weight. Each neuron calculates a weighted
sum of the outputs of neurons which are connected to it (and possibly adds a bias term). It then
passes the resulting number through an activation function ? : R ? R and outputs the resulting
number. We focus on feed-forward neural networks, where the neurons are arranged in layers, in
which the output of each layer forms the input of the next layer. Intuitively, the input goes through
several transformations, with higher-level concepts derived from lower-level ones. The depth of the
network is the number of layers and the size of the network is the total number of neurons.
From the perspective of statistical learning theory, by specifying a neural network architecture (i.e.
the underlying graph and the activation function) we obtain a hypothesis class, namely, the set of all
prediction rules obtained by using the same network architecture while changing the weights of the
network. Learning the class involves finding a specific set of weights, based on training examples,
which yields a predictor that has good performance on future examples. When studying a hypothesis
class we are usually concerned with three questions:
1. Sample complexity: how many examples are required to learn the class.
2. Expressiveness: what type of functions can be expressed by predictors in the class.
3. Training time: how much computation time is required to learn the class.
For simplicity, let us first consider neural networks with a threshold activation function (i.e. ?(z) =
1 if z > 0 and 0 otherwise), over the boolean input space, {0, 1}d , and with a single output in
{0, 1}. The sample complexity of such neural networks is well understood [3]. It is known that the
VC dimension grows linearly with the number of edges (up to log factors). It is also easy to see that
no matter what the activation function is, as long as we represent each weight of the network using
1

a constant number of bits, the VC dimension is bounded by a constant times the number of edges.
This implies that empirical risk minimization - or finding weights with small average loss over the
training data - can be an effective learning strategy from a statistical point of view.
As to the expressiveness of such networks, it is easy to see that neural networks of depth 2 and
sufficient size can express all functions from {0, 1}d to {0, 1}. However, it is also possible to show
that for this to happen, the size of the network must be exponential in d (e.g. [19, Chapter 20]).
Which functions can we express using a network of polynomial size? The theorem below shows that
all boolean functions that can be calculated in time O(T (d)), can also be expressed by a network of
depth O(T (d)) and size O(T (d)2 ).
Theorem 1. Let T : N ? N and for every d, let Fd be the set of functions that can be implemented
by a Turing machine using at most T (d) operations. Then there exist constants b, c ? R+ such that
for every d, there is a network architecture of depth c T (d) + b, size of (c T (d) + b)2 , and threshold
activation function, such that the resulting hypotesis class contains Fd .
The proof of the theorem follows directly from the relation between the time complexity of programs
and their circuit complexity (see, e.g., [22]), and the fact that we can simulate the standard boolean
gates using a fixed number of neurons.
We see that from the statistical perspective, neural networks form an excellent hypothesis class; On
one hand, for every runtime T (d), by using depth of O(T (d)) we contain all predictors that can be
run in time at most T (d). On the other hand, the sample complexity of the resulting class depends
polynomially on T (d).
The main caveat of neural networks is the training time. Existing theoretical results are mostly
negative, showing that successfully learning with these networks is computationally hard in the worst
case. For example, neural networks of depth 2 contain the class of intersection of halfspaces (where
the number of halfspaces is the number of neurons in the hidden layer). By reduction to k-coloring,
it has been shown that finding the weights that best fit the training set is NP-hard ([9]). [6] has
shown that even finding weights that result in close-to-minimal empirical error is computationally
infeasible. These hardness results focus on proper learning, where the goal is to find a nearly-optimal
predictor with a fixed network architecture A. However, if our goal is to find a good predictor, there
is no reason to limit ourselves to predictors with one particular architecture. Instead, we can try,
for example, to find a network with a different architecture A0 , which is almost as good as the
best network with architecture A. This is an example of the powerful concept of improper learning,
which has often proved useful in circumventing computational hardness results. Unfortunately, there
are hardness results showing that even with improper learning, and even if the data is generated
exactly from a small, depth-2 neural network, there are no efficient algorithms which can find a
predictor that performs well on test data. In particular, [15] and [12] have shown this in the case of
learning intersections of halfspaces, using cryptographic and average case complexity assumptions.
On a related note, [4] recently showed positive results on learning from data generated by a neural
network of a certain architecture and randomly connected weights. However, the assumptions used
are strong and unlikely to hold in practice.
Despite this theoretical pessimism, in practice, modern-day neural networks are trained successfully
in many learning problems. There are several tricks that enable successful training:
? Changing the activation function: The threshold activation function, ?(a) = 1a>0 , has zero
derivative almost everywhere. Therefore, we cannot apply gradient-based methods with this activation function. To circumvent this problem, we can consider other activation functions. Most
1
widely known is a sigmoidal activation, e.g. ?(a) = 1+e
a , which forms a smooth approximation of the threshold function. Another recent popular activation function is the rectified linear
unit (ReLU) function, ?(a) = max{0, a}. Note that subtracting a shifted ReLU from a ReLU
yields an approximation of the threshold function, so by doubling the number of neurons we can
approximate a network with threshold activation by a network with ReLU activation.
? Over-specification: It was empirically observed that it is easier to train networks which are larger
than needed. Indeed, we empirically demonstrate this phenomenon in Sec. 5.
? Regularization: It was empirically observed that regularizing the weights of the network speeds
up the convergence (e.g. [16]).
2

The goal of this paper is to revisit and re-raise the question of neural network?s computational efficiency, from a modern perspective. This is a challenging topic, and we do not pretend to give any
definite answers. However, we provide several results, both positive and negative. Most of them are
new, although a few appeared in the literature in other contexts. Our contributions are as follows:
? We make a simple observation that for sufficiently over-specified networks, global optima are
ubiquitous and in general computationally easy to find. Although this holds only for extremely
large networks which will overfit, it can be seen as an indication that the computational hardness of learning does decrease with the amount of over-specification. This is also demonstrated
empirically in Sec. 5.
? Motivated by the idea of changing the activation function, we consider the quadratic activation
function, ?(a) = a2 . Networks with the quadratic activation compute polynomial functions of
the input in Rd , hence we call them polynomial networks. Our main findings for such networks
are as follows:
? Networks with quadratic activation are as expressive as networks with threshold activation.
? Constant depth networks with quadratic activation can be learned in polynomial time.
? Sigmoidal networks of depth 2, and with `1 regularization, can be approximated by polynomial
networks of depth O(log log(1/)). It follows that sigmoidal networks with `1 regularization
can be learned in polynomial time as well.
? The aforementioned positive results are interesting theoretically, but lead to impractical algorithms. We provide a practical, provably correct, algorithm for training depth-2 polynomial
networks. While such networks can also be learned using a linearization trick, our algorithm is
more efficient and returns networks whose size does not depend on the data dimension. Our algorithm follows a forward greedy selection procedure, where each step of the greedy selection
procedure builds a new neuron by solving an eigenvalue problem.
? We generalize the above algorithm to depth-3, in which each forward greedy step involves an
efficient approximate solution to a tensor approximation problem. The algorithm can learn a
rich sub-class of depth-3 polynomial networks.
? We describe some experimental evidence, showing that our practical algorithm is competitive
with state-of-the-art neural network training methods for depth-2 networks.

2

Sufficiently Over-Specified Networks Are Easy to Train

We begin by considering the idea of over-specification, and make an observation that for sufficiently
over-specified networks, the optimization problem associated with training them is generally quite
easy to solve, and that global optima are in a sense ubiquitous. As an interesting contrast, note that
for very small networks (such as a single neuron with a non-convex activation function), the associated optimization problem is generally hard, and can exhibit exponentially many local (non-global)
minima [5]. We emphasize that our observation only holds for extremely large networks, which will
overfit in any reasonable scenario, but it does point to a possible spectrum where computational cost
decreases with the amount of over-specification.
To present the result, let X ? Rd,m be a matrix of m training examples in Rd . We can think of the
network as composed of two mappings. The first maps X into a matrix Z ? Rn,m , where n is the
number of neurons whose outputs are connected to the output layer. The second mapping is a linear
mapping Z 7? W Z, where W ? Ro,n , that maps Z to the o neurons in the output layer. Finally,
there is a loss function ` : Ro,m ? R, which we?ll assume to be convex, that assesses the quality of
the prediction on the entire data (and will of course depend on the m labels). Let V denote all the
weights that affect the mapping from X to Z, and denote by f (V ) the function that maps V to Z.
The optimization problem associated with learning the network is therefore minW,V `(W f (V )).
The function `(W f (V )) is generally non-convex, and may have local minima. However, if n ? m,
then it is reasonable to assume that Rank(f (V )) = m with large probability (under some random
choice of V ), due to the non-linear nature of the function computed by neural networks1 . In that
case, we can simply fix V and solve minW `(W f (V )), which is computationally tractable as ` is
1
For example, consider the function computed by the first layer, X 7? ?(Vd X), where ? is a sigmoid
function. Since ? is non-linear, the columns of ?(Vd X) will not be linearly dependent in general.

3

assumed to be convex. Since f (V ) has full rank, the solution of this problem corresponds to a global
optima of `, and hence to a global optima of the original optimization problem. Thus, for sufficiently
large networks, finding global optima is generally easy, and they are in a sense ubiquitous.

3

The Hardness of Learning Neural Networks

We now review several known hardness results and apply them to our learning setting. For simplicity, throughout most of this section we focus on the PAC model in the binary classification case, over
the Boolean cube, in the realizable case, and with a fixed target accuracy.2
Fix some , ? ? (0, 1). For every dimension d, let the input space be Xd = {0, 1}d and let H be a
hypothesis class of functions from Xd to {?1}. We often omit the subscript d when it is clear from
context. A learning algorithm A has access to an oracle that samples x according to an unknown
distribution D over X and returns (x, f ? (x)), where f ? is some unknown target hypothesis in H.
The objective of the algorithm is to return a classifier f : X ? {?1}, such that with probability of
at least 1 ? ?,
Px?D [f (x) 6= f ? (x)] ? .
We say that A is efficient if it runs in time poly(d) and the function it returns can also be evaluated
on a new instance in time poly(d). If there is such A, we say that H is efficiently learnable.
In the context of neural networks, every network architecture defines a hypothesis class, Nt,n,? ,
that contains all target functions f that can be implemented using a neural network with t layers, n
neurons (excluding input neurons), and an activation function ?. The immediate question is which
Nt,n,? are efficiently learnable. We will first address this question for the threshold activation function, ?0,1 (z) = 1 if z > 0 and 0 otherwise.
Observing that depth-2 networks with the threshold activation function can implement intersections
of halfspaces, we will rely on the following hardness results, due to [15].
Theorem 2 (Theorem 1.2 in [15]). Let X = {?1}d , let


	
H a = x ? ?0,1 w> x ? b ? 1/2 : b ? N, w ? Nd , |b| + kwk1 ? poly(d) ,
and let Hka = {x ? h1 (x) ? h2 (x) ? . . . ? hk (x) : ?i, hi ? H a }, where k = d? for some constant
? > 0. Then under a certain cryptographic assumption, Hka is not efficiently learnable.
Under a different complexity assumption, [12] showed a similar result even for k = ?(1).
As mentioned before, neural networks of depth ? 2 and with the ?0,1 activation function can
express intersections of halfspaces: For example, the first layer consists of k neurons computing the
P k halfspaces, and the second layer computes their conjunction by the mapping x 7?
?0,1 ( i xi ? k + 1/2). Trivially, if some class H is not efficiently learnable, then any class containing it is also not efficiently learnable. We thus obtain the following corollary:
Corollary 1. For every t ? 2, n = ?(1), the class Nt,n,?0,1 is not efficiently learnable (under the
complexity assumption given in [12]).
What happens when we change the activation function? In particular, two widely used activation
functions for neural networks are the sigmoidal activation function, ?sig (z) = 1/(1 + exp(?z)),
and the rectified linear unit (ReLU) activation function, ?relu (z) = max{z, 0}.
As a first observation, note that for |z|  1 we have that ?sig (z) ? ?0,1 (z). Our data domain is
the discrete Boolean cube, hence if we allow the weights of the network to be arbitrarily large, then
Nt,n,?0,1 ? Nt,n,?sig . Similarly, the function ?relu (z)??relu (z?1) equals ?0,1 (z) for every |z| ? 1.
As a result, without restricting the weights, we can simulate each threshold activated neuron by two
ReLU activated neurons, which implies that Nt,n,?0,1 ? Nt,2n,?relu . Hence, Corollary 1 applies to
both sigmoidal networks and ReLU networks as well, as long as we do not regularize the weights of
the network.
2
While we focus on the realizable case (i.e., there exists f ? ? H that provides perfect predictions), with a
fixed accuracy () and confidence (?), since we are dealing with hardness results, the results trivially apply to
the agnostic case and to learning with arbitrarily small accuracy and confidence parameters.

4

What happens when we do regularize the weights? Let Nt,n,?,L be all target functions that can be
implemented using a neural network of depth t, size n, activation function ?, and when we restrict
the input weights of each neuron to be kwk1 + |b| ? L.
One may argue that in many real world distributions, the difference between the two classes, Nt,n,?,L
and Nt,n,?0,1 is small. Roughly speaking, when the distribution density is low around the decision
boundary of neurons (similarly to separation with margin assumptions), then sigmoidal neurons will
be able to effectively simulate threshold activated neurons.
In practice, the sigmoid and ReLU activation functions are advantageous over the threshold activation function, since they can be trained using gradient based methods. Can these empirical successes
be turned into formal guarantees? Unfortunately, a closer examination of Thm. 2 demonstrates that
if L = ?(d) then learning N2,n,?sig ,L and N2,n,?relu ,L is still hard. Formally, to apply these networks to binary classification, we follow a standard definition of learning with a margin assumption:
We assume that the learner receives examples of the form (x, sign(f ? (x))) where f ? is a real-valued
function that comes from the hypothesis class, and we further assume that |f ? (x)| ? 1. Even under
this margin assumption, we have the following:
Corollary 2. For every t ? 2, n = ?(1), L = ?(d), the classes Nt,n,?sig ,L and Nt,n,?relu ,L are not
efficiently learnable (under the complexity assumption given in [12]).
A proof is provided in the appendix. What happens when L is much smaller? Later on in the paper
we will show positive results for L being a constant and the depth being fixed. These results will be
obtained using polynomial networks, which we study in the next section.

4

Polynomial Networks

In the previous section we have shown several strong negative results for learning neural networks
with the threshold, sigmoidal, and ReLU activation functions. One way to circumvent these hardness
results is by considering another activation function. Maybe the simplest non-linear function is
the squared function, ?2 (x) = x2 . We call networks that use this activation function polynomial
networks, since they compute polynomial functions of their inputs. As in the previous section, we
denote by Nt,n,?2 ,L the class of functions that can be implemented using a neural network of depth
t, size n, squared activation function, and a bound L on the `1 norm of the input weights of each
neuron. Whenever we do not specify L we refer to polynomial networks with unbounded weights.
Below we study the expressiveness and computational complexity of polynomial networks. We
note that algorithms for efficiently learning (real-valued) sparse or low-degree polynomials has been
studied in several previous works (e.g. [13, 14, 8, 2, 1]). However, these rely on strong distributional
assumptions, such as the data instances having a uniform or log-concave distribution, while we are
interested in a distribution-free setting.
4.1

Expressiveness

We first show that, similarly to networks with threshold activation, polynomial networks of polynomial size can express all functions that can be implemented efficiently using a Turing machine.
Theorem 3 (Polynomial networks can express Turing Machines). Let Fd and T be as in Thm. 1.
Then there exist constants b, c ? R+ such that for every d, the class Nt,n,?2 ,L , with t =
c T (d) log(T (d)) + b, n = t2 , and L = b, contains Fd .
The proof of the theorem relies on the result of [18] and is given in the appendix.
Another relevant expressiveness result, which we will use later, shows that polynomial networks can
approximate networks with sigmoidal activation functions:
?
Theorem 4. Fix 0 <  < 1, L ? 3 and t ? N. There are Bt ? O(log(tL
+ L log 1 )) and
1
?
Bn ? O(tL + L log  ) such that for every f ? Nt,n,?sig ,L there is a function g ? NtBt ,nBn ,?2 , such
that supkxk? <1 kf (x) ? g(x)k? ? .
The proof relies on an approximation of the sigmoid function based on Chebyshev polynomials, as
was done in [21], and is given in the appendix.
5

4.2

Training Time

We now turn to the computational complexity of learning polynomial networks. We first show that
it is hard to learn polynomial networks of depth ?(log(d)). Indeed, by combining Thm. 4 and
Corollary 2 we obtain the following:
Corollary 3. The class Nt,n,?2 , where t = ?(log(d)) and n = ?(d), is not efficiently learnable.
On the flip side, constant-depth polynomial networks can be learned in polynomial time, using a
simple linearization trick. Specifically, the class of polynomial networks of constant depth t is
contained in the class of multivariate polynomials of total degree at most s = 2t . This class can
be represented as a ds -dimensional linear space, where each vector is the coefficient vector of some
such polynomial. Therefore, the class of polynomial networks of depth t can be learned in time
t
poly(d2 ), by mapping each instance vector x ? Rd to all of its monomials, and learning a linear
predictor on top of this representation (which can be done efficiently in the realizable case, or when
a convex loss function is used). In particular, if t is a constant then so is 2t and therefore polynomial
networks of constant depth are efficiently learnable. Another way to learn this class is using support
vector machines with polynomial kernels.
An interesting application of this observation is that depth-2 sigmoidal networks are efficiently learnable with sufficient regularization, as formalized in the result below. This contrasts with corollary 2,
which provides a hardness result without regularization.
Theorem 5. The class N2,n,?sig ,L can be learned, to accuracy , in time poly(T ) where T =
2
(1/) ? O(d4L ln(11L +1) ).
The idea of the proof is as follows. Suppose that we obtain data from some f ? N2,n,?sig ,L . Based
on Thm. 4, there is g ? N2Bt ,nBn ,?2 that approximates f to some fixed accuracy 0 = 0.5, where Bt
and Bn are as defined in Thm. 4 for t = 2. Now we can learn N2Bt ,nBn ,?2 by considering the class
of all polynomials of total degree 22Bt , and applying the linearization technique discussed above.
Since f is assumed to separate the data with margin 1 (i.e. y = sign(f ? (x)),|f ? (x)| ? 1|), then g
separates the data with margin 0.5, which is enough for establishing accuracy  in sample and time
that depends polynomially on 1/.
4.3

Learning 2-layer and 3-layer Polynomial Networks

While interesting theoretically, the above results are not very practical, since the time and sample
complexity grow very fast with the depth of the network.3 In this section we describe practical,
provably correct, algorithms for the special case of depth-2 and depth-3 polynomial networks, with
some additional constraints. Although such networks can be learned in polynomial time via explicit
linearization (as described in section 4.2), the runtime and resulting network size scales quadratically
(for depth-2) or cubically (for depth-3) with the data dimension d. In contrast, our algorithms and
guarantees have a much milder dependence on d.
We first consider 2 layer polynomial networks, of the following form:
(
)
k
X
>
> 2
P2,k = x 7? b + w0 x +
?i (wi x) : ?i ? 1, |?i | ? 1, kwi k2 = 1 .
i=1

This networks corresponds to one hidden layer containing r neurons with the squared activation
function, where we restrict the input weights of all neurons in the network to have bounded `2 norm,
and where we also allow a direct linear dependency between the input layer and the output layer.
We?ll describe an efficient algorithm for learning this class, which is based on the GECO algorithm
for convex optimization with low-rank constraints [20].
3
If one uses SVM with polynomial kernels, the time and sample complexity may be small under margin
assumptions in a feature space corresponding to a given kernel. Note, however, that large margin in that space
is very different than the assumption we make here, namely, that there is a network with a small number of
hidden neurons that works well on the data.

6

The goal of the algorithm is to find f that minimizes the objective
m

R(f ) =

1 X
`(f (xi ), yi ),
m i=1

(1)

where ` : R ? R ? R is a loss function. We?ll assume that ` is ?-smooth and convex.
The basic idea of the algorithm is to gradually add hidden neurons to the hidden layer, in a greedy
manner, so as to decrease the loss function over the data. To do so, define V = {x 7? (w> x)2 :
kwk2 = 1} the set of functions that can be implemented by hidden neurons. Then every f ? P2,r
is an affine function plus a weighted sum of functions from V. The algorithm starts with f being
the minimizer of R over all affine functions. Then at each greedy step, we search for g ? V that
minimizes a first order approximation of R(f + ?g):
m

R(f + ?g) ? R(f ) + ?

1 X 0
` (f (xi ), yi )g(xi ) ,
m i=1

(2)

where `0 is the derivative of ` w.r.t. its first argument. Observe that for every g ? V there is some w
> 2
>
>
with kwk2 = 1 for which g(x) = (w
the right-hand side of Eq. (2) can

Pm x) 0 = w xx w.>Hence,
> 1
be rewritten as R(f ) + ? w m i=1 ` (f (xi ), yi )xi xi w . The vector w that minimizes this

Pm 0
1
>
expression (for positive ?) is the leading eigenvector of the matrix m
i=1 ` (f (xi ), yi )xi xi .
We add this vector as a hidden neuron to the network.4 Finally, we minimize R w.r.t. the weights
from the hidden layer to the output layer (namely, w.r.t. the weights ?i ).
The following theorem, which follows directly from Theorem 1 of [20], provides convergence guarantee for GECO. Observe that the theorem gives guarantee for learning P2,k if we allow to output
an over-specified network.
Theorem 6. Fix some  > 0. Assume that the loss function is convex and ?-smooth. Then if
2
the GECO Algorithm is run for r > 2?k
iterations, it outputs a network f ? N2,r,?2 for which

?
R(f ) ? minf ? ?P2,k R(f ) + .
We next consider a hypothesis class consisting of third degree polynomials, which is a subset of
3-layer polynomial networks (see Lemma 1nin the appendix) . The hidden neurons
o will be functions
Qi
from the class: V = ?3i=1 Vi where Vi = x 7? j=1 (wj> x) : ?j, kwj k2 = 1 . The hypothesis
n
o
Pk
class we consider is P3,k = x 7? i=1 ?i gi (x) : ?i, |?i | ? 1, gi ? V .
The basic idea of the algorithm is the same as for 2-layer networks. However, while in the 2-layer
case we could implement efficiently each greedy step by solving an eigenvalue problem, we now
face the following tensor approximation problem at each greedy step:
m

max
g?V3

m

1 X 0
1 X 0
` (f (xi ), yi )g(xi ) =
max
` (f (xi ), yi )(w> xi )(u> xi )(v> xi ) .
m i=1
kwk=1,kuk=1,kvk=1 m
i=1

While this is in general a hard optimization problem, we can approximate it ? and luckily, an approximate greedy step suffices for success of the greedy procedure. This procedure is given in Figure 1,
and is again based on an approximate eigenvector computation. A guarantee for the quality of approximation is given in the appendix, and this leads to the following theorem, whose proof is given
in the appendix.
Theorem 7. Fix some ?,  > 0. Assume that the loss function is convex and ?-smooth. Then if the
4d?k2
GECO Algorithm is run for r > (1??
)2 iterations, where each iteration relies on the approximation
procedure given in Fig. 1, then with probability (1??)r , it outputs a network f ? N3,5r,?2 for which
R(f ) ? minf ? ?P3,k R(f ? ) + .
4
It is also possible to find an approximate solution to the eigenvalue problem and still retain the performance
guarantees (see [20]). Since an approximate eigenvalue can be found in time O(d) using the power method, we
obtain the runtime of GECO depends linearly on d.

7

d
m
Input: {xi }m
i=1 ? R ? ? R , ? ,?
?
approximate
solution to
Output: A 1??
d
X
max
F (w, u, v) =
?i (w> xi )(u> xi )(v> xi )
kwk,kuk,kvk=1

i

Pick randomly w1 , . . . , ws iid according to N (0, Id ).
For t = 1, . . . , 2d log 1?
wt
wt ? kw
tk
P
Let A = i ?i (wt> xi )xi x>
i and set ut , vt s.t:
>
T r(u>
Av
)
?
(1
?
?
)
max
t
kuk,kvk=1 T r(u Av).
t
Return w, u, v the maximizers of maxi?s F (wi , ui , ui ).

Figure 1: Approximate tensor maximization.

5

Experiments

To demonstrate the practicality of GECO to train neural networks for real world problems, we considered a pedestrian detection problem as follows. We collected 200k training examples of image
patches of size 88x40 pixels containing either pedestrians (positive examples) or hard negative examples (containing images that were classified as pedestrians by applying a simple linear classifier in
a sliding window manner). See a few examples of images above. We used half of the examples as a
training set and the other half as a test set. We calculated HoG
features ([11]) from the images5 . We then trained, using GECO,
0.1
SGD ReLU
a depth-2 polynomial network on the resulting features. We
SGD Squared
GECO
9 ? 10
used 40 neurons in the hidden layer. For comparison we trained
the same network architecture (i.e. 40 hidden neurons with a
8 ? 10
squared activation function) by SGD. We also trained a similar
7 ? 10
network (40 hidden neurons again) with the ReLU activation
6 ? 10
function. For the SGD implementation we tried the following
tricks to speed up the convergence: heuristics for initialization
5 ? 10
of the weights, learning rate rules, mini-batches, Nesterov?s mo0
0.2
0.4
0.6
0.8
1
iterations
mentum (as explained in [23]), and dropout. The test errors of
?10
SGD as a function of the number of iterations are depicted on
4
1
2
the top plot of the Figure on the side. We also mark the perfor4
mance of GECO as a straight line (since it doesn?t involve SGD
3
8
iterations). As can be seen, the error of GECO is slightly bet2
ter than SGD. It should be also noted that we had to perform a
very large number of SGD iterations to obtain a good solution,
1
while the runtime of GECO was much faster. This indicates that
GECO may be a valid alternative approach to SGD for training
0
0
0.2
0.4
0.6
0.8
1
depth-2 networks. It is also apparent that the squared activation
#iterations
?10
function is slightly better than the ReLU function for this task.
?2

Error

?2

?2

?2

?2

MSE

5

5

The second plot of the side figure demonstrates the benefit of
over-specification for SGD. We generated random examples in R150 and passed them through a
random depth-2 network that contains 60 hidden neurons with the ReLU activation function. We
then tried to fit a new network to this data with over-specification factors of 1, 2, 4, 8 (e.g., overspecification factor of 4 means that we used 60 ? 4 = 240 hidden neurons). As can be clearly seen,
SGD converges much faster when we over-specify the network.
Acknowledgements: This research is supported by Intel (ICRI-CI). OS was also supported by
an ISF grant (No. 425/13), and a Marie-Curie Career Integration Grant. SSS and RL were also
supported by the MOS center of Knowledge for AI and ML (No. 3-9243). RL is a recipient of the
Google Europe Fellowship in Learning Theory, and this research is supported in part by this Google
Fellowship. We thank Itay Safran for spotting a mistake in a previous version of Sec. 2 and to James
Martens for helpful discussions.
5
Using the Matlab implementation provided in http://www.mathworks.com/matlabcentral/
fileexchange/33863-histograms-of-oriented-gradients.

8

References
[1] A. Andoni, R. Panigrahy, G. Valiant, and L. Zhang. Learning polynomials with neural networks. In ICML, 2014.
[2] A. Andoni, R. Panigrahy, G. Valiant, and L. Zhang. Learning sparse polynomial functions. In
SODA, 2014.
[3] M. Anthony and P. Bartlett. Neural Network Learning - Theoretical Foundations. Cambridge
University Press, 2002.
[4] S. Arora, A. Bhaskara, R. Ge, and T. Ma. Provable bounds for learning some deep representations. arXiv preprint arXiv:1310.6343, 2013.
[5] P. Auer, M. Herbster, and M. Warmuth. Exponentially many local minima for single neurons.
In NIPS, 1996.
[6] P. L. Bartlett and S. Ben-David. Hardness results for neural network approximation problems.
Theor. Comput. Sci., 284(1):53?66, 2002.
[7] Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35:1798?1828,
2013.
[8] E. Blais, R. O?Donnell, and K. Wimmer. Polynomial regression under arbitrary product distributions. Machine Learning, 80(2-3):273?294, 2010.
[9] A. Blum and R. Rivest. Training a 3-node neural network is np-complete. Neural Networks,
5(1):117?127, 1992.
[10] G. Dahl, T. Sainath, and G. Hinton. Improving deep neural networks for lvcsr using rectified
linear units and dropout. In ICASSP, 2013.
[11] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In CVPR, 2005.
[12] A. Daniely, N. Linial, and S. Shalev-Shwartz. From average case complexity to improper
learning complexity. In FOCS, 2014.
[13] A. Kalai, A. Klivans, Y. Mansour, and R. Servedio. Agnostically learning halfspaces. SIAM J.
Comput., 37(6):1777?1805, 2008.
[14] A. Kalai, A. Samorodnitsky, and S.-H. Teng. Learning and smoothed analysis. In FOCS, 2009.
[15] A. Klivans and A. Sherstov. Cryptographic hardness for learning intersections of halfspaces.
In FOCS, 2006.
[16] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional
neural networks. In NIPS, 2012.
[17] Q. V. Le, M.-A. Ranzato, R. Monga, M. Devin, G. Corrado, K. Chen, J. Dean, and A. Y. Ng.
Building high-level features using large scale unsupervised learning. In ICML, 2012.
[18] N. Pippenger and M. Fischer. Relations among complexity measures. Journal of the ACM
(JACM), 26(2):361?381, 1979.
[19] S. Shalev-Shwartz and S. Ben-David. Understanding Machine Learning: From Theory to
Algorithms. Cambridge University Press, 2014.
[20] S. Shalev-Shwartz, A. Gonen, and O. Shamir. Large-scale convex minimization with a lowrank constraint. In ICML, 2011.
[21] S. Shalev-Shwartz, O. Shamir, and K. Sridharan. Learning kernel-based halfspaces with the
0-1 loss. SIAM Journal on Computing, 40(6):1623?1646, 2011.
[22] M. Sipser. Introduction to the Theory of Computation. Thomson Course Technology, 2006.
[23] I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the importance of initialization and
momentum in deep learning. In ICML, 2013.
[24] M. Zeiler and R. Fergus. Visualizing and understanding convolutional neural networks. arXiv
preprint arXiv:1311.2901, 2013.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

