query sentence: Neural networks
---------------------------------------------------------------------
title: 232-analog-neural-networks-of-limited-precision-i-computing-with-multilinear-threshold-functions.pdf

obradov pclrberri analog neural network limit precis i comput multilinear threshold function preliminari version zoran obradov ian parberri depart comput scienc penn state univers univers park pa. abstract experiment evid shown analog neural network ex~m fault-toler particular perform appear signific impair precis limit analog neuron limit precis essenti comput k-ari weight multilinear threshold function divid region k-l hyperplan behaviour k-ari neural network investig canon set threshold valu although exist binari ternari neural network weight made integ log bit number processor without increas hardwar run time weight made increas run time constant multipl hardwar small polynomi binari neuron use run time allow increas larger constant multipl hardwar allow increas slight larger polynomi symmetr k-ari function comput constant depth size k-ari function comput constant depth size altern neural network olafsson abu-mostafa quantiz neural network fleisher close relat model analog neural network limit precis i introduct neural network typic circuit construct process unit comput simpl function form f wl wli rii-+ ser wier wii xl xli =g lwi output function two choic set current popular literatur first discret model s=b denot boolean set case typic linear threshold function iff call weight linear threshold function second analog model denot case typic monoton increas function sigmoid function constant r. analog neural network model popular easi construct processor requir characterist use transistor digit model popular behaviour easi analyz experiment evid indic analog neural network produc accur comput precis compon limit consid actual happen analog model precis limit suppos neuron take distinct excit valu exampl restrict number digit binari decim expans isomorph show essenti multilinear threshold function hloh2 defin here throughout paper assum conveni defin ho=-oo h/c=oo call k-ari weight multilinear threshold function multilinear threshold function studi neural network construct k-ari multilinear threshold function call k-ari neural network order distinguish standard 2-ari binari neural network particular concern resourc time size number processor weight sum weight k-ari neural network use accord classic comput paradigm reader refer parberri similar result binari neural network companion paper obradov parberri deal learn k-ari neural network detail version paper appear obradov parberri k-ari neural network model k-ari neural network weight graph set processor cvxv set connect processor function w vxv assign weight interconnect h v assign set k-l threshold processor assum ee size defin number processor weight obradov parberri processor k-ari neural network relat limit comput power k-ari function function let denot set n-input k-ari function defin iff hi set k-ari weight multilinear threshold function union n. rang processor k-ari neural network comput k-ari weight multilinear threshold function input processor one state initi input processor place state encod input processor updat interv state time output time state k-ari neural network comput processor chang state stabl configur reach output state output processor stabl state reach neural network said equival iff input everi comput input termin time comput input termin time output neural network said equival iff equival analog neural network let function rang ani limited-precis devic purport comput must actual comput function rang ration valu ken suffici practic purpos provid larg enough sinc isomorph z formal defin limit precis variant function defin f x =round j round r~n natur round function defin round x =n iff theorem letf wlo wier defin lwixi i=l monoton increas invert then f wlo k-ari weight multilinear threshold function proof easi verifi f wlo hi thus see analog neural network limit precis essenti k-ari neural network analog neural network limit precis i canon threshold binari neural network advantag threshold taken equal zero see exampl theorem parberri similar result hold ternari neural network theorem everi n-input ternari weight multilinear threshold function equival i -input ternari weight multilinear threshold function threshold valu equal zero one proof suppos wii hloh2 r. without loss general assum l h defin w= wl rii+i wj=wjl hrh wli demonstr simpl case analysi xll e z l hz x xll choic threshold valu theorem arbitrari unfortun canon set threshold theorem everi 1o hk 1e r. exist n-input k-ari weight multilinear threshold function input k-ari weight multilinear threshold function wii+m hk-l proof sketch suppos i tk-l canon set threshold assum let 1o hk l=h hi 4si hypothesi exist wlo y= ylo xezi let i wi+2yi sinc follow wl+wz+s sinc follow obradov pdrberri inequ impli similar argument conclud but contradict network bound weight although model allow weight take infinit number possibl valu finit number threshold function sinc finit number k-ari function fix number input thus number input threshold function bound function fact someth stronger shown weight made integr log bit suffici describ one theorem everi k-ari neural network size exist equival k-ari neural network m2 size weight integ weight proof sketch suffici prove everi weight threshold function f wlt nen equival we1f.ht threshold function w .hi extend techniqu use muroga toda takasu binari case we see weight bound maximum determin matrix dimens lover z thus bound polynomi we guarante abl describ weight use polynomi number bit threshold circuit k-ari neural network weight drawn said unit weight unit-weight direct acycl k-ari neural network call k-ari threshold circuit k-ari threshold circuit divid layer each layer receiv input layer it depth k-ari threshold circuit defin number layer weight equal number edg bound squar size despit appar handicap limit weight kari threshold circuit surpris power much interest focuss comput symmetr function neural network motiv fact visual system appear abl recogn object regardless posit retina function call symmetr output remain matter input permut analog neural network limit precis i theorem ani symmetr k-ari function input comput k-ari threshold circuit depth size proof omit it note mani time neural network comput ani boolean function constant depth true k-ari neural network although result appear requir exponenti size mani interest function theorem ani k-ari function input comput k-ari threshold circuit size depth proof similar chandra parberri interest problem remain determin function requir exponenti size achiev constant depth comput polynomi size constant depth we consid problem ad integ repres k-ari notat theorem sum two k-ari integ size comput k-ari threshold circuit size depth proof first comput carri luadrat size depth use standard elementari school algorithm then it posit result comput tit posit operand carri propag posit constant size depth theorem sum integ size comput k-ari threshold circuit size constant depth proof similar proof use theorem chandra parberri theorem everi k-ari neural network size exist equival unit-weight k-ari neural network m2 size proof theorem we bound weight size binari notat theorem we replac everi processor non-unit weight threshold circuit size constant depth theorem impli we assum unit weight increas size polynomi run time constant multipl provid number logic level bound polynomi size network number threshold also reduc one size increas larger polynomi theorem everi k-ari neural network size exist equival unit-weight binari neural network size 4k log output binari encod requir result proof similar proof theorem result primarili theoret interest binari neural network appear simpler henc desir analog neural network howev analog neural network actual desir sinc easier build in mind theorem simpli serv limit the function analog neural network obradov parberri expect comput effici we concern construct model the comput abil neural network rather model implement detail nonmonoton multilinear neural network olafsson abu-mostafa studi f wlt w er inform capac function xli =g the altern threshold function monoton increas h er defin if we call altern weight multilinear threshold function neural network construct function form altern multilinear neural network altern multilinear neural network close relat k-ari neural network theorem for everi k-ari neural network size weight there equival altern multilinear neural network size log weight log produc the output the former in binari notat proof sketch each k-ari gate replac log gate togeth essenti perform binari search determin each bit the k-ari gate weight increas exponenti use provid the correct output valu theorem for everi altern multilinear neural network size weight there a 3t-equival k-ari neural network size 4z weight proof sketch without loss general assum odd each altern gate replac a k-ari gate ident weight threshold the output this gate goe weight one a k-ari gate threshold weight minus one a k-ari gate threshold the output gate goe a binari gate threshold both k-ari altern multilinear neural network a special case nonmonoton multilinear neural network the defin by iff hi~ h +lt for monoton increas h er co ck-1ezk nonmonoton neural network correspond analog neural network whose output function necessarili monoton nondecreas mani the result this paper includ theorem also appli nonmonoton neural network the size weight run time mani the upper-bound also improv by a small amount by use nonmonoton neural network instead k-ari one the detail left the interest reader mul tilinear hopfield network a multilinear version the hopfield network call the quantiz neural network studi by fleisher use the terminolog parberri a quantiz neural network a simpl symmetr k-ari neural network interconnect pattern undirect graph without self-loop the addit properti all processor ident set threshold although the latter assumpt analog neural network limit precis i reason for binari neural network see for exampl theorem parberri ternari neural network theorem it necessarili for k-ari neural network theorem howev it easi extend fleisher main result give the follow theorem ani product sequenti comput a simpl symmetr k-ari neural network converg conclus it shown analog neural network with limit precis essenti k-ari neural network if limit a polynomi then polynomi size constant depth k-ari neural network equival polynomi size constant depth binari neural network nonetheless the save in time a constant multipl hardwar a polynomi aris use k-ari neural network rather binari one quit signific we suggest one actual construct binari k-ari neural network analog neural network construct by exploit the analog behaviour transistor rather use extra hardwar inhibit it rather we suggest k-ari neural network a tool for reason the behaviour analog neural network acknowledg the financi support the air forc offic of scientif research air forc ystern command dsaf grant number afosr afosr nsf grant ian parberri grate acknowledg
----------------------------------------------------------------

title: 2111-computing-time-lower-bounds-for-recurrent-sigmoidal-neural-networks.pdf

comput time lower bound recurr sigmoid neural network michael schmitt lehrstuhl mathematik und informatik fakultat fur mathematik ruhr-universitat bochum bochum germani mschmitt lmi.ruhr-uni-bochum.d abstract recurr neural network analog unit comput realvalu function studi time complex real comput general recurr neural network sigmoid linear product unit unlimit order node restrict weight network oper discret time exhibit famili function arbitrarili high complex deriv almost tight bound time requir comput function thus evid given comput limit time-bound analog recurr neural network subject introduct analog recurr neural network known comput capabl exceed classic ture machin see siegelmann sontag kilian siegelmann siegelmann littl howev known limit among rare result direct instanc one sima orponen show continuous-tim hopfield network may requir exponenti time converg stabl state bound howev express term size network henc appli fixed-s network given number node bound comput power analog recurr network establish maass orponen maass sontag show discretetim recurr neural network recogn subset regular languag presenc nois model comput recurr network howev receiv input sequenc therefor comput time issu sinc network halt input sequenc termin analog recurr neural network howev also run real comput get input vector real number comput yield real output valu result avail thus far regard time complex analog recurr neural network given size investig time complex discrete-tim recurr neural network comput function real network node allow sigmoid unit linear unit product unit monomi expon ad justabl weight durbin rumelhart studi complex real comput sens blum ai mean consid real number entiti repres exact process without restrict precis moreov assum inform content network weight bound done work balcazar ai gavalda siegelmann general type network question aris function comput given number node limit amount time follow exhibit famili real-valu function ft one variabl comput fix size network time our main result show everi recurr neural network comput function ft requir least time nw thus obtain almost tight time bound real comput recurr neural network analog comput recurr neural network studi comprehens type discrete-tim recurr neural network call general recurr neural network figur everi recurr neural architectur consist comput node yi yk input node xl size network defin number ofit comput node comput node form fulli connect recurr network everi comput node also receiv connect everi input node input node play role input variabl system connect parameter real-valu adjust weight three type comput node product unit sigmoid unit linear unit assum comput node connect comput node weight wil wi input node weight vii vi let yi yk xl valu comput node input node time respect node product unit comput time valu weight exponenti incom valu multipli sigmoid linear unit addit paramet associ threshold bias sigmoid unit comput valu standard sigmoid simpli output weight sum node linear unit allow network heterogen may contain three type comput node simultan thus model encompass wide class network type consid research applic instanc architectur propos includ second layer linear comput node recurr connect comput node serv output node see koiran sontag haykin siegelmann clear definit given linear unit function output node weight outgo connect set also common use sigmoid unit higher-ord comput node recurr network see omlin gile gavalda siegelmann carrasco obvious model includ higher-ord network special case sinc comput higher-ord sigmoid unit simul first comput higher-ord term use product unit pass i i sigmoid product linear unit comput node yl yk input node xl xn i figur general recurr neural network size comput node may serv output node output sigmoid unit product unit howev even power higher-ord term sinc allow perform divis oper use negat weight moreov negat input valu weight non-integ weight output product unit may complex number shall ensur comput real-valu sinc main interest lower bound howev bound obvious remain valid comput network extend complex domain defin mean recurr neural network comput function llt assum input node let given te say comput step initi time input node comput node fix valu perform comput step defin equat one comput node yield valu we assum input node remain unchang comput we say comput time everi network comput step note may depend must independ input vector we emphas veri general definit analog comput recurr neural network particular we specifi definit output node allow output occur node moreov even requir network reach stabl state attractor hopfield network suffici output valu appear point trajectori network perform similar view comput recurr network captur model propos maass clear lower bound remain valid restrict definit analog comput requir output node stabl state moreov hold architectur input node receiv input initi valu comput node thus bound serv lower bound also transit time real-valu state discrete-tim dynam system compris network consid our main tool investig vapnik-chervonenki dimens neural network defin follow also anthoni bartlett dichotomi set partit two disjoint subset sd satisfi so s1 s. class function map i said shatter everi dichotomi so sd satisfi f so vapnik-chervonenki dimens defin i output y5 y4 figur recurr neural network comput function fl time 2l largest number set element shatter f. neural network given term architectur repres class function obtain assign real number adjust paramet weight threshold subset thereof output network assum threshold fix constant so output valu binari vc dimens neural network defin vc dimens class function comput network deriv lower bound next section we make use follow result network product sigmoid unit previous establish schmitt we emphas constraint paramet product unit they yield real-valu complex-valu function mean statement hold network arbitrari order it impos ani restrict magnitud weight product unit proposit schmitt theorem suppos feedforward neural network consist sigmoid product linear unit let size number adjust weight vc dimens restrict real-valu function bound comput time we establish bound time requir recurr neural network comput famili function fl jr jr consid measur complex fl specif fl defin term dynam system lth iter logist map fl x we observ singl recurr network capabl comput everi fl time lemma general recurr neural network comput fl time 2l everi proof network shown figur it consist linear second-ord unit comput node initi except yl start output all follow step purpos yl let input output figur network nt enter node y2 time keep it away later time clear valu fl result node y5 2l step network use comput fl requir linear second-ord unit follow result show establish upper bound asymptot almost tight gap order four moreov lower bound hold network unrestrict order sigmoid unit theorem everi general recurr neural network size requir least time cl comput function fl constant proof idea construct higher-ord network nt small size compar larg vc dimens network consist linear product unit hypothet unit comput function fj certain valu we shall deriv lower bound vc dimens network assum hypothet unit replac time-bound general recurr network we determin upper bound vc dimens result network term size comput time use idea koiran sontag proposit comparison lower upper vc dimens bound give estim time requir comput network nt shown figur feedforward network compos three network jvi jvi ach networ jvi lnput no es xl jvi 2l comput node y~r~l figur one adjust paramet nt denot all weight fix comput node defin follow omit time paramet j.l j.l j.l yih j.l lor j.l node consid addit input node get this input j.l node y~r~l output node node also output node nt thus entir network 3l node linear product unit 3l node comput function fl output i i i input output figur network we show ni shatter set cardin particular set ei ei unit vector posit elsewher everi dichotomi program network paramet use follow fact logist function koiran sontag lemma everi binari vector b1 bm there real number bi bi henc everi dichotomi sd paramet chosen everi ei2 satisfi if eillei2 ei so if eillei2 eij s1 sinc this valu comput ni input eill ei input given network input ei select function henc shatter ni impli ni vc dimens least assum ii comput general recurr neural network size kj time tj use idea koiran sontag we unfold network obtain feedforward network size kjtj comput fj thus we replac node comput ft ft fl2 nz network size kltl respect we a feedforward network consist sigmoid product linear unit sinc there 3l unit nl comput ft ft fl2 3l product linear unit size nt c1lkl2tl2 constant c1 use nt one adjust weight we get proposit vc dimens c2l2kr2tr2 constant c2 hand sinc nz nt shatter vc dimens nt least henc l3 kr2 tr2 hold impli tl2 cl kl2 henc tl cl kl lemma show a singl recurr network capabl comput everi function fl time follow consequ theorem establish this bound much improv corollari everi general recurr neural network requir least time comput function fl conclus perspect we establish bound comput time analog recurr neural network result show everi network given size there function arbitrarili high time complex this fact reli a bound the magnitud weight we deriv upper lower bound rather tight a polynomi gap order four hold the comput a specif famili real-valu function one variabl interest the upper bound shown use second-ord network without sigmoid unit wherea the lower bound valid even network sigmoid unit arbitrari product unit this indic ad these unit might decreas the comput time margin the deriv made use upper bound the vc dimens higher-ord sigmoid network this bound known optim ani futur improv therefor lead a better lower bound the comput time we focuss on product sigmoid unit as nonlinear comput element howev the construct present generic thus it possibl deriv similar result radial basi function unit model spike neuron other unit type known yield network with bound vc dimens the question whether such result obtain for continuous-tim network for network oper in the domain complex number challeng a assumpt made the network comput the function exact a detail analysi use the fact the shatter set requir the output lie threshold similar result obtain for network approxim the function less close for network that subject nois acknowledg the author grate acknowledg fund the deutsch forschungsgemeinschaft this work also support in part by the esprit work group in neural comput learn neurocolt2 no
----------------------------------------------------------------

title: 211-a-large-scale-neural-network-which-recognizes-handwritten-kanji-characters.pdf

large-scal neural network large-scal neural network recogn handwritten kanji charact yoshihiro mori kazuki joe atr auditori visual percept research laboratori sanpeidani inuidani seika-cho soraku-gun kyoto japan abstract propos new way construct large-scal neural network handwritten kanji charact recognit neural network consist part collect small-scal network train individu small number kanji charact network integr output small-scal network process facilit integr nework recognit rate total system compar small-scal network our result indic propos method effect construct large-scal network without loss recognit perform introduct neural network appli recognit task mani field good result denker they perform better convent method howev network current oper categori japanes write system present compos charact network recogn mani charact must given larg number categori maintain level perform train small-scal neural network difficult task therefor explor method integr small-scal neural network import construct large-scal network if method could integr small-scal network without loss perform scale neural network would extend dramat paper propos method construct large-scal network whose object recogn handwritten kanji charact report result part network method limit system charact recognit appli system recogn mani categori strategi large-scal network know current recognit general capac neural network realiz construct large-scal monolith network would effici mori joe effect instead start decid build block approach two strategi mix mani small-scal network select neural network snn strategi large-scal neural network made mani small-scal network train individu small number categori network snn select appropri small-scal network i advantag strategi inform pass select small-scal network alway appropri network therefor train small-scal network easi hand increas number categori substanti increas train time snn may make harder snn retain high perfonn furthennor error rate snn limit perfonn whole system integr neural network inn strategi large-scal neural network made mani small-scal network train individu small number categori network inn integr output small-scal network fig advantag this strategi everi small-scal network get inform contribut find right answer therefor possibl use knowledg distribut among small-scal network but respect various devic need make integr easier common advantag strategi mention size neural network relat small take long time train network small-scal network consid independ part whole system therefor retrain network improv perform whole system take long o utput sub net neural network select type suspend network snn strategi large-scal neural network output neural network integr type inn strategi structur large-scal network whole system construct use three kind neural network ftrst one call subnet ordinari three layer feed forward type neural network train use back propag learn algorithm second kind network call supernet this neural network make decis integr output subnet this network also 3-layer feed-forward net but larger subnet last network we call otherfilt devis improv integr upernet this otherfilt network design use vq algorithm there also chang made bp learn algorithm especi for pattern recognit we decid base time take for learn there categori small-scal network charact separ small group k-mean cluster method allow similar charact group togeth separ occur two stage first group charact form group separ smaller unit this way group charact obtain we choos inn strategi use distribut knowledg full advantag 9-charact unit subnet integr stage first subnet integr a higher level network supernet altogeth supernet need recogn charact supernet in turn integr a higher level network hypernet more precis role structur kind network follow subnet a featur vector extract handwritten pattern use input describ in section number unit in output layer number categori recogn subnet in short role a subnet output the similar the input pattern the categori allot the subnet supernet the output each subnet filter the otherfilt network use the input mori joe the supernet the number unit in output layer the the number subnet belong a supernet in shortt the role of supernet select the subnet which cover the categori correspond to the input pattern output horizont diagon vertic origin pattern ubnet otherfiit
----------------------------------------------------------------

title: 1175-generating-accurate-and-diverse-members-of-a-neural-network-ensemble.pdf

generat accur divers member neural-network ensembl david opitz comput scienc depart univers minnesota duluth mn opitz d.umn.edu jude w. shavlik comput scienc depart univers wisconsin madison wi shavlik cs.wisc.edu abstract neural-network ensembl shown accur classif techniqu previous work shown effect ensembl consist network high correct one make error differ part input space well exist techniqu howev indirect address problem creat set network paper present techniqu call addemup use genet algorithm direct search accur divers set train network addemup work first creat initi popul use genet oper continu creat new network keep set network accur possibl disagre much possibl experi three dna problem show addemup abl generat set train network accur sever exist approach experi also show addemup abl effect incorpor prior knowledg avail improv qualiti ensembl introduct mani research shown simpli combin output mani classifi generat accur predict individu classifi clemen wolpert particular combin separ train neural network common refer neural-network ensembl demonstr particular success alpaydin drucker hansen salamon hashem krogh vedelsbi maclin shavlik perron theoret hansen salamon krogh vedelsbi empir hashem d. w. opitz j. w. sha vlik maclin shavlik work shown good ensembl one individu network accur make error differ part input space howev previous work either focuss combin output multipl train network indirect address generat good set network present algorithm addemup accur divers ensemble-mak give unit predict use genet algorithm generat popul neural network high accur time minim overlap make error thadit ensembl techniqu generat network random tri differ topolog initi weight set paramet set use part train set hope produc network disagre make error henceforth refer divers measur disagr propos instead activ search good set network key idea behind approach consid mani network keep subset network minim object function consist accuraci divers term mani domain care general perform generat solut quick coupl fact comput power rapid grow motiv us effect util avail cpu cycl continu consid network possibl place ensembl proceed first creat initi set network continu produc new individu use genet oper crossov mutat defin overal fit individu combin accuraci divers thus addemup keep popul set high fit individu high accur make mistak differ part input space also activ tri generat good candid emphas current popul 's erron exampl backpropag train experi report herein demonstr addemup abl generat effect set network ensembl addemup import accur divers ensembl figur illustr basic framework neural-network ensembl network ensembl network network case first train use train instanc exampl predict output network oi figur combin produc output ensembl figur mani research alpaydin hashem krogh vedelsbi mani demonstr effect combin scheme simpli weight averag network l ien wi oi l ien wi type ensembl focus paper hansen salamon prove neural-network ensembl averag error rate pattern less network ensembl independ product error expect error pattern reduc zero number network combin goe infin howev assumpt rare hold practic krogh vedelsbi later prove divers di network measur di oi x xw ensembl general error ce consist two distinct portion krogh vedelsbi refer term ambigu generat accur divers member neural-network ensembl ensembl output inw lllnw 21-lnw ni 1j input figur neural-network ensembl li wi di li wi ei ei error rate network wi 's sum equat show want ensembl consist high correct network disagre much possibl creat set network focus paper addemup algorithm tabl summar new algorithm addemup use genet algorithm generat set neural network accur divers classif although addemup current use neural network could easili extend incorpor type learn algorithm well addemup start creat train initi popul network creat new network use standard genet oper crossov mutat addemup train new individu emphas exampl misclassifi current popul explain addemup add new network popul score popul member fit function fitnessi accuracyi diversityi defin tradeoff accuraci divers final addemup prune popul most-fit member defin current ensembl repeat process defin accuraci term network i 's validation-set accuraci training-set accuraci valid set use use equat lover valid set calcul divers term di separ normal term valu rang normal term allow mean across domain sinc it alway clear valu one set therefor develop rule automat set first never chang ensembl error decreas consid new network otl erwis chang one follow two thing happen popul error increas popul divers decreas divers seem under-emphas increas increas decreas divers seem over-emphas decreas start result paper use network add ensembl one correct classifi mani exampl possibl make mistak primarili exampl d. w. opitz w. sha vlik tabl addemup algorithm goal genet creat accur divers ensembl network creat train initi popul network until stop criterion reach use genet oper creat new network thain new network use equat add popul measur divers network respect current popul equat normal accuraci score divers score individu network calcul fit popul member equat prune popul fittest network adjust ox text explan report current popul network ensembl combin output network accord equat current popul member correct classifi address backpropag train multipli usual cost function term measur combin popul error exampl cost ket it k target network activ exampl train set t. notic sinc network yet member ensembl depend network new term thus constant calcul deriv backpropag normal ensembl error averag valu new term around regardless correct ensembl especi import high accur popul sinc tk close exampl network would get train exampl expon a~l repres ratio import divers term fit function instanc ox close divers consid import network train usual cost function howev ox larg divers consid import new term cost function take import combin predict network take weight sum output network weight base validation-set accuraci network thus defin weight combin network follow simpli averag output generat good composit model clemen we includ predict accuraci weight sinc one believ accur model inaccur one generat accur divers member neural-network ensembl experiment studi genet algorithm we use generat new network topolog regent algorithm opitz shavlik regent use genet algorithm search space knowledge-bas neural network knn topolog knns network whose topolog determin result direct map set background rule repres we current know task kbann towel shavlik instanc translat set proposit rule neural network then refin result network 's weight use backpropag thain knns kbann network shown frequent general better mani inductive-learn techniqu standard neural network opitz towel shavlik use knns allow us high correct network ensembl howev sinc network ensembl initi set domain-specif rule we expect much disagr among network altern we consid experi random generat initi popul network topolog sinc domain-specif rule sometim avail we ran addemup nynex 's max problem set three problem human genom project aid locat gene dna sequenc recogn promot splice-junct ribosome-bind site rbs domain accompani a set approxim correct rule describ what current known task opitz opitz shavlik detail experi measur test-set error addemup task ensembl consist network regent addemup algorithm consid network genet search tabl 2a present result case learner random creat topolog network use domain-specif knowledg tabl first row best-network result a single-lay neural network fold we train network contain uniform hidden node use a valid set choos best network next row bag contain result run breiman 's bag algorithm standard single-hidden-iay network number hidden node random set network bag a bootstrap ensembl method train network ensembl a differ partit train set it generat partit random draw replac exampl train set size train set breiman show bag effect unstabl learn algorithm neural network small chang train set result larg chang predict bottom row tabl aooemup contain result a run addemup initi popul size random generat result show domain combin output multipl train network general better tri pick single-best network top tabl show power neural-network ensembl tabl 2b demonstr addemup 's abil util prior knowledg first row tabl 2b contain general result kbann algorithm while next row kbann-bag contain result ensembl each individu network ensembl kbann network train a differ partit train set even though each network start topolog 2we also tri ensembl approach random creat vari multilay network topolog initi weight set bag signific better dataset three dna domain d. w. opitz j. w. sha vllk tabl test-set error a ten-fold cross valid tabl show result run three learner without domain-specif knowledg tabl show result run three learner knowledg pairwis one-tail t-test indic aooemup tabl differ algorithm tabl confid level except regent splice-junct domain i standard neural network domain-specif knowledg use best-network bag aooemup promot splice junction rbs i max knowledge-bas neural network domain-specif knowledg use kbann kbann-bag regent-combin aooemup promot splice junction rbs max larg initi weight set weight result domain-specif knowledg small chang train set still produc signific chang predict also notic dataset kbann-bag good better run bag random generat network bag tabl next row regent-combin contain result simpli combin use equat network regent 's final popul aooemup final row tabl main differ regent-combin two way fit function equat take account divers rather network accuraci it train new network emphas erron exampl current ensembl therefor compar aooemup regent-combin help direct test addemup 's diversity-achiev heurist though addit result report opitz show addemup get most improv fit function there two main reason we think result addemup tabl 2b especi encourag compar addemup regent-combin we explicit test qualiti heurist demonstr effect addemup abl effect util background knowledg decreas error individu network ensembl while still abl creat enough divers among improv overal qualiti ensembl conclus previous work neural-network ensembl shown effect techniqu classifi ensembl high correct disagre each much possibl new algorithm addemup use genet algorithm search a correct divers popul neural network use in ensembl it this collect set network best fit object function measur both the accuraci the network the disagr network respect the member the set addemup tri generat accur divers member a neural-network ensembl activ generat qualiti network search emphas the current ensembl 's erron exampl backpropag train experi demonstr method abl find effect set network for our ensembl experi also show addemup abl effect incorpor prior knowledg avail improv the qualiti this ensembl in fact use domain-specif rule our algorithm show statist signific improv the singl best network seen the search a previous propos ensembl method call bag breiman and a similar algorithm whose object function simpli the validation-set correct the network in summari addemup success in generat a set neural network work well togeth in produc an accur predict acknowledg this work support offic naval research grant
----------------------------------------------------------------

title: 1692-lower-bounds-on-the-complexity-of-approximating-continuous-functions-by-sigmoidal-neural-networks.pdf

lower bound complex approxim continu function sigmoid neural network michael schmitt lehrstuhl mathematik und informatik fakuwit ftir mathematik ruhr-universitat bochum bochum germani mschmitt lmi.ruhr-uni-bochum.d abstract calcul lower bound size sigmoid neural network approxim continu function particular show approxim polynomi network size grow degre polynomi bound valid input dimens independ number variabl result obtain introduc new method employ upper bound vapnik-chervonenki dimens prove lower bound size network approxim continu function introduct sigmoid neural network known univers approxim one theoret result frequent cite justifi use sigmoid neural network applic statement one refer fact sigmoid neural network shown abl approxim continu function arbitrarili well numer result literatur establish variant univers approxim properti consid distinct function class approxim network architectur use differ type neural activ function respect various approxim criteria see instanc see particular scarselli tsoi recent survey refer result mani other referenc construct mere exist proof provid upper bound network size assert good approxim possibl suffici mani network node avail howev partial answer question main aris practic applic given function mani network node need approxim much attent focus establish lower bound network size particular approxim function real far comput binary-valu complex ofapproxim continu function neural network function sigmoid network concern output valu network threshold yield result direct specif boolean function koiran show network use standard sigmoid activ function must size number input measur network size count input node follow maass establish larger lower bound construct binary-valu function irn show standard sigmoid network requir mani network node comput function first work complex sigmoid network approxim continu function due dasgupta schnitger show standard sigmoid network node replac type activ function without increas size network polynomi yield indirect lower bound size sigmoid network term network type dasgupta schnitger also claim size bound ao i/d sigmoid network layer approxim function sin ax paper consid problem use standard sigmoid neural network approxim polynomi show least network node requir approxim polynomi degre small error loo norm bound valid arbitrari input dimens depend number variabl lower bound also obtain result binary-valu function mention interpol correspond function polynomi howev requir grow input dimens yield lower bound term degre bound establish hold network number layer far know first lower bound result approxim polynomi comput point view simpl class function comput use basic oper addit multipl polynomi also play import role approxim theori sinc dens class continu function approxim result neural network reli approxim polynomi sigmoid network see obtain result introduc new method employ upper bound vapnik-chervonenki dimens neural network establish lower bound network size first use vapnik-chervonenki dimens obtain lower bound due koiran calcul above-ment bound size sigmoid network boolean function koiran 's method develop extend maass use similar argument anoth combinatori dimens paper deriv lower bound comput binary-valu function koiran input maass input irn present new techniqu show lower bound obtain network approxim continu function rest two fundament result vapnik-chervonenki dimens neural network one hand use construct provid koiran sontag build network larg vapnik-chervonenki dimens consist gate comput certain arithmet function hand follow line reason karpinski macintyr deriv upper bound vapnikchervonenki dimens network estim khovanskil result due warren follow section give definit sigmoid network vapnikchervonenki dimens present lower bound result function approxim final conclud discuss open question schmitt sigmoid neural network vc dimens briefli recal definit sigmoid neural network vapnikchervonenki dimens see consid eed/orward neural network certain number input node one output node node input node call comput node associ real number threshold further edg label real number call weight comput network take place follow input valu assign input node comput node appli standard sigmoid sum w1xl wrxr xl valu comput node 's predecessor wi weight correspond edg threshold output valu network defin valu comput output node common approxim result mean neural network assum output node linear gate output sum wixi wrxr clear comput function finit set output rang output node may appli standard sigmoid well sinc sigmoid function consid here refer network sigmoid neural network sigmoid function general need satisfi much weaker assumpt definit natur general network employ type gate make use linear multipl divis gate vapnik-chervonenki dimens combinatori dimens function class defin follow dichotomi set irn partit two disjoint subset sl so si s. given set offunct map irn i dichotomi sd say induc dichotomi sd f sd say further shatter induc dichotomi s. vapnikchervonenki dimens denot vcdim f defin largest number set element shatter f. refer vc dimens neural network given term feedforward architectur direct acycl graph vc dimens class function obtain assign real number programm paramet general weight threshold network subset thereof further assum output valu network threshold obtain binari valu lower bound network size present lower bound size sigmoid network requir approxim polynomi first give brief outlin proof idea defin sequenc univari polynomi pn n l mean show construct neural architectur consistmg various type gate linear multipl divis gate particular gate comput polynomi further architectur singl weight programm paramet weight threshold fix demonstr assum gate comput polynomi approxim sigmoid neural network suffici well architectur nn shatter certain set assign suitabl valu programm weight final step reason along line karpinski macintyr obtain via khovanskil 's estim warren 's result upper bound vc dimens term number comput node note direct appli theorem sinc deal divis gate compar this bound cardin shatter set then abl complex ofapproxim continu function neural network p3 w1 w1 wi w1 p2 wj wk wn wn p1 wn figur network valu assign input node xl x4 respect weight programm paramet network conclud lower bound number comput node thus network approxim polynomi let sequenc polynomi ir induct defin pn x p pn-dx clear this uniqu defin pn everi readili seen pn degre main lower bound result made precis follow statement theorem sigmoid neural network approxim polynomi pn n interv error norm must least comput node proof neural architectur construct follow network four input node xl x4 figur show network input valu assign input node order x4 x3 x2 xl one weight consid programm paramet associ edg outgo input node x4 denot comput node partit six level indic box figur level network let us first assum sake simplic all comput real number exact three level label input node one output node comput so-cal project 7r irnh ir yn ya level label p3 p2 pi one input node output node level p3 receiv constant input thus valu paramet network defin output valu level wb pbon -l denot input valu level this valu equal xa+l oth erwls out id vve observ wb+l calcu ate rom 7r wi schmitt therefor comput level implement use gate comput function show nn shatter set cardin let shown lemma 1y exist pq w pq w this impli dichotomi sd everi pk pj.n pi.n so note pk pj.n pi.n2 valu comput given input valu therefor choos suitabl valu paramet nn network induc dichotomi s. word shatter nn weight chosen function comput this network satisfi lim ~o ya moreov this architectur consist comput node linear multipl divis gate note size depend therefor choos suffici small implement project 1r network comput node result network still shatter s. comput node implement three level label we each level number comput node comput respect assum now comput node replac sigmoid network input paramet valu defin result network comput function n~ note comput node programm paramet shown lemma there an architectur n we estim size accord theorem karpinski macintyr sigmoid neural network i programm paramet comput node vc dimens we general this result slight befor abl appli it it readili seen proof theorem result also hold network addit contain linear multipl gate divis gate we deriv bound take account gate comput divis say we introduc defin equal new variabl see proceed thus we network i programm paramet comput node linear multipl divis sigmoid gate vc dimens particular number comput node vc dimens shatter set o hand as we shown cardin sinc there sigmoid network comput function sinc number linear multipl divis gate bound valu singl network comput must size least this yield lower bound size a sigmoid network comput pn thus far we assum polynomi pn comput exact sinc polynomi continu function sinc we requir calcul a finit set input valu result from paramet valu chosen shatter an approxim polynomi suffici a straightforward analysi base fact output valu network a toler close show pn approxim error complex ofapproxim continu function neural network loo norm result network still shatter set s. this complet proof theorem statement previous theorem restrict approxim polynomi on input domain howev result immedi general arbitrari interv llt moreov it remain valid multivari polynomi arbitrari input dimens corollari approxim polynomi degre sigmoid neural network approxim error o ljk norm requir network size o log this hold polynomi number variabl conclus open question we establish lower bound on size sigmoid network approxim continu function particular a concret class polynomi we calcul a lower bound in term degre polynomi the main result alreadi hold the approxim univari polynomi intuit approxim multivari polynomi seem becom harder the dimens increas therefor it would interest lower bound both in term the degre the input dimens further in result the approxim error the degre coupl natur one would expect the number node grow each fix function when the error decreas present we know lower bound we not aim at calcul the constant in the bound practic applic valu indispens refin method use tighter result it straightforward obtain number further we expect better lower bound obtain consid network restrict depth establish the result we introduc a new method deriv lower bound on network size one the main argument use the function approxim construct network larg vc dimens the method seem suitabl obtain bound also the approxim type function as long as they comput power enough moreov the method could adapt obtain lower bound also network use activ function general sigmoid function ridg function radial basi function this may lead new separ result for the approxim capabl differ type neural network in order for this to accomplish howev an essenti requir small upper bound calcul for the vc dimens network acknowledg i thank han u. simon for help discuss this work support in part by the esprit work group in neural comput learn neurocolt2 no
----------------------------------------------------------------

title: 1037-quadratic-type-lyapunov-functions-for-competitive-neural-networks-with-different-time-scales.pdf

quadratic-typ lyapunov function competit neural network differ time-scal ank meyer-bas institut technic informat technic univers darmstadt darmstadt germani abstract dynam complex neural network model selforgan process cortic map must includ aspect long short-term memori behaviour network character equat neural activ fast phenomenon equat synapt modif slow part neural system present quadratic-typ lyapunov function flow competit neural system fast slow dynam variabl also show consequ stabil analysi neural net paramet introduct paper investig special class later inhibit neural network particular examin dynam restrict class later inhibit neural network rigor analyt standpoint network model retinotop somatotop cortic map usual compos sever layer neuron sensori receptor cortic unit feedforward excit layer later recurr connect within layer standard techniqu includ hebbian rule variat modifi synapt efficaci later inhibit establish topograph organ cortex adiabat approxim decoupl dynam relax fast time scale dynam learn slow time scale network howev case comput simul result obtain therefor provid limit mathemat understand self-organiz neural respons field network studi model dynam neural activ level a. meyer-bas short-term memori dynam synapt modif long-term memori actual network model consider may consid extens grossberg 's shunt network amari 's model primit neuron competit earlier network consid pool mutual inhibitori neuron fix synapt connect result extend earlier studi system synaps modifi extern stimuli dynam competit system may extrem complex exhibit converg point attractor period attractor network model dynam neural activ level cohen grossberg found lyapunov function necessari condit converg behavior point attractor paper appli result theori lyapunov function singular perturb system large-scal neural network two type state variabl ltm stm describ slow fast dynam system so find lyapunov function neural system differ time-scal give design concept store desir pattern stabl equilibrium point class neural network differ time-scal section defin network differenti equat character later inhibit neural network consid later inhibit network determinist signal hebbian learn law similar spatiotempor system amari general neural network equat describ tempor evolut stm activ modif ltm state synapt modif jth neuron an-neuron network equat xj ajxj j xi bjsj i=l xj current activ level aj time constant neuron bj contribut extern stimulus term neuron 's output ij later inhibit term yi extern stimulus dynam variabl sj repres synapt modif state lyl21 defin lyl2 yti assum input stimuli normal vector unit magnitud lyl2 these system subject our analysi consider regard stabil equilibrium point asymptot stabil neural network differ time-scal show section possibl determin asymptot stabil class neural network interpret nonlinear singular perturb system singular perturb theori tradit tool fluid dynam nonlinear mechan embrac wide varieti dynam phenomena posses slow fast mode show singular perturb present mani quadratic-typ lyapunov function competit neural network neurodynam problem sens appli paper result valuabl analysi tool dynam later inhibit network shown quadratic-typ lyapunov function singular perturb system obtain weight sum quadratic-typ lyapunov function two lower order system so-cal reduc boundary-lay system assum two system asymptot stabl lyapunov function condit deriv guarante suffici small perturb paramet asymptot stabil singular perturb system establish mean lyapunov function compos weight sum lyapunov function reduc boundary-lay system adopt notat consid singular perturb system bx assum bx origin uniqu equilibrium point uniqu solut reduc system defin set obtain assum bx uniqu root reduc system rewritten fr x boundary-lay system defin ay tic stretch time scale vector treat fix unknown paramet take valu bx aim establish stabil properti singular perturb system small reduc system boundary-lay system lyapunov function system quadratic-typ shown mild assumpt suffici small weight sum lyapunov function reduc boundary-lay system quadratic-typ lyapunov function singular perturb system necessari assumpt state reduc system lyapunov function xe bx scalar-valu function vanish differ zero bx condit guarante asymptot stabl equilibrium point reduc system 2the symbol bx indic close sphere center oj by defin way a. meyer-bas boundary-lay system lyapunov function bx by scalar-valu function vanish differ zero bx by this condit guarante asymptot stabl equilibrium point boundary-lay system follow three inequ hold ix bx vy by ff x constant k1 k2 nonneg inequ determin permiss interact slow fast variabl they basic smooth requir these introductori remark stabil criterion state theorem suppos condit hold let posit number let posit number given by ih f l gl origin asymptot stabl equilibrium point dw x lyapunov function put global neural time constant equat determin two lyapunov function one boundary-lay system reduced-ord system mention global lyapunov function competit neural network activ dynam constraint mij mji ai xi fj xj this lyapunov-funct aken one boundary-lay system stmequat if ltm contribut si consid fix unknown paramet quadratic-typ lyapunov function competit neural network j=l bjsj j=l dij xj k xk j=l reduced-ord system ltm equat we take lyapunov-funct sts i=l lyapunov-funct coupl stm ltm dynam sum two lyapunov-funct vex dw x design stabl competit neural network competit neural network learn rule move equilibria learn process concept asymptot stabil deriv matrix perturb theori captur this phenomenon we design in this section competit neural network abl store desir pattern stabl equilibrium theoret implic illustr in exampl two neuron network exampl let dii dij nonlinear linear function f xj xj in equat we get boundary-lay system xj axj dijf xd bsj i=l reduced-ord system la-a a-a then we get lyapunov-funct a. meyer-bas jj oj ij lis ij jj time in msec figur time histori neural network origin equilibrium point stm state nonneg constant we get al a2 c2 i3l i l cl we get interest implic the result the impic interpret follow achiev stabl equilibrium point we negat contribut the extern stimulus term the sum the excitatori inhibitori contribut the neuron less the time constant neuron evolut the trajectori the stm ltm state for two neuron system shown in figur the stm state exhibit first oscil the expect equilibrium point while the ltm state reach monoton the equilibrium point we see the pictur the equilibrium point reach after msec by the stm ltm-state from the formula we see maximum at choos we obtain for conclus we present in this paper quadratic-typ lyapunov function for analyz the stabil equilibrium point competit neural network with fast slow dynam this global stabil analysi method interpret neural network nonlinear singular perturb system the equilibrium point constrain neighborhood this techniqu suppos monoton increas non-linear symmetr later inhibit matrix the learn rule a determinist hebbian this method give upper bound the perturb quadratic-typ lyapunov function for competit neural network iii iii iii time in msec figur time histori of the neural network with the origin equilibrium point ltm state paramet an estim of a maxim posit neural time-const the practic implic ofth theoret problem the design of a competit neural network abl to store a desir pattern a stabl equilibrium
----------------------------------------------------------------

title: 503-refining-pid-controllers-using-neural-networks.pdf

refin pin control use neural network gari m. scott depart chemic engin johnson drive univers wisconsin madison wi jude w. shavlik depart comput scienc w. dayton street univers wisconsin madison wi w. harmon ray depart chemic engin johnson drive univers wisconsin madison wi abstract kbann approach use neural network refin knowledg written form simpl proposit rule extend idea present manncon algorithm mathemat equat govern pid control determin topolog initi weight network train use backpropag appli method task control outflow temperatur water tank produc statistically-signific gain accuraci standard neural network approach non-learn pid control furthermor use pid knowledg initi weight network produc statist less variat testset accuraci compar network initi small random number introduct research design neural network process control larg ignor exist knowledg task hand one form knowledg often call domain theori take embodi tradit control paradigm scott shavlik ray recently-develop kbann knowledge-bas artifici neural network approach towel address issu task domain theori written use simpl proposit rule avail basi approach use exist knowledg determin appropri network topolog initi weight network begin learn process good start point paper describ manncon multivari artifici neural network control algorithm method use tradit control paradigm determin topolog initi weight network use pid control way elimin network-design problem choic network topolog number hidden unit reduc sensit network initi valu weight furthermor initi configur network closer final state would normal randomly-configur network thus manncon network perform better consist standard randomly-initi three-lay approach task examin learn control multiple-input multiple-output mimo system number reason investig task use neural network one usual involv nonlinear input-output relationship match nonlinear natur neural network two number success applic neural network task bhat mcavoy jordan jacob miller final there number exist control paradigm use determin topolog initi weight network control network manncon algorithm use proportional-integral-deriv pid control stephanopoulo one simplest tradit feedback control scheme basi construct initi neural network control basic idea pid control control action vector proport error integr error time tempor deriv error sever tune paramet determin contribut various compon figur depict result network topolog base pid control paradigm first layer network y p desir process output setpoint actual process output past time step calcul simpl error simpl vector differ e=i p-i accomplish second layer calcul actual error pass pid mechan effect layer act steady-st pre-compens ray gie produc current error error signal past two time step compens constant matrix i valu interact steadi state various control loop elimin final layer control output/pl input calcul control action refin pid control use neural network fd td den water tank yen wco who wci whi wc2 wh2 figur manncon network show weight initi use ziegler-nichol tune paramet base veloc form discret pid control uc n uc n-l wcoci n wcici n-l wc2 wca wcb wc2 constant determin tune paramet control loop similar set equat constant who whi exist control loop figur show schemat water tank ray network control figur also show control variabl fc tank output variabl disturb variabl fd td control measur disturb repres nois system mann con initi weight figur network va.lu mimic behavior pid control tune ziegler-nichol paramet stephanopoulo particular oper condit use kbann approach towel add weight network unit layer connect unit subsequ layer initi weight small random number sever order magnitud smaller weight determin pid paramet scale input output network rang initi weight network manner given assum activ function unit network linear scott shavlik ray cold stream fe hot stream th dis urban ce fd td i temperatur flow rate output i i figur stir mix tank requir outflow temperatur control tabl topolog initi network network standard neural network manncon network i manncon network topolog 3-layer hidden unit pid topolog pid topolog weight initi random random z-n tune strength neural network howev lie nonlinear typic sigmoid activ function reason manncon system initi set weight bias unit linear respons dictat pid initi approxim sigmoid output rang unit unit output rang activ function becom exp wji wjioi linear weight describ onc manncon configur initi weight network use set train exampl backpropag improv accuraci network weight initi pid inform well initi small random number chang backpropag train experiment detail compar perform three network differ topolog and/or method initi tabl summar network topolog weight initi method network tabl pid topolog network structur shown figur random weight initi set refin pid control use neural network tabl rang averag durat setpoint experi experi train set instanc instanc instanc test set instanc instanc instanc weight small random number center around zero we also compar network non-learn pid control we train network use backpropag randomly-determin schedul setpoint ysp disturb chang repeat setpoint repres desir output valu control maintain temperatur outflow tank disturb repres nois inflow rate temperatur disturb stream magnitud setpoint disturb form gaussian distribut center number train exampl chang setpoint disturb exponenti distribut we perform three experi characterist train and/or test set differ tabl summar rang setpoint well averag durat data set experi seen experi train set test set qualit similar experi test set longer durat setpoint experi train set restrict subrang test set we period interrupt train test network result averag run scott we use error output tank figur determin network error propag error backward plant psalti method error signal input tank given 8u yi netui 8y oui 8yj repres simpl error output water tank 8ui error signal input tank sinc we use model process real tank we calcul partial deriv process model equat result figur compar perform three network experi seen manncon network show increas correct standard neural network approach statist analysi error use t-test show differ signific confid level furthermor differ perform manncon network i manncon network scott shavlik ray standard neural network manncon network i mann con network pid control non-learn train instanc figur mean squar error network testset function number train instanc present experi signific differ varianc test error differ run signific confid level final manncon network perform signific better confid level non-learn pid control perform standard neural network repres best sever trial vari number hidden unit rang second observ figur manncon network learn much quick standard neural-network approach manncon network requir signific fewer train instanc reach perform level within final error rate experi tabl summar final mean error well number train instanc requir achiev perform within valu experi we see signific gain correct man ncon network standard neural network approach confid level well non-learn pid control confid level experi manncon network initi z-n tune also learn signific quicker confid level standard neural network futur work one question whether introduct extra hidden unit network would improv perform give network room learn concept outsid given domain theori addit extra hidden unit well remov unneed unit area much ongo research refin pid control use neural network tabl comparison network perform i mean squar error i train instanc method experi standard neural network mann con network i mann con network pid control tune fix control action experi standard neural network mann con network i mann con network pid control tune fix con trol action experi standard neural network mann con network i mann con network pid control tune fix control action indic true valu lie within bound confid level the valu given fix control action repres the error result fix the control action level produc output steadi state ring rapid chang control action occur the train network futur enhanc approach would creat network architectur prevent ring perhap limit the chang the control action relat small valu anoth import goal approach the applic real-world process the water tank project illustr the approach quit simpl much difficult problem contain signific time delay exist explor there sever control paradigm could use as a basi for network construct initi there sever differ digit control as deadbeat dahlin 's stephanopoulo could use place the digit pid control use in this project dynam matrix control dmc pratt intern model control imc garcia morari also candid for consider for this approach final neural network general consid black box in inner work complet uninterpret sinc the neural network in this approach initi inform may possibl interpret the weight the network extract use inform the train network scott shavlik ray conclus we describ the manncon algorithm use the inform a pid control determin a relev network topolog without resort trialand-error method in addit the algorithm initi the weight prior knowledg give the backpropagt algorithm appropri direct in continu learn final we shown use the manncon algorithm signific improv the perform the train network in the follow way improv mean testset accuraci less variabl run faster rate learn better general extrapol abil acknowledg this materi base upon work partial support a nation scienc foundat graduat fellowship scott offic naval research grant nation scienc foundat grant
----------------------------------------------------------------

title: 657-optimal-depth-neural-networks-for-multiplication-and-related-problems.pdf

optim depth neural network multipl relat problem kai-yeung siu dept electr compo engin univers california irvin irvin ca vwani roychowdhuri school electr engin purdu univers west lafayett abstract artifici neural network ann common model threshold circuit network interconnect process unit call linear threshold gate depth network repres number unit delay time parallel comput size circuit number gate measur amount hardwar known tradit logic circuit consist unbound fan-in gate would requir least o log n/log log depth comput common arithmet function product quotient two n-bit number unless allow size fan-in increas exponenti show paper ann much power tradit logic circuit particular prove iter addit comput depth-2 ann multipl divis comput depth-3 ann polynomi size polynomi bound integ weight respect moreov follow known lower bound result ann optim depth also indic techniqu appli construct polynomial-s depth-3 ann power depth-4 ann multipl product introduct recent interest applic artifici neural network spur research interest theoret studi network model neural network basic process unit boolean gate comput linear siu roychowdhuri threshold function analog element comput sigmoid function artifici neural network view circuit process unit massiv interconnect togeth while neural network found wide applic mani area behavior limit network far understood one common model neural network threshold circuit incident studi threshold circuit motiv complex theoret issu also gain much interest area comput scienc threshold circuit boolean circuit gate comput linear threshold function wherea classic model unbound fan-in boolean circuit gate allow boolean circuit usual arrang layer gate layer comput concurr circuit comput layer layer increas depth order defin depth number layer circuit thus layer repres unit delay depth repres overal delay comput circuit relat work theoret comput scientist use unbound fan-in boolean circuit model understand fundament issu parallel comput specif comput model refer unbound fan-in parallel sinc number input gate boolean circuit bound constant theoret studi unbound fan-in parallel may give us insight devis faster algorithm various comput problem would possibl bound fan-in parallel fact nondegener boolean function variabl requir least o log depth comput bound fan-in circuit hand practic situat exampl larg fan-in circuit programm logic array plas multipl processor simultan access share bus unbound fan-in parallel seem natur model exampl pla consid depth-2 and/or circuit boolean circuit model amount resourc usual measur number gate consid reason long bound polynomi oppos exponenti number input exampl boolean circuit comput sum two n-bit number o n gate reason though circuit design might consid size circuit impract moder larg one import theoret issu parallel comput follow given number gate boolean circuit bound polynomi size input minimum depth number layer need comput certain function first step toward answer import question taken furst independ ajtai follow result mani basic function pariti major boolean variabl multipl two n-bit number constant depth independ classic boolean circuit unbound fan-in and/or gate comput function must polynomi number gate lower bound size subsequ improv yao hastad prove optim depth neural network multipl relat problem inde exponenti number and/or gate need so function pariti major comput hard respect constant depth polynomi size classic boolean circuit anoth way interpret result circuit and/or gate comput hard function use polynomi amount chip area must unbound delay delay increas fact lower bound result impli minimum possibl delay multipli polynomi number and/or gate o logn/loglogn result also give theoret justif imposs circuit design implement fast pariti circuit or multipli small chip area use or gate basic build block one hard function mention major function special case threshold function weight or paramet restrict natur extens studi boolean circuit contain major gate type boolean circuit call threshold circuit believ captur aspect comput brain rest paper term neural network refer threshold circuit model addit major gate result boolean circuit model seem much power classic one inde it first shown muroga three decad ago symmetr boolean function pariti comput two-lay neural network gate recent chandra show multipl two n-bit number sort n-bit number comput neural network constant depth polynomi size these constant signific reduc siu bruck case wherea lower bound depth-3 prove hajnal case multipl it known size depth-4 neural network multipl reduc o n howev exist depth-3 polynomial-s neural network multipl left open problem sinc lower bound result depth-effici neural network construct divis relat arithmet problem network optim depth our main contribut this paper show small constant depth neural network multipl divis relat problem construct problem iter addit multipl divis neural network construct shown optim depth these result follow implic practic signific suppos use analog devic build threshold gate cost term delay chip area compar or logic gate comput mani basic function much faster use tradit circuit clear particular weight depth fan-in size give realist measur network 's cost speed depend technolog use build it one case circuit depth would seem import paramet circuit implement use optic devic we refer interest optic implement neural network due space limit we shall state import result result detail proof appear journal version this paper siu roychowdhuri main result definit given n-bit integ zi zi i2i zi we defin iter addit problem comput log bit sum zi integ definit given n-bit integ xi2i yi2i we defin multipl problem comput product use notat let us denot class depth-d polynomial-s neural network integ weight polynomi bound correspond class weight unrestrict ltd it easi see it~at addit comput multipl comput lt we first prove result iter addit our result hing recent strike result goldmann hcistad razborov key observ iter addit comput sum polynomi mani linear threshold ltd function exponenti weight let us first state result goldmann hastad razborov lemma let ltd denot class depth-d polynomial-s neural network weight output gate polynomi bound integ with restrict on weight gate ltd fix integ follow lemma general result inform result say function weight sum possibl exponenti input lie one polynomi mani interv otherwis then function comput sum polynomi mani lti function lemma let wixi function ud nand otherwis polynomi bound comput sum polynomi mani lti function thus lt2 combin two lemma yield depth-2 neural network iter addit theorem iter addit lt2 it also easi see iter addit not comput lti simpli observ first bit sum pariti function not belong lt1 thus neural network iter addit minimum possibl depth theorem multipl n-bit integ comput lt3 it follow result depth-3 neural network multipl state theorem optim depth optim depth neural network multipl and relat problem we appli result construct small depth neural network for divis power and multipl product let us give a formal definit these problem definit let an input n-bit integ bit represent we defin power definit given n-bit integ zi we defin multipl product bit represent zi suppos we want comput quotient two integ some quotient in binari represent might requir infinit mani bit howev a circuit comput signific bit quotient if a number finit and infinit binari represent for exampl we shall alway express number in finit binari represent we interest in comput the truncat quotient defin definit let and two input bit integ let zi 2i the quotient divid y we defin divk x/i x/i truncat to the k -bit number in particular divo x the greatest integ theorem power comput in lt3 divk x/i comput in lr3 multipl product comput in lt4 it shown the lower-bound result in the neural network for divis optim in depth
----------------------------------------------------------------

title: 6427-toward-deeper-understanding-of-neural-networks-the-power-of-initialization-and-a-dual-view-on-expressivity.pdf

toward deeper understand neural network power initi dual view express amit dani googl brain roy frostig googl brain yoram singer googl brain abstract develop general dualiti neural network composit kernel hilbert space introduc notion comput skeleton acycl graph succinct describ famili neural network kernel space random neural network generat skeleton node replic follow sampl normal distribut assign weight kernel space consist function aris composit averag non-linear transform govern skeleton graph topolog activ function prove random network induc represent approxim kernel space particular follow random weight initi often yield favor start point optim despit worst-cas intract train neural network introduct neural network learn underpin state art empir result numer appli machin learn task see instanc nonetheless theoret analys neural network learn still lack sever regard notabl remain unclear train algorithm nd good weight learn impact network architectur activ function work analyz represent power neural network within vicin random initi we show regim practic interest random initi neural network well-approxim rich famili hypothes thus despit worst-cas intract train neural network common use initi procedur constitut favor start point train concret we de ne comput skeleton succinct descript feed-forward network skeleton induc famili network architectur well hypothesi class function obtain non-linear composit mandat skeleton structur we analyz set function express vari weight last layer simpl region train domain object convex we show high probabl choic initi network weight function approxim select nal layer weight befor delv technic detail we posit result context previous research current theoret understand nn learn standard result complex theori impli ef cientli comput function express network moder size barron theorem state even two-lay network express rich set function general abil algorithm train neural network also fair well studi inde classic recent result statist learn theori show number exampl grow comparison size network empir risk approach popul risk contrast remain puzzl ef cient algorithm stochast gradient method yield solut perform well learn algorithm succeed in most this work perform while author stanford univers confer neural inform process system nip barcelona spain practic theoret analys over pessimist for exampl hard result suggest in worst case even simpl 2-layer network intract learn concret hard construct hypothesi predict margin better random in the meantim recent empir success neural network prompt a surg theoret result nn learn for instanc we refer the reader the
----------------------------------------------------------------

title: 5267-on-the-computational-efficiency-of-training-neural-networks.pdf

comput effici train neural network roi livni hebrew univers roi.livni mail.huji.ac.il shai shalev-shwartz hebrew univers shai cs.huji.ac.il ohad shamir weizmann institut scienc ohad.shamir weizmann.ac.il abstract well-known neural network comput hard train hand practic modern day neural network train effici use sgd varieti trick includ differ activ function relu over-specif train network larger need regular paper revisit comput complex train neural network modern perspect provid posit negat result yield new provabl effici practic algorithm train certain type neural network introduct one signific recent develop machin learn resurg deep learn usual form artifici neural network combin algorithm advanc well increas comput power data size led breakthrough effect neural network use obtain impress practic perform varieti domain recent exampl includ neural network describ direct acycl graph vertex graph correspond neuron edg associ weight neuron calcul weight sum output neuron connect possibl add bias term pass result number activ function output result number focus feed-forward neural network neuron arrang layer output layer form input next layer intuit input goe sever transform higher-level concept deriv lower-level one depth network number layer size network total number neuron perspect statist learn theori specifi neural network architectur under graph activ function obtain hypothesi class name set predict rule obtain use network architectur chang weight network learn class involv find specif set weight base train exampl yield predictor good perform futur exampl studi hypothesi class usual concern three question sampl complex mani exampl requir learn class express type function express predictor class train time much comput time requir learn class simplic let us first consid neural network threshold activ function otherwis boolean input space singl output sampl complex neural network well understood known vc dimens grow linear number edg log factor also easi see matter activ function long repres weight network use constant number bit vc dimens bound constant time number edg impli empir risk minim find weight small averag loss train data effect learn strategi statist point view express network easi see neural network depth suffici size express function howev also possibl show happen size network must exponenti chapter function express use network polynomi size theorem show boolean function calcul time o also express network depth o t size o t theorem let everi let fd set function implement ture machin use oper exist constant everi network architectur depth size threshold activ function result hypotesi class contain fd proof theorem follow direct relat time complex program circuit complex see fact simul standard boolean gate use fix number neuron see statist perspect neural network form excel hypothesi class one hand everi runtim use depth o t contain predictor run time hand sampl complex result class depend polynomi main caveat neural network train time exist theoret result most negat show success learn network comput hard worst case exampl neural network depth contain class intersect halfspac number halfspac number neuron hidden layer reduct k-color shown find weight best fit train set np-hard shown even find weight result close-to-minim empir error comput infeas hard result focus proper learn goal find nearly-optim predictor fix network architectur howev goal find good predictor reason limit predictor one particular architectur instead tri exampl find network differ architectur a0 almost good best network architectur exampl power concept improp learn often prove use circumv comput hard result unfortun hard result show even improp learn even data generat exact small depth-2 neural network effici algorithm find predictor perform well test data particular shown case learn intersect halfspac use cryptograph averag case complex assumpt relat note recent show posit result learn data generat neural network certain architectur random connect weight howev assumpt use strong unlik hold practic despit theoret pessim practic modern-day neural network train success mani learn problem sever trick enabl success train chang activ function threshold activ function zero deriv almost everywher therefor appli gradient-bas method activ function circumv problem consid activ function wide known sigmoid activ form smooth approxim threshold function anoth recent popular activ function rectifi linear unit relu function note subtract shift relu relu yield approxim threshold function doubl number neuron approxim network threshold activ network relu activ over-specif empir observ easier train network larger need inde empir demonstr phenomenon sec regular empir observ regular weight network speed converg goal paper revisit re-rais question neural network comput effici modern perspect challeng topic pretend give definit answer howev provid sever result posit negat new although appear literatur context contribut follow make simpl observ suffici over-specifi network global optima ubiquit general comput easi find although hold extrem larg network overfit seen indic comput hard learn decreas amount over-specif also demonstr empir sec motiv idea chang activ function consid quadrat activ function a2 network quadrat activ comput polynomi function input rd henc call polynomi network main find network follow network quadrat activ express network threshold activ constant depth network quadrat activ learn polynomi time sigmoid network depth regular approxim polynomi network depth o log follow sigmoid network regular learn polynomi time well aforement posit result interest theoret lead impract algorithm provid practic provabl correct algorithm train depth-2 polynomi network network also learn use linear trick algorithm effici return network whose size depend data dimens algorithm follow forward greedi select procedur step greedi select procedur build new neuron solv eigenvalu problem general algorithm depth-3 forward greedi step involv effici approxim solut tensor approxim problem algorithm learn rich sub-class depth-3 polynomi network describ experiment evid show practic algorithm competit state-of-the-art neural network train method depth-2 network suffici over-specifi network easi train begin consid idea over-specif make observ suffici over-specifi network optim problem associ train general quit easi solv global optima sens ubiquit interest contrast note small network singl neuron non-convex activ function associ optim problem general hard exhibit exponenti mani local non-glob minima emphas our observ hold extrem larg network overfit reason scenario point possibl spectrum comput cost decreas amount over-specif present result let rd matrix train exampl rd think network compos two map first map matrix rn number neuron whose output connect output layer second map linear map ro n map neuron output layer final loss function ro assum convex assess qualiti predict entir data cours depend label let denot weight affect map denot function map z optim problem associ learn network therefor minw v function general non-convex may local minima howev reason assum rank f larg probabl random choic due non-linear natur function comput neural networks1 case simpli fix solv minw comput tractabl exampl consid function comput first layer sigmoid function sinc non-linear column linear depend general assum convex sinc full rank solut problem correspond global optima henc global optima origin optim problem thus suffici larg network find global optima general easi sens ubiquit hard learn neural network review sever known hard result appli our learn set simplic throughout most section focus pac model binari classif case boolean cube realiz case fix target accuracy.2 fix everi dimens let input space xd let hypothesi class function xd often omit subscript clear context learn algorithm access oracl sampl accord unknown distribut return unknown target hypothesi h. object algorithm return classifi probabl least px say effici run time poli function it return also evalu new instanc time poli say effici learnabl context neural network everi network architectur defin hypothesi class contain target function implement use neural network layer neuron exclud input neuron activ function immedi question effici learnabl first address question threshold activ function otherwis observ depth-2 network threshold activ function implement intersect halfspac reli follow hard result due theorem theorem let let nd kwk1 poli d let hka h1 h2 hk hi constant certain cryptograph assumpt hka effici learnabl differ complex assumpt show similar result even mention neural network depth activ function express intersect halfspac exampl first layer consist neuron comput halfspac second layer comput conjunct map trivial class effici learnabl class contain it also effici learnabl thus obtain follow corollari corollari everi class effici learnabl complex assumpt given what happen chang activ function particular two wide use activ function neural network sigmoid activ function sig rectifi linear unit relu activ function relu max z first observ note sig our data domain discret boolean cube henc allow weight network arbitrarili larg nt n sig similar function relu z relu equal everi result without restrict weight simul threshold activ neuron two relu activ neuron impli nt,2n relu henc corollari appli sigmoid network relu network well long regular weight network focus realiz case exist provid perfect predict fix accuraci confid sinc deal hard result result trivial appli agnost case learn arbitrarili small accuraci confid paramet what happen regular weight let target function implement use neural network depth size activ function restrict input weight neuron kwk1 l. one may argu mani real world distribut differ two class small rough speak distribut densiti low around decis boundari neuron similar separ margin assumpt sigmoid neuron abl effect simul threshold activ neuron practic sigmoid relu activ function advantag threshold activ function sinc train use gradient base method empir success turn formal guarante unfortun closer examin thm demonstr learn n2 n sig n2 n relu still hard formal appli network binari classif follow standard definit learn margin assumpt assum learner receiv exampl form sign f real-valu function come hypothesi class assum even margin assumpt follow corollari everi class nt n sig nt n relu effici learnabl complex assumpt given proof provid appendix what happen much smaller later paper show posit result constant depth fix result obtain use polynomi network studi next section polynomi network previous section shown sever strong negat result learn neural network threshold sigmoid relu activ function one way circumv these hard result consid anoth activ function mayb simplest non-linear function squar function call network use activ function polynomi network sinc comput polynomi function input previous section denot class function implement use neural network depth size squar activ function bound norm input weight neuron whenev specifi refer polynomi network unbound weight studi express comput complex polynomi network note algorithm effici learn real-valu spars low-degre polynomi studi sever previous work howev these reli strong distribut assumpt data instanc uniform log-concav distribut interest distribution-fre set express first show similar network threshold activ polynomi network polynomi size express function implement effici use ture machin theorem polynomi network express ture machin let fd thm exist constant everi class log t t2 contain fd proof theorem reli result given appendix anoth relev express result use later show polynomi network approxim network sigmoid activ function theorem fix n. bt o log tl log bn o tl log everi nt n sig function ntbt nbn supkxk kf proof reli approxim sigmoid function base chebyshev polynomi done given appendix train time turn comput complex learn polynomi network first show it hard learn polynomi network depth inde combin thm corollari we obtain follow corollari class effici learnabl flip side constant-depth polynomi network learn polynomi time use simpl linear trick specif class polynomi network constant depth contain class multivari polynomi total degre most 2t class repres ds dimension linear space vector coeffici vector polynomi therefor class polynomi network depth learn time poli d2 map instanc vector rd monomi learn linear predictor top represent done effici realiz case convex loss function use particular constant 2t therefor polynomi network constant depth effici learnabl anoth way learn class use support vector machin polynomi kernel interest applic observ depth-2 sigmoid network effici learnabl suffici regular formal result below contrast corollari provid hard result without regular theorem class n2 n sig learn accuraci time poli t o d4l idea proof follow suppos we obtain data n2 n sig base thm n2bt nbn approxim fix accuraci bt bn defin thm we learn n2bt nbn consid class polynomi total degre appli linear techniqu discuss sinc assum separ data margin sign f separ data margin enough establish accuraci sampl time depend polynomi learn 2-layer 3-layer polynomi network while interest theoret result practic sinc time sampl complex grow fast depth network.3 section we describ practic provabl correct algorithm special case depth-2 depth-3 polynomi network addit constraint although network learn polynomi time via explicit linear describ section runtim result network size scale quadrat depth-2 cubic depth-3 data dimens contrast our algorithm guarante much milder depend we first consid layer polynomi network follow form w0 kwi network correspond one hidden layer contain neuron squar activ function we restrict input weight neuron network bound norm we also allow direct linear depend input layer output layer we describ effici algorithm learn class base geco algorithm convex optim low-rank constraint one use svm polynomi kernel time sampl complex may small under margin assumpt featur space correspond given kernel note howev larg margin space differ assumpt we make name there network small number hidden neuron work well data goal algorithm find minim object r f loss function we assum smooth convex basic idea algorithm gradual add hidden neuron hidden layer greedi manner decreas loss function data defin kwk2 set function implement hidden neuron then everi affin function plus weight sum function v. algorithm start minim affin function then greedi step we search minim first order approxim r f r f r f g xi deriv first argument observ everi there kwk2 right-hand side pm xx w. henc rewritten r f vector minim pm express posit lead eigenvector matrix we add this vector hidden neuron network.4 final we minim weight hidden layer output layer name weight follow theorem follow direct theorem provid converg guarante geco observ theorem give guarante learn we allow output over-specifi network theorem fix assum loss function convex smooth then if geco algorithm run iter it output network r f minf r f we next consid hypothesi class consist third degre polynomi subset 3-layer polynomi network lemma 1nin appendix hidden neuron function qi class vi vi kwj hypothesi pk class we consid gi gi basic idea algorithm 2-layer network howev while 2-layer case we could implement effici greedi step solv eigenvalu problem we now face follow tensor approxim problem greedi step max g xi max kwk=1 kuk=1 kvk=1 while this general hard optim problem we approxim it luckili approxim greedi step suffic success greedi procedur this procedur given figur base on approxim eigenvector comput guarante qualiti approxim given appendix this lead follow theorem whose proof given appendix theorem fix assum loss function convex smooth then if geco algorithm run iter each iter reli on approxim procedur given in then probabl it output network which r f minf r f it also possibl find approxim solut eigenvalu problem still retain perform guarante sinc approxim eigenvalu found in time use the power method we obtain the runtim geco depend linear on input approxim solut output max kwk kuk kvk=1 pick random w1 ws iid accord id for 2d log wt wt kw tk let set ut vt av max kuk kvk=1 r u av return the maxim maxi ui ui figur approxim tensor maxim experi demonstr the practic geco train neural network for real world problem we consid pedestrian detect problem follow we collect train exampl imag patch size pixel contain either pedestrian posit exampl hard negat exampl contain imag classifi as pedestrian appli simpl linear classifi in slide window manner see exampl imag we use half the exampl as train set the half as test set we calcul hog featur from the images5 we then train use geco sgd relu depth-2 polynomi network on the result featur we sgd squar geco use neuron in the hidden layer for comparison we train the network architectur hidden neuron squar activ function sgd we also train a similar network hidden neuron the relu activ function for the sgd implement we tri the follow trick speed the converg heurist for initi the weight learn rate rule mini-batch nesterov mo0 iter mentum as explain in dropout the test error sgd as a function the number iter are depict on the top plot the figur on the side we also mark the perfor4 manc geco as a straight line sinc it t involv sgd iter as seen the error geco slight bet2 ter sgd it also note we perform a larg number sgd iter obtain a good solut while the runtim geco much faster this indic geco may a valid altern approach to sgd for train depth-2 network it also appar the squar activ iter function slight better the relu function for this task error mse the second plot the side figur demonstr the benefit over-specif for sgd we generat random exampl in pass a random depth-2 network contain hidden neuron the relu activ function we then tri to fit a new network to this data over-specif factor overspecif factor mean we use hidden neuron as can clear seen sgd converg much faster when we over-specifi the network acknowledg this research support intel icri-ci os also support an isf grant a marie-curi career integr grant sss rl also support the mos center knowledg for ai ml rl a recipi the googl europ fellowship in learn theori this research support in part by this googl fellowship we thank itay safran for spot a mistak in a previous version sec to jame marten for help discuss use the matlab implement provid in http //www.mathworks.com/matlabcentr fileexchange/33863-histograms-of-oriented-gradi
----------------------------------------------------------------

