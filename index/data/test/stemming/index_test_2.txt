query sentence: image processing algorithms
---------------------------------------------------------------------
title: 4361-spatial-distance-dependent-chinese-restaurant-processes-for-image-segmentation.pdf

Spatial distance dependent Chinese restaurant
processes for image segmentation
Soumya Ghosh1 , Andrei B. Ungureanu2 , Erik B. Sudderth1 , and David M. Blei3
1

Department of Computer Science, Brown University, {sghosh,sudderth}@cs.brown.edu
2
Morgan Stanley, andrei.b.ungureanu@gmail.com
3
Department of Computer Science, Princeton University, blei@cs.princeton.edu

Abstract
The distance dependent Chinese restaurant process (ddCRP) was recently introduced to accommodate random partitions of non-exchangeable data [1]. The ddCRP clusters data in a biased way: each data point is more likely to be clustered
with other data that are near it in an external sense. This paper examines the ddCRP in a spatial setting with the goal of natural image segmentation. We explore
the biases of the spatial ddCRP model and propose a novel hierarchical extension better suited for producing ?human-like? segmentations. We then study the
sensitivity of the models to various distance and appearance hyperparameters, and
provide the first rigorous comparison of nonparametric Bayesian models in the image segmentation domain. On unsupervised image segmentation, we demonstrate
that similar performance to existing nonparametric Bayesian models is possible
with substantially simpler models and algorithms.

1 Introduction
The Chinese restaurant process (CRP) is a distribution on partitions of integers [2]. When used
in a mixture model, it provides an alternative representation of a Bayesian nonparametric Dirichlet
process mixture?the data are clustered and the number of clusters is determined via the posterior
distribution. CRP mixtures assume that the data are exchangeable, i.e., their order does not affect the distribution of cluster structure. This can provide computational advantages and simplify
approximate inference, but is often an unrealistic assumption.
The distance dependent Chinese restaurant process (ddCRP) was recently introduced to model random partitions of non-exchangeable data [1]. The ddCRP clusters data in a biased way: each data
point is more likely to be clustered with other data that are near it in an external sense. For example,
when clustering time series data, points that closer in time are more likely to be grouped together.
Previous work [1] developed the ddCRP mixture in general, and derived posterior inference algorithms based on Gibbs sampling [3]. While they studied the ddCRP in time-series and sequential
settings, ddCRP models can be used with any type of distance and external covariates. Recently,
other researchers [4] have also used the ddCRP in non-temporal settings.
In this paper, we study the ddCRP in a spatial setting. We use a spatial distance function between
pixels in natural images and cluster them to provide an unsupervised segmentation. The spatial distance encourages the discovery of connected segments. We also develop a region-based hierarchical
generalization, the rddCRP. Analogous to the hierarchical Dirichlet process (HDP) [5], the rddCRP
clusters groups of data, where cluster components are shared across groups. Unlike the HDP, however, the rddCRP allows within-group clusterings to depend on external distance measurements.
To demonstrate the power of this approach, we develop posterior inference algorithms for segmenting images with ddCRP and rddCRP mixtures. Image segmentation is an extensively studied area,
1

C

C

Removing C leaves clustering unchanged
Adding C leaves the clustering unchanged

Removing C splits the cluster
Adding C merges the cluster

Figure 1: Left: An illustration of the relationship between the customer assignment representation and the
table assignment representation. Each square is a data point (a pixel or superpixel) and each arrow is a customer
assignment. Here, the distance window is of length 1. The corresponding table assignments, i.e., the clustering
of these data, is shown by the color of the data points. Right: Intuitions behind the two cases considered by the
Gibbs sampler. Consider the link from node C. When removed, it may leave the clustering unchanged or split
a cluster. When added, it may leave the clustering unchanged or merge two clusters.
which we will not attempt to survey here. Influential existing methods include approaches based
on kernel density estimation [6], Markov random fields [3, 7], and the normalized cut spectral clustering algorithm [8, 9]. A recurring difficulty encountered by traditional methods is the need to
determine an appropriate segment resolution for each image; even among images of similar scene
types, the number of observed objects can vary widely. This has usually been dealt via heuristics
with poorly understood biases, or by simplifying the problem (e.g., partially specifying each image?s
segmentation via manual user input [7]).
Recently, several promising segmentation algorithms have been proposed based on nonparametric
Bayesian methods [10, 11, 12]. In particular, an approach which couples Pitman-Yor mixture models [13] via thresholded Gaussian processes [14] has lead to very promising initial results [10], and
provides a baseline for our later experiments. Expanding on the experiments in [10], we analyze
800 images of different natural scene types, and show that the comparatively simpler ddCRP-based
algorithms perform similarly to this work. Moreover, unlike previous nonparametric Bayesian approaches, the structure of the ddCRP allows spatial connectivity of the inferred segments to (optionally) be enforced. In some applications, this is a known property of all reasonable segmentations.
Our results demonstrate the practical utility of spatial ddCRP and hierarchical rddCRP models. We
also provide the first rigorous comparison of nonparametric Bayesian image segmentation models.

2 Image Segmentation with Distance Dependent CRPs
Our goal is to develop a probabilistic method to segment images of complex scenes. Image segmentation is the problem of partitioning an image into self-similar groups of adjacent pixels. Segmentation is an important step towards other tasks in image understanding, such as object recognition,
detection, or tracking. We model images as observed collections of ?superpixels? [15], which are
small blocks of spatially adjacent pixels. Our goal is to associate the features xi in the ith superpixel
with some cluster zi ; these clusters form the segments of that image.
Image segmentation is thus a special kind of clustering problem where the desired solution has two
properties. First, we hope to find contiguous regions of the image assigned to the same cluster. Due
to physical processes such as occlusion, it may be appropriate to find segments that contain two
or three contiguous image regions, but we do not want a cluster that is scattered across individual
image pixels. Traditional clustering algorithms, such as k-means or probabilistic mixture models,
do not account for external information such as pixel location and are not biased towards contiguous regions. Image locations have been heuristically incorporated into Gaussian mixture models
by concatenating positions with appearance features in a vector [16], but the resulting bias towards
elliptical regions often produces segmentation artifacts. Second, we would like a solution that deter2

mines the number of clusters from the image. Image segmentation algorithms are typically applied
to collections of images of widely varying scenes, which are likely to require different numbers of
segments. Except in certain restricted domains such as medical image analysis, it is not practical to
use an algorithm that requires knowing the number of segments in advance.
In the following sections, we develop a Bayesian algorithm for image segmentation based on the
distance dependent Chinese restaurant process (ddCRP) mixture model [1]. Our algorithm finds
spatially contiguous segments in the image and determines an image-specific number of segments
from the observed data.
2.1 Chinese restaurant process mixtures
The ddCRP mixture is an extension of the traditional Chinese restaurant process (CRP) mixture.
CRP mixtures provide a clustering method that determines the number of clusters from the data?
they are an alternative formulation of the Dirichlet process mixture model. The assumed generative
process is described by imagining a restaurant with an infinite number of tables, each of which is
endowed with a parameter for some family of data generating distributions (in our experiments,
Dirichlet). Customers enter the restaurant in sequence and sit at a randomly chosen table. They sit
at the previously occupied tables with probability proportional to how many customers are already
sitting at each; they sit at an unoccupied table with probability proportional to a scaling parameter.
After the customers have entered the restaurant, the ?seating plan? provides a clustering. Finally,
each customer draws an observation from a distribution determined by the parameter at the assigned
table.
Conditioned on observed data, the CRP mixture provides a posterior distribution over table assignments and the parameters attached to those tables. It is a distribution over clusterings, where the
number of clusters is determined by the data. Though described sequentially, the CRP mixture is an
exchangeable model: the posterior distribution over partitions does not depend on the ordering of
the observed data.
Theoretically, exchangeability is necessary to make the connection between CRP mixtures and
Dirichlet process mixtures. Practically, exchangeability provides efficient Gibbs sampling algorithms for posterior inference. However, exchangeability is not an appropriate assumption in image
segmentation problems?the locations of the image pixels are critical to providing contiguous segmentations.
2.2 Distance dependent CRPs
The distance dependent Chinese Restaurant Process (ddCRP) is a generalization of the Chinese
restaurant process that allows for a non-exchangeable distribution on partitions [1]. Rather than
represent a partition by customers assigned to tables, the ddCRP models customers linking to other
customers. The seating plan is a byproduct of these links?two customers are sitting at the same
table if one can reach the other by traversing the customer assignments. As in the CRP, tables are
endowed with data generating parameters. Once the partition is determined, the observed data for
each customer are generated by the per-table parameters.
As illustrated in Figure 1, the generative process is described in terms of customer assignments ci
(as opposed to partition assignments or tables, zi ). The distribution of customer assignments is

f (dij ) j 6= i,
(1)
p (ci = j | D, f, ?) ?
?
j = i.
Here dij is a distance between data points i and j and f (d) is called the decay function. The decay
function mediates how the distance between two data points affects their probability of connecting
to each other, i.e., their probability of belonging to the same cluster.
Details of the ddCRP are found in [1]. We note that the traditional CRP is an instance of a ddCRP.
However, in general, the ddCRP does not correspond to a model based on a random measure, like
the Dirichlet process. The ddCRP is appropriate for image segmentation because it can naturally
account for the spatial structure of the superpixels through its distance function. We use a spatial
distance between pixels to enforce a bias towards contiguous clusters. Though the ddCRP has been
previously described in general, only time-based distances are studied in [1].
3

Figure 2: Comparison of distance-dependent segmentation priors. From left to right, we show segmentations
produced by the ddCRP with a = 1, the ddCRP with a = 2, the ddCRP with a = 5, and the rddCRP with
a = 1.

Restaurants represent images, tables represent segment assignment, and customers represent superpixels. The distance between superpixels is modeled as the number of hops required to reach one
superpixel from another, with hops being allowed only amongst spatially neighboring superpixels.
A ?window? decay function of width a, f (d) = 1[d ? a], determines link probabilities. If a = 1,
superpixels can only directly connect to adjacent superpixels. Note this does not explicitly restrict
the size of segments, because any pair of pixels for which one is reachable from the other (i.e., in the
same connected component of the customer assignment graph) are in the same image segment. For
this special case segments are guaranteed with probability one to form spatially connected subsets
of the image, a property not enforced by other Bayesian nonparametric models [10, 11, 12].
The full generative process for the observed features x1:N within a N -superpixel image is as follows:
1. For each table, sample parameters ?k ? G0 .
2. For each customer, sample a customer assignment ci ? ddCRP(?, f, D). This indirectly
determines the cluster assignments z1:N , and thus the segmentation.
3. For each superpixel, independently sample observed data xi ? P (? | ?zi ).
The customer assignments are sampled using the spatial distance between pixels. The partition
structure, derived from the customer assignments, is used to sample the observed image features.
Given an image, the posterior distribution of the customer assignments induces a posterior over the
cluster structure; this provides the segmentation. See Figure 1 for an illustration of the customer
assignments and their derived table assignments in a segmentation setting.
As in [10], the data generating distribution for the observed features studied in Section 4 is multinomial, with separate distributions for color and texture. We place conjugate Dirichlet priors on these
cluster parameters.
2.3 Region-based hierarchical distance dependent CRPs
The ddCRP model, when applied to an image with window size a = 1, produces a collection
of contiguous patches (tables) homogeneous in color and texture features (Figure 2). While such
segmentations are useful for various applications [16], they do not reflect the statistics of manual
human segmentations, which contain larger regions [17]. We could bias our model to produce such
regions by either increasing the window size a, or by introducing a hierarchy wherein the produced
patches are grouped into a small number of regions. This region level model has each patch (table)
associated with a region k from a set of potentially infinite regions. Each region in turn is associated
with an appearance model ?k . The corresponding generative process is described as follows:
1. For each customer, sample customer assignments ci ? ddCRP (?, f, D). This determines
the table assignments t1:N .
2. For each table t, sample region assignments kt ? CRP (?).
3. For each region, sample parameters ?k ? G0 .
4. For each superpixel, independently sample observed data xi ? P (? | ?zi ), where zi = kti .
Note that this region level rddCRP model is a direct extension of the Chinese restaurant franchise
(CRF) representation of the HDP [5], with the image partition being drawn from a ddCRP instead
4

of a CRP. In contrast to prior applications of the HDP, our region parameters are not shared amongst
images, although it would be simple to generalize to this case. Figure 3 plots samples from the
rddCRP and ddCRP priors with increasing a. The rddCRP produces larger partitions than the ddCRP
with a = 1, while avoiding the noisy boundaries produced by a ddCRP with large a (see Figure 2).

3 Inference with Gibbs Sampling
A segmentation of an observed image is found by posterior inference. The problem is to compute
the conditional distribution of the latent variables?the customer assignments c1:N ?conditioned
on the observed image features x1:N , the scaling parameter ?, the distances between pixels D, the
window size a, and the base distribution hyperparameter ?:
Q

N
p(c
|
D,
a,
?)
p(x1:N | z(c1:N ), ?)
i
i=1
Q

p(c1:N | x1:N , ?, d, a, ?) = P
(2)
N
i=1 p(ci | D, a, ?) p(x1:N | z(c1:N ), ?)
c1:N

where z(c1:N ) is the cluster representation that is derived from the customer representation c1:N .
Notice again that the prior term uses the customer representation to take into account distances
between data points; the likelihood term uses the cluster representation.
The posterior in Equation (2) is not tractable to directly evaluate, due to the combinatorial sum in
the denominator. We instead use Gibbs sampling [3], a simple form of Monte Carlo Markov chain
(MCMC) inference [18]. We define the Markov chain by iteratively sampling each latent variable ci
conditioned on the others and the observations,
p(ci | c?i , x1:N , D, ?, ?) ? p(ci | D, ?)p(x1:N | z(c1:N ), ?).

(3)

The prior term is given in Equation (1). We can decompose the likelihood term as follows:
K(c1:N )

p(x1:N | z(c1:N ), ?) =

Y

p(xz(c1:N )=k | z(c1:N ), ?).

(4)

k=1

We have introduced notation to more easily move from the customer representation?the primary
latent variables of our model?and the cluster representation. Let K(c1:N ) denote the number of
unique clusters in the customer assignments, z(c1:N ) the cluster assignments derived from the customer assignments, and xz(c1:N )=k the collection of observations assigned to cluster k. We assume
that the cluster parameters ?k have been analytically marginalized. This is possible when the base
distribution G0 is conjugate to the data generating distribution, e.g. Dirichlet to multinomial.
Sampling from Equation (3) happens in two stages. First, we remove the customer link ci from the
current configuration. Then, we consider the prior probability of each possible value of ci and how
it changes the likelihood term, by moving from p(x1:N | z(c?i ), ?) to p(x1:N | z(c1:N ), ?).
In the first stage, removing ci either leaves the cluster structure intact, i.e., z(cold
1:N ) = z(c?i ), or
splits the cluster assigned to data point i into two clusters. In the second stage, randomly reassigning
ci either leaves the cluster structure intact, i.e., z(c?i ) = z(c1:N ), or joins the cluster assigned to
data point i to another. See Figure 1 for an illustration of these cases. Via these moves, the sampler
explores the space of possible segmentations.
Let ? and m be the indices of the tables that are joined to index k. We first remove ci , possibly
splitting a cluster. Then we sample from

p(ci | D, ?)?(x, z, ?) if ci joins ? and m;
p(ci | c?i , x1:N , D, ?, ?) ?
(5)
p(ci | D, ?)
otherwise,
where
?(x, z, ?) =

p(xz(c1:N )=k | ?)
.
p(xz(c1:N )=? | ?)p(xz(c1:N )=m | ?)

(6)

This defines a Markov chain whose stationary distribution is the posterior of the spatial ddCRP
defined in Section 2. Though our presentation is slightly different, this algorithm is equivalent to the
one developed for ddCRP mixtures in [1].
5

In the rddCRP, the algorithm for sampling the customer indicators is nearly the same, but with
two differences. First, when ci is removed, it may spawn a new cluster. In that case, the region
identity of the new table must be sampled from the region level CRP. Second, the likelihood term in
Equation (4) depends only on the superpixels in the image assigned to the segment in question. In
the rddCRP, it also depends on other superpixels assigned to segments that are assigned to the same
region. Finally, the rddCRP also requires resampling of region assignments as follows:
 ?t
m? p(xt | x?t , ?) if ? is used;
p(kt = ? | k?t , x1:N , t(c1:N ), ?, ?) ?
(7)
?p(xt | ?)
if ? is new.
Here, xt is the set of customers sitting at table t, x?t is the set of all customers associated with
region ? excluding xt , and m?t
? is the number of tables associated with region ? excluding xt .

4 Empirical Results
We compare the performance of the ddCRP to manual segmentations of images drawn from eight
natural scene categories [19]. Non-expert users segmented each image into polygonal shapes, and
labeled them as distinct objects. The collection, which is available from LabelMe [17], contains a
total of 2,688 images.1 We randomly select 100 images from each category. This image collection
has been previously used to analyze an image segmentation method based on spatially dependent
Pitman-Yor (PY) processes [10], and we compare both methods using an identical feature set. Each
image is first divided into approximately 1000 superpixels [15, 20]2 using the normalized cut algorithm [9].3 We describe the texture of each superpixel via a local texton histogram [21], using
band-pass filter responses quantized to 128 bins. A 120-bin HSV color histogram is also computed.
Each superpixel i is summarized via these histograms xi .
Our goal is to make a controlled comparison to alternative nonparametric Bayesian methods on a
challenging task. Performance is assessed via agreement with held out human segmentations, via
the Rand index [22]. We also present segmentation results for qualitative evaluation in Figures 3
and 4 .
4.1 Sensitivity to Hyperparameters
Our models are governed by the CRP concentration parameters ? and ?, the appearance base measure hyperparameter ? = (?0 , ...?0 ), and the window size a. Empirically, ? has little impact on the
segmentation results, due to the high-dimensional and informative image features; all our experiments set ? = 1. ? and ?0 induce opposing biases: a small ? encourages larger segments, while a
large ?0 encourages larger segments. We found ? = 10?8 and ?0 = 20 to work well.
The most influential prior parameter is the window size a, the effect of which is visualized in Figure 3. For the ddCRP model, setting a = 1 (ddCRP1) produces a set of small but contiguous
segments. Increasing to a = 2 (ddCRP2) results in fewer segments, but the produced segments are
typically spatially fragmented. This phenomenon is further exacerbated with larger values of a. The
rddCRP model groups segments produced by a ddCRP. Because it is hard to recover meaningful
partitions if these initial segments are poor, the rddCRP performs best when a = 1.
4.2 Image Segmentation Performance
We now quantitatively measure the performance of our models. The ddCRP and the rddCRP samplers were run for 100 and 500 iterations, respectively. Both samplers displayed rapid mixing and
often stabilized withing the first 50 iterations. Note that similar rapid mixing has been observed in
other applications of the ddCRP [1].
We also compare to two previous models [10]: a PY mixture model with no spatial dependence
(pybof20), and a PY mixture with spatial coupling induced via thresholded Gaussian processes (pydist20). To control the comparison as much as possible, the PY models are tested with identical
features and base measure ?, and other hyperparameters as in [10]. We also compare to the nonspatial PY with ?0 = 1, the best bag-of-feature model in our experiments (pybof ). We employ
1

http://labelme.csail.mit.edu/browseLabelMe/
http://www.cs.sfu.ca/?mori/
3
http://www.eecs.berkeley.edu/Research/Projects/CS/vision/
2

6

Models
Images

Figure 3: Segmentations produced by various Bayesian nonparametric methods. From left to right, the
columns display natural images, segmentations for the ddCRP with a = 1, the ddCRP with a = 2, the rddCRP
with a = 1, and thresholded Gaussian processes (pydist20) [10]. The top row displays partitions sampled from
the corresponding priors, which have 130, 54, 5, and 6 clusters, respectively.

Figure 4: Top left: Average segmentation performance on the database of natural scenes, as measured by the
Rand index (larger is better), and those pairs of methods for which a Wilcoxon?s signed rank test indicates comparable performance with 95% confidence. In the binary image, dark pixels indicate pairs that are statistically
indistinguishable. Note that the rddCRP, spatial PY, and mean shift methods are statistically indistinguishable,
and significantly better than all others. Bottom left: Scatter plots comparing the pydist20 and rddCRP methods
in the Mountain and Street scene categories. Right: Example segmentations produced by the rddCRP.

non-hierarchical versions of the PY models, so that each image is analyzed independently, and perform inference via the previously developed mean field variational method. Finally, from the vision
literature we also compare to the normalized cuts (Ncuts) [8] and mean shift (MS) [6] segmentation
algorithms.4
4
We used the EDISON implementation of mean shift. The parameters of mean shift and normalized cuts
were tuned by performing a grid search over a training set containing 25 images from each of the 8 categories.
For normalized cuts the optimal number of segments was determined to be 5. For mean shift we held the spatial

7

Quantitative performance is summarized in Figure 4. The rddCRP outscores both versions of the
ddCRP model, in terms of Rand index. Nevertheless, the patchy ddCRP1 segmentations are interesting for applications where segmentation is an intermediate step rather than the final goal. The bag
of features model with ?0 = 20 performs poorly; with optimized ?0 = 1 it is better, but still inferior
to the best spatial models.
In general, the spatial PY and rddCRP perform similarly. The scatter plots in Fig. 4, which show
Rand indexes for each image from the mountain and street categories, provide insights into when
one model outperforms the other. For the street images rddCRP is better, while for images containing mountains spatial PY is superior. In general, street scenes contain more objects, many of which
are small, and thus disfavored by the smooth Gaussian processes underlying the PY model. To most
fairly compare priors, we have tested a version of the spatial PY model employing a covariance function that depends only on spatial distance. Further performance improvements were demonstrated in
[10] via a conditionally specified covariance, which depends on detected image boundaries. Similar
conditional specification of the ddCRP distance function is a promising direction for future research.
Finally, we note that the ddCRP (and rddCRP) models proposed here are far simpler than the spatial
PY model, both in terms of model specification and inference. The ddCRP models only require
pairwise superpixel distances to be specified, as opposed to the positive definite covariance function
required by the spatial PY model. Furthermore, the PY model?s usage of thresholded Gaussian
processes leads to a complex likelihood function, for which inference is a significant challenge. In
contrast, ddCRP inference is carried out through a straightforward sampling algorithm,5 and thus
may provide a simpler foundation for building rich models of visual scenes.

5 Discussion
We have studied the properties of spatial distance dependent Chinese restaurant processes, and applied them to the problem of image segmentation. We showed that the spatial ddCRP model is
particularly well suited for segmenting an image into a collection of contiguous patches. Unlike
previous Bayesian nonparametric models, it can produce segmentations with guaranteed spatial
connectivity. To go from patches to coarser, human-like segmentations, we developed a hierarchical region-based ddCRP. This hierarchical model achieves performance similar to state-of-the-art
nonparametric Bayesian segmentation algorithms, using a simpler model and a substantially simpler
inference algorithm.

References
[1] D. M. Blei and P. I. Frazier. Distant dependent chinese restaurant processes. Journal of Machine Learning Research, 12:2461?2488, August 2011.
[2] J. Pitman. Combinatorial Stochastic Processes. Lecture Notes for St. Flour Summer School.
Springer-Verlag, New York, NY, 2002.
[3] S. Geman and D. Geman. Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. IEEE Transactions on pattern analysis and machine intelligence, 6(6):721?741,
November 1984.
[4] Richard Socher, Andrew Maas, and Christopher D. Manning. Spectral chinese restaurant processes: Nonparametric clustering based on similarities. In Fourteenth International Conference
on Artificial Intelligence and Statistics (AISTATS), 2011.
[5] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. Hierarchical Dirichlet processes. Journal
of American Statistical Association, 25(2):1566 ? 1581, 2006.
[6] D. Comaniciu and P. Meer. Mean shift: A robust approach toward feature space analysis. IEEE
Transactions on pattern analysis and machine intelligence, pages 603?619, 2002.
bandwidth constant at 7, and found optimal values of feature bandwidth and minimum region size to be 25 and
4000 pixels, respectively.
5
In our Matlab implementations, the core ddCRP code was less than half as long as the corresponding PY
code. For the ddCRP, the computation time was 1 minute per iteration, and convergence typically happened
after only a few iterations. The PY code, which is based on variational approximations, took 12 minutes per
image.

8

[7] C. Rother, V. Kolmogorov, and A. Blake. Grabcut: Interactive foreground extraction using
iterated graph cuts. In ACM Transactions on Graphics (TOG), volume 23, pages 309?314,
2004.
[8] J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Trans. PAMI, 22(8):888?
905, 2000.
[9] C. Fowlkes, D. Martin, and J. Malik. Learning affinity functions for image segmentation:
Combining patch-based and gradient-based approaches. CVPR, 2:54?61, 2003.
[10] E. B. Sudderth and M. I. Jordan. Shared segmentation of natural scenes using dependent
pitman-yor processes. NIPS 22, 2008.
[11] P. Orbanz and J. M. Buhmann. Smooth image segmentation by nonparametric Bayesian inference. In ECCV, volume 1, pages 444?457, 2006.
[12] Lan Du, Lu Ren, David Dunson, and Lawrence Carin. A bayesian model for simultaneous
image clustering, annotation and object segmentation. In NIPS 22, pages 486?494. 2009.
[13] J. Pitman and M. Yor. The two-parameter Poisson?Dirichlet distribution derived from a stable
subordinator. Annals of Probability, 25(2):855?900, 1997.
[14] J. A. Duan, M. Guindani, and A. E. Gelfand. Generalized spatial Dirichlet process models.
Biometrika, 94(4):809?825, 2007.
[15] X. Ren and J. Malik. Learning a classification model for segmentation. ICCV, 2003.
[16] C. Carson, S. Belongie, H. Greenspan, and J. Malik. Blobworld: Image segmentation using
expectation-maximization and its application to image querying. PAMI, 24(8):1026?1038,
August 2002.
[17] B. C. Russell, A. Torralba, K. P. Murphy, and W. T. Freeman. Labelme: A database web-based
tool for image annotation. IJCV, 77:157?173, 2008.
[18] C. Robert and G. Casella. Monte Carlo Statistical Methods. Springer Texts in Statistics.
Springer-Verlag, New York, NY, 2004.
[19] A. Oliva and A. Torralba. Modeling the shape of the scene: A holistic representation of the
spatial envelope. IJCV, 42(3):145 ? 175, 2001.
[20] G. Mori. Guiding model search using segmentation. ICCV, 2005.
[21] D. R. Martin, C.C. Fowlkes, and J. Malik. Learning to detect natural image boundaries using
local brightness, color, and texture cues. IEEE Trans. PAMI, 26(5):530?549, 2004.
[22] W.M. Rand. Objective criteria for the evaluation of clustering methods. Journal of the American Statistical Association, pages 846?850, 1971.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 2981-chained-boosting.pdf

Chained Boosting

Christian R. Shelton
University of California
Riverside CA 92521
cshelton@cs.ucr.edu

Wesley Huie
University of California
Riverside CA 92521
whuie@cs.ucr.edu

Kin Fai Kan
University of California
Riverside CA 92521
kkan@cs.ucr.edu

Abstract
We describe a method to learn to make sequential stopping decisions, such as
those made along a processing pipeline. We envision a scenario in which a series
of decisions must be made as to whether to continue to process. Further processing
costs time and resources, but may add value. Our goal is to create, based on historic data, a series of decision rules (one at each stage in the pipeline) that decide,
based on information gathered up to that point, whether to continue processing
the part. We demonstrate how our framework encompasses problems from manufacturing to vision processing. We derive a quadratic (in the number of decisions)
bound on testing performance and provide empirical results on object detection.

1 Pipelined Decisions
In many decision problems, all of the data do not arrive at the same time. Often further data collection can be expensive and we would like to make a decision without accruing the added cost.
Consider silicon wafer manufacturing. The wafer is processed in a series of stages. After each stage
some tests are performed to judge the quality of the wafer. If the wafer fails (due to flaws), then the
processing time, energy, and materials are wasted. So, we would like to detect such a failure as early
as possible in the production pipeline.
A similar problem can occur in vision processing. Consider the case of object detection in images.
Often low-level pixel operations (such as downsampling an image) can be performed in parallel by
dedicated hardware (on a video capture board, for example). However, searching each subimage
patch of the whole image to test whether it is the object in question takes time that is proportional to
the number of pixels. Therefore, we can imagine a image pipeline in which low resolution versions
of the whole image are scanned first. Subimages which are extremely unlikely to contain the desired
object are rejected and only those which pass are processed at higher resolution. In this way, we
save on many pixel operations and can reduce the cost in time to process an image.
Even if downsampling is not possible through dedicated hardware, for most object detection
schemes, the image must be downsampled to form an image pyramid in order to search for the
object at different scales. Therefore, we can run the early stages of such a pipelined detector at the
low resolution versions of the image and throw out large regions of the high resolution versions.
Most of the processing is spent searching for small faces (at the high resolutions), so this method
can save a lot of processing.
Such chained decisions also occur if there is a human in the decision process (to ask further clarifying
questions in database search, for instance). We propose a framework that can model all of these
scenarios and allow such decision rules to be learned from historic data. We give a learning algorithm
based on the minimization of the exponential loss and conclude with some experimental results.

1.1 Problem Formulation
Let there be s stages to the processing pipeline. We assume that there is a static distribution from
which the parts, objects, or units to be processed are drawn. Let p(x, c) represent this distribution in
which x is a vector of the features of this unit and c represents the costs associated with this unit. In
particular, let xi (1 ? i ? s) be the set of measurements (features) available to the decision maker
immediately following stage i. Let c i (1 ? i ? s) be the cost of rejecting (or stopping the processing
of) this unit immediately following stage i. Finally, let c s+1 be the cost of allowing the part to pass
through all processing stages.
Note that ci need not be monotonic in i. To take our wafer manufacturing example, for wafers that
are good we might let c i = i for 1 ? i ? s, indicating that if a wafer is rejected at any stage, one
unit of work has been invested for each stage of processing. For the same good wafers, we might
let cs+1 = s ? 1000, indicating that the value of a completed wafer is 1000 units and therefore the
total cost is the processing cost minus the resulting value. For a flawed wafer, the values might be
the same, except for c s+1 which we would set to s, indicating that there is no value for a bad wafer.
Note that the costs may be either positive or negative. However, only their relative values are important. Once a part has been drawn from the distribution, there is no way of affecting the ?base
level? for the value of the part. Therefore, we assume for the remainder of this paper that c i ? 0 for
1 ? i ? s + 1 and that ci = 0 for some value of i (between 1 and s + 1).
Our goal is to produce a series of decision rules f i (xi ) for 1 ? i ? s. We let fi have a range of
{0, 1} and let 0 indicate that processing should continue and 1 indicate that processing should be
halted. We let f denote the collection of these s decision rules and augment the collection with an
additional rule f s+1 which is identically 1 (for ease of notation). The cost of using these rules to
halt processing an example is therefore
L(f (x), c) =

s+1


ci fi (xi )

i=1

i?1


(1 ? fj (xj )) .

j=1

We would like to find a set of decision rules that minimize E p [L(f (x), c)].
While p(x, c) is not known, we do have a series of samples (training set) D =
{(x1 , c1 ), (x2 , c2 ), . . . , (xn , cn )} of n examples drawn from the distribution p. We use superscripts
to denote the example index and subscripts to denote the stage index.

2 Boosting Solution
For this paper, we consider constructing the rules f i from simpler decision rules, much as in the
Adaboost algorithm [1, 2]. We assume that each decision f i (xi ) is computed as the threshold of
another function g i (xi ): fi (xi ) = I(gi (xi ) > 0).1 We bound the empirical risk:
n


L(f (xk ), ck ) =

n 
s+1


cki I(gi (xki ) > 0)

k=1 i=1

k=1

?

n 
s+1

k=1 i=1

i?1


I(gj (xkj ) ? 0)

j=1
k

cki egi (xi )

i?1

j=1

k

e?gj (xj ) =

n 
s+1


k

cki egi (xi )?

Pi?1

j=1

gj (xk
j)

.

(1)

k=1 i=1

Our decision to make all costs positive ensures that the bounds hold. Our decision to make the
optimal cost zero helps to ensure that the bound is reasonably tight.
 i
As in boosting, we restrict g i (xi ) to take the form m
l=1 ?i,l hi,l (xi ), the weighted sum of m i subclassifiers, each of which returns either ?1 or +1. We will construct these weighted sums incrementally and greedily, adding one additional subclassifier and associated weight at each step. We will
pick the stage, weight, and function of the subclassifier in order to make the largest negative change
in the exponential bound to the empirical risk. The subclassifiers, h i,l will be drawn from a small
class of hypotheses, H.
1

I is the indicator function that equals 1 if the argument is true and 0 otherwise.

1. Initialize gi (x) = 0 for all stages i
2. Initialize wik = cki for all stages i and examples k.
3. For each stage i:
(a) Calculate targets for each training example, as shown in equation 5.
(b) Let h be the result of running the base learner on this set.
(c) Calculate the corresponding ? as per equation 3.
(d) Score this classification as per equation 4
? and ?
4. Select the stage ?? with the best (highest) score. Let h
? be the classifier and
weight found at that stage.
?
? h(x).
5. Let g?? (x) ? g?? (x) + ?
6. Update the weights (see equation 2):
?

k

? ?1 ? k ? n, multiply w??k by e?? h(x?? ) .

?

k

? ?1 ? k ? n, j > ??, multiply wjk by e???h(x?? ) .
7. Repeat from step 3
Figure 1: Chained Boosting Algorithm
2.1 Weight Optimization
We first assume that the stage at which to add a new subclassifier and the subclassifier to add have
? respectively. That is, h
? will become h??,m +1 but we simplify it for
already been chosen: ?? and h,
?
?
ease of expression. Our goal is to find ? ??,m?? +1 which we similarly abbreviate to ?
? . We first define
k

wik = cki egi (xi )?

Pi?1

j=1

gj (xk
j)

(2)

as the weight of example k at stage i, or its current contribution to our risk bound. If we let D h?+ be
? returns +1, and let D ?? be similarly defined for
the set of indexes of the members of D for which h
h
? returns ?1, we can further define
those for which h
W??+ =



w??k +

+
k?Dh
?

s+1
 

wik

W??? =

? i=?
?+1
k?Dh
?



w??k +

?
k?Dh
?

s+1
 

wik .

+ i=?
?+1
k?Dh
?

? will emphasize. That is, it corresponds to
We interpret W??+ to be the sum of the weights which h
? selects: For those examples for which h
? recommends termination,
the weights along the path that h
we add the current weight (related to the cost of stopping the processing at this stage). For those
? recommends continued processing, we add in all future weights (related to all
examples for which h
future costs associated with this example). W??? can be similarly interpreted to be the weights (or
? recommends skipping.
costs) that h
If we optimize the loss bound of Equation 1 with respect to ?
? , we obtain
?
?=

1
W?
log ??+ .
2
W??

(3)

The more weight (cost) that the rule recommends to skip, the higher its ? coefficient.
2.2 Full Optimization
Using Equation 3 it is straight forward to show that the reduction in Equation 1 due to the addition
of this new subclassifier will be
W??+ (1 ? e?? ) + W??? (1 ? e??? ) .

(4)

We know of no efficient method for determining ??, the stage at which to add a subclassifier, except
by exhaustive search. However, within a stage, the choice of which subclassifier to use becomes one

of maximizing
n



? k)
z??k h(x
?
?

, where

z??k

=

s+1



wik

? w??k

(5)

i=?
?+1

k=1

? This is equivalent to an weighted empirical risk minimization where the training
with respect to h.
set is {x?1? , x?2? , . . . , x?n? }. The label of x?k? is the sign of z??k , and the weight of the same example is the
magnitude of z??k .
2.3 Algorithm
The resulting algorithm is only slightly more complex than standard Adaboost. Instead of a weight
vector (one weight for each data example), we now have a weight matrix (one weight for each
data example for each stage). We initialize each weight to be the cost associated with halting the
corresponding example at the corresponding stage. We start with all g i (x) = 0. The complete
algorithm is as in Figure 1.
Each time through steps 3 through 7, we complete one ?round? and add one additional rule to one
stage of the processing. We stop executing this loop when ?
? ? 0 or when an iteration counter
exceeds a preset threshold.
Bottom-Up Variation
In situations where information is only gained after each stage (such as in section 4), we can also
train the classifiers ?bottom-up.? That is, we can start by only adding classifiers to the last stage.
Once finished with it, we proceed to the previous stage, and so on. Thus instead of selecting the
best stage, i, in each round, we systematically work our way backward through the stages, never
revisiting previously set stages.

3 Performance Bounds
Using the bounds in [3] we can provide a risk bound for this problem. We let E denote the expecta?n denote the empirical average with respect to
tion with respect to the true distribution p(x, c) and E
the n training samples. We first bound the indicator function with a piece-wise linear function, b ? ,
with a maximum slope of ?1 :


z 
,0 .
I(z > 0) ? b? (z) = max min 1, 1 +
?
We then bound the loss: L(f (x), c) ? ? ? (f (x), c) where
?? (f (x), c) =
=

s+1

i=1
s+1


ci min{b? (gi (xi )), b? (?gi?1 (xi?1 )), b? (?gi?2 (xi?2 )), . . . , b? (?g1 (x1 ))}
ci B?i (gi (xi ), gi?1 (xi?1 ), . . . , g1 (x1 ))

i=1

We replaced the product of indicator functions with a minimization and then bounded each indicator
with b? . B?i is just a more compact presentation of the composition of the function b ? and the
minimization. We assume that the weights ? at each stage have been scaled to sum to 1. This has
no affect on the resulting classifications, but is necessary for the derivation below. Before stating the
theorem, for clarity, we state two standard definition:
Definition 1. Let p(x) be a probability distribution on the set X and let {x 1 , x2 , . . . , xn } be n
independent samples from p(x). Let ? 1 , ? 2 , . . . , ? n be n independent samples from a Rademacher
random variable (a binary variable that takes on either +1 or ?1 with equal probability). Let F be
a class of functions mapping X to .
Define the Rademacher Complexity of F to be




n

1  i
i 
Rn (F ) = E sup 
? f (x )

f ?F n  i=1

where the expectation is over the random draws of x 1 through xn and ? 1 through ? n .

Definition 2. Let p(x), {x1 , x2 , . . . , xn }, and F be as above. Let g 1 , g 2 , . . . , g n be n independent
samples from a Gaussian distribution with mean 0 and variance 1.
Analogous to the above definition, define the Gaussian Complexity of G to be
 n



1  i
i 
Gn (F ) = E sup 
g f (x ) .

f ?F n 
i=1

We can now state our theorem, bounding the true risk by a function of the empirical risk:
Theorem 3. Let H1 , H2 , . . . , Hs be the sequence of the sets of functions from which the base classifier draws for chain boosting. If H i is closed under negation for all i, all costs are bounded between
0 and 1, and the weights for the classifiers at each stage sum to 1, then with probability 1 ? ?,
	
s

8 ln 2?
k
?n [?? (f (x), c)] +
(i + 1)Gn (Hi ) +
E [L(f (x), c)] ? E
? i=1
n
for some constant k.
Proof. Theorem 8 of [3] states
?n (?? (f (x), c)) + 2Rn (?? ? F ) +
E [L(x, c)] ? E

	

8 ln 2?
n

and therefore we need only bound the R n (?? ? F ) term to demonstrate our theorem. For our case,
we have
 n


1  i
i
i 
? ?? (f (x ), c )
Rn (?? ? F ) = E sup 

f ?F n  i=1



 n
s+1

1  i  i s
i
i
i 
?
cj B? (gj (xj ), gj?1 (xj?1 ), . . . , g1 (x1 ))
= E sup 
n
f ?F

 i=1 j=1


s+1
n
s+1
 

1  i s

?
E sup 
? B? (gj (xij ), gj?1 (xij?1 ), . . . , g1 (xi1 )) =
Rn (B?s ? G j )

f ?F n 
j=1

i=1

j=1

where Gi is the space of convex combinations of functions from H i and G i is the cross product of
G1 through Gi . The inequality comes from switching the expectation and the maximization and then
from dropping the c ij (see [4], lemma 5).
Lemma 4 of [3] states that there exists a k such that R n (B?s ? G j ) ? kGn (B?s ? G j ). Theorem 14

of the same paper allows us to conclude that G n (B?s ? G j ) ? ?2 ji=1 Gn (Gi ). (Because B?s is the
minimum over a set of functions with maximum slope of 1? , the maximum slope of B ?s is also 1? .)
Theorem 12, part 2 states G n (Gi ) = Gn (Hi ). Taken together, this proves our result.
Note that this bound has only quadratic dependence on s, the length of the chain and does not
explicitly depend on the number of rounds of boosting (the number of rounds affects ? ? which, in
turn, affects the bound).

4 Application
We tested our algorithm on the MIT face database [5]. This database contains 19-by-19 gray-scale
images of faces and non-faces. The training set has 2429 face images and 4548 non-face images.
The testing set has 472 faces and 23573 non-faces. We weighted the training set images so that the
ratio of the weight of face images to non-face images matched the ratio in the testing set.

0.4

0.4
CB Global
CB Bottom?up
SVM
Boosting
False positive rate

0.3
0.25
0.2
0.15
0.1

0.3

150

0.2

100

0.1

50

Average number of pixels

0.35
average cost/error per example

200

training cost
training error
testing cost
testing error

0.05
0
100

200

300
400 500
number of rounds

700

1000

(a)

0
0

0.2

0.4
0.6
False negative rate

0.8

0
1

(b)

Figure 2: (a) Accuracy verses the number of rounds for a typical run, (b) Error rates and average
costs for a variety of cost settings.

4.1 Object Detection as Chained Boosting
Our goal is to produce a classifier that can identify non-face images at very low resolutions, thereby
allowing for quick processing of large images (as explained later). Most image patches (or subwindows) do not contain faces. We, therefore, built a multi-stage detection system where any early
rejection is labeled as a non-face. The first stage looks at image patches of size 3-by-3 (i.e. a lowerresolution version of the 19-by-19 original image). The next stage looks at the same image, but at
a resolution of 6-by-6. The third stage considers the image at 12-by-12. We did not present the full
19-by-19 images as the classification did not significantly improve over the 12-by-12 versions.
We employ a simple base classifier: the set of all functions that look at a single pixel and predict the
class by thresholding the pixel?s value. The total classifier at any stage is a linear combination of
these simple classifiers. For a given stage, all of the base classifiers that target a particular pixel are
added together producing a complex function of the value of the pixel. Yet, this pixel can only take
on a finite number of values (256 in this case). Therefore, we can compile this set of base classifiers
into a single look-up function that maps the brightness of the pixel into a real number. The total
classifier for the whole stage is merely the sum of these look-up functions. Therefore, the total work
necessary to compute the classification at a stage is proportional to the number of pixels in the image
considered at that stage, regardless of the number of base classifiers used.
We therefore assign a cost to each stage of processing proportional to the number of pixels at that
stage. If the image is a face, we add a negative cost (i.e. bonus) if the image is allowed to pass
through all of the processing stages (and is therefore ?accepted? as a face). If the image is a nonface, we add a bonus if the image is rejected at any stage before completion (i.e. correctly labelled).
While this dataset has only segmented image patches, in a real application, the classifier would be
run on all sub-windows of an image. More importantly, it would also be run at multiple resolutions
in order to detect faces of different sizes (or at different distances from the camera). The classifier
chain could be run simultaneously at each of these resolutions. To wit, while running the final 12-by12 stage at one resolution of the image, the 6-by-6 (previous) stage could be run at the same image
resolution. This 6-by-6 processing would be the necessary pre-processing step to running the 12-by12 stage at a higher resolution. As we run our final scan for big faces (at a low resolution), we can
already (at the same image resolution) be performing initial tests to throw out portions of the image
as not worthy of testing for smaller faces (at a higher resolution). Most of the work of detecting
objects must be done at the high resolutions because there are many more overlapping subwindows.
This chained method allows the culling of most of this high-resolution image processing.
4.2 Experiments
For each example, we construct a vector of stage costs as above. We add a constant to this vector to
ensure that the minimal element is zero, as per section 1.1. We scale all vectors by the same amount

to ensure that the maximal value is 1.This means that the number of misclassifications is an upper
bound on the total cost that the learning algorithm is trying to minimize.
There are three flexible quantities in this problem formulation: the cost of a pixel evaluation, the
bonus for a correct face classification, and the bonus for a correct non-face classification. Changing
these quantities will control the trade-off between false positives and true positives, and between
classification error and speed.
Figure 2(a) shows the result of a typical run of the algorithm. As a function of the number of
rounds, it plots the cost (that which the algorithm is trying to minimize) and the error (number of
misclassified image patches), for both the training and testing sets (where the training set has been
reweighted to have the same proportion of faces to non-faces as the testing set).
We compared our algorithm?s performance to the performance of support vector machines (SVM)
[6] and Adaboost [1] trained and tested on the highest resolution, 12-by-12, image patches. We
employed SVM-light [7] with a linear kernels. Figure 2(b) compares the error rates for the methods
(solid lines, read against the left vertical axis). Note that the error rates are almost identical for the
methods. The dashed lines (read against the right vertical axis) show the average number of pixels
evaluated (or total processing cost) for each of the methods. The SVM and Adaboost algorithms
have a constant processing cost. Our method (by either training scheme) produces lower processing
cost for most error rates.

5 Related Work
Cascade detectors for vision processing (see [8] or [9] for example) may appear to be similar to the
work in this paper. Especially at first glance for the area of object detection, they appear almost the
same. However, cascade detection and this work (chained detection) are quite different.
Cascade detectors are built one at a time. A coarse detector is first trained. The examples which
pass that detector are then passed to a finer detector for training, and so on. A series of targets for
false-positive rates define the increasing accuracy of the detector cascade.
By contrast, our chain detectors are trained as an ensemble. This is necessary because of two differences in the problem formulation. First, we assume that the information available at each stage
changes. Second, we assume there is an explicit cost model that dictates the cost of proceeding from
stage to stage and the cost of rejection (or acceptance) at any particular stage. By contrast, cascade
detectors are seeking to minimize computational power necessary for a fixed decision. Therefore,
the information available to all of the stages is the same, and there are no fixed costs associated with
each stage.
The ability to train all of the classifiers at the same time is crucial to good performance in our
framework. The first classifier in the chain cannot determine whether it is advantageous to send an
example further along unless it knows how the later stages will process the example. Conversely,
the later stages cannot construct optimal classifications until they know the distribution of examples
that they will see.
Section 4.1 may further confuse the matter. We demonstrated how chained boosting can be used to
reduce the computational costs of object detection in images. Cascade detectors are often used for
the same purpose. However, the reductions in computational time come from two different sources.
In cascade detectors, the time taken to evaluate a given image patch is reduced. In our chained
detector formulation, image patches are ignored completely based on analysis of lower resolution
patches in the image pyramid. To further illustrate the difference, cascade detectors can always
be used to speed up asymmetric classification tasks (and are often applied to image detection).
By contrast, in Section 4.1 we have exploited the fact that object detection in images is typically
performed at multiple scales to turn the problem into a pipeline and apply our framework.
Cascade detectors address situations in which prior class probabilities are not equal, while chained
detectors address situations in which information is gained at a cost. Both are valid (and separate)
ways of tackling image processing (and other tasks as well). In many ways, they are complementary
approaches.

Classic sequence analysis [10, 11] also addresses the problem of optimal stopping. However, it
assumes that the samples are drawn i.i.d. from (usually) a known distribution. Our problem is
quite different in that each consecutive sample is drawn from a different (and related) distribution
and our goal is to find a decision rule without producing a generative model. WaldBoost [12] is a
boosting algorithm based on this. It builds a series of features and a ratio comparison test in order
to decide when to stop. For WaldBoost, the available features (information) not change between
stages. Rather, any feature is available for selection at any point in the chain. Again, this is a
different problem than the one considered in this paper.

6 Conclusions
We feel this framework of staged decision making is useful in a wide variety of areas. This paper
demonstrated how the framework applies to one vision processing task. Obviously it also applies
to manufacturing pipelines where errors can be introduced at different stages. It should also be
applicable to scenarios where information gathering is costly.
Our current formulation only allows for early negative detection. In the face detection example
above, this means that in order to report ?face,? the classifier must process each stage, even if the
result is assured earlier. In Figure 2(b), clearly the upper-left corner (100% false positives and 0%
false negatives) is reachable with little effort: classify everything positive without looking at any
features. We would like to extend this framework to cover such two-sided early decisions. While
perhaps not useful in manufacturing (or even face detection, where the interesting part of the ROC
curve is far from the upper-left), it would make the framework more applicable to informationgathering applications.
Acknowledgements
This research was supported through the grant ?Adaptive Decision Making for Silicon Wafer Testing? from Intel Research and UC MICRO.

References
[1] Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning
and an application to boosting. In EuroCOLT, pages 23?37, 1995.
[2] Yoav Freund and Robert E. Schapire. Experiments with a new boosting algorithm. In ICML,
pages 148?156, 1996.
[3] Peter L. Bartlett and Shahar Mendelson. Rademacher and Gaussian complexities: Risk bounds
and structural results. JMLR, 2:463?482, 2002.
[4] Ron Meir and Tong Zhang. Generalization error bounds for Bayesian mixture algorithms.
JMLR, 4:839?860, 2003.
[5] MIT.
CBCL face database #1, 2000.
http://cbcl.mit.edu/cbcl/softwaredatasets/FaceData2.html.
[6] Bernhard E. Boser, Isabelle M. Guyon, and Vladimir N. Vapnik. A training algorithm for
optimal margin classifiers. In COLT, pages 144?152, 1992.
[7] T. Joachims. Making large-scale SVM learning practical. In B. Schlkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods ? Support Vector Learning. MIT-Press, 1999.
[8] Paul A. Viola and Michael J. Jones. Rapid object detection using a boosted cascade of simple
features. In CVPR, pages 511?518, 2001.
[9] Jianxin Wu, Matthew D. Mullin, and James M. Rehg. Linear asymmetric classifier for cascade
detectors. In ICML, pages 988?995, 2005.
[10] Abraham Wald. Sequential Analysis. Chapman & Hall, Ltd., 1947.
[11] K. S. Fu. Sequential Methods in Pattern Recognition and Machine Learning. Academic Press,
1968.
?
[12] Jan Sochman
and Ji?r?? Matas. Waldboost ? learning for time constrained sequential detection.
In CVPR, pages 150?156, 2005.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 2964-a-switched-gaussian-process-for-estimating-disparity-and-segmentation-in-binocular-stereo.pdf

A Switched Gaussian Process for Estimating
Disparity and Segmentation in Binocular Stereo
Oliver Williams
Microsoft Research Ltd.
Cambridge, UK
omcw2@cam.ac.uk

Abstract
This paper describes a Gaussian process framework for inferring pixel-wise
disparity and bi-layer segmentation of a scene given a stereo pair of images.
The Gaussian process covariance is parameterized by a foreground-backgroundocclusion segmentation label to model both smooth regions and discontinuities.
As such, we call our model a switched Gaussian process. We propose a greedy incremental algorithm for adding observations from the data and assigning segmentation labels. Two observation schedules are proposed: the first treats scanlines as
independent, the second uses an active learning criterion to select a sparse subset
of points to measure. We show that this probabilistic framework has comparable
performance to the state-of-the-art.

1

Introduction

Given two views of the same scene, this paper addresses the dual objectives of inferring depth
and segmentation in scenes with perceptually distinct foreground and background layers. We do
this in a probabilistic framework using a Gaussian process prior to model the geometry of typical
scenes of this type. Our approach has two properties of interest to practitioners: firstly, it can be
employed incrementally which is useful for circumstances in which the time allowed for processing
is constrained or variable; secondly it is probabilistic enabling fusion with other sources of scene
information.
Segmentation and depth estimation are well-studied areas (e.g., [1] and [2, 3, 4]). However the inspiration for the work in this paper is [5] in which both segmentation and depth are estimated in
a unified framework based around graph cuts. In [5] the target application was video conferencing, however such an algorithm is also applicable to areas such as robotics and augmented reality.
Gaussian process regression has previously been used in connection with stereo images in [6] to
learn the non-linear mapping between matched left-right image points and scene points as an alternative to photogrammetric camera calibration [7]. In this paper we use a Gaussian process to help
discover the initially unknown left-right matches in a complex scene: a camera calibration procedure
might then be used to determine actual 3D scene geometry.
The paper is organized as follows: Sec. 2 describes our Gaussian process framework for inferring
depth (disparity) and segmentation from stereo measurements. Sec. 3 proposes and demonstrates
two observation schedules: the first operates along image scanlines independently, the second treats
the whole image jointly, and makes a sparse set of stereo observations at locations selected by an
active learning criterion [8]; we also show how colour information may be fused with predictions by
the switched GP, the results of which are comparable to those of [5]. Sec. 4 concludes the paper.

Figure 1: Anatomy of a disparity map. This schematic shows some of the important features in
short baseline binocular stereo for an horizontal strip of pixels. Transitions between foreground and
background at the right edge of a foreground object will induce a discontinuity from high to low
disparity. Background?foreground transitions at the left edge of the foreground induce an occlusion
region in which scene points visible in the left image are not visible in the right. We use the data
from [5] which are available on their web site: http://research.microsoft.com/vision/cambridge/i2i

2

Single frame disparity estimation

This framework is intended for use with short baseline stereo, in which the two images are taken
slightly to the left and the right of a midpoint (see Fig. 1). This means that most features visible in
one image are visible in the other, albeit at a different location: for a given point x in the left image
L(x), our aim is therefore to infer the location of the same scene point in the right image R(x0 ). We
assume that both L and R have been recti?ed [7] such that all corresponding points have the same
vertical coordinate; hence if x = [x y]T then x0 = [x ? d(x) y]T where d(x) is called the disparity
map for points x in the left image.
Because objects typically have smooth variations in depth, d(x) is generally smooth. However, there
are two important exceptions to this and, because they occur at the boundaries between an object
and the background, it is essential that they be modelled correctly (see also Fig. 1):
Discontinuity Discontinuities occur where one pixel belongs to the foreground and its neighbour
belongs to the background.

Occlusion At background?foreground transitions (travelling horizontally from left to right), there
will be a region of pixels in the left image that are not visible in the right since they are occluded by the foreground [3]. Such locations correspond to scene points in the background
layer, however their disparity is unde?ned.
The next subsection describes a prior for disparity that attempts to capture these characteristics by
modelling the bi-layer segmentation.
2.1

A Gaussian process prior for disparity

We model the prior distribution of a disparity map to be a Gaussian process (GP) [9]. GPs are de?ned
by a mean function f (?) and a covariance function c(?, ?) which in turn de?ne the joint distribution
of disparities at a set of points {xi , . . . , xn } as a multivariate Gaussian

P d(xi ), . . . , d(xn )|f, c = Normal (f , C)
(1)
where fi = f (xi ) and Cij = c(xi , xj ).
In order to specify a mean and covariance function that give typical disparity maps an high probability, we introduce a latent segmentation variable s(x) ? {F, B, O} for each point in the left image.
This encodes whether a point belongs to the foreground (F), background (B) or is occluded (O) and
makes it possible to model the fact that disparities in the background/foreground are smooth (spatially correlated) within their layers and are independent across layers. For a given segmentation,

the covariance function is
?
2
? De??kxi ?xj k
c(xi , xj ; s) =
D?(xi ? xj )
?
0

s(xi ) = s(xj ) 6= O
s(xi ) = s(xj ) = O
s(xi ) 6= s(xj )

(2)

where D is the maximum disparity in the scene and ? is the Dirac delta function. The covariance of
two points will be zero (i.e., the disparities are independent) unless they share the same segmentation
label. Disparity is undefined within occlusion regions so these points are treated as independent
with high variance to capture the noisy observations that occur here, pixels with other labels have
disparities whose covariance falls off with distance engendering smoothness in the disparity map;
the parameter ? controls the smoothness and is set to ? = 0.01 for all of the experiments shown in
this paper (the points x are measured in pixel units). It will be convenient in what follows to define
0
the covariance for sets of points such that c(X , X 0 ; s) = C(s) ? Rn?n where the element Cij is the
th
th
0
covariance of the i element of X and j element of X . The prior mean is also defined according
to segmentation to reflect the fact that the foreground is at greater disparity (nearer the camera) than
the background
(
0.2D s(x) = B
0.8D s(x) = F .
f (x; s) =
(3)
0.5D s(x) = O
Because of the independence induced by the discrete labels s(x), we call this prior model a switched
Gaussian process. Switching between Gaussian processes for different parts of the input space
has been discussed previously by [10] in which switching was demonstrated for a 1D regression
problem.
2.2

Stereo measurement process

A proposed disparity d(x) is compared to the data via the normalized sum of squared differences
(NSSD) matching cost over a region ? (here
P a 5 ? 5 pixel patch centred? at the origin) using the
1
?
normalized intensity is L(x)
= L(x) ? 25
a?? L(x + a) (likewise for R(x))

P
? + a) ? R(x
? + a ? d) 2
L(x

m(x, d) = Pa?? ? 2
(4)
? 2 (x + a + d) .
2 a?? L (x + a) + R
This cost has been shown in practice to be effective for disparity estimation [11].
To incorporate this information with the GP prior it must be expressed probabilistically. We follow
the approach of [12] for this in which a parabola is fitted around the disparity with minimum score
m(x, d) ? ad2 + bd + c. Interpreting this as the inverse logarithm of a Gaussian distribution gives
d(x) = ?(x) +  where  ? Normal (0, v(x))
with ?(x) =

? ab

and v(x) =

1
2a

(5)

being the observation mean and variance.

Given a segmentation and a set of noisy measurements at locations X = {xi , . . . , xn }, the GP
can be used to predict the disparity at a new point P (d(x)|X ). This is a Gaussian distribution
Normal (?
?(x), v?(x)) with [9]
? ?1 c(X , x; s) and v?(x; s) = c(x, x; s) ? c(X , x; s)T C(s)
? ?1 c(x; s)
?
?(x; s) = ?T C(s)

?
where C(s)
= c(X , X ; s) + diag v(x1 ), . . . , v(xn ) and ? = [?(x1 ), . . . , ?(xn )]T .
2.3

(6)

Segmentation likelihood

The previous discussion has assumed that the segmentation is known, yet this will rarely be the case
in practice: s must therefore be inferred from the data together with the disparity. For a given set of
observations, the probability that they are a sample from the GP prior is given by


T


n
? ?1 ? ? f (s) ? log det C(s)
?
(7)
E(X ) = log P ?|s, v = ? ? ? f (s) C(s)
? log 2?.
2
This is the evidence for the parameters of the prior model and constitutes a data likelihood for the
segmentation. The next section describes an algorithm that uses this quantity to infer a segmentation
whilst incorporating observations.

3

Incremental incorporation of measurements and model selection

We propose an incremental and greedy algorithm for finding a segmentation. Measurements are
incorporated one at a time and the evidence of adding the ith observation to each of the three segmentation layers is computed based on the preceding i ? 1 observations and their labels. The ith
point is labelled according to which gave the greatest evidence. The first i ? 1 observation points
Xi?1 = {x1 , . . . , xi?1 } are partitioned according to their labelling into the mutually independent
sets XF , XB and XO . Since the three segmentation layers are independent, some of the cost of com? ?1 instead where
puting and storing the large matrix C? ?1 is avoided by constructing F? ?1 and B
?
?
F = c(XF , XF ) and B = c(XB , XB ). Observations assigned to the occlusion layer are independent
of all other points and contain no useful information. There is therefore no need to keep a covariance
matrix for these.
As shown in [13], the GP framework easily facilitates incremental incorporation of observations by
repeatedly updating the matrix inverse required in the prediction equations (6). For example, to add
the ith example to the foreground (the process is identical for the background layer) compute

?1  ?1

F?i?1 + qF qTF /rF qF
F?i?1
c(XF , xi )
?1
?
Fi =
=
(8)
c(XF , xi )T c(xi , xi ) + v(x)
qTF
rF
where
?1
rF?1 = c(xn , xn ) + v(xi ) ? c(XF , x)T F?i?1
c(XF , x)
?1
qF = ?rF F? c(XF , x).
i?1

(9)

Similarly, there is an incremental form for computing the evidence of a particular segmentation as
E(Xi |s(xi ) = j) = E(Xi?1 ) + ?Ej (xi ) where
(?(Xj )T qj )2
(10)
? 2?(xi )qTj ?(Xj ) ? rj ?(xi )2 ? 21 log 2?
rj
By computing ?Ej for the three possible segmentations, a new point can be greedily labelled as
that which gives the greatest increase in evidence.
?Ej (xi ) = log(rj ) ?

Algorithm 1 gives pseudo-code for the incremental incorporation of a measurement and greedy
labelling. As with Gaussian Process regression in general, this algorithm scales as O(n2 ) for storage
and O(n3 ) for time and it is therefore impractical to make an observation at every pixel for images
of useful size. We propose two mechanisms to overcome this:
1. Factorize the image into several sub-images and treat each one independently. The next
subsection demonstrates this when each scanline (row of pixels) is handled independently.
2. Only make measurements at a sparse subset of locations. Subsection 3.2 describes an active
learning approach for identifying optimally informative observation points.
3.1

Independent scanline observation schedule

By handling the image pixels one row at a time, the problem becomes one-dimensional. Points
are processed in order from right to left: for each point the disparity is measured as described in
Algorithm 1 Add and label measurement at xi
? ?1 , XF , XB and XO
input F? ?1 , B
Compute matrix building blocks rj?{F,B} and qj?{F,B} from (9)
Compute change in evidence for adding to each layer ?Ej?{F,B,O} from (10)
Label point s(xi ) = arg maxj?{F,B,O} ?Ej (xi )
Add point to set Xs(xi ) ? Xs(xi ) ? xi
if s(xi ) ? F ? B then
? ?1 as (8)
Update matrix F? ?1 or B
end if
i=i+1
? ?1 , XF , XB and XO
return F? ?1 , B

(a)

(b)

(c)

Figure 2: Scanline predictions. Disparity and segmentation maps inferred by treating each scanline
independently. (a) 320 ? 240 pixel left input images. (b) Mean predicted disparity map ?
?(x). (c)
Inferred segmentation s(x) with F = white, B = grey (orange) and O = black.
Sec. 2.2 and incorporated/labelled according to Algorithm 1. In this setting there are constraints on
which labels may be neighbours along a scanline. Fig. 1 shows the segmentation for a typical image
from which it can be seen that, moving horizontally from right to left, the only ?legal? transitions in
segmentation are B ? F, F ? O and O ? B. Algorithm 1 is therefore modified to consider legal
segmentations only.
Fig. 2 shows some results of this approach. Both the disparities and segmentation are, subjectively,
accurate however there are a number of ?streaky? artifacts caused by the fact that there is no vertical
sharing of information. There are also a number of artifacts where an incorrect segmentation label
has been assigned; in many cases this is where a point in the foreground or background has been
labelled as occluded because there is no texture in that part of an image and measurements made for
such points have an high variance. The occlusion class could therefore be more accurately described
as a general outlier category.
3.2

Active selection of sparse measurement locations

As shown above, our GP model scales badly with the number of observations. The previous subsection used measurements at all locations by treating each scanline as independent, however a
shortcoming of this approach is that no information is propagated vertically, introducing streaky
artifacts and reducing the model?s ability to reason about occlusions and discontinuities.
Rather than introduce artificial independencies, the observation schedule in this section copes with
the O(n3 ) scaling by making measurements at only a sparse set of locations. Obvious ways of
implementing this include choosing n locations either at random or in a grid pattern, however these
fail to exploit information that can be readily obtained from both the image data and the current
predictions made by the model. Hence, we propose an active approach, similar to that in [14]: given
the first i ? 1 observations, observe the point which maximally reduces the entropy of the GP [8]


?H(x) = H P (d|Xi?1 ) ? H P (d|Xi?1 ? x) = ? 12 log det ? + 21 log det ?0 + const. (11)
where ? and ?0 are the posterior covariances of the GP over all points in the image before and
after making an observation at x. To compute the entire posterior for each observation would be
prohibitively expense; instead we approximate it by the product of the marginal distributions at each

(a)

(b)

(c)

Figure 3: Predictions after sparse active observation schedule. This figure shows the predictions
made by the GP model with observations at 1000 image locations for the images used in Fig. 2. (a)
Mean predicted disparity ?
?(x); (b) Predictive uncertainty v?(x); (c) Inferred segmentation.

point (i.e., ignore off-diagonal elements in ?) which gives ?H(x) ? 12 log v?(x) ? log v(x) where
v?(x) is the predicted variance from (6) and v(x) is the measurement variance. Since the logarithm
is monotonic, an equivalent utility function is used:
 v?(x)
.
U x|Xi?1 =
v(x)

(12)

Here the numerator drives the system to make observations at points with greatest predictive uncertainty. However, this is balanced by the denominator to avoid making observations at points where
there is no information to be obtained from the data (e.g., the textureless regions in Fig. 2). To initialize the active algorithm, 64 initial observations are made in a evenly spaced grid over the image.
Following this, points are selected using the utility function (12) and incorporated into the GP model
using Algorithm 1.
Predicting disparity in the scanline factorization was straightforward because a segmentation label
had been assigned to every pixel. With sparse measurements, only the observation points have been
labelled and to predict disparity at an arbitrary location a segmentation label must also be inferred.
Our simple strategy for this is to label a point according to which gives the least predictive variance,
i.e.:
s(x) = arg

min
j?{F,B,O}

v?(x; s(x) = j).

(13)

Fig. 3 shows the results of using this active observation schedule with n = 1000 for the images of
Fig. 2. As expected, by restoring 2D spatial coherence the results are smoother and have none of the
streaky artifacts induced by the scanline factorization. Despite observing only 1.3% of the points
used by the scanline factorization, the active algorithm has still managed to capture the important
features in the scenes. Fig. 4a shows the locations of the n observation points; the observations are
clustered around the boundary of the foreground object in an attempt to minimize the uncertainty
at discontinuities/occlusions; the algorithm is dedicating its computational resources to the parts of
the image which are most interesting, important and informative. Fig. 4b demonstrates further the
benefits of selecting the points in an active framework compared to taking points at random.

10
active
random
% mislabelled points

8

6

4

2

0

0

(a)

500
1000
1500
number of observations n

2000

(b)

Figure 4: Advantage of active point selection. (a) The inferred segmentation from Fig. 3 with
spots (blue) corresponding to observation locations selected by the active criterion. (b) This plot
compares the accuracy of the segmentation against the number of sparse observations when the
observation locations are chosen at random and using our active schedule. Accuracy is measured
as the percentage of mislabelled pixels compared to a hand-labelled ground truth segmentation. The
active strategy achieves better accuracy with fewer observations.

(a)

(b)

(c)

Figure 5: Improved segmentation by fusion with colour. (a) Pixel-wise energy term V (x) combining segmentation predictions from both the switched GP posterior and a colour model; (b) Segmentation returned by the Viterbi algorithm. This contains 0.5% labelling errors by area. (c) Inferred
foreground image pixels.
3.3

Adding colour information

The best segmentation accuracies using stereo information alone are around 1% labelling errors
(with n ? 1000). In [5], superior segmentation results are achieved by incorporating colour information. We do the same here by computing a foreground ?energy? V (x) at each location based
on the variances
predicted by the foreground/background layers and a known colour distribution

P F|Lc (x) where Lc (x) is the RGB colour of the left image at x:

V (x) = log v?(x; s(x) = B) ? log v?(x; s(x) = F) ? log P F|Lc (x) .
(14)
We represent the colour distribution using a 10 ? 10 ? 10 bin histogram in red-green-blue colour
space. Fig. 5a shows this energy for the first image in Fig. 2. As in [5], we treat each scanline as
a binary HMM and use the Viterbi algorithm to find a segmentation. A result of this is shown in
Fig. 5c which contains 0.58% erroneous labels. This is comparable to the errors in [5] which are
around 0.25% for this image. We suspect that our result can be improved with a more sophisticated
colour model.

4

Discussion

We have proposed a Gaussian process model for disparity, switched by a latent segmentation variable. We call this a switched Gaussian process and have proposed an incremental greedy algorithm

for fitting this model to data and inferring a segmentation. We have demonstrated that by using
a sparse model with points selected according to an active learning criterion, an accuracy can be
achieved that is comparable to the state of the art [5].
We believe there are four key strengths to this probabilistic framework:
Flexibility The incremental nature of the algorithm makes it possible to set the number of observations n according to time or quality constraints.
Extensibility This method is probabilistic so fusion with other sources of information is possible
(e.g., laser range scanner on a robot).
Ef?ciency For small n, this approach is very fast ( 30ms per pair of images for n = 200 on a 3GHz
PC). However, higher quality results require n > 1000 observations which reduces the
execution speed to a few seconds per image.
Accuracy We have shown that (for large n) this technique achieves an accuracy comparable to the
state of the art.
Future work will investigate the use of approximate techniques to overcome the O(n3 ) scaling
problem [15]. The framework described in this paper can operate at real time for low n, however any
technique that combats the scaling will allow higher accuracy for the same execution time. Also,
improving the approximation to the likelihood in (5), e.g., by expectation propagation [16], may
increase accuracy.

References
[1] D. Comaniciu and P. Meer. Robust analysis of feature spaces: color image sgementation. In
Proc. Conf. Computer Vision and Pattern Recognition, pages 750?755, 1997.
[2] Y. Ohta and T. Kanade. Stereo by intra- and inter-scanline search using dynamic programming.
IEEE Trans. on Pattern Analysis and Machine Intelligence, 7(2):139?154, 1985.
[3] D. Geiger, B. Ladendorf, and A. Yuille. Occlusions and binocular stereo. Int. J. Computer
Vision, 14:211?226, 1995.
[4] V. Kolmogorov and R. Zabih. Computing visual correspondence with occlusions using graph
cuts. In Proc. Int. Conf. Computer Vision, 2001.
[5] V. Kolmogorov, A. Criminisi, A. Blake, G. Cross, and C. Rother. Bi-layer segmentation of
binocular stereo video. In Proc. Conf. Computer Vision and Pattern Recognition, 2005.
[6] F. Sinz, J. Qui?nonero-Candela, G.H. Bakir, C.E. Rasmussen, and M.O. Franz. Learning depth
from stereo. In Pattern Recognition, Proc. 26th DAGM Symposium, pages 245?252, 2004.
[7] R. Hartley and A. Zisserman. Multiple View Geometry. Cambridge University Press, 2000.
[8] D.J.C. MacKay. Information-based objective functions for active data selection. Neural Computation, 4(4):589?603, 1992.
[9] C.E. Rasmussen and C.K.I. Williams. Gaussian Processes for Machine Learning. MIT Press,
2006.
[10] A. Storkey. Gaussian processes for switching regimes. In Proc. ICANN, 1998.
[11] D. Scharstein and R. Szeliski. A taxonomy and evaluation of desnse two-frame stereo correspondence algorithms. Int. J. Computer Vision, 47(1):7?42, 2002.
[12] L. Matthies, R. Szeliski, and T. Kanade. Incremental estimation of dense depth maps from
image sequences. In Proc. Conf. Computer Vision and Pattern Recognition, 1988.
[13] M. Gibbs and D.J.C. MacKay. Efficient implementation of gaussian processes. Technical
report, University of Cambridge, 1997.
[14] M. Seeger, C.K.I. Williams, and N. Lawrence. Fast forward selection to speed up sparse
gaussian process regression. In Proc. AI-STATS, 2003.
[15] J. Qui?nonero-Candela and C.E. Rasmussen. A unifying view of sparse approximate Gaussian
process regression. J. Machine Learning Research, 6:1939?1959, 2005.
[16] T.P. Minka. Expectation propagation for approximate Bayesian inference. In Proc. UAI, pages
362?369, 2001.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 2689-modeling-nonlinear-dependencies-in-natural-images-using-mixture-of-laplacian-distribution.pdf

Modeling Nonlinear Dependencies in
Natural Images using Mixture of
Laplacian Distribution
Hyun Jin Park and Te Won Lee
Institute for Neural Computation, UCSD
9500 Gilman Drive, La Jolla, CA 92093-0523
{hjinpark, tewon}@ucsd.edu

Abstract
Capturing dependencies in images in an unsupervised manner is
important for many image processing applications. We propose a
new method for capturing nonlinear dependencies in images of
natural scenes. This method is an extension of the linear Independent
Component Analysis (ICA) method by building a hierarchical model
based on ICA and mixture of Laplacian distribution. The model
parameters are learned via an EM algorithm and it can accurately
capture variance correlation and other high order structures in a
simple manner. We visualize the learned variance structure and
demonstrate applications to image segmentation and denoising.

1

In trod u ction

Unsupervised learning has become an important tool for understanding biological
information processing and building intelligent signal processing methods. Real
biological systems however are much more robust and flexible than current artificial
intelligence mostly due to a much more efficient representations used in biological
systems. Therefore, unsupervised learning algorithms that capture more sophisticated
representations can provide a better understanding of neural information processing
and also provide improved algorithm for signal processing applications. For example,
independent component analysis (ICA) can learn representations similar to simple cell
receptive fields in visual cortex [1] and is also applied for feature extraction, image
segmentation and denoising [2,3]. ICA can approximate statistics of natural image
patches by Eq.(1,2), where X is the data and u is a source signal whose distribution is
a product of sparse distributions like a generalized Laplacian distribution.

X = Au

(1)

P (u ) = ? P (u i )

(2)

But the representation learned by the ICA algorithm is relatively low-level. In
biological systems there are more high-level representations such as contours,
textures and objects, which are not well represented by the linear ICA model. ICA
learns only linear dependency between pixels by finding strongly correlated linear

axis. Therefore, the modeling capability of ICA is quite limited. Previous approaches
showed that one can learn more sophisticated high-level representations by capturing
nonlinear dependencies in a post-processing step after the ICA step [4,5,6,7,8].
The focus of these efforts has centered on variance correlation in natural images. After
ICA, a source signal is not linearly predictable from others. However, given variance
dependencies, a source signal is still ?predictable? in a nonlinear manner. It is not
possible to de-correlate this variance dependency using a linear transformation.
Several researchers have proposed extensions to capture the nonlinear dependencies.
Portilla et al. used Gaussian Scale Mixture (GSM) to model variance dependency in
wavelet domain. This model can learn variance correlation in source prior and showed
improvement in image denoising [4]. But in this model, dependency is defined only
between a subset of wavelet coefficients. Hyvarinen and Hoyer suggested using a
special variance related distribution to model the variance correlated source prior.
This model can learn grouping of dependent sources (Subspace ICA) or topographic
arrangements of correlated sources (Topographic ICA) [5,6]. Similarly, Welling et al.
suggested a product of expert model where each expert represents a variance
correlated group [7]. The product form of the model enables applications to image
denoising. But these models don?t reveal higher-order structures explicitly.
Our model is motivated by Lewicki and Karklin who proposed a 2-stage model where
the 1st stage is an ICA model (Eq. (3)) and the 2 nd-stage is a linear generative model
where another source v generates logarithmic variance for the 1st stage (Eq. (4)) [8].
This model captures variance dependency structure explicitly, but treating variance as
an additional random variable introduces another level of complexity and requires
several approximations. Thus, it is difficult to obtain a simple analytic PDF of source
signal u and to apply the model for image processing problems.

(

P (u | ? ) = c exp ? u / ?

q

)

(3)

log[? ] = Bv

(4)

We propose a hierarchical model based on ICA and a mixture of Laplacian
distribution. Our model can be considered as a simplification of model in [8] by
constraining v to be 0/1 random vector where only one element can be 1. Our model is
computationally simpler but still can capture variance dependency. Experiments show
that our model can reveal higher order structures similar to [8]. In addition, our model
provides a simple parametric PDF of variance correlated priors, which is an important
advantage for adaptive signal processing. Utilizing this, we demonstrate simple
applications on image segmentation and image denoising. Our model provides an
improved statistic model for natural images and can be used for other applications
including feature extraction, image coding, or learning even higher order structures.

2

Modeling nonlinear dependencies

We propose a hierarchical or 2-stage model where the 1 st stage is an ICA source signal
model and the 2nd stage is modeled by a mixture model with different variances (figure
1). In natural images, the correlation of variance reflects different types of regularities
in the real world. Such specialized regularities can be summarized as ?context?
information. To model the context dependent variance correlation, we use mixture
models where Laplacian distributions with different variance represent different
contexts. For each image patch, a context variable Z ?selects? which Laplacian
distribution will represent ICA source signal u. Laplacian distributions have 0-mean

but different variances. The advantage of Laplacian distribution for modeling context
is that we can model a sparse distribution using only one Laplacian distribution. But
we need more than two Gaussian distributions to do the same thing. Also conventional
ICA is a special case of our model with one Laplacian. We define the mixture model
and its learning algorithm in the next sections.

Figure 1: Proposed hierarchical model (1st stage is ICA generative model. 2nd stage is
mixture of ?context dependent? Laplacian distributions which model U. Z is a random
variable that selects a Laplacian distribution that generates the given image patch)
2.1

Mixture of Laplacian Distribution

We define a PDF for mixture of M-dimensional Laplacian Distribution as Eq.(5),
where N is the number of data samples, and K is the number of mixtures.
N
N K
M
N K
r
r r
P(U | ?, ?) = ? P(u n | ?, ?) = ?? ? k P(u n | ?k ) = ?? ? k ?
n

n

k

n

k

m

1

(2? )
k ,m

? u n,m
exp? ?
? ?k , m
?

?
?
?
?

(5)

r
r r
r r
un = (un,1 , un , 2 , , , un,M ) : n-th data sample, U = (u1 , u 2 , , , ui , , , u N )
r
r r
r
r
?k = (?k ,1 , ?k , 2 ,..., ?k ,M ) : Variance of k-th Laplacian distribution, ? = (?1 , ?2 , , , ?k , , , ?K )

?k

: probability of Laplacian distribution k, ? = (? 1 , , , ? K ) and

?

k

?k =1

It is not easy to maximize Eq.(5) directly, and we use EM (expectation maximization)
algorithm for parameter estimation. Here we introduce a new hidden context variable
Z that represents which Laplacian k, is responsible for a given data point. Assuming
we know the hidden variable Z, we can write the likelihood of data and Z as Eq.(6),
zkn
?K ?
??
N
?
u n ,m
r
? ? 1 ??
z kn
?
?
P(U , Z | ?, ? ) = ? P(u n , Z | ?, ? ) = ? ? (? k ) ? ? ?
? exp? ? z kn
?
?
?
?
?k , m
n
n
k
m ? ? 2?k ,m ?
?
?? ??
?
N

? ??? ??
? ?
? ??? ?
? ??? ??

(6)

z kn : Hidden binary random variable, 1 if n-th data sample is generated from k-th
Laplacian, 0 other wise. ( Z = (z kn ) and ? z kn = 1 for all n = 1?N)
k

2.2

EM algorithm for learning the mixture model

The EM algorithm maximizes the log likelihood of data averaged over hidden variable
Z. The log likelihood and its expectation can be computed as in Eq.(7,8).

?
?
u
1
log P(U , Z | ?, ? ) = ? ? z kn log(? k ) + ? z kn ? log(
) ? n ,m
?
2
?
?
n ,k ?
m
k ,m
k ,m
?
?

??
??
??
??

(7)

?
?
u
1
E {log P (U , Z | ?, ? )} = ? E z kn ?log(? k ) + ? ? log(
) ? n ,m
?
2? k , m
?k , m
n ,k
m ?
??

{ }

??
??
??
??

(8)

The expectation in Eq.(8) can be evaluated, if we are given the data U and estimated
parameters ? and ?. For ? and ?, EM algorithm uses current estimation ?? and ??.

{ } {

} ? z P( z

E z kn ? E z kn | U , ? ' , ? ' =

1

zkn =0

n
k

n
k

| u n , ?' , ? ' ) = P ( z kn = 1 | u n , ? ' , ? ' )

(9)

=

P (u n | z kn = 1, ?' , ? ' ) P( z kn = 1 | ? ' , ? ' )
P(u n | ?' , ? ' )

=

M
u n ,m
1
1
1
exp(?
) ?? k ' =
?
?k , m '
P (u n | ? ' , ? ' ) m 2?k ,m '
cn

M

?k '

? 2?
m

k ,m

'

exp(?

u n ,m

?k , m '

)

Where the normalization constant can be computed as
K

K

M

k

k =1

m =1

cn = P (u n | ? ' , ? ' ) = ? P (u n | z kn , ? ' , ? ' ) P ( z kn | ? ' , ? ' ) = ? ? k ?

1

(2? )

exp( ?

k ,m

u n ,m

?k ,m

)

(10)

The EM algorithm works by maximizing Eq.(8), given the expectation computed from
Eq.(9,10). Eq.(9,10) can be computed using ?? and ?? estimated in the previous
iteration of EM algorithm. This is E-step of EM algorithm. Then in M-step of EM
algorithm, we need to maximize Eq.(8) over parameter ? and ?.
First, we can maximize Eq.(8) with respect to ?, by setting the derivative as 0.
?? 1
u n,m ??
?E{log P (U , Z | ?, ? )}
?? = 0
= ? E z kn ?? ?
+
?
?? k ,m
?
(? k ,m ) 2 ???
???
n
k ,m
?

{ }

? ? k ,m

? E{z }? u
=
? E{z }
n
k

n ,m

n

(11)

n
k

n

Second, for maximization of Eq.(8) with respect to ?, we can rewrite Eq.(8) as below.
(12)
E {log P (U , Z | ? , ? )} = C + ? E {z kn' }log(? k ' )
n ,k '

As we see, the derivative of Eq.(12) with respect to ? cannot be 0. Instead, we need to
use Lagrange multiplier method for maximization. A Lagrange function can be
defined as Eq.(14) where ? is a Lagrange multiplier.

{ }

(13)

L (? , ? ) = ? ? E z kn' log(? k ' ) + ? (? ? k ' ? 1)
n,k '

k'

By setting the derivative of Eq.(13) to be 0 with respect to ? and ?, we can simply get
the maximization solution with respect to ?. We just show the solution in Eq.(14).
?L(?, ? )
?L(?, ? )
=0
= 0,
??
??

?
? ?
?
? ? k = ? ? E z kn ? / ? ?? E z kn ?
?
? ? k n
? n

{ }

{ }

(14)

Then the EM algorithm can be summarized as figure 2. For the convergence criteria,
we can use the expectation of log likelihood, which can be calculated from Eq. (8).

?k =

{ }

, ?k , m = E um + e (e is small random noise)
2. Calculate the Expectation by
1. Initialize

1
K

u n ,m
1 M ?k '
exp( ?
)
?
c n m 2? k , m '
? k ,m '
3. Maximize the log likelihood given the Expectation

{ } {

}

E z kn ? E z kn | U , ? ' , ? ' =
?

? ?

?

?k ,m ? ? ? E {z kn }? u n,m ? / ? ? E {z kn }? ,

?
? ?
?
? k ? ? ? E {z kn }? / ? ?? E {z kn }?
? ? n
?
? n
? ? k n
?
4. If (converged) stop, otherwise repeat from step 2.

?

n

Figure 2: Outline of EM algorithm for Learning the Mixture Model

3

Experimental Results

Here we provide examples of image data and show how the learning procedure is
performed for the mixture model. We also provide visualization of learned variances
that reveal the structure of variance correlation and an application to image denoising.
3.1

Learning Nonlinear Dependencies in Natural images

As shown in figure 1, the 1 st stage of the proposed model is simply the linear ICA. The
ICA matrix A and W(=A-1) are learned by the FastICA algorithm [9]. We sampled
105(=N) data from 16x16 patches (256 dim.) of natural images and use them for both
first and second stage learning. ICA input dimension is 256, and source dimension is
set to be 160(=M). The learned ICA basis is partially shown in figure 1. The 2nd stage
mixture model is learned given the ICA source signals. In the 2 nd stage the number of
mixtures is set to 16, 64, or 256(=K). Training by the EM algorithm is fast and several
hundred iterations are sufficient for convergence (0.5 hour on a 1.7GHz Pentium PC).
For the visualization of learned variance, we adapted the visualization method from
[8]. Each dimension of ICA source signal corresponds to an ICA basis (columns of A)
and each ICA basis is localized in both image and frequency space. Then for each
Laplacian distribution, we can display its variance vector as a set of points in image
and frequency space. Each point can be color coded by variance value as figure 3.

(a1)

(a2)

(b1)

(b2)

Figure 3: Visualization of learned variances (a1 and a2 visualize variance of
Laplacian #4 and b1 and 2 show that of Laplacian #5. High variance value is mapped
to red color and low variance is mapped to blue. In Laplacian #4, variances for
diagonally oriented edges are high. But in Laplacian #5, variances for edges at
spatially right position are high. Variance structures are related to ?contexts? in the
image. For example, Laplacian #4 explains image patches that have oriented textures
or edges. Laplacian #5 captures patches where left side of the patch is clean but right
side is filled with randomly oriented edges.)

A key idea of our model is that we can mix up independent distributions to get nonlinearly dependent distribution. This modeling power can be shown by figure 4.

Figure 4: Joint distribution of nonlinearly dependent sources. ((a) is a joint histogram
of 2 ICA sources, (b) is computed from learned mixture model, and (c) is from learned
Laplacian model. In (a), variance of u2 is smaller than u1 at center area (arrow A), but
almost equal to u1 at outside (arrow B). So the variance of u2 is dependent on u1. This
nonlinear dependency is closely approximated by mixture model in (b), but not in (c).)
3.2

Unsupervised Image Segmentation

The idea behind our model is that the image can be modeled as mixture of different
variance correlated ?contexts?. We show how the learned model can be used to
classify different context by an unsupervised image segmentation task. Given learned
model and data, we can compute the expectation of a hidden variable Z from Eq. (9).
Then for an image patch, we can select a Laplacian distribution with highest
probability, which is the most explaining Laplacian or ?context?. For segmentation,
we use the model with 16 Laplacians. This enables abstract partitioning of images and
we can visualize organization of images more clearly (figure 5).

Figure 5: Unsupervised image segmentation (left is original image, middle is color
labeled image, right image shows color coded Laplacians with variance structure.
Each color corresponds to a Laplacian distribution, which represents surface or
textural organization of underlying contexts. Laplacian #14 captures smooth surface
and Laplacian #9 captures contrast between clear sky and textured ground scenes.)
3.3

Application to Image Restoration

The proposed mixture model provides a better parametric model of the ICA source
distribution and hence an improved model of the image structure. An advantage is in
the MAP (maximum a posterior) estimation of a noisy image. If we assume Gaussian
noise n, the image generation model can be written as Eq.(15). Then, we can compute
MAP estimation of ICA source signal u by Eq.(16) and reconstruct the original image.

(15)

X = Au + n

(16)

u? = argmax log P (u | X , A) = argmax (log P ( X | u , A) + log P (u ) )
u

u

Since we assumed Gaussian noise, P(X|u,A) in Eq. (16) is Gaussian. P(u) in Eq. (16)
can be modeled as a Laplacian or a mixture of Laplacian distribution. The mixture
distribution can be approximated by a maximum explaining Laplacian. We evaluated
3 different methods for image restoration including ICA MAP estimation with simple
Laplacian prior, same with Laplacian mixture prior, and the Wiener filter. Figure 6
shows an example and figure 7 summarizes the results obtained with different noise
levels. As shown MAP estimation with the mixture prior performs better than the
others in terms of SNR and SSIM (Structural Similarity Measure) [10].

Figure 6: Image restoration results (signal variance 1.0, noise variance 0.81)
16
ICA MAP (Mixture prior)
ICA MAP (Laplacian prior)
W iener

14

0.8
SSIM Index

SNR

12
10
8
6

0.6
0.4
0.2

4
2

ICA MAP(Mixture prior)
ICA MAP(Laplacian prior)
W iener
Noisy Image

1

0

0.5

1
1.5
Noise variance

2

2.5

0

0

0.5

1
1.5
Noise variance

2

2.5

Figure 7: SNR and SSIM for 3 different algorithms (signal variance = 1.0)

4

D i s c u s s i on

We proposed a mixture model to learn nonlinear dependencies of ICA source signals
for natural images. The proposed mixture of Laplacian distribution model is a
generalization of the conventional independent source priors and can model variance
dependency given natural image signals. Experiments show that the proposed model
can learn the variance correlated signals grouped as different mixtures and learn highlevel structures, which are highly correlated with the underlying physical properties

captured in the image. Our model provides an analytic prior of nearly independent and
variance-correlated signals, which was not viable in previous models [4,5,6,7,8].
The learned variances of the mixture model show structured localization in image and
frequency space, which are similar to the result in [8]. Since the model is given no
information about the spatial location or frequency of the source signals, we can
assume that the dependency captured by the mixture model reveals regularity in the
natural images. As shown in image labeling experiments, such regularities correspond
to specific surface types (textures) or boundaries between surfaces.
The learned mixture model can be used to discover hidden contexts that generated
such regularity or correlated signal groups. Experiments also show that the labeling of
image patches is highly correlated with the object surface types shown in the image.
The segmentation results show regularity across image space and strong correlation
with high-level concepts.
Finally, we showed applications of the model for image restoration. We compare the
performance with the conventional ICA MAP estimation and Wiener filter. Our
results suggest that the proposed model outperforms other traditional methods. It is
due to the estimation of the correlated variance structure, which provides an improved
prior that has not been considered in other methods.
In our future work, we plan to exploit the regularity of the image segmentation result
to lean more high-level structures by building additional hierarchies on the current
model. Furthermore, the application to image coding seems promising.
References
[1] A. J. Bell and T. J. Sejnowski, The ?Independent Components? of Natural Scenes are Edge
Filters, Vision Research, 37(23):3327?3338, 1997.
[2] A. Hyvarinen, Sparse Code Shrinkage: Denoising of Nongaussian Data by Maximum
Likelihood Estimation,Neural Computation, 11(7):1739-1768, 1999.
[3] T. Lee, M. Lewicki, and T. Sejnowski., ICA Mixture Models for unsupervised
Classification of non-gaussian classes and automatic context switching in blind separation.
PAMI, 22(10), October 2000.
[4] J. Portilla, V. Strela, M. J. Wainwright and E. P Simoncelli, Image Denoising using Scale
Mixtures of Gaussians in the Wavelet Domain, IEEE Trans. On Image Processing, Vol.12, No.
11, 1338-1351, 2003.
[5] A. Hyvarinen, P. O. Hoyer. Emergence of phase and shift invariant features by
decomposition of natural images into independent feature subspaces. Neurocomputing, 1999.
[6] A. Hyvarinen, P.O. Hoyer, Topographic Independent component analysis as a model of V1
Receptive Fields, Neurocomputing, Vol. 38-40, June 2001.
[7] M. Welling and G. E. Hinton, S. Osindero, Learning Sparse Topographic Representations
with Products of Student-t Distributions, NIPS, 2002.
[8] M. S. Lewicki and Y. Karklin, Learning higher-order structures in natural images, Network:
Comput. Neural Syst. 14 (August 2003) 483-499.
[9] A.Hyvarinen, P.O. Hoyer, Fast ICA matlab code.,
http://www.cis.hut.fi/projects/compneuro/extensions.html/
[10] Z. Wang, A. C. Bovik, H. R. Sheikh and E. P. Simoncelli, The SSIM Index for Image
Quality Assessment, IEEE Transactions on Image Processing, vol. 13, no. 4, Apr. 2004.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 3016-dynamic-foregroundbackground-extraction-from-images-and-videos-using-random-patches.pdf

Dynamic Foreground/Background Extraction from
Images and Videos using Random Patches

Le Lu?
Integrated Data Systems Department
Siemens Corporate Research
Princeton, NJ 08540
le-lu@siemens.com

Gregory Hager
Department of Computer Science
Johns Hopkins University
Baltimore, MD 21218
hager@cs.jhu.edu

Abstract
In this paper, we propose a novel exemplar-based approach to extract dynamic
foreground regions from a changing background within a collection of images
or a video sequence. By using image segmentation as a pre-processing step, we
convert this traditional pixel-wise labeling problem into a lower-dimensional supervised, binary labeling procedure on image segments. Our approach consists of
three steps. First, a set of random image patches are spatially and adaptively sampled within each segment. Second, these sets of extracted samples are formed into
two ?bags of patches? to model the foreground/background appearance, respectively. We perform a novel bidirectional consistency check between new patches
from incoming frames and current ?bags of patches? to reject outliers, control
model rigidity and make the model adaptive to new observations. Within each
bag, image patches are further partitioned and resampled to create an evolving
appearance model. Finally, the foreground/background decision over segments
in an image is formulated using an aggregation function defined on the similarity measurements of sampled patches relative to the foreground and background
models. The essence of the algorithm is conceptually simple and can be easily implemented within a few hundred lines of Matlab code. We evaluate and validate
the proposed approach by extensive real examples of the object-level image mapping and tracking within a variety of challenging environments. We also show that
it is straightforward to apply our problem formulation on non-rigid object tracking
with difficult surveillance videos.

1 Introduction
In this paper, we study the problem of object-level figure/ground segmentation in images and video
sequences. The core problem can be defined as follows: Given an image X with known figure/ground
labels L, infer the figure/ground labels L? of a new image X? closely related to X. For example, we
may want to extract a walking person in an image using the figure/ground mask of the same person
in another image of the same sequence. Our approach is based on training a classifier from the
appearance of a pixel and its surrounding context (i.e., an image patch centered at the pixel) to
recognize other similar pixels across images. To apply this process to a video sequence, we also
evolve the appearance model over time.
A key element of our approach is the use of a prior segmentation to reduce the complexity of the
segmentation process. As argued in [22], image segments are a more natural primitive for image
modeling than pixels. More specifically, an image segmentation provides a natural dimensional
reduction from the spatial resolution of the image to a much smaller set of spatially compact and
relatively homogeneous regions. We can then focus on representing the appearance characteristics
?

The work has been done while the first author was a graduate student in Johns Hopkins University.

of these regions. Borrowing a term from [22], we can think of each region as a ?superpixel? which
represents a complex connected spatial region of the image using a rich set of derived image features. We can then consider how to classify each superpixel (i.e. image segment) as foreground or
background, and then project this classification back into the original image to create the pixel-level
foreground-background segmentation we are interested in.
The original superpixel representation in [22, 19, 18] is a feature vector created from the image segment?s color histogram [19], filter bank responses [22], oriented energy [18] and contourness [18].
These features are effective for image segmentation [18], or finding perceptually important boundaries from segmentation by supervised training [22]. However, as shown in [17], those parameters
do not work well for matching different classes of image regions from different images. Instead,
we propose using a set of spatially randomly sampled image patches as a non-parametric, statistical
superpixel representation. This non-parametric ?bag of patches? model1 can be easily and robustly
evolved with the spatial-temporal appearance information from video, while maintaining the model
size (the number of image patches per bag) using adaptive sampling. Foreground/background classification is then posed as the problem of matching sets of random patches from the image with
these models. Our major contributions are demonstrating the effectiveness and computational simplicity of a nonparametric random patch representation for semantically labelling superpixels and
a novel bidirectional consistency check and resampling strategy for robust foreground/background
appearance adaptation over time.

(a)

(b)

(c)

(d)

Figure 1: (a) An example indoor image, (b) the segmentation result using [6] coded in random colors, (c)
the boundary pixels between segments shown in red, the image segments associated with the foreground, a
walking person here, shown in blue, (d) the associated foreground/background mask. Notice that the color
in (a) is not very saturated. This is a common fact in our indoor experiments without any specific lighting
controls.

We organize the paper as follows. We first address several image patch based representations and
the associated matching methods are described. In section 3, the algorithm used in our approach is
presented with details. We demonstrate the validity of the proposed approach using experiments on
real examples of the object-level figure/ground image mapping and non-rigid object tracking under
dynamic conditions from videos of different resolutions in section 4. Finally, we summarize the
contributions of the paper and discuss possible extensions and improvements.

2 Image Patch Representation and Matching
Building stable appearance representations of images patches is fundamental to our approach. There
are many derived features that can be used to represent the appearance of an image patch. In this
paper, we evaluate our algorithm based on: 1) an image patch?s raw RGB intensity vector, 2) mean
color vector, 3) color + texture descriptor (filter bank response or Haralick feature [17]), and 4) PCA,
LDA and NDA (Nonparametric Discriminant Analysis) features [7, 3] on the raw RGB vectors. For
completeness, we give a brief description of each of these techniques.
Texture descriptors: To compute texture descriptions, we first apply the Leung-Malik (LM) filter
bank [13] which consists of 48 isotropic and anisotropic filters with 6 directions, 3 scales and 2
phases. Thus each image patch is represented by a 48 component feature vector. The Haralick
texture descriptor [10] was used for image classification in [17]. Haralick features are derived from
the Gray Level Co-occurrence Matrix, which is a tabulation of how often different combinations
of pixel brightness values (grey levels) occur in an image region. We selected 5 out of 14 texture
1
Highly distinctive local features [16] are not the adequate substitutes for image patches. Their spatial
sparseness nature limits their representativity within each individual image segment, especially for the nonrigid,
nonstructural and flexible foreground/background appearance.

descriptors [10] including dissimilarity, Angular Second Moment (ASM), mean, standard deviation
(STD) and correlation. For details, refer to [10, 17].
Dimension reduction representations: The Principal Component Analysis (PCA) algorithm is
used to reduce the dimensionality of the raw color intensity vectors of image patches. PCA makes
no prior assumptions about the labels of data. However, recall that we construct the ?bag of patches?
appearance model from sets of labelled image patches. This supervised information can be used to
project the bags of patches into a subspace where they are best separated using Linear discriminant Analysis (LDA) or Nonparametric Discriminant Analysis (NDA) algorithm [7, 3] by assuming
Gaussian or Non-Gaussian class-specific distributions.
Patch matching: After image patches are represented using one of the above methods, we must
match them against the foreground/background models. There are 2 methods investigated in this
paper: the nearest neighbor matching using Euclidean distance and KDE (Kernel Density Estimation) [12] in PCA/NDA subspaces. For nearest-neighbor matching, we find, for each patch p, its
B
F
F
nearest neighbors pF
n , pn in foreground/background bags, and then compute dp =k p ? pn k,
B
B
F
B
dp =k p ? pn k. On the other hand, an image patch?s matching scores mp and mp are evaluated
as probability density values from the KDE functions KDE(p, ?F ) and KDE(p, ?B ) where ?F |B
are bags of patch models. Then the segmentation-level classification is performed as section 3.2.

3 Algorithms
We briefly summarize our labeling algorithm as follows. We assume that each image of interest
has been segmented into spatial regions. A set of random image patches are spatially and adaptively sampled within each segment. These sets of extracted samples are formed into two ?bags of
patches? to model the foreground/background appearance respectively. The foreground/background
decision for any segment in a new image is computed using one of two aggregation functions on
the appearance similarities from its inside image patches to the foreground and background models.
Finally, for videos, within each bag, new patches from new frames are integrated through a robust
bidirectional consistency check process and all image patches are then partitioned and resampled to
create an evolving appearance model. As described below, this process prune classification inaccuracies in the nonparametric image patch representations and adapts them towards current changes in
foreground/background appearances for videos. We describe each of these steps for video tracking
of foreground/background segments in more detail below, and for image matching, which we treat
as a special case by simply omitting step 3 and 4 in Figure 2.
Non-parametric Patch Appearance Modelling-Matching Algorithm
inputs: Pre-segmented Images Xt , t = 1, 2, ..., T ; Label L1
F |B
outputs: Labels Lt , t = 2, ..., T ; 2 ?bags of patches? appearance model for foreground/background ?T
1. Sample segmentation-adaptive random image patches {P1 } from image X1 .
F |B

2. Construct 2 new bags of patches ?1 for foreground/background using patches {P1 } and label
L1 ; set t = 1.
3. t = t + 1; Sample segmentation-adaptive random image patches {Pt } from image Xt ; match
F |B
{Pt } with ?t?1 and classify segments of Xt to generate label Lt by aggregation.
4. Classify and reject ambiguous patch samples, probable outliers and redundant appearance patch
F |B
samples from new extracted image patches {Pt } against ?t?1 ; Then integrate the filtered {Pt }
F |B
F |B
into ?t?1 and evaluate the probability of survival ps for each patch inside ?t?1 against the
original unprocessed {Pt } (Bidirectional Consistency Check).
5. Perform the random partition and resampling process according to the normalized product of
F |B
F |B
probability of survival ps and partition-wise sampling rate ? ? inside ?t?1 to generate ?t .
F |B

6. If t = T , output Lt , t = 2, ..., T and ?T

; exit. If t < T , go to (3).

Figure 2: Non-parametric Patch Appearance Modelling-Matching Algorithm

Figure 3: Left: Segment adaptive random patch sampling from an image with known figure/ground labels.
Green dots are samples for background; dark brown dots are samples for foreground. Right: Segment adaptive
random patch sampling from a new image for figure/ground classification, shown as blue dots.

3.1 Sample Random Image Patches
We first employ an image segmentation algorithm2 [6] to pre-segment all the images or video frames
in our experiments. A typical segmentation result is shown in Figure 1. We use Xt , t = 1, 2, ..., T
to represent a sequence of video frames. Given an image segment, we formulate its representation
as a distribution on the appearance variation over all possible extracted image patches inside the
segment. To keep this representation to a manageable size, we approximate this distribution by
sampling a random subset of patches.
We denote an image segment as Si with SiF for a foreground segment, and SiB for a background
segment, where i is the index of the (foreground/background)image segment within an image. Accordingly, Pi , PiF and PiB represent a set of random image patches sampled from Si , SiF and SiB
respectively. The cardinality Ni of an image segment Si generated by [6] typically ranges from 50
to thousands. However small or large superpixels are expected to have roughly the same amount
of uniformity. Therefore the sampling rate ?i of Si , defined as ?i = size(Pi )/Ni , should decrease
with increasing Ni . For simplicity, we keep ?i as a constant for all superpixels, unless Ni is above a
predefined threshold ? , (typically 2500 ? 3000), above which size(Pi ) is held fixed. This sampling
adaptivity is illustrated in Figure 3. Notice that large image segments have much more sparsely
sampled patches than small image segments. From our experiments, this adaptive spatial sampling
strategy is sufficient to represent image segments of different sizes.
3.2 Label Segments by Aggregating Over Random Patches
For an image segment Si from a new frame to be classified, we again first sample a set of random
patches Pi as its representative set of appearance samples. For each patch p ? Pi , we calculate its
B
B
F
distances dF
p , dp or matching scores mp , mp towards the foreground and background appearance
models respectively as described in Section 2.
The decision of assigning Si to foreground or background, is an aggregating process over all
B
B
F
{dF
p , dp } or {mp ; mp } where p ? Pi . Since Pi is considered as a set of i.i.d. samples of the
B
B
F
appearance distribution of Si , we use the average of {dF
p , dp } or {mp ; mp } (ie. first-order statisB
F
B
F
tics) as its distances DPi , DPi or fitness values MPi , MPi with the foreground/background model.
B
F
B
B
F
In terms of distances {dF
p , dp }, DPi = meanp?Pi (dp ) and DPi = meanp?Pi (dp ). Then the
F
segment?s foreground/background fitness is set as the inverse of the distances: MPFi = 1/DP
i
F
B
F
F
B
B
and MPi = 1/DPi . In terms of KDE matching scores {mp ; mp }, MPi = meanp?Pi (mp ) and
B
F
MPBi = meanp?Pi (mB
p ). Finally, Si is classified as foreground if MPi > MPi , and vice versa. The
Median robust operator can also be employed in our experiments, without noticeable difference in
F
performance. Another choice is to classify each p ? Pi from mB
p and mp , then vote the majority
foreground/background decision for Si . The performance is similar with mean and median.
2

Because we are not focused on image segmentation algorithms, we choose Felzenszwalb?s segmentation
code which generates good results and is publicly available at http://people.cs.uchicago.edu/?pff/segment/.

3.3 Construct a Robust Online Nonparametric Foreground/Background Appearance Model
with Temporal Adaptation
From sets of random image patches extracted from superpixels with known figure/ground labels, 2
foreground/background ?bags of patches? are composed. The bags are the non-parametric form of
the foreground/background appearance distributions. When we intend to ?track? the figure/ground
model sequentially though a sequence, these models need to be updated by integrating new image
patches extracted from new video frames. However the size (the number of patches) of the bag will
be unacceptably large if we do not also remove the some redundant information over time. More
importantly, imperfect segmentation results from [6] can cause inaccurate segmentation level figure/ground labels. For robust image patch level appearance modeling of ?t , we propose a novel
bidirectional consistency check and resampling strategy to tackle various noise and labelling uncertainties.
More precisely, we classify new extracted image patches {Pt } as {PtF } or {PtB } according to
F |B
F |B
B
?t?1 ; and reject ambiguous patch samples whose distances dF
p , dp towards respective ?t?1 have
B
no good contrast (simply, the ratio between dF
p and dp falls into the range of 0.8 to 1/0.8). We
further sort the distance list of the newly classified foreground patches {PtF } to ?F
t?1 , filter out
image patches on the top of the list which have too large distances and are probably to be outliers,
and ones from the bottom of the list which have too small distances and contain probably redundant
3
B
B
appearances compared with ?F
t?1 . We perform the same process with {Pt } according to ?t?1 .
F |B

F ? |B ?

Then the filtered {Pt } are integrated into ?t?1 to form ?t?1 , and we evaluate the probability of
F ? |B ?

survival ps for each patch inside ?t?1

against the original unprocessed {Pt } with their labels4 .
F ? |B ?

Next, we cluster all image patches of ?t?1 into k partitions [8], and randomly resample image
patches within each partition. This is roughly equivalent to finding the modes of an arbitrary distribution and sampling for each mode. Ideally, the resampling rate ? ? should decrease with increasing
partition size, similar to the segment-wise sampling rate ?. For simplicity, we define ? ? as a constant
value for all partitions, unless setting a threshold ? ? to be the minimal required size5 of partitions after resampling. If we perform resampling directly over patches without partitioning, some modes of
the appearance distribution may be mistakenly removed. This strategy represents all partitions with
sufficient number of image patches, regardless of their different sizes. In all, we resample image
F |B
patches of ?t?1 , according to the normalized product of probability of survival ps and partitionF |B
wise sampling rate ? ? , to generate ?t . By approximately fixing the expected bag model size, the
number of image patches extracted from a certain frame Xt in the bag decays exponentially in time.
The problem of partitioning image patches in the bag can be formulated as the NP-hard k-center
problem. The definition of k-center is as follows: given a data set of n points and a predefined
cluster number k, find a partition of the points into k subgroups P1 , P2 , ..., Pk and the data centers c1 , c2 , ..., ck , to minimize the maximum radius of clusters maxi maxp?Pi k p ? ci k, where i
is the index of clusters. Gonzalez [8] proposed an efficient greedy algorithm, farthest-point clustering, which proved to give an approximation factor of 2 of the optimum. The algorithm operates as follows: pick a random point p1 as the first cluster center and add it to the center set C;
for iterations i = 2, ..., k, find the point pi with the farthest distance to the current center set C:
di (pi , C) = minc?C k pi ? c k and add pi to set C; finally assign data points to its nearest center and recompute the means of clusters in C. Compared with the popular k-means algorithm, this
algorithm is computationally efficient and theoretically bounded6. In this paper, we employ the Eu3

Simply, we reject patches with distances dF
that are larger than mean(dF
) + ? ? std(dF
) or smaller
pF
pF
pF
t

t

t

F |B

than mean(dF
) ? ? ? std(dF
) where ? controls the range of accepting patch samples of ?t?1 , called model
pF
pF
t
t
rigidity.
?
4
F
For example, we compute the distance of each patch in ?F
t?1 to {Pt }, and covert them as surviving
probabilities using a exponential function over negative covariance normalized distances. Patches with smaller
distances have higher survival chances during resampling; and vice versa. We perform the same process with
?
B
?B
t?1 according to {Pt }.
5
All image patches from partitions that are already smaller than ? ? are kept during resampling.
6
The random initialization of all k centers and the local iterative smoothing process in k-means, which is
time-consuming in high dimensional space and possibly converges to undesirable local minimum, are avoided.

clidean distance between an image patch and a cluster center, using the raw RGB intensity vector or
the feature representations discussed in section 2.

4 Experiments
We have evaluated the image patch representations described in Section 2 for figure/ground mapping
between pairs of image on video sequences taken with both static and moving cameras. Here we
summarize our results.
4.1 Evaluation on Object-level Figure/Ground Image Mapping
We first evaluate our algorithm on object-level figure/ground mapping between pairs of images under
eight configurations of different image patch representations and matching criteria. They are listed
as follows: the nearest neighbor distance matching on the image patch?s mean color vector (MCV);
raw color intensity vector of regular patch scanning (RCV) or segment-adaptive patch sampling over
image (SCV); color + filter bank response (CFB); color + Haralick texture descriptor (CHA); PCA
feature vector (PCA); NDA feature vector (NDA) and kernel density evaluation on PCA features
(KDE). In general, 8000 ? 12000 random patches are sampled per image. There is no apparent
difference on classification accuracy for the patch size ranging from 9 to 15 pixels and the sample
rate from 0.02 to 0.10. The PCA/NDA feature vector has 20 dimensions, and KDE is evaluated on
the first 3 ? 6 PCA features.
Because the foreground figure has fewer of pixels than background, we conservatively measure the
classification accuracy from the foreground?s detection precision and recall on pixels. Precision
is the ratio of the number of correctly detected foreground pixels to the total number of detected
foreground pixels; recall is is the ratio of the number of correctly detected foreground pixels to the
total number of foreground pixels in the image. The patch size is 11 by 11 pixels, and the segmentwise patch sampling rate ? is fixed as 0.06, unless stated otherwise. Using 40 pairs of (720 ?
480) images with the labelled figure/ground segmentation, we compare their average classification
accuracies in Tables 1.
MCV
0.46
0.28

RCV
0.81
0.89

SCV
0.97
0.95

CFB
0.92
0.85

CHA
0.89
0.81

PCA
0.93
0.85

NDA
0.96
0.87

KDE
0.69
0.98

Table 1: Evaluation on classification accuracy (ratio). The first row is precision; the second row is
recall.
For figure/ground extraction accuracy, SCV has the best classification ratio using the raw color intensity vector without any dimension reduction. MCV has the worst accuracy, which shows that pixelcolor leads to poor separability between figure and ground in our data set. Four feature based representations, CFB, CHA, PCA, NDA with reduced dimensions, have similar performance, whereas
NDA is slightly better than the others. KDE tends to be more biased towards the foreground class
because background usually has a wider, flatter density distribution. The superiority of SCV over
RCV proves that our segment-wise random patch sampling strategy is more effective at classifying
image segments than regularly scanning the image, even with more samples. As shown in Figure 4
(b), some small or irregularly-shaped image segments do not have enough patch samples to produce
stable classifications.

(a) MCV

(b) RCV

(c) SCV

(d) CFB

(e) CHA

(f) PCA

(g) NDA

(h) KDE

Figure 4: An example of evaluation on object-level figure/ground image mapping. The labeled figure image
segments are coded in blue.

4.2 Figure/Ground Segmentation Tracking with a Moving Camera
From Figure 4 (h), we see KDE tends to produce some false positives for the foreground. However
the problem can be effectively tackled by multiplying the appearance KDE by the spatial prior
which is also formulated as a KDE function of image patch coordinates. By considering videos with
complex appearance-changing figure/ground, imperfect segmentation results [6] are not completely
avoidable which can cause superpixel based figure/ground labelling errors. However our robust
bidirectional consistency check and resampling strategy, as shown below, enables to successfully
track the dynamic figure/ground segmentations in challenging scenarios with outlier rejection, model
rigidity control and temporal adaptation (as described in section 3.3).
Karsten.avi shows a person walking in an uncontrolled indoor environment while tracked with a
handheld camera. After we manually label the frame 1, the foreground/background appearance
model starts to develop, classify new frames and get updated online. Eight Example tracking frames
are shown in Figure 5. Notice that the significant non-rigid deformations and large scale changes of
the walking person, while the original background is completely substituted after the subject turned
his way. In frame 258, we manually eliminate some false positives of the figure. The reason for
this failure is that some image regions which were behind the subject begin to appear when the
person is walking from left to the center of image (starting from frame 220). Compared to the online
foreground/background appearance models by then, these newly appearing image regions have quite
different appearance from both the foreground and the background. Therefore the foreground?s
spatial prior dominates the classification. We leave this issue for future work.

(a) 12#

(b) 91#

(c) 155#

(d) 180#

(e) 221#

(f) 257#

(g) 308#

(h) 329#

Figure 5: Eight example frames (720 by 480 pixels) from the video sequence Karsten.avi of 330 frames. The
video is captured using a handheld Panasonic PV-GS120 in standard NTSC format. Notice that the significant non-rigid deformations and large scale changes of the walking person, while the original background is
completely substituted after the subject turned his way. The red pixels are on the boundary of segments; the
tracked image segments associated with the foreground walking person is coded in blue.

4.3 Non-rigid Object Tracking from Surveillance Videos
We can also apply our nonparametric treatment of dynamic random patches in Figure 2 into tracking non-rigid interested objects from surveillance videos. The difficult is that surveillance cameras
normally capture small non-rigid figures, such as a walking person or running car, in low contrast
and low resolution format. Thus to adapt our method to solve this problem, we make the following modifications. Because our task changes to localizing figure object automatically overtime, we
can simply model figure/ground regions using rectangles and therefore no pre-segmentation [6] is
needed. Random figure/ground patches are then extracted from the image regions within these two
rectangles. Using two sets of random image patches, we train an online classifier for figure/ground
classes at each time step, generate a figure appearance confidence map of classification for the next
frame and, similarly to [1], apply mean shift [4] to find the next object location by mode seeking.
In our problem solution, the temporal evolution of dynamic image patch appearance models are
executed by the bidirectional consistency check and resampling described in section 3.3. Whereas
[1] uses boosting for both temporal appearance model updating and classification, our online binary classification training can employ any off-the-shelf classifiers, such as k-Nearest Neighbors
(KNN), support vector machine (SVM). Our results are favorably competitive to the state-of-the-art
algorithms [1, 9], even under more challenging scenario.

5 Conclusion and Discussion
Although quite simple both conceptually and computationally, our algorithm of performing dynamic foreground-background extraction in images and videos using non-parametric appearance

models produces very promising and reliable results in a wide variety of circumstances. For tracking figure/ground segments, to our best knowledge, it is the first attempt to solve this difficult ?video
matting? problem [15, 25] by robust and automatic learning. For surveillance video tracking, our
results are very competitive with the state-of-art [1, 9] under even more challenging conditions.
Our approach does not depend on an image segmentation algorithm that totally respects the boundaries of the foreground object. Our novel bidirectional consistency check and resampling process
has been demonstrated to be effectively robust and adaptive. We leave the explorations on supervised dimension reduction and density modeling techniques on image patch sets, optimal random
patch sampling strategy, and self-tuned optimal image patch size searching as our future work.
In this paper, we extract foreground/background by classifying on individual image segments. It
might improve the figure/ground segmentation accuracy by modeling their spatial pairwise relationships as well. This problem can be further solved using generative or discriminative random field
(MRF/DRF) model or the boosting method on logistic classifiers [11]. In this paper, we focus on
learning binary dynamic appearance models by assuming figure/ground are somewhat distributionwise separatable. Other cues, as object shape regularization and motion dynamics for tracking, can
be combined to improve performance.

References
[1] S. Avidan, Ensemble Tracking, CVPR 2005.
[2] Y. Boykov and M. Jolly, Interactive Graph Cuts for Optimal boundary and Region Segmentation of Objects
in n-d Images, ICCV, 2001.
[3] M. Bressan and J. Vitri`
a, Nonparametric discriminative analysis and nearest neighbor classification, Pattern Recognition Letter, 2003.
[4] D. Comaniciu and P. Meer, Mean shift: A robust approach toward feature space analysis. IEEE Trans.
PAMI, 2002.
[5] A. Efros, T. Leung, Texture Synthesis by Non-parametric Sampling, ICCV, 1999.
[6] P. Felzenszwalb and D. Huttenlocher, Efficient Graph-Based Image Segmentation, IJCV, 2004.
[7] K. Fukunaga and J. Mantock, Nonparametric discriminative analysis, IEEE Trans. on PAMI, Nov. 1983.
[8] T. Gonzalez, Clustering to minimize the maximum intercluster distance, Theoretical Computer Science,
38:293-306, 1985.
[9] B. Han and L. Davis, On-Line Density-Based Appearance Modeling for Object Tracking, ICCV 2005.
[10] R. Haralick, K. Shanmugam, I. Dinstein, Texture features for image classification. IEEE Trans. on SMC,
1973.
[11] D. Hoiem, A. Efros and M. Hebert, Automatic Photo Pop-up, SIGGRAPH, 2005.
[12] A. Ihler, Kernel Density Estimation Matlab Toolbox, http://ssg.mit.edu/ ihler/code/kde.shtml.
[13] T. Leung and J. Malik, Representing and Recognizing the Visual Appearance of Materials using ThreeDimensional Textons, IJCV, 2001.
[14] Y. Li, J. Sun, C.-K. Tang and H.-Y. Shum, Lazy Snapping, SIGGRAPH, 2004.
[15] Y. Li, J. Sun and H.-Y. Shum. Video Object Cut and Paste, SIGGRAPH, 2005.
[16] D. Lowe, Distinctive image features from scale-invariant keypoints, IJCV, 2004.
[17] L. Lu, K. Toyama and G. Hager, A Two Level Approach for Scene Recognition, CVPR, 2005.
[18] J. Malik, S. Belongie, T. Leung, J. Shi, Contour and Texture Analysis for Image Segmentation, IJCV,
2001.
[19] D. Martin, C. Fowlkes, J. Malik, Learning to Detect Natural Image Boundaries Using Local Brightness,
Color, and Texture Cues, IEEE Trans. on PAMI, 26(5):530-549, May 2004.
[20] A. Mittal and N. Paragios, Motion-based Background Substraction using Adaptive Kernel Density Estimation, CVPR, 2004.
[21] Eric Nowak, Frdric Jurie, Bill Triggs, Sampling Strategies for Bag-of-Features Image Classification,
ECCV, 2006.
[22] X. Ren and J. Malik, Learning a classification model for segmentation, ICCV, 2003.
[23] C. Rother, V. Kolmogorov and A. Blake, Interactive Foreground Extraction using Iterated Graph Cuts,
SIGGRAPH, 2004.
[24] Yaser Sheikh and Mubarak Shah, Bayesian Object Detection in Dynamic Scenes, CVPR, 2005.
[25] J. Wang, P. Bhat, A. Colburn, M. Agrawala and M. Cohen, Interactive Video Cutout. SIGGRAPH, 2005.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 3309-the-infinite-gamma-poisson-feature-model.pdf

The Infinite Gamma-Poisson Feature Model
Michalis K. Titsias
School of Computer Science,
University of Manchester, UK
mtitsias@cs.man.ac.uk

Abstract
We present a probability distribution over non-negative integer valued matrices
with possibly an infinite number of columns. We also derive a stochastic process
that reproduces this distribution over equivalence classes. This model can play
the role of the prior in nonparametric Bayesian learning scenarios where multiple
latent features are associated with the observed data and each feature can have
multiple appearances or occurrences within each data point. Such data arise naturally when learning visual object recognition systems from unlabelled images.
Together with the nonparametric prior we consider a likelihood model that explains the visual appearance and location of local image patches. Inference with
this model is carried out using a Markov chain Monte Carlo algorithm.

1

Introduction

Unsupervised learning using mixture models assumes that one latent cause is associated with each
data point. This assumption can be quite restrictive and a useful generalization is to consider factorial
representations which assume that multiple causes have generated the data [11]. Factorial models
are widely used in modern unsupervised learning algorithms; see e.g. algorithms that model text
data [2, 3, 4]. Algorithms for learning factorial models should deal with the problem of specifying
the size of the representation. Bayesian learning and especially nonparametric methods such as the
Indian buffet process [7] can be very useful for solving this problem.
Factorial models usually assume that each feature occurs once in a given data point. This is inefficient to model the precise generation mechanism of several data such as images. An image can
contain views of multiple object classes such as cars and humans and each class may have multiple
occurrences in the image. To deal with features having multiple occurrences, we introduce a probability distribution over sparse non-negative integer valued matrices with possibly an unbounded
number of columns. Each matrix row corresponds to a data point and each column to a feature
similarly to the binary matrix used in the Indian buffet process [7]. Each element of the matrix
can be zero or a positive integer and expresses the number of times a feature occurs in a specific
data point. This model is derived by considering a finite gamma-Poisson distribution and taking
the infinite limit for equivalence classes of non-negative integer valued matrices. We also present a
stochastic process that reproduces this infinite model. This process uses the Ewens?s distribution [5]
over integer partitions which was introduced in population genetics literature and it is equivalent to
the distribution over partitions of objects induced by the Dirichlet process [1].
The infinite gamma-Poisson model can play the role of the prior in a nonparametric Bayesian learning scenario where both the latent features and the number of their occurrences are unknown. Given
this prior, we consider a likelihood model which is suitable for explaining the visual appearance and
location of local image patches. Introducing a prior for the parameters of this likelihood model, we
apply Bayesian learning using a Markov chain Monte Carlo inference algorithm and show results in
some image data.

2

The finite gamma-Poisson model

Let X = {X1 , . . . , XN } be some data where each data point Xn is a set of attributes. In section
4 we specify Xn to be a collection of local image patches. We assume that each data point is
associated with a set of latent features and each feature can have multiple occurrences. Let znk
denote the number of times feature k occurs in the data point Xn . Given K features, Z = {znk } is
a N ? K non-negative integer valued matrix that collects together all the znk values so as each row
corresponds to a data point and each column to a feature. Given that znk is drawn from a Poisson
with a feature-specific parameter ?k , Z follows the distribution
P (Z|{?k }) =

N Y
K
K
k
Y
Y
?zknk exp{??k }
?m
k exp{?N ?k }
,
=
QN
znk !
n=1
n=1 znk !
k=1

(1)

k=1

PN
where mk = n=1 znk . We further assume that each ?k parameter follows a gamma distribution
that favors sparsity (in a sense that will be explained shortly):
?

?K
?
G(?k ; , 1) = k
K

?1

exp{??k }
.
?
?( K
)

(2)

The hyperparameter ? itself is given a vague gamma prior G(?; ?0 , ?0 ). Using the above equations
we can easily integrate out the parameters {?k } as follows
P (Z|?) =

K
Y

k=1

?(mk +

?
K)
?

?
?( K
)(N + 1)mk + K

QN

n=1 znk !

,

(3)

which shows that given the hyperparameter ? the columns of Z are independent. Note that the above
distribution is exchangeable since reordering the rows of Z does not alter the probability. Also as
K increases the distribution favors sparsity. This can be shown by taking the expectation of the sum
PN
of all elements of Z. Since the columns are independent this expectation is K n=1 E(znk ) and
E(znk ) is given by
?
X
? 1
?
E(znk ) =
znk N B(znk ; , ) = ,
(4)
K
2
K
z =0
nk

where N B(znk ; r, p), with r > 0 and 0 < p < 1, denotes the negative binomial distribution over
positive integers
?(r + znk ) r
p (1 ? p)znk ,
(5)
N B(znk ; r, p) =
znk !?(r)

that has a mean equal to r(1?p)
. Using Equation (4) the expectation of the sum of znk s is ?N and
p
is independent of the number of features. As K increases, Z becomes sparser and ? controls the
sparsity of this matrix.
There is an alternative way of deriving the joint distribution P (Z|?) according to the following
generative process:
?
(?1 , . . . , ?K ) ? D
, ? ? G(?; ?, 1),
K

Y
K
Ln
Ln ? P oisson(?), (zn1 , . . . , znK ) ?
?kznk , n = 1, . . . , N,
zn1 . . . znK
k=1

?
D( K
)

where
denotes the symmetric Dirichlet. Marginalizing out ? and ? gives rise to the same
distribution P (Z|?). The above process generates a gamma random variable and multinomial parameters and then samples the rows of Z independently by using the Poisson-multinomial pair. The
connection with the Dirichlet-multinomial pair implies that the infinite limit of the gamma-Poisson
model must be related to the Dirichlet process. In the next section we see how this connection is
revealed through the Ewens?s distribution [5].
Models that combine gamma and Poisson distributions are widely applied in statistics. We point out
that the above finite model shares similarities with the techniques presented in [3, 4] that model text
data.

3

The infinite limit and the stochastic process

To express the probability distribution in (3) for infinite many features K we need to consider equivalence classes of Z matrices similarly to [7]. The association of columns in Z with features defines
an arbitrary labelling of the features. Given that the likelihood p(X|Z) is not affected by relabelling
the features, there is an equivalence class of matrices that all can be reduced to the same standard
form after column reordering. We define the left-ordered form of non-negative integer valued matrices as follows. We assume that for any possible znk holds znk ? c ? 1, where c is a sufficiently
large integer. We define h = (z1k . . . zN k ) as the integer number associated with column k that is
expressed in a numeral system with basis c. The left-ordered form is defined so as the columns of Z
appear from left to right in a decreasing order according to the magnitude of their numbers.
Starting from Equation (3) we wish to define the probability distribution over matrices constrained in
a left-ordered standard form. Let Kh be the multiplicity of the column with number h; for example
K0 is the number of zero columns. An equivalence class [Z] consists of PcNK!
different matri?1
h=0

Kh !

ces that they are generated from the distribution in (3) with equal probabilities and can be reduced
to the same left-ordered form. Thus, the probability of [Z] is
K!
P ([Z]) = PcN ?1
h=0

K
Y

?(mk +

?
K)

?
mk + K

?
)(N + 1)
Kh ! k=1 ?( K

QN

n=1 znk !

.

(6)

We assume that the first K+ features are represented i.e. mk > 0 for k ? K+ , while the rest K ?K+
features are unrepresented i.e. mk = 0 for k > K+ . The infinite limit of (6) is derived by following
a similar strategy with the one used for expressing the distribution over partitions of objects as a
limit of the Dirichlet-multinomial pair [6, 9]. The limit takes the following form:
QK+
? K+
1
k=1 (mk ? 1)!
P (Z|?) = PcN ?1
,
(7)
QK+ QN
m+?
(N + 1)
n=1 znk !
k=1
h=1 Kh !
PK+
where m = k=1
mk . This expression defines an exchangeable joint distribution over non-negative
integer valued matrices with infinite many columns in a left-ordered form. Next we present a sequential stochastic process that reproduces this distribution.
3.1

The stochastic process

The distribution in Equation (7) can be derived from a simple stochastic process that constructs
the matrix Z sequentially so as the data arrive one at each time in a fixed order. The steps of this
stochastic process are discussed below.
When the first data point arrives all the features are currently unrepresented. We sample feature
occurrences from the set of unrepresented features as follows. Firstly, we draw an integer number
g1 from the negative binomial N B(g1 ; ?, 21 ) which has a mean value equal to ?. g1 is the total
number of feature occurrences for the first data point. Given g1 , we randomly select a partition
(z11 , . . . , z1K1 ) of the integer g1 into parts1 , i.e. z11 + . . . + z1K1 = g1 and 1 ? K1 ? g1 , by
drawing from Ewens?s distribution [5] over integer partitions which is given by
P (z11 , . . . , z1K1 ) = ?K1
(1)

g1
Y
?(?)
g1 !
1
,
?(g1 + ?) z11 ? . . . ? z1K1 i=1 v (1) !
i

(8)

where vi is the multiplicity of integer i in the partition (z11 , . . . , z1K1 ). The Ewens?s distribution
is equivalent to the distribution over partitions of objects induced by the Dirichlet process and the
Chinese restaurant process since we can derive the one from the other using simple combinatorics
arguments. The difference between them is that the former is a distribution over integer partitions
while the latter is a distribution over partitions of objects.
Let Kn?1 be the number of represented features when the nth data point arrives. For each feature
k, with k ? Kn?1 , we choose znk based on the popularity of this feature in the previous n ? 1 data
1
The partition of a positive integer is a way of writing this integer as a sum of positive integers where order
does not matter, e.g. the partitions of 3 are: (3),(2,1) and (1,1,1).

points. This popularity is expressed by the total number of occurrences for the feature k which is
Pn?1
n
given by mk = i=1 zik . Particularly, we draw znk from N B(znk ; mk , n+1
) which has a mean
mk
value equal to n . Once we have sampled from all represented features we need to consider a
sample from the set of unrepresented features. Similarly to the first data point, we first draw an
n
integer gn from N B(gn ; ?, n+1
), and subsequently we select a partition of that integer by drawing
from the Ewens?s formula. This process produces the following distribution:
Q K+
1
? K+
k=1 (mk ? 1)!
P (Z|?) = Qg
,
(9)
QK+ QN
QgN (N )
m+?
(1)
1
! (N + 1)
n=1 znk !
k=1
i=1 vi ! ? . . . ?
i=1 vi
(n)

where {vi } are the integer-multiplicities for the nth data point which arise when we draw from
the Ewens?s distribution. Note that the above expression does not have exactly the same form as the
distribution in Equation (7) and is not exchangeable since it depends on the order the data arrive.
However, if we consider only the left-ordered class of matrices generated by the stochastic process
then we obtain the exchangeable distribution in Equation (7). Note that a similar situation arises
with the Indian buffet process.
3.2

Conditional distributions

When we combine the prior P (Z|?) with a likelihood model p(X|Z) and we wish to do inference over Z using Gibbs-type sampling, we need to express the conditionals of the form
P (znk |Z?(nk) , ?) where Z?(nk) = Z \ znk . We can derive such conditionals by taking limits
of the conditionals for the finite model or by using the stochastic process.
Suppose that for the currentPvalue of Z, there exist K+ represented features i.e. mk > 0 for
k ? K+ . Let m?n,k =
ek . When m?n,k > 0, the conditional of znk is given by
n
e6=n zn
N B(znk ; m?n,k , NN+1 ). In all different cases, we need a special conditional that samples from
new features2 and accounts for all k such that m?n,k = 0. This conditional draws an integer number from N B(gn ; a, NN+1 ) and then determines the occurrences for the new features by choosing a
partition of the integer gn using the Ewens?s distribution. Finally the conditional p(?|Z), which can
be directly expressed from Equation (7) and the prior of ?, is given by
? K+
p(?|Z) ? G(?; ?0 , ?0 )
.
(10)
(N + 1)?
Typically the likelihood model does not depend on ? and thus the above quantity is also the posterior
conditional of ? given data and Z.

4

A likelihood model for images

An image can contain multiple objects of different classes. Each object class can have more than
one occurrences, i.e. multiple instances of the class may appear simultaneously in the image. Unsupervised learning should deal with the unknown number of object classes in the images and also
the unknown number of occurrences of each class in each image separately. If object classes are the
latent features, what we wish to infer is the underlying feature occurrence matrix Z. We consider
an observation model that is a combination of latent Dirichlet allocation [2] and Gaussian mixture
models. Such a combination has been used before [12]. Each image n is represented by dn local
patches that are detected in the image so as Xn = (Yn , Wn ) = {(yni , wni ), i = 1, . . . , dn }. yni
is the two-dimensional location of patch i and wni is an indicator vector (i.e. is binary and satisfies
PL
?
?=1 wni = 1) that points into a set of L possible visual appearances. X, Y , and W denote all
the data the locations and the appearances, respectively. We will describe the probabilistic model
starting from the joint distribution of all variables which is given by
joint = p(?)P (Z|?)p({?k }|Z)?
"
#
dn
N
Y
Y
p(?n |Zn )p(mn , ?n |Zn )
P (sni |?n )P (wni |sni , {?k })p(yni |sni , mn , ?n ) .
n=1

(11)

i=1

2
Features of this kind are the unrepresented features (k > K+ ) as well as all the unique features that occur
only in the data point n (i.e. m?n,k = 0, but znk > 0).

Z

?

{?k }
(mn, ?n)

?n
sni
wni

yni
dn

N

Figure 1: Graphical model for the joint distribution in Equation (11).

The graphical representation of this distribution is depicted in Figure 1. We now explain all the
pieces of this joint distribution following the causal structure of the graphical model. Firstly, we
generate ? from its prior and then we draw the feature occurrence matrix Z using the infinite
gamma-Poisson prior P (Z|?). The matrix Z defines the structure for the remaining part of the
model. The parameter vector ?k = {?k1 , . . . , ?kL } describes the appearance of the local patches W
for the feature (object class) k. Each ?k is generated from a symmetric Dirichlet so as the whole
Q K+
set of {?k } vectors is drawn from p({?k }|Z) = k=1
D(?k |?), where ? is the hyperparameter of
the symmetric Dirichlet and it is common for all features. Note that the feature appearance parameters {?k } depend on Z only through the number of represented features K+ which is obtained by
counting the non-zero columns of Z.
The parameter vector ?n = {?nkj } defines the image-specific mixing proportions for the mixture
model associated with image n. To see how this mixture model arises, notice that a local patch in
image n belongs to a certain occurrence of a feature. We use the double index kj to denote the j
occurrence of feature k where j = 1, . . . , znk and k ? {e
k : znek > 0}. This mixture model has
PK +
Mn = k=1
znk components, i.e. as many as the total number of feature occurrences in image n.
The assignment variable sni = {skj
ni }, which takes Mn values, indicates the feature occurrence of
patch i. ?n is drawn from a symmetric Dirichlet given by p(?n |Zn ) = D(?n |?/Mn ), where Zn
denotes the nth row of Z and ? is a hyperparameter shared by all images. Notice that ?n depends
only on the nth row of Z.
The parameters (mn , ?n ) determine the image-specific distribution for the locations {yni } of the
local patches in image n. We assume that each occurrence of a feature forms a Gaussian cluster
of patch locations. Thus yni follows a image-specific Gaussian mixture with Mn components. We
assume that the component kj has mean mnkj and covariance ?nkj . mnkj describes object location
and ?nkj object shape. mn and ?n collect all the means and covariances of the clusters in the image
n. Given that any object can be anywhere in the image and have arbitrary scale and orientation,
(mnkj , ?nkj ) should be drawn from a quite vague prior. We use a conjugate normal-Wishart prior
for the pair (mnkj , ?nkj ) so as
p(mn , ?n |Zn ) =

Y

znk
Y

N (mnkj |?, ? ?nkj )W (??1
nkj |v, V ),

(12)

k:znk >0 j=1

where (?, ?, v, V ) are the hyperparameters shared by all features and images. The assignment sni
which determines the allocation of a local patch in a certain feature occurrence follows a multinoQ
Qznk
kj
mial: P (sni |?n ) = k:znk >0 j=1
(?nkj )sni . Similarly the observed data pair (wni , yni ) of a
local image patch is generated according to
P (wni |sni , {?k }) =

K+ L
Y
Y

k=1 ?=1

w?

?k?ni

Pznk

j=1

skj
ni

and
p(yni |sni , mn , ?n ) =

Y

znk
Y

skj
ni

[N (yni |mnkj , ?nkj )]

.

k:znk >0 j=1

The hyperparameters (?, ?, ?, ?, v, V ) take fixed values that give vague priors and they are not
depicted in the graphical model shown in Figure 1.
Since we have chosen conjugate priors, we can analytically marginalize out from the joint distribution all the parameters {?n }, {?k }, {mn } and {?n } and obtain p(X, S, Z, ?). Marginalizing
out the assignments S is generally intractable and the MCMC algorithm discussed next produces
samples from the posterior P (S, Z, ?|X).
4.1

MCMC inference

Inference with our model involves expressing the posterior P (S, Z, ?|X) over the feature occurrences Z, the assignments S and the parameter ?. Note that the joint P (S, Z, ?, X) factorizes
QN
according to p(?)P (Z|?)P (W |S, Z) n=1 P (Sn |Zn )p(Yn |Sn , Zn ) where Sn denotes the assignments associated with image n. Our algorithm uses mainly Gibbs-type sampling from conditional
posterior distributions. Due to space limitations we briefly discuss the main points of this algorithm.
The MCMC algorithm processes the rows of Z iteratively and updates its values. A single step can
PK+
new
old
change an element of Z by one so as |znk
? znk
| ? 1. Initially Z is such that Mn = k=1
znk ?
1, for any n which means that at least one mixture component explains the data of each image. The
proposal distribution for changing znk s ensures that this constraint is satisfied.
Suppose we wish to sample a new value for znk using the joint model p(S, Z, ?, X). Simply witting
P (znk |S, Z?(nk) , ?, X) is not useful since when znk changes the number of states the assignments
Sn can take also changes. This is clear since znk is a structural variable that affects the number of
PK+
components Mn = k=1
znk of the mixture model associated with image n and assignments Sn .
On the other hand the dimensionality of the assignments S?n = S \ Sn of all other images is not
affected when znk changes. To deal with the above we marginalize out Sn and we sample znk from
the marginalized posterior conditional P (znk |S?n , Z?(nk) , ?, X) which is computed according to
X
P (znk |S?n , Z?(nk) , ?, X) ? P (znk |Z?(nk) , ?)
P (W |S, Z)p(Yn |Sn , Zn )P (Sn |Zn ), (13)
Sn

where P (znk |Z?n,k , ?) for the infinite case is computed as described in section 3.2 while computing
the sum requires an approximation. This sum is a marginal likelihood and we apply importance
sampling using as an importance distribution the posterior conditional P (Sn |S?n , Z, W, Yn ) [10].
Sampling from P (Sn |S?n , Z, W, Yn ) is carried out by applying local Gibbs sampling moves and
global Metropolis moves that allow two occurrences of different features to exchange their data
clusters. In our implementation we consider a single sample drawn from this posterior distribution
so that the sum is approximated by P (W |Sn? , S?n , Z)p(Yn |Sn? , Zn ) and Sn? is a sample accepted
after a burn in period. Additionally to scans that update Z and S we add few Metropolis-Hastings
steps that update the hyperparameter ? using the posterior conditional given by Equation (10).

5

Experiments

In the first experiment we use a set of 10 artificial images. We consider four features that have
the regular shapes shown in Figure 2. The discrete patch appearances correspond to pixels and
can take 20 possible grayscale values. Each feature has its own multinomial distribution over the
appearances. To generate an image we first decide to include each feature with probability 0.5.
Then for each included feature we randomly select the number of occurrences from the range [1, 3].
For each feature occurrence we select the pixels using the appearance multinomial and place the
respective feature shape in a random location so that feature occurrences do not occlude each other.
The first row of Figure 2 shows a training image (left), the locations of pixels (middle) and the
discrete appearances (right). The MCMC algorithm was initialized with K+ = 1, ? = 1 and
zn1 = 1, n = 1, . . . , 10. The third row of Figure 2 shows how K+ (left) and the sum of all znk s
(right) evolve through the first 500 MCMC iterations. The algorithm in the first 20 iterations has

training image n

locations Yn

appearances Wn

1331

3230

0212

Figure 2: The first row shows a training image (left), the locations of pixels (middle) and the discrete
appearances (right). The second row shows the localizations of all feature occurrences in three
images. Below of each image the corresponding row of Z is also shown. The third row shows how
K+ (left) and the sum of all znk s (right) evolve through the first 500 MCMC iterations.

Figure 3: The left most plot on the first row shows the locations of detected patches and the bounding
boxes in one of the annotated images. The remaining five plots show examples of detections and
localizations of the three most dominant features (including the car-category) in five non-annotated
images.

visited the matrix Z that was used to generate the data and then stabilizes. For 86% of the samples
K+ is equal to four. For the state (Z, S) that is most frequently visited, the second row of Figure
2 shows the localizations of all different feature occurrences in three images. Each ellipse is drawn
using the posterior mean values for a pair (mnkj , ?nkj ) and illustrates the predicted location and
shape of a feature occurrence. Note that ellipses with the same color correspond to the different
occurrences of the same feature.
In the second experiment we consider 25 real images from the UIUC3 cars database. We used the
patch detection method presented in [8] and we constructed a dictionary of 200 visual appearances
by clustering the SIFT [8] descriptors of the patches using K-means. Locations of detected patches
are shown in the first row (left) of Figure 3. We partially labelled some of the images. Particularly,
for 7 out of 25 images we annotated the car views using bounding boxes (Figure 3). This allows
us to specify seven elements of the first column of the matrix Z (the first feature will correspond
to the car-category). These znk s values plus the assignments of all patches inside the boxes do not
change during sampling. Also the patches that lie outside the boxes in all annotated images are not
allowed to be part of car occurrences. This is achieved by applying partial Gibbs sampling updates
and Metropolis moves when sampling the assignments S. The algorithm is initialized with K+ = 1,
after 30 iterations stabilizes and then fluctuates between nine to twelve features. To keep the plots
uncluttered, Figure 3 shows the detections and localizations of only the three most dominant features
(including the car-category) in five non-annotated images. The red ellipses correspond to different
occurrences of the car-feature, the green ones to a tree-feature and the blue ones to a street-feature.

6

Discussion

We presented the infinite gamma-Poisson model which is a nonparametric prior for non-negative
integer valued matrices with infinite number of columns. We discussed the use of this prior for
unsupervised learning where multiple features are associated with our data and each feature can
have multiple occurrences within each data point. The infinite gamma-Poisson prior can be used for
other purposes as well. For example, an interesting application can be Bayesian matrix factorization
where a matrix of observations is decomposed into a product of two or more matrices with one of
them being a non-negative integer valued matrix.

References
[1] C. Antoniak. Mixture of Dirichlet processes with application to Bayesian nonparametric problems. The
Annals of Statistics, 2:1152?1174, 1974.
[2] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet allocation. JMLR, 3, 2003.
[3] W. Buntime and A. Jakulin. Applying discrete PCA in data analysis. In UAI, 2004.
[4] J. Canny. GaP: A factor model for discrete data. In SIGIR, pages 122?129. ACM Press, 2004.
[5] W. Ewens. The sampling theory of selectively neutral alleles. Theoretical Population Biology, 3:87?112,
1972.
[6] P. Green and S. Richardson. Modelling heterogeneity with and without the Dirichlet process. Scandinavian Journal of Statistics, 28:355?377, 2001.
[7] T. Griffiths and Z. Ghahramani. Infinite latent feature models and the Indian buffet process. In NIPS 18,
2006.
[8] D. G. Lowe. Distinctive image features from scale-invariant keypoints. International Journal of Computer
Vision, 60(2):91?110, 2004.
[9] R. M. Neal. Bayesian mixture modeling. In 11th International Workshop on Maximum Entropy and
Bayesian Methods of Statistical Analysis, pages 197?211, 1992.
[10] M. A. Newton and A. E Raftery. Approximate Bayesian inference by the weighted likelihood bootstrap.
Journal of the Royal Statistical Society, Series B, 3:3?48, 1994.
[11] E. Saund. A multiple cause mixture model for unsupervised learning. Neural Computation, 7:51?71,
1995.
[12] E. Sudderth, A. Torralba, W. T. Freeman, and A. Willsky. Describing Visual Scenes using Transformed
Dirichlet Processes. In NIPS 18, 2006.
3

available from http://l2r.cs.uiuc.edu/?cogcomp/Data/Car/.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 779-address-block-location-with-a-neural-net-system.pdf

Address Block Location with a Neural Net System

Eric Cosatto

Hans Peter Graf
AT&T Bell Laboratories
Crawfords Corner Road
Holmdel, NJ 07733, USA

Abstract
We developed a system for finding address blocks on mail pieces that can
process four images per second. Besides locating the address block, our
system also determines the writing style, handwritten or machine printed, and
moreover, it measures the skew angle of the text lines and cleans noisy
images. A layout analysis of all the elements present in the image is
performed in order to distinguish drawings and dirt from text and to separate
text of advertisement from that of the destination address.
A speed of more than four images per second is obtained on a modular
hardware platform, containing a board with two of the NET32K neural net
chips, a SPARC2 processor board, and a board with 2 digital signal
processors. The system has been tested with more than 100,000 images. Its
performance depends on the quality of the images, and lies between 85%
correct location in very noisy images to over 98% in cleaner images.

1

INTRODUCTION

The system described here has been integrated into an address reading machine developed for
the 'Remote Computer Reader' project of the United States Postal Service. While the actual
reading of the text is done by other modules, this system solves one of the major problems,
namely, finding reliably the location of the destination address. There are only a few constraints
on how and where an address has to be written, hence they may appear in a wide variety of
styles and layouts. Often an envelope contains advertising that includes images as well as text.

785

786

Graf and Cosatto

Sometimes. dirt covers part of the envelope image. including the destination address. Moreover.
the image captured by the camera is thresholded and the reader is given a binary image. This
binarization process introduces additional distortions; in particular. often the destination address
is surrounded by a heavy texture. The high complexity of the images and their poor quality make
it difficult to find the location of the destination address. requiring an analysis of all the elements
present in the image. Such an analysis is compute-intensive and in our system it turned out to
be the major bottleneck for a fast throughput. In fact. finding the address requires much more
computation than reading it. Special-purpose hardware in the form of the NET32K neural net
chips (Graf. Henderson. 90) is used to solve the address location problem.
Finding address blocks has been the focus of intensive research recently. as several companies
are developing address reading machines (United States Postal Service 92). The wide variety
of images that have to be handled has led other researchers to apply several different analysis
techniques to each image and then try to combine the results at the end. see e.g. (palumbo et a1.
92). In order to achieve the throughput required in an industrial application. special purpose
processors for finding connected components and/or for executing Hough transforms have been
applied.

In our system we use the NET32K processor to extract geometrical features from an image. The
high compute power of this chip allows the extraction of a large number of features
simultaneously. From this feature representation. an interpretation of the image's content can
then be achieved with a standard processor. Compared to an analysis of the original image. the
analysis of the feature maps requires several orders of magnitude less computation. Moreover.
the feature representation introduces a high level of robustness against noise. This paper gives
a brief overview of the hardware platfOlm in section 2 and then describes the algorithms to find
the address blocks in section 3.

2 THE HARDWARE
The NET32K system has been designed to serve as a high-speed image processing platform.
where neural nets as well as conventional algorithms can be executed. Three boards form the
whole system. Two NET32K neural net chips are integrated with a sequencer and data
formatting circuits on one board. The second board contains two digital signal processors
(DSPs). together with 6 Mbytes of memOly. Control of the whole system is provided by a board
containing a SPARC2 processor plus 64 Mbytes of memory. A schematic of this system is
shown in Figure 1.
Image buffering and communication with other modules in the address reader are handled by
the board with the SPARC2 processor. When an image is received. it is sent to the DSP board
and from there over to the NET32K processor. The feature maps produced by the NET32K
processor are stored on the DSP board. while the SPARC2 starts with the analysis of the feature
maps. The DSP's main task is formatting of the data. while the NET32K processor extracts all
the features. Its speed of computation is more than 100 billion multiply-accumulates per second
with operands that have one or two bits of resolution. Images with a size of Sl2xS 12 pixels are
processed at a rate of more than 10 frames per second. and 64 convolution kernels. each with
a size of 16x 16 pixels. can be scanned simultaneously over the image. Each such kernel IS
tuned to detect the presence of a feature. such as a line, an edge or a comer.

Address Block Location with a Neural Net System

....................................................................................

!

IN~K IN~
i

NET32K MODULE

r1~?~?~?:?A?T..

::::.fr=:::::::.~~~::=::::.~~=.=:::.=n:::=::
. . ._._. . . _. . .
v.
.

r ........-....-.-.......-.....

:

I

!

?

I

~

~

It

~

~

"

">

~

DSP32C

SRAM
1 MEG

~~
~"

"

~r

).

~lt

Afr

~U'I

~

DSP32C

DRAM
4 MEG

~ .... '1

'1

SRAM
1 MEG

l-.. . .- . _. .+. ._. .

L.. . ~~~. ~.~.~.~~.,. . -.-... ---.-..-..
~

. . . . ..l

:;:~

________________________

~.

SPARC

VME BUS

Figure 1: Schematic of the whole NET32K system. Each of the dashed
boxes represents one 6U VME board. The
conununication paths.

aITOWS

show the

3. SEQUENCE OF ALGORITHMS
The final result of the address block location system is a box describing a tight bmmd around
the destination address, if the address is machine printed. Of handwritten addresses, only the
zip code is read, and hence, one has to find a tight boundary around the zip code. This
information is then passed along to reader modules of the address reading machine. There is no
a priori knowledge about the writing style. Therefore the system first has to discriminate
between handwritten and machine Plinted text. At the end of the address block location process,
additional algorithms are executed to improve the accuracy of the reader. An overview of the
sequence of algorithms used to solve these tasks is shown in Figure 2. The whole process is
divided into three major steps: Preprocessing, feature extraction. and high-level analysis based
on the feature information.

3.1. Preprocessing
To quickly get an idea about the complexity of the image, a coarse evaluation of its layout is
done. By sampling the density of the black pixels in various places of the image, one can see
already whether the image is clean or noisy and whether the text is lightly printed or is dark.

787

788

Oraf and Cosatto

The images are divided into four categories, depending on their darkness and the level of noise.
'This infonnation is used in the subsequent processing to guide the choice of the features. Only
about one percent of the pixels are taken into account for this analysis, therefore, it can be
executed quickly on the SPARC2 processor.
clean. light

Preprocessing

clean. dark

IF.
~ ----.
....-=.... =-.P

~

-

~.=

::~,

16 Feature
maps

'

,,'

.....

-I'"

Extract features
NET32K

8 Feature
maps

Extract text lines
Cluster lines into groups
- - - Classify groups of lines
MACHINE PRINT
Analyse group of lines
Determine level of noise
Clean with NET32K;

HANDWRITIEN
Cluster text segments into lines
Analyse group of lines
Segment lines to find ZIP
Determine slanVskew angle;

Figure 2: Schematic of the sequence of algorithms for finding the
position of the address blocks.
3.2. Feature Extraction
After the preprocessing, the image is sent to the NET32K board where simple geometrical
features, such as edges, corners and lines are extracted. Up to 16 different feature maps are
generated, where a pixel in one of the maps indicates the presence of a feature in this location.
Some of these feature maps are used by the host processor, for example, to decide whether text
is handwritten or machine printed. Other feature maps are combined and sent once more
through the NET32K processor in order to search for combinations of features representing
more complex features. Typically, the feature maps are thresholded, so that only one bit per
pixel is kept. More resolution of the computation results is available from the neural net chips.
but in this way the amount of data that has to be analyzed is minimal. and one bit of resolution
turned out to be sufficient.
Examples of kernels used for the detection of strokes and text lines are shown in Figure 3. In
the chip, usually four line detectors of increasing height plus eight stroke detectors of different
orientations are stored. Other detectors are tuned to edges and strokes of machine printed text.
The line detectors respond to any black line of the proper height. Due to the large width of 16

Address Block Location with a Neural Net System

pixels. a kernel stretches over one or even several characters. Hence a text line gives a response
similar to that produced by a continuous black line. When the threshold is set properly. a text
line in the original image produces a continuous line in the feature map. even across the gaps
between characters and across small empty spaces between words. For an interpretation of a
line feature map only the left and right end points of each connected component are stored. In
this way one obtains a compact representation of the lines' positions that are well suited for the
high-level analysis of the layout.
Kernel: Line detector

?

Image

t

the NET32K syste

IC::GUla

Feature

Kernel: Stroke detector

Feature map

Figure 3:Examples of convolution kernels and their results. The kernels' sizes
are 16x16 pixels, and their pixels' values are + 1, O. -1 . The upper part illustrates
the response of a line detector on a machine printed text line. The lower kernel
extracts strokes of a celtain orientation from handwritten text.
Handwritten lines are detected by a second technique, because they are more irregular in height
and the characters may be spaced apm1 widely. Detectors for strokes, of the type shown in the
lower half ofFigw-e 3. are well suited for sensing the presence of handwritten text. The feature
maps resulting form handwritten text tend to exhibit blobs of pixels along the text line. By
smearing such feature maps in horizontal direction the responses of individual strokes are
merged into lines that can then be used in the same way as described for the machine printed
lines.
Horizontal smearing of text lines. combined with connected component analysis is a well-known

789

790

Graf and Cosatto

technique, often applied in layout analysis, to find words and whole lines of text. But when
applied to the pixels of an image, such an approach works well only in clean images. As soon
as there is noise present, this technique produces ilTegular responses. The key to success in a
real world environment is robustness against noise. By extracting features first and then
analyzing the feature maps, we drastically reduce the influence of noise. Each of the convolution
kernels covers a range of 256 pixels and its response depends on several dozens of pixels inside
this area. If pixels in the image are corrupted by noise, this has only a minor effect on the result
of the convolution and, hence, the appearance of the feature map.
When the analysis is started, it is unknown, whether the address is machine printed or hand
written. In order to distinguish between the two writing styles, a simple one-layer classifier
looks at the results of four stroke detectors and of four line detectors. It can determine reliably
whether text is handwritten or machine printed. Additional useful information that can be
extracted easily from the feature maps, is the skew angle of handwritten text. People tend to
write with a skew anywhere from -45 degrees to almost +90 degrees. In order to improve the
accuracy of a reader, the text is first deskewed. The most time consuming part of this operation
is to determine the skew angle of the writing. The stroke detector with the maximum response
over a line is a good indicator of the skew angle of the text. We compared this simple technique
with several alternatives and found it to be as reliable as the best other algorithm and much
faster to compute.

3.3. High-level Analysis
The results of the feature extraction process are line segments, each one marked as handwritten
or machine printed. Only the left and right end points of such lines are stored. At this point,
there may still be line segments in this group that do not correspond to text, but rather to solid
black lines or to line drawings. Therefore each line segment is checked, to determine whether
the ratio of black and white pixels is that found typically in text.
Blocks of lines are identified by clustering the line segments into groups. Then each block is
analyzed, to see whether it can represent the destination address. For this purpose such features
as the number of lines in the block, its size, position, etc. are used. These features are entered
into a classifier that ranks each of the blocks. Certain conditions, such as a size that is too large,
or if there are too many text lines in the block, will lead to an attempt to split blocks. If no good
result is obtained, clustering is tried again with a changed distance metric, where the horizontal
and the vertical distances between lines are weighted differently.

If an address is machine printed, the whole address block is passed on to the reader, since not
only the zip code, but the whole address, including the city name, the street name and the name
of the recipient have to be read. A big problem for the reader present images of poor quality,
particularly those with background noise and texture. State-of-the-art readers handle machine
printed text reliably if the image quality is good, but they may fail totally if the text is buried in
noise. For that reason, an address block is cleaned before sending it to the reader. Feature
extraction with the NET32K board is used once more for this task, this time with detectors tuned
to find all the strokes of the machine printed text. Applying stroke detectors with the proper
width allows a good discrimination between the text and any noise. Even texture that consists
of lines can be rejected reliably, if the line thickness of the texture is not the same as that of the
text.

Address Block Location with a Neural Net System

.:

.

"3"" /"ksiQ \i~.\. Cal! [~

"

', '

~"S'~e".I ?

. ~..~

~

===t

,o;;;r;;;a.e;2 .

. : .

.....

t1r

????ee-5AT'fO??t;~a.?????

"'~;Au'j'':f;,:.)i'''\i?bl,..~~???~t .......
"?~?S\;.?\?.cs.",~?A'???""

.

-~.W"..-,\e"'..4*!~ _Q33.~2..:-

Figure 4: Example of an envelope image at various stages of the processing. Top: The
result of the clustering process to find the bounding box of the address. Bottom right: The
text lines within the address block are marked. Bottom left: Cuts in the text line with the
zip code and below that the result of the reader. (The zip code is actually the second
segment sent to the reader; the first one is the string 'USA').

If the address is handwritten, only the zip code is sent to the reader. In order to find the zip code,
an analysis of the internal stmcture of the address block has to be done, which starts with finding
the true text lines. Handwritten lines are often not straight, may be heavily skewed, and may
contain large gaps. Hence simple techniques, such as connected component analysis, do not
provide proper results. ClusteJing of the line segments obtained from the feature maps, provides
a reliable solution of this problem. Once the lines are found, each one is segmented into words
and some of them are selected as candidates for the zip code and are sent to the reader. Figure
4 shows an example of an envelope image as it progresses through the various processing steps.
The system has been tested extensively on overall more than 100,000 images. Most of these
tests were done in the assembled address reader, but during development of the system, large

791

792

Graf and Cosatto

tests were also done with the address location module alone. One of the problems for evaluating
the peIformance is the lack of an objective quality measure. When has an address been located
correctly? Cutting off a small part of the address may not be detrimental to the final
interpretation, while a bounding box that includes some additional text may slow the reader
down too much. or it may throw off the interpretation. Therefore, it is not always clear when a
bounding box, describing the address' location, is tight enough. Another important factor
affecting the accw-acy numbers is, how many candidate blocks one actually considers. For all
these reasons, accw-acy numbers given for address block location have to be taken with some
caution. The results mentioned here were obtained by judging the images by eye. If images are
clean and the address is surrounded by a white space larger than two line heights, the location
is found correctly in more than 98% of the cases. Often more than one text block is found and
of these the destination address is the first choice in 90% of the images, for a typical layout. If
the image is very noisy, which actually happens surprisingly often, a tight bound around the
address is found in 85% of the cases. These results were obtained with 5,000 images, chosen
from more than 100,000 images to represent as much variety as possible. Of these 5,000 images
more than 1,200 have a texture around the address, and often this texture is so dark that a
human has difficulties to make out each character.

4. CONCLUSION
Most of our algorithms described here consist of two parts: feature extraction implemented with
a convolution and interpretation, typically implemented with a small classifier. Surprisingly
many algorithms can be cast into such a fOimat. This common framework for algorithms has
the advantage of facilitating the implementation, in particular when algorithms are mapped into
hardware. Moreover, the feature extraction with large convolution kernels makes the system
robust against noise. This robustness is probably the biggest advantage of our approach. Most
existing automatic reading systems are very good as long as the images are clean, but they
deteriorate rapidly with decreasing image quality.

'The biggest drawback of convolutions is that they require a lot of computation. In fact, without
special purpose hardware, convolutions are often too slow. Our system relies on the NET32K
new-al net chips to obtain the necessary throughput. The NET32K system is, we believe, at the
moment the fastest board system for this type of computation. This speed is obtained by
systematically exploiting the fact that only a low resolution of the computation is required. This
allows to use analog computation inside the chip and hence much smaller circuits than would
be the case in an all-digital circuit.

References
United States Postal Service, (1992), Proc. Advanced Technology Conf., Vol. 3, Section on
address block location: pp. 1221 - 1310.
P.W. Palumbo, S.N. Srihari, J. Soh, R. Sridhar, V. Demjanenko, (1992), !'Postal Address Block
Location in Real Time", IEEE COMPUTER, Vol. 25n, pp. 34 - 42.
H.P. Oraf and D. Henderson, (1990), "A Reconfigurable CMOS Neural Network", Digest
IEEE Int. Solid State Circuits Conf. p. 144.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 2622-the-power-of-feature-clustering-an-application-to-object-detection.pdf

The power of feature clustering: An application
to object detection

Shai Avidan
Mitsibishi Electric Research Labs
201 Broadway
Cambridge, MA 02139
avidan@merl.com

Moshe Butman
Adyoron Intelligent Systems LTD.
34 Habarzel St.
Tel-Aviv, Israel
mosheb@adyoron.com

Abstract
We give a fast rejection scheme that is based on image segments and
demonstrate it on the canonical example of face detection. However, instead of focusing on the detection step we focus on the rejection step and
show that our method is simple and fast to be learned, thus making it
an excellent pre-processing step to accelerate standard machine learning
classifiers, such as neural-networks, Bayes classifiers or SVM. We decompose a collection of face images into regions of pixels with similar
behavior over the image set. The relationships between the mean and
variance of image segments are used to form a cascade of rejectors that
can reject over 99.8% of image patches, thus only a small fraction of the
image patches must be passed to a full-scale classifier. Moreover, the
training time for our method is much less than an hour, on a standard PC.
The shape of the features (i.e. image segments) we use is data-driven,
they are very cheap to compute and they form a very low dimensional
feature space in which exhaustive search for the best features is tractable.

1

Introduction

This work is motivated by recent advances in object detection algorithms that use a cascade
of rejectors to quickly detect objects in images. Instead of using a full fledged classifier on
every image patch, a sequence of increasingly more complex rejectors is applied. Nonface image patches will be rejected early on in the cascade, while face image patches will
survive the entire cascade and will be marked as a face.
The work of Viola & Jones [15] demonstrated the advantages of such an approach. Other
researchers suggested similar methods [4, 6, 12]. Common to all these methods is the
realization that simple and fast classifiers are enough to reject large portions of the image, leaving more time to use more sophisticated, and time consuming, classifiers on the
remaining regions of the image.
All these ?fast? methods must address three issues. First, is the feature space in which to
work, second is a fast method to calculate the features from the raw image data and third is
the feature selection algorithm to use.
Early attempts assumed the feature space to be the space of pixel values. Elad et al. [4]

suggest the maximum rejection criteria that chooses rejectors that maximize the rejection
rate of each classifier. Keren et al. [6] use anti-face detectors by assuming normal distribution on the background. A different approach was suggested by Romdhani et al. [12],
that constructed the full SVM classifier first and then approximated it with a sequence or
support vector rejectors that were calculated using non-linear optimization. All the above
mentioned method need to ?touch? every pixel in an image patch at least once before they
can reject the image patch.
Viola & Jones [15], on the other hand, construct a huge feature space that consists of
combined box regions that can be quickly computed from the raw pixel data using the
?integral image? and use a sequential feature selection algorithm for feature selection. The
rejectors are combined using a variant of AdaBoost [2]. Li et al [7] replaced the sequential
forward searching algorithm with a float search algorithm (which can backtrack as well).
An important advantage of the huge feature space advocated by Viola & Jones is that now
image patches can be rejected with an extremely small number of operations and there is
no need to ?touch? every pixel in the image patch at least once.
Many of these methods focus on developing fast classifiers that are often constructed in a
greedy manner. This precludes classifiers that might demonstrate excellent classification
results but are slower to compute, such as the methods suggested by Schneiderman et al.
[8], Rowley et al. [13], Sung and Poggio [10] or Heisele et al [5].
Our method offers a way to accelerate ?slow? classification methods by using a preprocessing rejection step. Our rejection scheme is fast to be trained and very effective
in rejecting the vast majority of false patterns. On the canonical face detection example, it
took our method much less than an hour to train and it was able to reject over 99.8% of the
image patches, meaning that we can effectively accelerate standard classifiers by several
orders of magnitude, without changing the classifier at all.
Like other, ?fast?, methods we use a cascade of rejectors, but we use a different type of
filters and a different type of feature selection method. We take our features to be the
approximated mean and variance of image segments, where every image segment consists
of pixels that have similar behavior across the entire image set. As a result, our features
are derived from the data and do not have to be hand crafted for the particular object of
interest. In fact they do not even have to form contiguous regions. We use only a small
number of representative pixels to calculate the approximated mean and variance, which
makes our features very fast to compute during detection (in our experiments we found that
our first rejector rejects almost 50% of all image patches, using just 8 pixels). Finally, the
number of segments we use is quite small which makes it possible to exhaustively calculate
all possible rejectors based on single, pairs and triplets of segments in order to find the best
rejectors in every step of the cascade. This is in contrast to methods that construct a huge
feature bank and use a greedy feature selection algorithm to choose ?good? features from
it. Taken together, our algorithm is fast to train and fast to test. In our experiments we train
on a database that contains several thousands of face images and roughly half-a-million
non-faces in less than an hour on an average PC and our rejection module runs at several
frames per second.

2

Algorithm

At the core of our algorithm is the realization that feature representation is a crucial ingredient in any classification system. For instance, the Viola-Jones box filters are extremely efficient to compute using the ?integral image? but they form a large feature space, thus placing
a heavy computational burden on the feature selection algorithm that follows. Moreover,
empirically they show that the first feature selected by their method correspond to meaningful regions in the face. This suggests that it might be better to focus on features that

correspond to coherent regions in the image. This leads to the idea of image segmentation,
that breaks an ensemble of images into regions of pixels that exhibit similar temporal behavior. Given the image segmentation we take our features to be the mean and variance of
each segment, giving us a very small feature space to work on (we chose to segment the
face image into eight segments). Unfortunately, calculating the mean and variance of an
image segment requires going over all the pixels in the segment, a time consuming process. However, since the segments represent similar-behaving pixels we found that we can
approximate the calculation of the mean and variance of the entire segment using quite a
small number of representative pixels. In our experiments, four pixels were enough to adequately represent segments that contain several tens of pixels. Now that we have a very
small feature space to work with, and a fast way to extract features from raw pixels data
we can exhaustively search for all possible combinations of single, pairs or triplets of features to find the best rejector in every stage. The remaining patterns should be passed to a
standard classifier for final validation.
2.1

Image Segments

Image segments were already presented in the past [1] for the problem of classification of
objects such as faces or vehicles. We briefly repeat the presentation for the paper to be
self-contained. An ensemble of scaled, cropped and aligned images of a given object (say
faces) can be approximated by its leading principal components. This is done by stacking
the images (in vector form) in a design matrix A and taking the leading eigenvectors of the
covariance matrix C = N1 AAT , where N is the number of images. The leading principal
components are the leading eigenvectors of the covariance matrix C and they form a basis
that approximates the space of all the columns of the design matrix A [11, 9]. But instead
of looking at the columns of A look at the rows of A. Each row in A gives the intensity
profile of a particular pixel, i.e., each row represents the intensity values that a particular
pixel takes in the different images in the ensemble. If two pixels come from the same
region of the face they are likely to have the same intensity values and hence have a strong
temporal correlation. We wish to find this correlations and segment the image plane into
regions of pixels that have similar temporal behavior. This approach broadly falls under
the category of Factor Analysis [3] that seeks to find a low-dimensional representation that
captures the correlations between features.
Let Ax be the x-th row of the design matrix A. Then Ax is the intensity profile of pixel x
(We address pixels with a single number because the images are represented in a scan-line
vector form). That is, Ax is an N -dimensional vector (where N is the number of images)
that holds the intensity values of pixel x in each image in the ensemble. Pixels x and y
are temporally correlated if the dot product of rows Ax and Ay is approaching 1 and are
temporally uncorrelated if the dot-product is approaching 0.
Thus, to find temporally correlated pixels all we need to do is run a clustering algorithm
on the rows of the design matrix A. In particular, we used the k-means algorithm on the
rows of the matrix A but any method of Factor Analysis can be used. As a result, the
image-plane is segmented into several (possibly non-continuous) segments of temporally
correlated pixels. Experiments in the past [1] showed good classification results on different
objects such as faces and vehicles.
2.2

Finding Representative Pixels

Our algorithm works by comparing the mean and variance properties of one or more image
segments. Unfortunately this requires touching every pixel in the image segment during
test time, thus slowing the classification process considerably. Therefor, during train time
we find a set of representative pixels that will be used during test time. Specifically, we
approximate every segment in a face image with a small number of representative pixels

Face segments

2

4

6

8

10

12

14

16

18

20
2

4

6

8

10

12

14

16

18

20

(a)
(b)
Figure 1: Face segmentation and representative pixels. (a) Face segmentation and representative pixels. The face segmentation was computed using 1400 faces, each segment is
marked with a different color and the segments need not be contiguous. The crosses overlaid on the segments mark the representative pixels that were automatically selected by
our method. (b) Histogram of the difference between an approximated mean and the exact
mean of a particular segment (the light blue segment on the left). The histogram is peaked
at zero, meaning that the representative pixels give a good approximation.
that approximate the mean and variance of the entire image segment. Define ? i (xj ) to be
the true mean of segment i of face j, and let ?
? i (xj ) be its approximation, defined as
Pk
j=1 xj
?
?i (xj ) =
k
where {xj }kj=1 are a subset of pixels in segment i of pattern j. We use a greedy algorithm
that incrementally searches for the next representative pixel that minimize
n
X

(?
?i (xj )) ? ?i (xj ))2

j=1

and add it to the collection of representative pixels of segment i. In practice we use four
representative pixels per segment. The representative pixels computed this way are used
for computing both the approximated mean and the approximated variance of every test
pattern. Figure 1 show how well this approximation works in practice.
Given the representative pixels, the approximated variance ?
? i (xj ) of segment i of pattern j
is given by:
k
X
?
?i (xj ) =
|xj ? ?
?i (xj )|
j=1

2.3

The rejection cascade

We construct a rejection cascade that can quickly reject image patches, with minimal computational load. Our feature space consist of the approximated mean and variance of the
image segments. In our experiments we have 8 segments, each represented by its mean and
variance, giving rise to a 16D feature space. This feature space is very fast to compute, as
we need only four pixels to calculate the approximate mean and variance of the segment.
Because the feature space is so small we can exhaustively search for all classifiers on single,
pairs and triplets of segments. In addition this feature space gives enough information to
reject texture-less regions without the need to normalize the mean or variance of the entire
image patch. We next describe our rejectors in detail.

2.3.1

Feature rejectors

Now, that we have segmented every image into several segments and approximated every
segment with a small number of representative pixels, we can exhaustively search for the
best combination of segments that will reject the largest number of non-face images. We
repeat this process until the improvement in rejection is negligible.
Given a training set of P positive examples (i.e. faces) and N negative examples we construct the following linear rejectors and adjust the parameter ? so that they will correctly
classify d ? P (we use d = 0.95) of the face images and save r, the number of negative
examples they correctly rejected, as well as the parameter ?.
1. For each segment i, find a bound on its approximated mean. Formally, find ? s.t.
?
?i (x) > ? or ?
?i (x) < ?
2. For each segment i, find a bound on its approximated variance. Formally, find ?
s.t.
?
?i (x) > ? or ?
?i (x) < ?
3. For each pair of segments i, j, find a bound on the difference between their approximated means. Formally, find ? s.t.
?
?i (x) ? ?
?j (x) > ? or ?
?i (x) ? ?
?j (x) < ?
4. For each pair of segments i, j, find a bound on the difference between their approximated variance. Formally, find ? s.t.
?
?i (x) ? ?
?j (x) > ? or ?
?i (x) ? ?
?j (x) < ?
5. For each triplet of segments i, j, k find a bound on the difference of the absolute
difference of their approximated means. Formally, find ? s.t.
|?
?i (x) ? ?
?j (x)| ? |?
?i (x) ? ?
?k (x)| > ?
This process is done only once to form a pool of rejectors. We do not re-train rejectors after
selecting a particular rejector.
2.3.2

Training

We form the cascade of rejectors from a large pattern vs. rejector binary table T, where
each entry T(i, j) is 1 if rejector j rejects pattern i. Because the table is binary we can
store every entry in a single bit and therefor a table of 513, 000 patterns and 664 rejectors
can easily fit in the memory. We then use a greedy algorithm to pick the next rejector with
the highest rejection score r. We repeat this process until r falls below some predefined
threshold.
1. Sum each column and choose column (rejector) j with the highest sum.
2. For each entry T (i, j), in column j, that is equal to one, zero row i.
3. Go to step 1
The entire process is extremely fast and takes only several minutes, including I/O. The idea
of creating a rejector pool in advance was independently suggested by [16] to accelerate
the Viola-Jones training time. We obtain 50 rejectors using this method. Figure 2a shows
the rejection rate of this cascade on a training set of 513, 000 images, as well as the number
of arithmetic operations it takes. Note that roughly 50% of all patterns are rejected by the
first rejector using only 12 operations. During testing we compute the approximated mean
and variance only when they are needed and not before hand.

Comparing different image segmentations

Rejection rate
90

90

85

80

80

70

75
rejection rate

rejection rate

60

70

65

50

40

60
30

55
random segments
vertical segments
horizontal segments
image segments

20

50

45

10

0

50

100
150
number of operations

200

250

0

5

10

15

20

25

number of rejectors

(a)
(b)
Figure 2: (a) Rejection rate on training set. The x-axis counts the number of arithmetic
operations needed for rejection. The y-axis is the rejection rate on a training set of about
half-a-million non-faces and about 1500 faces. Note that almost 50% of the false patterns
are rejected with just 12 operations. Overall rejection rate of the feature rejectors on the
training set is 88%, it drops to about 80% on the CMU+MIT database. (b) Rejection rate
as a function of image segmentation method. We trained our system using four types of
image segmentation and show the rejector. We compare our image segmentation approach
against naive segmentation of the image plane into horizontal blocks, vertical blocks or
random segmentation. In each case we trained a cascade of 21 rejectors and calculated their
accumulative rejection rate on our training set. Clearly working with our image segments
gives the best results.
We wanted to confirm our intuition that indeed only meaningful regions in the image can
produce such results and we therefor performed the following experiment. We segmented
the pixels in the image using four different methods. (1) using our image segments (2)
into 8 horizontal blocks (3) into 8 vertical blocks (4) into 8 randomly generated segments.
Figure 2b show that image segments gives the best results, by far.
The remaining false positive patterns are passed on to the next rejectors, as described next.
2.4

Texture-less region rejection

We found that the feature rejectors defined in the previous section are doing poorly in
rejecting texture-less regions. This is because we do not perform any sort of variance
normalization on the image patch, a step that will slow us down. However, by now we
have computed the approximated mean and variance of all the image segments and we
can construct rejectors based on all of them to reject texture-less regions. In particular we
construct the following two rejectors
1. Reject all image patches where the variance of all 8 approximated means falls
below a threshold. Formally, find ? s.t.
?
? (?
?i (x)) < ? i = 1...8
2. Reject all image patches where the variance of all 8 approximated variances falls
below a threshold. Formally, find ? s.t.
?
? (?
?i (x)) < ? i = 1...8
2.5

Linear classifier

Finally, we construct a cascade of 10 linear rejectors, using all 16 features (i.e. the approximated means and variance of all 8 segments).

(a)
(b)
Figure 3: Examples. We show examples from the CMU+MIT dataset. Our method correctly rejected over 99.8% of the image patches in the image, leaving only a handful of
image patches to be tested by a ?slow?, full scale classifier.
2.6

Multi-detection heuristic

As noted by previous authors [15] face classifiers are insensitive to small changes in position and scale and therefor we adopt the heuristic that only four overlapping detections are
declared a face. This help reduce the number of detected rectangles around and face, as
well as reject some spurious false detections.

3

Experiments

We have tested our rejection scheme on the standard CMU+MIT database [13]. We created
a pyramid at increasing scales of 1.1 and scanned every scale for rectangles of size 20 ? 20
in jumps of two pixels. We calculate the approximated mean and variance only when they
are needed, to save time.
Overall, our rejection scheme rejected over 99.8% of the image patches, while correctly detecting 93% of the faces. On average the feature rejectors rejected roughly 80% of all image
patches, the textureless region rejectors rejected additional 10% of the image patches, the
linear rejectors rejected additional 5% and the multi-detection heuristic rejected the remaining image patterns. The average rejection rate per image is over 99.8%. This is not enough
for face detection, as there are roughly 615, 000 image patches per image in the CMU+MIT
database, and our rejector cascade passes, on average, 870 false positive image patches, per
image. This patterns will have to be passed to a full-scale classifier to be properly rejected.
Figure 3 give some examples of our system. Note that the system correctly detects all the
faces, while allowing a small number of false positives.
We have also experimented with rescaling the features, instead of rescaling the image, but
noted that the number of false positives increased by about 5% for every fixed detection
rate we tried (All the results reported here use image pyramids).

4

Summary and Conclusions

We presented a fast rejection scheme that is based on image segments and demonstrated it
on the canonical example of face detection. Image segments are made of regions of pixels
with similar behavior over the image set. The shape of the features (i.e. image segments)
we use is data-driven and they are very cheap to compute The relationships between the
mean and variance of image segments are used to form a cascade of rejectors that can reject
over 99.8% of the image patches, thus only a small fraction of the image patches must be

passed to a full-scale classifier. The training time for our method is much less than an hour,
on a standard PC. We believe that our method can be used to accelerate standard machine
learning algorithms that are too slow for object detection, by serving as a gate keeper that
rejects most of the false patterns.

References
[1] Shai Avidan. EigenSegments: A spatio-temporal decomposition of an ensemble of
image. In European Conference on Computer Vision (ECCV), May 2002, Copenhagen,
Denmark.
[2] Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line
learning and an application to boosting. In Computational Learning Theory: Eurocolt
95, pages 2337. Springer-Verlag, 1995.
[3] R. O. Duda and P. E. Hart. Pattern Classification and Scene Analysis. WileyInterscience publication, 1973.
[4] M. Elad, Y. Hel-Or and R. Keshet. Rejection based classifier for face detection. Pattern
Recognition Letters 23 (2002) 1459-1471.
[5] B. Heisele, T. Serre, S. Mukherjee, and T. Poggio. Feature reduction and hierarchy of
classifiers for fast object detection in video images. In Proc. CVPR, volume 2, pages
1824, 2001.
[6] D. Keren, M. Osadchy, and C. Gotsman. Antifaces: A novel, fast method for image
detection. IEEE Trans. on Pattern Analysis and Machine Intelligence, 23(7):747761,
2001.
[7] S.Z. Li, L. Zhu, Z.Q. Zhang, A. Blake, H.J. Zhang and H. Shum. Statistical Learning of Multi-View Face Detection. In Proceedings of the 7th European Conference on
Computer Vision, Copenhagen, Denmark, May 2002.
[8] Henry Schneiderman and Takeo Kanade. A statistical model for 3d object detection
applied to faces and cars. In IEEE Conference on Computer Vision and Pattern Recognition. IEEE, June 2000.
[9] L. Sirovich and M. Kirby. Low-dimensional procedure for the characterization of human faces. In Journal of the Optical Society of America 4, 510-524.
[10] K.-K. Sung and T. Poggio. Example-based Learning for View-Based Human Face Detection. In IEEE Transactions on Pattern Analysis and Machine Intelligence 20(1):3951, 1998.
[11] M. Turk and A. Pentland. Eigenfaces for recognition. In Journal of Cognitive Neuroscience, vol. 3, no. 1, 1991.
[12] S. Romdhani, P. Torr, B. Schoelkopf, and A. Blake. Computationally efficient face
detection. In Proc. Intl. Conf. Computer Vision, pages 695700, 2001.
[13] H. A. Rowley, S. Baluja, and T. Kanade. Neural network-based face detection. IEEE
Trans. on Pattern Analysis and Machine Intelligence, 20(1):2338, 1998.
[14] V. Vapnik. The Nature of Statistical Learning Theory. Springer, N.Y., 1995.
[15] P. Viola and M. Jones. Rapid Object Detection using a Boosted Cascade of Simple
Features. In IEEE Conference on Computer Vision and Pattern Recognition, Hawaii,
2001.
[16] J. Wu, J. M. Rehg, and M. D. Mullin. Learning a Rare Event Detection Cascade
by Direct Feature Selection. To appear in Advances in Neural Information Processing
Systems 16 (NIPS*2003), MIT Press, 2004.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 2409-a-mixed-signal-vlsi-for-real-time-generation-of-edge-based-image-vectors.pdf

A Mixed-Signal VLSI for Real-Time
Generation of Edge-Based Image Vectors

Masakazu Yagi, Hideo Yamasaki, and Tadashi Shibata*
Department of Electronic Engineering
*Department of Frontier Informatics
The University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo, 113-8656, Japan
mgoat@dent.osaka-u.ac.jp, hideo@if.t.u-tokyo.ac.jp, shibata@ee.t.u-tokyo.ac.jp

Abstract
A mixed-signal image filtering VLSI has been developed aiming at
real-time generation of edge-based image vectors for robust image
recognition. A four-stage asynchronous median detection architecture based on analog digital mixed-signal circuits has been introduced to determine the threshold value of edge detection, the key
processing parameter in vector generation. As a result, a fully
seamless pipeline processing from threshold detection to edge feature map generation has been established. A prototype chip was
designed in a 0.35-?m double-polysilicon three-metal-layer CMOS
technology and the concept was verified by the fabricated chip. The
chip generates a 64-dimension feature vector from a 64x64-pixel
gray scale image every 80?sec. This is about 104 times faster than the
software computation, making a real-time image recognition system
feasible.

1

In tro du c ti o n

The development of human-like image recognition systems is a key issue in information technology. However, a number of algorithms developed for robust image
recognition so far [1]-[3] are mostly implemented as software systems running on
general-purpose computers. Since the algorithms are generally complex and include a
lot of floating point operations, they are computationally too expensive to build
real-time systems. Development of hardware-friendly algorithms and their direct
VLSI implementation would be a promising solution for real-time response systems.
Being inspired by the biological principle that edge information is firstly detected in
the visual cortex, we have developed an edge-based image representation algorithm
compatible to hardware processing. In this algorithm, multiple-direction edges extracted from an original gray scale image is utilized to form a feature vector. Since the
spatial distribution of principal edges is represented by a vector, it was named Projected Principal-Edge Distribution (PPED) [4],[5], or formerly called Principal Axis

Projection (PAP) [6],[7]. (The algorithm is explained later.) Since the PPED vectors
very well represent the human perception of similarity among images, robust image
recognition systems have been developed using PPED vectors in conjunction with the
analog soft pattern classifier [4],[8], the digital VQ (Vector Quantization) processor
[9], and support vector machines [10] .
The robust nature of PPED representation is demonstrated in Fig. 1, where the system
was applied to cephalometric landmark identification (identifying specific anatomical
landmarks on medical radiographs) as an example, one of the most important clinical
practices of expert dentists in orthodontics [6],[7]. Typical X-ray images to be experienced by apprentice doctors were converted to PPED vectors and utilized as
templates for vector matching. The system performance has been proven for 250 head
film samples regarding the fundamental 26 landmarks [11]. Important to note is the
successful detection of the landmark on the soft tissue boundary (the tip of the lower
lip) shown in Fig. 1(c). Landmarks on soft tissues are very difficult to detect as
compared to landmarks on hard tissues (solid bones) because only faint images are
captured on radiographs. The successful detection is due to the median algorithm that
determines the threshold value for edge detection.

Sella

Nasion

Orbitale

By our system
(a)

By expert dentists

Landmark on soft tissue

(b)

(c)

Fig. 1: Image recognition using PPED vectors: (a,b) cephalometric landmark identification; (c) successful landmark detection on soft tissue.
We have adopted the median value of spatial variance of luminance within the filtering kernel (5x5 pixels), which allows us to extract all essential features in a delicate
gray scale image. However, the problem is the high computational cost in determining
the median value. It takes about 0.6 sec to generate one PPED vector from a
64x64-pixel image (a standard image size for recognition in our system) on a SUN
workstation, making real time processing unrealistic. About 90% of the computation
time is for edge detection from an input image, in which most of the time is spent for
median detection.
Then the purpose of this work is to develop a new architecture median-filter VLSI
subsystem for real-time PPED-vector generation. Special attention has been paid to
realize a fully seamless pipeline processing from threshold detection to edge feature
map generation by employing the four-stage asynchronous median detection architecture.

2

P r o je c t e d P r i n c i pa l E dg e Dis tribution (PPED )

Projected Principal Edge Distribution (PPED) algorithm [5],[6] is briefly explained
using Fig. 2(a). A 5x5-pixel block taken from a 64x64-pixel target image is subjected
to edge detection filtering in four principal directions, i.e. horizontal, vertical, and
?45-degree directions. In the figure, horizontal edge filtering is shown as an example.
(The filtering kernels used for edge detection are given in Fig. 2(b).) In order to determine the threshold value for edge detection, all the absolute-value differences
between two neighboring pixels are calculated in both vertical and horizontal directions and the median value is taken as the threshold. By scanning the 5x5-pixel filtering kernels in the target image, four 64x64 edge-flag maps are generated, which are
called feature maps. In the horizontal feature map, for example, edge flags in every
four rows are accumulated and spatial distribution of edge flags are represented by a
histogram having 16 elements. Similar procedures are applied to other three directions
to form respective histograms each having 16 elements. Finally, a 64-dimension
vector is formed by series-connecting the four histograms in the order of horizontal,
+45-degree, vertical, and ?45-degree.

64x64

Feature Map (64x64)

(Horizontal)
0 0 0 0 0
1 1 1 1 1
0 0 0 0 0
-1-1-1-1-1
0 0 0 0 0

(Horizontal)

Threshold
||
Median

Scan

(16 elements)

Edge Detection

Edge Filter

PPED Vector
(Horizontal Section)

0 0 0 0 0
1 1 1 1 1
0 0 0 0 0
-1 -1 -1 -1 -1
0 0 0 0 0

0 0 0 1 0
0 1 1 0 -1
0 1 0 -1 0
1 0 -1 -1 0
0 -1 0 0 0

Horizontal

+45-degree

0
0
0
0
0

Threshold Detection
Absolute value
difference between
neiboring pels.

1
1
1
1
1

0 -1
0 -1
0 -1
0 -1
0 -1

0
0
0
0
0

0 -1 0 0 0
1 0 -1 -1 0
0 1 0 -1 0
0 1 1 0 -1
0 0 0 1 0

Vertical

(a)

-45-degree

(b)

Fig. 2: PPED algorithm (a) and filtering kernels for edge detection (b).

3

Sy stem Orga ni za ti o n

The system organization of the feature map generation VLSI is illustrated in Fig. 3.
The system receives one column of data (8-b x 5 pixels) at each clock and stores the
data in the last column of the 5x6 image buffer. The image buffer shifts all the stored
data to the right at every clock. Before the edge filtering circuit (EFC) starts detecting
four direction edges with respect to the center pixel in the 5x5 block, the threshold
value calculated from all the pixel data in the 5x5 block must be ready in time for the
processing. In order to keep the coherence of the threshold detection and the edge
filtering processing, the two last-in data locating at column 5 and 6 are given to median filter circuit (MFC) in advance via absolute value circuit (AVC). AVC calculates
all luminance differences between two neighboring pixels in columns 5 and 6.
In this manner, a fully seamless pipeline processing from threshold detection to edge
feature map generation has been established. The key requirement here is that MFC
must determine the median value of the 40 luminance difference data from the
5x5-pixel block fast enough to carry out the seamless pipeline processing. For this
purpose, a four-stage asynchronous median detection architecture has been developed
which is explained in the following.

Edge Filtering Circuit (EFC)

6 5 4 3 2 1

Edge flags

H
+45
V

Image buffer

8-b x 5 pixels
(One column)

Absolute Value
Circuit (AVC)

Threshold
value

Median Filter
Circuit (MFC)

-45
Feature maps

Fig. 3: System organization of feature map generation VLSI.
The well-known binary search algorithm was adopted for fast execution of median
detection. The median search processing for five 4-b data is illustrated in Fig. 4 for the
purpose of explanation. In the beginning, majority voting is carried out for the MSB?s
of all data. Namely, the number of 1?s is compared with the number of 0?s and the
majority group wins. The majority group flag (?0? in this example) is stored as the
MSB of the median value. In addition, the loser group is withdrawn in the following
voting by changing all remaining bits to the loser MSB (?1? in this example). By
repeating the processing, the median value is finally stored in the median value register.
Elapse of time
Median Register :

0 1 X X

0 1 1 0

0 0 1 1

1
0 0 1 1

1
0 0 0 0

0
0 0 0 0

1 1 0 1

1 1 1 1

1 1 1 1

1 1 1 1

0 1 1 0

0 1 1 0

0 1 1 0

0 1 1 0

0 1 0 1

0 1 0 1

0 1 0 1

0 1 0 0

1 0 1 1

1 1 1 1

1 1 1 1

1 1 1 1

MVC0
MVC1
MVC2
MVC3

MVC0
MVC1
MVC2
MVC3

MVC0
MVC1
MVC2
MVC3

MVC0
MVC1
MVC2
MVC3

Majority Flag : 0

0 X X X

Majority Voting Circuit (MVC)

Fig. 4: Hardware algorithm for median detection by binary search.
How the median value is detected from all the 40 8-b data (20 horizontal luminance
difference data and 20 vertical luminance difference data) is illustrated in Fig. 5. All
the data are stored in the array of median detection units (MDU?s). At each clock, the
array receives four vertical luminance difference data and five horizontal luminance
difference data calculated from the data in column 5 and 6 in Fig. 3. The entire data are
shifted downward at each clock. The median search is carried out for the upper four
bits and the lower four bits separately in order to enhance the throughput by pipelining.
For this purpose, the chip is equipped with eight majority voting circuits (MVC 0~7).
The upper four bits from all the data are processed by MVC 4~7 in a single clock cycle
to yield the median value. In the next clock cycle, the loser information is transferred
to the lower four bits within each MDU and MVC0~3 carry out the median search for
the lower four bits from all the data in the array.

Vertical Luminance Difference

AVC AVC AVC AVC

Horizontal Luminance Difference

AVC AVC AVC AVC AVC
Shift

Shift

Median Detection Unit (MDU)
x (40 Units)
Lower 4bit

Upper 4bit

MVC0

MVC2

MVC1

MVC3

MVC4

MVC5

MVC6

MVC7

MVCs for upper 4bit

MVCs for lower 4bit

Fig. 5: Median detection architecture for all 40 luminance difference data.
The majority voting circuit (MVC) is shown in Fig. 6. Output connected CMOS inverters are employed as preamplifiers for majority detection which was first proposed
in Ref. [12]. In the present implementation, however, two preamps receiving input
data and inverted input data are connected to a 2-stage differential amplifier. Although this doubles the area penalty, the instability in the threshold for majority
detection due to process and temperature variations has been remarkably improved as
compared to the single inverter thresholding in Ref. [12]. The MVC in Fig. 6 has 41
input terminals although 40 bits of data are inputted to the circuit at one time. Bit ?0?
is always given to the terminal IN40 to yield ?0? as the majority when there is a tie in
the majority voting.

PREAMP
IN0

PREAMP
2W/L

IN0

2W/L

OUT
W/L

ENBL

W/L
W/L

IN1

IN1

2W/L

2W/L

W/L

ENBL
IN40

W/L
W/L

IN40

Fig. 6: Majority voting circuit (MVC).
The edge filtering circuit (EFC) in Fig. 3 is composed as a four-stage pipeline of
regular CMOS digital logic. In the first two stages, four-direction edge gradients are
computed, and in the succeeding two stages, the detection of the largest gradient and
the thresholding is carried out to generate four edge flags.

4

E x p e r i m e n t a l R es u l t s

The feature map generation VLSI was fabricated in a 0.35-?m double-poly
three-metal-layer CMOS technology. A photomicrograph of the proof-of-concept
chip is shown in Fig. 7. The measured waveforms of the MVC at operating frequencies of 10MHz and 90MHz are demonstrated in Fig. 8. The input condition is in the
worst case. Namely, 21 ?1? bits and 20 ?0? bits were fed to the inputs. The observed
computation time is about 12 nsec which is larger than the simulation result of 2.5
nsec. This was caused by the capacitance loading due to the probing of the test circuit.
In the real circuit without external probing, we confirmed the average computation
time of 4~5 nsec.

Edge-detection
Filtering Circuit
Processing Technology 0.35?m CMOS 2-Poly 3-Metal
Median Filter Control Unit

Chip Size 4.5mm x 4.5mm

MVC

Majority Voting Circuit X8

Supply Voltage 3.3 V
Operation Frequengy 50MHz

Vector
Generator

Fig. 7: Photomicrograph and specification of the fabricated proof-of-concept chip.
1V/div 5ns/div

MVC_Output

1V/div 8ns/div

MVC_OUT

IN
IN

1

Majority Voting operation

(a)

Majority Voting operation

(b)

Fig. 8: Measured waveforms of majority voting circuit (MVC) at operation frequencies of 10MHz (a) and 90 MHz (b) for the worst-case input data.
The feature maps generated by the chip at the operation frequency of 25 MHz are
demonstrated in Fig. 9. The power dissipation was 224 mW. The difference between
the flag bits detected by the chip and those obtained by computer simulation are also
shown in the figure. The number of error flags was from 80 to 120 out of 16,384 flags,
only a 0.6% of the total. The occurrence of such error bits is anticipated since we
employed analog circuits for median detection. However, such error does not cause
any serious problems in the PPED algorithm as demonstrated in Figs. 10 and 11.
The template matching results with the top five PPED vector candidates in Sella
identification are demonstrated in Fig. 11, where Manhattan distance was adopted as
the dissimilarity measure. The error in the feature map generation processing yields a
constant bias to the dissimilarity and does not affect the result of the maximum likelihood search.

Generated Feature maps

Difference as compared
to computer simulation

Sella
Horizontal

Plus 45-degrees

Vertical

Minus 45-degrees

Fig. 9: Feature maps for Sella pattern generated by the chip.
Generated PPED vector by the chip

Sella
Difference as compared
to computer simulation

Dissimilarity (by Manhattan Distance)

Fig. 10: PPED vector for Sella pattern generated by the chip. The difference in the
vector components between the PPED vector generated by the chip and that obtained
by computer simulation is also shown.
1200

Measured Data

1000

800

Computer Simulation
600

400

200

0
1st (Correct)

2nd

3rd

4th

5th

Candidates in Sella recognition

Fig. 11: Comparison of template matching results.

5

Conclusion

A mixed-signal median filter VLSI circuit for PPED vector generation is presented. A
four-stage asynchronous median detection architecture based on analog digital
mixed-signal circuits has been introduced. As a result, a fully seamless pipeline
processing from threshold detection to edge feature map generation has been established. A prototype chip was designed in a 0.35-?m CMOS technology and the fab-

ricated chip generates an edge based image vector every 80 ?sec, which is about 10 4
times faster than the software computation.
Acknowledgments
The VLSI chip in this study was fabricated in the chip fabrication program of VLSI
Design and Education Center (VDEC), the University of Tokyo with the collaboration
by Rohm Corporation and Toppan Printing Corporation. The work is partially supported by the Ministry of Education, Science, Sports, and Culture under Grant-in-Aid
for Scientific Research (No. 14205043) and by JST in the program of CREST.
References
[1] C. Liu and Harry Wechsler, ?Gabor feature based classification using the enhanced fisher
linear discriminant model for face recognition?, IEEE Transactions on Image Processing, Vol.
11, No.4, Apr. 2002.
[2] C. Yen-ting, C. Kuo-sheng, and L. Ja-kuang, ?Improving cephalogram analysis through
feature subimage extraction?, IEEE Engineering in Medicine and Biology Magazine, Vol. 18,
No. 1, 1999, pp. 25-31.
[3] H. Potlapalli and R. C. Luo, ?Fractal-based classification of natural textures?, IEEE
Transactions on Industrial Electronics, Vol. 45, No. 1, Feb. 1998.
[4] T. Yamasaki and T. Shibata, ?Analog Soft-Pattern-Matching Classifier Using Floating-Gate MOS Technology,? Advances in Neural Information Processing Systems 14, Vol. II,
pp. 1131-1138.
[5] Masakazu Yagi, Tadashi Shibata, ?An Image Representation Algorithm Compatible to
Neural-Associative-Processor-Based Hardware Recognition Systems,? IEEE Trans. Neural
Networks, Vol. 14, No. 5, pp. 1144-1161, September (2003).
[6] M. Yagi, M. Adachi, and T. Shibata, "A hardware-friendly soft-computing algorithm for
image recognition," in Proc. EUSIPCO 2000, Sept. 2000, pp. 729-732.
[7] M. Yagi, T. Shibata, and K. Takada, "Human-perception-like image recognition system
based on the associative processor architecture," in Proc. EUSIPCO 2002, Vol. I, pp. 103-106,
Sept. 2002.
[8] M. Yagi and T. Shibata, "An associative-processor-based mixed signal system for robust
image recognition," in Proc. ISCAS 2002, May 2002, pp. V-137-V-140.
[9] M. Ogawa, K. Ito, and T. Shibata, "A general-purpose vector-quantization processor employing two-dimensional bit-propagating winner-take-all," in Symp. on VLSI Circuits Dig.
Tech. Papers, Jun. 2002, p.p. 244-247.
[10] S. Chakrabartty, M. Yagi, T. Shibata, and G. Cauwenberghs, ?Robust Cephalometric
Landmark Identification Using Support Vector Machines,? ICASSP 2003, Hong Kong, April
6-10, 2003, pp. II-825-II-828.
[11] Masakazu Yagi, Tadashi Shibata, Chihiro Tanikawa, and Kenji Takada, ?A Robust
Medical Image Recognition System Employing Edge-Based Feature Vector Representation,?
in the Proceeding of 13th Scandinavian Conference on Image Analysis (SCIA2003),
pp.534-540, Goteborg, Sweden, Jun. 29-Jul. 2, 2003.
[12] C.L. Lee and C.-W. Jen, ?Bit-sliced median filter design based on majority gate,? in IEE
Proceedings-G, Vol. 139, No. 1, Feb. 1992, pp. 63-71.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 153-range-image-restoration-using-mean-field-annealing.pdf

594
Range Image Restoration
using Mean Field Annealing
Wesley E. Snyder

Griff L. Bilbro

Center for Communications and Signal Processing
North Carolina State University
Raleigh, NC

Abstract
A new optimization strategy, Mean Field Annealing, is presented.
Its application to MAP restoration of noisy range images is derived
and experimentally verified.

1

Introduction

The application which motivates this paper is image analysis; specifically the analysis of range images. We [BS86] [GS87] and others [YA85][BJ88] have found that
surface curvature has the potential for providing an excellent, view-invariant feature with which to segment range images. Unfortunately, computation of curvature
requires, in turn, computation of second derivatives of noisy data.
We cast this task as a restoration problem: Given a measurement g(z, y), we assume
that g(z, y) resulted from the addition of noise to some "ideal" image fez, y) which
we must estimate from three things:
1. The measurement g(z, y).
2. The statistics of the noise, here assumed to be zero mean with variance (1'2.
3. Some a priori knowledge of the smoothness of the underlying surface(s).
We will turn this restoration problem into a minimization, and solve that minimization using a strategy called Mean Field A nnealing. A neural net appears to be
the ideal architecture for the reSUlting algorithm, and some work in this area has
already been reported [CZVJ88].

2

Simulated Annealing and Mean Field Anneal?
Ing

The strategy of SSA may be summarized as follows:
Let H(f) be the objective function whose minimum we se~k, wher~ /is somt' parameter vector.
A parameter T controls the algorithm. The SSA algorithm begins at a relatively
high value of T which is gradually reduced. Under certain conditions, SSA will
converge to a global optimum, [GGB4] [RS87]

H (f) = min{ H (fie)} Vfie

(1)

Range Image Restoration Using Mean Field Annealing

even though local minima may occur. However, SSA suffers from two drawbacks:
? It is slow, and
? there is no way to directly estimate [MMP87] a continuously-valued
derivatives.

I

or its

The algorithm presented in section 2.1 perturbs (typically) a single element of fat
each iteration. In Mean Field Annealing, we perturb the entire vector f at each
iteration by making a deterministic calculation which lowers a certain average of
H, < H(f) >, at the current temperature. We thus perform a rather conventional
non-linear minimization (e.g. gradient descent), until a minimum is found at that
temperature. We will refer to the minimization condition at a given T as the
equilibrium for that T. Then, T is reduced, and the previous equilibrium is used as
the initial condition for another minimization.
MFA thus converts a hard optimization problem into a sequence of easier problems.
In the next section, we justify this approach by relating it to SSA.

2.1

Stochastic Simulated Annealing

The problem to be solved is to find j where
minimization with the following strategy:

j

minimizes H(f). SSA solves this

1. Define PT ex e- H / T .

2. Find the equilibrium conditions on PT, at the current temperature, T. By equilibrium, we mean that any statistic ofpT(f) is constant. These statistics could
be derived from the Markov chain which SSA constructs: jO, p, ... , IN, ... , although in fact such statistical analysis is never done in normal running of an
SSA algorithm.
3. Reduce T gradually.
4. As T --+ 0, PT(f) becomes sharply peaked at j, the minimum.

2.2

Mean Field Annealing

In Mean Field Annealing, we provide an analytic mechanism for approximating the
equilibrium at arbitrary T. In MFA, we define an error function,
-H

fe--ordl
EMF(Z, T) = Tln--=-H- f eTdl

+

fe

-Hfl

T

(H-Ho)dl
- / j ---

f e- TdJ

- --.

(2)

which follows from Peierl's inequality [BGZ76]:

F
-H

~

Fo+ < H - Ho >
-Hg

(3)

where F = -Tlnf e---r-dl and Fo = -Tlnf e T dl . The significance of EMF is as
follows: the minimum of EMF determines the best approximation given the form

595

596

Bilbro and Snyder

of Ho to the equilibrium statistics of the SSA-generated MRF at T. We will then
anneal on T. In the next section, we choose a special form for Ho to simplify this
process even further.
1. Define some Ho(f, z) which will be used to estimate H(f).
2. At temperature T, minimize EMF(Z) where EMF is a functional of Ho and
H which characterizes the difference between Ho and H. The process of
minimizing EMF will result in a value of the parameter z, which we will
denote as ZT.
3. Define HT(f) = Ho(f, ZT) and for(f) ex e- iiT / T .

3

Image Restoration Using MFA

We choose a Hamiltonian which represents both the noise in the image, and our a
priori knowledge of the local shape of the image data.
" -2
1 2 (Ii - gil 2
HN = "
L.J
?
(1'

,

(4)
(5)

where 18( represents [Bes86] the set of values of pixels neighboring pixel i (e.g. the
value of I at i along with the I values at the four nearest neighbors of i); A is some
scalar valued function of that set of pixels (e.g. the 5 pixel approximation to the
Laplacian or the 9 pixel approximation to the quadratic variation); and

(6)
The noise term simply says that the image should be similar to the data, given noise
of variance (1'2. The prior term drives toward solutions which are locally planar. Recently, a simpler V(z) = z2 and a similar A were successfully used to design a neural
net [CZVJ88] which restores images consisting of discrete, but 256-valued pixels.
Our formulation of the prior term emphasizes the importance of "point processes,"
as defined [WP85] by Wolberg and Pavlidis. While we accept the eventual necessity
of incorporating line processes [MMP87] [Mar85] [GG84] [Gem87] into restoration,
our emphasis in this paper is to provide a rigorous relationship between a point
process, the prior model, and the more usual mathematical properties of surfaces.
Using range imagery in this problem makes these relationships direct. By adopting
this philosophy, we can exploit the results of Grimson [Gri83] as well as those of
Brady and Horn [BH83] to improve on the Laplacian.
The Gaussian functional form of V is chosen because it is mathematically convenient for Boltzmann statistics and beca.use it reflects the following shape properties
recommended for grey level images in the literature and is especially important if

Range Image Restoration Using Mean Field Annealing

line processes are to be omitted: Besag [Bes86] notes that lito encourage smooth
variation", V(A) "should be strictly increasing" in the absolute value of its argument and if "occasional abrupt changes" are expected, it should "quickly reach a
maximum" .
Rational functions with shapes similar to our V have been used in recent stochastic
approaches to image processing [GM85]. In Eq. 6, T is a "soft threshold" which
represents our prior knowledge of the probability of various values of \7 2 f (the
Laplacian of the undegraded image). For T large, we imply that high values of
the Laplacian are common - f is highly textured; for small values of T, we imply
that f is generally smooth. We note that for high values of T, the prior term is
insignificant, and the best estimate of the image is simply the data.
We choose the Mean Field Hamiltonian to be

(7)
and find that the optimal ZT approximately minimizes

(8)
both at very high and very low T . We have found experimentally that this approximation to ZT does anneal to a satisfactory restoration. At each temperature, we
use gradient descent to find ZT with the following approximation to the gradient of

<H>:
(9)
and

-b

V(r?) -

, - y'2;(T+T)

e-

.. ?
2( ..

h)

(lO)

.

Differentiating Eq. 8 with this new notation, we find
(11)

Since 6'+11,; is non-zero only when i

8

< H > _8 :J! .
)

+ v = i,

:J!j (1'

2

gj

we have

+L

L
-II

IT'(.

')+11

)

II

and this derivative can be used to find the equilibrium condition.

Algorithm

(12)

597

598

Bilbro and Snyder

1. Initially, we use the high temperature assumption, which eliminates the prior

term entirely, and results in
Z;

T

=g;; for

= 00.

(13)

This will provide the initial estimate of z. Any other estimate quickly converges to g.
2. Given an image z;, form the image ri
(L ? z);, where the ? indicates
convolution.

=

3. Create the image V. = V' (r?)
P

,

..?

= - -----l=
_-.!'L e - :II(T~
~T+T)T+T

.. ) ?

4. Using 12, perform ordinary non-linear minimization of < H > starting from
the current z. The particular strategy followed is not critical. We have
successfully used steepest descent and more sophisticated conjugate gradient [PFTV88] methods. The simpler methods seem adequate fot Gaussian
noise.
5. Update z to the minimizing z found in step 4.
6. Reduce T and go to 2. When T is sufficiently close to 0, the algorithm is
complete.
In step 6 above, T essentially defines the appropriate low-temperature stopping
point. In section 5, we will elaborate on the determination of T and other such
constants.

4

Performance

In this section, we describe the performance of the algorithm as it is applied to
several range images. We will use range images, in which the data is of the form
z = z(z, y).

4.1

(14)

Images With High Levels of Noise

Figure 1 illustrates a range image consisting of three objects, a wedge (upper left),
a cylinder with rounded end and hole (right), and a trapezoidal block viewed from
the top. The noise in this region is measured at (1' = 3units out of a total range of
about 100 units. Unsophisticated smoothing will not estimate second derivatives of
such data without blurring. Following the surface interpolation literature, [Gri83]
[BB83] we use the quadratic variation as the argument of the penalty function for
the prior term to

(15)
and performing the derivative in a manner analogous to Eq. 11 and 12. The
Laplacian of the restoration is shown in Figure 2. Figure 3 shows a cross-section
taken as indicated by the red line on Figure 2.

Fig. 1 Original rallge image

I

n

J~~ l\~lll
Fig. 2 Laplacian of the restored image

4.2

Fig. 3 Cross section
Through Laplacian along
Red Line

Comparison With Existing Techniques

Accurate computation of surface derivatives requires extremely good smoothing of
surface noise, while segmentation requires preservat.ion of edges. One suc.h adapt.ive
smoothing technique,[Ter87] iterative Gaussian smoothing (IGS) has been successfully applied to range imagery. [PB87] Following this strategy, step edges are first
detected, and smoothing is then applied using a small center-weighted kernel. At
edges, an even smaller kernel, called a "molecule", is used to smooth right up to
the edge without blurring the edge. The smoothing is then iterated.

600

Bilbro and Snyder

The results, restoration and Laplacian, of IGS are not nearly as sharp as those
shown in Figure 2.

Determining the Parameters

5

Although the optimization strategy described in section 3 has no hard thresholds,
several parameters exist either explicitly in Eq. 8 or implicitly in the iteration.
Good estimates of these parameters will result in improved performance, faster
convergence, or both. The parameters are:
(1' the standard deviation of the noise
b the relative magnitude of the prior term
11 = T + T the initial temperature and
T the final temperature.
The decrement in T which defines the annealing schedule could also be considered
a parameter. However, we have observed that 10% or less per step is always good
enough.
We find that for depth images of polyhedral scenes, T = 0 so that only one parameter
is problem dependent: (1'. For the more realistic case of images which also contain
curved surfaces, however, see our technical report [BS88], which also describes the
MFA derivation in much more detail.
The standard deviation of the noise must be determined independently for each
problem class. It is straightforward to estimate (1' to within 50%, and we have
observed experimentally that performance of the algorithm is not sensitive to this
order of error.
We can analytically show that annealing occurs in the region T:::::: IL12(1'2 and choose
TJ
2ILI2(1'2. Here, ILI2 is the squared norm of the operator Land ILI2
20 for
the usual Laplacian and ILI2 12.5 for the quadratic variation.
Further analysis shows that b .J2;ILI(1' is a good choice for the coefficient of the
prior term.

=

=

=

=

References
[Bes86]
[BGZ76]

[BH83]
[BJ88]

J. Besag. On the statistical analysis of dirty pictures. Journal of the
Royal Stati6ticCJl Society, B 48(3), 1986.
E. Brezin, J. C. Le Guillon, and J. Zinn-Justin. Field theoretical approach to critical phenomena. In C. Domb and M.S. Green, editors,
PhCJ6e Tran6ition6 and Critical Phenomena, chapter 3, Academic Press,
New York, 1976.
M. Brady and B.K.P Horn. Symm~tric operators for surface interpolation. CVGIP, 22, 1983.
P.J. Besl and R.C. Jain. Segmentation through variable-order surface
fitting. IEEE PAMI, 10(2), 1988.

Range Image Restoration Using Mean Field Annealing

G. Bilbro and W. Snyder. A Linear Time Theory for Recognizing Surface6 in 3-D. Technical Report CCSP-NCSU TR-86/8, Center for Communications and Signal Processing, North Carolina State University,
1986.
[BS88]
G. L. Bilbro and W. E. Snyder. Range Image Re6toration U6ing Mean
Field Annealing. Technical Report NETR 88-19, Available from the
Center for Communications and Signal Processing, North Carolina State
University, 1988.
[CZVJ88] R. Chellappa, Y.-T. Zhou, A. Vaid, and B. K. Jenkins. Image restoration
using a neural network. IEEE Tran6action6 on ASSP, 36(7):1141-1151,
July 1988.
[Gem87] D. Geman. Stochastic model for boundary detection. Vi6ion and Image
Computing, 5(2):61-65, 1987.
[GG84]
D. Geman and S. Geman. Stochastic relaxation, Gibbs Distributions,
and the Bayesian restoration of images. IEEE Tran6action6 on PAMI,
PAMI-6(6):721-741, November 1984.
[GM85]
S. Geman and D.E. McClure. Bayesian image analysis: an application
to single photon emission tomography. Proceeding6 of the American
Stati6tical A uociation, Stati6tical Computing Section, 12-18, 1985.
rGri83]
W.E.L. Grimson. An implementation of computational theory of visual
surface interpolation. CVGIP, 22, 1983.
[GS87]
B. R. Groshong and W. E. Snyder. Range image segmentation for object
recognition. In 18th Pitt6burgh Conference on Modeling and Simulation,
Pittsburgh, PA, April 1987.
[Mar85]
J .L. Marroquin. Probabili6tic Solution to Inver6e Problem6. PhD thesis,
M.LT, Cambridge, MA, September 1985.
[MMP87] J. Marroquin, S. Mitter, and T. Poggio. Probabilistic solution of illposed problems in computational vision. Journal of American Stati6tical
Auociation, 82(397):76-89, March 1987.
T. Ponce and M. Brady. Toward a surface primal sketch. In T. Kanade,
[PB87]
editor, Three Dimen6ional Machine Vi6ion, Kluwer Press, 1987.
[PFTV88] W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. T. Vetterling.
Numerical Recipe6 in C. Cambridge University Press, 1988.
[RS87]
F. Romeo and A. Sangiovanni-Vencentelli. Probabalistic hill climbing
algorithms: properties and applications. In Chapel Hill Conference on
VLSI, Computer Science Press, Chapel Hill, NC, 1987.
[Ter87]
D. Terzopoulos. The role of constraints and discontiuities in visiblesurface reconstruction. In Proc. of 7th International Conf. on AI,
pages 1073-1077, 1987.
[WP85]
G. Wolberg and T. Pavlidis. Restoration of binary images using stochastic relaxation with annealing. Pattern Recognition Letter6, 3(6):375-388,
December 1985.
M. Brady A. Yiulle and H. Asada. Describing surfaces. CVGIP, August
[YA85]
1985.
[BS86]

601


<<----------------------------------------------------------------------------------------------------------------------------------------->>

