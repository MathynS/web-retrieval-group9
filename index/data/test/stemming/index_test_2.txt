query sentence: image processing algorithms
---------------------------------------------------------------------
title: 2409-a-mixed-signal-vlsi-for-real-time-generation-of-edge-based-image-vectors.pdf

A Mixed-Signal VLSI for Real-Time
Generation of Edge-Based Image Vectors

Masakazu Yagi, Hideo Yamasaki, and Tadashi Shibata*
Department of Electronic Engineering
*Department of Frontier Informatics
The University of Tokyo
7-3-1 Hongo, Bunkyo-ku, Tokyo, 113-8656, Japan
mgoat@dent.osaka-u.ac.jp, hideo@if.t.u-tokyo.ac.jp, shibata@ee.t.u-tokyo.ac.jp

Abstract
A mixed-signal image filtering VLSI has been developed aiming at
real-time generation of edge-based image vectors for robust image
recognition. A four-stage asynchronous median detection architecture based on analog digital mixed-signal circuits has been introduced to determine the threshold value of edge detection, the key
processing parameter in vector generation. As a result, a fully
seamless pipeline processing from threshold detection to edge feature map generation has been established. A prototype chip was
designed in a 0.35-?m double-polysilicon three-metal-layer CMOS
technology and the concept was verified by the fabricated chip. The
chip generates a 64-dimension feature vector from a 64x64-pixel
gray scale image every 80?sec. This is about 104 times faster than the
software computation, making a real-time image recognition system
feasible.

1

In tro du c ti o n

The development of human-like image recognition systems is a key issue in information technology. However, a number of algorithms developed for robust image
recognition so far [1]-[3] are mostly implemented as software systems running on
general-purpose computers. Since the algorithms are generally complex and include a
lot of floating point operations, they are computationally too expensive to build
real-time systems. Development of hardware-friendly algorithms and their direct
VLSI implementation would be a promising solution for real-time response systems.
Being inspired by the biological principle that edge information is firstly detected in
the visual cortex, we have developed an edge-based image representation algorithm
compatible to hardware processing. In this algorithm, multiple-direction edges extracted from an original gray scale image is utilized to form a feature vector. Since the
spatial distribution of principal edges is represented by a vector, it was named Projected Principal-Edge Distribution (PPED) [4],[5], or formerly called Principal Axis

Projection (PAP) [6],[7]. (The algorithm is explained later.) Since the PPED vectors
very well represent the human perception of similarity among images, robust image
recognition systems have been developed using PPED vectors in conjunction with the
analog soft pattern classifier [4],[8], the digital VQ (Vector Quantization) processor
[9], and support vector machines [10] .
The robust nature of PPED representation is demonstrated in Fig. 1, where the system
was applied to cephalometric landmark identification (identifying specific anatomical
landmarks on medical radiographs) as an example, one of the most important clinical
practices of expert dentists in orthodontics [6],[7]. Typical X-ray images to be experienced by apprentice doctors were converted to PPED vectors and utilized as
templates for vector matching. The system performance has been proven for 250 head
film samples regarding the fundamental 26 landmarks [11]. Important to note is the
successful detection of the landmark on the soft tissue boundary (the tip of the lower
lip) shown in Fig. 1(c). Landmarks on soft tissues are very difficult to detect as
compared to landmarks on hard tissues (solid bones) because only faint images are
captured on radiographs. The successful detection is due to the median algorithm that
determines the threshold value for edge detection.

Sella

Nasion

Orbitale

By our system
(a)

By expert dentists

Landmark on soft tissue

(b)

(c)

Fig. 1: Image recognition using PPED vectors: (a,b) cephalometric landmark identification; (c) successful landmark detection on soft tissue.
We have adopted the median value of spatial variance of luminance within the filtering kernel (5x5 pixels), which allows us to extract all essential features in a delicate
gray scale image. However, the problem is the high computational cost in determining
the median value. It takes about 0.6 sec to generate one PPED vector from a
64x64-pixel image (a standard image size for recognition in our system) on a SUN
workstation, making real time processing unrealistic. About 90% of the computation
time is for edge detection from an input image, in which most of the time is spent for
median detection.
Then the purpose of this work is to develop a new architecture median-filter VLSI
subsystem for real-time PPED-vector generation. Special attention has been paid to
realize a fully seamless pipeline processing from threshold detection to edge feature
map generation by employing the four-stage asynchronous median detection architecture.

2

P r o je c t e d P r i n c i pa l E dg e Dis tribution (PPED )

Projected Principal Edge Distribution (PPED) algorithm [5],[6] is briefly explained
using Fig. 2(a). A 5x5-pixel block taken from a 64x64-pixel target image is subjected
to edge detection filtering in four principal directions, i.e. horizontal, vertical, and
?45-degree directions. In the figure, horizontal edge filtering is shown as an example.
(The filtering kernels used for edge detection are given in Fig. 2(b).) In order to determine the threshold value for edge detection, all the absolute-value differences
between two neighboring pixels are calculated in both vertical and horizontal directions and the median value is taken as the threshold. By scanning the 5x5-pixel filtering kernels in the target image, four 64x64 edge-flag maps are generated, which are
called feature maps. In the horizontal feature map, for example, edge flags in every
four rows are accumulated and spatial distribution of edge flags are represented by a
histogram having 16 elements. Similar procedures are applied to other three directions
to form respective histograms each having 16 elements. Finally, a 64-dimension
vector is formed by series-connecting the four histograms in the order of horizontal,
+45-degree, vertical, and ?45-degree.

64x64

Feature Map (64x64)

(Horizontal)
0 0 0 0 0
1 1 1 1 1
0 0 0 0 0
-1-1-1-1-1
0 0 0 0 0

(Horizontal)

Threshold
||
Median

Scan

(16 elements)

Edge Detection

Edge Filter

PPED Vector
(Horizontal Section)

0 0 0 0 0
1 1 1 1 1
0 0 0 0 0
-1 -1 -1 -1 -1
0 0 0 0 0

0 0 0 1 0
0 1 1 0 -1
0 1 0 -1 0
1 0 -1 -1 0
0 -1 0 0 0

Horizontal

+45-degree

0
0
0
0
0

Threshold Detection
Absolute value
difference between
neiboring pels.

1
1
1
1
1

0 -1
0 -1
0 -1
0 -1
0 -1

0
0
0
0
0

0 -1 0 0 0
1 0 -1 -1 0
0 1 0 -1 0
0 1 1 0 -1
0 0 0 1 0

Vertical

(a)

-45-degree

(b)

Fig. 2: PPED algorithm (a) and filtering kernels for edge detection (b).

3

Sy stem Orga ni za ti o n

The system organization of the feature map generation VLSI is illustrated in Fig. 3.
The system receives one column of data (8-b x 5 pixels) at each clock and stores the
data in the last column of the 5x6 image buffer. The image buffer shifts all the stored
data to the right at every clock. Before the edge filtering circuit (EFC) starts detecting
four direction edges with respect to the center pixel in the 5x5 block, the threshold
value calculated from all the pixel data in the 5x5 block must be ready in time for the
processing. In order to keep the coherence of the threshold detection and the edge
filtering processing, the two last-in data locating at column 5 and 6 are given to median filter circuit (MFC) in advance via absolute value circuit (AVC). AVC calculates
all luminance differences between two neighboring pixels in columns 5 and 6.
In this manner, a fully seamless pipeline processing from threshold detection to edge
feature map generation has been established. The key requirement here is that MFC
must determine the median value of the 40 luminance difference data from the
5x5-pixel block fast enough to carry out the seamless pipeline processing. For this
purpose, a four-stage asynchronous median detection architecture has been developed
which is explained in the following.

Edge Filtering Circuit (EFC)

6 5 4 3 2 1

Edge flags

H
+45
V

Image buffer

8-b x 5 pixels
(One column)

Absolute Value
Circuit (AVC)

Threshold
value

Median Filter
Circuit (MFC)

-45
Feature maps

Fig. 3: System organization of feature map generation VLSI.
The well-known binary search algorithm was adopted for fast execution of median
detection. The median search processing for five 4-b data is illustrated in Fig. 4 for the
purpose of explanation. In the beginning, majority voting is carried out for the MSB?s
of all data. Namely, the number of 1?s is compared with the number of 0?s and the
majority group wins. The majority group flag (?0? in this example) is stored as the
MSB of the median value. In addition, the loser group is withdrawn in the following
voting by changing all remaining bits to the loser MSB (?1? in this example). By
repeating the processing, the median value is finally stored in the median value register.
Elapse of time
Median Register :

0 1 X X

0 1 1 0

0 0 1 1

1
0 0 1 1

1
0 0 0 0

0
0 0 0 0

1 1 0 1

1 1 1 1

1 1 1 1

1 1 1 1

0 1 1 0

0 1 1 0

0 1 1 0

0 1 1 0

0 1 0 1

0 1 0 1

0 1 0 1

0 1 0 0

1 0 1 1

1 1 1 1

1 1 1 1

1 1 1 1

MVC0
MVC1
MVC2
MVC3

MVC0
MVC1
MVC2
MVC3

MVC0
MVC1
MVC2
MVC3

MVC0
MVC1
MVC2
MVC3

Majority Flag : 0

0 X X X

Majority Voting Circuit (MVC)

Fig. 4: Hardware algorithm for median detection by binary search.
How the median value is detected from all the 40 8-b data (20 horizontal luminance
difference data and 20 vertical luminance difference data) is illustrated in Fig. 5. All
the data are stored in the array of median detection units (MDU?s). At each clock, the
array receives four vertical luminance difference data and five horizontal luminance
difference data calculated from the data in column 5 and 6 in Fig. 3. The entire data are
shifted downward at each clock. The median search is carried out for the upper four
bits and the lower four bits separately in order to enhance the throughput by pipelining.
For this purpose, the chip is equipped with eight majority voting circuits (MVC 0~7).
The upper four bits from all the data are processed by MVC 4~7 in a single clock cycle
to yield the median value. In the next clock cycle, the loser information is transferred
to the lower four bits within each MDU and MVC0~3 carry out the median search for
the lower four bits from all the data in the array.

Vertical Luminance Difference

AVC AVC AVC AVC

Horizontal Luminance Difference

AVC AVC AVC AVC AVC
Shift

Shift

Median Detection Unit (MDU)
x (40 Units)
Lower 4bit

Upper 4bit

MVC0

MVC2

MVC1

MVC3

MVC4

MVC5

MVC6

MVC7

MVCs for upper 4bit

MVCs for lower 4bit

Fig. 5: Median detection architecture for all 40 luminance difference data.
The majority voting circuit (MVC) is shown in Fig. 6. Output connected CMOS inverters are employed as preamplifiers for majority detection which was first proposed
in Ref. [12]. In the present implementation, however, two preamps receiving input
data and inverted input data are connected to a 2-stage differential amplifier. Although this doubles the area penalty, the instability in the threshold for majority
detection due to process and temperature variations has been remarkably improved as
compared to the single inverter thresholding in Ref. [12]. The MVC in Fig. 6 has 41
input terminals although 40 bits of data are inputted to the circuit at one time. Bit ?0?
is always given to the terminal IN40 to yield ?0? as the majority when there is a tie in
the majority voting.

PREAMP
IN0

PREAMP
2W/L

IN0

2W/L

OUT
W/L

ENBL

W/L
W/L

IN1

IN1

2W/L

2W/L

W/L

ENBL
IN40

W/L
W/L

IN40

Fig. 6: Majority voting circuit (MVC).
The edge filtering circuit (EFC) in Fig. 3 is composed as a four-stage pipeline of
regular CMOS digital logic. In the first two stages, four-direction edge gradients are
computed, and in the succeeding two stages, the detection of the largest gradient and
the thresholding is carried out to generate four edge flags.

4

E x p e r i m e n t a l R es u l t s

The feature map generation VLSI was fabricated in a 0.35-?m double-poly
three-metal-layer CMOS technology. A photomicrograph of the proof-of-concept
chip is shown in Fig. 7. The measured waveforms of the MVC at operating frequencies of 10MHz and 90MHz are demonstrated in Fig. 8. The input condition is in the
worst case. Namely, 21 ?1? bits and 20 ?0? bits were fed to the inputs. The observed
computation time is about 12 nsec which is larger than the simulation result of 2.5
nsec. This was caused by the capacitance loading due to the probing of the test circuit.
In the real circuit without external probing, we confirmed the average computation
time of 4~5 nsec.

Edge-detection
Filtering Circuit
Processing Technology 0.35?m CMOS 2-Poly 3-Metal
Median Filter Control Unit

Chip Size 4.5mm x 4.5mm

MVC

Majority Voting Circuit X8

Supply Voltage 3.3 V
Operation Frequengy 50MHz

Vector
Generator

Fig. 7: Photomicrograph and specification of the fabricated proof-of-concept chip.
1V/div 5ns/div

MVC_Output

1V/div 8ns/div

MVC_OUT

IN
IN

1

Majority Voting operation

(a)

Majority Voting operation

(b)

Fig. 8: Measured waveforms of majority voting circuit (MVC) at operation frequencies of 10MHz (a) and 90 MHz (b) for the worst-case input data.
The feature maps generated by the chip at the operation frequency of 25 MHz are
demonstrated in Fig. 9. The power dissipation was 224 mW. The difference between
the flag bits detected by the chip and those obtained by computer simulation are also
shown in the figure. The number of error flags was from 80 to 120 out of 16,384 flags,
only a 0.6% of the total. The occurrence of such error bits is anticipated since we
employed analog circuits for median detection. However, such error does not cause
any serious problems in the PPED algorithm as demonstrated in Figs. 10 and 11.
The template matching results with the top five PPED vector candidates in Sella
identification are demonstrated in Fig. 11, where Manhattan distance was adopted as
the dissimilarity measure. The error in the feature map generation processing yields a
constant bias to the dissimilarity and does not affect the result of the maximum likelihood search.

Generated Feature maps

Difference as compared
to computer simulation

Sella
Horizontal

Plus 45-degrees

Vertical

Minus 45-degrees

Fig. 9: Feature maps for Sella pattern generated by the chip.
Generated PPED vector by the chip

Sella
Difference as compared
to computer simulation

Dissimilarity (by Manhattan Distance)

Fig. 10: PPED vector for Sella pattern generated by the chip. The difference in the
vector components between the PPED vector generated by the chip and that obtained
by computer simulation is also shown.
1200

Measured Data

1000

800

Computer Simulation
600

400

200

0
1st (Correct)

2nd

3rd

4th

5th

Candidates in Sella recognition

Fig. 11: Comparison of template matching results.

5

Conclusion

A mixed-signal median filter VLSI circuit for PPED vector generation is presented. A
four-stage asynchronous median detection architecture based on analog digital
mixed-signal circuits has been introduced. As a result, a fully seamless pipeline
processing from threshold detection to edge feature map generation has been established. A prototype chip was designed in a 0.35-?m CMOS technology and the fab-

ricated chip generates an edge based image vector every 80 ?sec, which is about 10 4
times faster than the software computation.
Acknowledgments
The VLSI chip in this study was fabricated in the chip fabrication program of VLSI
Design and Education Center (VDEC), the University of Tokyo with the collaboration
by Rohm Corporation and Toppan Printing Corporation. The work is partially supported by the Ministry of Education, Science, Sports, and Culture under Grant-in-Aid
for Scientific Research (No. 14205043) and by JST in the program of CREST.
References
[1] C. Liu and Harry Wechsler, ?Gabor feature based classification using the enhanced fisher
linear discriminant model for face recognition?, IEEE Transactions on Image Processing, Vol.
11, No.4, Apr. 2002.
[2] C. Yen-ting, C. Kuo-sheng, and L. Ja-kuang, ?Improving cephalogram analysis through
feature subimage extraction?, IEEE Engineering in Medicine and Biology Magazine, Vol. 18,
No. 1, 1999, pp. 25-31.
[3] H. Potlapalli and R. C. Luo, ?Fractal-based classification of natural textures?, IEEE
Transactions on Industrial Electronics, Vol. 45, No. 1, Feb. 1998.
[4] T. Yamasaki and T. Shibata, ?Analog Soft-Pattern-Matching Classifier Using Floating-Gate MOS Technology,? Advances in Neural Information Processing Systems 14, Vol. II,
pp. 1131-1138.
[5] Masakazu Yagi, Tadashi Shibata, ?An Image Representation Algorithm Compatible to
Neural-Associative-Processor-Based Hardware Recognition Systems,? IEEE Trans. Neural
Networks, Vol. 14, No. 5, pp. 1144-1161, September (2003).
[6] M. Yagi, M. Adachi, and T. Shibata, "A hardware-friendly soft-computing algorithm for
image recognition," in Proc. EUSIPCO 2000, Sept. 2000, pp. 729-732.
[7] M. Yagi, T. Shibata, and K. Takada, "Human-perception-like image recognition system
based on the associative processor architecture," in Proc. EUSIPCO 2002, Vol. I, pp. 103-106,
Sept. 2002.
[8] M. Yagi and T. Shibata, "An associative-processor-based mixed signal system for robust
image recognition," in Proc. ISCAS 2002, May 2002, pp. V-137-V-140.
[9] M. Ogawa, K. Ito, and T. Shibata, "A general-purpose vector-quantization processor employing two-dimensional bit-propagating winner-take-all," in Symp. on VLSI Circuits Dig.
Tech. Papers, Jun. 2002, p.p. 244-247.
[10] S. Chakrabartty, M. Yagi, T. Shibata, and G. Cauwenberghs, ?Robust Cephalometric
Landmark Identification Using Support Vector Machines,? ICASSP 2003, Hong Kong, April
6-10, 2003, pp. II-825-II-828.
[11] Masakazu Yagi, Tadashi Shibata, Chihiro Tanikawa, and Kenji Takada, ?A Robust
Medical Image Recognition System Employing Edge-Based Feature Vector Representation,?
in the Proceeding of 13th Scandinavian Conference on Image Analysis (SCIA2003),
pp.534-540, Goteborg, Sweden, Jun. 29-Jul. 2, 2003.
[12] C.L. Lee and C.-W. Jen, ?Bit-sliced median filter design based on majority gate,? in IEE
Proceedings-G, Vol. 139, No. 1, Feb. 1992, pp. 63-71.


----------------------------------------------------------------

title: 1352-a-simple-and-fast-neural-network-approach-to-stereovision.pdf

A Simple and Fast Neural Network
Approach to Stereovision

Rolf D. Henkel
Institute of Theoretical Physics
University of Bremen
P.O. Box 330 440, D-28334 Bremen
http://axon.physik.uni-bremen.de/-rdh

Abstract
A neural network approach to stereovision is presented based on
aliasing effects of simple disparity estimators and a fast coherencedetection scheme. Within a single network structure, a dense disparity map with an associated validation map and, additionally,
the fused cyclopean view of the scene are available. The network
operations are based on simple, biological plausible circuitry; the
algorithm is fully parallel and non-iterative.

1

Introduction

Humans experience the three-dimensional world not as it is seen by either their left
or right eye, but from a position of a virtual cyclopean eye, located in the middle
between the two real eye positions. The different perspectives between the left and
right eyes cause slight relative displacements of objects in the two retinal images
(disparities), which make a simple superposition of both images without diplopia
impossible. Proper fusion of the retinal images into the cyclopean view requires the
registration of both images to a common coordinate system, which in turn requires
calculation of disparities for all image areas which are to be fused.
1.1

The Problems with Classical Approaches

The estimation of disparities turns out to be a difficult task, since various random
and systematic image variations complicate this task. Several different techniques
have been proposed over time, which can be loosely grouped into feature-, area-

A Simple and Fast Neural Network Approach to Stereovision

809

and phase-based approaches. All these algorithms have a number of computational
problems directly linked to the very assumptions inherent in these approaches.
In feature-based stereo, intensity data is first converted to a set of features assumed
to be a more stable image property than the raw image intensities. Matching
primitives used include zerocrossings, edges and corner points (Frisby, 1991), or
higher order primitives like topological fingerprints (see for example: Fleck, 1991) .
Generally, the set of feature-classes is discrete, causing the two primary problems
of feature-based stereo algorithms: the famous "false-matches"-problem and the
problem of missing disparity estimates.

False matches are caused by the fact that a single feature in the left image can
potentially be matched with every feature of the same class in the right image.
This problem is basic to all feature-based stereo algorithms and can only be solved
by the introduction of additional constraints to the solution. In conjunction with
the extracted features these constraints define a complicated error measure which
can be minimized by cooperative processes (Marr, 1979) or by direct (Ohta, 1985)
or stochastic search techniques (Yuille, 1991). While cooperative processes and
stochastic search techniques can be realized easily on a neural basis, it is not immediately clear how to implement the more complicated algorithmic structures of
direct search techniques neuronally. Cooperative processes and stochastic search
techniques turn out to be slow, needing many iterations to converge to a local
minimum of the error measure.
The requirement of features to be a stable image property causes the second problem
of feature-based stereo: stable features can only be detected in a fraction of the
whole image area, leading to missing disparity estimates for most of the image area.
For those image parts, disparity estimates can only be guessed.
Dense disparity maps can be obtained with area-based approaches, where a suitable
chosen correlation measure is maximized between small image patches of the left and
right view. However, a neuronally plausible implementation of this seems to be not
readily available. Furthermore, the maximization turns out to be a computationally
expensive process, since extensive search is required in configuration space.
Hierarchical processing schemes can be utilized for speed-up, by using information
obtained at coarse spatial scales to restrict searching at finer scales. But, for general
image data, it is not guaranteed that the disparity information obtained at some
coarse scale is valid. The disparity data might be wrong, might have a different value
than at finer scales , or might not be present at all. Furthermore, by processing data
from coarse to fine spatial scales, hierarchical processing schemes are intrinsically
sequential. This creates additional algorithmic overhead which is again difficult to
realize with neuronal structures.
The same comments apply to phase-based approaches, where a locally extracted
Fourier-phase value is used for matching. Phase values are only defined modulo
211", and this wrap-around makes the use of hierarchical processing essential for
these types of algorithms. Moreover, since data is analyzed in different spatial
frequency channels, it is nearly certain that some phase values will be undefined
at intermediate scales, due to missing signal energy in this frequency band (Fleet,
1993) . Thus, in addition to hierarchical processing, some kind of exception handling
is needed with these approaches.

810

2

R. D. Henkel

Stereovision by Coherence Detection

In summary, classical approaches to stereovision seem to have difficulties with the
fast calculation of dense disparity-maps, at least with plausible neural circuitry.
In the following, a neural network implementation will be described which solves
this task by using simple disparity estimators based on motion-energy mechanisms
(Adelson, 1985; Qian, 1997), closely resembling responses of complex cells in visual
cortex (DeAngelis, 1991). Disparity units of these type belong to a class of disparity
estimators which can be derived from optical flow methods (Barron, 1994). Clearly,
disparity calculations and optical flow estimation share many similarities. The two
stereo views of a (static) scene can be considered as two time-slices cut out of
the space-time intensity pattern which would be recorded by an imaginary camera
moving from the position of the left to the position of the right eye. However,
compared to optical flow, disparity estimation is complicated by the fact that only
two discrete "time"-samples are available, namely the images of the left and right
view positions.
disparity calculations
<p

to

Left
Right 1
Right 2
correct

wrong

correct

Figure 1: The velocity of an image patch manifests itself as principal texture direction in the space-time flow field traced out by the intensity pattern in time (left).
Sampling such flow patterns at discrete times can create aliasing-effects which lead
to wrong estimates. If one is using optical flow estimation techniques for disparity
calculations, this problem is always present.
For an explanation consider Fig. 1. A surface patch shifting over time traces out
a certain flow pattern. The principal texture direction of this flow indicates the
relative velocity of the image patch (Fig. 1, left). Sampling the flow pattern only
at discrete time points, the shift between two "time-samples" can be estimated
without ambiguity provided the shift is not too large (Fig. 1, middle). However, if a
certain limit is exceeded, it becomes impossible to estimate the shift correctly, given
the data (Fig. 1, right). This is a simple aliasing-effect in the "time"-direction; an
everyday example can be seen as motion reversal in movies.
In the case of stereovision, aliasing-effects of this type are always present, and they
limit the range of disparities a simple disparity unit can estimate. Sampling theory
gives a relation between the maximal spatial wavevector k~ax (or, equivalently, the
minimum spatial wavelength >'~in) present in the data and the largest disparity
which can be estimated reliably (Henkel, 1997):

II
d <

7r

_1I{J

k~ax - '2>'min .

(1)

A Simple and Fast Neural Network Approach to Stereovision

811

A well-known example of the size-disparity scaling expressed in equation (1) is
found in the context of the spatial frequency channels assumed to exist in the
visual cortex. Cortical cells respond to spatial wavelengths down to about half
their peak wavelength Aopt; therefore, they can estimate reliable only disparities
less than 1/4 Aopt. This is known as Marr's quarter-cycle limit (Blake, 1991).
Equation (1) immediately suggests a way to extend the limited working range of
disparity estimators: a spatial smoothing of the image data before or during disparity calculation reduces k'f:tax, and in turn increases the disparity range. However,
spatial smoothing reduces also the spatial resolution of the resulting disparity map.
Another way of modifying the usable range of disparity estimators is the application of a fixed preshift to the input data before disparity calculation. This would
require prior knowledge of the correct preshift to be applied, which is a nontrivial
problem. One could resort to hierarchical coarse-to-fine schemes, but the difficulties
with hierarchical schemes have already been elal ')rated.
The aliasing effects discussed are a general feature of sampling visual space with
only two eyes; instead of counteracting, one can exploit them in a simple coherencedetection scheme, where the multi-unit activity in stacks of disparity detectors tuned
to a common view direction is analyzed.
Assuming that all disparity units i in a stack have random preshifts or presmoothing
applied to their input data, these units will have different, but slightly overlapping
working ranges Di = [di in , diax] for valid disparity estimates. An object with true
disparity d, seen in the common view direction of such a stack, will therefore split
the stack into two disjunct classes: the class C of estimators with dEDi for all
i E C, and the rest of the stack, C, with d ? D i . All disparity estimators E C will
code more or less the true disparity di ~ d, but the estimates of units belonging to C
will be subject to the random aliasing effects discussed, depending in a complicated
way on image content and disparity range Di of the unit.
We will thus have d i ~ d ~ d j whenever units i and j belong to C, and random relationships otherwise. A simple coherence detection within each stack, i.e. searching
for all units with di ~ d j and extracting the largest cluster found, will be sufficient
to single out C. The true disparity d in the view direction of the stack can be simply
estimated as an average over all coherently coding units:

3

Neural Network Implementation

Repeating this coherence detection scheme in every view direction results in a fully
parallel network structure for disparity calculation. Neighboring disparity stacks
responding to different view directions estimate disparity values independently from
each other, and within each stack, disparity units operate independently from each
other. Since coherence detection is an opportunistic scheme, extensions of the basic
algorithm to mUltiple spatial scales and combinations of different types of disparity
estimators are trivial. Additional units are simply included in the appropriate
coherence stacks. The coherence scheme will combine only the information from
the coherently coding units and ignore the rest of the data. For this reason, the
scheme also turns out to be extremely robust against single-unit failures.

R. D. Henkel

812

disparity data

"h'7"

-----------r?----------

Left eye?" .. ,

:

,

.............. ..

.'

Right eye

,

Cyclopean eye

Figure 2: The network structure for a single horizontal scan-line (left). The view
directions of the disparity stacks split the angle between the left and right lines
of sight in the network and 3D-space in half, therefore analyzing space along the
cyclopean view directions (right).

In the current implementation (Fig. 2), disparity units at a single spatial scale
are arranged into horizontal disparity layers. Left and right image data is fed
into this network along diagonally running data lines. This causes every disparity
layer to receive the stereo data with a certain fixed preshift applied, leading to the
required, slightly different working-ranges of neighboring layers. Disparity units
stacked vertically above each other are collected into a single disparity stack which
is then analyzed for coherent activity.

4

Results

The new stereo network performs comparable on several standard test image sets
(Fig. 3). The calculated disparity maps are similar to maps obtained by classical
area-based approaches, but they display subpixel-precision. Since no smoothing or
regularization is performed by the coherence-based stereo algorithm, sharp disparity
edges can be observed at object borders.
Within the network, a simple validation map is available locally. A measure of local

Figure 3: Disparity maps for some standard test images (small insets), calculated
by the coherence-based stereo algorithm.

A Simple and Fast Neural Network Approach to Stereovision

813

Figure 4: The performance of coherence-based stereo on a difficult scene with specular highlights, transparency and repetitive structures (left). The disparity map
(middle) is dense and correct, except for a few structure-less image regions. These
regions, as well as most object borders, are indicated in the validation map (right)
with a low [dark] validation count.

coherence can be obtained by calculating the relative number of coherently acting
disparity units in each stack, i.e. by calculating the ratio N(C)/ N(CUC), where N(C)
is the number of units in class C. In most cases, this validation map clearly marks
image areas where the disparity calculations failed (for various reasons, notably at
occlusions caused by object borders, or in large structure-less image regions, where
no reliable matching can be obtained - compare Fig 4).
Close inspection of disparity and validation maps reveals that these image maps
are not aligned with the left or the right view of the scene. Instead, both maps are
registered with the cyclopean view. This is caused by the structural arrangement of
data lines and disparity stacks in the network. Reprojecting data lines and stacks
back into 3D-space shows that the stacks analyze three-dimensional space along
lines splitting the angle between the left and right view directions in half. This is
the cyclopean view direction as defined by (Hering, 1879).
It is easy to obtain the cyclopean view of the scene itself. With If and If denoting
the left and right input data at the position of disparity-unit i, a summation over
all coherently coding disparity units in a stack, i.e.,

Figure 5: A simple superposition of the left and right stereo images results in
diplopia (left). By using a vergence system, the two stereo images can be aligned
better (middle), but diplopia is still prominent in most areas of the visual field.
The fused cyclopean view of the scene (left) was calculated by the coherence-based
stereo network.

814

R. D. Henkel

gives the image intensity I C in the cyclopean view-direction of this stack. Collecting
IC from all disparity stacks gives the complete cyclopean view as the third coregistered map of the network (Fig 5).

Acknowledgements
Thanks to Helmut Schwegler and Robert P. O'Shea for interesting discussions. Image data courtesy of G. Medoni, UCS Institute for Robotics & Intelligent Systems, B. Bolles, AIC, SRI International, and G. Sommer, Kiel
Cognitive Systems Group, Christian-Albrechts-Universitat Kiel. An internetbased implementation of the algorithm presented in this paper is available at
http://axon.physik.uni-bremen.de/-rdh/online~alc/stereo/.

References
Adelson, E.H. & Bergen, J.R. (1985): Spatiotemporal Energy Models for the Perception of Motion. J. Opt. Soc. Am. A2: 284-299.
Barron, J.L., Fleet, D.J. & Beauchemin, S.S. (1994): Performance of Optical Flow
Techniques. Int. J. Camp. Vis. 12: 43-77.
Blake, R. & Wilson, H.R. (1991): Neural Models of Stereoscopic Vision. TINS 14:
445-452.
DeAngelis, G.C., Ohzawa, I. & Freeman, R.D. (1991): Depth is Encoded in the
Visual Cortex by a Specialized Field Structure. Nature 11: 156-159.
Fleck, M.M. (1991): A Topological Stereo Matcher. Int. J. of Camp.
197-226.

Vis. 6:

Fleet, D.J. & Jepson, A.D. (1993): Stability of Phase Information. IEEE PAMI 2:
333-340.
Frisby, J.P. & and S. B. Pollard, S.B. (1991): Computational Issues in Solving the
Stereo Correspondence Problem. eds. M.S. Landy and J. A. Movshon, Computational Models of Visual Processing, pp. 331, MIT Press, Cambridge 1991.
Henkel, R.D. (1997): Fast Stereovision by Coherence Detection, in Proc. of
CAIP'97, Kiel, LCNS 1296, eds. G. Sommer, K. Daniilidis and J. Pauli, pp. 297,
LCNS 1296, Springer, Heidelberg 1997.
E. Hering (1879): Der Raumsinn und die Bewegung des Auges, in Handbuch der
Psychologie, ed. 1. Hermann, Band 3, Teil 1, Vogel, Leipzig 1879.
Marr, D. & Poggio, T. (1979): A Computational Theory of Human Stereo Vision.
Proc. R. Soc. Land. B 204: 301-328.
Ohta, Y, & Kanade, T. (1985): Stereo by Intra- and Inter-scanline Search using
dynamic programming. IEEE PAMI 7: 139-154.
Qian, N. & Zhu, Y. (1997): Physiological Computation of Binocular Disparity, to
appear in Vision Research.
Yuille, A.L., Geiger, D. & Biilthoff, H.H. (1991): Stereo Integration, Mean Field
Theory and Psychophysics. Network 2: 423-442.


----------------------------------------------------------------

title: 2689-modeling-nonlinear-dependencies-in-natural-images-using-mixture-of-laplacian-distribution.pdf

Modeling Nonlinear Dependencies in
Natural Images using Mixture of
Laplacian Distribution
Hyun Jin Park and Te Won Lee
Institute for Neural Computation, UCSD
9500 Gilman Drive, La Jolla, CA 92093-0523
{hjinpark, tewon}@ucsd.edu

Abstract
Capturing dependencies in images in an unsupervised manner is
important for many image processing applications. We propose a
new method for capturing nonlinear dependencies in images of
natural scenes. This method is an extension of the linear Independent
Component Analysis (ICA) method by building a hierarchical model
based on ICA and mixture of Laplacian distribution. The model
parameters are learned via an EM algorithm and it can accurately
capture variance correlation and other high order structures in a
simple manner. We visualize the learned variance structure and
demonstrate applications to image segmentation and denoising.

1

In trod u ction

Unsupervised learning has become an important tool for understanding biological
information processing and building intelligent signal processing methods. Real
biological systems however are much more robust and flexible than current artificial
intelligence mostly due to a much more efficient representations used in biological
systems. Therefore, unsupervised learning algorithms that capture more sophisticated
representations can provide a better understanding of neural information processing
and also provide improved algorithm for signal processing applications. For example,
independent component analysis (ICA) can learn representations similar to simple cell
receptive fields in visual cortex [1] and is also applied for feature extraction, image
segmentation and denoising [2,3]. ICA can approximate statistics of natural image
patches by Eq.(1,2), where X is the data and u is a source signal whose distribution is
a product of sparse distributions like a generalized Laplacian distribution.

X = Au

(1)

P (u ) = ? P (u i )

(2)

But the representation learned by the ICA algorithm is relatively low-level. In
biological systems there are more high-level representations such as contours,
textures and objects, which are not well represented by the linear ICA model. ICA
learns only linear dependency between pixels by finding strongly correlated linear

axis. Therefore, the modeling capability of ICA is quite limited. Previous approaches
showed that one can learn more sophisticated high-level representations by capturing
nonlinear dependencies in a post-processing step after the ICA step [4,5,6,7,8].
The focus of these efforts has centered on variance correlation in natural images. After
ICA, a source signal is not linearly predictable from others. However, given variance
dependencies, a source signal is still ?predictable? in a nonlinear manner. It is not
possible to de-correlate this variance dependency using a linear transformation.
Several researchers have proposed extensions to capture the nonlinear dependencies.
Portilla et al. used Gaussian Scale Mixture (GSM) to model variance dependency in
wavelet domain. This model can learn variance correlation in source prior and showed
improvement in image denoising [4]. But in this model, dependency is defined only
between a subset of wavelet coefficients. Hyvarinen and Hoyer suggested using a
special variance related distribution to model the variance correlated source prior.
This model can learn grouping of dependent sources (Subspace ICA) or topographic
arrangements of correlated sources (Topographic ICA) [5,6]. Similarly, Welling et al.
suggested a product of expert model where each expert represents a variance
correlated group [7]. The product form of the model enables applications to image
denoising. But these models don?t reveal higher-order structures explicitly.
Our model is motivated by Lewicki and Karklin who proposed a 2-stage model where
the 1st stage is an ICA model (Eq. (3)) and the 2 nd-stage is a linear generative model
where another source v generates logarithmic variance for the 1st stage (Eq. (4)) [8].
This model captures variance dependency structure explicitly, but treating variance as
an additional random variable introduces another level of complexity and requires
several approximations. Thus, it is difficult to obtain a simple analytic PDF of source
signal u and to apply the model for image processing problems.

(

P (u | ? ) = c exp ? u / ?

q

)

(3)

log[? ] = Bv

(4)

We propose a hierarchical model based on ICA and a mixture of Laplacian
distribution. Our model can be considered as a simplification of model in [8] by
constraining v to be 0/1 random vector where only one element can be 1. Our model is
computationally simpler but still can capture variance dependency. Experiments show
that our model can reveal higher order structures similar to [8]. In addition, our model
provides a simple parametric PDF of variance correlated priors, which is an important
advantage for adaptive signal processing. Utilizing this, we demonstrate simple
applications on image segmentation and image denoising. Our model provides an
improved statistic model for natural images and can be used for other applications
including feature extraction, image coding, or learning even higher order structures.

2

Modeling nonlinear dependencies

We propose a hierarchical or 2-stage model where the 1 st stage is an ICA source signal
model and the 2nd stage is modeled by a mixture model with different variances (figure
1). In natural images, the correlation of variance reflects different types of regularities
in the real world. Such specialized regularities can be summarized as ?context?
information. To model the context dependent variance correlation, we use mixture
models where Laplacian distributions with different variance represent different
contexts. For each image patch, a context variable Z ?selects? which Laplacian
distribution will represent ICA source signal u. Laplacian distributions have 0-mean

but different variances. The advantage of Laplacian distribution for modeling context
is that we can model a sparse distribution using only one Laplacian distribution. But
we need more than two Gaussian distributions to do the same thing. Also conventional
ICA is a special case of our model with one Laplacian. We define the mixture model
and its learning algorithm in the next sections.

Figure 1: Proposed hierarchical model (1st stage is ICA generative model. 2nd stage is
mixture of ?context dependent? Laplacian distributions which model U. Z is a random
variable that selects a Laplacian distribution that generates the given image patch)
2.1

Mixture of Laplacian Distribution

We define a PDF for mixture of M-dimensional Laplacian Distribution as Eq.(5),
where N is the number of data samples, and K is the number of mixtures.
N
N K
M
N K
r
r r
P(U | ?, ?) = ? P(u n | ?, ?) = ?? ? k P(u n | ?k ) = ?? ? k ?
n

n

k

n

k

m

1

(2? )
k ,m

? u n,m
exp? ?
? ?k , m
?

?
?
?
?

(5)

r
r r
r r
un = (un,1 , un , 2 , , , un,M ) : n-th data sample, U = (u1 , u 2 , , , ui , , , u N )
r
r r
r
r
?k = (?k ,1 , ?k , 2 ,..., ?k ,M ) : Variance of k-th Laplacian distribution, ? = (?1 , ?2 , , , ?k , , , ?K )

?k

: probability of Laplacian distribution k, ? = (? 1 , , , ? K ) and

?

k

?k =1

It is not easy to maximize Eq.(5) directly, and we use EM (expectation maximization)
algorithm for parameter estimation. Here we introduce a new hidden context variable
Z that represents which Laplacian k, is responsible for a given data point. Assuming
we know the hidden variable Z, we can write the likelihood of data and Z as Eq.(6),
zkn
?K ?
??
N
?
u n ,m
r
? ? 1 ??
z kn
?
?
P(U , Z | ?, ? ) = ? P(u n , Z | ?, ? ) = ? ? (? k ) ? ? ?
? exp? ? z kn
?
?
?
?
?k , m
n
n
k
m ? ? 2?k ,m ?
?
?? ??
?
N

? ??? ??
? ?
? ??? ?
? ??? ??

(6)

z kn : Hidden binary random variable, 1 if n-th data sample is generated from k-th
Laplacian, 0 other wise. ( Z = (z kn ) and ? z kn = 1 for all n = 1?N)
k

2.2

EM algorithm for learning the mixture model

The EM algorithm maximizes the log likelihood of data averaged over hidden variable
Z. The log likelihood and its expectation can be computed as in Eq.(7,8).

?
?
u
1
log P(U , Z | ?, ? ) = ? ? z kn log(? k ) + ? z kn ? log(
) ? n ,m
?
2
?
?
n ,k ?
m
k ,m
k ,m
?
?

??
??
??
??

(7)

?
?
u
1
E {log P (U , Z | ?, ? )} = ? E z kn ?log(? k ) + ? ? log(
) ? n ,m
?
2? k , m
?k , m
n ,k
m ?
??

{ }

??
??
??
??

(8)

The expectation in Eq.(8) can be evaluated, if we are given the data U and estimated
parameters ? and ?. For ? and ?, EM algorithm uses current estimation ?? and ??.

{ } {

} ? z P( z

E z kn ? E z kn | U , ? ' , ? ' =

1

zkn =0

n
k

n
k

| u n , ?' , ? ' ) = P ( z kn = 1 | u n , ? ' , ? ' )

(9)

=

P (u n | z kn = 1, ?' , ? ' ) P( z kn = 1 | ? ' , ? ' )
P(u n | ?' , ? ' )

=

M
u n ,m
1
1
1
exp(?
) ?? k ' =
?
?k , m '
P (u n | ? ' , ? ' ) m 2?k ,m '
cn

M

?k '

? 2?
m

k ,m

'

exp(?

u n ,m

?k , m '

)

Where the normalization constant can be computed as
K

K

M

k

k =1

m =1

cn = P (u n | ? ' , ? ' ) = ? P (u n | z kn , ? ' , ? ' ) P ( z kn | ? ' , ? ' ) = ? ? k ?

1

(2? )

exp( ?

k ,m

u n ,m

?k ,m

)

(10)

The EM algorithm works by maximizing Eq.(8), given the expectation computed from
Eq.(9,10). Eq.(9,10) can be computed using ?? and ?? estimated in the previous
iteration of EM algorithm. This is E-step of EM algorithm. Then in M-step of EM
algorithm, we need to maximize Eq.(8) over parameter ? and ?.
First, we can maximize Eq.(8) with respect to ?, by setting the derivative as 0.
?? 1
u n,m ??
?E{log P (U , Z | ?, ? )}
?? = 0
= ? E z kn ?? ?
+
?
?? k ,m
?
(? k ,m ) 2 ???
???
n
k ,m
?

{ }

? ? k ,m

? E{z }? u
=
? E{z }
n
k

n ,m

n

(11)

n
k

n

Second, for maximization of Eq.(8) with respect to ?, we can rewrite Eq.(8) as below.
(12)
E {log P (U , Z | ? , ? )} = C + ? E {z kn' }log(? k ' )
n ,k '

As we see, the derivative of Eq.(12) with respect to ? cannot be 0. Instead, we need to
use Lagrange multiplier method for maximization. A Lagrange function can be
defined as Eq.(14) where ? is a Lagrange multiplier.

{ }

(13)

L (? , ? ) = ? ? E z kn' log(? k ' ) + ? (? ? k ' ? 1)
n,k '

k'

By setting the derivative of Eq.(13) to be 0 with respect to ? and ?, we can simply get
the maximization solution with respect to ?. We just show the solution in Eq.(14).
?L(?, ? )
?L(?, ? )
=0
= 0,
??
??

?
? ?
?
? ? k = ? ? E z kn ? / ? ?? E z kn ?
?
? ? k n
? n

{ }

{ }

(14)

Then the EM algorithm can be summarized as figure 2. For the convergence criteria,
we can use the expectation of log likelihood, which can be calculated from Eq. (8).

?k =

{ }

, ?k , m = E um + e (e is small random noise)
2. Calculate the Expectation by
1. Initialize

1
K

u n ,m
1 M ?k '
exp( ?
)
?
c n m 2? k , m '
? k ,m '
3. Maximize the log likelihood given the Expectation

{ } {

}

E z kn ? E z kn | U , ? ' , ? ' =
?

? ?

?

?k ,m ? ? ? E {z kn }? u n,m ? / ? ? E {z kn }? ,

?
? ?
?
? k ? ? ? E {z kn }? / ? ?? E {z kn }?
? ? n
?
? n
? ? k n
?
4. If (converged) stop, otherwise repeat from step 2.

?

n

Figure 2: Outline of EM algorithm for Learning the Mixture Model

3

Experimental Results

Here we provide examples of image data and show how the learning procedure is
performed for the mixture model. We also provide visualization of learned variances
that reveal the structure of variance correlation and an application to image denoising.
3.1

Learning Nonlinear Dependencies in Natural images

As shown in figure 1, the 1 st stage of the proposed model is simply the linear ICA. The
ICA matrix A and W(=A-1) are learned by the FastICA algorithm [9]. We sampled
105(=N) data from 16x16 patches (256 dim.) of natural images and use them for both
first and second stage learning. ICA input dimension is 256, and source dimension is
set to be 160(=M). The learned ICA basis is partially shown in figure 1. The 2nd stage
mixture model is learned given the ICA source signals. In the 2 nd stage the number of
mixtures is set to 16, 64, or 256(=K). Training by the EM algorithm is fast and several
hundred iterations are sufficient for convergence (0.5 hour on a 1.7GHz Pentium PC).
For the visualization of learned variance, we adapted the visualization method from
[8]. Each dimension of ICA source signal corresponds to an ICA basis (columns of A)
and each ICA basis is localized in both image and frequency space. Then for each
Laplacian distribution, we can display its variance vector as a set of points in image
and frequency space. Each point can be color coded by variance value as figure 3.

(a1)

(a2)

(b1)

(b2)

Figure 3: Visualization of learned variances (a1 and a2 visualize variance of
Laplacian #4 and b1 and 2 show that of Laplacian #5. High variance value is mapped
to red color and low variance is mapped to blue. In Laplacian #4, variances for
diagonally oriented edges are high. But in Laplacian #5, variances for edges at
spatially right position are high. Variance structures are related to ?contexts? in the
image. For example, Laplacian #4 explains image patches that have oriented textures
or edges. Laplacian #5 captures patches where left side of the patch is clean but right
side is filled with randomly oriented edges.)

A key idea of our model is that we can mix up independent distributions to get nonlinearly dependent distribution. This modeling power can be shown by figure 4.

Figure 4: Joint distribution of nonlinearly dependent sources. ((a) is a joint histogram
of 2 ICA sources, (b) is computed from learned mixture model, and (c) is from learned
Laplacian model. In (a), variance of u2 is smaller than u1 at center area (arrow A), but
almost equal to u1 at outside (arrow B). So the variance of u2 is dependent on u1. This
nonlinear dependency is closely approximated by mixture model in (b), but not in (c).)
3.2

Unsupervised Image Segmentation

The idea behind our model is that the image can be modeled as mixture of different
variance correlated ?contexts?. We show how the learned model can be used to
classify different context by an unsupervised image segmentation task. Given learned
model and data, we can compute the expectation of a hidden variable Z from Eq. (9).
Then for an image patch, we can select a Laplacian distribution with highest
probability, which is the most explaining Laplacian or ?context?. For segmentation,
we use the model with 16 Laplacians. This enables abstract partitioning of images and
we can visualize organization of images more clearly (figure 5).

Figure 5: Unsupervised image segmentation (left is original image, middle is color
labeled image, right image shows color coded Laplacians with variance structure.
Each color corresponds to a Laplacian distribution, which represents surface or
textural organization of underlying contexts. Laplacian #14 captures smooth surface
and Laplacian #9 captures contrast between clear sky and textured ground scenes.)
3.3

Application to Image Restoration

The proposed mixture model provides a better parametric model of the ICA source
distribution and hence an improved model of the image structure. An advantage is in
the MAP (maximum a posterior) estimation of a noisy image. If we assume Gaussian
noise n, the image generation model can be written as Eq.(15). Then, we can compute
MAP estimation of ICA source signal u by Eq.(16) and reconstruct the original image.

(15)

X = Au + n

(16)

u? = argmax log P (u | X , A) = argmax (log P ( X | u , A) + log P (u ) )
u

u

Since we assumed Gaussian noise, P(X|u,A) in Eq. (16) is Gaussian. P(u) in Eq. (16)
can be modeled as a Laplacian or a mixture of Laplacian distribution. The mixture
distribution can be approximated by a maximum explaining Laplacian. We evaluated
3 different methods for image restoration including ICA MAP estimation with simple
Laplacian prior, same with Laplacian mixture prior, and the Wiener filter. Figure 6
shows an example and figure 7 summarizes the results obtained with different noise
levels. As shown MAP estimation with the mixture prior performs better than the
others in terms of SNR and SSIM (Structural Similarity Measure) [10].

Figure 6: Image restoration results (signal variance 1.0, noise variance 0.81)
16
ICA MAP (Mixture prior)
ICA MAP (Laplacian prior)
W iener

14

0.8
SSIM Index

SNR

12
10
8
6

0.6
0.4
0.2

4
2

ICA MAP(Mixture prior)
ICA MAP(Laplacian prior)
W iener
Noisy Image

1

0

0.5

1
1.5
Noise variance

2

2.5

0

0

0.5

1
1.5
Noise variance

2

2.5

Figure 7: SNR and SSIM for 3 different algorithms (signal variance = 1.0)

4

D i s c u s s i on

We proposed a mixture model to learn nonlinear dependencies of ICA source signals
for natural images. The proposed mixture of Laplacian distribution model is a
generalization of the conventional independent source priors and can model variance
dependency given natural image signals. Experiments show that the proposed model
can learn the variance correlated signals grouped as different mixtures and learn highlevel structures, which are highly correlated with the underlying physical properties

captured in the image. Our model provides an analytic prior of nearly independent and
variance-correlated signals, which was not viable in previous models [4,5,6,7,8].
The learned variances of the mixture model show structured localization in image and
frequency space, which are similar to the result in [8]. Since the model is given no
information about the spatial location or frequency of the source signals, we can
assume that the dependency captured by the mixture model reveals regularity in the
natural images. As shown in image labeling experiments, such regularities correspond
to specific surface types (textures) or boundaries between surfaces.
The learned mixture model can be used to discover hidden contexts that generated
such regularity or correlated signal groups. Experiments also show that the labeling of
image patches is highly correlated with the object surface types shown in the image.
The segmentation results show regularity across image space and strong correlation
with high-level concepts.
Finally, we showed applications of the model for image restoration. We compare the
performance with the conventional ICA MAP estimation and Wiener filter. Our
results suggest that the proposed model outperforms other traditional methods. It is
due to the estimation of the correlated variance structure, which provides an improved
prior that has not been considered in other methods.
In our future work, we plan to exploit the regularity of the image segmentation result
to lean more high-level structures by building additional hierarchies on the current
model. Furthermore, the application to image coding seems promising.
References
[1] A. J. Bell and T. J. Sejnowski, The ?Independent Components? of Natural Scenes are Edge
Filters, Vision Research, 37(23):3327?3338, 1997.
[2] A. Hyvarinen, Sparse Code Shrinkage: Denoising of Nongaussian Data by Maximum
Likelihood Estimation,Neural Computation, 11(7):1739-1768, 1999.
[3] T. Lee, M. Lewicki, and T. Sejnowski., ICA Mixture Models for unsupervised
Classification of non-gaussian classes and automatic context switching in blind separation.
PAMI, 22(10), October 2000.
[4] J. Portilla, V. Strela, M. J. Wainwright and E. P Simoncelli, Image Denoising using Scale
Mixtures of Gaussians in the Wavelet Domain, IEEE Trans. On Image Processing, Vol.12, No.
11, 1338-1351, 2003.
[5] A. Hyvarinen, P. O. Hoyer. Emergence of phase and shift invariant features by
decomposition of natural images into independent feature subspaces. Neurocomputing, 1999.
[6] A. Hyvarinen, P.O. Hoyer, Topographic Independent component analysis as a model of V1
Receptive Fields, Neurocomputing, Vol. 38-40, June 2001.
[7] M. Welling and G. E. Hinton, S. Osindero, Learning Sparse Topographic Representations
with Products of Student-t Distributions, NIPS, 2002.
[8] M. S. Lewicki and Y. Karklin, Learning higher-order structures in natural images, Network:
Comput. Neural Syst. 14 (August 2003) 483-499.
[9] A.Hyvarinen, P.O. Hoyer, Fast ICA matlab code.,
http://www.cis.hut.fi/projects/compneuro/extensions.html/
[10] Z. Wang, A. C. Bovik, H. R. Sheikh and E. P. Simoncelli, The SSIM Index for Image
Quality Assessment, IEEE Transactions on Image Processing, vol. 13, no. 4, Apr. 2004.


----------------------------------------------------------------

title: 2426-bayesian-color-constancy-with-non-gaussian-models.pdf

Bayesian Color Constancy
with Non-Gaussian Models
Charles Rosenberg

Thomas Minka

Alok Ladsariya

Computer Science Department
Carnegie Mellon University
Pittsburgh, PA 15213

Statistics Department
Carnegie Mellon University
Pittsburgh, PA 15213

Computer Science Department
Carnegie Mellon University
Pittsburgh, PA 15213

chuck@cs.cmu.edu

minka@stat.cmu.edu

alokl@cs.cmu.edu

Abstract
We present a Bayesian approach to color constancy which utilizes a nonGaussian probabilistic model of the image formation process. The parameters of this model are estimated directly from an uncalibrated image
set and a small number of additional algorithmic parameters are chosen
using cross validation. The algorithm is empirically shown to exhibit
RMS error lower than other color constancy algorithms based on the
Lambertian surface reflectance model when estimating the illuminants
of a set of test images. This is demonstrated via a direct performance
comparison utilizing a publicly available set of real world test images
and code base.

1 Introduction
Color correction is an important preprocessing step for robust color-based computer vision
algorithms. Because the illuminants in the world have varying colors, the measured color
of an object will change under different light sources. We propose an algorithm for color
constancy which, given an image, will automatically estimate the color of the illuminant
(assumed constant over the image), allowing the image to be color corrected.
This color constancy problem is ill-posed, because object color and illuminant color are
not uniquely separable. Historically, algorithms for color constancy have fallen into two
groups. The first group imposes constraints on the scene and/or the illuminant, in order to
remove the ambiguities. The second group uses a statistical model to quantify the probability of each illuminant and then makes an estimate from these probabilities. The statistical
approach is attractive, since it is more general and more automatic?hard constraints are a
special case of statistical models, and they can be learned from data instead of being specified in advance. But as shown by [3, 1], currently the best performance on real images
is achieved by gamut mapping, a constraint-based algorithm. And, in the words of some
leading researchers, even gamut mapping is not ?good enough? for object recognition [8].
In this paper, we show that it is possible to outperform gamut mapping with a statistical
approach, by using appropriate probability models with the appropriate statistical framework. We use the principled Bayesian color constancy framework of [4], but combine it
with rich, nonparametric image models, such as used by Color by Correlation [1]. The

result is a Bayesian algorithm that works well in practice and addresses many of the issues
with Color by Correlation, the leading statistical algorithm [1].
At the same time, we suggest that statistical methods still have much to learn from
constraint-based methods. Even though our algorithm outperforms gamut mapping on
average, there are cases in which gamut mapping provides better estimates, and, in fact,
the errors of the two methods are surprisingly uncorrelated. This is an interesting result,
because it suggests that gamut mapping exploits image properties which are different from
what is learned by our algorithm, and probably other statistical algorithms. If this is true,
and if our statistical model could be extended in a way that captures these additional properties, better algorithms should be possible in the future.

2 The imaging model
Our approach is to model the observed image pixels with a probabilistic generative model,
decomposing them as the product of unknown surface reflectances with an unknown illuminant. Using Bayes? rule, we obtain a posterior for the illuminant, and from this we
extract the estimate with minimum risk, e.g., the minimum expected chromaticity error.
Let y be an image pixel with three color channels: (yr , yg , yb ). The pixel is assumed to be
the result of light reflecting off of a surface under the Lambertian reflectance model. Denote
the power of the light in each channel by ` = (`r , `g , `b ), with each channel ranging from
zero to infinity. For each channel, a surface can reflect none of the light, all of the light,
or somewhere in between. Denote this reflectance by x = (xr , xg , xb ), with each channel
ranging from zero to one. The model for the pixel is the well-known diagonal lighting
model:
yr = `r xr
yg = `g xg
yb = `b xb
(1)
To simplify the equations below, we write this in matrix form as
L
y

= diag(`)
= Lx

(2)
(3)

This specifies the conditional distribution p(y|`, x). In reality, there are sensor noise and
other factors which affect the observed color, but we will consider these to be negligible.
Next we make the common assumption that the light and the surface have been chosen
independently, so that p(`, x) = p(`)p(x). The prior distribution for the illuminant (p(`))
will be uniform over a constraint set, described later in section 5.3.
The most difficult step is to construct a model for the surface reflectances in an image
containing many pixels:
Y =
X =

(y(1), ..., y(n))
(x(1), ..., x(n))

(4)
(5)

We need a distribution p(X) for all n reflectances. One approach is to assume that the
reflectances are independent and Gaussian, as in [4], which gives reasonable results but can
be improved upon. Our approach is to quantize the reflectance vectors into K bins, and
consider the reflectances to be exchangeable?a weaker assumption than independence.
Exchangeability implies that the probability only depends on the number of reflectances
in
P
each bin. Thus if we denote the reflectance histogram by (n1 , ..., nK ), where k nk = n,
then
p(x(1), ..., x(n)) ? f (n1 , ..., nK )
(6)
where f is a function to be specified. Independence is a special case of exchangeability.
If
P
mk is the probability of a surface having a reflectance value in bin k, so that k mk = 1,

then independence says
f (n1 , ..., nK )

Y

=

mnk k

(7)

k

As an alternative to this, we have experimented with the Dirichlet-multinomial model,
which employs a parameter s > 0 to control the amount of correlation. Under this model,
?(s) Y ?(nk + smk )
(8)
f (n1 , ..., nK ) =
?(n + s)
?(smk )
k

For large s, correlation is weak and the model reduces to (7). For small s, correlation is
strong and the model expects a few reflectances to be repeated many times, which is what
we see in real images. When s is very small, the expression (8) can be reduced to a simple
form:
1 Y
clip(nk )
f (n1 , ..., nK ) ?
(smk ?(nk ))
(9)
s?(n)
k

0 if nk = 0
clip(nk ) =
(10)
1 if nk > 0
This resembles a multinomial distribution on clipped counts. Unfortunately, this distribution strongly prefers that the image contains a small number of different reflectances,
which biases the light source estimate. Empirically we have achieved our best results using
a ?normalized count? modification of the model which removes this bias:
Y
m?kk
(11)
f (n1 , ..., nK ) =
k

clip(nk )
= nP
k clip(nk )

?k

(12)

The modified counts ?k sum to n just like the original counts nk , but are distributed equally
over all reflectances present in the image.

3 The color constancy algorithm
The algorithm for estimating the illuminant has two parts: (1) discretize the set of all
illuminants on a fine grid and compute their likelihood and (2) pick the illuminant which
minimizes the risk.
The likelihood of the observed image data Y for a given illuminant ` is
!
Z
Y
p(Y|`) =
p(y(i)|`, x(i)) p(X)dX

(13)

X

i
?1 n

= |L

| p(X = L?1 Y)

(14)

?1

The quantity L Y can be understood as the color-corrected image. The determinant term,
1/(`r `g `b )n , makes this a valid distribution over Y and has the effect of introducing a
preference for dimmer illuminants independently of the prior on reflectances. Also implicit
in this likelihood are the bounds on x, which require reflectances to be in the range of zero
and one and thus we restrict our search to illuminants that satisfy:
`r ? max yr (i)

`g ? max yg (i)

i

`b ? max yb (i)

i

i

(15)

The posterior probability for ` then follows:
p(`|Y)

?
?

p(Y|`)p(`)
?1 n

|L

(16)
?1

| p(X = L

Y)p(`)

(17)

The next step is to find the estimate of ` with minimum risk. An answer that the illuminant
is `? , when it is really `, incurs some cost, denoted R(`? |`). Let this function be quadratic
in some transformation g of the illuminant vector `:
R(`? |`) =

||g(`? ) ? g(`)||2

(18)

This occurs, for example, when the cost function is squared error in chromaticity. Then the
minimum-risk estimate satisfies
Z
g(`)p(`|Y)d`
(19)
g(`? ) =
`

The right-hand side, the posterior mean of g, and the normalizing constant of the posterior
can be computed in a single loop over the grid of illuminants.

4 Relation to other algorithms
In this section we describe related color constancy algorithms using the framework of the
imaging model introduced in section 2. This is helpful because it allows us to compare all
of these algorithms in a single framework and understand the assumptions made by each.
Independent, Gaussian reflectances The previous work most similar to our own is by
[10] and [4]; however, these methods are not tested on real images. They use a similar
imaging model and maximum-likelihood and minimum-risk estimation, respectively. The
difference is that they use a Gaussian prior for the reflectance vectors, and assume the
reflectances for different pixels are independent. The Gaussian assumption leads to a simple likelihood formula whose maximum can be found by gradient methods. However, as
mentioned by [4], this is a constraining assumption, and more appropriate priors would be
preferable.
Scale by max The scale by max algorithm (as tested e.g. in [3]) estimates the illuminant
by the simple formula
`r = max yr (i)
i

`g = max yg (i)
i

`b = max yb (i)
i

(20)

which is the dimmest illuminant in the valid set (15). In the Bayesian algorithm, this
solution can be achieved by letting the reflectances be independent and uniform over the
range 0 to 1. Then p(X) is constant and the maximum-likelihood illuminant is (20). This
connection was also noticed by [4].
Gray-world The gray-world algorithm [5] chooses the illuminant such that the average
value in each channel of the corrected image is a constant, e.g. 0.5. This is equivalent to the
Bayesian algorithm with a particular reflectance prior. Let the reflectances be independent
for each pixel and each channel, with distribution p(xc ) ? exp(?2xc ) in each channel c.
The log-likelihood for `c is then
log p(Yc |`c ) = ?n log `c ? 2

X yc (i)
i

whose maximum is (as desired)
`c =

2X
yc (i)
n i

`c

+ const.

(21)

(22)

Figure 1: Plots of slices of the three dimensional color surface reflectance distribution
along a single dimension. Row one plots green versus blue with 0,0 at the upper left of
each subplot and slices in red whose magnitude increases from left to right. Row two plots
red versus blue with slices in green. Row three plots red versus green with slices in blue.
Color by Correlation Color by Correlation [6, 1] also uses a likelihood approach, but
with a different imaging model that is not based on reflectance. Instead, observed pixels
are quantized into color bins, and the frequency of each bin is counted for each illuminant,
in a finite set of illuminants. (Note that this is different from quantizing reflectances, as
done in our approach.) Let mk (`) be the frequency of color bin k for illuminant `, and let
n1 ? ? ? nK be the color histogram of the image, then the likelihood of ` is computed as
Y
mk (`)clip(nk )
(23)
p(Y|`) =
k

While theoretically this is very general, there are practical limitations. First there are training issues. One must learn the color frequencies for every possible illuminant. Since
collecting real-world data whose illuminant is known is difficult, mk (`) is typically trained
synthetically with random surfaces, which may not represent the statistics of natural scenes.
The second issue is that colors and illuminants live in an unbounded 3D space [1], unlike
reflectances which are bounded. In order to store a color distribution for each illuminant,
brightness variation needs to be artificially bounded. The third issue is storage. To reduce
the storage of the mk (`)?s, Barnard et al [1] store the color distribution only for illuminants
of a fixed brightness. However, as they describe, this introduces a bias in the estimation
they refer to as the ?discretization problem? and try to solve it by penalizing bright illuminants. The other part of the bias is due to using clipped counts in the likelihood. As
explained in section 2, a multinomial likelihood with clipped counts is a special case of the
Dirichlet-multinomial, and prefers images with a small number of different colors. This
bias can be removed using a different likelihood function, such as (11).

5 Parameter estimation
5.1 Reflectance Distribution
To implement the Bayesian algorithm, we need to learn the real-world frequencies mk of
quantized reflectance vectors. The direct approach to this would require a set of images
with ground truth information regarding the associated illumination parameters or, alternately, a set of images captured under a canonical illuminant and camera.
Unfortunately, it is quite difficult to collect a large number of images under controlled
conditions. To avoid this issue, we use bootstrapping, as described in [9], to approximate
the ground truth. The estimates from some ?base? color constancy algorithm are used as
a proxy for the ground truth. This might seem to be problematic in that it would limit any
algorithm based on these estimates to perform only as well as the base algorithm. However,
this need not be the case if the errors made by the base algorithm are relatively unbiased.

We used approximately 2300 randomly selected JPEG images from news sites on the web
for bootstrapping, consisting mostly of outdoor scenes, indoor news conferences, and sporting event scenes. The scale by max algorithm was used as our ?base? algorithm. Figure
1 is a plot of the probability distribution collected, where lighter regions represent higher
probability values. The distribution is highly structured and varies with the magnitude of
the channel response. This structure is important because it allows our algorithm to disambiguate between potential solutions to the ill-posed illumination estimation problem.
5.2 Pre-processing and quantization
To increase robustness, pre-processing is performed on the image, similar to that performed
in [3]. The first pre-processing step scales down the image to reduce noise and speed up
the algorithm. A new image is formed in which each pixel is the mean of an m by m
block of the original image. The second pre-processing step removes dark pixels from the
computation, which, because of noise and quantization effects do not contain reliable color
information. Pixels whose yr + yg + yb channel sum is less than a given threshold are
excluded from the computation.
In addition to the reflectance prior, the parameters of our algorithm are: the number of
reflectance histogram bins, the scale down factor, and the dark pixel threshold value. To set
these parameters values, the algorithm was run over a large grid of parameter variations and
performance on the tuning set was computed. The tuning set was a subset of the ?model?
data set described in [7] and disjoint from the test set. A total of 20 images were used, 10
objects imaged under 2 illuminants. (The ?ball2? object was removed so that there was no
overlap between the tuning and test sets.) For the purpose of speed, only images captured
with the Philips Ultralume and the Macbeth Judge II fluorescent illuminants were included.
The best set of parameters was found to be: 32 ? 32 ? 32 reflectance bins, scale down by
m = 3, and omit pixels with a channel sum less than 8/(3 ? 255).
5.3 Illuminant prior
To facilitate a direct comparison, we adopt the two illuminant priors from [3]. Each is
uniform over a subset of illuminants. The first prior, full set, discretizes the illuminants
uniformly in polar coordinates. The second prior, hull set, is a subset of full set restricted
to be within the convex hull of the test set illuminants and other real world illuminants.
Overall brightness, `r + `g + `b , is discretized in the range of 0 to 6 in 0.01 steps.

6 Experiments
6.1 Evaluation Specifics
To test the algorithms we use the publicly available real world image data set [2] used
by Barnard, Martin, Coath and Funt in a comprehensive evaluation of color constancy
algorithms in [3]. The data set consists of images of 30 scenes captured under 11 light
sources, for a total of 321 images (after the authors removed images which had collection
problems) with ground truth illuminant information provided in the form of an RGB value.
As in the ?rg error? measure of [3], illuminant error is measured in chromaticity space:
`1 = `r /(`r + `g + `b )
`2 = `g /(`r + `g + `b )
(24)
R(`? |`) = (`?1 ? `1 )2 + (`?2 ? `2 )2
(25)
The Bayesian algorithm is adapted to minimize this risk by computing the posterior mean
in chromaticity space. The performance of an algorithm on the test set is reported as the
square root of the average R(`? |`) across all images, referred to as the RMS error.

Table 1: The average error of several color constancy algorithms on the test set. The value
in parentheses is 1.64 times the standard error of the average, so that if two error intervals
do not overlap the difference is significant at the 95% level.
Algorithm
Scale by Max
Gamut Mapping without Segmentation
Gamut Mapping with Segmentation
Bayes with Bootstrap Set Model
Bayes with Tuning Set Model

RMS Error for Full Set
0.0584 (+/- 0.0034)
0.0524 (+/- 0.0029)
0.0426 (+/- 0.0023)
0.0442 (+/- 0.0025)
0.0344 (+/- 0.0017)

RMS Error for Hull Set
0.0584 (+/- 0.0034)
0.0461 (+/- 0.0025)
0.0393 (+/- 0.0021)
0.0351 (+/- 0.0020)
0.0317 (+/- 0.0017)

Scale by Max
Gamut Mapping without Segmentation
Gamut Mapping with Segmentation
Bayes with Bootstrap Set Model
Bayes with Tuning Set Model

Full Set
Hull Set
0.030

0.035

0.040

0.045

0.050

0.055

0.060

RMS error

Figure 2: A graphical rendition of table 1. The standard errors are scaled by 1.64, so that if
two error bars do not overlap the difference is significant at the 95% level.
6.2 Results
The results1 are summarized in Table 1 and Figure 2. We compare two versions of our
Bayesian method to the gamut mapping and scale by max algorithms. The appropriate
preprocessing for each algorithm was applied to the images to achieve the best possible
performance. (Note that we do not include results for color by correlation since the gamut
mapping results were found to be significantly better in [3].) In all configurations, our
algorithm exhibits the lowest RMS error except in a single case where it is not statistically different than that of gamut mapping. The differences for the hull set are especially
large. The hull set is clearly a useful constraint that improves the performance of all of the
algorithms evaluated.
The two versions of our Bayesian algorithm differ only in the data set used to build the
reflectance prior. The tuning set, while composed of separate images than the test set, is
very similar and has known illuminants, and, accordingly, gives the best results. Yet the
performance when trained on a very different set of images, the uncalibrated bootstrap set
of section 5.1, is not that different, particularly when the illuminant search is constrained.
The gamut mapping algorithm (called CRULE and ECRULE in [3]) is also presented in two
versions: with and without segmenting the images as a preprocessing step as described in
[3]. These results were computed using software provided by Barnard and used to generate
the results in [3]. In the evaluation of color constancy algorithms in [3] gamut mapping was
found on average to outperform all other algorithms when evaluated on real world images.
It is interesting to note that the gamut mapping algorithm is sensitive to segmentation. Since
fundamentally it should not be sensitive to the number of pixels of a particular color in the
image we must assume that this is because the segmentation is implementing some form of
noise filtering. The Bayesian algorithm currently does not use segmentation.
Scale by max is also included as a reference point and still performs quite well given its simplicity, often beating out much more complex constancy algorithms [8, 3]. Its performance
is the same for both illuminant sets since it does not involve a search over illuminants.
1

Result images can be found at http://www.cs.cmu.edu/?chuck/nips-2003/

Surprisingly, when the error of the Bayesian method is compared with the gamut mapping
method on individual test images, the correlation coefficient is -0.04. Thus the images
which confuse the Bayesian method are quite different from the images which confuse
gamut mapping. This suggests that an algorithm which could jointly model the image
properties exploited by both algorithms might give dramatic improvements. As an example of the potential improvement, the RMS error of an ideal algorithm whose error is the
minimum of Bayes and gamut on each image in the test set is only 0.019.

7 Conclusions and Future Work
We have demonstrated empirically that Bayesian color constancy with the appropriate nonGaussian models can outperform gamut mapping on a standard test set. This is true regardless of whether a calibrated or uncalibrated training set is used, or whether the full set
or a restricted set of illuminants is searched. This should give new hope to the pursuit of
statistical methods as a unifying framework for color constancy.
The results also suggest ways to improve the Bayesian algorithm. The particular image
model we have used, the normalized count model, is only one of many that could be tried.
This is simply an image modeling problem which can be attacked using standard statistical
methods. A particularly promising direction is to pursue models which can enforce constraints like that in the gamut mapping algorithm, since the images where Bayes has the
largest errors appear to be relatively easy for gamut mapping.
Acknowledgments
We would like to thank Kobus Barnard for making his test images and code publicly available. We would also like to thank Martial Hebert for his valuable insight and advice and
Daniel Huber and Kevin Watkins for their help in revising this document. This work was
sponsored in part by a fellowship from the Eastman Kodak company.
References
[1] K. Barnard, L. Martin, and B. Funt, ?Colour by correlation in a three dimensional colour space,?
Proceedings of the 6th European Conference on Computer Vision, pp. 275?289, 2000.
[2] K. Barnard, L. Martin, B. Funt, and A. Coath, ?A data set for colour research,? Color Research and Application, Volume 27, Number 3, pp. 147-151, 2002,
http://www.cs.sfu.ca/?colour/data/colour constancy test images/
[3] K. Barnard, L. Martin, A. Coath, and B. Funt, ?A comparison of color constancy algorithms;
Part Two. Experiments with Image Data,? IEEE Transactions in Image Processing, vol. 11. no.
9. pp. 985-996, 2002.
[4] D. H. Brainard and W. T. Freeman, ?Bayesian color constancy,? Journal of the Optical Society
of America A, vol. 14, no. 7, pp. 1393-1411, 1997.
[5] G. Buchsbaum, ?A spatial processor model for object colour perception,? Journal of the Franklin
Institute, vol. 10, pp. 1-26, 1980.
[6] G. D. Finlayson and S. D. Hordley and P. M. Hubel, ?Colour by correlation: a simple, unifying
approach to colour constancy,? The Proceedings of the Seventh IEEE International Conference
on Computer Vision, vol. 2, pp. 835-842, 1999.
[7] B. Funt and V. Cardei and K. Barnard, ?Learning color constancy,? Proceedings of Imaging
Science and Technology / Society for Information Display Fourth Color Imaging Conference.
pp. 58-60, 1996.
[8] B. Funt and K. Barnard and L. Martin, ?Is colour constancy good enough?,? Proceedings of the
Fifth European Conference on Computer Vision, pp. 445-459, 1998.
[9] B. Funt and V. Cardei. ?Bootstrapping color constancy,? Proceedings of SPIE: Electronic Imaging IV, 3644, 1999.
[10] H. J. Trussell and M. J. Vrhel, ?Estimation of illumination for color correction,? Proc ICASSP,
pp. 2513-2516, 1991.


----------------------------------------------------------------

title: 779-address-block-location-with-a-neural-net-system.pdf

Address Block Location with a Neural Net System

Eric Cosatto

Hans Peter Graf
AT&T Bell Laboratories
Crawfords Corner Road
Holmdel, NJ 07733, USA

Abstract
We developed a system for finding address blocks on mail pieces that can
process four images per second. Besides locating the address block, our
system also determines the writing style, handwritten or machine printed, and
moreover, it measures the skew angle of the text lines and cleans noisy
images. A layout analysis of all the elements present in the image is
performed in order to distinguish drawings and dirt from text and to separate
text of advertisement from that of the destination address.
A speed of more than four images per second is obtained on a modular
hardware platform, containing a board with two of the NET32K neural net
chips, a SPARC2 processor board, and a board with 2 digital signal
processors. The system has been tested with more than 100,000 images. Its
performance depends on the quality of the images, and lies between 85%
correct location in very noisy images to over 98% in cleaner images.

1

INTRODUCTION

The system described here has been integrated into an address reading machine developed for
the 'Remote Computer Reader' project of the United States Postal Service. While the actual
reading of the text is done by other modules, this system solves one of the major problems,
namely, finding reliably the location of the destination address. There are only a few constraints
on how and where an address has to be written, hence they may appear in a wide variety of
styles and layouts. Often an envelope contains advertising that includes images as well as text.

785

786

Graf and Cosatto

Sometimes. dirt covers part of the envelope image. including the destination address. Moreover.
the image captured by the camera is thresholded and the reader is given a binary image. This
binarization process introduces additional distortions; in particular. often the destination address
is surrounded by a heavy texture. The high complexity of the images and their poor quality make
it difficult to find the location of the destination address. requiring an analysis of all the elements
present in the image. Such an analysis is compute-intensive and in our system it turned out to
be the major bottleneck for a fast throughput. In fact. finding the address requires much more
computation than reading it. Special-purpose hardware in the form of the NET32K neural net
chips (Graf. Henderson. 90) is used to solve the address location problem.
Finding address blocks has been the focus of intensive research recently. as several companies
are developing address reading machines (United States Postal Service 92). The wide variety
of images that have to be handled has led other researchers to apply several different analysis
techniques to each image and then try to combine the results at the end. see e.g. (palumbo et a1.
92). In order to achieve the throughput required in an industrial application. special purpose
processors for finding connected components and/or for executing Hough transforms have been
applied.

In our system we use the NET32K processor to extract geometrical features from an image. The
high compute power of this chip allows the extraction of a large number of features
simultaneously. From this feature representation. an interpretation of the image's content can
then be achieved with a standard processor. Compared to an analysis of the original image. the
analysis of the feature maps requires several orders of magnitude less computation. Moreover.
the feature representation introduces a high level of robustness against noise. This paper gives
a brief overview of the hardware platfOlm in section 2 and then describes the algorithms to find
the address blocks in section 3.

2 THE HARDWARE
The NET32K system has been designed to serve as a high-speed image processing platform.
where neural nets as well as conventional algorithms can be executed. Three boards form the
whole system. Two NET32K neural net chips are integrated with a sequencer and data
formatting circuits on one board. The second board contains two digital signal processors
(DSPs). together with 6 Mbytes of memOly. Control of the whole system is provided by a board
containing a SPARC2 processor plus 64 Mbytes of memory. A schematic of this system is
shown in Figure 1.
Image buffering and communication with other modules in the address reader are handled by
the board with the SPARC2 processor. When an image is received. it is sent to the DSP board
and from there over to the NET32K processor. The feature maps produced by the NET32K
processor are stored on the DSP board. while the SPARC2 starts with the analysis of the feature
maps. The DSP's main task is formatting of the data. while the NET32K processor extracts all
the features. Its speed of computation is more than 100 billion multiply-accumulates per second
with operands that have one or two bits of resolution. Images with a size of Sl2xS 12 pixels are
processed at a rate of more than 10 frames per second. and 64 convolution kernels. each with
a size of 16x 16 pixels. can be scanned simultaneously over the image. Each such kernel IS
tuned to detect the presence of a feature. such as a line, an edge or a comer.

Address Block Location with a Neural Net System

....................................................................................

!

IN~K IN~
i

NET32K MODULE

r1~?~?~?:?A?T..

::::.fr=:::::::.~~~::=::::.~~=.=:::.=n:::=::
. . ._._. . . _. . .
v.
.

r ........-....-.-.......-.....

:

I

!

?

I

~

~

It

~

~

"

">

~

DSP32C

SRAM
1 MEG

~~
~"

"

~r

).

~lt

Afr

~U'I

~

DSP32C

DRAM
4 MEG

~ .... '1

'1

SRAM
1 MEG

l-.. . .- . _. .+. ._. .

L.. . ~~~. ~.~.~.~~.,. . -.-... ---.-..-..
~

. . . . ..l

:;:~

________________________

~.

SPARC

VME BUS

Figure 1: Schematic of the whole NET32K system. Each of the dashed
boxes represents one 6U VME board. The
conununication paths.

aITOWS

show the

3. SEQUENCE OF ALGORITHMS
The final result of the address block location system is a box describing a tight bmmd around
the destination address, if the address is machine printed. Of handwritten addresses, only the
zip code is read, and hence, one has to find a tight boundary around the zip code. This
information is then passed along to reader modules of the address reading machine. There is no
a priori knowledge about the writing style. Therefore the system first has to discriminate
between handwritten and machine Plinted text. At the end of the address block location process,
additional algorithms are executed to improve the accuracy of the reader. An overview of the
sequence of algorithms used to solve these tasks is shown in Figure 2. The whole process is
divided into three major steps: Preprocessing, feature extraction. and high-level analysis based
on the feature information.

3.1. Preprocessing
To quickly get an idea about the complexity of the image, a coarse evaluation of its layout is
done. By sampling the density of the black pixels in various places of the image, one can see
already whether the image is clean or noisy and whether the text is lightly printed or is dark.

787

788

Oraf and Cosatto

The images are divided into four categories, depending on their darkness and the level of noise.
'This infonnation is used in the subsequent processing to guide the choice of the features. Only
about one percent of the pixels are taken into account for this analysis, therefore, it can be
executed quickly on the SPARC2 processor.
clean. light

Preprocessing

clean. dark

IF.
~ ----.
....-=.... =-.P

~

-

~.=

::~,

16 Feature
maps

'

,,'

.....

-I'"

Extract features
NET32K

8 Feature
maps

Extract text lines
Cluster lines into groups
- - - Classify groups of lines
MACHINE PRINT
Analyse group of lines
Determine level of noise
Clean with NET32K;

HANDWRITIEN
Cluster text segments into lines
Analyse group of lines
Segment lines to find ZIP
Determine slanVskew angle;

Figure 2: Schematic of the sequence of algorithms for finding the
position of the address blocks.
3.2. Feature Extraction
After the preprocessing, the image is sent to the NET32K board where simple geometrical
features, such as edges, corners and lines are extracted. Up to 16 different feature maps are
generated, where a pixel in one of the maps indicates the presence of a feature in this location.
Some of these feature maps are used by the host processor, for example, to decide whether text
is handwritten or machine printed. Other feature maps are combined and sent once more
through the NET32K processor in order to search for combinations of features representing
more complex features. Typically, the feature maps are thresholded, so that only one bit per
pixel is kept. More resolution of the computation results is available from the neural net chips.
but in this way the amount of data that has to be analyzed is minimal. and one bit of resolution
turned out to be sufficient.
Examples of kernels used for the detection of strokes and text lines are shown in Figure 3. In
the chip, usually four line detectors of increasing height plus eight stroke detectors of different
orientations are stored. Other detectors are tuned to edges and strokes of machine printed text.
The line detectors respond to any black line of the proper height. Due to the large width of 16

Address Block Location with a Neural Net System

pixels. a kernel stretches over one or even several characters. Hence a text line gives a response
similar to that produced by a continuous black line. When the threshold is set properly. a text
line in the original image produces a continuous line in the feature map. even across the gaps
between characters and across small empty spaces between words. For an interpretation of a
line feature map only the left and right end points of each connected component are stored. In
this way one obtains a compact representation of the lines' positions that are well suited for the
high-level analysis of the layout.
Kernel: Line detector

?

Image

t

the NET32K syste

IC::GUla

Feature

Kernel: Stroke detector

Feature map

Figure 3:Examples of convolution kernels and their results. The kernels' sizes
are 16x16 pixels, and their pixels' values are + 1, O. -1 . The upper part illustrates
the response of a line detector on a machine printed text line. The lower kernel
extracts strokes of a celtain orientation from handwritten text.
Handwritten lines are detected by a second technique, because they are more irregular in height
and the characters may be spaced apm1 widely. Detectors for strokes, of the type shown in the
lower half ofFigw-e 3. are well suited for sensing the presence of handwritten text. The feature
maps resulting form handwritten text tend to exhibit blobs of pixels along the text line. By
smearing such feature maps in horizontal direction the responses of individual strokes are
merged into lines that can then be used in the same way as described for the machine printed
lines.
Horizontal smearing of text lines. combined with connected component analysis is a well-known

789

790

Graf and Cosatto

technique, often applied in layout analysis, to find words and whole lines of text. But when
applied to the pixels of an image, such an approach works well only in clean images. As soon
as there is noise present, this technique produces ilTegular responses. The key to success in a
real world environment is robustness against noise. By extracting features first and then
analyzing the feature maps, we drastically reduce the influence of noise. Each of the convolution
kernels covers a range of 256 pixels and its response depends on several dozens of pixels inside
this area. If pixels in the image are corrupted by noise, this has only a minor effect on the result
of the convolution and, hence, the appearance of the feature map.
When the analysis is started, it is unknown, whether the address is machine printed or hand
written. In order to distinguish between the two writing styles, a simple one-layer classifier
looks at the results of four stroke detectors and of four line detectors. It can determine reliably
whether text is handwritten or machine printed. Additional useful information that can be
extracted easily from the feature maps, is the skew angle of handwritten text. People tend to
write with a skew anywhere from -45 degrees to almost +90 degrees. In order to improve the
accuracy of a reader, the text is first deskewed. The most time consuming part of this operation
is to determine the skew angle of the writing. The stroke detector with the maximum response
over a line is a good indicator of the skew angle of the text. We compared this simple technique
with several alternatives and found it to be as reliable as the best other algorithm and much
faster to compute.

3.3. High-level Analysis
The results of the feature extraction process are line segments, each one marked as handwritten
or machine printed. Only the left and right end points of such lines are stored. At this point,
there may still be line segments in this group that do not correspond to text, but rather to solid
black lines or to line drawings. Therefore each line segment is checked, to determine whether
the ratio of black and white pixels is that found typically in text.
Blocks of lines are identified by clustering the line segments into groups. Then each block is
analyzed, to see whether it can represent the destination address. For this purpose such features
as the number of lines in the block, its size, position, etc. are used. These features are entered
into a classifier that ranks each of the blocks. Certain conditions, such as a size that is too large,
or if there are too many text lines in the block, will lead to an attempt to split blocks. If no good
result is obtained, clustering is tried again with a changed distance metric, where the horizontal
and the vertical distances between lines are weighted differently.

If an address is machine printed, the whole address block is passed on to the reader, since not
only the zip code, but the whole address, including the city name, the street name and the name
of the recipient have to be read. A big problem for the reader present images of poor quality,
particularly those with background noise and texture. State-of-the-art readers handle machine
printed text reliably if the image quality is good, but they may fail totally if the text is buried in
noise. For that reason, an address block is cleaned before sending it to the reader. Feature
extraction with the NET32K board is used once more for this task, this time with detectors tuned
to find all the strokes of the machine printed text. Applying stroke detectors with the proper
width allows a good discrimination between the text and any noise. Even texture that consists
of lines can be rejected reliably, if the line thickness of the texture is not the same as that of the
text.

Address Block Location with a Neural Net System

.:

.

"3"" /"ksiQ \i~.\. Cal! [~

"

', '

~"S'~e".I ?

. ~..~

~

===t

,o;;;r;;;a.e;2 .

. : .

.....

t1r

????ee-5AT'fO??t;~a.?????

"'~;Au'j'':f;,:.)i'''\i?bl,..~~???~t .......
"?~?S\;.?\?.cs.",~?A'???""

.

-~.W"..-,\e"'..4*!~ _Q33.~2..:-

Figure 4: Example of an envelope image at various stages of the processing. Top: The
result of the clustering process to find the bounding box of the address. Bottom right: The
text lines within the address block are marked. Bottom left: Cuts in the text line with the
zip code and below that the result of the reader. (The zip code is actually the second
segment sent to the reader; the first one is the string 'USA').

If the address is handwritten, only the zip code is sent to the reader. In order to find the zip code,
an analysis of the internal stmcture of the address block has to be done, which starts with finding
the true text lines. Handwritten lines are often not straight, may be heavily skewed, and may
contain large gaps. Hence simple techniques, such as connected component analysis, do not
provide proper results. ClusteJing of the line segments obtained from the feature maps, provides
a reliable solution of this problem. Once the lines are found, each one is segmented into words
and some of them are selected as candidates for the zip code and are sent to the reader. Figure
4 shows an example of an envelope image as it progresses through the various processing steps.
The system has been tested extensively on overall more than 100,000 images. Most of these
tests were done in the assembled address reader, but during development of the system, large

791

792

Graf and Cosatto

tests were also done with the address location module alone. One of the problems for evaluating
the peIformance is the lack of an objective quality measure. When has an address been located
correctly? Cutting off a small part of the address may not be detrimental to the final
interpretation, while a bounding box that includes some additional text may slow the reader
down too much. or it may throw off the interpretation. Therefore, it is not always clear when a
bounding box, describing the address' location, is tight enough. Another important factor
affecting the accw-acy numbers is, how many candidate blocks one actually considers. For all
these reasons, accw-acy numbers given for address block location have to be taken with some
caution. The results mentioned here were obtained by judging the images by eye. If images are
clean and the address is surrounded by a white space larger than two line heights, the location
is found correctly in more than 98% of the cases. Often more than one text block is found and
of these the destination address is the first choice in 90% of the images, for a typical layout. If
the image is very noisy, which actually happens surprisingly often, a tight bound around the
address is found in 85% of the cases. These results were obtained with 5,000 images, chosen
from more than 100,000 images to represent as much variety as possible. Of these 5,000 images
more than 1,200 have a texture around the address, and often this texture is so dark that a
human has difficulties to make out each character.

4. CONCLUSION
Most of our algorithms described here consist of two parts: feature extraction implemented with
a convolution and interpretation, typically implemented with a small classifier. Surprisingly
many algorithms can be cast into such a fOimat. This common framework for algorithms has
the advantage of facilitating the implementation, in particular when algorithms are mapped into
hardware. Moreover, the feature extraction with large convolution kernels makes the system
robust against noise. This robustness is probably the biggest advantage of our approach. Most
existing automatic reading systems are very good as long as the images are clean, but they
deteriorate rapidly with decreasing image quality.

'The biggest drawback of convolutions is that they require a lot of computation. In fact, without
special purpose hardware, convolutions are often too slow. Our system relies on the NET32K
new-al net chips to obtain the necessary throughput. The NET32K system is, we believe, at the
moment the fastest board system for this type of computation. This speed is obtained by
systematically exploiting the fact that only a low resolution of the computation is required. This
allows to use analog computation inside the chip and hence much smaller circuits than would
be the case in an all-digital circuit.

References
United States Postal Service, (1992), Proc. Advanced Technology Conf., Vol. 3, Section on
address block location: pp. 1221 - 1310.
P.W. Palumbo, S.N. Srihari, J. Soh, R. Sridhar, V. Demjanenko, (1992), !'Postal Address Block
Location in Real Time", IEEE COMPUTER, Vol. 25n, pp. 34 - 42.
H.P. Oraf and D. Henderson, (1990), "A Reconfigurable CMOS Neural Network", Digest
IEEE Int. Solid State Circuits Conf. p. 144.


----------------------------------------------------------------

title: 50-an-adaptive-and-heterodyne-filtering-procedure-for-the-imaging-of-moving-objects.pdf

662

AN ADAPTIVE AND HETERODYNE FILTERING PROCEDURE
FOR THE IMAGING OF MOVING OBJECTS
F. H. Schuling, H. A. K. Mastebroek and W. H. Zaagman
Biophysics Department, Laboratory for General Physics
Westersingel 34, 9718 eM Groningen, The Netherlands

ABSTRACT
Recent experimental work on the stimulus velocity dependent time resolving
power of the neural units, situated in the highest order optic ganglion of the
blowfly, revealed the at first sight amazing phenomenon that at this high level of
the fly visual system, the time constants of these units which are involved in the
processing of neural activity evoked by moving objects, are -roughly spokeninverse proportional to the velocity of those objects over an extremely wide range.
In this paper we will discuss the implementation of a two dimensional heterodyne
adaptive filter construction into a computer simulation model. The features of this
simulation model include the ability to account for the experimentally observed
stimulus-tuned adaptive temporal behaviour of time constants in the fly visual
system. The simulation results obtained, clearly show that the application of such
an adaptive processing procedure delivers an improved imaging technique of
moving patterns in the high velocity range.

A FEW REMARKS ON THE FLY VISUAL SYSTEM
The visual system of the diptera, including the blowfly Calliphora
erythrocephala (Mg.) is very regularly organized and allows therefore very precise
optical stimulation techniques. Also, long term electrophysiological recordings can
be made relatively easy in this visual system, For these reasons the blowfly (which
is well-known as a very rapid and 'clever' pilot) turns out to be an extremely
suitable animal for a systematic study of basic principles that may underlie the
detection and further processing of movement information at the neural level.
In the fly visual system the input retinal mosaic structure is precisely
mapped onto the higher order optic ganglia (lamina, medulla, lobula), This means
that each neural column in each ganglion in this visual system corresponds to a
certain optical axis in the visual field of the compound eye. In the lobula complex
a set of wide-field movement sensitive neurons is found, each of which integrates
the input signals over the whole visual field of the entire eye, One of these wide
field neurons, that has been classified as H I by Hausen J has been extensively
studied both anatomically2, 3, 4 as well as electrophysiologically5, 6, 7, The
obtained results generally agree very well with those found in behavioral
optomotor experiments on movement detection 8 and can be understood in terms of
Reichardts correlation model 9, 10.
The H I neuron is sensitive to horizontal movement and directionally
selective: very high rates of action potentials (spikes) up to 300 per second can be
recorded from this element in the case of visual stimuli which move horizontally
inward, i.e. from back to front in the visual field (pre/erred direction), whereas
movement horizontally outward, i.e, from front to back (null direction) suppresses
its activity,

? American Institute of Physics 1988

663

EXPERIMENTAL RESULTS AS A MODELLING BASE
When the H I neuron is stimulated in its preferred direction with a step wise
pattern displacement, it will respond with an increase of neural activity. By
repeating this stimulus step over and over one can obtain the averaged response:
after a 20 ms latency period the response manifests itself as a sharp increase in
average firing rate followed by a much slower decay to the spontaneous activity
level. Two examples of such averaged responses are shown in the Post Stimulus
Time Histograms (PSTH's) of figure 1. Time to peak and peak height are related
and depend on modulation depth, stimulus step size and spatial extent of the
stimulus. The tail of the responses can be described adequately by an exponential
decay toward a constant spontaneous firing rate:
R(t)=c+a - e( -t/1)

(1)

For each setting of the stimulus parameters, the response parameters,
defined by equation (1), can be estimated by a least-squares fit to the tail of the
PSTH. The smooth lines in figure 1 are the results of two such fits.

tlmsl
300

!~,

.o "
o

"o

'00 -

OJ

8

)0

w= 11'/s

~ I'JO

"

0\

~

tf

10

100

? M:O.'O
o MoO IO

" Mdl05

1.00

Fig.l

600

800

_ _ .L... _ - - ' -_

0.3

I

_---',_ _

L-'

_-----L,_ _-'--_ _

10)0
W ("lsI

100

A veraged responses (PSTH's) obtained from the H I neuron, being
adapted to smooth stimulus motion with velocities 0.36 /s (top) and
11 /s (bottom) respectively. The smooth lines represent least-squares
fits to the PSTH's of the form R(t)=c+a-e(-t/1). Values of f for the
two PSTH's are 331 and 24 ms respectively (de Ruyter van Steveninck et
al.7).
Fitted values of f as a function of adaptation velocity for three
modulation depths M. The straight line is a least-squares fit to represent
the data for M=0.40 in the region w:0.3-100 o/s. It has the form
f=Q - w-13 with Q=150 ms and 13=0.7 (de Ruyter van Steveninck et al. 7 ).
0

0

Fig.2

)00

664

Figure 2 shows fitted values of the response time constant T as a function of
the angular velocity of a moving stimulus (a square wave grating in most
experiments) which was presented to the animal during a period long enough to let
its visual system adapt to this moving pattern and before the step wise pattern
displacement (which reveals 1') was given. The straight line, described by
(2)

(with W in Is and T in ms) represents a least-squares fit to the data over the
velocity range from 0.36 to 125 0 Is. For this range, T varies from 320 to roughly
10 ms, with a=150?1O ms and ~=0.7?0.05. Defining the adaptation range of 1 as
that interval of velocities for which 1 decreases with increasing velocity, we may
conclude from figure 2 that within the adaptation range, 1 is not very sensitive to
the modulation depth.
The outcome of similar experiments with a constant modulation depth of the
pattern (M=0.40) and a constant pattern velocity but with four different values of
the contrast frequency fc (Le. the number of spatial periods per second that
traverse an individual visual axis as determined by the spatial wavelength As of the
pattern and the pattern velocity v according to fc=v lAs) reveal also an almost
complete independency of the behaviour of 1 on contrast frequency. Other
experiments in which the stimulus field was subdivided into regions with different
adaptation velocities, made clear that the time constants of the input channels of
the H I neuron were set locally by the values of the stimulus velocity in each
stimulus sub-region. Finally, it was found that the adaptation of 1 is driven by
the stimulus velocity, independent of its direction.
These findings can be summarized qualitatively as follows: in steady state,
the response time constants 1 of the neural units at the highest level in the fly
visual system are found to be tuned locally within a large velocity range
exclusively by the magnitude of the velocity of the moving pattern and not by its
direction, despite the directional selectivity of the neuron itself. We will not go
into the question of how this amazing adaptive mechanism may be hard-wired in
the fly visual system. Instead we will make advantage of the results derived thus
far and attempt to fit the experimental observations into an image processing
approach. A large number of theories and several distinct classes of algorithms to
encode velocity and direction of movement in visual systems have been suggested
by, for example, Marr and Ullman I I and van Santen and Sperling12.
We hypothesize that the adaptive mechanism for the setting of the time
constants leads to an optimization for the overall performance of the visual system
by realizing a velocity independent representation of the moving object. In other
words: within the range of velocities for which the time constants are found to be
tuned by the velocity, the representation of that stimulus at a certain level within
the visual circuitry, should remain independent of any variation in stimulus
velocity.
0

OBJECT MOTION DEGRADATION: MODELLING
Given the physical description of motion and a linear space invariant model,
the motion degradation process can be represented by the following convolution
integral:
co co

JJ

g(x,y)=
-00

(h(x - u,y-v) ? flu, v? dudv

-00

(3)

665

where f(u,v) is the object intensity at position (u,v) in the object coordinate
frame, h(x-u,y-v) is the Point Spread Function (PSF) of the imaging system,
which is the response at (x,y) to a unit pulse at (u,v) and g(x,y) is the image
intensity at the spatial position (x,y) as blurred by the imaging system. Any
possible additive white noise degradation of the already motion blurred image is
neglected in the present considerations.
For a review of principles and techniques in the field of digital image
degradation and restoration, the reader is referred to Harris 13, Sawchuk 14,
Sondhi 15, Nahi 16, A boutalib et al. 17, 18, Hildebrand 19, Rajala de Figueiredo20 .
It has been demonstrated first by Aboutalib et al.17 that for situations in which
the motion blur occurs in a straight line along one spatial coordinate, say along the
horizontal axis, it is correct to look at the blurred image as a collection of
degraded line scans through the entire image. The dependence on the vertical
coordinate may then be dropped and eq. (3) reduces to:

g~~

J~x-u)

-f(u)du

Given the mathematical description of the relative movement,
corresponding PSF can be derived exactly and equation (4) becomes:
g(x)=

k

b(x - u) - f(u)du

(4)

the

(5)

where R is the extent of the motion blur. Typically, a discrete version of (5),
applicable for digital image processing purposes, is described by:
L

g(k)=l: h(k-I)? f(l)

; k=I, ... ,N

(6)

I

where k and I take on integer values and L is related to the motion blur extent.
According to Aboutalib et al. 18 a scalar difference equation model (M,a,b,c)
can then be derived to model the motion degradation process:
x(k+l)

=

M? x(k)+a? f(k)

g(k) = b? x(k)+c ? f(k)

; k=I, ... ,N

(7)

h(i) = cof1(i)+Cl~(i-l)+ ...... +cmA(i-m)
where x(k) is the m-dimensional state vector at position k along a scan line, f(k) is
the input intensity at position k, g(k) is the output intensity, m is the blur extent,
N is the number of elements in a line, c is a scalar, M, a and b are constant
matrices of order (mxm), (mxl) and (lxm) respectively, containing the discrete
values Cj of the blurring PSF h(j) for j=O, ... ,m and 1::.(.) is the Kronecker delta
function.

666

INFLUENCE OF BOTH TIME CONSTANT AND VELOCITY
ON THE AMOUNT OF MOTION BLUR IN AN ARTIFICIAL
RECEPTOR ARRAY
To start with, we incorporate in our simulation model a PSF, derived from
equation (1), to model the performance of all neural columnar arranged filters in
the lobula complex, with the restriction that the time constants f remain fixed
throughout the whole range of stimulus velocities. Realization of this PSF can
easily be achieved via the just mentioned state space model.

300
250
I.

200
\

150

\

\

\

\
\

:3 100

.

<{

w

0

50

,,,
7

\
\

..

I.

..
", ,

,

.

\
\
\

\

\

\

\

\

\

\

\

", ,

\

""

"-

0

1a..::i 250
:::>

.....

~
<{

200
150
100

"

50

O~--~~----~~~~--~~--~

o

Fig.3

5

10

15

20

POSITION IN
ARTIFICIAL RECEPTOR ARRAY
?

upper part. Demonstration of the effect that an increase in magnitude of

the time constants of an one-dimensional array of filters will result in
increase in motion blur (while the pattern velocity remains constant).
Original pattern shown in solid lines is a square-wave grating with a
spatial wavelength equal to 8 artificial receptor distances. The three
other wave forms drawn, show that for a gradual increase increase in
magnitude of the time constants, the representation of the original
square-wave will consequently degrade. lower part. A gradual increase in
velocity of the moving square-wave (while the filter time constants are
kept fixed) results also in a clear increase of degradation.

667

First we demonstrate the effect that an increase in time constant (while the
pattern velocity remains the same) will result in an increase in blur. Therefore we
introduce an one dimensional array of filters all being equipped with the same
time constant in their impulse response. The original pattern shown in square and
solid lines in the upper part of figure 3 consists of a square wave grating with a
spatial period overlapping 8 artificial receptive filters. The 3 other patterns drawn
there show that for the same constant velocity of the moving grating, an increase
in the magnitude of the time constants of the filters results in an increased blur in
the representation of that grating. On the other hand, an increase in velocity
(while the time constants of the artificial receptive units remain the same) also
results in a clear increase in motion blur, as demonstrated in the lower part of
figure 3.
Inspection of the two wave forms drawn by means of the dashed lines in
both upper and lower half of the figure, yields the conclusion, that (apart from
rounding errors introduced by the rather small number of artificial filters
available), equal amounts of smear will be produced when the product of time
constant and pattern velocity is equal. For the upper dashed wave form the
velocity was four times smaller but the time constant four times larger than for its
equivalent in the lower part of the figure.

ADAPTIVE SCHEME
In designing a proper image processing procedure our next step is to
incorporate the experimentally observed flexibility property of the time constants
in the imaging elements of our device. In figure 4a a scheme is shown, which
filters the information with fixed time constants, not influenced by the pattern
velocity. In figure 4b a network is shown where the time constants also remain
fixed no matter what pattern movement is presented, but now at the next level of
information processing, a spatially differential network is incorporated in order to
enhance blurred contrasts.
In the filtering network in figure 4c , first a measurement of the magnitude
of the velocity of the moving objects is done by thus far hypothetically introduced
movement processing algorithms, modelled here as a set of receptive elements
sampling the environment in such a manner that proper estimation of local pattern
velocities can be done. Then the time constants of the artificial receptive elements
will be tuned according to the estimated velocities and finally the same
differential network as in scheme 4b , is used.
The actual tuning mechanism used for our simulations is outlined in figure
5: once given the range of velocities for which the model is supposed to be
operational, and given a lower limit for the time constant 'f min ('f min can be the
smallest value which physically can be realized), the time constant will be tuned to
a new value according to the experimentally observed reciprocal relationship, and
will, for all velocities within the adaptive range, be larger than the fixed minimum
value. As demonstrated in the previous section the corresponding blur in the
representation of the moving stimulus will thus always be larger than for the
situation in which the filtering is done with fixed and smallest time constants
.,. min. More important however is the fact that due to this tuning mechanism the
blur will be constant since the product of velocity and time constant is kept
constant. So, once the information has been processed by such a system, a velocity
independent representation of the image will be the result, which can serve as the
input for the spatially differentiating network as outlined in figure 4c .
The most elementary form for this differential filtering procedure is the one

668

in which the gradient of two filters K-I and K+l which are the nearest neighbors
of filter K, is taken and then added with a constant weighing factor to the central
output K as drawn in figure 4 b and 4 c , where the sign of the gradient depends on
the direction of the estimated movement. Essential for our model is that we claim
that this weighing factor should be constant throughout the whole set of filters
and for the whole high velocity range in which the heterodyne imaging has to be
performed. Important to notice is the existence of a so-called settling time, i.e. the
minimal time needed for our movement processing device to be able to accurately
measure the object velocity. [Note: this time can be set equal to zero in the case
that the relative stimulus velocity is known a priori, as demonstrated in figure 3].
Since, without doubt, within this settling period estimated velocity values will
come out erroneously and thus no optimal performance of our imaging device can
be expected, in all further examples, results after this initial settling procedure
will be shown.
2
3
,
5
A

B

;

C

Fig. 4

~

v

yV

9'

r39 rYO

/ y

i

7.' r~ ;/Y ?. Y
[~l [~l i~J
~'

't"if ~' n

Pattern movement in this figure is to the right.
A: Network consisting of a set of filters with a fixed, pattern velocity
independent, time constant in their impulse response.
B: Identical network as in figure 4A now followed by a spatially
differentiating circuitry which adds the weighed gradients of two
neighboring filter outputs K-l and K+I to the central filter output

K.
C: The time constants of the filtering network are tuned by a
hypothetical movement estimating mechanism, visualized here as a
number of receptive elements, of which the combined output tunes
the filters. A detailed description of this mechanism is shown in
figure 5. This tuned network is followed by an identical spatially
differentiating circuit as described in figure 4B.

669

increasing velocity

?

v

(<?s)

1:

1:
----_
.
decreasing time constant

Fig. 5

min

Detailed description of the mechanism used to tune the time constants.
The time constant f of a specific neural channel is set by the pattern
velocity according to the relationship shown in the insert, which is
derived from eq. (2) with cx=- I and 13= I.
6 r
i',

4r

"

,,'I,
,, ,,
"

I,

'"

\

2

=f

< 0

~

,

v
,

~

I
I
I

, ,,

w

o::;)

-

I

I
I

~

,,-

\

h

,/'

,

I

:J:

<

4V

r;-"
, ::-:-

- ~be-.--1
,

=-.:!

I

I

, -... -

-~

"
"\

:'

a.

2V

"V

2

- /,-----

o Wi

8V

12 V

I

J/---'1;"- -- ---

1:

16 V
J

\

. r------l...- --

POSITION IN ARTIFICIAL RECEPTOR ARRAY

Fig.6

Thick lines: square-wave stimulus pattern with a spatial wavelength
overlapping 32 artificial receptive elements. Thick lines: responses for 6
different pattern velocities in a system consisting of paralleling neural
filters equipped with time constants, tuned by this velocity, and followed
by a spatially differentiating network as described.
Dashed lines: responses to the 6 different pattern velocities in a filtering
system with fixed time constants, followed by the same spatial
differentiating circuitry as before. Note the sharp over- and under
shoots for this case.

670

Results obtained with an imaging procedure as drawn in figure 4 b and 4c
are shown in figure 6. The pattern consists of a square wave, overlapping 32
picture elements. The pattern moves (to the left) with 6 different velocities v, 2v,
4v, 8v, 12v, 16v. At each velocity only one wavelength is shown. Thick lines:
square wave pattern. Dashed lines: the outputs of an imaging device as depicted in
figure 4 b: constant time constants and a constant weighing factor in the spatial
processing stage. Note the large differences between the several outputs. Thin
continuous lines: the outputs of an imaging device as drawn in figure 4c: tuned
time constants according to the reciprocal relationship between pattern velocity
and time constant and a constant weighing factor in the spatial processing stage.
For further simulation details the reader is referred to Zaagman et al. 21 . Now the
outputs are almost completely the same and in good agreement with the original
stimulus throughout the whole velocity range.
Figure 7 shows the effect of the gradient weighing factor on the overall
filter performance, estimated as the improvement of the deblurred images as
compared with the blurred image, measured in dB. This quantitative measure has
been determined for the case of a moving square wave pattern with motion blur
7.-------~------~r-------_r------~

6

5

IX)

"0

4
0)

u
C
ItI

E

-

3

c-

o

~ 2

a.
c-

O)

~

;z:

1

0
-1

0

1

2
weighing factor

Fig. 7

3

4

?

Effect of the weighing factor on the overall filter performance. Curve
measured for the case of a moving square-wave grating. Filter
performance is estimated as the improvement in signal to noise ratio:
1=10? 1010g (

I:iI:j?V(i,j)-U(i,j?2)
I:iI: j? O(i,j) - u(i,j? 2

where u(i,j) is the original intensity at position (i,j) in the image, v(i,j)
is the intensity at the same position (i,j) in the motion blurred image and
O(i,j) is the intensity at (i,j) in the image, generated with the adaptive
tuning procedure.

671

extents comparable to those used for the simulations to be discussed in section IV.
From this curve it is apparent that for this situation there is an optimum value for
this weighing factor. Keeping the weight close to this optimum value will result in
a constant output of our adaptive scheme, thus enabling an optimal deblurring of
the smeared image of the moving object.
On the other hand, starting from the point of view that the time constants
should remain fixed throughout the filtering process, we should had have to tune
the gradient weights to the velocity in order to produce a constant output as
demonstrated in figure 6 where the dashed lines show strongly differing outputs of
a fixed time constant system with spatial processing with constant weight (figure
4b ). In other words, tuning of the time constants as proposed in this section results
in: I) the realization of the blur-constancy criterion as formulated previously, and
2) -as a consequence- the possibility to deblur the obtained image oPtimally with
one and the same weighing factor of the gradient in the final spatial processing
layer over the whole heterodyne velocity range.

COMPUTER SIMULATION RESULTS AND
CONCLUSIONS
The image quality improvement algorithm developed in the present
contribution has been implemented on a general purpose DG Eclipse Sjl40 minicomputer for our two dimensional simulations. Figure Sa shows an undisturbed
image, consisting of 256 lines of each 256 pixels, with S bit intensity resolution.
Figure Sb shows what happens with the original image if the PSF is modelled
according to the exponential decay (2). In this case the time constants of all
spatial information processing channels have been kept fixed. Again, information
content in the higher spatial frequencies has been reduced largely. The
implementation of the heterodyne filtering procedure was now done as follows:
first the adaptation range was defined by setting the range of velocities. This
means that our adaptive heterodyne algorithm is supposed to operate adequately
only within the thus defined velocity range and that -in that range- the time
constants are tuned according to relationship (2) and will always come out larger
than the minimum value 1 min. For demonstration purposes we set Q=I and /3=1 in
eq. (2), thus introducing the phenomenon that for any velocity, the two
dimensional set of spatial filters with time constants tuned by that velocity, will
always produce a constant output, independent of this velocity which introduces
the motion blur. Figure Sc shows this representation. It is important to note here
that this constant output has far more worse quality than any set of filters with
smallest and fixed time constants 1 min would produce for velocities within the
operational range. The advantage of a velocity independent output at this level in
our simulation model, is that in the next stage a differential scheme can be
implemented as discussed in detail in the preceding paragraph. Constancy of the
weighing factor which is used in this differential processing scheme is guaranteed
by the velocity independency of the obtained image representation.
Figure Sd shows the result of the differential operation with an optimized
gradient weighing factor. This weighing factor has been optimized based on an
almost identical performance curve as described previously in figure 7. A clear
and good restoration is apparent from this figure, though close inspection reveals
fine structure (especially for areas with high intensities) which is unrelated with
the original intensity distribution. These artifacts are caused by the phenomenon
that for these high intensity areas possible tuning errors will show up much more
pronounced than for low intensities.

672

Fig.8a
Fig.8b
Fig. 8c
Fig.8d

a

(

b

d

Original 256x256x8 bit picture.
Motion degraded image with a PSF derived from R(t)=c+a -e( -t/r).
where T is kept fixed to 12 pixels and the motion blur extent is 32
pixels.
Worst case, i.e. the result of motion degradation of the original image
with a PSF as in figure 8b , but with tuning of the time constants based
on the velocity.
Restored version of the degraded image using the heterodyne adaptive
processing scheme.

In conclusion: a heterodyne adaptive image processing technique, inspired by
the fly visual system, has been presented as an imaging device for moving objects.
A scalar difference equation model has been used to represent the motion blur
degradation process. Based on the experimental results described and on this state
space model, we developed an adaptive filtering scheme. which produces at a
certain level within the system a constant output, permitting further differential
operations in order to produce an optimally deblurred representation of the
moving object.

ACKNOWLEDGEMENTS
The authors wish to thank mT. Eric Bosman for his expert programming

673

assistance, mr. Franco Tommasi for many inspiring discussions and advises during
the implementation of the simulation model and dr. Rob de Ruyter van Steveninck
for experimental help. This research was partly supported by the Netherlands
Organization lor the Advancement 01 Pure Research (Z.W.O.) through the
foundation Stichting voor Biolysica.

REFERENCES
I. K. Hausen, Z. Naturforschung 31c, 629-633 (1976).
2. N. J. Strausfeld, Atlas of an insect brain (Springer Verlag, Berlin, Heidelberg,
New York, 1976).
3. K. Hausen, BioI. Cybern. 45, 143-156 (1982).
4. R. Hengstenberg, J. Compo Physiol. 149, 179-193 (1982).
5. W. H. Zaagman, H. A. K. Mastebroek, J. W. Kuiper, BioI. Cybern. 31, 163-168
( 1978).
6. H. A. K. Mastebroek, W. H. Zaagman, B. P. M. Lenting, Vision Res. 20, 467474 (1980)
7. R. R. de Ruyter van Steveninck, W. H. Zaagman, H. A. K. Mastebroek, BioI.
Cybern., 54, 223-236 (1986).
8. W. Reichardt, T. Poggio, Q. Rev. Biophys. 9, 311-377 (1976).
9. W. Reichardt, in Reichardt, W. (Ed.) Processing of optical Data by Organisms
and Machines (Academic Press, New York, 1969), pp. 465-493.
10. T. Poggio, W. Reichardt, Q. Rev. Bioph. 9, 377-439 (1976).
11. D. Marr, S. Ullman, Proc. R. Soc. Lond. 211, 151-180 (1981).
12. J. P. van Santen, G. Sperling, J. Opt. Soc. Am. A I, 451-473 (1984).
13. J. L. Harris SR., J. Opt. Soc. Am. 56, 569-574 (1966).
14. A. A. Sawchuk, Proc. IEEE, Vol. 60, No.7, 854-861 (1972).
15. M. M.Sondhi, Proc. IEEE, Vol. 60, No.7, 842-853 (1972).
16. N. E. Nahi, Proc. IEEE, Vol. 60, No.7, 872-877 (1972).
17. A. O. Aboutalib, L. M. Silverman, IEEE Trans. On Circuits And Systems TCAS 75, 278-286 (1975).
18. A. O. Aboutalib, M. S. Murphy, L.M. Silverman, IEEE Trans. Automat. Contr.
AC 22, 294-302 (1977).
19. Th. Hildebrand, BioI. Cybern. 36, 229-234 (1980).
20. S. A. Rajala, R. J. P. de Figueiredo, IEEE Trans. On Acoustics, Speech and
Signal Processing, Vol. ASSSP-29, No.5, 1033-1042 (1981).
21. W. H. Zaagman, H. A. K. Mastebroek, R. R. de Ruyter van Steveninck, IEEE
Trans, Syst. Man Cybern. SMC 13, 900-906 (1983).


----------------------------------------------------------------

title: 4346-select-and-sample-a-model-of-efficient-neural-inference-and-learning.pdf

Select and Sample ? A Model of Efficient
Neural Inference and Learning

Jacquelyn A. Shelton, J?org Bornschein, Abdul-Saboor Sheikh
Frankfurt Institute for Advanced Studies
Goethe-University Frankfurt, Germany
{shelton,bornschein,sheikh}@fias.uni-frankfurt.de

Pietro Berkes
Volen Center for Complex Systems
Brandeis University, Boston, USA

?
J?org Lucke
Frankfurt Institute for Advanced Studies
Goethe-University Frankfurt, Germany

berkes@brandeis.edu

luecke@fias.uni-frankfurt.de

Abstract
An increasing number of experimental studies indicate that perception encodes a
posterior probability distribution over possible causes of sensory stimuli, which
is used to act close to optimally in the environment. One outstanding difficulty
with this hypothesis is that the exact posterior will in general be too complex to
be represented directly, and thus neurons will have to represent an approximation
of this distribution. Two influential proposals of efficient posterior representation
by neural populations are: 1) neural activity represents samples of the underlying distribution, or 2) they represent a parametric representation of a variational
approximation of the posterior. We show that these approaches can be combined
for an inference scheme that retains the advantages of both: it is able to represent
multiple modes and arbitrary correlations, a feature of sampling methods, and it
reduces the represented space to regions of high probability mass, a strength of
variational approximations. Neurally, the combined method can be interpreted as
a feed-forward preselection of the relevant state space, followed by a neural dynamics implementation of Markov Chain Monte Carlo (MCMC) to approximate
the posterior over the relevant states. We demonstrate the effectiveness and efficiency of this approach on a sparse coding model. In numerical experiments on
artificial data and image patches, we compare the performance of the algorithms
to that of exact EM, variational state space selection alone, MCMC alone, and
the combined select and sample approach. The select and sample approach integrates the advantages of the sampling and variational approximations, and forms
a robust, neurally plausible, and very efficient model of processing and learning
in cortical networks. For sparse coding we show applications easily exceeding a
thousand observed and a thousand hidden dimensions.

1

Introduction

According to the recently quite influential statistical approach to perception, our brain represents
not only the most likely interpretation of a stimulus, but also its corresponding uncertainty. In
other words, ideally the brain would represent the full posterior distribution over all possible interpretations of the stimulus, which is statistically optimal for inference and learning [1, 2, 3] ? a
hypothesis supported by an increasing number of psychophysical and electrophysiological results
[4, 5, 6, 7, 8, 9].
1

Although it is generally accepted that humans indeed maintain a complex posterior representation,
one outstanding difficulty with this approach is that the full posterior distribution is in general very
complex, as it may be highly correlated (due to explaining away effects), multimodal (multiple
possible interpretations), and very high-dimensional. One approach to address this problem in neural
circuits is to let neuronal activity represent the parameters of a variational approximation of the real
posterior [10, 11]. Although this approach can approximate the full posterior, the number of neurons
explodes with the number of variables ? for example, approximation via a Gaussian distribution
requires N 2 parameters to represent the covariance matrix over N variables. Another approach
is to identify neurons with variables and interpret neural activity as samples from their posterior
[12, 13, 3]. This interpretation is consistent with a range of experimental observations, including
neural variability (which would result from the uncertainty in the posterior) and spontaneous activity
(corresponding to samples from the prior in the absence of a stimulus) [3, 9]. The advantage of
using sampling is that the number of neurons scales linearly with the number of variables, and
it can represent arbitrarily complex posterior distributons given enough samples. The latter part
is the issue: collecting a sufficient number of samples to form such a complex, high-dimensional
representation is quite time-costly. Modeling studies have shown that a small number of samples
are sufficient to perform well on low-dimensional tasks (intuitively, this is because taking a lowdimensional marginal of the posterior accumulates samples over all dimensions) [14, 15]. However,
most sensory data is inherently very high-dimensional. As such, in order to faithfully represent
visual scenes containing potentially many objects and object parts, one requires a high-dimensional
latent space to represent the high number of potential causes, which returns to the problem sampling
approaches face in high dimensions.
The goal of the line of research pursued here is to address the following questions: 1) can we find
a sophisticated representation of the posterior for very high-dimensional hidden spaces? 2) as this
goal is believed to be shared by the brain, can we find a biologically plausible solution reaching it?
In this paper we propose a novel approach to approximate inference and learning that addresses the
drawbacks of sampling as a neural processing model, yet maintains its beneficial posterior representation and neural plausibility. We show that sampling can be combined with a preselection of
candidate units. Such a selection connects sampling to the influential models of neural processing
that emphasize feed-forward processing ([16, 17] and many more), and is consistent with the popular view of neural processing and learning as an interplay between feed-forward and recurrent stages
of processing [18, 19, 20, 21, 12]. Our combined approach emerges naturally by interpreting feedforward selection and sampling as approximations to exact inference in a probabilistic framework
for perception.

2

A Select and Sample Approach to Approximate Inference

Inference and learning in neural circuits can be regarded as the task of inferring the true hidden
causes of a stimulus. An example is inferring the objects in a visual scene based on the image
projected on the retina. We will refer to the sensory stimulus (the image) as a data point, ~y =
(y1 , . . . , yD ), and we will refer to the hidden causes (the objects) as ~s = (s1 , . . . , sH ) with sh
denoting hidden variablePor hidden unit h. The data distribution can then be modeled by a generative
data model: p(~y | ?) = ~s p(~y | ~s, ?) p(~s | ?) with ? denoting the parameters of the model1 . If we
assume that the data distribution can be optimally modeled by the generative distribution for optimal
parameters ?? , then the posterior probability p(~s | ~y , ?? ) represents optimal inference given a data
point ~y . The parameters ?? given a set of N data points Y = {~y1 , . . . , ~yN } are given by the
maximum likelihood parameters ?? = argmax? {p(Y | ?)}.
A standard procedure to find the maximum likelihood solution is expectation maximization (EM).
EM iteratively optimizes a lower bound of the data likelihood by inferring the posterior distribution
over hidden variables given the current parameters (the E-step), and then adjusting the parameters to
maximize the likelihood of the data averaged over this posterior (the M-step). The M-step updates
typically depend only on a small number of expectation values of the posterior as given by
P
hg(~s)ip(~s | ~y (n) ,?) = ~s p(~s | ~y (n) , ?) g(~s) ,
(1)
where g(~s) is usually an elementary function of the hidden variables (e.g., g(~s) = ~s or g(~s) = ~s~sT
in the case of standard sparse coding). For any non-trivial generative model, the computation of
1
In the case of continuous variables the sum is replaced by an integral. For a hierarchical model, the prior
distribution p(~s | ?) may be subdivided hierarchically into different sets of variables.

2

expectation values (1) is the computationally demanding part of EM optimization. Their exact computation is often intractable and many well-known algorithms (e.g., [22, 23]) rely on estimations.
The EM iterations can be associated with neural processing by the assumption that neural activity represents the posterior over hidden variables (E-step), and that synaptic plasticity implements
changes to model parameters (M-step). Here we will consider two prominent models of neural processing on the ground of approximations to the expectation values (1) and show how they can be
combined.
Selection. Feed-forward processing has frequently been discussed as an important component of
neural processing [16, 24, 17, 25]. One perspective on this early component of neural activity is
as a preselection of candidate units or hypotheses for a given sensory stimulus ([18, 21, 26, 19]
and many more), with the goal of reducing the computational demand of an otherwise too complex
computation. In the context of probabilistic approaches, it has recently been shown that preselection
can be formulated as a variational approximation to exact inference [27]. The variational distribution
in this case is given by a truncated sum over possible hidden states:
p(~s | ~y (n) , ?)
p(~s, ~y (n) | ?)
p(~s | ~y (n) , ?) ? qn (~s; ?) = X
?(~s ? Kn ) = X
?(~s ? Kn ) (2)
p(~s 0 | ~y (n) , ?)
p(~s 0 , ~y (n) | ?)
~
s 0 ?Kn

~
s 0 ?Kn

where ?(~s ? Kn ) = 1 if ~s ? Kn and zero otherwise. The subset Kn represents the preselected
latent states. Given a data point ~y (n) , Eqn. 2 results in good approximations to the posterior if Kn
contains most posterior mass. Since for many applications the posterior mass is concentrated in
small volumes of the state space, the approximation quality can stay high even for relatively small
sets Kn . This approximation can be used to compute efficiently the expectation values needed in the
P
M-step (1):
s, ~y (n) | ?) g(~s)
~
s?Kn p(~
.
(3)
hg(~s)ip(~s | ~y (n) ,?) ? hg(~s)iqn (~s;?) = P
s 0 , ~y (n) | ?)
~
s 0 ?Kn p(~
Eqn. 3 represents a reduction in required computational resources as it involves only summations (or
integrations) over the smaller state space Kn . The requirement is that the set Kn needs to be selected
prior to the computation of expectation values, and the final improvement in efficiency relies on such
selections being efficiently computable. As such, a selection function Sh (~y , ?) needs to be carefully
chosen in order to define Kn ; Sh (~y , ?) efficiently selects the candidate units sh that are most likely
to have contributed to a data point ~y (n) . Kn can then be defined by:
Kn = {~s | for all h 6? I : sh = 0} ,

(4)

where I contains the H 0 indices h with the highest values of Sh (~y , ?) (compare Fig. 1). For sparse
coding models, for instance, we can exploit that the posterior mass lies close to low dimensional
subspaces to define the sets Kn [27, 28], and appropriate Sh (~y , ?) can be found by deriving efficiently computable upper-bounds for probabilities p(sh = 1 | ~y (n) , ?) [27, 28] or by derivations
based on taking limits for no data noise [27, 29]. For more complex models, see [27] (Sec. 5.3-4)
for a discussion of suitable selection functions. Often the precise form of Sh (~y , ?) has limited influence on the final approximation accuracy because a) its values are not used for the approximation
(3) itself and b) the size of sets Kn can often be chosen generously to easily contain the regions with
large posterior mass. The larger Kn the less precise the selection has to be. For Kn equal to the
entire state space, no selection is required and the approximations (2) and (3) fall back to the case of
exact inference.
Sampling. An alternative way to approximate the expectation values in eq. 1 is by sampling from
the posterior distribution, and using the samples to compute the average:
PM
1
hg(~s)ip(~s | ~y (n) ,?) ? M
s(m) ) with ~s(m) ? p(~s | ~y , ?).
(5)
m=1 g(~
The challenging aspect of this approach is to efficiently draw samples from the posterior. In a
high-dimensional sample space, this is mostly done by Markov Chain Monte Carlo (MCMC). This
class of methods draws samples from the posterior distribution such that each subsequent sample is
drawn relative to the current state, and the resulting sequence of samples form a Markov chain. In
the limit of a large number of samples, Monte Carlo methods are theoretically able to represent any
probability distribution. However, the number of samples required in high-dimensional spaces can
be very large (Fig. 1A, sampling).
3

A

MAP estimate

exact EM

preselection

X

X

p(~
s|~
y )g(~
s)

qn (~
s; ?)g(~
s)

~
s?Kn

~
s

selected units

Sh (~y (n) )

B

select and
sample

Kn

Kn

~smax
g(~
smax )

sampling

M
1 X
g(~
s)
M m=1
with
~
s(m) ? p(~
s|~
y (n) , ?)

C

M
1 X
g(~
s)
M m=1
with
~
s(m) ? qn (~
s; ?)

selected units

s1

s1

sH
Wdh

y1

sH
Wdh
yD

y1

yD

Figure 1: A Simplified illustration of the posterior mass and the respective regions each approximation approach uses to compute the expectation values. B Graphical model showing each connection Wdh between the observed variables ~y and hidden variables ~s, and how H 0 = 2 hidden
variables/units are selected to form a set Kn . C Graphical model resulting from the selection of
hidden variables and associated weights Wdh (black).
Select and Sample. Although preselection is a deterministic approach very different than the
stochastic nature of sampling, its formulation as approximation to expectation values (3) allows for
a straight-forward combination of both approaches: given a data point, ~y (n) , we first approximate
the expectation value (3) using the variational distribution qn (~s; ?) as defined by preselection (2).
Second, we approximate the expectations w.r.t. qn (~s; ?) using sampling. The combined approach
is thus given by:
PM
1
s(m) ) with ~s(m) ? qn (~s; ?),
(6)
hg(~s)ip(~s | ~y (n) ,?) ? hg(~s)iqn (~s;?) ? M
m=1 g(~
where ~s(m) denote samples from the truncated distribution qn . Instead of drawing from a distribution
over the entire state space, approximation (6) requires only samples from a potentially very small
subspace Kn (Fig. 1). In the subspace Kn , most of the original probability mass is concentrated in a
smaller volume, thus MCMC algorithms perform more efficiently, which results in a smaller space
to explore, shorter burn-in times, and a reduced number of required samples. Compared to selection
alone, the select and sample approach will represent an increase in efficiency as soon as the number
of samples required for a good approximation is less then the number of states in Kn .

3

Sparse Coding: An Example Application

We systematically investigate the computational efficiency, performance, and biological plausibility
of the select and sample approach in comparison with selection and sampling alone using a sparse
coding model of images. The choice of a sparse coding model has numerous advantages. First, it
is a non-trivial model that has been extremely well-studied in machine learning research, and for
which efficient algorithms exist (e.g., [23, 30]). Second, it has become a standard (albeit somewhat
simplistic) model of the organization of receptive fields in primary visual cortex [22, 31, 32]. Here
we consider a discrete variant of this model known as Binary Sparse Coding (BSC; [29, 27], also
compare [33]), which has binary hidden variables but otherwise the same features as standard sparse
coding versions. The generative model for BSC is expressed by
1?sh
QH
p(~s|?) = h=1 ? sh 1 ? ?
,
p(~y |~s, W, ?) = N (~y ; W ~s, ? 2 1) ,
(7)
where W ? RD?H denotes the basis vectors and ? parameterizes the sparsity (~s and ~y as above).
The M-step updates of the BSC learning algorithm (see e.g. [27]) are given by:

 T  ?1
PN
T  PN
W new =
y (n) h~s iqn
s ~s qn
,
(8)
n=1 ~
n=1 ~
(? 2 )new =

1
ND

2 
P 
  (n)
~y ? W ~s q,n ? new =
n

1
N

P

n

| < ~s >qn |, where |~x| =

1
H

P

h

xh . (9)




The only expectation values needed for the M-step are thus h~siqn and ~s~sT qn . We will compare
learning and inference between the following algorithms:
4

BSCexact . An EM algorithm without approximations is obtained if we use the exact posterior for
the expectations: qn = p(~s | ~y (n) , ?). We will refer to this exact algorithm as BSCexact . Although
directly computable, the expectation values for BSCexact require sums over the entire state space,
i.e., over 2H terms. For large numbers of latent dimensions, BSCexact is thus intractable.
BSCselect . An algorithm that more efficiently scales with the number of hidden dimensions is
obtained by applying preselection. PFor the BSC model we use qn as given in (3) and Kn =
{~s | (for all h 6? I : sh = 0) or
h sh = 1}. Note that in addition to states as in (4) we include all states with one non-zero unit (all singletons). Including them avoids EM iterations in the
initial phases of learning that leave some basis functions unmodified (see [27]). As selection function Sh (~y (n) ) to define Kn we use:
q
~ T / ||W
~ h ||) ~y (n) ,
~ h || = PD (Wdh )2 .
Sh (~y (n) ) = (W
with
||
W
(10)
h
d=1
~ h as a component
A large value of Sh (~y (n) ) strongly indicates that ~y (n) contains the basis function W
(see Fig. 1C). Note that (10) can be related to a deterministic ICA-like selection of a hidden state
~s(n) in the limit case of no noise (compare [27]). Further restrictions of the state space are possible
but require modified M-step equations (see [27, 29]), which will not be considered here.
BSCsample . An alternative non-deterministic approach can be derived using Gibbs sampling. Gibbs
sampling is an MCMC algorithm which systematically explores the sample space by repeatedly
drawing samples from the conditional distributions of the individual hidden dimensions. In other
words, the transition probability from the current sample to a new candidate sample is given by
current
p(snew
s\h
). In our case of a binary sample space, this equates to selecting one random axis
h |~
h ? {1, . . . , H} and toggling its bit value (thereby changing the binary state in that dimension),
leaving the remaining axes unchanged. Specifically, the posterior probability computed for each
candidate sample is expressed by:
p(sh = 1, ~s\h , ~y )?
,
(11)
p(sh = 1 | ~s\h , ~y ) =
p(sh = 0, ~s\h , ~y )? + p(sh = 1, ~s\h , ~y )?
where we have introduced a parameter ? that allows for smoothing of the posterior distribution.
To ensure an appropriate mixing behavior of the MCMC chains over a wide range of ? (note that
? is a model parameter that changes with learning), we define ? = ?T2 , where T is a temperature
parameter that is set manually and selected such that good mixing is achieved. The samples drawn
in this manner can then be used to approximate the expectation values in (8) to (9) using (5).
BSCs+s . The EM learning algorithm given by combining selection and sampling is obtained by
applying (6). First note that inserting the BSC generative model into (2) results in:
N (~y ; W ~s, ? 2 1) BernoulliKn (~s; ?)
?(~s ? Kn )
(12)
y ; W ~s 0 , ? 2 1) BernoulliKn (~s 0 ; ?)
~
s 0 ?Kn N (~
Q
where BernoulliKn (~s; ?) = h?I ? sh (1 ? ?)1?sh . The remainder of the Bernoulli distribution
cancels out. If we define ~s? to be the binary vector consisting of all entries of ~s of the selected
? ? RD?H 0 contains all basis functions of those selected, we observe that the
dimensions, and if W
distribution is equal to the posterior w.r.t. a BSC model with H 0 instead of H hidden dimensions:
? ~s?, ? 2 1 H 0 ) Bernoulli(~s?; ?)
N (~y ; W
p(~s? | ~y , ?) = P
? ~s? 0 , ? 2 1 H 0 ) Bernoulli(~s? 0 ; ?)
y; W
~
s? 0 N (~
qn (~s; ?)

=

P

Instead of drawing samples from qn (~s; ?) we can thus draw samples from the exact posterior w.r.t.
the BSC generative model with H 0 dimensions. The sampling procedure for BSCsample can thus
be applied simply by ignoring the non-selected dimensions and their associated parameters. For
different data points, different latent dimensions will be selected such that averaging over data points
can update all model parameters. For selection we again use Sh (~y , ?) (10), defining Kn as in (4),
where I now contains the H 0 ?2 indices h with the highest values of Sh (~y , ?) and two randomly
selected dimensions (drawn from a uniform distribution over all non-selected dimensions). The
two randomly selected dimensions fulfill the same purpose as the inclusion of singleton states for
BSCselect . Preselection and Gibbs sampling on the selected dimensions define an approximation to
the required expectation values (3) and result in an EM algorithm referred to as BSCs+s .
5

Complexity. Collecting the number of operations necessary to compute the expectation values for

all four BSC cases, we arrive at
O N S( |{z}
D + |{z}
1 + |{z}
H )
(13)
p(~
s,~
y)

h~
si

h~
s~
sT i

where S denotes the number of hidden states that contribute to the calculation of the expectation
values. For the approaches with preselection (BSCselect , BSCs+s ), all the calculations of the expectation values can be performed on the reduced latent space; therefore the H is replaced by H 0 . For
BSCexact this number scales exponentially in H: S exact = 2H , and in in the BSCselect case, it scales
0
exponentially in the number of preselected hidden variables: S select = 2H . However, for the sampling based approaches (BSCsample and BSCs+s ), the number S directly corresponds to the number
of samples to be evaluated and is obtained empirically. As we will show later, S s+s = 200 ? H 0 is
a reasonable choice for the interval of H 0 that we investigate in this paper (1 ? H 0 ? 40).

4

Numerical Experiments

We compare the select and sample approach with selection and sampling applied individually on
different data sets: artifical images and natural image patches. For all experiments using the two
sampling approaches, we draw 20 independent chains that are initialized at random states in order to
increase the mixing of the samples. Also, of the samples drawn per chain, 13 were used to as burn-in
samples, and 23 were retained samples.
Artificial data. Our first set of experiments investigate the select and sample approach?s convergence properties on artificial data sets where ground truth is available. As the following experiments
were run on a small scale problem, we can compute the exact data likelihood for each EM step in all
four algorithms (BSCexact , BSCselect , BSCsample and BSCs+s ) to compare convergence on ground
truth likelihood.
B

A

L(?)

C

1

EM step

BSCsample

BSCselect

BSCexact
50 1

EM step

50 1

EM step

BSCs+s
50 1

EM step

50

Figure 2: Experiments using artificial bars data with H = 12, D = 6 ? 6. Dotted line indicates the
ground truth log-likelihood value. A Random selection of the N = 2000 training data points ~y (n) .
B Learned basis functions Wdh after a successful training run. C Development of the log-likelihood
over a period of 50 EM steps for all 4 investigated algorithms.
~ gt
Data for these experiments consisted of images generated by creating H = 12 basis functions W
h
in the form of horizontal and vertical bars on a D = 6 ? 6 = 36 pixel grid. Each bar was randomly
gt
assigned to be either positive (Wdh
? {0.0, 10.0}) or negative (Whgt0 d ? {?10.0, 0.0}). N = 2000
(n)
data points ~y
were generated by linearly combining these basis functions (see e.g., [34]). Using
2
a sparseness value of ?gt = H
resulted in, on average, two active bars per data point. According to
the model, we added Gaussian noise (?gt = 2.0) to the data (Fig. 2A).
We applied all algorithms to the same dataset and monitored the exact likelihood over a period of 50
EM steps (Fig. 2C). Although the calculation of the exact likelihood requires O(N 2H (D + H)) operations, this is feasible for such a small scale problem. For models using preselection (BSCselect and
BSCs+s ), we set H 0 to 6, effectively halving the number of hidden variables participating in the
calculation of the expectation values. For BSCsample and BSCs+s we drew 200 samples from the
posterior p(~s | ~y (n) ) of each data point, as such the number of states evaluated totaled S sample =
200 ? H = 2400 and S s+s = 200 ? H 0 = 1200, respectively. To ensure an appropriate mixing
behavior annealing temperature was set to T = 50. In each experiment the basis functions were
initialized at the data mean plus Gaussian noise, the prior probability to ?init = H1 and the data
noise to the variance of the data. All algorithms recover the correct set of bases functions in > 50%
of the trials, and the sparseness prior ? and the data noise ? with high accuracy. Comparing the
computational costs of algorithms shows the benefits of preselection already for this small scale
problem: while BSCexact evaluates the expectation values using the full set of 2H = 4096 hidden
6

0

states, BSCselect only considers 2H + (H ? H 0 ) = 70 states. The pure sampling based approaches
performs 2400 evaluations while BSCs+s requires 1200 evaluations.
Image patches. We test the select and sample approach on natural image data at a more challenging scale, to include biological plausibility in the demonstration of its applicability to larger scale
problems. We extracted N = 40, 000 patches of size D = 26 ? 26 = 676 pixels from the van
Hateren image database [31] 2 , and preprocessed them using a Difference of Gaussians (DoG) filter,
which approximates the sensitivity of center-on and center-off neurons found in the early stages of
the mammalian visual processing. Filter parameters where chosen as in [35, 28]. For the following
experiments we ran 100 EM iterations to ensure proper convergence. The annealing temperature
was set to T = 20.

?40

104

B

SC
s

el

SC
s

# of states

400 ? H 0

B

100 ? H 0

s

10

3
ec
t

-5.53e7

105

le

-5.51e7

106

+

-5.49e7

p

L(?)

S = 200 ? H 0

107

D

SC
s

-5.47e7

B

C

am

B

# of states

A

Figure 3: Experiments on image patches with D = 26 ? 26, H = 800 and H 0 = 20. A Random
selection of used patches (after DoG preprocessing). B Random selection of learned basis functions
(number of samples set to 200). C End approx. log-likelihood after 100 EM-steps vs. number of
samples per data point. D Number of states that had to be evaluated for the different approaches.
The first series of experiments investigate the effect of the number of drawn samples on the performance of the algorithm (as measured by the approximate data likelihood) across the entire range
of H 0 values between 12 and 36. We observe with BSCs+s that 200 samples per hidden dimension
(total states = 200 ? H 0 ) are sufficient: the final value of the likelihood after 100 EM steps begins
to saturate. Particularly, increasing the number of samples does not increase the likelihood by more
than 1%. In Fig. 3C we report the curve for H 0 = 20, but the same trend is observed for all other
values of H 0 . In another set of experiments, we used this number of samples (200 ? H) in the pure
sampling case (BSCsample ) in order to monitor the likelihood behavior. We observed two consistent
trends: 1) the algorithm was never observed to converge to a high-likelihood solution, and 2) even
when initialized at solutions with high likelihood, the likelihood always decreases. This example
demonstrates the gains of using select and sample above pure sampling: while BSCs+s only needs
200 ? 20 = 4, 000 samples to robustly reach a high-likelihood solutions, by following the same
regime with BSCsample , not only did the algorithm poorly converge on a high-likelihood solution,
but it used 200 ? 800 = 160, 000 samples to do so (Fig. 3D).
Large scale experiment on image patches. Comparison of the above results shows that the most
efficient algorithm is obtained by a combination of preselection and sampling, our select and sample approach (BSCs+s ), with no or only minimal effect on the performance of the algorithm ? as
depicted in Fig. 2 and 3. This efficiency allows for applications to much larger scale problems
than would be possible by individual approximation approaches. To demonstrate the efficiency of
the combined approach we applied BSCs+s to the same image dataset, but with a very high number of observed and hidden dimensions. We extracted from the database N = 500, 000 patches of
size D = 40 ? 40 = 1, 600 pixels. BSCs+s was applied with the number of hidden units set to
H = 1, 600 and with H 0 = 34. Using the same conditions as in the previous experiments (notably
S = 200 ? H 0 = 64, 000 samples and 100 EM iterations) we again obtain a set of Gabor-like
basis functions (see Fig. 4A) with relatively very few necessary states (Fig. 4B). To our knowledge,
the presented results illustrate the largest application of sparse coding with a reasonably complete
representation of the posterior.

5

Discussion

We have introduced a novel and efficient method for unsupervised learning in probabilistic models ? one which maintains a complex representation of the posterior for problems consistent with
2
We restricted the set of images to 900 images without man-made structures (see Fig 3A). The brightest 2%
of the pixels were clamped to the max value of the remaining 98% (reducing influences of light-reflections)

7

A

B

1012

# of states

BSCselect : S = 2H

0

8

10

BSCs+s : S = 200 ? H 0

104

100
0

H0

34

40

Figure 4: A Large-scale application of BSCs+s with H 0 = 34 to image patches (D = 40?40 = 1600
pixels and H = 1600 hidden dimensions). A random selection of the inferred basis functions is
shown (see Suppl for all basis functions and model parameters). B Comparison the of computational
complexity: BSCselect scales exponentially with H 0 whereas BSCs+s scales linearly. Note the large
difference at H 0 = 34 as used in A.

real-world scales. Furthermore, our approach is biologically plausible and models how the brain
can make sense of its environment for large-scale sensory inputs. Specifically, the method could
be implemented in neural networks using two mechanisms, both of which have been independently
suggested in the context of a statistical framework for perception: feed-forward preselection [27],
and sampling [12, 13, 3]. We showed that the two seemingly contrasting approaches can be combined based on their interpretation as approximate inference methods, resulting in a considerable
increase in computational efficiency (e.g., Figs. 3-4).
We used a sparse coding model of natural images ? a standard model for neural response properties
in V1 [22, 31] ? in order to investigate, both numerically and analytically, the applicability and efficiency of the method. Comparisons of our approach with exact inference, selection alone, and sampling alone showed a very favorable scaling with the number of observed and hidden dimensions. To
the best of our knowledge, the only other sparse coding implementation that reached a comparable
problem size (D = 20 ? 20, H = 2 000) assumed a Laplace prior and used a MAP estimation of the
posterior [23]. However, with MAP estimations, basis functions have to be rescaled (compare [22])
and data noise or prior parameters cannot be inferred (instead a regularizer is hand-set). Our method
does not require any of these artificial mechanisms because of its rich posterior representation. Such
representations are, furthermore, crucial for inferring all parameters such as data noise and sparsity
(learned in all of our experiments), and to correctly act when faced with uncertain input [2, 8, 3].
Concretely, we used a sparse coding model with binary latent variables. This allowed for a systematic comparison with exact EM for low-dimensional problems, but extension to the continuous case
should be straight-forward. In the model, the selection step results in a simple, local and neurally
plausible integration of input data, given by (10). We used this in combination with Gibbs sampling,
which is also neurally plausible because neurons can individually sample their next state based on
the current state of the other neurons, as transmitted through recurrent connections [15]. The idea
of combining sampling with feed-forward mechanisms has previously been explored, but in other
contexts and with different goals. Work by Beal [36] used variational approximations as proposal
distributions within importance sampling, and Zhu et al. [37] guided a Metropolis-Hastings algorithm by a data-driven proposal distribution. Both approaches are different from selecting subspaces
prior to sampling and are more difficult to link to neural feed-forward sweeps [18, 21].
We expect the select and sample strategy to be widely applicable to machine learning models whenever the posterior probability masses can be expected to be concentrated in a small sub-space of the
whole latent space. Using more sophisticated preselection mechanisms and sampling schemes could
lead to a further reduction in computational efforts, although the details will depend in general on
the particular model and input data.
Acknowledgements. We acknowledge funding by the German Research Foundation (DFG) in the project
LU 1196/4-1 (JL), by the German Federal Ministry of Education and Research (BMBF), project 01GQ0840
(JAS, JB, ASS), by the Swartz Foundation and the Swiss National Science Foundation (PB). Furthermore,
support by the Physics Dept. and the Center for Scientific Computing (CSC) in Frankfurt are acknowledged.

8

References
[1] P. Dayan and L. F. Abbott. Theoretical Neuroscience. MIT Press, Cambridge, 2001.
[2] R. P. N. Rao, B. A. Olshausen, and M. S. Lewicki. Probabilistic Models of the Brain: Perception and
Neural Function. MIT Press, 2002.
[3] J. Fiser, P. Berkes, G. Orban, and M. Lengye. Statistically optimal perception and learning: from behavior
to neural representations. Trends in Cognitive Sciences, 14:119?130, 2010.
[4] M. D. Ernst and M. S. Banks. Humans integrate visual and haptic information in a statistically optimal
fashion. Nature, 415:419?433, 2002.
[5] Y. Weiss, E. P. Simoncelli, and E. H. Adelson. Motion illusions as optimal percepts. Nature Neuroscience,
5:598?604, 2002.
[6] K. P. Kording and D. M. Wolpert. Bayesian integration in sensorimotor learning. Nature, 427:244?247,
2004.
[7] J. M. Beck, W. J. Ma, R. Kiani, T. Hanksand A. K. Churchland, J. Roitman, M. N.. Shadlen, P. E. Latham,
and A. Pouget. Probabilistic population codes for bayesian decision making. Neuron, 60(6), 2008.
[8] J. Trommersh?auser, L. T. Maloney, and M. S. Landy. Decision making, movement planning and statistical
decision theory. Trends in Cognitive Science, 12:291?297, 2008.
[9] P. Berkes, G. Orban, M. Lengyel, and J. Fiser. Spontaneous cortical activity reveals hallmarks of an
optimal internal model of the environment. Science, 331(6013):83?87, 2011.
[10] W. J. Ma, J. M. Beck, P. E. Latham, and A. Pouget. Bayesian inference with probabilistic population
codes. Nature Neuroscience, 9:1432?1438, 2006.
[11] R. Turner, P. Berkes, and J. Fiser. Learning complex tasks with probabilistic population codes. In Frontiers
in Neuroscience, 2011. Comp. and Systems Neuroscience 2011.
[12] T. S. Lee and D. Mumford. Hierarchical Bayesian inference in the visual cortex. Journal of the Optical
Society of America A, 20(7):1434?1448, 2003.
[13] P. O. Hoyer and A. Hyvarinen. Interpreting neural response variability as Monte Carlo sampling from the
posterior. In Adv. Neur. Inf. Proc. Syst. 16, pages 293?300. MIT Press, 2003.
[14] E. Vul, N. D. Goodman, T. L. Griffiths, and J. B. Tenenbaum. One and done? Optimal decisions from
very few samples. In 31st Annual Meeting of the Cognitive Science Society, 2009.
[15] P. Berkes, R. Turner, and J. Fiser. The army of one (sample): the characteristics of sampling-based probabilistic neural representations. In Frontiers in Neuroscience, 2011. Comp. and Systems Neuroscience
2011.
[16] F. Rosenblatt. The perceptron: A probabilistic model for information storage and organization in the
brain. Psychological Review, 65(6), 1958.
[17] M. Riesenhuber and T. Poggio. Hierarchical models of object recognition in cortex. Nature Neuroscience,
211(11):1019 ? 1025, 1999.
[18] V. A. F.. Lamme and P. R. Roelfsema. The distinct modes of vision offered by feedforward and recurrent
processing. Trends in Neurosciences, 23(11):571 ? 579, 2000.
[19] A. Yuille and D. Kersten. Vision as bayesian inference: analysis by synthesis? Trends in Cognitive
Sciences, 10(7):301?308, 2006.
[20] G. E. Hinton, P. Dayan, B. J. Frey, and R. M. Neal. The ?wake-sleep? algorithm for unsupervised neural
networks. Science, 268:1158 ? 1161, 1995.
[21] E. K?orner, M. O. Gewaltig, U. K?orner, A. Richter, and T. Rodemann. A model of computation in neocortical architecture. Neural Networks, 12:989 ? 1005, 1999.
[22] B. A. Olshausen and D. J. Field. Emergence of simple-cell receptive field properties by learning a sparse
code for natural images. Nature, 381:607?609, 1996.
[23] H. Lee, A. Battle, R. Raina, and A. Ng. Efficient sparse coding algorithms. NIPS, 20:801?808, 2007.
[24] Y. LeCun. Backpropagation applied to handwritten zip code recognition.
[25] M. Riesenhuber and T. Poggio. How visual cortex recognizes objects: The tale of the standard model.
2002.
[26] T. S. Lee and D. Mumford. Hierarchical bayesian inference in the visual cortex. J Opt Soc Am A Opt
Image Sci Vis, 20(7):1434?1448, July 2003.
[27] J. L?ucke and J. Eggert. Expectation Truncation And the Benefits of Preselection in Training Generative
Models. Journal of Machine Learning Research, 2010.
[28] G. Puertas, J. Bornschein, and J. L?ucke. The maximal causes of natural scenes are edge filters. NIPS, 23,
2010.
[29] M. Henniges, G. Puertas, J. Bornschein, J. Eggert, and J. L?ucke. Binary sparse coding. Latent Variable
Analysis and Signal Separation, 2010.
[30] J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online learning for matrix factorization and sparse coding.
The Journal of Machine Learning Research, 11, 2010.
[31] J. Hateren and A. Schaaf. Independent Component Filters of Natural Images Compared with Simple Cells
in Primary Visual Cortex. Proc Biol Sci, 265(1394):359?366, 1998.
[32] D. L. Ringach. Spatial Structure and Symmetry of Simple-Cell Receptive Fields in Macaque Primary
Visual Cortex. J Neurophysiol, 88:455?463, 2002.
[33] M. Haft, R. Hofman, and V. Tresp. Generative binary codes. Pattern Anal Appl, 6(4):269?284, 2004.
[34] P. O. Hoyer. Non-negative sparse coding. Neural Networks for Signal Processing XII: Proceedings of the
IEEE Workshop, pages 557?565, 2002.
[35] J. L?ucke. Receptive Field Self-Organization in a Model of the Fine Structure in V1 Cortical Columns.
Neural Computation, 2009.
[36] M. J. Beal. Variational Algorithms for Approximate Bayesian Inference. PhD thesis, Gatsby Computational Neuroscience Unit, University College London., 2003.
[37] Z. Tu and S. C. Zhu. Image Segmentation by Data-Driven Markov Chain Monte Carlo. PAMI, 24(5):657?
673, 2002.

9


----------------------------------------------------------------

title: 4331-unsupervised-learning-models-of-primary-cortical-receptive-fields-and-receptive-field-plasticity.pdf

Unsupervised learning models of primary cortical
receptive fields and receptive field plasticity

Andrew Saxe, Maneesh Bhand, Ritvik Mudur, Bipin Suresh, Andrew Y. Ng
Department of Computer Science
Stanford University
{asaxe, mbhand, rmudur, bipins, ang}@cs.stanford.edu

Abstract
The efficient coding hypothesis holds that neural receptive fields are adapted to
the statistics of the environment, but is agnostic to the timescale of this adaptation,
which occurs on both evolutionary and developmental timescales. In this work we
focus on that component of adaptation which occurs during an organism?s lifetime, and show that a number of unsupervised feature learning algorithms can
account for features of normal receptive field properties across multiple primary
sensory cortices. Furthermore, we show that the same algorithms account for
altered receptive field properties in response to experimentally altered environmental statistics. Based on these modeling results we propose these models as
phenomenological models of receptive field plasticity during an organism?s lifetime. Finally, due to the success of the same models in multiple sensory areas, we
suggest that these algorithms may provide a constructive realization of the theory,
first proposed by Mountcastle [1], that a qualitatively similar learning algorithm
acts throughout primary sensory cortices.

1

Introduction

Over the last twenty years, researchers have used a number of unsupervised learning algorithms to
model a range of neural phenomena in early sensory processing. These models have succeeded in
replicating many features of simple cell receptive fields in primary visual cortex [2, 3], as well as
cochlear nerve fiber responses in the subcortical auditory system [4]. Though these algorithms do
not perfectly match the experimental data (see [5]), they continue to improve in recent work (e.g.
[6, 7]). However, each phenomenon has generally been fit by a different algorithm, and there has
been little comparison of an individual algorithm?s breadth in simultaneously capturing different
types of data. In this paper we test whether a single learning algorithm can provide a reasonable
fit to data from three different primary sensory cortices. Further, we ask whether such algorithms
can account not only for typical data from normal environments but also for experimental data from
animals raised with drastically different environmental statistics.
Our motivation for exploring the breadth of each learning algorithm?s applicability is partly biological. Recent reviews of the experimental literature regarding the functional consequences of plasticity have remarked on the surprising similarity in plasticity outcomes across sensory cortices [8, 9].
These empirical results raise the possibility that a single phenomenological model of plasticity (a
?learning algorithm? in our terminology) might account for receptive field properties independent of
modality. Finding such a model, if it exists, could yield broad insight into early sensory processing
strategies. As an initial step in this direction, we evaluate the match between current unsupervised
learning algorithms and receptive field properties in visual, auditory, and somatosensory cortex. We
find that many current algorithms achieve qualitatively similar matches to receptive field properties
in all three modalities, though differences between the models and experimental data remain.
In the second part of this paper, we examine the sensitivity of these algorithms to changes in their
input statistics. Most previous work that uses unsupervised learning algorithms to explain neural
1

receptive fields makes no claim about the relative contributions of adaptation on evolutionary as
compared to developmental timescales, but rather models the end point of these complex processes,
that is, the receptive field ultimately measured in the adult animal. In this work, we consider the alternative view that significant adaptation occurs during an organism?s lifetime, i.e., that the learning
algorithm operates predominantly during development rather than over the course of evolution.
One implication of lifetime adaptation is that experimental manipulations of early sensory experience should result in altered receptive field properties. We therefore ask whether current unsupervised learning algorithms can reproduce appropriately altered receptive field properties in response
to experimentally altered inputs. Our results show that the same unsupervised learning algorithm can
model normal and altered receptive fields, yielding an account of sensory receptive fields focused
heavily on activity dependent plasticity processes operating during an organism?s lifetime.

2

Modeling approach

We use the same three stage processing pipeline to model each modality; the first stage models peripheral end-receptors, namely rods and cones in the retina, hair cells in the cochlea, and mechanoreceptors in glabrous skin; the second stage crudely models subcortical processing as a whitening
transformation of the data; and the third stage models cortical receptive field plasticity mechanisms
as an unsupervised learning algorithm. We note that the first two stages cannot do justice to the
complexities of subcortical processing, and the simple approximation built into these stages limits
the quality of fit we can expect from the models.
We consider five unsupervised learning algorithms: independent component analysis [10], sparse
autoencoder neural networks [11], restricted Boltzmann machines (RBMs) [12], K-means [13], and
sparse coding [2]. These algorithms were chosen on two criteria. First, all of the algorithms share the
property of learning a sparse representation of the input, though they clearly differ in their details,
and have at least qualitatively been shown to yield Gabor-like filters when applied to naturalistic
visual input. Second, we selected algorithms to span a number of reasonable approaches and popular
formalisms, i.e., efficient coding ideas, backpropagation in artificial neural networks, probabilistic
generative models, and clustering methods. As we will show in the rest of the paper, in fact these
five algorithms turn out to yield very similar results, with no single algorithm being decisively better.
Each algorithm contains a number of parameters which control the learning process, which we fit to
the experimental data by performing extensive grid searches through the parameter space. To obtain
an estimate of the variability in our results, we trained multiple models at each parameter setting but
with different randomly drawn datasets and different initial weights. All error bars are the standard
error of the mean. The results reported in this paper are for the best-fitting parameter settings for
each algorithm per modality. We worried that we might overfit the experimental data due to the
large number of models we trained (? 60, 000). As one check against this, we performed a crossvalidation-like experiment by choosing the parameters of each algorithm to maximize the fit to one
modality, and then evaluating the performance of these parameters on the other two modalities. We
found that, though quantitatively the results are slightly worse as expected, qualitatively the results
follow the same patterns of which phenomena are well-fit (see supplementary material). Because
we have fit model parameters to experimental data, we cannot assess the efficiency of the resulting
code. Rather, our aim is to evaluate the single learning algorithm hypothesis, which is orthogonal to
the efficient coding hypothesis. A learning algorithm could potentially learn a non-efficient code, for
instance, but nonetheless describe the establishment of receptive fields seen in adult animals. Details
of the algorithms, parameters, and fitting methods can be found in the supplementary information.
Results from our grid searches are available at http://www.stanford.edu/?asaxe/rf_
plasticity.html.

3

Naturalistic experience and normal receptive field properties

In this section we focus on whether first-order, linear properties of neural responses can be captured
by current unsupervised learning algorithms applied to naturalistic visual, auditory, and somatosensory inputs. Such a linear description of neural responses has been broadly studied in all sensory
cortices [14, 15, 16, 17]. Though a more complete model would incorporate nonlinear components,
these more sophisticated nonlinear models often have as their first step a convolution with a linear
kernel (see [18] for an overview); and it is this kernel which we suggest might be learned over the
course of development, by a qualitatively similar learning algorithm across modalities.
2

Figure 1: Top left: K-means bases learned from natural images. Histograms: Black lines show
population statistics for K-means bases, gray bars show V1 simple cell data from Macaque. Far
right: Distribution of receptive field shapes; Red triangles are V1 simple cells from [5], blue circles
are K-means bases.

3.1

Primary visual cortex

A number of studies have shown that response properties in V1 can be successfully modeled using
a variety of unsupervised learning algorithms [2, 19, 3, 12, 10, 6, 7]. We replicate these findings for
the particular algorithms we employ and make the first detailed comparisons to experiment for the
sparse autoencoder, sparse RBM, and K-means algorithms.
Our natural image dataset consists of ten gray scale images of outdoor scenes [2]. Multiple nonoverlapping patches were sampled to form the first stage of our model, meant to approximate the
response of rods and cones. This raw data was then whitened using PCA whitening in the second stage of the model, corresponding to retinal ganglion or LGN responses.1 These inputs were
supplied to each of the five learning algorithms.
Fig. 1 shows example bases learned by the K-means algorithm. All five algorithms learn localized,
band-pass receptive field structures for a broad range of parameter settings, in qualitative agreement
with the spatial receptive fields of simple cells in primary visual cortex. To better quantify the match,
we compare five properties of model neuron receptive fields to data from macaque V1, namely the
spatial frequency bandwidth, orientation tuning bandwidth, length, aspect ratio, and peak spatial
frequency of the receptive fields. We compare population histograms of these metrics to those
measured in macaque V1 by [14, 15] as reported in [3]. Fig. 1 shows these histograms for the bestfitting K-means bases according to the average L1 distance between model and data histograms. For
all five algorithms, the histograms show general agreement with the distribution of parameters in
primary visual cortex except for the peak spatial frequency, consistent with the results of previous
studies for ICA and sparse coding [2, 3]. Additional plots for the other algorithms can be found in
the supplementary materials.
Next, we compare the shape of simulated receptive fields to experimentally-derived receptive fields.
As had been done for the experimental data, we fit Gabor functions to our simulated receptive fields
and calculated the ?normalized? receptive field sizes nx = ?x f and ny = ?y f where ?x is the
standard deviation of the gaussian envelope along the axis with sinusoidal modulation, ?y is the
stardard deviation of the gaussian envelope along the axis in which the filter is low pass, and f is
the frequency of the sinusoid. The parameters nx and ny measure the number of sinusoidal cycles
that fit within an interval of length ?x and ?y respectively. Hence they capture the number of
excitatory and inhibitory lobes of significant power in each receptive field. The right panel of Fig. 1
shows the distribution of nx and ny for K-means compared to those reported experimentally [5].
The model bases lie within the experimentally derived values, though our models fail to exhibit as
much variability in shape as the experimentally-derived data. As had been noted for ICA and sparse
coding in [5], all five of our algorithms fail to capture low frequency bases near the origin. These
low frequency bases correspond to ?blobs? with just a single excitatory region.
1

Taking the log of the image intensities before whitening, as in [3], yielded similar fits to V1 data.

3

Figure 2: Comparison to A1. Left: RBM bases. Second from left, top: Composite MTF in cat A1,
reproduced from [16]. Bottom: Composite MTF for RBM. Second from right, top: temporal MTF
in A1 (dashed gray) and for our model (black). Bottom: spectral MTF. Right, top: frequency sweep
preference. Bottom: Spectrum width vs center frequency for A1 neurons (red triangles) and model
neurons (blue circles).
3.2 Primary auditory cortex
In contrast to the large amount of work in the visual system, few efficient coding studies have
addressed response properties in primary auditory cortex (but see [20]). We base our comparison
on natural sound data consisting of a mixture of data from the Pittsburgh Natural Sounds database
and the TIMIT speech corpus. A mix of speech and natural sounds was reported to be necessary
to achieve a good match to auditory nerve fiber responses in previous sparse coding work [4]. We
transform the raw sound waveform into a representation of its frequency content over time meant
to approximate the response of the cochlea [21]. In particular, we pass the input sound signal to a
gammatone filterbank which approximates auditory nerve fiber responses [21]. The energy of the
filter responses is then summed within fixed time-bins at regular intervals, yielding a representation
similar to a spectrogram. We then whiten the data to model subcortical processing. Although there
is evidence for temporal whitening in the responses of afferents to auditory cortex, this is certainly
a very poor aproximation of subcortical auditory processing [16]. After whitening, we applied
unsupervised learning models, yielding the bases shown in Fig. 2 for RBMs. These bases map from
our spectrogram input to the model neuron output, and hence represent the spectrotemporal receptive
field (STRF) of the model neurons.
We then compared properties of our model STRFs to those measured in cortex. First, based on
the experiments reported in O?Connor et al. [22], we analyze the relationship between spectrum
bandwidth and center frequency. O?Connor et al. found a nearly linear relationship between these,
which matches well with the scaling seen in our model bases (see Fig. 2 bottom right). Next we
compared model receptive fields to the composite cortical modulation transfer function reported in
[16]. The modulation transfer function (MTF) of a neuron is the amplitude of the 2D Fourier transform of its STRF. The STRF contains one spectral and one temporal axis, and hence its 2D Fourier
transform contains one spectral modulation and one temporal modulation axis. The composite MTF
is the average of the MTFs computed for each neuron, and for all five algorithms it has a characteristic inverted ?V? shape evident in Fig. 2. Summing the composite MTF over time yields the
spectral MTF, which is low-pass for our models and well-matched to the spectral MTF reported in
cat A1[16]. Summing over the spectral dimension yields the temporal MTF, which is low-pass in
our models but band-pass in the experimental data. Finally, we investigate the preference of neurons
for upsweeps in frequency versus downsweeps, which can be cast in terms of the MTF by measuring
the energy in the left half compared to the right half. The difference in these energies normalized
by their sum is the spectrotemporal asymmetry, shown in Fig. 2 top right. All algorithms showed
qualitatively similar distributions of spectrotemporal asymmetry to that found in cat A1. Hence
the model bases are broadly consistent with receptive field properties measured in primary auditory
cortex such as a roughly linear scaling of center frequency with spectrum bandwidth; a low-pass
4

Figure 3: Left: Data collection pipeline. Center: Top two rows, sparse autoencoder bases. Bottom
two rows, first six PCA components. Right: Histograms of receptive field structure for the sparse
autoencoder algorithm. Black, model distribution. Gray, experimental data from [17]. (Best viewed
in color)
spectral MTF of appropriate slope; and a similar distribution of spectrotemporal asymmetry. The
models differ from experiment in their temporal structure, which is band-pass in the experimental
data but low-pass in our models.
3.3

Primary somatosensory cortex

Finally, we test whether these learning algorithms can model somatosensory receptive fields on
the hand. To enable this comparison we collected a naturalistic somatosensory dataset meant to
capture the statistics of contact points on the hand during normal primate grasping behavior. A
variety of objects were dusted with fine white powder and then grasped by volunteers wearing blue
latex gloves. To match the natural statistics of primate grasps, we performed the same grip types
in the same proportions as observed ecologically in a study of semi-free ranging Macaca mulatta
[23]. Points of contact were indicated by the transfer of powder to the gloved hand, which was then
placed palm-up on a green background and imaged using a digital camera. The images were then
post-processed to yield an estimate of the pressure applied to the hand during the grasp (Fig. 3, left).
The dataset has a number of limitations: it contains no temporal information, but rather records all
areas of contact for the duration of the grip. Most significantly, it contains only 1248 individual
grasps due to the high effort required to collect such data (?4 minutes/sample), and hence is an
order of magnitude smaller than the datasets used for the vision and auditory analyses. Given these
limitations, we decided to compare our receptive fields to those found in area 3b of primary somatosensory cortex. Neurons in area 3b respond to light cutaneous stimulation of restricted regions
of glabrous skin [24], the same sort of contact that would transfer powder to the glove. Area 3b
neurons also receive a large proportion of inputs from slowly adapting mechanoreceptor afferents
with sustained responses to static skin indentation [25], making the lack of temporal information
less problematic.
Bases learned by the algorithms are shown in Fig. 3. These exhibit a number of qualitative features
that accord with the biology. As in area 3b, the model receptive fields are localized to a single digit
[24], and receptive field sizes are larger on the palm than on the fingers [25]. These qualitative
features are not shared by PCA bases, which typically span multiple fingers. As a more quantitative
assesment, we compared model receptive fields on the finger tips to those derived for area 3b neurons
in [17]. We computed the ratio between excitatory and inhibitory area for each basis, and plot a
population histogram of this ratio, shown for the sparse autoencoder algorithm in the right panel of
Fig. 3. Importantly, because this comparison is based on the ratio of the areas, it is not affected by the
unknown scale factor between the dimensions of our glove images and those of the macaque hand.
We also plot the ratio of the excitatory and inhibitory mass, where excitatory and inhibitory mass is
defined as the sum of the positive and negative coefficients in the receptive field, respectively. We
find good agreement for all the algorithms we tested.
5

Figure 4: Top row: Input image; Resulting goggle image, reproduced from [26]; Our simulated
goggle image. Bottom row: Natural image; Simulated goggle image; Bases learned by sparse
coding. Right: Orientation histogram for model neurons is biased towards goggle orientation (90? ).

4

Adaptation to altered environmental statistics

Numerous studies in multiple sensory areas and species document plasticity of receptive field properties in response to various experimental manipulations during an organism?s lifetime. In visual
cortex, for instance, orientation selectivity can be altered by rearing animals in unidirectionally oriented environments [26]. In auditory cortex, pulsed-tone rearing results in an expansion in the area
of auditory cortex tuned to the pulsed tone frequency [27]. And in somatosensory cortex, surgically
fusing digits 3 and 4 (the middle and ring fingers) of the hand to induce an artificial digital syndactyly
(webbed finger) condition results in receptive fields that span these digits [28]. In this section we
ask whether the same learning algorithms that explain features of normal receptive fields can also
explain these alterations in receptive field properties due to manipulations of sensory experience.
4.1 Goggle-rearing alters V1 orientation tuning
The preferred orientations of neurons in primary visual cortex can be strongly influenced by altering
visual inputs during development; Tanaka et al. fitted goggles that severly restricted orientation
information to kittens at postnatal week three, and documented a massive overrepresentation of the
goggle orientation subsequently in primary visual cortex [26]. Hsu and Dayan [29] have shown
that an unsupervised learning algorithm, the product-of-experts model (closely related to ICA), can
reproduce aspects of the goggle-rearing experiment. Here we follow their methods, extending the
analysis to the other four algorithms we consider.
To simulate the effect of the goggles on an input image, we compute the 2D Fourier transform of
the image and remove all energy except at the preferred orientation of the goggles. We slightly
blur the resulting image with a small Gaussian filter. Because the kittens receive some period of
natural experience, we trained the models on mixtures of patches from natural and altered images,
adding one parameter in addition to the algorithmic parameters. Fig. 4 shows resulting receptive
fields obtained using the sparse coding algorithm. After learning, the preferred orientations of the
bases were derived using the analysis described in Section 3.1. All five algorithms demonstrated an
overrepresentation of the goggle orientation, consistent with the experimental data.
4.2 Pulsed-tone rearing alters A1 frequency tuning
Early sensory experience can also profoundly alter properties of neural receptive fields in primary
auditory cortex. Along similar lines to the results for V1 in Section 4.1, early exposure to a pulsed
tone can induce shifts in the preferred center frequency of A1 neurons. In particular, de VillersSidani et al. raised rats in an environment with a free field speaker emitting a tone with 40Hz amplitude modulation that repeatedly cycled on for 250ms then off for 500ms [27]. Mapping the preferred
center frequencies of neurons in tone-exposed rats revealed a corresponding overrepresentation in
A1 around the pulsed-tone frequency.
We instantiated this experimental paradigm by adding a pulsed tone to the raw sound waveforms
of the natural sounds and speech before computing the gammatone responses. Example bases for
ICA are shown in the center panel of Fig. 5, many of which are tuned to the pulsed-tone frequency.
We computed the preferred frequency of each model receptive field by summing the square of each
patch along the temporal dimension. The right panel of Fig. 5 shows population histograms of the
6

Figure 5: Left: Example spectrograms before and after adding a 4kHz pulsed tone. Center: ICA
bases learned from pulsed tone data. Right: Population histograms of preferred frequency reveal a
strong preference for the pulsed-tone frequency of 4kHz.
preferred center frequencies for models trained on natural and pulsed-tone data for ICA and Kmeans. We find that all algorithms show an overrepresentation in the frequency band containing the
tone, in qualitative agreement with the results reported in [27]. Intuitively, this overrepresentation
is due to the fact that many bases are necessary to represent the temporal information present in
the pulsed-tone, that is, the phase of the amplitude modulation and the onset or offset time of the
stimulus.
4.3 Artificial digital syndactyly in S1
Allard et al. [28] surgically fused adjacent skin on
digits 3 and 4 in adult owl monkeys to create an artificial sydactyly, or webbed finger, condition. After
14, 25, or 33 weeks, many receptive fields of neurons in area 3b of S1 were found to span digits 3
and 4, a qualitative change from the normally strict
localization of receptive fields to a single digit. Additionally, at the tips of digits 3 and 4 where there
is no immediately adjacent skin on the other digit,
some neurons showed discontinuous double-digit receptive fields that responded to stimulation on either Figure 6: Bases trained on artificial synfinger tip [28]. In contrast to the shifts in receptive dactyly data. Top row: Sparse coding. Botfield properties described in the preceding two sec- tom row: K-means.
tions, these striking changes are qualitatively different, and as such provide an important test for functional models of plasticity.
We modeled the syndactyly condition by fusing digits 3 and 4 of our gloves and collecting 782 additional grip samples according to the method in Section 3.3. Bases learned from this syndactyly
dataset are shown in Fig. 4.3. All models learned double-digit receptive fields that spanned digits
3 and 4, in qualitative agreement with the findings reported in [28]. Additionally, a small number
of bases contained discontinuous double-digit receptive fields consisting of two well-separated excitatory regions on the extreme finger tips (e.g., Fig. 4.3 top right). In contrast to the experimental
findings, model receptive fields spanning digits 3 and 4 also typically have a discontinuity along the
seam. We believe this reflects a limitation of our dataset; although digits 3 and 4 of our data collection glove are fused together and must move in concert, the seam between these digits remains inset
from the neighboring fingers, and hence grasps rarely transfer powder to this area. In the experiment,
the skin was sutured to make the seam flush with the neighboring fingers.

5

Discussion

Taken together, our results demonstrate that a number of unsupervised learning algorithms can account for certain normal and altered linear receptive field properties across multiple primary sensory
cortices. Each of the five algorithms we tested obtained broadly consistent fits to experimental data
in V1, A1 and S1. Although these fits were not perfect?notably, missing ?blob? receptive fields
in V1 and bandpass temporal structure in A1?they demonstrate the feasibility of applying a single
learning algorithm to experimental data from multiple modalities.
7

In no setting did one of our five algorithms yield qualitatively different results from any other. This
finding likely reflects the underlying similarities between the algorithms, which all attempt to find
a sparse representation of the input while preserving information about it. The relative robustness
of our results to the details of the algorithms offers one explanation of the empirical observation of
similar plasticity outcomes at a functional level despite potentially very different underlying mechanisms [8]. Even if the mechanisms differ, provided that they still incorporate some version of
sparsity, they can produce qualitatively very similar outcomes.
The success of these models in capturing the effects of experimental manipulations of sensory input
suggests that the adaptation of receptive field properties to natural statistics, as proposed by efficient
coding models, may occur significantly on developmental timescales. If so, this would allow the
extensive literature on plasticity to constrain further modeling efforts.
Furthermore, the ability of a single algorithm to capture responses in multiple sensory cortices shows
that, in principle, a qualitatively similar plasticity process could operate throughout primary sensory
cortices. Experimentally, such a possibility has been addressed most directly by cortical ?rewiring?
experiments, where visual input is rerouted to either auditory or somatosensory cortex [30, 31, 32,
33, 34, 35]. In neonatal ferrets, visual input normally destined for lateral geniculate nucleus can
be redirected to the auditory thalamus, which then projects to primary auditory cortex. Roe et
al. [32] and Sharma et al. [34] found that rewired ferrets reared to adulthood had neurons in auditory
cortex responsive to oriented edges, with orientation tuning indistinguishable from that in normal
V1. Further, Von Melchner et al. [33] found that rewired auditory cortex can mediate behavior such
as discriminating between different grating stimuli and navigating toward a light source. Rewiring
experiments in hamster corroborate these results, and in addition show that rewiring visual input to
somatosensory cortex causes S1 to exhibit light-evoked responses similar to normal V1 [31, 35].
Differences between rewired and normal cortices do exist?for example, the period of the orientation
map is larger in rewired animals [34]. However, these experiments are consistent with the hypothesis
that sensory cortices share a common learning algorithm, and that it is through activity dependent
development that they specialize to a specific modality. Our results provide a possible explanation
of these experiments, as we have shown constructively that the exact same algorithm can produce
V1-, A1-, or S1-like receptive fields depending on the type of input data it receives.
Acknowledgements We give warm thanks to Andrew Maas, Cynthia Henderson, Daniel Hawthorne
and Conal Sathi for code and ideas. This work is supported by the DARPA Deep Learning program
under contract number FA8650-10- C-7020. Andrew Saxe is supported by a NDSEG and Stanford
Graduate Fellowship.

References
[1] V.B. Mountcastle. An organizing principle for cerebral function: The unit module and the distributed
system., pages 7?50. MIT Press, Cambridge, MA, 1978.
[2] B.A. Olshausen and D.J. Field. Emergence of simple-cell receptive field properties by learning a sparse
code for natural images. Nature, 381(6583):607?9, 1996.
[3] J.H. van Hateren and D.L. Ruderman. Independent component analysis of natural image sequences
yields spatio-temporal filters similar to simple cells in primary visual cortex. Proc. R. Soc. Lond. B,
265(1412):2315?20, December 1998.
[4] E.C. Smith and M.S. Lewicki. Efficient auditory coding. Nature, 439(7079):978?82, 2006.
[5] D.L. Ringach. Spatial structure and symmetry of simple-cell receptive fields in macaque primary visual
cortex. J. Neurophysiol., 88(1):455?63, July 2002.
[6] M. Rehn and F.T. Sommer. A network that uses few active neurones to code visual input predicts the
diverse shapes of cortical receptive fields. J. Comput. Neurosci., 22(2):135?46, April 2007.
[7] G. Puertas, J. Bornschein, and J. Lucke. The Maximal Causes of Natural Scenes are Edge Filters. In
NIPS, 2010.
[8] D.E. Feldman. Synaptic mechanisms for plasticity in neocortex. Annu. Rev. Neurosci., 32:33?55, January
2009.
[9] K. Fox and R.O.L. Wong. A comparison of experience-dependent plasticity in the visual and somatosensory systems. Neuron, 48(3):465?77, November 2005.
[10] A.J. Bell and T.J. Sejnowski. The ?independent components? of natural scenes are edge filters. Vision
Res., 37(23):3327?38, December 1997.

8

[11] P. Vincent, H. Larochelle, Y. Bengio, and P. Manzagol. Extracting and Composing Robust Features with
Denoising Autoencoders. In ICML, 2008.
[12] H. Lee, C. Ekanadham, and A.Y. Ng. Sparse deep belief net model for visual area V2. In NIPS, 2008.
[13] A. Coates, H. Lee, and A.Y. Ng. An Analysis of Single-Layer Networks in Unsupervised Feature Learning. In AISTATS, 2011.
[14] R.L. De Valois, D.G. Albrecht, and L.G. Thorell. Spatial frequency selectivity of cells in macaque visual
cortex. Vision Res., 22(5):545?59, January 1982.
[15] R.L. De Valois, E.W. Yund, and N. Hepler. The orientation and direction selectivity of cells in macaque
visual cortex. Vision Res., 22(5):531?544, 1982.
[16] L.M. Miller, M.A. Escab??, H.L. Read, and C.E. Schreiner. Spectrotemporal receptive fields in the lemniscal auditory thalamus and cortex. J. Neurophysiol., 87(1):516?27, January 2002.
[17] J.J. DiCarlo, K.O. Johnson, and S.S. Hsiao. Structure of receptive fields in area 3b of primary somatosensory cortex in the alert monkey. J. Neurosci., 18(7):2626?45, April 1998.
[18] M. Carandini, J.B. Demb, V. Mante, D.J. Tolhurst, Y. Dan, B.A. Olshausen, J.L. Gallant, and N.C. Rust.
Do we know what the early visual system does? J. Neurosci., 25(46):10577?97, November 2005.
[19] A. Hyv?arinen, J. Hurri, and P.O. Hoyer. Natural Image Statistics. Springer, London, 2009.
[20] D.J. Klein, P. K?onig, and K.P. K?ording. Sparse Spectrotemporal Coding of Sounds. EURASIP J. Adv. Sig.
Proc., 7:659?667, 2003.
[21] R.D. Patterson, K. Robinson, J. Holdsworth, D. McKeown, C. Zhang, and M. Allerhand. Complex sounds
and auditory images. In Adv. Biosci., 1992.
[22] K.N. O?Connor, C.I. Petkov, and M.L. Sutter. Adaptive stimulus optimization for auditory cortical neurons. J. Neurophysiol., 94(6):4051?67, 2005.
[23] N.B.W. Macfarlane and M.S.A. Graziano. Diversity of grip in Macaca mulatta. Exp. Brain Res.,
197(3):255?68, August 2009.
[24] M. Sur. Receptive fields of neurons in areas 3b and 1 of somatosensory cortex in monkeys. Brain Res.,
198(2):465?471, October 1980.
[25] R.L. Paul, M.M. Merzenich, and H. Goodman. Representation of slowly and rapidly adapting cutaneous
mechanoreceptors of the hand in Brodmann?s areas 3 and 1 of Macaca mulatta. Brain Res., 36(2):229?49,
January 1972.
[26] S. Tanaka, J. Ribot, K. Imamura, and T. Tani. Orientation-restricted continuous visual exposure induces
marked reorganization of orientation maps in early life. NeuroImage, 30(2):462?77, April 2006.
[27] E. de Villers-Sidani, E.F. Chang, S. Bao, and M.M. Merzenich. Critical period window for spectral tuning
defined in the primary auditory cortex (A1) in the rat. J. Neurosci., 27(1):180?9, 2007.
[28] T. Allard, S.A. Clark, W.M. Jenkins, and M.M. Merzenich. Reorganization of somatosensory area 3b
representations in adult owl monkeys after digital syndactyly. J. Neurophysiol., 66(3):1048?58, September
1991.
[29] A.S. Hsu and P. Dayan. An unsupervised learning model of neural plasticity: Orientation selectivity in
goggle-reared kittens. Vision Res., 47(22):2868?77, October 2007.
[30] M. Sur, P. Garraghty, and A. Roe. Experimentally induced visual projections into auditory thalamus and
cortex. Science, 242(4884):1437?1441, December 1988.
[31] C. M?etin and D.O. Frost. Visual responses of neurons in somatosensory cortex of hamsters with experimentally induced retinal projections to somatosensory thalamus. PNAS, 86(1):357?61, January 1989.
[32] A.W. Roe, S.L. Pallas, Y.H. Kwon, and M. Sur. Visual projections routed to the auditory pathway in ferrets: receptive fields of visual neurons in primary auditory cortex. J. Neurosci., 12(9):3651?64, September
1992.
[33] L. von Melchner, S.L. Pallas, and M. Sur. Visual behaviour mediated by retinal projections directed to
the auditory pathway. Nature, 404(6780):871?876, 2000.
[34] J. Sharma, A. Angelucci, and M. Sur. Induction of visual orientation modules in auditory cortex. Nature,
404(April):841?847, 2000.
[35] D.O. Frost, D. Boire, G. Gingras, and M. Ptito. Surgically created neural pathways mediate visual pattern
discrimination. PNAS, 97(20):11068?73, September 2000.

9


----------------------------------------------------------------

title: 224-acoustic-imaging-computations-by-echolocating-bats-unification-of-diversely-represented-stimulus-features-into-whole-images.pdf

2

Simmons

Acoustic-Imaging Computations by Echolocating Bats:
Unification of Diversely-Represented Stimulus
Features into Whole Images.
James A. Simmons
Department of Psychology
and Section of Neurobiology,
Division of Biology and Medicine
Brown University, Providence, RI 02912.

ABSTRACT

The echolocating bat, Eptesicus fuscus, perceives the distance to
sonar targets from the delay of echoes and the shape of targets
from the spectrum of echoes. However, shape is perceived in
terms of the target's range proftle. The time separation of echo
components from parts of the target located at different distances
is reconstructed from the echo spectrum and added to the
estimate of absolute delay already derived from the arrival-time
of echoes. The bat thus perceives the distance to targets and
depth within targets along the same psychological range
dimension, which is computed. The image corresponds to the
crosscorrelation function of echoes. Fusion of physiologically
distinct time- and frequency-domain representations into a fmal,
common time-domain image illustrates the binding of withinmodality features into a unified, whole image. To support the
structure of images along the dimension of range, bats can
perceive echo delay with a hyperacuity of 10 nanoseconds.

Acoustic-Imaging Computations by Echolocating Bats
THE SONAR

O.~

BATS

Bats are flying mammals, whose lives are largely nocturnal. They have evolved
the capacity to orient in darkness using a biological sonar called echolocation,
which they use to avoid obstacles to flight and to detect, identify, and track flying
insects for interception (Griffm, 1958). Echolocating bats emit brief, mostly
ultrasonic sonar sounds and perceive objects from echoes that return to their ears.
The bat's auditory system acts as the sonar receiver, processing echoes to
reconstruct images of the objects themselves.
Many bats emit frequencymodulated (FM) signals; the big brown bat, Eptesicus fuscus, transmits sounds
with durations of several milliseconds containing frequencies from about 20 to
100 kHz arranged in two or three hannonic sweeps (Fig. 1). The images that
Eptesicus ultimately perceives retain crucial features of the original sonar wave100

N 80
I

~

Figure I: Spectrogram of a
sonar sound emitted by the
big brown bat, Eptesicus
fuscus (Simmons, 1989).

-;:: 60
()

c
~ 40
cO>

. . . 20
o~

.........

____________
1 msec

~

forms, thus revealing how echoes are processed to reconstruct a display of the
object itself. Several important general aspects of perception are embodied in
specific echo-processing operations in the bat's sonar. By recognizing constraints
imposed when echoes are encoded in terms of neural activity in the bat's auditory
system, recent experiments have identified a nove) use of time- and frequencydomain techniques as the basis for acoustic imaging in FM echolocation. The
intrinsically reciprocal properties of time- and frequency-domain representations
are exploited in the neural algorithms which the bat uses to unify disparate
features into whole images.

IMAGES OF SINGLE-GI.JNT TARGETS
A simple sonar target consists of a single reflecting point, or glint, located at a
discrete range and reflecting a single replica of the incident sonar signal. A
complex target consists of several glints at slightly different ranges. It thus reflects
compound echoes composed of individual replicas of the incident sound arriving

3

4

Simmons

at slightly different delays. To dctennine the distance to a target, or target range,
echolocating bats estimate the delay of echoes (Simmons, 1989). The bat's image
of a single-glint target is constructed around its estimate of echo delay, and the
shape of the image can be measured behaviorally. The performance of bats
trained to discriminate between echoes that jitter in delay and echoes that are
stationary in delay yields a graph of the image itself (Altes, 1989), together with
an indication of the accuracy of the delay estimate that underlies it (Simmons,
1979; Simmons, Perragamo, Moss, Stevenson, & Altes, in press). Fig. 2 shows

Jitter Performonce

Crasscorrelatian Function

1"-....

/\

./.\

-"..-. '\./.\ / j \\ /.'/
/

-50 -40 -030 -20 -10

0

10

~o

)0

Time (mIcroseconds)

40

50

.

-50 -40 -JO -20 -10

.

0

10

20

/-

.......
JO

40

50

Time (microseconds)

Figure 2: Graphs showing the bat's image of a single-glint target

from jitter discrimination experilnents (left) for comparison with
the crosscorrelation function of echoes (right). The zero point
on each time axis corresponds to the objective arrival-time of the
echoes (about 3 msec in this experiment; Sinlmons, Perragamo,
et aI., in press).

the image of a single-glint target perceived by Eptesicus, expressed in terms of
echo delay (58 Ilsec/cm of range).
Prom the bat's jitter discrimination
performance, the target is perceived at its true range. Also, the image has a fme
structure consisting of a central peak corresponding to the location of the target
and two prominent side-peaks as ghost images located about 35 }lsec or 0.6 cm
nearer and farther than the main peak. This image fme structure reflects the
composition of the waveform of the echoes themselves; it approximates the
crosscorrelation function of echoes (Fig. 2).
The discovery that the bat perceives an image corresponding to the crosscorrelation function of echoes provides a view of the hidden machinery of the
bat's sonar receiver. The bat's estimate of echo delay evidently is based upon a
capacity of the auditory system to represent virtually all of the information
available in echo waveforms that is relevant to determining delay, including the
phase of echoes relative to emissions (Simmons, Ferragamo, et al, in press). The
bat's initial auditory representation of these FM signals resembles spectrograms

Acoustic-Imaging Computations by Echolocating Bats

that consist of neural impulses marking the time-of-occurrence of succeSSlve
frequencies in the FM sweeps of the sounds (Fig. 3). Each nerve im150
120
100
80

N

60
50

I
.x:

40

"

.
\.

..

'-

.\~.

":.

I~

':~

.::\

.

-=\.

"I

25

.

~

.~

30

+,

~

)

'.

20
15
0

5

time (msec)

10

Hgure 3: Neural spectrograms
representing a sonar emission
(left) and an echo from a target
located about I m away (right),
The individual dots are neural
impulses
conveying
the
instantaneous frequency of the
FM sweeps (see Fig. 1). The 6msec time separation of the two
spectrograms indicates target
range in the bat's sonar receiver
(Simmons & Kick, 1984).

pulse travels in a "channel" that is tuned to a particular excitatory frequency
(Bodenhamer & Pollak, 1981) as a consequence of the frequency analyzing
properties of the cochlea.. The cochlear filters are followed by rectification and
low-pass filtering, so in a conventional sense the phase of the filtered signals is
destroyed in the course of forming the spectrograms. However, Fig. 2 shows that
the bat is able to reconstruct the crosscorrclation function of echoes from its
spectrogram-like auditory representation. The individual neural "points" in the
spectrogram signify instantaneous frequency, and the recovery of the fIne
structure in the image may exploit properties of instantaneous frequency when
the images are assembled by integrating numerous separate delay measurements
across different frequencies. The fact that the crosscorrelation function emerges
from these neural computations is provocative from theoretical and technological
viewpoints--the bat appears to employ novel real-time algorithms that can
transform echoes into spectrograms and then into the sonar ambiguity function
itself.
The range-axis image of a single-glint target has a fIne structure surrounding a
central peak that constitutes the bat's estimate of echo delay (Fig. 2). The width
of this peak corresponds to the limiting accuracy of the bat's delay estimate,
allowing for the ambiguity represented by the side-peaks located about 35 Jlsec
away. In Fig. 2, the data-points arc spaced 5 Jlsec apart along the time axis
(approximately the Nyquist sampling interval for the bat's signals), and the true
width of the central peak is poorly shown. Fig. 4 shows the performance of three
Eptesicus in an experiment to measure this width with smaller delay steps. The

5

6

Simmons
100

""
~
g.
~"

"~
u
c

e
"

/~-~-------

90

1'.
1

80

?

70

~.

60

I,'

50

1

0..

40

0

5

Oeloy line
Bot #I 1 . - .

Bot. 3 . - - .
Bot. 50-0
Cable
Bat.3
Bot'5

0--0

.-0

10 15 20 25 30 35 40 45 50 55 60
TIme (nanosetonds)

Figure 4: A graph of the
pelformance
of
Eptesicus
discriminating
echo-delay
jitters that change m small
steps.
The bats' limiting
acuity IS about 10 nsec for
75%
correct
responses
(Simmons, Perragamo, et a1.,
in press).

bats can detect a shift of as little as 10 nsec as a hyperacuity (Altes, 1989) for
echo delay in the jitter task. In estimating echo delay, the bat must integrate
spectrogram delay estimates across separate frequencies in the FM sweeps of
emissions and echoes (see Fig. 3), and it arrives at a very accurate composite
estimate indeed. Timing accuracy in the nanosecond range is a previously
unsuspected capahility of the nervous system, and it is likely that more complex
algorithms than just integration of information across frequencies lie behind this
fine acuity (see below on amplitude-latency trading and perceived delay).

IMAGES

OI<~

lWO-GLINT TARGETS

Complex targets such as airborne insects reflect echoes composed of several
replicas of the incident sound separated by short intervals of time (Simmons &
Chen, 1989). Por insect-sized targets, with dimensions of a few centimeters, this
time separation of echo components is unlikely to exceed 100 to 150 Jlsec.
Because the bat's signals arc several milliseconds long, the echoes from complex
targets thus will contain echo components that largely overlap. The auditory
system of Eptesicus has an integration-time of about 350 Jlsec for reception of
sonar echoes (Simmons, Freedman, et at., 1989). Two echo components that
arrive together within this integration-time will merge together into a single
compound echo having an arrival-time as a whole that indicates the delay of the
first echo component, and having a series of notches in its spectrum that indicates
the time separation of the first and second components. In the bat's auditory
representation, echo delay corresponds to the time separation of the emission and
echo spectrograms (see Fig. 3), while the notches in the compound echo
spectrum appear as '1101es" in the spectrogram--that is, as frequencies that fail to
appear in echoes. The location and spacing of these notches or holes in
frequency is related to the separation of the two echo components in lime. The
crucial point is that the constraint imposed by the 350-Jlsec integration-time for
echo reception disperses the information required to reconstruct the detailed range

Acoustic-Imaging Computations by Echolocating Bats
structure of the complex target into both the time and the frequency dimensions
of the neural spectrograms.
FptesicuJ extracts an estimate of the overall delay of the waveform of compound
echoes from two-glint targets. This time estimate leads to a range-axis image of
the closer of the two glints in the target (the target's leading edge). This part of
the image exhibits the same properties as the image of a single-glint target--it is
encoded by the time-of-occurrence of neural discharges in the spectrograms and it
resembles the crosscorrclation function for the first echo component (Simmons,
Moss, & Perragamo, 1990; Simmons, Ferragamo, et al., in press; see Simmons,
1989). The bat also perceives a range-axis image of the farther of the two glints
(the target's trailing edge). This image is located at a perceived distance that
corresponds to the bat's estimate of the time separation of the two echo
components that make up the compound echo. Fig. 5 shows the performance of
EpleJicuJ in a jitter discrimination experiment in which one of the

8,

a'i

i~~I

!

o

I

I

I

20

lime (psec)

I

40

,

Figure 5: A graph comparing
the crosscorrelation function of
echoes from a two-glint target
with a delay separation of 10
Jlsec (top) with the bat's jitter
discrimination
performance
using tlus compound echo as a
stimulus (bottom). The two
glints arc indicated as a I and
aI' (Simmons, 1989).

jittering stimulus echoes contained two replicas of the bat's emitted sound
separated by 10 Jlsec. The bat perceives two distinct reflecting points along the
range axis. Both glints appear as events along the range axis in a time-domain
image even though the existence of the second glint could only be inferred from
the frequency domain because the delay separation of 10 Jlsec is much shorter
than the receiver's integration time. The image of the second glint resembles the
crosscorrelation function of the later of the two echo components. The bat adds
it to the crosscorrelation function for the earlier component when the whole
image is formed.

7

8

Simmons
ACOUSTIC-IMA(;E PROCESSING BY FM BATS
Somehow Eptesicus recovers sufficient information from the timing of neural
discharges across the frequencies in the PM sweeps of emissions and echoes to
reconstruct the crosscorrelation function of echoes from the flfst glint in the
complex target and to estimate delay with nanosecond accuracy.
This
fundamentally time-domain image is derived from the processing of information
initially also represented in the time domain, as demonstrated by the occurrence
of changes in apparent delay as echo amplitude increases or decreases: The
location of the perceived crosscorrelation function for the flfst glint can be shifted
by predictable amounts along the time axis according to the separately-measured
amplitude-latency trading relation for Eptesicus (about -17 }lsec/dB; Simmons,
Moss, & Perragamo, 1990; Simmons, Ferragamo, et aI., in press), indicating that
neural response latency--that is, neural discharge timing--conveys the crucial
information about delay in the bat's auditory system.
The second glint in the complex target manifests itself as a crosscorrelation-like
image component, too. However, the bat must transform spectral information
into the time domain to arrive at such a time- or range-axis representation for the
second glint. This transformed time-domain image is added to the time-domain
image for the first glint in such a way that the absolute range of the second glint
is referred to that of the first glint. Shifts in the apparent range of the flfst glint
caused by neural discharges undergoing amplitude-latency trading will carry the
image of the second glint along with it to a new range value (Simmons, Moss, &
Perragamo, 1990). Evidently, the psychological dimension of absolute range
supports the image of the target as a whole. This helps to explain the bat's
extraordinary IO-nsec accuracy for perceiving delay. For the psychological range
or delay axis to accept fine-grain range infonnation about the separation of glints
in complex targets, its intrinsic accuracy must be adequate to receive the
information that is transformed from the frequency domain. The bat achieves
fusion of image components by transfonning one component into the numerical
fonnat for the other and then adding them together.
The experimental
dissociation of the images of the first and second glints from different effects of
latency shifts demonstrates the independence of their initial physiological
representations. Furthennore, the expected latency shift does not occur for
frequencies whose amplitudes are low because they coincide with spectral
notches; the bat's fine nanosecond acuity thus seems to involve removal of
discharges at "untrustworthy" frequencies prior to integration of discharge timing
across frequencies. The delay-tuning of neurons is usually thought to represent
the conversion of a temporal code (timing of neural discharges) into a "place"
code (the location of activity on the neural map). The bat's unusual acuity of 10
nsec suggests that this conversion of a temporal to a "place" code is only partial.

Acoustic-Imaging Computations by EchoIocating Bats

Not. only does the site of activity on the neural map convey information about
delay, but the timing of discharges in map neurons may also play a critical role in
the map-reading operation. The bat's fIne acuity may emerge in the behavioral
data because initial neural encoding of the stimulus conditions in the jitter task
involves the same parameter of neural rcsponses--timing--that later is intimately
associated with map-reading in the brain. Echolocation may thus fortuitously be
a good system in which to explore this basic perceptual process.

Ackllowledgmen ts

Research supported by grants from ONR, NIH, NIMH, ORF, and SOF.

References

R. A. Altes (1989) Ubiquity of hyperacuity, 1. Acoust. Soc. Am. 85: 943-952.
R. D. Bodenhamer & G. O. Pollak (1981) Time and frequency domain
processing in the inferior colliculus of echolocating bats, Hearing Res. 5:
317-355.
O. R. Griffin (1958) Listening in the Dark, Yale Univ. Press.
1. A. Simmons (1979) Perception of echo phase information in bat sonar,
Science, 207: 1336-1338.
1. A. Simmons (1989) A view of the world through the bat's ear: the formation of
acoustic images in echolocation, Cognition 33: 155-199.
J. A. Simmons & L. Chen (1989) The acoustic basis for target discrimination by
PM echolocating bats, 1. Acoust. Soc. Am. 86: 1333-1350.
1. A. Simmons, M. Ferragamo, C. F. Moss, S. B. Stevenson, & R. A. Altes (in

press) Discrimination of jittered sonar echoes by the echolocating bat,
Eplesicus fuscus: the shape of target unages in echolocation, 1. Compo
Physiol. A.
1. A. Simmons, E. G. Freedman, S. B. Stevenson, L. Chen, & T. 1. Wohlgenant
(1989) Clutter interference and the integration tUne of echoes in the
echolocating bat, Eptesicus fuscus, J. Acoust. Soc. Am. 86: 1318-1332.
1. A. Simmons & S. A. Kick (1984) Physiological mechanisms for spatial fIltering
and unage enhancement in the sonar of bats, Ann. Rev. Physiol. 46: 599614.

J. A. Simmons, C. F. Moss, & M. Ferragamo (1990) Convergence of temporal
and spectral information into acoustic images perceived by the
echolocating bat, Eptesicus fuscus, 1. Compo Physiol. A 166:

9


----------------------------------------------------------------

title: 6257-joint-line-segmentation-and-transcription-for-end-to-end-handwritten-paragraph-recognition.pdf

Joint Line Segmentation and Transcription for
End-to-End Handwritten Paragraph Recognition

Th?odore Bluche
A2iA SAS
39 rue de la Bienfaisance
75008 Paris
tb@a2ia.com

Abstract
Offline handwriting recognition systems require cropped text line images for both
training and recognition. On the one hand, the annotation of position and transcript
at line level is costly to obtain. On the other hand, automatic line segmentation
algorithms are prone to errors, compromising the subsequent recognition. In this
paper, we propose a modification of the popular and efficient Multi-Dimensional
Long Short-Term Memory Recurrent Neural Networks (MDLSTM-RNNs) to
enable end-to-end processing of handwritten paragraphs. More particularly, we
replace the collapse layer transforming the two-dimensional representation into
a sequence of predictions by a recurrent version which can select one line at a
time. In the proposed model, a neural network performs a kind of implicit line
segmentation by computing attention weights on the image representation. The
experiments on paragraphs of Rimes and IAM databases yield results that are
competitive with those of networks trained at line level, and constitute a significant
step towards end-to-end transcription of full documents.

1

Introduction

Offline handwriting recognition consists in recognizing a sequence of characters in an image of
handwritten text. Unlike printed texts, images of handwriting are difficult to segment into characters.
Early methods tried to compute segmentation hypotheses for characters, for example by performing a
heuristic over-segmentation, followed by a scoring of groups of segments (e.g. in [4]). In the nineties,
this kind of approach was progressively replaced by segmentation-free methods, where a whole
word image is fed to a system providing a sequence of scores. A lexicon constrains a decoding step,
allowing to retrieve the character sequence. Some examples are the sliding window approach [25], in
which features are extracted from vertical frames of the line image, or space-displacement neural
networks [4]. In the last decade, word segmentations were abandoned in favor of complete text line
recognition with statistical language models [10].
Nowadays, the state of the art handwriting recognition systems are Multi-Dimensional Long ShortTerm Memory Recurrent Neural Networks (MDLSTM-RNNs [18]), which consider the whole image,
alternating MDLSTM layers and convolutional layers. The transformation of the 2D structure into
a sequence is computed by a simple collapse layer summing the activations along the vertical axis.
Connectionist Temporal Classification (CTC [17]) allows to train the network to both align and
recognize sequences of characters. These models have become very popular and won the recent
evaluations of handwriting recognition [9, 34, 37].
However, current models still need segmented text lines, and full document processing pipelines
should include automatic line segmentation algorithms. Although the segmentation of documents
into lines is assumed in most descriptions of handwriting recognition systems, several papers or
30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

surveys state that it is a crucial step for handwriting text recognition systems [8, 28]. The need
of line segmentation to train the recognition system has also motivated several efforts to map a
paragraph-level or page-level transcript to line positions in the image (e.g. recently [7, 16]).
Handwriting recognition systems evolved from character to word segmentation, and to complete
line processing nowadays. The performance has always improved by making less segmentation
hypotheses. In this paper, we pursue this traditional tendency. We propose a model for multiline recognition based on the popular MDLSTM-RNNs, augmented with an attention mechanism
inspired from the recent models for machine translation [3], image caption generation [38], or speech
recognition [11, 12]. In the proposed model, the ?collapse? layer is modified with an attention
network, providing weights to modulate the importance given at different positions in the input. By
iteratively applying this layer to a paragraph image, the network can transcribe each text line in turn,
enabling a purely segmentation-free recognition of full paragraphs.
We carried out experiments on two public datasets of handwritten paragraphs: Rimes and IAM. We
report results that are competitive with the state-of-the-art systems, which use the ground-truth line
segmentation. The remaining of this paper is organized as follows. Section 2 presents methods related
to the one presented here, in terms of the tackled problem and modeling choices. In Section 3, we
introduce the baseline model: MDLSTM-RNNs. We expose in Section 4 the proposed modification,
and we give the details of the system. Experimental results are reported in Section 5, and followed by
a short discussion in Section 6, in which we explain how the system could be improved, and present
the challenge of generalizing it to complete documents.

2

Related Work

Our work is clearly related to MDLSTM-RNNs [18], which we improve by replacing the simple
collapse layer by a more elaborated mechanism, itself made of MDLSTM layers. The model we
propose iteratively performs an implicit line segmentation at the level of intermediate representations.
Classical text line segmentation algorithms are mostly based on image processing techniques and
heuristics. However, some methods were devised using statistical models and machine learning
techniques such as hidden Markov models [8], conditional random fields [21], or neural networks [24,
31, 32]. In our model, the line segmentation is performed implicitly and integrated in the neural
network. The intermediate features are shared by the transcription and the segmentation models, and
they are jointly trained to minimize the transcription error.
Recently, many ?attention-based? models were proposed to iteratively select in an encoded signal
the relevant parts to make the next prediction. This paradigm, already suggested by Fukushima
in 1987 [15], was successfully applied to various problems such as machine translation [3], image
caption generation [38], speech recognition [11, 12], or cropped words in scene text [27]. Attention
mechanisms were also parts of systems that can generate or recognize small pieces of handwriting
(e.g. a few digits with DRAW [20] or RAM [2], or short online handwritten sequences [19]). Our
system is designed to handle long sequences and multiple lines.
In the field of computer vision, and particularly object detection and recognition, many neural
architectures were proposed to both locate and recognize the objects, such as OverFeat [35] or spatial
transformer networks (STN [22]). In a sense, our model is quite related to the DenseCap model for
image captioning [23], itself similar to STNs. However, we do not aim at explicitly predicting line
positions, and STNs are not as good with a large amount of small objects.
We recently proposed an attention-based model to transcribe full paragraphs of handwritten text,
which predicts each character in turn [6]. Outputting one token at a time turns out to be prohibitive in
terms of memory and time consumption for full paragraphs, which typically contain about hundreds
of characters. In the proposed system, the encoded image is not summarized as a single vector at each
timestep, but as a sequence of vectors representing full text lines. It represents a huge speedup, and
a comeback to the original MDLSTM-RNN architecture, in which the collapse layer is augmented
with an MDLSTM attention network similar to the one presented in [6].

3

Handwriting Recognition with MDLSTM and CTC

MDLSTM-RNNs [18] were first introduced in the context of handwriting recognition. The Multi2

Figure 1: MDLSTM-RNN architecture for handwriting recognition. LSTM layers in four scanning
directions are followed by convolutions. The feature maps of the top layer are are summed in the
vertical dimension, and character predictions are obtained after a softmax normalization.
Dimensional Long Short-Term Memory layers scan the input in the four possible directions. The
LSTM cell inner state and output are computed from the states and outputs of previous positions in
the considered horizontal and vertical directions. Each MDLSTM layer is followed by a convolutional
layer. At the top of this network, there is one feature map for each character. These maps are collapsed
into a sequence of prediction vectors, normalized with a softmax activation. The whole architecture
is depicted in Figure 1. The Connectionist Temporal Classification (CTC [17]) algorithm, which
considers all possible labellings of the sequence, may be applied to train the network to recognize
text lines.
The 2D to 1D conversion happens in the collapsing layer, which computes a simple aggregation of
the feature maps into vector sequences, i.e. maps of height 1. This is achieved by a simple sum across
the vertical dimension:
H
X
zi =
aij
(1)
j=1

where zi is the i-th output vector and aij is the input feature vector at coordinates (i, j). All the
information in the vertical dimension is reduced to a single vector, regardless of its position in the
feature maps, preventing the recognition of multiple lines within this framework.

4

An Iterative Weighted Collapse for End-to-End Handwriting Recognition

In this paper, we replace the sum of Eqn. 1 by a weighted sum, in order to focus on a specific part of
the input. The weighted collapse is defined as follows:
(t)

zi

=

H
X

(t)

?ij aij

(2)

j=1
(t)

where ?ij are scalar weights between 0 and 1, computed at every time t for each position (i, j). The
weights are provided by a recurrent neural network, illustrated in Figure 2, enabling the recognition
of a text line at each timestep.

Figure 2: Proposed modification of the collapse layer. While the standard collapse (left, top) computes
a simple sum, the weighted collapse (right, bottom) includes a neural network to predict the weights
of a weighted sum.
3

This collapse, weighted with a neural network, may be interpreted as the ?attention? module of an
attention-based neural network similar to those of [3, 38]. This mechanism is differentiable and can
be trained with backpropagation. The complete architecture may be described as follows.
An encoder extracts feature maps from the input image I:
a = (aij )(i,j)?[1,W ]?[1,H] = Encoder(I)

(3)

where (i, j) are coordinates in the feature maps. In this work, the Encoder module is an MDLSTM
network with same architecture as the model presented in Section 3.
A weighted collapse provides a view of the encoded image at each timestep in the form of a weighted
sum of feature vector sequences. The attention network computes a score for the feature vectors at
every position:
(t)
?ij = Attention(a, ? (t?1) )
(4)
(t)

We refer to ? (t) = {?ij }(1?i?W, 1?j?H) as the attention map at time t, which computation depends
not only on the encoded image, but also on the previous attention features. A softmax normalization
is applied to each column:
X ?(t)
(t)
(t)
?ij = e?ij /
e ij0
(5)
j0

In this work, the Attention module is an MDLSTM network.
This module is applied several times to the features from the encoder. The output of the attention
module at iteration t, computed with Eqn. 2, is a sequence of feature vectors z, intended to represent
a text line. Therefore, we may see this module as a soft line segmentation neural network. The
advantages over the neural networks trained for line segmentation [13, 24, 32, 31] are that (i) it works
on the same features as those used for the transcription (multi-task encoder) and (ii) it is trained to
maximize the transcription accuracy (i.e. more closely related to the goal of handwriting recognition
systems, and easily interpretable).
A decoder predicts a character sequence from the feature vectors:
y = Decoder(z)
(1)

(2)

(6)

(T )

where z is the concatenation of z , z , . . . , z . Alternatively, the decoder may be applied to
z (i) s sub-sequences to get y (i) s and y is the concatenation of y (1) , y (2) , . . . , y (T ) .
In the standard MDLSTM architecture of Section 3, the decoder is a simple softmax. However, a
Bidirectional LSTM (BLSTM) decoder could be applied to the collapsed representations. This is
particularly interesting in the proposed model, as the BLSTM would potentially process the whole
paragraph, allowing a modeling of dependencies across text lines.
This model can be trained with CTC. If the line breaks are known in the transcript, the CTC could
be applied to the segments corresponding to each line prediction. Otherwise, one can directly apply
CTC to the whole paragraph. In this work, we opted for that strategy, with a BLSTM decoder applied
to the concatenation of all collapsing steps.

5
5.1

Experiments
Experimental Setup

We carried out the experiments on two public databases. The IAM database [29] is made of
handwritten English texts copied from the LOB corpus. There are 747 documents (6,482 lines) in the
training set, 116 documents (976 lines) in the validation set and 336 documents (2,915 lines) in the
test set. The Rimes database [1] contains handwritten letters in French. The data consist of a training
set of 1,500 paragraphs (11,333 lines), and a test set of 100 paragraphs (778 lines). We held out the
last 100 paragraphs of the training set as a validation set.
The networks have the following architecture. The encoder first computes a 2x2 tiling of the input
and alternate MDLSTM layers of 4, 20 and 100 units and 2x4 convolutions of 12 and 32 filters
with no overlap. The last layer is a linear layer with 80 outputs for IAM and 102 for Rimes. The
attention network is an MDLSTM network with 2x16 units in each direction followed by a linear
4

layer with one output, and a softmax on columns (Eqn. 5). The decoder is a BLSTM network with 256
units. Dropout is applied after each LSTM layer [33]. The networks are trained with RMSProp [36]
with a base learning rate of 0.001 and mini-batches of 8 examples, to minimize the CTC loss over
entire paragraphs. The measure of performance is the Character (or Word) Error Rate (CER%),
corresponding to the edit distance between the recognition and ground-truth, normalized by the
number of ground-truth characters.
5.2

Impact of the Decoder

In our model, the weighted collapse method is followed by a BLSTM decoder. In this experiment,
we compare the baseline system (standard collapse followed by a softmax) with the proposed model.
In order to dissociate the impact of the weighted collapse from that of the BLSTM decoder, we also
trained an intermediate architecture with a BLSTM layer after the standard collapse, but still limited
to text lines.
Table 1: Character Error Rates (%) of CTC-trained RNNs on 150 dpi images. The Standard models
are trained on segmented lines. The Attention models are trained on paragraphs.
Collapse
Standard
Standard
Attention

Decoder
Softmax
BLSTM + Softmax
BLSTM + Softmax

IAM
8.4
7.5
6.8

Rimes
4.9
4.8
2.5

The character error rates (CER%) on the validation sets are reported in Table 1 for 150dpi images.
We observe that the proposed model outperforms the baseline by a large margin (relative 20%
improvement on IAM, 50% on Rimes), and that the gain may be attributed to both the BLSTM
decoder, and the attention mechanism.
5.3

Impact of Line Segmentation

Our model performs an implicit line segmentation to transcribe paragraphs. The baseline considered
in the previous section is somehow cheating, because it was evaluated on the ground-truth line
segmentation. In this experiment, we add to the comparison the baseline models evaluated in a real
scenario where they are applied to the result of an automatic line segmentation algorithm.
Table 2: Character Error Rates (%) of CTC-trained RNNs on ground-truth lines and automatic
segmentation of paragraphs with different resolutions. The last column contains the error rate of the
attention-based model presented in this work, without an explicit line segmentation.
Database
IAM
Rimes

Resolution
150 dpi
300 dpi
150 dpi
300 dpi

GroundTruth
8.4
6.6
4.8
3.6

Line segmentation
Projection Shredding
15.5
9.3
13.8
7.5
6.3
5.9
5.0
4.5

Energy
10.2
7.9
8.2
6.6

This work
6.8
4.9
2.8
2.5

In Table 2, we report the CERs obtained with the ground-truth line positions, with three different
segmentation algorithms, and with our end-to-end system, on the validation sets of both databases with
different input resolutions. We see that applying the baseline networks on automatic segmentations
increases the error rates, by an absolute 1% in the best case. We also observe that the models are
better with higher resolutions.
Our models yield better performance than methods based on an explicit and automatic line segmentation, and comparable or better results than with ground-truth segmentation, even with a resolution
divided by two. Two factors may explain why our model yields better results than the line recognition
from ground-truth segmentation. First, the ground-truth line positions are bounding boxes that may
include some parts of adjacent lines and include irrelevant data, whereas the attention model will
focus on smaller areas. But the main reason is probably that the proposed model includes a BLSTM
operating on the whole paragraph, which may capture linguistic dependencies across text lines.
5

In Figure 3, we display a visualisation of the implicit line segmentation computed by the network.
Each color corresponds to one step of the iterative weighted collapse. On the images, the color
represents the weights given by the attention network (the transparency encodes their intensity). The
texts below are the predicted transcriptions, and chunks are colored according to the corresponding
timestep of the attention mechanism.

Figure 3: Transcription of full paragraphs of text and implicit line segmentation learnt by the network
on IAM (left) and Rimes (right). Best viewed in color.

5.4

Comparison to Published Results

In this section, we also compute the word error rates (WER%) and evaluate our models on the test
sets to compare the proposed approach to existing systems. For IAM, we applied a 3-gram language
model with a lexicon of 50,000 words, trained on the LOB, Brown and Wellington corpora.1 This
language model has a perplexity of 298 and out-of-vocabulary rate of 4.3% on the validation set (329
and 3.7% on the test set).
The results are presented in Table 3 for different input resolutions. When comparing the error rates, it
is important to note that all systems in the literature used an explicit (ground-truth) line segmentation
and a language model. [14, 26, 30] used a hybrid character/word language model to tackle the issue
of out-of-vocabulary words. Moreover, all systems except [30, 33] carefully pre-processed the line
image (e.g. corrected the slant or skew, normalized the height, ...), whereas we just normalized the
pixel values to zero mean and unit variance. Finally, [5] is a combination of four systems.
Table 3: Final results on Rimes and IAM databases

150 dpi
300 dpi

1

no language model
with language model
no language model
with language model
Bluche, 2015 [5]
Doetsch et al., 2014 [14]
Kozielski et al. 2013 [26]
Pham et al., 2014 [33]
Messina & Kermorvant, 2014 [30]

Rimes
WER% CER%
13.6
3.2
12.6

2.9

11.2
12.9
13.7
12.3
13.3

3.5
4.3
4.6
3.3
-

IAM
WER% CER%
29.5
10.1
16.6
6.5
24.6
7.9
16.4
5.5
10.9
4.4
12.2
4.7
13.3
5.1
13.6
5.1
19.1
-

The parts of the LOB corpus used in the validation and evaluation sets were removed.

6

On Rimes, the system applied to 150 dpi images already outperforms the state of the art in CER%,
while being competitive in terms of WER%. The system for 300 dpi images is comparable to the best
single system [33] in WER% with a significantly better CER%.
On IAM, the language model turned out to be quite important, probably because there is more
variability in the language.2 On 150 dpi images, the results are not too far from the state of the art
results. The WER% does not improve much on 300 dpi images, but we get a lower CER%. When
analysing the errors, we noticed that there is a lot of punctuation in IAM, which was often missed by
the attention mechanism. It may happen because punctuation marks are significantly smaller than
characters. With the attention-based collapse and the weighted sum, they will be more easily missed
than with the standard collapse, which gives the same weight to all vertical positions.

6

Discussion

Table 4: Comparison of decoding times of different methods: using ground-truth line information,
with explicit segmentation, with the attention-based method of [6] and with the system presented in
this paper.
Method
GroundTruth
Shredding
Scan, Attend and Read [6]
This Work

(crop+reco)
(segment+crop+reco)
(reco)
(reco)

Processing time (s)
0.21 ? 0.07
0.78 ? 0.26
21.2 ? 5.6
0.62 ? 0.14

The proposed model can transcribe complete paragraphs without segmentation and is orders of
magnitude faster that the model of [6] (cf. Table 4). However, the mechanism cannot handle
arbitrary reading orders. Rather, it implements a sort of implicit line segmentation. In the current
implementation, the iterative collapse runs for a fixed number of timesteps. Yet, the model can handle
a variable number of text lines, and, interestingly, the focus is put on interlines in the additional steps.
A more elegant solution should include the prediction of a binary variable indicating when to stop
reading.
Our method was applied to paragraph images, so a document layout analysis is required to detect
those paragraphs before applying the model. Naturally, the next step should be the transcription of
complex documents without an explicit or assumed paragraph extraction. The limitation to paragraphs
is inherent to this system. Indeed, the weighted collapse always outputs sequences corresponding to
the whole width of the encoded image, which, in paragraphs, may correspond to text lines. In order to
switch to full documents, several issues arise. On the one hand, the size of the lines is determined by
the size of the text block. Thus a method should be devised to only select a smaller part of the feature
maps, representing only the considered text line. This is not possible in the presented framework. A
potential solution could come from spatial transformer networks [22], performing a differentiable
crop. On the other hand, training will in practice become more difficult, not only because of the
complexity of the task, but also because the reading order of text blocks in complex documents cannot
be exactly inferred in many cases (even defining arbitrary rules may be tricky).

7

Conclusion

We have presented a model to transcribe full paragraphs of handwritten texts without an explicit
line segmentation. Contrary to classical methods relying on a two-step process (segment, then
recognize), our system directly considers the paragraph image without an elaborated pre-processing,
and outputs the complete transcription. We proposed a simple modification of the collapse layer
in the standard MDLSTM architecture to iteratively focus on single text lines. This implicit line
segmentation is learnt with backpropagation along with the rest of the network to minimize the
CTC error at the paragraph level. We reported error rates comparable to the state of the art on two
public databases. After switching from explicit to implicit character, then word segmentation for
handwriting recognition, we showed that line segmentation can also be learnt inside the transcription
model. The next step towards end-to-end handwriting recognition is now at the full page level.
2

A simple language model yields a perplexity of 18 on Rimes [5].

7

References
[1] E. Augustin, M. Carr?, E. Grosicki, J.-M. Brodin, E. Geoffrois, and F. Preteux. RIMES evaluation campaign
for handwritten mail processing. In Proceedings of the Workshop on Frontiers in Handwriting Recognition,
number 1, 2006.
[2] Jimmy Ba, Volodymyr Mnih, and Koray Kavukcuoglu. Multiple object recognition with visual attention.
arXiv preprint arXiv:1412.7755, 2014.
[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning
to align and translate. arXiv preprint arXiv:1409.0473, 2014.
[4] Yoshua Bengio, Yann LeCun, Craig Nohl, and Chris Burges. Lerec: A NN/HMM hybrid for on-line
handwriting recognition. Neural Computation, 7(6):1289?1303, 1995.
[5] Th?odore Bluche. Deep Neural Networks for Large Vocabulary Handwritten Text Recognition. Theses,
Universit? Paris Sud - Paris XI, May 2015.
[6] Th?odore Bluche, J?r?me Louradour, and Ronaldo Messina. Scan, Attend and Read: End-to-End Handwritten Paragraph Recognition with MDLSTM Attention. arXiv preprint arXiv:1604.03286, 2016.
[7] Th?odore Bluche, Bastien Moysset, and Christopher Kermorvant. Automatic line segmentation and groundtruth alignment of handwritten documents. In International Conference on Frontiers in Handwriting
Recognition (ICFHR), 2014.
[8] Vicente Bosch, Alejandro Hector Toselli, and Enrique Vidal. Statistical text line analysis in handwritten
documents. In Frontiers in Handwriting Recognition (ICFHR), 2012 International Conference on, pages
201?206. IEEE, 2012.
[9] Sylvie Brunessaux, Patrick Giroux, Bruno Grilh?res, Mathieu Manta, Maylis Bodin, Khalid Choukri,
Olivier Galibert, and Juliette Kahn. The Maurdor Project: Improving Automatic Processing of Digital
Documents. In Document Analysis Systems (DAS), 2014 11th IAPR International Workshop on, pages
349?354. IEEE, 2014.
[10] Horst Bunke, Samy Bengio, and Alessandro Vinciarelli. Offline recognition of unconstrained handwritten
texts using hmms and statistical language models. Pattern Analysis and Machine Intelligence, IEEE
Transactions on, 26(6):709?720, 2004.
[11] William Chan, Navdeep Jaitly, Quoc V Le, and Oriol Vinyals. Listen, attend and spell. arXiv preprint
arXiv:1508.01211, 2015.
[12] Jan K Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio. Attentionbased models for speech recognition. In Advances in Neural Information Processing Systems, pages
577?585, 2015.
[13] Manolis Delakis and Christophe Garcia. text detection with convolutional neural networks. In VISAPP (2),
pages 290?294, 2008.
[14] Patrick Doetsch, Michal Kozielski, and Hermann Ney. Fast and robust training of recurrent neural networks
for offline handwriting recognition. pages ?, 2014.
[15] Kunihiko Fukushima. Neural network model for selective attention in visual pattern recognition and
associative recall. Applied Optics, 26(23):4985?4992, 1987.
[16] Basilis Gatos, Georgios Louloudis, Tim Causer, Kris Grint, Veronica Romero, Joan-Andreu S?nchez,
Alejandro Hector Toselli, and Enrique Vidal. Ground-truth production in the transcriptorium project. In
Document Analysis Systems (DAS), 2014 11th IAPR International Workshop on, pages 237?241. IEEE,
2014.
[17] A Graves, S Fern?ndez, F Gomez, and J Schmidhuber. Connectionist temporal classification: labelling
unsegmented sequence data with recurrent neural networks. In International Conference on Machine
learning, pages 369?376, 2006.
[18] A. Graves and J. Schmidhuber. Offline Handwriting Recognition with Multidimensional Recurrent Neural
Networks. In Advances in Neural Information Processing Systems, pages 545?552, 2008.
[19] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.
[20] Karol Gregor, Ivo Danihelka, Alex Graves, and Daan Wierstra. DRAW: A recurrent neural network for
image generation. arXiv preprint arXiv:1502.04623, 2015.

8

[21] David Hebert, Thierry Paquet, and Stephane Nicolas. Continuous crf with multi-scale quantization feature
functions application to structure extraction in old newspaper. In Document Analysis and Recognition
(ICDAR), 2011 International Conference on, pages 493?497. IEEE, 2011.
[22] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. In Advances in
Neural Information Processing Systems, pages 2008?2016, 2015.
[23] Justin Johnson, Andrej Karpathy, and Li Fei-Fei. Densecap: Fully convolutional localization networks for
dense captioning. arXiv preprint arXiv:1511.07571, 2015.
[24] Keechul Jung. Neural network-based text location in color images. Pattern Recognition Letters,
22(14):1503?1515, 2001.
[25] Alfred Kaltenmeier, Torsten Caesar, Joachim M Gloger, and Eberhard Mandler. Sophisticated topology
of hidden Markov models for cursive script recognition. In Document Analysis and Recognition, 1993.,
Proceedings of the Second International Conference on, pages 139?142. IEEE, 1993.
[26] Michal Kozielski, Patrick Doetsch, Hermann Ney, et al. Improvements in RWTH?s System for Off-Line
Handwriting Recognition. In Document Analysis and Recognition (ICDAR), 2013 12th International
Conference on, pages 935?939. IEEE, 2013.
[27] Chen-Yu Lee and Simon Osindero. Recursive recurrent nets with attention modeling for ocr in the wild.
arXiv preprint arXiv:1603.03101, 2016.
[28] Laurence Likforman-Sulem, Abderrazak Zahour, and Bruno Taconet. Text line segmentation of historical
documents: a survey. International Journal of Document Analysis and Recognition (IJDAR), 9(2-4):123?
138, 2007.
[29] U-V Marti and Horst Bunke. The IAM-database: an English sentence database for offline handwriting
recognition. International Journal on Document Analysis and Recognition, 5(1):39?46, 2002.
[30] R. Messina and C. Kermorvant. Surgenerative Finite State Transducer n-gram for Out-Of-Vocabulary Word
Recognition. In 11th IAPR Workshop on Document Analysis Systems (DAS2014), pages 212?216, 2014.
[31] Bastien Moysset, Pierre Adam, Christian Wolf, and J?r?me Louradour. Space displacement localization
neural networks to locate origin points of handwritten text lines in historical documents. In International
Workshop on Historical Document Imaging and Processing (HIP), 2015.
[32] Bastien Moysset, Christopher Kermorvant, Christian Wolf, and J?r?me Louradour. Paragraph text segmentation into lines with recurrent neural networks. In International Conference of Document Analysis and
Recognition (ICDAR), 2015.
[33] Vu Pham, Th?odore Bluche, Christopher Kermorvant, and J?r?me Louradour. Dropout improves recurrent
neural networks for handwriting recognition. In 14th International Conference on Frontiers in Handwriting
Recognition (ICFHR2014), pages 285?290, 2014.
[34] Joan Andreu S?nchez, Ver?nica Romero, Alejandro Toselli, and Enrique Vidal. ICFHR 2014 HTRtS:
Handwritten Text Recognition on tranScriptorium Datasets. In International Conference on Frontiers in
Handwriting Recognition (ICFHR), 2014.
[35] Pierre Sermanet, David Eigen, Xiang Zhang, Micha?l Mathieu, Rob Fergus, and Yann LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. arXiv preprint
arXiv:1312.6229, 2013.
[36] Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average of
its recent magnitude. COURSERA: Neural Networks for Machine Learning, 4, 2012.
[37] A. Tong, M. Przybocki, V. Maergner, and H. El Abed. NIST 2013 Open Handwriting Recognition and
Translation (OpenHaRT13) Evaluation. In 11th IAPR Workshop on Document Analysis Systems (DAS2014),
2014.
[38] Kelvin Xu, Jimmy Ba, Ryan Kiros, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, and Yoshua
Bengio. Show, attend and tell: Neural image caption generation with visual attention. arXiv preprint
arXiv:1502.03044, 2015.

9


----------------------------------------------------------------

