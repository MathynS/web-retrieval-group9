query sentence: Associative database
---------------------------------------------------------------------
title: 2614-mass-meta-analysis-in-talairach-space.pdf

Mass meta-analysis in Talairach space

Finn ?
Arup Nielsen
Neurobiology Research Unit, Rigshospitalet
Copenhagen, Denmark
and
Informatics and Mathematical Modelling, Technical University of Denmark,
Lyngby, Denmark
fn@imm.dtu.dk

Abstract
We provide a method for mass meta-analysis in a neuroinformatics
database containing stereotaxic Talairach coordinates from neuroimaging experiments. Database labels are used to group the individual experiments, e.g., according to cognitive function, and the
consistent pattern of the experiments within the groups are determined. The method voxelizes each group of experiments via
a kernel density estimation, forming probability density volumes.
The values in the probability density volumes are compared to
null-hypothesis distributions generated by resamplings from the
entire unlabeled set of experiments, and the distances to the nullhypotheses are used to sort the voxels across groups of experiments. This allows for mass meta-analysis, with the construction
of a list with the most prominent associations between brain areas and group labels. Furthermore, the method can be used for
functional labeling of voxels.

1

Introduction

Neuroimaging experimenters usually report their results in the form of 3dimensional coordinates in the standardized stereotaxic Talairach system [1]. Automated meta-analytic and information retrieval methods are enabled when such data
are represented in databases such as the BrainMap DBJ ([2], www.brainmapdbj.org)
or the Brede database [3]. Example methods include outlier detection [4] and identification of similar volumes [5].
Apart from the stereotaxic coordinates, the databases record details of the experimental situation, e.g., the behavioral domain and the scanning modality. In the
Brede database the main annotation is the so-called ?external components?1 which
are heuristically organized in a simple ontology: A directed graph (more specifically,
a causal network) with the most general components as the roots of the graph, e.g.,
1

External components might be thought of as ?cognitive components? or simply ?brain
functions?, but they are more general, e.g., they also incorporate neuroreceptors. The
components are called ?external? since they are external variables to the brain image.

WOEXT: 41
Cold pain
WOEXT: 40
Pain

WOEXT: 261
Thermal pain
WOEXT: 69
Hot pain

Figure 1: The external components around ?thermal pain? with ?pain? as the
parent of ?thermal pain? and ?cold pain? and ?hot pain? as children.
?hot pain? is a child of ?thermal pain? that in turn is a child of ?pain? (see Figure 1).
The simple ontology is setup from information typically found in the introduction
section of scientific articles, and it is compared with the Medical Subject Headings
ontology of the National Library of Medicine. The ontology is stored in a simple
XML file.
The Brede database is organized, like the BrainMap DBJ, on different levels with
scientific papers on the highest level. Each scientific paper contains one or more
?experiments?, which each in turn contains one or more locations. The individual
experiments are typically labeled with an external component. The experiments
that are labeled with the same external component form a group, and the distribution of locations within the group become relevant: If a specific external component
is localized to a specific brain region, then the locations associated with the external
component should cluster in Talairach space.
We will describe a meta-analytic method that identifies important associations between external components and clustered Talairach coordinates. We have previously
modeled the relation between Talairach coordinates and neuroanatomical terms
[4, 6] and the method that we propose here can be seen as an extension describing
the relationship between Talairach coordinates and, e.g., cognitive components.

2

Method

The data from the Brede database [3] was used, which at the time contained data
from 126 scientific article containing 391 experiments and 2734 locations. There
were 380 external components. The locations referenced with respect to the MNI
atlas were realigned to the Talairach atlas [7].
To form a vectorial representation, each location was voxelized by convolving the
0
location l at position vl = [x, y, z] with a Gaussian kernel [4, 8, 9]. This constructed
a probability density in Talairach space v


(v ? vl )0 (v ? vl )
p(v|l) = (2?? 2 )?3/2 exp ?
,
(1)
2? 2
with the width ? fixed to 1 centimeter. To form a resulting probability density
volume p(v|t) for an external component t the individual components from each
location were multiplied by the appropriate priors and summed
X
p(v|t) =
p(v|l) P (l|e) P (e|t),
(2)
l,e

with P (l|e) = 0 if the l location did not appear in the e experiment and P (e|t) = 0
if the e experiment is not associated with the t external components. The precise

normalization of these priors is an unresolved problem. A paper with many locations
and experiments should not be allowed to dominate the results. This can be the case
if all locations are given equal weight. On the other hand a paper which reports
just a single coordinate should probably not be weighted as much as one with
many experiments and locations: Few reported locations might be due to limited
(statistical) power of the experiment. As a compromise between the two extremes
we used the square root of the number of the locations within an experiment and
the square root of the number of experiments within a paper for the prior P (l|e).
The square root normalization is also an appropriate normalization in certain voting
systems [10]. The second prior was uniform P (e|t) ? 1 for those experiments that
were labeled with the t external component.
The continuous volume were sampled at regular grid points to establish a vector w t
for each external component
wt ? p(v|t).
(3)
Null-hypothesis distributions for the maximum statistics u across the voxels in the
volume were built up by resampling: A number of experiments E was selected
and E experiments were resampled, with replacement, from the entire set of 391
experiments, ignoring the grouping imposed by the external component labeling.
The experiments were resampled without regard to the paper they originated from.
The maximum across voxels was found as:
ur (E) = max [wr (j)] ,
j

(4)

where j is an index over voxels and r is the resample index. With R resamplings we
obtain a vector u(E) = [u1 (E) . . . ur (E) . . . uR (E)] that is a function of the number
of experiments and which forms an empirical distribution u(E). When the value
wt,j of the j voxel of the t external component was compared with the distribution,
a distance to the null-hypothesis can be generated
dt,j = Prob [wt,j > u(Et )] ,

(5)

where 1 ? d is a statistical P -value and where Et is the number of experiment
associated with the t external component. Thus the resampling allows us to convert
the probability density value to a probability that is comparable across external
components of different sizes. The maximum statistics deal automatically with the
multiple comparison problem across voxels [11].
dt,j can be computed by counting the fraction of the resampled values ur that are
below the value of wt,j . The resampling distribution can also be approximated and
smoothed by modeling it with a non-linear function. In our case we used a standard
two-layer feed-forward neural network with hyperbolic tangent hidden units [12, 13]
modeling the function f (E, u) = atanh(2d ? 1) with a quadratic cost function.
The non-linear function allows for a more compact representation of the empirical
distribution of the resampled maximum statistics.
As a final step, the probability volumes for the external components wt were thresholded on selected levels and isosurfaces generated in the distance volume for visualization. Connected voxels within the thresholded volume were found by region
identification and the local maxima in the regions were determined.
Functional labeling of specified voxels is also possible: The distances d t,j were collected in a (external component ? voxel)-matrix D and the elements in the j column
sorted. Lastly, the voxel were labeled with the top external component.
Only the bottom nodes of the causal networks of external components are likely
to be directly associated with experiments. To label the ancestors, the labels from

6

Randomization test statistics

10

test statistics (max pdf)

0.5
0.75
0.9
0.95
0.99

5

10

4

10
0
10

1

10

2

10

Number of experiments

Figure 2: The test statistics at various distances to the null-hypothesis (d = 1 ? P )
after 1000 resamplings. The distance is shown as a function of the number of
experiments E in the resampling.
their descendants were back propagated, e.g., a study explicitly labeled as ?hot
pain? were also be labeled as ?thermal pain? and ?pain?. Apart from this simple
back propagation the hierarchical structure of the external components was not
incorporated into the prior.

3

Results

Figure 2 shows isolines in the cumulative distribution of the resampled maximum
statistics u(E) as a function of the resampling set size (number of experiments)
from E = 1 to E = 100. Since the vectorized volume is not normalized to form a
probability density the curves are increasing with our selected normalization.
Table 1 shows the result of sorting the maximum distances across voxel within the
external components. Topping the list are external components associated with
movement. The voxel with the largest distance is localized in v = (0, ?8, 56) which
most likely is due to movement studies activating the supplementary motor area. In
the Brede database the mean is (6, ?7, 55) for the locations in the right hemisphere
labeled as supplementary motor area. Other voxels with a high distance for the
movement external components are located in the primary motor area.
A number of other entries on the list are associated with pain, with the main voxel
at (0, 8, 32) in the right anterior cingulate. Other important areas are shown in
Figure 3 with isosurfaces in the distance volume for the external component ?pain?
(WOEXT: 40). These are localized in the anterior cingulate, right and left insula
and thalamus.
Other external components high on the list are ?audition? together with ?voice?

#
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

d
1.00
1.00
1.00
1.00
1.00
1.00
1.00
0.99
0.99
0.99
0.99
0.99
0.99
0.99
0.99

x
0
0
0
0
56
0
0
0
0
24
56
0
24
0
0

y
?8
?8
8
8
?16
8
8
?56
8
?8
?16
?56
?8
?56
?56

z
56
56
32
32
0
32
32
16
32
?8
0
16
?8
16
16

Name (WOEXT)
Localized movement (266)
Motion, movement, locomotion (4)
Pain (40)
Thermal pain (261)
Audition (14)
Temperature sensation (204)
Somesthesis (17)
Memory retrieval (24)
Warm temperature sensation (207)
Unpleasantness (153)
Voice (167)
Memory (9)
Emotion (3)
Long-term memory (112)
Declarative memory (319)

Table 1: The top 15 elements of the list, showing the external components that
score the highest, the distance to the null-hypothesis d, and the associated Talairach
x, y and z coordinates. The numbers in the parentheses are the Brede database
identifiers for the external components (WOEXT). This list was generated with
coarse 8 ? 8 ? 8mm3 voxels and using the non-linear model approximation for the
cumulative distribution functions.
appearing in left and right superior temporal gyrus, and memory emerging in the
posterior cingulate area. Unpleasantness and emotion are high on the list due to,
e.g., ?fear? and ?disgust? experiments that report activation in the right amygdala
and nearby areas.
An example of the functional labeling of a voxel appears in Table 2. The chosen
voxel is (0, ?56, 16) that appears in the posterior cingulate. Memory retrieval is the
first on the list in accordance with Table 1. Many of the other external components
on the list are also related to memory.

4

Discussion

The Brede database contains many thermal pain experiments, and it causes high
scores for voxels from external components such as ?pain? and ?thermal pain?. The
four focal ?brain activations? that appear in Figure 3 are localized in areas (anterior
cingulate, insula and thalamus) that an expert reviewer has previously identified
as important in pain [14]. Thus there is consistency between our automated metaanalytic technique and a ?manual? expert review.
Many experiments that report activation in the posterior cingulate area have been
included in the Brede database, and this is probably why memory is especially associated with this area. A major review of 275 functional neuroimaging studies
found that episodic memory retrieval is the cognitive function with highest association with the posterior cingulate [15], so our finding is again in alignment with an

Figure 3: Plot of the important areas associated with the external component
?pain?. The red opaque isosurface is on the level d = 0.95 in the distance volume while the gray transparent surface appears at d = 0.05. Yellow glyphs appear
at the local maxima in the thresholded volume. The viewpoint is situated nearest
to the left superior posterior corner of the brain.

expert review.
A number of the substantial associations between brain areas and external components are not surprising, e.g., audition associating with superior temporal gyrus.
Our method has no inherent knowledge of what is already known, and thus not able
distinguish novel associations from trivial.
A down-side with the present method is that it requires the labeling of experiments
during database entry and the construction of the hierarchy of the labels (Figure 1).
Both are prone to ?interpretation? and this is particularly a problem for complex
cognitive functions. Our methodology, however, does not necessarily impose a single
organization of the external components, and it is possible to rearrange these by
defining another adjacency matrix for the graph.
In Table 1 the brain areas are represented in terms of Talairach coordinates. It
should be possible to convert these coordinates further to neuroanatomical terms

#
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

d
0.99
0.99
0.99
0.99
0.99
0.96
0.94
0.94
0.58
0.16
0.14
0.14
0.11
0.09
0.02

Name (WOEXT)
Memory retrieval (24)
Memory (9)
Long-term memory (112)
Declarative memory (319)
Episodic memory (49)
Autobiographical memory (259)
Cognition (2)
Episodic memory retrieval (109)
Disease (79)
Recognition (190)
Psychiatric disorders (82)
Neurotic, stress and somatoform disorders (227)
Severe stress reactions and adjustment disorders (228)
Emotion (3)
Semantic memory (318)

Table 2: Example of a functional label list of a voxel v = (0, ?56, 16) in the posterior
cingulate area.
by using the models between coordinates and lobar anatomy that we previously
have established [4, 6].
Functional labeling should allow us to build a complete functional atlas for the entire
brain. The utility of this approach is, however, limited by the small size of the Brede
database and its bias towards specific brain regions and external components. But
such a functional atlas will serve as a neuroinformatic organizer for the increasing
number of neuroimaging studies.
Acknowledgment
I am grateful to Matthew G. Liptrot for reading and commenting on the manuscript.
Lars Kai Hansen is thanked for discussion, Andrew C. N. Chen for identifying some
of the thermal pain studies and the Villum Kann Rasmussen Foundation for their
generous support of the author.

References
[1] Jean Talairach and Pierre Tournoux. Co-planar Stereotaxic Atlas of the Human
Brain. Thieme Medical Publisher Inc, New York, January 1988.
[2] Peter T. Fox and Jack L. Lancaster. Mapping context and content: the BrainMap model. Nature Reviews Neuroscience, 3(4):319?321, April 2002.
[3] Finn ?
Arup Nielsen. The Brede database: a small database for functional neuroimaging. NeuroImage, 19(2), June 2003. Presented at the 9th International
Conference on Functional Mapping of the Human Brain, June 19?22, 2003,
New York, NY. Available on CD-Rom.

[4] Finn ?
Arup Nielsen and Lars Kai Hansen. Modeling of activation data in
the BrainMapTM database: Detection of outliers. Human Brain Mapping,
15(3):146?156, March 2002.
[5] Finn ?
Arup Nielsen and Lars Kai Hansen. Finding related functional neuroimaging volumes. Artificial Intelligence in Medicine, 30(2):141?151, February 2004.
?rup Nielsen and Lars Kai Hansen. Automatic anatomical labeling of
[6] Finn A
Talairach coordinates and generation of volumes of interest via the BrainMap database. NeuroImage, 16(2), June 2002. Presented at the 8th International Conference on Functional Mapping of the Human Brain, June 2?6,
2002, Sendai, Japan. Available on CD-Rom.
[7] Matthew Brett. The MNI brain and the Talairach atlas. http://www.mrccbu.cam.ac.uk/Imaging/mnispace.html, August 1999. Accessed 2003 March
17.
[8] Peter E. Turkeltaub, Guinevere F. Eden, Karen M. Jones, and Thomas A.
Zeffiro. Meta-analysis of the functional neuroanatomy of single-word reading:
method and validation. NeuroImage, 16(3 part 1):765?780, July 2002.
[9] J. M. Chein, K. Fissell, S. Jacobs, and Julie A. Fiez. Functional heterogeneity
within Broca?s area during verbal working memory. Physiology & Behavior,
77(4-5):635?639, December 2002.
[10] Lionel S. Penrose. The elementary statistics of majority voting. Journal of the
Royal Statistical Society, 109:53?57, 1946.
[11] Andrew. P. Holmes, R. C. Blair, J. D. G. Watson, and I. Ford. Non-parametric
analysis of statistic images from functional mapping experiments. Journal of
Cerebral Blood Flow and Metabolism, 16(1):7?22, January 1996.
[12] Claus Svarer, Lars Kai Hansen, and Jan Larsen. On the design and evaluation
of tapped-delay lines neural networks. In Proceedings of the IEEE International
Conference on Neural Networks, San Francisco, California, USA, volume 1,
pages 46?51, 1993.
?rup Nielsen, Peter Toft, Matthew George Liptrot,
[13] Lars Kai Hansen, Finn A
Cyril Goutte, Stephen C. Strother, Nicholas Lange, Anders Gade, David A.
Rottenberg, and Olaf B. Paulson. ?lyngby? ? a modeler?s Matlab toolbox for
spatio-temporal analysis of functional neuroimages. NeuroImage, 9(6):S241,
June 1999.
[14] Martin Ingvar. Pain and functional imaging. Philosophical Transactions of the
Royal Society of London. Series B, Biological Sciences, 354(1387):1347?1358,
July 1999.
[15] Roberto Cabeza and Lars Nyberg. Imaging cognition II: An empirical review
of 275 PET and fMRI studies. Journal of Cognitive Neuroscience, 12(1):1?47,
January 2000.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 1-self-organization-of-associative-database-and-its-applications.pdf

767

SELF-ORGANIZATION OF ASSOCIATIVE DATABASE
AND ITS APPLICATIONS
Hisashi Suzuki and Suguru Arimoto
Osaka University, Toyonaka, Osaka 560, Japan
ABSTRACT
An efficient method of self-organizing associative databases is proposed together with
applications to robot eyesight systems. The proposed databases can associate any input
with some output. In the first half part of discussion, an algorithm of self-organization is
proposed. From an aspect of hardware, it produces a new style of neural network. In the
latter half part, an applicability to handwritten letter recognition and that to an autonomous
mobile robot system are demonstrated.

INTRODUCTION
Let a mapping f : X -+ Y be given. Here, X is a finite or infinite set, and Y is another
finite or infinite set. A learning machine observes any set of pairs (x, y) sampled randomly
from X x Y. (X x Y means the Cartesian product of X and Y.) And, it computes some
estimate j : X -+ Y of f to make small, the estimation error in some measure.
Usually we say that: the faster the decrease of estimation error with increase of the number of samples, the better the learning machine. However, such expression on performance
is incomplete. Since, it lacks consideration on the candidates of J of j assumed preliminarily. Then, how should we find out good learning machines? To clarify this conception,
let us discuss for a while on some types of learning machines. And, let us advance the
understanding of the self-organization of associative database .
. Parameter Type
An ordinary type of learning machine assumes an equation relating x's and y's with
parameters being indefinite, namely, a structure of f. It is equivalent to define implicitly a
set F of candidates of
(F is some subset of mappings from X to Y.) And, it computes
values of the parameters based on the observed samples. We call such type a parameter
type.
For a learning machine defined well, if F 3 f, j approaches f as the number of samples
increases. In the alternative case, however, some estimation error remains eternally. Thus,
a problem of designing a learning machine returns to find out a proper structure of f in this
sense.
On the other hand, the assumed structure of f is demanded to be as compact as possible
to achieve a fast learning. In other words, the number of parameters should be small. Since,
if the parameters are few, some j can be uniquely determined even though the observed
samples are few. However, this demand of being proper contradicts to that of being compact.
Consequently, in the parameter type, the better the compactness of the assumed structure
that is proper, the better the learning machine. This is the most elementary conception
when we design learning machines .

1.

. Universality and Ordinary Neural Networks
Now suppose that a sufficient knowledge on f is given though J itself is unknown. In
this case, it is comparatively easy to find out proper and compact structures of J. In the
alternative case, however, it is sometimes difficult. A possible solution is to give up the
compactness and assume an almighty structure that can cover various 1's. A combination
of some orthogonal bases of the infinite dimension is such a structure. Neural networks 1 ,2
are its approximations obtained by truncating finitely the dimension for implementation.

? American Institute of Physics 1988

768
A main topic in designing neural networks is to establish such desirable structures of 1.
This work includes developing practical procedures that compute values of coefficients from
the observed samples. Such discussions are :flourishing since 1980 while many efficient methods have been proposed. Recently, even hardware units computing coefficients in parallel
for speed-up are sold, e.g., ANZA, Mark III, Odyssey and E-1.
Nevertheless, in neural networks, there always exists a danger of some error remaining
eternally in estimating /. Precisely speaking, suppose that a combination of the bases of a
finite number can define a structure of 1 essentially. In other words, suppose that F 3 /, or
1 is located near F. In such case, the estimation error is none or negligible. However, if 1
is distant from F, the estimation error never becomes negligible. Indeed, many researches
report that the following situation appears when 1 is too complex. Once the estimation
error converges to some value (> 0) as the number of samples increases, it decreases hardly
even though the dimension is heighten. This property sometimes is a considerable defect of
neural networks .
. Recursi ve Type
The recursive type is founded on another methodology of learning that should be as
follows. At the initial stage of no sample, the set Fa (instead of notation F) of candidates
of I equals to the set of all mappings from X to Y. After observing the first sample
(Xl, Yl) E X x Y, Fa is reduced to Fi so that I(xt) = Yl for any I E F. After observing
the second sample (X2' Y2) E X x Y, Fl is further reduced to F2 so that i(xt) = Yl and
I(X2) = Y2 for any I E F. Thus, the candidate set F becomes gradually small as observation
of samples proceeds. The after observing i-samples, which we write
is one of the most
likelihood estimation of 1 selected in fi;. Hence, contrarily to the parameter type, the
recursive type guarantees surely that j approaches to 1 as the number of samples increases.
The recursive type, if observes a sample (x" yd, rewrites values 1,-l(X),S to I,(x)'s for
some x's correlated to the sample. Hence, this type has an architecture composed of a rule
for rewriting and a free memory space. Such architecture forms naturally a kind of database
that builds up management systems of data in a self-organizing way. However, this database
differs from ordinary ones in the following sense. It does not only record the samples already
observed, but computes some estimation of l(x) for any x E X. We call such database an
associative database.
The first subject in constructing associative databases is how we establish the rule for
rewri ting. For this purpose, we adap t a measure called the dissimilari ty. Here, a dissimilari ty
means a mapping d : X x X -+ {reals > O} such that for any (x, x) E X x X, d(x, x) > 0
whenever l(x) t /(x). However, it is not necessarily defined with a single formula. It is
definable with, for example, a collection of rules written in forms of "if? .. then?? .. "
The dissimilarity d defines a structure of 1 locally in X x Y. Hence, even though
the knowledge on f is imperfect, we can re:flect it on d in some heuristic way. Hence,
contrarily to neural networks, it is possible to accelerate the speed of learning by establishing
d well. Especially, we can easily find out simple d's for those l's which process analogically
information like a human. (See the applications in this paper.) And, for such /'s, the
recursive type shows strongly its effectiveness.
We denote a sequence of observed samples by (Xl, Yd, (X2' Y2),???. One of the simplest
constructions of associative databases after observing i-samples (i = 1,2,.,,) is as follows.

i

i"

I,

Algorithm 1. At the initial stage, let So be the empty set. For every i =
1,2" .. , let i,-l(x) for any x E X equal some y* such that (x*,y*) E S,-l and

d(x, x*) =

min
(%,y)ES.-t

d(x, x) .

Furthermore, add (x" y,) to S;-l to produce Sa, i.e., S, = S,_l U {(x"

(1)

y,n.

769

Another version improved to economize the memory is as follows.

Algorithm 2, At the initial stage, let So be composed of an arbitrary element
in X x Y. For every i = 1,2"", let ii-lex) for any x E X equal some y. such
that (x?, y.) E Si-l and
d(x, x?) =

min

d(x, x) .

(i,i)ES.-l

Furthermore, if ii-l(Xi) # Yi then let Si = Si-l, or add (Xi, Yi) to Si-l to
produce Si, i.e., Si = Si-l U {(Xi, Yi)}'
In either construction, ii approaches to f as i increases. However, the computation time
grows proportionally to the size of Si. The second subject in constructing associative
databases is what addressing rule we should employ to economize the computation time. In
the subsequent chapters, a construction of associative database for this purpose is proposed.
It manages data in a form of binary tree.

SELF-ORGANIZATION OF ASSOCIATIVE DATABASE
Given a sample sequence (Xl, Yl), (X2' Y2), .. " the algorithm for constructing associative
database is as follows.

Algorithm 3,'

Step I(Initialization): Let (x[root], y[root]) = (Xl, Yd. Here, x[.] and y[.] are
variables assigned for respective nodes to memorize data.. Furthermore, let t = 1.
Step 2: Increase t by 1, and put x, in. After reset a pointer n to the root, repeat
the following until n arrives at some terminal node, i.e., leaf.
Notations nand
d(xt, x[n)), let n

n mean the descendant nodes of n.
=n. Otherwise, let n =n.

If d(x" r[n)) ~

Step 3: Display yIn] as the related information. Next, put y, in. If yIn] = y" back
to step 2. Otherwise, first establish new descendant nodes n and n. Secondly,
let

(x[n], yIn))
(x[n], yIn))

(x[n], yIn)),
(Xt, y,).

(2)
(3)

Finally, back to step 2. Here, the loop of step 2-3 can be stopped at any time
and also can be continued.
Now, suppose that gate elements, namely, artificial "synapses" that play the role of branching by d are prepared. Then, we obtain a new style of neural network with gate elements
being randomly connected by this algorithm.

LETTER RECOGNITION
Recen tly, the vertical slitting method for recognizing typographic English letters3 , the
elastic matching method for recognizing hand written discrete English letters4 , the global
training and fuzzy logic search method for recognizing Chinese characters written in square
styleS, etc. are published. The self-organization of associative database realizes the recognition of handwritten continuous English letters.

770

9 /wn"

NOV

~ ~ ~ -xk :La.t

~~ ~ ~~~

dw1lo'

~~~~~of~~

~~~ 4,-?~~4Fig. 1. Source document.
2~~---------------'

lOO~---------------'

H

o

o
Fig. 2. Windowing.

1000

2000

3000

4000

Number of samples

o

1000

2000

3000

4000

NUAlber of sampl es

Fig. 3. An experiment result.

An image scanner takes a document image (Fig. 1). The letter recognizer uses a parallelogram window that at least can cover the maximal letter (Fig. 2), and processes the
sequence of letters while shifting the window. That is, the recognizer scans a word in a
slant direction. And, it places the window so that its left vicinity may be on the first black
point detected. Then, the window catches a letter and some part of the succeeding letter.
If recognition of the head letter is performed, its end position, namely, the boundary line
between two letters becomes known. Hence, by starting the scanning from this boundary
and repeating the above operations, the recognizer accomplishes recursively the task. Thus
the major problem comes to identifying the head letter in the window.
Considering it, we define the following.
? Regard window images as x's, and define X accordingly.
? For a (x, x) E X x X, denote by B a black point in the left area from the boundary on
window image X. Project each B onto window image x. Then, measure the Euclidean
distance 6 between fj and a black point B on x being the closest to B. Let d(x, x) be
the summation of 6's for all black points B's on x divided by the number of B's.
? Regard couples of the "reading" and the position of boundary as y's, and define Y
accordingly.
An operator teaches the recognizer in interaction the relation between window image and
reading& boundary with algorithm 3. Precisely, if the recalled reading is incorrect, the
operator teaches a correct reading via the console. Moreover, if the boundary position is
incorrect, he teaches a correct position via the mouse.
Fig. 1 shows partially a document image used in this experiment. Fig. 3 shows the
change of the number of nodes and that of the recognition rate defined as the relative
frequency of correct answers in the past 1000 trials. Speciiications of the window are height
= 20dot, width = 10dot, and slant angular = 68deg. In this example, the levels of tree
were distributed in 6-19 at time 4000 and the recognition rate converged to about 74%.
Experimentally, the recognition rate converges to about 60-85% in most cases, and to 95% at
a rare case. However, it does not attain 100% since, e.g., "c" and "e" are not distinguishable
because of excessive lluctuation in writing. If the consistency of the x, y-relation is not
assured like this, the number of nodes increases endlessly (d. Fig. 3). Hence, it is clever to
stop the learning when the recognition rate attains some upper limit. To improve further
the recognition rate, we must consider the spelling of words. It is one of future subjects.

771

OBSTACLE AVOIDING MOVEMENT
Various systems of camera type autonomous mobile robot are reported flourishingly6-1O.
The system made up by the authors (Fig. 4) also belongs to this category. Now, in mathematical methodologies, we solve usually the problem of obstacle avoiding movement as
a cost minimization problem under some cost criterion established artificially. Contrarily,
the self-organization of associative database reproduces faithfully the cost criterion of an
operator. Therefore, motion of the robot after learning becomes very natural.
Now, the length, width and height of the robot are all about O.7m, and the weight is
about 30kg. The visual angle of camera is about 55deg. The robot has the following three
factors of motion. It turns less than ?30deg, advances less than 1m, and controls speed less
than 3km/h. The experiment was done on the passageway of wid th 2.5m inside a building
which the authors' laboratories exist in (Fig. 5). Because of an experimental intention, we
arrange boxes, smoking stands, gas cylinders, stools, handcarts, etc. on the passage way at
random. We let the robot take an image through the camera, recall a similar image, and
trace the route preliminarily recorded on it. For this purpose, we define the following.
? Let the camera face 28deg downward to take an image, and process it through a low
pass filter. Scanning vertically the filtered image from the bottom to the top, search
the first point C where the luminance changes excessively. Then, su bstitu te all points
from the bottom to C for white, and all points from C to the top for black (Fig. 6).
(If no obstacle exists just in front of the robot, the white area shows the ''free'' area
where the robot can move around.) Regard binary 32 x 32dot images processed thus
as x's, and define X accordingly.
? For every (x, x) E X x X, let d(x, x) be the number of black points on the exclusive-or
image between x and X.
? Regard as y's the images obtained by drawing routes on images x's, and define Y
accordingly.
The robot superimposes, on the current camera image x, the route recalled for x, and
inquires the operator instructions. The operator judges subjectively whether the suggested
route is appropriate or not. In the negative answer, he draws a desirable route on x with the
mouse to teach a new y to the robot. This opera.tion defines implicitly a sample sequence
of (x, y) reflecting the cost criterion of the operator.

.::l" !
-

IibUBe

_. -

22

11

Roan

12

{-

13

Stationary uni t

Fig. 4. Configuration of
autonomous mobile robot system.

~

I

,

23

24

North
14

rmbi Ie unit (robot)

-

Roan

y

t

Fig. 5. Experimental
environment.

772

Wall

Camera image

Preprocessing

A

::: !fa

?

Preprocessing

0

O

Course
suggest ion

??

..

Search

A

Fig. 6. Processing for
obstacle avoiding movement.

x

Fig. 1. Processing for
position identification.
We define the satisfaction rate by the relative frequency of acceptable suggestions of
route in the past 100 trials. In a typical experiment, the change of satisfaction rate showed
a similar tendency to Fig. 3, and it attains about 95% around time 800. Here, notice that
the rest 5% does not mean directly the percentage of collision. (In practice, we prevent the
collision by adopting some supplementary measure.) At time 800, the number of nodes was
145, and the levels of tree were distributed in 6-17.
The proposed method reflects delicately various characters of operator. For example, a
robot trained by an operator 0 moves slowly with enough space against obstacles while one
trained by another operator 0' brushes quickly against obstacles. This fact gives us a hint
on a method of printing "characters" into machines.
POSITION IDENTIFICATION
The robot can identify its position by recalling a similar landscape with the position data
to a camera image. For this purpose, in principle, it suffices to regard camera images and
position data as x's and y's, respectively. However, the memory capacity is finite in actual
compu ters. Hence, we cannot but compress the camera images at a slight loss of information.
Such compression is admittable as long as the precision of position identification is in an
acceptable area. Thus, the major problem comes to find out some suitable compression
method.
In the experimental environment (Fig. 5), juts are on the passageway at intervals of
3.6m, and each section between adjacent juts has at most one door. The robot identifies
roughly from a surrounding landscape which section itself places in. And, it uses temporarily
a triangular surveying technique if an exact measure is necessary. To realize the former task,
we define the following .
? Turn the camera to take a panorama image of 360deg. Scanning horizontally the
center line, substitute the points where the luminance excessively changes for black
and the other points for white (Fig. 1). Regard binary 360dot line images processed
thus as x's, and define X accordingly.
? For every (x, x) E X x X, project each black point A on x onto x. And, measure the
Euclidean distance 6 between A and a black point A on x being the closest to A. Let
the summation of 6 be S. Similarly, calculate S by exchanging the roles of x and X.
Denoting the numbers of A's and A's respectively by nand n, define

773

d(x, x) =

~(~
+ ~).
2 n
n

(4)

? Regard positive integers labeled on sections as y's (cf. Fig. 5), and define Y accordingly.
In the learning mode, the robot checks exactly its position with a counter that is reset periodically by the operator. The robot runs arbitrarily on the passageways within 18m area
and learns the relation between landscapes and position data. (Position identification beyond 18m area is achieved by crossing plural databases one another.) This task is automatic
excepting the periodic reset of counter, namely, it is a kind of learning without teacher.
We define the identification rate by the relative frequency of correct recalls of position
data in the past 100 trials. In a typical example, it converged to about 83% around time
400. At time 400, the number of levels was 202, and the levels oftree were distributed in 522. Since the identification failures of 17% can be rejected by considering the trajectory, no
pro blem arises in practical use. In order to improve the identification rate, the compression
ratio of camera images must be loosened. Such possibility depends on improvement of the
hardware in the future.
Fig. 8 shows an example of actual motion of the robot based on the database for obstacle
avoiding movement and that for position identification. This example corresponds to a case
of moving from 14 to 23 in Fig. 5. Here, the time interval per frame is about 40sec.

,~. .~ (
;~"i..
~

"

"

.

..I

I

?
?

"

I'
.
'.1
t

;

i

-:
, . . , 'II

Fig. 8. Actual motion of the robot.

774

CONCLUSION
A method of self-organizing associative databases was proposed with the application to
robot eyesight systems. The machine decomposes a global structure unknown into a set of
local structures known and learns universally any input-output response. This framework
of problem implies a wide application area other than the examples shown in this paper.
A defect of the algorithm 3 of self-organization is that the tree is balanced well only
for a subclass of structures of f. A subject imposed us is to widen the class. A probable
solution is to abolish the addressing rule depending directly on values of d and, instead, to
establish another rule depending on the distribution function of values of d. It is now under
investigation.

REFERENCES
1. Hopfield, J. J. and D. W. Tank, "Computing with Neural Circuit: A Model/'

Science 233 (1986), pp. 625-633.
2. Rumelhart, D. E. et al., "Learning Representations by Back-Propagating Errors," Nature 323 (1986), pp. 533-536.

3. Hull, J. J., "Hypothesis Generation in a Computational Model for Visual Word
Recognition," IEEE Expert, Fall (1986), pp. 63-70.
4. Kurtzberg, J. M., "Feature Analysis for Symbol Recognition by Elastic Matching," IBM J. Res. Develop. 31-1 (1987), pp. 91-95.

5. Wang, Q. R. and C. Y. Suen, "Large Tree Classifier with Heuristic Search and
Global Training," IEEE Trans. Pattern. Anal. & Mach. Intell. PAMI 9-1
(1987) pp. 91-102.
6. Brooks, R. A. et al, "Self Calibration of Motion and Stereo Vision for Mobile
Robots," 4th Int. Symp. of Robotics Research (1987), pp. 267-276.
7. Goto, Y. and A. Stentz, "The CMU System for Mobile Robot Navigation," 1987
IEEE Int. Conf. on Robotics & Automation (1987), pp. 99-105.
8. Madarasz, R. et al., "The Design of an Autonomous Vehicle for the Disabled,"
IEEE Jour. of Robotics & Automation RA 2-3 (1986), pp. 117-125.
9. Triendl, E. and D. J. Kriegman, "Stereo Vision and Navigation within Buildings," 1987 IEEE Int. Conf. on Robotics & Automation (1987), pp. 1725-1730.
10. Turk, M. A. et al., "Video Road-Following for the Autonomous Land Vehicle,"
1987 IEEE Int. Conf. on Robotics & Automation (1987), pp. 273-279.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 609-probability-estimation-from-a-database-using-a-gibbs-energy-model.pdf

Probability Estimation from a Database
Using a Gibbs Energy Model

John W. Miller
Microsoft Research (9/1051)
One Microsoft Way
Redmond, WA 98052

Rodney M. Goodman
Dept. of Electrical Engineering (116-81)
California Institute of Technology
Pasadena, CA 91125

Abstract
We present an algorithm for creating a neural network which produces accurate probability estimates as outputs. The network implements a Gibbs probability distribution model of the training
database. This model is created by a new transformation relating
the joint probabilities of attributes in the database to the weights
(Gibbs potentials) of the distributed network model. The theory
of this transformation is presented together with experimental results. One advantage of this approach is the network weights are
prescribed without iterative gradient descent. Used as a classifier
the network tied or outperformed published results on a variety of
databases.

1

INTRODUCTION

This paper addresses the problem of modeling a discrete database. The database
is viewed as a collection of independent samples from a probability distribution.
This distribution is called the underlying distribution. In contrast, the empirical
distribution is the distribution obtained if you take independent random samples
from the database (with replacement). The task of creating a probability model
can be separated into two parts. The first part is the problem of choosing statistics
of the samples which are expected to accurately represent the underlying distribution. The second part is the problem of choosing a model which is consistent with
these statistics. Under reasonable assumptions, the optimal solution to the second
problem is the method of Maximum Entropy. For a broad class of statistics, the

531

532

Miller and Goodman

Maximum Entropy solution is a Gibbs probability distribution (Slepian, 1972). In
this paper, the background and theoretical result of a transformation from joint
statistics to a Gibbs energy (or network weight) representation is presented. We
then outline the experimental test results of an efficient algorithm implementing
this transform without using gradient descent iteration.

2 BACKGROUND
Define a set T to be the set of attributes (or fields) in a database. For a particular
entry (or record) of the database, define the associated set of attribute values to be
the configuration W of the attributes. The set of attribute values associated with
a subset beT is called a sub configuration Wb. Using this set notation the Gibbs
probability distribution may be defined:
pew) = Z-l . eVT(w)
(1)
where

(2)

bCT
The function V is called the energy. The function Jb, called the potential junction,
defines a real value for every sub configuration of the set b. Z is the normalizing
constant that makes the sum of probabilities of all configurations equal to unity.
Prior work in the neural network literature using the Gibbs distribution (such as
the Boltzmann Machine) has primarily used second order models (Jb = 0 if Ibl > 2)
(Hinton, 1986). By adding new attributes not in the original database, second order
potentials have been used to model complex distributions. The work presented in
this paper, in contrast, uses higher order potentials to model complex probability
distributions. We begin by considering the case where every potential of every order
is used to model the distribution.
The Principle of Inclusion-Exclusion from set theory states that the following two
equations are equivalent:

g(A)

Lf(b)

(3)

b~A

L(-l)IA-b l g(b).
bCA

f(A)

(4)

The method of inverting an equation from the form of (3) into one in the form of
(4) is a special case of Mobius Inversion. Clifford-Hammersley (Kindermann, 1980)
used this relation to invert formula (2):

JA(w) = L(-l)IA-b l Vb(w)
(5)
bCA
Define the probability of a sub configuration p(Wb) to be the probability that the
attributes in set b take on the values defined in the configuration w. Using (1)
to describe the probability distribution of sub configurations, equation (5) can be
written:
JA(w) = L(-l)IA-b l In( p(Wb?
(6)
b~A

Probability Estimation from a Database Using a Gibbs Energy Model

3

A TRANSFORMATION TO GIBBS POTENTIALS

Equation (6) provides a technique for modeling distributions by potential functions
rather than directly through the observable joint statistics of sets of attributes. If
the model is truncated by setting high order potentials to zero, then the energy
model becomes an estimate of the model obtained by collecting the joint statistics,
rather than an exact equivalent. If equation (6) is used directly, the error in the
energy due to setting all potentials of order d to zero grows quickly with d. For
this reason (6) must be normalized if it is going to be used in a truncated modeling
scheme. A normalization version of equation (2) that corrects for the unequal
number of potentials of different orders is:

VA(w)

IAI- 1)-1

= L ( Ibl- 1

Jb(W)

(7)

b~A

This equation can be inverted to show the surprising result, a weight associated
with WA:
(8)
JA(W) In(PA(w? - (IAI- 1)-1
In(Pb(w?

L

=

tEA
b=A-t

For example, with three attribute values {x, y, z}, the following potentials are defined:
J{x}
In(p(x))

=

J{y} = In(p(y))
J{z} = In(p(z))
p(xy) )
J{xy} = In ( p(x)p(y)
p(yz) )
J{yz} = In ( p(y)p(z)
p(xz) )
J{xz} = In ( p(x)p(z)
J{xyz} = In (

p(xyz)
)
ylp(xy)p(yz)p(xz)

For a given database sample, a potential is activated if all of its defined attribute
values are true for the sample. The weighted sum of all activated potentials recovers
an approximation of the probability of the database sample. If all potentials of every
order have been used to create the model, then this approximation is exactly the
probability of the sample in the empirical distribution. The correct weighting is
given by equation (7). For example it is easily verified that:

In(p(xyz))

(22) J{xyz} + (2)-1
1
(J{xy} + J{xz} + J{yz})
2)
+ (0 (J{x} + J{y} + J{z}).
-1

-1

533

534

Miller and Goodman

The Gibbs model truncated to second order potentials would estimate the probability in this example by:

In(p( xyz))

~

~

4

(2) -1 (J{xy} + J{xz} + J{yz}) + (2)-1
? (J{x} +
1

J{y}

+ J{z})'

In Vp(xy)p(yz)p(xz)

PROOF OF THE INVERSION FORMULA

Theorem:
Let T be a finite set . Each element of T will be called an attribute. Each attribute
can take on one of a finite set of states called attribute values. A collection of
attribute values for every element of T is called a configuration w. For all A ~ T
(including both the empty set A 0 and the full set A T), let VA(w) and JA(W)
be functions mapping the states of the elements of A to the real numbers. Define
(r;:) = m!j(( m - n)! . n!) to be "m choose n."
Let V0(w) = 0, J0(W) = 0, and let VA(W) = JA(w) if IAI = l.
Then for IAI > 1:

=

=

(9)
and

L

(IAI- 1)-1

. Vb(w)

(10)

bCA
Ibl=IAI-l

are equivalent in that any assignment of VA and J A values for all A
(9) if and only if they also satisfy (1 0).

~

T will satisfy

Proof:
Let .:7 be any assignment ofthe values JA(W) for all A ~ T. Let V be any assignment
of all the values VA(W) for all A ~ T. Then clearly (9) maps any assignment .:7 to a
unique V. We will represent this mapping by the function I, so (9) is abbreviated
V = 1(.:7). Similarly (10) maps any assignment V to a unique.:7. Equation (10)
will be abbreviated .:7 = g(V). The result of Lemma Cl below, applied with the
value 1) set to n, shows that l(g(V))
V. In Lemma C2 below, it is shown
g(/(.:7)) .:7. Therefore the equations (9) and (10) are inverse one-to-one mappings
and the association of assignments between .:7 and V are identical for the two
Q.E.D.
equations.

=

=

Lemma Cl:
Rather than simply showing l(g(V)) = V, a more general result will be shown. Since
the number of potentials of a given order increases exponentially with the order, it
is useful to approximate the energy of a configuration by defining a maximum order
1) such that all potentials of greater order are assumed to be zero

Jb(W)

=0

\:I b such that

Ibl > 1).

Let VA(W) be the resulting approximation to the energy VA(W). Let

IAI = n.

Probability Estimation from a Database Using a Gibbs Energy Model

Given

L

JA(W) = VA(W) -

(n -

1)-1 . ~(w)

(11)

bCA

Ibl=n-l

and the order V approximation to equation (7):

VA(w) =

Lv

(

1)-1 L

~~1

i=l

then

(12)

Jb(W),

bCA

IbT=.

( 1)
VA(W) = L
nV-I

A

-1

Vb(w).

bCA

Ib!=V

Note:
For the case V = n, the approximation is exact

VA(w)
and so f(g(V?

= V is shown.

= VA(W),

The lemma's result has a simple interpretation. The energy of a configuration is
approximated by a scaled average of the energies of the configurations of order
V. Using equation (1) to relate energies to probabilities, shows that the estimated
probability is a scaled geometric mean of the order V marginal probabilities.
Proof:
We start with the given equation for VA(W)

Lv ( ~ ~ 11)-1 L Jb(w).
i=l

b~A

Ibl=?

Use equation (11) to substitute Jb(W) out of the equation:

VA(W)

t (~~ :)

-1

~ (~(W) - cCt;;~l 1)-1 . Vc(W?)
(i -

Ibl=.

Icl=lbl-l

Separate the term in the first sum where i = V

VA(w)

,E;( ~ 1)-1
1

V.(w)

("-1 (1)-1 ~. V.(w) )

+ ~ : ~1

-2:"(~~11)-1 L 2:
i=l

b~A

(i-1)-1?Vc(w).

cCb,lbl~l

Ibl=. 1c1=lbl-l

By subtracting VA(W) from both sides using equation (12) and noting the second
summation over i has no terms when i = 1 we see that it is sufficient to show

I:(:~;rLV.(W)
i=l

bCA

Ihl=.

t(~~;rL
i=2

L

cCb
Ibl=. Icl=lbl-l
b~A

(i-W'? V,(w).

535

536

Miller and Goodman

The right hand side inner double summation counts a given llc(w) once for every b
such that eC b ~ A with i Ibl lei + 1. This occurs exactly IAI- lei n - i + 1
times. Thus

= =

L

V-I (

~~

i=l

11)-1 L ~(w)

=

=

Lv (1)-1
~~1 L

i=2

"CA

1"1=.

?+1
1 . Vc(w).

~~

n

cCA

Icl=i-l

= i-Ion the right hand side
~(w) = ~ (n ~ 1) -1 L n. j . Vc(w).

Now perform a change of variables. Let j

~ (~ i=l

; ) -1

t -

L

J

j=l

"CA

1"1=.

J

cCA

!cl=j

Clearly both sides are identical since
n-t

Q.E.D.

=

Lemma C2: g(/(:1)) :1
Let IAI = n. It is sufficient to show that substituting
an identity:

JA(W)

=

VA(w) -

L

~

out of (10) using (9) yields

-1

(Ib l - 1)-1
L
lel- 1
Jc(w).
cCb

(n _1)-1 ?1Ib(w)

bCA,n;tl

Ibl=n-l

n - 1)
( IblL
1
bCA

L

-1

Jb(W) -

(n -1)

bCA,n;tl

-

Ibl=n-l-

Separate the term in the first sum for which b
JA(W) = JA(w)
n_l)-l

+ L ( Ibl- 1

h(w) -

bCA
b;tA

=A

L

(n - 1)

-1

"CA,n;tl

(lb l -l)-l
L
lel- 1
Jc(w).
cCb

Ibl=n-l-

Subtract J A (w) from both sides. The right hand side double sum counts a given
Jc(w) once for every b such that e ~ b C A with Ibl = IAI- 1 n - 1. This occurs
IAI- lei n - lei times. It is sufficient to show

=

=

cCA,ctA
Both sides are identical since:

_1)-1

( ~Z -

1

n- z
n-l

n -lei
n-l

(

n-2 )

lel- 1

-1

Jc(w).

_2)-1

( ~Z -

1

Q.E.D.

Probability Estimation from a Database Using a Gibbs Energy Model

5

USING THE INVERSION FORMULA TO SET
NETWORK WEIGHTS

Our method of probability estimation is to first collect empirical frequencies of
patterns (sub configurations) from the database. (An efficient hash table implementation of the algorithm is described in (Miller, 1993). The basic idea is to remove
from the database a pattern with low potential whenever there is a hash collision
which prevents a new pattern count from being stored.) Second, interpreting these
frequencies as probabilities, we convert each pattern frequency to a potential using
equation (8). We assume patterns with unknown or uncalculated frequencies have
zero potential. Low order patterns which never occur are assigned a large negative
potential (this approximation is needed to model events with zero probability in
the empirical distribution). Finally, we calculate the probability of any new pattern
not in the training set using the neural network implementation of equations (7)
and (1).

6

RESULTS

One way to validate the performance of a probability model is to test its performance
as a classifier. The probability model is used as a classifier by calculating the
probabilities of each unknown class value together with the known attribute values.
The most probable combination is then chosen as the predicted class. Used as a
classifier the Gibbs model tied or outperformed published results on a variety of
databases. Table 1 outlines results on three datasets taken from the UC Irvine
archive (Murphy, 1992). The Gibbs model results were collected from the very
first experiment using the algorithm with the datasets. No difficult parameter
adjustment is necessary to get the algorithm to classify at these rates. The iris
database has 4 real value attributes. Each attribute was quantized into a decile
ranking for use by the algorithm.

7

CONCLUSION

A new method of extracting a Gibbs probability model from a database has been
presented. The approach uses the Principle of Inclusion-Exclusion to invert a set of
collected statistics into a set of potentials for a Gibbs energy model. A hash table
implementation is used to efficiently process database records in order to collect the
most important potentials, or weights, which can be stored in the available memory.
Although the model is designed to give accurate probability estimates rather than
simply class labels, the model in practice works well as a classifier on a variety of
databases.

Acknowledgements
This work is funded in part by DARPA and ONR under grant NOOOI4-92-J-1860.

537

538

Miller and Goodman

Table 1: Summary of Classification Results
Database

A

C

R

Train

Test

Trials

Gibbs Rate

House Voting
Iris
Iris
Breast Cancer
Breast Cancer

16
4
4
9
9

2
3
3
2
2

435
150
150
699
369

335
120
149
599
200

100
30
1
100
169

50
100
1000
100
100

95.3%
96.3%
97.1%
97.3%
95.7%

Compare
95%
n.a.
98.0%
n.a.
93.7%

=
=
=

A Attribute count in the database, excluding the class attribute
C = Class count
R Record count
Train Number of records used to create the energy for one trial
Test = Number of records tested in a single trial
Trials = Number of independent train-test trials used to calculate the rate
Gibbs Rate = Gibbs energy model classification rate
Compare = Baseline classification result of other methods (Schlimmer, 1987),
(Weiss, 1992),(Zhang, 1992) respectively

References

D. Slepian, "On Maxentropic Discrete Stationary Processes," Bell System
Technical Journal, 51, pp.629-653, 1972.
G.E. Hinton and T .J. Sejnowski, "Learning and Relearning in Boltzmann
Machines," in Parallel Distributed Processing, Vol. I., pp.282-317, Cambridge
MA: MIT Press, 1986.
R. Kindermann, J .L. Snell, Markov Random Fields and their Applications,
Providence, RI: American Mathematical Society, 1980.
J. W. Miller, "Building Probabilistic Models from Databases" California Institute
of Technology, Ph.D. Thesis 1993.
P. Murphy, and D. Aha, UCI Repository of Machine Learning Databases
[Machine-readable data repository at ics.uci.edu in directory
/pub/machine-Iearning-databases]. Irvine, CA: University of California,
Department of Information and Computer Science, 1992.
Schlimmer, J. C., "Concept Acquisition Through Representational Adjustment"
University of California at Irvine, Ph.D. Thesis 1987.
S. Weiss, and I. Kapouleas, "An Empirical Comparison of Pattern Recognition,
Neural Nets, and Machine Learning Classification Methods," in Proceedings of
the 11th International Joint Conference on Artificial Intelligence Vol. 1,
pp.781-787, Los Gatos, CA: Morgan Kaufmann, 1992.
J. Zhang, "Selecting Typical Instances in Instance-Based Learning," in
Proceedings of the Ninth International Machine Learning Conference Aberdeen,
Scotland, pp.470-479, San Mateo CA: Morgan Kaufmann, 1992.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 2262-name-that-song-a-probabilistic-approach-to-querying-on-music-and-text.pdf

?Name That Song!?: A Probabilistic Approach
to Querying on Music and Text

Eric Brochu
Department of Computer Science
University of British Columbia
Vancouver, BC, Canada
ebrochu@cs.ubc.ca

Nando de Freitas
Department of Computer Science
University of British Columbia
Vancouver, BC, Canada
nando@cs.ubc.ca

Abstract
We present a novel, flexible statistical approach for modelling music and
text jointly. The approach is based on multi-modal mixture models and
maximum a posteriori estimation using EM. The learned models can be
used to browse databases with documents containing music and text, to
search for music using queries consisting of music and text (lyrics and
other contextual information), to annotate text documents with music,
and to automatically recommend or identify similar songs.

1 Introduction
Variations on ?name that song?-types of games are popular on radio programs. DJs play a
short excerpt from a song and listeners phone in to guess the name of the song. Of course,
callers often get it right when DJs provide extra contextual clues (such as lyrics, or a piece
of trivia about the song or band). We are attempting to reproduce this ability in the context
of information retrieval (IR). In this paper, we present a method for querying with words
and/or music.
We focus on monophonic and polyphonic musical pieces of known structure (MIDI files,
full music notation, etc.). Retrieving these pieces in multimedia databases, such as the
Web, is a problem of growing interest [1, 2]. A significant step was taken by Downie [3],
who applied standard text IR techniques to retrieve music by, initially, converting music to
text format. Most research (including [3]) has, however, focused on plain music retrieval.
To the best of our knowledge, there has been no attempt to model text and music jointly.
We propose a joint probabilistic model for documents with music and/or text. This model
is simple, easily extensible, flexible and powerful. It allows users to query multimedia
databases using text and/or music as input. It is well-suited for browsing applications as
it organizes the documents into ?soft? clusters. The document of highest probability in
each cluster serves as a music thumbnail for automated music summarisation. The model
allows one to query with an entire text document to automatically annotate the document
with musical pieces. It can be used to automatically recommend or identify similar songs.
Finally, it allows for the inclusion of different types of text, including website content,
lyrics, and meta-data such as hyper-text links. The interested reader may further wish to
consult [4], in which we discuss an application of our model to the problem of jointly

modelling music, as well as text and images.

2 Model specification
The training data consists of documents with text (lyrics or information about the song) and
musical scores in GUIDO notation [5]. (GUIDO is a powerful language for representing
musical scores in an HTML-like notation. MIDI files, plentiful on the World Wide Web,
can be easily converted to this format.) We model the data with a Bayesian multi-modal
mixture model. Words and scores are assumed to be conditionally independent given the
mixture component label.
We model musical scores with first-order Markov chains, in which each state corresponds
to a note, rest, or the start of a new voice. Notes? pitches are represented by the interval
change (in semitones) from the previous note, rather than by absolute pitch, so that a score
or query transposed to a different key will still have the same Markov chain. Rhythm is
similarly represented as a scalar to the previous value. Rest states are represented similarly,
save that pitch is not represented. See Figure 1 for an example.
Polyphonic scores are represented by chaining the beginning of a new voice to the end of
a previous one. In order to ensure that the first note in each voice appears in both the row
and column of the Markov transition matrix, a special ?new voice? state with no interval or
rhythm serves as a dummy state marking the beginning of a new voice. The first note of a
voice has a distinguishing ?first note? interval value and the first note or rest has a duration
value of one.
[ *3/4 b&1*3/16 b1/16 c#2*11/16 b&1/16 a&1*3/16 b&1/16 f#1/2 ]

0
1
2
3
4
5
6
7
8

INTERVAL
newvoice
rest
firstnote
+1
+2
-2
-2
+3
-5

DURATION
0




	





Figure 1: Sample melody ? the opening notes to ?The Yellow Submarine? by The Beatles
? in different notations. From top: GUIDO notation, standard musical notation (generated
automatically from GUIDO notation), and as a series of states in a first-order Markov
chain (also generated automatically from GUIDO notation).
The Markov chain representation of a piece of music  is then mapped to a sparse transition
frequency table 
 , where   
 denotes the number of times we observe the transition
from state  to state  in document  . We use  
  to denote the initial state of the Markov
chain. The associated text is modeled using a standard sparse term frequency vector 
 ,
where  
 denotes the number of times word  appears in document  . For notational
simplicity, we group the music and text variable as follows:  
 !  
#"  
#$ . In essence,
this Markovian approach is akin to a text bigram model, save that the states are transitions
between musical notes and rests rather than words.

Our multi-modal mixture model is as follows:






  



	

12














 














!#" $&%





'







 '

  "

.-



)(+*,"  "


/'





0

-

(1)
"



where

!
" 
"   " " 
$ encompasses all the model parameters and
4 first
'
 
where 5   
  '3 'if the
entry
of  4
  belongs to state  and is : otherwise. The three

7698
  "
dimensional
matrix
denotes the
estimated probability of transitioning from state

denotes the initial probabilities of being in state
 to state  in cluster' , the matrix 
 in cluster . The


 , given membership
vector
denotes the probability of each cluster.


 word 
The matrix 
denotes the probability
of the
in cluster . The mixture model
' the standard probability simplex !



is defined on
and
: for all
<
$ .







!

;

4=6>8


We introduce the latent allocation variables ? 
A@ ! "4BBB"&C $ to indicate that 
a 3particular
8
sequence D 
 belongs to a specific cluster . These indicator
variables ! ? 
FE 
"BB4B")CIH $

6G8
? 
correspond to an i.i.d. sample from the distribution
.


6JK6L

This simple model is easy to extend. For browsing applications, we might prefer a hierarchical structure with levels M :


  N





	

	P



O



34


M


 

4,








"
M

,'







"

(2)
M


This is still a multinomial model, but by applying appropriate parameter constraints we can
produce a tree-like browsing structure [6]. It is also easy to formulate the model in terms
of aspects and clusters as suggested in [7, 8].
2.1 Prior specification


We follow a hierarchical Bayesian strategy, where the unknown parameters and the allocation variables Q are regarded as being drawn from appropriate prior distributions. We
acknowledge our uncertainty about the exact form of the prior by specifying it in terms
of some unknown parameters (hyperparameters). The allocation variables ? 
 are assumed
. We place a conjugate
to be drawn from a multinomial distribution, ? 
 SR
E
UT
)
 8 . Similarly,
Dirichlet prior on the mixing coefficients 
we place Dirich
T
T
, T
3VWeach

 on
let prior
distributions
on
each



,
on each
"

,

Y
X







3

[
Z








- 3\]

, and assume that these
priors are independent.







The posterior for the allocation variables will be required. It can be obtained easily using
Bayes? rule:

3






?


6

'3K_a`
<


eN
 





" 

6^

 '


'3df[_a`












K6






 bN!#" $)% `

'













"


,  




 


fN   N!#" $)% `







 
`







`



'



  "
'

 (+*,"  "


  "

`

dfN (g*h"  "

- '
`




-




 0


'





"

dc

df, 0 - " 

(3)
e
c

3 Computation
The parameters of the mixture model cannot be computed analytically unless one knows
the mixture indicator variables. We have to resort to numerical methods. One can implement a Gibbs sampler to compute the parameters and allocation variables. This is done by
sampling the parameters from their Dirichlet posteriors and the allocation variables from
their multinomial posterior. However, this algorithm is too computationally intensive for

the applications we have in mind. Instead we opt for expectation maximization (EM) algorithms to compute the maximum likelihood (ML) and maximum a posteriori (MAP) point
estimates of the mixture model.
3.1 Maximum likelihood estimation with the EM algorithm
After initialization, the EM algorithm for ML estimation iterates between the following
two steps:
1. E step: Compute the expectation of the complete log-likelihood
with 
respect
to the  dis



    
	 old 
tribution of the allocation variables ML
,
Q " 
" 


where

6

old


%



2. M step: Maximize over the parameters:
ML



%
6

ML

function expands to
	

ML
6

  


new


The



represents the value of the parameters
time step.
 at
 the previous
%






'

	



 











12




34



 




 



  N

 



#" $ %



 



'





  "


 '

*,"  "

)(

 -




a




)0

-

B
"



In the E step, we have to compute
 using equation (3). The corresponding M step
3
 the constraints that all probabilities for the parequires that we maximize ML subject
to
rameters sum up to 1. This constrained maximization can be carried out by introducing
Lagrange multipliers. The resulting parameter estimates are:



	

]3

8
C H
I










6
<




  


<

4

6








'

 


<

(5)








  


 
   
  
<

 

 
 '3  

6




  





 h3









 

 






<


5

<





'
 

 
<

  "









]

6






(6)





 





(7)


(8)


3.2 Maximum a posteriori estimation with the EM algorithm
The EM formulation for MAP estimation is straightforward. One simply has to augment
the objective function in the M step, ML , by adding to it the log prior densities. That is,
the MAP objective function is



MAP
6

The MAP parameter estimates are:

& (

 '













6



6



6



6





  "


]





%

Q
'

%

"$#
"







ML

8

e
 

&

<


e

'



% 


6









 
 '% 3  
C 

CIH

) 
' %


 5


 

<

 '



8)

,'3  
%
e  
'



'


e


<
+
C
*
<
 

 

  
,     
 ' % < 
 
  

8
,
% <  
 < 
 '
  
<
  e  
    e  
 ' C *
 


-  
 ' % < 
.  


 
3
 
- 8
% < 
 


<
.- e
  e  
 ' C 
 
'3

<



 


 !  " 	 old 

(9)



(10)






  

'3



(11)


(12)

CLUSTER
2
2
2
..
.
4
4
4
4
..
.
6
..
.
7
7
7
..
.
9
9
9

SONG
Moby ? Porcelain
Nine Inch Nails ? Terrible Lie
other ? ?Addams Family? theme
..
.
J. S. Bach ? Invention #1
J. S. Bach ? Invention #8
J. S. Bach ? Invention #15
The Beatles ? Yellow Submarine
..
.
other ? ?Wheel of Fortune? theme
..
.
The Beatles ? Taxman
The Beatles ? Got to Get You Into My Life
The Cure ? Saturday Night
..
.
R.E.M ? Man on the Moon
Soft Cell ? Tainted Love
The Beatles ? Got to Get You Into My Life

 	
1
1
1
..
.
1
1
1
0.9975
..
.
1
..
.
1
0.7247
1
..
.
1
1
0.2753

Figure 2: Representative probabilistic cluster allocations using MAP estimation.

These expressions can also be derived by considering the posterior
modes and by replacing

the cluster indicator variable with its posterior estimate
 . This observation opens up


room for various stochastic and deterministic ways of improving
EM.

4 Experiments
To test the model with text and music, we clustered a database of musical scores with
associated text documents. The database is composed of various types of musical scores ?
jazz, classical, television theme songs, and contemporary pop music ? as well as associated
text files. The scores are represented in GUIDO notation. The associated text files are a
song?s lyrics, where applicable, or textual commentary on the score for instrumental pieces,
all of which were extracted from the World Wide Web.
The experimental database contains 100 scores, each with a single associated text document. There is nothing in the model, however, that requires this one-to-one association
of text documents and scores ? this was done solely for testing simplicity and efficiency.
In a deployment such as the world wide web, one would routinely expect one-to-many or
many-to-many mappings between the scores and text.
We carried out ML and MAP estimation with
EM. The The Dirichlet hyper-parameters


: "
: "
were set to
. The MAP approach resulted in sparser (reg"
V
6 coherent
8 X96
8 clusters.
Z
6
8 Figure
\
6 2 shows some representative cluster probability
ularised), more
assignments obtained with MAP estimation.
By and large, the MAP clusters are intuitive. The 15 pieces by J. S. Bach each have very
 : B 
	 ) probabilities of membership in the same cluster. A few curious anomalies
high (
exist.  The Beatles? song The Yellow Submarine is included in the same cluster as the Bach
pieces, though all the other Beatles songs in the database are assigned to other clusters.

4.1 Demonstrating the utility of multi-modal queries
A major intended use of the text-score model is for searching documents on a combination
of text and music.
Consider a hypothetical example, using our database: A music fan is struggling to recall a
dimly-remembered song with a strong repeating single-pitch, dotted-eight-note/sixteenthnote bass line, and lyrics containing the words come on, come on, get down. A search on
the text portion alone turns up four documents which contain the lyrics. A search on the
notes alone returns seven documents which have matching transitions. But a combined
search returns only the correct document (figure 3).
QUERY

RETRIEVED SONGS

come on, come on, get down
Erksine Hawkins ? Tuxedo Junction
Moby ? Bodyrock
Nine Inch Nails ? Last
Sherwood Schwartz ? ?The Brady Bunch? theme song

The Beatles ? Got to Get You Into My Life
The Beatles ? I?m Only Sleeping
The Beatles ? Yellow Submarine
Moby ? Bodyrock
Moby ? Porcelain
Gary Portnoy ? ?Cheers? theme song
Rodgers & Hart ? Blue Moon

come on, come on, get down
Moby ? Bodyrock

Figure 3: Examples of query matches, using only text, only musical notes, and both text
and music. The combined query is more precise.

4.2 Precision and recall
We evaluated our retrieval system with randomly generated queries. A query is composed of a random series of 1 to 5 note transitions,
and 1 to 5 words,
. We then
determine the actual number of matches C in the database, where a match is defined as a
song  
 such that all elements of
and
have a frequency of 1 or greater. In order to
 : .
avoid skewing the results unduly, we reject any query that has C
or C









	

To perform a query, we simply sample probabilistically without
replacement from the clus
ters. The probability of sampling from each cluster,
, is computed using equation 3.
'3it
 is  assigned a sampling probability
If a cluster contains no items or later becomes empty,
of zero, and the probabilities of the remaining clusters are re-normalized.
In each iteration  , a cluster is selected, and the matching criteria are applied against each



piece of music that has been assigned to that cluster until a match is found. If no match is
found, an arbitrary piece is selected. The selected piece is returned as the rank-  result.
Once all the matches have been returned, we compute the standard precision-recall curve
[9], as shown in Figure 4.


Our querying method enjoys a high precision until recall is approximately : , and experiences a relatively modest deterioration of precision thereafter. By choosing clusters before

Figure 4: Precision-recall curve showing average results, over 1000 randomly-generated
queries, combining music and text matching criteria.
matching, we overcome the polysemy problem. For example, river banks and money banks
appear in separate clusters. We also deal with synonimy since automobiles and cars have
high probability of belonging to the same clusters.
4.3 Association
The probabilistic nature of our approach allows us the flexibility to use our techniques and
database for tasks beyond traditional querying. One of the more promising avenues of
exploration is associating documents with each other probabilistically. This could be used,
for example, to find suitable songs for web sites or presentations (matching on text), or for
recommending songs similar to one a user enjoys (matching on scores).
Given an input document, , we first cluster by finding the most likely cluster as de 

termined by computing 
(equation 3). Input documents containing text or


music only can be clustered using 
only
those
components of the database. Input documents
that combine text and music are clustered using all the data. We can then find the closest association by computing the distance from the input document to the other document vectors
in the cluster using a similarity metric such as Euclidean distance, or cosine measures after
carrying out latent semantic indexing [10]. A few selected examples of associations we
found are shown in figure 5. The results are often reasonable, though unexpected behavior
occasionally occurs.

5 Conclusions
We feel that the probabilistic approach to querying on music and text presented here is
powerful, flexible, and novel, and suggests many interesting areas of future research. In
the future, we should be able to incorporate audio by extracting suitable features from the

INPUT
J. S. Bach ? Toccata and Fugue in D Minor (score)
Nine Inch Nails ? Closer (score & lyrics)
T. S. Eliot ? The Waste Land (text poem)

CLOSEST MATCH
J. S. Bach ? Invention #5
Nine Inch Nails ? I Do Not Want This
The Cure ? One Hundred Years

Figure 5: The results of associating songs in the database with other text and/or musical
input. The input is clustered probabilistically and then associated with the existing song
that has the least Euclidean distance in that cluster. The association of The Wasteland with
The Cure?s thematically similar One Hundred Years is likely due to the high co-occurance
of relatively uncommon words such as water, death, and year(s).
signals. This will permit querying by singing, humming, or via recorded music. There are
a number of ways of combining our method with images [6, 4], opening up room for novel
applications in multimedia [11].
Acknowledgments
We would like to thank Kobus Barnard, J. Stephen Downie, Holger Hoos and Peter Carbonetto for their advice and expertise in preparing this paper.

References
[1] D Huron and B Aarden. Cognitive issues and approaches in music information retrieval. In
S Downie and D Byrd, editors, Music Information Retrieval. 2002.
[2] J Pickens. A comparison of language modeling and probabilistic text information retrieval
approaches to monophonic music retrieval. In International Symposium on Music Information
Retrieval, 2000.
[3] J S Downie. Evaluating a Simple Approach to Music Information Retrieval: Conceiving
Melodic N-Grams as Text. PhD thesis, University of Western Ontario, 1999.
[4] E Brochu, N de Freitas, and K Bao. The sound of an album cover: Probabilistic multimedia and
IR. In C M Bishop and B J Frey, editors, Ninth International Workshop on Artificial Intelligence
and Statistics, Key West, Florida, 2003. To appear.
[5] H H Hoos, K A Hamel, K Renz, and J Kilian. Representing score-level music using the GUIDO
music-notation format. Computing in Musicology, 12, 2001.
[6] K Barnard and D Forsyth. Learning the semantics of words and pictures. In International
Conference on Computer Vision, volume 2, pages 408? 415, 2001.
[7] T Hofmann. Probabilistic latent semantic analysis. In Uncertainty in Artificial Intelligence,
1999.
[8] D M Blei, A Y Ng, and M I Jordan. Latent Dirichlet allocation. In T G Dietterich, S Becker, and
Z Ghahramani, editors, Advances in Neural Information Processing Systems 14, Cambridge,
MA, 2002. MIT Press.
[9] R Baeza-Yates and B Ribeiro-Neto. Modern Information Retrieval. Addison-Wesley, 1999.
[10] S Deerwester, S T Dumais, G W Furnas, T K Landauer, and R Harshman. Indexing by latent
semantic indexing. Journal of the American Society for Information Science, 41(6):391? 407,
1990.
[11] P Duygulu, K Barnard, N de Freitas, and D Forsyth. Object recognition as machine translation:
Learning a lexicon for a fixed image vocabulary. In ECCV, 2002.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 4858-adaptive-anonymity-via-b-matching.pdf

Adaptive Anonymity via b-Matching

Krzysztof Choromanski
Columbia University
kmc2178@columbia.edu

Tony Jebara
Columbia University
tj2008@columbia.edu

Kui Tang
Columbia University
kt2384@columbia.edu

Abstract
The adaptive anonymity problem is formalized where each individual shares their
data along with an integer value to indicate their personal level of desired privacy.
This problem leads to a generalization of k-anonymity to the b-matching setting.
Novel algorithms and theory are provided to implement this type of anonymity.
The relaxation achieves better utility, admits theoretical privacy guarantees that
are as strong, and, most importantly, accommodates a variable level of anonymity
for each individual. Empirical results confirm improved utility on benchmark and
social data-sets.

1 Introduction
In many situations, individuals wish to share their personal data for machine learning applications
and other exploration purposes. If the data contains sensitive information, it is necessary to protect
it with privacy guarantees while maintaining some notion of data utility [18, 2, 24]. There are
various definitions of privacy. These include k-anonymity [19], l-diversity [16], t-closeness [14]
and differential1 privacy [3, 22]. All these privacy guarantees fundamentally treat each contributed
datum about an individual equally. However, the acceptable anonymity and comfort-level of each
individual in a population can vary widely. This article explores the adaptive anonymity setting and
shows how to generalize the k-anonymity framework to handle it. Other related approaches have
been previously explored [20, 21, 15, 5, 6, 23] yet herein we contribute novel efficient algorithms
and formalize precise privacy guarantees. Note also that there are various definitions of utility. This
article focuses on the use of suppression since it is well-formalized. Therein, we hide certain values
in the data-set by replacing them with a ? symbol (fewer ? symbols indicate higher utility). The
overall goal is to maximize utility while preserving each individual?s level of desired privacy.
This article is organized as follows. ? 2 formalizes the adaptive anonymity problem and shows
how k-anonymity does not handle it. This leads to a relaxation of k-anonymity into symmetric
and asymmetric bipartite regular compatibility graphs. ? 3 provides algorithms for maximizing
utility under these relaxed privacy criteria. ? 4 provides theorems to ensure the privacy of these
relaxed criteria for uniform anonymity as well as for adaptive anonymity. ? 5 shows experiments on
benchmark and social data-sets. Detailed proofs are provided in the Supplement.

2 Adaptive anonymity and necessary relaxations to k-anonymity
The adaptive anonymity problem considers a data-set X ? Zn?d consisting of n ? N observations
{x1 , . . . , xn } each of which is a d-dimensional discrete vector, in other words, xi ? Zd . Each user
i contributes an observation vector xi which contains discrete attributes pertaining to that user2 .
Furthermore, each user i provides an adaptive anonymity parameter ?i ? N they desire to keep
when the database is released. Given such a data-set and anonymity parameters, we wish to output
an obfuscated data-set denoted by Y ? {Z ? ?}n?d which consists of vectors {y1 , . . . , yn } where
1
2

Differential privacy often requires specifying the data application (e.g. logistic regression) in advance [4].
For instance, a vector can contain a user?s gender, race, height, weight, age, income bracket and so on.

1

yi (k) ? {xi (k), ?}. The star symbol ? indicates that the k?th attribute has been masked in the i?th
user-record. We say that vector xi is compatible with vector yj if xi (k) = yj (k) for all elements of
yj (k) $= ?. The goal of this article is to create a Y which contains a minimal number of ? symbols
such that each entry yi of Y is compatible with at least ?i entries of X and vice-versa.
The most pervasive method for anonymity in the released data is the k-anonymity method [19, 1].
However, it is actually more constraining than the above desiderata. If all users have the same value
?i = k, then k-anonymity suppresses data in the database such that, for each user?s data vector in the
released (or anonymized) database, there are at least k ? 1 identical copies in the released database.
The existence of copies is used by k-anonymity to justify some protection to attack.
We will show that the idea of k ? 1 copies can be understood as forming a compatibility graph between the original database and the released database which is composed of several fully-connected
k-cliques. However, rather than guaranteeing copies or cliques, the anonymity problem can be
relaxed into a k-regular compatibility to achieve nearly identical resilience to attack. More interestingly, this relaxation will naturally allow users to select different ?i anonymity values or degrees in
the compatibility graph and allow them to achieve their desired personal protection level.
Why can?t k-anonymity handle heterogeneous anonymity levels ?i ? Consider the case where the
population contains many liberal users with very low anonymity levels yet one single paranoid user
(user i) wants to have a maximal anonymity with ?i = n. In the k-anonymity framework, that user
will require n ? 1 identical copies of his data in the released database. Thus, a single paranoid user
will destroy all the information of the database which will merely contain completely redundant
vectors. We will propose a b-matching relaxation to k-anonymity which prevents this degeneracy
since it does not merely handle compatibility queries by creating copies in the released data.
While k-anonymity is not the only criterion for privacy, there are situations in which it is sufficient
as illustrated by the following scenario. First assume the data-set X is associated with a set of
identities (or usernames) and Y is associated with a set of keys. A key may be the user?s password
or some secret information (such as their DNA sequence). Represent the usernames and keys using
integers x1 , . . . , xn and y1 , . . . , yn , respectively. Username xi ? Z is associated with entry xi and
key yj ? Z is associated with entry yj . Furthermore, assume that these usernames and keys are
diverse, unique and independent of their corresponding attributes. These x and y values are known
as the sensitive attributes and the entries of X and Y are the non-sensitive attributes [16]. We aim to
release an obfuscated database Y and its keys with the possibility that an adversary may have access
to all or a subset of X and the identities.
The goal is to ensure that the success of an attack (using a username-key pair) is low. In other
words, the attack succeeds with probability no larger than 1/?i for a user which specified ?i ? N.
Thus, the attack we seek to protect against is the use of the data to match usernames to keys (rather
than attacks in which additional non-sensitive attributes about a user are discovered). In the uniform
?i setting, k-anonymity guarantees that a single one-time attack using a single username-key pair
succeeds with probability at most 1/k. In the extreme case, it is easy to see that replacing all of Y
with ? symbols will result in an attack success probability of 1/n if the adversary attempts a single
random attack-pair (username and key). Meanwhile, releasing a database Y = X with keys could
allow the adversary to succeed with an initial attack with probability 1.
We first assume that all degrees ?i are constant and set to ? and discuss how the proposed b-matching
privacy output subtly differs from standard k-anonymity [19]. First, define quasi-identifiers as sets
of attributes like gender and age that can be linked with external data to uniquely identify an individual in the population. The k-anonymity criterion says that a data-set such as Y is protected against
linking attacks that exploit quasi-identifiers if every element is indistinguishable from at least k ? 1
other elements with respect to every set of quasi-identifier attributes. We will instead use a compatibility graph G to more precisely characterize how elements are indistinguishable in the data-sets and
which entries of Y are compatible with entries in the original data-set X. The graph places edges
between entries of X which are compatible with entries of Y. Clearly, G is an undirected bipartite
graph containing two equal-sized partitions (or color-classes) of nodes A and B each of cardinality
n where A = {a1 , . . . , an } and B = {b1 , . . . , bn }. Each element of A is associated with an entry of
X and each element of B is associated with an entry of Y. An edge e = (i, j) ? G that is adjacent
to a node in A and a node in B indicates that the entries xi and yj are compatible. The absence of
an edge means nothing: entries are either compatible or not compatible.

2

For ?i = ?, b-matching produces ?-regular bipartite graphs G while k-anonymity produces ?-regular
clique-bipartite graphs3 defined as follows.
Definition 2.1 Let G(A, B) be a bipartite graph with color classes: A, B where A =
{a1 , ..., an }, B = {b1 , ..., bn }. We call a k-regular bipartite graph G(A, B) a clique-bipartite
graph if it is a union of pairwise disjoint and nonadjacent complete k-regular bipartite graphs.
Denote by Gbn,? the family of ?-regular bipartite graphs with n nodes. Similarly, denote by Gkn,?
the family of ?-regular graphs clique-bipartite graphs. We will also denote by Gsn,? the family of
symmetric b-regular graphs using the following definition of symmetry.
Definition 2.2 Let G(A, B) be a bipartite graph with color classes: A, B where A =
{a1 , ..., an }, B = {b1 , ..., bn }. We say that G(A, B) is symmetric if the existence of an edge (ai , bj )
in G(A, B) implies the existence of an edge (aj , bi ), where 1 ? i, j ? n.
For values of n that are not trivially small, it is easy to see that the graph families satisfy
Gkn,? ? Gsn,? ? Gbn,? . This holds since symmetric ?-regular graphs are ?-regular with the additional
symmetry constraint. Clique-bipartite graphs are ?-regular graphs constrained to be clique-bipartite
and the latter property automatically yields symmetry.
This article introduces graph families Gbn,? and Gsn,? to enforce privacy since these are relaxations
of the family Gkn,b as previously explored in k-anonymity research. These relaxations will achieve
better utility in the released database. Furthermore, they will allow us to permit adaptive anonymity
levels across the users in the database. We will drop the superscripts n and ? whenever the meaning
is clear from the context. Additional properties of these graph families will be formalized in ? 4 but
we first informally illustrate how they are useful in achieving data privacy.
username

alice
bob
carol
dave
eve
fred

key

1
0
0
1
1
0

0
0
0
0
1
1

0
0
1
1
0
1

0
0
1
1
0
1

*
*
*
*
*
*

0
0
0
0
1
1

0
0
1
1
*
*

0
0
1
1
*
*

ggacta
tacaga
ctagag
tatgaa
caacgc
tgttga

Figure 1: Traditional k-anonymity (in Gk ) for n = 6, d = 4, ? = 2 achieves #(?) = 10. Left to
right: usernames with data (x, X), compatibility graph (G) and anonymized data with keys (Y, y).

username

alice
bob
carol
dave
eve
fred

key

1
0
0
1
1
0

0
0
0
0
1
1

0
0
1
1
0
1

0
0
1
1
0
1

*
*
*
*
1
0

0
*
0
*
*
*

0
0
1
1
0
1

0
0
1
1
0
1

ggacta
tacaga
ctagag
tatgaa
caacgc
tgttga

Figure 2: The b-matching anonymity (in Gb ) for n = 6, d = 4, ? = 2 achieves #(?) = 8. Left to
right: usernames with data (x, X), compatibility graph (G) and anonymized data with keys (Y, y).
In figure 1, we see an example of k-anonymity with a graph from Gk . Here each entry of the
anonymized data-set Y appears k = 2 times (or ? = 2). The compatibility graph shows 3 fully
connected cliques since each of the k copies in Y has identical entries. By brute force exploration
3
Traditional k-anonymity releases an obfuscated database of n rows where there are k copies of each row.
So, each copy has the same neighborhood. Similarly, the entries of the original database all have to be connected
to the same k copies in the obfuscated database. This induces a so-called bipartite clique-connectivity.

3

we find that the minimum number of stars to achieve this type of anonymity is #(?) = 10. Moreover, since this problem is NP-hard [17], efficient algorithms rarely achieve this best-possible utility
(minimal number of stars).
Next, consider figure 2 where we have achieved superior utility by only introducing #(?) = 8 stars
to form Y. The compatibility graph is at least ? = 2-regular. It was possible to find a smaller
number of stars since ?-regular bipartite graphs are a relaxation of k-clique graphs as shown in
figure 1. Another possibility (not shown in the figures) is a symmetric version of figure 2 where
nodes on the left hand side and nodes on the right hand side have a symmetric connectivity. Such an
intermediate solution (since Gk ? Gs ? Gb ) should potentially achieve #(?) between 8 and 10.

It is easy to see why all graphs have to have a minimum degree of ? at least (i.e. must contain a
?-regular graph). If one of the nodes has a degree of 1, then the adversary will know the key (or the
username) for that node with certainty. If each node has degree ? or larger, then the adversary will
have probability at most 1/? of choosing the correct key (or username) for any random victim.

We next describe algorithms which accept X and integers ?1 , . . . , ?n and output Y such that each
entry i in Y is compatible with at least ?i entries in X and vice-versa. These algorithms operate by
finding a graph in Gb or Gs and achieve similar protection as k-anonymity (which finds a graph in
the most restrictive family Gk and therefore requires more stars). We provide a theoretical analysis
of the topology of G in these two new families to show resilience to single and sustained attacks
from an all-powerful adversary.

3 Approximation algorithms
While the k-anonymity suppression problem is known to be NP-hard, a polynomial time method
with an approximation guarantee is the forest algorithm [1] which has an approximation ratio of 3k?
3. In practice, though, the forest algorithm is slow and achieves poor utility compared to clustering
methods [10]. We provide an algorithm
for the b-matching anonymity problem with approximation
?
ratio of ? and runtime of O(?m n) where n is the number of users in the data, ? is the largest
anonymity level in {?1 , . . . , ?n } and m is the number of edges to explore (in the worst case with
no prior knowledge, we have m = O(n2 ) edges between all possible users). One algorithm solves
for minimum weight bipartite b-matchings which is easy to implement using linear programming,
max-flow methods or belief propagation in the bipartite case [9, 11]. The other algorithm uses a
general non-bipartite solver which involves Blossom structures and requires O(?mn log(n)) time[8,
9, 13]. Fortunately, minimum weight general matching has recently been shown to require only
O(m"?1 log "?1 ) time to achieve a (1 ? ") approximation [7].
First, we define two quantities of interest. Given a graph
matrix G ? Bn?n and a
!
! G
!with adjacency
data-set X, the Hamming error is defined as h(G) = i j Gij k (Xik $= Xjk ). The number of
! ! "
stars to achieve G is s(G) = nd ? i k j (1 ? Gij (Xik $= Xjk )) .

Recall Gb is the family of regular bipartite graphs. Let minG?Gb s(G) be the minimum number of
stars (or suppressions) that one can place in Y while keeping the entries in Y compatible with at
least ? entries in X and vice-versa. We propose the following polynomial time algorithm which,
in its first iteration, minimizes h(G) over the family Gb and then iteratively minimizes a variational
upper bound [12] on s(G) using a weighted version of the Hamming distance.
Algorithm 1 variational bipartite b-matching
Input X ? Zn?d , ?i ? N for i ? {1, . . . , n}, ? > 0 and initialize W ? Rn?d to the all 1s matrix
While not converged {
? = arg minG?Bn?n ! Gij ! Wik (Xik $= Xjk ) s.t. ! Gij = ! Gji ? ?i
Set G
ij #
k
j
j
$
! ?
For all i and k set Wik = exp
Gij (Xik $= Xjk ) ln ?
}
j

1+?

? ij = 1 and Xjk $= Xik for any j
For all i and k set Yik = ? if G
Choose random permutation M as matrix M ? Bn?n and output Ypublic = MY

We can further restrict the b-matching solver such that the graph G is symmetric with respect to both
the original data X and the obfuscated data Y. To do so, we require that G is a symmetric matrix.
? is recovered by a general
This will produce a graph G ? Gs . In such a situation, the value of G
4

unipartite b-matching algorithm rather than a bipartite b-matching program. Thus, the set of possible
output solutions is strictly smaller (the bipartite formulation relaxes the symmetric one).
Algorithm 2 variational symmetric b-matching
Input X ? Zn?d , ?i ? N for i ? {1, . . . , n}, ? > 0 and initialize W ? Rn?d to the all 1s matrix
While not converged {
? = arg minG?Bn?n ! Gij ! Wik (Xik $= Xjk ) s.t. ! Gij ? ?i , Gij = Gji
Set G
k
j
ij #
$
! ?
For all i and k set Wik = exp
Gij (Xik $= Xjk ) ln ?
}
j

1+?

? ij = 1 and Xjk $= Xik for any j
For all i and k set Yik = ? if G
Choose random permutation M as matrix M ? Bn?n and output Ypublic = MY

? such that s(G)
? ? ? minG?G s(G).
Theorem 1 For ?i ? ?, iteration #1 of algorithm 1 finds G
b
?
Theorem 2 Each iteration of algorithm 1 monotonically decreases s(G).
Theorem 1 and 2 apply to both algorithms. Both algorithms4 manipulate a bipartite regular graph
G(A, B) containing the true matching {(a1 , b1 ), . . . , (an , bn )}. However, they ultimately release the
data-set Ypublic after randomly shuffling Y according to some matching or permutation M which
hides the true matching. The random permutation or matching M can be represented as a matrix
M ? Bn?n or as a function ? : {1, . . . , n} ? {1, . . . , n}. We now discuss how an adversary can
attack privacy by recovering this matching or parts of it.

4 Privacy guarantees
We now characterize the anonymity provided by a compatibility graph G ? Gb (or G ? Gs ) under
several attack models. The goal of the adversary is to correctly match people to as many records as
possible. In other words, the adversary wishes to find the random matching M used in the algorithms
(or parts of M ) to connect the entries of X to the entries of Ypublic (assuming the adversary has
stolen X and Ypublic or portions of them). More precisely, we have a bipartite graph G(A, B) with
color classes A, B, each of size n. Class A corresponds to n usernames and class B to n keys. Each
username in A is matched to its key in B through some unknown matching M .
We consider the model where the graph G(A, B) is ?-regular, where ? ? N is a parameter chosen by
the publisher. The latter is especially important if we are interested in guaranteeing different levels
of privacy for different users and allowing ? to vary with the user?s index i.
Sometimes it is the case that the adversary has some additional information and at the very beginning
knows some complete records that belong to some people. In graph-theoretic terms, the adversary
thus knows parts of the hidden matching M in advance. Alternatively, the adversary may have
come across such additional information through sustained attack where previous attempts revealed
the presence or absence of an edge. We are interested in analyzing how this extra knowledge can
help him further reveal other edges of the matching. We aim to show that, for some range of the
parameters of the bipartite graphs, this additional knowledge does not help him much. We will
compare the resilience to attack relative to the resilience of k-anonymity. We say that a person v is
k-anonymous if his or her real data record can be confused with at least k ? 1 records from different
people. We first discuss the case of single attacks and then discuss sustained attacks.
4.1 One-Time Attack Guarantees
Assume first that the adversary has no extra information about the matching and performs a one-time
attack. Then, lemma 4.1 holds which is a direct implication of lemma 4.2.
Lemma 4.1 If G(A, B) is an arbitrary ?-regular graph and the adversary does not know any edges
of the matching he is looking for then every person is ?-anonymous.
4
It is straightforward to put a different weight on certain suppressions over others if the utility of the data
is not uniform for each entry or bit. This done by using an n ? d weight matrix in the optimization. It is also
straightforward to handle missing data by allowing initial stars in X before anonymizing.

5

Lemma 4.2 Let G(A, B) be a ?-regular bipartite graph. Then for every edge e of G(A, B) there
exists a perfect matching in G(A, B) that uses e.
The result does not assume any structure in the graph beyond its ?-regularity. Thus, for a single
attack, b-matching anonymity (symmetric or asymmetric) is equivalent to k-anonymity when b = k.
Corollary 4.1 Assume the bipartite graph G(A, B) is either ?-regular, symmetric ?-regular or
clique-bipartite and ?-regular. An adversary attacking G once succeeds with probability ? 1/?.
4.2 Sustained Attack on k-Cliques
Now consider the situation of sustained attacks or attacks with prior information. Here, the adversary may know c ? N edges in M a priori by whatever means (previous attacks or through side
information). We begin by analyzing the resilience of k-anonymity where G is a cliques-structured
graph. In the clique-bipartite graph, even if the adversary knows some edges of the matching (but
not too many) then there still is hope of good anonymity for all people. The anonymity of every
person decreases from ? to at least (? ? c). So, for example, if the adversary knows in advance ?2
edges of the matching then we get the same type of anonymity for every person as for the model
with two times smaller degree in which the adversary has no extra knowledge. So we will be able to
show the following:
Lemma 4.3 If G(A, B) is clique-bipartite ?-regular graph and the adversary knows in advance c
edges of the matching then every person is (? ? c)-anonymous.
The above is simply a consequence of the following lemma.
Lemma 4.4 Assume that G(A, B) is clique-bipartite ?-regular graph. Denote by M some perfect
matching in G(A, B). Let C be some subset of the edges of M and let c = |C|. Fix some vertex
v ? A not matched in C. Then there are at least (? ? c) edges adjacent to v such that, for each of
these edges e, there exists some perfect matching M e in G(A, B) that uses both e and C.
Corollary 4.2 Assume graph G(A, B) is a clique-bipartite and ?-regular. Assume that the adversary knows in advance c edges of the matching. The adversary selects uniformly at random a vertex
the privacy of which he wants to break from the set of vertices he does not know in advance. Then
1
he succeeds with probability at most ??c
.
We next show that b-matchings achieve comparable resilience under sustained attack.
4.3 Sustained attack on asymmetric bipartite b-matching
We now consider the case where we do not have a graph G(A, B) which is clique-bipartite but rather
is only ?-regular and potentially asymmetric (as returned by algorithm 1).
Theorem 4.1 Let G(A,B) be a ?-regular bipartite graph with color classes: A and B. Assume
that |A| = |B| = n. Denote by M some perfect matching M in G(A, B). Let C be some
$
subset of the edges of M and let c = |C|.
% Take some ? ? c. Denote n = n ? c. Fix

any function ? : N ? R satisfying ?? (?
?(?)+

? =

?

2? +

1
4

< ?(?) < ?). Then for all but at most

?2 (?)?2? 2 ?

2c? 2 n" ?(1+
)
2??
r
?(1? c )
2?
?
1
c
+
)
)(
?3 (?)(1+ 1? ?2?
?
2 (?)
?
?(?)
?(?)

+

c?
?(?)

vertices v ? A not matched in C the following

holds: The size of the set of edges e adjacent to v and having the additional property that there exists some perfect matching M v in G(A, B) that uses e and edges from C is: at least (? ? c ? ?(?)).
Essentially, theorem 4.1 says that all %
but at most a small number ? of people are (? ? c ? ?(?))anonymous for every ? satisfying: c 2? + 41 < ?(?) < ? if the adversary knows in advance c
edges of the matching. For example, take ?(?) := ?? for ? ? (0, 1). Fix ? = c and assume that
1
the adversary knows in advance at most ? 4 edges of the matching. Then, using the formula from
6

theorem 4.1, we obtain that (for n large enough) all but at most
1
4

4n"
?

1
4

?3

1

+

?4
?

people from those that

the adversary does not know in advance are ((1 ? ?)? ? ? )-anonymous. So if ? is large enough
then all but approximately a small fraction 14 3 of all people not known in advance are almost
(1 ? ?)?-anonymous.

?4?

Again take ?(?) := ?? where ? ? (0, 1). Take ? = 2c. Next assume that 1 ? c ? min( ?4 , ?(1 ?
? ? ?2 )). Assume that the adversary selects uniformly at random a person to attack. Our goal is to
find an upper bound on the probability he succeeds. Then, using theorem 4.1, we can conclude that
all but at most F n$ people whose records are not known in advance are ((1 ? ?)? ? c)-anonymous
2
1
for F = 33c
? 2 ? . The probability of success is at most: F + (1 ? F ) (1??)??c . Using the expression
on F that we have and our assumptions, we can conclude that the probability we are looking for is
2
at most 34c
? 2 ? . Therefore we have:
Theorem 4.2 Assume graph G(A, B) is ?-regular and the adversary knows in advance c edges of
the matching, where c satisfies: 1 ? c ? min( ?4 , ?(1 ? ? ? ?2 )). The adversary selects uniformly at
random a vertex the privacy of which he wants to break from those that he does not know in advance.
2
Then he succeeds with probability at most 34c
?2? .
4.4 Sustained attack on symmetric b-matching with adaptive anonymity
We now consider the case where the graph is not only ?-regular but also symmetric as defined in
definition 2.2 and as recovered by algorithm 2. Furthermore, we consider the case where we have
varying values of ?i for each node since some users want higher privacy than others. It turns out
that if the corresponding bipartite graph is symmetric (we define this term below) we can conclude
that each user is (?i ? c)-anonymous, where ?i is the degree of a vertex associated with the user
of the bipartite matching graph. So we get results completely analogous to those for the much
simpler models described before. We will use a slightly more elaborate definition of symmetric5,
however, since this graph has one if its partitions permuted by a random matching (the last step in
both algorithms before releasing the data).
Definition 4.1 Let G(A, B) be a bipartite graph with color classes: A, B and matching M =
{(a1 , b1 ), ...(an , bn )}, where A = {a1 , ..., an }, B = {b1 , ..., bn }. We say that G(A, B) is symmetric
with respect to M if the existence of an edge (ai , bj ) in G(A, B) implies the existence of an edge
(aj , bi ), where 1 ? i, j ? n.
From now on, the matching M with respect to which G(A, B) is symmetric is a canonical matching
of G(A, B). Assume that G(A, B) is symmetric with respect to its canonical matching M (it does
not need to be a clique-bipartite graph). In such a case, we will prove that, if the adversary knows
in advance c edges of the matching, then every person from the class A of degree ?i is (?i ? c)anonymous. So we obtain the same type of anonymity as in a clique-bipartite graph (see: lemma 4.3).
Lemma 4.5 Assume that G(A, B) is a bipartite graph, symmetric with respect to its canonical
matching M . Assume furthermore that the adversary knows in advance c edges of the matching.
Then every person that he does not know in advance is (?i ? c)-anonymous, where ?i is a degree of
the related vertex of the bipartite graph.
As a corollary, we obtain the same privacy guarantees in the symmetric case as the k-cliques case.
Corollary 4.3 Assume bipartite graph G(A, B) is symmetric with respect to its canonical matchings M . Assume that the adversary knows in advance c edges of the matching. The adversary selects
uniformly at random a vertex the privacy of which he wants to break from the set of vertices he does
not know in advance. Then he succeeds with probability at most ?i1?c , where ?i is a degree of a
vertex of the matching graph associated with the user.
5
A symmetric graph G(A, B) may not remain symmetric according to definition 2.2 if nodes in B are
shuffled by a permutation M . However, it will still be symmetric with respect to M according to definition 4.1.

7

In summary, the symmetric case is as resilient to sustained attack as the cliques-bipartite case, the
usual one underlying k-anonymity if we set ?i = ? = k everywhere. The adversary succeeds with
probability at most 1/(?i ?c). However, the asymmetric case is potentially weaker and the adversary
2
can succeed with probability at most 34c
? 2 ? . Interestingly, in the symmetric case with variable ?i
degrees, however, we can provide guarantees that are just as good without forcing all individuals to
agree on a common level of anonymity.
1

1

1

0.8

0.8

0.95

0.9

0.9

0.6

utility

0.7

utility

utility

utility

0.8
0.6

0.6

0.4
0

5

10
anonymity

15

0.2
0

20

5

10
anonymity

15

0.2
0

20

1

1

1

0.95

0.8

0.9

0.85
0.8
0.75
0.7
0

b?matching
b?symmetric
k?anonymity
5

10
anonymity

15

20

0
0

5

10
anonymity

0.8
0.75

15

0.7
5

20

b?matching
b?symmetric
k?anonymity
10

15
20
anonymity

25

30

25

30

0.95
0.9

0.4
0.2

b?matching
b?symmetric
k?anonymity

0.8

0.6

utility

utility

0.9
utility

0.4

b?matching
b?symmetric
k?anonymity

utility

0.5

0.4

b?matching
b?symmetric
k?anonymity

0.85

0.7

0.85

0.6
b?matching
b?symmetric
k?anonymity
5

10
anonymity

0.5
15

20

0.4
0

(a)

0.8

b?matching
b?symmetric
k?anonymity
5

10
anonymity

15

20

0.75
5

b?matching
b?symmetric
k?anonymity
10

15
20
anonymity

(b)

Figure 3: Utility (1 ? #(?)
nd ) versus anonymity on (a) Bupa (n = 344, d = 7), Wine (n = 178, d =
14), Heart (n = 186, d = 23), Ecoli (n = 336, d = 8), and Hepatitis (n = 154, d = 20) and Forest
Fires (n = 517, d = 44) data-sets and (b) CalTech University Facebook (n = 768, d = 101) and
Reed University Facebook (n = 962, d = 101) data-sets.

5 Experiments
We compared algorithms 1 and 2 against an agglomerative clustering competitor (optimized to minimize stars) which is known to outperform the forest method [10]. Agglomerative clustering starts
with singleton clusters and keeps unifying the two closest clusters with smallest increase in stars until
clusters grow to a size at least k. Both algorithms release data with suppressions to achieve a desired constant anonymity level ?. For our algorithms, we swept values of ? in {2?1 , 2?2 , . . . , 2?10 }
from largest to smallest and chose the solution that produced the least number of stars. Furthermore, we warm-started the symmetric algorithm with the star-pattern solution of the asymmetric
algorithm to make it converge more quickly. We first explored six standard data-sets from UCI
http://archive.ics.uci.edu/ml/ in the uniform anonymity setting. Figure 3(a) summarizes the results where utility is plotted against ?. Fewer stars imply greater utility and larger ? implies higher
anonymity. We discretized each numerical dimension in a data-set into a binary attribute by finding
all elements above and below the median and mapped categorical values in the data-sets into a binary
code (potentially increasing the dimensionality). Algorithms 1 achieved significantly better utility
for any given fixed constant anonymity level ? while algorithm 2 achieved a slight improvement.
We next explored Facebook social data experiments where each user has a different level of desired
anonymity and has 7 discrete profile attributes which were binarized into d = 101 dimensions. We
used the number of friends fi a user has to compute their desired anonymity level (which decreases
as the number of friends increases). We set F = maxi=1,...n -log fi . and, for each value of ? in the
plot, we set user i?s privacy level to ?i = ? ? (F ? -log fi .). Figure 3(b) summarizes the results
where utility is plotted against ?. Since the k-anonymity agglomerative clustering method requires
a constant ? for all users, we set k = maxi ?i in order to have a privacy guarantee. Algorithms 1
and 2 consistently achieved significantly better utility in the adaptive anonymity setting while also
achieving the desired level of privacy protection.

6 Discussion
We described the adaptive anonymity problem where data is obfuscated to respect each individual
user?s privacy settings. We proposed a relaxation of k-anonymity which is straightforward to implement algorithmically. It yields similar privacy protection while offering greater utility and the ability
to handle heterogeneous anonymity levels for each user.
8

References
[1] G. Aggarwal, T. Feder, K. Kenthapadi, R. Motwani, R. Panigrahy, D. Thomas, and A. Zhu.
Approximation algorithms for k-anonymity. Journal of Privacy Technology, 2005.
[2] M. Allman and V. Paxson. Issues and etiquette concerning use of shared measurement data. In
Proceedings of the 7th ACM SIGCOMM conference on Internet measurement, 2007.
[3] M. Bugliesi, B. Preneel, V. Sassone, I Wegener, and C. Dwork. Lecture Notes in Computer Science - Automata, Languages and Programming, chapter Differential Privacy. Springer Berlin
/ Heidelberg, 2006.
[4] K. Chaudhuri, C. Monteleone, and A.D. Sarwate. Differentially private empirical risk minimization. Journal of Machine Learning Research, (12):1069?1109, 2011.
[5] G. Cormode, D. Srivastava, S. Bhagat, and B. Krishnamurthy. Class-based graph anonymization for social network data. In PVLDB, volume 2, pages 766?777, 2009.
[6] G. Cormode, D. Srivastava, T. Yu, and Q. Zhang. Anonymizing bipartite graph data using safe
groupings. VLDB J., 19(1):115?139, 2010.
[7] R. Duan and S. Pettie. Approximating maximum weight matching in near-linear time. In
Proceedings 51st Symposium on Foundations of Computer Science, 2010.
[8] J. Edmonds. Paths, trees and flowers. Canadian Journal of Mathematics, 17, 1965.
[9] H.N. Gabow. An efficient reduction technique for degree-constrained subgraph and bidirected
network flow problems. In Proceedings of the fifteenth annual ACM symposium on Theory of
computing, 1983.
[10] A. Gionis, A. Mazza, and T. Tassa. k-anonymization revisited. In ICDE, 2008.
[11] B. Huang and T. Jebara. Fast b-matching via sufficient selection belief propagation. In Artificial
Intelligence and Statistics, 2011.
[12] M.I. Jordan, Z. Ghahramani, T. Jaakkola, and L.K. Saul. An introduction to variational methods for graphical models. Machine Learning, 37(2):183?233, 1999.
[13] V.N. Kolmogorov. Blossom V: A new implementation of a minimum cost perfect matching
algorithm. Mathematical Programming Computation, 1(1):43?67, 2009.
[14] N. Li, T. Li, and S. Venkatasubramanian. t-closeness: Privacy beyond k-anonymity and ldiversity. In ICDE, 2007.
[15] S. Lodha and D. Thomas. Probabilistic anonymity. In PinKDD, 2007.
[16] A. Machanavajjhala, D. Kifer, J. Gehrke, and M. Venkitasubramaniam. L-diversity: Privacy
beyond k-anonymity. ACM Transactions on Knowledge Discovery from Data (TKDD), 1, 2007.
[17] A. Meyerson and R. Williams. On the complexity of optimal k-anonymity. In PODS, 2004.
[18] P. Samarati and L. Sweeney. Generalizing data to provide anonymity when disclosing information. In PODS, 1998.
[19] L. Sweeney. Achieving k-anonymity privacy protection using generalization and suppression.
International Journal on Uncertainty, Fuzziness and Knowledge-based Systems, 10(5):571?
588, 2002.
[20] Y. Tao and X. Xiao. Personalized privacy preservation. In SIGMOD Conference, 2006.
[21] Y. Tao and X. Xiao. Personalized privacy preservation. In Privacy-Preserving Data Mining,
2008.
[22] O. Williams and F. McSherry. Probabilistic inference and differential privacy. In NIPS, 2010.
[23] M. Xue, P. Karras, C. Rassi, J. Vaidya, and K.-L. Tan. Anonymizing set-valued data by nonreciprocal recoding. In KDD, 2012.
[24] E. Zheleva and L. Getoor. Preserving the privacy of sensitive relationships in graph data. In
KDD, 2007.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 2273-graph-driven-feature-extraction-from-microarray-data-using-diffusion-kernels-and-kernel-cca.pdf

Graph-Driven Features Extraction from
Microarray Data using Diffusion Kernels and
Kernel CCA
Jean-Philippe Vert
Ecole des Mines de Paris
Jean-Philippe.Vert@mines.org

Minoru Kanehisa
Bioinformatics Center, Kyoto University
kanehisa@kuicr.kyoto-u.ac.jp

Abstract
We present an algorithm to extract features from high-dimensional gene
expression profiles, based on the knowledge of a graph which links together genes known to participate to successive reactions in metabolic
pathways. Motivated by the intuition that biologically relevant features
are likely to exhibit smoothness with respect to the graph topology, the
algorithm involves encoding the graph and the set of expression profiles into kernel functions, and performing a generalized form of canonical correlation analysis in the corresponding reproducible kernel Hilbert
spaces.
Function prediction experiments for the genes of the yeast S. Cerevisiae
validate this approach by showing a consistent increase in performance
when a state-of-the-art classifier uses the vector of features instead of the
original expression profile to predict the functional class of a gene.

1 Introduction
Microarray technology (DNA chips) is quickly becoming a major data provider in the postgenomics era, enabling the monitoring of the quantity of messenger RNA present in a cell
for several thousands genes simultaneously. By submitting cells to various experimental
conditions and comparing the expression profiles of different genes, a better understanding of the regulation mechanisms and functions of each gene is expected. As a matter of
fact, early experiments confirmed that many genes with similar function yield similar expression patterns [4], and systematic use of state-of-the-art machine learning classification
algorithms highlighted the possibility of gene function prediction from microarray data, at
least for some functional categories [2].
Independently of microarray technology, decades of research in molecular biology have
characterized the roles played by many genes as catalyzing chemical reactions in the cell.
This information has now been integrated into databases such as KEGG [8], where series
of successive chemical reactions arranged into pathways are represented, together with the
genes catalyzing them. In particular one can extract from such a database a graph of genes,
where two genes are linked whenever they catalyze two successive reactions.
The question motivating this report is whether the knowledge of this graph can help improve the performance of gene function prediction algorithms based on microarray data

only. To this end we propose a graph-driven feature extraction process, based on the idea
that expression patterns which correspond to actual biological events, such as the activation
or inhibition of a particular pathway, are more likely to be shared by genes close to each
other in the graph than non-relevant patterns. Our approach consists in translating this intuition as a regularized version of canonical component analysis between the genes mapped
to two reproducible kernel Hilbert spaces, defined respectively by a diffusion kernel [9]
on the graph and a linear kernel on the expression profiles. This formulation leads to a
well-posed problem equivalent to a generalized eigenvector problem [1].

2 Problem formulation
The set of genes is represented by 
	
a discrete
set
of cardinality   . The set of

 , where
 is the number of measurements and
expression profiles is a mapping 
 is the expression profile of gene  . In the sequel we assume that the set of profiles has
been centered, i.e.,  "! .
The
of genes extracted from the pathway database is represented by a simple graph
# $graph
 &%('  , with the genes as vertices. Our goal is use this graph to extract features
from the expression profiles. To this
end
formally define a feature
+	,
 ,we
  tothebeseta real-valued
mapping on the set of genes )*
and we denote by -.
of possible
features. The set of centered features is denoted by -0/132)546-78"96): !; .

In particular
features extracted from expression profiles )8<>= ? are defined, for any
@ 4 A
 , by ) <>linear
= ? @ 0 @8B  , for any C4 @ B (here and often in the sequel we use matrix
notations, where is a column vector and its transpose). We call DFE*- / the set of linear
features. The normalized variance of a linear feature is defined by:

G ) <>= ? 4HD %JI K) <>= ?   &) <>= ? ML
N @ O L
P

(1)

It is a first indicator of the possible relevance of a linear vector. Indeed biological events
such as the synthesis of new molecules usually require the coordinated actions of many
proteins: they are therefore likely to have characteristic patterns in terms of gene expression
which capture variation between the genes involved and the others, and should therefore
have large variance. Linear features with a large normalized variance (1) are called relevant
in the sequel, as opposed to irrelevant features. Relevant features can be extracted by PCA.
While the normalized
# variance (1) is an intrinsic property of the set of profiles, the knowledge of the graph suggests another criterion to judge ?good? features. As genes linked
together in the graph are supposed to participate in successive reactions in the cell, it is
likely that the activation/inhibition of a biochemical pathway has a characteristic expression pattern shared by clusters of genes in the graph. More globally, the graph defines a
structure on the set of genes, and therefore a notion of smoothness for any feature )Q45- .
A feature is called smooth if it varies slowly between adjacent nodes in the graph, and
rugged otherwise. As just stated, features of interest are more likely to be smooth than
other features.
We therefore end up with two criteria for extracting ?good? features: they should simultaneously be relevant and smooth, the latter being defined with respect
% to the gene graph.
One way to extract such features is to look for pairs of features, K)R ) S4T-VUWD , such
L
that )R be smooth, ) be a relevant linear feature, and the correlation between )R and )
L
L
be as large as possible. The decoupling of the two criteria enables us to state the problem
mathematically as follows.

	[]\

for any feature, and a
Suppose we can define a smoothness
XYRZ0	^:\ forfunctional
relevance functional X D
linear features, in such a way that lower values of

L

the functional XR (resp. X ) correspond to smoother (resp. more relevant) features. Then
L
the following optimization problem:

%
) R B ) L




	 = 
	 
 ) R B ) R X R K) R  ) B )  X  ) 
L L
L L




(2)

where
! is a regularization parameter, is a way to extract smooth and relevant features.
Irrelevance and ruggedness penalize any candidate pair through the functionals XR and
X L , and controls the trade-off between relevance and smoothness on the one hand, and
correlation on the other hand. ! amounts to finding )R and ) as correlated as possible
(which is obtained by taking )8RS ) ), while
! forces )R toL be relevant and ) L to be
L
smooth.





In order to turn (2) into an algorithm we remark that if X R and X can be expressed as norms
L
in reproducible kernel Hilbert spaces (RKHS, see Section 3), then (2) takes the form of a
generalization of canonical correlation analysis (CCA) known as kernel-CCA [1], which is
equivalent to a generalized eigenvector problem. Let us therefore show how to build two
RKHS on the set of genes whose norms are smoothness (Section 4) and relevance (Section
5) functionals, respectively.

3 Reproducible kernel Hilbert spaces and smoothness functionals
Let us briefly review basic properties of RKHS relevant for the sequel. The reader is referred to [12, 14] for more details.



	 

   %  =%    = %$  
 be
"#T P  F4 , and

 L
Let
be a Mercer kernel in the sense that the matrix
E - be the linear span of
symmetric positive semidefinite. Let
consider a decomposition of as:



!

(-,(., (
'( &
B%
V
(3)
,
,
) +R *
% % &  4 - & is an
& are the eigenvalues
where !/
of( ,  ( and the set  R
R0/
/
(
P P P basis
P P P of any )W42!
*
* of eigenvectors
associated orthonormal
 . The decomposition
& in 1L , where
on this basis can be expressed as )5  )43 \ R65
7 is the multiplicity of ! as an
eigenvalue. An inner product can be defined in ! as follows: ( (
8 ( ' & ( , ( ( ' & ( , (;:=< ( ' &
%
5 (
(4)

5
)43 \ R
)43 \ R9
)>3 \ R * 9 P
The resulting Hilbert space ! is called a reproducing< kernel Hilbert space, due to the
following reproducing property:
G  %  B 4 L %@? C %  % C %  B BA CC %  B 
(5)
P
P
P
The inner product in ! can be easily expressed
in a dual form as follows. Each ) 4D! can

 %  , where E is unique up to the addition of an
be decomposed as ):    9%E C
P of  and is calledP the dual coordinate of ) . In a matrix form,
element of the null space
< (5) one can easily check that the inner product between two
this reads )H
%H FGE , and using
features  )
 4I6
! L with dual coordinates E %BJ  4 - L respectively is given by:
? ) %H A  '
J LK BC
% K  CE B  J
E









(6)


  =   <
P
In particular the ! -norm of a feature )54I! with dual coordinates F
E 4&- is given by:
%
B
O ) O L ME GE
(7)

%BH %!

and the inner product between two features K)
 4 &L with dual coordinates 
in the original space L9  can also be expressed in dual form:

1



'
) BH 
): H   E B  L J
P




E 	% J  4&- L
(8)

<

When
is a subspace of
then it is known that the norm in the RKHS defined by
several popular kernels such as the Gaussian radial basis kernel are smoothing functionals,
in the sense that larger values of N ) N correspond to functions ) with more energy at
high frequency in their Fourier decomposition. This fact has been much exploited e.g. in
regularization theory [14, 5], and we now adapt it to the discrete setting.

4 Smoothness functional on a graph
A natural way to quantify the smoothness of a feature on a graph is by its energy at high
frequency, as computed from its Fourier transform. Fourier transforms on graphs is a classical tool of spectral graph analysis
[3, 11] which we briefly recall now. Let be the 6U 
#
adjacency matrix of the graph (  = 
if there is an edge between  and , ! otherwise)
and the diagonal
matrix of vertex degrees. Then the U  matrix C
is called the
#
Laplacian of , and is known to share many properties with the continuous
Laplacian
[11].
% %  belongs
It is symmetric, semidefinite positive, and singular. The eigenvector 
to
P P P components
the# eigenvalue R ! , whose multiplicity is equal to the number of connected
of .







K
 
	 
,(
*
%
  % %  $ an
Let us denote by ,!C
(  * R / P P P / * & the eigenvalues of 1 and "
P and
orthonormal set of associated eigenvectors. This basis is a discrete

 Fourier basisP P [3],
it is known that
oscillates more and more on the graph as increases. The Fourier
decomposition of any feature )546- is the expansion
( ,( in terms of this basis:
'( & 
 %
) 
)
(9)
( ,(
)
R

 B ) and )6
   )
 R % % ) 
 & is called the discrete Fourier transform of ) .
where ) 
PPP
 \ 	 :\ " ! $ , let us now consider the funcFor any monotonic
decreasing
mapping  
	

tion 
  L defined by:
( ,( ,(
'( &
G  % K  4 L %
%
(10)
  K  
    LK  P
)R *
The mapping  being assumed to take only positive values, the matrix  is definite positive and is therefore a Mercer kernel on the set . The corresponding RKHS is the set of
(
features - , with norm given by:
&'( 
 (
G ) 46- % O ) N L 
)L
(11)


)
R * P
(
(


As increases,
increases so    decreases. As a result the norm (11) has a higher
*
*



1

value on features which have a lot of energy at high frequency, and is therefore a natural
smoothing functional.







 

An example of valid function with rapid decay is the exponential  7
, where
is a parameter. In that case we recover the diffusion kernel introduced and discussed in
[9]. Considering other mapping would be beyond the scope of this report, so we restrict
ourselves to this diffusion kernel in the sequel. Observe that it can be expressed using the
matrix exponential as 1

.





  	  1

5 Relevance functional

A

$

%

If @ 4
has a projection @ / onto the linear span of 8 "4
then )<>= ?  )9<>= ? .
As a result the set of linear features D can be parametrized by directions of the form @ 
 9 M , where 4W- is called the dual coordinate of @ and is defined up to the
addition of an element of the null space of the Gram matrix  =   B   . The RKHS
E - associated with this semidefinite
positive matrix consists of the set of features of
 C % 0 ) ? = < , where @   9 (8 . In other words
the form ):    
P the set of linear features,
P
this is exactly
 D .

J

"

J

!



J 



LK

!

J

D can be expressed by (1), (6) and (8) <as follows:
I K)9< = ?9  9 ) <>= ?  L  J J B  L J J  N ) <>= ? O 
   
@
B
N ) <>= ? O P
< N N L

The variance of a feature )54


 

As a result, a natural relevance functional to balance the term N ) N  in (2) is the norm
in the% RKHS: X K) < = ?   N ) <>= ? O , where is the RKHS associated with the linear kernel
C  " L B   .



K

!

LK

6 Extracting smooth correlations

<
 % R    B  1  denote the diffusion kernel and  L denote the linear kernel
Taking XR9K) 
 L   K  7 8K  , with associated RKHS !&R and ! L respectively.
N ) N as a smoothness function for any ) 4&- , and X K)   N ) N 
 as a relevance func-

Let

<

L

tional for any linear feature )54&D , we can express the maximization Problem (2) in a dual
form as:

	% J
 =   
 
  E  


E B  
 R 
E B . R L 
   R E]  J







L

J


B .  L    J  
 P
L
L

(12)

%

<

At first sight it seems that (12) is the dual formulation of an optimization over  ) R ) S4
L
R U L  -
U D , and not - / U6D as in (2). However it can be checked that any solution
of (12) is in fact in - / UZD . Indeed the numerator remains unchanged when a constant
function is added to )R 
R 4 - , while both N )RO  and N )R9N are minimized
when ) has mean ! (for the latter case, this results from the fact that the constant vector is
an eigenvector of the diffusion kernel, so the norm defined by (4) is minimized when the
corresponding projection of ) , namely its average, is null).

! !


 

 E



Formulated as (12) the problem appears to be a generalization of canonical correlation
analysis (CCA)
as kernel-CCA, discussed in [1]. In particular Bach and Jordan
%  known
show that 
is a solution of (12) if and only if it satisfies the following generalized
eigenvalue problem:

E 	J

 R
EJ 
5RL   R
!
EJ
(13)
 L 6R ! L
!
5L L   L
( (
( (
with %BJ the% 
 largest
possible.
Moreover, solving
(13) provides a series of pairs of features
%
%
$
%
"8 E    P P P  , where  
  , with decreasing values of E ( %BJ  for(
= is null, equivalent to the extraction of successive canonical diwhich the
( gradient
(
rections with decreasing
correlation in classical CCA. The resulting features ) R =   R E
J are therefore
and ) =   

a set of features likely to have decreasing biological releL
L




!





	











	












 

vance when increases, and are the features we propose to extract in this report.



As discussed in [1] we regularize the problem (13) by adding L on the diagonal of the
matrix on the right-side, to be able to perform the Cholesky decomposition necessary to

solve this problem. Hence we end up with the following problem:



B

 R
EJ 
  R  L
! 
EJ % (14)
B
 L R ! L
L
!
. L  
%
B
J
B
where     . If E
eigenvector solution
of (14) belonging to the
 is an generalized
(
%BJ  belong
generalized eigenvalue
, % then% 	=E
to  . As a result the
spectrum of (14) is
%
%
&  &  with R  & , ! for 
   .
symmetric :  R  R
!





	









	

	

	

PPP

	

	

	

PPP

	

	

	

7 Experiments
We extracted from the LIGAND database of chemical compounds of reactions in biological
pathways [6] a graph made of 774 genes of the budding yeast S. Cerevisiae, linked through
16,650 edges, where two genes are linked when they have the possibility to catalyze two
successive reactions in the LIGAND database (i.e, two reactions such that the main product
of the first one be the main substrate of the second one). Expression data were collected
from the Stanford Microarray Database [13]. Concatenating several publicly available data,
we ended up with 330 measurements for 6075 genes of the yeast, i.e., almost all its known
or predicted genes. Following [4, 2] we work with the normalized logarithm of the ratio
of expression levels of the genes between two experimental conditions. The functional
classes of the yeast genes we consider are the one defined by the January 10, 2002 version
of the Comprehensive Yeast Genome Database (CYGD) [10], which is a comprehensive
classification of 3,936 genes into 259 categories.
The 669 genes in the gene graph with known expression profiles were first used to perform
the feature extraction process described in this report. The resulting linear features were
then extracted from the expression profiles of the disjoint set of 2,688 genes which are
in the CYGD functional catalogue but not in the pathway database. We then performed
functional classification experiments on this set of 2,688 genes, using either the profiles
themselves or the features extracted. All functional classes with more than 20 members in
this set were tested (which amount to 115 categories).
Experiments were carried out with SVM Light [7], a public and free implementation of
SVM. All vectors were scaled to unit length before
sent to the SVM, and all SVM
% being
use a radial basis kernel with unit width, i.e.,  

 SN  YN L  . The trade-off
parameter between training error and margin error was set to its default value ( in that
case), and the cost of errors on positive and and negative examples were adjusted to have
the same total.

  	

K



K

 





Preliminary experiments to tune the two parameters of the algorithm, namely the width of
the diffusion kernel and the regularization parameter , showed that 
and "! !!
P
provide good performances. For these values we first tested whether there exists an optimal
number of features to be extracted for optimal gene function prediction. Figure 1 shows the
performance of SVM using different numbers of features, in terms of ROC index averaged
over all 115 classes. The ROC index is the area under the curve of false negative vs true
positive, normalized to !! for a perfect classifier and 9 ! for a random classifier. For each
category the ROC index was averaged over ! random splitting of the data into training and
 ! 9
 ! . It appears that the more features are included, the better the
test set, in the proportion 
performance averaged over all categories. A more precise analysis of the different classes
shows however that some classes don?t follow the average trend and are better predicted
by a smaller number of features, as shown on Figure 2 for  categories best predicted by
less than !! features. Finally Figure 3 compares, for each of the 115 categories, the ROC
index for a SVM using the original expression profiles with a SVM using the vectors of
330 features. It demonstrates that the representation of genes as vectors of features helps
improve the performance of SVM (the ROC index averaged over all categories increases











62

61

Average ROC index

60

59

58

57

56

55
50

100

150

200
Number of features

250

300

350

Figure 1: ROC index averaged over 115 categories, for various number of features
Prediction performance for several functional classes
75
"fermentation"
"ionic_homeostasis"
"protein_complexes"
"vacuolar_transport"
"nucleus_organization"

70

ROC index

65

60

55

50

45
50

100

150

200
Number of features

250

300

350

Figure 2: ROC index for 5 functional categories, for various number of features



 ). The difference is especially important for classes such as heavy metal
to
from  
P
P (  vs    ), ribosome biogenesis (  vs  !  ), protein synthesis ( 
ion transporters
%
P
P
P
P
vs
) or morphogenesis
(P   vs    )

P

P

8 Discussion and Conclusion
Results reported in the previous section are encouraging for at least two reasons. First of
all, the performance reached for some classes such as heavy ion metal transporters shows
that a ROC above 80% can be expected for several classes. Second, while many classes are
apparently not learned by the SVM based on expression profiles (ROC around 50), the ROC
based on extracted features of the same classes is around 60. This shows that there is hope
to be able to predict more functional classes than previously thought [2] from microarray
data, which is a good news since the amount of microarray data is expected to explode in
the coming years.
The method presented in this paper can be seen as an attempt to explore the possibilities of
data mining and analysis provided by kernel methods. Few studies have used kernel methods other than SVM, and have used kernels other than Gaussian or polynomial kernels. In
this report we tried to show how ?exotic? kernels such as the diffusion kernel, and ?exotic?
methods such as kernel-CCA, can be adapted to particular problems, graph-driven feature
extraction in our case. Exploring other possibilities of kernel methods in the data-rich field
of computational biology is among our future plans.

100

ROC index based on expression profiles

90

80

70

60

50

40

30
40

50

60
70
80
ROC index based on extracted features

90

100

Figure 3: ROC index of a SVM classifier based on expression profiles (y axis) or extracted
features (x axis). Each point represents one functional category.

References
[1] F. R. Bach and M. I. Jordan. Kernel independent component analysis. Journal of Machine
Learning Research, 3:1?48, 2002.
[2] Michael P. S. Brown, William Noble Grundy, David Lin, Nello Cristianini, Charles Walsh Sugnet, Terence S. Furey, Jr. Manuel Ares, and David Haussler. Knowledge-based analysis of
microarray gene expression data by using support vector machines. Proc. Natl. Acad. Sci. USA,
97:262?267, 2000.
[3] Fan R.K. Chung. Spectral graph theory, volume 92 of CBMS Regional Conference Series.
American Mathematical Society, Providence, 1997.
[4] Michael B. Eisen, Paul T. Spellman, Patrick O. Brown, and David Botstein. Cluster analysis
and display of genome-wide expression patterns. Proc. Natl. Acad. Sci. USA, 95:14863?14868,
Dec 1998.
[5] Frederico Girosi, Michael Jones, and Tomaso Poggio. Regularization theory and neural networks architectures. Neural Computation, 7(2):219?269, 1995.
[6] S. Goto, Y. Okuno, M. Hattori, T. Nishioka, and M. Kanehisa. LIGAND: database of chemical
compounds and reactions in biological pathways. Nucleic Acid Research, 30:402?404, 2002.
[7] Thorsten Joachims. Making large-scale svm learning practical. In B. Sch?olkopf, C. Burges,
and A. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 169?184.
MIT Press, 1999.
[8] M. Kanehisa, S. Goto, S. Kawashima, and A. Nakaya. The KEGG databases at GenomeNet.
Nucleic Acid Research, 30:42?46, 2002.
[9] R. I. Kondor and J. Lafferty. Diffusion kernels on graphs and other discrete input. In ICML
2002, 2002.
[10] H.W. Mewes, D. Frishman, U. G?uldener, G. Mannhaupt, K. Mayer, M. Mokrejs, B. Morgenstern, M. M?unsterkoetter, S. Rudd, and B. Weil. MIPS: a database for genomes and protein
sequences. Nucleic Acid Research, 30(1):31?34, 2002.
[11] B. Mohar. Some applications of laplace eigenvalues of graphs. In G. Hahn and G. Sabidussi,
editors, Graph Symmetry: Algebraic Methods and Applications, volume 497 of NATO ASI
Series C, pages 227?275. Kluwer, Dordrecht, 1997.
[12] S. Saitoh. Theory of reproducing Kernels and its applications. Longman Scientific & Technical,
Harlow, UK, 1988.
[13] G. Sherlock, T. Hernandez-Boussard, A. Kasarskis, G. Binkley, J.C. Matese, S.S. Dwight,
M. Kaloper, S. Weng, H. Jin, C.A. Ball, M.B. Eisen, and P.T. Spellman. The stanford microarray database. Nucleic Acid Research, 29(1):152?155, Jan 2001.
[14] G. Wahba. Spline Models for Observational Data, volume 59 of CBMS-NSF Regional Conference Series in Applied Mathematics. SIAM, Philadelphia, 1990.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 5279-improved-multimodal-deep-learning-with-variation-of-information.pdf

Improved Multimodal Deep Learning
with Variation of Information
Kihyuk Sohn, Wenling Shang and Honglak Lee
University of Michigan Ann Arbor, MI, USA
{kihyuks,shangw,honglak}@umich.edu

Abstract
Deep learning has been successfully applied to multimodal representation learning problems, with a common strategy to learning joint representations that are
shared across multiple modalities on top of layers of modality-specific networks.
Nonetheless, there still remains a question how to learn a good association between data modalities; in particular, a good generative model of multimodal data
should be able to reason about missing data modality given the rest of data modalities. In this paper, we propose a novel multimodal representation learning framework that explicitly aims this goal. Rather than learning with maximum likelihood, we train the model to minimize the variation of information. We provide a
theoretical insight why the proposed learning objective is sufficient to estimate the
data-generating joint distribution of multimodal data. We apply our method to restricted Boltzmann machines and introduce learning methods based on contrastive
divergence and multi-prediction training. In addition, we extend to deep networks
with recurrent encoding structure to finetune the whole network. In experiments,
we demonstrate the state-of-the-art visual recognition performance on MIR-Flickr
database and PASCAL VOC 2007 database with and without text features.

1

Introduction

Different types of multiple data modalities can be used to describe the same event. For example,
images, which are often represented with pixels or image descriptors, can also be described with
accompanying text (e.g., user tags or subtitles) or audio data (e.g., human voice or natural sound).
There have been several applications of multimodal learning from multiple domains such as emotion [13] and speech [10] recognition with audio-visual data, robotics applications with visual and
depth data [15, 17, 32, 23], or medical applications with visual and temporal data [26]. These data
from multiple sources are semantically correlated, and sometimes provide complementary information to each other. In order to exchange such information, it is important to capture a high-level
association between data modalities with a compact set of latent variables. However, learning associations between multiple heterogeneous data distributions is a challenging problem.
A naive approach is to concatenate the data descriptors from different sources of input to construct a
single high-dimensional feature vector and use it to solve a unimodal representation learning problem. Unfortunately, this approach has been unsuccessful since the correlation between features in
each data modality is much stronger than that between data modalities [21]. As a result, the learning
algorithms are easily tempted to learn dominant patterns in each data modality separately while giving up learning patterns that occur simultaneously in multiple data modalities. To resolve this issue,
deep learning methods, such as deep autoencoders [9] or deep Boltzmann machines (DBM) [24],
have been used to this problem [21, 27], with a common strategy to learning joint representations that
are shared across multiple modalities at the higher layer of the deep network after learning layers of
modality-specific networks. The rationale is that the learned features may have less within-modality
correlation than raw features, and this makes it easier to capture patterns across data modalities. Despite the promise, there still remains a challenging question how to learn a good association between
multiple data modalities that can effectively deal with missing data modalities in the testing time.
One necessary condition of being a good generative model of multimodal data is to have an ability
to predict or reason about missing data modalities given partial observation. To this end, we propose
1

a novel multimodal representation learning framework that explicitly aims this goal. The key idea
is to minimize the information distance between data modalities through the shared latent representations. More concretely, we train the model to minimize the variation of information (VI), an
information theoretic measure that computes the distance between random variables, i.e., multiple
data modalities. Note that this is in contrast to the previous approaches on multimodal deep learning, which are based on maximum (joint) likelihood (ML) learning [21, 27]. We provide an intuition
how our method could be more effective in learning the joint representation of multimodal data than
ML learning, and show theoretical insights why the proposed learning objective is sufficient to estimate the data-generating joint distribution of multimodal data. We apply the proposed framework to
multimodal restricted Boltzmann machine (MRBM). We introduce two learning algorithms, based
on contrastive divergence [19] and multi-prediction training [6]. Finally, we extend to multimodal
deep recurrent neural network (MDRNN) for unsupervised finetuning of whole network. In experiments, we demonstrate the state-of-the-art visual recognition performance on MIR-Flickr database
and PASCAL VOC 2007 database with and without text features.

2

Multimodal Learning with Variation of Information

In this section, we propose a novel training objective based on the VI. We make a comparison to
the ML objective, a typical learning objective for training models of multimodal data, to give an
insight how our proposal outperforms the baseline. Finally, we establish a theorem showing that the
proposed learning objective is sufficient to obtain a good generative model that fully recovers the
joint data-generating distribution of multimodal data.
Notation. We use uppercase letters X, Y to denote random variables, lowercase letters x, y for
realizations. Let PD be the data-generating distribution and P? the model distribution parameterized by ?. For presentation clarity, we slightly abuse the notation for Q to denote conditional
(Q(x|y), Q(y|x)), marginal (Q(x), Q(y)), as well as joint distributions (Q(x, y)) that are derived
from the joint distribution Q(x, y). The type of distribution for Q should be clear from the context.
2.1

Minimum Variation of Information Learning

Motivated from the necessary condition of good generative models to reason about the missing data
modality, it seems natural to learn to maximize the amount of information that one data modality has
about the others. We quantify such an amount of information between data modalities using variation
of information (VI). The VI is an information theoretic measure that computes the information
distance between two random variables (e.g., data modalities), and is written as follows:1


(1)
VIQ (X, Y ) = ?EQ(X,Y ) log Q(X|Y ) + log Q(Y |X)
where Q(X, Y ) = P? (X, Y ) is any joint distribution on random variables (X, Y ) parametrized by
?. Informally, VI is small when the conditional LLs Q(X|Y ) and Q(Y |X) are ?peaked?, meaning
that X has low entropy conditioned on Y and vice versa. Following the intuition, we define new
multimodal learning criteria, a minimum variation of information (MinVI) learning, as follows:


(2)
MinVI: min? LVI (?), LVI (?) = ?EPD (X,Y ) log P? (X|Y ) + log P? (Y |X)
Note the difference in LVI (?) that we take the expectation over PD in LVI (?). Furthermore, we
observe that the MinVI objective can be decomposed into a sum of two negative conditional LLs.
This indeed well aligns with our initial motivation about reasoning missing data modality. In the
following, we provide a more insight of our MinVI objective in relation to the ML objective, which
is a standard learning objective in generative models.
2.2 Relation to Maximum Likelihood Learning
The ML objective function can be written as a minimization of the negative LL (NLL) as follows:


ML: min? LNLL (?), LNLL (?) = ?EPD (X,Y ) log P? (X, Y ) ,
(3)
and we can show that the NLL objective function is reformulated as follows:
2LNLL (?) = KL (PD (X)kP? (X)) + KL (PD (Y )kP? (Y )) +
|
{z
}
(a)





EPD (X) KL (PD (Y |X)kP? (Y |X)) + EPD (Y ) KL (PD (X|Y )kP? (X|Y )) + C,
|
{z
}

(4)

(b)

1
In practice, we use finite samples of the training data and use a regularizer (e.g., l2 regularizer) to avoid
overfitting to the finite sample distribution.

2

where C is a constant which is irrelevant to ?. Note that (b) is equivalent to LVI (?) in Equation (2)
up to a constant. We provide a full derivation of Equation (4) in supplementary material.
Ignoring the constant, the NLL objective is composed of four terms of KL divergence. Since KL
divergence is non-negative and is 0 only when two distributions match, the ML learning in Equation (3) can be viewed as a distribution matching problem involving (a) marginal likelihoods and (b)
conditional likelihoods. Here, we argue that (a) is more difficult to optimize than (b) because there
are often too many modes in the marginal distribution. Compared to that, the number of modes can
be dramatically reduced in the conditional distribution since the conditioning variables may restrict
the support of random variable effectively. Therefore, (a) may become a dominant factor to be minimized during the optimization process and as a trade-off, (b) will be easily compromised, which
makes it difficult to learn a good association between data modalities. On the other hand, the MinVI
objective focuses on modelling the conditional distributions (Equation (4)), which is arguably easier to optimize. Indeed, similar argument has been made for generalized denoising autoencoders
(DAEs) [1] and generative stochastic networks (GSNs) [2], which focus on learning the transition
? where X
? is a corrupted version of data X, or P? (X|H), where H can be
operators (e.g., P? (X|X),
arbitrary latent variables) to bypass an intractable problem of learning density model P? (X).
2.3

Theoretical Results

Bengio et al. [1, 2] proved that learning transition operators of DAEs or GSNs is sufficient to learn
a good generative model that estimates a data-generating distribution. Under similar assumptions,
we establish a theoretical result that we can obtain a good density estimator for joint distribution
of multimodal data by learning the transition operators derived from the conditional distributions of
one data modality given the other. In multimodal learning framework, the transition operators TnX
and TnY with model distribution P?n (X, Y ) are defined
P for Markov chains of data modalities X and
Y , respectively. Specifically, TnX (x[t]|x[t ? 1]) = y?Y P?n (x[t]|y) P?n (y|x[t ? 1]) and TnY is
defined in a similar way. Now, we formalize the theorem as follows:
Theorem 2.1 For finite state space X , Y, if, ?x ? X , ?y ? Y, P?n (?|y) and P?n (?|x) converges in
probability to PD (?|y) and PD (?|x), respectively, and TnX and TnY are ergodic Markov chains, then,
as the number of examples n ? ?, the asymptotic distribution ?n (X) and ?n (Y ) converge to datagenerating marginal distributions PD (X) and PD (Y ), respectively. Moreover, the joint probability
distribution P?n (x, y) converges to PD (x, y) in probability.
The proof is provided in supplementary material. The theorem ensures that the MinVI objective
can lead to a good generative model estimating the joint data-generating distribution of multimodal
data. The theorem holds under two assumptions, consistency of density estimators and ergodicity
of transition operators. The ergodicity of transition operators are satisfied for wide variety of neural
networks, such as an RBM or DBM. 2 The consistency assumption is more difficult to satisfy and
the aforementioned deep energy-based models nor RNN may not satisfy the condition due to the
approximated posteriors using factorized distribution. Probably, deep networks that allow exact
posterior inference, such as stochastic feedforward neural networks [20, 29], could be a better model
in our multimodal learning framework, but we leave this as a future work.

3

Application to Multimodal Deep Learning

In this section, we describe the MinVI learning in multimodal deep learning framework. To overview
our pipeline, we use the commonly used network architecture that consists of layers of modalityspecific deep networks followed by a layer of neural network that jointly models the multiple modalities [21, 27]. The network is trained in two steps: In layer-wise pretraining, each layer of modalityspecific deep network is trained using restricted Boltzmann machines (RBMs). For the top-layer
shared network, we train MRBM with MinVI objective (Section 3.2). Then, we finetune the whole
deep network by constructing multimodal deep recurrent neural network (MDRNN) (Section 3.3).
3.1

Restricted Boltzmann Machines for Multimodal Learning

The restricted Boltzmann machine (RBM) is an undirected graphical model that defines the distribution of visible units using hidden units. For multimodal input, we define the joint distribution of
2
For energy-based models like RBM and DBM, it is straightforward to see that every state has non-zero
probability and can be reached from any other state. However, the mixing of the chain might be slow in practice.

3

multimodal RBM (MRBM) [21, 27] as P (x, y, h) =

E(x, y, h) = ?

Nx X
K
X

x
xi Wik
hk ?

i=1 k=1

Ny K
X
X

1
Z


exp ?E(x, y, h) with the energy function:

y
yj Wjk
hk ?

j=1 k=1

K
X

bk hk ?

Nx
X
i=1

k=1

cxi xi ?

Ny
X

cyj yj ,

(5)

j=1

where Z is the normalizing constant, x ? {0, 1}Nx , y ? {0, 1}Ny are the binary visible (i.e.,
observation) variables of multimodal input, and h ? {0, 1}K are the binary hidden (i.e., latent)
variables. W x ? RNx ?K defines the weights between x and h, and W y ? RNy ?K defines the
weights between y and h. cx ? RNx , cy ? RNy , and b ? RK are bias vectors corresponding to x,
y, and h, respectively. Note that the MRBM is equivalent to an RBM whose visible variables are
constructed by concatenating the visible variables of multiple input modalities, i.e., v = [x ; y].
Due to bipartite structure, variables in the same layer are conditionally independent given the variables of the other layer, and the conditional probabilities are written as follows:
X
X y

x
P (hk = 1 | x, y) = ?
Wik
xi +
Wjk yj + bk ,
(6)
i

P (xi = 1 | h) = ?

X

j
x
Wik
hk

X y


+ cxi , P (yj = 1 | h) = ?
Wjk hk + cyj ,

k

(7)

k

1
. Similarly to the standard RBM, the MRBM can be trained to maximize
where ?(x) = 1+exp(?x)
the joint LL (log P (x, y)) using stochastic gradient descent (SGD) while approximating the gradient
with contrastive divergence (CD) [8] or persistent CD (PCD) [30]. In our case, however, we train
the MRBM in MinVI criteria. We will discuss the inference and training algorithms in Section 3.2.

When we have access to all data modalities, we can use Equation (6) for exact posterior inference.
On the other hand, when some of the input modalities are missing, the inference is intractable, and
we resort to the variational method. For example, when we are given
x but no y, the true posterior can
Q Q
be approximated with a fully factorized distribution Q(y, h) = j k Q(yj )Q(hk ) by minimizing

the KL Q(y, h)kP? (y, h|x) . This leads to the following fixed-point equations:
X
X y
X y


x
?k = ?
? k + cy ,
h
Wik
xi +
Wjk y?j + bk , y?j = ?
Wjk h
(8)
j
i

j

k

? k = Q(hk ) and y?j = Q(yj ). The variational inference proceeds by alternately updating the
where h
? and y? that are initialized with all 0?s.
mean-field parameters h
3.2

Training Algorithms

CD-PercLoss. As in Equation (2), the objective function can be decomposed into two conditional
LLs, and the MRBM with MinVI objective can be trained equivalently by training the two conditional RBMs (CRBMs) while sharing the weights. Since the objective functions are the sum of
two conditional LLs, we compute the (approximate) gradient of each CRBM separately using CDPercLoss [19] and accumulate them to update parameters.3
Multi-Prediction. We found a few practical issues of CD-PercLoss training: First, the gradient
estimates are inaccurate. Second, there exists a difference between encoding process of training and
testing, especially when the unimodal query (e.g., one of the data modality is missing) is considered for testing. As an alternative objective, we propose multi-prediction (MP) training of MRBM
in MinVI criteria. The MP training was originally proposed to train deep Boltzmann machines
(DBMs) [6] as an alternative to the stochastic approximation procedure learning [24]. The idea is to
train the model good at predicting any subset of input variables given the rest of them by constructing
the recurrent network with encoding function derived from the variational inference problem.
The MP training can be adapted to train MRBM with MinVI objective with some modifications.
For example, the CRBM with an objective log P (y|x) can be trained by randomly selecting the
subset of variables to be predicted only from the target modality y, but the conditioning modality x
3
In CD-PercLoss learning, we run separate Gibbs chains for different conditioning variables and select the
negative particles with the lowest free energy among sampled particles. We refer [19] for further details.

4

Wx(1)

x=hx(0)

Wx(2)

hx(1)

Wx(3)

hx(2)

Wy(3)

h(3)

Wy(2)

hy(2)

Wy(1)

hy(1)

y=hy(0)

Figure 1: An instance of MDRNN with target y given x. Multiple iterations of bottom-up updates
(y ? h(3) ; Equation (11)) and top-down updates (h(3) ? y; Equation (13)) are performed. The
arrow indicates encoding direction.
is assumed to be given in all cases. Specifically, given an arbitrary subset s ? {1, ? ? ? , Ny } drawn
from the independent Bernoulli distribution PS , the MP algorithm predicts ys = {yj : j ? s} given
x and y\s = {yj : j ?
/ s} through the iterative encoding function derived from fixed-point equations
?k = ?
h

X

x
Wik
xi +

i

X
j?s

y
Wjk
y?j +

X

X y


y
? k + cy , j ? s,
Wjk
yj + bk , y?j = ?
Wjk h
j

j ?s
/

(9)

k


which is a solution to the variationalQ
inference
Q problem minQ KL Q(ys , h)kP? (ys , h|x, y\s ) with
factorized distribution Q(ys , h) = j?s k Q(yj )Q(hk ). Note that Equation (9) is similar to the
Equation (8) except that only yj , j ? s are updated. Using an iterative encoding function, the
network parameters are trained using SGD while computing the gradient by backpropagating the
error between the prediction and the ground truth of ys through the derived recurrent network. The
MP formulation (e.g., encoding function) of the CRBM with log P (x|y) can be derived similarly,
and the gradients are simply the addition of two gradients that are computed individually.
We have two additional hyper parameters, the number of mean-field updates and the sampling ratio
of a subset s to be predicted from the target data modality. In our experiments, it was sufficient to
use 10 ? 20 iterations until convergence. We used the sampling ratio of 1 (i.e., all the variables in
the target data modality are to be predicted) since we are already conditioned on one data modality,
which is sufficient to make a good prediction of variables in the target data modality.
3.3

Finetuning Multimodal Deep Network with Recurrent Neural Network

Motivated from the MP training of MRBM, we propose multimodal deep recurrent neural network
(MDRNN) that tries to predict the target modality given the input modality through the recurrent
encoding function, which iteratively performs a full pass of bottom-up and top-down encoding from
bottom-layer visible variables to top-layer joint representation back to bottom-layer through the
modality-specific deep networks. We show an instance of L = 3 layer MDRNN in Figure 1, and the
encoding functions are written as follows:4


(l)
x,(l)> (l?1)
x,(l)
x ? h(L?1)
:
h
=
?
W
h
+
b
, l = 1, ? ? ? , L ? 1 (10)
x
x
x


(l)
y ? h(L?1)
:
hy
= ? W y,(l)> h(l?1)
+ by,(l) , l = 1, ? ? ? , L ? 1 (11)
y
y


h(L?1)
, h(L?1)
? h(L) : h(L) = ? W x,(L)> h(L?1)
+ W y,(L)> h(L?1)
+ b(L)
(12)
x
y
x
y


(l?1)
y,(l?1)
h(L) ? y : hy
= ? W y,(l) h(l)
, l = L, ? ? ? , 1
(13)
y +b
(0)

(0)

where hx = x and hy = y. The visible variables of the target modality are initialized with 0?s.
In other words, in the initial bottom-up update, we compute h(L) only from x while setting y = 0
using Equation (10),(11),(12). Then, we run multiple iterations of top-down (Equation (13)) and
bottom-up updates (Equation (11), (12)). Finally, we compute the gradient by backpropagating the
reconstruction error of target modality through the network.
4
There could be different ways of constructing MDRNN; for instance, one can construct the RNN with
DBM-style mean-field updates. In our empirical evaluation, however, running full pass of bottom-up and topdown updates performed the best, and DBM-style updates didn?t give competitive results.

5

Ground Truth
Query
ML (PCD)
MinVI (CDPercLoss)
MinVI (MP)

Figure 2: Visualization of samples with inferred missing modality. From top to bottom, we visualize
ground truth, left or right halves of digits, generated samples with inferred missing modality using
MRBM with ML objective, MinVI objective using CD-PercLoss and MP training methods.
Input modalities at test time
ML (PCD)
MinVI (CD-PercLoss)
MinVI (MP)

Left+Right
1.57%
1.71%
1.73%

Left
14.98%
9.42%
6.58%

Right
18.88%
11.02%
7.27%

Table 1: Test set handwritten digit recognition errors of MRBMs trained with different objectives
and learning algorithms. Linear SVM was used for classification with joint feature representations.

4

Experiments

4.1

Toy Example on MNIST

In our first experiment, we evaluate the proposed learning algorithm on the MNIST handwritten
digit recognition dataset [16]. We consider left and right halves of the digit images as two input
modalities and report the recognition performance with different combinations of input modalities
at the test time, such as full (left + right) or missing (left or right) data modalities. We compare
the performance of the MRBM trained with 1) ML objective using PCD [30], or MinVI objectives
with 2) CD-PercLoss or 3) MP training. The recognition errors are provided in Table 1. Compared
to ML training, the recognition errors for unimodal queries are reduced by more than a half with
MP training of MinVI objective. For multimodal queries, the model trained with ML objective
performed the best, although the performance gain was incremental. CD-PercLoss training of MinVI
objective also showed significant improvement over ML training, but the errors were not as low
as those obtained with MP training. We believe that, although it is an approximation of MinVI
objective, the exact gradient for MP algorithm makes learning more efficient than CD-PercLoss.
For the rest of the paper, we focus on MP training method.
In Figure 2, we visualize the generated samples conditioned on one input modality (e.g., left or right
halves of digits). There are many samples generated by the models with MinVI objective that look
clearly better than those generated by the model with ML objective.
4.2

MIR-Flickr Database

In this section, we evaluate our methods on MIR-Flickr database [11], which is composed of 1 million examples of image and their user tags collected from the social photo-sharing website Flickr.5
Among those, 25000 examples are annotated with 24 potential topics and 14 regular topics, which
leads to 38 classes in total with distributed class membership. The topics include object categories
such as dog, flower, and people, or scenic concepts such as sky, sea, and night.
We used the same visual and text features as in [27].6 Specifically, the image feature is 3857 dimensional vector composed of Pyramid Histogram of Words (PHOW) features [3], GIST [22], and
MPEG-7 descriptors [18]. We preprocessed the image features to have zero mean and unit variance for each dimension across all examples. The text feature is a word count vector of 2000 most
frequent tags. The number of tags varies from 0 to 72, with 5.15 tags per example in average.
Following the experimental protocol [12, 27], we randomly split the labeled data into 15000 for
training and 10000 for testing, and used 5000 from training set for validation. We iterate the procedure for 5 times and report the mean average precision (mAP) over 38 classes.
Model Architecture. As used in [27], the network is composed of [3857, 1024, 1024] variables
for visual pathway, [2000, 1024, 1024] variables for text pathway, and 2048 variables for top-layer
MRBM. As described in Section 3, we pretrain the modality-specific deep networks in a greedy
5
6

http://www.flickr.com
http://www.cs.toronto.edu/?nitish/multimodal/index.html

6

layerwise manner, and finetune the whole network by initializing MDRNN with the pretrained network. Specifically, we used gaussian RBM for the bottom layer of visual pathway and binary RBM
for text pathway.7 The intermediate layers are trained with binary RBMs, and the top-layer MRBM
is trained using MP training algorithm. For the layer-wise pretraining of RBMs, we used PCD [30]
to approximate gradient. Since our algorithm requires both data modalities during the training, we
excluded examples with too sparse or no tags from unlabeled dataset and used about 750K examples with at least 2 tags. After unsupervised training, we extract joint feature representations of the
labeled training data and use them to train multiclass logistic regression classifiers.
Recognition Tasks. For recognition tasks,
Model
Multimodal query
we train multiclass logistic regression classiAutoencoder
0.610
fiers using joint representations as input feaMultimodal DBM [27]
0.609
tures. Depending on the availability of data
Multimodal DBM? [28]
0.641
modalities at testing time, we evaluate the perMK-SVM [7]
0.623
formance using multimodal queries (i.e., both
TagProp [31]
0.640
MDRNN
0.686 ? 0.003
visual and text data are available) and unimodal
queries (i.e., visual data is available while the
Model
Unimodal query
text data is missing). The summary results are
Autoencoder
0.495
Multimodal DBM [27]
0.531
in Table 2. We report the test set mAPs of our
0.530
MK-SVM [7]
proposed model and compared to other methMDRNN
0.607 ? 0.005
ods. The proposed MDRNN outperformed the
previous state-of-the-art in multimodal queries
Table 2: Test set mAPs on MIR-Flickr database.
by 4.5% in mAP. The performance improveWe implemented autoencoder following the dement becomes more significant for unimodal
scription in [21]. Multimodal DBM? is supervised
queries, achieving 7.6% improvement in mAP
finetuned model. See [28] for details.
over the best published result. As we used the
same input features in [27], the results suggest that our proposed algorithm learns better representations shared across multiple modalities.
To take a closer look into our model, we performed additional control experiment. In particular, we
explore the benefit of recurrent encoding network structure of MDRNN. We compare the performance of the models with different number of mean-field iterations.8 We report the validation set
mAPs of models with different number of iterations (0 ? 10) in Table 3. For multimodal query, the
MDRNN with 10 iterations improves the recognition performance by only 0.8% compared to the
model with 0 iterations. However, the improvement becomes significant for unimodal query, achieving 5.0% performance gain. In addition, we note that the largest improvement was made when we
have at least one iteration (from 0 to 1 iteration, 3.4% gain; from 1 to 10 iteration, 1.6% gain). This
suggests that the most crucial factor of improvement comes from the inference with reconstructed
missing data modality (e.g., text features), and the quality of inferred missing modality improves as
we increase the number of iterations.
# iterations
Multimodal query
Unimodal query

0
0.677
0.557

1
0.678
0.591

2
0.679
0.599

3
0.680
0.602

5
0.682
0.605

10
0.685
0.607

Table 3: Validation set mAPs on MIR-Flickr database with different number of mean-field iterations.
Retrieval Tasks. We perform retrieval tasks using multimodal and unimodal input queries. Following the experimental setting in [27], we select 5000 image-text pairs from the test set to form a
database and use 1000 disjoint set of examples from the test set as queries. For each query example,
we compute the relevance score to the data points as a cosine similarity of joint representations.
The binary relevance label between query and the data points are determined 1 if any of the 38
class labels are overlapped. Our proposed model achieves 0.633 mAP with multimodal query and
0.638 mAP with unimodal query. This significantly outperforms the performance of multimodal
DBM [27], which reported 0.622 mAP with multimodal query and 0.614 mAP with unimodal query.
7
We assume text features as binary, which is different from [27] where they modeled using replicatedsoftmax RBM [25]. The rationale is that the tags are not likely to be assigned more than once for single image.
8
In [21], they proposed the ?video-only? deep autoencoder whose objective is to predict audio data and
reconstruct video data when only video data is given as an input during the training. Our baseline model
(MDRNN with 0 iterations) is similar, but different since we don?t have a reconstruction training objective.

7

night, city, river,
dark, buildings, skyline

night, long exposure,
reflection, buildings,
massachusetts, boston

sunset, explore, sun

sunset, platinumphoto,
trees, silhouette

toys

lego

skyline, indiana, 1855mm

city, lights, buildings,
fireworks, skyscrapers

nikon, night, d80, asia,
skyline, hongkong, harbour

sunset, sol, searchthebest,
atardecer, nubes, abigfave

sunset

canon, naturesfinest, 30d

diy, robot

toy, plastic,
kitty, miniature

lego

Figure 3: Retrieval results with multimodal queries. The leftmost image-text pairs are multimodal
query samples and those in the right side of the bar are retrieved samples with the highest similarities
to the query sample from the database. We include more results in supplementary material.
4.3

PASCAL VOC 2007

We evaluate the proposed algorithm on PASCAL VOC 2007 database. The original dataset doesn?t
contain user tags, but Guillaumin et al. [7] has collected the user tags from Flickr website.9
Motivated from the success of convolutional neural networks (CNNs) on large-scale visual object
recognition [14], we used the DeCAF7 features [5] as an input features for visual pathway, where
DeCAF7 is 4096 dimensional feature extracted from the CNN trained on ImageNet [4]. For text
features, we used the vocabulary of size 804 suggested by [7]. For unsupervised feature learning of
MDRNN, we used unlabeled data of MIR-Flickr database while converting the text features using
the new vocabulary from PASCAL database. The network architecture used in this experiment is as
follows: [4096, 1536, 1536] variables for the visual pathway, [804, 512, 1536] variables for the text
pathway, and 2048 variables for top-layer joint network.
Following the standard practice, we report the mAP over 20 object classes. The performance improvement of our proposed method was significant, achieving 81.5% mAP with multimodal queries
and 76.2% mAP with unimodal queries, whereas the performance of baseline model was 74.5%
mAP with multimodal queries (DeCAF7 + Text) and 74.3% mAP with unimodal queries (DeCAF7 ).

5

Conclusion

Motivated from the property of good generative models of multimodal data, we proposed a novel
multimodal deep learning framework based on variation of information. The minimum variation of
information objective enables to learn a good shared representations of multiple heterogeneous data
modalities with a better prediction of missing input modality. We demonstrated the effectiveness
of our proposed method on multimodal RBM and its deep extensions and showed state-of-the-art
recognition performance on MIR-Flickr database and competitive performance on PASCAL VOC
2007 database with multimodal (visual + text) and unimodal (visual only) queries.

Acknowledgments
This work was supported in part by ONR N00014-13-1-0762, Toyota, and Google Faculty Research
Award.

References
[1] Y. Bengio, L. Yao, G. Alain, and P. Vincent. Generalized denoising auto-encoders as generative models.
In NIPS, 2013.
[2] Y. Bengio, E. Thibodeau-Laufer, G. Alain, and J. Yosinski. Deep generative stochastic networks trainable
by backprop. In ICML, 2014.

9

http://lear.inrialpes.fr/people/guillaumin/data.php

8

[3] A. Bosch, A. Zisserman, and X. Munoz. Image classification using random forests and ferns. In ICCV,
2007.
[4] J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-Fei. ImageNet: A large-scale hierarchical image
database. In CVPR, 2009.
[5] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. DeCAF: A deep convolutional activation feature for generic visual recognition. CoRR, abs/1310.1531, 2013.
[6] I. Goodfellow, M. Mirza, A. Courville, and Y. Bengio. Multi-prediction deep Boltzmann machines. In
NIPS, 2013.
[7] M. Guillaumin, J. Verbeek, and C. Schmid. Multimodal semi-supervised learning for image classification.
In CVPR, 2010.
[8] G. E. Hinton. Training products of experts by minimizing contrastive divergence. Neural Computation,
14(8):1771?1800, 2002.
[9] G. E. Hinton and R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science,
313(5786):504?507, 2006.
[10] J. Huang and B. Kingsbury. Audio-visual deep learning for noise robust speech recognition. In ICASSP,
2013.
[11] M. J. Huiskes and M. S. Lew. The MIR Flickr retrieval evaluation. In ICMIR, 2008.
[12] M. J. Huiskes, B. Thomee, and M. S. Lew. New trends and ideas in visual concept detection: The MIR
Flickr retrieval evaluation initiative. In ICMIR, 2010.
[13] Y. Kim, H. Lee, and E. M. Provost. Deep learning for robust feature generation in audiovisual emotion
recognition. In ICASSP, 2013.
[14] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classification with deep convolutional neural
networks. In NIPS, 2012.
[15] K. Lai, L. Bo, X. Ren, and D. Fox. RGB-D object recognition: Features, algorithms, and a large scale
benchmark. In Consumer Depth Cameras for Computer Vision, pages 167?192. Springer, 2013.
[16] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.
Proceedings of the IEEE, 86(11):2278?2324, 1998.
[17] I. Lenz, H. Lee, and A. Saxena. Deep learning for detecting robotic grasps. In RSS, 2013.
[18] B. S. Manjunath, J-R. Ohm, V. V. Vasudevan, and A. Yamada. Color and texture descriptors. IEEE
Transactions on Circuits and Systems for Video Technology, 11(6):703?715, 2001.
[19] V. Mnih, H. Larochelle, and G. E. Hinton. Conditional restricted boltzmann machines for structured
output prediction. In UAI, 2011.
[20] R. M. Neal. Learning stochastic feedforward networks. Department of Computer Science, University of
Toronto, 1990.
[21] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng. Multimodal deep learning. In ICML, 2011.
[22] A. Oliva and A. Torralba. Modeling the shape of the scene: A holistic representation of the spatial
envelope. International Journal of Computer Vision, 42(3):145?175, 2001.
[23] D. Rao, M. De Deuge, N. Nourani-Vatani, B. Douillard, S. B. Williams, and O. Pizarro. Multimodal
learning for autonomous underwater vehicles from visual and bathymetric data. In ICRA, 2014.
[24] R. Salakhutdinov and G. E. Hinton. Deep Boltzmann machines. In AISTATS, 2009.
[25] R. Salakhutdinov and G. E. Hinton. Replicated softmax: an undirected topic model. In NIPS, 2009.
[26] H-C. Shin, M. R. Orton, D. J. Collins, S. J. Doran, and M. O. Leach. Stacked autoencoders for unsupervised feature learning and multiple organ detection in a pilot study using 4D patient data. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 35(8):1930?1943, 2013.
[27] N. Srivastava and R. Salakhutdinov. Multimodal learning with deep Boltzmann machines. In NIPS, 2012.
[28] N. Srivastava and R. Salakhutdinov. Discriminative transfer learning with tree-based priors. In NIPS,
2013.
[29] Y. Tang and R. Salakhutdinov. Learning stochastic feedforward neural networks. In NIPS, 2013.
[30] T. Tieleman. Training restricted Boltzmann machines using approximations to the likelihood gradient. In
ICML, 2008.
[31] J. Verbeek, M. Guillaumin, T. Mensink, and C. Schmid. Image annotation with tagprop on the MIR Flickr
set. In ICMIR, 2010.
[32] A. Wang, J. Lu, G. Wang, J. Cai, and T-J. Cham. Multi-modal unsupervised feature learning for RGB-D
scene labeling. In ECCV. Springer, 2014.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 5231-general-table-completion-using-a-bayesian-nonparametric-model.pdf

General Table Completion using a Bayesian
Nonparametric Model

Zoubin Ghahramani
Department of Engineering
University of Cambridge
zoubin@eng.cam.ac.uk

Isabel Valera
Department of Signal Processing
and Communications
University Carlos III in Madrid
ivalera@tsc.uc3m.es

Abstract
Even though heterogeneous databases can be found in a broad variety of applications, there exists a lack of tools for estimating missing data in such databases. In
this paper, we provide an efficient and robust table completion tool, based on a
Bayesian nonparametric latent feature model. In particular, we propose a general
observation model for the Indian buffet process (IBP) adapted to mixed continuous
(real-valued and positive real-valued) and discrete (categorical, ordinal and count)
observations. Then, we propose an inference algorithm that scales linearly with
the number of observations. Finally, our experiments over five real databases show
that the proposed approach provides more robust and accurate estimates than the
standard IBP and the Bayesian probabilistic matrix factorization with Gaussian
observations.

1

Introduction

A full 90% of all the data in the world has been generated over the last two years and this expansion
rate will not diminish in the years to come [17]. This extreme availability of data explains the great
investment that both the industry and the research community are expending in data science. Data is
usually organized and stored in databases, which are often large, noisy, and contain missing values.
Missing data may occur in diverse applications due to different reasons. For example, a sensor in
a remote sensor network may be damaged and transmit corrupted data or even cease to transmit;
participants in a clinical study may drop out during the course of the study; or users of a recommendation system rate only a small fraction of the available books, movies, or songs. The presence
of missing values can be challenging when the data is used for reporting, information sharing and
decision support, and as a consequence, missing data treatment has captured the attention in diverse
areas of data science such as machine learning, data mining, and data warehousing and management.
Several studies have shown that probabilistic modeling can help to estimate missing values, detect
errors in databases, or provide probabilistic responses to queries [19]. In this paper, we exclusively
focus on the use of probabilistic modeling for missing data estimation, and assume that the data
are missing completely at random (MCAR). There is extensive literature in probabilistic missing
data estimation and imputation in homogeneous databases, where all the attributes that describe
each object in the database present the same (continuous or discrete) nature. Most of the work
assumes that databases contain only either continuous data, usually modeled as Gaussian variables
[21], or discrete, that can be either modeled by discrete likelihoods [9] or simply treated as Gaussian
variables [15, 21]. However, there still exists a lack of work dealing with heterogeneous databases,
which in fact are common in real applications and where the standard approach is to treat all the
attributes, either continuous or discrete, as Gaussian variables. As a motivating example, consider a
database that contains the answers to a survey, including diverse information about the participants
such as age (count data), gender (categorical data), salary (continuous non negative data), etc.
1

In this paper, we provide a general Bayesian approach for estimating and replacing the missing data
in heterogeneous databases (being the data MCAR), where the attributes describing each object can
be either discrete, continuous or mixed variables. Specifically, we account for real-valued, positive
real-valued, categorical, ordinal and count data. To this end, we assume that the information in
the database can be stored in a matrix (or table), where each row corresponds to an object and
the columns are the attributes that describe the different objects. We propose a novel Bayesian
nonparametric approach for general table completion based on feature modeling, in which each
object is represented by a set of latent variables and the observations are generated from a distribution
determined by those latent features. Since the number of latent variables needed to explain the data
depends on the specific database, we use the Indian buffet process (IBP) [8], which places a prior
distribution over binary matrices where the number of columns (latent variables) is unbounded.
The standard IBP assumes real-valued observations combined with conjugate likelihood models
that allow for fast inference algorithms [4]. Here, we aim at dealing with heterogeneous databases,
which may contain mixed continuous and discrete observations.
We propose a general observation model for the IBP that accounts for mixed continuous and discrete data, while keeping the properties of conjugate models. This allows us to propose an inference
algorithm that scales linearly with the number of observations. The proposed algorithm does not
only infer the latent variables for each object in the table, but it also provides accurate estimates for
its missing values. Our experiments over five real databases show that our approach for table completion outperforms, in terms of accuracy, the Bayesian probabilistic matrix factorization (BPMF)
[15] and the standard IBP which assume Gaussian observations. We also observe that the approach
based on treating mixed continuous and discrete data as Gaussian fails in estimating some attributes,
while the proposed approach provides robust estimates for all the missing values regardless of their
discrete or continuous nature.
The main contributions in this paper are: i) A general observation model (for mixed continuous and
discrete data) for the IBP that allows us to derive an inference algorithm that scales linearly with
the number of objects, and its application to build ii) a general and scalable tool to estimate missing
values in heterogeneous databases. An efficient C-code implementation for Matlab of the proposed
table completion tool is also released on the authors website.

2

Related Work

In recent years, probabilistic modeling has become an attractive option for building database management systems since it allows estimating missing values, detecting errors, visualizing the data, and
providing probabilistic answers to queries [19]. BayesDB,1 for instance, is a database management
system that resorts to Crosscat [18], which originally appeared as a Bayesian approach to model human categorization of objects. BayesDB provides missing data estimates and probabilistic answer
to queries, but it only considers Gaussian and multinomial likelihood functions.
In the literature, probabilistic low-rank matrix factorization approaches have been broadly applied to
table completion (see, e.g., [14, 15, 21]). In these approaches, the table database X is approximated
by a low-rank matrix representation X ? ZB, where Z and B are usually assumed to be Gaussian
distributed. Most of the works in this area have focused on building automatic recommendation
systems, which appears as the most popular application of missing data estimation [14, 15, 21].
More specific models to build recommendation systems can be found in [7, 22], where the authors
assume that the rates each user assign to items are generated by a probabilistic generative model
which, based on the available data, accounts for similarities among users and among items to provide
good estimates of the missing rates.
Probabilistic matrix factorization can also be viewed as latent feature modeling, where each object
is represented by a vector of continuous latent variables. In contrast, the IBP and other latent feature
models (see, e.g., [16]) assume binary latent features to represent each object. Latent feature models
usually assume homogeneous databases with either real [14, 15, 21] or categorical data [9, 12, 13],
and only a few works consider heterogeneous data, such as mixed real and categorical data [16].
However, up to our knowledge, there are no general latent feature models (nor table completion
tools) to directly deal with heterogeneous databases. To fill this gap, in this paper we provide a
general table completion approach for heterogeneous databases, based on a generalized IBP, that
allows for efficient inference.
1

http://probcomp.csail.mit.edu/bayesdb/

2

3

Model Description

Let us assume a table with N objects, where each object is defined by D attributes. We can store
the data in an N ? D observation matrix X, in which each D-dimensional row vector is denoted by
d
d
xn = [x1n , . . . , xD
n ] and each entry is denoted by xn . We consider that column vectors x (i.e., each
dimension in the observation matrix X) may contain the following types of data:
? Continuous variables:
1. Real-valued, i.e., xdn ? <
2. Positive real-valued, i.e., xdn ? <+ .
? Discrete variables:
1. Categorical data, i.e., xdn takes values in a finite unordered set, e.g., xdn ? {?blue?,
?red?, ?black?}.
2. Ordinal data, i.e., xdn takes values in a finite ordered set, e.g., xdn ? {?never?, ?sometimes?, ?often?, ?usually?, ?always?}.
3. Count data, i.e., xdn ? {0, . . . , ?},

We assume that each observation xdn can be explained by a K-length vector of latent variables
associated to the n-th data point zn = [zn1 , . . . , znK ] and a weighting vector2 Bd = [bd1 , . . . , bdK ]
(being K the number of latent variables), whose elements bdk weight the contribution of k-th the
latent feature to the d-th dimension of X. We gather the latent binary feature vectors zn in a N ? K
matrix Z, which follows an IBP with concentration parameter ?, i.e., Z ? IBP(?) [8]. We place a
2
Gaussian distribution with zero mean and covariance matrix ?B
IK over the weighting vectors Bd .
d
For convenience, zn is a K-length row vector, while B is a K-length column vector.
To accommodate for all kinds of observed random variables described above, we introduce an auxiliary Gaussian variable ynd , such that when conditioned on the auxiliary variables, the latent variable
model behaves as a standard IBP with Gaussian observations. In particular, we assume ynd is Gaussian distributed with mean zn Bd and variance ?y2 , i.e.,
p(ynd |zn , Bd ) = N (ynd |zn Bd , ?y2 ),
and assume that there exists a transformation function over the variables ynd to obtain the observations xdn , mapping the real line < into the observation space. The resulting generative model is
shown in Figure 1, where Z is the IBP latent matrix, and Yd and Bd contain, respectively, the
auxiliary Gaussian variables ynd and the weighting factors bdk for the d-dimension of the data. Additionally, ?d denotes the set of auxiliary random variables needed to obtain the observation vector
xd given Yd , and Hd contains the hyper-parameters associated to the random variables in ?d . This
model assumes that the observations xdn are independent given the latent matrix Z, the weighting
matrices Bd and the auxiliary variables ?d . Therefore, the likelihood can be factorized as
d

p(X|Z, {B

, ?d }D
d=1 )

=

D
Y
d=1

p(x |Z, B , ? ) =
d

d

d

D Y
N
Y
d=1 n=1

p(xdn |zn , Bd , ?d ).

Note that, if we assume Gaussian observations and set Yd = xd , this model resembles the standard
IBP with Gaussian observations [8]. In addition, conditioned on the variables Yd , we can infer the
latent matrix Z as in the standard IBP. We also remark that auxiliary Gaussian variables to link a
latent model with the observations have been previously used in Gaussian processes for multi-class
classification [6] and for ordinal regression [2]. However, up to our knowledge, this simple approach
has not been used to account for mixed continuous and discrete data, and the existent approaches
for the IBP with discrete observations propose non-conjugate likelihood models and approximate
inference algorithms [12, 13].
3.1

Likelihood Functions

Now, we define the set of transformations that map from the Gaussian variables ynd to the corresponding observations xdn . We consider that each dimension in the table X may contain any of the
discrete or continuous variables detailed above, provide a likelihood function for each kind of data
and, in turn, also a likelihood function for mixed data.
2

For convenience, we capitalized here the notation for the weighting vectors Bd .

3

Real-valued Data. In this case, we assume that xd = Yd in the model in Figure 1 and consider
the standard approach when dealing with real-valued observations, which consist of assuming a
Gaussian likelihood function. In particular, as in the standard linear-Gaussian IBP [8], we assume
that each observation xdn is distributed as
p(xdn |zn , Bd ) = N (xdn |zn Bd , ?y2 ).
Positive Real-valued Data. In order to obtain positive real-valued observations, i.e., xdn ? <+ , we
apply a transformation over ynd that maps from the real numbers to the positive real numbers, i.e.,
xdn = f (ynd + udn ),
where udn is a Gaussian noise variable with variance ?u2 , and f : < ? <+ is a monotonic differentiable function. By change of variables, we obtain the likelihood function for positive real-valued
observations as





1
1
?1 d
d 2  d
?1 d 
p(xdn |zn , Bd ) = q
f
exp ?
(f
(x
)
?
z
B
)
(x
)
n
n
n  , (1)

2
2
d
2(?y + ?u )
dxn
2?(?y2 + ?u2 )
where f ?1 : <+ ? < is the inverse function of the transformation f (?), i.e, f ?1 (f (v)) = v. Note
that in this case we resort to the Gaussian variable udn in order to obtain xdn from ynd , and therefore,
?d = udd and Hd = ?u2 .
Categorical Data. Now we account for categorical observations, i.e., each observation xdn can take
values in the unordered index set {1, . . . , Rd }. Hence, assuming a multinomial probit model, we
can write
d
xdn = arg max ynr
,
(2)
r?{1,...,Rd }

d
|zn bdr , ?y2 )
N (ynr

bdr

denotes the K-length weighting vector, in which each bdkr
where
?
being
weights the influence of the k-th feature for the observation xdn taking value r. Note that, under this
d
likelihood model, since we have a Gaussian auxiliary variable ynr
and a weighting factor bdkr for
each possible value of the observation r ? {1, . . . , Rd }, we need to gather all the weighting factors
d
in the N ? Rd matrix Yd .
bdkr in a K ? Rd matrix Bd , and all the Gaussian auxiliary variables ynr
d
ynr

d
= zn bdr + udnr , where udnr is a Gaussian noise
Under this observation model, we can write ynr
2
variable with variance ?y , and therefore, we can obtain the probability of each element xdn taking
value r ? {1, . . . , Rd } as [6]
"R
#
d


Y
d
d
d
d
p(xn = r|zn , B ) = Ep(u)
? u + zn (br ? bj ) ,
(3)
j=1
j6=r

where subscript r in bdr states for the column in Bd (r ? {1, . . . , Rd }), ?(?) denotes the cumulative
density function of the standard normal distribution and Ep(u) [?] denotes expectation with respect to
the distribution p(u) = N (0, ?y2 ).
Ordinal Data. Consider ordinal data, in which each element xdn takes values in the ordered index
set {1, . . . , Rd }. Then, assuming an ordered probit model, we can write
?
if ynd ? ?1d
?
? 1
?
? 2
if ?1d < ynd ? ?2d
xdn =
(4)
..
?
.
?
?
?
d
Rd
if ?R
< ynd
d ?1
where again ynd is Gaussian distributed with mean zn Bd and variance ?y2 , and ?rd for r ?
{1, . . . , Rd ? 1} are the thresholds that divide the real line into Rd regions. We assume the thresholds ?rd are sequentially generated from the truncated Gaussian distribution ?rd ? N (?rd |0, ??2 )I(?rd >
d
d
?r?1
), where ?0d = ?? and ?R
= +?. As opposed to the categorical case, now we have a unique
d
4

weighting vector Bd and a unique Gaussian variable ynd for each observation xdn . Hence, the value
of xdn is determined by the region in which ynd falls.
Under the ordered probit model [2], the probability of each element xdn taking value r ? {1, . . . , Rd }
can be written as
!
!
d
d
d
d
?
?
z
B
?
?
z
B
n
n
r?1
r
p(xdn = r|zn , Bd ) = ?
??
.
(5)
?y
?y
Let us remark that, if the d-dimension of the observation matrix contains ordinal data, the set of
d
auxiliary variables reduces to the Gaussian thresholds ?d = {?1d , . . . , ?R
} and Hd = ??2 .
d ?1
Count Data. In count data each observation xdn takes non-negative integer values, i.e., xdn ?
{0, . . . , ?}. Then, we assume
xdn = bf (ynd )c,
(6)
where bvc returns the floor of v, that is the largest integer that does not exceed v, and f : < ? <+
is a monotonic differentiable function that maps from the real numbers to the positive real numbers.
We can therefore write the likelihood function as
!
!
d
?1 d
d
?1 d
)
?
z
B
f
(x
+
1)
?
z
B
f
(x
n
n
n
n
??
(7)
p(xdn |zn , Bd ) = ?
?y
?y
where f ?1 : <+ ? < is the inverse function of the transformation f (?).
2
y

?

Z

Yd

2
B

Bd

X
d

Hd
d = 1, . . . , D

Figure 1: Generalized IBP for mixed continuous and discrete observations.

4

Inference Algorithm

In this section we describe our algorithm for inferring the latent variables given the observation
matrix. Under our model, detailed in Section 3, the probability distribution over the observation
matrix is fully characterized by the latent matrices Z and {Bd }D
d=1 (as well as the auxiliary variables
?d ). Hence, if we assume the latent vector zn for the n-th datapoint and the weighting factors
Bd (and the auxiliary variables ?d ) to be known, we have a probability distribution over missing
observations xdn from which we can obtain estimates for xdn by sampling from this distribution,3 or
by simply taking either its mean, mode or median value. However, this procedure requires the latent
matrix Z and the latent weighting factors Bd (and ?d ) to be known.
We use Markov Chain Monte Carlo (MCMC) methods, which have been broadly applied to infer
the IBP matrix (see, e.g., in [8, 23, 20]). The proposed inference algorithm is summarized in Algorithm 1. This algorithm exploits the information in the available data to learn the similarities among
the objects (captured in our model by the latent feature matrix Z), and how these latent features
show up in the attributes that describe the objects (captured in our model by Bd ). In Algorithm 1,
we first need to update the latent matrix Z. Note that conditioned on {Yd }D
d=1 , both the latent
are
independent
of
the
observation
matrix X. Admatrix Z and the weighting matrices {Bd }D
d=1
d D
ditionally, since {Bd }D
and
{Y
}
are
Gaussian
distributed,
we
can
analytically
marginalize
d=1
d=1
d D
out the weighting matrices {Bd }D
to
obtain
p({Y
}
|Z).
Therefore,
to
infer
the
matrix
Z, we
d=1
d=1
can apply the collapsed Gibbs sampler which presents better mixing properties than the uncollapsed
3
Note that sampling from this distribution might be computationally expensive. In this case, we can easily
obtain samples of xdn by exploiting the structure of our model. In particular, we can simply sample the auxiliary
Gaussian variables ynd given zn and Bd , and then obtain an estimate for xdn by applying the corresponding
transformation, detailed in Section 3.1.

5

Algorithm 1 Inference Algorithm.
Input: X
Initialize: initialize Z and {Yd }D
d=1
1: for each iteration do
2:
Update Z given {Yd }D
d=1 .
3:
for d = 1, . . . , D do
4:
Sample Bd given Z and Yd according to (8).
5:
Sample Yd given X, Z and Bd (as shown in the Supplementary Material).
6:
Sample ?d if needed (as shown in the Supplementary Material).
7:
end for
8: end for
d D
Output: Z, {Bd }D
d=1 and {? }d=1

Gibbs sampler and, in consequence, is the standard method of choice in the context of the standard
linear-Gaussian IBP [8]. However, this algorithm suffers from a high computational cost (being
complexity per iteration cubic with the number of data points N ), which is prohibitive when dealing
with large databases. In order to solve this limitation, we resort to the accelerated Gibbs sampler [4]
instead. This algorithm presents linear complexity with the number of datapoints and is detailed in
the Supplementary Material.
Second, we need to sample the weighting factors in Bd , which is a K ? Rd matrix in the case of
categorical attributes, and a K-length column vector otherwise. We denote each column vector in
Bd by bdr . The posterior over the weighting vectors are given by
p(bdr |yrd , Z) = N (bdr |P?1 ?dr , P?1 ),

(8)

2
where P = Z> Z + 1/?B
Ik and ?dr = Z> yrd . Note that the covariance matrix P?1 depend neither
on the dimension d nor on r, so we only need to invert the K ? K matrix P once at each iteration.
We describe in the Supplementary Material how to efficiently compute P after changes in the Z
matrix by rank one updates, without the need of computing the matrix product Z> Z.

Once we have updated Z and Bd , we sample each element in Yd from the distribution
d
d
|xdn , zn , bd ) spec|zn bd , ?y2 ) if the observation xdn is missing, and from the posterior p(ynr
N (ynr
ified in the Supplementary Material, otherwise. Finally, we sample the auxiliary variables in ?d
from their posterior distribution (detailed in the Supplementary Material) if necessary. This two latter steps involve, in the worst case, sampling from a doubly truncated univariate normal distribution
(see the Supplementary Material for further details), for which we make use of the algorithm in [11].

5

Experimental evaluation

We now validate the proposed algorithm for table completion on five real databases, which are
summarized in Table 1. The datasets contain different numbers of instances and attributes, which
cover all the discrete and continuous variables described in Section 3. We compare, in terms of
predictive log-likelihood, the following methods for table completion:
? The proposed general table completion approach denoted by GIBP (detailed in Section 3).
? The standard linear-Gaussian IBP [8] denoted by SIBP, treating all the attributes as Gaussian.
? The Bayesian probabilistic matrix factorization approach [15] denoted by BPMF, that also
treats all the attributes in X as Gaussian distributed.
For the GIBP, we consider for the real positive and the count data the following transformation,
that maps from the real numbers to the real positive numbers, f (x) = log(exp(wx) + 1), where
w is a user hyper-parameter. Before running the SIBP and the BPMF methods we normalize each
column in matrix X to have zero-mean and unit-variance. Then, in order to provide estimates for
the missing data, we denormalize the inferred Gaussian variable. Additionally, since both the SIBP
and the BPMF assume continuous observations, when dealing with discrete data, we estimate each
missing value as the closest integer value to the (denormalized) Gaussian variable.
6

Dataset
Statlog German credit dataset
[5]
QSAR biodegradation dataset
[10]
Internet usage survey dataset
[1]
Wine quality Dataset [3]

N
1,000

6,497

D
20 (10 C + 4 O
+ 6 N)
41 (2 R + 17 P
+ 4 C + 18 N)
32 (23 C + 8 O
+ 1 N)
12 (11 P + 1 N)

NESARC dataset [13]

43,000

55 C

1,055
1,006

Description
Collects information about the credit risks of
the applicants.
Contains molecular descriptors of biodegradable and non-biodegradable chemicals.
Contains the responses of the participants to a
survey related to the usage of internet.
Contains the results of physicochemical tests realized to different wines.
Contains the responses of the participants to a
survey related to personality disorders.

0

?2

?2

?3
?4

GIBP
SIBP
BPMF

?5
?6
10

20

30
40
% of missing data

50

(a) Statlog.

?1
Log?likelihood

?1
Log?likelihood

Log?likelihood

Table 1: Description of datasets. ?R? states for real-valued variables, ?P? for positive real-valued
variables, ?C? for categorical variables, ?O? for ordinal variables and ?N? for count variables

?4
GIBP
SIBP
BPMF

?6
?8
?10
10

30
40
% of missing data

50

(b) QSAR biodegradation.

10

20

30

40 50 60 70
% of missing data

80

90

(c) Internet usage survey.

?0.5
Log?likelihood

Log?likelihood

GIBP
SIBP
BPMF

?2

?2.5
20

0
GIBP
SIBP
BPMF

?5

?10

?0.6
?0.7
GIBP
SIBP

?0.8
10

?1.5

20

30

40 50 60 70
% of missing data

80

90

10

(d) Wine quality.

20

30

40 50 60 70
% of missing data

80

90

(e) Nesarc database

Figure 2: Average test log-likelihood per missing datum. The ?whiskers? show a standard deviations
from the average test log-likelihood.
In Figure 2, we plot the average predictive log-likelihood per missing value as a function of the
percentage of missing data. Each value in Figure 2 has been obtained by averaging the results in
20 independent sets where the missing values have been randomly chosen. In Figures 2a and 2b,
we cut the plot in 50% because, in these two databases, the discrete attributes present a mode value
that is present for more than 80% of the instances. As a consequence, the SIBP and the BPMF
algorithms assign probability close to one to the mode, which results in an artificial increase in the
average test log-likelihood for larger percentages of missing data. For the BPMF model, we have
used different numbers of latent features (in particular, 10, 20 and 50), although we only show the
best results for each database, specifically, K = 10 for the NESARC and the wine databases, and
K = 50 for the remainder. Both the GIBP and the SIBP have not inferred a number of (binary)
latent features above 25 in any case. Note that in Figure 2e, we only plot the test log-likelihood for
the GIBP and the SIBP because the BPMF provides much lower values. As expected, we observe
in Figure 2 that the average test log-likelihood decreases for the three models when the number of
missing values increases (flat shape of the curves are due to the y-axis scale). In this figure, we also
observe that the proposed general IBP model outperforms the SIBP and the BPMF for four of the
the databases, being the SIBP slightly better for the Internet database. The BPMF model presents
the lowest test-log-likelihood in all the databases.
Now, we analyze the performance of the three models for each kind of discrete and continuous
variables. Figure 3 shows average predictive likelihood per missing value for each attribute in the
table, i.e., for each dimension in X. In this figure we have grouped the dimensions according to the
kind of data that they contain, showing in the x-axis the number of considered categories for the case
of categorical and ordinal data. In this figure, we observe that the GIBP presents similar performance
7

for all the attributes in the five databases, while for the SIBP and the BPMF models, the test-loglikelihood falls drastically for some of the attributes, being this effect worse in the case of the BPMF
(it explains the low log-likelihood in Figure 2). This effect is even more evident in Figures 2b
and 2d. We also observe, in Figures 2 and 3, that both IBP based approaches (the GIBP and the
SIBP) outperform the BPMF, with the proposed GIBP being the one that best performs across all
the databases. We can conclude that, unlike to the BPMF and the GIBP, the GIBP provides accurate
estimates for the missing data regardless of their discrete or continuous nature.

6

Conclusions

In this paper, we have proposed a table completion approach for heterogeneous databases, based on
an IBP with a generalized likelihood that allows for mixed discrete and continuous data. We have
then derived an inference algorithm that scales linearly with the number of observations. Finally, our
experimental results over five real databases have shown than the proposed approach outperforms,
in terms of robustness and accuracy, approaches that treat all the attributes as Gaussian variables.
Log?likelihood

0
?10
GIBP
SIBP
BPMF

?20
?30

C5

C10

C5

C3

C4

C3

C3

C4

C2

C2 O4
Attribute

O5

O5

O2

N

N

N

N

N

N

(a) Statlog.
Log?likelihood

10
0
?10
?20
?30

GIBP
SIBP
BPMF
R R P P P P P P P P P P P P P P P P P C2C2C4C2 N N N N N N N N N N N N N N N N N N
Attribute

(b) QSAR biodegradation.
Log?likelihood

0
?2
?4
GIBP
SIBP
BPMF

?6
?8

C3 C3 C3 C3 C3 C3 C4 C4 C4 C5 C5 C6 C6 C6 C6 C6 C5 C5 C3 C2 C2 C2 C9 O6 O7 O7 O7 O7 O7 O8 O6 N
Attribute

(c) Internet usage survey.
Log?likelihood

10
0
?10
GIBP
SIBP
BPMF

?20
?30

P

P

P

P

P

P
P
Attribute

P

P

P

P

N

(d) Wine quality.
Log?likelihood

0
?10
GIBP
SIBP
BPMF

?20
?30

CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC
Attribute

(e) Nesarc database

Figure 3: Average test log-likelihood per missing datum in each dimension of the data with 50% of
missing data. In the x-axis ?R? states for real-valued variables, ?P? for positive real-valued variables,
?C? for categorical variables, ?O? for ordinal variables and ?N? for count variables. The number that
accompanies the ?C? or ?O? corresponds to the number of categories.
Acknowledgments
Isabel Valera acknowledge the support of Plan Regional-Programas I+D of Comunidad de Madrid
(AGES-CM S2010/BMD-2422), Ministerio de Ciencia e Innovaci?on of Spain (project DEIPRO
TEC2009-14504-C02-00 and program Consolider-Ingenio 2010 CSD2008-00010 COMONSENS).
Zoubin Ghahramani is supported by the EPSRC grant EP/I036575/1 and a Google Focused Research
Award.
8

References
[1] Pew Research Centre.
25th anniversary of the web.
Available on:
http://www.pewinternet.org/datasets/january-2014-25th-anniversary-of-the-web-omnibus/.
[2] W. Chu and Z. Ghahramani. Gaussian processes for ordinal regression. J. Mach. Learn. Res., 6:1019?
1041, December 2005.
[3] P. Cortez, A. Cerdeira, F. Almeida, T. Matos, and J. Reis.
Modeling wine preferences by
data mining from physicochemical properties. Decision Support Systems. Dataset available on:
http://archive.ics.uci.edu/ml/datasets.html, 47(4):547?553, 2009.
[4] F. Doshi-Velez and Z. Ghahramani. Accelerated sampling for the indian buffet process. In Proceedings of
the 26th Annual International Conference on Machine Learning, ICML ?09, pages 273?280, New York,
NY, USA, 2009. ACM.
[5] J. Eggermont, J. N. Kok, and W. A. Kosters. Genetic programming for data classification: Partitioning
the search space. In In Proceedings of the 2004 Symposium on applied computing (ACM SAC04). Dataset
available on: http://archive.ics.uci.edu/ml/datasets.html, pages 1001?1005. ACM, 2004.
[6] M. Girolami and S. Rogers. Variational Bayesian multinomial probit regression with Gaussian process
priors. Neural Computation, 18:2006, 2005.
[7] P. Gopalan, F. J. R. Ruiz, R. Ranganath, and D. M. Blei. Bayesian Nonparametric Poisson Factorization for Recommendation Systems. nternational Conference on Artificial Intelligence and Statistics
(AISTATS), 2014.
[8] T. L. Griffiths and Z. Ghahramani. The Indian buffet process: an introduction and review. Journal of
Machine Learning Research, 12:1185?1224, 2011.
[9] X.-B. Li. A Bayesian approach for estimating and replacing missing categorical data. J. Data and
Information Quality, 1(1):3:1?3:11, June 2009.
[10] K. Mansouri, T. Ringsted, D. Ballabio, R. Todeschini, and V. Consonni. Quantitative structureactivity
relationship models for ready biodegradability of chemicals. Journal of Chemical Information and Modeling. Dataset available on: http://archive.ics.uci.edu/ml/datasets.html.
[11] C. P. Robert. Simulation of truncated normal variables. Statistics and computing, 5(2):121?125, 1995.
[12] F. J. R. Ruiz, I. Valera, C. Blanco, and F. Perez-Cruz. Bayesian nonparametric modeling of suicide
attempts. Advances in Neural Information Processing Systems, 25:1862?1870, 2012.
[13] F. J. R. Ruiz, I. Valera, C. Blanco, and F. Perez-Cruz. Bayesian nonparametric comorbidity analysis of psychiatric disorders. Journal of Machine Learning Research (To appear). Available on
http://arxiv.org/pdf/1401.7620v1.pdf, 2013.
[14] R. Salakhutdinov and A. Mnih. Probabilistic matrix factorization. In Advances in Neural Information
Processing Systems, 2007.
[15] R. Salakhutdinov and A. Mnih. Bayesian probabilistic matrix factorization using Markov Chain Monte
Carlo. In Proceedings of the 25th International Conference on Machine Learning, ICML ?08, pages
880?887, New York, NY, USA, 2008. ACM.
[16] E. Salazar, M. Cain, E. Darling, S. Mitroff, and L. Carin. Inferring latent structure from mixed real and
categorical relational data. In Proceedings of the 29th International Conference on Machine Learning
(ICML-12), ICML ?12, pages 1039?1046, New York, NY, USA, July 2012. Omnipress.
[17] ScienceDaily. Big data, for better or worse: 90% of world?s data generated over last two years.
[18] P. Shafto, C. Kemp, Mansinghka V., and Tenenbaum J. B. A probabilistic model of cross-categorization.
Cognition, 120(1):1 ? 25, 2011.
[19] S. Singh and T. Graepel. Automated probabilistic modelling for relational data. In Proceedings of the
ACM of Information and Knowledge Management, CIKM ?13, New York, NY, USA, 2013. ACM.
[20] M. Titsias. The infinite gamma-Poisson feature model. Advances in Neural Information Processing
Systems, 19, 2007.
[21] A. Todeschini, F. Caron, and M. Chavent. Probabilistic low-rank matrix completion with adaptive spectral
regularization algorithms. In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger,
editors, Advances in Neural Information Processing Systems 26, pages 845?853. Curran Associates, Inc.,
Dec. 2013.
[22] C. Wang and D. M. Blei. Collaborative topic modeling for recommending scientific articles. In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
KDD ?11, pages 448?456, New York, NY, USA, 2011. ACM.
[23] S. Williamson, C. Wang, K. Heller, and D. Blei. The IBP compound Dirichlet process and its application to focused topic modeling. Proceedings of the 27th Annual International Conference on Machine
Learning, 2010.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 6274-generalized-correspondence-lda-models-gc-lda-for-identifying-functional-regions-in-the-brain.pdf

Generalized Correspondence-LDA Models (GC-LDA)
for Identifying Functional Regions in the Brain

Timothy N. Rubin
SurveyMonkey

Oluwasanmi Koyejo
Univ. of Illinois, Urbana-Champaign

Michael N. Jones
Indiana University

Tal Yarkoni
University of Texas at Austin

Abstract
This paper presents Generalized Correspondence-LDA (GC-LDA), a generalization
of the Correspondence-LDA model that allows for variable spatial representations
to be associated with topics, and increased flexibility in terms of the strength
of the correspondence between data types induced by the model. We present
three variants of GC-LDA, each of which associates topics with a different spatial
representation, and apply them to a corpus of neuroimaging data. In the context of
this dataset, each topic corresponds to a functional brain region, where the region?s
spatial extent is captured by a probability distribution over neural activity, and the
region?s cognitive function is captured by a probability distribution over linguistic
terms. We illustrate the qualitative improvements offered by GC-LDA in terms
of the types of topics extracted with alternative spatial representations, as well
as the model?s ability to incorporate a-priori knowledge from the neuroimaging
literature. We furthermore demonstrate that the novel features of GC-LDA improve
predictions for missing data.

1

Introduction

One primary goal of cognitive neuroscience is to find a mapping from neural activity onto cognitive
processes?that is, to identify functional networks in the brain and the role they play in supporting
macroscopic functions. A major milestone towards this goal would be the creation of a ?functionalanatomical atlas? of human cognition, where, for each putative cognitive function, one could identify
the regions and brain networks within the region that support the function.
Efforts to create such functional brain atlases are increasingly common in recent years. Most studies
have proceeded by applying dimensionality reduction or source decomposition methods such as
Independent Component Analysis (ICA) [4] and clustering analysis [9] to large fMRI datasets such
as the Human Connectome Project [10] or the meta-analytic BrainMap database [8]. While such
work has provided valuable insights, these approaches also have significant drawbacks. In particular,
they typically do not jointly estimate regions along with their mapping onto cognitive processes.
Instead, they first extract a set of neural regions (e.g., via ICA performed on resting-state data), and
then in a separate stage?if at all?estimate a mapping onto cognitive functions. Such approaches do
not allow information regarding cognitive function to constrain the spatial characterization of the
regions. Moreover, many data-driven parcellation approaches involve a hard assignment of each brain
voxel to a single parcel or cluster, an assumption that violates the many-to-many nature of functional
brain networks. Ideally, a functional-anatomical atlas of human cognition should allow the spatial
and functional correlates of each atom or unit to be jointly characterized, where the function of each
region constrains its spatial boundaries, and vice-versa.
30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

In the current work, we propose Generalized Correspondence LDA (GC-LDA) ? a novel generalization of the Correspondence-LDA model [2] for modeling multiple data types, where one data type
describes the other. While the proposed approach is general and can be applied to a variety of data,
our work is motivated by its application to neuroimaging meta-analysis. To that end, we consider
several GC-LDA models that we apply to the Neurosynth [12] corpus, consisting of the document
text and neural activation data from a large body of neuroimaging publications. In this context, the
models extract a set of neural ?topics?, where each topic corresponds to a functional brain region. For
each topic, the model describes its spatial extent (captured via probability distributions over neural
activation) and cognitive function (captured via probability distributions over linguistic terms). These
models provide a novel approach for jointly identifying the spatial location and cognitive mapping of
functional brain regions, that is consistent with the many-to-many nature of functional brain networks.
Furthermore, to the best of our knowledge, one of the GC-LDA variants provides the first automated
measure of the lateralization of cognitive functions based on large-scale imaging data.
The GC-LDA and Correspondence-LDA models are extensions of Latent Dirichlet Allocation (LDA)
[3]. Several Bayesian methods with similarities (or equivalences) to LDA have been applied to
different types of neuroimaging data. Poldrack et al. (2012) used standard LDA to derive topics
from the text of the Neurosynth database and then projected the topics onto activation space based on
document-topic loadings [7]. Yeo et al. (2014) used a variant of the Author-Topic model to model the
BrainMap Database [13]. Manning et al. (2014) described a Bayesian method ?Topographic Factor
Analysis? to identify brain regions based on the raw fMRI images (but not text) extracted from a set
of controlled experiments, which can later be mapped on functional categories [5].
Relative to the Correspondence-LDA model, the GC-LDA model incorporates: (i) the ability to
associate different types of spatial distributions with each topic, (ii) flexibility in how strictly the
model enforces a correspondence between the textual and spatial data within each document, and (iii)
the ability to incorporate a-priori spatial structure, e.g., encouraging relatively homologous functional
regions located in each brain hemisphere. As we show, these aspects of GC-LDA have a significant
effect on the quality of the estimated topics, as well as on the models? ability to predict missing data.

2

Models

In this paper we propose a set of unsupervised generative models based on the Correspondence-LDA
model [2] that we use to jointly model text and brain activations from the Neurosynth meta-analytic
database [12]. Each of these models, as well as Correspondence-LDA, can be viewed as special cases
of a broader model that we will refer to as Generalized Correspondence-LDA (GC-LDA). In the
section below, we describe the GC-LDA model and its relationship to Correspondence-LDA. We
then detail the specific instances of the model that we use throughout the remainder of the paper. A
summary of the notation used throughout the paper is provided in Table 1.
2.1

Generalized Correspondence LDA (GC-LDA)

Each document d in the corpus is comprised of two types of data: a set of word tokens
 (d) (d)
(d) 	
w1 , w2 , ..., w (d) consisting of unigrams and/or n-grams, and a set of peak activation to (d) (d) Nw (d) 	
(d)
(d)
kens x1 , x2 , ..., x (d) , where Nw and Nx are the number of word and activation tokens in
Nx
document d, respectively. In the target application, each token xi is a 3-dimensional vector corresponding to the peak activation coordinates of a value reported in fMRI publications. However, we
note that this model can be directly applied to other types of data, such as segmented images, where
each xi corresponds to a vector of real-valued features extracted from each image segment (c.f. [2]).
GC-LDA is described by the following generative process (depicted in Figure 1.A):

	
1. For each topic t ? 1, ..., T 1 :
(a) Sample a Multinomial distribution over word types ?(t) ? Dirichlet(?)
2. For each document d ? {1, ..., D}:
1

To make the model fully generative, one could additionally put a prior on the spatial distribution parameters
?(t) and sample them. For the purposes of the present paper we do not specify a prior on these parameters, and
therefore leave this out of the generative process.

2

Table 1: Table of notation used throughout the paper
Notation
wi , xi
(d)
(d)
Nw , Nx
D
T
R
zi
yi
z(d) , y(d)
YD
Ntd
ci
?(t)
?(t) , ? (t)
(t)
(t)
?r , ? r
?(t)
(t)
?w
(d)
?
(d)
?t
? (t)
(t)
?r
?, ?, ?
?

Model specification
Meaning
The ith word token and peak activation token in the corpus, respectively
The number of word tokens and peak activation tokens in document d, respectively
The number of documents in the corpus
The number of topics in the model
The number of components/subregions in each topic?s spatial distribution (subregions model)
Indicator variable assigning word token wi to a topic
Indicator variable assigning activation token xi to a topic
The set of all indicator variables for word tokens and activation tokens in document d
The number of activation tokens within document d that are assigned to topic t
Indicator variable assigning activation token yi to a subregion (subregion models)
Placeholder for all spatial parameters for topic t
Gaussian parameters for topic t
Gaussian parameters for subregion r in topic t (subregion models)
Multinomial distribution over word types for topic t
Probability of word type w given topic t
Multinomial distribution over topics for document d
Probability of topic t given document d
Multinomial distribution over subregions for topic t (subregion models)
Probability of subregion r given topic t (subregion models)
Model hyperparameters
Model hyperparameter (subregion models)

(a) Sample a Multinomial distribution over topics ?(d) ? Dirichlet(?)

(d) 	
(b) For each peak activation token xi , i ? 1, ..., Nx :
i. Sample indicator variable yi from Multinomial(?(d) )
ii. Sample a peak activation token xi from the spatial distribution: xi ? f (?(yi ) )

(d) 	
(c) For each word token wi , i ? 1, ..., Nw :
 YD
YD
+?
N1d +?
N2d
i. Sample indicator variable zi from Multinomial (d)
, (d)
, ...,
Nx

+??T

Nx

+??T

YD
NT
d +?
(d)
Nx +??T


,

YD
where Ntd
is the number of activation tokens y in document d that are assigned to topic t,
(d)
Nx is the total number of activation tokens in d, and ? is a hyperparameter
ii. Sample a word token wi from Multinomial(?(zi ) )

Intuitively, in the present application of GC-LDA, each topic corresponds to a functional region of the
brain, where the linguistic features for the topic describe the cognitive processes associated with the
spatial distribution of the topic. The resulting joint distribution of all observed peak activation tokens,
word tokens, and latent parameters for each individual document in the GC-LDA model is as follows:
? (d)
? ? (d)
?
Nx
Nw
Y
Y
p(x, w, z, y, ?) = p(?|?)? ?
p(yi |?(d) )p(xi |?(yi ) )? ? ?
p(zj |y(d) , ?)p(wj |?(zj ) )? (1)
i=1

j=1

Note that when ? = 0, and the spatial distribution for each topic is specified as a single multivariate
Gaussian distribution, the model becomes equivalent to a smoothed version of the Correspondence
LDA model described by Blei & Jordan (2003) [2].2
2

We note that [2] uses a different generative description for how the zi variables are sampled conditional on
(d)
(d)
the yi indicator variables; in [2], zi is sampled uniformly from (1, ..., Ny ), and then wi is sampled from
(d)
the multinomial distribution of the topic yi that zi points to. This ends up being functionally equivalent to
the generative description for zi given here when ? = 0. Additionally, in [2], no prior is put on ?(t) , unlike in
GC-LDA. Therefore, when using GC-LDA with a single multivariate Gaussian and ? = 0, it is equivalent to a
smoothed version of Correspondence-LDA. Dirichlet priors have been demonstrated to be beneficial to model
performance [1], so including a prior on ?(t) in GC-LDA should have a positive impact.

3

?	

?

?	

(C)

(B)

(A)

y

z
w

?	

x
NW

?	

?

y

z
w

?	

x
NW

NX

?	

?	

?1	

?	 ?N	

T

y

z
w

x

c	

NW

NX

D

?	

?

NX

D

?	

?	

?	

?	

T

D

?	

?	

?	

?	

R

?	

?
T

Figure 1: (A) Graphical model for the Generalized Correspondence-LDA model, GC-LDA. (B)
Graphical model for GC-LDA with spatial distributions modeled as a single multivariate Gaussian
(equivalent to a smoothed version of Correspondence-LDA if ? = 0)2 . (C) Graphical model for
GC-LDA with subregions, with spatial distributions modeled as a mixture of multivariate Gaussians
A key aspect of this model is that it induces a correspondence between the number of activation
tokens and the number of word tokens within a document that will be assigned to the same topic. The
hyperparameter ? controls the strength of this correspondence. If ? = 0, then there is zero probability
that a word for document d will be sampled from topic t if no peak activations in d were sampled
from t. As ? becomes larger, this constraint is relaxed. Although intuitively one might want ? to be
zero in order to maximize the correspondence between the spatial and linguistic information, we have
found that setting ? > 0 leads to significantly better model performance. We conjecture that using a
non-zero ? allows the parameter space to be more efficiently explored during inference, and that it
improves the model?s ability to handle data sparsity and noise in high dimensional spaces, similar to
the role that the ? and ? hyperparameters serve in standard LDA [1].
2.2

Versions of GC-LDA Employed in Current Paper

There are multiple reasonable choices for the spatial distribution p(xi |?(yi ) ) in GC-LDA, depending
upon the application and the goals of the modeler. For the purposes of the current paper, we considered
three variants that are motivated by the target application. The first model shown in Figure 1.B
employs a single multivariate Gaussian distribution for each topic?s spatial distribution ? and is
therefore equivalent to a smoothed version of Correspondence-LDA if setting ? = 0. The generative
process for this model is the same as specified above, with generative step (b.ii) modified as follows:
Sample peak activation token xi from from a Gaussian distribution with parameters ?(yi ) and ? (yi ) .
We refer to this model as the ?no-subregions? model.
The second model and third model both employ Gaussian mixtures with R = 2 components for
each topic?s spatial distribution, and are shown in Figure 1.C. Employing a Gaussian mixture gives
the model more flexibility in terms of the types of spatial distributions that can be associated with
a topic. This is notably useful in modeling spatial distributions associated with neural activity, as
it allows the model to learn topics where a single cognitive function (captured by the linguistic
distribution) is associated with spatially discontiguous patterns of activations. In the second GC-LDA
model we present?which we refer to as the ?unconstrained subregions? model?the Gaussian
mixture components are unconstrained. In the third version of GC-LDA?which we refer to as the
?constrained subregions? model?the Gaussian components are constrained to have symmetric means
with respect to their distance from the origin along the horizontal spatial axis (a plane corresponding
to the longitudinal fissure in the brain). This constraint is consistent with results from meta-analyses
of the fMRI literature, where most studied functions display a high degree of bilateral symmetry
[6, 12].
The use of mixture models for representing the spatial distribution in GC-LDA requires the additional
parameters c, ?, and hyperparameter ?, as well as additional modifications to the description of
the generative process. Each topic?s spatial distribution in these models is now associated with a
multinomial probability distribution ? (t) giving the probability of sampling each component r from
(t)
each topic t, where ?r is the probability of sampling the rth component (which we will refer to as a
4

subregion) from the tth topic. Variable ci is an indicator variable that assigns each activation token
xi to a subregion r of the topic to which it is assigned via yi . A full description of the generative
process for these models is provided in Section 1 of the supplementary materials3 .
2.3

Inference for GC-LDA

Exact probabilistic inference for the GC-LDA model is intractable. We employed collapsed Gibbs
sampling for posterior inference ? collapsing out ?(d) , ?(t) , and ? (t) while sampling the indicator
variables yi , zi and ci . Spatial distribution parameters ?(t) are estimated via maximum likelihood.
The per-iteration computational complexity of inference is O(T (NW + NX R)), where T is the
number of topics, R is the number of subregions, and NW and NX are the total number of word
tokens and activation tokens in the corpus, respectively. Details of the inference methods and sampling
equations are provided in Section 2 of the supplement.

3

Experimental Evaluation

We refer to the three versions of GC-LDA described in Section 2 as (1) the ?no subregions? model,
for the model in which each topic?s spatial distribution is a single multivariate Gaussian distribution,
(2) the ?unconstrained subregions? model, for the model in which each topic?s spatial distribution is a
mixture of R = 2 unconstrained Gaussian distributions, and (3) the ?constrained subregions? model,
for the model in which each topic?s spatial distribution is a mixture of R = 2 Gaussian distributions
whose means are constrained to be symmetric along the horizontal spatial dimension with respect to
their distance from the origin.
Our empirical evaluations of the GC-LDA model are based on the application of these models to the
Neurosynth meta-analytic database [12]. We first illustrate and contrast the qualitative properties of
topics that are extracted by the three versions of GC-LDA4 . We then provide a quantitative model
comparison, in which the models are evaluated in terms of their ability to predict held out data. These
results highlight the promise of GC-LDA and this type of modeling for jointly extracting the spatial
extent and cognitive functions of neuroanatomical brain regions.
Neurosynth Database: Neurosynth [12] is a publicly available database consisting of data automatically extracted from a large collection of functional magnetic resonance imaging (fMRI) publications5 .
For each publication, the database contains the abstract text and all reported 3-dimensional peak
activation coordinates (in MNI space) in the study. The text was pre-processed to remove common
stop-words. For the version of the Neurosynth database employed in the current paper, there were
11,362 total publications, which had on average 35 peak activation tokens and 46 word tokens after
preprocessing (corresponding to approximately 400k activation and 520k word tokens in total).
3.1

Visualizing GC-LDA Topics

In Figure 2 we present several illustrative examples of topics for all three GC-LDA variants that we
considered. For each topic, we illustrate the topic?s distribution over word types via a word cloud,
(t)
where the sizes of words are proportional to their probabilities ?w in the model. Each topic?s spatial
distribution over neural activations is illustrated via a kernel-smoothed representation of all activation
tokens that were assigned to the topic, overlaid on an image of the brain. For the models that
represent spatial distributions using Gaussian mixtures (the unconstrained and constrained subregions
models), activations are color-coded based on which subregion they are assigned to, and the mixture
(t)
weights for the subregions ?r are depicted above the activation image on the left. In the constrained
subregions model (where the means of the two Gaussians were constrained to be symmetric along
the horizontal axis) the two subregions correspond to a ?left? and ?right? hemisphere subregion. The
following parameter settings were used for generating the images in Figure 2: T = 200, ? = .1,
? = .01, ? = .01, and for the models with subregions, ? = 1.0.
3

Note that these models are still instances of GC-LDA as presented in Figure 1.1; they can be equivalently
formulated by marginalizing out the ci variables, such that the probability f (xi |?(t) ) depends directly on the
parameters of each component, and the component probabilities given by ? (t) .
4
A brief discussion of the stability of topics extracted by GC-LDA is provided in Section 3 of the supplement
5
Additional details and Neurosynth data can be found at http://neurosynth.org/

5

Figure 2: Illustrative examples of topics extracted for the three GC-LDA variants. Probability
distributions over word types ?(t) are represented via word clouds, where word sizes are proportional
(t)
to ?w . Spatial distributions are illustrated using kernel-smoothed representations of all activation
tokens assigned to each topic. For the models with subregions, each activation token?s color (blue or
red) corresponds to the subregion r that the token is assigned to.
For nearly all of the topics shown in Figure 2, the spatial and linguistic distributions closely correspond
to functional regions that are extensively described in the literature (e.g., motor function in primary
motor cortex; face processing in the fusiform gyrus, etc.). We note that a key feature of all versions
of the GC-LDA model, relative to the majority of existing methods in the literature, is that the
model is able to capture the one-to-many mapping from neural regions onto cognitive functions.
For example, in all model variants, we observe topics corresponding to auditory processing and
language processing (e.g., the topics shown in panels B1 and B3 for the subregions model). While
these cognitive processes are distinct, they have partial overlap with respect to the brain networks
they recruit ? specifically, the superior temporal sulcus in the left hemisphere.
For functional regions that are relatively medial, the no-subregions model is able to capture bilateral
homologues by consolidating them into a single distribution (e.g., the topic shown in A2, which
spans the medial primary somatomotor cortex in both hemispheres). However, for functional regions
that are more laterally localized, the model cannot capture bilateral homologues using a single topic.
For cognitive processes that are highly lateralized (such as language processing, shown in A1, B1
6

and C1) this poses no concern. However, for functional regions that are laterally distant and do have
spatial symmetry, the model ends up distributing the functional region across multiple topics?see,
e.g., the topics shown in A3 and A4 in the no-subregions model, which correspond to the auditory
cortex in the left and right hemisphere respectively. Given that these two topics (and many other pairs
of topics that are not shown) correspond to a single cognitive function, it would be preferable if they
were represented using a single topic. This can potentially be achieved by increasing the flexibility
of the spatial representations associated with each topic, such that the model can capture functional
regions with distant lateral symmetry or other discontiguous spatial features using a single topic. This
motivates the unconstrained and constrained subregions models, in which topic?s spatial distributions
are represented by Gaussian mixtures.
In Figure 2, the topics in panels B3 and C3 illustrate how the subregions models are able to handle
symmetric functional regions that are located on the lateral surface of the brain. The lexical distribution for each of these individual topics in the subregions models is similar to that of both the
topics shown in A3 and A4 of the no-subregions model. However, the spatial distributions in B3 and
C3 each capture a summation of the two topics from the no subregions model. In the case of the
constrained subregion model, the symmetry between the means of the spatial distributions for the
subregions is enforced, while for the unconstrained model the symmetry is data-driven and falls out
of the model.
We note that while the unconstrained subregions model picks up spatial symmetry in a significant
subset of topics, it does not always do so. In the case of language processing (panel A1), the
lack of spatial symmetry is consistent with a large fMRI literature demonstrating that language
processing is highly left-lateralized [11]. And in fact, the two subregions in this topic correspond
approximately to Wernicke?s and Broca?s areas, which are integral to language comprehension and
production, respectively. In other cases, (e.g., the topics in panels B2 and B4), the unconstrained
subregions model partially captures spatial symmetry with a highly-weighted subregion near the
horizontal midpoint, but also has an additional low-weighted region that is lateralized. While this
result is not necessarily wrong per se, it is somewhat inelegant from a neurobiological standpoint.
Moreover, there are theoretical reasons to prefer a model in which subregions are always laterallysymmetrical. Specifically, in instances where the subregions are symmetric (the topic in panel B3
for the unconstrained subregions model and all topics for the constrained subregions model), the
subregion weights provide a measure of the relative lateralization of function. For example, the
language topic in panel C1 of the constrained subregions model illustrates that while there is neural
activation corresponding to linguistic processing in the right hemisphere of the brain, the function is
strongly left-lateralized (and vice-versa for face processing, illustrated in panel C2). By enforcing
(t)
the lateral symmetry in the constrained subregions model, the subregion weights ?r (illustrated
above the left activation images) for each topic inherently correspond to an automated measure of the
lateralization of the topic?s function. Thus, the constrained model produces what is, to our knowledge,
the first data-driven estimation of region-level functional hemispheric asymmetry across the whole
brain.
3.2

Predicting Held Out Data

This section describes quantitative comparisons between three GC-LDA models in terms of their
ability to predict held-out data. We split the Neurosynth dataset into a training and test set, where
approximately 20%j of all kdata in the corpus was put jinto thektest set. For each document, we
(d)
(d)
randomly removed .2Nx
peak activation tokens and .2Nw word tokens from each document.
We trained the models on the remaining data, and then for each model we computed the log-likelihood
of the test data, both for the word tokens and peak tokens.
The space of possible hyperparameters to explore in GC-LDA is vast, so we restrict our comparison
to the aspects of the model which are novel relative to the original Correspondence-LDA model.
Specifically, for all three modelvariants, we compared
	 the log-likelihood of the test data across
different values of ?, where ? ? 0, 0.001, 0.01, 0.1, 1 . We note again here that the no-subregions
model with ? = 0 is equivalent to a smoothed version of Correspondence-LDA [2] (see footnote 2
for additional clarification). The remainder of the parameters were fixed as follows (chosen based on
a combination of precedent from the topic modeling literature and preliminary model exploration):
T = 100, ? = .1, and ? = .01 for all models, and ? = 1.0 for the models with subregions. All
models were trained for 1000 iterations.
7

Figure 3 presents the held out log-likelihoods for all models across different settings of ?, in terms
of (i) the total log-likelihood for both activation tokens and word tokens (left) (ii) log-likelihood
for activation tokens only (middle), and (iii) log likelihood for word tokens only (right). For both
activation tokens and word tokens, for all three versions of GC-LDA, using a non-zero ? leads
to significant improvement in performance. In terms of predicting activation tokens alone, there
is a monotonic relationship between the size of ? and log-likelihood. This is unsurprising, since
increasing ? reduces the extent that word tokens constrain the spatial fit of the model. In terms of
predicting word tokens (and overall log-likelihood), the effect of ? shows an inverted-U function,
with the best performance in the range of .01 to .1. These patterns were consistent across all three
variants of GC-LDA. Taken together, our results suggest that using a non-zero ? results in a significant
improvement over the Correspondence-LDA model.
In terms of comparisons across model variants, we found that both subregions models were significant
improvements over the no-subregions models in terms of total log-likelihood, although the nosubregions model performed slightly better than the constrained subregions model at predicting word
tokens. In terms of the two subregions models, performance is overall fairly similar. Generally,
the constrained subregions model performs slightly better than the unconstrained model in terms
of predicting peak tokens, but slightly worse in terms of predicting word tokens. The differences
between the two subregions models in terms of total log-likelihood were negligible. These results do
not provide a strong statistical case for choosing one subregions model over the other; instead, they
suggest that the modeler ought to choose between models based on their respective theoretical or
qualitative properties (e.g., biological plausibility, as discussed in Section 3.1).
Activations only

Words only

Log-Likelihood

Activations + Words

Figure 3: Log Likelihoods of held out data for the three GC-LDA models as a function of model
parameter ?. Left: total log-likelihood (activation tokens + word tokens). Middle: log-likelihood of
activation tokens only. Right: log-likelihood of word tokens only.

4

Summary

We have presented generalized correspondence LDA (GC-LDA) ? a generalization of the
Correspondence-LDA model, with a focus on three variants that capture spatial properties motivated by neuroimaging applications. We illustrated how this model can be applied to a novel type of
metadata?namely, the spatial peak activation coordinates reported in fMRI publications?and how it
can be used to generate a relatively comprehensive atlas of functional brain regions. Our quantitative
comparisons demonstrate that the GC-LDA model outperforms the original Correspondence-LDA
model at predicting both missing word tokens and missing activation peak tokens. This improvement
was demonstrated in terms of both the introduction of the ? parameter, and with respect to alternative
parameterizations of topics? spatial distributions.
Beyond these quantitative results, our qualitative analysis demonstrates that the model can recover
interpretable topics corresponding closely to known functional regions of the brain. We also showed
that one variant of the model can recover known features regarding the hemispheric lateralization
of certain cognitive functions. These models show promise for the field of cognitive neuroscience,
both for summarizing existing results and for generating novel hypotheses. We also expect that novel
features of GC-LDA can be carried over to other extensions of Correspondence-LDA in the literature.
In future work, we plan to explore other spatial variants of these models that may better capture the
morphological features of distinct brain regions ? e.g., using hierarchical priors that can capture the
hierarchical organization of brain systems. We also hope to improve the model by incorporating
features such as the correlation between topics. Applications and extensions of our approach for
more standard image processing applications may also be a fruitful area of research.
8

References
[1] Arthur Asuncion, Max Welling, Padhraic Smyth, and Yee Whye Teh. On smoothing and inference for
topic models. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,
pages 27?34. AUAI Press, 2009.
[2] David M Blei and Michael I Jordan. Modeling annotated data. In Proceedings of the 26th annual
international ACM SIGIR conference on Research and development in informaion retrieval, pages 127?134.
ACM, 2003.
[3] David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. the Journal of machine
Learning research, 3:993?1022, 2003.
[4] Vince D Calhoun, Jingyu Liu, and T?lay Adal?. A review of group ica for fmri data and ica for joint
inference of imaging, genetic, and erp data. Neuroimage, 45(1):S163?S172, 2009.
[5] Jeremy R Manning, Rajesh Ranganath, Kenneth A Norman, and David M Blei. Topographic factor analysis:
a bayesian model for inferring brain networks from neural data. PloS one, 9(5):e94914, 2014.
[6] Adrian M Owen, Kathryn M McMillan, Angela R Laird, and Ed Bullmore. N-back working memory
paradigm: A meta-analysis of normative functional neuroimaging studies. Human brain mapping, 25(1):46?
59, 2005.
[7] Russell A Poldrack, Jeanette A Mumford, Tom Schonberg, Donald Kalar, Bishal Barman, and Tal Yarkoni.
Discovering relations between mind, brain, and mental disorders using topic mapping. PLoS Comput Biol,
8(10):e1002707, 2012.
[8] Stephen M Smith, Peter T Fox, Karla L Miller, David C Glahn, P Mickle Fox, Clare E Mackay, Nicola
Filippini, Kate E Watkins, Roberto Toro, Angela R Laird, et al. Correspondence of the brain?s functional
architecture during activation and rest. Proceedings of the National Academy of Sciences, 106(31):13040?
13045, 2009.
[9] Bertrand Thirion, Ga?l Varoquaux, Elvis Dohmatob, and Jean-Baptiste Poline. Which fmri clustering gives
good brain parcellations? Frontiers in neuroscience, 8(167):13, 2014.
[10] David C Van Essen, Stephen M Smith, Deanna M Barch, Timothy EJ Behrens, Essa Yacoub, Kamil Ugurbil,
WU-Minn HCP Consortium, et al. The wu-minn human connectome project: an overview. Neuroimage,
80:62?79, 2013.
[11] Mathieu Vigneau, Virginie Beaucousin, Pierre-Yves Herve, Hugues Duffau, Fabrice Crivello, Olivier
Houde, Bernard Mazoyer, and Nathalie Tzourio-Mazoyer. Meta-analyzing left hemisphere language areas:
phonology, semantics, and sentence processing. Neuroimage, 30(4):1414?1432, 2006.
[12] Tal Yarkoni, Russell A Poldrack, Thomas E Nichols, David C Van Essen, and Tor D Wager. Large-scale
automated synthesis of human functional neuroimaging data. Nature methods, 8(8):665?670, 2011.
[13] BT Thomas Yeo, Fenna M Krienen, Simon B Eickhoff, Siti N Yaakub, Peter T Fox, Randy L Buckner,
Christopher L Asplund, and Michael WL Chee. Functional specialization and flexibility in human
association cortex. Cerebral Cortex, page bhu217, 2014.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 3276-hidden-common-cause-relations-in-relational-learning.pdf

Hidden Common Cause Relations in
Relational Learning

Ricardo Silva?
Gatsby Computational Neuroscience Unit
UCL, London, UK WC1N 3AR
rbas@gatsby.ucl.ac.uk

Wei Chu
Center for Computational Learning Systems
Columbia University, New York, NY 10115
chuwei@cs.columbia.edu

Zoubin Ghahramani
Department of Engineering
University of Cambridge, UK CB2 1PZ
zoubin@eng.cam.ac.uk

Abstract
When predicting class labels for objects within a relational database, it is often
helpful to consider a model for relationships: this allows for information between
class labels to be shared and to improve prediction performance. However, there
are different ways by which objects can be related within a relational database.
One traditional way corresponds to a Markov network structure: each existing
relation is represented by an undirected edge. This encodes that, conditioned on
input features, each object label is independent of other object labels given its
neighbors in the graph. However, there is no reason why Markov networks should
be the only representation of choice for symmetric dependence structures. Here
we discuss the case when relationships are postulated to exist due to hidden common causes. We discuss how the resulting graphical model differs from Markov
networks, and how it describes different types of real-world relational processes.
A Bayesian nonparametric classification model is built upon this graphical representation and evaluated with several empirical studies.

1 Contribution
Prediction problems, such as classification, can be easier when class labels share a sort of relational
dependency that is not accounted by the input features [10]. If the variables to be predicted are attributes of objects in a relational database, such dependencies are often postulated from the relations
that exist in the database. This paper proposes and evaluates a new method for building classifiers
that uses information concerning the relational structure of the problem.
Consider the following standard example, adapted from [3]. There are different webpages, each
one labeled according to some class (e.g., ?student page? or ?not a student page?). Features such
as the word distribution within the body of each page can be used to predict each webpage?s class.
However, webpages do not exist in isolation: there are links connecting them. Two pages having a
common set of links is evidence for similarity between such pages. For instance, if W1 and W3 both
link to W2 , this is commonly considered to be evidence for W1 and W3 having the same class. One
way of expressing this dependency is through the following Markov network [5]:
?

Now at the Statistical Laboratory, University of Cambridge. E-mail: silva@statslab.cam.ac.uk

F1

F2

F3

C1

C2

C3

Here Fi are the features of page Wi , and Ci is its respective page label. Other edges linking F
variables to C variables (e.g., F1 ?C2 ) can be added without affecting the main arguments presented
in this section. The semantics of the graph, for a fixed input feature set {F1 , F2 , F3 }, are as follows:
C1 is marginally dependent on C3 , but conditionally independent given C2 . Depending on the
domain, this might be either a suitable or unsuitable representation of relations. For instance, in some
domains it could be the case that the most sensible model would state that C1 is only informative
about C3 once we know what C2 is: that is, C1 and C3 are marginally independent, but dependent
given C2 . This can happen if the existence of a relation (Ci , Cj ) corresponds to the existence of
hidden common causes generating this pair of random variables.
Consider the following example, loosely based on a problem described by [12]. We have three
objects, Microsoft (M ), Sony (S) and Philips (P ). The task is a regression task where we want
to predict the stock market price of each company given its profitability from last year. The given
relationships are that M and S are direct competitors (due to the videogame console market), as
well S and P (due to the TV set market).
M.Profit

S.Profit

P.Profit

M.Profit

S.Profit

P.Profit

M.Profit

S.Profit

P.Profit

M.Stock

S.Stock

P.Stock

M.Stock

S.Stock

P.Stock

M.Stock

S.Stock

P.Stock

?m

?s

?p

?m

?s

?p

(a)

(b)

(c)

Figure 1: (a) Assumptions that relate Microsoft, Sony and Philips stock prices through hidden common cause mechanisms, depicted as unlabeled gray vertices; (b) A graphical representation for
generic hidden common causes relationships by using bi-directed edges; (c) A depiction of the same
relationship skeleton by a Markov network model, which has different probabilistic semantics.
It is expected that several market factors that affect stock prices are unaccounted by the predictor
variable Past Year Profit. For example, a shortage of Microsoft consoles is a hidden common factor for both Microsoft?s and Sony?s stock. Another hidden common cause would be a high price
for Sony?s consoles. Assume here that these factors have no effect on Philips? stock value. A depiction of several hidden common causes that correpond to the relations Competitor(M, S) and
Competitor(S, P ) is given in Figure 1(a) as unlabeled gray vertices.
Consider a linear regression model for this setup. We assume that for each object Oi ? {M, S, P },
the stock price Oi .Stock, centered at the mean, is given by
Oi .Stock = ? ? Oi .P rof it + i

(1)

where each i is a Gaussian random variable.
The fact that there are several hidden common causes between M and S can be modeled by the
covariance of m and s , ?ms . That is, unlike in standard directed Gaussian models, ?ms is allowed
to be non-zero. The same holds for ?sp . Covariances of error terms of unrelated objects should
be zero (?mp = 0). This setup is very closely related to the classic seemingly unrelated regression
model popular in economics [12].
A graphical representation for this type of model is the directed mixed graph (DMG) [9, 11], with
bi-directed edges representing the relationship of having hidden common causes between a pair
of vertices. This is shown in Figure 1(b). Contrast this to the Markov network representation in
Figure 1(c). The undirected representation encodes that m and p are marginally dependent, which

does not correspond to our assumptions1 . Moreover, the model in Figure 1(b) states that once
we observe Sony?s stock price, Philip?s stocks (and profit) should have a non-zero association with
Microsoft?s profit: this follows from a extension of d-separation to DMGs [9]. This is expected from
the assumptions (Philip?s stocks should tell us something about Microsoft?s once we know Sony?s,
but not before it), but does not hold in the graphical model in Figure 1(c). While it is tempting
to use Markov networks to represent relational models (free of concerns raised by cyclic directed
representations), it is clear that there are problems for which they are not a sensible choice.
This is not to say that Markov networks are not the best representation for large classes of relational
problems. Conditional random fields [4] are well-motivated Markov network models for sequence
learning. The temporal relationship is closed under marginalization: if we do not measure some steps
in the sequence, we will still link the corresponding remaining vertices accordingly, as illustrated in
Figure 2. Directed mixed graphs are not a good representation for this sequence structure.
X1

X2

X3

X4

X5

Y1

Y2

Y3

Y4

Y5

X1

X2

X3

X4

X5

Y1

Y2

Y3

Y4

Y5

(a)

(b)

X1

X3

X5

Y1

Y3

Y5

(c)

Figure 2: (a) A conditional random field (CRF) graph for sequence data; (b) A hypothetical scenario
where two of the time slices are not measured, as indicated by dashed boxes; (c) The resulting CRF
graph for the remaining variables, which corresponds to the same criteria for construction of (a).
To summarize, the decision between using a Markov network or a DMG reduces to the following
modeling issue: if two unlinked object labels yi , yj are statistically associated when some chain
of relationships exists between yi and yj , then the Markov network semantics should apply (as in
the case for temporal relationships). However, if the association arises only given the values of the
other objects in the chain, then this is accounted by the dependence semantics of the directed mixed
graph representation. The DMG representation propagates training data information through other
training points. The Markov network representation propagates training data information through
test points. Propagation through training points is relevant in real problems. For instance, in a
webpage domain where each webpage has links to pages of several kinds (e.g., [3]), a chain of
intermediated points between two classes labels yi and yj is likely to be more informative if we
know the values of the labels in this chain. The respective Markov network would ignore all training
points in this chain besides the endpoints.
In this paper, we introduce a non-parametric classification model for relational data that factorizes
according to a directed mixed graph. Sections 2 and 3 describes the model and contrasts it to a
closely related approach which bears a strong analogy to the Markov network formulation. Experiments in text classification are described in Section 4.

2 Model
Chu et al. [2] describe an approach for Gaussian process classification using relational information,
which we review and compare to our proposed model.
Previous approach: relational Gaussian processes through indicators ? For each point x
in the input space X , there is a corresponding function value fx . Given observed input points
x1 , x2 , . . . , xn , a Gaussian process prior over f = [f1 , f2 , . . . , fn ]T has the shape
P(f ) =

1
(2?)n/2 |?|1/2



1 T ?1
exp ? f ? f
2

(2)

1
For Gaussian models, the absence of an edge in the undirected representation (i.e., Gaussian Markov
random fields) corresponds to a zero entry in the inverse covariance matrix, where in the DMG it corresponds
to a zero in the covariance matrix [9].

X1

X2

f1

f2
? 12

X3

X1

X2

X3

X1

X2

X3

f3

f1

f2

f3

f1

f2

f3

Y2

Y3

? 23

Y1

Y2

Y3

Y1

Y2

Y3

Y1

?1

?2

?3

?1

?2

?3

?1

(a)

?1

(b)

?2
?2

?3
?3

(c)

Figure 3: (a) A prediction problem where y3 is unknown and the training set is composed of other
two datapoints. Dependencies between f1 , f2 and f3 are given by a Gaussian process prior and not
represented in the picture. Indicators ?ij are known and set to 1; (b) The extra associations that
arise by conditioning on ? = 1 can be factorized as the Markov network model here depicted, in the
spirit of [9]; (c) Our proposed model, which ties the error terms and has origins in known statistical
models such as seemingly unrelated regression and structural equation models [11].
where the ijth entry of ? is given by a Mercer kernel function K(xi , xj ) [8].
The idea is to start from a standard Gaussian process prior, and add relational information by conditioning on relational indicators. Let ?ij be an indicator that assumes different values, e.g., 1 or 0.
The indicator values are observed for each pair of data points (xi , xj ): they are an encoding of the
given relational structure. A model for P (?ij = 1|fi , fj ) is defined. This evidence is incorporated
into the Gaussian process by conditioning on all indicators ?ij that are positive. Essentially, the idea
boils down to using P(f |? = 1) as the prior for a Gaussian process classifier. Figure 3(a) illustrates a problem with datapoints {(x1 , y1 ), (x2 , y2 ), (x3 , y3 )}. Gray vertices represent unobserved
variables. Each yi is a binary random variable, with conditional probability given by
P(yi = 1|fi ) = ?(fi /?)

(3)

where ?(?) is the standard normal cumulative function and ? is a hyperparameter. This can be
interpreted as the cumulative distribution of fi + i , where fi is given and i is a normal random
variable with zero mean and variance ? 2 .
In the example of Figure 3(a), one has two relations: (x1 , x2 ), (x2 , x3 ). This information is incorporated by conditioning on the evidence (?12 = 1, ?23 = 1). Observed points (x1 , y1 ), (x2 , y2 ) form
the training set. The prediction task is to estimate y3 . Notice that ?12 is not used to predict y3 : the
Markov blanket for f3 includes (f1 , f2 , ?23 , y3 , 3 ) and the input features. Essentially, conditioning
on ? = 1 corresponds to a pairwise Markov network structure, as depicted in Figure 3(b) [9]2 .
Our approach: mixed graph relational model ? Figure 3(c) illustrates our proposed setup. For
reasons that will become clear in the sequel, we parameterize the conditional probability of yi as
?
(4)
P(yi = 1|gi , vi ) = ?(gi / vi )
where gi = fi + ?i . As before, Equation (4) can be interpreted as the cumulative distribution of
gi + ?i , with ?i as a normal random variable with zero mean and variance vi = ? 2 ? ??2i , the last
term being the variance of ?i . That is, we break the original error term as i = ?i + ?i , where ?i
and ?j are independent for all i 6= j. Random vector ? is a multivariate normal with zero mean and
covariance matrix ?? . The key aspect in our model is that the covariance of ?i and ?j is non-zero
only if objects i and j are related (that is, bi-directed edge yi ? yj is in the relational graph).
Parameterizing ?? for relational problems is non-trivial and discussed in the next section.
In the example of Figure 3, one noticeable difference of our model 3(c) to a standard Markov network
models 3(b) is that now the Markov blanket for f3 includes error terms for all variables (both  and
? terms), following the motivation presented in Section 1.
2
In the figure, we are not representing explicitly that f1 , f2 and f3 are not independent (the prior covariance matrix ? is complete). The figure is meant as a representation of the extra associations that arise when
conditioning on ? = 1, and the way such associations factorize.

As before, the prior for f in our setup is the Gaussian process prior (2). This means that g has the
following Gaussian process prior (implicitly conditioned on x):
P(g) =

1
(2?)n/2 |R|1/2



1
exp ? g> R?1 g
2



(5)

where R = K + ?? is the covariance matrix of g = f + ?, with Kij = K(xi , xj ).

3 Parametrizing a mixed graph model for relational classification
For simplicity, in this paper we will consider only relationships that induce positive associations
between labels. Ideally, the parameterization of ?? has to fulfill two desiderata: (i). it should respect
the marginal independence constraints as encoded by the graphical model (i.e., zero covariance for
vertices that are not adjacent), and be positive definite; (ii). it has to be parsimonious in order to
facilitate hyperparameter selection, both computationally and statistically. Unlike the multivariate
analysis problems in [11], the size of our covariance matrix grows with the number of data points.
As shown by [11], exact inference in models with covariance matrices with zero-entry constraints is
computationally demanding. We provide two alternative parameterizations that are not as flexible,
but which lead to covariance matrices that are simple to compute and easy to implement. We will
work under the transductive scenario, where training and all test points are given in advance. The
corresponding graph thus contain unobserved and observed label nodes.
3.1 Method I
The first method is an automated method to relax some of the independence constraints, while
guaranteeing positive-definiteness, and a parameterization that depends on a single scalar ?. This
allows for more efficient inference and is done as follows:
1. Let G? be the corresponding bi-directed subgraph of our original mixed graph, and let U0
be a matrix with n ? n entries, n being the number of nodes in G?

2. Set U0ij to be the number of cliques in G? where yi and yj appear together;
3. Set U0ii to be the number of cliques containing yi , plus a small constant ?;

4. Set U to be the corresponding correlation matrix obtained by intepreting U0 as a covariance
matrix and rescaling it;
Finally, set ?? = ?U, where ? ? [0, 1] is a given hyperparameter. Matrix U is always guaranteed to
be positive definite: it is equivalent to obtaining the covariance matrix of y from a linear latent variable model, where there is an independent standard Gaussian latent variable as a common parent to
every clique, and every observed node yi is given by the sum of its parents plus an independent error
term of variance ?. Marginal independencies are respected, since independent random variables
will never be in a same clique in G? . In practice, this method cannot be used as is since the number
of cliques will in general grow at an exponential rate as a function of n. Instead, we first triangulate
the graph: in this case, extracting cliques can be done in polynomial time. This is a relaxation of the
original goal, since some of the original marginal independence constraints will not be enforced due
to the triangulation3.
3.2 Method II
The method suggested in the previous section is appealing under the assumption that vertices that
appear in many common cliques are more likely to have more hidden common causes, and hence
should have stronger associations. However, sometimes the triangulation introduces bad artifacts,
with lots of marginal independence constraints being violated. In this case, this will often result in
a poor prediction performance. A cheap alternative approach is not generating cliques, and instead
3

The need for an approximation is not a shortcoming only of the DMG approach. Notice that the relational
Gaussian process of [2] also requires an approximation of its relational kernel.

10

10

10

20

20

20

30

30

30

40

40

40

50

50

50

60

60

60

70

70

70

80

80

80

90

90

90

100

100

100

10

20

30

40

50

60

(a)

70

80

90

100

10

20

30

40

50

60

70

80

90

(b)

100

10

20

30

40

50

60

70

80

90

100

(c)

Figure 4: (a) The link matrix for the political books dataset. (b) The relational kernel matrix obtained
with the approximated Method I. (c) The kernel matrix obtained with Method II, which tends to
produce much weaker associations but does not introduce spurious relations.
getting a marginal covariance matrix from a different latent variable model. In this model, we
create an independent standard Gaussian variable for each edge yi ? yj instead of each clique. No
triangulation will be necessary, and all marginal independence constraints will be respected. This,
however, has shortcomings of its own: for all pairs (yi , yj ) connected by an edge, it will be the case
that U0ij = 1, while U0ii can be as large as n. This means that the resulting correlation in Uij can be
close to zero even if yi and yj are always in the same cliques. In Section 4, we will choose between
Methods I and II according to the marginal likelihood of the model.
3.3 Algorithm
Recall that our model is a Gaussian process classifier with error terms i of variance ? such that
i = ?i + ?i . Without loss of generality, we will assume that ? = 1. This results in the following
parameterization of the full error covariance matrix:
? = (1 ? ?)I + ?U

(6)

where I is an n ? n identity matrix. Matrix (1 ? ?)I corresponds to the covariance matrix ?? .

The usefulness of separating  as ? and ? becomes evident when we use an expectation-propagation
(EP) algorithm [7] to perform inference in our relational classifier. Instead of approximating the
posterior of f , we approximate the posterior density P(g|D), D = {(x1 , y1 ), . . . , Q
(xn , yn )} being
the given training data. The approximate posterior has the form Q(g) ? P(g) i t?i (gi ) where
P(g) is the Gaussian process prior with kernel matrix R = K + ?? as defined in the previous
section. Since the covariance matrix
Qn ?? is diagonal, the true likelihood of y given g factorizes
over each datapoint: P(y|g) = i=1 P(yi |gi ), and standard EP algorithms for Gaussian process
classification can be used [8] (with the variance given by ?? instead of ? , and kernel matrix R
instead of K).
The final algorithm defines a whole new class of relational models, depends on a single hyperparameter ? which can be optimized by grid search in [0, 1], and requires virtually no modification of
code written for EP-based Gaussian process classifiers4 .

4 Results
We now compare three different methods in relational classification tasks. We will compare a
standard Gaussian process classifier (GPC), the relational Gaussian process (RGP) of [2] and our
method, the mixed graph Gaussian process (XGP). A linear kernel K(x, z) = x ? z is used, as described by [2]. We set ? = 10?4 and the hyperparameter ? is found by a grid search in the space
{0.1, 0.2, 0.3, . . . , 1.0} maximizing the approximate EP marginal likelihood5.
4

We provide MATLAB/Octave code for our method in http://www.statslab.cam.ac.uk/?silva.
For triangulation, we used the MATLAB implementation of the Reverse Cuthill McKee vertex ordering
available at http://people.scs.fsu.edu/?burkardt/m src/rcm/rcm.html
5

Table 1: The averaged AUC scores of citation prediction on test cases of the Cora database are
recorded along with standard deviation over 100 trials. ?n? denotes the number of papers in one
class. ?Citations? denotes the citation count within the two paper classes.
Group
n
Citations
GPC
GPC with Citations
XGP
5vs1
346/488
2466
0.905 ? 0.031
0.891 ? 0.022
0.945 ? 0.053
5vs2
346/619
3417
0.900 ? 0.032
0.905 ? 0.044
0.933 ? 0.059
5vs3 346/1376
3905
0.863 ? 0.040
0.893 ? 0.017
0.883 ? 0.013
5vs4
346/646
2858
0.916 ? 0.030
0.887 ? 0.018
0.951 ? 0.042
5vs6
346/281
1968
0.887 ? 0.054
0.843 ? 0.076
0.955 ? 0.041
5vs7
346/529
2948
0.869 ? 0.045
0.867 ? 0.041
0.926 ? 0.076
4.1 Political books
We consider first a simple classification problem where the goal is to classify whether a particular book is of liberal political inclination or not. The features of each book are given
by the words in the Amazon.com front page for that particular book. The choice of books,
labels, and relationships are given in the data collected by Valdis Krebs and available at
http://www-personal.umich.edu/ mejn/netdata. The data containing book features can be found at
http://www.statslab.cam.ac.uk/?silva. There are 105 books, 43 of which are labeled as liberal books.
The relationships are pairs of books which are frequently purchased together by a same customer.
Notice this is an easy problem, where labels are strongly associated if they share a relationship.
We performed evaluation by sampling 100 times from the original pool of books, assigning half of
them as trainining data. The evaluation criterion was the area under the curve (AUC) for this binary
problem. This is a problem where Method I is suboptimal. Figure 4(a) shows the original binary
link matrix. Figure 4(b) depicts the corresponding U0 matrix obtained with Method I, where entries
closer to red correspond to stronger correlations. Method II gives a better performance here (Method
I was better in the next two experiments). The AUC result for GPC was of 0.92, while both RGP
and XGP achieved 0.98 (the difference between XGP and GPC having a std. deviation of 0.02).
4.2 Cora
The Cora collection [6] contains over 50,000 computer science research papers including bibliographic citations. We used a subset in our experiment. The subset consists of 4,285 machine learning
papers categorized into 7 classes. The second column of Table 1 shows the class sizes. Each paper
was preprocessed as a bag-of-words, a vector of ?term frequency? components scaled by ?inverse
document frequency?, and then normalized to unity length. This follows the pre-processing used in
[2]. There is a total of 20,082 features. For each class, we randomly selected 1% of the labelled
samples for training and tested on the remainder. The partition was repeated 100 times. We used
the fact that the database is composed of fairly specialized papers as an illustration of when XGP
might not be as optimal as RGP (whose AUC curves are very close to 1), since the population of
links tends to be better separated between different classes (but this is also means that the task is
fairly easy, and differences disappear very rapidly with increasing sample sizes). The fact there is
very little training data also favors RGP, since XGP propagates information through training points.
Still, XGP does better than the non-relational GPC. Notice that adding the citation adjacency matrix
as a binary input feature for each paper does not improve the performance of the GPC, as shown in
Table 1. Results for other classes are of similar qualitative nature and not displayed here.
4.3 WebKB
The WebKB dataset consists of homepages from 4 different universities: Cornell, Texas, Washington
and Wisconsin [3]. Each webpage belongs to one out of 7 categories: student, professor, course,
project, staff, department and ?other?. The relations come from actual links in the webpages. There
is relatively high heterogeneity of types of links in each page: in terms of mixed graph modeling,
this linkage mechanism is explained by a hidden common cause (e.g., a student and a course page
are associated because that person?s interest in enrolling as a student also creates demand for a
course). The heterogeneity also suggests that two unlinked pages should not, on average, have an
association if they link to a common page W . However, observing the type of page W might create

Table 2: Comparison of the three algorithms on the task ?other? vs. ?not-other? in the WebKB
domain. Results for GPC and RGP taken from [2]. The same partitions for training and test are used
to generate the results for XGP. Mean and standard deviation of AUC results are reported.
University
Numbers
Other or Not
Other
All
Link
GPC
RGP
XGP
Cornell
617
865 13177 0.708 ? 0.021 0.884 ? 0.025 0.917 ? 0.022
Texas
571
827 16090 0.799 ? 0.021 0.906 ? 0.026 0.949 ? 0.015
Washington
939
1205 15388 0.782 ? 0.023 0.877 ? 0.024 0.923 ? 0.016
Wisconsin
942
1263 21594 0.839 ? 0.014 0.899 ? 0.015 0.941 ? 0.018
the association. We compare how the three algorithms perform when trying to predict if a webpage
is of class ?other? or not (the other classifications are easier, with smaller differences. Results are
omitted for space purposes). The proportion of ?other? to non-?other? is about 4:1, which makes the
area under the curve (AUC) a more suitable measure of success. We used the same 100 subsamples
from [2], where 10% of the whole data is sampled from the pool for a specific university, and the
remaining is used for test. We also used the same features as in [2], pre-processed as described in the
previous section. The results are shown in Table 2. Both relational Gaussian processes are far better
than the non-relational GPC. XGP gives significant improvements over RGP in all four universities.

5 Conclusion
We introduced a new family of relational classifiers by extending a classical statistical model [12]
to non-parametric relational classification. This is inspired by recent advances in relational Gaussian processes [2] and Bayesian inference for mixed graph models [11]. We showed empirically
that modeling the type of latent phenomena that our approach postulates can sometimes improve
prediction performance in problems traditionally approached by Markov network structures.
Several interesting problems can be treated in the future. It is clear that there are many different ways
by which the relational covariance matrix can be parameterized. Intermediate solutions between
Methods I and II, approximations through matrix factorizations and graph cuts are only a few among
many alternatives that can be explored. Moreover, there is a relationship between our model and
multiple kernel learning [1], where one of the kernels comes from error covariances. This might
provide alternative ways of learning our models, including multiple types of relationships.
Acknowledgements: We thank Vikas Sindhwani for the preprocessed Cora database.

References
[1] F. Bach, G. Lanckriet, and M. Jordan. Multiple kernel learning, conic duality, and the SMO algorithm.
21st International Conference on Machine Learning, 2004.
[2] W. Chu, V. Sindhwani, Z. Ghahramani, and S. Keerthi. Relational learning with Gaussian processes.
Neural Information Processing Systems, 2006.
[3] M. Craven, D. DiPasquo, D. Freitag, A. McCallum, T. Mitchell, K. Nigam, and S. Slattery. Learning to
extract symbolic knowledge from the World Wide Web. Proceedings of AAAI?98, pages 509?516, 1998.
[4] J. Lafferty, A. McCallum, and F. Pereira. Conditional random fields: Probabilistic models for segmenting
and labeling sequence data. 18th International Conference on Machine Learning, 2001.
[5] S. Lauritzen. Graphical Models. Oxford University Press, 1996.
[6] A. McCallum, K. Nigam, J. Rennie, and K. Seymore. Automating the construction of Internet portals
with machine learning. Information Retrieval Journal, 3:127?163, 2000.
[7] T. Minka. A family of algorithms for approximate Bayesian inference. PhD Thesis, MIT, 2001.
[8] C. Rasmussen and C. Williams. Gaussian Processes for Machine Learning. MIT Press, 2006.
[9] T. Richardson and P. Spirtes. Ancestral graph Markov models. Annals of Statistics, 30:962?1030, 2002.
[10] P. Sen and L. Getoor. Link-based classification. Report CS-TR-4858, University of Maryland, 2007.
[11] R. Silva and Z. Ghahramani. Bayesian inference for Gaussian mixed graph models. UAI, 2006.
[12] A. Zellner. An efficient method of estimating seemingly unrelated regression equations and tests for
aggregation bias. Journal of the American Statistical Association, 1962.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

