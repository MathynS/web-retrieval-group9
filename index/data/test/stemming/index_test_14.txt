query sentence: Associative database
---------------------------------------------------------------------
title: 1-self-organization-of-associative-database-and-its-applications.pdf

self-organ associ databas applic hisashi suzuki suguru arimoto osaka univers toyonaka osaka japan abstract effici method self-organ associ databas propos togeth applic robot eyesight system propos databas associ input output first half part discuss algorithm self-organ propos aspect hardwar produc new style neural network latter half part applic handwritten letter recognit autonom mobil robot system demonstr introduct let map given here finit infinit set anoth finit infinit set learn machin observ set pair sampl random y. mean cartesian product comput estim make small estim error measur usual say faster decreas estim error increas number sampl better learn machin howev express perform incomplet sinc lack consider candid assum preliminarili find good learn machin clarifi concept let us discuss type learn machin let us advanc understand self-organ associ databas paramet type ordinari type learn machin assum equat relat x 's 's paramet indefinit name structur equival defin implicit set candid subset map comput valu paramet base observ sampl call type paramet type learn machin defin well approach number sampl increas altern case howev estim error remain etern thus problem design learn machin return find proper structur sens hand assum structur demand compact possibl achiev fast learn word number paramet small sinc paramet uniqu determin even though observ sampl howev demand proper contradict compact consequ paramet type better compact assum structur proper better learn machin elementari concept design learn machin univers ordinari neural network suppos suffici knowledg given though unknown case compar easi find proper compact structur j altern case howev sometim difficult possibl solut give compact assum almighti structur cover various combin orthogon base infinit dimens structur neural network approxim obtain truncat finit dimens implement american institut physic main topic design neural network establish desir structur work includ develop practic procedur comput valu coeffici observ sampl discuss flourish sinc mani effici method propos recent even hardwar unit comput coeffici parallel speed-up sold anza mark iii odyssey e-1 nevertheless neural network alway exist danger error remain etern estim precis speak suppos combin base finit number defin structur essenti word suppos locat near f. case estim error none neglig howev distant estim error never becom neglig inde mani research report follow situat appear complex onc estim error converg valu number sampl increas decreas hard even though dimens heighten properti sometim consider defect neural network recursi type recurs type found anoth methodolog learn follow initi stage sampl set fa instead notat candid equal set map observ first sampl yl fa reduc fi i xt yl i f. observ second sampl fl reduc f2 i xt yl y2 i f. thus candid set becom gradual small observ sampl proceed observ i-sampl write one likelihood estim select henc contrarili paramet type recurs type guarante sure approach number sampl increas recurs type observ sampl yd rewrit valu x 's correl sampl henc type architectur compos rule rewrit free memori space architectur form natur kind databas build manag system data self-organ way howev databas differ ordinari one follow sens record sampl alreadi observ comput estim call databas associ databas first subject construct associ databas establish rule rewri ting purpos adap measur call dissimilari ty here dissimilari ty mean map real whenev howev necessarili defin singl formula defin exampl collect rule written form dissimilar defin structur local henc even though knowledg imperfect flect heurist way henc contrarili neural network possibl acceler speed learn establish well especi easili find simpl 's l 's process analog inform like human see applic paper recurs type show strong effect denot sequenc observ sampl yd one simplest construct associ databas after observ i-sampl follow i algorithm initi stage let empti set everi let equal such min furthermor add produc sa anoth version improv econom memori follow algorithm initi stage let so compos arbitrari element everi let ii-lex equal such si-l min furthermor ii-l xi yi then let si si-l add yi si-l produc si si si-l either construct ii approach increas howev comput time grow proport size si second subject construct associ databas address rule employ econom comput time subsequ chapter construct associ databas purpos propos manag data form binari tree self-organ associ databas given sampl sequenc algorithm construct associ databas follow algorithm step i initi let x root y root yd here variabl assign respect node memor data furthermor let step increas put after reset pointer root repeat follow arriv termin node leaf notat nand xt let mean descend node otherwis let step display yin relat inform next put yin back step otherwis first establish new descend node second let yin yin final back step here loop step stop time also continu now suppos gate element name artifici synaps play role branch prepar then obtain new style neural network gate element random connect algorithm letter recognit recen tli vertic slit method recogn typograph english letters3 elast match method recogn hand written discret english letters4 global train fuzzi logic search method recogn chines charact written squar style etc publish self-organ associ databas realiz recognit handwritten continu english letter nov xk la.t dw1lo sourc document window number sampl nualber sampl es experi result imag scanner take document imag letter recogn use parallelogram window least cover maxim letter process sequenc letter shift window recogn scan word slant direct place window so left vicin may first black point detect then window catch letter part succeed letter recognit head letter perform end posit name boundari line two letter becom known henc start scan boundari repeat oper recogn accomplish recurs task thus major problem come identifi head letter window consid defin follow regard window imag defin accord denot black point left area boundari window imag project onto window imag then measur euclidean distanc fj black point closest b let summat black point b 's divid number b 's regard coupl read posit boundari defin accord oper teach recogn interact relat window imag read boundari algorithm precis if recal read incorrect oper teach correct read via consol moreov if boundari posit incorrect teach correct posit via mous show partial document imag use experi show chang number node recognit rate defin relat frequenc correct answer past trial speciiic window height width slant angular exampl level tree distribut time recognit rate converg experiment recognit rate converg case rare case howev attain sinc distinguish excess lluctuat write if consist y-relat assur like number node increas endless henc clever stop learn recognit rate attain upper limit improv recognit rate must consid spell word one futur subject obstacl avoid movement various system camera type autonom mobil robot report flourishingly6-1o system made author also belong categori now mathemat methodolog solv usual problem obstacl avoid movement cost minim problem cost criterion establish artifici contrarili self-organ associ databas reproduc faith cost criterion oper therefor motion robot after learn becom natur now length width height robot weight visual angl camera robot follow three factor motion turn less advanc less control speed less experi done passageway wid th insid build author laboratori exist becaus experiment intent arrang box smoke stand gas cylind stool handcart etc passag way random let robot take imag camera recal similar imag trace rout preliminarili record this purpos we defin follow let camera face 28deg downward take an imag process it low pass filter scan vertic filter imag bottom top search first point lumin chang excess then su bstitu te point bottom white point top black if obstacl exist front robot white area show area robot move around regard binari 32dot imag process thus defin accord everi let number black point exclusive-or imag regard y 's imag obtain draw rout imag defin accord robot superimpos current camera imag rout recal inquir oper instruct oper judg subject whether suggest rout appropri negat answer draw desir rout mous teach new robot this opera.t defin implicit sampl sequenc reflect cost criterion oper iibub roan stationari uni configur autonom mobil robot system i north rmbi ie unit robot roan experiment environ wall camera imag preprocess fa preprocess cours suggest ion search process for obstacl avoid movement process for posit identif we defin satisfact rate relat frequenc accept suggest rout in past trial in typic experi chang satisfact rate show similar tendenc it attain around time here notic rest mean direct percentag collis in practic we prevent collis adopt supplementari measur at time number node level tree distribut in propos method reflect delic various charact oper for exampl robot train an oper move slowli enough space obstacl one train anoth oper brush quick obstacl this fact give us hint method print charact machin posit identif robot identifi posit recal similar landscap posit data camera imag for this purpos in principl it suffic regard camera imag posit data x 's respect howev memori capac finit in actual compu ter henc we compress camera imag at slight loss inform such compress admitt long precis posit identif in an accept area thus major problem come find suitabl compress method in experiment environ jut passageway at interv section adjac jut at one door robot identifi rough surround landscap section place in it use temporarili triangular survey techniqu if an exact measur necessari realiz former task we defin follow turn camera take panorama imag scan horizont center line substitut point lumin excess chang for black point for white regard binari line imag process thus defin accord for everi project black point onto measur euclidean distanc black point a closest a let summat s. similar calcul exchang role denot number a 's and a 's respect nand defin regard posit integ label section y 's and defin accord in learn mode robot check exact it posit a counter reset period oper robot run arbitrarili passageway within area and learn relat landscap and posit data posit identif beyond area achiev cross plural databas one anoth this task automat except period reset counter name it a kind learn without teacher we defin identif rate relat frequenc correct recal posit data in past trial in a typic exampl it converg around time at time the number level and the level oftre distribut in sinc the identif failur reject consid the trajectori pro blem aris in practic use in order improv the identif rate the compress ratio camera imag must loosen such possibl depend improv the hardwar in the futur show an exampl actual motion the robot base the databas for obstacl avoid movement and for posit identif this exampl correspond a case move from in here the time interv per frame i i actual motion the robot conclus a method self-organ associ databas propos the applic robot eyesight system the machin decompos a global structur unknown a set local structur known and learn univers input-output respons this framework problem impli a wide applic area the exampl shown in this paper a defect the algorithm self-organ that the tree balanc well for a subclass structur a subject impos us to widen the class a probabl solut to abolish the address rule depend direct valu and instead to establish anoth rule depend on the distribut function of valu of it now investig
----------------------------------------------------------------

title: 81-invariant-object-recognition-using-a-distributed-associative-memory.pdf

invari object recognit use distribut associ memori harri wechsler georg lee zimmerman depart electr engin univers minnesota minneapoli mn abstract paper describ approach 2-dimension object recognit complex-log conform map combin distribut associ memori creat system recogn object regardless chang rotat scale recal inform memor databas use classifi object reconstruct memor version object estim magnitud chang scale rotat system respons resist moder amount nois occlus sever experi use real gray scale imag present show feasibl approach introduct challeng visual recognit problem stem fact project object onto imag confound sever dimens variabl uncertain perspect chang orient scale sensor nois occlus non-uniform illumin vision system must abl sens ident object despit variabl must also abl character variabl variabl inher carri much valuabl inform world goal deriv function characterist imag represent suitabl invari recognit use distribut associ memori main question find appropri transform interact intern structur result represent distribut associ memori yield invari recognit simon point mathemat deriv view simpli chang represent make evid previous true obscur view extend problem solv solv problem mean transform make solut transpar approach problem object recognit three requir classif reconstruct character classif impli abil distinguish object previous encount reconstruct process memor imag drawn memori given distort version exist input character involv extract inform object chang way memor goal paper discuss system abl recogn memor 2-dimension object regardless geometr distort like chang scale orient character transform system also allow nois occlus toler memori fault follow section invari represent distribut associ memori respect describ various compon system detail experi section present result sever experi perform real data paper conclud discuss result implic futur research american institut physic invari represent goal section examin various compon use produc vector associ distribut associ memori block diagram describ various function unit involv obtain invari imag represent shown figur imag complex-log conform map rotat scale chang becom translat transform domain along conform map imag also filter space variant filter reduc effect alias conform map imag process laplacian order solv problem associ conform map fourier transform conform map imag laplacian process imag produc four output vector magnitud output vector i-ii invari linear transform object input imag phase output vector contain inform concern spatial properti object input imag complex-log map space variant filter first box block diagram given figur consist two compon complex-log map space variant filter complex-log map transform imag rectangular coordin polar exponenti coordin transform chang rotat scale translat imag map onto complex plane pixel cartesian plane describ mathemat jy complex-log map point ware describ lzl jij system sampl pixel imag construct complex-log map imag sampl taken along radial line space degre apart along radial line step size sampl increas power number deriv number pixel origin imag number sampl complex-log map imag excel examin differ condit involv select appropri number sampl complex-log map imag given non-linear sampl split two distinct part along radial line toward center imag sampl dens enough anti-alias filter need sampl taken edg imag larg anti-alias filter necessari imag filter manner circular region around center correspond area highest resolut size region function number angular sampl radial sampl filter done time sampl convolv truncat bessel function imag space domain width bessel function main lobe invers proport eccentr sampl point problem associ complex-log map sensit center misalign sampl imag small shift center caus dramat distort complex-log map imag system assum object center imag frame slight misalign consid nois larg misalign consid translat could account chang gaze way bring object center frame decis bring center frame activ function determin task exampl system could use guid translat process develop anderson burt pyramid system analyz input imag differ tem invers process reconstruct imag i compl .lo map space variant filter i i i 1ransform i i i laplacian fourier transform distribut associ memori rotat scale estim i-ii classif figur block diagram system poral spatial resolut level smart sensor abl shift fixat interest part imag ie someth larg move brought central part frame recognit fourier transform second box block diagram figur fourier transform fourier transform 2-dimension imag given f x e-i ux+vi dx dy describ two 2-dimension function correspond magnitud phase magnitud compon fourier trans~rm invari translat carri much contrast inform imag phase compon fourier transform carri inform thing ar place imag translat correspond addit linear phase cpmponent complex-log map transform rotat scale translat tije magnitud fourier transform invari translat i-ii ivil chang signific rotat scale object imag laplacian laplacian use difference-of-gaussian dog approxim function given marr 2g 1rtt result convolv laplacian imag view two step process imag blur gaussian kernel specifi width oo isotrop second deriv blur imag comput width gaussian kernel chosen conform map imag visibl approxim pixel experi laplacian sharpen edg object imag set region chang much zero below describ benefit use laplacian laplacian elimin stretch problem encount complex-log map due chang object size object expand complex-log map imag translat pixel vacat translat fill pixel sampl center scale object these new pixel signific differ displac pixel result look like stretch complex-log map imag laplacian complex-log map imag set new pixel zero signific chang surround pixel laplacian elimin high frequenc spread due finit structur discret fourier transform enhanc differ memor object accentu edg de-emphas area littl chang distribut associ memori dam particular form distribut associ memori deal paper memori matrix modifi flow inform stimulus vector associ respons vector result associ spread entir memori space distribut manner mean inform small portion associ found larg area memori new associ place older one allow interact mean size memori matrix stay regardless number associ memor associ allow interact implicit represent structur relationship contextu inform develop consequ rich level interact captur restrict vector associ exist extens index cross-referenc memori distribut associ memori captur distribut represent context depend quit differ simplist behavior model construct stage assum pair m-dimension vector associ distribut associ memori written iv i th stlmu i us vector th correspon mg respons vech enot tel enot tel tor want construct memori matrix kth stimulus vector project onto space defin result project correspond more specif want solv follow equat respons vector ms=r s2 s1 umqu soiutlon lor h equat necessarili exist arbitrari gr~up associ might chosen usual number associ smaller length vector associ system equat underconstrain constraint use solv uniqu matrix minim squar error iim rj1 result solut known moore-penros general invers recal oper project unknown stimulus vector m. result project yield respons vector ms onto memori space if memor stimulus vector independ unknown stimulus vector one memor vector recal vector associ respons if memor stimulus vector depend vector recal vector one memor stimulus vector contain associ respons vector crosstalk store respons vector recal view weight sum respons vector recal begin assign weight accord well unknown stimulus vector match memor stimulus vector use linear least squar classifi respons vector multipli weight sum togeth build recal respons vector recal respons vector usual domin memor respons vector closest unknown stimulus vector assum associ memori associ stimulus respons vector element this mean memori matrix element also assum nois ad element memor stimulus vector memori then independ zero mean varianc recal tt input nois vector t1 output nois vector ratio averag output nois varianc averagg input nois varianc mmt tr autoassoci case this simplifi this say noisi version memor input vector appli memori recal improv factor correspond ratio number memor vector number element vector heteroassoci memori matrix similar formula hold long less fault toler byproduct distribut natur error correct capabl distribut associ memori distribut inform singl memori cell carri signific portion inform critic overal perform memori experi this section discuss result comput simul system imag object first preprocess subsystem outlin section output subsystem four vector i-i construct memori associ stimulus vector i-ii respons vector object databas perform recal mejilori unknown imag preprocess same_subsystem produc vector i-ii result stimulus vector i-i project onto m~mori matrix produc respojls vector stimatel memor phase estim phase vector ci magnitud i-ii ate use reconstruct memor object differ estim phase unknown phase use estim amount rotat scale experienc object databas imag consist twelv object four key four mechan part four leav object chosen their essenti two-dimension structur object photograph use digit video camera black background we emphas imag use creat test recognit system taken differ time use various camera rotat distanc imag digit eight bit quantiz pixel object cover area pixel this small object size relat background necessari due non-linear sampl complex-log map object center within frame hand this sourc much nois could done automat use object 's center mass criteria determin task orient memor object arbitrarili chosen their major axi vertic 2-dimension imag output invari represent subsystem scan horizont form vector memor databas use these experi shown figur figur databas object use experi origin unknown recal rotat memory:6 snr db figur recal use rotat scale key first exampl oper system shown figur figur imag one key memor figur unknown object present system unknown object this case key rotat degre scale figur recal reconstruct imag round edg recal imag artifact complex-log map notic reconstruct recal unrot memor key nois caus error recal phase figur histogram graphic display classif vector correspond s+s histogram show interplay memor imag unknown imag the the bargraph indic the twelv class the unknown object belong the histogram give valu the best linear estim the imag relat the memor object anoth measur the signal-to-nois ratio given the bottom the recal imag snr compar the varianc the ideal recal process the varianc the differ the ideal actual recal this measur the amount nois the recal the snr carri rr.uch inform the q jaliti the recal imag the nois measur the snp jue mani factor misalign the center chang reflect depend memor object affect qualiti varieti way rotat scale estim made use vector correspond the dlll'erenc the unknown vector the recal vector ideal situat plane whose e radient indic the exact amount r .otat scale the recal object experienc system the recal vector corrupt nois mean rotat scale estim ned the estim made let the first order differ point the plane vote specifi rang rotat scale origin unknown recal memory:4 figur recal use scale rotat occlus figur exampl occlus the unknown object this case curv larger slight tilt the memor curv portion the bottom curv occlud the result reconstruct noisi fill the miss part the bottom curv the noisi recal reflect the snr the interplay betw~en the memori shown the hi~togram ideal recal remov remov remov figur recal memori matrix random set zero figur the result random set the element the memori matrix zero figur show the ideal recal figur the recal percent the memori matrix set zero figur the recal percent figur the recal percent even when percent the memori matrix set zero faint outlin the pin could still seen the recal this result import two way first show the distribut associ memori robust the presenc nois second show complet connect network necessari consequ scheme data compress the memori matrix could found conclus this paper we demonstr comput vision system recogn 2dimension object invari rotat scale the system combin invari represent the input imag distribut associ memori object classifi reconstruct character the distribut associ memori resist moder amount nois occlus sever experi demonstr the abil comput vision system oper real grey scale imag present neural network model the di~tribut associ memori one exampl origin develop simul biolog memori they character larg number high interconnect simpl processor oper p2..rallel an excel review the mani neural network model given in the distrib-ut associ memori we use linear result there certain desir properti exhibit comput vision system for exampl feedback our system improv recal the memori recal could improv if non-linear element sigmoid function introduc the feedback loop non-linear neural network as propos by hopfield anderson achiev this type improv becaus each memor pattern js associ sta~l point in an energi space the price paid for the introduct non-linear memori system the system difficult analyz unstabl implement our comput vision system use non-linear distribut associ memori is goal our futur research we present extend our work toward 3-dimension object recognit much the present research in 3-dimension object recognit is limit polyhedr nonocclud object in clean high control environ most system edg base use a generate-and-test paradigm estim the posit orient recogn object we propos use an approach base characterist view llj aspect suggest the infinit 2-dimension project a 3-dimension object group a finit number topolog equival class an effici .t 3dimension recognit system would requir a parallel index method search for object model in the presenc geometr distort nois occlus our object recognit system use distribut associ memori fulfil requir respect to characterist view
----------------------------------------------------------------

title: 1047-selective-attention-for-handwritten-digit-recognition.pdf

select attent handwritten digit recognit ethem alpaydm depart comput engin bogazi 1i ni versi ty istanbul tr-sos15 turkey alpaydin boun.edu.tr abstract complet parallel object recognit np-complet achiev recogn feasibl complex requir compromis parallel sequenti process system select focus part given imag one anoth success fixat generat sampl imag sampl process abstract generat tempor context result integr time comput model base partial recurr feedforward network propos made credibl test real-world problem recognit handwritten digit encourag result introduct all-parallel bottom-up recognit alloc one separ unit possibl featur combin conjunct encod impli combinatori explos shown complet parallel bottom-up visual object recognit np-complet tsotso exchang space time system much less complex may design exampl phone someon press button one need button phone sequenti altern button phone press one time seven time propos recognit base select attent analyz small part imag detail step combin result time oton stark scanpath theori advoc object intern repres feature-r tempor sequenc featur extract fixat posit motor command eye movement approach eye look imag realli see small part part imag examin detail fovea e. alpaydin associ class probabl level softmax class unit loxi t1 hidden unit level attent level featur map eye posit map pxp fovea i wta subsampl blur i salienc map bitmap imag figur block diagram implement system fovea 's content examin pre-attent level basic featur extract take place featur thus extract fed a660ciat part togeth current eye posit if accumul inform suffici recognit eye move anoth part imag make saccad minim recognit time number saccad minim done defin criterion interest salienc fixat interest thus sucess fixat generat sampl imag sampl process abstract generat tempor context result integr time larg amount literatur select attent neurosci psycholog review see respect posner peterson treisman point stress paper approach also use engin exampl system ocr structur implement system recognit handwritten digit given select attent handwritten digit recognit binari imag fovea minim recognit time system attend part imag carri discrimin inform defin criterion interest salienc appli imag locat parallel generat 8alienc map s. salienc measur chosen draw attent part highest inform content salienc criterion low-pass filter rough count number pixel correspond region input imag m. stroke handwritten digit most one two pixel wide count pixel good measur discontinu thus inform also simpl comput i+lm/2j sij hlm/2j mkin2 i j k=i-lm/2j l=j-lm/2j bivari normal mean covari e. note want convolut kernel effect lm/2j also normal zero simul typic digit recognit locat salient posit ofth next fixat defin new center fovea locat attend longer interest fixat salienc locat current scope fovea set inhibit anoth fixat attent level thus control scope pre-attent level maximum salienc map winner-take-al give eye posit fixat arg~b xsij thus follow salient region get input-depend emerg sequenc time eye-posit map eye p08ition map store posit eye current fixat chosen smaller dimension reduct decreas complex introduc effect regular give invari small translat factor comput also simpler also blur immedi neighbor smoother represent blur subsampl winner-take-al pre-attent level featur extract pre-attent level extract detail featur fovea generat featur map inform current eye posit pass associ system recognit trade-off fovea size number saccad requir recognit oper pre-attent level carri parallel minim complex featur extract mani fovea larg fovea expens comput take place hand fovea larg enough extract discrimin featur thus complet recognit small amount time featur extract learn supervis method feedback avail e. alpaydin region symmetr around extract fovea i fed featur extractor featur extract pass associ level featur map f. typic ug denot weight featur fg valu featur found convolv fovea input featur weight vector sigmoid function i j associ level classif fixat associ level fed featur map pre-attent level eye posit map attent level number fixat may necessari recogn imag associ system shortterm memori abl accumul input come time learn similar time use classif class unit organ compet recognit activ class unit evolv till one class get suffici activ suppress other train set avail tempor supervis method use train associ level note may one scanpath object learn one sequenc object fail see task accumul two type inform time featur extract eye posit fovea map eye posit map concaten make dimension input fed associ level use artifici neural network one hidden layer unit experi various architectur notic recurr output layer best output unit vhgfg whabpab gab ltchhh rckpk exp oc lk exp ok denot softmax ed output probabl bridl p valu preced fixat initi use cross-entropi good measur dk dc requir output class learn gradient-asc good measur fraction lit give weight initi fixat later one connect output unit updat follow learn factor select attent handwritten digit recognit note assum 8pic lc connect hidden unit back-propag one step train featur extractor thus updat equat connect featur unit cg ch vhg seri fixat made one class unit suffici activ pc typic when salient point salienc less certain threshold condit rare met first epoch then comput chang sum updat made like exapl backpropag time recurr connect unfold time work well task explain class one scanpath above-ment approach like real-tim recurr learn william zipser partial deriv previous time step thus ignor this tempor depend result discuss we experi various paramet set final chose architectur given when input class fovea featur there hidden unit there imag train cross-valid test result given tabl it seen scan less half imag we get general addit local high-resolut imag provid fovea low-resolut imag surround parafovea given associ level better recognit exampl we low-pass filter undersampl origin imag get imag we fed class unit addit attention-bas hidden unit success went quit high fewer fixat necessari compar row tabl inform provid map actual much seen row tabl given as input thus idea when we coars input look quarter imag detail suffici get accuraci featur eye posit necessari good recognit when one use without success quit low as seen row last row we see the perform multi layer percept ron hidden unit all-parallel recognit beyond certain network size increas the number featur help much decreas the certainti threshold decreas the number fixat necessari e. alpaydin tabl result handwritten digit recognit select attent valu given averag standard deviat independ run see text comment no param test success train epoch no fixat sa system sa+parafovea onli parafovea onli info onli info mlp hidden method we want decreas success we n't smaller fovea decreas the number free paramet decreas success requir larger number fixat similar larger fovea decreas the number fixat increas complex the simpl low-pass filter use as salienc measur the simplest measur previous it use fukushima imagawa find the next charact segment also olshausen translat invari robust measur the expens comput possibl see rimey brown milanes salient region conspici differ surround there chang bright color edg orient corner time motion etc it also possibl top-down task-depend salienc measur integr minim recognit time impli rememb explicit sequenc analog skill motor behaviour probabl gain mani repetit here partial recurr network use tempor process hidden markov model like use speech recognit anoth possibl rimey brown haclsalihzad they probabilist finit automata train to classifi sequenc one one model object it note here better approach the problem exist le cun here we advoc a comput model make it plausibl test it a real-world problem it necessari for more complic problem an all-parallel approach would work for exampl le cun model for the type input free paramet here there mx r+ r+pxp it free paramet make when this the main advantag select attent the complex the system heavili reduc at the expens slower recognit both in overt form of attent foveat in covert form for bind featur for this latter type of attent discuss here see ahmad also note low-level featur extract oper like carri in the pre-attent level local convolut select attent for handwritten digit recognit appropri for parallel process a simd machin higherlevel oper requir larger connect better carri sequenti natur also seem to taken this direct acknowledg this work support by tiibitak grant eeeag-143 and bogazi i univers research fund cenk kaynak prepar the handwritten digit databas base on the program provid by nist garri
----------------------------------------------------------------

title: 6057-using-fast-weights-to-attend-to-the-recent-past.pdf

use fast weight attend recent past jimmi ba univers toronto geoffrey hinton univers toronto googl brain jimmi psi.toronto.edu geoffhinton google.com volodymyr mnih googl deepmind joel z. leibo googl deepmind catalin ionescu googl deepmind vmnih google.com jzl google.com cdi google.com abstract until recent research artifici neural network larg restrict system two type variabl neural activ repres current recent input weight learn captur regular among input output payoff good reason restrict synaps dynam mani differ time-scal suggest artifici neural network might benefit variabl chang slower activ much faster standard weight fast weight use store temporari memori recent past provid neural plausibl way implement type attent past recent prove help sequence-to-sequ model use fast weight avoid need store copi neural activ pattern introduct ordinari recurr neural network typic two type memori differ time scale differ capac differ comput role histori sequenc current process store hidden activ vector act short-term memori updat everi time step capac memori number hidden unit long-term memori convert current input hidden vector next hidden vector predict output vector store weight matric connect hidden unit input output matric typic updat end sequenc capac o h o ih o ho i number input output unit long short-term memori network hochreit schmidhub complic type rnn work better discov long-rang structur sequenc two main reason first comput increment hidden activ vector time step rather recomput full vector1 encourag inform hidden state persist much longer second allow hidden activ determin state gate scale effect weight multipl interact allow effect weight dynam adjust input hidden activ via gate howev lstms still limit short-term memori capac histori current sequenc sever research hinton plaut schmidhub suggest neural network could benefit third form memori much higher storag capac neural activ much faster dynam standard slow weight memori could store inform specif histori current sequenc inform avail influenc ongo process without use memori capac hidden activ assum rememb gate lstm memori cell set one until recent howev surpris littl investig form memori recurr net despit strong psycholog evid exist obvious comput reason need evid physiolog temporari memori may store neural activ process like work memori attent prime oper timescal minut simultan slow mediat neural activ without dynam attractor state timescal fast long-term synapt plastic mechan kick minut hour artifici neural network research typic focus method maintain temporari state activ dynam focus may inconsist evid brain also perhap primarili maintain temporari state inform short-term synapt plastic mechan tsodyk abbott regehr barak tsodyk brain implement varieti short-term plastic mechan oper intermedi timescal exampl short term facilit implement leftov axon termin depolar short term depress implement presynapt neurotransmitt deplet zucker regehr spike-tim depend plastic also invok timescal markram bi poo these plastic mechan synapsespecif thus accur model memori o h capac standard recurr artifici recurr neural net lstms fast associ memori one main preoccup neural network research earli willshaw kohonen anderson hinton hopfield idea memori store somehow keep copi pattern neural activ instead these pattern reconstruct need inform store weight associ network weight could store mani differ memori auto-associ memori weight expect store real-valu vector compon close come upper bound depend storag rule use hopfield net use simpl one-shot outer-product storag rule achiev capac approxim binari vector use weight requir log n bit much effici use made weight use iter error correct storag rule learn weight retriev bit pattern bit gardner purpos maxim capac less import simpl non-it storag rule use outer product rule store hidden activ vector fast weight decay rapid usual weight rnn call slow weight learn stochast gradient descent object function take account fact chang slow weight lead chang get store automat fast associ memori fast associ memori sever advantag compar type memori assum neural ture machin ntm grave neural stack grefenstett memori network weston first clear real brain would implement exot structur these model tape ntm wherea clear brain could implement fast associ memori synaps appropri dynam second fast associ memori need decid write memori read memori fast memori updat time write superimpos fast chang compon strength synaps everi time input chang transit new hidden state determin combin three sourc inform new input via slow input-to-hidden weight previous hidden state via slow transit weight recent histori hidden state vector via fast weight effect first two sourc inform new hidden state comput maintain sustain boundari condit brief iter settl process allow fast weight influenc new hidden state assum fast weight decay exponenti show effect fast weight hidden vector iter settl phase provid addit input proport sum sustain boundari condit slow transit weight fast transit weight figur fast associ memori model recent hidden activ vector scalar product recent hidden vector current hidden activ vector term sum weight decay rate rais power long ago hidden vector occur fast weight act like kind attent recent past strength attent determin scalar product current hidden vector earlier hidden vector rather determin separ parameter comput type use neural machin translat model bahdanau updat rule fast memori weight matrix simpli multipli current fast weight decay rate add outer product hidden state vector multipli learn rate next vector hidden activ h comput two step preliminari vector h0 determin combin effect input vector previous hidden vector h0 slow weight matric nonlinear use hidden unit preliminari vector use initi inner loop iter process run step progress chang hidden state h hs hs term squar bracket sustain boundari condit real neural net could implement rapid chang synaps comput simul use sequenc fewer time step dimension less full rank effici comput term hs without ever comput full fast weight matrix assum begin sequenc hs hs term squar bracket scalar product earlier hidden state vector current hidden state vector hs iter inner loop iter inner loop fast weight matrix exact equival attend past hidden vector proport scalar product current hidden vector weight decay factor inner loop iter attent becom focuss past hidden state manag attract current hidden state equival use fast weight matrix compar set store hidden state vector help comput simul allow us explor done fast weight without incur huge penalti abandon use mini-batch dure train first sight mini-batch use fast weight matrix differ everi sequenc compar set store hidden vector allow mini-batch layer normal fast weight potenti problem fast associ memori scalar product two hidden vector could vanish explod depend norm hidden vector recent layer normal ba shown effect stabliz hidden state dynam rnns reduc train time layer normal appli vector sum input recurr unit particular time step use mean varianc compon vector re-cent re-scal sum input appli nonlinear includ learn neuron-specif bias gain appli layer normal fast associ memori follow ln cx hs ln denot layer normal found appli layer normal iter inner loop make fast associ memori robust choic learn rate decay hyper-paramet rest paper fast weight model train use layer normal outer product learn rule fast learn rate decay rate unless otherwis note experiment result demonstr effect fast associ memori first investig problem associ retriev section mnist classif section compar fast weight model regular rnns lstm variant appli propos fast weight facial express recognit task use fast associ memori model store result process one level while examin sequenc detail finer level section hyper-paramet experi select grid search valid set model train use mini-batch size adam optim kingma ba descript train protocol hyper-paramet set use found appendix last show fast weight also use effect implement reinforc learn agent memori section associ retriev start demonstr method propos store retriev temporari memori work effect toy task well suit consid task multipl key-valu pair present sequenc end sequenc one key present model must predict valu temporarili associ key use string contain charact english alphabet togeth digit construct train sequenc first random sampl charact alphabet without replac first key singl digit sampl associ valu key generat sequenc character-digit pair one differ charact select random queri network must predict associ digit exampl string sequenc target shown input string target token separ queri key-valu pair generat train exampl valid exampl test exampl solv task standard rnn end hidden activ somehow store key-valu pair key valu present sequenti make signific challeng model use slow weight use neural network singl recurr layer experi recurr network process input sequenc one charact time input charact first convert learn 100-dimension embed vector provid input recurr layer2 make architectur task similar architectur next task first comput dimension embed vector expand 100-dimension embed model irnn lstm a-lstm fast weight negat log likelihood tabl classif error rate comparison associ retriev task a-lstm irnn lstm fw updat figur comparison test log likelihood associ retriev task recurr hidden unit output recurr layer end sequenc then process anoth hidden layer relus final softmax layer augment relu rnn fast associ memori compar lstm model architectur although origin lstms explicit long-term storag capac recent work danihelka extend lstms ad complex associ memori experi compar fast associ memori lstm variant figur tabl show number recurr unit small fast associ memori signific outperform lstms number recurr unit result fit hypothesi fast associ memori allow rnn use recurr unit effect addit higher retriev accuraci model fast weight also converg faster lstm model integr glimps visual attent model despit mani success convolut neural network comput expens represent learn hard interpret recent visual attent model mnih ba xu shown overcom limit convnet one understand signal algorithm use see model look also visual attent model abl select focus import part visual space thus avoid detail process much background clutter section show visual attent model use fast weight store inform object part though use restrict set glimps correspond natur part object given input imag visual attent model comput sequenc glimps region imag model determin look next also rememb seen far work memori make correct classif later visual attent model learn find multipl object larg static input imag classifi correct learnt glimps polici typic over-simplist use singl scale glimps they tend scan imag rigid way human eye movement fixat far complex abil focus differ part whole object differ scale allow human appli knowledg weight network mani differ scale requir form temporari memori allow network integr discov set glimps improv model abil rememb recent glimps help visual attent model discov non-trivi glimps polici becaus fast weight store glimps inform sequenc hidden activ vector freed learn intellig integr visual inform retriev appropri memori content final classifi explicit verifi larger memori capac benefici visual attention-bas model simplifi learn process follow way first provid pre-defin glimps control signal model know attend rather learn control polici reinforc learn second we introduc addit control signal memori cell so attent model know store glimps inform typic visual attent model complex high varianc perform due need learn polici network classifi time our simplifi learn procedur enabl us discern perform improv contribut use fast weight rememb recent past updat fast weight wipe hidden state integr transit weight slow transit weight fast transit weight figur multi-level fast associ memori model model irnn featur featur featur lstm convnet fast weight tabl classif error rate mnist we consid simpl recurr visual attent model similar architectur rnn previous experi it predict attend rather given fix sequenc locat static input imag broken four non-overlap quadrant recurs two scale level four coars region down-sampl along four quadrant present singl sequenc shown figur notic two glimps scale form two-level hierarchi visual space order solv task success attent model need integr glimps inform differ level hierarchi one solut use model hidden state store integr glimps differ scale much effici solut use temporari cach store unfinish glimps comput process glimps finer scale hierarchi onc comput finish scale result integr partial result higher level pop previous result cach fast weight therefor act neural plausibl cach store partial result slow weight model then special integr glimps scale becaus slow weight share glimps scale model abl store partial result sever level set fast weight though we demonstr use fast weight storag singl level we evalu multi-level visual attent model mnist handwritten digit dataset mnist well-studi problem mani techniqu benchmark it contain ten class handwritten digit rang task predict class label isol rough normal imag digit glimps sequenc this case consist patch pixel tabl compar classif result relu rnn multi-level fast associ memori lstm get sequenc glimps again result show number hidden unit limit fast weight give signific improv model we increas memori capac multi-level fast associ memori consist outperform lstm classif accuraci figur exampl near frontal face multipi dataset test accuraci irnn lstm convnet fast weight tabl classif accuraci comparison facial express recognit task unlik model must integr sequenc glimps convolut neural network process glimps parallel use layer hidden unit hold intermedi comput result we demonstr effect fast weight compar three-lay convolut neural network use patch glimps present visual attent model tabl we see multi-level model fast weight reach similar perform convnet model without requir biolog implaus weight share facial express recognit investig benefit use fast weight multi-level visual attent model we perform facial express recognit task cmu multi-pi face databas gross dataset preprocess align face eye nose fiduci point it downsampl greyscal full dataset contain photo taken camera differ viewpoint illumin express ident session condit we use imag taken three central camera correspond view sinc facial express discern extrem viewpoint result dataset contain imag ident appear train set remain ident test set given input face imag goal classifi subject facial express one six differ categori neutral smile surpris squint disgust scream task realist challeng previous mnist experi dataset unbalanc number label some express exampl squint disgust hard distinguish order perform well this task model need general differ light condit viewpoint we use multi-level attent model mnist experi recurr hidden unit model sequenti attend non-overlap pixel patch two differ scale there total glimps similar we design two layer convnet recept field from tabl we see multi-level fast weight model know store inform outperform lstm irnn result consist previous mnist experi howev convnet abl perform better multi-level attent model this near frontal face dataset we think effici weight-shar architectur engin convnet combin simultan avail all inform at level process allow convnet general better this task our use rigid predetermin polici glimps elimin one main potenti advantag multi-level attent model it process inform detail at high resolut whilst ignor irrelev detail realiz this advantag we need combin use fast weight learn complic polici rnn rnn+fw lstm avgerag reward avgerag reward rnn rnn+fw lstm step step figur sampl screen from game catch perform curv catch perform curv catch agent memori while differ kind memori attent studi extens supervis learn set grave mnih bahdanau use model learn long rang depend reinforc learn receiv less attent we compar differ memori architectur partial observ variant game catch describ mnih game play screen binari pixel episod consist frame trial begin singl pixel repres ball appear somewher in first row column a two pixel paddl control agent in bottom row observ a frame agent get either keep paddl stationari move it right left one pixel ball descend a singl pixel frame episod end ball pixel reach bottom row agent receiv a reward paddl touch ball a reward it solv fulli observ task straightforward requir the agent move the paddl the column the ball we make the task partiallyobserv provid the agent blank observ after the th frame solv the partiallyobserv version the game requir rememb the posit the paddl ball after frame move the paddl the correct posit use the store inform we use the recent propos asynchron advantag actor-crit method mnih train agent three type memori differ size the partial observ catch task the three agent includ a relu rnn lstm a fast weight rnn figur show learn progress the differ agent two variant the game the agent use the fast weight architectur as polici represent shown in green abl learn faster the agent use relu rnn lstm repres the polici the improv obtain fast weight also signific the larger version the game requir memori conclus this paper contribut machin learn show the perform rnns a varieti differ task improv introduc a mechan allow each new state the hidden unit attract toward recent hidden state in proport scalar product the current state layer normal make this kind attent work much better this a form attent the recent past somewhat similar the attent mechan recent use dramat improv the sequence-to-sequ rnns use in machin translat the paper interest implic for comput neurosci cognit scienc the abil peopl to recurs appli the knowledg process apparatus to a whole sentenc to embed claus within sentenc to a complex object to a major part object long use to argu neural network not a good model higher-level cognit abil by use fast weight to implement an associ memori for the recent past we shown how the state neuron could freed so the knowledg in the connect a neural network appli recurs this overcom the object these model recurs by store copi neural activ vector biolog implaus
----------------------------------------------------------------

title: 1369-learning-continuous-attractors-in-recurrent-networks.pdf

learn continu attractor recurr network h. sebastian seung bell lab lucent technolog murray hill nj seung~bell-labs.com abstract one approach invari object recognit employ recurr neural network associ memori standard depict network 's state space memori object store attract fix point dynam i argu modif pictur object continu famili instanti repres continu attractor idea illustr network learn complet pattern perform task fill miss inform network develop continu attractor model manifold pattern drawn statist viewpoint pattern complet task allow formul unsupervis learn term regress rather densiti estim classic approach invari object recognit use recurr neural network associ memori l spite intuit appeal biolog plausibl approach larg abandon practic applic paper introduc two new concept could help resurrect object represent continu attractor learn attractor pattern complet model associ memori memori store attract fix point discret locat state space l discret attractor may appropri pattern continu variabl like imag three-dimension object differ viewpoint instanti object lie continu pattern manifold appropri repres object attract manifold fix point continu attractor make idea practic import find method learn attractor exampl naiv method train network retain exampl shortterm memori method defici prevent network store spurious fix point unrel exampl superior method train network restor exampl corrupt learn complet pattern fill miss inform learn continu attractor recurr network figur repres object dynam attractor discret attractor continu attractor learn pattern complet understood dynam statist perspect sinc complet task requir larg basin attract around memori spurious fix point suppress complet task also lead formul unsupervis learn regress problem estim function depend variabl sensori input densiti estim rather regress domin formul unsupervis learn stochast neural network like boltzmann machin densiti estim virtu suppress spurious fix point automat also serious drawback intract mani network architectur regress tractabl nonetheless power altern densiti estim number recent neurobiolog model continu attractor use repres continu quantiti like eye position- direct reach head direct orient visual stimulus along model present work part new paradigm neural comput base continu attractor discret versus continu attractor figur depict two way repres object attractor recurr neural network dynam standard way repres object attract fix point l figur recal memori trigger sensori input set initi condit network dynam converg fix point thus retriev memori differ instanti one object lie basin attract trigger retriev memori result many-to-on map requir invari recognit figur object repres continu manifold fix point one-dimension manifold shown general attractor multidimension parametr instanti pose paramet object exampl visual object recognit coordin would includ viewpoint object seen reader caution term continu attractor ideal taken liter real network continu attractor approxim manifold state space along drift slow illustr simpl exampl descent dynam trough-shap energi landscap bottom trough perfect level line fix point ideal continu attract dynam howev slight imperfect caus slow drift along line sort approxim continu attract found real network includ train learn s. seung hidden layer visibl layer figur recurr network feedforward autoencod algorithm discuss dynam memori retriev preced discuss motiv idea repres pattern manifold continu attractor idea develop simpl network shown figur consist visibl layer xl rnl hidden layer x2 rn2 architectur recurr contain bottom-up connect n2 nl matrix w2d top-down connect nl n2 matrix vector bl b2 repres bias ofth neuron neuron rectif nonlinear max x act vector compon compon mani variant recurr network dynam conveni choic follow discrete-tim version updat hidden visibl layer altern time visibl layer initi input vector xl dynam evolv xl w2ixi if memori store attractor iter dynam regard memori retriev activ circul around feedback loop two layer one iter loop map xl singl iter equival feedforward architectur figur case hidden layer smaller visibl layer this architectur known auto encod network therefor recurr network dynam equival repeat iter feedforward autoencod this standard trick unfold dynam recurr network time yield equival feedforward network mani layer close relationship recurr network figur 2a autoencod figur surpris learn algorithm two network also relat explain learn retain pattern littl trace arbitrari input vector xl remain time step dynam howev network retain input vector short-term memori reverber pattern activ correspond fix point dynam pattern chang activ circul around feedback loop learn continu attraclor recurr networlc this suggest formul learn optim network 's abil retain exampl short-term memori suitabl cost function squar differ ixi xl exampl pattern xl network short-term memori xl after time step gradient descent this cost function done via backpropag if network train pattern drawn continu famili learn perform short-term memori task oy develop continu attractor lie near exampl train hidden layer smaller visibl layer dimension attractor limit size hidden layer case singl time step train recurr network figur 2a retain pattern equival train autoencod figur 2b minim squar differ input output layer averag exampl inform theoret perspect small hidden layer figur 2b act bottleneck input output layer forc autoencod learn effici encod input special case linear network natur learn encod understood complet then input output vector relat simpl matrix multipl rank matrix equal number hidden unit averag distort minim when this matrix becom project oper onto subspac span princip compon exampl dynam perspect princip subspac continu attractor dynam linear network dynam converg this attractor singl iter start initi condit therefor interpret princip compon analysi variant method learn continu attractor lo learn complet pattern learn retain pattern short-term memori work proper architectur small hidden layer problem larg hidden layer evid when hidden visibl layer size neuron linear then cost function learn minim set weight matric equal ident l2 i this trivial minimum everi input vector fix point recurr network figur equival feedforward network figur exact realiz ident map clear network learn anyth therefor case larg hidden layer learn retain pattern inadequ without bottleneck architectur pressur feedforward network learn effici encod without constraint dimens attractor recurr network develop spurious fix point noth exampl problem solv differ formul learn base task pattern complet complet task figur network initi corrupt version exampl learn done minim complet error squar differ ixi dl uncorrupt pattern final visibl vector xl gradient descent complet error done backpropag time this new formul learn elimin trivial ident map solut men h. s. seung retent completio topograph featur map patch miss sensori input retriev memori figur pattern retent versus complet dynam pattern complet recept field figur local connect architectur recept field hidden neuron tion ident network retain exampl restor corrupt exampl pristin form complet task forc network enlarg basin attract store memori suppress spurious fix point also forc network learn associ variabl sensori input local connect architectur experi conduct imag handwritten digit from usp databas describ exampl imag gray scale rang from network train specif digit class goal learn singl pattern manifold both network architectur natur complet task chosen suit topograph structur present visual imag network architectur given topograph organ constrain synapt connect local shown figur both visibl hidden layer network visibl layer repres imag hidden layer topograph featur map neuron recept project field except neuron near edg restrict connect pattern complet task exampl imag corrupt zero pixel insid patch chosen random locat shown figur locat patch random each present exampl size patch substanti fraction imag much larger recept field size this method corrupt exampl gave complet task topograph natur it involv set spatial contigu pixel this topograph natur would lack if exampl corrupt exampl addit spatial uncorrel nois figur 3b illustr dynam pattern complet perform network learn continu attractor recurr network train exampl digit class network initi corrupt exampl after first itex ation dynam imag partial restor second iter lead superior restor sharpen imag fill phenomenon also evid in hidden layer network first train retriev dynam one iter result bias synapt weight then use initi condit train retriev dynam two iter hidden layer develop topograph featur map suitabl repres imag digit figur 4b depict bottom-up recept field hidden neuron top-down project field these neuron similar shown this featur map distinct from other becaus use top-down bottom-up connect in feedback loop bottom-up connect analyz imag constitu featur top-down connect synthes imag compos featur featur in top-down connect regard vocabulari for synthesi imag sinc combin featur proper pattern there must grammat constraint combin network 's abil complet pattern suggest these constraint embed in dynam equat the network therefor the relax dynam regard process massiv parallel constraint satisfact conclus i argu continu attractor natur represent for pattern manifold one method learn attractor train the network retain exampl in short-term memori this method equival autoencod learn work if the number hidden unit larg better method train the network complet pattern for local connect network this method demonstr learn topograph featur map the train network abl complet pattern indic syntact constraint the combin featur embed in the network dynam empir evid the network inde learn continu attractor obtain local linear the network the linear dynam mani eigenvalu close uniti indic the exist approxim continu attractor learn increas number iter in the retriev dynam improv the qualiti the approxim there one aspect the learn algorithm specif tailor for continu attractor this aspect the limit the retriev dynam iter rather iter it the way true fix point as mention earlier continu attractor ideal in real network it consist true fix point manifold relax fast along drift slow adjust the shape this manifold the goal learn the exact locat the true fix point relev the use fast retriev dynam remov one long-stand object attractor neural network true converg fix point take long if desir fast relax approxim continu attractor attractor neural network much slower feedforward network in the experi discuss learn done backpropag time contrast hebbian learn simpler altern part the imag s. seung held clamp the miss valu fill in converg fix point anti-hebbian updat made then the miss valu clamp correct valu the network converg new fix point a hebbian updat made this procedur the disadvantag requir true converg to a fix point take mani iter it also requir symmetr connect may a represent handicap this paper address the learn a singl attractor to repres a singl pattern manifold the problem of learn multipl attractor to repres multipl pattern class discuss elsewher along the extens to network architectur mani layer acknowledg this work support bell laboratori i thank j. j. hopfield d. d. lee l. k. saul n. d. socci h. sompolinski d. w. tank for help discuss
----------------------------------------------------------------

title: 177-neural-network-star-pattern-recognition-for-spacecraft-attitude-determination-and-control.pdf

neural network star pattern recognit spacecraft attitud determin control phillip alvelda a. miguel san martin jet propuls laboratori california institut technolog pasadena ca abstract current complex spacecraft attitud determin control task ultim govern ground-bas system personnel convent on-board system face sever comput bottleneck introduc serial microprocessor oper inher parallel problem new comput architectur base anatomi human brain seem promis high speed fault-toler solut limit serial process paper discuss latest applic artifici neural network problem star pattern recognit spacecraft attitud determin introduct design convent on-board microprocessor perform one comparison calcul time imag pattern recognit problem involv larg templat set high resolut requir astronom number comparison given databas typic mission plan optim task requir calcul involv multitud paramet element inher degre import reliabl nois even advanc supercomput run latest softwar requir second even minut execut complex pattern recognit expert system task often provid incorrect ineffici solut problem prove trivial ground control specialist intent ongo research develop neural network base satellit attitud determin system prototyp capabl determin current three-axi inerti orient system determin real-tim direct satellit face need order aim antenna scienc instrument navig equip satellit autonom import criterion interplanetari mission particular event system failur task must perform reason amount time due consider actual environment nois precis constraint celesti attitud determin normal oper condit whole repertoir spacecraft system oper conjunct perform attitud determin task backbon gyro gyro measur chadl orient current attitud store neural network star pattern recognit volatil on-board memori updat yro system int rate veloc provid chanl anlular posit power system failur reason sinlle-event-upset due cosmic radiat current store attitud lafor.atloa lost one attract way recoverinl attitud inform priori knowledg usinl on-board imalinl comput system imag portion sky compar characterist pattern star sensor fieldof-view on-board star catalog therebi identifi star sensor fov field view retriev identifi star coordin transform correl fov real-ski coordin determin spacecraft attitud but problem match limit field view contain small number star billion billion onboard full-ski catalol contain perhap thousand star lonl sever comput bottleneck pair pair pair pair store pair address pair pair geometr constraint ficun serial tar i.d catalol rorma rnethodololi latest serial allorithm perform task requir approxim kbyte ram store on-board star catalol incorpor hilh optim allorithm use motorola search sort databas star-pair distanc valu correl decompos star pattern sensor fov perform identif process order i second alvelda san martin success rate percent but it dot fit id spacecraft od-board memori therefor system flown planetari spacecraft use sun sensor attitud maneuv sun sun canopus ficun current spacecraft attitud inrorm recoveri lequenc result state-of-the-art interplanetari spacecraft use sever independ sensor system onjunct determin attitud priori knowledg first craft command slew sun sensor align spacecraft 's major axi locked-on sun craft must rotat around axi appropri star pattern approxim nineti degre sun acquir provid three-axi orient inform entir attitud acquisit sequenc requir absolut minimum thirti minut presuppos spacecraft actuat maneuv system oper phenomen rendezv speed involv interplanetari navig system failur near mission culmin could mean almost complet loss valuabl scientif data spacecraft perform initi attitud acquisit sequenc neural motiv parallel architectur collect comput properti neural network base system address sever problem associ implement perform serial star id algorithm instead search lengthi databas one element time store star pattern correl field view concurr wherea standard memori storag technolog requir one address ram per star-pair distanc neural star pattern represent store characterist matric interconnect neuron distribut data set represent sever desir properti first 2n redund serial star-p.air scheme star end pair discard new compress represent emerg neuromorph architectur second nois statist thermal nois systemat sensor precis limit pattern invari characterist neural network star pattern recognit incorpor direct preprocess neural architectur without extra circuitri first neural approach primari motiv nasa perspect improv satellit attitud determin perform enabl on-board system implement problem methodolog neural architectur slight differ serial model instead identifi everi detect st~r field view neural system identifi singl guid star respect pattern dimmer star around it correl star 's known posit sensor fov determin point axi if need one star requir fix roll angl axi so core celesti attitud determin problem chang multipl star identif correl singl star pattern identif entir system consist sever modul marriag differ technolog first neural system architectur use alreadi matur i.e sensor/preprocessor technolog perform well neural technolog convent system prove intract eye toward rapid prototyp implement system design technolog neural vlsi avail less one year system architectur star tracker sensor system system input base astro star tracker develop guidanc control section jet propuls laboratori star tracker optic system imag defocuss portion sky star sub-field onto charg coupl devic tracker electron generat star centroid posit intens inform pass list preprocess system preprocessln8 system centroia ind intens inform pass preprocess subsystem star pattern treat extract nois pattern invari pattern field-of-view defin center aroun brightest guid star central portion sensor field-ofview sinc pattern fov radius one half sensor fov pattern guid star base portion imag complet invari translat perturb preprocessor introduc rotat invari guide-star pattern use distanc dimmer star insid pattern fov central guid star these distanc map preprocessor onto two dimension coordin system distanc versus relat magnitud normal guid star brightest star pattern fov sampl neural associ star catalog motiv this distanc map format becom clear issu involv nois invari memori capac consid alvelda san martin astro star tracker limit precis instrument particular absolut relat intens measur two major problem aris first dimmer star intens near bottom dynam rang mayor may includ star pattern so entir distanc map scale brightest star such bright high-confid measur weight heavili dimmer possibl transient star less import given pattern second sinc larg number star sky uniqu given star pattern govern most relat star distanc measur way highest precis measur provid star tracker addit limit expect neural hardwar discret number neuron must sampl continu function retain maximum sampl precis minimum number neuron neural system use biolog mechan recept field hyperacu word number neuron respond singl distanc stimulus process analog use defocuss imag point sourc integr sever pixel generat centroid sub-pixel accuraci relax demand hardwar develop neural modul this point smooth perform preprocessor instead introduc neural network architectur dynam equival neural respons function becom x i ili ill k=l sampl activ neuron number star pattern field view ili posit neuron sampl axi ilk posit stimulus star sampl axi magnitud scale factor star normal brightest star pfov guid star width gaussian point spread function neural system neural system neuron three-lay feed-forward network sampl scale smooth distanc map provid output vector highest neural output activ repres best match one pre-train guid star pattern network train algorithm use standard backward error propag neural network star pattern recognit algorithm set network interconnect weight train set guid star pattern deriv softwar simul sky sensor model simul testb comput simul testb includ realist celesti field model well detector model proper repres achiev posit intens resolut sensor scan rate dynam rang signal nois properti rapid identif star pattern observ limit train set simul tracker orient random within celesti sphere perform result project term improv perform neural system quit success but howev area initialji expect vlsi implement might yield consider system speed-up the digit simul testb neural process time the order as the serial algorithm perhap slight better the success rate the serial system alreadi better the neural net system achiev accuraci when the systemat nois drop star the sensor neglect when the drop star effect introduc the perform figur drop it later discov the reason this low rate due most the limit size the yale bright star catalog higher magnitud lower star bright spars region the sky the pattern the sensor fov present the limit sky model occasion consist two three dim star when one two drop becaus the star sensor magnitud precis limit time pattern left identifi further experi parametr studi under way use complet harvard smithsonian catalog the big gain term requir memori the serial algorithm store star pair high precis in addit code for rather complex heurist artifici intellig type algorithm for total size kbyte the neural algorithm use connectionist data represent abl to abstract the star catalog pa ttern class similar orthagon in varianc in high compress fashion network perform remain essenti constant interconnect precis decreas to less four bit per synaps synaps four bit per synaps requir littl comput memori these simul result deriv mont carlo run approxim iter use the simul testb alvelda san martin conclus by mean clever combin sever technolog appropri data set represent star system use one the simpl neural algorithm outperform use the classic serial one in sever aspect even run softwar simul neural network the neural simul is approxim ten time faster the equival serial algorithm requir less one seventh the comput memori the transfer to neural vlsi technolog memori requir virtual disappear process speed increas by at least order magnitud w1 ere power and weight requir scale the hardwar chip count and everi pound must launch space cost million of dollar neural technolog enabl real-tim on-board absolut attitud determin with a priori inform may eventu make sever accessori satellit system like horizon and sun sensor obsolet while increas the overal reliabl of spacecraft system ackaowledgmeat we would like to acknowledg mani fruitful convers with c. e. bell j. barhen and s. gulati
----------------------------------------------------------------

title: 2-the-capacity-of-the-kanerva-associative-memory-is-exponential.pdf

capac kanerva associ memori exponenti p. a. choul stanford univers stanford ca abstract capac associ memori defin maximum number vord store retriev reliabl address vithin given sphere attract shown sphere pack argument address length increas capac associati memori limit exponenti grovth rate h2 vhere binari entropi function bit radius sphere attract exponenti grovth capac actual achiev kanerva associ memori paramet optim set formula op.tim valu provid exponenti grovth capac kanerva associ memori contrast sharpli vith sub-linear grovth capac hopfield associ memori associ memori capac model associ memori follov let address datum pair vhere vector ls vector let address datum pair store associ memori associ memori present input vith address close store address produc output vord close correspond content specif let us say associ memori correct fraction error vi thin ham distanc retriev equal ham sphere around vill call sphere attract viii call radius attract one notion capac associ memori maximum number vord store vhile correct fraction error unfortun notion capac ill-defin depend exact vhich address datum pair store clear associ memori correct fraction error everi sequenc store address datum pair consid exampl sequenc vhich sever differ vord vritten address memori reliabl retriev content overvritten vord extrem associ memori store unlimit number vord retriev reliabl content ident use definit capac must lie somevher betveen tvo extrem paper interest largest sequenc address sequenc data memori correct fraction error defin ithi vork vas support nation scienc foundat nsf grant ibm doctor fellovship american institut physic sequenc probabilist sens set sequenc th total probabl greater say when sequenc equiprobab1 reduc determinist version sequenc practic difficult comput capac given associ memori yith input length output length tn fortun though easier comput asymptot rate a1 increas tn increas given famili associ memori approach taken mceliec toyard capac hopfield associ memori take approach tovard capac kanerva associ memori tovard capac associ memori general next section provid upper bound rate grovth capac associ memori fit general model shown sphere pack argument capac limit exponenti rate grovth vhere binari entropi function bit radius attract later section vill turn exponenti grovth capac actual achiev kanerva associ memori paramet optim set exponenti grovth capac kanerva associ memori contrast sharpli yith sub-linear grovth capac hopfield associ memori univers upper bound capac recal definit capac associ memori largest a1 sequenc address sequenc data memori correct fraction error clear upper bound capac largest af vhich exist sequenc address sequenc data memori correct fraction error we nov deriv express upper bound let radius attract let dh x i sphere attract set xs ham distanc ln8j sinc assumpt memori correct fraction error everi address dh xu retriev vord yw size dh xu easili shown independ xu equal vn.d vhere binomi coeffici n jk n thus total n-bit address least vn.d address retriev least vn.d address retriev least vn.d address retriev forth fol10v total number distinct yu jv nov stirl 's formula shovn vn.d vhere log2 binari entropi function bit logn function yhose magnitud grov slovli constant time log thus total number distinct j sinc set contain i sequenc af tn-bit vord vill contain larg number distinct vord tn figur neural net represent kanerva associ memori signal propag bottom input top output arc multipli signal weight node add incom signal threshold suffici larg see detail follov general function fen said f n fg n bound exist constant thus say exist constant n l-h s +alogn emphas sinc unknow bound mean fix hovev indic asymptot maximum exponenti rate grovth h2 intui ti veli sequenc address optim pack address space hope achiev upper bound remark sequenc optim sens vhen larg kanerva associ memori take advantag fact kanerva associ memori kanerva associ memori regard tvo-lay neural netvork shovn figur vhere first layer preprocessor second layer usual hopfield style array preprocessor essenti encod n-bit input address larg k-bit intern represent vhose size permit grov exponenti it seem surpris capac kanerva associ memori grov exponenti it knovn capac hopfield array grov almost linear assum coordin k-vector dravn random independ flip fair coin figur matrix represent kanerva associ memori signal propag right input left output dimens shown box corner circl stand function composit dot stand matrix multipl situat hovev assumpt ridicul sinc k-bit intern represent function n-bit input address it contain bit inform wherea independ flip fair coin contain bit inform kanerva 's primari contribut therefor specif preprocessor specif map each n-bit input address larg k-bit intern represent oper preprocessor easili describ consid matrix represent shovn figur matrix random popul vith this random assumpt requir eas analysi function fr ith coordin ith row within ham distanc oothervis this accomplish threshold ith input paramet rand two essenti paramet kanerva associ memori rand set correct number 1s represent fr zx vill small comparison number os henc fr z~i consid spars intern represent second stage memori oper usual way except intern represent g w vhere l-v lyu jr zxu i=l threshold function whose ith coordin ith input greater ith input less ith column l-v regard memori locat vhose address ith row z everi vi thin ham distanc ith rov access this locat henc known access radius number memori locat approach taken this paper fix linear rate grov vith fix exponenti rate grov it turn capac grov fix exponenti rate depend exponenti rate suffici overcom standard loos simpl polynomi bound error due combinatori approxim capac kanerva associ memori fix let input address length let tn output word length it assum tn polynomi in tn exp logn let ijmj access radius let l nj number memori locat let lonj radius attract let afn number store word compon n-vector x mn m-vector matrix assum lid equiprob random variabl final given n-vector let g w fr zx ef nl yu jr zxw jf defin quantiti cp cp ico p o ko p theorem af if if suffici larg je l afn dh x j p see interpret if exponenti growth rate number store word afn asymptot less everi suffici larg address length realize nx 2n preprocessor matrix associ memori correct fraction error sequenc afn address datum pair thus cp ic lover bound exponenti growth rate capac kanerva associ memori access radius np number memori locat 2nic figur show cp ic o function radius attract fix access radius cp ico p decreas increas this reflect fact fewer address datum pair store if greater fraction error must correct increas begin lower point fall less steepli in moment we shall see adjust provid optim perform given shovil in figur behavior cp function k howev behavior simpl remain unchang simpli shift dovil differ this establish condit kanerva associ memori robust random compon failur although increas number memori locat beyond increas capac it increas robust random proof iil. figur graph cp lco p o defin upper envelop compon failur affect capac mani compon fail number surviv memori locat less 2nlco perhap import curv exhibit in figur sphere pack upper bound h2 achiev particular equival upper bound achiev particular equal poco jt io l thus specifi optim valu paramet p. respect function shown in figur these optim valu simplifi sphere pack bound it also seen in exponenti growth rate capac asymptot equal k. exponenti growth rate number memori locat mn o logn logn kanerva keeler argu capac at proport number memori locat i.e mn constant thus result consist kanerva keeler provid polynomi logn prove constant howev usual statement result that capac simpli proport number memori locat fals sinc in light univers lil rij. figur graph ko p invers defin upper bound it imposs capac grow without bound no depend dimens in formul this difficulti aris we explicit relat number memori locat input dimens kn in fact our formul provid explicit coher relationship follow variabl capac the number memori locat the input output dimens tn the radius attract the access radius we therefor abl general the result the case provid explicit express the asymptot optim valu as well conclus we describ fair general model associ memori select use definit capac a univers upper bound the growth the capac associ memori shown a sphere pack argument exponenti with rate the binari entropi function the radius attract we review the oper the kanerva associ memori state a lower bound on the exponenti growth rate it capac this lower bound meet the univers upper bound optim valu the memori paramet k. we provid explicit formula for these optim valu previous result for state that the capac of the kanerva associ memori is proport the number of memori locat not strict true our formul correct the problem and general result to the case
----------------------------------------------------------------

title: 100-storing-covariance-by-the-associative-long-term-potentiation-and-depression-of-synaptic-strengths-in-the-hippocampus.pdf

store covari associ long term potenti depress synapt strength hippocampus patric k. stanton terrenc j. sejnowski depart biophys john hopkin univers baltimor md abstract model studi memori base neural network select enhanc depress synapt strength requir ror effid storag inrorm sejnowski kohonen bienenstock ai sejnowski tesauro test assumpt hippocampus cortic structur brain involv long-term memori brier high-frequ activ excitatori synaps hippocampus produc increas synapt strength known long-term potenti ltp buss lomo last ror mani day ltp known hebbian sinc requir simultan releas neurotransmitt presynapt termin coupl postsynapt depolar kelso al malinow miller gustatrson al howev mechan ror persist reduct synapt strength could balanc ltp yet demonstr studi associ interact separ input onto dendrit tree hippocamp pyramid cell field cal round low-frequ input itselr persist chang synapt strength either increas associ ltp decreas strength associ long-term depress ltd depend upon whether posit negat correl time second high-frequ burst input ltp synapt strength hebbian ltd anti-hebbian sinc elicit pair presynapt fire postsynapt hyperpolar suffici block postsynapt activ thus associ ltp associ lto capabl store inrorm contain covari separ converg hippocamp input present address dep~ent new'osci neurolog albert einstein colleg medicin pelham parkway south bronx ny usa tpresent address comput neurobiolog laboratori salk institut p.o box san diego ca usa store covari synapt strength hippocampus introduct associ ltp produc hippocamp neuroo lowfrequ weak high-frequ strong input cell simultan activ levi steward levi steward barrionuevo brown stimul alon weak input long-last effect synapt strength howev pair stimul separ strong input suffici produc homo synapt ltp pathway weak pathway associ potenti neural network model studi predict addit hebbian form plastic synapt strength weaken weak strong input anti-correl sejnowski kohonen bienenstock al sejnowski tesauro evid heterosynapt depress hippocampus found input inact levi steward lynch al weak activ levi steward stimul strong input depress depend pattern weak input activ typic long-last ltp therefor search condit stimul hippocamp pathway rather inact could produc either long-term depress potenti synapt strength depend pattern stimul stimulus paradigm use illustr i base find burst stimuli hz optim elicit ltp hippocampus larson lynch highfrequ burst s'irong stimulus appli schaffer collater axon lowfrequ weak stimulus given separ subicular input come opposit side record site termin dendrit popul cal pyramid neuron due rhythmic natur strong input burst weak input shock could either superimpos middl burst strong input phase place symmetr burst phase result extracellular evok field potenti record apic dendrit somat layer cal pyramid cell weak stimulus train first appli alon induc long-last chang strong site stimul alon elicit homosynapt ltp strong pathway signific alter amplitud respons weak input weak strong input activ phase associ ltp weak input synaps shown synapt excitatori post-synapt potenti popul action potenti pike signific enhanc least min min follow stimul contrast weak strong input appli phase elicit associ long-term depress lto weak input synaps shown there mark reduct popul spike smaller decreas note stimulus pattern appli input ident two experi relat stanton sejnowski phase weak strong stimuli alter stimulus pattern synapt strength could repeat enhanc depress singl slice illustr fig control experi determin whether inform concern covari input actual determin plastic combin phase phase condit give weak input shock superimpos burst plus burst net frequenc hz pattern result zero covari weak strong input produc net chang weak input synapt strength measm extracellular evok potenti thus assoa a.ssocia.t stimulus pa.ra.digm posjtive.li corkela ted phase si1iong njo\it u.jj1l negat correl phase w akin'ltf stiong i figur hippocamp slice prepar stimulus paradigm vitro hippocamp slice show record site cal pyramid cell somat stratum pyramidal dendrit stratum radiatum layer stimulus site activ schaffer collater strong commissur weak affer hippocamp slice jlm thick incub interfac slice chamber c. extracellular m l resist 2m naci fill intracellular 2m k-acet fill record electrod bipolar glass-insul platinum wire stimul electrod jlm tip diamet prepar standard method modi al stimulus paradigm use strong input stimuli strong input four train hz burst burst stimuli interburst interv msec train last second total stimuli weak input stimuli weak input four train shock hz frequenc train last second when input phase weak singl shock superimpos middl each burst strong input when weak input phase singl shock place symmetr burst store covari synapt strength hippocampus ciativ ltp ltd mechan appear balanc manner ideal storag tempor covari relat simultan depolar postsynapt membran activ glutam receptor n-methyl-d-aspart nmda subtyp appear necessari ltp induct collingridg harri al wigstrom gustaffson sj read current strong weak synaps dendrit tree associ lon .te i'otenti long-t de /tession associ i i i i i i figur mustrat associ long-term potenti ltp associ longterm depress ltd use extracellular record associ ltp evok excitatori postsynapt potenti popul action potenti respons weak inpul test respons shown pre min post applic weak stimuli phase coactiv strong input associ ltd evok popul spike respons weak input test respons shown pre min post applic weak stimuli phase coactiv strong input time cours chang popul spike amplitud observ each input typic experi test respons strong input open circl show high-frequ burst pulses/l00 hz msec interburst interv elicit synapse-specif ltp independ input activ test respons weak input fill circl show stimul weak pathway out phase strong one produc associ ltd assoc ltd input associ ltp assoc ltp pathway elicit follow phase stimul amplitud durat associ ltd ltp could increas stimul input pathway train shock stanton sejnowski coupl releas glutam weak input could account abil strong pathway associ potenti weak one kelso al malinow miller gustaffson al consist hypothesi we find nmda receptor antagonist 2-amino-s-phosphonovaler acid ap block induct associ ltp cal pyramid neuron data shown contrast applic ap bath solut concentr signific effect associ ltd data shown thus induct ltd seem involv cellular mechan differ associ ltp condit necessari ltd induct explor anoth seri experi use intracellular record cal pyramid neuron made use standard techniqu modi al induct associ ltp fig weak s+w phase produc increas amplitud singl cell evok lower action potenti threshold weak pathway report previous barrionuevo brown convers induct associ ltd weak s+w out phase accompani long-last reduct amplitud reduc abil elicit action potenti fire control extracellular experi weak input alon produc long-last alter intracellular fire properti strong input alon yield specif increas strong pathway without alter elicit weak input stimul pre min post s+w out phase min post s+w phase figur demonstr associ ltp ltd use intracellular record cal pyramid neuron intracellular prior repetit stimul min out phase stimul out phase min subsequ phase stimuli phase strong input schaffer collater side lower trace exhibit ltp evok independ weak input activ out phase stimul weak subicular side upper trace pathway produc mark persist reduct amplitud cell subsequ phase stimuli result associ ltp weak input revers ltd enhanc amplitud past origin baselin rmp my rn mo store covari synapt strength hippocampus weak stimulus out phase strong one aniv when postsynapt neuron hyperpolar consequ inhibitori postsynapt potenti afterhyperpolar mechan intrins pyramid neuron this suggest postsynapt hyperpolar coupl presynapt activ may trigger l'id test this hypothesi we inject current intracellular microelectrod hyperpolar depolar cell stimul synapt input pair inject depolar current weak input led ltp synaps stim pre idpost s'i1m depol coi'itrol jj i w.c ulvllj pre loliiin post stlm hyperpol figur pair postsynapt hyperpolar stimul synaps cal hippocamp pyramid neuron produc l'id specif activ pathway pair postsynapt depolar synapt stimul produc synapsespecif ltp intracellular evok shown stimul stim unstimul control pathway synaps pre min post pair my depolar constant current na hz synapt stimul stimul pathway exhibit associ ltp control unstimul input show chang synapt strength rmp my rn mfl intracellular shown evok stimul control pathway synaps pre min post pair mv hyperpolar constant current na hz synapt stimul input stim activ hyperpolar show associ ltd synapt evok synapt strength silent input control unalt rmp mv rn stanton sejnowski control input inact stimul chang control report previous kelso al malinow miller gustaffson al convers prolong hyperpolar current inject pair low-frequ stimuli led induct ltd stimul pathway stim unstimul pathway control applic either depolar current hyperpolar current weak hz synapt stimul alon induc long-term alter in synapt strength thus hyperpolar simultan presynapt activ suppli suffici condit induct ltd in cal pyramid neuron conclus these experi identifi a novel fono anti-hebbian synapt plastic in the hippocampus confirm predict made model studi inform storag in neural network unlik previous report of synapt depress in the hippocampus the plastic associ long-last produc when presynapt activ occur the postsynapt membran hyperpolar in combin hebbian mechan also present hippocamp synaps associ ltp associ ltd may allow neuron in the hippocampus comput store covari input sejnowski stanton sejnowski these find make tempor as well as spatial context import featur of memori mechan in the hippocampus elsewher in the brain the recept field properti of cell in cat visual cortex alter visual experi pair iontophoret excit depress of cellular activ fregnac al greuel al in particular the chronic hyperpolar of neuron in visual cortex coupl with presynapt transmitt releas lead a long-teno depress of the activ inact input the later genicul nucleus reiter stryker thus both hebbian anti-hebbian mechan found in the hippocampus seem also present in brain area covari of fire pattern converg input a like key understand higher cognit function this research support by grant the nation scienc foundat the offic of naval research to tjs we thank drs charl steven and richard morri discuss relat experi
----------------------------------------------------------------

title: 6121-dense-associative-memory-for-pattern-recognition.pdf

dens associ memori pattern recognit dmitri krotov simon center system biolog institut advanc studi princeton usa krotov ias.edu john j. hopfield princeton neurosci institut princeton univers princeton usa hopfield princeton.edu abstract model associ memori studi store reliabl retriev mani pattern number neuron network propos simpl dualiti dens associ memori neural network common use deep learn associ memori side dualiti famili model smooth interpol two limit case construct one limit refer feature-match mode pattern recognit one prototyp regim deep learn side dualiti famili correspond feedforward neural network one hidden layer various activ function transmit activ visibl neuron hidden layer famili activ function includ logist rectifi linear unit rectifi polynomi higher degre propos dualiti make possibl appli energy-bas intuit associ memori analyz comput properti neural network unusu activ function higher rectifi polynomi use deep learn util dens memori illustr two test case logic gate xor recognit handwritten digit mnist data set introduct pattern recognit model associ memori close relat consid imag classif exampl pattern recognit problem network present imag task label imag case associ memori network store set memori vector typic queri network present incomplet pattern resembl ident one store memori task recov full memori pixel intens imag combin togeth label imag one vector serv memori associ memori imag thought partial memori cue task identifi appropri label subpart associ memori reconstruct limit use idea pattern recognit standard model associ memori work well limit number store pattern much smaller number neuron equival number pixel imag order pattern recognit small error rate one would need store mani memori typic number pixel present imag serious problem solv modifi standard energi function associ memori quadrat interact neuron includ higher order interact proper design energi function hamiltonian model higher order interact one store reliabl retriev mani memori number neuron network deep neural network proven use broad rang problem machin learn includ imag classif speech recognit object detect etc model compos sever layer neuron output one layer serv input next layer confer neural inform process system nip barcelona spain neuron calcul weight sum input pass result non-linear activ function tradit deep neural network use activ function hyperbol tangent logist learn weight network use backpropag algorithm face serious problem these issu larg resolv introduc unsupervis pre-train made possibl initi weight way subsequ backpropag could gentl move boundari class without destroy featur detector recent realiz use rectifi linear unit relu instead logist function speed learn improv general rectifi linear function usual interpret fire rate biolog neuron these rate equal zero input certain threshold linear grow input threshold mimic biolog output small zero input threshold much less clear behavior activ function input exceed threshold grow linear sub-linear faster linear how choic affect comput properti neural network there function would work even better rectifi linear unit these question best knowledg remain open paper examin these question len associ memori start discuss famili model associ memori larg capac these model use higher order higher quadrat interact neuron energi function associ memori descript then map onto neural network one hidden layer unusu activ function relat hamiltonian we show vari power interact vertex energi function equival chang activ function neural network one forc model learn represent data either term featur term prototyp associ memori larg capac standard model associ memori use system binari neuron valu configur neuron denot vector model store memori denot moment also assum binari model defin energi function given by tij tij dynam updat rule decreas energi everi updat basic problem follow present new pattern network should respond store memori close resembl input there larg amount work communiti statist physicist investig capac model maxim number memori network store reliabl retriev demonstr case random memori maxim valu order max if one tri store pattern sever neighbor memori configur space merg togeth produc ground state hamiltonian noth store memori by modifi hamiltonian way remov second order correl store memori it possibl improv capac max mathemat reason model get confus mani memori store sever memori produc contribut energi order word energi decreas slowli pattern approach memori configur space order take care problem consid modif standard energi this formula smooth function summat index assum comput capabl model illustr two case first integ number refer polynomi energi function second rectifi polynomi energi function case polynomi function network reduc standard model associ memori if term becom sharper compar case thus memori pack configur space cross-talk interven have defin energi function one deriv iter updat rule lead decreas energi we use asynchron updat flip one unit time updat rule sign argument sign function differ two energi one configur i-th unit clump current state i-th unit state one similar configur i-th unit state this rule mean system updat unit given state rest network in way energi entir configur decreas for case polynomi energi function similar famili model consid in updat rule in model base induc magnet field howev differ energi two slight differ due presenc self-coupl term throughout this paper we use energy-bas updat rule how mani memori model store reliabl retriev consid case random pattern each element memori equal equal probabl imagin system initi in state equal one memori pattern number one deriv stabil criterion upper bound number memori network stay in initi state defin energi differ initi state state spin flip polynomi energi function use this quantiti mean ei 2nn come term varianc limit larg i-th bit becom unstabl magnitud fluctuat exceed energi gap ei sign fluctuat opposit sign energi gap thus probabl state singl neuron unstabl limit are larg nois effect gaussian equal z1 nn dx perror ei requir this probabl less small valu say one find the upper limit the number pattern the network store max numer constant depend on the arbitrari threshold the case correspond the standard model associ memori give the well known result for the perfect recoveri a memori perror one obtain nn max kno error ln n for higher power the capac rapid grow in a non-linear way allow the network to store reliabl retriev mani more pattern the number neuron it in accord1 this non-linear scale relationship the capac the size the network the phenomenon we exploit the n-depend coeffici in depend on the exact form the hamiltonian the updat rule
----------------------------------------------------------------

title: 968-capacity-and-information-efficiency-of-a-brain-like-associative-net.pdf

capac inform effici brain-lik associ net bruce graham david willshaw centr cognit scienc univers edinburgh buccleuch place edinburgh eh8 uk email bruce cns.ed.ac.uk david cns.ed.ac.uk abstract determin capac inform effici associ net configur brain-lik way partial connect noisi input cue recal theori use calcul capac pattern recal achiev use winners-takeal strategi transform dendrit sum accord input activ unit usag great increas capac associ net condit moder spars pattern maximum inform effici achiev low connect level correspond level connect common seen brain invit specul brain connect inform effici way introduct standard network associ memori becom plausibl model associ memori brain incorpor partial connect spars activ recal noisi cue paper consid capac binari associ net willshaw buneman longuet-higgin willshaw buckingham contain featur associ net simpl model associ memori behaviour storag devic trivial yet tractabl theoret analysi abl calcul bruce graham david willshaw capac net differ configur differ pattern recal strategi consid capac function connect level winners-take-al recal use associ net heteroassoci memori pair binari pattern store alter connect weight input output unit via hebbian learn rule after pattern storag output pattern recal present previous store input pattern input unit output unit becom activ recal determin appli threshold activ measur output unit make input cue pattern common use measur weight sum input dendrit sum amongst simpler threshold strategi winners-take-al wta approach choos requir number output unit highest dendrit sum activ work well net fulli connect input unit connect everi output unit input cue noise-fre howev recal perform deterior rapid net partial connect input unit connect output unit cue noisi marr recognis associ net partial connect anoth use measur threshold set total input activ sum input regardless connect weight ratio dendrit sum input activ better discrimin output unit activ dendrit sum alon buckingham willshaw show differ unit usag number pattern output unit activ storag caus variat dendrit sum make accur recal difficult input cue noisi they incorpor input activ unit usag measur recal strategi minimis number error output pattern set activ threshold unit unit basi rather complex threshold set strategi simpl winners-take-al we previous demonstr via comput simul graham will haw wta threshold strategi achiev recal perform minimis approach dendrit sum transform certain function input activ unit usag threshold appli we calcul capac associ net wta recal use three differ function dendrit sum pure dendrit sum modifi input activ modifi input activ unit usag result show four time capac obtain transform dendrit sum function input activ unit usag increas capac obtain without loss inform effici moder spars pattern use wta recal inform effici low level connect minimis approach threshold set buckingham this connect rang similar common seen brain capac infonn effici brain-lik associ net notat oper associ net consist binari output unit connect proport binari input unit pair binari pattern store net input output pattern contain ma mb activ unit respect activ level connect weight start zero present net pattern pair storag connect weight activ input unit activ output unit set dure recal input cue pattern present input unit input cue noisi version previous store input pattern fraction ma activ unit come store pattern threshold strategi appli output unit determin activ activ respons input cue call high unit those inact call low unit we consid winners-take-al wta threshold strategi choos activ mb output unit highest valu three function dendrit sum input activ unit usag function list tabl normalis strategi deal partial connect transform strategi reduc variat dendrit sum due differ unit usag this function minimis varianc low unit dendrit sum respect unit usag graham willshaw tabl wta strategi wta strategi basic normalis transform function d/a d/a l/r recal theori capac associ net defin number pattern pair store one bit error recal output pattern this calcul analyt net configur studi howev determin numer wta recal strategi calcul recal respons differ number store pattern minimum valu found which recal error occur wta recal respons calcul theoret use express distribut dendrit sum low high output unit probabl dendrit sum low high output unit particular valu respect buckingham willshaw buckingham zp rj ma bruce graham david willshaw p dh probabl arbitrarili select activ input connect weight low unit oa r high unit good approxim sp r l oay probabl particular activ input cue pattern genuin belong store pattern spurious respect buckingham willshaw basic wta respons calcul use distribut find threshold give number fals posit fals negat error respons given actual distribut normalis dendrit sum distribut dja purpos calcul normalis wta respons possibl use basic distribut situat everi unit mean input activ maz this case low high unit distribut approxim p til p d'h due nonlinear transform use possibl calcul transform distribut simpl sum binomi follow approach use generat transform wta respons given transform threshold possibl valu unit usag equival normalis threshold calcul via transform cumul probabl calcul normalis distribut p dj di til normalis transform wta respons calcul manner basic respons use appropri probabl distribut capac inform effici brain-lik associ net nois nois ca co ca connect connect figur capac versus connect result extens simul previous carri wta recal larg associ net follow specif graham willshaw na ma nb mb agreement simul theoret recal describ extrem good indic approxim use theori valid here we use theoret recal calcul capac result this larg associ net easili obtain via simul result shown generat use theori describ previous section figur show capac function connect differ wta strategi nois input cue nois cue legend basic wta normalis wta transform wta clariti individu data point omit nois cue normalis transform method perform ident normalis result shown figur highlight effect normalis dendrit sum input activ net partial connect figur show effect nois capac capac recal strategi given connect level much reduc compar noise-fre case howev connect greater capac transform wta much greater either normalis basic wta relat capac differ strategi shown figur legend nib ratio normalis basic capac i ratio transform basic tin ratio transform normalis noise-fre case figur low level connect relat capac distort basic capac bruce graham david willshaw drop near zero even low normalis capac relat larg connect level normalis wta provid time capac basic wta noisi case figur normalis capac time basic capac this rang connect transform wta howev provid near time basic capac near time normalis capac connect greater capac interpret inform theoret term consid inform effici net this ratio amount inform retriev net number bit storag avail given ro10jznanb ro capac amount inform contain output pattern nanb number weight bit storag requir willshaw buckingham willshaw inform effici function connect shown figur there distinct peak inform effici recal strategi low level connect peak inform effici effici full connect summaris in tabl greatest contrast full partial connect seen normalis wta noise-fre cue connect normalis wta near time effici full connect in absolut term howev normalis capac connect compar with full connect peak effici obtain normalis wta approach theoret approxim maximum for fulli connect net willshaw discuss previous simul graham willshaw shown input cue noisi recal perform winners-take-al threshold strategi appli partial connect associ net great improv dendrit sum output unit transform function input activ unit usag we confirm extend these result here calcul theoret capac associ net function connect for moder spars pattern use all recal strategi inform effici low level connect howev optimum connect level depend on pattern code rate extend analysi willshaw partial connect net use normalis wta recal yield maximum inform effici obtain zma log2 nb so for input code rate higher log2 nb partial connect net inform effici for input code rate use here this relationship give optimum connect level close to obtain recal theori compar peak effici across differ strategi for noisi cue case the normalis wta twice effici the basic wta while the transform wta three time effici this comparison includ the capac inform effici brain-lik associ net nois nois nib tib iii i iii a connect i i i nib t/b i tin i iii connect figur relat capac versus connect nois nois ffic ia r-o connect f.l i i connect figur inform effici versus connect tabl inform effici wta strategi basic normalis transform peak tjo nois tjo peak peak t nois tjo peak bruce graham david willshaw cost store input activ unit usag inform if one bit storag per connect requir for the input activ anoth bit for the unit usag the inform effici the normalis wta halv the inform effici the transform wta reduc two third this result in all the strategi the peak effici howev the absolut capac the differ strategi peak effici for the basic normalis transform wta respect so the level effici the transform wta deliv four time the capac the basic wta in conclus numer calcul the capac the associ net show inform effici at a low level connect moder spars pattern store includ input activ and unit usag inform the recal calcul result in a four-fold increas in storag capac without loss effici acknowledg to the medic research council for financi support programm grant pg
----------------------------------------------------------------

