query sentence: Associative database
---------------------------------------------------------------------
title: 49-connecting-to-the-past.pdf

connect past bruce a. macdonald assist professor knowledg scienc laboratori comput scienc depart univers calgari univers drive nw calgari alberta t2n in4 abstract recent renew interest neural-lik process system evidenc exampl two volum parallel distribut process edit rumelhart mcclelland discuss parallel distribut system connectionist model neural net valu pass system multipl context system dissatisfact symbol manipul paradigm artifici intellig seem part respons attent encourag promis massiv parallel system implement hardwar paper relat simpl neural-lik system base multipl context well-known formalisms-nam product system k-iength sequenc predict finite-st machin ture machines-and present earlier sequenc predict result new light introduct reviv neural net research strong exemplifi recent rumelhart mcclelland new journal number meet net also describ parallel distribut system connectionist model valu pass systems3 multipl context learn systems4 symbol manipul paradigm artifici intellig seem success hope seem last real promis massiv parallel system implement hardwar howev flurri new work import consolid new idea place solid alongsid establish one paper relat simpl neural-lik system well-known notions-nam product system k-iength sequenc predict finite-st machin ture machines-and present earlier result abil network new light general form connectionist system lo simplifi three layer net binari fix weight hidden layer therebi avoid mani difficulties-and challengesof recent work neural net hidden unit weight regular pattern use templat sophist expens learn algorithm avoid simpl method use determin output unit weight way gain advantag multilay net retain simplic two layer net train method certain noth lost comput power-a i explain-and limit two layer net carri simplifi three layer one biolog system may similar avoid need learn algorithm simul anneal method common use connectionist model one thing biolog system clear distinguish train phase briefli simplifi net product system implement three layer neuron-lik unit output layer input layer hidden layer product hidden product unit potenti connect predetermin set input output k-iength sequenc predictor form ie level delay unit introduc input layer k-iength predictor unabl distinguish simpl sequenc ba aa sinc ie charact system forgotten whether appear first k-iength predictor augment auxiliari action abl learn regular languag sinc auxiliari action equival state input aamong 1st intern confer neural net san diego ca june con.fer brough equival singl context system andrea 's multipl context system see also macdonald american institut physic figur general form connectionist system form unit oper within unit in~ut excitation-.i weight sum activ w output typic typic product unit enabl predict depend previous state combin sever augment sequenc predictor thring machin tape simul along finite-st control give net comput power univers ture machin relat simpl neural-lik system lack comput abil previous implement abil product system equival simplifi net organ paper next section briefli review general form connectionist system section simplifi section explain result equival product system deal input output net section extend simplifi version enabl learn predict sequenc section explain comput power sequenc predictor increas thring machin input unit receiv auxiliari action fact system learn ture machin section discuss possibl number net combin output form overal net associ area general form connectionist system figur show general form connectionist system unit neuron ce1l figur unit input output oj possibl unit network output net input excit net weight sum input vij weight connect output unit input unit activ unit function fi net input excit typic fi semilinear non-decreas differenti function least larg group unit output function fi activ typic kind threshold function i assum quantiti vari discret time step exampl activ time given fi neti general restrict connect may made unit unit connect direct input output hidden unit complex net describ paper may one type connect figur show common connect topolog three layer units-input hidden output-with cycl connect net train present input combin along desir output combin train system produc desir output given figur basic structur three layer connectionist system input unit hidden unit output unit input dure train weight adjust fashion reduc discrep desir actual output general method lo desir train activ equat general form hebb 's classic rule adjust weight two unit high activ lo weight adjust product two function one depend desir actual activ often difference-and anoth depend input weight weight simpl exampl suppos differ output oj weight chang product output error input excit weight constant determin learn rate widrow-hoff delta rule may use net without hidden unit 1o import contribut recent work connectionist system implement equat hidden unit train signal ti direct avail boltzmann learn method iter vari weight hidden unit train activ use control gradual decreas random method simul anneal backpropag also iter perform gradient descent propag train signal error back net hidden unit i avoid need determin train signal hidden unit fix weight hidden unit section simplifi system assum simplif made general connectionist system section system three layer topolog shown figur ie cycl hidden layer unit weight fix say uniti zero unit linear threshold unit lo mean activ function unit ident function give net weight sum input output function simpl binari threshold form i output threshold activ output binari oft hidden unit threshold requir input activ output activ like gate output unit threshold requir two activ high weight input output generat like gate keep product system view net explain section learning-which occur output unit weights-giv weight adjust accord wij wij oj otherwis weight turn input unit output otherwis wij oj simpl exampl given figur section simpl form net made probabilist replac adjust weight wij estim condit probabl unit output output wij estim p odoj assum independ input unit output unit turn condit probabl occurr output exceed threshold output function onc simplif made need learn hidden unit also iter learn requir weight either assign binari valu estim condit probabl paper present characterist simplifi net section discuss motiv simplifi neural net way product system simplifi net kind simpl product system product system compris global databas set product rule control system databas net system interact provid input reaction output t.he net hidden unit network product rule form precondit action precondit satisfi input excit exceed threshold hidden unit action repres output unit hidden product unit activ control system product system choos rule whose action perform set rule whose precondit met neural net control system distribut throughout net output unit exampl output unit might form winner-takeal net product system complex control involv forward backward chain choos action seek goal discuss elsewhere4.12.16 figur illust.r simpl product implement neural net figur show input hidden unit element precondit appropri input combin present associ hidden product unit fire onc weight leam connect hidden unit output unit fire product result output simplifi neural net direct equival product system whose element input output product system symbol element variabl given valu product action neural net direct implement sinc output predetermin set howev see later extens t.o framework enabl abil cthis might refer sensory-motor product system sinc implement ill l'eal system robot deal sens input execut motor action may includ auxiliari action section figur product implement simplifi neural net product rule icloudi i ipressur fall i then iit rain i rule implement hidden unit threshold hidden unit gate threshold output unit gate learn weight if net probabilist otherwis estim p rainlcloud pressur fall rain weight figur net predict next charact sequenc base last charact net product unit hidden unit combin input unit exampl net could predict sequenc abcabcabc product form if last charact then next charact learn rule wij if inputj outputi output wijoj input neural net output learn procedur clamp input output desir valu system calcul weight valu repeat requir input/output combin sequenc predict product system neural net predict sequenc given exampl repeat sequenc product learn predict futur event basi recent one figur show trivial simpl sequenc predictor predict next charact sequenc base previous one figur also give detail learn procedur simplifi net net need train onc input combin then predict output everi charact seen current one probabilist form net would estim condit probabl next charact condit current one mani figur use delay input neural net implement k-length sequenc predictor net last three charact input input hidden output 2nd last exampl product if last three charact then present possibl charact pair would need proper estim probabl net would learn probabl distribut charact pair predictor like one figur extend general k-iength predictor long input delay step avail then illustr figur 3-length predict hidden product unit repres possibl combin symbol again output weight train respond previous seen input combin three charact delay provid dedic neural net shown figur note net assum synchron updat input feedback around unit chang one step output chang various way implement delay neuron andrea investig purpose-delay inputs-in detail simul similar net work sequenc predict neural net feldman ballard find connectionist system initi suit repres chang time one form chang sequenc suggest two method repres sequenc net first unit connect each sequenc sequenti task repres fire these unit success second method buffer input time input recent past avail well current input delay input avail suggest import differ necessari length buffer feldman ballard suggest buffer long enough hold phrase natur languag i expect use buffer longer andrea symbol input repres complex inform effect give length seven buffer inform recent seven simpl input discuss section method back-propagation13 enabl recurr network learn sequenti task dfeldman ballard2 give dedic neural net connect varieti flulction figur input delay dedic neural subnet two stage delay shown delay network time diagram tml origin signal delay one step delay two step manner similar first suggest last paragraph sequenc connect unit repres sequenc event one exampl net learn complet sequenc charact given first two charact six charact sequenc next four output error must propag around cycl recurr net number time serial may also achiev sequenc state distribut activ exampl net play side tic-tac-to game sequenti natur net 's behavior deriv sequenti natur respons net 's action tic-tac-to move net model sequenc intern model sequenti part environ exampl tic-tac-to play net model oppon k-iength sequenc predictor unabl learn sequenc repeat frequent everi charact their k-iength context includ inform last event howev there two way inform kth last input retain net first method latch input second involv auxiliari action latch unit input latch held indefinit use combin shown figur input would normal latch andrea discuss this techniqu thread latch event among non-latch event give net inform arbitrarili far back input-output histori inform immedi past briefli sequenc ba distinguish aa if first charact latch howev this ad hoc solut this problem auxiliari action output fed back net input signal this enabl system choos next output least part base previous one indic figur if particular fed back output also one without extern manifest whose extern manifest independ task perform then output auxiliari action las interest reader refer andrea extens analysi given figur thread latch circuit rememb event anoth come along this two input latch two letter number unit may similar connect form mutual inhibit layer winner-take-al connect along posit feedback keep select output activ when input disappear figur auxiliari actions-th outputs-ar fed back input net enabl net rememb state here part net exampl product shown there two type action charact action sinput output charact input if input charact input charact output then output charact llij ill direct effect task system perform sinc it evok relev input use net symbol action if auxiliari action latch input then symbol inform rememb indefinit lost when anoth auxiliari action kind input take latch thus auxiliari action act like rememb state system perform action remind particular state figur illustr this system that predict charact state chang given previous charact state an obvious candid auxiliari action speech blank oval figur would repres net 's environ speech action heard although it extern manifest speech direct effect physic interact the world symbol abil provid the power auxiliari action also includ other speaker the interact simul abstract automata the exampl figur give the essenc simul finit state automaton product system or it neural net equival it illustr the transit function an automaton the new state output function the previous state input thus neural net simul finit state automaton long it addit auxiliari action thring machin finit state automaton control plus an unbound memori neural net could simul a lure machin two way way demonstr product system implementations-equival neural net all multipl context learn system briefli explain section the first thring machin simul the system simul the finit state control abl use an unbound extern memori fsee john andrea 's colleagu work4 figur multipl context learn system implement multipl neural net each:3 layer net the simplifi form present a number elabor extra connect goal-seek forward backward chain output channel the real world much like the paper ture 's origin work the second simnlat.ion emb the memori the multipl context learn system along a counter access this simul memori both learn the productions-equival learn output unit weights-requir the simul the second abl add intern memori requir a limit depend the size the network easili larg enough allow year comput the second could also employ extern memori the first briefli the second simul compris multipl sequenc predictor predict auxiliari action rememb the state the control the current memori posit the memori element updat relearn the product repres that element the precondit the address the product action the store item multipl system form associ area a multipl context learn system product system version a multipl neural net although a simpl version implement a simul net it effect compris sever net or associ areas-which may output input in common indic in figur hidden unit weight specifi templat one each net a templat give the input a zero weight the hidden unit a net the input a weight uniti delay latch input also avail the actual output select the combin predict the net in a winner-take-al fashion i see the design real neural net say control real robot requir a larg degre predetermin connect a robot control could one three layer net wit.h everi input connect everi hidden unit in turn connect everi output there need some connect constraint so the net reflect the function special in the control requir the multipl context learn system all the hidden layer connect predetermin allow output connect learn this avoid the credit assign problem therefor also the need learn algorithm boltzmann learn back-propag howev as the multipl context learn system auxiliari action delay latch input it lack comput power futur work in this area investig for exampl the abil differ kind net learn auxiliari act.ion this may difficult as symbol action may provid in train input and output. for exampl a control for a robot bodi would deal vision manipul motion etc conclus this paper present a simplifi three layer connectionist model fix weight for hidden unit delay and latch for input sequenc predict abil auxiliari state action and the abil use intern and extern memori the result abl learn to simul a ture machin simpl neural-lik system not lack comput power acknowledg this work support by the natur scienc and engin council canada
----------------------------------------------------------------

title: 32-synchronization-in-neural-nets.pdf

synchron neural net jacqu j. vidal univers california los angel los angel ca john haggerti abstract paper present artifici neural network concept synchroniz oscil network instant individu fire form point process constitut form inform transmit join neuron type communic contrast assum model typic continu discret value-pass network limit messag receiv process unit time marker signal fire unit present signific implemen tation advantag model neuron fire spontan regular absenc perturb interact present schedul fire advanc delay fire neighbor neuron network neuron becom global oscil exhibit multipl synchron attractor arbitrari initi state energi minim learn procedur make network converg oscillatori mode satisfi multi-dimension constraint network direct repres rout schedul problem consist order sequenc event introduct most neural network model deriv variant rosenblatt origin perceptron value-pass network case particular network propos fukushima i hopfield rumelhart mani other everi case input process element either binari continu amplitud signal weight synapt gain subsequ sum integr result activ pass sigmoid threshold filter produc continu quantiz output may becom input neuron behavior model relat live neuron even fall consider short account complex inde observ mani real neuron action potenti spike fire propag axon branch intern activ reach threshold higher john haggerti interact system los angel w. 6th st. la ca american institut physic input rate level result rapid fire behind tradit model assumpt averag frequenc action potenti carrier inform neuron becaus integr fire individu neuron consid effect extent contribut averag intens therefor assum activ simpli frequenc code exact time individu fire ignor view howev cover well known aspect neural communic inde precis time spike arriv make crucial differ outcom neural interact one classic exampl pre-synapt inhibit widespread mechan brain machineri sever studi also demonstr occurr function import precis time phase relationship cooper neuron local network model present paper contrast one mention network fire consid individu output event input side node fire node presynapt neuron either delay inhibit advanc excit node fire seen earlier type neuron interact would call phase-modul engin system also find rational experiment neurophysiolog neurophysiolog plausibl howev major concern rather propos explor potenti use mechan parallel distribut comput merit approach artifici neural network digit puls use internod communic instead analog voltag model particular well suit time-ord sequenc found larg class rout trajectori control problem neuron synchroniz oscil model process element neuron relax oscil built-in self-inhibit relax oscil dynam system capabl accumul potenti energi threshold breakdown point reach point energi abrupt releas new cycl begin descript fit dynam behavior neuron membran rich structur empir model this behavior found well-establish differenti formul hodgkin huxley simplifi version given fitzhugh7 differenti equat account foundat neuron activ also capabl repres subthreshold behavior refractori follow fire membran potenti enter critic region abrupt depolar collaps potenti differ across membran occur follow somewhat slower recoveri this brief electr short membran call action potenti spike constitut output event neuron if caus initi depolar maintain oscil limit-cycl develop generat multipl fire depend input level membran paramet oscil limit singl spike may produc oscillatori burst even continu sustain activ present model share general properti use much simpler descript relax oscil illustr figur activ energy exdt3toij oj input injrjh1~olj input perturb utl intemilf l neju inpul ty figur relax oscil perturb input fire occur energi level reach critic level ec assum constant rate energi influx fire occur natur period ec t=a when pre-synapt puls imping on cours energi accumul fire schedul disturb let repres instant last fire cell tj intant imping arriv cell e act wj uo til ec uo repres unit impuls dramat complex synchron dynam appreci consid simplest possibl case master slave interact two regular fire oscil unit natur period ta tb instant fire unit unidirect send spike signal unit receiv interv measur from last time fire upon recept spike transform quantum energi 6e depend upon post-fir arriv time relationship shape repres refractori post-spik properti here it assum simpl ramp function if interact inhibitori consequ this arriv next fire unit delay respect schedul would absenc perturb posit interv figur becaus shape delay action nil immedi fire becom longer imping pre-synapt spike arriv later interv if interact excitatori delay negat le shorten natur fire interv general assumpt regard function tend synchron within given rang coupl gain phase self-adjust equilibrium achiev given this equilibrium correspond distribut maximum entropi point cell receiv amouint activ common cycl i i inhibit excit figur relationship phase delay when input effici increas linear after-spik interv synchron dynam present attractor ration frequenc pair ratio associ rang stabil ratio lowest cardin wide zone phaselock figur wider stabil wnes correspond one one ratio fa fb invers ta tbl kohn segundo demonstr such phase lock occur live invertebr neuron point paradox natur phase-lock inhibit within stabil region take appear excit sinc small increas input fire rate local result increas output rate area these rang stabil appear unstabl transit fact as recent point out bak9 form infin lock step known as devil 's staircas correspond infin intermedi ration pair figur bak show staircas self-similar scale transit form fractal cantor set fractal dimens univers constant dynam system i excit inhibiti~~v i li l figur unilater synchron constraint satisfact in oscil network global synchron interconnect network mutual phase-lock oscil constraint satisfact problem synchron equilibrium node fire in interlock pattern organ inter-spik interv integ ratio often cite travel salesman problem archetyp for a class import hard problem a special case when ratio must node must fire at frequenc here the equilibrium condit everi node accumul the the amount energi the global cycl furthermor the fire must order along a minim path use stochast energi minim simul anneal the first simul demonstr the feasibl the approach with a limit number node the tsp isomorph mani sequenc problem involv distribut constraint fall the oscil array neural net paradigm in a particular natur way work pursu rigor establish the limit applic the model i annea/./ng figur the travel salesman problem in the global oscil oj minim energi node constrain fire at the rate in the order correspond to the minim path acknowledg research support in part aerojet electro-system under the aerojet-ucla cooper research master agreement no nasa nag
----------------------------------------------------------------

title: 56-discovering-structure-from-motion-in-monkey-man-and-machine.pdf

discov strucfur motion monkey man machin ralph m. siegel salk institut biolog la jolla ca abstract abil obtain three-dimension structur visual motion import surviv human non-human primat use parallel process model current work explor biolog visual system might solv problem neurophysiologist might go understand solut introducnon psychophys experi shown monk man equal adept obtain three dimension structur motion present work much effort expend mimick visual system done one main reason model design help direct physiolog experi primat hope approach understand model could develop approach could direct primat 's visual system earli centuri von helmholtz2 describ problem extract three-dimension structur motion suppos instanc person stand still thick wood imposs distinguish except vagu rough mass foliag branch around belong one tree anoth far apart separ tree etc moment begin move forward everyth disentangl immedi get appercept materi content wood relat space look good stereoscop view object move rather observ percept threedimension structur motion still obtain object-cent structur motion examin report lesion studi monkey demonstr two extra-stri visual cortic call middl tempor area abbrevi mt current address laboratori neurobiolog rockefel univers york avenu new york ny american institut physic medial superior tempor area involv obtain structur motion present model meant mimic v5-mst part cortic circuitri involv obtain structur motion model attempt determin ifth visual imag correspond three-dimension object tile strucfur motion stimulus problem model solv pose studi monkey man structur unstructur motion display hollow orthograph project cylind comput figur cylind rotat vertic axi unstructur stimulus generat shuffl veloc vector random display screen overal veloc spatial distribut two display ident spatial relationship chang unstructur stimulus human subject report point move surfac hollow cylind view structur stimulus unstructur stimulus subject report sens three-dimension structur b. orthograph project rotat cylind unstructur display figur structur unstructur motion stimulus pomt random place surfac cylind point orthograph project motion give strong percept hollow cylind unstructur stimulus generat shuffl veloc vector random screen functionalarchitecfureoftilemodel primat subject model requir indic whether display structur subject requir describ shape veloc size cylind thus output cell model signal cell i mean process unit model may correspond singl neuron group neuron term neuron refer actual wetwar brain structur structur output layer correspond cortic area mst macaqu monkey appear sensit global organ motion image5 known if mst neuron distinguish structur unstructur imag input model base physiolog studi maca~u monkey neuron area v5 retinotop represent visual space retinotop locat encod wide rang velocitiess thus model 's input rep1 resent cell cil repres differ combin veloc retinotop spatial posi0cil tion furthermor motion veloc neuron v5 center-surround oppon organization9 width recept field taken data albright retin posit deg s. typic recept field model shown figur figur recept field input layer cell optim veloc lt possibl determin activ input cell would rotat cylind given represent activ pattern set input cell comput convolv veloc point differ gaussian activ input cell imag point angular veloc so/sec present figur relinotop map retinotop map structur structur figur input cell 's activ pattern structur unstructur stimuius circl correspond cell input layer contour com pute use linear interpol individu cell horizont axi correspond posit along horizont meridian vertic axi correspond speed along horizont meridian thus activ cell upper right hand corner graph correspond veloc sec toward right locat right along horizont meridian inspect input pattern suggest problem detect three-dimension structur motion may reduc pattern recognit task problem given spars sampl input motion flow field determin whether correspond best structur unstructur object itwa next necessari determin connect two input output layer model abl correct signal structur structur wide rang cylind radii rotat veloc parallel distribut network type use rosenberg sejnowski provid function architectur figur i figur parallel architectur use extract structur motion input layer correspond area map posit speed along horizont axi output layer correspond area mst propos signal structur middl layer may exist either v5 mst input layer cell fulli connect middl layer cell middl layer cell repres intermedi stage process may either v5 mst cell middl layer fulli connect output cell input cell lower layer next higher level sum linear threshold use hill equat weight layer initi chosen valu weight adjust use back-propag method steepest descent network would learn correct predict structur input imag model learn correct perform task iter figur figur educ network perform structur motion problem iter number plot mean squar error error defin differ model 's predict known structur model train set structur unstructur cylind wi e rang adii number point rotatlon veloc iter number psychophys perform model model 's perform compar monkey man respect fraction structur number point display figur model inde perform global analysi shown allow model view portion imag like man monkey model 's perform suffer thus appear model 's perform quit similar known monkey human psychophys output monkey man machin monkey man machin fraction structur number point figur psychophys perform model effect vari fraction structur fraction structur increas the model 's perform improv thirti repetit averag valu structur the model the fraction structur defin rs the radius shuffl the motion vector rc the radius the cylind the human monkey data taken psychophys studi done the model similar perform monkey man next possibl examin artifici network order obtain hint studi the biolog system follow the approach electrophysiologist recept field map all the cell the middl ou tput layer made activ individu inpu cell the recept field middl layer cell shown figur the layout map quit similar figur howev the activ one cell the middl layer plot function the locat speed motion stimulus the input layer one could imagin electrod place one the cell the middl layer the experimentalist move bar the horizont meridian differ locat speed the activ the cell plot function posit space relinolop map rj i figur the activ two differ cell the middl layer activ plot contour map function horizont posit speed dot line indic inhibit middl layer recept field map interest appear quit simpl symmetr the inhibitori central region the recept field surround by excitatori region figur complementari cell also found other inhibitori band adjac excitatori band figur the result suggest neuron involv extract structur motion may relat simpl recept field the spatial veloc domain recept field might thought break the imag compon part basi set correct recombin second order cell could use detect the presenc three-dimension structur the output cell also simpl recept field interest symmetri figur howev the recept field analysi insuffici indic the role the cell therefor order proper understand the mean the cell 's recept field it necessari use stimuli real world relev case the structur motion stimuli the output cell would give maxim respons cylind stimulus present figur the recept field map the output layer cell noth recept field structur indic the cell involv obtain structur motion work predict neuron cortex involv extract structur motion relat simpl recept field order test this hypothesi it necessari make care map these cell use small patch motion figur known qualit result area v5 mst consist but prove this hypothesi as well it necessari use relev stimuli three-dimension object if simpl recept field inde use structur motion support found the idea simpl cortic circuit center-surround use for mani differ visual analys motion patch consist random dot with variabl veloc ru fix point figur it may necessari make care map these neuron use small patch motion order observ the postul simpl recept field properti cortic neuron involv extract structur motion such structur may appar use hand move bar stimuli discuss conclus it possibl extract the three-dimension structur rotat cylind use a parallel network base a similar function architectur as found primat cortex the present model similar psychophys monkey man the recept field structur underli the present model simpl view use a spatial-veloc represent it suggest order understand the visual system extract structur motion quantit spatial-veloc map cortic neuron involv need made one also need use stimuli deriv from the real world in order understand how may use in visual field analysi there similar the shape the recept field involv in analyz structur from motion recept field in striat cortex it may similar cortic mechan and connect use perform differ function in differ cortic area last this model demonstr the use parallel architectur close model the cortic represent is a comput effici mean solv problem in vision thus as a final caveat i would like advis the creator of network solv etholog realist problem use solut evolut provid
----------------------------------------------------------------

title: 49-connecting-to-the-past.pdf

connect past bruce a. macdonald assist professor knowledg scienc laboratori comput scienc depart univers calgari univers drive nw calgari alberta t2n in4 abstract recent renew interest neural-lik process system evidenc exampl two volum parallel distribut process edit rumelhart mcclelland discuss parallel distribut system connectionist model neural net valu pass system multipl context system dissatisfact symbol manipul paradigm artifici intellig seem part respons attent encourag promis massiv parallel system implement hardwar paper relat simpl neural-lik system base multipl context well-known formalisms-nam product system k-iength sequenc predict finite-st machin ture machines-and present earlier sequenc predict result new light introduct reviv neural net research strong exemplifi recent rumelhart mcclelland new journal number meet net also describ parallel distribut system connectionist model valu pass systems3 multipl context learn systems4 symbol manipul paradigm artifici intellig seem success hope seem last real promis massiv parallel system implement hardwar howev flurri new work import consolid new idea place solid alongsid establish one paper relat simpl neural-lik system well-known notions-nam product system k-iength sequenc predict finite-st machin ture machines-and present earlier result abil network new light general form connectionist system lo simplifi three layer net binari fix weight hidden layer therebi avoid mani difficulties-and challengesof recent work neural net hidden unit weight regular pattern use templat sophist expens learn algorithm avoid simpl method use determin output unit weight way gain advantag multilay net retain simplic two layer net train method certain noth lost comput power-a i explain-and limit two layer net carri simplifi three layer one biolog system may similar avoid need learn algorithm simul anneal method common use connectionist model one thing biolog system clear distinguish train phase briefli simplifi net product system implement three layer neuron-lik unit output layer input layer hidden layer product hidden product unit potenti connect predetermin set input output k-iength sequenc predictor form ie level delay unit introduc input layer k-iength predictor unabl distinguish simpl sequenc ba aa sinc ie charact system forgotten whether appear first k-iength predictor augment auxiliari action abl learn regular languag sinc auxiliari action equival state input aamong 1st intern confer neural net san diego ca june con.fer brough equival singl context system andrea 's multipl context system see also macdonald american institut physic figur general form connectionist system form unit oper within unit in~ut excitation-.i weight sum activ w output typic typic product unit enabl predict depend previous state combin sever augment sequenc predictor thring machin tape simul along finite-st control give net comput power univers ture machin relat simpl neural-lik system lack comput abil previous implement abil product system equival simplifi net organ paper next section briefli review general form connectionist system section simplifi section explain result equival product system deal input output net section extend simplifi version enabl learn predict sequenc section explain comput power sequenc predictor increas thring machin input unit receiv auxiliari action fact system learn ture machin section discuss possibl number net combin output form overal net associ area general form connectionist system figur show general form connectionist system unit neuron ce1l figur unit input output oj possibl unit network output net input excit net weight sum input vij weight connect output unit input unit activ unit function fi net input excit typic fi semilinear non-decreas differenti function least larg group unit output function fi activ typic kind threshold function i assum quantiti vari discret time step exampl activ time given fi neti general restrict connect may made unit unit connect direct input output hidden unit complex net describ paper may one type connect figur show common connect topolog three layer units-input hidden output-with cycl connect net train present input combin along desir output combin train system produc desir output given figur basic structur three layer connectionist system input unit hidden unit output unit input dure train weight adjust fashion reduc discrep desir actual output general method lo desir train activ equat general form hebb 's classic rule adjust weight two unit high activ lo weight adjust product two function one depend desir actual activ often difference-and anoth depend input weight weight simpl exampl suppos differ output oj weight chang product output error input excit weight constant determin learn rate widrow-hoff delta rule may use net without hidden unit 1o import contribut recent work connectionist system implement equat hidden unit train signal ti direct avail boltzmann learn method iter vari weight hidden unit train activ use control gradual decreas random method simul anneal backpropag also iter perform gradient descent propag train signal error back net hidden unit i avoid need determin train signal hidden unit fix weight hidden unit section simplifi system assum simplif made general connectionist system section system three layer topolog shown figur ie cycl hidden layer unit weight fix say uniti zero unit linear threshold unit lo mean activ function unit ident function give net weight sum input output function simpl binari threshold form i output threshold activ output binari oft hidden unit threshold requir input activ output activ like gate output unit threshold requir two activ high weight input output generat like gate keep product system view net explain section learning-which occur output unit weights-giv weight adjust accord wij wij oj otherwis weight turn input unit output otherwis wij oj simpl exampl given figur section simpl form net made probabilist replac adjust weight wij estim condit probabl unit output output wij estim p odoj assum independ input unit output unit turn condit probabl occurr output exceed threshold output function onc simplif made need learn hidden unit also iter learn requir weight either assign binari valu estim condit probabl paper present characterist simplifi net section discuss motiv simplifi neural net way product system simplifi net kind simpl product system product system compris global databas set product rule control system databas net system interact provid input reaction output t.he net hidden unit network product rule form precondit action precondit satisfi input excit exceed threshold hidden unit action repres output unit hidden product unit activ control system product system choos rule whose action perform set rule whose precondit met neural net control system distribut throughout net output unit exampl output unit might form winner-takeal net product system complex control involv forward backward chain choos action seek goal discuss elsewhere4.12.16 figur illust.r simpl product implement neural net figur show input hidden unit element precondit appropri input combin present associ hidden product unit fire onc weight leam connect hidden unit output unit fire product result output simplifi neural net direct equival product system whose element input output product system symbol element variabl given valu product action neural net direct implement sinc output predetermin set howev see later extens t.o framework enabl abil cthis might refer sensory-motor product system sinc implement ill l'eal system robot deal sens input execut motor action may includ auxiliari action section figur product implement simplifi neural net product rule icloudi i ipressur fall i then iit rain i rule implement hidden unit threshold hidden unit gate threshold output unit gate learn weight if net probabilist otherwis estim p rainlcloud pressur fall rain weight figur net predict next charact sequenc base last charact net product unit hidden unit combin input unit exampl net could predict sequenc abcabcabc product form if last charact then next charact learn rule wij if inputj outputi output wijoj input neural net output learn procedur clamp input output desir valu system calcul weight valu repeat requir input/output combin sequenc predict product system neural net predict sequenc given exampl repeat sequenc product learn predict futur event basi recent one figur show trivial simpl sequenc predictor predict next charact sequenc base previous one figur also give detail learn procedur simplifi net net need train onc input combin then predict output everi charact seen current one probabilist form net would estim condit probabl next charact condit current one mani figur use delay input neural net implement k-length sequenc predictor net last three charact input input hidden output 2nd last exampl product if last three charact then present possibl charact pair would need proper estim probabl net would learn probabl distribut charact pair predictor like one figur extend general k-iength predictor long input delay step avail then illustr figur 3-length predict hidden product unit repres possibl combin symbol again output weight train respond previous seen input combin three charact delay provid dedic neural net shown figur note net assum synchron updat input feedback around unit chang one step output chang various way implement delay neuron andrea investig purpose-delay inputs-in detail simul similar net work sequenc predict neural net feldman ballard find connectionist system initi suit repres chang time one form chang sequenc suggest two method repres sequenc net first unit connect each sequenc sequenti task repres fire these unit success second method buffer input time input recent past avail well current input delay input avail suggest import differ necessari length buffer feldman ballard suggest buffer long enough hold phrase natur languag i expect use buffer longer andrea symbol input repres complex inform effect give length seven buffer inform recent seven simpl input discuss section method back-propagation13 enabl recurr network learn sequenti task dfeldman ballard2 give dedic neural net connect varieti flulction figur input delay dedic neural subnet two stage delay shown delay network time diagram tml origin signal delay one step delay two step manner similar first suggest last paragraph sequenc connect unit repres sequenc event one exampl net learn complet sequenc charact given first two charact six charact sequenc next four output error must propag around cycl recurr net number time serial may also achiev sequenc state distribut activ exampl net play side tic-tac-to game sequenti natur net 's behavior deriv sequenti natur respons net 's action tic-tac-to move net model sequenc intern model sequenti part environ exampl tic-tac-to play net model oppon k-iength sequenc predictor unabl learn sequenc repeat frequent everi charact their k-iength context includ inform last event howev there two way inform kth last input retain net first method latch input second involv auxiliari action latch unit input latch held indefinit use combin shown figur input would normal latch andrea discuss this techniqu thread latch event among non-latch event give net inform arbitrarili far back input-output histori inform immedi past briefli sequenc ba distinguish aa if first charact latch howev this ad hoc solut this problem auxiliari action output fed back net input signal this enabl system choos next output least part base previous one indic figur if particular fed back output also one without extern manifest whose extern manifest independ task perform then output auxiliari action las interest reader refer andrea extens analysi given figur thread latch circuit rememb event anoth come along this two input latch two letter number unit may similar connect form mutual inhibit layer winner-take-al connect along posit feedback keep select output activ when input disappear figur auxiliari actions-th outputs-ar fed back input net enabl net rememb state here part net exampl product shown there two type action charact action sinput output charact input if input charact input charact output then output charact llij ill direct effect task system perform sinc it evok relev input use net symbol action if auxiliari action latch input then symbol inform rememb indefinit lost when anoth auxiliari action kind input take latch thus auxiliari action act like rememb state system perform action remind particular state figur illustr this system that predict charact state chang given previous charact state an obvious candid auxiliari action speech blank oval figur would repres net 's environ speech action heard although it extern manifest speech direct effect physic interact the world symbol abil provid the power auxiliari action also includ other speaker the interact simul abstract automata the exampl figur give the essenc simul finit state automaton product system or it neural net equival it illustr the transit function an automaton the new state output function the previous state input thus neural net simul finit state automaton long it addit auxiliari action thring machin finit state automaton control plus an unbound memori neural net could simul a lure machin two way way demonstr product system implementations-equival neural net all multipl context learn system briefli explain section the first thring machin simul the system simul the finit state control abl use an unbound extern memori fsee john andrea 's colleagu work4 figur multipl context learn system implement multipl neural net each:3 layer net the simplifi form present a number elabor extra connect goal-seek forward backward chain output channel the real world much like the paper ture 's origin work the second simnlat.ion emb the memori the multipl context learn system along a counter access this simul memori both learn the productions-equival learn output unit weights-requir the simul the second abl add intern memori requir a limit depend the size the network easili larg enough allow year comput the second could also employ extern memori the first briefli the second simul compris multipl sequenc predictor predict auxiliari action rememb the state the control the current memori posit the memori element updat relearn the product repres that element the precondit the address the product action the store item multipl system form associ area a multipl context learn system product system version a multipl neural net although a simpl version implement a simul net it effect compris sever net or associ areas-which may output input in common indic in figur hidden unit weight specifi templat one each net a templat give the input a zero weight the hidden unit a net the input a weight uniti delay latch input also avail the actual output select the combin predict the net in a winner-take-al fashion i see the design real neural net say control real robot requir a larg degre predetermin connect a robot control could one three layer net wit.h everi input connect everi hidden unit in turn connect everi output there need some connect constraint so the net reflect the function special in the control requir the multipl context learn system all the hidden layer connect predetermin allow output connect learn this avoid the credit assign problem therefor also the need learn algorithm boltzmann learn back-propag howev as the multipl context learn system auxiliari action delay latch input it lack comput power futur work in this area investig for exampl the abil differ kind net learn auxiliari act.ion this may difficult as symbol action may provid in train input and output. for exampl a control for a robot bodi would deal vision manipul motion etc conclus this paper present a simplifi three layer connectionist model fix weight for hidden unit delay and latch for input sequenc predict abil auxiliari state action and the abil use intern and extern memori the result abl learn to simul a ture machin simpl neural-lik system not lack comput power acknowledg this work support by the natur scienc and engin council canada
----------------------------------------------------------------

title: 32-synchronization-in-neural-nets.pdf

synchron neural net jacqu j. vidal univers california los angel los angel ca john haggerti abstract paper present artifici neural network concept synchroniz oscil network instant individu fire form point process constitut form inform transmit join neuron type communic contrast assum model typic continu discret value-pass network limit messag receiv process unit time marker signal fire unit present signific implemen tation advantag model neuron fire spontan regular absenc perturb interact present schedul fire advanc delay fire neighbor neuron network neuron becom global oscil exhibit multipl synchron attractor arbitrari initi state energi minim learn procedur make network converg oscillatori mode satisfi multi-dimension constraint network direct repres rout schedul problem consist order sequenc event introduct most neural network model deriv variant rosenblatt origin perceptron value-pass network case particular network propos fukushima i hopfield rumelhart mani other everi case input process element either binari continu amplitud signal weight synapt gain subsequ sum integr result activ pass sigmoid threshold filter produc continu quantiz output may becom input neuron behavior model relat live neuron even fall consider short account complex inde observ mani real neuron action potenti spike fire propag axon branch intern activ reach threshold higher john haggerti interact system los angel w. 6th st. la ca american institut physic input rate level result rapid fire behind tradit model assumpt averag frequenc action potenti carrier inform neuron becaus integr fire individu neuron consid effect extent contribut averag intens therefor assum activ simpli frequenc code exact time individu fire ignor view howev cover well known aspect neural communic inde precis time spike arriv make crucial differ outcom neural interact one classic exampl pre-synapt inhibit widespread mechan brain machineri sever studi also demonstr occurr function import precis time phase relationship cooper neuron local network model present paper contrast one mention network fire consid individu output event input side node fire node presynapt neuron either delay inhibit advanc excit node fire seen earlier type neuron interact would call phase-modul engin system also find rational experiment neurophysiolog neurophysiolog plausibl howev major concern rather propos explor potenti use mechan parallel distribut comput merit approach artifici neural network digit puls use internod communic instead analog voltag model particular well suit time-ord sequenc found larg class rout trajectori control problem neuron synchroniz oscil model process element neuron relax oscil built-in self-inhibit relax oscil dynam system capabl accumul potenti energi threshold breakdown point reach point energi abrupt releas new cycl begin descript fit dynam behavior neuron membran rich structur empir model this behavior found well-establish differenti formul hodgkin huxley simplifi version given fitzhugh7 differenti equat account foundat neuron activ also capabl repres subthreshold behavior refractori follow fire membran potenti enter critic region abrupt depolar collaps potenti differ across membran occur follow somewhat slower recoveri this brief electr short membran call action potenti spike constitut output event neuron if caus initi depolar maintain oscil limit-cycl develop generat multipl fire depend input level membran paramet oscil limit singl spike may produc oscillatori burst even continu sustain activ present model share general properti use much simpler descript relax oscil illustr figur activ energy exdt3toij oj input injrjh1~olj input perturb utl intemilf l neju inpul ty figur relax oscil perturb input fire occur energi level reach critic level ec assum constant rate energi influx fire occur natur period ec t=a when pre-synapt puls imping on cours energi accumul fire schedul disturb let repres instant last fire cell tj intant imping arriv cell e act wj uo til ec uo repres unit impuls dramat complex synchron dynam appreci consid simplest possibl case master slave interact two regular fire oscil unit natur period ta tb instant fire unit unidirect send spike signal unit receiv interv measur from last time fire upon recept spike transform quantum energi 6e depend upon post-fir arriv time relationship shape repres refractori post-spik properti here it assum simpl ramp function if interact inhibitori consequ this arriv next fire unit delay respect schedul would absenc perturb posit interv figur becaus shape delay action nil immedi fire becom longer imping pre-synapt spike arriv later interv if interact excitatori delay negat le shorten natur fire interv general assumpt regard function tend synchron within given rang coupl gain phase self-adjust equilibrium achiev given this equilibrium correspond distribut maximum entropi point cell receiv amouint activ common cycl i i inhibit excit figur relationship phase delay when input effici increas linear after-spik interv synchron dynam present attractor ration frequenc pair ratio associ rang stabil ratio lowest cardin wide zone phaselock figur wider stabil wnes correspond one one ratio fa fb invers ta tbl kohn segundo demonstr such phase lock occur live invertebr neuron point paradox natur phase-lock inhibit within stabil region take appear excit sinc small increas input fire rate local result increas output rate area these rang stabil appear unstabl transit fact as recent point out bak9 form infin lock step known as devil 's staircas correspond infin intermedi ration pair figur bak show staircas self-similar scale transit form fractal cantor set fractal dimens univers constant dynam system i excit inhibiti~~v i li l figur unilater synchron constraint satisfact in oscil network global synchron interconnect network mutual phase-lock oscil constraint satisfact problem synchron equilibrium node fire in interlock pattern organ inter-spik interv integ ratio often cite travel salesman problem archetyp for a class import hard problem a special case when ratio must node must fire at frequenc here the equilibrium condit everi node accumul the the amount energi the global cycl furthermor the fire must order along a minim path use stochast energi minim simul anneal the first simul demonstr the feasibl the approach with a limit number node the tsp isomorph mani sequenc problem involv distribut constraint fall the oscil array neural net paradigm in a particular natur way work pursu rigor establish the limit applic the model i annea/./ng figur the travel salesman problem in the global oscil oj minim energi node constrain fire at the rate in the order correspond to the minim path acknowledg research support in part aerojet electro-system under the aerojet-ucla cooper research master agreement no nasa nag
----------------------------------------------------------------

title: 49-connecting-to-the-past.pdf

connect past bruce a. macdonald assist professor knowledg scienc laboratori comput scienc depart univers calgari univers drive nw calgari alberta t2n in4 abstract recent renew interest neural-lik process system evidenc exampl two volum parallel distribut process edit rumelhart mcclelland discuss parallel distribut system connectionist model neural net valu pass system multipl context system dissatisfact symbol manipul paradigm artifici intellig seem part respons attent encourag promis massiv parallel system implement hardwar paper relat simpl neural-lik system base multipl context well-known formalisms-nam product system k-iength sequenc predict finite-st machin ture machines-and present earlier sequenc predict result new light introduct reviv neural net research strong exemplifi recent rumelhart mcclelland new journal number meet net also describ parallel distribut system connectionist model valu pass systems3 multipl context learn systems4 symbol manipul paradigm artifici intellig seem success hope seem last real promis massiv parallel system implement hardwar howev flurri new work import consolid new idea place solid alongsid establish one paper relat simpl neural-lik system well-known notions-nam product system k-iength sequenc predict finite-st machin ture machines-and present earlier result abil network new light general form connectionist system lo simplifi three layer net binari fix weight hidden layer therebi avoid mani difficulties-and challengesof recent work neural net hidden unit weight regular pattern use templat sophist expens learn algorithm avoid simpl method use determin output unit weight way gain advantag multilay net retain simplic two layer net train method certain noth lost comput power-a i explain-and limit two layer net carri simplifi three layer one biolog system may similar avoid need learn algorithm simul anneal method common use connectionist model one thing biolog system clear distinguish train phase briefli simplifi net product system implement three layer neuron-lik unit output layer input layer hidden layer product hidden product unit potenti connect predetermin set input output k-iength sequenc predictor form ie level delay unit introduc input layer k-iength predictor unabl distinguish simpl sequenc ba aa sinc ie charact system forgotten whether appear first k-iength predictor augment auxiliari action abl learn regular languag sinc auxiliari action equival state input aamong 1st intern confer neural net san diego ca june con.fer brough equival singl context system andrea 's multipl context system see also macdonald american institut physic figur general form connectionist system form unit oper within unit in~ut excitation-.i weight sum activ w output typic typic product unit enabl predict depend previous state combin sever augment sequenc predictor thring machin tape simul along finite-st control give net comput power univers ture machin relat simpl neural-lik system lack comput abil previous implement abil product system equival simplifi net organ paper next section briefli review general form connectionist system section simplifi section explain result equival product system deal input output net section extend simplifi version enabl learn predict sequenc section explain comput power sequenc predictor increas thring machin input unit receiv auxiliari action fact system learn ture machin section discuss possibl number net combin output form overal net associ area general form connectionist system figur show general form connectionist system unit neuron ce1l figur unit input output oj possibl unit network output net input excit net weight sum input vij weight connect output unit input unit activ unit function fi net input excit typic fi semilinear non-decreas differenti function least larg group unit output function fi activ typic kind threshold function i assum quantiti vari discret time step exampl activ time given fi neti general restrict connect may made unit unit connect direct input output hidden unit complex net describ paper may one type connect figur show common connect topolog three layer units-input hidden output-with cycl connect net train present input combin along desir output combin train system produc desir output given figur basic structur three layer connectionist system input unit hidden unit output unit input dure train weight adjust fashion reduc discrep desir actual output general method lo desir train activ equat general form hebb 's classic rule adjust weight two unit high activ lo weight adjust product two function one depend desir actual activ often difference-and anoth depend input weight weight simpl exampl suppos differ output oj weight chang product output error input excit weight constant determin learn rate widrow-hoff delta rule may use net without hidden unit 1o import contribut recent work connectionist system implement equat hidden unit train signal ti direct avail boltzmann learn method iter vari weight hidden unit train activ use control gradual decreas random method simul anneal backpropag also iter perform gradient descent propag train signal error back net hidden unit i avoid need determin train signal hidden unit fix weight hidden unit section simplifi system assum simplif made general connectionist system section system three layer topolog shown figur ie cycl hidden layer unit weight fix say uniti zero unit linear threshold unit lo mean activ function unit ident function give net weight sum input output function simpl binari threshold form i output threshold activ output binari oft hidden unit threshold requir input activ output activ like gate output unit threshold requir two activ high weight input output generat like gate keep product system view net explain section learning-which occur output unit weights-giv weight adjust accord wij wij oj otherwis weight turn input unit output otherwis wij oj simpl exampl given figur section simpl form net made probabilist replac adjust weight wij estim condit probabl unit output output wij estim p odoj assum independ input unit output unit turn condit probabl occurr output exceed threshold output function onc simplif made need learn hidden unit also iter learn requir weight either assign binari valu estim condit probabl paper present characterist simplifi net section discuss motiv simplifi neural net way product system simplifi net kind simpl product system product system compris global databas set product rule control system databas net system interact provid input reaction output t.he net hidden unit network product rule form precondit action precondit satisfi input excit exceed threshold hidden unit action repres output unit hidden product unit activ control system product system choos rule whose action perform set rule whose precondit met neural net control system distribut throughout net output unit exampl output unit might form winner-takeal net product system complex control involv forward backward chain choos action seek goal discuss elsewhere4.12.16 figur illust.r simpl product implement neural net figur show input hidden unit element precondit appropri input combin present associ hidden product unit fire onc weight leam connect hidden unit output unit fire product result output simplifi neural net direct equival product system whose element input output product system symbol element variabl given valu product action neural net direct implement sinc output predetermin set howev see later extens t.o framework enabl abil cthis might refer sensory-motor product system sinc implement ill l'eal system robot deal sens input execut motor action may includ auxiliari action section figur product implement simplifi neural net product rule icloudi i ipressur fall i then iit rain i rule implement hidden unit threshold hidden unit gate threshold output unit gate learn weight if net probabilist otherwis estim p rainlcloud pressur fall rain weight figur net predict next charact sequenc base last charact net product unit hidden unit combin input unit exampl net could predict sequenc abcabcabc product form if last charact then next charact learn rule wij if inputj outputi output wijoj input neural net output learn procedur clamp input output desir valu system calcul weight valu repeat requir input/output combin sequenc predict product system neural net predict sequenc given exampl repeat sequenc product learn predict futur event basi recent one figur show trivial simpl sequenc predictor predict next charact sequenc base previous one figur also give detail learn procedur simplifi net net need train onc input combin then predict output everi charact seen current one probabilist form net would estim condit probabl next charact condit current one mani figur use delay input neural net implement k-length sequenc predictor net last three charact input input hidden output 2nd last exampl product if last three charact then present possibl charact pair would need proper estim probabl net would learn probabl distribut charact pair predictor like one figur extend general k-iength predictor long input delay step avail then illustr figur 3-length predict hidden product unit repres possibl combin symbol again output weight train respond previous seen input combin three charact delay provid dedic neural net shown figur note net assum synchron updat input feedback around unit chang one step output chang various way implement delay neuron andrea investig purpose-delay inputs-in detail simul similar net work sequenc predict neural net feldman ballard find connectionist system initi suit repres chang time one form chang sequenc suggest two method repres sequenc net first unit connect each sequenc sequenti task repres fire these unit success second method buffer input time input recent past avail well current input delay input avail suggest import differ necessari length buffer feldman ballard suggest buffer long enough hold phrase natur languag i expect use buffer longer andrea symbol input repres complex inform effect give length seven buffer inform recent seven simpl input discuss section method back-propagation13 enabl recurr network learn sequenti task dfeldman ballard2 give dedic neural net connect varieti flulction figur input delay dedic neural subnet two stage delay shown delay network time diagram tml origin signal delay one step delay two step manner similar first suggest last paragraph sequenc connect unit repres sequenc event one exampl net learn complet sequenc charact given first two charact six charact sequenc next four output error must propag around cycl recurr net number time serial may also achiev sequenc state distribut activ exampl net play side tic-tac-to game sequenti natur net 's behavior deriv sequenti natur respons net 's action tic-tac-to move net model sequenc intern model sequenti part environ exampl tic-tac-to play net model oppon k-iength sequenc predictor unabl learn sequenc repeat frequent everi charact their k-iength context includ inform last event howev there two way inform kth last input retain net first method latch input second involv auxiliari action latch unit input latch held indefinit use combin shown figur input would normal latch andrea discuss this techniqu thread latch event among non-latch event give net inform arbitrarili far back input-output histori inform immedi past briefli sequenc ba distinguish aa if first charact latch howev this ad hoc solut this problem auxiliari action output fed back net input signal this enabl system choos next output least part base previous one indic figur if particular fed back output also one without extern manifest whose extern manifest independ task perform then output auxiliari action las interest reader refer andrea extens analysi given figur thread latch circuit rememb event anoth come along this two input latch two letter number unit may similar connect form mutual inhibit layer winner-take-al connect along posit feedback keep select output activ when input disappear figur auxiliari actions-th outputs-ar fed back input net enabl net rememb state here part net exampl product shown there two type action charact action sinput output charact input if input charact input charact output then output charact llij ill direct effect task system perform sinc it evok relev input use net symbol action if auxiliari action latch input then symbol inform rememb indefinit lost when anoth auxiliari action kind input take latch thus auxiliari action act like rememb state system perform action remind particular state figur illustr this system that predict charact state chang given previous charact state an obvious candid auxiliari action speech blank oval figur would repres net 's environ speech action heard although it extern manifest speech direct effect physic interact the world symbol abil provid the power auxiliari action also includ other speaker the interact simul abstract automata the exampl figur give the essenc simul finit state automaton product system or it neural net equival it illustr the transit function an automaton the new state output function the previous state input thus neural net simul finit state automaton long it addit auxiliari action thring machin finit state automaton control plus an unbound memori neural net could simul a lure machin two way way demonstr product system implementations-equival neural net all multipl context learn system briefli explain section the first thring machin simul the system simul the finit state control abl use an unbound extern memori fsee john andrea 's colleagu work4 figur multipl context learn system implement multipl neural net each:3 layer net the simplifi form present a number elabor extra connect goal-seek forward backward chain output channel the real world much like the paper ture 's origin work the second simnlat.ion emb the memori the multipl context learn system along a counter access this simul memori both learn the productions-equival learn output unit weights-requir the simul the second abl add intern memori requir a limit depend the size the network easili larg enough allow year comput the second could also employ extern memori the first briefli the second simul compris multipl sequenc predictor predict auxiliari action rememb the state the control the current memori posit the memori element updat relearn the product repres that element the precondit the address the product action the store item multipl system form associ area a multipl context learn system product system version a multipl neural net although a simpl version implement a simul net it effect compris sever net or associ areas-which may output input in common indic in figur hidden unit weight specifi templat one each net a templat give the input a zero weight the hidden unit a net the input a weight uniti delay latch input also avail the actual output select the combin predict the net in a winner-take-al fashion i see the design real neural net say control real robot requir a larg degre predetermin connect a robot control could one three layer net wit.h everi input connect everi hidden unit in turn connect everi output there need some connect constraint so the net reflect the function special in the control requir the multipl context learn system all the hidden layer connect predetermin allow output connect learn this avoid the credit assign problem therefor also the need learn algorithm boltzmann learn back-propag howev as the multipl context learn system auxiliari action delay latch input it lack comput power futur work in this area investig for exampl the abil differ kind net learn auxiliari act.ion this may difficult as symbol action may provid in train input and output. for exampl a control for a robot bodi would deal vision manipul motion etc conclus this paper present a simplifi three layer connectionist model fix weight for hidden unit delay and latch for input sequenc predict abil auxiliari state action and the abil use intern and extern memori the result abl learn to simul a ture machin simpl neural-lik system not lack comput power acknowledg this work support by the natur scienc and engin council canada
----------------------------------------------------------------

title: 48-a-neural-network-classifier-based-on-coding-theory.pdf

neural network classifi base code theori tzt-dar chlueh rodney goodman eanrornla instltut technolog pasadena eanromla abstract new neural network classifi propos transform classif problem code theori problem decod noisi codeword input vector featur space transform intern represent codeword code space error correct decod space classifi input featur vector class two class code give high perform hadamard matrix code maxim length sequenc code show number class store n-neuron system linear signific obtain use hopfield type memori classifi i introduct associ recal use neural network recent receiv great deal attent hopfield paper describ mechan iter feedback loop stabil memori element nearest input provid mani memori vector store machin he also shown number memori store n-neuron system mceliec work show synchron oper hopfield memori n/ 2iogn data vector store reliabl larg abu-mostafa predict upper bound number data vector n-neuron hopfield machin n. believ one abl devis machin number data vector linear larger achiev hopfield method featur space code space figur classif problem versus error control decod problem paper specif concern problem classif pattern recognit propos new method build neural network classifi base well establish techniqu error control code consid typic classif problem one given priori set class m. associ class featur vector label class exemplar class i.e american institut physic repres point class region input classifi class nearest exemplar input henc class region n-dimension binari featur space bn everi vector classifi correspond class similar problem decod codeword error correct code shown case codeword construct design usual least dmtn apart receiv corrupt codeword input decod find nearest codeword input principl distanc codeword greater 2t possibl decod classifi noisi codeword featur vector correct codeword exemplar provid ham distanc noisi codeword correct codeword note guarante exemplar uniform distribut bn consequ attract radius maximum number error occur given featur vector vector correct classifi depend minimum distanc exemplar mani solut minimum ham distanc classif propos one common use deriv idea match filter communic theori lippmann propos two-stag neural network solv classif problem first correl input exemplar pick maximum winner-take-al circuit network compos two-input compar figur fi.f2 fn input bit si.s2 sm match score similartti exemplar second block pick maximum si.s2 sm produc index exemplar largest score main disadvantag classifi complex maximum-pick circuit exampl winner-take-al net need connect weight larg dynam rang graded-respons neuron whilst compar maximum net demand m-i compar organ log2m stage decoder~ss f i closs f featur space code space match filter type classifi structur propos classifi main idea thus transform everi vector featur space vector code space way everi exemplar correspond codeword code code prefer necessarili properti codeword uniform distribut code space ham distanc everi pair codeword transform turn problem classif code problem decod noisi codeword error correct decod vector code space obtain index noisi codeword henc classifi origin featur vector shown figur paper develop construct classif machin follow first consid problem transform input vector featur space code space describ two hetero-associ memori do first method use outer product matrix techniqu similar hopfield 's second method generat matrix pseudoinvers techruqu s.7j given transform problem associ recal classif problem decod noisi codeword next consid suitabl code machin requir codeword code properti orthogon pseudo-orthogon ratio cross-correl auto-correl codeword small show two class good code particular decod problem hadamard matrix code maxim length sequenc code 8j next formul complet decod algorithm describ overal structur classifi term two layer neural network first layer perform map oper input second one decod output produc index class input belong second part paper concern perform classifi first analyz perform new classifi find relat maximum number class store classif error rate show use transform base outer product method neglig misclassif rate larg n. tight lower bound m. number store class present comprehens simul result confirm exceed theoret expect simul result compar method hopfield model outer product pseudo-invers method analog hard limit connect matric case classifi exceed perform hopfield memori term number class reliabl recov d. transform techniqu object build machin discrimin among input vector classifi one appropri class suppos bn exemplar ofth correspond class given input want machin abl identifi class whose exemplar closest want calcul follow function class i i i i i i denot ham distanc bn approach problem seek transform map exemplar bn correspond codeword bl input featur vector dey thus map noisi codeword wli error ad exemplar correspond error pattern code space error correct decod get index correspond codeword note may ham weight transform may either generat error elimin error present origin input featur vector requir satisfi follow equat m-l implement use single-lay feedfoiward network thus first construct matrix accord set call defin sgn threshold oper map vector rl bl field real number let matrix whose lth column matrix whose th column two possibl method construct matrix follow scheme outer product method scheme matrix defin sum outer product exemplar-codeword pair m-l die equival wdt scheme pseudo-invers method want find matrix satisfi follow equat general squar matrix moreov may singular d-l may exist circumv difficulti calcul pseudo-invers denot dt matrix instead real invers let dtd -ldt formul dt ot nt code code look prefer properti codeword distribut uniform bl distanc two codeword must larg possibl thus seek class equidist code two class hadamard matrix code maxim length sequenc code first defin word pseudo-orthogon defmit let wo wl wl-l bl ath codeword code code said pseudo-orthogon iff l-l wl denot inner product two vector hadamard matric orthogon code length whose codeword row column hadamard matrix case distanc two codeword l/2 conjectur exist code multipl thus provid larg class code mazlmal length sequenc code exist famili maxim length sequenc also call pseudo-random pn sequenc code generat shift regist satisfi pseudo-orthogon suppos primit polynomi degre d. let 2d f xl l/g xl ck xk k=o co.cl period sequenc period sinc i code made cyclic shift co. cl il code satisfi pseudo-orthogon one easili see minimum distanc code give correct power approxim error larg l. iv overal classifi structur shall describ overal classifi structur essenti consist map follow error correct decod maxim length sequenc code hadamard matrix code decod oper correl input vector everi codeword threshold result rational algorithm follow sinc distanc everi two codeword code exact bit decod abl correct error pattern less error threshold set halfway land i.e suppos input vector decod ham weight nonzero compon 2s 2s equat less error away less result arriv follow decod algorithm deax1 sgn vector case less error input output vector sm one compon posit index index class input vector belong howev there error output either negat vector decod failur anoth vector one posit compon decod error function class defin composit decod overal structur new classifi depict figur view two-lay neural network hidden unit output neuron first layer map input featur vector noisi codeword code space intern represent second one decod first output produc index class input belong f1 f2 fn 9l figur overal architectur new neural network classifi perform analysi previous section know classifi make error transform vector code space input decod less error proceed find error rate classifi case input one exemplar error say outer product connect matrix follow approach mcel1ec we n-l m-l sgl wl dj j=o n-l sgn m-l wl dl j=o a=o assum without loss general if n-l m-l j=o wl dl a=o notic we assum random name compon outcom bernoulli trial accord sum independ ident distribut random variabl mean varianc asymptot case nand larg approxim normal distribut mean varianc nm thus pr q vln/m vi tt foo dt next we calcul misclassif rate new classifi follow assum pe k=il/4j pk l_p l-k integ floor sinc generallt possibl express summat explicit we use chernoff method bound pe multipli term summat number larger uniti et k sum from instead pe t k k=o differenti rhs equat set we find optim eto condit impli sinc we deal case small automat satisfi substitut optim we obtain from express pe we estim number class classifi negllgibl misclassif rate follow way suppos pe land small we log sinc fix valu approach infin we from lower bound one easili see new machln abl classifi constant time class better number memori item hopfield model store le although analysi done assumlng approach lnfinlti simul result next section show moder larg lower bound appll vi simul result charact recognit exampl we simul hopfield model new machin use maxlmallength sequenc code follow four case respect connect matrix generat outer product method connect matrix generat pseudo-invers method ill connect matrix generat outer product method compon connect matrix hard limit connect matrix generat pseudo-invers method compon connect matrix hard limit case choic n. program fix number error in input vector random generat set exemplar comput connect matrix machin machin random pick exemplar add nois random complement specifi number bit generat trial input vector simul machin check whether input classifi nearest class report percentag success machin simul result shown in figur in graph horizont axi vertic axi attract radius data we show obtain collect case success rate fix largest attract radius number bit in error input vector success rate here we use attract radius denot particular m. input exemplar success rate less in machin hopfield model new classifi op new classtfier pi analog connect matrix binari connect matrix cil lit td i en tl.a binari connect matrix analog connect matrix figur simul result hopfield memori new classifi hopfield model new classifi op.l=63 new classifi op.l=31 ril u.a figur perfonn new classifi use code differ length in case classifi exceed perfonn hopfield model in tenn number class reliabl recov exampl consid case hard limit connect matrix new classifi hopfield model we find attract radius zero error in input vector hopfield model classif capac approxim new model store also attract radius averag error in input vector hopfield model reliabl store class our new model store class anoth simul use shorter code instead reveal shorten code perform classifi degrad slight we therefor conjectur it possibl use tradit error correct code bch code intern represent howev go higher rate code one trade minimum distanc code error toler for complex number hidden unit impli possibl poorer perform classifi we also notic superior pseudoinvers method outer product method appear connect matric hard limit reason for pseudoinvers method best for decorrel depend among exemplar yet exemplar in simul generat random presum independ consequ one see advantag pseudoinvers method for correl exemplar we expect pseudoinvers method clear better next exampl next we present exampl appli classifi recogn charact each charact repres pixel array input generat flip everi pixel probabl input pass five machin hopfield memori new classifi either pseudotnvers method outer product method figur show result machin for pixel flip probabl respect a blank output mean the classifi refus make a decis first note the case necessarili wors the case confirm the earlier conjectur fewer hidden unit shorter code degrad perfonn slight also one easili see the pseudoinvers method is better the outer product method the correl exemplar both method outperform the hopfield memori sinc the latter mix exemplar to rememb produc a blend exemplar rather the exemplar accord it classifi the input without mistak tf figur the charact recognit exampl pixel revers probabl input correct output hopfield model new classifi op pi pi figur the charact recognit exampl pixel revers probabl input correct output hopfield model new classifi op pi pi vll conclus in this paper we present a new neural network classifi design base code theori techniqu the classifi use codeword from an error correct code intern represent two class code give high perform the hadamard matrix code the maxim length sequenc code in penorm term we shown the new machin is signific better use the hopfield model as a classifi we also note compar the new classifi the hopfield model the increas perform the new classifi entail extra complex sinc it need hard limit neuron and l n connect weight versus neuron and n2 weight in a hopfield memori in conclus we believ our model form the basi a fast practic method of classif with an effici greater previous neural network techniqu
----------------------------------------------------------------

title: 52-teaching-artificial-neural-systems-to-drive-manual-training-techniques-for-autonomous-systems.pdf

teach artifici neural system drive manual train techniqu autonom system j. f. shepanski s. a. maci trw inc one space park redondo beach ca abetract develop methodolog manual train autononl control system base artifici neural system applic rule set govern expert decis difficult formul an use ext.ra.c rule associ inform expert receiv action take proper construct network imit rule behavior permit function autonom train span set possibl situat train provid manual either direct supervis system trainer indirect use background mode network assimil train data expert perrorm day-to-day task demonstr method train an network drive vehicl simul rreeway traffic i ntjooducticn comput system employ fine grain parallel revolution way approach number long stand problem involv pattern recognit cognit process field span wide varieti comput network rrom construct emul neural runction crystallin configur resembl systol array sever titl use describ broad area research use term artifici neural system concern work use an ror manual train certain type autonom system desir rule behavior difficult rormul artifici neural system consist number process element interconnect weight user-specifi fashion interconnect weight act memori ror system process element calcul output valu base weight sum input addit input data correl output desir output specifi instruct agent train rule use adjust interconnect weight way ne work learn pattern imit rule behavior decis make particular an architectur use variat rummelhart lj multi-lay perceptron employ general delta rule gd r instead singl multi-lay structur final network multipl compon block configur one blot'k output reed anoth figur train methodolog develop tie particular train rule architectur work well altern network like grossberg 's adapt reson model 2j american institut physic equat describ network deriv describ detail rumelhart summari transfer function sj wjioi i-o weight adapt rule error calcul oj l awp.revi oj e0.tw.ti oj output process element sensor input interconnect weight lead element ito number input aw adjust train constant train momentum oj calcul error element canout oc given element element zero constant input equal one wjo equival bias threshold element factor equat differ standard gdr formul use keep track relat magnitud two term network 's output layer summat equat replac differ desir actual output valu element network usual train present system set input/output data vector cyclic fashion entir cycl databas present repeat dozen time method effect train agent comput oper batch mode would intoler human instructor two develop help real-tim human train first effici incorpor data/respons pattern network second address paper suitabl environ wherein man an network interact train situat minimum inconveni boredom human 's part abil systemat train network fashion extrem use develop certain type expert system includ automat signal processor autopilot robot autonom machin report number techniqu aim facilit type train propos general method teach network system develop work focus util an system control began applic barto sutton 's associ search network although approach use number way fell short tri use captur subtleti human decision-mak respons shift emphasi rrom construct goal runction automat learn method train network use direct human instruct integr part develop suitabl interrac human network outsid world simul section report various approach end describ general methodolog manual teach an network demonstr techniqu taught network drive robot vehicl simul highway traffic applic combin binari decis make control continu paramet initi investig use automat learn base goal function train control system train network-control vehicl maintain accept follow distanc car ahead graphic workstat one lane circular track construct occupi two vehicl network-control robot car pace car vari speed random input data network consist separ distanc speed robot vehicl valu goal function translat desir output gdr train output control consist three binari decis element acceler one increment speed maintain speed deceler one increment speed time desir output vector exact one three element activ goal runction quadrat minimum correspond optim follow distanc although direct control simul goal function posit negat reinforc system 's behavior network given complet control robot vehicl human trainer influenc except abil start termin train prove unsatisractori initi system behavior govern random interconnect weight unstabl robot tend run car rront signific train occur carerulli halt restart train achiev stabl system behavior first rollow distanc maintain robot car oscil ir vehicl attach sj ring pace car activ gradual damp arter one thousand train step vehicl maintain optim follow distanc respond quick chang pace car 's speed construct composit goal function promot sophist abil prove difficult even ill-defin there mani unspecifi paramet generat goal runction ror abil would similar convent program type labor want circumv use an hand human adept assess complex situat make decis base qualit data goal runction difficult ir imposs captur analyt one attract an imit behavior base elus rule without rormal specifi point turn effort manual train techniqu initi train network graft larger system augment addit input distanc speed inrorm nearbi pace car second traffic lane output control signal govern lane chang origin network 's abil maintain safe follow distanc retain intact thts graft procedur one two method studi ad ne abil existin system second employ block structur describ network remain direct control robot vehicl human trainer instruct chang lane command interpret desir output use gdr train algorithm techniqu call coach prove userul network quick correl environment input teacher instruct network becam adept chang lane weav traffic found network took behavior pattern trainer conserv teacher produc timid network aggress tzainer produc network tend cut automobil squeez tight open despit success coach method train solv problem initi network instabl stabil problem solv give trainer direct control simul system configur figur allow expert exert control releas work dure initi tzain expert driver 's seat network act role apprentic receiv sensor inform predict system command compar predict desir output trainer 's command figur show data command flow detail input data process differ channel present trainer network visual audio format effect human network use inform vector form differenti data present limit system remov cask futur search trainer issu control command accord assign network take trainer 's action desir system respons correl input refer procedur master/apprentic train network train proceed invis background expert proceed day day work avoid instabl problem network free make error without advers consequ throw oper environ disarray i input world sensor simul actuat i ne work i expert command figur scheme manual train an network input data receiv network trainer trainer issu command actuat solid command line coach network ought respond broken command line command preprocess tortunan input data preprocess network twork predict command actuat coaching/emphasi train rule fegur data convnand flow train system input data process present trainer network master/appre~ic train solid command hne trainer order actuat network treat command system 's desir output coach network 's predict oonvnand actuat broken command line trainer influenc weight adapt specifi desir system output controlhng valu trail constants-hi suggest cirec tti actuat onc initi bacqround wainmg complet expert proceed formal manner teach network he releas control command system network order evalu ita behavior weak he resum control work seri scenario design train t.he network bad behavior switch back forth human network control expert assess network 's reliabl teach correct respons need find master/apprentic train work well behavior involv continu function like steer hand coach appropri decis cunction like ule car ought pass our methodolog employ techniqu drive network fulli develop freeway simul consist two lane highway made join straight curv segment vari at random length curvatur sever pace car move at random speed near robot vehicl network given task track road negoti curv return road place far afield maintain safe distanc pace car chang lane appropri instead singl multi-lay structur network compos two block one control steer regul speed decid vehicl chang lane figur first block receiv inform posit speed robot vehicl relat ear vicin output use determin automobil 's speed whet.her robot chang lane pass signal convert lane assign base car 's current lane posit second block receiv lane assign data pertin posit orient vehicl respect road output use determin steer angl robot car block input output constant speed disl ahead pl disl ahead ol dist behind ol rei speed ahead pl rei speed ahead ol rei speed behind ol i speed chang lane steer angl convert lane chang lane number constant rei orient lane nurmer later dist curvatur figur two block drive an network heavi arrow indic total interconnect layer pl design traffic lane present oca.api robot vehicl ol refer lane qjrvatur refer road lane nurrber either relat orient later distanc refer robot car 's direct podion relat road'l direct center line respect input data display pictori textual form drive instructor he view road nearbi vehicl perspect driver 's seat overhead network receiv inform form vector whose element scale unitari order wide rang input paramet like distanc compress use hyperbol tangent logarithm function block input layer total interconnect ou put hidden layer our scheme train real time discuss later train smooth small modif train algorithm output interpret two way binari decis continu vari paramet first simpli compar sigmoid output threshold second scale output appropri rang applic exampl steer output element valu interpret zero steer angl left right turn vari degre initi output respect network divid two block train separ besid conceptu easier understand find this compon approach easi train systemat each block restrict well-defin set task trainer concentr specif function without concern aspect network behavior deterior train system bottom first teach network stay road negoti curv chan~ lane return vehicl stray highway block respons steer learn these skill minut use master/apprentic mode it tend steer slowli human train progress improv respons experi differ trammg constant momentum valu larg valu caus weight chang coars valu order magnitud smaller work well we found advantag use momentum this method train fact system respond three time slowli momentt term drop our standard train paramet cl figur typic behavior network-control vehicl dam rectangl train conserv miyer iti i reckless driver speed indic length arrow after block train we gave steer control network concentr on teach network chang lane adjust speed speed control this continu variabl best taught use master/apprentic train on hand binari decis chang lane was best taught coach about ten minut train need teach network weav traffic we found network readili adapt behavior pattern trainer conserv trainer generat network hard ever pass aggress trainer produc network drove reckless tend cut other-car figur discuss one strength el pert 5ystf'm base on an use input data decis make control proc~ss specifi network adapt intern weight conform input output correlat.ion it discov it import howev data use human expert also avail network differ process sensor data man network may import consequ key inform may present the man the machin this differ data process particular worrisom imag data human abil extract detail vast superior our au tomat imag process capabl though we would requir imag process system understand imag it would extract relev inform clutter background until we suffici sophist algorithm network do this our effort at construct expert system halldl imag data handicap scale input data the unitari order magnitud import train stabil evid equat the sigmoid transfer function rang in approximat.eiy four unit domain system respons must chang in reaction larg swing given input paramet the weight associ input train toward magnitud on the hand the system respond input whose rang associ weight also the weight adjust equat recogn differ in weight magnitud therefor relat small weight undergo wild magnitud adjust converg weak on the hand input paramet the magnitud associ weight reflect this the train constant adjust gentl weight converg becaus the output hidden unit constrain zero one good target rang input paramet both the hyperbol tangent logarithm function use scale wide rang input use form the latter ifx -o where defin the limit the intermedi linear section scale factor this symmetr logarithm function continu in first deriv use network behavior chang slowli paramet increas without bound on the othl'r hand the system approach limit behavior the tanh function appropri weight adapt also complic relax the common practic restrict interconnect adjac layer equat show the calcul error for hidden layergiven compar weight fanout output errors-wil one quarter less the output layer this caus the slope ractor oil the differ in error magnitud notic in network restrict adjac layer interconnect this constraint releas the effect error origin direct output unit time the magnitud effect an error origin hidden unit remov layer the output layer compar to the correct aris the output unit the hidden unit littl influenc on weight adjust the power a multilay structur weaken the system train if we restrict connect to adjac layer but it train slowli to compens for this effect we attenu the error magnitud origin the output layer by the factor this heurist procedur work well racilit smooth learn though we made progress in real-tim learn system use gdr compar to humans-who learn a singl data presentation-they remain relat sluggish in learn respons rate we interest in improv the gdr algorithm altern architectur facilit one-shot rapid learn in the latter case we consid least squar restor techniquesl4 grossberg carpent 's adapt reson modelsi3,5 the construct autom expert system by observ human personnel attract becaus it effici use the expert 's time effort though the classic ai approach rule base infer applic rule clear cut well organ often a human expert put his decis make process in word specifi the valu paramet influenc the attract an base system is imit expert behavior emerg a natur consequ train
----------------------------------------------------------------

title: 55-mathematical-analysis-of-learning-behavior-of-neuronal-models.pdf

mathemat analysi learn behavior neuron model john y. cheung massoud omidvar school electr engin comput scienc univers oklahoma norman ok present ieee confer neural inform process systemsnatur synthet denver novemb publish collect paper ieee confer nip pleas address correspond john y. cheung school eec w. boyd cec norman ok novemb american institut physic mathemat analysi learn behavior neuron model john y. cheung massoud omidvar school electr engin comput scienc abstract paper wish analyz converg behavior number neuron plastic model recent neurophysiolog research suggest neuron behavior adapt particular memori store within neuron associ synapt weight vari adjust achiev learn number adapt neuron model propos literatur three specif model analyz paper specif hebb model sutton-barto model recent trace model paper examin condit converg posit converg rate converg model appli classic condit simul result also present verifi analysi introduct number static model describ behavior neuron use past decad recent research neurophysiolog suggest static view may insuffici rather paramet within neuron tend vari past histori achiev learn it suggest alter intern paramet neuron may adapt repetit input stimuli becom condit learn thus occur neuron condit describ behavior neuron plastic number model propos earliest one may postul hebb more recent sutton barto also introduc new model recent trace mrt model paper primari object paper howev analyz converg behavior model adapt general neuron model use paper shown figur number neuron input n. each input scale correspond synapt weight n. weight input arithmet sum taken zero neuron input assum take numer valu rang zero one inclus synapt weight allow take reason valu purpos paper though realiti weight may well bound sinc relat magnitud weight neuron input well defin point put bound magnitud weight also neuron output normal result sigmoid transform simplic approxim oper linear transform sigmodi transfonut neuron output rilur lener aeuron adel converg analysi assum two neuron input tradit classic condit environ simplic cours analysi techniqu extend number input classic condit two input condit stimulus xc uncondit stimulus sutton-barto model more recent sutton barto propos adapt model base signal trace output trace given xi axi xi posit constant condit converg order simplifi analysi choos word becom wi wi cxi assumpt serv simplifi analysi affect converg condit bounded depend xi respect previous section recogn recurr relat converg check ratio test it also possibl rewrit matrix format due recurs neuron output equat includ neuron output paramet vector also show converg need set magnitud determin less uniti henc condit converg see adapt constant must chosen less reciproc euclidean sum energi input techniqu extend number input prove mere follow procedur outlin posit converg have prove converg sutton-barto model equat neuron plastic want find next locat system remain converg seen earlier converg weight ceas chang neuron output denot converg posit word sinc arbitrari paramet vector alway decompos weight sum eigenvector constant ql q3 easili found invert eigenvalu shown within region converg magnitud third eigenvalu less uniti mean converg there contribut third eigenvector henc predict precis converg posit would given initi condit rate converg seen care chosen sutton-barto model converg also deriv express converg posit next want find fast converg attain rate converg measur fast initi paramet approach optim posit asymptot rate converg sea spectral radius equal case complet converg analysi sutton-barto model neuron plastic mrt model neuron plastic recent trace mrt model neuron plastic develop author consid cross sutton-barto model klopf 's model adapt synapt weight express follow comparison sutton-barto model ahowl cond term right hand aid contain extra factor ii use ape converg ahowd later output trace hu replac recent output henc name recent trace model input trace also replac recent input condit converg proceed analyz condit converg mrt model due presenc wi factor second term ratio test appli analyz converg behavior let us rewrit matrix format wi superscript denot matrix transpos oper equat quadrat complet converg analysi equat extrem difficult order understand converg behavior note domin term determin converg main relat second quadrat term henc converg analysi we ignor first term we readili see primari converg factor bt sinc depend converg obtain if durat synapt input activ bound it shown condit converg bound we readili see adapt constant chosen accord ensur converg t. simul verifi theoret analysi three adapt neuron model base classic condit model simul mm mainfram use fortran languag singl precis sever test scenario design compar analyt predict actual simul result verifi condit converg we vari valu adapt constant condit uncondit stimuli set to uniti valu vari to sutton-barto model simul given show converg obtain expect theoret analysi mrt model simul result given show converg obtain also expect theoret analysi theoret locat converg sutton barto model also shown in figur it readili seen simul result confirm theoret expect i output figur lou murod l tpuu yet.us ule er suttoa-barto el witb 1frerent alu aptat1on codstant lleuroul output i i i ju.ber iteratioga figur plotl oeuroaal outputl craus uuaber iterati mat el differ alu adantatlon i ddstaut to illustr rate converg we plot trajectori deviat in synapt weight from optim valu in logarithm scale sinc error logarithm as found earlier slope line yield rate converg trajectori sutton-barto model given in figur mrt model given in figur it clear from figur trajectori in the logarithm form a straight line the slope readili calcul the curv the mrt model given in figur also a straight line a much larger slope show faster converg summari in paper we sought to discov analyt the converg behavior three adapt neuron model from the analysi we see the hebb model converg constant activ input the output grow exponenti in spite lack converg the hebb model still a workabl model realiz the diverg behavior would curtail by the sigmoid transform to yield realist output the i uroul output dniatiotl lto i i u.ber iterationa figur trajectori of deuron output deviationa froa atat alu the sutton- rt el lfferent valu adapt coiistallt c. lleurod l output deviat ltl i nuaber of iter figur trajectori of neuron output deviat fra atat valu for tbe krt el witb differ valu of adapt constant analysi the sutton barto model show model converg when the adapt constant care chosen the bound for also found for this model due to the structur of this model the locat at converg the rate of converg also found we also introduc a new model of neuron plastic call the recent trace mrt model certain similar exist the mrt model the sutton-barto model also the mrt model and the klopf model analysi show the updat equat for the synapt weight quadrat result in polynomi rate of converg simul result also show that much faster converg rate obtain with the mrt model
----------------------------------------------------------------

title: 32-synchronization-in-neural-nets.pdf

synchron neural net jacqu j. vidal univers california los angel los angel ca john haggerti abstract paper present artifici neural network concept synchroniz oscil network instant individu fire form point process constitut form inform transmit join neuron type communic contrast assum model typic continu discret value-pass network limit messag receiv process unit time marker signal fire unit present signific implemen tation advantag model neuron fire spontan regular absenc perturb interact present schedul fire advanc delay fire neighbor neuron network neuron becom global oscil exhibit multipl synchron attractor arbitrari initi state energi minim learn procedur make network converg oscillatori mode satisfi multi-dimension constraint network direct repres rout schedul problem consist order sequenc event introduct most neural network model deriv variant rosenblatt origin perceptron value-pass network case particular network propos fukushima i hopfield rumelhart mani other everi case input process element either binari continu amplitud signal weight synapt gain subsequ sum integr result activ pass sigmoid threshold filter produc continu quantiz output may becom input neuron behavior model relat live neuron even fall consider short account complex inde observ mani real neuron action potenti spike fire propag axon branch intern activ reach threshold higher john haggerti interact system los angel w. 6th st. la ca american institut physic input rate level result rapid fire behind tradit model assumpt averag frequenc action potenti carrier inform neuron becaus integr fire individu neuron consid effect extent contribut averag intens therefor assum activ simpli frequenc code exact time individu fire ignor view howev cover well known aspect neural communic inde precis time spike arriv make crucial differ outcom neural interact one classic exampl pre-synapt inhibit widespread mechan brain machineri sever studi also demonstr occurr function import precis time phase relationship cooper neuron local network model present paper contrast one mention network fire consid individu output event input side node fire node presynapt neuron either delay inhibit advanc excit node fire seen earlier type neuron interact would call phase-modul engin system also find rational experiment neurophysiolog neurophysiolog plausibl howev major concern rather propos explor potenti use mechan parallel distribut comput merit approach artifici neural network digit puls use internod communic instead analog voltag model particular well suit time-ord sequenc found larg class rout trajectori control problem neuron synchroniz oscil model process element neuron relax oscil built-in self-inhibit relax oscil dynam system capabl accumul potenti energi threshold breakdown point reach point energi abrupt releas new cycl begin descript fit dynam behavior neuron membran rich structur empir model this behavior found well-establish differenti formul hodgkin huxley simplifi version given fitzhugh7 differenti equat account foundat neuron activ also capabl repres subthreshold behavior refractori follow fire membran potenti enter critic region abrupt depolar collaps potenti differ across membran occur follow somewhat slower recoveri this brief electr short membran call action potenti spike constitut output event neuron if caus initi depolar maintain oscil limit-cycl develop generat multipl fire depend input level membran paramet oscil limit singl spike may produc oscillatori burst even continu sustain activ present model share general properti use much simpler descript relax oscil illustr figur activ energy exdt3toij oj input injrjh1~olj input perturb utl intemilf l neju inpul ty figur relax oscil perturb input fire occur energi level reach critic level ec assum constant rate energi influx fire occur natur period ec t=a when pre-synapt puls imping on cours energi accumul fire schedul disturb let repres instant last fire cell tj intant imping arriv cell e act wj uo til ec uo repres unit impuls dramat complex synchron dynam appreci consid simplest possibl case master slave interact two regular fire oscil unit natur period ta tb instant fire unit unidirect send spike signal unit receiv interv measur from last time fire upon recept spike transform quantum energi 6e depend upon post-fir arriv time relationship shape repres refractori post-spik properti here it assum simpl ramp function if interact inhibitori consequ this arriv next fire unit delay respect schedul would absenc perturb posit interv figur becaus shape delay action nil immedi fire becom longer imping pre-synapt spike arriv later interv if interact excitatori delay negat le shorten natur fire interv general assumpt regard function tend synchron within given rang coupl gain phase self-adjust equilibrium achiev given this equilibrium correspond distribut maximum entropi point cell receiv amouint activ common cycl i i inhibit excit figur relationship phase delay when input effici increas linear after-spik interv synchron dynam present attractor ration frequenc pair ratio associ rang stabil ratio lowest cardin wide zone phaselock figur wider stabil wnes correspond one one ratio fa fb invers ta tbl kohn segundo demonstr such phase lock occur live invertebr neuron point paradox natur phase-lock inhibit within stabil region take appear excit sinc small increas input fire rate local result increas output rate area these rang stabil appear unstabl transit fact as recent point out bak9 form infin lock step known as devil 's staircas correspond infin intermedi ration pair figur bak show staircas self-similar scale transit form fractal cantor set fractal dimens univers constant dynam system i excit inhibiti~~v i li l figur unilater synchron constraint satisfact in oscil network global synchron interconnect network mutual phase-lock oscil constraint satisfact problem synchron equilibrium node fire in interlock pattern organ inter-spik interv integ ratio often cite travel salesman problem archetyp for a class import hard problem a special case when ratio must node must fire at frequenc here the equilibrium condit everi node accumul the the amount energi the global cycl furthermor the fire must order along a minim path use stochast energi minim simul anneal the first simul demonstr the feasibl the approach with a limit number node the tsp isomorph mani sequenc problem involv distribut constraint fall the oscil array neural net paradigm in a particular natur way work pursu rigor establish the limit applic the model i annea/./ng figur the travel salesman problem in the global oscil oj minim energi node constrain fire at the rate in the order correspond to the minim path acknowledg research support in part aerojet electro-system under the aerojet-ucla cooper research master agreement no nasa nag
----------------------------------------------------------------

title: 54-a-method-for-the-design-of-stable-lateral-inhibition-networks-that-is-robust-in-the-presence-of-circuit-parasitics.pdf

method design stabl later inhibit network robust presenc circuit parasit j.l wyatt jr d.l standley depart electr engin comput scienc massachusett institut technolog cambridg massachusett abstract analog vlsi implement neural system sometim conveni build later inhibit network use local connect on-chip resist grid serious problem unwant spontan oscil often aris circuit render unus practic paper report design approach guarante system stabl even though valu design element parasit element resist grid may unknown method base rigor somewhat novel mathemat analysi use tellegen theorem idea popov multipli control theori thorough practic criteria local sens overal analysi interconnect system requir empir sens involv measur frequenc respons data individu cell robust sens unmodel parasit resist capacit interconnect network affect analysi i introduct term later inhibit first aros neurophysiolog describ common form neural circuitri output neuron popul use inhibit respons neighbor perhap best understood exampl horizont cell layer vertebr retina later inhibit simultan enhanc intens edg act automat lain control extend dynam rang retina whole principl use design artifici neural system algorithm kohonen other electron design neural chip carver mead vlsi implement neural system conveni build later inhibit network use local connect on-chip resist grid linear resistor fabric polysilicon yield compact realize nonlinear resist grid made mos transistor found use imag segment network type divid two class feedback system feedforward-on system feedforward case one set amplifi impos signal voltag american institut physic current grid anoth set read result respons subsequ process amplifi write grid read feedback arrang feedforward network type inher stabl feedback network need practic exampl one carver meadl retina chips3 achiev edg enhanc mean later inhibit resist grid figur show singl cell continuous-tim version chip note capacitor voltag affect local light intens incid cell capacitor voltag neighbor cell ident design cell drive neighbor drive distant neighbor origin cell turn thus necessari ingredi instabl activ element signal feedback present system fact continuous-tim version oscil bad origin design scarc usabl practic later inhibit path enabl oscil i incid light figur photoreceptor signal processor circuit use two mos transconduct amplifi realiz later inhibit communic similar unit resist grid readili occur resist grid circuit activ element feedback even individu cell quit stabl analysi condit instabl straightforward method appear hopeless sinc repeat array contain mani cell influenc mani other direct indirect influenc turn number simultan activ feedback loop enorm paper report practic design approach rigor guarante system stabl simplest version idea intuit obvious design individu cell although intern activ act like passiv system seen resist grid circuit theori languag design goal celli output imped positive-r function sometim difficult practic show origin network satisfi condit absenc certain parasit element import perhap condit one verifi experiment frequency-respons measur physic appar collect cell appear passiv termin form stabl system interconnect passiv medium resist grid research contribut report summari form demonstr passiv positive-r condit much stronger actual need weaker condit easili achiev practic suffic guarante stabil linear network model ii extens nonlinear domain furthermor rule large-sign oscil certain condit ii first-ord linear analysi singl cell begin linear analysi elementari model circuit initi approxim output admitt cell simplifi topolog without loss relev inform use naive'model transconduct amplifi shown figur simplifi network topolog transconduct amplifi model circuit capacitor absorb co2 straightforward calcul show output admitt given yes positive-r passiv admitt sinc alway realiz network form shown ro2 gmlgm2rol coi/gmlgm2 although origin circuit contain inductor realize capacitor inductor thus capabl damp oscil nonetheless transamp model perfect accur network creat interconnect cell resist grid parasit capacit could exhibit sustain oscil element valu may typic practic model light damp reson around i khz this disturb high suggest cell high sensit parasit element captur simpl model our preliminari rl yes figur passiv network realize output admitt circuit analysi much complex model extract physic circuit layout creat carver mead 's laboratori indic output imped passiv valu transamp bias current definit explan instabl await more care circuit model effort perhap design on-chip imped measur instrument iii positive-r function e-positlv function stabil linear network model follow discuss cr+jw complex variabl ration function ratio polynomi real coeffici assum simplic pure imaginari pole term close right hale plane refer set complex number re def i function said positive-r pole right half plane re h jw know outset right half plane pole def i reduc simpl graphic criterion positiver nyquist diagram plot h jw lie entir close right half plane note positive-r function necessarili stabl sinc right half plane pole stabl function necessarili positive-r exampl show deep link posit real function physic network passiv establish classic result linear circuit theori state positive-r possibl synthes 2-termin network posit linear resistor capacitor inductor ideal transform driving-point imped admitt oef function said a-posit particular valu e e pole right half plane nyquist plot lie strict right straight line pass origin angl real posit axi note everi a-posit function stabl function e-posit necessarili positive-r i re g jw figur nyquist diagram function a-posit positive-r exampl function a-posit stabl but positive-r sinc nyquist diagram shown cross left half plane import e-posit function lie follow observ interconnect passiv linear resistor capacitor cell stabl linear imped result unstabl network instabl result imped also positive-r a-posit imped form larger class positive-r one henc a-posit less demand synthesi goal theorem show instabl result imped a-posit even positive-r theorem consid linear network arbitrari topolog consist number passiv 2-termin resistor capacitor arbitrari valu driven number activ cell output imped activ cell a-posit common network stabl proof theorem reli lemma lemma a-posit fix close first quadrant complex plane h lie strict right straight line pass origin angl real posit axi re im proof lemma outlin let function assign close right half plane perpendicular distanc des line defin def note des harmon close right half plane sinc analyt follow applic maximum modulus principle8 harmon function take minimum valu boundari domain imaginari axi this establish lemma proof theorem outlin network unstabl margin stabl natur frequenc close right half plane so natur frequenc network equat nonzero solut so let denot complex branch current solut tellegen i theorern9 sum complex power absorb circuit element must vanish solut iik12/s0ck capac~t cell termin pair second term delet special case so=o sinc complex power capacitor vanish so=o if network natur frequenc close right half plane must one close first quadrant sinc natur frequenc either real els occur complex conjug pair but satisfi so close first quadrant we see divid side iiki2 sum taken network branch after this divis assert zero convex combin term form rk term form ckso -i term form zk so visual term lie complex plane first set lie real posit axi second set lie close adrant sinc so lie close 1st quadrant assumpt third set lie right a line pass origin angl a lemma thus term lie strict right this line impli convex combin equal zero henc network stabl iv stabil result network nonlinear resistor capacitor previous result linear network afford limit insight behavior nonlinear network first nonlinear equat linear equilibrium point theorem appli linear model if linear model stabl equilibrium point origin nonlinear network local stabl network return equilibrium point if initi condit suffici near it but result this section contrast appli full nonlinear circuit model allow one conclud certain circumst network oscil even if the initi state arbitrarili far the equilibrium point def a function describ in section iii said tc satisfi the popov criterion lo if exist a real number r o such re l+jwr note posit real function satisfi the popov criterion the reader easili verifi in exam~l i satisfi the popov criterion a rang valu the import effect the term l+jwr in def rotat the nyquist plot counterclockwis progress greater amount increas theorem consid a network consist nonlinear 2-termin resistor capacitor cell linear output imped suppos the resistor curv character continu diffefenti function gk vk gk o gk vk valu of vk ii the capacitor character by ck vk ~k with ci ck v c2 valu of vk iii the imped zk pole in the close right half plane satisfi the popov criterion common valu of if condit satisfi the network stabl in the sens that for ani initi condit oo i branch dt the proof base tellegen 's theorem is rather involv omit and appear elsewher it acknowledg we sincer thank professor carver mead of cal tech for enthusiast support this work and for make it possibl for us present earli report it in this confer proceed this work supportedj defens advanc research project agenc the offic of naval research arpa order no contract no and defens advanc research project agenc darpa contract no
----------------------------------------------------------------

title: 48-a-neural-network-classifier-based-on-coding-theory.pdf

neural network classifi base code theori tzt-dar chlueh rodney goodman eanrornla instltut technolog pasadena eanromla abstract new neural network classifi propos transform classif problem code theori problem decod noisi codeword input vector featur space transform intern represent codeword code space error correct decod space classifi input featur vector class two class code give high perform hadamard matrix code maxim length sequenc code show number class store n-neuron system linear signific obtain use hopfield type memori classifi i introduct associ recal use neural network recent receiv great deal attent hopfield paper describ mechan iter feedback loop stabil memori element nearest input provid mani memori vector store machin he also shown number memori store n-neuron system mceliec work show synchron oper hopfield memori n/ 2iogn data vector store reliabl larg abu-mostafa predict upper bound number data vector n-neuron hopfield machin n. believ one abl devis machin number data vector linear larger achiev hopfield method featur space code space figur classif problem versus error control decod problem paper specif concern problem classif pattern recognit propos new method build neural network classifi base well establish techniqu error control code consid typic classif problem one given priori set class m. associ class featur vector label class exemplar class i.e american institut physic repres point class region input classifi class nearest exemplar input henc class region n-dimension binari featur space bn everi vector classifi correspond class similar problem decod codeword error correct code shown case codeword construct design usual least dmtn apart receiv corrupt codeword input decod find nearest codeword input principl distanc codeword greater 2t possibl decod classifi noisi codeword featur vector correct codeword exemplar provid ham distanc noisi codeword correct codeword note guarante exemplar uniform distribut bn consequ attract radius maximum number error occur given featur vector vector correct classifi depend minimum distanc exemplar mani solut minimum ham distanc classif propos one common use deriv idea match filter communic theori lippmann propos two-stag neural network solv classif problem first correl input exemplar pick maximum winner-take-al circuit network compos two-input compar figur fi.f2 fn input bit si.s2 sm match score similartti exemplar second block pick maximum si.s2 sm produc index exemplar largest score main disadvantag classifi complex maximum-pick circuit exampl winner-take-al net need connect weight larg dynam rang graded-respons neuron whilst compar maximum net demand m-i compar organ log2m stage decoder~ss f i closs f featur space code space match filter type classifi structur propos classifi main idea thus transform everi vector featur space vector code space way everi exemplar correspond codeword code code prefer necessarili properti codeword uniform distribut code space ham distanc everi pair codeword transform turn problem classif code problem decod noisi codeword error correct decod vector code space obtain index noisi codeword henc classifi origin featur vector shown figur paper develop construct classif machin follow first consid problem transform input vector featur space code space describ two hetero-associ memori do first method use outer product matrix techniqu similar hopfield 's second method generat matrix pseudoinvers techruqu s.7j given transform problem associ recal classif problem decod noisi codeword next consid suitabl code machin requir codeword code properti orthogon pseudo-orthogon ratio cross-correl auto-correl codeword small show two class good code particular decod problem hadamard matrix code maxim length sequenc code 8j next formul complet decod algorithm describ overal structur classifi term two layer neural network first layer perform map oper input second one decod output produc index class input belong second part paper concern perform classifi first analyz perform new classifi find relat maximum number class store classif error rate show use transform base outer product method neglig misclassif rate larg n. tight lower bound m. number store class present comprehens simul result confirm exceed theoret expect simul result compar method hopfield model outer product pseudo-invers method analog hard limit connect matric case classifi exceed perform hopfield memori term number class reliabl recov d. transform techniqu object build machin discrimin among input vector classifi one appropri class suppos bn exemplar ofth correspond class given input want machin abl identifi class whose exemplar closest want calcul follow function class i i i i i i denot ham distanc bn approach problem seek transform map exemplar bn correspond codeword bl input featur vector dey thus map noisi codeword wli error ad exemplar correspond error pattern code space error correct decod get index correspond codeword note may ham weight transform may either generat error elimin error present origin input featur vector requir satisfi follow equat m-l implement use single-lay feedfoiward network thus first construct matrix accord set call defin sgn threshold oper map vector rl bl field real number let matrix whose lth column matrix whose th column two possibl method construct matrix follow scheme outer product method scheme matrix defin sum outer product exemplar-codeword pair m-l die equival wdt scheme pseudo-invers method want find matrix satisfi follow equat general squar matrix moreov may singular d-l may exist circumv difficulti calcul pseudo-invers denot dt matrix instead real invers let dtd -ldt formul dt ot nt code code look prefer properti codeword distribut uniform bl distanc two codeword must larg possibl thus seek class equidist code two class hadamard matrix code maxim length sequenc code first defin word pseudo-orthogon defmit let wo wl wl-l bl ath codeword code code said pseudo-orthogon iff l-l wl denot inner product two vector hadamard matric orthogon code length whose codeword row column hadamard matrix case distanc two codeword l/2 conjectur exist code multipl thus provid larg class code mazlmal length sequenc code exist famili maxim length sequenc also call pseudo-random pn sequenc code generat shift regist satisfi pseudo-orthogon suppos primit polynomi degre d. let 2d f xl l/g xl ck xk k=o co.cl period sequenc period sinc i code made cyclic shift co. cl il code satisfi pseudo-orthogon one easili see minimum distanc code give correct power approxim error larg l. iv overal classifi structur shall describ overal classifi structur essenti consist map follow error correct decod maxim length sequenc code hadamard matrix code decod oper correl input vector everi codeword threshold result rational algorithm follow sinc distanc everi two codeword code exact bit decod abl correct error pattern less error threshold set halfway land i.e suppos input vector decod ham weight nonzero compon 2s 2s equat less error away less result arriv follow decod algorithm deax1 sgn vector case less error input output vector sm one compon posit index index class input vector belong howev there error output either negat vector decod failur anoth vector one posit compon decod error function class defin composit decod overal structur new classifi depict figur view two-lay neural network hidden unit output neuron first layer map input featur vector noisi codeword code space intern represent second one decod first output produc index class input belong f1 f2 fn 9l figur overal architectur new neural network classifi perform analysi previous section know classifi make error transform vector code space input decod less error proceed find error rate classifi case input one exemplar error say outer product connect matrix follow approach mcel1ec we n-l m-l sgl wl dj j=o n-l sgn m-l wl dl j=o a=o assum without loss general if n-l m-l j=o wl dl a=o notic we assum random name compon outcom bernoulli trial accord sum independ ident distribut random variabl mean varianc asymptot case nand larg approxim normal distribut mean varianc nm thus pr q vln/m vi tt foo dt next we calcul misclassif rate new classifi follow assum pe k=il/4j pk l_p l-k integ floor sinc generallt possibl express summat explicit we use chernoff method bound pe multipli term summat number larger uniti et k sum from instead pe t k k=o differenti rhs equat set we find optim eto condit impli sinc we deal case small automat satisfi substitut optim we obtain from express pe we estim number class classifi negllgibl misclassif rate follow way suppos pe land small we log sinc fix valu approach infin we from lower bound one easili see new machln abl classifi constant time class better number memori item hopfield model store le although analysi done assumlng approach lnfinlti simul result next section show moder larg lower bound appll vi simul result charact recognit exampl we simul hopfield model new machin use maxlmallength sequenc code follow four case respect connect matrix generat outer product method connect matrix generat pseudo-invers method ill connect matrix generat outer product method compon connect matrix hard limit connect matrix generat pseudo-invers method compon connect matrix hard limit case choic n. program fix number error in input vector random generat set exemplar comput connect matrix machin machin random pick exemplar add nois random complement specifi number bit generat trial input vector simul machin check whether input classifi nearest class report percentag success machin simul result shown in figur in graph horizont axi vertic axi attract radius data we show obtain collect case success rate fix largest attract radius number bit in error input vector success rate here we use attract radius denot particular m. input exemplar success rate less in machin hopfield model new classifi op new classtfier pi analog connect matrix binari connect matrix cil lit td i en tl.a binari connect matrix analog connect matrix figur simul result hopfield memori new classifi hopfield model new classifi op.l=63 new classifi op.l=31 ril u.a figur perfonn new classifi use code differ length in case classifi exceed perfonn hopfield model in tenn number class reliabl recov exampl consid case hard limit connect matrix new classifi hopfield model we find attract radius zero error in input vector hopfield model classif capac approxim new model store also attract radius averag error in input vector hopfield model reliabl store class our new model store class anoth simul use shorter code instead reveal shorten code perform classifi degrad slight we therefor conjectur it possibl use tradit error correct code bch code intern represent howev go higher rate code one trade minimum distanc code error toler for complex number hidden unit impli possibl poorer perform classifi we also notic superior pseudoinvers method outer product method appear connect matric hard limit reason for pseudoinvers method best for decorrel depend among exemplar yet exemplar in simul generat random presum independ consequ one see advantag pseudoinvers method for correl exemplar we expect pseudoinvers method clear better next exampl next we present exampl appli classifi recogn charact each charact repres pixel array input generat flip everi pixel probabl input pass five machin hopfield memori new classifi either pseudotnvers method outer product method figur show result machin for pixel flip probabl respect a blank output mean the classifi refus make a decis first note the case necessarili wors the case confirm the earlier conjectur fewer hidden unit shorter code degrad perfonn slight also one easili see the pseudoinvers method is better the outer product method the correl exemplar both method outperform the hopfield memori sinc the latter mix exemplar to rememb produc a blend exemplar rather the exemplar accord it classifi the input without mistak tf figur the charact recognit exampl pixel revers probabl input correct output hopfield model new classifi op pi pi figur the charact recognit exampl pixel revers probabl input correct output hopfield model new classifi op pi pi vll conclus in this paper we present a new neural network classifi design base code theori techniqu the classifi use codeword from an error correct code intern represent two class code give high perform the hadamard matrix code the maxim length sequenc code in penorm term we shown the new machin is signific better use the hopfield model as a classifi we also note compar the new classifi the hopfield model the increas perform the new classifi entail extra complex sinc it need hard limit neuron and l n connect weight versus neuron and n2 weight in a hopfield memori in conclus we believ our model form the basi a fast practic method of classif with an effici greater previous neural network techniqu
----------------------------------------------------------------

title: 53-the-connectivity-analysis-of-simple-association.pdf

connect analysi simpl associ orhow mani connect you need dan hammerstrom oregon graduat center beaverton abstract effici realize use current silicon technolog larg connect network vlcn billion connect requir network exhibit high degre communic local real neural network exhibit signific local yet connectionist/neur network model littl paper connect requir simpl associ network analyz use communic theori sever techniqu base communic theori present improv robust network face spars local interconnect structur also discuss potenti problem inform distribut wide introduct connectionist/neur network research learn program network exhibit broad rang cognit behavior unfortun exist comput system limit abil emul network effici cost emul network whether special purpos high parallel silicon-bas architectur tradit parallel architectur direct proport number connect network number tend increas geometr number node increas even larg massiv parallel architectur connect take time silicon area mani exist neural network model scale poor learn time connect preclud larg implement connect cost network direct relat local network exhibit local communic process element connect physic adjac process element reason map element onto planar surfac much evid real neural network exhibit locality2 paper techniqu present analyz effect local process associ network use complex node similar higher-ord learn unit maxwel network model network model use paper defin figur definit recurs neural network call c-graph graph structur set cns network node whose output take rang posit real valu vi n. node set set codon take rang posit real valu eij codon node ne codon dedic cn output codon use local total ne n. codon network fan-in order codon ie assum lei codon ne cn work support part semiconductor research corpor contract joint offic naval research air forc offic scientif research onr contract nooo14 american institut physic ie codon figur cijk set connect on codon ne cijk take two valu indic exist connect codon definit valu vi f 8+~eijl j-l function continu non-linear monoton function sigmoid function definit defin map input vector rand ie element input vector codon element element zk cijk=1 function indic subset seen codon differ input vector may map codon vector d j zj-i definit codon valu eij determin follow let input vector learn input vector on codon eij on let tij set i cdimension vector lij e ij vector lij tij consist subvector codon ii 's recept field variabl index vector ij number distinct vector tij may less total number learn vector though distinct subset lij need sinc possibl mani one map vector onto vector lij let xl subset vector vi=l on suppos output vector vi=o defin izeof map i number xo vector map iij l tlj-o number vector map iii compreaaion codon vector defin i ij hqj l =o nt output codon eii maximum-likelyhood decod he indic like hood vector map input vector i i current input vector word vector set subset learn vector codon ij receiv closest use distanc measur subset input vector output codon most-lik output accord input exampl code compress codon closest term measur vector distanc ham distanc subvector recept field codon belong learn vector cn output codon describ veri similar propos marr implement ne lrest-neighbor classif assum codon function determin static prior network oper desir categori alreadi learn measur perform network capac use definit input noiae averag input vector closest minimum learn vector measur differ two vector bit vector ham distanc output nois averag distanc network output learn output vector associ closest learn input vector in/orm gain gr gt i definit capac network maximum number learn vector inform gain gr strict posit communic analog consid singl connect network node cn remaind paper restrict singl assum cn output valu space restrict two valu therefor cn must decid whether input see belong class code code remain class code code becom activ input see recept field constitut subset input vector function network also assum cn ideal i-nn nearest neighbor classifi featur detector given particular set learn vector cn classifi arbitrari input accord class nearest use measur distanc learn vector situat equival case singl cn singl codon whose recept field size equival cn imagin sender wish send one bit inform noisi channel sender probabilist encod chose code word learn vector accord probabl distribut receiv know code set though knowledg bit sent nois ad code word transmiss channel analog appli input vector network 's input vector lie within learn vector 's region nois repres distanc input vector associ learn vector code word sent channel consist bit seen recept field on model associ map input vector output vector on must respond appropri output associ learn output vector therefor on decod estim class receiv code word belong classic block encod problem increas field size equival increas code length recept field size increas perform decod improv presenc nois use communic theori trade-off interconnect cost relat field size function node relat correct decis make process output error character recept field size node increas redund input though depend on particular code use learn vector sinc situat increas field size provid addit inform point diminish return addit bit provid ever less reduct output error anoth factor interconnect cost increas exponenti field size result two trend cost perform measur singl global maximum valu word given set learn vector probabl set interconnect cost best recept field size determin beyond increas connect bring diminish return singl codon code compress singl neural element singl codon code compress model exact communic channel figur network node assum singl codon whose recept field size equal recept field size node sender i i encod noisi i i transmitt receiv i decod on figur transmiss channel recelv oper channel follow bit input channel encod select random code length transmit code channel receiv use nearest neighbor classif decid origin messag either let number code word use encod rate indic densiti code space definit rate communic channel block length correspond direct recept field size codon deriv later section use relat measur definit code util number learn vector assign particular code written term 2n approach code compress increas essenti unbound sinc may signific larger decod error inform loss due code compress random variabl depend on compress rate priori probabl therefor differ differ learn vector set codon within set averag code util codon approach code compress occur often codon decod error unavoid let zi vector output encod input channel element zi either let vi vector output channel input decod element either noisi channel code theorem present general case individu input code distinguish result extend cn even though input code use on need distinguish code must output must output theorem gallag random code assum throughout theorem let discret memoryless channel transit probabl pnu/k posit integ posit number consid ensembl block code letter code word independ select accord fe probabl assign messag nr ensembl averag probabl decod error use maximum-likelyhood decod satisfi definit given theorem notat gall ager use mani definit theorem also gallag q k pu/kp p i-il l+p k-il result adjust ror special case theorem singl cn averag channel error rate ror random code vector pe i probabl input vector bit result cover wide rang model easili comput express deriv recogn restrict inher cn model first assum channel code bit equal like i error model binari symmetr channel error ident distribut independ bit probabl error independ code word bit posit code word simplifi version theorem deriv maxim give tightest bound o p~l maxp p let codon input block length i minimum valu express obtain eo log single-codon code compress unfortun implement complex codon grow exponenti size codon limit practic size altern approxim singl codon function singl cn mani smaller overlap codon goal maintain perform reduc implement cost thus improv cost/perform decod process codon get smaller recept field size becom smaller relat number cns network happen codon compress vector alia6 introduc error decod process due inform loss network overcom error use multipl redund codon overlap recept field tend correct compress error compress occur two code word requir differ decod output share represent within recept field codon follow theorem give probabl incorrect codon output without compress error theorem bsc model codon recept field ic code util channel bit select random independ probabl codon decod error approxim expect compress error per codon approxim pc equat exp log i-ri proof given hammerstrom6 grow pc approach asymptot thus perform singl codon degrad rapid presenc even small amount compress multipl codon code compress use multipl small codon effici larg codon fundament perform constraint codon split two smaller codon origin recept field subdivid accord there sever effect consid first error rate new codon increas due decreas recept field size codon 's block code length second effect code util increas codon sinc number learn vector map smaller recept field chang also increas error rate per codon due code compress fact individu codon recept field get smaller signific code compress occur higher-ord input code there ad error occur order individu codon decreas sinc random code assum effect consid third effect mass action larg number codon even though individu codon may error major correct on correct output effect decreas total error rate assum on one codon union recept field codon recept field on no restrict on degre overlap various codon recept field within on on larg number codon codon overlap general random uniform distribut also assum transmiss error seen differ recept field independ now consid happen codon 's compress error rate ignor transmiss error time codon replac two smaller co don cover recept field replac process continu there codon incident analog current neural model multipl codon on assum codon vote summat unit total inform output major codon vote etc theorem probabl on error due compress error pc pc dy given equat pc incorpor two effect move multipl smaller codon ad codon use equat give total error probabl per bit pen proof hammerstrom6 network perform associ defin this paper connect weight rapid approach singl uniform valu size network grow inform theoret term inform content weight approach zero compress increas whi simpl non-conjunct network 1-codon equival work ali next section i defin connect cost constraint show answer first question general associ structur defin scale costeffect import there limit degre distribut inform connect cost much easier assess cost implement medium assum i chosen standard silicon two dimension surfac on 's codon take surfac area accord recept field size in addit there area devot metal line interconnect on specif vlsi technolog need assum sinc comparison relat thus keep on codon metal in proper proport accord standard metal width also includ inter-met pitch analys perform it assum level metal possibl in previous section i establish relationship network perform in term transmiss error rate network capac m. in this section i present implement cost total silicon area this figur use deriv cost/perform figur use compar factor codon size recept field size there two compon total area on area on ami area metal interconnect on aon consist silicon area requir codon on metal area local intra-on interconnect consid much smaller codon global inter-on interconnect consid area per on rough aon cfeme maximum number vector codon must distinguish for theorem assum rectangular un6ound grid on on equi-dist four nearest neighbor on bound recept field non nearest on on recept field size for on non c~e number codon intra-on redund ratio input synaps r=l on input use on input use on averag two site metal area requir support on 's recept field proof give hammerstrom6 ami total area per on anoth implement iitrategi ill place enii along diagon givel area howev thill techniqu work ror bound number enii when dendrit comput lipread larg area limit the rang p08llibl en implementationl the theorem iitat cover infinit plane enii bound recept held even with the assumpt maximum local the total metal interconnect area increas as the cube the per cn recept field size singl cn simul what do the bound tell us cn connect requir from simul increas the cn 's recept field size improv the perform increas capac there also increas cost increas faster the perform anoth observ redund quit effect as mean for increas the effect cn with constrain connect there limit sinc it reach point the intra-cn connect approach inter-cn for situat with fix non increas cost-effect possibl increas order redund in order verifi the deriv bound i also wrote discret event simul cn random set learn vector chosen the cn 's codon program accord the model present earlier learn vector chosen random subject random nois the cn then attempt categor these input two major group cn output cn output for the part the analyt bound agre with the simul though tend optimist in slight underestim the error these differ easili explain the simplifi assumpt made make the analyt bound mathemat tractabl distrmut vs local throughout this paper it tacit assum represent distribut across a number cns singl cn particip in a number represent in a local represent cn repres a singl concept featur it the distribut represent make the cn 's decod job difficult sinc it the caus the code compress problem there much debat in the connectionist/neuromodel communiti as the advantag disadvantag approach the interest reader refer hinton7 baum ballardq some the result deriv relev this debat a1 the distribut represent increas the compress per cn increas accord it shown the mean error in a codon 's respons quick approach independ the input nois this result also hold the cn level for individu cn this error offset ad codon this expens tend obviat one the argument in favor distribut represent the multi-us advantag where fewer cns need complex redund encod a1 the degre distribut increas the requir connect the code compress increas the ad inform each codon add cn 's decod process goe zero equival weight approach a uniform valu summari conclus in this paper a singl cn node perform model develop that base on communic theori likewis an implement cost model deriv the communic model introduc the codon as a higher-ord decod element and show that for small codon much less total cn fan-in converg code compress or vector alias within the codon 's recept field a sever problem for larg network as code compress increas the inform ad individu codon the cn 's decod task rapid approach zero the cost model show that for 2-dimension silicon the area requir for inter-nod metal connect grow as the cube a cn 's fan-in the combin these two trend indic that past a certain point high depend on the probabl structur the learn vector space increas the fan-in a cn as done for exampl when the distribut represent increas yield diminish return in term total cost-perform though the rate diminish return decreas the use redund higher-ord connect the next step appli these techniqu to ensembl node cns oper in a competit learn or featur extract environ
----------------------------------------------------------------

title: 55-mathematical-analysis-of-learning-behavior-of-neuronal-models.pdf

mathemat analysi learn behavior neuron model john y. cheung massoud omidvar school electr engin comput scienc univers oklahoma norman ok present ieee confer neural inform process systemsnatur synthet denver novemb publish collect paper ieee confer nip pleas address correspond john y. cheung school eec w. boyd cec norman ok novemb american institut physic mathemat analysi learn behavior neuron model john y. cheung massoud omidvar school electr engin comput scienc abstract paper wish analyz converg behavior number neuron plastic model recent neurophysiolog research suggest neuron behavior adapt particular memori store within neuron associ synapt weight vari adjust achiev learn number adapt neuron model propos literatur three specif model analyz paper specif hebb model sutton-barto model recent trace model paper examin condit converg posit converg rate converg model appli classic condit simul result also present verifi analysi introduct number static model describ behavior neuron use past decad recent research neurophysiolog suggest static view may insuffici rather paramet within neuron tend vari past histori achiev learn it suggest alter intern paramet neuron may adapt repetit input stimuli becom condit learn thus occur neuron condit describ behavior neuron plastic number model propos earliest one may postul hebb more recent sutton barto also introduc new model recent trace mrt model paper primari object paper howev analyz converg behavior model adapt general neuron model use paper shown figur number neuron input n. each input scale correspond synapt weight n. weight input arithmet sum taken zero neuron input assum take numer valu rang zero one inclus synapt weight allow take reason valu purpos paper though realiti weight may well bound sinc relat magnitud weight neuron input well defin point put bound magnitud weight also neuron output normal result sigmoid transform simplic approxim oper linear transform sigmodi transfonut neuron output rilur lener aeuron adel converg analysi assum two neuron input tradit classic condit environ simplic cours analysi techniqu extend number input classic condit two input condit stimulus xc uncondit stimulus sutton-barto model more recent sutton barto propos adapt model base signal trace output trace given xi axi xi posit constant condit converg order simplifi analysi choos word becom wi wi cxi assumpt serv simplifi analysi affect converg condit bounded depend xi respect previous section recogn recurr relat converg check ratio test it also possibl rewrit matrix format due recurs neuron output equat includ neuron output paramet vector also show converg need set magnitud determin less uniti henc condit converg see adapt constant must chosen less reciproc euclidean sum energi input techniqu extend number input prove mere follow procedur outlin posit converg have prove converg sutton-barto model equat neuron plastic want find next locat system remain converg seen earlier converg weight ceas chang neuron output denot converg posit word sinc arbitrari paramet vector alway decompos weight sum eigenvector constant ql q3 easili found invert eigenvalu shown within region converg magnitud third eigenvalu less uniti mean converg there contribut third eigenvector henc predict precis converg posit would given initi condit rate converg seen care chosen sutton-barto model converg also deriv express converg posit next want find fast converg attain rate converg measur fast initi paramet approach optim posit asymptot rate converg sea spectral radius equal case complet converg analysi sutton-barto model neuron plastic mrt model neuron plastic recent trace mrt model neuron plastic develop author consid cross sutton-barto model klopf 's model adapt synapt weight express follow comparison sutton-barto model ahowl cond term right hand aid contain extra factor ii use ape converg ahowd later output trace hu replac recent output henc name recent trace model input trace also replac recent input condit converg proceed analyz condit converg mrt model due presenc wi factor second term ratio test appli analyz converg behavior let us rewrit matrix format wi superscript denot matrix transpos oper equat quadrat complet converg analysi equat extrem difficult order understand converg behavior note domin term determin converg main relat second quadrat term henc converg analysi we ignor first term we readili see primari converg factor bt sinc depend converg obtain if durat synapt input activ bound it shown condit converg bound we readili see adapt constant chosen accord ensur converg t. simul verifi theoret analysi three adapt neuron model base classic condit model simul mm mainfram use fortran languag singl precis sever test scenario design compar analyt predict actual simul result verifi condit converg we vari valu adapt constant condit uncondit stimuli set to uniti valu vari to sutton-barto model simul given show converg obtain expect theoret analysi mrt model simul result given show converg obtain also expect theoret analysi theoret locat converg sutton barto model also shown in figur it readili seen simul result confirm theoret expect i output figur lou murod l tpuu yet.us ule er suttoa-barto el witb 1frerent alu aptat1on codstant lleuroul output i i i ju.ber iteratioga figur plotl oeuroaal outputl craus uuaber iterati mat el differ alu adantatlon i ddstaut to illustr rate converg we plot trajectori deviat in synapt weight from optim valu in logarithm scale sinc error logarithm as found earlier slope line yield rate converg trajectori sutton-barto model given in figur mrt model given in figur it clear from figur trajectori in the logarithm form a straight line the slope readili calcul the curv the mrt model given in figur also a straight line a much larger slope show faster converg summari in paper we sought to discov analyt the converg behavior three adapt neuron model from the analysi we see the hebb model converg constant activ input the output grow exponenti in spite lack converg the hebb model still a workabl model realiz the diverg behavior would curtail by the sigmoid transform to yield realist output the i uroul output dniatiotl lto i i u.ber iterationa figur trajectori of deuron output deviationa froa atat alu the sutton- rt el lfferent valu adapt coiistallt c. lleurod l output deviat ltl i nuaber of iter figur trajectori of neuron output deviat fra atat valu for tbe krt el witb differ valu of adapt constant analysi the sutton barto model show model converg when the adapt constant care chosen the bound for also found for this model due to the structur of this model the locat at converg the rate of converg also found we also introduc a new model of neuron plastic call the recent trace mrt model certain similar exist the mrt model the sutton-barto model also the mrt model and the klopf model analysi show the updat equat for the synapt weight quadrat result in polynomi rate of converg simul result also show that much faster converg rate obtain with the mrt model
----------------------------------------------------------------

title: 32-synchronization-in-neural-nets.pdf

synchron neural net jacqu j. vidal univers california los angel los angel ca john haggerti abstract paper present artifici neural network concept synchroniz oscil network instant individu fire form point process constitut form inform transmit join neuron type communic contrast assum model typic continu discret value-pass network limit messag receiv process unit time marker signal fire unit present signific implemen tation advantag model neuron fire spontan regular absenc perturb interact present schedul fire advanc delay fire neighbor neuron network neuron becom global oscil exhibit multipl synchron attractor arbitrari initi state energi minim learn procedur make network converg oscillatori mode satisfi multi-dimension constraint network direct repres rout schedul problem consist order sequenc event introduct most neural network model deriv variant rosenblatt origin perceptron value-pass network case particular network propos fukushima i hopfield rumelhart mani other everi case input process element either binari continu amplitud signal weight synapt gain subsequ sum integr result activ pass sigmoid threshold filter produc continu quantiz output may becom input neuron behavior model relat live neuron even fall consider short account complex inde observ mani real neuron action potenti spike fire propag axon branch intern activ reach threshold higher john haggerti interact system los angel w. 6th st. la ca american institut physic input rate level result rapid fire behind tradit model assumpt averag frequenc action potenti carrier inform neuron becaus integr fire individu neuron consid effect extent contribut averag intens therefor assum activ simpli frequenc code exact time individu fire ignor view howev cover well known aspect neural communic inde precis time spike arriv make crucial differ outcom neural interact one classic exampl pre-synapt inhibit widespread mechan brain machineri sever studi also demonstr occurr function import precis time phase relationship cooper neuron local network model present paper contrast one mention network fire consid individu output event input side node fire node presynapt neuron either delay inhibit advanc excit node fire seen earlier type neuron interact would call phase-modul engin system also find rational experiment neurophysiolog neurophysiolog plausibl howev major concern rather propos explor potenti use mechan parallel distribut comput merit approach artifici neural network digit puls use internod communic instead analog voltag model particular well suit time-ord sequenc found larg class rout trajectori control problem neuron synchroniz oscil model process element neuron relax oscil built-in self-inhibit relax oscil dynam system capabl accumul potenti energi threshold breakdown point reach point energi abrupt releas new cycl begin descript fit dynam behavior neuron membran rich structur empir model this behavior found well-establish differenti formul hodgkin huxley simplifi version given fitzhugh7 differenti equat account foundat neuron activ also capabl repres subthreshold behavior refractori follow fire membran potenti enter critic region abrupt depolar collaps potenti differ across membran occur follow somewhat slower recoveri this brief electr short membran call action potenti spike constitut output event neuron if caus initi depolar maintain oscil limit-cycl develop generat multipl fire depend input level membran paramet oscil limit singl spike may produc oscillatori burst even continu sustain activ present model share general properti use much simpler descript relax oscil illustr figur activ energy exdt3toij oj input injrjh1~olj input perturb utl intemilf l neju inpul ty figur relax oscil perturb input fire occur energi level reach critic level ec assum constant rate energi influx fire occur natur period ec t=a when pre-synapt puls imping on cours energi accumul fire schedul disturb let repres instant last fire cell tj intant imping arriv cell e act wj uo til ec uo repres unit impuls dramat complex synchron dynam appreci consid simplest possibl case master slave interact two regular fire oscil unit natur period ta tb instant fire unit unidirect send spike signal unit receiv interv measur from last time fire upon recept spike transform quantum energi 6e depend upon post-fir arriv time relationship shape repres refractori post-spik properti here it assum simpl ramp function if interact inhibitori consequ this arriv next fire unit delay respect schedul would absenc perturb posit interv figur becaus shape delay action nil immedi fire becom longer imping pre-synapt spike arriv later interv if interact excitatori delay negat le shorten natur fire interv general assumpt regard function tend synchron within given rang coupl gain phase self-adjust equilibrium achiev given this equilibrium correspond distribut maximum entropi point cell receiv amouint activ common cycl i i inhibit excit figur relationship phase delay when input effici increas linear after-spik interv synchron dynam present attractor ration frequenc pair ratio associ rang stabil ratio lowest cardin wide zone phaselock figur wider stabil wnes correspond one one ratio fa fb invers ta tbl kohn segundo demonstr such phase lock occur live invertebr neuron point paradox natur phase-lock inhibit within stabil region take appear excit sinc small increas input fire rate local result increas output rate area these rang stabil appear unstabl transit fact as recent point out bak9 form infin lock step known as devil 's staircas correspond infin intermedi ration pair figur bak show staircas self-similar scale transit form fractal cantor set fractal dimens univers constant dynam system i excit inhibiti~~v i li l figur unilater synchron constraint satisfact in oscil network global synchron interconnect network mutual phase-lock oscil constraint satisfact problem synchron equilibrium node fire in interlock pattern organ inter-spik interv integ ratio often cite travel salesman problem archetyp for a class import hard problem a special case when ratio must node must fire at frequenc here the equilibrium condit everi node accumul the the amount energi the global cycl furthermor the fire must order along a minim path use stochast energi minim simul anneal the first simul demonstr the feasibl the approach with a limit number node the tsp isomorph mani sequenc problem involv distribut constraint fall the oscil array neural net paradigm in a particular natur way work pursu rigor establish the limit applic the model i annea/./ng figur the travel salesman problem in the global oscil oj minim energi node constrain fire at the rate in the order correspond to the minim path acknowledg research support in part aerojet electro-system under the aerojet-ucla cooper research master agreement no nasa nag
----------------------------------------------------------------

title: 49-connecting-to-the-past.pdf

connect past bruce a. macdonald assist professor knowledg scienc laboratori comput scienc depart univers calgari univers drive nw calgari alberta t2n in4 abstract recent renew interest neural-lik process system evidenc exampl two volum parallel distribut process edit rumelhart mcclelland discuss parallel distribut system connectionist model neural net valu pass system multipl context system dissatisfact symbol manipul paradigm artifici intellig seem part respons attent encourag promis massiv parallel system implement hardwar paper relat simpl neural-lik system base multipl context well-known formalisms-nam product system k-iength sequenc predict finite-st machin ture machines-and present earlier sequenc predict result new light introduct reviv neural net research strong exemplifi recent rumelhart mcclelland new journal number meet net also describ parallel distribut system connectionist model valu pass systems3 multipl context learn systems4 symbol manipul paradigm artifici intellig seem success hope seem last real promis massiv parallel system implement hardwar howev flurri new work import consolid new idea place solid alongsid establish one paper relat simpl neural-lik system well-known notions-nam product system k-iength sequenc predict finite-st machin ture machines-and present earlier result abil network new light general form connectionist system lo simplifi three layer net binari fix weight hidden layer therebi avoid mani difficulties-and challengesof recent work neural net hidden unit weight regular pattern use templat sophist expens learn algorithm avoid simpl method use determin output unit weight way gain advantag multilay net retain simplic two layer net train method certain noth lost comput power-a i explain-and limit two layer net carri simplifi three layer one biolog system may similar avoid need learn algorithm simul anneal method common use connectionist model one thing biolog system clear distinguish train phase briefli simplifi net product system implement three layer neuron-lik unit output layer input layer hidden layer product hidden product unit potenti connect predetermin set input output k-iength sequenc predictor form ie level delay unit introduc input layer k-iength predictor unabl distinguish simpl sequenc ba aa sinc ie charact system forgotten whether appear first k-iength predictor augment auxiliari action abl learn regular languag sinc auxiliari action equival state input aamong 1st intern confer neural net san diego ca june con.fer brough equival singl context system andrea 's multipl context system see also macdonald american institut physic figur general form connectionist system form unit oper within unit in~ut excitation-.i weight sum activ w output typic typic product unit enabl predict depend previous state combin sever augment sequenc predictor thring machin tape simul along finite-st control give net comput power univers ture machin relat simpl neural-lik system lack comput abil previous implement abil product system equival simplifi net organ paper next section briefli review general form connectionist system section simplifi section explain result equival product system deal input output net section extend simplifi version enabl learn predict sequenc section explain comput power sequenc predictor increas thring machin input unit receiv auxiliari action fact system learn ture machin section discuss possibl number net combin output form overal net associ area general form connectionist system figur show general form connectionist system unit neuron ce1l figur unit input output oj possibl unit network output net input excit net weight sum input vij weight connect output unit input unit activ unit function fi net input excit typic fi semilinear non-decreas differenti function least larg group unit output function fi activ typic kind threshold function i assum quantiti vari discret time step exampl activ time given fi neti general restrict connect may made unit unit connect direct input output hidden unit complex net describ paper may one type connect figur show common connect topolog three layer units-input hidden output-with cycl connect net train present input combin along desir output combin train system produc desir output given figur basic structur three layer connectionist system input unit hidden unit output unit input dure train weight adjust fashion reduc discrep desir actual output general method lo desir train activ equat general form hebb 's classic rule adjust weight two unit high activ lo weight adjust product two function one depend desir actual activ often difference-and anoth depend input weight weight simpl exampl suppos differ output oj weight chang product output error input excit weight constant determin learn rate widrow-hoff delta rule may use net without hidden unit 1o import contribut recent work connectionist system implement equat hidden unit train signal ti direct avail boltzmann learn method iter vari weight hidden unit train activ use control gradual decreas random method simul anneal backpropag also iter perform gradient descent propag train signal error back net hidden unit i avoid need determin train signal hidden unit fix weight hidden unit section simplifi system assum simplif made general connectionist system section system three layer topolog shown figur ie cycl hidden layer unit weight fix say uniti zero unit linear threshold unit lo mean activ function unit ident function give net weight sum input output function simpl binari threshold form i output threshold activ output binari oft hidden unit threshold requir input activ output activ like gate output unit threshold requir two activ high weight input output generat like gate keep product system view net explain section learning-which occur output unit weights-giv weight adjust accord wij wij oj otherwis weight turn input unit output otherwis wij oj simpl exampl given figur section simpl form net made probabilist replac adjust weight wij estim condit probabl unit output output wij estim p odoj assum independ input unit output unit turn condit probabl occurr output exceed threshold output function onc simplif made need learn hidden unit also iter learn requir weight either assign binari valu estim condit probabl paper present characterist simplifi net section discuss motiv simplifi neural net way product system simplifi net kind simpl product system product system compris global databas set product rule control system databas net system interact provid input reaction output t.he net hidden unit network product rule form precondit action precondit satisfi input excit exceed threshold hidden unit action repres output unit hidden product unit activ control system product system choos rule whose action perform set rule whose precondit met neural net control system distribut throughout net output unit exampl output unit might form winner-takeal net product system complex control involv forward backward chain choos action seek goal discuss elsewhere4.12.16 figur illust.r simpl product implement neural net figur show input hidden unit element precondit appropri input combin present associ hidden product unit fire onc weight leam connect hidden unit output unit fire product result output simplifi neural net direct equival product system whose element input output product system symbol element variabl given valu product action neural net direct implement sinc output predetermin set howev see later extens t.o framework enabl abil cthis might refer sensory-motor product system sinc implement ill l'eal system robot deal sens input execut motor action may includ auxiliari action section figur product implement simplifi neural net product rule icloudi i ipressur fall i then iit rain i rule implement hidden unit threshold hidden unit gate threshold output unit gate learn weight if net probabilist otherwis estim p rainlcloud pressur fall rain weight figur net predict next charact sequenc base last charact net product unit hidden unit combin input unit exampl net could predict sequenc abcabcabc product form if last charact then next charact learn rule wij if inputj outputi output wijoj input neural net output learn procedur clamp input output desir valu system calcul weight valu repeat requir input/output combin sequenc predict product system neural net predict sequenc given exampl repeat sequenc product learn predict futur event basi recent one figur show trivial simpl sequenc predictor predict next charact sequenc base previous one figur also give detail learn procedur simplifi net net need train onc input combin then predict output everi charact seen current one probabilist form net would estim condit probabl next charact condit current one mani figur use delay input neural net implement k-length sequenc predictor net last three charact input input hidden output 2nd last exampl product if last three charact then present possibl charact pair would need proper estim probabl net would learn probabl distribut charact pair predictor like one figur extend general k-iength predictor long input delay step avail then illustr figur 3-length predict hidden product unit repres possibl combin symbol again output weight train respond previous seen input combin three charact delay provid dedic neural net shown figur note net assum synchron updat input feedback around unit chang one step output chang various way implement delay neuron andrea investig purpose-delay inputs-in detail simul similar net work sequenc predict neural net feldman ballard find connectionist system initi suit repres chang time one form chang sequenc suggest two method repres sequenc net first unit connect each sequenc sequenti task repres fire these unit success second method buffer input time input recent past avail well current input delay input avail suggest import differ necessari length buffer feldman ballard suggest buffer long enough hold phrase natur languag i expect use buffer longer andrea symbol input repres complex inform effect give length seven buffer inform recent seven simpl input discuss section method back-propagation13 enabl recurr network learn sequenti task dfeldman ballard2 give dedic neural net connect varieti flulction figur input delay dedic neural subnet two stage delay shown delay network time diagram tml origin signal delay one step delay two step manner similar first suggest last paragraph sequenc connect unit repres sequenc event one exampl net learn complet sequenc charact given first two charact six charact sequenc next four output error must propag around cycl recurr net number time serial may also achiev sequenc state distribut activ exampl net play side tic-tac-to game sequenti natur net 's behavior deriv sequenti natur respons net 's action tic-tac-to move net model sequenc intern model sequenti part environ exampl tic-tac-to play net model oppon k-iength sequenc predictor unabl learn sequenc repeat frequent everi charact their k-iength context includ inform last event howev there two way inform kth last input retain net first method latch input second involv auxiliari action latch unit input latch held indefinit use combin shown figur input would normal latch andrea discuss this techniqu thread latch event among non-latch event give net inform arbitrarili far back input-output histori inform immedi past briefli sequenc ba distinguish aa if first charact latch howev this ad hoc solut this problem auxiliari action output fed back net input signal this enabl system choos next output least part base previous one indic figur if particular fed back output also one without extern manifest whose extern manifest independ task perform then output auxiliari action las interest reader refer andrea extens analysi given figur thread latch circuit rememb event anoth come along this two input latch two letter number unit may similar connect form mutual inhibit layer winner-take-al connect along posit feedback keep select output activ when input disappear figur auxiliari actions-th outputs-ar fed back input net enabl net rememb state here part net exampl product shown there two type action charact action sinput output charact input if input charact input charact output then output charact llij ill direct effect task system perform sinc it evok relev input use net symbol action if auxiliari action latch input then symbol inform rememb indefinit lost when anoth auxiliari action kind input take latch thus auxiliari action act like rememb state system perform action remind particular state figur illustr this system that predict charact state chang given previous charact state an obvious candid auxiliari action speech blank oval figur would repres net 's environ speech action heard although it extern manifest speech direct effect physic interact the world symbol abil provid the power auxiliari action also includ other speaker the interact simul abstract automata the exampl figur give the essenc simul finit state automaton product system or it neural net equival it illustr the transit function an automaton the new state output function the previous state input thus neural net simul finit state automaton long it addit auxiliari action thring machin finit state automaton control plus an unbound memori neural net could simul a lure machin two way way demonstr product system implementations-equival neural net all multipl context learn system briefli explain section the first thring machin simul the system simul the finit state control abl use an unbound extern memori fsee john andrea 's colleagu work4 figur multipl context learn system implement multipl neural net each:3 layer net the simplifi form present a number elabor extra connect goal-seek forward backward chain output channel the real world much like the paper ture 's origin work the second simnlat.ion emb the memori the multipl context learn system along a counter access this simul memori both learn the productions-equival learn output unit weights-requir the simul the second abl add intern memori requir a limit depend the size the network easili larg enough allow year comput the second could also employ extern memori the first briefli the second simul compris multipl sequenc predictor predict auxiliari action rememb the state the control the current memori posit the memori element updat relearn the product repres that element the precondit the address the product action the store item multipl system form associ area a multipl context learn system product system version a multipl neural net although a simpl version implement a simul net it effect compris sever net or associ areas-which may output input in common indic in figur hidden unit weight specifi templat one each net a templat give the input a zero weight the hidden unit a net the input a weight uniti delay latch input also avail the actual output select the combin predict the net in a winner-take-al fashion i see the design real neural net say control real robot requir a larg degre predetermin connect a robot control could one three layer net wit.h everi input connect everi hidden unit in turn connect everi output there need some connect constraint so the net reflect the function special in the control requir the multipl context learn system all the hidden layer connect predetermin allow output connect learn this avoid the credit assign problem therefor also the need learn algorithm boltzmann learn back-propag howev as the multipl context learn system auxiliari action delay latch input it lack comput power futur work in this area investig for exampl the abil differ kind net learn auxiliari act.ion this may difficult as symbol action may provid in train input and output. for exampl a control for a robot bodi would deal vision manipul motion etc conclus this paper present a simplifi three layer connectionist model fix weight for hidden unit delay and latch for input sequenc predict abil auxiliari state action and the abil use intern and extern memori the result abl learn to simul a ture machin simpl neural-lik system not lack comput power acknowledg this work support by the natur scienc and engin council canada
----------------------------------------------------------------

title: 51-reflexive-associative-memories.pdf

reflex associ memori hendrlcus g. loo laguna research laboratori fallbrook ca abstract synchron discret model averag memori capac bidirect associ memori bam compar hopfield memori mean calculat10n percentag good recal random bam dimens differ number store vector memori capac1ti found much smal1er kosko upper bound lesser two dimens bam averag bam capac correspond hopfield memori number neuron orthonorm code bam increas effect storag capac memori capac limit due spurious stabl state aris bam much way hopfleld memori occurr spurious stabl state avoid replac threshold backlay bam anoth nonl1near process call domin label select simplest dls wlnner-take-al net give fault-sensit memori fault toler improv use orthogon unitari transform optic applic latter fourier transform implement simpli len introduct ion reflex associ memori also call bidirect associ memori two-lay neural net bidirect connect layer architectur impli dana anderson optic reson similar configur bart kosk0 coin name bidirect associ memori investig sever basic propertl concern memori capac1ti bam relat bam hopfleld memori certain variat bam american institut physic bam structur use discret model state layer neuron describ bipolar vector dirac notat use i i denot respect column row vector al la transpos alb scalar product la bl outer product depict bam two layer neuron front layer nneuron tth state vector back layer back layer neuron back neuron state vector state vector stroke ib bidirect connectlon layer allow signal flow two direct front1ay~r euron forward front stroke give state vector stroke blf connecfig bam structur tlon matrix threshold function oper zero back stroke result 1n u~grad front state also may wr1tten ib superscr1pt denot transpos1t10n consid synchron model neuron layer updat s1multan front back layer updat d1fferent t1mes bam act10n 1s shown 1n f1g forward stroke ental take scalar product front state vector row enter1ng threshold result element back state vector ib back stroke take threshold ing reflect lid nxp fig bam act threshold ing reflect hreshold 4j nxn feedback ftg autoassoc1at1v memori act10n scalar product ib w1th column vector enter threshold result element upgrad state vector contrast act10n autoassoc1at1v memori 1s shown 1n f1gure bam may also describ autoassoc1at1v memory5 concaten front back vector tnto s1ngle state vector iv =lf b take connect matrtx shown f1g autoassoclat1v memori number neuron bam viz n+p bam operat1on initi front state 1s specif threshold zero idt feedback f1ed may obtain correspond autoassoc1 memori lid zero initi spectfi ib zero bam autoassoarrang threshold1ng operat1on ctativ memori alter state vector compon hopfteld memory7 connect matrix 1s md mi m=l i store vector i tdentiti matr1x write n+p d1mens1on vector concaten idm c take form i ldm dml+lcm cml+ldm cml+lcm dmd -mi m=l w1th proper block plactng submatr1c understood write llcm dml m=l hd= lldm dmd-mi l'lcm cml -mi m=l m=l i ident appropri subspac hopfield matrix may partit shown bam matrix given kosko previous use kohonen linear heteroassoclatjv memori comparison fig show synchron discret model bam connect matrix equival hopfield memori diagon block hd hc delet sinc hopfleld memori robust prun1ng may affect much associ recal store vector if small howev averag prune improv memori capaclti it follow averag discret synchron bam matrix best capac hopfleld memori number neuron perform comput averag memori capac bam correspond hopfleld memori mont carlo calcul done memori store random bipolar vector straight recal vector check al10wtng iter bam iter start forward stroke one store vector idm use input percentag good recal standard deviat calcul result plot show squar bam capac correspond hopfleld memori although total number neuron bam need number connect hopfield memori storag capac found much smaller kosko upper bound min partit hopfield matrix m. number store vector good recal versus code bam far consid front back state use data anoth use bam front state use data back state seen provid code label pointer front state use anticip express bam matrix store data vector idm label code lem squar bam arrang cut inform contain singl store data vector jn half howev freedom choos label fc may perhap put good use part problem spurious stabl state plagu bam well hopf1eld memori load due lack orthogon store vector code bam opportun remov part problem choos label orthonorma1 label use previous kohonen 1n linear heteroassoci memori question whether memori capac improv manner explor take bat1 label chosen hadamard vector latter bipolar vector euclidean norm form orthonorm set these vector row pxp hadamard matrix discuss see harwtt sloan storag capac hadamard-cod bam calcul function number store vector case valu manner discuss percentag good recal standard deviat shown 1n it seen hadamard code give factor compar ordinari bam howev code bam half store data vector dimens account factor reduct data vector dimens effect storag capac advantag obtain hadamard code come half bat1 hadamard code code bam option delet threshold oper front layer result architectur may call half half bam threshold done label consequ data may taken analog vector although arrang diminish robust memori somewhat applic interest we calcul percentag good recal case found give data threshold cut storag capac hadamard-cod bat1 select reflex memori memori capac limit shown due occur spurious state memori load consid discret bam store data vector orthonorm label icm connect matrix input data vector iv closest store data vector one 1n forward stroke ib =s clc amlcm llv am= mlv although c vector compon sum amlc may accumul larg valu affect threshold result ib problem would avoid jf threshold oper back layer bam replac anoth nonl1near oper select i inear combin clc amlcm domin label ic hypothet devic perform oper call domin label selector dls we call result memori architectur select reflex memori back state select domin label ic back stroke give orthogon iti label icm it follow srm g1ves perfect assoc1attv recal nearest store data vector for number vector store cours llnear independ p-dimensionallabel vector icm requir dls must select linear combin orthonorm label domin label trivial case obtain choos label icm basi vector ium compon zero except for mth compon 1s uniti choic label dls may taken winnertake-al net shown winner case appear includ take-al net adapt ive reson theori art special sjmpllf1ed case relationship simplest reflex ordinari bam art memori dls point kosko in art there is cons1der fault sensit tn memori store data vector appear in connectton matrix row memori better fault toler may obtain use orthogon label basi vector dls taken orthogon transform follow winner-take-an net shown 1n is chosen 1t transform label icm rthogon i transform winner take-al net f1g select1v reflex1v memori tnto vector proport bast vector alway done tak1ng g= iup cpl p=l icp form complet orthonorm set contain label icm m=l m. neuron in dls serv grandmoth cell onc singl win cell activ state layer is singl basi vector say lu i this vector must pass back appllcat transform produc label back bam sinc 1s orthogon we so requ1r 1nvers transform may accompl1sh sfmpli send bas1 vector back transform this give 1iup cpl= c p=l requir half srm srm may modifi delet threshold oper in front layer front neuron i inear output is reflect back srm shown in in this case store data vector input data vector may taken i near neuron orthogon analog vector we transfor.1 qu1r store vector mation norm act winnerof srm proceed in i take-al net way describ except we requir orthofig half srm l1near normal label unit norm it follow l1ke neuron in front layer full srm half srm give perfect associ recal nearest store vector for number store vector dimens label latter condit 1s due fact p-dimension vector space conta1n orthonorm vector in srm the output transform gis 1ntroduc in order improv the faujt toler the connect matrix k. this is accomplish the cost fault sensit the extent need investig in this regard 1t is note in certatn optic implementat ion reflex memori such dana anderson 's reson i similar conflgur the transform is fourier transform is implement simpli len such implement ts quit insent the common semiconductor damag mechan equival autoassoci memori concaten the front back state vector allow descript the srms tn term autoassoci memori for the srm use basi vector label the correspond autoassoci memori js shown tn fjg this connect jon matrtx structur also propos guest the wtnner-take-al net need given t1me settl basi vector state the state ib slow thres hold influenc the front state if feedback this may perhap achiev zero i arrang the network ast thres threshold feedback wi bl old feedback fast compar the network an altern method equival automay equip the network associat lve memori w1th an output gate is open the net sett led these arrang present compucatlon caus delay in appllcat may 1nappropri in other may accept in trade speed memori densiti for the srm wtth output transform orthonormal1abel basi vector a corresponf eedback ding autoassoclat1v memori may compos shown in fig.l1 threshold oj oj an output gate in the layer is linear oj gt i chosen as the devic which threshold wi prevent the backstrok output gate the bam take place the w1nner-take- net settl autoassoc1at1v memori equival srm with transform the effect may perhap achiev choos differ respons time for the neuron output gate layer these matter wr winner-take-al requir investig unless woutput the output transform 1s alreadi t back layer requir for reason as in linear optic reson the dls front layer bam connect with output transform is clumsi orthogon transformat on i would far better combin winner-take-al net the transform the net a singl network to find structur of srm such a dls consid a cha eng the wort part support the defens advanc research project agenc arpa order contract daahoi-86-c with the u.s. armi missil command
----------------------------------------------------------------

title: 54-a-method-for-the-design-of-stable-lateral-inhibition-networks-that-is-robust-in-the-presence-of-circuit-parasitics.pdf

method design stabl later inhibit network robust presenc circuit parasit j.l wyatt jr d.l standley depart electr engin comput scienc massachusett institut technolog cambridg massachusett abstract analog vlsi implement neural system sometim conveni build later inhibit network use local connect on-chip resist grid serious problem unwant spontan oscil often aris circuit render unus practic paper report design approach guarante system stabl even though valu design element parasit element resist grid may unknown method base rigor somewhat novel mathemat analysi use tellegen theorem idea popov multipli control theori thorough practic criteria local sens overal analysi interconnect system requir empir sens involv measur frequenc respons data individu cell robust sens unmodel parasit resist capacit interconnect network affect analysi i introduct term later inhibit first aros neurophysiolog describ common form neural circuitri output neuron popul use inhibit respons neighbor perhap best understood exampl horizont cell layer vertebr retina later inhibit simultan enhanc intens edg act automat lain control extend dynam rang retina whole principl use design artifici neural system algorithm kohonen other electron design neural chip carver mead vlsi implement neural system conveni build later inhibit network use local connect on-chip resist grid linear resistor fabric polysilicon yield compact realize nonlinear resist grid made mos transistor found use imag segment network type divid two class feedback system feedforward-on system feedforward case one set amplifi impos signal voltag american institut physic current grid anoth set read result respons subsequ process amplifi write grid read feedback arrang feedforward network type inher stabl feedback network need practic exampl one carver meadl retina chips3 achiev edg enhanc mean later inhibit resist grid figur show singl cell continuous-tim version chip note capacitor voltag affect local light intens incid cell capacitor voltag neighbor cell ident design cell drive neighbor drive distant neighbor origin cell turn thus necessari ingredi instabl activ element signal feedback present system fact continuous-tim version oscil bad origin design scarc usabl practic later inhibit path enabl oscil i incid light figur photoreceptor signal processor circuit use two mos transconduct amplifi realiz later inhibit communic similar unit resist grid readili occur resist grid circuit activ element feedback even individu cell quit stabl analysi condit instabl straightforward method appear hopeless sinc repeat array contain mani cell influenc mani other direct indirect influenc turn number simultan activ feedback loop enorm paper report practic design approach rigor guarante system stabl simplest version idea intuit obvious design individu cell although intern activ act like passiv system seen resist grid circuit theori languag design goal celli output imped positive-r function sometim difficult practic show origin network satisfi condit absenc certain parasit element import perhap condit one verifi experiment frequency-respons measur physic appar collect cell appear passiv termin form stabl system interconnect passiv medium resist grid research contribut report summari form demonstr passiv positive-r condit much stronger actual need weaker condit easili achiev practic suffic guarante stabil linear network model ii extens nonlinear domain furthermor rule large-sign oscil certain condit ii first-ord linear analysi singl cell begin linear analysi elementari model circuit initi approxim output admitt cell simplifi topolog without loss relev inform use naive'model transconduct amplifi shown figur simplifi network topolog transconduct amplifi model circuit capacitor absorb co2 straightforward calcul show output admitt given yes positive-r passiv admitt sinc alway realiz network form shown ro2 gmlgm2rol coi/gmlgm2 although origin circuit contain inductor realize capacitor inductor thus capabl damp oscil nonetheless transamp model perfect accur network creat interconnect cell resist grid parasit capacit could exhibit sustain oscil element valu may typic practic model light damp reson around i khz this disturb high suggest cell high sensit parasit element captur simpl model our preliminari rl yes figur passiv network realize output admitt circuit analysi much complex model extract physic circuit layout creat carver mead 's laboratori indic output imped passiv valu transamp bias current definit explan instabl await more care circuit model effort perhap design on-chip imped measur instrument iii positive-r function e-positlv function stabil linear network model follow discuss cr+jw complex variabl ration function ratio polynomi real coeffici assum simplic pure imaginari pole term close right hale plane refer set complex number re def i function said positive-r pole right half plane re h jw know outset right half plane pole def i reduc simpl graphic criterion positiver nyquist diagram plot h jw lie entir close right half plane note positive-r function necessarili stabl sinc right half plane pole stabl function necessarili positive-r exampl show deep link posit real function physic network passiv establish classic result linear circuit theori state positive-r possibl synthes 2-termin network posit linear resistor capacitor inductor ideal transform driving-point imped admitt oef function said a-posit particular valu e e pole right half plane nyquist plot lie strict right straight line pass origin angl real posit axi note everi a-posit function stabl function e-posit necessarili positive-r i re g jw figur nyquist diagram function a-posit positive-r exampl function a-posit stabl but positive-r sinc nyquist diagram shown cross left half plane import e-posit function lie follow observ interconnect passiv linear resistor capacitor cell stabl linear imped result unstabl network instabl result imped also positive-r a-posit imped form larger class positive-r one henc a-posit less demand synthesi goal theorem show instabl result imped a-posit even positive-r theorem consid linear network arbitrari topolog consist number passiv 2-termin resistor capacitor arbitrari valu driven number activ cell output imped activ cell a-posit common network stabl proof theorem reli lemma lemma a-posit fix close first quadrant complex plane h lie strict right straight line pass origin angl real posit axi re im proof lemma outlin let function assign close right half plane perpendicular distanc des line defin def note des harmon close right half plane sinc analyt follow applic maximum modulus principle8 harmon function take minimum valu boundari domain imaginari axi this establish lemma proof theorem outlin network unstabl margin stabl natur frequenc close right half plane so natur frequenc network equat nonzero solut so let denot complex branch current solut tellegen i theorern9 sum complex power absorb circuit element must vanish solut iik12/s0ck capac~t cell termin pair second term delet special case so=o sinc complex power capacitor vanish so=o if network natur frequenc close right half plane must one close first quadrant sinc natur frequenc either real els occur complex conjug pair but satisfi so close first quadrant we see divid side iiki2 sum taken network branch after this divis assert zero convex combin term form rk term form ckso -i term form zk so visual term lie complex plane first set lie real posit axi second set lie close adrant sinc so lie close 1st quadrant assumpt third set lie right a line pass origin angl a lemma thus term lie strict right this line impli convex combin equal zero henc network stabl iv stabil result network nonlinear resistor capacitor previous result linear network afford limit insight behavior nonlinear network first nonlinear equat linear equilibrium point theorem appli linear model if linear model stabl equilibrium point origin nonlinear network local stabl network return equilibrium point if initi condit suffici near it but result this section contrast appli full nonlinear circuit model allow one conclud certain circumst network oscil even if the initi state arbitrarili far the equilibrium point def a function describ in section iii said tc satisfi the popov criterion lo if exist a real number r o such re l+jwr note posit real function satisfi the popov criterion the reader easili verifi in exam~l i satisfi the popov criterion a rang valu the import effect the term l+jwr in def rotat the nyquist plot counterclockwis progress greater amount increas theorem consid a network consist nonlinear 2-termin resistor capacitor cell linear output imped suppos the resistor curv character continu diffefenti function gk vk gk o gk vk valu of vk ii the capacitor character by ck vk ~k with ci ck v c2 valu of vk iii the imped zk pole in the close right half plane satisfi the popov criterion common valu of if condit satisfi the network stabl in the sens that for ani initi condit oo i branch dt the proof base tellegen 's theorem is rather involv omit and appear elsewher it acknowledg we sincer thank professor carver mead of cal tech for enthusiast support this work and for make it possibl for us present earli report it in this confer proceed this work supportedj defens advanc research project agenc the offic of naval research arpa order no contract no and defens advanc research project agenc darpa contract no
----------------------------------------------------------------

title: 32-synchronization-in-neural-nets.pdf

synchron neural net jacqu j. vidal univers california los angel los angel ca john haggerti abstract paper present artifici neural network concept synchroniz oscil network instant individu fire form point process constitut form inform transmit join neuron type communic contrast assum model typic continu discret value-pass network limit messag receiv process unit time marker signal fire unit present signific implemen tation advantag model neuron fire spontan regular absenc perturb interact present schedul fire advanc delay fire neighbor neuron network neuron becom global oscil exhibit multipl synchron attractor arbitrari initi state energi minim learn procedur make network converg oscillatori mode satisfi multi-dimension constraint network direct repres rout schedul problem consist order sequenc event introduct most neural network model deriv variant rosenblatt origin perceptron value-pass network case particular network propos fukushima i hopfield rumelhart mani other everi case input process element either binari continu amplitud signal weight synapt gain subsequ sum integr result activ pass sigmoid threshold filter produc continu quantiz output may becom input neuron behavior model relat live neuron even fall consider short account complex inde observ mani real neuron action potenti spike fire propag axon branch intern activ reach threshold higher john haggerti interact system los angel w. 6th st. la ca american institut physic input rate level result rapid fire behind tradit model assumpt averag frequenc action potenti carrier inform neuron becaus integr fire individu neuron consid effect extent contribut averag intens therefor assum activ simpli frequenc code exact time individu fire ignor view howev cover well known aspect neural communic inde precis time spike arriv make crucial differ outcom neural interact one classic exampl pre-synapt inhibit widespread mechan brain machineri sever studi also demonstr occurr function import precis time phase relationship cooper neuron local network model present paper contrast one mention network fire consid individu output event input side node fire node presynapt neuron either delay inhibit advanc excit node fire seen earlier type neuron interact would call phase-modul engin system also find rational experiment neurophysiolog neurophysiolog plausibl howev major concern rather propos explor potenti use mechan parallel distribut comput merit approach artifici neural network digit puls use internod communic instead analog voltag model particular well suit time-ord sequenc found larg class rout trajectori control problem neuron synchroniz oscil model process element neuron relax oscil built-in self-inhibit relax oscil dynam system capabl accumul potenti energi threshold breakdown point reach point energi abrupt releas new cycl begin descript fit dynam behavior neuron membran rich structur empir model this behavior found well-establish differenti formul hodgkin huxley simplifi version given fitzhugh7 differenti equat account foundat neuron activ also capabl repres subthreshold behavior refractori follow fire membran potenti enter critic region abrupt depolar collaps potenti differ across membran occur follow somewhat slower recoveri this brief electr short membran call action potenti spike constitut output event neuron if caus initi depolar maintain oscil limit-cycl develop generat multipl fire depend input level membran paramet oscil limit singl spike may produc oscillatori burst even continu sustain activ present model share general properti use much simpler descript relax oscil illustr figur activ energy exdt3toij oj input injrjh1~olj input perturb utl intemilf l neju inpul ty figur relax oscil perturb input fire occur energi level reach critic level ec assum constant rate energi influx fire occur natur period ec t=a when pre-synapt puls imping on cours energi accumul fire schedul disturb let repres instant last fire cell tj intant imping arriv cell e act wj uo til ec uo repres unit impuls dramat complex synchron dynam appreci consid simplest possibl case master slave interact two regular fire oscil unit natur period ta tb instant fire unit unidirect send spike signal unit receiv interv measur from last time fire upon recept spike transform quantum energi 6e depend upon post-fir arriv time relationship shape repres refractori post-spik properti here it assum simpl ramp function if interact inhibitori consequ this arriv next fire unit delay respect schedul would absenc perturb posit interv figur becaus shape delay action nil immedi fire becom longer imping pre-synapt spike arriv later interv if interact excitatori delay negat le shorten natur fire interv general assumpt regard function tend synchron within given rang coupl gain phase self-adjust equilibrium achiev given this equilibrium correspond distribut maximum entropi point cell receiv amouint activ common cycl i i inhibit excit figur relationship phase delay when input effici increas linear after-spik interv synchron dynam present attractor ration frequenc pair ratio associ rang stabil ratio lowest cardin wide zone phaselock figur wider stabil wnes correspond one one ratio fa fb invers ta tbl kohn segundo demonstr such phase lock occur live invertebr neuron point paradox natur phase-lock inhibit within stabil region take appear excit sinc small increas input fire rate local result increas output rate area these rang stabil appear unstabl transit fact as recent point out bak9 form infin lock step known as devil 's staircas correspond infin intermedi ration pair figur bak show staircas self-similar scale transit form fractal cantor set fractal dimens univers constant dynam system i excit inhibiti~~v i li l figur unilater synchron constraint satisfact in oscil network global synchron interconnect network mutual phase-lock oscil constraint satisfact problem synchron equilibrium node fire in interlock pattern organ inter-spik interv integ ratio often cite travel salesman problem archetyp for a class import hard problem a special case when ratio must node must fire at frequenc here the equilibrium condit everi node accumul the the amount energi the global cycl furthermor the fire must order along a minim path use stochast energi minim simul anneal the first simul demonstr the feasibl the approach with a limit number node the tsp isomorph mani sequenc problem involv distribut constraint fall the oscil array neural net paradigm in a particular natur way work pursu rigor establish the limit applic the model i annea/./ng figur the travel salesman problem in the global oscil oj minim energi node constrain fire at the rate in the order correspond to the minim path acknowledg research support in part aerojet electro-system under the aerojet-ucla cooper research master agreement no nasa nag
----------------------------------------------------------------

title: 49-connecting-to-the-past.pdf

connect past bruce a. macdonald assist professor knowledg scienc laboratori comput scienc depart univers calgari univers drive nw calgari alberta t2n in4 abstract recent renew interest neural-lik process system evidenc exampl two volum parallel distribut process edit rumelhart mcclelland discuss parallel distribut system connectionist model neural net valu pass system multipl context system dissatisfact symbol manipul paradigm artifici intellig seem part respons attent encourag promis massiv parallel system implement hardwar paper relat simpl neural-lik system base multipl context well-known formalisms-nam product system k-iength sequenc predict finite-st machin ture machines-and present earlier sequenc predict result new light introduct reviv neural net research strong exemplifi recent rumelhart mcclelland new journal number meet net also describ parallel distribut system connectionist model valu pass systems3 multipl context learn systems4 symbol manipul paradigm artifici intellig seem success hope seem last real promis massiv parallel system implement hardwar howev flurri new work import consolid new idea place solid alongsid establish one paper relat simpl neural-lik system well-known notions-nam product system k-iength sequenc predict finite-st machin ture machines-and present earlier result abil network new light general form connectionist system lo simplifi three layer net binari fix weight hidden layer therebi avoid mani difficulties-and challengesof recent work neural net hidden unit weight regular pattern use templat sophist expens learn algorithm avoid simpl method use determin output unit weight way gain advantag multilay net retain simplic two layer net train method certain noth lost comput power-a i explain-and limit two layer net carri simplifi three layer one biolog system may similar avoid need learn algorithm simul anneal method common use connectionist model one thing biolog system clear distinguish train phase briefli simplifi net product system implement three layer neuron-lik unit output layer input layer hidden layer product hidden product unit potenti connect predetermin set input output k-iength sequenc predictor form ie level delay unit introduc input layer k-iength predictor unabl distinguish simpl sequenc ba aa sinc ie charact system forgotten whether appear first k-iength predictor augment auxiliari action abl learn regular languag sinc auxiliari action equival state input aamong 1st intern confer neural net san diego ca june con.fer brough equival singl context system andrea 's multipl context system see also macdonald american institut physic figur general form connectionist system form unit oper within unit in~ut excitation-.i weight sum activ w output typic typic product unit enabl predict depend previous state combin sever augment sequenc predictor thring machin tape simul along finite-st control give net comput power univers ture machin relat simpl neural-lik system lack comput abil previous implement abil product system equival simplifi net organ paper next section briefli review general form connectionist system section simplifi section explain result equival product system deal input output net section extend simplifi version enabl learn predict sequenc section explain comput power sequenc predictor increas thring machin input unit receiv auxiliari action fact system learn ture machin section discuss possibl number net combin output form overal net associ area general form connectionist system figur show general form connectionist system unit neuron ce1l figur unit input output oj possibl unit network output net input excit net weight sum input vij weight connect output unit input unit activ unit function fi net input excit typic fi semilinear non-decreas differenti function least larg group unit output function fi activ typic kind threshold function i assum quantiti vari discret time step exampl activ time given fi neti general restrict connect may made unit unit connect direct input output hidden unit complex net describ paper may one type connect figur show common connect topolog three layer units-input hidden output-with cycl connect net train present input combin along desir output combin train system produc desir output given figur basic structur three layer connectionist system input unit hidden unit output unit input dure train weight adjust fashion reduc discrep desir actual output general method lo desir train activ equat general form hebb 's classic rule adjust weight two unit high activ lo weight adjust product two function one depend desir actual activ often difference-and anoth depend input weight weight simpl exampl suppos differ output oj weight chang product output error input excit weight constant determin learn rate widrow-hoff delta rule may use net without hidden unit 1o import contribut recent work connectionist system implement equat hidden unit train signal ti direct avail boltzmann learn method iter vari weight hidden unit train activ use control gradual decreas random method simul anneal backpropag also iter perform gradient descent propag train signal error back net hidden unit i avoid need determin train signal hidden unit fix weight hidden unit section simplifi system assum simplif made general connectionist system section system three layer topolog shown figur ie cycl hidden layer unit weight fix say uniti zero unit linear threshold unit lo mean activ function unit ident function give net weight sum input output function simpl binari threshold form i output threshold activ output binari oft hidden unit threshold requir input activ output activ like gate output unit threshold requir two activ high weight input output generat like gate keep product system view net explain section learning-which occur output unit weights-giv weight adjust accord wij wij oj otherwis weight turn input unit output otherwis wij oj simpl exampl given figur section simpl form net made probabilist replac adjust weight wij estim condit probabl unit output output wij estim p odoj assum independ input unit output unit turn condit probabl occurr output exceed threshold output function onc simplif made need learn hidden unit also iter learn requir weight either assign binari valu estim condit probabl paper present characterist simplifi net section discuss motiv simplifi neural net way product system simplifi net kind simpl product system product system compris global databas set product rule control system databas net system interact provid input reaction output t.he net hidden unit network product rule form precondit action precondit satisfi input excit exceed threshold hidden unit action repres output unit hidden product unit activ control system product system choos rule whose action perform set rule whose precondit met neural net control system distribut throughout net output unit exampl output unit might form winner-takeal net product system complex control involv forward backward chain choos action seek goal discuss elsewhere4.12.16 figur illust.r simpl product implement neural net figur show input hidden unit element precondit appropri input combin present associ hidden product unit fire onc weight leam connect hidden unit output unit fire product result output simplifi neural net direct equival product system whose element input output product system symbol element variabl given valu product action neural net direct implement sinc output predetermin set howev see later extens t.o framework enabl abil cthis might refer sensory-motor product system sinc implement ill l'eal system robot deal sens input execut motor action may includ auxiliari action section figur product implement simplifi neural net product rule icloudi i ipressur fall i then iit rain i rule implement hidden unit threshold hidden unit gate threshold output unit gate learn weight if net probabilist otherwis estim p rainlcloud pressur fall rain weight figur net predict next charact sequenc base last charact net product unit hidden unit combin input unit exampl net could predict sequenc abcabcabc product form if last charact then next charact learn rule wij if inputj outputi output wijoj input neural net output learn procedur clamp input output desir valu system calcul weight valu repeat requir input/output combin sequenc predict product system neural net predict sequenc given exampl repeat sequenc product learn predict futur event basi recent one figur show trivial simpl sequenc predictor predict next charact sequenc base previous one figur also give detail learn procedur simplifi net net need train onc input combin then predict output everi charact seen current one probabilist form net would estim condit probabl next charact condit current one mani figur use delay input neural net implement k-length sequenc predictor net last three charact input input hidden output 2nd last exampl product if last three charact then present possibl charact pair would need proper estim probabl net would learn probabl distribut charact pair predictor like one figur extend general k-iength predictor long input delay step avail then illustr figur 3-length predict hidden product unit repres possibl combin symbol again output weight train respond previous seen input combin three charact delay provid dedic neural net shown figur note net assum synchron updat input feedback around unit chang one step output chang various way implement delay neuron andrea investig purpose-delay inputs-in detail simul similar net work sequenc predict neural net feldman ballard find connectionist system initi suit repres chang time one form chang sequenc suggest two method repres sequenc net first unit connect each sequenc sequenti task repres fire these unit success second method buffer input time input recent past avail well current input delay input avail suggest import differ necessari length buffer feldman ballard suggest buffer long enough hold phrase natur languag i expect use buffer longer andrea symbol input repres complex inform effect give length seven buffer inform recent seven simpl input discuss section method back-propagation13 enabl recurr network learn sequenti task dfeldman ballard2 give dedic neural net connect varieti flulction figur input delay dedic neural subnet two stage delay shown delay network time diagram tml origin signal delay one step delay two step manner similar first suggest last paragraph sequenc connect unit repres sequenc event one exampl net learn complet sequenc charact given first two charact six charact sequenc next four output error must propag around cycl recurr net number time serial may also achiev sequenc state distribut activ exampl net play side tic-tac-to game sequenti natur net 's behavior deriv sequenti natur respons net 's action tic-tac-to move net model sequenc intern model sequenti part environ exampl tic-tac-to play net model oppon k-iength sequenc predictor unabl learn sequenc repeat frequent everi charact their k-iength context includ inform last event howev there two way inform kth last input retain net first method latch input second involv auxiliari action latch unit input latch held indefinit use combin shown figur input would normal latch andrea discuss this techniqu thread latch event among non-latch event give net inform arbitrarili far back input-output histori inform immedi past briefli sequenc ba distinguish aa if first charact latch howev this ad hoc solut this problem auxiliari action output fed back net input signal this enabl system choos next output least part base previous one indic figur if particular fed back output also one without extern manifest whose extern manifest independ task perform then output auxiliari action las interest reader refer andrea extens analysi given figur thread latch circuit rememb event anoth come along this two input latch two letter number unit may similar connect form mutual inhibit layer winner-take-al connect along posit feedback keep select output activ when input disappear figur auxiliari actions-th outputs-ar fed back input net enabl net rememb state here part net exampl product shown there two type action charact action sinput output charact input if input charact input charact output then output charact llij ill direct effect task system perform sinc it evok relev input use net symbol action if auxiliari action latch input then symbol inform rememb indefinit lost when anoth auxiliari action kind input take latch thus auxiliari action act like rememb state system perform action remind particular state figur illustr this system that predict charact state chang given previous charact state an obvious candid auxiliari action speech blank oval figur would repres net 's environ speech action heard although it extern manifest speech direct effect physic interact the world symbol abil provid the power auxiliari action also includ other speaker the interact simul abstract automata the exampl figur give the essenc simul finit state automaton product system or it neural net equival it illustr the transit function an automaton the new state output function the previous state input thus neural net simul finit state automaton long it addit auxiliari action thring machin finit state automaton control plus an unbound memori neural net could simul a lure machin two way way demonstr product system implementations-equival neural net all multipl context learn system briefli explain section the first thring machin simul the system simul the finit state control abl use an unbound extern memori fsee john andrea 's colleagu work4 figur multipl context learn system implement multipl neural net each:3 layer net the simplifi form present a number elabor extra connect goal-seek forward backward chain output channel the real world much like the paper ture 's origin work the second simnlat.ion emb the memori the multipl context learn system along a counter access this simul memori both learn the productions-equival learn output unit weights-requir the simul the second abl add intern memori requir a limit depend the size the network easili larg enough allow year comput the second could also employ extern memori the first briefli the second simul compris multipl sequenc predictor predict auxiliari action rememb the state the control the current memori posit the memori element updat relearn the product repres that element the precondit the address the product action the store item multipl system form associ area a multipl context learn system product system version a multipl neural net although a simpl version implement a simul net it effect compris sever net or associ areas-which may output input in common indic in figur hidden unit weight specifi templat one each net a templat give the input a zero weight the hidden unit a net the input a weight uniti delay latch input also avail the actual output select the combin predict the net in a winner-take-al fashion i see the design real neural net say control real robot requir a larg degre predetermin connect a robot control could one three layer net wit.h everi input connect everi hidden unit in turn connect everi output there need some connect constraint so the net reflect the function special in the control requir the multipl context learn system all the hidden layer connect predetermin allow output connect learn this avoid the credit assign problem therefor also the need learn algorithm boltzmann learn back-propag howev as the multipl context learn system auxiliari action delay latch input it lack comput power futur work in this area investig for exampl the abil differ kind net learn auxiliari act.ion this may difficult as symbol action may provid in train input and output. for exampl a control for a robot bodi would deal vision manipul motion etc conclus this paper present a simplifi three layer connectionist model fix weight for hidden unit delay and latch for input sequenc predict abil auxiliari state action and the abil use intern and extern memori the result abl learn to simul a ture machin simpl neural-lik system not lack comput power acknowledg this work support by the natur scienc and engin council canada
----------------------------------------------------------------

title: 55-mathematical-analysis-of-learning-behavior-of-neuronal-models.pdf

mathemat analysi learn behavior neuron model john y. cheung massoud omidvar school electr engin comput scienc univers oklahoma norman ok present ieee confer neural inform process systemsnatur synthet denver novemb publish collect paper ieee confer nip pleas address correspond john y. cheung school eec w. boyd cec norman ok novemb american institut physic mathemat analysi learn behavior neuron model john y. cheung massoud omidvar school electr engin comput scienc abstract paper wish analyz converg behavior number neuron plastic model recent neurophysiolog research suggest neuron behavior adapt particular memori store within neuron associ synapt weight vari adjust achiev learn number adapt neuron model propos literatur three specif model analyz paper specif hebb model sutton-barto model recent trace model paper examin condit converg posit converg rate converg model appli classic condit simul result also present verifi analysi introduct number static model describ behavior neuron use past decad recent research neurophysiolog suggest static view may insuffici rather paramet within neuron tend vari past histori achiev learn it suggest alter intern paramet neuron may adapt repetit input stimuli becom condit learn thus occur neuron condit describ behavior neuron plastic number model propos earliest one may postul hebb more recent sutton barto also introduc new model recent trace mrt model paper primari object paper howev analyz converg behavior model adapt general neuron model use paper shown figur number neuron input n. each input scale correspond synapt weight n. weight input arithmet sum taken zero neuron input assum take numer valu rang zero one inclus synapt weight allow take reason valu purpos paper though realiti weight may well bound sinc relat magnitud weight neuron input well defin point put bound magnitud weight also neuron output normal result sigmoid transform simplic approxim oper linear transform sigmodi transfonut neuron output rilur lener aeuron adel converg analysi assum two neuron input tradit classic condit environ simplic cours analysi techniqu extend number input classic condit two input condit stimulus xc uncondit stimulus sutton-barto model more recent sutton barto propos adapt model base signal trace output trace given xi axi xi posit constant condit converg order simplifi analysi choos word becom wi wi cxi assumpt serv simplifi analysi affect converg condit bounded depend xi respect previous section recogn recurr relat converg check ratio test it also possibl rewrit matrix format due recurs neuron output equat includ neuron output paramet vector also show converg need set magnitud determin less uniti henc condit converg see adapt constant must chosen less reciproc euclidean sum energi input techniqu extend number input prove mere follow procedur outlin posit converg have prove converg sutton-barto model equat neuron plastic want find next locat system remain converg seen earlier converg weight ceas chang neuron output denot converg posit word sinc arbitrari paramet vector alway decompos weight sum eigenvector constant ql q3 easili found invert eigenvalu shown within region converg magnitud third eigenvalu less uniti mean converg there contribut third eigenvector henc predict precis converg posit would given initi condit rate converg seen care chosen sutton-barto model converg also deriv express converg posit next want find fast converg attain rate converg measur fast initi paramet approach optim posit asymptot rate converg sea spectral radius equal case complet converg analysi sutton-barto model neuron plastic mrt model neuron plastic recent trace mrt model neuron plastic develop author consid cross sutton-barto model klopf 's model adapt synapt weight express follow comparison sutton-barto model ahowl cond term right hand aid contain extra factor ii use ape converg ahowd later output trace hu replac recent output henc name recent trace model input trace also replac recent input condit converg proceed analyz condit converg mrt model due presenc wi factor second term ratio test appli analyz converg behavior let us rewrit matrix format wi superscript denot matrix transpos oper equat quadrat complet converg analysi equat extrem difficult order understand converg behavior note domin term determin converg main relat second quadrat term henc converg analysi we ignor first term we readili see primari converg factor bt sinc depend converg obtain if durat synapt input activ bound it shown condit converg bound we readili see adapt constant chosen accord ensur converg t. simul verifi theoret analysi three adapt neuron model base classic condit model simul mm mainfram use fortran languag singl precis sever test scenario design compar analyt predict actual simul result verifi condit converg we vari valu adapt constant condit uncondit stimuli set to uniti valu vari to sutton-barto model simul given show converg obtain expect theoret analysi mrt model simul result given show converg obtain also expect theoret analysi theoret locat converg sutton barto model also shown in figur it readili seen simul result confirm theoret expect i output figur lou murod l tpuu yet.us ule er suttoa-barto el witb 1frerent alu aptat1on codstant lleuroul output i i i ju.ber iteratioga figur plotl oeuroaal outputl craus uuaber iterati mat el differ alu adantatlon i ddstaut to illustr rate converg we plot trajectori deviat in synapt weight from optim valu in logarithm scale sinc error logarithm as found earlier slope line yield rate converg trajectori sutton-barto model given in figur mrt model given in figur it clear from figur trajectori in the logarithm form a straight line the slope readili calcul the curv the mrt model given in figur also a straight line a much larger slope show faster converg summari in paper we sought to discov analyt the converg behavior three adapt neuron model from the analysi we see the hebb model converg constant activ input the output grow exponenti in spite lack converg the hebb model still a workabl model realiz the diverg behavior would curtail by the sigmoid transform to yield realist output the i uroul output dniatiotl lto i i u.ber iterationa figur trajectori of deuron output deviationa froa atat alu the sutton- rt el lfferent valu adapt coiistallt c. lleurod l output deviat ltl i nuaber of iter figur trajectori of neuron output deviat fra atat valu for tbe krt el witb differ valu of adapt constant analysi the sutton barto model show model converg when the adapt constant care chosen the bound for also found for this model due to the structur of this model the locat at converg the rate of converg also found we also introduc a new model of neuron plastic call the recent trace mrt model certain similar exist the mrt model the sutton-barto model also the mrt model and the klopf model analysi show the updat equat for the synapt weight quadrat result in polynomi rate of converg simul result also show that much faster converg rate obtain with the mrt model
----------------------------------------------------------------

title: 55-mathematical-analysis-of-learning-behavior-of-neuronal-models.pdf

mathemat analysi learn behavior neuron model john y. cheung massoud omidvar school electr engin comput scienc univers oklahoma norman ok present ieee confer neural inform process systemsnatur synthet denver novemb publish collect paper ieee confer nip pleas address correspond john y. cheung school eec w. boyd cec norman ok novemb american institut physic mathemat analysi learn behavior neuron model john y. cheung massoud omidvar school electr engin comput scienc abstract paper wish analyz converg behavior number neuron plastic model recent neurophysiolog research suggest neuron behavior adapt particular memori store within neuron associ synapt weight vari adjust achiev learn number adapt neuron model propos literatur three specif model analyz paper specif hebb model sutton-barto model recent trace model paper examin condit converg posit converg rate converg model appli classic condit simul result also present verifi analysi introduct number static model describ behavior neuron use past decad recent research neurophysiolog suggest static view may insuffici rather paramet within neuron tend vari past histori achiev learn it suggest alter intern paramet neuron may adapt repetit input stimuli becom condit learn thus occur neuron condit describ behavior neuron plastic number model propos earliest one may postul hebb more recent sutton barto also introduc new model recent trace mrt model paper primari object paper howev analyz converg behavior model adapt general neuron model use paper shown figur number neuron input n. each input scale correspond synapt weight n. weight input arithmet sum taken zero neuron input assum take numer valu rang zero one inclus synapt weight allow take reason valu purpos paper though realiti weight may well bound sinc relat magnitud weight neuron input well defin point put bound magnitud weight also neuron output normal result sigmoid transform simplic approxim oper linear transform sigmodi transfonut neuron output rilur lener aeuron adel converg analysi assum two neuron input tradit classic condit environ simplic cours analysi techniqu extend number input classic condit two input condit stimulus xc uncondit stimulus sutton-barto model more recent sutton barto propos adapt model base signal trace output trace given xi axi xi posit constant condit converg order simplifi analysi choos word becom wi wi cxi assumpt serv simplifi analysi affect converg condit bounded depend xi respect previous section recogn recurr relat converg check ratio test it also possibl rewrit matrix format due recurs neuron output equat includ neuron output paramet vector also show converg need set magnitud determin less uniti henc condit converg see adapt constant must chosen less reciproc euclidean sum energi input techniqu extend number input prove mere follow procedur outlin posit converg have prove converg sutton-barto model equat neuron plastic want find next locat system remain converg seen earlier converg weight ceas chang neuron output denot converg posit word sinc arbitrari paramet vector alway decompos weight sum eigenvector constant ql q3 easili found invert eigenvalu shown within region converg magnitud third eigenvalu less uniti mean converg there contribut third eigenvector henc predict precis converg posit would given initi condit rate converg seen care chosen sutton-barto model converg also deriv express converg posit next want find fast converg attain rate converg measur fast initi paramet approach optim posit asymptot rate converg sea spectral radius equal case complet converg analysi sutton-barto model neuron plastic mrt model neuron plastic recent trace mrt model neuron plastic develop author consid cross sutton-barto model klopf 's model adapt synapt weight express follow comparison sutton-barto model ahowl cond term right hand aid contain extra factor ii use ape converg ahowd later output trace hu replac recent output henc name recent trace model input trace also replac recent input condit converg proceed analyz condit converg mrt model due presenc wi factor second term ratio test appli analyz converg behavior let us rewrit matrix format wi superscript denot matrix transpos oper equat quadrat complet converg analysi equat extrem difficult order understand converg behavior note domin term determin converg main relat second quadrat term henc converg analysi we ignor first term we readili see primari converg factor bt sinc depend converg obtain if durat synapt input activ bound it shown condit converg bound we readili see adapt constant chosen accord ensur converg t. simul verifi theoret analysi three adapt neuron model base classic condit model simul mm mainfram use fortran languag singl precis sever test scenario design compar analyt predict actual simul result verifi condit converg we vari valu adapt constant condit uncondit stimuli set to uniti valu vari to sutton-barto model simul given show converg obtain expect theoret analysi mrt model simul result given show converg obtain also expect theoret analysi theoret locat converg sutton barto model also shown in figur it readili seen simul result confirm theoret expect i output figur lou murod l tpuu yet.us ule er suttoa-barto el witb 1frerent alu aptat1on codstant lleuroul output i i i ju.ber iteratioga figur plotl oeuroaal outputl craus uuaber iterati mat el differ alu adantatlon i ddstaut to illustr rate converg we plot trajectori deviat in synapt weight from optim valu in logarithm scale sinc error logarithm as found earlier slope line yield rate converg trajectori sutton-barto model given in figur mrt model given in figur it clear from figur trajectori in the logarithm form a straight line the slope readili calcul the curv the mrt model given in figur also a straight line a much larger slope show faster converg summari in paper we sought to discov analyt the converg behavior three adapt neuron model from the analysi we see the hebb model converg constant activ input the output grow exponenti in spite lack converg the hebb model still a workabl model realiz the diverg behavior would curtail by the sigmoid transform to yield realist output the i uroul output dniatiotl lto i i u.ber iterationa figur trajectori of deuron output deviationa froa atat alu the sutton- rt el lfferent valu adapt coiistallt c. lleurod l output deviat ltl i nuaber of iter figur trajectori of neuron output deviat fra atat valu for tbe krt el witb differ valu of adapt constant analysi the sutton barto model show model converg when the adapt constant care chosen the bound for also found for this model due to the structur of this model the locat at converg the rate of converg also found we also introduc a new model of neuron plastic call the recent trace mrt model certain similar exist the mrt model the sutton-barto model also the mrt model and the klopf model analysi show the updat equat for the synapt weight quadrat result in polynomi rate of converg simul result also show that much faster converg rate obtain with the mrt model
----------------------------------------------------------------

title: 32-synchronization-in-neural-nets.pdf

synchron neural net jacqu j. vidal univers california los angel los angel ca john haggerti abstract paper present artifici neural network concept synchroniz oscil network instant individu fire form point process constitut form inform transmit join neuron type communic contrast assum model typic continu discret value-pass network limit messag receiv process unit time marker signal fire unit present signific implemen tation advantag model neuron fire spontan regular absenc perturb interact present schedul fire advanc delay fire neighbor neuron network neuron becom global oscil exhibit multipl synchron attractor arbitrari initi state energi minim learn procedur make network converg oscillatori mode satisfi multi-dimension constraint network direct repres rout schedul problem consist order sequenc event introduct most neural network model deriv variant rosenblatt origin perceptron value-pass network case particular network propos fukushima i hopfield rumelhart mani other everi case input process element either binari continu amplitud signal weight synapt gain subsequ sum integr result activ pass sigmoid threshold filter produc continu quantiz output may becom input neuron behavior model relat live neuron even fall consider short account complex inde observ mani real neuron action potenti spike fire propag axon branch intern activ reach threshold higher john haggerti interact system los angel w. 6th st. la ca american institut physic input rate level result rapid fire behind tradit model assumpt averag frequenc action potenti carrier inform neuron becaus integr fire individu neuron consid effect extent contribut averag intens therefor assum activ simpli frequenc code exact time individu fire ignor view howev cover well known aspect neural communic inde precis time spike arriv make crucial differ outcom neural interact one classic exampl pre-synapt inhibit widespread mechan brain machineri sever studi also demonstr occurr function import precis time phase relationship cooper neuron local network model present paper contrast one mention network fire consid individu output event input side node fire node presynapt neuron either delay inhibit advanc excit node fire seen earlier type neuron interact would call phase-modul engin system also find rational experiment neurophysiolog neurophysiolog plausibl howev major concern rather propos explor potenti use mechan parallel distribut comput merit approach artifici neural network digit puls use internod communic instead analog voltag model particular well suit time-ord sequenc found larg class rout trajectori control problem neuron synchroniz oscil model process element neuron relax oscil built-in self-inhibit relax oscil dynam system capabl accumul potenti energi threshold breakdown point reach point energi abrupt releas new cycl begin descript fit dynam behavior neuron membran rich structur empir model this behavior found well-establish differenti formul hodgkin huxley simplifi version given fitzhugh7 differenti equat account foundat neuron activ also capabl repres subthreshold behavior refractori follow fire membran potenti enter critic region abrupt depolar collaps potenti differ across membran occur follow somewhat slower recoveri this brief electr short membran call action potenti spike constitut output event neuron if caus initi depolar maintain oscil limit-cycl develop generat multipl fire depend input level membran paramet oscil limit singl spike may produc oscillatori burst even continu sustain activ present model share general properti use much simpler descript relax oscil illustr figur activ energy exdt3toij oj input injrjh1~olj input perturb utl intemilf l neju inpul ty figur relax oscil perturb input fire occur energi level reach critic level ec assum constant rate energi influx fire occur natur period ec t=a when pre-synapt puls imping on cours energi accumul fire schedul disturb let repres instant last fire cell tj intant imping arriv cell e act wj uo til ec uo repres unit impuls dramat complex synchron dynam appreci consid simplest possibl case master slave interact two regular fire oscil unit natur period ta tb instant fire unit unidirect send spike signal unit receiv interv measur from last time fire upon recept spike transform quantum energi 6e depend upon post-fir arriv time relationship shape repres refractori post-spik properti here it assum simpl ramp function if interact inhibitori consequ this arriv next fire unit delay respect schedul would absenc perturb posit interv figur becaus shape delay action nil immedi fire becom longer imping pre-synapt spike arriv later interv if interact excitatori delay negat le shorten natur fire interv general assumpt regard function tend synchron within given rang coupl gain phase self-adjust equilibrium achiev given this equilibrium correspond distribut maximum entropi point cell receiv amouint activ common cycl i i inhibit excit figur relationship phase delay when input effici increas linear after-spik interv synchron dynam present attractor ration frequenc pair ratio associ rang stabil ratio lowest cardin wide zone phaselock figur wider stabil wnes correspond one one ratio fa fb invers ta tbl kohn segundo demonstr such phase lock occur live invertebr neuron point paradox natur phase-lock inhibit within stabil region take appear excit sinc small increas input fire rate local result increas output rate area these rang stabil appear unstabl transit fact as recent point out bak9 form infin lock step known as devil 's staircas correspond infin intermedi ration pair figur bak show staircas self-similar scale transit form fractal cantor set fractal dimens univers constant dynam system i excit inhibiti~~v i li l figur unilater synchron constraint satisfact in oscil network global synchron interconnect network mutual phase-lock oscil constraint satisfact problem synchron equilibrium node fire in interlock pattern organ inter-spik interv integ ratio often cite travel salesman problem archetyp for a class import hard problem a special case when ratio must node must fire at frequenc here the equilibrium condit everi node accumul the the amount energi the global cycl furthermor the fire must order along a minim path use stochast energi minim simul anneal the first simul demonstr the feasibl the approach with a limit number node the tsp isomorph mani sequenc problem involv distribut constraint fall the oscil array neural net paradigm in a particular natur way work pursu rigor establish the limit applic the model i annea/./ng figur the travel salesman problem in the global oscil oj minim energi node constrain fire at the rate in the order correspond to the minim path acknowledg research support in part aerojet electro-system under the aerojet-ucla cooper research master agreement no nasa nag
----------------------------------------------------------------

title: 50-an-adaptive-and-heterodyne-filtering-procedure-for-the-imaging-of-moving-objects.pdf

adapt heterodyn filter procedur imag move object f. h. schule h. a. k. mastebroek w. h. zaagman biophys depart laboratori general physic westersingel em groningen netherland abstract recent experiment work stimulus veloc depend time resolv power neural unit situat highest order optic ganglion blowfli reveal first sight amaz phenomenon high level fli visual system time constant unit involv process neural activ evok move object rough spokeninvers proport veloc object extrem wide rang paper discuss implement two dimension heterodyn adapt filter construct comput simul model featur simul model includ abil account experiment observ stimulus-tun adapt tempor behaviour time constant fli visual system simul result obtain clear show applic adapt process procedur deliv improv imag techniqu move pattern high veloc rang few remark fli visual system visual system diptera includ blowfli calliphora erythrocephala regular organ allow therefor precis optic stimul techniqu also long term electrophysiolog record made relat easi visual system reason blowfli well-known rapid clever pilot turn extrem suitabl anim systemat studi basic principl may underli detect process movement inform neural level fli visual system input retin mosaic structur precis map onto higher order optic ganglia lamina medulla lobula mean neural column ganglion visual system correspond certain optic axi visual field compound eye lobula complex set wide-field movement sensit neuron found integr input signal whole visual field entir eye one wide field neuron classifi i hausen extens studi anatomically2 well electrophysiologically5 obtain result general agre well found behavior optomotor experi movement detect understood term reichardt correl model i neuron sensit horizont movement direct select high rate action potenti spike per second record element case visual stimuli move horizont inward back front visual field pre/er direct wherea movement horizont outward front back null direct suppress activ american institut physic experiment result model base i neuron stimul prefer direct step wise pattern displac respond increas neural activ repeat stimulus step one obtain averag respons ms latenc period respons manifest sharp increas averag fire rate follow much slower decay spontan activ level two exampl averag respons shown post stimulus time histogram psth 's figur time peak peak height relat depend modul depth stimulus step size spatial extent stimulus tail respons describ adequ exponenti decay toward constant spontan fire rate set stimulus paramet respons paramet defin equat estim least-squar fit tail psth smooth line figur result two fit tlmsl oj i'jo tf moo io mdl05 fig.l i lsi verag respons psth 's obtain i neuron adapt smooth stimulus motion veloc top bottom respect smooth line repres least-squar fit psth 's form valu two psth 's ms respect de ruyter van steveninck fit valu function adapt veloc three modul depth m. straight line least-squar fit repres data region form f=q ms de ruyter van steveninck fig.2 figur show fit valu respons time constant function angular veloc move stimulus squar wave grate experi present anim period long enough let visual system adapt move pattern step wise pattern displac reveal given straight line describ ms repres least-squar fit data veloc rang rang vari rough ms ms defin adapt rang interv veloc decreas increas veloc may conclud figur within adapt rang sensit modul depth outcom similar experi constant modul depth pattern constant pattern veloc four differ valu contrast frequenc fc number spatial period per second travers individu visual axi determin spatial wavelength pattern pattern veloc accord fc=v las reveal also almost complet independ behaviour contrast frequenc experi stimulus field subdivid region differ adapt veloc made clear time constant input channel i neuron set local valu stimulus veloc stimulus sub-region final found adapt driven stimulus veloc independ direct find summar qualit follow steadi state respons time constant neural unit highest level fli visual system found tune local within larg veloc rang exclus magnitud veloc move pattern direct despit direct select neuron go question amaz adapt mechan may hard-wir fli visual system instead make advantag result deriv thus far attempt fit experiment observ imag process approach larg number theori sever distinct class algorithm encod veloc direct movement visual system suggest exampl marr ullman i i van santen sperling12 hypothes adapt mechan set time constant lead optim overal perform visual system realiz veloc independ represent move object word within rang veloc time constant found tune veloc represent stimulus certain level within visual circuitri remain independ variat stimulus veloc object motion degrad model given physic descript motion linear space invari model motion degrad process repres follow convolut integr co co jj flu dudv object intens posit object coordin frame point spread function psf imag system respons unit puls imag intens spatial posit blur imag system possibl addit white nois degrad alreadi motion blur imag neglect present consider review principl techniqu field digit imag degrad restor reader refer harri sawchuk sondhi nahi boutalib hildebrand rajala de figueiredo20 demonstr first aboutalib situat motion blur occur straight line along one spatial coordin say along horizont axi it correct look blur imag collect degrad line scan entir imag depend vertic coordin may drop eq reduc f u du given mathemat descript relat movement correspond psf deriv exact equat becom b x f u du extent motion blur typic discret version applic digit imag process purpos describ i i take integ valu relat motion blur extent accord aboutalib scalar differ equat model deriv model motion degrad process cma i-m m-dimension state vector posit along scan line input intens posit output intens blur extent number element line scalar constant matric order mxl lxm respect contain discret valu cj blur psf kroneck delta function influenc time constant veloc amount motion blur artifici receptor array start incorpor simul model psf deriv equat model perform neural columnar arrang filter lobula complex restrict time constant remain fix throughout whole rang stimulus veloc realize psf easili achiev via mention state space model i. i. fig.3 posit artifici receptor array upper part demonstr effect increas magnitud time constant one-dimension array filter result increas motion blur pattern veloc remain constant origin pattern shown solid line square-wav grate spatial wavelength equal artifici receptor distanc three wave form drawn show gradual increas increas magnitud time constant represent origin square-wav consequ degrad lower part gradual increas veloc move square-wav filter time constant kept fix result also clear increas degrad first demonstr effect increas time constant pattern veloc remain result increas blur therefor introduc one dimension array filter equip time constant impuls respons origin pattern shown squar solid line upper part figur consist squar wave grate spatial period overlap artifici recept filter pattern drawn show constant veloc move grate increas magnitud time constant filter result increas blur represent grate hand increas veloc time constant artifici recept unit remain also result clear increas motion blur demonstr lower part figur inspect two wave form drawn mean dash line both upper lower half figur yield conclus apart round error introduc rather small number artifici filter avail equal amount smear produc when product time constant pattern veloc equal upper dash wave form veloc four time smaller time constant four time larger equival lower part figur adapt scheme design proper imag process procedur next step incorpor experiment observ flexibl properti time constant imag element devic figur 4a scheme shown filter inform fix time constant influenc pattern veloc figur 4b network shown time constant also remain fix matter pattern movement present next level inform process spatial differenti network incorpor order enhanc blur contrast filter network figur 4c first measur magnitud veloc move object done thus far hypothet introduc movement process algorithm model set recept element sampl environ manner proper estim local pattern veloc done time constant artifici recept element tune accord estim veloc final differenti network scheme 4b use actual tune mechan use simul outlin figur given rang veloc model suppos oper given lower limit time constant min min smallest valu physic realiz time constant tune new valu accord experiment observ reciproc relationship veloc within adapt rang larger fix minimum valu demonstr previous section correspond blur represent move stimulus thus alway larger situat filter done fix smallest time constant min import howev fact due tune mechan blur constant sinc product veloc time constant kept constant so inform process system veloc independ represent imag result serv input spatial differenti network outlin figur 4c elementari form differenti filter procedur one gradient two filter k-i k+l nearest neighbor filter taken then ad constant weigh factor central output drawn figur sign gradient depend direct estim movement essenti model claim weigh factor constant throughout whole set filter whole high veloc rang heterodyn imag perform import notic exist so-cal settl time minim time need movement process devic abl accur measur object veloc note time set equal zero case relat stimulus veloc known priori demonstr figur sinc without doubt within settl period estim veloc valu come erron thus optim perform imag devic expect exampl result initi settl procedur shown yv ryo i~j pattern movement figur right network consist set filter fix pattern veloc independ time constant impuls respons ident network figur 4a follow spatial differenti circuitri add weigh gradient two neighbor filter output k-l k+i central filter output k. time constant filter network tune hypothet movement estim mechan visual number recept element combin output tune filter detail descript mechan shown figur tune network follow ident spatial differenti circuit describ figur increas veloc decreas time constant min detail descript mechan use tune time constant time constant specif neural channel set pattern veloc accord relationship shown insert deriv eq i i 4r i i i i i i i i 4v i i 2v wi 8v i posit artifici receptor array fig.6 thick line square-wav stimulus pattern spatial wavelength overlap artifici recept element thick line respons differ pattern veloc system consist parallel neural filter equip time constant tune veloc follow spatial differenti network describ dash line respons differ pattern veloc filter system fix time constant follow spatial differenti circuitri note sharp shoot case result obtain imag procedur drawn figur 4c shown figur pattern consist squar wave overlap pictur element pattern move left differ veloc veloc one wavelength shown thick line squar wave pattern dash line output imag devic depict figur constant time constant constant weigh factor spatial process stage note larg differ sever output thin continu line output imag devic drawn figur tune time constant accord reciproc relationship pattern veloc time constant constant weigh factor spatial process stage simul detail reader refer zaagman output almost complet good agreement origin stimulus throughout whole veloc rang figur show effect gradient weigh factor overal filter perform estim improv deblur imag compar blur imag measur db quantit measur determin case move squar wave pattern motion blur ix iti weigh factor effect weigh factor overal filter perform curv measur case move square-wav grate filter perform estim improv signal nois ratio i ii origin intens posit imag intens posit motion blur imag intens imag generat adapt tune procedur extent compar use simul discuss section iv curv it appar situat optimum valu weigh factor keep weight close optimum valu result constant output adapt scheme thus enabl optim deblur smear imag move object hand start point view time constant remain fix throughout filter process tune gradient weight veloc order produc constant output demonstr figur dash line show strong differ output fix time constant system spatial process constant weight figur 4b other word tune time constant propos section result i realize blur-const criterion formul previous consequ possibl deblur obtain imag optim one weigh factor gradient final spatial process layer whole heterodyn veloc rang comput simul result conclus imag qualiti improv algorithm develop in present contribut implement general purpos dg eclips sjl40 minicomput two dimension simul figur sa show undisturb imag consist line pixel bit intens resolut figur sb show happen origin imag psf model accord exponenti decay in case time constant spatial inform process channel kept fix again inform content in higher spatial frequenc reduc larg implement heterodyn filter procedur now done follow first adapt rang defin set rang veloc mean adapt heterodyn algorithm suppos oper adequ within thus defin veloc rang in rang time constant tune accord relationship alway come larger minimum valu min demonstr purpos we set q=i in eq thus introduc phenomenon veloc two dimension set spatial filter time constant tune veloc alway produc a constant output independ veloc introduc motion blur figur sc show represent it import note constant output far wors qualiti ani set filter smallest fix time constant min would produc for veloc within the oper rang the advantag a veloc independ output this level in simul model in the next stage a differenti scheme implement discuss in detail in the preced paragraph constanc the weigh factor use in this differenti process scheme guarante the veloc independ the obtain imag represent figur sd show the result the differenti oper optim gradient weigh factor this weigh factor optim base on almost ident perform curv as describ previous in figur a clear good restor appar this figur though close inspect reveal fine structur especi for area high intens unrel the origin intens distribut these artifact caus the phenomenon for these high intens area possibl tune error show much more pronounc for low intens fig.8a fig.8b 8c fig.8d a origin bit pictur motion degrad imag a psf deriv from is kept fix pixel the motion blur extent is pixel worst case the result motion degrad the origin imag a psf as in figur 8b tune the time constant base on the veloc restor version of the degrad imag use the heterodyn adapt process scheme in conclus a heterodyn adapt imag process techniqu inspir the fli visual system present as imag devic for move object a scalar differ equat model use repres the motion blur degrad process base on the experiment result describ and on this state space model we develop an adapt filter scheme produc at a certain level within the system a constant output permit differenti oper in order produc an optim deblur represent of the move object acknowledg the author wish to thank mt eric bosman for expert program assist mr franco tommasi for mani inspir discuss and advis the implement of the simul model and dr rob de ruyter van steveninck for experiment help this research part support by the netherland organ lor the advanc pure research the foundat sticht voor biolysica
----------------------------------------------------------------

title: 32-synchronization-in-neural-nets.pdf

synchron neural net jacqu j. vidal univers california los angel los angel ca john haggerti abstract paper present artifici neural network concept synchroniz oscil network instant individu fire form point process constitut form inform transmit join neuron type communic contrast assum model typic continu discret value-pass network limit messag receiv process unit time marker signal fire unit present signific implemen tation advantag model neuron fire spontan regular absenc perturb interact present schedul fire advanc delay fire neighbor neuron network neuron becom global oscil exhibit multipl synchron attractor arbitrari initi state energi minim learn procedur make network converg oscillatori mode satisfi multi-dimension constraint network direct repres rout schedul problem consist order sequenc event introduct most neural network model deriv variant rosenblatt origin perceptron value-pass network case particular network propos fukushima i hopfield rumelhart mani other everi case input process element either binari continu amplitud signal weight synapt gain subsequ sum integr result activ pass sigmoid threshold filter produc continu quantiz output may becom input neuron behavior model relat live neuron even fall consider short account complex inde observ mani real neuron action potenti spike fire propag axon branch intern activ reach threshold higher john haggerti interact system los angel w. 6th st. la ca american institut physic input rate level result rapid fire behind tradit model assumpt averag frequenc action potenti carrier inform neuron becaus integr fire individu neuron consid effect extent contribut averag intens therefor assum activ simpli frequenc code exact time individu fire ignor view howev cover well known aspect neural communic inde precis time spike arriv make crucial differ outcom neural interact one classic exampl pre-synapt inhibit widespread mechan brain machineri sever studi also demonstr occurr function import precis time phase relationship cooper neuron local network model present paper contrast one mention network fire consid individu output event input side node fire node presynapt neuron either delay inhibit advanc excit node fire seen earlier type neuron interact would call phase-modul engin system also find rational experiment neurophysiolog neurophysiolog plausibl howev major concern rather propos explor potenti use mechan parallel distribut comput merit approach artifici neural network digit puls use internod communic instead analog voltag model particular well suit time-ord sequenc found larg class rout trajectori control problem neuron synchroniz oscil model process element neuron relax oscil built-in self-inhibit relax oscil dynam system capabl accumul potenti energi threshold breakdown point reach point energi abrupt releas new cycl begin descript fit dynam behavior neuron membran rich structur empir model this behavior found well-establish differenti formul hodgkin huxley simplifi version given fitzhugh7 differenti equat account foundat neuron activ also capabl repres subthreshold behavior refractori follow fire membran potenti enter critic region abrupt depolar collaps potenti differ across membran occur follow somewhat slower recoveri this brief electr short membran call action potenti spike constitut output event neuron if caus initi depolar maintain oscil limit-cycl develop generat multipl fire depend input level membran paramet oscil limit singl spike may produc oscillatori burst even continu sustain activ present model share general properti use much simpler descript relax oscil illustr figur activ energy exdt3toij oj input injrjh1~olj input perturb utl intemilf l neju inpul ty figur relax oscil perturb input fire occur energi level reach critic level ec assum constant rate energi influx fire occur natur period ec t=a when pre-synapt puls imping on cours energi accumul fire schedul disturb let repres instant last fire cell tj intant imping arriv cell e act wj uo til ec uo repres unit impuls dramat complex synchron dynam appreci consid simplest possibl case master slave interact two regular fire oscil unit natur period ta tb instant fire unit unidirect send spike signal unit receiv interv measur from last time fire upon recept spike transform quantum energi 6e depend upon post-fir arriv time relationship shape repres refractori post-spik properti here it assum simpl ramp function if interact inhibitori consequ this arriv next fire unit delay respect schedul would absenc perturb posit interv figur becaus shape delay action nil immedi fire becom longer imping pre-synapt spike arriv later interv if interact excitatori delay negat le shorten natur fire interv general assumpt regard function tend synchron within given rang coupl gain phase self-adjust equilibrium achiev given this equilibrium correspond distribut maximum entropi point cell receiv amouint activ common cycl i i inhibit excit figur relationship phase delay when input effici increas linear after-spik interv synchron dynam present attractor ration frequenc pair ratio associ rang stabil ratio lowest cardin wide zone phaselock figur wider stabil wnes correspond one one ratio fa fb invers ta tbl kohn segundo demonstr such phase lock occur live invertebr neuron point paradox natur phase-lock inhibit within stabil region take appear excit sinc small increas input fire rate local result increas output rate area these rang stabil appear unstabl transit fact as recent point out bak9 form infin lock step known as devil 's staircas correspond infin intermedi ration pair figur bak show staircas self-similar scale transit form fractal cantor set fractal dimens univers constant dynam system i excit inhibiti~~v i li l figur unilater synchron constraint satisfact in oscil network global synchron interconnect network mutual phase-lock oscil constraint satisfact problem synchron equilibrium node fire in interlock pattern organ inter-spik interv integ ratio often cite travel salesman problem archetyp for a class import hard problem a special case when ratio must node must fire at frequenc here the equilibrium condit everi node accumul the the amount energi the global cycl furthermor the fire must order along a minim path use stochast energi minim simul anneal the first simul demonstr the feasibl the approach with a limit number node the tsp isomorph mani sequenc problem involv distribut constraint fall the oscil array neural net paradigm in a particular natur way work pursu rigor establish the limit applic the model i annea/./ng figur the travel salesman problem in the global oscil oj minim energi node constrain fire at the rate in the order correspond to the minim path acknowledg research support in part aerojet electro-system under the aerojet-ucla cooper research master agreement no nasa nag
----------------------------------------------------------------

title: 49-connecting-to-the-past.pdf

connect past bruce a. macdonald assist professor knowledg scienc laboratori comput scienc depart univers calgari univers drive nw calgari alberta t2n in4 abstract recent renew interest neural-lik process system evidenc exampl two volum parallel distribut process edit rumelhart mcclelland discuss parallel distribut system connectionist model neural net valu pass system multipl context system dissatisfact symbol manipul paradigm artifici intellig seem part respons attent encourag promis massiv parallel system implement hardwar paper relat simpl neural-lik system base multipl context well-known formalisms-nam product system k-iength sequenc predict finite-st machin ture machines-and present earlier sequenc predict result new light introduct reviv neural net research strong exemplifi recent rumelhart mcclelland new journal number meet net also describ parallel distribut system connectionist model valu pass systems3 multipl context learn systems4 symbol manipul paradigm artifici intellig seem success hope seem last real promis massiv parallel system implement hardwar howev flurri new work import consolid new idea place solid alongsid establish one paper relat simpl neural-lik system well-known notions-nam product system k-iength sequenc predict finite-st machin ture machines-and present earlier result abil network new light general form connectionist system lo simplifi three layer net binari fix weight hidden layer therebi avoid mani difficulties-and challengesof recent work neural net hidden unit weight regular pattern use templat sophist expens learn algorithm avoid simpl method use determin output unit weight way gain advantag multilay net retain simplic two layer net train method certain noth lost comput power-a i explain-and limit two layer net carri simplifi three layer one biolog system may similar avoid need learn algorithm simul anneal method common use connectionist model one thing biolog system clear distinguish train phase briefli simplifi net product system implement three layer neuron-lik unit output layer input layer hidden layer product hidden product unit potenti connect predetermin set input output k-iength sequenc predictor form ie level delay unit introduc input layer k-iength predictor unabl distinguish simpl sequenc ba aa sinc ie charact system forgotten whether appear first k-iength predictor augment auxiliari action abl learn regular languag sinc auxiliari action equival state input aamong 1st intern confer neural net san diego ca june con.fer brough equival singl context system andrea 's multipl context system see also macdonald american institut physic figur general form connectionist system form unit oper within unit in~ut excitation-.i weight sum activ w output typic typic product unit enabl predict depend previous state combin sever augment sequenc predictor thring machin tape simul along finite-st control give net comput power univers ture machin relat simpl neural-lik system lack comput abil previous implement abil product system equival simplifi net organ paper next section briefli review general form connectionist system section simplifi section explain result equival product system deal input output net section extend simplifi version enabl learn predict sequenc section explain comput power sequenc predictor increas thring machin input unit receiv auxiliari action fact system learn ture machin section discuss possibl number net combin output form overal net associ area general form connectionist system figur show general form connectionist system unit neuron ce1l figur unit input output oj possibl unit network output net input excit net weight sum input vij weight connect output unit input unit activ unit function fi net input excit typic fi semilinear non-decreas differenti function least larg group unit output function fi activ typic kind threshold function i assum quantiti vari discret time step exampl activ time given fi neti general restrict connect may made unit unit connect direct input output hidden unit complex net describ paper may one type connect figur show common connect topolog three layer units-input hidden output-with cycl connect net train present input combin along desir output combin train system produc desir output given figur basic structur three layer connectionist system input unit hidden unit output unit input dure train weight adjust fashion reduc discrep desir actual output general method lo desir train activ equat general form hebb 's classic rule adjust weight two unit high activ lo weight adjust product two function one depend desir actual activ often difference-and anoth depend input weight weight simpl exampl suppos differ output oj weight chang product output error input excit weight constant determin learn rate widrow-hoff delta rule may use net without hidden unit 1o import contribut recent work connectionist system implement equat hidden unit train signal ti direct avail boltzmann learn method iter vari weight hidden unit train activ use control gradual decreas random method simul anneal backpropag also iter perform gradient descent propag train signal error back net hidden unit i avoid need determin train signal hidden unit fix weight hidden unit section simplifi system assum simplif made general connectionist system section system three layer topolog shown figur ie cycl hidden layer unit weight fix say uniti zero unit linear threshold unit lo mean activ function unit ident function give net weight sum input output function simpl binari threshold form i output threshold activ output binari oft hidden unit threshold requir input activ output activ like gate output unit threshold requir two activ high weight input output generat like gate keep product system view net explain section learning-which occur output unit weights-giv weight adjust accord wij wij oj otherwis weight turn input unit output otherwis wij oj simpl exampl given figur section simpl form net made probabilist replac adjust weight wij estim condit probabl unit output output wij estim p odoj assum independ input unit output unit turn condit probabl occurr output exceed threshold output function onc simplif made need learn hidden unit also iter learn requir weight either assign binari valu estim condit probabl paper present characterist simplifi net section discuss motiv simplifi neural net way product system simplifi net kind simpl product system product system compris global databas set product rule control system databas net system interact provid input reaction output t.he net hidden unit network product rule form precondit action precondit satisfi input excit exceed threshold hidden unit action repres output unit hidden product unit activ control system product system choos rule whose action perform set rule whose precondit met neural net control system distribut throughout net output unit exampl output unit might form winner-takeal net product system complex control involv forward backward chain choos action seek goal discuss elsewhere4.12.16 figur illust.r simpl product implement neural net figur show input hidden unit element precondit appropri input combin present associ hidden product unit fire onc weight leam connect hidden unit output unit fire product result output simplifi neural net direct equival product system whose element input output product system symbol element variabl given valu product action neural net direct implement sinc output predetermin set howev see later extens t.o framework enabl abil cthis might refer sensory-motor product system sinc implement ill l'eal system robot deal sens input execut motor action may includ auxiliari action section figur product implement simplifi neural net product rule icloudi i ipressur fall i then iit rain i rule implement hidden unit threshold hidden unit gate threshold output unit gate learn weight if net probabilist otherwis estim p rainlcloud pressur fall rain weight figur net predict next charact sequenc base last charact net product unit hidden unit combin input unit exampl net could predict sequenc abcabcabc product form if last charact then next charact learn rule wij if inputj outputi output wijoj input neural net output learn procedur clamp input output desir valu system calcul weight valu repeat requir input/output combin sequenc predict product system neural net predict sequenc given exampl repeat sequenc product learn predict futur event basi recent one figur show trivial simpl sequenc predictor predict next charact sequenc base previous one figur also give detail learn procedur simplifi net net need train onc input combin then predict output everi charact seen current one probabilist form net would estim condit probabl next charact condit current one mani figur use delay input neural net implement k-length sequenc predictor net last three charact input input hidden output 2nd last exampl product if last three charact then present possibl charact pair would need proper estim probabl net would learn probabl distribut charact pair predictor like one figur extend general k-iength predictor long input delay step avail then illustr figur 3-length predict hidden product unit repres possibl combin symbol again output weight train respond previous seen input combin three charact delay provid dedic neural net shown figur note net assum synchron updat input feedback around unit chang one step output chang various way implement delay neuron andrea investig purpose-delay inputs-in detail simul similar net work sequenc predict neural net feldman ballard find connectionist system initi suit repres chang time one form chang sequenc suggest two method repres sequenc net first unit connect each sequenc sequenti task repres fire these unit success second method buffer input time input recent past avail well current input delay input avail suggest import differ necessari length buffer feldman ballard suggest buffer long enough hold phrase natur languag i expect use buffer longer andrea symbol input repres complex inform effect give length seven buffer inform recent seven simpl input discuss section method back-propagation13 enabl recurr network learn sequenti task dfeldman ballard2 give dedic neural net connect varieti flulction figur input delay dedic neural subnet two stage delay shown delay network time diagram tml origin signal delay one step delay two step manner similar first suggest last paragraph sequenc connect unit repres sequenc event one exampl net learn complet sequenc charact given first two charact six charact sequenc next four output error must propag around cycl recurr net number time serial may also achiev sequenc state distribut activ exampl net play side tic-tac-to game sequenti natur net 's behavior deriv sequenti natur respons net 's action tic-tac-to move net model sequenc intern model sequenti part environ exampl tic-tac-to play net model oppon k-iength sequenc predictor unabl learn sequenc repeat frequent everi charact their k-iength context includ inform last event howev there two way inform kth last input retain net first method latch input second involv auxiliari action latch unit input latch held indefinit use combin shown figur input would normal latch andrea discuss this techniqu thread latch event among non-latch event give net inform arbitrarili far back input-output histori inform immedi past briefli sequenc ba distinguish aa if first charact latch howev this ad hoc solut this problem auxiliari action output fed back net input signal this enabl system choos next output least part base previous one indic figur if particular fed back output also one without extern manifest whose extern manifest independ task perform then output auxiliari action las interest reader refer andrea extens analysi given figur thread latch circuit rememb event anoth come along this two input latch two letter number unit may similar connect form mutual inhibit layer winner-take-al connect along posit feedback keep select output activ when input disappear figur auxiliari actions-th outputs-ar fed back input net enabl net rememb state here part net exampl product shown there two type action charact action sinput output charact input if input charact input charact output then output charact llij ill direct effect task system perform sinc it evok relev input use net symbol action if auxiliari action latch input then symbol inform rememb indefinit lost when anoth auxiliari action kind input take latch thus auxiliari action act like rememb state system perform action remind particular state figur illustr this system that predict charact state chang given previous charact state an obvious candid auxiliari action speech blank oval figur would repres net 's environ speech action heard although it extern manifest speech direct effect physic interact the world symbol abil provid the power auxiliari action also includ other speaker the interact simul abstract automata the exampl figur give the essenc simul finit state automaton product system or it neural net equival it illustr the transit function an automaton the new state output function the previous state input thus neural net simul finit state automaton long it addit auxiliari action thring machin finit state automaton control plus an unbound memori neural net could simul a lure machin two way way demonstr product system implementations-equival neural net all multipl context learn system briefli explain section the first thring machin simul the system simul the finit state control abl use an unbound extern memori fsee john andrea 's colleagu work4 figur multipl context learn system implement multipl neural net each:3 layer net the simplifi form present a number elabor extra connect goal-seek forward backward chain output channel the real world much like the paper ture 's origin work the second simnlat.ion emb the memori the multipl context learn system along a counter access this simul memori both learn the productions-equival learn output unit weights-requir the simul the second abl add intern memori requir a limit depend the size the network easili larg enough allow year comput the second could also employ extern memori the first briefli the second simul compris multipl sequenc predictor predict auxiliari action rememb the state the control the current memori posit the memori element updat relearn the product repres that element the precondit the address the product action the store item multipl system form associ area a multipl context learn system product system version a multipl neural net although a simpl version implement a simul net it effect compris sever net or associ areas-which may output input in common indic in figur hidden unit weight specifi templat one each net a templat give the input a zero weight the hidden unit a net the input a weight uniti delay latch input also avail the actual output select the combin predict the net in a winner-take-al fashion i see the design real neural net say control real robot requir a larg degre predetermin connect a robot control could one three layer net wit.h everi input connect everi hidden unit in turn connect everi output there need some connect constraint so the net reflect the function special in the control requir the multipl context learn system all the hidden layer connect predetermin allow output connect learn this avoid the credit assign problem therefor also the need learn algorithm boltzmann learn back-propag howev as the multipl context learn system auxiliari action delay latch input it lack comput power futur work in this area investig for exampl the abil differ kind net learn auxiliari act.ion this may difficult as symbol action may provid in train input and output. for exampl a control for a robot bodi would deal vision manipul motion etc conclus this paper present a simplifi three layer connectionist model fix weight for hidden unit delay and latch for input sequenc predict abil auxiliari state action and the abil use intern and extern memori the result abl learn to simul a ture machin simpl neural-lik system not lack comput power acknowledg this work support by the natur scienc and engin council canada
----------------------------------------------------------------

title: 48-a-neural-network-classifier-based-on-coding-theory.pdf

neural network classifi base code theori tzt-dar chlueh rodney goodman eanrornla instltut technolog pasadena eanromla abstract new neural network classifi propos transform classif problem code theori problem decod noisi codeword input vector featur space transform intern represent codeword code space error correct decod space classifi input featur vector class two class code give high perform hadamard matrix code maxim length sequenc code show number class store n-neuron system linear signific obtain use hopfield type memori classifi i introduct associ recal use neural network recent receiv great deal attent hopfield paper describ mechan iter feedback loop stabil memori element nearest input provid mani memori vector store machin he also shown number memori store n-neuron system mceliec work show synchron oper hopfield memori n/ 2iogn data vector store reliabl larg abu-mostafa predict upper bound number data vector n-neuron hopfield machin n. believ one abl devis machin number data vector linear larger achiev hopfield method featur space code space figur classif problem versus error control decod problem paper specif concern problem classif pattern recognit propos new method build neural network classifi base well establish techniqu error control code consid typic classif problem one given priori set class m. associ class featur vector label class exemplar class i.e american institut physic repres point class region input classifi class nearest exemplar input henc class region n-dimension binari featur space bn everi vector classifi correspond class similar problem decod codeword error correct code shown case codeword construct design usual least dmtn apart receiv corrupt codeword input decod find nearest codeword input principl distanc codeword greater 2t possibl decod classifi noisi codeword featur vector correct codeword exemplar provid ham distanc noisi codeword correct codeword note guarante exemplar uniform distribut bn consequ attract radius maximum number error occur given featur vector vector correct classifi depend minimum distanc exemplar mani solut minimum ham distanc classif propos one common use deriv idea match filter communic theori lippmann propos two-stag neural network solv classif problem first correl input exemplar pick maximum winner-take-al circuit network compos two-input compar figur fi.f2 fn input bit si.s2 sm match score similartti exemplar second block pick maximum si.s2 sm produc index exemplar largest score main disadvantag classifi complex maximum-pick circuit exampl winner-take-al net need connect weight larg dynam rang graded-respons neuron whilst compar maximum net demand m-i compar organ log2m stage decoder~ss f i closs f featur space code space match filter type classifi structur propos classifi main idea thus transform everi vector featur space vector code space way everi exemplar correspond codeword code code prefer necessarili properti codeword uniform distribut code space ham distanc everi pair codeword transform turn problem classif code problem decod noisi codeword error correct decod vector code space obtain index noisi codeword henc classifi origin featur vector shown figur paper develop construct classif machin follow first consid problem transform input vector featur space code space describ two hetero-associ memori do first method use outer product matrix techniqu similar hopfield 's second method generat matrix pseudoinvers techruqu s.7j given transform problem associ recal classif problem decod noisi codeword next consid suitabl code machin requir codeword code properti orthogon pseudo-orthogon ratio cross-correl auto-correl codeword small show two class good code particular decod problem hadamard matrix code maxim length sequenc code 8j next formul complet decod algorithm describ overal structur classifi term two layer neural network first layer perform map oper input second one decod output produc index class input belong second part paper concern perform classifi first analyz perform new classifi find relat maximum number class store classif error rate show use transform base outer product method neglig misclassif rate larg n. tight lower bound m. number store class present comprehens simul result confirm exceed theoret expect simul result compar method hopfield model outer product pseudo-invers method analog hard limit connect matric case classifi exceed perform hopfield memori term number class reliabl recov d. transform techniqu object build machin discrimin among input vector classifi one appropri class suppos bn exemplar ofth correspond class given input want machin abl identifi class whose exemplar closest want calcul follow function class i i i i i i denot ham distanc bn approach problem seek transform map exemplar bn correspond codeword bl input featur vector dey thus map noisi codeword wli error ad exemplar correspond error pattern code space error correct decod get index correspond codeword note may ham weight transform may either generat error elimin error present origin input featur vector requir satisfi follow equat m-l implement use single-lay feedfoiward network thus first construct matrix accord set call defin sgn threshold oper map vector rl bl field real number let matrix whose lth column matrix whose th column two possibl method construct matrix follow scheme outer product method scheme matrix defin sum outer product exemplar-codeword pair m-l die equival wdt scheme pseudo-invers method want find matrix satisfi follow equat general squar matrix moreov may singular d-l may exist circumv difficulti calcul pseudo-invers denot dt matrix instead real invers let dtd -ldt formul dt ot nt code code look prefer properti codeword distribut uniform bl distanc two codeword must larg possibl thus seek class equidist code two class hadamard matrix code maxim length sequenc code first defin word pseudo-orthogon defmit let wo wl wl-l bl ath codeword code code said pseudo-orthogon iff l-l wl denot inner product two vector hadamard matric orthogon code length whose codeword row column hadamard matrix case distanc two codeword l/2 conjectur exist code multipl thus provid larg class code mazlmal length sequenc code exist famili maxim length sequenc also call pseudo-random pn sequenc code generat shift regist satisfi pseudo-orthogon suppos primit polynomi degre d. let 2d f xl l/g xl ck xk k=o co.cl period sequenc period sinc i code made cyclic shift co. cl il code satisfi pseudo-orthogon one easili see minimum distanc code give correct power approxim error larg l. iv overal classifi structur shall describ overal classifi structur essenti consist map follow error correct decod maxim length sequenc code hadamard matrix code decod oper correl input vector everi codeword threshold result rational algorithm follow sinc distanc everi two codeword code exact bit decod abl correct error pattern less error threshold set halfway land i.e suppos input vector decod ham weight nonzero compon 2s 2s equat less error away less result arriv follow decod algorithm deax1 sgn vector case less error input output vector sm one compon posit index index class input vector belong howev there error output either negat vector decod failur anoth vector one posit compon decod error function class defin composit decod overal structur new classifi depict figur view two-lay neural network hidden unit output neuron first layer map input featur vector noisi codeword code space intern represent second one decod first output produc index class input belong f1 f2 fn 9l figur overal architectur new neural network classifi perform analysi previous section know classifi make error transform vector code space input decod less error proceed find error rate classifi case input one exemplar error say outer product connect matrix follow approach mcel1ec we n-l m-l sgl wl dj j=o n-l sgn m-l wl dl j=o a=o assum without loss general if n-l m-l j=o wl dl a=o notic we assum random name compon outcom bernoulli trial accord sum independ ident distribut random variabl mean varianc asymptot case nand larg approxim normal distribut mean varianc nm thus pr q vln/m vi tt foo dt next we calcul misclassif rate new classifi follow assum pe k=il/4j pk l_p l-k integ floor sinc generallt possibl express summat explicit we use chernoff method bound pe multipli term summat number larger uniti et k sum from instead pe t k k=o differenti rhs equat set we find optim eto condit impli sinc we deal case small automat satisfi substitut optim we obtain from express pe we estim number class classifi negllgibl misclassif rate follow way suppos pe land small we log sinc fix valu approach infin we from lower bound one easili see new machln abl classifi constant time class better number memori item hopfield model store le although analysi done assumlng approach lnfinlti simul result next section show moder larg lower bound appll vi simul result charact recognit exampl we simul hopfield model new machin use maxlmallength sequenc code follow four case respect connect matrix generat outer product method connect matrix generat pseudo-invers method ill connect matrix generat outer product method compon connect matrix hard limit connect matrix generat pseudo-invers method compon connect matrix hard limit case choic n. program fix number error in input vector random generat set exemplar comput connect matrix machin machin random pick exemplar add nois random complement specifi number bit generat trial input vector simul machin check whether input classifi nearest class report percentag success machin simul result shown in figur in graph horizont axi vertic axi attract radius data we show obtain collect case success rate fix largest attract radius number bit in error input vector success rate here we use attract radius denot particular m. input exemplar success rate less in machin hopfield model new classifi op new classtfier pi analog connect matrix binari connect matrix cil lit td i en tl.a binari connect matrix analog connect matrix figur simul result hopfield memori new classifi hopfield model new classifi op.l=63 new classifi op.l=31 ril u.a figur perfonn new classifi use code differ length in case classifi exceed perfonn hopfield model in tenn number class reliabl recov exampl consid case hard limit connect matrix new classifi hopfield model we find attract radius zero error in input vector hopfield model classif capac approxim new model store also attract radius averag error in input vector hopfield model reliabl store class our new model store class anoth simul use shorter code instead reveal shorten code perform classifi degrad slight we therefor conjectur it possibl use tradit error correct code bch code intern represent howev go higher rate code one trade minimum distanc code error toler for complex number hidden unit impli possibl poorer perform classifi we also notic superior pseudoinvers method outer product method appear connect matric hard limit reason for pseudoinvers method best for decorrel depend among exemplar yet exemplar in simul generat random presum independ consequ one see advantag pseudoinvers method for correl exemplar we expect pseudoinvers method clear better next exampl next we present exampl appli classifi recogn charact each charact repres pixel array input generat flip everi pixel probabl input pass five machin hopfield memori new classifi either pseudotnvers method outer product method figur show result machin for pixel flip probabl respect a blank output mean the classifi refus make a decis first note the case necessarili wors the case confirm the earlier conjectur fewer hidden unit shorter code degrad perfonn slight also one easili see the pseudoinvers method is better the outer product method the correl exemplar both method outperform the hopfield memori sinc the latter mix exemplar to rememb produc a blend exemplar rather the exemplar accord it classifi the input without mistak tf figur the charact recognit exampl pixel revers probabl input correct output hopfield model new classifi op pi pi figur the charact recognit exampl pixel revers probabl input correct output hopfield model new classifi op pi pi vll conclus in this paper we present a new neural network classifi design base code theori techniqu the classifi use codeword from an error correct code intern represent two class code give high perform the hadamard matrix code the maxim length sequenc code in penorm term we shown the new machin is signific better use the hopfield model as a classifi we also note compar the new classifi the hopfield model the increas perform the new classifi entail extra complex sinc it need hard limit neuron and l n connect weight versus neuron and n2 weight in a hopfield memori in conclus we believ our model form the basi a fast practic method of classif with an effici greater previous neural network techniqu
----------------------------------------------------------------

title: 48-a-neural-network-classifier-based-on-coding-theory.pdf

neural network classifi base code theori tzt-dar chlueh rodney goodman eanrornla instltut technolog pasadena eanromla abstract new neural network classifi propos transform classif problem code theori problem decod noisi codeword input vector featur space transform intern represent codeword code space error correct decod space classifi input featur vector class two class code give high perform hadamard matrix code maxim length sequenc code show number class store n-neuron system linear signific obtain use hopfield type memori classifi i introduct associ recal use neural network recent receiv great deal attent hopfield paper describ mechan iter feedback loop stabil memori element nearest input provid mani memori vector store machin he also shown number memori store n-neuron system mceliec work show synchron oper hopfield memori n/ 2iogn data vector store reliabl larg abu-mostafa predict upper bound number data vector n-neuron hopfield machin n. believ one abl devis machin number data vector linear larger achiev hopfield method featur space code space figur classif problem versus error control decod problem paper specif concern problem classif pattern recognit propos new method build neural network classifi base well establish techniqu error control code consid typic classif problem one given priori set class m. associ class featur vector label class exemplar class i.e american institut physic repres point class region input classifi class nearest exemplar input henc class region n-dimension binari featur space bn everi vector classifi correspond class similar problem decod codeword error correct code shown case codeword construct design usual least dmtn apart receiv corrupt codeword input decod find nearest codeword input principl distanc codeword greater 2t possibl decod classifi noisi codeword featur vector correct codeword exemplar provid ham distanc noisi codeword correct codeword note guarante exemplar uniform distribut bn consequ attract radius maximum number error occur given featur vector vector correct classifi depend minimum distanc exemplar mani solut minimum ham distanc classif propos one common use deriv idea match filter communic theori lippmann propos two-stag neural network solv classif problem first correl input exemplar pick maximum winner-take-al circuit network compos two-input compar figur fi.f2 fn input bit si.s2 sm match score similartti exemplar second block pick maximum si.s2 sm produc index exemplar largest score main disadvantag classifi complex maximum-pick circuit exampl winner-take-al net need connect weight larg dynam rang graded-respons neuron whilst compar maximum net demand m-i compar organ log2m stage decoder~ss f i closs f featur space code space match filter type classifi structur propos classifi main idea thus transform everi vector featur space vector code space way everi exemplar correspond codeword code code prefer necessarili properti codeword uniform distribut code space ham distanc everi pair codeword transform turn problem classif code problem decod noisi codeword error correct decod vector code space obtain index noisi codeword henc classifi origin featur vector shown figur paper develop construct classif machin follow first consid problem transform input vector featur space code space describ two hetero-associ memori do first method use outer product matrix techniqu similar hopfield 's second method generat matrix pseudoinvers techruqu s.7j given transform problem associ recal classif problem decod noisi codeword next consid suitabl code machin requir codeword code properti orthogon pseudo-orthogon ratio cross-correl auto-correl codeword small show two class good code particular decod problem hadamard matrix code maxim length sequenc code 8j next formul complet decod algorithm describ overal structur classifi term two layer neural network first layer perform map oper input second one decod output produc index class input belong second part paper concern perform classifi first analyz perform new classifi find relat maximum number class store classif error rate show use transform base outer product method neglig misclassif rate larg n. tight lower bound m. number store class present comprehens simul result confirm exceed theoret expect simul result compar method hopfield model outer product pseudo-invers method analog hard limit connect matric case classifi exceed perform hopfield memori term number class reliabl recov d. transform techniqu object build machin discrimin among input vector classifi one appropri class suppos bn exemplar ofth correspond class given input want machin abl identifi class whose exemplar closest want calcul follow function class i i i i i i denot ham distanc bn approach problem seek transform map exemplar bn correspond codeword bl input featur vector dey thus map noisi codeword wli error ad exemplar correspond error pattern code space error correct decod get index correspond codeword note may ham weight transform may either generat error elimin error present origin input featur vector requir satisfi follow equat m-l implement use single-lay feedfoiward network thus first construct matrix accord set call defin sgn threshold oper map vector rl bl field real number let matrix whose lth column matrix whose th column two possibl method construct matrix follow scheme outer product method scheme matrix defin sum outer product exemplar-codeword pair m-l die equival wdt scheme pseudo-invers method want find matrix satisfi follow equat general squar matrix moreov may singular d-l may exist circumv difficulti calcul pseudo-invers denot dt matrix instead real invers let dtd -ldt formul dt ot nt code code look prefer properti codeword distribut uniform bl distanc two codeword must larg possibl thus seek class equidist code two class hadamard matrix code maxim length sequenc code first defin word pseudo-orthogon defmit let wo wl wl-l bl ath codeword code code said pseudo-orthogon iff l-l wl denot inner product two vector hadamard matric orthogon code length whose codeword row column hadamard matrix case distanc two codeword l/2 conjectur exist code multipl thus provid larg class code mazlmal length sequenc code exist famili maxim length sequenc also call pseudo-random pn sequenc code generat shift regist satisfi pseudo-orthogon suppos primit polynomi degre d. let 2d f xl l/g xl ck xk k=o co.cl period sequenc period sinc i code made cyclic shift co. cl il code satisfi pseudo-orthogon one easili see minimum distanc code give correct power approxim error larg l. iv overal classifi structur shall describ overal classifi structur essenti consist map follow error correct decod maxim length sequenc code hadamard matrix code decod oper correl input vector everi codeword threshold result rational algorithm follow sinc distanc everi two codeword code exact bit decod abl correct error pattern less error threshold set halfway land i.e suppos input vector decod ham weight nonzero compon 2s 2s equat less error away less result arriv follow decod algorithm deax1 sgn vector case less error input output vector sm one compon posit index index class input vector belong howev there error output either negat vector decod failur anoth vector one posit compon decod error function class defin composit decod overal structur new classifi depict figur view two-lay neural network hidden unit output neuron first layer map input featur vector noisi codeword code space intern represent second one decod first output produc index class input belong f1 f2 fn 9l figur overal architectur new neural network classifi perform analysi previous section know classifi make error transform vector code space input decod less error proceed find error rate classifi case input one exemplar error say outer product connect matrix follow approach mcel1ec we n-l m-l sgl wl dj j=o n-l sgn m-l wl dl j=o a=o assum without loss general if n-l m-l j=o wl dl a=o notic we assum random name compon outcom bernoulli trial accord sum independ ident distribut random variabl mean varianc asymptot case nand larg approxim normal distribut mean varianc nm thus pr q vln/m vi tt foo dt next we calcul misclassif rate new classifi follow assum pe k=il/4j pk l_p l-k integ floor sinc generallt possibl express summat explicit we use chernoff method bound pe multipli term summat number larger uniti et k sum from instead pe t k k=o differenti rhs equat set we find optim eto condit impli sinc we deal case small automat satisfi substitut optim we obtain from express pe we estim number class classifi negllgibl misclassif rate follow way suppos pe land small we log sinc fix valu approach infin we from lower bound one easili see new machln abl classifi constant time class better number memori item hopfield model store le although analysi done assumlng approach lnfinlti simul result next section show moder larg lower bound appll vi simul result charact recognit exampl we simul hopfield model new machin use maxlmallength sequenc code follow four case respect connect matrix generat outer product method connect matrix generat pseudo-invers method ill connect matrix generat outer product method compon connect matrix hard limit connect matrix generat pseudo-invers method compon connect matrix hard limit case choic n. program fix number error in input vector random generat set exemplar comput connect matrix machin machin random pick exemplar add nois random complement specifi number bit generat trial input vector simul machin check whether input classifi nearest class report percentag success machin simul result shown in figur in graph horizont axi vertic axi attract radius data we show obtain collect case success rate fix largest attract radius number bit in error input vector success rate here we use attract radius denot particular m. input exemplar success rate less in machin hopfield model new classifi op new classtfier pi analog connect matrix binari connect matrix cil lit td i en tl.a binari connect matrix analog connect matrix figur simul result hopfield memori new classifi hopfield model new classifi op.l=63 new classifi op.l=31 ril u.a figur perfonn new classifi use code differ length in case classifi exceed perfonn hopfield model in tenn number class reliabl recov exampl consid case hard limit connect matrix new classifi hopfield model we find attract radius zero error in input vector hopfield model classif capac approxim new model store also attract radius averag error in input vector hopfield model reliabl store class our new model store class anoth simul use shorter code instead reveal shorten code perform classifi degrad slight we therefor conjectur it possibl use tradit error correct code bch code intern represent howev go higher rate code one trade minimum distanc code error toler for complex number hidden unit impli possibl poorer perform classifi we also notic superior pseudoinvers method outer product method appear connect matric hard limit reason for pseudoinvers method best for decorrel depend among exemplar yet exemplar in simul generat random presum independ consequ one see advantag pseudoinvers method for correl exemplar we expect pseudoinvers method clear better next exampl next we present exampl appli classifi recogn charact each charact repres pixel array input generat flip everi pixel probabl input pass five machin hopfield memori new classifi either pseudotnvers method outer product method figur show result machin for pixel flip probabl respect a blank output mean the classifi refus make a decis first note the case necessarili wors the case confirm the earlier conjectur fewer hidden unit shorter code degrad perfonn slight also one easili see the pseudoinvers method is better the outer product method the correl exemplar both method outperform the hopfield memori sinc the latter mix exemplar to rememb produc a blend exemplar rather the exemplar accord it classifi the input without mistak tf figur the charact recognit exampl pixel revers probabl input correct output hopfield model new classifi op pi pi figur the charact recognit exampl pixel revers probabl input correct output hopfield model new classifi op pi pi vll conclus in this paper we present a new neural network classifi design base code theori techniqu the classifi use codeword from an error correct code intern represent two class code give high perform the hadamard matrix code the maxim length sequenc code in penorm term we shown the new machin is signific better use the hopfield model as a classifi we also note compar the new classifi the hopfield model the increas perform the new classifi entail extra complex sinc it need hard limit neuron and l n connect weight versus neuron and n2 weight in a hopfield memori in conclus we believ our model form the basi a fast practic method of classif with an effici greater previous neural network techniqu
----------------------------------------------------------------

title: 32-synchronization-in-neural-nets.pdf

synchron neural net jacqu j. vidal univers california los angel los angel ca john haggerti abstract paper present artifici neural network concept synchroniz oscil network instant individu fire form point process constitut form inform transmit join neuron type communic contrast assum model typic continu discret value-pass network limit messag receiv process unit time marker signal fire unit present signific implemen tation advantag model neuron fire spontan regular absenc perturb interact present schedul fire advanc delay fire neighbor neuron network neuron becom global oscil exhibit multipl synchron attractor arbitrari initi state energi minim learn procedur make network converg oscillatori mode satisfi multi-dimension constraint network direct repres rout schedul problem consist order sequenc event introduct most neural network model deriv variant rosenblatt origin perceptron value-pass network case particular network propos fukushima i hopfield rumelhart mani other everi case input process element either binari continu amplitud signal weight synapt gain subsequ sum integr result activ pass sigmoid threshold filter produc continu quantiz output may becom input neuron behavior model relat live neuron even fall consider short account complex inde observ mani real neuron action potenti spike fire propag axon branch intern activ reach threshold higher john haggerti interact system los angel w. 6th st. la ca american institut physic input rate level result rapid fire behind tradit model assumpt averag frequenc action potenti carrier inform neuron becaus integr fire individu neuron consid effect extent contribut averag intens therefor assum activ simpli frequenc code exact time individu fire ignor view howev cover well known aspect neural communic inde precis time spike arriv make crucial differ outcom neural interact one classic exampl pre-synapt inhibit widespread mechan brain machineri sever studi also demonstr occurr function import precis time phase relationship cooper neuron local network model present paper contrast one mention network fire consid individu output event input side node fire node presynapt neuron either delay inhibit advanc excit node fire seen earlier type neuron interact would call phase-modul engin system also find rational experiment neurophysiolog neurophysiolog plausibl howev major concern rather propos explor potenti use mechan parallel distribut comput merit approach artifici neural network digit puls use internod communic instead analog voltag model particular well suit time-ord sequenc found larg class rout trajectori control problem neuron synchroniz oscil model process element neuron relax oscil built-in self-inhibit relax oscil dynam system capabl accumul potenti energi threshold breakdown point reach point energi abrupt releas new cycl begin descript fit dynam behavior neuron membran rich structur empir model this behavior found well-establish differenti formul hodgkin huxley simplifi version given fitzhugh7 differenti equat account foundat neuron activ also capabl repres subthreshold behavior refractori follow fire membran potenti enter critic region abrupt depolar collaps potenti differ across membran occur follow somewhat slower recoveri this brief electr short membran call action potenti spike constitut output event neuron if caus initi depolar maintain oscil limit-cycl develop generat multipl fire depend input level membran paramet oscil limit singl spike may produc oscillatori burst even continu sustain activ present model share general properti use much simpler descript relax oscil illustr figur activ energy exdt3toij oj input injrjh1~olj input perturb utl intemilf l neju inpul ty figur relax oscil perturb input fire occur energi level reach critic level ec assum constant rate energi influx fire occur natur period ec t=a when pre-synapt puls imping on cours energi accumul fire schedul disturb let repres instant last fire cell tj intant imping arriv cell e act wj uo til ec uo repres unit impuls dramat complex synchron dynam appreci consid simplest possibl case master slave interact two regular fire oscil unit natur period ta tb instant fire unit unidirect send spike signal unit receiv interv measur from last time fire upon recept spike transform quantum energi 6e depend upon post-fir arriv time relationship shape repres refractori post-spik properti here it assum simpl ramp function if interact inhibitori consequ this arriv next fire unit delay respect schedul would absenc perturb posit interv figur becaus shape delay action nil immedi fire becom longer imping pre-synapt spike arriv later interv if interact excitatori delay negat le shorten natur fire interv general assumpt regard function tend synchron within given rang coupl gain phase self-adjust equilibrium achiev given this equilibrium correspond distribut maximum entropi point cell receiv amouint activ common cycl i i inhibit excit figur relationship phase delay when input effici increas linear after-spik interv synchron dynam present attractor ration frequenc pair ratio associ rang stabil ratio lowest cardin wide zone phaselock figur wider stabil wnes correspond one one ratio fa fb invers ta tbl kohn segundo demonstr such phase lock occur live invertebr neuron point paradox natur phase-lock inhibit within stabil region take appear excit sinc small increas input fire rate local result increas output rate area these rang stabil appear unstabl transit fact as recent point out bak9 form infin lock step known as devil 's staircas correspond infin intermedi ration pair figur bak show staircas self-similar scale transit form fractal cantor set fractal dimens univers constant dynam system i excit inhibiti~~v i li l figur unilater synchron constraint satisfact in oscil network global synchron interconnect network mutual phase-lock oscil constraint satisfact problem synchron equilibrium node fire in interlock pattern organ inter-spik interv integ ratio often cite travel salesman problem archetyp for a class import hard problem a special case when ratio must node must fire at frequenc here the equilibrium condit everi node accumul the the amount energi the global cycl furthermor the fire must order along a minim path use stochast energi minim simul anneal the first simul demonstr the feasibl the approach with a limit number node the tsp isomorph mani sequenc problem involv distribut constraint fall the oscil array neural net paradigm in a particular natur way work pursu rigor establish the limit applic the model i annea/./ng figur the travel salesman problem in the global oscil oj minim energi node constrain fire at the rate in the order correspond to the minim path acknowledg research support in part aerojet electro-system under the aerojet-ucla cooper research master agreement no nasa nag
----------------------------------------------------------------

title: 54-a-method-for-the-design-of-stable-lateral-inhibition-networks-that-is-robust-in-the-presence-of-circuit-parasitics.pdf

method design stabl later inhibit network robust presenc circuit parasit j.l wyatt jr d.l standley depart electr engin comput scienc massachusett institut technolog cambridg massachusett abstract analog vlsi implement neural system sometim conveni build later inhibit network use local connect on-chip resist grid serious problem unwant spontan oscil often aris circuit render unus practic paper report design approach guarante system stabl even though valu design element parasit element resist grid may unknown method base rigor somewhat novel mathemat analysi use tellegen theorem idea popov multipli control theori thorough practic criteria local sens overal analysi interconnect system requir empir sens involv measur frequenc respons data individu cell robust sens unmodel parasit resist capacit interconnect network affect analysi i introduct term later inhibit first aros neurophysiolog describ common form neural circuitri output neuron popul use inhibit respons neighbor perhap best understood exampl horizont cell layer vertebr retina later inhibit simultan enhanc intens edg act automat lain control extend dynam rang retina whole principl use design artifici neural system algorithm kohonen other electron design neural chip carver mead vlsi implement neural system conveni build later inhibit network use local connect on-chip resist grid linear resistor fabric polysilicon yield compact realize nonlinear resist grid made mos transistor found use imag segment network type divid two class feedback system feedforward-on system feedforward case one set amplifi impos signal voltag american institut physic current grid anoth set read result respons subsequ process amplifi write grid read feedback arrang feedforward network type inher stabl feedback network need practic exampl one carver meadl retina chips3 achiev edg enhanc mean later inhibit resist grid figur show singl cell continuous-tim version chip note capacitor voltag affect local light intens incid cell capacitor voltag neighbor cell ident design cell drive neighbor drive distant neighbor origin cell turn thus necessari ingredi instabl activ element signal feedback present system fact continuous-tim version oscil bad origin design scarc usabl practic later inhibit path enabl oscil i incid light figur photoreceptor signal processor circuit use two mos transconduct amplifi realiz later inhibit communic similar unit resist grid readili occur resist grid circuit activ element feedback even individu cell quit stabl analysi condit instabl straightforward method appear hopeless sinc repeat array contain mani cell influenc mani other direct indirect influenc turn number simultan activ feedback loop enorm paper report practic design approach rigor guarante system stabl simplest version idea intuit obvious design individu cell although intern activ act like passiv system seen resist grid circuit theori languag design goal celli output imped positive-r function sometim difficult practic show origin network satisfi condit absenc certain parasit element import perhap condit one verifi experiment frequency-respons measur physic appar collect cell appear passiv termin form stabl system interconnect passiv medium resist grid research contribut report summari form demonstr passiv positive-r condit much stronger actual need weaker condit easili achiev practic suffic guarante stabil linear network model ii extens nonlinear domain furthermor rule large-sign oscil certain condit ii first-ord linear analysi singl cell begin linear analysi elementari model circuit initi approxim output admitt cell simplifi topolog without loss relev inform use naive'model transconduct amplifi shown figur simplifi network topolog transconduct amplifi model circuit capacitor absorb co2 straightforward calcul show output admitt given yes positive-r passiv admitt sinc alway realiz network form shown ro2 gmlgm2rol coi/gmlgm2 although origin circuit contain inductor realize capacitor inductor thus capabl damp oscil nonetheless transamp model perfect accur network creat interconnect cell resist grid parasit capacit could exhibit sustain oscil element valu may typic practic model light damp reson around i khz this disturb high suggest cell high sensit parasit element captur simpl model our preliminari rl yes figur passiv network realize output admitt circuit analysi much complex model extract physic circuit layout creat carver mead 's laboratori indic output imped passiv valu transamp bias current definit explan instabl await more care circuit model effort perhap design on-chip imped measur instrument iii positive-r function e-positlv function stabil linear network model follow discuss cr+jw complex variabl ration function ratio polynomi real coeffici assum simplic pure imaginari pole term close right hale plane refer set complex number re def i function said positive-r pole right half plane re h jw know outset right half plane pole def i reduc simpl graphic criterion positiver nyquist diagram plot h jw lie entir close right half plane note positive-r function necessarili stabl sinc right half plane pole stabl function necessarili positive-r exampl show deep link posit real function physic network passiv establish classic result linear circuit theori state positive-r possibl synthes 2-termin network posit linear resistor capacitor inductor ideal transform driving-point imped admitt oef function said a-posit particular valu e e pole right half plane nyquist plot lie strict right straight line pass origin angl real posit axi note everi a-posit function stabl function e-posit necessarili positive-r i re g jw figur nyquist diagram function a-posit positive-r exampl function a-posit stabl but positive-r sinc nyquist diagram shown cross left half plane import e-posit function lie follow observ interconnect passiv linear resistor capacitor cell stabl linear imped result unstabl network instabl result imped also positive-r a-posit imped form larger class positive-r one henc a-posit less demand synthesi goal theorem show instabl result imped a-posit even positive-r theorem consid linear network arbitrari topolog consist number passiv 2-termin resistor capacitor arbitrari valu driven number activ cell output imped activ cell a-posit common network stabl proof theorem reli lemma lemma a-posit fix close first quadrant complex plane h lie strict right straight line pass origin angl real posit axi re im proof lemma outlin let function assign close right half plane perpendicular distanc des line defin def note des harmon close right half plane sinc analyt follow applic maximum modulus principle8 harmon function take minimum valu boundari domain imaginari axi this establish lemma proof theorem outlin network unstabl margin stabl natur frequenc close right half plane so natur frequenc network equat nonzero solut so let denot complex branch current solut tellegen i theorern9 sum complex power absorb circuit element must vanish solut iik12/s0ck capac~t cell termin pair second term delet special case so=o sinc complex power capacitor vanish so=o if network natur frequenc close right half plane must one close first quadrant sinc natur frequenc either real els occur complex conjug pair but satisfi so close first quadrant we see divid side iiki2 sum taken network branch after this divis assert zero convex combin term form rk term form ckso -i term form zk so visual term lie complex plane first set lie real posit axi second set lie close adrant sinc so lie close 1st quadrant assumpt third set lie right a line pass origin angl a lemma thus term lie strict right this line impli convex combin equal zero henc network stabl iv stabil result network nonlinear resistor capacitor previous result linear network afford limit insight behavior nonlinear network first nonlinear equat linear equilibrium point theorem appli linear model if linear model stabl equilibrium point origin nonlinear network local stabl network return equilibrium point if initi condit suffici near it but result this section contrast appli full nonlinear circuit model allow one conclud certain circumst network oscil even if the initi state arbitrarili far the equilibrium point def a function describ in section iii said tc satisfi the popov criterion lo if exist a real number r o such re l+jwr note posit real function satisfi the popov criterion the reader easili verifi in exam~l i satisfi the popov criterion a rang valu the import effect the term l+jwr in def rotat the nyquist plot counterclockwis progress greater amount increas theorem consid a network consist nonlinear 2-termin resistor capacitor cell linear output imped suppos the resistor curv character continu diffefenti function gk vk gk o gk vk valu of vk ii the capacitor character by ck vk ~k with ci ck v c2 valu of vk iii the imped zk pole in the close right half plane satisfi the popov criterion common valu of if condit satisfi the network stabl in the sens that for ani initi condit oo i branch dt the proof base tellegen 's theorem is rather involv omit and appear elsewher it acknowledg we sincer thank professor carver mead of cal tech for enthusiast support this work and for make it possibl for us present earli report it in this confer proceed this work supportedj defens advanc research project agenc the offic of naval research arpa order no contract no and defens advanc research project agenc darpa contract no
----------------------------------------------------------------

title: 49-connecting-to-the-past.pdf

connect past bruce a. macdonald assist professor knowledg scienc laboratori comput scienc depart univers calgari univers drive nw calgari alberta t2n in4 abstract recent renew interest neural-lik process system evidenc exampl two volum parallel distribut process edit rumelhart mcclelland discuss parallel distribut system connectionist model neural net valu pass system multipl context system dissatisfact symbol manipul paradigm artifici intellig seem part respons attent encourag promis massiv parallel system implement hardwar paper relat simpl neural-lik system base multipl context well-known formalisms-nam product system k-iength sequenc predict finite-st machin ture machines-and present earlier sequenc predict result new light introduct reviv neural net research strong exemplifi recent rumelhart mcclelland new journal number meet net also describ parallel distribut system connectionist model valu pass systems3 multipl context learn systems4 symbol manipul paradigm artifici intellig seem success hope seem last real promis massiv parallel system implement hardwar howev flurri new work import consolid new idea place solid alongsid establish one paper relat simpl neural-lik system well-known notions-nam product system k-iength sequenc predict finite-st machin ture machines-and present earlier result abil network new light general form connectionist system lo simplifi three layer net binari fix weight hidden layer therebi avoid mani difficulties-and challengesof recent work neural net hidden unit weight regular pattern use templat sophist expens learn algorithm avoid simpl method use determin output unit weight way gain advantag multilay net retain simplic two layer net train method certain noth lost comput power-a i explain-and limit two layer net carri simplifi three layer one biolog system may similar avoid need learn algorithm simul anneal method common use connectionist model one thing biolog system clear distinguish train phase briefli simplifi net product system implement three layer neuron-lik unit output layer input layer hidden layer product hidden product unit potenti connect predetermin set input output k-iength sequenc predictor form ie level delay unit introduc input layer k-iength predictor unabl distinguish simpl sequenc ba aa sinc ie charact system forgotten whether appear first k-iength predictor augment auxiliari action abl learn regular languag sinc auxiliari action equival state input aamong 1st intern confer neural net san diego ca june con.fer brough equival singl context system andrea 's multipl context system see also macdonald american institut physic figur general form connectionist system form unit oper within unit in~ut excitation-.i weight sum activ w output typic typic product unit enabl predict depend previous state combin sever augment sequenc predictor thring machin tape simul along finite-st control give net comput power univers ture machin relat simpl neural-lik system lack comput abil previous implement abil product system equival simplifi net organ paper next section briefli review general form connectionist system section simplifi section explain result equival product system deal input output net section extend simplifi version enabl learn predict sequenc section explain comput power sequenc predictor increas thring machin input unit receiv auxiliari action fact system learn ture machin section discuss possibl number net combin output form overal net associ area general form connectionist system figur show general form connectionist system unit neuron ce1l figur unit input output oj possibl unit network output net input excit net weight sum input vij weight connect output unit input unit activ unit function fi net input excit typic fi semilinear non-decreas differenti function least larg group unit output function fi activ typic kind threshold function i assum quantiti vari discret time step exampl activ time given fi neti general restrict connect may made unit unit connect direct input output hidden unit complex net describ paper may one type connect figur show common connect topolog three layer units-input hidden output-with cycl connect net train present input combin along desir output combin train system produc desir output given figur basic structur three layer connectionist system input unit hidden unit output unit input dure train weight adjust fashion reduc discrep desir actual output general method lo desir train activ equat general form hebb 's classic rule adjust weight two unit high activ lo weight adjust product two function one depend desir actual activ often difference-and anoth depend input weight weight simpl exampl suppos differ output oj weight chang product output error input excit weight constant determin learn rate widrow-hoff delta rule may use net without hidden unit 1o import contribut recent work connectionist system implement equat hidden unit train signal ti direct avail boltzmann learn method iter vari weight hidden unit train activ use control gradual decreas random method simul anneal backpropag also iter perform gradient descent propag train signal error back net hidden unit i avoid need determin train signal hidden unit fix weight hidden unit section simplifi system assum simplif made general connectionist system section system three layer topolog shown figur ie cycl hidden layer unit weight fix say uniti zero unit linear threshold unit lo mean activ function unit ident function give net weight sum input output function simpl binari threshold form i output threshold activ output binari oft hidden unit threshold requir input activ output activ like gate output unit threshold requir two activ high weight input output generat like gate keep product system view net explain section learning-which occur output unit weights-giv weight adjust accord wij wij oj otherwis weight turn input unit output otherwis wij oj simpl exampl given figur section simpl form net made probabilist replac adjust weight wij estim condit probabl unit output output wij estim p odoj assum independ input unit output unit turn condit probabl occurr output exceed threshold output function onc simplif made need learn hidden unit also iter learn requir weight either assign binari valu estim condit probabl paper present characterist simplifi net section discuss motiv simplifi neural net way product system simplifi net kind simpl product system product system compris global databas set product rule control system databas net system interact provid input reaction output t.he net hidden unit network product rule form precondit action precondit satisfi input excit exceed threshold hidden unit action repres output unit hidden product unit activ control system product system choos rule whose action perform set rule whose precondit met neural net control system distribut throughout net output unit exampl output unit might form winner-takeal net product system complex control involv forward backward chain choos action seek goal discuss elsewhere4.12.16 figur illust.r simpl product implement neural net figur show input hidden unit element precondit appropri input combin present associ hidden product unit fire onc weight leam connect hidden unit output unit fire product result output simplifi neural net direct equival product system whose element input output product system symbol element variabl given valu product action neural net direct implement sinc output predetermin set howev see later extens t.o framework enabl abil cthis might refer sensory-motor product system sinc implement ill l'eal system robot deal sens input execut motor action may includ auxiliari action section figur product implement simplifi neural net product rule icloudi i ipressur fall i then iit rain i rule implement hidden unit threshold hidden unit gate threshold output unit gate learn weight if net probabilist otherwis estim p rainlcloud pressur fall rain weight figur net predict next charact sequenc base last charact net product unit hidden unit combin input unit exampl net could predict sequenc abcabcabc product form if last charact then next charact learn rule wij if inputj outputi output wijoj input neural net output learn procedur clamp input output desir valu system calcul weight valu repeat requir input/output combin sequenc predict product system neural net predict sequenc given exampl repeat sequenc product learn predict futur event basi recent one figur show trivial simpl sequenc predictor predict next charact sequenc base previous one figur also give detail learn procedur simplifi net net need train onc input combin then predict output everi charact seen current one probabilist form net would estim condit probabl next charact condit current one mani figur use delay input neural net implement k-length sequenc predictor net last three charact input input hidden output 2nd last exampl product if last three charact then present possibl charact pair would need proper estim probabl net would learn probabl distribut charact pair predictor like one figur extend general k-iength predictor long input delay step avail then illustr figur 3-length predict hidden product unit repres possibl combin symbol again output weight train respond previous seen input combin three charact delay provid dedic neural net shown figur note net assum synchron updat input feedback around unit chang one step output chang various way implement delay neuron andrea investig purpose-delay inputs-in detail simul similar net work sequenc predict neural net feldman ballard find connectionist system initi suit repres chang time one form chang sequenc suggest two method repres sequenc net first unit connect each sequenc sequenti task repres fire these unit success second method buffer input time input recent past avail well current input delay input avail suggest import differ necessari length buffer feldman ballard suggest buffer long enough hold phrase natur languag i expect use buffer longer andrea symbol input repres complex inform effect give length seven buffer inform recent seven simpl input discuss section method back-propagation13 enabl recurr network learn sequenti task dfeldman ballard2 give dedic neural net connect varieti flulction figur input delay dedic neural subnet two stage delay shown delay network time diagram tml origin signal delay one step delay two step manner similar first suggest last paragraph sequenc connect unit repres sequenc event one exampl net learn complet sequenc charact given first two charact six charact sequenc next four output error must propag around cycl recurr net number time serial may also achiev sequenc state distribut activ exampl net play side tic-tac-to game sequenti natur net 's behavior deriv sequenti natur respons net 's action tic-tac-to move net model sequenc intern model sequenti part environ exampl tic-tac-to play net model oppon k-iength sequenc predictor unabl learn sequenc repeat frequent everi charact their k-iength context includ inform last event howev there two way inform kth last input retain net first method latch input second involv auxiliari action latch unit input latch held indefinit use combin shown figur input would normal latch andrea discuss this techniqu thread latch event among non-latch event give net inform arbitrarili far back input-output histori inform immedi past briefli sequenc ba distinguish aa if first charact latch howev this ad hoc solut this problem auxiliari action output fed back net input signal this enabl system choos next output least part base previous one indic figur if particular fed back output also one without extern manifest whose extern manifest independ task perform then output auxiliari action las interest reader refer andrea extens analysi given figur thread latch circuit rememb event anoth come along this two input latch two letter number unit may similar connect form mutual inhibit layer winner-take-al connect along posit feedback keep select output activ when input disappear figur auxiliari actions-th outputs-ar fed back input net enabl net rememb state here part net exampl product shown there two type action charact action sinput output charact input if input charact input charact output then output charact llij ill direct effect task system perform sinc it evok relev input use net symbol action if auxiliari action latch input then symbol inform rememb indefinit lost when anoth auxiliari action kind input take latch thus auxiliari action act like rememb state system perform action remind particular state figur illustr this system that predict charact state chang given previous charact state an obvious candid auxiliari action speech blank oval figur would repres net 's environ speech action heard although it extern manifest speech direct effect physic interact the world symbol abil provid the power auxiliari action also includ other speaker the interact simul abstract automata the exampl figur give the essenc simul finit state automaton product system or it neural net equival it illustr the transit function an automaton the new state output function the previous state input thus neural net simul finit state automaton long it addit auxiliari action thring machin finit state automaton control plus an unbound memori neural net could simul a lure machin two way way demonstr product system implementations-equival neural net all multipl context learn system briefli explain section the first thring machin simul the system simul the finit state control abl use an unbound extern memori fsee john andrea 's colleagu work4 figur multipl context learn system implement multipl neural net each:3 layer net the simplifi form present a number elabor extra connect goal-seek forward backward chain output channel the real world much like the paper ture 's origin work the second simnlat.ion emb the memori the multipl context learn system along a counter access this simul memori both learn the productions-equival learn output unit weights-requir the simul the second abl add intern memori requir a limit depend the size the network easili larg enough allow year comput the second could also employ extern memori the first briefli the second simul compris multipl sequenc predictor predict auxiliari action rememb the state the control the current memori posit the memori element updat relearn the product repres that element the precondit the address the product action the store item multipl system form associ area a multipl context learn system product system version a multipl neural net although a simpl version implement a simul net it effect compris sever net or associ areas-which may output input in common indic in figur hidden unit weight specifi templat one each net a templat give the input a zero weight the hidden unit a net the input a weight uniti delay latch input also avail the actual output select the combin predict the net in a winner-take-al fashion i see the design real neural net say control real robot requir a larg degre predetermin connect a robot control could one three layer net wit.h everi input connect everi hidden unit in turn connect everi output there need some connect constraint so the net reflect the function special in the control requir the multipl context learn system all the hidden layer connect predetermin allow output connect learn this avoid the credit assign problem therefor also the need learn algorithm boltzmann learn back-propag howev as the multipl context learn system auxiliari action delay latch input it lack comput power futur work in this area investig for exampl the abil differ kind net learn auxiliari act.ion this may difficult as symbol action may provid in train input and output. for exampl a control for a robot bodi would deal vision manipul motion etc conclus this paper present a simplifi three layer connectionist model fix weight for hidden unit delay and latch for input sequenc predict abil auxiliari state action and the abil use intern and extern memori the result abl learn to simul a ture machin simpl neural-lik system not lack comput power acknowledg this work support by the natur scienc and engin council canada
----------------------------------------------------------------

title: 50-an-adaptive-and-heterodyne-filtering-procedure-for-the-imaging-of-moving-objects.pdf

adapt heterodyn filter procedur imag move object f. h. schule h. a. k. mastebroek w. h. zaagman biophys depart laboratori general physic westersingel em groningen netherland abstract recent experiment work stimulus veloc depend time resolv power neural unit situat highest order optic ganglion blowfli reveal first sight amaz phenomenon high level fli visual system time constant unit involv process neural activ evok move object rough spokeninvers proport veloc object extrem wide rang paper discuss implement two dimension heterodyn adapt filter construct comput simul model featur simul model includ abil account experiment observ stimulus-tun adapt tempor behaviour time constant fli visual system simul result obtain clear show applic adapt process procedur deliv improv imag techniqu move pattern high veloc rang few remark fli visual system visual system diptera includ blowfli calliphora erythrocephala regular organ allow therefor precis optic stimul techniqu also long term electrophysiolog record made relat easi visual system reason blowfli well-known rapid clever pilot turn extrem suitabl anim systemat studi basic principl may underli detect process movement inform neural level fli visual system input retin mosaic structur precis map onto higher order optic ganglia lamina medulla lobula mean neural column ganglion visual system correspond certain optic axi visual field compound eye lobula complex set wide-field movement sensit neuron found integr input signal whole visual field entir eye one wide field neuron classifi i hausen extens studi anatomically2 well electrophysiologically5 obtain result general agre well found behavior optomotor experi movement detect understood term reichardt correl model i neuron sensit horizont movement direct select high rate action potenti spike per second record element case visual stimuli move horizont inward back front visual field pre/er direct wherea movement horizont outward front back null direct suppress activ american institut physic experiment result model base i neuron stimul prefer direct step wise pattern displac respond increas neural activ repeat stimulus step one obtain averag respons ms latenc period respons manifest sharp increas averag fire rate follow much slower decay spontan activ level two exampl averag respons shown post stimulus time histogram psth 's figur time peak peak height relat depend modul depth stimulus step size spatial extent stimulus tail respons describ adequ exponenti decay toward constant spontan fire rate set stimulus paramet respons paramet defin equat estim least-squar fit tail psth smooth line figur result two fit tlmsl oj i'jo tf moo io mdl05 fig.l i lsi verag respons psth 's obtain i neuron adapt smooth stimulus motion veloc top bottom respect smooth line repres least-squar fit psth 's form valu two psth 's ms respect de ruyter van steveninck fit valu function adapt veloc three modul depth m. straight line least-squar fit repres data region form f=q ms de ruyter van steveninck fig.2 figur show fit valu respons time constant function angular veloc move stimulus squar wave grate experi present anim period long enough let visual system adapt move pattern step wise pattern displac reveal given straight line describ ms repres least-squar fit data veloc rang rang vari rough ms ms defin adapt rang interv veloc decreas increas veloc may conclud figur within adapt rang sensit modul depth outcom similar experi constant modul depth pattern constant pattern veloc four differ valu contrast frequenc fc number spatial period per second travers individu visual axi determin spatial wavelength pattern pattern veloc accord fc=v las reveal also almost complet independ behaviour contrast frequenc experi stimulus field subdivid region differ adapt veloc made clear time constant input channel i neuron set local valu stimulus veloc stimulus sub-region final found adapt driven stimulus veloc independ direct find summar qualit follow steadi state respons time constant neural unit highest level fli visual system found tune local within larg veloc rang exclus magnitud veloc move pattern direct despit direct select neuron go question amaz adapt mechan may hard-wir fli visual system instead make advantag result deriv thus far attempt fit experiment observ imag process approach larg number theori sever distinct class algorithm encod veloc direct movement visual system suggest exampl marr ullman i i van santen sperling12 hypothes adapt mechan set time constant lead optim overal perform visual system realiz veloc independ represent move object word within rang veloc time constant found tune veloc represent stimulus certain level within visual circuitri remain independ variat stimulus veloc object motion degrad model given physic descript motion linear space invari model motion degrad process repres follow convolut integr co co jj flu dudv object intens posit object coordin frame point spread function psf imag system respons unit puls imag intens spatial posit blur imag system possibl addit white nois degrad alreadi motion blur imag neglect present consider review principl techniqu field digit imag degrad restor reader refer harri sawchuk sondhi nahi boutalib hildebrand rajala de figueiredo20 demonstr first aboutalib situat motion blur occur straight line along one spatial coordin say along horizont axi it correct look blur imag collect degrad line scan entir imag depend vertic coordin may drop eq reduc f u du given mathemat descript relat movement correspond psf deriv exact equat becom b x f u du extent motion blur typic discret version applic digit imag process purpos describ i i take integ valu relat motion blur extent accord aboutalib scalar differ equat model deriv model motion degrad process cma i-m m-dimension state vector posit along scan line input intens posit output intens blur extent number element line scalar constant matric order mxl lxm respect contain discret valu cj blur psf kroneck delta function influenc time constant veloc amount motion blur artifici receptor array start incorpor simul model psf deriv equat model perform neural columnar arrang filter lobula complex restrict time constant remain fix throughout whole rang stimulus veloc realize psf easili achiev via mention state space model i. i. fig.3 posit artifici receptor array upper part demonstr effect increas magnitud time constant one-dimension array filter result increas motion blur pattern veloc remain constant origin pattern shown solid line square-wav grate spatial wavelength equal artifici receptor distanc three wave form drawn show gradual increas increas magnitud time constant represent origin square-wav consequ degrad lower part gradual increas veloc move square-wav filter time constant kept fix result also clear increas degrad first demonstr effect increas time constant pattern veloc remain result increas blur therefor introduc one dimension array filter equip time constant impuls respons origin pattern shown squar solid line upper part figur consist squar wave grate spatial period overlap artifici recept filter pattern drawn show constant veloc move grate increas magnitud time constant filter result increas blur represent grate hand increas veloc time constant artifici recept unit remain also result clear increas motion blur demonstr lower part figur inspect two wave form drawn mean dash line both upper lower half figur yield conclus apart round error introduc rather small number artifici filter avail equal amount smear produc when product time constant pattern veloc equal upper dash wave form veloc four time smaller time constant four time larger equival lower part figur adapt scheme design proper imag process procedur next step incorpor experiment observ flexibl properti time constant imag element devic figur 4a scheme shown filter inform fix time constant influenc pattern veloc figur 4b network shown time constant also remain fix matter pattern movement present next level inform process spatial differenti network incorpor order enhanc blur contrast filter network figur 4c first measur magnitud veloc move object done thus far hypothet introduc movement process algorithm model set recept element sampl environ manner proper estim local pattern veloc done time constant artifici recept element tune accord estim veloc final differenti network scheme 4b use actual tune mechan use simul outlin figur given rang veloc model suppos oper given lower limit time constant min min smallest valu physic realiz time constant tune new valu accord experiment observ reciproc relationship veloc within adapt rang larger fix minimum valu demonstr previous section correspond blur represent move stimulus thus alway larger situat filter done fix smallest time constant min import howev fact due tune mechan blur constant sinc product veloc time constant kept constant so inform process system veloc independ represent imag result serv input spatial differenti network outlin figur 4c elementari form differenti filter procedur one gradient two filter k-i k+l nearest neighbor filter taken then ad constant weigh factor central output drawn figur sign gradient depend direct estim movement essenti model claim weigh factor constant throughout whole set filter whole high veloc rang heterodyn imag perform import notic exist so-cal settl time minim time need movement process devic abl accur measur object veloc note time set equal zero case relat stimulus veloc known priori demonstr figur sinc without doubt within settl period estim veloc valu come erron thus optim perform imag devic expect exampl result initi settl procedur shown yv ryo i~j pattern movement figur right network consist set filter fix pattern veloc independ time constant impuls respons ident network figur 4a follow spatial differenti circuitri add weigh gradient two neighbor filter output k-l k+i central filter output k. time constant filter network tune hypothet movement estim mechan visual number recept element combin output tune filter detail descript mechan shown figur tune network follow ident spatial differenti circuit describ figur increas veloc decreas time constant min detail descript mechan use tune time constant time constant specif neural channel set pattern veloc accord relationship shown insert deriv eq i i 4r i i i i i i i i 4v i i 2v wi 8v i posit artifici receptor array fig.6 thick line square-wav stimulus pattern spatial wavelength overlap artifici recept element thick line respons differ pattern veloc system consist parallel neural filter equip time constant tune veloc follow spatial differenti network describ dash line respons differ pattern veloc filter system fix time constant follow spatial differenti circuitri note sharp shoot case result obtain imag procedur drawn figur 4c shown figur pattern consist squar wave overlap pictur element pattern move left differ veloc veloc one wavelength shown thick line squar wave pattern dash line output imag devic depict figur constant time constant constant weigh factor spatial process stage note larg differ sever output thin continu line output imag devic drawn figur tune time constant accord reciproc relationship pattern veloc time constant constant weigh factor spatial process stage simul detail reader refer zaagman output almost complet good agreement origin stimulus throughout whole veloc rang figur show effect gradient weigh factor overal filter perform estim improv deblur imag compar blur imag measur db quantit measur determin case move squar wave pattern motion blur ix iti weigh factor effect weigh factor overal filter perform curv measur case move square-wav grate filter perform estim improv signal nois ratio i ii origin intens posit imag intens posit motion blur imag intens imag generat adapt tune procedur extent compar use simul discuss section iv curv it appar situat optimum valu weigh factor keep weight close optimum valu result constant output adapt scheme thus enabl optim deblur smear imag move object hand start point view time constant remain fix throughout filter process tune gradient weight veloc order produc constant output demonstr figur dash line show strong differ output fix time constant system spatial process constant weight figur 4b other word tune time constant propos section result i realize blur-const criterion formul previous consequ possibl deblur obtain imag optim one weigh factor gradient final spatial process layer whole heterodyn veloc rang comput simul result conclus imag qualiti improv algorithm develop in present contribut implement general purpos dg eclips sjl40 minicomput two dimension simul figur sa show undisturb imag consist line pixel bit intens resolut figur sb show happen origin imag psf model accord exponenti decay in case time constant spatial inform process channel kept fix again inform content in higher spatial frequenc reduc larg implement heterodyn filter procedur now done follow first adapt rang defin set rang veloc mean adapt heterodyn algorithm suppos oper adequ within thus defin veloc rang in rang time constant tune accord relationship alway come larger minimum valu min demonstr purpos we set q=i in eq thus introduc phenomenon veloc two dimension set spatial filter time constant tune veloc alway produc a constant output independ veloc introduc motion blur figur sc show represent it import note constant output far wors qualiti ani set filter smallest fix time constant min would produc for veloc within the oper rang the advantag a veloc independ output this level in simul model in the next stage a differenti scheme implement discuss in detail in the preced paragraph constanc the weigh factor use in this differenti process scheme guarante the veloc independ the obtain imag represent figur sd show the result the differenti oper optim gradient weigh factor this weigh factor optim base on almost ident perform curv as describ previous in figur a clear good restor appar this figur though close inspect reveal fine structur especi for area high intens unrel the origin intens distribut these artifact caus the phenomenon for these high intens area possibl tune error show much more pronounc for low intens fig.8a fig.8b 8c fig.8d a origin bit pictur motion degrad imag a psf deriv from is kept fix pixel the motion blur extent is pixel worst case the result motion degrad the origin imag a psf as in figur 8b tune the time constant base on the veloc restor version of the degrad imag use the heterodyn adapt process scheme in conclus a heterodyn adapt imag process techniqu inspir the fli visual system present as imag devic for move object a scalar differ equat model use repres the motion blur degrad process base on the experiment result describ and on this state space model we develop an adapt filter scheme produc at a certain level within the system a constant output permit differenti oper in order produc an optim deblur represent of the move object acknowledg the author wish to thank mt eric bosman for expert program assist mr franco tommasi for mani inspir discuss and advis the implement of the simul model and dr rob de ruyter van steveninck for experiment help this research part support by the netherland organ lor the advanc pure research the foundat sticht voor biolysica
----------------------------------------------------------------

title: 49-connecting-to-the-past.pdf

connect past bruce a. macdonald assist professor knowledg scienc laboratori comput scienc depart univers calgari univers drive nw calgari alberta t2n in4 abstract recent renew interest neural-lik process system evidenc exampl two volum parallel distribut process edit rumelhart mcclelland discuss parallel distribut system connectionist model neural net valu pass system multipl context system dissatisfact symbol manipul paradigm artifici intellig seem part respons attent encourag promis massiv parallel system implement hardwar paper relat simpl neural-lik system base multipl context well-known formalisms-nam product system k-iength sequenc predict finite-st machin ture machines-and present earlier sequenc predict result new light introduct reviv neural net research strong exemplifi recent rumelhart mcclelland new journal number meet net also describ parallel distribut system connectionist model valu pass systems3 multipl context learn systems4 symbol manipul paradigm artifici intellig seem success hope seem last real promis massiv parallel system implement hardwar howev flurri new work import consolid new idea place solid alongsid establish one paper relat simpl neural-lik system well-known notions-nam product system k-iength sequenc predict finite-st machin ture machines-and present earlier result abil network new light general form connectionist system lo simplifi three layer net binari fix weight hidden layer therebi avoid mani difficulties-and challengesof recent work neural net hidden unit weight regular pattern use templat sophist expens learn algorithm avoid simpl method use determin output unit weight way gain advantag multilay net retain simplic two layer net train method certain noth lost comput power-a i explain-and limit two layer net carri simplifi three layer one biolog system may similar avoid need learn algorithm simul anneal method common use connectionist model one thing biolog system clear distinguish train phase briefli simplifi net product system implement three layer neuron-lik unit output layer input layer hidden layer product hidden product unit potenti connect predetermin set input output k-iength sequenc predictor form ie level delay unit introduc input layer k-iength predictor unabl distinguish simpl sequenc ba aa sinc ie charact system forgotten whether appear first k-iength predictor augment auxiliari action abl learn regular languag sinc auxiliari action equival state input aamong 1st intern confer neural net san diego ca june con.fer brough equival singl context system andrea 's multipl context system see also macdonald american institut physic figur general form connectionist system form unit oper within unit in~ut excitation-.i weight sum activ w output typic typic product unit enabl predict depend previous state combin sever augment sequenc predictor thring machin tape simul along finite-st control give net comput power univers ture machin relat simpl neural-lik system lack comput abil previous implement abil product system equival simplifi net organ paper next section briefli review general form connectionist system section simplifi section explain result equival product system deal input output net section extend simplifi version enabl learn predict sequenc section explain comput power sequenc predictor increas thring machin input unit receiv auxiliari action fact system learn ture machin section discuss possibl number net combin output form overal net associ area general form connectionist system figur show general form connectionist system unit neuron ce1l figur unit input output oj possibl unit network output net input excit net weight sum input vij weight connect output unit input unit activ unit function fi net input excit typic fi semilinear non-decreas differenti function least larg group unit output function fi activ typic kind threshold function i assum quantiti vari discret time step exampl activ time given fi neti general restrict connect may made unit unit connect direct input output hidden unit complex net describ paper may one type connect figur show common connect topolog three layer units-input hidden output-with cycl connect net train present input combin along desir output combin train system produc desir output given figur basic structur three layer connectionist system input unit hidden unit output unit input dure train weight adjust fashion reduc discrep desir actual output general method lo desir train activ equat general form hebb 's classic rule adjust weight two unit high activ lo weight adjust product two function one depend desir actual activ often difference-and anoth depend input weight weight simpl exampl suppos differ output oj weight chang product output error input excit weight constant determin learn rate widrow-hoff delta rule may use net without hidden unit 1o import contribut recent work connectionist system implement equat hidden unit train signal ti direct avail boltzmann learn method iter vari weight hidden unit train activ use control gradual decreas random method simul anneal backpropag also iter perform gradient descent propag train signal error back net hidden unit i avoid need determin train signal hidden unit fix weight hidden unit section simplifi system assum simplif made general connectionist system section system three layer topolog shown figur ie cycl hidden layer unit weight fix say uniti zero unit linear threshold unit lo mean activ function unit ident function give net weight sum input output function simpl binari threshold form i output threshold activ output binari oft hidden unit threshold requir input activ output activ like gate output unit threshold requir two activ high weight input output generat like gate keep product system view net explain section learning-which occur output unit weights-giv weight adjust accord wij wij oj otherwis weight turn input unit output otherwis wij oj simpl exampl given figur section simpl form net made probabilist replac adjust weight wij estim condit probabl unit output output wij estim p odoj assum independ input unit output unit turn condit probabl occurr output exceed threshold output function onc simplif made need learn hidden unit also iter learn requir weight either assign binari valu estim condit probabl paper present characterist simplifi net section discuss motiv simplifi neural net way product system simplifi net kind simpl product system product system compris global databas set product rule control system databas net system interact provid input reaction output t.he net hidden unit network product rule form precondit action precondit satisfi input excit exceed threshold hidden unit action repres output unit hidden product unit activ control system product system choos rule whose action perform set rule whose precondit met neural net control system distribut throughout net output unit exampl output unit might form winner-takeal net product system complex control involv forward backward chain choos action seek goal discuss elsewhere4.12.16 figur illust.r simpl product implement neural net figur show input hidden unit element precondit appropri input combin present associ hidden product unit fire onc weight leam connect hidden unit output unit fire product result output simplifi neural net direct equival product system whose element input output product system symbol element variabl given valu product action neural net direct implement sinc output predetermin set howev see later extens t.o framework enabl abil cthis might refer sensory-motor product system sinc implement ill l'eal system robot deal sens input execut motor action may includ auxiliari action section figur product implement simplifi neural net product rule icloudi i ipressur fall i then iit rain i rule implement hidden unit threshold hidden unit gate threshold output unit gate learn weight if net probabilist otherwis estim p rainlcloud pressur fall rain weight figur net predict next charact sequenc base last charact net product unit hidden unit combin input unit exampl net could predict sequenc abcabcabc product form if last charact then next charact learn rule wij if inputj outputi output wijoj input neural net output learn procedur clamp input output desir valu system calcul weight valu repeat requir input/output combin sequenc predict product system neural net predict sequenc given exampl repeat sequenc product learn predict futur event basi recent one figur show trivial simpl sequenc predictor predict next charact sequenc base previous one figur also give detail learn procedur simplifi net net need train onc input combin then predict output everi charact seen current one probabilist form net would estim condit probabl next charact condit current one mani figur use delay input neural net implement k-length sequenc predictor net last three charact input input hidden output 2nd last exampl product if last three charact then present possibl charact pair would need proper estim probabl net would learn probabl distribut charact pair predictor like one figur extend general k-iength predictor long input delay step avail then illustr figur 3-length predict hidden product unit repres possibl combin symbol again output weight train respond previous seen input combin three charact delay provid dedic neural net shown figur note net assum synchron updat input feedback around unit chang one step output chang various way implement delay neuron andrea investig purpose-delay inputs-in detail simul similar net work sequenc predict neural net feldman ballard find connectionist system initi suit repres chang time one form chang sequenc suggest two method repres sequenc net first unit connect each sequenc sequenti task repres fire these unit success second method buffer input time input recent past avail well current input delay input avail suggest import differ necessari length buffer feldman ballard suggest buffer long enough hold phrase natur languag i expect use buffer longer andrea symbol input repres complex inform effect give length seven buffer inform recent seven simpl input discuss section method back-propagation13 enabl recurr network learn sequenti task dfeldman ballard2 give dedic neural net connect varieti flulction figur input delay dedic neural subnet two stage delay shown delay network time diagram tml origin signal delay one step delay two step manner similar first suggest last paragraph sequenc connect unit repres sequenc event one exampl net learn complet sequenc charact given first two charact six charact sequenc next four output error must propag around cycl recurr net number time serial may also achiev sequenc state distribut activ exampl net play side tic-tac-to game sequenti natur net 's behavior deriv sequenti natur respons net 's action tic-tac-to move net model sequenc intern model sequenti part environ exampl tic-tac-to play net model oppon k-iength sequenc predictor unabl learn sequenc repeat frequent everi charact their k-iength context includ inform last event howev there two way inform kth last input retain net first method latch input second involv auxiliari action latch unit input latch held indefinit use combin shown figur input would normal latch andrea discuss this techniqu thread latch event among non-latch event give net inform arbitrarili far back input-output histori inform immedi past briefli sequenc ba distinguish aa if first charact latch howev this ad hoc solut this problem auxiliari action output fed back net input signal this enabl system choos next output least part base previous one indic figur if particular fed back output also one without extern manifest whose extern manifest independ task perform then output auxiliari action las interest reader refer andrea extens analysi given figur thread latch circuit rememb event anoth come along this two input latch two letter number unit may similar connect form mutual inhibit layer winner-take-al connect along posit feedback keep select output activ when input disappear figur auxiliari actions-th outputs-ar fed back input net enabl net rememb state here part net exampl product shown there two type action charact action sinput output charact input if input charact input charact output then output charact llij ill direct effect task system perform sinc it evok relev input use net symbol action if auxiliari action latch input then symbol inform rememb indefinit lost when anoth auxiliari action kind input take latch thus auxiliari action act like rememb state system perform action remind particular state figur illustr this system that predict charact state chang given previous charact state an obvious candid auxiliari action speech blank oval figur would repres net 's environ speech action heard although it extern manifest speech direct effect physic interact the world symbol abil provid the power auxiliari action also includ other speaker the interact simul abstract automata the exampl figur give the essenc simul finit state automaton product system or it neural net equival it illustr the transit function an automaton the new state output function the previous state input thus neural net simul finit state automaton long it addit auxiliari action thring machin finit state automaton control plus an unbound memori neural net could simul a lure machin two way way demonstr product system implementations-equival neural net all multipl context learn system briefli explain section the first thring machin simul the system simul the finit state control abl use an unbound extern memori fsee john andrea 's colleagu work4 figur multipl context learn system implement multipl neural net each:3 layer net the simplifi form present a number elabor extra connect goal-seek forward backward chain output channel the real world much like the paper ture 's origin work the second simnlat.ion emb the memori the multipl context learn system along a counter access this simul memori both learn the productions-equival learn output unit weights-requir the simul the second abl add intern memori requir a limit depend the size the network easili larg enough allow year comput the second could also employ extern memori the first briefli the second simul compris multipl sequenc predictor predict auxiliari action rememb the state the control the current memori posit the memori element updat relearn the product repres that element the precondit the address the product action the store item multipl system form associ area a multipl context learn system product system version a multipl neural net although a simpl version implement a simul net it effect compris sever net or associ areas-which may output input in common indic in figur hidden unit weight specifi templat one each net a templat give the input a zero weight the hidden unit a net the input a weight uniti delay latch input also avail the actual output select the combin predict the net in a winner-take-al fashion i see the design real neural net say control real robot requir a larg degre predetermin connect a robot control could one three layer net wit.h everi input connect everi hidden unit in turn connect everi output there need some connect constraint so the net reflect the function special in the control requir the multipl context learn system all the hidden layer connect predetermin allow output connect learn this avoid the credit assign problem therefor also the need learn algorithm boltzmann learn back-propag howev as the multipl context learn system auxiliari action delay latch input it lack comput power futur work in this area investig for exampl the abil differ kind net learn auxiliari act.ion this may difficult as symbol action may provid in train input and output. for exampl a control for a robot bodi would deal vision manipul motion etc conclus this paper present a simplifi three layer connectionist model fix weight for hidden unit delay and latch for input sequenc predict abil auxiliari state action and the abil use intern and extern memori the result abl learn to simul a ture machin simpl neural-lik system not lack comput power acknowledg this work support by the natur scienc and engin council canada
----------------------------------------------------------------

title: 32-synchronization-in-neural-nets.pdf

synchron neural net jacqu j. vidal univers california los angel los angel ca john haggerti abstract paper present artifici neural network concept synchroniz oscil network instant individu fire form point process constitut form inform transmit join neuron type communic contrast assum model typic continu discret value-pass network limit messag receiv process unit time marker signal fire unit present signific implemen tation advantag model neuron fire spontan regular absenc perturb interact present schedul fire advanc delay fire neighbor neuron network neuron becom global oscil exhibit multipl synchron attractor arbitrari initi state energi minim learn procedur make network converg oscillatori mode satisfi multi-dimension constraint network direct repres rout schedul problem consist order sequenc event introduct most neural network model deriv variant rosenblatt origin perceptron value-pass network case particular network propos fukushima i hopfield rumelhart mani other everi case input process element either binari continu amplitud signal weight synapt gain subsequ sum integr result activ pass sigmoid threshold filter produc continu quantiz output may becom input neuron behavior model relat live neuron even fall consider short account complex inde observ mani real neuron action potenti spike fire propag axon branch intern activ reach threshold higher john haggerti interact system los angel w. 6th st. la ca american institut physic input rate level result rapid fire behind tradit model assumpt averag frequenc action potenti carrier inform neuron becaus integr fire individu neuron consid effect extent contribut averag intens therefor assum activ simpli frequenc code exact time individu fire ignor view howev cover well known aspect neural communic inde precis time spike arriv make crucial differ outcom neural interact one classic exampl pre-synapt inhibit widespread mechan brain machineri sever studi also demonstr occurr function import precis time phase relationship cooper neuron local network model present paper contrast one mention network fire consid individu output event input side node fire node presynapt neuron either delay inhibit advanc excit node fire seen earlier type neuron interact would call phase-modul engin system also find rational experiment neurophysiolog neurophysiolog plausibl howev major concern rather propos explor potenti use mechan parallel distribut comput merit approach artifici neural network digit puls use internod communic instead analog voltag model particular well suit time-ord sequenc found larg class rout trajectori control problem neuron synchroniz oscil model process element neuron relax oscil built-in self-inhibit relax oscil dynam system capabl accumul potenti energi threshold breakdown point reach point energi abrupt releas new cycl begin descript fit dynam behavior neuron membran rich structur empir model this behavior found well-establish differenti formul hodgkin huxley simplifi version given fitzhugh7 differenti equat account foundat neuron activ also capabl repres subthreshold behavior refractori follow fire membran potenti enter critic region abrupt depolar collaps potenti differ across membran occur follow somewhat slower recoveri this brief electr short membran call action potenti spike constitut output event neuron if caus initi depolar maintain oscil limit-cycl develop generat multipl fire depend input level membran paramet oscil limit singl spike may produc oscillatori burst even continu sustain activ present model share general properti use much simpler descript relax oscil illustr figur activ energy exdt3toij oj input injrjh1~olj input perturb utl intemilf l neju inpul ty figur relax oscil perturb input fire occur energi level reach critic level ec assum constant rate energi influx fire occur natur period ec t=a when pre-synapt puls imping on cours energi accumul fire schedul disturb let repres instant last fire cell tj intant imping arriv cell e act wj uo til ec uo repres unit impuls dramat complex synchron dynam appreci consid simplest possibl case master slave interact two regular fire oscil unit natur period ta tb instant fire unit unidirect send spike signal unit receiv interv measur from last time fire upon recept spike transform quantum energi 6e depend upon post-fir arriv time relationship shape repres refractori post-spik properti here it assum simpl ramp function if interact inhibitori consequ this arriv next fire unit delay respect schedul would absenc perturb posit interv figur becaus shape delay action nil immedi fire becom longer imping pre-synapt spike arriv later interv if interact excitatori delay negat le shorten natur fire interv general assumpt regard function tend synchron within given rang coupl gain phase self-adjust equilibrium achiev given this equilibrium correspond distribut maximum entropi point cell receiv amouint activ common cycl i i inhibit excit figur relationship phase delay when input effici increas linear after-spik interv synchron dynam present attractor ration frequenc pair ratio associ rang stabil ratio lowest cardin wide zone phaselock figur wider stabil wnes correspond one one ratio fa fb invers ta tbl kohn segundo demonstr such phase lock occur live invertebr neuron point paradox natur phase-lock inhibit within stabil region take appear excit sinc small increas input fire rate local result increas output rate area these rang stabil appear unstabl transit fact as recent point out bak9 form infin lock step known as devil 's staircas correspond infin intermedi ration pair figur bak show staircas self-similar scale transit form fractal cantor set fractal dimens univers constant dynam system i excit inhibiti~~v i li l figur unilater synchron constraint satisfact in oscil network global synchron interconnect network mutual phase-lock oscil constraint satisfact problem synchron equilibrium node fire in interlock pattern organ inter-spik interv integ ratio often cite travel salesman problem archetyp for a class import hard problem a special case when ratio must node must fire at frequenc here the equilibrium condit everi node accumul the the amount energi the global cycl furthermor the fire must order along a minim path use stochast energi minim simul anneal the first simul demonstr the feasibl the approach with a limit number node the tsp isomorph mani sequenc problem involv distribut constraint fall the oscil array neural net paradigm in a particular natur way work pursu rigor establish the limit applic the model i annea/./ng figur the travel salesman problem in the global oscil oj minim energi node constrain fire at the rate in the order correspond to the minim path acknowledg research support in part aerojet electro-system under the aerojet-ucla cooper research master agreement no nasa nag
----------------------------------------------------------------

title: 54-a-method-for-the-design-of-stable-lateral-inhibition-networks-that-is-robust-in-the-presence-of-circuit-parasitics.pdf

method design stabl later inhibit network robust presenc circuit parasit j.l wyatt jr d.l standley depart electr engin comput scienc massachusett institut technolog cambridg massachusett abstract analog vlsi implement neural system sometim conveni build later inhibit network use local connect on-chip resist grid serious problem unwant spontan oscil often aris circuit render unus practic paper report design approach guarante system stabl even though valu design element parasit element resist grid may unknown method base rigor somewhat novel mathemat analysi use tellegen theorem idea popov multipli control theori thorough practic criteria local sens overal analysi interconnect system requir empir sens involv measur frequenc respons data individu cell robust sens unmodel parasit resist capacit interconnect network affect analysi i introduct term later inhibit first aros neurophysiolog describ common form neural circuitri output neuron popul use inhibit respons neighbor perhap best understood exampl horizont cell layer vertebr retina later inhibit simultan enhanc intens edg act automat lain control extend dynam rang retina whole principl use design artifici neural system algorithm kohonen other electron design neural chip carver mead vlsi implement neural system conveni build later inhibit network use local connect on-chip resist grid linear resistor fabric polysilicon yield compact realize nonlinear resist grid made mos transistor found use imag segment network type divid two class feedback system feedforward-on system feedforward case one set amplifi impos signal voltag american institut physic current grid anoth set read result respons subsequ process amplifi write grid read feedback arrang feedforward network type inher stabl feedback network need practic exampl one carver meadl retina chips3 achiev edg enhanc mean later inhibit resist grid figur show singl cell continuous-tim version chip note capacitor voltag affect local light intens incid cell capacitor voltag neighbor cell ident design cell drive neighbor drive distant neighbor origin cell turn thus necessari ingredi instabl activ element signal feedback present system fact continuous-tim version oscil bad origin design scarc usabl practic later inhibit path enabl oscil i incid light figur photoreceptor signal processor circuit use two mos transconduct amplifi realiz later inhibit communic similar unit resist grid readili occur resist grid circuit activ element feedback even individu cell quit stabl analysi condit instabl straightforward method appear hopeless sinc repeat array contain mani cell influenc mani other direct indirect influenc turn number simultan activ feedback loop enorm paper report practic design approach rigor guarante system stabl simplest version idea intuit obvious design individu cell although intern activ act like passiv system seen resist grid circuit theori languag design goal celli output imped positive-r function sometim difficult practic show origin network satisfi condit absenc certain parasit element import perhap condit one verifi experiment frequency-respons measur physic appar collect cell appear passiv termin form stabl system interconnect passiv medium resist grid research contribut report summari form demonstr passiv positive-r condit much stronger actual need weaker condit easili achiev practic suffic guarante stabil linear network model ii extens nonlinear domain furthermor rule large-sign oscil certain condit ii first-ord linear analysi singl cell begin linear analysi elementari model circuit initi approxim output admitt cell simplifi topolog without loss relev inform use naive'model transconduct amplifi shown figur simplifi network topolog transconduct amplifi model circuit capacitor absorb co2 straightforward calcul show output admitt given yes positive-r passiv admitt sinc alway realiz network form shown ro2 gmlgm2rol coi/gmlgm2 although origin circuit contain inductor realize capacitor inductor thus capabl damp oscil nonetheless transamp model perfect accur network creat interconnect cell resist grid parasit capacit could exhibit sustain oscil element valu may typic practic model light damp reson around i khz this disturb high suggest cell high sensit parasit element captur simpl model our preliminari rl yes figur passiv network realize output admitt circuit analysi much complex model extract physic circuit layout creat carver mead 's laboratori indic output imped passiv valu transamp bias current definit explan instabl await more care circuit model effort perhap design on-chip imped measur instrument iii positive-r function e-positlv function stabil linear network model follow discuss cr+jw complex variabl ration function ratio polynomi real coeffici assum simplic pure imaginari pole term close right hale plane refer set complex number re def i function said positive-r pole right half plane re h jw know outset right half plane pole def i reduc simpl graphic criterion positiver nyquist diagram plot h jw lie entir close right half plane note positive-r function necessarili stabl sinc right half plane pole stabl function necessarili positive-r exampl show deep link posit real function physic network passiv establish classic result linear circuit theori state positive-r possibl synthes 2-termin network posit linear resistor capacitor inductor ideal transform driving-point imped admitt oef function said a-posit particular valu e e pole right half plane nyquist plot lie strict right straight line pass origin angl real posit axi note everi a-posit function stabl function e-posit necessarili positive-r i re g jw figur nyquist diagram function a-posit positive-r exampl function a-posit stabl but positive-r sinc nyquist diagram shown cross left half plane import e-posit function lie follow observ interconnect passiv linear resistor capacitor cell stabl linear imped result unstabl network instabl result imped also positive-r a-posit imped form larger class positive-r one henc a-posit less demand synthesi goal theorem show instabl result imped a-posit even positive-r theorem consid linear network arbitrari topolog consist number passiv 2-termin resistor capacitor arbitrari valu driven number activ cell output imped activ cell a-posit common network stabl proof theorem reli lemma lemma a-posit fix close first quadrant complex plane h lie strict right straight line pass origin angl real posit axi re im proof lemma outlin let function assign close right half plane perpendicular distanc des line defin def note des harmon close right half plane sinc analyt follow applic maximum modulus principle8 harmon function take minimum valu boundari domain imaginari axi this establish lemma proof theorem outlin network unstabl margin stabl natur frequenc close right half plane so natur frequenc network equat nonzero solut so let denot complex branch current solut tellegen i theorern9 sum complex power absorb circuit element must vanish solut iik12/s0ck capac~t cell termin pair second term delet special case so=o sinc complex power capacitor vanish so=o if network natur frequenc close right half plane must one close first quadrant sinc natur frequenc either real els occur complex conjug pair but satisfi so close first quadrant we see divid side iiki2 sum taken network branch after this divis assert zero convex combin term form rk term form ckso -i term form zk so visual term lie complex plane first set lie real posit axi second set lie close adrant sinc so lie close 1st quadrant assumpt third set lie right a line pass origin angl a lemma thus term lie strict right this line impli convex combin equal zero henc network stabl iv stabil result network nonlinear resistor capacitor previous result linear network afford limit insight behavior nonlinear network first nonlinear equat linear equilibrium point theorem appli linear model if linear model stabl equilibrium point origin nonlinear network local stabl network return equilibrium point if initi condit suffici near it but result this section contrast appli full nonlinear circuit model allow one conclud certain circumst network oscil even if the initi state arbitrarili far the equilibrium point def a function describ in section iii said tc satisfi the popov criterion lo if exist a real number r o such re l+jwr note posit real function satisfi the popov criterion the reader easili verifi in exam~l i satisfi the popov criterion a rang valu the import effect the term l+jwr in def rotat the nyquist plot counterclockwis progress greater amount increas theorem consid a network consist nonlinear 2-termin resistor capacitor cell linear output imped suppos the resistor curv character continu diffefenti function gk vk gk o gk vk valu of vk ii the capacitor character by ck vk ~k with ci ck v c2 valu of vk iii the imped zk pole in the close right half plane satisfi the popov criterion common valu of if condit satisfi the network stabl in the sens that for ani initi condit oo i branch dt the proof base tellegen 's theorem is rather involv omit and appear elsewher it acknowledg we sincer thank professor carver mead of cal tech for enthusiast support this work and for make it possibl for us present earli report it in this confer proceed this work supportedj defens advanc research project agenc the offic of naval research arpa order no contract no and defens advanc research project agenc darpa contract no
----------------------------------------------------------------

title: 56-discovering-structure-from-motion-in-monkey-man-and-machine.pdf

discov strucfur motion monkey man machin ralph m. siegel salk institut biolog la jolla ca abstract abil obtain three-dimension structur visual motion import surviv human non-human primat use parallel process model current work explor biolog visual system might solv problem neurophysiologist might go understand solut introducnon psychophys experi shown monk man equal adept obtain three dimension structur motion present work much effort expend mimick visual system done one main reason model design help direct physiolog experi primat hope approach understand model could develop approach could direct primat 's visual system earli centuri von helmholtz2 describ problem extract three-dimension structur motion suppos instanc person stand still thick wood imposs distinguish except vagu rough mass foliag branch around belong one tree anoth far apart separ tree etc moment begin move forward everyth disentangl immedi get appercept materi content wood relat space look good stereoscop view object move rather observ percept threedimension structur motion still obtain object-cent structur motion examin report lesion studi monkey demonstr two extra-stri visual cortic call middl tempor area abbrevi mt current address laboratori neurobiolog rockefel univers york avenu new york ny american institut physic medial superior tempor area involv obtain structur motion present model meant mimic v5-mst part cortic circuitri involv obtain structur motion model attempt determin ifth visual imag correspond three-dimension object tile strucfur motion stimulus problem model solv pose studi monkey man structur unstructur motion display hollow orthograph project cylind comput figur cylind rotat vertic axi unstructur stimulus generat shuffl veloc vector random display screen overal veloc spatial distribut two display ident spatial relationship chang unstructur stimulus human subject report point move surfac hollow cylind view structur stimulus unstructur stimulus subject report sens three-dimension structur b. orthograph project rotat cylind unstructur display figur structur unstructur motion stimulus pomt random place surfac cylind point orthograph project motion give strong percept hollow cylind unstructur stimulus generat shuffl veloc vector random screen functionalarchitecfureoftilemodel primat subject model requir indic whether display structur subject requir describ shape veloc size cylind thus output cell model signal cell i mean process unit model may correspond singl neuron group neuron term neuron refer actual wetwar brain structur structur output layer correspond cortic area mst macaqu monkey appear sensit global organ motion image5 known if mst neuron distinguish structur unstructur imag input model base physiolog studi maca~u monkey neuron area v5 retinotop represent visual space retinotop locat encod wide rang velocitiess thus model 's input rep1 resent cell cil repres differ combin veloc retinotop spatial posi0cil tion furthermor motion veloc neuron v5 center-surround oppon organization9 width recept field taken data albright retin posit deg s. typic recept field model shown figur figur recept field input layer cell optim veloc lt possibl determin activ input cell would rotat cylind given represent activ pattern set input cell comput convolv veloc point differ gaussian activ input cell imag point angular veloc so/sec present figur relinotop map retinotop map structur structur figur input cell 's activ pattern structur unstructur stimuius circl correspond cell input layer contour com pute use linear interpol individu cell horizont axi correspond posit along horizont meridian vertic axi correspond speed along horizont meridian thus activ cell upper right hand corner graph correspond veloc sec toward right locat right along horizont meridian inspect input pattern suggest problem detect three-dimension structur motion may reduc pattern recognit task problem given spars sampl input motion flow field determin whether correspond best structur unstructur object itwa next necessari determin connect two input output layer model abl correct signal structur structur wide rang cylind radii rotat veloc parallel distribut network type use rosenberg sejnowski provid function architectur figur i figur parallel architectur use extract structur motion input layer correspond area map posit speed along horizont axi output layer correspond area mst propos signal structur middl layer may exist either v5 mst input layer cell fulli connect middl layer cell middl layer cell repres intermedi stage process may either v5 mst cell middl layer fulli connect output cell input cell lower layer next higher level sum linear threshold use hill equat weight layer initi chosen valu weight adjust use back-propag method steepest descent network would learn correct predict structur input imag model learn correct perform task iter figur figur educ network perform structur motion problem iter number plot mean squar error error defin differ model 's predict known structur model train set structur unstructur cylind wi e rang adii number point rotatlon veloc iter number psychophys perform model model 's perform compar monkey man respect fraction structur number point display figur model inde perform global analysi shown allow model view portion imag like man monkey model 's perform suffer thus appear model 's perform quit similar known monkey human psychophys output monkey man machin monkey man machin fraction structur number point figur psychophys perform model effect vari fraction structur fraction structur increas the model 's perform improv thirti repetit averag valu structur the model the fraction structur defin rs the radius shuffl the motion vector rc the radius the cylind the human monkey data taken psychophys studi done the model similar perform monkey man next possibl examin artifici network order obtain hint studi the biolog system follow the approach electrophysiologist recept field map all the cell the middl ou tput layer made activ individu inpu cell the recept field middl layer cell shown figur the layout map quit similar figur howev the activ one cell the middl layer plot function the locat speed motion stimulus the input layer one could imagin electrod place one the cell the middl layer the experimentalist move bar the horizont meridian differ locat speed the activ the cell plot function posit space relinolop map rj i figur the activ two differ cell the middl layer activ plot contour map function horizont posit speed dot line indic inhibit middl layer recept field map interest appear quit simpl symmetr the inhibitori central region the recept field surround by excitatori region figur complementari cell also found other inhibitori band adjac excitatori band figur the result suggest neuron involv extract structur motion may relat simpl recept field the spatial veloc domain recept field might thought break the imag compon part basi set correct recombin second order cell could use detect the presenc three-dimension structur the output cell also simpl recept field interest symmetri figur howev the recept field analysi insuffici indic the role the cell therefor order proper understand the mean the cell 's recept field it necessari use stimuli real world relev case the structur motion stimuli the output cell would give maxim respons cylind stimulus present figur the recept field map the output layer cell noth recept field structur indic the cell involv obtain structur motion work predict neuron cortex involv extract structur motion relat simpl recept field order test this hypothesi it necessari make care map these cell use small patch motion figur known qualit result area v5 mst consist but prove this hypothesi as well it necessari use relev stimuli three-dimension object if simpl recept field inde use structur motion support found the idea simpl cortic circuit center-surround use for mani differ visual analys motion patch consist random dot with variabl veloc ru fix point figur it may necessari make care map these neuron use small patch motion order observ the postul simpl recept field properti cortic neuron involv extract structur motion such structur may appar use hand move bar stimuli discuss conclus it possibl extract the three-dimension structur rotat cylind use a parallel network base a similar function architectur as found primat cortex the present model similar psychophys monkey man the recept field structur underli the present model simpl view use a spatial-veloc represent it suggest order understand the visual system extract structur motion quantit spatial-veloc map cortic neuron involv need made one also need use stimuli deriv from the real world in order understand how may use in visual field analysi there similar the shape the recept field involv in analyz structur from motion recept field in striat cortex it may similar cortic mechan and connect use perform differ function in differ cortic area last this model demonstr the use parallel architectur close model the cortic represent is a comput effici mean solv problem in vision thus as a final caveat i would like advis the creator of network solv etholog realist problem use solut evolut provid
----------------------------------------------------------------

title: 32-synchronization-in-neural-nets.pdf

synchron neural net jacqu j. vidal univers california los angel los angel ca john haggerti abstract paper present artifici neural network concept synchroniz oscil network instant individu fire form point process constitut form inform transmit join neuron type communic contrast assum model typic continu discret value-pass network limit messag receiv process unit time marker signal fire unit present signific implemen tation advantag model neuron fire spontan regular absenc perturb interact present schedul fire advanc delay fire neighbor neuron network neuron becom global oscil exhibit multipl synchron attractor arbitrari initi state energi minim learn procedur make network converg oscillatori mode satisfi multi-dimension constraint network direct repres rout schedul problem consist order sequenc event introduct most neural network model deriv variant rosenblatt origin perceptron value-pass network case particular network propos fukushima i hopfield rumelhart mani other everi case input process element either binari continu amplitud signal weight synapt gain subsequ sum integr result activ pass sigmoid threshold filter produc continu quantiz output may becom input neuron behavior model relat live neuron even fall consider short account complex inde observ mani real neuron action potenti spike fire propag axon branch intern activ reach threshold higher john haggerti interact system los angel w. 6th st. la ca american institut physic input rate level result rapid fire behind tradit model assumpt averag frequenc action potenti carrier inform neuron becaus integr fire individu neuron consid effect extent contribut averag intens therefor assum activ simpli frequenc code exact time individu fire ignor view howev cover well known aspect neural communic inde precis time spike arriv make crucial differ outcom neural interact one classic exampl pre-synapt inhibit widespread mechan brain machineri sever studi also demonstr occurr function import precis time phase relationship cooper neuron local network model present paper contrast one mention network fire consid individu output event input side node fire node presynapt neuron either delay inhibit advanc excit node fire seen earlier type neuron interact would call phase-modul engin system also find rational experiment neurophysiolog neurophysiolog plausibl howev major concern rather propos explor potenti use mechan parallel distribut comput merit approach artifici neural network digit puls use internod communic instead analog voltag model particular well suit time-ord sequenc found larg class rout trajectori control problem neuron synchroniz oscil model process element neuron relax oscil built-in self-inhibit relax oscil dynam system capabl accumul potenti energi threshold breakdown point reach point energi abrupt releas new cycl begin descript fit dynam behavior neuron membran rich structur empir model this behavior found well-establish differenti formul hodgkin huxley simplifi version given fitzhugh7 differenti equat account foundat neuron activ also capabl repres subthreshold behavior refractori follow fire membran potenti enter critic region abrupt depolar collaps potenti differ across membran occur follow somewhat slower recoveri this brief electr short membran call action potenti spike constitut output event neuron if caus initi depolar maintain oscil limit-cycl develop generat multipl fire depend input level membran paramet oscil limit singl spike may produc oscillatori burst even continu sustain activ present model share general properti use much simpler descript relax oscil illustr figur activ energy exdt3toij oj input injrjh1~olj input perturb utl intemilf l neju inpul ty figur relax oscil perturb input fire occur energi level reach critic level ec assum constant rate energi influx fire occur natur period ec t=a when pre-synapt puls imping on cours energi accumul fire schedul disturb let repres instant last fire cell tj intant imping arriv cell e act wj uo til ec uo repres unit impuls dramat complex synchron dynam appreci consid simplest possibl case master slave interact two regular fire oscil unit natur period ta tb instant fire unit unidirect send spike signal unit receiv interv measur from last time fire upon recept spike transform quantum energi 6e depend upon post-fir arriv time relationship shape repres refractori post-spik properti here it assum simpl ramp function if interact inhibitori consequ this arriv next fire unit delay respect schedul would absenc perturb posit interv figur becaus shape delay action nil immedi fire becom longer imping pre-synapt spike arriv later interv if interact excitatori delay negat le shorten natur fire interv general assumpt regard function tend synchron within given rang coupl gain phase self-adjust equilibrium achiev given this equilibrium correspond distribut maximum entropi point cell receiv amouint activ common cycl i i inhibit excit figur relationship phase delay when input effici increas linear after-spik interv synchron dynam present attractor ration frequenc pair ratio associ rang stabil ratio lowest cardin wide zone phaselock figur wider stabil wnes correspond one one ratio fa fb invers ta tbl kohn segundo demonstr such phase lock occur live invertebr neuron point paradox natur phase-lock inhibit within stabil region take appear excit sinc small increas input fire rate local result increas output rate area these rang stabil appear unstabl transit fact as recent point out bak9 form infin lock step known as devil 's staircas correspond infin intermedi ration pair figur bak show staircas self-similar scale transit form fractal cantor set fractal dimens univers constant dynam system i excit inhibiti~~v i li l figur unilater synchron constraint satisfact in oscil network global synchron interconnect network mutual phase-lock oscil constraint satisfact problem synchron equilibrium node fire in interlock pattern organ inter-spik interv integ ratio often cite travel salesman problem archetyp for a class import hard problem a special case when ratio must node must fire at frequenc here the equilibrium condit everi node accumul the the amount energi the global cycl furthermor the fire must order along a minim path use stochast energi minim simul anneal the first simul demonstr the feasibl the approach with a limit number node the tsp isomorph mani sequenc problem involv distribut constraint fall the oscil array neural net paradigm in a particular natur way work pursu rigor establish the limit applic the model i annea/./ng figur the travel salesman problem in the global oscil oj minim energi node constrain fire at the rate in the order correspond to the minim path acknowledg research support in part aerojet electro-system under the aerojet-ucla cooper research master agreement no nasa nag
----------------------------------------------------------------

title: 10-a-mean-field-theory-of-layer-iv-of-visual-cortex-and-its-application-to-artificial-neural-networks.pdf

mean field theori layer iv visual cortex it applic artifici neural network christoph l. scofield center neural scienc physic depart brown univers provid rhode island nestor inc richmond squar provid rhode island abstract singl cell theori develop select ocular domin visual cortex present previous bienenstock cooper munrol extend network applic layer iv visual cortex paper present mean field approxim captur fair transpar manner qualit mani quantit result network theori final consid applic theori artifici neural network show signific reduct architectur complex possibl singl layer network mean field approxim consid singl layer network ideal neuron receiv signal outsid layer cell within layer figur activ ith cell network cell vector affer signal network receiv input fiber outsid cortic network matrix synaps mi intra-lay input cell transmit matrix cortico-cort synaps l. american institut physic affer signal m2 m1 mn figur general singl layer recurr network light circl lgn cortic synaps dark circl nonmodifi cortico-cort synaps expand respons th cell individu term describ number cortic synaps travers signal arriv synaps lij cell expand cj respons cell becom ci mi mj jl ljk mk 2ljk lkn mn note term contain factor form factor describ first order effect cell cortic transform signal mean field approxim consist estim factor constant independ cell locat assumpt impli cell network select pattern thus mi rather assumpt vector sum constant amount assum cell network surround popul cell repres averag possibl pattern prefer thus vector sum affer synapt state describ pattern prefer constant independ locat final assum later connect strength function i-j lij becom circular matrix lij lji lo constan respons cell becom i i defin spatial averag cortic cell activ averag number intracort synaps here manner similar theori magnet replac effect individu cortic cell averag effect though cortic cell replac effect cell figur note retain order synapt travers signal thus focus activ layer relax equilibrium mean field approxim therefor write mean field asum inhibitori affer signal lo network averag figur singl layer mean field network detail connect cell network replac singl nonmodifi synaps effect cell learn cortic network first consid evolut network accord synapt modif rule studi detail singl cell elsewher consid lgn cortic synaps site plastic assum maximum simplic modif cortico-cort synaps then lij follow denot spatial averag cortic cell cj denot time averag activ th cortic cell function cj discuss extens elsewher here we note cj describ function cell respons hebbian anti-hebbian region lead complex set non-linear stochast equat analyz partial elsewher general affer synapt state fix point stabl select unstabl fix point nonselect argument may general network mean field approxim mean field time depend compon vari averag network modifi synaps environment situat chang slowli compar chang modifi synaps singl cell then approxim we write cj mi we see map mi mica everi mj exist correspond map point mj satisfi origin equat mean field zero theori shown everi fix point mj exist correspond fix point mj select stabil properti fix point avail neuron suffici inhibit network ilo i suffici larg applic mean field network layer iv visual cortex neuron primari visual cortex normal adult cat sharpli tune orient elong slit light activ stimul either eye properti orient select binocular depend type visual environ experienc critic period earli postnat develop exampl depriv pattern input critic period lead loss orient select monocular depriv result dramat shift ocular domin cortic neuron respons exclus open eye ocular domin shift md best known intens studi type visual cortic plastic behavior visual cortic cell various rear condit suggest cell respond rapid environment chang other monocular depriv exampl cell remain respons close eye spite larg shift cell open eye singer found use intracellular record geniculo-cort synaps inhibitori interneuron resist monocular depriv synaps pyramid cell dendrit recent work suggest densiti inhibitori gabaerg synaps kitten striat cortex also unaffect md cortic period result suggest lgn cortic synaps modifi rapid other modifi relat slowli slow modif cortico-cort synaps excitatori lgncortic synaps excitatori cell may modifi primarili embodi these fact we introduc two type lgn cortic synaps modifi remain relat constant simpl limit we we assum simplic consist physiolog interpret these two type synaps confin two differ class cell left right eye similar synaps both zk given cell then binocular cell mean field approxim binocular term ital dl r explicit left right eye time averag signal arriv form lgn note contain term modifi non-modifi synaps al r condit monocular depriv anim rear one eye close sake analysi assum right eye close noise-lik signal arriv cortex right eye then environ cortic cell further assum left eye synaps reach select fix point select pattern then ixil linear analysi close eye follow method bcm local function employ show a. nmin ratio number modifi cell total number cell network asymptot state close eye synaps scale function meanfield due non-modifi inhibitori cortic cell scale state set proport non-modifi cell addit averag intracort synapt strength lo thus contrast mean field zero theori depriv eye lgn-cortic synaps go zero rather approach constant valu depend averag inhibit produc non-modifi cell way asymptot output cortic cell zero driven depriv eye howev lessen effect inhibitori synaps applic inhibitori block agent bicuculin reduc magnitud one could obtain respons depriv eye we find consist previous theori experi learn occur lgn-cortic synaps inhibitori cortico-cort synaps need modifi non-modifi lgn-cortic synaps requir mean field approxim artifici neural network mean field approxim may appli network cortico-cort feedback general function cell activ particular feedback may measur differ network activ memori network activ this way network may use content address memori we discuss properti mean field network equilibrium reach we focus detail time depend relax cell activ state equilibrium hopfield8 introduc simpl formal analysi time depend network activ this model network activ map onto physic system state neuron activ consid particl potenti energi surfac identif pattern occur activ relax nearbi minima energi thus mlmma employ site memori hopfield network neuron intra-lay connect requir order n2 this connect signific constraint practic implement system larg scale problem further hopfield model allow storag capac limit memori this result prolifer unwant local minima energi surfac recent bachmann propos model relax network activ memori activ pattern site negat charg activ caus test pattern posit test charg then this model energi function electrostat energi unit test charg collect charg memori site ill qj i xj i jl vector describ initi network activ caus test pattern xj site jth memori paramet relat network size this model advantag storag densiti restrict network size hopfield model in addit architectur employ connect order n. note stage in settl jl memori network activ xj feedback network cell scalar q. i jl i this quantiti integr measur distanc current network state store memori import this measur cell it singl virtual cell comput distanc in activ space current state store state result comput this then broadcast cell in network general idea detail activ cell in network need fed back each cell rather some global measur perform singl effect cell suffici in feedback discuss we discuss formal for analysi network ideal neuron base mean field approxim detail activ cell in network we find simpl assumpt concern spatial distribut pattern prefer cell allow great simplif the analysi in particular the detail activ the cell the network may replac a mean field in effect comput a singl effect cell further the applic this formal the cortic layer iv visual cortex allow the predict much learn in cortex may local the lgn-cortic synapt state cortico-cort plastic relat unimport we find in agreement experi monocular depriv the cortic cell drive closed-ey respons to zero chemic blockag the cortic inhibitori pathway would reveal non-zero closed-ey synapt state final the mean field approxim allow the develop singl layer model of memori storag that unrestrict in storag densiti requir a connect of order mxn this signific for the fabric of practic content address memori acknowleoo i would like to thank leon cooper for mani help discuss and the contribut made to this work this work support the offic of naval research and the armi research offic under contract and
----------------------------------------------------------------

