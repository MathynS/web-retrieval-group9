query sentence: Associative database
---------------------------------------------------------------------
title: 1-self-organization-of-associative-database-and-its-applications.pdf

767

SELF-ORGANIZATION OF ASSOCIATIVE DATABASE
AND ITS APPLICATIONS
Hisashi Suzuki and Suguru Arimoto
Osaka University, Toyonaka, Osaka 560, Japan
ABSTRACT
An efficient method of self-organizing associative databases is proposed together with
applications to robot eyesight systems. The proposed databases can associate any input
with some output. In the first half part of discussion, an algorithm of self-organization is
proposed. From an aspect of hardware, it produces a new style of neural network. In the
latter half part, an applicability to handwritten letter recognition and that to an autonomous
mobile robot system are demonstrated.

INTRODUCTION
Let a mapping f : X -+ Y be given. Here, X is a finite or infinite set, and Y is another
finite or infinite set. A learning machine observes any set of pairs (x, y) sampled randomly
from X x Y. (X x Y means the Cartesian product of X and Y.) And, it computes some
estimate j : X -+ Y of f to make small, the estimation error in some measure.
Usually we say that: the faster the decrease of estimation error with increase of the number of samples, the better the learning machine. However, such expression on performance
is incomplete. Since, it lacks consideration on the candidates of J of j assumed preliminarily. Then, how should we find out good learning machines? To clarify this conception,
let us discuss for a while on some types of learning machines. And, let us advance the
understanding of the self-organization of associative database .
. Parameter Type
An ordinary type of learning machine assumes an equation relating x's and y's with
parameters being indefinite, namely, a structure of f. It is equivalent to define implicitly a
set F of candidates of
(F is some subset of mappings from X to Y.) And, it computes
values of the parameters based on the observed samples. We call such type a parameter
type.
For a learning machine defined well, if F 3 f, j approaches f as the number of samples
increases. In the alternative case, however, some estimation error remains eternally. Thus,
a problem of designing a learning machine returns to find out a proper structure of f in this
sense.
On the other hand, the assumed structure of f is demanded to be as compact as possible
to achieve a fast learning. In other words, the number of parameters should be small. Since,
if the parameters are few, some j can be uniquely determined even though the observed
samples are few. However, this demand of being proper contradicts to that of being compact.
Consequently, in the parameter type, the better the compactness of the assumed structure
that is proper, the better the learning machine. This is the most elementary conception
when we design learning machines .

1.

. Universality and Ordinary Neural Networks
Now suppose that a sufficient knowledge on f is given though J itself is unknown. In
this case, it is comparatively easy to find out proper and compact structures of J. In the
alternative case, however, it is sometimes difficult. A possible solution is to give up the
compactness and assume an almighty structure that can cover various 1's. A combination
of some orthogonal bases of the infinite dimension is such a structure. Neural networks 1 ,2
are its approximations obtained by truncating finitely the dimension for implementation.

? American Institute of Physics 1988

768
A main topic in designing neural networks is to establish such desirable structures of 1.
This work includes developing practical procedures that compute values of coefficients from
the observed samples. Such discussions are :flourishing since 1980 while many efficient methods have been proposed. Recently, even hardware units computing coefficients in parallel
for speed-up are sold, e.g., ANZA, Mark III, Odyssey and E-1.
Nevertheless, in neural networks, there always exists a danger of some error remaining
eternally in estimating /. Precisely speaking, suppose that a combination of the bases of a
finite number can define a structure of 1 essentially. In other words, suppose that F 3 /, or
1 is located near F. In such case, the estimation error is none or negligible. However, if 1
is distant from F, the estimation error never becomes negligible. Indeed, many researches
report that the following situation appears when 1 is too complex. Once the estimation
error converges to some value (> 0) as the number of samples increases, it decreases hardly
even though the dimension is heighten. This property sometimes is a considerable defect of
neural networks .
. Recursi ve Type
The recursive type is founded on another methodology of learning that should be as
follows. At the initial stage of no sample, the set Fa (instead of notation F) of candidates
of I equals to the set of all mappings from X to Y. After observing the first sample
(Xl, Yl) E X x Y, Fa is reduced to Fi so that I(xt) = Yl for any I E F. After observing
the second sample (X2' Y2) E X x Y, Fl is further reduced to F2 so that i(xt) = Yl and
I(X2) = Y2 for any I E F. Thus, the candidate set F becomes gradually small as observation
of samples proceeds. The after observing i-samples, which we write
is one of the most
likelihood estimation of 1 selected in fi;. Hence, contrarily to the parameter type, the
recursive type guarantees surely that j approaches to 1 as the number of samples increases.
The recursive type, if observes a sample (x" yd, rewrites values 1,-l(X),S to I,(x)'s for
some x's correlated to the sample. Hence, this type has an architecture composed of a rule
for rewriting and a free memory space. Such architecture forms naturally a kind of database
that builds up management systems of data in a self-organizing way. However, this database
differs from ordinary ones in the following sense. It does not only record the samples already
observed, but computes some estimation of l(x) for any x E X. We call such database an
associative database.
The first subject in constructing associative databases is how we establish the rule for
rewri ting. For this purpose, we adap t a measure called the dissimilari ty. Here, a dissimilari ty
means a mapping d : X x X -+ {reals > O} such that for any (x, x) E X x X, d(x, x) > 0
whenever l(x) t /(x). However, it is not necessarily defined with a single formula. It is
definable with, for example, a collection of rules written in forms of "if? .. then?? .. "
The dissimilarity d defines a structure of 1 locally in X x Y. Hence, even though
the knowledge on f is imperfect, we can re:flect it on d in some heuristic way. Hence,
contrarily to neural networks, it is possible to accelerate the speed of learning by establishing
d well. Especially, we can easily find out simple d's for those l's which process analogically
information like a human. (See the applications in this paper.) And, for such /'s, the
recursive type shows strongly its effectiveness.
We denote a sequence of observed samples by (Xl, Yd, (X2' Y2),???. One of the simplest
constructions of associative databases after observing i-samples (i = 1,2,.,,) is as follows.

i

i"

I,

Algorithm 1. At the initial stage, let So be the empty set. For every i =
1,2" .. , let i,-l(x) for any x E X equal some y* such that (x*,y*) E S,-l and

d(x, x*) =

min
(%,y)ES.-t

d(x, x) .

Furthermore, add (x" y,) to S;-l to produce Sa, i.e., S, = S,_l U {(x"

(1)

y,n.

769

Another version improved to economize the memory is as follows.

Algorithm 2, At the initial stage, let So be composed of an arbitrary element
in X x Y. For every i = 1,2"", let ii-lex) for any x E X equal some y. such
that (x?, y.) E Si-l and
d(x, x?) =

min

d(x, x) .

(i,i)ES.-l

Furthermore, if ii-l(Xi) # Yi then let Si = Si-l, or add (Xi, Yi) to Si-l to
produce Si, i.e., Si = Si-l U {(Xi, Yi)}'
In either construction, ii approaches to f as i increases. However, the computation time
grows proportionally to the size of Si. The second subject in constructing associative
databases is what addressing rule we should employ to economize the computation time. In
the subsequent chapters, a construction of associative database for this purpose is proposed.
It manages data in a form of binary tree.

SELF-ORGANIZATION OF ASSOCIATIVE DATABASE
Given a sample sequence (Xl, Yl), (X2' Y2), .. " the algorithm for constructing associative
database is as follows.

Algorithm 3,'

Step I(Initialization): Let (x[root], y[root]) = (Xl, Yd. Here, x[.] and y[.] are
variables assigned for respective nodes to memorize data.. Furthermore, let t = 1.
Step 2: Increase t by 1, and put x, in. After reset a pointer n to the root, repeat
the following until n arrives at some terminal node, i.e., leaf.
Notations nand
d(xt, x[n)), let n

n mean the descendant nodes of n.
=n. Otherwise, let n =n.

If d(x" r[n)) ~

Step 3: Display yIn] as the related information. Next, put y, in. If yIn] = y" back
to step 2. Otherwise, first establish new descendant nodes n and n. Secondly,
let

(x[n], yIn))
(x[n], yIn))

(x[n], yIn)),
(Xt, y,).

(2)
(3)

Finally, back to step 2. Here, the loop of step 2-3 can be stopped at any time
and also can be continued.
Now, suppose that gate elements, namely, artificial "synapses" that play the role of branching by d are prepared. Then, we obtain a new style of neural network with gate elements
being randomly connected by this algorithm.

LETTER RECOGNITION
Recen tly, the vertical slitting method for recognizing typographic English letters3 , the
elastic matching method for recognizing hand written discrete English letters4 , the global
training and fuzzy logic search method for recognizing Chinese characters written in square
styleS, etc. are published. The self-organization of associative database realizes the recognition of handwritten continuous English letters.

770

9 /wn"

NOV

~ ~ ~ -xk :La.t

~~ ~ ~~~

dw1lo'

~~~~~of~~

~~~ 4,-?~~4Fig. 1. Source document.
2~~---------------'

lOO~---------------'

H

o

o
Fig. 2. Windowing.

1000

2000

3000

4000

Number of samples

o

1000

2000

3000

4000

NUAlber of sampl es

Fig. 3. An experiment result.

An image scanner takes a document image (Fig. 1). The letter recognizer uses a parallelogram window that at least can cover the maximal letter (Fig. 2), and processes the
sequence of letters while shifting the window. That is, the recognizer scans a word in a
slant direction. And, it places the window so that its left vicinity may be on the first black
point detected. Then, the window catches a letter and some part of the succeeding letter.
If recognition of the head letter is performed, its end position, namely, the boundary line
between two letters becomes known. Hence, by starting the scanning from this boundary
and repeating the above operations, the recognizer accomplishes recursively the task. Thus
the major problem comes to identifying the head letter in the window.
Considering it, we define the following.
? Regard window images as x's, and define X accordingly.
? For a (x, x) E X x X, denote by B a black point in the left area from the boundary on
window image X. Project each B onto window image x. Then, measure the Euclidean
distance 6 between fj and a black point B on x being the closest to B. Let d(x, x) be
the summation of 6's for all black points B's on x divided by the number of B's.
? Regard couples of the "reading" and the position of boundary as y's, and define Y
accordingly.
An operator teaches the recognizer in interaction the relation between window image and
reading& boundary with algorithm 3. Precisely, if the recalled reading is incorrect, the
operator teaches a correct reading via the console. Moreover, if the boundary position is
incorrect, he teaches a correct position via the mouse.
Fig. 1 shows partially a document image used in this experiment. Fig. 3 shows the
change of the number of nodes and that of the recognition rate defined as the relative
frequency of correct answers in the past 1000 trials. Speciiications of the window are height
= 20dot, width = 10dot, and slant angular = 68deg. In this example, the levels of tree
were distributed in 6-19 at time 4000 and the recognition rate converged to about 74%.
Experimentally, the recognition rate converges to about 60-85% in most cases, and to 95% at
a rare case. However, it does not attain 100% since, e.g., "c" and "e" are not distinguishable
because of excessive lluctuation in writing. If the consistency of the x, y-relation is not
assured like this, the number of nodes increases endlessly (d. Fig. 3). Hence, it is clever to
stop the learning when the recognition rate attains some upper limit. To improve further
the recognition rate, we must consider the spelling of words. It is one of future subjects.

771

OBSTACLE AVOIDING MOVEMENT
Various systems of camera type autonomous mobile robot are reported flourishingly6-1O.
The system made up by the authors (Fig. 4) also belongs to this category. Now, in mathematical methodologies, we solve usually the problem of obstacle avoiding movement as
a cost minimization problem under some cost criterion established artificially. Contrarily,
the self-organization of associative database reproduces faithfully the cost criterion of an
operator. Therefore, motion of the robot after learning becomes very natural.
Now, the length, width and height of the robot are all about O.7m, and the weight is
about 30kg. The visual angle of camera is about 55deg. The robot has the following three
factors of motion. It turns less than ?30deg, advances less than 1m, and controls speed less
than 3km/h. The experiment was done on the passageway of wid th 2.5m inside a building
which the authors' laboratories exist in (Fig. 5). Because of an experimental intention, we
arrange boxes, smoking stands, gas cylinders, stools, handcarts, etc. on the passage way at
random. We let the robot take an image through the camera, recall a similar image, and
trace the route preliminarily recorded on it. For this purpose, we define the following.
? Let the camera face 28deg downward to take an image, and process it through a low
pass filter. Scanning vertically the filtered image from the bottom to the top, search
the first point C where the luminance changes excessively. Then, su bstitu te all points
from the bottom to C for white, and all points from C to the top for black (Fig. 6).
(If no obstacle exists just in front of the robot, the white area shows the ''free'' area
where the robot can move around.) Regard binary 32 x 32dot images processed thus
as x's, and define X accordingly.
? For every (x, x) E X x X, let d(x, x) be the number of black points on the exclusive-or
image between x and X.
? Regard as y's the images obtained by drawing routes on images x's, and define Y
accordingly.
The robot superimposes, on the current camera image x, the route recalled for x, and
inquires the operator instructions. The operator judges subjectively whether the suggested
route is appropriate or not. In the negative answer, he draws a desirable route on x with the
mouse to teach a new y to the robot. This opera.tion defines implicitly a sample sequence
of (x, y) reflecting the cost criterion of the operator.

.::l" !
-

IibUBe

_. -

22

11

Roan

12

{-

13

Stationary uni t

Fig. 4. Configuration of
autonomous mobile robot system.

~

I

,

23

24

North
14

rmbi Ie unit (robot)

-

Roan

y

t

Fig. 5. Experimental
environment.

772

Wall

Camera image

Preprocessing

A

::: !fa

?

Preprocessing

0

O

Course
suggest ion

??

..

Search

A

Fig. 6. Processing for
obstacle avoiding movement.

x

Fig. 1. Processing for
position identification.
We define the satisfaction rate by the relative frequency of acceptable suggestions of
route in the past 100 trials. In a typical experiment, the change of satisfaction rate showed
a similar tendency to Fig. 3, and it attains about 95% around time 800. Here, notice that
the rest 5% does not mean directly the percentage of collision. (In practice, we prevent the
collision by adopting some supplementary measure.) At time 800, the number of nodes was
145, and the levels of tree were distributed in 6-17.
The proposed method reflects delicately various characters of operator. For example, a
robot trained by an operator 0 moves slowly with enough space against obstacles while one
trained by another operator 0' brushes quickly against obstacles. This fact gives us a hint
on a method of printing "characters" into machines.
POSITION IDENTIFICATION
The robot can identify its position by recalling a similar landscape with the position data
to a camera image. For this purpose, in principle, it suffices to regard camera images and
position data as x's and y's, respectively. However, the memory capacity is finite in actual
compu ters. Hence, we cannot but compress the camera images at a slight loss of information.
Such compression is admittable as long as the precision of position identification is in an
acceptable area. Thus, the major problem comes to find out some suitable compression
method.
In the experimental environment (Fig. 5), juts are on the passageway at intervals of
3.6m, and each section between adjacent juts has at most one door. The robot identifies
roughly from a surrounding landscape which section itself places in. And, it uses temporarily
a triangular surveying technique if an exact measure is necessary. To realize the former task,
we define the following .
? Turn the camera to take a panorama image of 360deg. Scanning horizontally the
center line, substitute the points where the luminance excessively changes for black
and the other points for white (Fig. 1). Regard binary 360dot line images processed
thus as x's, and define X accordingly.
? For every (x, x) E X x X, project each black point A on x onto x. And, measure the
Euclidean distance 6 between A and a black point A on x being the closest to A. Let
the summation of 6 be S. Similarly, calculate S by exchanging the roles of x and X.
Denoting the numbers of A's and A's respectively by nand n, define

773

d(x, x) =

~(~
+ ~).
2 n
n

(4)

? Regard positive integers labeled on sections as y's (cf. Fig. 5), and define Y accordingly.
In the learning mode, the robot checks exactly its position with a counter that is reset periodically by the operator. The robot runs arbitrarily on the passageways within 18m area
and learns the relation between landscapes and position data. (Position identification beyond 18m area is achieved by crossing plural databases one another.) This task is automatic
excepting the periodic reset of counter, namely, it is a kind of learning without teacher.
We define the identification rate by the relative frequency of correct recalls of position
data in the past 100 trials. In a typical example, it converged to about 83% around time
400. At time 400, the number of levels was 202, and the levels oftree were distributed in 522. Since the identification failures of 17% can be rejected by considering the trajectory, no
pro blem arises in practical use. In order to improve the identification rate, the compression
ratio of camera images must be loosened. Such possibility depends on improvement of the
hardware in the future.
Fig. 8 shows an example of actual motion of the robot based on the database for obstacle
avoiding movement and that for position identification. This example corresponds to a case
of moving from 14 to 23 in Fig. 5. Here, the time interval per frame is about 40sec.

,~. .~ (
;~"i..
~

"

"

.

..I

I

?
?

"

I'
.
'.1
t

;

i

-:
, . . , 'II

Fig. 8. Actual motion of the robot.

774

CONCLUSION
A method of self-organizing associative databases was proposed with the application to
robot eyesight systems. The machine decomposes a global structure unknown into a set of
local structures known and learns universally any input-output response. This framework
of problem implies a wide application area other than the examples shown in this paper.
A defect of the algorithm 3 of self-organization is that the tree is balanced well only
for a subclass of structures of f. A subject imposed us is to widen the class. A probable
solution is to abolish the addressing rule depending directly on values of d and, instead, to
establish another rule depending on the distribution function of values of d. It is now under
investigation.

REFERENCES
1. Hopfield, J. J. and D. W. Tank, "Computing with Neural Circuit: A Model/'

Science 233 (1986), pp. 625-633.
2. Rumelhart, D. E. et al., "Learning Representations by Back-Propagating Errors," Nature 323 (1986), pp. 533-536.

3. Hull, J. J., "Hypothesis Generation in a Computational Model for Visual Word
Recognition," IEEE Expert, Fall (1986), pp. 63-70.
4. Kurtzberg, J. M., "Feature Analysis for Symbol Recognition by Elastic Matching," IBM J. Res. Develop. 31-1 (1987), pp. 91-95.

5. Wang, Q. R. and C. Y. Suen, "Large Tree Classifier with Heuristic Search and
Global Training," IEEE Trans. Pattern. Anal. & Mach. Intell. PAMI 9-1
(1987) pp. 91-102.
6. Brooks, R. A. et al, "Self Calibration of Motion and Stereo Vision for Mobile
Robots," 4th Int. Symp. of Robotics Research (1987), pp. 267-276.
7. Goto, Y. and A. Stentz, "The CMU System for Mobile Robot Navigation," 1987
IEEE Int. Conf. on Robotics & Automation (1987), pp. 99-105.
8. Madarasz, R. et al., "The Design of an Autonomous Vehicle for the Disabled,"
IEEE Jour. of Robotics & Automation RA 2-3 (1986), pp. 117-125.
9. Triendl, E. and D. J. Kriegman, "Stereo Vision and Navigation within Buildings," 1987 IEEE Int. Conf. on Robotics & Automation (1987), pp. 1725-1730.
10. Turk, M. A. et al., "Video Road-Following for the Autonomous Land Vehicle,"
1987 IEEE Int. Conf. on Robotics & Automation (1987), pp. 273-279.


----------------------------------------------------------------

title: 81-invariant-object-recognition-using-a-distributed-associative-memory.pdf

830

Invariant Object Recognition Using a Distributed Associative Memory
Harry Wechsler and George Lee Zimmerman
Department or Electrical Engineering
University or Minnesota
Minneapolis, MN 55455

Abstract
This paper describes an approach to 2-dimensional object recognition. Complex-log conformal mapping is combined with a distributed associative memory to create a system
which recognizes objects regardless of changes in rotation or scale. Recalled information
from the memorized database is used to classify an object, reconstruct the memorized version of the object, and estimate the magnitude of changes in scale or rotation. The system
response is resistant to moderate amounts of noise and occlusion. Several experiments, using real, gray scale images, are presented to show the feasibility of our approach.
Introduction
The challenge of the visual recognition problem stems from the fact that the projection of an object onto an image can be confounded by several dimensions of variability
such as uncertain perspective, changing orientation and scale, sensor noise, occlusion, and
non-uniform illumination. A vision system must not only be able to sense the identity of an
object despite this variability, but must also be able to characterize such variability -- because the variability inherently carries much of the valuable information about the world.
Our goal is to derive the functional characteristics of image representations suitable for invariant recognition using a distributed associative memory. The main question is that of
finding appropriate transformations such that interactions between the internal structure
of the resulting representations and the distributed associative memory yield invariant
recognition. As Simon [1] points out, all mathematical derivation can be viewed simply as
a change of representation, making evident what was previously true but obscure. This
view can be extended to all problem solving. Solving a problem then means transforming it
so as to make the solution transparent .
We approach the problem of object recognition with three requirements:
classification, reconstruction, and characterization. Classification implies the ability to distinguish objects that were previously encountered. Reconstruction is the process by which
memorized images can be drawn from memory given a distorted version exists at the input. Characterization involves extracting information about how the object has changed
from the way in which it was memorized. Our goal in this paper is to discuss a system
which is able to recognize memorized 2-dimensional objects regardless of geometric distortions like changes in scale and orientation, and can characterize those transformations.
The system also allows for noise and occlusion and is tolerant of memory faults.
The following sections, Invariant Representation and Distributed Associative
Memory, respectively, describe the various components of the system in detail. The Experiments section presents the results from several experiments we have performed on real
data. The paper concludes with a discussion of our results and their implications for future
research.

? American Institute of Physics 1988

831

1. Invariant Representation

The goal of this section is to examine the various components used to produce the
vectors which are associated in the distributed associative memory. The block diagram
which describes the various functional units involved in obtaining an invariant image
representation is shown in Figure 1. The image is complex-log conformally mapped so that
rotation and scale changes become translation in the transform domain . Along with the
conformal mapping, the image is also filtered by a space variant filter to reduce the effects
of aliasing. The conformally mapped image is then processed through a Laplacian in order
to solve some problems associated with the conformal mapping. The Fourier transform of
both the conformally mapped image and the Laplacian processed image produce the four
output vectors. The magnitude output vector I-II is invariant to linear transformations of
the object in the input image. The phase output vector <1>2 contains information concerning the spatial properties of the object in the input image.

1.1 Complex-Log Mapping and Space Variant Filtering
The first box of the block diagram given in Figure 1 consists of two components:
Complex-log mapping and space variant filtering. Complex-log mapping transforms an
image from rectangular coordinates to polar exponential coordinates. This transformation
changes rotation and scale into translation. If the image is mapped onto a complex plane
then each pixel (x,y) on the Cartesian plane can be described mathematically by z = x +
jy. The complex-log mapped points ware described by
w =In{z) =In(lzl} +jiJ z

(1)

Our system sampled 256x256 pixel images to construct 64x64 complex-log mapped
images. Samples were taken along radial lines spaced 5.6 degrees apart. Along each radial
line the step size between samples increased by powers of 1.08. These numbers are derived
from the number of pixels in the original image and the number of samples in the
complex-log mapped image. An excellent examination of the different conditions involved
in selecting the appropriate number of samples for a complex-log mapped image is given in
[2J. The non-linear sampling can be split into two distinct parts along each radial line. Toward the center of the image the samples are dense enough that no anti-aliasing filter is
needed. Samples taken at the edge of the image are large and an anti-aliasing filter is
necessary. The image filtered in this manner has a circular region around the center which
corresponds to an area of highest resolution. The size of this region is a function of the
number of angular samples and radial samples. The filtering is done, at the same time as
the sampling, by convolving truncated Bessel functions with the image in the space
domain. The width of the Bessel functions main lobe is inversely proportional to the eccentricity of the sample point.
A problem associated with the complex-log mapping is sensitivity to center
misalignment of the sampled image. Small shifts from the center causes dramatic distortions in the complex-log mapped image. Our system assumes that the object is centered in
the image frame. Slight misalignments are considered noise. Large misalignments are considered as translations and could be accounted for by changing the gaze in such a way as
to bring the object into the center of the frame. The decision about what to bring into the
center of the frame is an active function and should be determined by the task. An example of a system which could be used to guide the translation process was developed by
Anderson and Burt [3J. Their pyramid system analyzes the input image at different tem-

00

c..:>

~

Inverse
Processing
and
Reconstruction

.

Image

~

I

Compl".lo,
Mapping
and
Space Variant
Filtering

I
I
I

~-?-FO",i"

II

'

1ransform

I

2

-1-1

I

I

-~

Laplacian

Fourier
Transform

2

_~I

Distributed
Associative
Memory

~

Rotation
and
Scale
Estimation

I-II
Classification

Figure 1. Block Diagram of the System.

833

poral and spatial resolution levels. Their smart sensor was then able to shift its fixation
such that interesting parts of the image (ie . something large and moving) was brought into
the central part of the frame for recognition .

1.2 Fourier Transform
The second box in the block diagram of Figure 1 is the Fourier transform. The
Fourier transform of a 2-dimensional image f(x,y) is given by
F(u,v) =

j j

f(x,y)e-i(ux+vy) dx dy

(2)

-00 -00

and can be described by two 2-dimensional functions corresponding to the magnitude
IF(u,v)1 and phase <l>F(u,v). The magnitude component of the Fourier trans~rm which is
invariant to translatIOn, carries much of the contrast information of the image . The phase
component of the Fourier transform carries information about how things ar} placed in an
image. Translation of f(x,y) corresponds to the addition of a linear phase cpmponent. The
complex-log mapping transforms rotation and scale into translation and tije magnitude of
the Fourier transform is invariant to those translations so that I-II ivill not change
significantly with rotation and scale of the object in the image .

1.3 Laplacian
The Laplacian that we use is a difference-of-Gaussians (DOG) approximation to the
function as given by Marr [4).
'V 2G

2

2

=h [1 - r2/2oo 2) e{ -r /200 }

(3)

'1rtT

The result of convolving the Laplacian with an image can be viewed as a two step process.
The image is blurred by a Gaussian kernel of a specified width oo. Then the isotropic
second derivative of the blurred image is computed. The width of the Gaussian kernel is
chosen such that the conformally mapped image is visible -- approximately 2 pixels in our
experiments. The Laplacian sharpens the edges of the object in the image and sets any region that did not change much to zero. Below we describe the benefits from using the Laplacian.
The Laplacian eliminates the stretching problem encountered by the complex-log
mapping due to changes in object size. When an object is expanded the complex-log
mapped image will translate . The pixels vacated by this translation will be filled with
more pixels sampled from the center of the scaled object. These new pixels will not be
significantly different than the displaced pixels so the result looks like a stretching in the
complex-log mapped image . The Laplacian of the complex-log mapped image will set the
new pixels to zero because they do not significantly change from their surrounding pixels.
The Laplacian eliminates high frequency spreading due to the finite structure of the
discrete Fourier transform and enhances the differences between memorized objects by accentuating edges and de-emphasizing areas of little change.

2. Distributed Associative Memory (DAM)
The particular form of distributed associative memory that we deal with in this paper is a memory matrix which modifies the flow of information. Stimulus vectors are associated with response vectors and the result of this association is spread over the entire
memory space . Distributing in this manner means that information about a small portion
of the association can be found in a large area of the memory. New associations are placed

834

over the older ones and are allowed to interact. This means that the size of the memory
matrix stays the same regardless of the number of associations that have been memorized.
Because the associations are allowed to interact with each other an implicit representation
of structural relationships and contextual information can develop, and as a consequence a
very rich level of interactions can be captured. There are few restrictions on what vectors
can be associated there can exist extensive indexing and cross-referencing in the memory.
Distributed associative memory captures a distributed representation which is context
dependent. This is quite different from the simplistic behavioral model [5].
The construction stage assumes that there are n pairs of m-dimensional vectors that
are to be associated by the distributed associative memory. This can be written as
"l.K:::+.
IV~

1

= -r.
1

~or 1?
I'

= 1 , ... ,n

(4)

-d
h ?th stlmu
. I us vector an d -d
h .th correspon d?mg response Vech
were
s. enotes tel
r. enotes tel
tor. W~ want to construct a memory matrix M such that when the kth stimulus vector S;
is projected onto the space defined by M the resulting projection will be the corresponding
More specifically we want to solve the following equation:
response vector

r;.

(5)

MS=R

- 11 s2
11 ? ??11 ?
?
~
S = [ s1
h
were
S ] an d R = [ -r 1 11 -r 2 11 ???11 r]. A
umque
soIutlOn
lor
t h?IS equation does not necessarily n exist for any arbitrary gr~up of associations that might be
chosen. Usually, the number of associations n is smaller than m, the length of the vector to
be associated, so the system of equations is underconstrained. The constraint used to solve
for a unique matrix M is that of minimizing the square error, IIMS - RJ1 2, which results in
the solution

(6)
where S+ is known as the Moore-Penrose generalized inverse of S [6J.
The recall operation projects an unknown stimulus vector
M. The resulting projection yields the response vector r

r =Ms

s onto

the memory space

(7)

If the memorized stimulus vectors are independent and the unknown stimulus vector s is
one of the memorized vectors
then the recalled vector will be the associated response
If the memorized stimulus vectors are dependent, then the vector recalled by
vector
one of the memorized stimulus vectors will contain the associated response vector and
some crosstalk from the other stored response vectors.

r;.

S;,

The recall can be viewed as the weighted sum of the response vectors. The recall
begins by assigning weights according to how well the unknown stimulus vector matches
with the memorized stimulus vector using a linear least squares classifier. The response
vectors are multiplied by the weights and summed together to build the recalled response
vector. The recalled response vector is usually dominated by the memorized response vector that is closest to the unknown stimulus vector.
Assume that there are n associations in the memory and each of the associated
stimulus and response vectors have m elements. This means that the memory matrix has
m 2 elements. Also assume that the noise that. is added to each element of a memorized

835

stimulus vector
memory is then

IS

independent, Zero mean, with a variance of O'~ The recall from the
1

(8)
where tt is the input noise vector and t1 is the output noise vector. The ratio of the average output noise variance to the averagg input noise variance is

0'2o/0'.12

1
[MMT]
= -Tr
m

(9)

For the autoassociative case this simplifies to

(10)
This says that when a noisy version of a memorized input vector is applied to the memory
the recall is improved by a factor corresponding to the ratio of the number of memorized
vectors to the number of elements in the vectors. For the heteroassociative memory matrix a similar formula holds as long as n is less than m [7].

(11)
Fault tolerance is a byproduct of the distributed nature and error correcting capabilities of the distributed associative memory. By distributing the information, no single
memory cell carries a significant portion of the information critical to the overall performance of the memory.
3. Experiments

In this section we discuss the result of computer simulations of our system. Images
of objects are first preprocessed through the sUbsystem outlined in section 2. The output of
such a subsystem is four vectors: I-I , <1>1' 1-1 2, and <1>2' We construct the memory by associating the stimulus vector I-II with ?he response vector <1>2 for each object in the database.
To perform a recall from the meJIlory the.. unknown image is preprocessed by the same_subsystem to produce the vectors I-II' <1>1' 1-12, and <1>2' The resulting stimulus vector I-I is
projected onto the m~mory matrix to produce a respOJlse vector which is an ~stimatel of
the memorized phase <1>2' The estimated phase vector cI> 2 and the magnitude I-II ate used
to reconstruct the memorized object. The difference between the estimated phase <1>2 and
the unknown phase <1>2 is used to estimate the amount of rotation and scale experienced by
the object.
The database of images consists of twelve objects: four keys, four mechanical parts,
and four leaves. The objects were chosen for their essentially two-dimensional structure.
Each object was photographed using a digitizing video camera against a black background. We emphasize that all of the images used in creating and testing the recognition
system were taken at different times using various camera rotations and distances. The images are digitized to 256x256, eight bit quantized pixels, and each object covers an area of
about 40x40 pixels. This small object size relative to the background is necessary due to
the non-linear sampling of the complex-log mapping. The objects were centered within the
frame by hand. This is the source of much of the noise and could have been done automatically using the object's center of mass or some other criteria determined by the task. The
orientation of each memorized object was arbitrarily chosen such that their major axis

836

was vertical. The 2-dimensional images that are the output from the invariant representation subsystem are scanned horizontally to form the vectors for memorization. The database used for these experiments is shown in Figure 2.

Figure 2. The Database of Objects Used in the Experiments

a) Original

b) Unknown

c) Recall: rotated 135?

d) Memory:6
SNR: -3.37 Db

Figure 3. :Recall Using a Rotated and scaled key
The first example of the operation of our system is shown in Figure 3. Figure 3a) is
the image of one of the keys as it was memorized. Figure 3b) is the unknown object
presented to our system. The unknown object in this caSe is the same key that has been
rotated by 180 degrees and scaled. Figure 3c) is the recalled, reconstructed image. The

837

rounded edges of the recalled image are artifacts of the complex-log mapping. Notice that
the reconstructed recall is the unrotated memorized key with some noise caused by errors
in the recalled phase. Figure 3d) is a histogram which graphically displays the
classification vector which corresponds to S+S. The histogram shows the interplay between
the memorized images and the unknown image. The" 6" on the bargraph indicates which
of the twelve classes the unknown object belongs. The histogram gives a value which is
the best linear estimate of the image relative to the memorized objects. Another measure,
the signal-to-noise ratio (SNR), is given at the bottom of the recalled image. SNR compares the variance of the ideal recall after processing with the variance of the difference
between the ideal and actual recall. This is a measure of the amount of noise in the recall.
The SNR does not carry rr.uch information about the q"Jality of the recall image because
the noise measured by the SNP.. is jue to many factors such as misalignment of the center,
changing reflections, and dependence between other memorized objects -- each affecting.
quality in a variety of ways. Rotation and scale estimate~ are made using a vector_ D
corresponding to the dlll'erence between the unknown vector <1>2 and the recalled vector <I> 2'
In an ideal situation D will be a plane whose E;radient indicates the exact amount of r:.otation and scale the recalled object has experienced. In our system the recalled vector <I> 2 is
corrupted with noise which means rotation...and scale have to be estim:ned. The estimate is
made by letting the first order difference D at each point in the plane vote for a specified
range of rotation or scale.

a) Original

b) Unknown

c) Recall

d) Memory:4

Figure 4 Recall Using Scaled and Rotated" S" with Occlusion
Figure 4 is an example of occlusion. The unknown object in this case is an "s"
curve which is larger and slightly tilted from the memorized "s" curve. A portion of the
bottom curve was occluded. The resulting reconstruction is very noisy but has filled in the
missing part of the bottom curve. The noisy recall is reflected in both the SNR and the interplay betw~en the memories shown by the hi~togram.

a) Ideal recall

b) 30% removed

c) 50% removed

d) 75% removed

Figure 5. Recall for Memory Matrix Randomly Set to Zero
Figure 5 is the result of randomly setting the elements of the memory matrix to

838

zero. Figure 5a) shows is the ideal recall. Figure 5b) is the recall after 30 percent of the
memory matrix has been set to zero. Figure 5c) is the recall for 50 percent and Figure 5d)
is the recall for 75 percent. Even when 90 percent of the memory matrix has been set to
zero a faint outline of the pin could still be seen in the recall. This result is important in
two ways. First, it shows that the distributed associative memory is robust in the presence
of noise. Second, it shows that a completely connected network is not necessary and as a
consequence a scheme for data compression of the memory matrix could be found.

4. Conclusion
In this paper we demonstrate a computer vIsIon system which recognIzes 2dimensional objects invariant to rotation or scale. The system combines an invariant
representation of the input images with a distributed associative memory such that objects
can be classified, reconstructed, and characterized. The distributed associative memory is
resistant to moderate amounts of noise and occlusion. Several experiments, demonstrating
the ability of our computer vision system to operate on real, grey scale images, were
presented.
Neural network models, of which the di~tributed associative memory is one example,
were originally developed to simulate biological memory. They are characterized by a
large number of highly interconnected simple processors which operate in p2..rallel. An excellent review of the many neural network models is given in [8J. The distrib-uted associative memory we use is linear, and as a result there are certain desirable properties which
will not be exhibited by our computer vision system. For example, feedback through our
system will not improve recall from the memory. Recall could be improved if a non-linear
element, such as a sigmoid function, is introduced into the feedback loop. Non-linear neural networks, such as those proposed by Hopfield [9] or Anderson et. al. [10J, can achieve
this type of improvement because each memorized pattern js associated with sta~le points
in an energy space. The price to be paid for the introduction of non-linearities into a
memory system is that the system will be difficult to analyze and can be unstable. Implementing our computer vision system using non-linear distributed associative memory is a
goal of our future research.
We are presently extending our work toward 3-dimensional object recognition. Much
of the present research in 3-dimensional object recognition is limited to polyhedral, nonoccluded objects' in a clean, highly controlled environment. Most systems are edge based
and use a generate-and-test paradigm to estimate the position and orientation of recognized objects. We propose to use an approach based on characteristic views [llJ or aspects
[12J which suggests that the infinite 2-dimensional projections of a 3-dimensional object
can be grouped into a finite number of topological equivalence classes. An efficie:.t 3dimensional recognition system would require a parallel indexing method to search for object models in the presence of geometric distortions, noise, and occlusion. Our object recognition system using distributed associative memory can fulfill those requirements with
respect to characteristic views.

Referenees
[lJ Simon, H. A., (1984), The Seienee of the Artifldal (2nd ed.), MIT Press.
[2J Massone, L., G. Sandini, and V. Tagliasco (1985), "Form-invariant" topological mapping strategy for 2D shape recognition, CVGIP, 30, 169-188.
[3J Anderson, C. H., P. J. Burt, and G. S. Van Der Wal (1985), Change detection and
tracking using pyramid transform techniques, Proe. of the SPIE Conferenee on
Intelligenee, Robots, and Computer Vision, Vol. 579, 72-78.

839

Marr, D. (1982), Vision, W. H. Freeman, 1982.
Hebb, O. D. (1949), The Organization of Behavior, New York: Wiley.
Kohonen, T. (1984), Self-Organization and Associative-Memories, Springer-Verlag.
Stiles, G. S. and D. L. Denq (1985), On the effect of noise on the Moore-Penrose generalized inverse associative memory, IEEE Trans. on PAMI, 7, 3,358-360.
[8J MCClelland, J. L., and D . E. Rumelhart, and the PDP Research Group (Eds.) (1986),
Parallel Distributed, Processing, Vol. 1, 2, MIT Press.
[9] Hopfield, J. J. (1982), Neural networks and physical systems with emergent collective
computational abilities, Proc. Natl. Acad. Sci. USA, 79, April 1982.
[10J Anderson, J. A., J. W. Silversteir., S. A. Ritz, and R. S. Jones (1977), Distinctive
features, categorical perception, and probability learning: some applications of a
neural model, Psychol. Rev., 84,413-451.
[11] Chakravarty, I., and H. Freeman (1982), Characteristic views as a basis for 3-D object
recognition, Proc. SPIE on Robot Vision, 336,37-45.
[12] Koenderink, J. J., and A . J . Van Doorn (1979), Internal representation of solid shape
with respect to vision, Bioi. Cybern., 32,4,211-216.

[4]
[5]
[6J
[7]


----------------------------------------------------------------

title: 1047-selective-attention-for-handwritten-digit-recognition.pdf

Selective Attention for Handwritten
Digit Recognition

Ethem Alpaydm
Department of Computer Engineering
Bogazi<1i U ni versi ty
Istanbul, TR-SOS15 Turkey
alpaydin@boun.edu.tr

Abstract
Completely parallel object recognition is NP-complete. Achieving
a recognizer with feasible complexity requires a compromise between parallel and sequential processing where a system selectively
focuses on parts of a given image, one after another. Successive
fixations are generated to sample the image and these samples are
processed and abstracted to generate a temporal context in which
results are integrated over time. A computational model based on a
partially recurrent feedforward network is proposed and made credible by testing on the real-world problem of recognition of handwritten digits with encouraging results.

1

INTRODUCTION

For all-parallel bottom-up recognition, allocating one separate unit for each possible
feature combination, i.e., conjunctive encoding, implies combinatorial explosion. It
has been shown that completely parallel, bottom-up visual object recognition is
NP-complete (Tsotsos, 1990). By exchanging space with time, systems with much
less complexity may be designed. For example, to phone someone at the press of a
button, one needs 10 7 buttons on the phone; the sequential alternative is to have
10 buttons on the phone and press one at a time, seven times.
We propose recognition based on selective attention where we analyze only a small
part of the image in detail at each step, combining results in time. N oton and Stark's
(1971) "scanpath" theory advocates that each object is internally represented as a
feature-ring which is a temporal sequence of features extracted at each fixation and
the positions or the motor commands for the eye movements in between. In this
approach, there is an "eye" that looks at an image but which can really see only a
small part of it. This part of the image that is examined in detail is the fovea. The

772

E. ALPAYDIN

ASSOCIATIVE

Class Probabilities
(lOx!)

LEVEL

P~r------7-"7

L-L_ _ _ _ _ _/ '

t

softmax
Class Units
(lOxI) 0 /

7

T1

Hidden Units (s x I)
H L..../~_....;...._-_-_-_-~_-_-_-_-_-~_-_-~7-."

I~-----------------;t~---------- ----;;------------------PRE-ATTENTIVE LEVEL

ATTENTIVE LEVEL

,-------------------- -----,

------

-------------------,

I

:
F

Feature Map
I
(rxI):

Eye Position Map
(pxp)

//p
1

Fovea
1-------'---1-

~

-I

WTA

subsample
and blur

I

- -:- - - - - - - ~

Saliency Map
(n x n)

M
Bitmap Image (n x n)

Figure 1: The block diagram of the implemented system.

fovea's content is examined by the pre-attentive level where basic feature extraction
takes place. The features thus extracted are fed to an a660ciative part together
with the current eye position. If the accumulated information is not sufficient for
recognition, the eye is moved to another part of the image, making a saccade. To
minimize recognition time, the number of saccades should be minimized. This is
done through defining a criterion of being "interesting" or saliency and by fixating
only at the most interesting. Thus sucessive fixations are generated to sample the
image and these samples are processed and abstracted to generate a temporal context in which results are integrated over time. There is a large amount of literature
on selective attention in neuroscience and psychology; for reviews see respectively
(Posner and Peterson, 1990) and (Treisman, 1988). The point stressed in this paper
is that the approach is also useful in engineering.

2

AN EXAMPLE SYSTEM FOR OCR

The structure of the implemented system for recognition of handwritten digits is
given in Fig. 1.

Selective Attention for Handwritten Digit Recognition

773

We have an n x n binary image in which the fovea is m x m with m < n. To
minimize recognition time, the system should only attend to the parts of the image
that carry discriminative information. We define a criterion of being "interesting"
or saliency which is applied to all image locations in parallel to generate a 8aliency
map, S. The saliency measure should be chosen to draw attention to parts that
have the highest information content. Here, the saliency criterion is a low-pass filter
which roughly counts the number of on pixels in the corresponding m x m region
of the input image M. As the strokes in handwritten digits are mostly one or two
pixels wide, a count of the on pixels is a good measure of the discontinuity (and
thus information). It is also simple to compute:

i+lm/2J
Sij =

L

HLm/2J

L

MkIN2 ((i,jl, (Lm/6J)2 *1), i,j = 1. .. n

k=i-Lm/2J l=j-Lm/2J
where N 2 (p., E) is the bivariate normal with mean p. and the covariance E. Note
that we want the convolution kernel to have effect up to Lm/2J and also that the
normal is zero after p.? 30-. In our simulations where n is 16 and m is 5 (typical for
digit recognition), 0- ~ 1. The location that is most salient is the position ofthe next
fixation and as such defines the new center of the fovea. A location once attended
to is no longer interesting; after each fixation, the saliency of all the locations that
currently are in the scope of the fovea are set to 0 to inhibit another fixation there.
The attentive level thus controls the scope of the pre-attentive level. The maximum
of the saliency map through a winner-take-all gives the eye position (i*, j*) at
fixation t.
(i*(t),j*(t))
arg~B:XSij
',J
By thus following the salient regions, we get an input-dependent emergent sequence
in time.

=

Eye-Position Map
The eye p08ition map, P, stores the position of the eye in the current fixation. It is
p x p. p is chosen to be smaller than n for dimensionality reduction for decreasing

complexity and introducing an effect of regularization (giving invariance to small
translations). When p is a factor of n, computations are also simpler. We also blur
the immediate neighbors for a smoother representation:

P( t)

= blur(subsample( winner-take-all( S)))

Pre-Attentive Level: Feature Extraction
The pre-attentive level extracts detailed features from the fovea to generate a feature
map. This information and the current eye position is passed to the associative
system for recognition. There is a trade-off between the fovea size and the number
of saccades required for recognition: As the operation in the pre-attentive level is
carried out in parallel, to minimize complexity the features extracted there should
not be many and the fovea should not be large: Fovea is where the expensive
computation takes place. On the other hand, the fovea should be large enough to
extract discriminative features and thus complete recognition in a small amount of
time. The features to be extracted can be learned through an supervised method
when feedback is available .

774

E. ALPAYDIN

The m x m region symmetrically around (i*, j*) is extracted as the fovea I and is
fed to the feature extractors. The r features extracted there are passed on to the
associative level as the feature map, F. r is typically 4 to 8. Ug denote the weights
of feature 9 and Fg is the value of feature 9 that is found by convolving the fovea
input with the feature weight vector (1(.) is the sigmoid function):

M i o(t)-Lm/2J+i,jo(t)-Lm/2J+j, i,j = 1 ... m

f (

~ ~ U"jI,j(t?) , g = 1. ..

r

Associative Level: Classification
At each fixation, the associative level is fed the feature map from the pre-attentive
level and the eye position map from the attentive level. As a number of fixations
may be necessary to recognize an image, the associative system should have a shortterm memory able to accumulate inputs coming through time. Learning similarly
should be through time. When used for classification, the class units are organized
so as to compete and during recognition the activations of the class units evolve
till one class gets sufficiently active and suppresses the others. When a training
set is available, a temporal supervised method can be used to train the associative
level. Note that there may be more than one scanpath for each object and learning
one sequence for each object fails. We see it is a task of accumulating two types of
information through time: the "what" (features extracted) and the "where" (eye
position).
The fovea map, F, and the eye position map, P, are concatenated to make a
r + p X P dimensional input that is fed to the associative level. Here we use an
artificial neural network with one hidden layer of 8 units. We have experimented
with various architectures and noticed that recurrency at the output layer is the
best. There are 10 output units.

f (L VhgFg(t) + L L WhabPab(t)) , h =

1. .. s

gab

LTchHh + L RckPk(t - 1), c = 1. .. 10
h

k

exp[Oc(t)]
Lk exp[Ok(t)]
where P denotes the "softmax"ed output probabilities (Bridle, 1990) and P(t - 1)
are the values in the preceding fixation (initially 0). We use the cross-entropy as
the goodness measure:
C=

L
t

1

t L Dk 10gPc(t), t ~

1

c

Dc is the required output for class c. Learning is gradient-ascent on this goodness
measure. The fraction lit is to give more weight to initial fixations than later ones.
Connections to the output units are updated as follows (11 is the learning factor):

Selective Attention for Handwritten Digit Recognition

Note that we assume 8PIc(t -1)/8Rc lc =
we have:

o.

775

For the connections to the hidden units

c

We can back-propagate one step more to train the feature extractors. Thus the
update equations for the connections to feature units are:

Cg(t) =

L Ch(t)Vhg
h

A series of fixations are made until one of the class units is sufficiently active:
3c, Pc > 8 (typically 0.99), or when the most salient point has a saliency less than a
certain threshold (this condition is rarely met after the first few epochs). Then the
computed changes are summed up and the updates are made like the exaple below:

Backpropagation through time where the recurrent connections are unfolded in time
did not work well in this task because as explained before, for the same class, there is
more than one scanpath. The above-mentioned approach is like real-time recurrent
learning (Williams and Zipser, 1989) where the partial derivatives in the previous
time step is 0, thus ignoring this temporal dependence.

3

RESULTS AND DISCUSSION

We have experimented with various parameter settings and finally chose the architecture given above: When input is 16 x 16 and there are 10 classes, the fovea is
5 x 5 with 8 features and there are 16 hidden units. There are 1,934 images for
training, 946 for cross-validation and 943 for testing. Results are given in Table
1. ( It can be seen that by scanning less than half of the image, we get 80% generalization. Additional to the local high-resolution image provided by the fovea, a
low-resolution image of the surrounding parafovea can be given to the associative
level for better recognition. For example we low-pass filtered and undersampled the
original image to get a 4 x 4 image which we fed to the class units additional to
the attention-based hidden units. Success went up quite high and fewer fixations
were necessary; compare rows 1 and 2 of the Table. The information provided by
the 4 x 4 map is actually not much as can be seen from row 3 of the table where
only that is given as input. Thus the idea is that when we have a coarse input,
looking only at a quarter of the image in detail is sufficient to get 93% accuracy.
Both features (what) and eye positions (where) are necessary for good recognition.
When only one is used without the other, success is quite low as can be seen in rows
4 and 5. In the last row, we see the performance of a multi layer percept ron with
10 hidden units that does all-parallel recognition.
Beyond a certain network size, increasing the number of features do not help much.
Decreasing 8, the certainty threshold, decreases the number of fixations necessary

776

E. ALPAYDIN

Table 1: Results of handwritten digit recognition with selective attention. Values
given are average and standard deviation of 10 independent runs. See text for
comments.
NO OF
PARAMS

TEST
SUCCESS

TRAINING
EPOCHS

NO OF
FIXATIONS

SA system
SA+parafovea
Only parafovea
Only what info
Only where info

878
1,038
170
622
440

79.7, 1.8
92.5,0.8
86.9,0.2
49.0,21.0
54.2, 1.4

74.5, 17.1
54.2, 10.2
52.3,8.2
66.6, 30.6
92.9,6.5

6.5,0.2
3.9,0.3
1.0, 0.0
7.5,0.1
7.6,0.0

MLP, 10 hiddens

2,680

95.1, 0.6

13.5,4.1

1.0,0.0

METHOD

which we want, but decreases success too which we don't. Smaller foveas decrease
the number of free parameters but decrease success and require a larger number
of fixations. Similarly larger foveas decrease the number of fixations but increase
complexity.
The simple low-pass filter used here as a saliency measure is the simplest measure.
Previously it has been used by Fukushima and Imagawa (1993) for finding the next
character, i.e., segmentation, and also by Olshausen et al. (1992) for translation
invariance. More robust measures at the expense of more computations, are possible; see (Rimey and Brown, 1990; Milanese et al., 1993). Salient regions are those
that are conspicious, i.e., different from their surrounding where there is a change
in X where X can be brightness or color (edges), orientation (corners), time (motion), etc. It is also possible that top-down, task-dependent saliency measures be
integrated to minimize further recognition time implying a remembered explicit
sequence analogous to skilled motor behaviour (probably gained after many repetitions).
Here a partially recurrent network is used for temporal processing. Hidden Markov
Models like used in speech recognition are another possibility (Rimey and Brown,
1990; Haclsalihzade et al., 1992). They are probabilistic finite automata which can
be trained to classify sequences and one can have more than one model for an object.
It should be noted here that better approaches for the same problem exists (Le Cun
et al., 1989). Here we advocate a computational model and make it plausible by
testing it on a real-world problem. It is necessary for more complicated problems
where an all-parallel approach would not work. For example Le Cun et al. 's model
for the same type of inputs has 2,578 free parameters. Here there are

(mx m+1) x r+(r+pxp+ 1) x 8+(S+ 1) x 10+10 x 10
,

#'

iT

v';w

#~~

T

R

free parameters which make 878 when m = 5, r = 8, S = 16. This is the main
advantage of selective attention which is that the complexity of the system is heavily
reduced at the expense of slower recognition, both in overt form of attention through
foveation and in its covert form, for binding features - For this latter type of
attention not discussed here, see (Ahmad, 1992). Also note that low-level feature
extraction operations like carried out in the pre-attentive level are local convolutions

Selective Attention for Handwritten Digit Recognition

777

and are appropriate for parallel processing, e.g., on a SIMD machine. Higherlevel operations require larger connectivity and are better carried out sequentially.
Nature also seems to have taken this direction.
Acknowledgements
This work is supported by Tiibitak Grant EEEAG-143 and Bogazi<;;i University
Research Funds 95HA108. Cenk Kaynak prepared the handwritten digit database
based on the programs provided by NIST (Garris et al., 1994).
References
S. Ahmad. (1992) VISIT: A Neural Model of Covert Visual Attention. In J. Moody,
S. Hanson, R. Lippman (Eds.) Advances in Neural Information Processing Systems
4,420-427. San Mateo, CA: Morgan Kaufmann.
J.S. Bridle. (1990) Probabilistic Interpretation of Feedforward Classification Network Outputs with Relationships to Statistical Pattern Recognition. In Neurocomputing, F. Fogelman-Soulie, J. Herault, Eds. Springer, Berlin, 227-236.
K. Fukushima, T. Imagawa. (1993) Recognition and Segmentation of Connected
Characters with Selective Attention, Neural Networks, 6: 33-41.
M.D. Garris et al. (1994) NIST Form-Based Handprint Recognition System, NISTIR 5469, NIST Computer Systems Laboratory.
S.S. Haclsalihzade, L.W. Stark, J .S. Allen. (1992) Visual Perception and Sequences
of Eye Movement Fixations: A Stochastic Modeling Approach, IEEE SMC, 22,
474-481.
Y. Le Cun et al. (1991) Handwritten Digit Recognition with a Back-Propagation
Network. In D.S. Touretzky (ed.) Advances in Neural Information Processing
Systems 2, 396-404. San Mateo, CA: Morgan Kaufmann.
R. Milanese et al. (1994) Integration of Bottom-U p and Top- Down Cues for Visual
Attention using Non-Linear Relaxation IEEE Int'l Conf on CVPR, Seattle, WA,
USA.

D. Noton and L. Stark. (1971) Eye Movements and Visual Perception, Scientific
American, 224: 34-43.
B. Olshausen, C. Anderson, D. Van Essen. (1992) A Neural Model of Visual Attention and Invariant Pattern Recognition, CNS Memo 18, CalTech.
M.L Posner, S.E. Petersen. (1990) The Attention System of the Human Brain,
Ann. Rev. Neurosci., 13:25-42.
R.D. Rimey, C.M. Brown. (1990) Selective Attention as Sequential Behaviour: Modelling Eye Movements with an Augmented Hidden Markov Model, TR-327, Computer Science, Univ of Rochester.
A. Treisman. (1988) Features and Objects, Quarterly Journ. of Ezp . Psych., 40:
201-237.
J.K. Tsotsos. (1990) Analyzing Vision at the Complexity Level, Behav. and Brain
Sci. 13: 423-469.
R.J. Williams, D. Zipser. (1989) A Learning Algorithm for Continually Running
Fully Recurrent Neural Networks Neural Computation, 1, 270-280.


----------------------------------------------------------------

title: 6057-using-fast-weights-to-attend-to-the-recent-past.pdf

Using Fast Weights to Attend to the Recent Past

Jimmy Ba
University of Toronto

Geoffrey Hinton
University of Toronto and Google Brain

jimmy@psi.toronto.edu

geoffhinton@google.com

Volodymyr Mnih
Google DeepMind

Joel Z. Leibo
Google DeepMind

Catalin Ionescu
Google DeepMind

vmnih@google.com

jzl@google.com

cdi@google.com

Abstract
Until recently, research on artificial neural networks was largely restricted to systems with only two types of variable: Neural activities that represent the current
or recent input and weights that learn to capture regularities among inputs, outputs
and payoffs. There is no good reason for this restriction. Synapses have dynamics at many different time-scales and this suggests that artificial neural networks
might benefit from variables that change slower than activities but much faster
than the standard weights. These ?fast weights? can be used to store temporary
memories of the recent past and they provide a neurally plausible way of implementing the type of attention to the past that has recently proved very helpful in
sequence-to-sequence models. By using fast weights we can avoid the need to
store copies of neural activity patterns.

1

Introduction

Ordinary recurrent neural networks typically have two types of memory that have very different time
scales, very different capacities and very different computational roles. The history of the sequence
currently being processed is stored in the hidden activity vector, which acts as a short-term memory
that is updated at every time step. The capacity of this memory is O(H) where H is the number
of hidden units. Long-term memory about how to convert the current input and hidden vectors into
the next hidden vector and a predicted output vector is stored in the weight matrices connecting the
hidden units to themselves and to the inputs and outputs. These matrices are typically updated at the
end of a sequence and their capacity is O(H 2 ) + O(IH) + O(HO) where I and O are the numbers
of input and output units.
Long short-term memory networks [Hochreiter and Schmidhuber, 1997] are a more complicated
type of RNN that work better for discovering long-range structure in sequences for two main reasons:
First, they compute increments to the hidden activity vector at each time step rather than recomputing
the full vector1 . This encourages information in the hidden states to persist for much longer. Second,
they allow the hidden activities to determine the states of gates that scale the effects of the weights.
These multiplicative interactions allow the effective weights to be dynamically adjusted by the input
or hidden activities via the gates. However, LSTMs are still limited to a short-term memory capacity
of O(H) for the history of the current sequence.
Several researchers [Hinton and Plaut, 1987, Schmidhuber, 1992] have suggested that neural networks could benefit from a third form of memory that has much higher storage capacity than the
neural activities but much faster dynamics than the standard ?slow? weights. This memory could
store information specific to the history of the current sequence so that this information is available
to influence the ongoing processing without using up the memory capacity of the hidden activities.
1

This assumes the ?remember gates ? of the LSTM memory cells are set to one.

Until recently, however, there was surprisingly little investigation of other forms of memory in recurrent nets despite strong psychological evidence that it exists and obvious computational reasons
why it was needed.

2

Evidence from physiology that temporary memory may not be stored as
neural activities

Processes like working memory, attention, and priming operate on timescale of 100ms to minutes.
This is simultaneously too slow to be mediated by neural activations without dynamical attractor
states (10ms timescale) and too fast for long-term synaptic plasticity mechanisms to kick in (minutes
to hours). While artificial neural network research has typically focused on methods to maintain
temporary state in activation dynamics, that focus may be inconsistent with evidence that the brain
also?or perhaps primarily?maintains temporary state information by short-term synaptic plasticity
mechanisms [Tsodyks et al., 1998, Abbott and Regehr, 2004, Barak and Tsodyks, 2007].
The brain implements a variety of short-term plasticity mechanisms that operate on intermediate
timescale. For example, short term facilitation is implemented by leftover [Ca2+ ] in the axon terminal after depolarization while short term depression is implemented by presynaptic neurotransmitter
depletion Zucker and Regehr [2002]. Spike-time dependent plasticity can also be invoked on this
timescale [Markram et al., 1997, Bi and Poo, 1998]. These plasticity mechanisms are all synapsespecific. Thus they are more accurately modeled by a memory with O(H 2 ) capacity than the O(H)
of standard recurrent artificial recurrent neural nets and LSTMs.

3

Fast Associative Memory

One of the main preoccupations of neural network research in the 1970s and early 1980s [Willshaw
et al., 1969, Kohonen, 1972, Anderson and Hinton, 1981, Hopfield, 1982] was the idea that memories
were not stored by somehow keeping copies of patterns of neural activity. Instead, these patterns
were reconstructed when needed from information stored in the weights of an associative network
and the very same weights could store many different memories An auto-associative memory that
has N 2 weights cannot be expected to store more that N real-valued vectors with N components
each. How close we can come to this upper bound depends on which storage rule we use. Hopfield
nets use a simple, one-shot, outer-product storage rule and achieve a capacity of approximately
0.15N binary vectors using weights that require log(N ) bits each. Much more efficient use can
be made of the weights by using an iterative, error correction storage rule to learn weights that can
retrieve each bit of a pattern from all the other bits [Gardner, 1988], but for our purposes maximizing
the capacity is less important than having a simple, non-iterative storage rule, so we will use an outer
product rule to store hidden activity vectors in fast weights that decay rapidly. The usual weights in
an RNN will be called slow weights and they will learn by stochastic gradient descent in an objective
function taking into account the fact that changes in the slow weights will lead to changes in what
gets stored automatically in the fast associative memory.
A fast associative memory has several advantages when compared with the type of memory assumed
by a Neural Turing Machine (NTM) [Graves et al., 2014], Neural Stack [Grefenstette et al., 2015], or
Memory Network [Weston et al., 2014]. First, it is not at all clear how a real brain would implement
the more exotic structures in these models e.g., the tape of the NTM, whereas it is clear that the brain
could implement a fast associative memory in synapses with the appropriate dynamics. Second, in
a fast associative memory there is no need to decide where or when to write to memory and where
or when to read from memory. The fast memory is updated all the time and the writes are all
superimposed on the same fast changing component of the strength of each synapse. Every time the
input changes there is a transition to a new hidden state which is determined by a combination of
three sources of information: The new input via the slow input-to-hidden weights, C, the previous
hidden state via the slow transition weights, W , and the recent history of hidden state vectors via
the fast weights, A. The effect of the first two sources of information on the new hidden state can be
computed once and then maintained as a sustained boundary condition for a brief iterative settling
process which allows the fast weights to influence the new hidden state. Assuming that the fast
weights decay exponentially, we now show that the effect of the fast weights on the hidden vector
during an iterative settling phase is to provide an additional input that is proportional to the sum over
2

Sustained
boundary
condition

.
.
.
.

Slow
transition
weights
Fast
transition
weights

Figure 1: The fast associative memory model.
all recent hidden activity vectors of the scalar product of that recent hidden vector with the current
hidden activity vector, with each term in this sum being weighted by the decay rate raised to the
power of how long ago that hidden vector occurred. So fast weights act like a kind of attention to
the recent past but with the strength of the attention being determined by the scalar product between
the current hidden vector and the earlier hidden vector rather than being determined by a separate
parameterized computation of the type used in neural machine translation models [Bahdanau et al.,
2015].
The update rule for the fast memory weight matrix, A, is simply to multiply the current fast weights
by a decay rate, ?, and add the outer product of the hidden state vector, h(t), multiplied by a learning
rate, ?:
A(t) = ?A(t ? 1) + ?h(t)h(t)T
(1)
The next vector of hidden activities, h(t + 1), is computed in two steps. The ?preliminary? vector
h0 (t + 1) is determined by the combined effects of the input vector x(t) and the previous hidden
vector: h0 (t + 1) = f (W h(t) + Cx(t)), where W and C are slow weight matrices and f (.)
is the nonlinearity used by the hidden units. The preliminary vector is then used to initiate an
?inner loop? iterative process which runs for S steps and progressively changes the hidden state into
h(t + 1) = hS (t + 1)
hs+1 (t + 1) = f ([W h(t) + Cx(t)] + A(t)hs (t + 1)),
(2)
where the terms in square brackets are the sustained boundary conditions. In a real neural net,
A could be implemented by rapidly changing synapses but in a computer simulation that uses sequences which have fewer time steps than the dimensionality of h, A will be of less than full rank
and it is more efficient to compute the term A(t)hs (t+1) without ever computing the full fast weight
matrix, A. Assuming A is 0 at the beginning of the sequence,
? =t
X
A(t) = ?
?t?? h(? )h(? )T
(3)
? =1

A(t)hs (t + 1) = ?

? =t
X

?t?? h(? )[h(? )T hs (t + 1)]

(4)

? =1

The term in square brackets is just the scalar product of an earlier hidden state vector, h(? ), with the
current hidden state vector, hs (t + 1), during the iterative inner loop. So at each iteration of the inner
loop, the fast weight matrix is exactly equivalent to attending to past hidden vectors in proportion
to their scalar product with the current hidden vector, weighted by a decay factor. During the inner
loop iterations, attention will become more focussed on past hidden states that manage to attract the
current hidden state.
The equivalence between using a fast weight matrix and comparing with a set of stored hidden state
vectors is very helpful for computer simulations. It allows us to explore what can be done with fast
weights without incurring the huge penalty of having to abandon the use of mini-batches during
training. At first sight, mini-batches cannot be used because the fast weight matrix is different for
every sequence, but comparing with a set of stored hidden vectors does allow mini-batches.
3

3.1

Layer normalized fast weights

A potential problem with fast associative memory is that the scalar product of two hidden vectors
could vanish or explode depending on the norm of the hidden vectors. Recently, layer normalization
[Ba et al., 2016] has been shown to be very effective at stablizing the hidden state dynamics in RNNs
and reducing training time. Layer normalization is applied to the vector of summed inputs to all the
recurrent units at a particular time step. It uses the mean and variance of the components of this
vector to re-center and re-scale those summed inputs. Then, before applying the nonlinearity, it includes a learned, neuron-specific bias and gain. We apply layer normalization to the fast associative
memory as follows:
hs+1 (t + 1) = f (LN [W h(t) + Cx(t) + A(t)hs (t + 1)])

(5)

where LN [.] denotes layer normalization. We found that applying layer normalization on each
iteration of the inner loop makes the fast associative memory more robust to the choice of learning
rate and decay hyper-parameters. For the rest of the paper, fast weight models are trained using
layer normalization and the outer product learning rule with fast learning rate of 0.5 and decay rate
of 0.95, unless otherwise noted.

4

Experimental results

To demonstrate the effectiveness of the fast associative memory, we first investigated the problems
of associative retrieval (section 4.1) and MNIST classification (section 4.2). We compared fast
weight models to regular RNNs and LSTM variants. We then applied the proposed fast weights
to a facial expression recognition task using a fast associative memory model to store the results
of processing at one level while examining a sequence of details at a finer level (section 4.3). The
hyper-parameters of the experiments were selected through grid search on the validation set. All
the models were trained using mini-batches of size 128 and the Adam optimizer [Kingma and Ba,
2014]. A description of the training protocols and the hyper-parameter settings we used can be
found in the Appendix. Lastly, we show that fast weights can also be used effectively to implement
reinforcement learning agents with memory (section 4.4).
4.1

Associative retrieval

We start by demonstrating that the method we propose for storing and retrieving temporary memories works effectively for a toy task to which it is very well suited. Consider a task where multiple
key-value pairs are presented in a sequence. At the end of the sequence, one of the keys is presented
and the model must predict the value that was temporarily associated with the key. We used strings
that contained characters from English alphabet, together with the digits 0 to 9. To construct a training sequence, we first randomly sample a character from the alphabet without replacement. This is
the first key. Then a single digit is sampled as the associated value for that key. After generating a
sequence of K character-digit pairs, one of the K different characters is selected at random as the
query and the network must predict the associated digit. Some examples of such string sequences
and their targets are shown below:
Input string Target
c9k8j3f1??c 9
j0a5s5z2??a 5
where ??? is the token to separate the query from the key-value pairs. We generated 100,000 training
examples, 10,000 validation examples and 20,000 test examples. To solve this task, a standard RNN
has to end up with hidden activities that somehow store all of the key-value pairs after the keys and
values are presented sequentially. This makes it a significant challenge for models only using slow
weights.
We used a neural network with a single recurrent layer for this experiment. The recurrent network
processes the input sequence one character at a time. The input character is first converted into a
learned 100-dimensional embedding vector which then provides input to the recurrent layer2 . The
2

To make the architecture for this task more similar to the architecture for the next task we first compute a
50 dimensional embedding vector and then expand this to a 100-dimensional embedding.

4

Model
IRNN

R=20
62.11%

R=50
60.23%

R=100
0.34%

LSTM

60.81%

1.85%

0%

A-LSTM

60.13%

1.62%

0%

Fast weights

1.81%

0%

0%

Negative log likelihood

2.0

1
Table 1: Classification error rate comparison on the
associative retrieval task.

1.5
1.0

A-LSTM 50
IRNN 50
LSTM 50
FW 50

0.5
0.00

20 40 60 80 100 120 140
Updates x 5000

Figure 2: Comparison of the test log likelihood on
the associative retrieval task with 50 recurrent hidden
units.

output of the recurrent layer at the end of the sequence is then processed by another hidden layer
of 100 ReLUs before the final softmax layer. We augment the ReLU RNN with a fast associative
memory and compare it to an LSTM model with the same architecture. Although the original
LSTMs do not have explicit long-term storage capacity, recent work from Danihelka et al. [2016]
extended LSTMs by adding complex associative memory. In our experiments, we compared fast
associative memory to both LSTM variants.
Figure 1 and Table 1 show that when the number of recurrent units is small, the fast associative
memory significantly outperforms the LSTMs with the same number of recurrent units. The result
fits with our hypothesis that the fast associative memory allows the RNN to use its recurrent units
more effectively. In addition to having higher retrieval accuracy, the model with fast weights also
converges faster than the LSTM models.
4.2

Integrating glimpses in visual attention models

Despite their many successes, convolutional neural networks are computationally expensive and the
representations they learn can be hard to interpret. Recently, visual attention models [Mnih et al.,
2014, Ba et al., 2015, Xu et al., 2015] have been shown to overcome some of the limitations in
ConvNets. One can understand what signals the algorithm is using by seeing where the model is
looking. Also, the visual attention model is able to selectively focus on important parts of visual
space and thus avoid any detailed processing of much of the background clutter. In this section,
we show that visual attention models can use fast weights to store information about object parts,
though we use a very restricted set of glimpses that do not correspond to natural parts of the objects.
Given an input image, a visual attention model computes a sequence of glimpses over regions of the
image. The model not only has to determine where to look next, but also has to remember what it has
seen so far in its working memory so that it can make the correct classification later. Visual attention
models can learn to find multiple objects in a large static input image and classify them correctly,
but the learnt glimpse policies are typically over-simplistic: They only use a single scale of glimpses
and they tend to scan over the image in a rigid way. Human eye movements and fixations are far
more complex. The ability to focus on different parts of a whole object at different scales allows
humans to apply the very same knowledge in the weights of the network at many different scales,
but it requires some form of temporary memory to allow the network to integrate what it discovered
in a set of glimpses. Improving the model?s ability to remember recent glimpses should help the
visual attention model to discover non-trivial glimpse policies. Because the fast weights can store
all the glimpse information in the sequence, the hidden activity vector is freed up to learn how to
intelligently integrate visual information and retrieve the appropriate memory content for the final
classifier.
To explicitly verify that larger memory capacity is beneficial to visual attention-based models, we
simplify the learning process in the following way: First, we provide a pre-defined glimpse control
signal so the model knows where to attend rather than having to learn the control policy through
reinforcement learning. Second, we introduce an additional control signal to the memory cells so
the attention model knows when to store the glimpse information. A typical visual attention model is
complex and has high variance in its performance due to the need to learn the policy network and the
classifier at the same time. Our simplified learning procedure enables us to discern the performance
improvement contributed by using fast weights to remember the recent past.
5

Update fast
weights and
wipe out
hidden state
Integration
transition
weights
Slow
transition
weights
Fast
transition
weights

Figure 3: The multi-level fast associative memory model.
Model
IRNN

50 features
12.95%

100 features
1.95%

200 features
1.42%

LSTM

12%

1.55%

1.10%

ConvNet

1.81%

1.00%

0.9%

Fast weights

7.21%

1.30%

0.85%

Table 2: Classification error rates on MNIST.

We consider a simple recurrent visual attention model that has a similar architecture to the RNN from
the previous experiment. It does not predict where to attend but rather is given a fixed sequence of
locations: the static input image is broken down into four non-overlapping quadrants recursively
with two scale levels. The four coarse regions, down-sampled to 7 ? 7, along with their the four
7 ? 7 quadrants are presented in a single sequence as shown in Figure 1. Notice that the two glimpse
scales form a two-level hierarchy in the visual space. In order to solve this task successfully, the
attention model needs to integrate the glimpse information from different levels of the hierarchy.
One solution is to use the model?s hidden states to both store and integrate the glimpses of different
scales. A much more efficient solution is to use a temporary ?cache? to store any of the unfinished
glimpse computation when processing the glimpses from a finer scale in the hierarchy. Once the
computation is finished at that scale, the results can be integrated with the partial results at the
higher level by ?popping? the previous result from the ?cache?. Fast weights, therefore, can act as
a neurally plausible ?cache? for storing partial results. The slow weights of the same model can
then specialize in integrating glimpses at the same scale. Because the slow weights are shared for
all glimpse scales, the model should be able to store the partial results at several levels in the same
set of fast weights, though we have only demonstrated the use of fast weights for storage at a single
level.
We evaluated the multi-level visual attention model on the MNIST handwritten digit dataset. MNIST
is a well-studied problem on which many other techniques have been benchmarked. It contains the
ten classes of handwritten digits, ranging from 0 to 9. The task is to predict the class label of an
isolated and roughly normalized 28x28 image of a digit. The glimpse sequence, in this case, consists
of 24 patches of 7 ? 7 pixels.
Table 2 compares classification results for a ReLU RNN with a multi-level fast associative memory against an LSTM that gets the same sequence of glimpses. Again the result shows that when
the number of hidden units is limited, fast weights give a significant improvement over the other
models. As we increase the memory capacities, the multi-level fast associative memory consistently
outperforms the LSTM in classification accuracy.
6

Figure 4: Examples of the near frontal faces from the MultiPIE dataset.

Test accuracy

IRNN
81.11

LSTM
81.32

ConvNet
88.23

Fast Weights
86.34

Table 3: Classification accuracy comparison on the facial expression recognition task.

Unlike models that must integrate a sequence of glimpses, convolutional neural networks process all
the glimpses in parallel and use layers of hidden units to hold all their intermediate computational
results. We further demonstrate the effectiveness of the fast weights by comparing to a three-layer
convolutional neural network that uses the same patches as the glimpses presented to the visual
attention model. From Table 2, we see that the multi-level model with fast weights reaches a very
similar performance to the ConvNet model without requiring any biologically implausible weight
sharing.
4.3

Facial expression recognition

To further investigate the benefits of using fast weights in the multi-level visual attention model, we
performed facial expression recognition tasks on the CMU Multi-PIE face database [Gross et al.,
2010]. The dataset was preprocessed to align each face by eyes and nose fiducial points. It was
downsampled to 48 ? 48 greyscale. The full dataset contains 15 photos taken from cameras with
different viewpoints for each illumination ? expression ? identity ? session condition. We used
only the images taken from the three central cameras corresponding to ?15? , 0? , 15? views since
facial expressions were not discernible from the more extreme viewpoints. The resulting dataset
contained > 100, 000 images. 317 identities appeared in the training set with the remaining 20
identities in the test set.
Given the input face image, the goal is to classify the subject?s facial expression into one of the six
different categories: neutral, smile, surprise, squint, disgust and scream. The task is more realistic
and challenging than the previous MNIST experiments. Not only does the dataset have unbalanced
numbers of labels, some of the expressions, for example squint and disgust, are are very hard to distinguish. In order to perform well on this task, the models need to generalize over different lighting
conditions and viewpoints. We used the same multi-level attention model as in the MNIST experiments with 200 recurrent hidden units. The model sequentially attends to non-overlapping 12x12
pixel patches at two different scales and there are, in total, 24 glimpses. Similarly, we designed a
two layer ConvNet that has a 12x12 receptive fields.
From Table 3, we see that the multi-level fast weights model that knows when to store information
outperforms the LSTM and the IRNN. The results are consistent with previous MNIST experiments.
However, ConvNet is able to perform better than the multi-level attention model on this near frontal
face dataset. We think the efficient weight-sharing and architectural engineering in the ConvNet
combined with the simultaneous availability of all the information at each level of processing allows
the ConvNet to generalize better in this task. Our use of a rigid and predetermined policy for where
to glimpse eliminates one of the main potential advantages of the multi-level attention model: It can
process informative details at high resolution whilst ignoring most of the irrelevant details. To realize
this advantage we will need to combine the use of fast weights with the learning of complicated
policies.
7

1.0

1.0

RNN
RNN+FW
LSTM

0.8

0.5

Avgerage Reward

Avgerage Reward

0.6
0.4
0.2
0.0
0.2

0.5

0.4
RNN
RNN+FW
LSTM

0.6
0.8

0.0

0

2

(a)

4

6

8

steps

10

12

(b)

14

1.0

0

5

10

15

steps

20

25

30

(c)

Figure 5: a) Sample screen from the game ?Catch? b) Performance curves for Catch with N =
16, M = 3. c) Performance curves for Catch with N = 24, M = 5.
4.4

Agents with memory

While different kinds of memory and attention have been studied extensively in the supervised
learning setting [Graves, 2014, Mnih et al., 2014, Bahdanau et al., 2015], the use of such models for
learning long range dependencies in reinforcement learning has received less attention.
We compare different memory architectures on a partially observable variant of the game ?Catch?
described in [Mnih et al., 2014]. The game is played on an N ? N screen of binary pixels and each
episode consists of N frames. Each trial begins with a single pixel, representing a ball, appearing
somewhere in the first row of the column and a two pixel ?paddle? controlled by the agent in the
bottom row. After observing a frame, the agent gets to either keep the paddle stationary or move it
right or left by one pixel. The ball descends by a single pixel after each frame. The episode ends
when the ball pixel reaches the bottom row and the agent receives a reward of +1 if the paddle
touches the ball and a reward of ?1 if it doesn?t. Solving the fully observable task is straightforward
and requires the agent to move the paddle to the column with the ball. We make the task partiallyobservable by providing the agent blank observations after the M th frame. Solving the partiallyobservable version of the game requires remembering the position of the paddle and ball after M
frames and moving the paddle to the correct position using the stored information.
We used the recently proposed asynchronous advantage actor-critic method [Mnih et al., 2016] to
train agents with three types of memory on different sizes of the partially observable Catch task. The
three agents included a ReLU RNN, an LSTM, and a fast weights RNN. Figure 5 shows learning
progress of the different agents on two variants of the game N = 16, M = 3 and N = 24, M = 5.
The agent using the fast weights architecture as its policy representation (shown in green) is able to
learn faster than the agents using ReLU RNN or LSTM to represent the policy. The improvement
obtained by fast weights is also more significant on the larger version of the game which requires
more memory.

5

Conclusion

This paper contributes to machine learning by showing that the performance of RNNs on a variety
of different tasks can be improved by introducing a mechanism that allows each new state of the
hidden units to be attracted towards recent hidden states in proportion to their scalar products with
the current state. Layer normalization makes this kind of attention work much better. This is a form
of attention to the recent past that is somewhat similar to the attention mechanism that has recently
been used to dramatically improve the sequence-to-sequence RNNs used in machine translation.
The paper has interesting implications for computational neuroscience and cognitive science. The
ability of people to recursively apply the very same knowledge and processing apparatus to a whole
sentence and to an embedded clause within that sentence or to a complex object and to a major part
of that object has long been used to argue that neural networks are not a good model of higher-level
cognitive abilities. By using fast weights to implement an associative memory for the recent past,
we have shown how the states of neurons could be freed up so that the knowledge in the connections
of a neural network can be applied recursively. This overcomes the objection that these models can
only do recursion by storing copies of neural activity vectors, which is biologically implausible.
8

References
Sepp Hochreiter and J?urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735?1780,
1997.
Geoffrey E Hinton and David C Plaut. Using fast weights to deblur old memories. In Proceedings of the ninth
annual conference of the Cognitive Science Society, pages 177?186. Erlbaum, 1987.
J?urgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent networks.
Neural Computation, 4(1):131?139, 1992.
Misha Tsodyks, Klaus Pawelzik, and Henry Markram. Neural networks with dynamic synapses. Neural
computation, 10(4):821?835, 1998.
LF Abbott and Wade G Regehr. Synaptic computation. Nature, 431(7010):796?803, 2004.
Omri Barak and Misha Tsodyks. Persistent activity in neural networks with dynamic synapses. PLoS Comput
Biol, 3(2):e35, 2007.
Robert S Zucker and Wade G Regehr. Short-term synaptic plasticity. Annual review of physiology, 64(1):
355?405, 2002.
Henry Markram, Joachim L?ubke, Michael Frotscher, and Bert Sakmann. Regulation of synaptic efficacy by
coincidence of postsynaptic aps and epsps. Science, 275(5297):213?215, 1997.
Guo-qiang Bi and Mu-ming Poo. Synaptic modifications in cultured hippocampal neurons: dependence on
spike timing, synaptic strength, and postsynaptic cell type. The Journal of neuroscience, 18(24):10464?
10472, 1998.
David J Willshaw, O Peter Buneman, and Hugh Christopher Longuet-Higgins. Non-holographic associative
memory. Nature, 1969.
Teuvo Kohonen. Correlation matrix memories. Computers, IEEE Transactions on, 100(4):353?359, 1972.
James A Anderson and Geoffrey E Hinton. Models of information processing in the brain. Parallel models of
associative memory, pages 9?48, 1981.
John J Hopfield. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences, 79(8):2554?2558, 1982.
Elizabeth Gardner. The space of interactions in neural network models. Journal of physics A: Mathematical
and general, 21(1):257, 1988.
Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014.
Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, and Phil Blunsom. Learning to transduce with
unbounded memory. In Advances in Neural Information Processing Systems, pages 1819?1827, 2015.
Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. arXiv preprint arXiv:1410.3916, 2014.
D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. In
International Conference on Learning Representations, 2015.
J. Ba, R. Kiros, and G. Hinton. Layer normalization. arXiv:1607.06450, 2016.
D. Kingma and J. L. Ba. Adam: a method for stochastic optimization. arXiv:1412.6980, 2014.
Ivo Danihelka, Greg Wayne, Benigno Uria, Nal Kalchbrenner, and Alex Graves. Associative long short-term
memory. arXiv preprint arXiv:1602.03032, 2016.
V. Mnih, N. Heess, A. Graves, and K. Kavukcuoglu. Recurrent models of visual attention. In Neural Information Processing Systems, 2014.
J. Ba, V. Mnih, and K. Kavukcuoglu. Multiple object recognition with visual attention. In International
Conference on Learning Representations, 2015.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In International Conference on Machine Learning, 2015.
Ralph Gross, Iain Matthews, Jeffrey Cohn, Takeo Kanade, and Simon Baker. Multi-pie. Image and Vision
Computing, 28(5):807?813, 2010.
A. Graves. Generating sequences with recurrent neural networks. arXiv:1308.0850, 2014.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy P Lillicrap, Tim Harley,
David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, 2016.

9


----------------------------------------------------------------

title: 1369-learning-continuous-attractors-in-recurrent-networks.pdf

Learning Continuous Attractors in
Recurrent Networks
H. Sebastian Seung
Bell Labs, Lucent Technologies
Murray Hill, NJ 07974
seung~bell-labs.com

Abstract
One approach to invariant object recognition employs a recurrent neural network as an associative memory. In the standard depiction of the
network's state space, memories of objects are stored as attractive fixed
points of the dynamics. I argue for a modification of this picture: if an
object has a continuous family of instantiations, it should be represented
by a continuous attractor. This idea is illustrated with a network that
learns to complete patterns. To perform the task of filling in missing information, the network develops a continuous attractor that models the
manifold from which the patterns are drawn. From a statistical viewpoint, the pattern completion task allows a formulation of unsupervised
learning in terms of regression rather than density estimation .

A classic approach to invariant object recognition is to use a recurrent neural network as an associative memory[l]. In spite of the intuitive appeal and biological
plausibility of this approach, it has largely been abandoned in practical applications.
This paper introduces two new concepts that could help resurrect it: object representation by continuous attractors, and learning attractors by pattern completion.

In most models of associative memory, memories are stored as attractive fixed points
at discrete locations in state space[l]. Discrete attractors may not be appropriate for
patterns with continuous variability, like the images of a three-dimensional object
from different viewpoints. When the instantiations of an object lie on a continuous
pattern manifold, it is more appropriate to represent objects by attractive manifolds
of fixed points, or continuous attractors.
To make this idea practical, it is important to find methods for learning attractors
from examples. A naive method is to train the network to retain examples in shortterm memory. This method is deficient because it does not prevent the network
from storing spurious fixed points that are unrelated to the examples. A superior
method is to train the network to restore examples that have been corrupted, so
that it learns to complete patterns by filling in missing information.

Learning Continuous Attractors in Recurrent Networks

(a)

655

(b)

Figure 1: Representing objects by dynamical attractors. (a) Discrete attractors.
(b) Continuous attractors.

Learning by pattern completion can be understood from both dynamical and statistical perspectives. Since the completion task requires a large basin of attraction
around each memory, spurious fixed points are suppressed. The completion task
also leads to a formulation of unsupervised learning as the regression problem of
estimating functional dependences between variables in the sensory input.
Density estimation, rather than regression, is the dominant formulation of unsupervised learning in stochastic neural networks like the Boltzmann machine[2] . Density
estimation has the virtue of suppressing spurious fixed points automatically, but it
also has the serious drawback of being intractable for many network architectures.
Regression is a more tractable, but nonetheless powerful, alternative to density
estimation.
In a number of recent neurobiological models, continuous attractors have been used
to represent continuous quantities like eye position-[3], direction of reaching[4], head
direction[5], and orientation of a visual stimulus[6]. Along with these models, the
present work is part of a new paradigm for neural computation based on continuous
attractors.

1

DISCRETE VERSUS CONTINUOUS ATTRACTORS

Figure 1 depicts two ways of representing objects as attractors of a recurrent neural
network dynamics. The standard way is to represent each object by an attractive
fixed point[l], as in Figure 1a. Recall of a memory is triggered by a sensory input,
which sets the initial conditions. The network dynamics converges to a fixed point,
thus retrieving a memory. If different instantiations of one object lie in the same
basin of attraction, they all trigger retrieval of the same memory, resulting in the
many-to-one map required for invariant recognition.
In Figure 1b, each object is represented by a continuous manifold of fixed points.
A one-dimensional manifold is shown, but generally the attractor should be multidimensional, and is parametrized by the instantiation or pose parameters of the
object . For example, in visual object recognition, the coordinates would include the
viewpoint from which the object is seen.
The reader should be cautioned that the term "continuous attractor" is an idealization and should not be taken too literally. In real networks, a continuous attractor
is only approximated by a manifold in state space along which drift is very slow.
This is illustrated by a simple example, a descent dynamics on a trough-shaped
energy landscape[3]. If the bottom of the trough is perfectly level, it is a line of
fixed points and an ideal continuous attract or of the dynamics. However, any slight
imperfections cause slow drift along the line. This sort of approximate continuous
attract or is what is found in real networks, including those trained by the learning

656

H S. Seung

(a)

hidden layer

(b)

~
visible layer

Figure 2: (a) Recurrent network. (b) Feedforward autoencoder.
algorithms to be discussed below.

2

DYNAMICS OF MEMORY RETRIEVAL

The preceding discussion has motivated the idea of representing pattern manifolds
by continuous attractors. This idea will be further developed with the simple network shown in Figure 2a, which consists of a visible layer Xl E Rnl and a hidden
layer X2 E Rn2. The architecture is recurrent, containing both bottom-up connections (the n2 x nl matrix W2d and top-down connections (the nl x n2 matrix
WI2). The vectors bl and b2 represent the biases ofthe neurons. The neurons have
a rectification nonlinearity [x]+ = max{x, O}, which acts on vectors component by
component.
There are many variants of recurrent network dynamics: a convenient choice is the
following discrete-time version, in which updates of the hidden and visible layers
alternate in time. After the visible layer is initialized with the input vector Xl (0),
the dynamics evolves as

X2(t)
Xl (t)

=

=

[b 2 + W2IXI(t -1)]+ ,
[b l + W12X2(t)]+ .

(1)

If memories are stored as attractors, iteration of this dynamics can be regarded as

memory retrieval.
Activity circulates around the feedback loop between the two layers. One iteration
of this loop is the map Xl(t - 1) ~ X2(t) ~ Xl(t). This single iteration is equivalent to the feedforward architecture of Figure 2b. In the case where the hidden
layer is smaller than the visible layers, this architecture is known as an auto encoder network[7]. Therefore the recurrent network dynamics (1) is equivalent to
repeated iterations of the feedforward autoencoder. This is just the standard trick
of unfolding the dynamics of a recurrent network in time, to yield an equivalent
feedforward network with many layers[7]. Because of the close relationship between
the recurrent network of Figure 2a and the autoencoder of Figure 2b, it should not
be surprising that learning algorithms for these two networks are also related, as
will be explained below.

3

LEARNING TO RETAIN PATTERNS

Little trace of an arbitrary input vector Xl (0) remains after a few time steps of the
dynamics (1). However, the network can retain some input vectors in short-term
memory as "reverberating" patterns of activity. These correspond to fixed points of
the dynamics (1); they are patterns that do not change as activity circulates around
the feedback loop.

Learning Continuous Attraclors in Recurrent Networlcs

657

This suggests a formulation of learning as the optimization of the network's ability to
retain examples in short-term memory. Then a suitable cost function is the squared
difference IXI (T) - Xl (0)12 between the example pattern Xl (0) and the network's
short-term memory Xl (T) of it after T time steps. Gradient descent on this cost
function can be done via backpropagation through time[7].
If the network is trained with patterns drawn from a continuous family, then it can

learn to perform the short-term memory task oy developing a continuous attractor
that lies near the examples it is trained on. When the hidden layer is smaller than
the visible layer, the dimensionality of the attractor is limited by the size of the
hidden layer.
For the case of a single time step (T = 1), training the recurrent network of Figure
2a to retain patterns is equivalent to training the autoencoder of Figure 2b by
minimizing the squared difference between its input and output layers, averaged
over the examples[8]. From the information theoretic perspective, the small hidden
layer in Figure 2b acts as a bottleneck between the input and output layers, forcing
the autoencoder to learn an efficient encoding of the input.
For the special case of a linear network, the nature of the learned encoding is
understood completely. Then the input and output vectors are related by a simple
matrix multiplication. The rank of the matrix is equal to the number of hidden
units. The average distortion is minimized when this matrix becomes a projection
operator onto the subspace spanned by the principal components of the examples[9].
From the dynamical perspective, the principal subspace is a continuous attractor
of the dynamics (1). The linear network dynamics converges to this attractor in
a single iteration, starting from any initial condition. Therefore we can interpret
principal component analysis and its variants as methods of learning continuous
attractors[lO].

4

LEARNING TO COMPLETE PATTERNS

Learning to retain patterns in short-term memory only works properly for architectures with a small hidden layer. The problem with a large hidden layer is evident
when the hidden and visible layers are the same size, and the neurons are linear.
Then the cost function for learning can be minimized by setting the weight matrices
equal to the identity, W 21 = W l2 = I. For this trivial minimum, every input vector
is a fixed point of the recurrent network (Figure 2a), and the equivalent feedforward
network (Figure 2b) exactly realizes the identity map. Clearly these networks have
not learned anything.
Therefore in the case of a large hidden layer, learning to retain patterns is inadequate. Without the bottleneck in the architecture, there is no pressure on the
feedforward network to learn an efficient encoding. Without constraints on the dimension of the attractor, the recurrent network develops spurious fixed points that
have nothing to do with the examples.
These problems can be solved by a different formulation of learning based on the
task of pattern completion. In the completion task of Figure 3a, the network is
initialized with a corrupted version of an example. Learning is done by minimizing
the completion error, which is the squared difference IXI (T) - dl 2 between the uncorrupted pattern d and the final visible vector Xl (T). Gradient descent on completion
error can be done with backpropagation through time[ll].
This new formulation of learning eliminates the trivial identity map solution men-

H. S. Seung

658

(a)

~1
L

_

retention.

~1
..

_

1

~ completio~ ~
It

~

It

___

(b)

topographic feature map

9x9 patch
missing

sensory
Input

retrieved
memory

Figure 3: (a) Pattern retention versus completion. (b) Dynamics of pattern completion.
(b)

5x5 receptive fields

Figure 4: (a) Locally connected architecture. (b) Receptive fields of hidden neurons.
tioned above: while the identity network can retain any example, it cannot restore
corrupted examples to their pristine form. The completion task forces the network
to enlarge the basins of attraction of the stored memories, which suppresses spurious fixed points. It also forces the network to learn associations between variables
in the sensory input.

5

LOCALLY CONNECTED ARCHITECTURE

Experiments were conducted with images of handwritten digits from the USPS
database described in [12]. The example images were 16 x 16, with a gray scale
ranging from a to 1. The network was trained on a specific digit class, with the
goal of learning a single pattern manifold. Both the network architecture and the
nature of the completion task were chosen to suit the topographic structure present
in visual images.
The network architecture was given a topographic organization by constraining the
synaptic connectivity to be local, as shown in Figure 4a. Both the visible and hidden
layers of the network were 16 x 16. The visible layer represented an image, while
the hidden layer was a topographic feature map. Each neuron had 5 x 5 receptive
and projective fields, except for neurons near the edges, which had more restricted
connectivity.
In the pattern completion task, example images were corrupted by zeroing the
pixels inside a 9 x 9 patch chosen at a random location, as shown in Figure 3a.
The location of the patch was randomized for each presentation of an example.
The size of the patch was a substantial fraction of the 16 x 16 image, and much
larger than the 5 x 5 receptive field size. This method of corrupting the examples
gave the completion task a topographic nature, because it involved a set of spatially
contiguous pixels. This topographic nature would have been lacking if the examples
had been corrupted by, for example, the addition of spatially uncorrelated noise.
Figure 3b illustrates the dynamics of pattern completion performed by a network

Learning Continuous Attractors in Recurrent Networks

659

trained on examples of the digit class "two." The network is initialized with a
corrupted example of a "two." After the first itex:ation of the dynamics, the image
is partially restored. The second iteration leads to superior restoration, with further
sharpening of the image. The "filling in" phenomenon is also evident in the hidden
layer.
The network was first trained on a retrieval dynamics of one iteration. The resulting
biases and synaptic weights were then used as initial conditions for training on a
retrieval dynamics of two iterations. The hidden layer developed into a topographic
feature map suitable for representing images of the digit "two." Figure 4b depicts
the bottom-up receptive fields of the 256 hidden neurons. The top-down projective
fields of these neurons were similar, but are not shown.
This feature map is distinct from others[13) because of its use of top-down and
bottom-up connections in a feedback loop. The bottom-up connections analyze
images into their constituent features, while the top-down connections synthesize
images by composing features. The features in the top-down connections can be
regarded as a "vocabulary" for synthesis of images. Since not all combinations of
features are proper patterns, there must be some "grammatical" constraints on their
combination. The network's ability to complete patterns suggests that some of these
constraints are embedded in the dynamical equations of the network. Therefore the
relaxation dynamics (1) can be regarded as a process of massively parallel constraint
satisfaction.

6

CONCLUSION

I have argued that continuous attractors are a natural representation for pattern
manifolds. One method of learning attractors is to train the network to retain
examples in short-term memory. This method is equivalent to autoencoder learning,
and does not work if the number of hidden units is large. A better method is to train
the network to complete patterns. For a locally connected network, this method
was demonstrated to learn a topographic feature map. The trained network is able
to complete patterns, indicating that syntactic constraints on the combination of
features are embedded in the network dynamics.
Empirical evidence that the network has indeed learned a continuous attractor is
obtained by local linearization of the network (1). The linearized dynamics has
many eigenvalues close to unity, indicating the existence of an approximate continuous attractor. Learning with an increased number of iterations in the retrieval
dynamics should improve the quality of the approximation.
There is only one aspect of the learning algorithm that is specifically tailored for
continuous attractors. This aspect is the limitation of the retrieval dynamics (1)
to a few iterations, rather than iterating it all the way to a true fixed point. As
mentioned earlier, a continuous attractor is only an idealization; in a real network
it does not consist of true fixed points, but is just a manifold to which relaxation is
fast and along which drift is slow. Adjusting the shape of this manifold is the goal
of learning; the exact locations of the true fixed points are not relevant.
The use of a fast retrieval dynamics removes one long-standing objection to attractor
neural networks, which is that true convergence to a fixed point takes too long. If all
that is desired is fast relaxation to an approximate continuous attractor, attractor
neural networks are not much slower than feedforward networks.
In the experiments discussed here, learning was done with backpropagation through

time. Contrastive Hebbian learning[14] is a simpler alternative. Part of the image

660

H S. Seung

is held clamped, the missing values are filled in by convergence to a fixed point,
and an anti-Hebbian update is made. Then the missing values are clamped at their
correct values, the network converges to a new fixed point, and a Hebbian update
is made. This procedure has the disadvantage of requiring true convergence to a
fixed point, which can take many iterations. It also requires symmetric connections,
which may be a representational handicap.
This paper addressed only the learning of a single attractor to represent a single
pattern manifold. The problem of learning multiple attractors to represent mUltiple
pattern classes will be discussed elsewhere, along with the extension to network
architectures with many layers.
Acknowledgments This work was supported by Bell Laboratories. I thank J. J.
Hopfield, D. D. Lee, L. K. Saul, N. D. Socci, H. Sompolinsky, and D. W. Tank for
helpful discussions.

References
[1] J. J. Hopfield. Neural networks and physical systems with emergent collective computational abilities. Proc. Nat. Acad. Sci. USA, 79:2554-2558, 1982.
[2] D. H. Ackley, G. E. Hinton, and T. J. Sejnowski. A learning algorithm for Boltzmann
machines. Cognitive Science, 9:147-169, 1985.
[3] H. S. Seung. How the brain keeps the eyes still. Proc . Natl. Acad. Sci. USA,93:1333913344, 1996.
[4] A. P. Georgopoulos, M. Taira, and A. Lukashin. Cognitive neurophysiology of the
motor cortex. Science, 260:47-52, 1993.
[5] K Zhang. Representation of spatial orientation by the intrinsic dynamics of the
head-direction cell ensemble: a theory. J. Neurosci., 16:2112-2126, 1996.
[6] R . Ben-Yishai, R. L. Bar-Or, and H. Sompolinsky. Theory of orientation tuning in
visual cortex. Proc. Nat. Acad. Sci. USA, 92:3844-3848, 1995 .
[7] D.E. Rumelhart, G.E. Hinton, and R.J . Williams. Learning internal representations
by error propagation. In D.E. Rumelhart and J.L. McClelland, editors, Parallel Distributed Processing, volume 1, chapter 8, pages 318-362. MIT Press, Cambridge, 1986.
[8] G. W . Cottrell, P. Munro, and D. Zipser. Image compression by back propagation: an
example of extensional programming. In N. E. Sharkey, editor, Models of cognition:
a review of cognitive science. Ablex, Norwood, NJ, 1989.
[9] P. Baldi and K Hornik. Neural networks and principal component analysis: Learning
from examples without local minima. Neural Networks, 2:53-58, 1989.
[10] H. S. Seung. Pattern analysis and synthesis in attractor neural networks. In K-Y. M.
Wong, 1. King, and D.-y' Yeung, editors, Theoretical Aspects of Neural Computation :
A Multidisciplinary Perspective, Singapore, 1997. Springer-Verlag.
[11] F.-S. Tsung and G. W . Cottrell. Phase-space learning. Adv. Neural Info . Proc. Syst.,
7:481-488, 1995.
[12] Y. LeCun et al. Learning algorithms for classification: a comparison on handwritten
digit recognition. In J.-H. Oh, C. Kwon, and S. Cho, editors, Neural networks: the
statistical mechanics perspective, pages 261-276, Singapore, 1995. World Scientific.
[13] T . Kohonen. The self-organizing map. Proc. IEEE, 78:1464-1480, 1990.
[14] J. J . Hopfield, D. I. Feinstein, and R. G. Palmer. "Unlearning" has a stabilizing effect
in collective memories. Nature, 304:158-159, 1983.


----------------------------------------------------------------

title: 177-neural-network-star-pattern-recognition-for-spacecraft-attitude-determination-and-control.pdf

314

NEURAL NETWORK STAR PATTERN
RECOGNITION FOR SPACECRAFT ATTITUDE
DETERMINATION AND CONTROL
Phillip Alvelda, A. Miguel San Martin
The Jet Propulsion Laboratory,
California Institute of Technology,
Pasadena, Ca. 91109
ABSTRACT
Currently, the most complex spacecraft attitude determination
and control tasks are ultimately governed by ground-based
systems and personnel. Conventional on-board systems face
severe computational bottlenecks introduced by serial
microprocessors operating on inherently parallel problems. New
computer architectures based on the anatomy of the human brain
seem to promise high speed and fault-tolerant solutions to the
limitations of serial processing. This paper discusses the latest
applications of artificial neural networks to the problem of star
pattern recognition for spacecraft attitude determination.

INTRODUCTION
By design, a conventional on-board microprocessor can perform only
one comparison or calculation at a time. Image or pattern recognition
problems involving large template sets and high resolution can require
an astronomical number of comparisons to a given database. Typical
mission planning and optimization tasks require calculations involving
a multitude of parameters, where each element has an inherent degree
of importance, reliability and noise.
Even the most advanced
supercomputers running the latest software can require seconds and
even minutes to execute a complex pattern recognition or expert system
task, often providing incorrect or inefficient solutions to problems that
prove trivial to ground control specialists.
The intent of ongoing research is to develop a neural network based
satellite attitude determination system prototype capable of determining
its current three-axis inertial orientation. Such a system that can
determine in real-time, which direction the satellite is facing, is needed
in order to aim antennas, science instruments, and navigational
equipment. For a satellite to be autonomous (an important criterion in
interplanetary missions, and most particularly so in the event of a
system failure), this task must be performed in a reasonable amount of
time with all due consideration to actual environmental, noise and
precision constraints.
CELESTIAL ATTITUDE DETERMINATION
Under normal operating conditions there is a whole repertoire of
spacecraft systems that operate in conjunction to perform the attitude
determination task, the backbone of which is the Gyro. But a Gyro
measures only chaDles in orientation. The current attitude is stored in

Neural Network Star Pattern Recognition
volatile on-board memory and is updated as the ,yro system inte,rates
velocity to provide chanle in anlular position. When there is a power
system failure for any reason such as a sinlle-event-upset due to cosmic
radiation, an currently stored attitude lafor.atloa Is LOST!
One very attractive way of recoverinl attitude information with no
a priori knowledge is by USinl on-board imalinl and computer systems
to:
1.)
Image a portion of the sky,
2.)

Compare the characteristic pattern of stars in the sensor fieldof-view to an on-board star catalog,

3.)

Thereby identify the stars in the sensor FOV [Field Of View],

4.)

Retrieve the identified star coordinates,

5.)

Transform and correlate FOV and real-sky coordinates to
determine spacecraft attitude.

But the problem of matching a limited field of view that contains a
small number of stars (out of billions and billions of them), to an onboard fUll-sky catalol containing perhaps thousands of stars has lonl
been a severe computational bottleneck.

D14~---------;"':~~::.-r----?
D13'---~='7""T

/

, ; ', /,;

PAIR 21
PAIR 22

\

,

PAIR 703
PAIR 704

',STORED PAIR ADDRESS
,'"
PAIR 70121
PAIR 70122

GEOMETRIC
CONSTRAINTS

FicuN I.) Serial .tar I.D. catalol rorma' and rnethodololY.

The latest serial allorithm to perform this task requires
approximately 650 KBytes of RAM to store the on-board star catalol.
It incorporates a hilhly optimized allorithm which uses a motorola
68000 to search a sorted database of more than 70,000 star-pair distance
values for correlations with the decomposed star pattern in the sensor
FOV. It performs the identification process on the order of I second

318

316

Alvelda and San Martin
with a success rate of 99 percent. But it does Dot fit iD the spacecraft
oD-board memory, and therefore, no such system has flown on a
planetary spacecraft.
? USES SUN SENSOR AND ATTITUDE MANEUVERS
TO SUN

TO SUN

CANOPUS

FicuN J.) Current Spacecraft attitude inrormation recovery lequence.

As a result, state-of-the-art interplanetary spacecraft use several
independent sensor systems in ~onjunction to determine attitude with no
a priori knowledge. First, the craft is commanded to slew until a Sun
Sensor (aligned with the spacecraft's major axis) has locked-on to the
sun. The craft must then rotate around that axis until an appropriate
star pattern at approximately ninety degrees to the sun is acquired to
provide three-axis orientation information.
The entire attitude
acquisition sequence requires an absolute minimum of thirty minutes,
and presupposes that all spacecraft actuator and maneuvering systems
are operational. At the phenomenal rendezvous speeds involved in
interplanetary navigation, a system failure near mission culmination
could mean an almost complete loss of the most valuable scientific data
while the spacecraft performs its initial attitude acquisition sequence.
NEURAL MOTIVATION

The parallel architecture and collective computation properties of a
neural network based system address several problems associated with
the implementation and performance of the serial star ID algorithm.
Instead of searching a lengthy database one element at a time, each
stored star pattern is correlated with the field of view concurrently.
And whereas standard memory storage technology requires one address
in RAM per star-pair distance, the neural star pattern representations are
stored in characteristic matrices of interconnections between neurons.
This distributed data set representation has several desirable properties.
First of all, the 2N redundancy of the serial star-p.air scheme (i.e. which
star is at which end of a pair) is discarded and a new more compressed
representation emerges from the neuromorphic architecture. Secondly,
noise, both statistical (i.e thermal noise) and systematic (i.e. sensor
precision limitations), and pattern invariance characteristics are

Neural Network Star Pattern Recognition

incorporated directly into the preprocessing and neural architecture
without extra circuitry.
The first neural approach
The primary motivation from the NASA perspective is to improve
satellite attitude determination performance and enable on-board system
implementations. The problem methodology for the neural architecture
is then slightly different than that of the serial model.
Instead of identifying every detected st~r in the field of view, the
neural system identifies a single 'Guide Star' with respect to the pattern
of dimmer stars around it, and correlates that star's known position with
the sensor FOV to determine the pointing axis. If needed, only one other
star is then required to fix the roll angle about that axis. So the core
of the celestial attitude determination problem changes from multiple
star identification and correlation, single star pattern identification.
The entire system consists of several modules in a marriage of
different technologies. The first neural system architecture uses already
mature(i.e. sensor/preprocessor) technologies where they perform well,
and neural technology only where conventional systems prove
intractable. With an eye towards rapid prototyping and implementation,
the system was designed with technologies (such as neural VLSI) that
will be available in less than one year.
SYSTEM ARCHITECTURE
The Star Tracker sensor system
The system input is based on the ASTROS II star tracker under
development in the Guidance and Control section at the Jet Propulsion
Laboratory. The Star tracker optical system images a defocussed portion
of the sky (a star sub-field) onto a charged coupled device (C.C.D.). The
tracker electronics then generate star centroid position and intensity
information and passes this list to the preprocessing system.
The Preprocessln8 system
This centroia ind intensity information is passed to the preprocessing
subsystem where the star pattern is treated to extract noise and pattern
invariance. A 'pattern field-of-view' is defined as centered aroun~ ~he
brightest (Le. 'Guide Star') in the central portion of the sensor field-ofview. Since the pattern FOV radius is one half that of the sensor FOV
the pattern for that 'Guide Star' is then based on a portion of the image
that is complete, or invariant, under translational perturbation. The
preprocessor then introduces rotational invariance to the 'guide-star'
pattern by using only the distances of all other dimmer stars inside the
pattern FOV to the central guide star.
These distances are then mapped by the preprocessor onto a two
dimensional coordinate system of distance versus relative magnitude
(normalized to the guide star, the brightest star in the Pattern FOV) to
be sampled by the neural associative star catalog. The motivation for
this distance map format become clear when issues involving noise
invariance and memory capacity are considered.

31 7

318

Alvelda and San Martin
Because the ASTROS Star Tracker is a limited precision instrument,
most particularly in the absolute and relative intensity measures, two
major problems arise. First, dimmer stars with intensities near the
bottom of the dynamic range of the C.C.D. mayor may not be included
in the star pattern. So, the entire distance map is scaled to the brightest
star such that the bright, high-confidence measurements are weighted
more heavily, while the dimmer and possibly transient stars are of less
importance to a given pattern. Secondly, since there are a very large
number of stars in the sky, the uniqueness of a given star pattern is
governed mostly by the relative star distance measures (which, by the
way, are the highest precision measurements provided by the star
tracker).
In addition, because of the limitations in expected neural hardware,
a discrete number of neurons must sample a continuous function. To
retain the maximum sample precision with a minimum number of
neurons, the neural system uses the biological mechanism of a receptive
field for hyperacuity. In other words, a number of neurons respond to
a single distance stimulus. The process is analogous to that used on the
defocussed image of a point source on the C.C.D. which was integrated
over several pixels to generate a centroid at sub-pixel accuracies. To
relax the demands on hardware development for the neural module, this
point smoothing was performed in the preprocessor instead of being
introduced into the neural network architecture and dynamics. The
equivalent neural response function then becomes:
N

X?I

L

'l'k

e

-(Ili -

Ille )2/tl.

k=l

where:
Xi

is the sampling activity of neuron i

N

is the number of stars in the Pattern Field Of View

ILi

is the position of neuron i on the sample axis

ILk ? is the position of the stimulus from star k on the

sample axis
is the magnitude scale factor of star k, normalized
to the brightest star in the PFOV, the 'Guide star'
is the width of the gaussian point spread function

The Neural system
The neural system, a 106 neuron, three-layer, feed-forward network,
samples the scaled and smoothed distance map, to provide an output
vector with the highest neural output activity representing the best
match to one of the pre-trained guide star patterns. The network
training algorithm uses the standard backwards error propagation

Neural Network Star Pattern Recognition
algorithm to set network interconnect weights from a training set of
'Guide Star' patterns derived from the software simulated sky and
sensor models.
Simulation testbed
The computer simulation testbed includes a realistic celestial field
model, as well as a detector model that properly represents achievable
position and intensity resolution, sensor scan rates, dynamic range, and
signal to noise properties. Rapid identification of star patterns was
observed in limited training sets as the simulated tracker was oriented
randomly within the celestial sphere.

PERFORMANCE RESULTS AND PROJECTIONS

In terms of improved performance the neural system was quite a
success, but not however in the areas which were initialJy expected.
While a VLSI implementation might yield considerable system speed-up,
the digital simulation testbed neural processing time was of the same
order as the serial algorithm, perhaps slightly better. The success rate of
the serial system was already better than 99%. The neural net system
achieved an accuracy of 100% when the systematic noise (i.e. dropped
stars) of the sensor was neglected.
When the dropped star effect was introduced, the performance
figure dropped to 94%. It was later discovered that the reason for this
'low' rate was due mostly to the limited size of the Yale Bright Star
catalog at higher magnitudes (lower star brightness). In sparse regions
of the sky, the pattern in the sensor FOV presented by the limited sky
model occasionally consisted of only two or three dim stars. When one
or two of them drop out because of the Star sensor magnitude precision
limitations. at times. there was no pattern left to identify. Further
experiments and parametric studies are under way using a more
complete Harvard Smithsonian catalog.
The big gain was in terms of required memory. The serial algorithm
stored over 70,000 star pairs at high precision in addition to code for a
rather complex heuristic, artificial intelligence type of algorithm for a
total size of 650 KBytes. The Neural algorithm used a connectionist
data representation that was able to abstract from the star catalog,
pa ttern class similarities, orthagonalities, and in variances in a highly
compressed fashion.
Network performance remained essentially
constant until interconnect precision was decreased to less than four bits
per synapse. 3000 synapses at four bits per synapse requires very little
computer memory.
These simulation results were all derived from a monte carlo run of
approximately 200,000 iterations using the simulator testbed.

319

320

Alvelda and San Martin

CONCLUSIONS
By means of a clever combination of several technologies and an
appropriate data set representation, a star 10 system using one of the
most simple neural algorithms outperforms those using the classical
serial ones in several aspects, even while running a software simulated
neural network. The neural simulator is approximately ten times faster
than the equivalent serial algorithm and requires less than one seventh
the computer memory. With the transfer to neural VLSI technology,
memory requirements will virtually disappear and processing speed will
increase by at least an order of magnitude.
W1)ere power and weight requirements scale with the hardware chip
count, and every pound that must be launched into space costs millions
of dollars, neural technology has enabled real-time on-board absolute
attitude determination with no a priori information, that may
eventually make several accessory satellite systems like horizon and sun
sensors obsolete, while increasing the overall reliability of spacecraft
systems.
Ackaowledgmeats
We would like to acknowledge many fruitfull conversations with C. E.
Bell, J. Barhen and S. Gulati.

Refereaces
R. W. H. van Bezooijen. Automated Star Pattern Recognition for Use
With the Space Infrared Telescope Facility (SIRTIF). Paper for
internal use at The Jet Propulsion Laboratory.
P. Gorman, T. J. Sejnowski. Workshop on Neural Network Devices and
Applications (Jet Propulsion Laboratory, Pasaden, Ca.) Document 04406, pp.224-237.
J. L. Lunkins. Star pattern Recognition for Real Time Attitude
Determination. The Journal of Astronautical Science(l979).
D. E. Rummelhaft, G. E. Hinton. Parallel Distributed Processing, eds.
(MIT Press, Cambridge, Ma.) Vol. 1 pp. 318-364.
P. M Salomon, T. A. Glavich. Image Signal Processing and Sub-Pixel
Accuracy Star Trackers. SPfE vol. 252 Smart Sensors II (1980).

Neural Network Star Pattern Recognition

C.C.D . Image

Preprocessor

Distance
Map

Redtus from Guide Ster

Neural
Sampler
Neural
Output

aa DO a

II I
18

28

cD

DOc

I

I II

30
St

Star Attitude
Look-up Table

32
1/1

R.A .

41 50

Dec.

27 -1.3 2.45
:::::~f;r

29

}=:=n:, :::~J;=r:::=
0.2 0.68

a CDc

I

I

71

86

321

322

Alvelda and San Martin

PROTOTYPE HARDWARE IMPLEMENTATJON

SERIAL PR')CESS()R

TOACS

CCMROlLER PNO
SIC INTERFACE

:.......................................................................................................................................................................:

NEURAL
PROCESSOR
80

??????
q q Cl.JQ

q q

~Q

1
CORRELATOR

...............................................................................................................................................................................................................................


----------------------------------------------------------------

title: 2-the-capacity-of-the-kanerva-associative-memory-is-exponential.pdf

184

THE CAPACITY OF THE KANERVA ASSOCIATIVE MEMORY IS EXPONENTIAL
P. A. Choul
Stanford University. Stanford. CA 94305
ABSTRACT
The capacity of an associative memory is defined as the maximum
number of vords that can be stored and retrieved reliably by an address
vithin a given sphere of attraction. It is shown by sphere packing
arguments that as the address length increases. the capacity of any
associati ve memory is limited to an exponential grovth rate of 1 - h2 ( 0).
vhere h2(0) is the binary entropy function in bits. and 0 is the radius
of the sphere of attraction. This exponential grovth in capacity can
actually be achieved by the Kanerva associative memory. if its
parameters are optimally set . Formulas for these op.timal values are
provided. The exponential grovth in capacity for the Kanerva
associative memory contrasts sharply vith the sub-linear grovth in
capacity for the Hopfield associative memory.
ASSOCIATIVE MEMORY AND ITS CAPACITY
Our model of an associative memory is the folloving. Let ()(,Y) be
an (address. datum) pair. vhere )( is a vector of n ?ls and Y is a
vector of m ?ls. and let ()(l),y(I)), ... ,()(M) , y(M)). be M (address,
datum) pairs stored in an associative memory. If the associative memory
is presented at the input vith an address )( that is close to some
stored address )(W. then it should produce at the output a vord Y that
is close to the corresponding contents y(j). To be specific, let us say
that an associative memory can correct fraction 0 errors if an )( vi thin
Hamming distance no of )((j) retrieves Y equal to y(j). The Hamming
sphere around each )(W vill be called the sphere of attraction, and 0
viII be called the radius of attraction.
One notion of the capacity of this associative memory is the
maximum number of vords that it can store vhile correcting fraction 0
errors . Unfortunately. this notion of capacity is ill-defined. because
it depends on exactly vhich (address. datum) pairs have been stored.
Clearly. no associative memory can correct fraction 0 errors for every
sequence of stored (address, datum) pairs. Consider. for example, a
sequence in vhich several different vords are vritten to the same
address . No memory can reliably retrieve the contents of the
overvritten vords. At the other extreme. any associative memory ' can
store an unlimited number of vords and retrieve them all reliably. if
their contents are identical.
A useful definition of capacity must lie somevhere betveen these
tvo extremes. In this paper. ve are interested in the largest M such
that for most sequences of addresses XU), .. . , X(M) and most sequences of
data y(l), ... , y(M). the memory can correct fraction 0 errors. We define
IThis vork vas supported by the National Science Foundation under NSF
grant IST-8509860 and by an IBM Doctoral Fellovship.

? American Institute of Physics 1988

185

most sequences' in a probabilistic sense, as some set of sequences yi th
total probability greater than say, .99. When all sequences are
equiprobab1e, this reduces to the deterministic version: 991. of all
sequences.
In practice it is too difficult to compute the capacity of a given
associative memory yith inputs of length n and outputs of length Tn.
Fortunately, though, it is easier to compute the asymptotic rate at
which A1 increases, as n and Tn increase, for a given family of
associative memories. This is the approach taken by McEliece et al. [1]
toyards the capacity of the Hopfield associative memory. We take the
same approach tovards the capacity of the Kanerva associative memory,
and tovards the capacities of associative memories in general . In the
next section ve provide an upper bound on the rate of grovth of the
capacity of any associative memory fitting our general model. It is
shown by sphere packing arguments that capacity is limited to an
exponential rate of grovth of 1- h2(t5), vhere h2(t5) is the binary entropy
function in bits, and 8 is the radius of attraction. In a later section
it vill turn out that this exponential grovth in capacity can actually
be achieved by the Kanerva associative memory, if its parameters are
optimally set. This exponential grovth in capacity for the Kanerva
associative memory contrasts sharply yith the sub-linear grovth in
capacity for the Hopfield associative memory [1].
I

A UNIVERSAL UPPER BOUND ON CAPACITY
Recall that our definition of the capacity of an associative memory
is the largest A1 such that for most sequences of addresses
X(1), ... ,X(M) and most sequences of data y(l), ... , y(M), the memory can
correct fraction 8 errors. Clearly, an upper bound to this capacity is
the largest Af for vhich there exists some sequence of addresses
X(1), . . . , X(M) such that for most sequences of data y(l), ... , y(M), the
memory can correct fraction 8 errors. We nov derive an expression for
this upper bound.
Let 8 be the radius of attraction and let DH(X(i) , d) be the sphere
of attraction, i.e., the set of all Xs at most Hamming distance d= Ln8J
from .y(j). Since by assumption the memory corrects fraction 8 errors,
every address X E DH(XU),d) retrieves the vord yW. The size of
DH(XU),d) is easily shown to be independent of xU) and equal to
vn.d = 2:%=0
vhere
is the binomial coefficient n!jk!(n - k)!. Thus
n
out of a total of 2 n-bit addresses, at least vn.d addresses retrieve
y(l), at least Vn.d addresses retrieve y(2), at least Vn.d addresses
retrieve y(~, and so forth. It fol10vs that the total number of
distinct yU)s can be at most 2 n jv n .d ' Nov, from Stirling's formula it
can be shovn that if d:S; nj2, then vn.d = 2nh2 (d/n)+O(logn), vhere
h 2 ( 8) = -81og 2 8 - (1 - 8) log2( 1 - 8) is the binary entropy function in bits,
and O(logn) is some function yhose magnitude grovs more slovly than a
constant times log n. Thus the total number of distinct y(j)s can be at
most 2 n (1-h2(S?+O(logn)
Since any set containing I most sequences' of Af
Tn-bit vords vill contain a large number of distinct vords (if Tn is

(1:),

(I:)

186

Figure 1: Neural net representation of the Kanerva associative memory. Signals propagate from the bottom (input) to the top (output). Each arc multiplies the signal by its
weight; each node adds the incoming signals and then thresholds.
sufficiently large --- see [2] for details), it follovs that
M :5 2 n (l-h 2 (o?+O(logn).

(1)

In general a function fen) is said to be O(g(n)) if f(n)fg(n) is
bounded, i.e. , if there exists a constant a such that If(n)1 :5 a\g(n)1 for
all n. Thus (1) says that there exists a constant a such that
M :5 2 n(l-h 2 (S?+alogn. It should be emphasized that since a is unknow,
this bound has no meaning for fixed n. Hovever, it indicates that
asymptotically in n, the maximum exponential rate of grovth of M is
1 - h2 ( 6).
Intui ti vely, only a sequence of addresses X(l), ... , X(M) that
optimally pack the address space {-l,+l}n can hope to achieve this
upper bound. Remarkably, most such sequences are optimal in this sense,
vhen n is large. The Kanerva associative memory can take advantage of
this fact.

THE KANERVA ASSOCIATIVE MEMORY
The Kanerva associative memory [3,4] can be regarded as a tvo-layer
neural netvork, as shovn in Figure 1, vhere the first layer is a
preprocessor and the second layer is the usual Hopfield style array.
The preprocessor essentially encodes each n-bit input address into a
very large k-bit internal representation, k ~ n, vhose size will be
permitted to grov exponentially in n. It does not seem surprising,
then, that the capacity of the Kanerva associative memory can grov
exponentially in n, for it is knovn that the capacity of the Hopfield
array grovs almost linearly in k, assuming the coordinates of the
k-vector are dravn at random by independent flips of a fair coin [1].

187

Figure 2: Matrix representation of the Kanerva associative memory. Signals propagate
from the right (input) to the left (output). Dimensions are shown in the box corners.
Circles stand for functional composition; dots stand for matrix multiplication.
In this situation, hovever, such an assumption is ridiculous: Since the
k-bit internal representation is a function of the n-bit input address,
it can contain at most n bits of information, whereas independent flips
of a fair coin contain k bits of information. Kanerva's primary
contribution is therefore the specification of the preprocessor, that
is, the specification of how to map each n-bit input address into a very
large k-bit internal representation.
The operation of the preprocessor is easily described. Consider
the matrix representation shovn in Figure 2. The matrix Z is randomly
populated vith ?ls. This randomness assumption is required to ease the
analysis. The function fr is 1 in the ith coordinate if the ith row of
Z is within Hamming distance r of X, and is Oothervise. This is
accomplished by thresholding the ith input against n-2r. The
parameters rand k are two essential parameters in the Kanerva
associative memory. If rand k are set correctly, then the number of 1s
in the representation fr(ZX) vill be very small in comparison to the
number of Os. Hence fr(Z~Y) can be considered to be a sparse internal
representation of X.
The second stage of the memory operates in the usual way, except on
the internal representation of X. That is, Y = g(W fr(ZX)), vhere
M

l-V = LyU)[Jr(ZXU))]t,

(2)

i=l

and 9 is the threshold function whose ith coordinate is +1 if the ith
input is greater than 0 and -1 is the ith input is less than O. The ith
column of l-V can be regarded as a memory location vhose address is the
ith row of Z. Every X vi thin Hamming distance r of the ith rov of Z
accesses this location. Hence r is known as the access radius, and k is
the number of memory locations.
The approach taken in this paper is to fix the linear rate p at
which r grovs vith n, and to fix the exponential rate ~ at which k grovs
with n. It turns out that the capacity then grovs at a fixed
exponential rate Cp,~(t5), depending on p, ~, and 15. These exponential
rates are sufficient to overcome the standard loose but simple
polynomial bounds on the errors due to combinatorial approximations.

188

THE CAPACITY OF THE KANERVA ASSOCIATIVE MEMORY
Fix 0 $ K $1. 0 $ p$ 1/2. and 0 $ 0 $ min{2p,1/2}. Let n be the
input address length, and let Tn be the output word length. It is
assumed that Tn is at most polynomial in n, i.e., Tn = exp{O(logn)}. Let
r = IJmJ be the access radius, let k = 2 L"nJ be the number of memory
locations, and let d= LonJ be the radius of attraction. Let Afn be the
number of stored words. The components of the n-vectors X(l), .. . , X(Mn) ,
the m-vectors y(l), ... , y(,Yn), and the k X n matrix Z are assumed to be
lID equiprobable ?1 random variables. Finally, given an n-vector X,
let Y = g(W fr(ZX)) where W = Ef;nl yU)[Jr(ZXW)jf.
Define the quantity

Cp ,,(0) = { 26 + 2(1- 0)h(P;~~2)
'Cp,ICo(p)(o)
where
KO(p)

2h(p)

2; - 2(1- ;)h(P~242)

= 2h(p) -

and

~-

; =

Theorem:

+ K, -

If
Af

J

196 -

if K, $ K,o(p)
if K> K,O(p) ,

+ 1- he;)

(3)

(4)

2p(1 - p).

< 2nCp... (5)+O(logn)

n_

then for all f>O, all sufficiently large n, all jE{l, ... ,Afn }. and all
X E DH(X(j) , d),

P{y

-::J y(j)}

< f.

See [2].
Interpretation: If the exponential growth rate of the number of
stored words Afn is asymptotically less than C p ,,, ( 0), then for every
sufficiently large address length n. there is some realization of the
nx 2n " preprocessor matrix Z such that the associative memory can
correct fraction 0 errors for most sequences of Afn (address, datum)
pairs. Thus Cp,IC( 0) is a lover bound on the exponential growth rate of
the capacity of the Kanerva associative memory with access radius np and
number of memory locations 2nIC ?
Figure 3 shows Cp,IC(O) as a function of the radius of attraction 0,
for K,= K,o(p) and p=O.l, 0.2, 0.3, 0.4 and 0.45. For? any fixed access
radius p, Cp,ICO(p) (0) decreases as 0 increases. This reflects the fact
that fewer (address, datum) pairs can be stored if a greater fraction of
errors must be corrected. As p increases, Cp,,,o(p)(o) begins at a lower
point but falls off less steeply. In a moment we shall see that p can
be adjusted to provide the optimal performance for a given O.
Not ShOVIl in Figure 3 is the behavior of Cp ,,, ( 0) as a function of K,.
However, the behavior is simple. For K, > K,o(p), Cp,,,(o) remains
unchanged, while for K$ K,o(p), Cp,,,(o) is simply shifted doVIl by the
difference KO(p)-K,. This establishes the conditions under which the
Kanerva associative memory is robust against random component failures.
Although increasing the number of memory locations beyond 2rl11:o(p) does
not increase the capacity, it does increase robustness. Random
Proof:

189

0.8

0.6

'!I.2 ...... - - -

"

" ?1

1Il.2

IIl.S

1Il.3

Figure 3: Graphs of Cp,lCo(p)(o) as defined by (3). The upper envelope is 1- h2(0).
component failures will not affect the capacity until so many components
have failed that the number of surviving memory locations is less than
2nlCo (p) .

Perhaps the most important curve exhibited in Figure 3 is the
sphere packing upper bound 1 - h2 ( 0). which is achieved for a particular

J

p by b = ~ - 196 - 2p(1 - p). Equivalently. the upper bound is achieved
for a particular 0 by P equal to

poCo) =

t - Jt - iO(l -

~o).

(5)

Thus (4) and (5) specify the optimal values of the parameters K and P.
respectively. These functions are shown in Figure 4. With these
optimal values. (3) simplifies to

the sphere packing bound.
It can also be seen that for 0 = 0 in (3). the exponential growth
rate of the capacity is asymptotically equal to K. which is the
exponential growth rate of the number of memory locations. k n ? That is.
Mn = 2n1C +O(logn) = k n . 20 (logn). Kanerva [3] and Keeler [5] have argued
that the capacity at 8 =0 is proportional to the number of memory
locations, i.e .? Mn = k n . (3. for some constant (3. Thus our results are
consistent with those of Kanerva and Keeler. provided the 'polynomial'
20 (logn) can be proved to be a constant. However. the usual statement of
their result. M = k?(3. that the capacity is simply proportional to the
number of memory locations. is false. since in light of the universal

190

liLS

o
riJ.S

Figure 4: Graphs of KO(p) and co(p), the inverse of Po(<5), as defined by (4) and (5).

upper bound, it is impossible for the capacity to grow without bound,
with no dependence on the dimension n. In our formulation, this
difficulty does not arise because we have explicitly related the number
of memory locations to the input dimension: kn =2n~. In fact, our
formulation provides explicit, coherent relationships between all of the
following variables: the capacity .~, the number of memory locations k,
the input and output dimensions n and Tn, the radius of attraction C,
and the access radius p. We are therefore able to generalize the
results of [3,5] to the case C>0, and provide explicit expressions for
the asymptotically optimal values of p and K as well.
CONCLUSION
We described a fairly general model of associative memory and
selected a useful definition of its capacity. A universal upper bound
on the growth of the capacity of such an associative memory was shown by
a sphere packing argument to be exponential with rate 1 - h 2 ( c), where
h2(C) is the binary entropy function and 8 is the radius of attraction.
We reviewed the operation of the Kanerva associative memory, and stated
a lower bound on the exponential growth rate of its capacity. This
lower bound meets the universal upper bound for optimal values of the
memory parameters p and K. We provided explicit formulas for these
optimal values. Previous results for <5 =0 stating that the capacity of
the Kanerva associative memory is proportional to the number of memory
locations cannot be strictly true. Our formulation corrects the problem
and generalizes those results to the case C > o.

191

REFERENCES
1. R.J. McEliece, E.C. Posner, E.R. Rodemich, and S.S. Venkatesh,
"The capacity of the Hopfield associative memory," IEEE
Transactions on Information Theory, submi tt ed .
2. P.A. Chou, "The capacity of the Kanerva associative memory,"
IEEE Transactions on Information Theory, submitted.
3. P. Kanerva, "Self-propagating search: a unified theory of
memory," Tech. Rep. CSLI-84-7, Stanford Center for the Study of
Language and Information. Stanford. CA, March 1984.
4. P. Kanerva, "Parallel structures in human and computer memory,"
in Neural Networks for Computing, (J .S. Denker. ed.), Nev York:
American Institute of Physics. 1986.
5 . J.D. Keeler. "Comparison betveen sparsely distributed memory and
Hopfield-type neural netvork models," Tech . Rep. RIACS TR 86 . 31,
NASA Research Institute for Advanced Computer Science, Mountain
Viev. CA, Dec. 1986.


----------------------------------------------------------------

title: 100-storing-covariance-by-the-associative-long-term-potentiation-and-depression-of-synaptic-strengths-in-the-hippocampus.pdf

394

STORING COVARIANCE BY THE ASSOCIATIVE
LONG?TERM POTENTIATION AND DEPRESSION
OF SYNAPTIC STRENGTHS IN THE HIPPOCAMPUS
Patric K. Stanton? and Terrence J. Sejnowski t
Department of Biophysics
Johns Hopkins University
Baltimore, MD 21218
ABSTRACT

In modeling studies or memory based on neural networks, both the selective
enhancement and depression or synaptic strengths are required ror effident storage
or inrormation (Sejnowski, 1977a,b; Kohonen, 1984; Bienenstock et aI, 1982;
Sejnowski and Tesauro, 1989). We have tested this assumption in the hippocampus,
a cortical structure or the brain that is involved in long-term memory. A brier,
high-frequency activation or excitatory synapses in the hippocampus produces an
increase in synaptic strength known as long-term potentiation, or LTP (BUss and
Lomo, 1973), that can last ror many days. LTP is known to be Hebbian since it
requires the simultaneous release or neurotransmitter from presynaptic terminals
coupled with postsynaptic depolarization (Kelso et al, 1986; Malinow and Miller,
1986; Gustatrson et al, 1987). However, a mechanism ror the persistent reduction or
synaptic strength that could balance LTP has not yet been demonstrated. We studied the associative interactions between separate inputs onto the same dendritic
trees or hippocampal pyramidal cells or field CAl, and round that a low-frequency
input which, by itselr, does not persistently change synaptic strength, can either
increase (associative LTP) or decrease in strength (associative long-term depression
or LTD) depending upon whether it is positively or negatively correlated in time
with a second, high-frequency bursting input. LTP or synaptic strength is Hebbian,
and LTD is anti-Hebbian since it is elicited by pairing presynaptic firing with postsynaptic hyperpolarization sufficient to block postsynaptic activity. Thus, associative LTP and associative LTO are capable or storing inrormation contained in the
covariance between separate, converging hippocampal inputs?

?Present address: Dep~ents of NeW'Oscience and Neurology, Albert Einstein College
of Medicine, 1410 Pelham Parkway South, Bronx, NY 10461 USA.
tPresent address: Computational Neurobiology Laboratory, The Salk Institute, P.O. Box
85800, San Diego, CA 92138 USA.

Storing Covariance by Synaptic Strengths in the Hippocampus

INTRODUCTION
Associative LTP can be produced in some hippocampal neuroos when lowfrequency. (Weak) and high-frequency (Strong) inputs to the same cells are simultaneously activated (Levy and Steward, 1979; Levy and Steward, 1983; Barrionuevo and
Brown, 1983). When stimulated alone, a weak input does not have a long-lasting effect
on synaptic strength; however, when paired with stimulation of a separate strong input
sufficient to produce homo synaptic LTP of that pathway, the weak pathway is associatively potentiated. Neural network modeling studies have predicted that, in addition to
this Hebbian form of plasticity, synaptic strength should be weakened when weak and
strong inputs are anti-correlated (Sejnowski, 1977a,b; Kohonen, 1984; Bienenstock et al,
1982; Sejnowski and Tesauro, 1989). Evidence for heterosynaptic depression in the hippocampus has been found for inputs that are inactive (Levy and Steward, 1979; Lynch et
al, 1977) or weakly active (Levy and Steward, 1983) during the stimulation of a strong
input, but this depression did not depend on any pattern of weak input activity and was
not typically as long-lasting as LTP.
Therefore, we searched for conditions under which stimulation of a hippocampal
pathway, rather than its inactivity, could produce either long-term depression or potentiation of synaptic strengths, depending on the pattern of stimulation. The stimulus paradigm that we used, illustrated in Fig. I, is based on the finding that bursts of stimuli at 5
Hz are optimal in eliciting LTP in the hippocampus (Larson and Lynch, 1986). A highfrequency burst (S'IRONG) stimulus was applied to Schaffer collateral axons and a lowfrequency (WEAK) stimulus given to a separate subicular input coming from the opposite side of the recording site, but terminating on dendrites of the same population of CAl
pyramidal neurons. Due to the rhythmic nature of the strong input bursts, each weak
input shock could be either superimposed on the middle of each burst of the strong input
(IN PHASE), or placed symmetrically between bursts (OUT OF PHASE).

RESULTS
Extracellular evoked field potentials were recorded from the apical dendritic and
somatic layers of CAl pyramidal cells. The weak stimulus train was first applied alone
and did not itself induce long-lasting changes. The strong site was then stimulated alone,
which elicited homosynaptic LTP of the strong pathway but did not significantly alter
amplitude of responses to the weak input. When weak and strong inputs were activated
IN PHASE, there was an associative LTP of the weak input synapses, as shown in Fig.
2a. Both the synaptic excitatory post-synaptic potential (e.p.s.p.) (Ae.p.s.p. = +49.8 ?
7.8%, n=20) and population action potential (&Pike = +65.4 ? 16.0%, n=14) were
significantly enhanced for at least 60 min up to 180 min following stimulation.
In contrast, when weak and strong inputs were applied OUT OF PHASE, they elicited an associative long-term depression (LTO) of the weak input synapses, as shown in
Fig. 2b. There was a marked reduction in the population spike (-46.5 ? 11.4%, n=10)
with smaller decreases in the e.p.s.p. (-13.8 ? 3.5%, n=13). Note that the stimulus patterns applied to each input were identical in these two experiments, and only the relative

395

396

Stanton and Sejnowski

phase of the weak and strong stimuli was altered. With these stimulus patterns. synaptic
strength could be repeatedly enhanced and depressed in a single slice. as illustrated in Fig
2c. As a control experiment to determine whether information concerning covariance
between the inputs was actually a determinant of plasticity. we combined the in phase
and out of phase conditions, giving both the weak input shocks superimposed on the
bursts plus those between the bursts. for a net frequency of 10 Hz. This pattern. which
resulted in zero covariance between weak and strong inputs. produced no net change in
weak input synaptic strength measmed by extracellular evoked potentials. Thus. the assoa

b
A.SSOCIA.TIVE STIMULUS PA.RA.DIGMS
POSJTIVE.LY CORKELA TED ? "IN PHASE"

~K~~ _I~__~I____~I____~I_
SI1IONG,NJO\IT

. u.Jj1l 11l. -1---1&1111.....
11 ---1&1
111.....
11 ---,I~IIII

NEGATIVELY CORRELATED? 'our OF PHASE"
W[AKIN'lTf

STIONG 'N''''

~I

11111

--,-;

11111

11111

Figure 1. Hippocampal slice preparation and stimulus paradigms. a: The in vitro hippocampal slice showing recording sites in CAl pyramidal cell somatic (stratum pyramidale) and dendritic (stratum radiatum) layers. and stimulus sites activating Schaffer collateral (STRONG) and commissural (WEAK) afferents. Hippocampal slices (400 Jlm
thick) were incubated in an interface slice chamber at 34-35 0 C. Extracellular (1-5 M!l
resistance, 2M NaCI filled) and intracellular (70-120 M 2M K-acetate filled) recording electrodes. and bipolar glass-insulated platinum wire stimulating electrodes (50 Jlm
tip diameter). were prepared by standard methods (Mody et al, 1988). b: Stimulus paradigms used. Strong input stimuli (STRONG INPUT) were four trains of 100 Hz bursts.
Each burst had 5 stimuli and the interburst interval was 200 msec. Each train lasted 2
seconds for a total of 50 stimuli. Weak input stimuli (WEAK INPUT) were four trains of
shocks at 5 Hz frequency. each train lasting for 2 seconds. When these inputs were IN
PHASE. the weak single shocks were superimposed on the middle of each burst of the
strong input. When the weak input was OUT OF PHASE. the single shocks were placed
symmetrically between the bursts.

n.

Storing Covariance by Synaptic Strengths in the Hippocampus

ciative LTP and LTD mechanisms appear to be balanced in a manner ideal for the
storage of temporal covariance relations.
The simultaneous depolarization of the postsynaptic membrane and activation of
glutamate receptors of the N-methyl-D-aspartate (NMDA) subtype appears to be necessary for LTP induction (Collingridge et ai, 1983; Harris et al, 1984; Wigstrom and Gustaffson, 1984). The SJ?read of current from strong to weak synapses in the dendritic tree,
d

ASSOCIATIVE

LON(;.TE~

I'OTENTIATION

LONG-TE~

DE,/tESSION

-

!!Ll!!!!.

b

ASSOCIATIVE

I

11111

?

11111.
I

c

e...

I

I

I

I

Figure 2. mustration of associative long-term potentiation (LTP) and associative longterm depression (LTD) using extracellular recordings. a: Associative LTP of evoked
excitatory postsynaptic potentials (e.p.s.p.'s) and population action potential responses in
the weak inpuL Test responses are shown before (Pre) and 30 min after (post) application of weak stimuli in phase with the coactive strong input. b: Associative LTD of
evoked e.p.s.p.'s and population spike responses in the weak input. Test responses are
shown before (Pre) and 30 min after (post) application of weak stimuli out of phase with
the coactive strong input. c: Time course of the changes in population spike amplitude
observed at each input for a typical experiment. Test responses from the strong input (S,
open circles), show that the high-frequency bursts (5 pulses/l00 Hz, 200 msec interburst
interval as in Fig. 1) elicited synapse-specific LTP independent of other input activity.
Test responses from the weak input (W. filled circles) show that stimulation of the weak
pathway out of phase with the strong one produced associative LTD (Assoc LTD) of this
input. Associative LTP (Assoc LTP) of the same pathway was then elicited following in
phase stimulation. Amplitude and duration of associative LTD or LTP could be increased
by stimulating input pathways with more trains of shocks.

397

398

Stanton and Sejnowski

coupled with release of glutamate from the weak inputs, could account for the ability of
the strong pathway to associatively potentiate a weak one (Kelso et al, 1986; Malinow
and Miller, 1986; Gustaffson et al, 1987). Consistent with this hypothesis, we find that
the NMDA receptor antagonist 2-amino-S-phosphonovaleric acid (APS, 10 J.1M) blocks
induction of associative LTP in CAl pyramidal neurons (data not shown, n=S). In contrast, the application of APS to the bathing solution at this same concentration had no
significant effect on associative LTD (data not shown, n=6). Thus, the induction of LTD
seems to involve cellular mechanisms different from associative LTP.
The conditions necessary for LTD induction were explored in another series of
experiments using intracellular recordings from CAl pyramidal neurons made using
standard techniques (Mody et al, 1988). Induction of associative LTP (Fig 3; WEAK
S+W IN PHASE) produced an increase in amplitude of the single cell evoked e.p.s.p. and
a lowered action potential threshold in the weak pathway, as reported previously (Barrionuevo and Brown, 1983). Conversely, the induction of associative LTD (Fig. 3;
WEAK S+W OUT OF PHASE) was accompanied by a long-lasting reduction of e.p.s.p.
amplitude and reduced ability to elicit action potential firing. As in control extracellular
experiments, the weak input alone produced no long-lasting alterations in intracellular
e.p.s.p.'s or firing properties, while the strong input alone yielded specific increases of
the strong pathway e.p.s.p. without altering e.p.s.p. 's elicited by weak input stimulation.

PRE

30 min POST
S+W OUT OF PHASE

30 min POST
S+W IN PHASE

Figure 3. Demonstration of associative LTP and LTD using intracellular recordings from
a CAl pyramidal neuron. Intracellular e.p.s.p.'s prior to repetitive stimulation (pre), 30
min after out of phase stimulation (S+W OUT OF PHASE), and 30 min after subsequent in phase stimuli (S+W IN PHASE). The strong input (Schaffer collateral side,
lower traces) exhibited LTP of the evoked e.p.s.p. independent of weak input activity.
Out of phase stimulation of the weak (Subicular side, upper traces) pathway produced a
marked, persistent reduction in e.p.s.p. amplitude. In the same cell, subsequent in phase
stimuli resulted in associative LTP of the weak input that reversed the LTD and enhanced
amplitude of the e.p.s.p. past the original baseline. (RMP = -62 mY, RN = 30 MO)

Storing Covariance by Synaptic Strengths in the Hippocampus

A weak stimulus that is out of phase with a strong one anives when the postsynaptic neuron is hyperpolarized as a consequence of inhibitory postsynaptic potentials and
afterhyperpolarization from mechanisms intrinsic to pyramidal neurons. This suggests
that postsynaptic hyperpolarization coupled with presynaptic activation may trigger L'ID.
To test this hypothesis, we injected current with intracellular microelectrodes to hyperpolarize or depolarize the cell while stimulating a synaptic input. Pairing the injection of
depolarizing current with the weak input led to LTP of those synapses (Fig. 4a; STIM;

a

PRE

? ?IDPOST
S'I1M ? DEPOL

~l"V
lS.,.c

r
," i

COI'ITROL

-Jj

b

I

--" \

"----

(W.c:ULVllj

PRE

lOlIIin POST
STlM ? HYPERPOL

Figure 4. Pairing of postsynaptic hyperpolarization with stimulation of synapses on CAl
hippocampal pyramidal neurons produces L'ID specific to the activated pathway, while
pairing of postsynaptic depolarization with synaptic stimulation produces synapsespecific LTP. a: Intracellular evoked e.p.s.p.'s are shown at stimulated (STIM) and
unstimulated (CONTROL) pathway synapses before (Pre) and 30 min after (post) pairing a 20 mY depolarization (constant current +2.0 nA) with 5 Hz synaptic stimulation.
The stimulated pathway exhibited associative LTP of the e.p.s.p., while the control,
unstimulated input showed no change in synaptic strength. (RMP = -65 mY; RN = 35
Mfl) b: Intracellular e.p.s.p. 's are shown evoked at stimulated and control pathway
synapses before (Pre) and 30 min after (post) pairing a 20 mV hyperpolarization (constant current -1.0 nA) with 5 Hz synaptic stimulation. The input (STIM) activated during
the hyperpolarization showed associative LTD of synaptic evoked e.p.s.p.'s, while
synaptic strength of the silent input (CONTROL) was unaltered. (RMP =-62 mV; RN =
38M!l)

399

400

Stanton and Sejnowski

+64.0 -9.7%, n=4), while a control input inactive during the stimulation did not change
(CONTROL), as reported previously (Kelso et al, 1986; Malinow and Miller, 1986; Gustaffson et al, 1987). Conversely, prolonged hyperpolarizing current injection paired with
the same low-frequency stimuli led to induction of LTD in the stimulated pathway (Fig.
4b; STIM; -40.3 ? 6.3%, n=6). but not in the unstimulated pathway (CONTROL). The
application of either depolarizing current, hyperpolarizing current, or the weak 5 Hz
synaptic stimulation alone did not induce long-term alterations in synaptic strengths.
Thus. hyperpolarization and simultaneous presynaptic activity supply sufficient conditions for the induction of LTD in CAl pyramidal neurons.

CONCLUSIONS
These experiments identify a novel fono of anti-Hebbian synaptic plasticity in the
hippocampus and confirm predictions made from modeling studies of information storage
in neural networks. Unlike previous reports of synaptic depression in the hippocampus,
the plasticity is associative, long-lasting, and is produced when presynaptic activity
occurs while the postsynaptic membrane is hyperpolarized. In combination with Hebbian
mechanisms also present at hippocampal synapses. associative LTP and associative LTD
may allow neurons in the hippocampus to compute and store covariance between inputs
(Sejnowski, 1977a,b; Stanton and Sejnowski. 1989). These finding make temporal as
well as spatial context an important feature of memory mechanisms in the hippocampus.
Elsewhere in the brain, the receptive field properties of cells in cat visual cortex
can be altered by visual experience paired with iontophoretic excitation or depression of
cellular activity (Fregnac et al, 1988; Greuel et al, 1988). In particular, the chronic hyperpolarization of neurons in visual cortex coupled with presynaptic transmitter release leads
to a long-teno depression of the active. but not inactive, inputs from the lateral geniculate
nucleus (Reiter and Stryker, 1988). Thus. both Hebbian and anti-Hebbian mechanisms
found in the hippocampus seem to also be present in other brain areas, and covariance of
firing patterns between converging inputs a likely key to understanding higher cognitive
function.
This research was supported by grants from the National Science Foundation and
the Office of Naval research to TJS. We thank Drs. Charles Stevens and Richard Morris
for discussions about related experiments.

Rererences
Bienenstock, E., Cooper. LN. and Munro. P. Theory for the development of neuron
selectivity: orientation specificity and binocular interaction in visual cortex. J. Neurosci. 2. 32-48 (1982).
Barrionuevo, G. and Brown, T.H. Associative long-teno potentiation in hippocampal
slices. Proc. Nat. Acad. Sci. (USA) 80, 7347-7351 (1983).
Bliss. T.V.P. and Lomo, T. Long-lasting potentiation of synaptic ttansmission in the dentate area of the anaesthetized rabbit following stimulation of the perforant path. J.
Physiol. (Lond.) 232. 331-356 (1973).

Storing Covariance by Synaptic Strengths in the Hippocampus

Collingridge, GL., Kehl, SJ. and McLennan, H. Excitatory amino acids in synaptic
transmission in the Schaffer collateral-commissural pathway of the rat hippocampus. J.
Physiol. (Lond.) 334, 33-46 (1983).
Fregnac, Y., Shulz, D., Thorpe, S. and Bienenstock, E. A cellular analogue of visual cortical plasticity. Nature (Lond.) 333, 367-370 (1988).
Greuel. J.M.. Luhmann. H.J. and Singer. W. Pharmacological induction of usedependent receptive field modifications in visual cortex. Science 242,74-77 (1988).
Gustafsson, B., Wigstrom, H., Abraham, W.C. and Huang. Y.Y. Long-term potentiation
in the hippocampus using depolarizing current pulses as the conditioning stimulus to
single volley synaptic potentials. J. Neurosci. 7, 774-780 (1987).
Harris. E.W., Ganong, A.H. and Cotman, C.W. Long-term potentiation in the hippocampus involves activation of N-metbyl-D-aspartate receptors. Brain Res. 323, 132137 (1984).
Kelso, S.R.. Ganong, A.H. and Brown, T.H. Hebbian synapses in hippocampus. Proc.
Natl. Acad. Sci. USA 83, 5326-5330 (1986).
Kohonen. T. Self-Organization and Associative Memory. (Springer-Verlag. Heidelberg,
1984).
Larson. J. and Lynch. G. Synaptic potentiation in hippocampus by patterned stimulation
involves two events. Science 232, 985-988 (1986).
Levy. W.B. and Steward, O. Synapses as associative memory elements in the hippocampal formation. Brain Res. 175,233-245 (1979).
Levy. W.B. and Steward, O. Temporal contiguity requirements for long-term associative
potentiation/depression in the hippocampus. Neuroscience 8, 791-797 (1983).
Lynch. G.S., Dunwiddie. T. and Gribkoff. V. Heterosynaptic depression: a postsynaptic
correlate oflong-term potentiation. Nature (Lond.) 266. 737-739 (1977).
Malinow. R. and Miller, J.P. Postsynaptic hyperpolarization during conditioning reversibly blocks induction of long-term potentiation Nature (Lond.)32.0. 529-530 (1986).
Mody. I.. Stanton. PK. and Heinemann. U. Activation of N-methyl-D-aspartate
(NMDA) receptors parallels changes in cellular and synaptic properties of dentate
gyrus granule cells after kindling. J. Neurophysiol. 59. 1033-1054 (1988).
Reiter, H.O. and Stryker, M.P. Neural plasticity without postsynaptic action potentials:
Less-active inputs become dominant when kitten visual cortical cells are pharmacologically inhibited. Proc. Natl. Acad. Sci. USA 85, 3623-3627 (1988).
Sejnowski, T J. and Tesauro, G. Building network learning algorithms from Hebbian
synapses, in: Brain Organization and Memory JL. McGaugh, N.M. Weinberger, and
G. Lynch, Eds. (Oxford Univ. Press, New York, in press).
Sejnowski, TJ. Storing covariance with nonlinearly interacting neurons. J. Math. Biology 4, 303-321 (1977).
Sejnowski, T. J. Statistical constraints on synaptic plasticity. J. Theor. Biology 69, 385389 (1977).
Stanton, P.K. and Sejnowski, TJ. Associative long-term depression in the hippocampus:
Evidence for anti-Hebbian synaptic plasticity. Nature (Lond.), in review.
Wigstrom, H. and Gustafsson, B. A possible correlate of the postsynaptic condition for
long-lasting potentiation in the guinea pig hippocampus in vitro. Neurosci. Lett. 44,
327?332 (1984).

401


----------------------------------------------------------------

title: 6121-dense-associative-memory-for-pattern-recognition.pdf

Dense Associative Memory for Pattern Recognition
Dmitry Krotov
Simons Center for Systems Biology
Institute for Advanced Study
Princeton, USA
krotov@ias.edu

John J. Hopfield
Princeton Neuroscience Institute
Princeton University
Princeton, USA
hopfield@princeton.edu

Abstract
A model of associative memory is studied, which stores and reliably retrieves many
more patterns than the number of neurons in the network. We propose a simple
duality between this dense associative memory and neural networks commonly used
in deep learning. On the associative memory side of this duality, a family of models
that smoothly interpolates between two limiting cases can be constructed. One limit
is referred to as the feature-matching mode of pattern recognition, and the other
one as the prototype regime. On the deep learning side of the duality, this family
corresponds to feedforward neural networks with one hidden layer and various
activation functions, which transmit the activities of the visible neurons to the
hidden layer. This family of activation functions includes logistics, rectified linear
units, and rectified polynomials of higher degrees. The proposed duality makes
it possible to apply energy-based intuition from associative memory to analyze
computational properties of neural networks with unusual activation functions ? the
higher rectified polynomials which until now have not been used in deep learning.
The utility of the dense memories is illustrated for two test cases: the logical gate
XOR and the recognition of handwritten digits from the MNIST data set.

1

Introduction

Pattern recognition and models of associative memory [1] are closely related. Consider image
classification as an example of pattern recognition. In this problem, the network is presented with an
image and the task is to label the image. In the case of associative memory the network stores a set of
memory vectors. In a typical query the network is presented with an incomplete pattern resembling,
but not identical to, one of the stored memories and the task is to recover the full memory. Pixel
intensities of the image can be combined together with the label of that image into one vector [2],
which will serve as a memory for the associative memory. Then the image itself can be thought of
as a partial memory cue. The task of identifying an appropriate label is a subpart of the associative
memory reconstruction. There is a limitation in using this idea to do pattern recognition. The standard
model of associative memory works well in the limit when the number of stored patterns is much
smaller than the number of neurons [1], or equivalently the number of pixels in an image. In order
to do pattern recognition with small error rate one would need to store many more memories than
the typical number of pixels in the presented images. This is a serious problem. It can be solved by
modifying the standard energy function of associative memory, quadratic in interactions between the
neurons, by including in it higher order interactions. By properly designing the energy function (or
Hamiltonian) for these models with higher order interactions one can store and reliably retrieve many
more memories than the number of neurons in the network.
Deep neural networks have proven to be useful for a broad range of problems in machine learning
including image classification, speech recognition, object detection, etc. These models are composed
of several layers of neurons, so that the output of one layer serves as the input to the next layer. Each
30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

neuron calculates a weighted sum of the inputs and passes the result through a non-linear activation
function. Traditionally, deep neural networks used activation functions such as hyperbolic tangents or
logistics. Learning the weights in such networks, using a backpropagation algorithm, faced serious
problems in the 1980s and 1990s. These issues were largely resolved by introducing unsupervised
pre-training, which made it possible to initialize the weights in such a way that the subsequent
backpropagation could only gently move boundaries between the classes without destroying the
feature detectors [3, 4]. More recently, it was realized that the use of rectified linear units (ReLU)
instead of the logistic functions speeds up learning and improves generalization [5, 6, 7]. Rectified
linear functions are usually interpreted as firing rates of biological neurons. These rates are equal
to zero if the input is below a certain threshold and linearly grow with the input if it is above the
threshold. To mimic biology the output should be small or zero if the input is below the threshold, but
it is much less clear what the behavior of the activation function should be for inputs exceeding the
threshold. Should it grow linearly, sub-linearly, or faster than linearly? How does this choice affect
the computational properties of the neural network? Are there other functions that would work even
better than the rectified linear units? These questions to the best of our knowledge remain open.
This paper examines these questions through the lens of associative memory. We start by discussing
a family of models of associative memory with large capacity. These models use higher order (higher
than quadratic) interactions between the neurons in the energy function. The associative memory
description is then mapped onto a neural network with one hidden layer and an unusual activation
function, related to the Hamiltonian. We show that by varying the power of interaction vertex in
the energy function (or equivalently by changing the activation function of the neural network) one
can force the model to learn representations of the data either in terms of features or in terms of
prototypes.

2

Associative memory with large capacity

The standard model of associative memory [1] uses a system of N binary neurons, with values ?1. A
configuration of all the neurons is denoted by a vector i . The model stores K memories, denoted by
?i? , which for the moment are also assumed to be binary. The model is defined by an energy function,
which is given by
E=

N
1 X
2 i,j=1

i Tij j ,

Tij =

K
X

?i? ?j? ,

(1)

?=1

and a dynamical update rule that decreases the energy at every update. The basic problem is the
following: when presented with a new pattern the network should respond with a stored memory
which most closely resembles the input.
There has been a large amount of work in the community of statistical physicists investigating
the capacity of this model, which is the maximal number of memories that the network can store
and reliably retrieve. It has been demonstrated [1, 8, 9] that in case of random memories this
maximal value is of the order of K max ? 0.14N . If one tries to store more patterns, several
neighboring memories in the configuration space will merge together producing a ground state of
the Hamiltonian (1), which has nothing to do with any of the stored memories. By modifying the
Hamiltonian (1) in a way that removes second order correlations between the stored memories, it is
possible [10] to improve the capacity to K max = N .
The mathematical reason why the model (1) gets confused when many memories are stored is that
several memories produce contributions to the energy which are of the same order. In other words the
energy decreases too slowly as the pattern approaches a memory in the configuration space. In order
to take care of this problem, consider a modification of the standard energy
E=

K
X

F ?i?

i

(2)

?=1

In this formula F (x) is some smooth function (summation over index i is assumed). The computational capabilities of the model will be illustrated for two cases. First, when F (x) = xn (n is
an integer number), which is referred to as a polynomial energy function. Second, when F (x) is a
2

rectified polynomial energy function

?

xn , x 0
(3)
0, x < 0
In the case of the polynomial function with n = 2 the network reduces to the standard model of
associative memory [1]. If n > 2 each term in (2) becomes sharper compared to the n = 2 case, thus
more memories can be packed into the same configuration space before cross-talk intervenes.
F (x) =

Having defined the energy function one can derive an iterative update rule that leads to decrease of
the energy. We use asynchronous updates flipping one unit at a time. The update rule is:
?X
K ? ?
?
X ? (t) ?
X ? (t) ??
(t+1)
?
?
=
Sign
F
?
+
?
F
?
+
?j j
,
(4)
i
i
j j
i
?=1

j6=i

j6=i

The argument of the sign function is the difference of two energies. One, for the configuration with
all but the i-th units clumped to their current states and the i-th unit in the ?off? state. The other one
for a similar configuration, but with the i-th unit in the ?on? state. This rule means that the system
updates a unit, given the states of the rest of the network, in such a way that the energy of the entire
configuration decreases. For the case of polynomial energy function a very similar family of models
was considered in [11, 12, 13, 14, 15, 16]. The update rule in those models was based on the induced
magnetic fields, however, and not on the difference of energies. The two are slightly different due to
the presence of self-coupling terms. Throughout this paper we use energy-based update rules.
How many memories can model (4) store and reliably retrieve? Consider the case of random patterns,
so that each element of the memories is equal to ?1 with equal probability. Imagine that the system
is initialized in a state equal to one of the memories (pattern number ?). One can derive a stability
criterion, i.e. the upper bound on the number of memories such that the network stays in that initial
state. Define the energy difference between the initial state and the state with spin i flipped
K ?
K ?
?n X
?n
X
X
X
E=
?i? ?i? +
?j? ?j?
?i? ?i? +
?j? ?j? ,
?=1

?=1

j6=i

j6=i

where the polynomial energy function is used. This quantity has a mean h Ei = N n (N 2)n ?
2nN n 1 , which comes from the term with ? = ?, and a variance (in the limit of large N )
?2 = ?n (K 1)N n 1 ,
where ?n = 4n2 (2n 3)!!
The i-th bit becomes unstable when the magnitude of the fluctuation exceeds the energy gap h Ei
and the sign of the fluctuation is opposite to the sign of the energy gap. Thus the probability that the
state of a single neuron is unstable (in the limit when both N and K are large, so that the noise is
effectively gaussian) is equal to
r
Z1
Nn 1
x2
dx
(2n 3)!! K
p
Perror =
e 2?2 ?
e 2K(2n 3)!!
n
1
2?
N
2??2
h Ei

Requiring that this probability is less than a small value, say 0.5%, one can find the upper limit on
the number of patterns that the network can store
K max = ?n N n 1 ,
(5)
where ?n is a numerical constant, which depends on the (arbitrary) threshold 0.5%. The case
n = 2 corresponds to the standard model of associative memory and gives the well known result
K = 0.14N . For the perfect recovery of a memory (Perror < 1/N ) one obtains
1
Nn 1
max
Kno
(6)
errors ?
2(2n 3)!! ln(N )
For higher powers n the capacity rapidly grows with N in a non-linear way, allowing the network
to store and reliably retrieve many more patterns than the number of neurons that it has, in accord1
with [13, 14, 15, 16]. This non-linear scaling relationship between the capacity and the size of the
network is the phenomenon that we exploit.
1

The n-dependent coefficient in (6) depends on the exact form of the Hamiltonian and the update rule.
References [13, 14, 15] do not allow repeated indices in the products over neurons in the energy function,
therefore obtain a different coefficient. In [16] the Hamiltonian coincides with ours, but the update rule is
different, which, however, results in exactly the same coefficient as in (6).

3

We study a family of models of this kind as a function of n. At small n many terms contribute to the
sum over ? in (2) approximately equally. In the limit n ! 1 the dominant contribution to the sum
comes from a single memory, which has the largest overlap with the input. It turns out that optimal
computation occurs in the intermediate range.

3

The case of XOR

The case of XOR is elementary, yet instructive. It is presented here for three reasons. First, it illustrates
the construction (2) in this simplest case. Second, it shows that as n increases, the computational
capabilities of the network also increase. Third, it provides the simplest example of a situation in
which the number of memories is larger than the number of neurons, yet the network works reliably.
The problem is the following: given two inputs x and y produce an output z such that the truth table
x
-1
-1
1
1

y
-1
1
-1
1

z
-1
1
1
-1

is satisfied. We will treat this task as an associative memory problem and will simply embed the
four examples of the input-output triplets x, y, z in the memory. Therefore the network has N = 3
identical units: two of which will be used for the inputs and one for the output, and K = 4 memories
?i? , which are the four lines of the truth table. Thus, the energy (2) is equal to
En (x, y, z) =

x

y

z

n

x+y+z

n

x

y+z

n

x+y

z

n

,

(7)

where the energy function is chosen to be a polynomial of degree n. For odd n, energy (7) is an odd
function of each of its arguments, En (x, y, z) = En (x, y, z). For even n, it is an even function.
For n = 1 it is equal to zero. Thus, if evaluated on the corners of the cube x, y, z = ?1, it reduces to
8
n=1
<0,
En (x, y, z) = Cn ,
(8)
n = 2, 4, 6, ...
:
Cn xyz, n = 3, 5, 7, ...,
where coefficients Cn denote numerical constants.

In order to solve the XOR problem one can present to the network an ?incomplete pattern? of inputs
(x, y) and let the output z adjust to minimize the energy of the three-spin configuration, while holding
the inputs fixed. The network clearly cannot solve this problem for n = 1 and n = 2, since the energy
does not depend on the spin configuration. The case n = 2 is the standard model of associative
memory. It can also be thought of as a linear perceptron, and the inability to solve this problem
represents the well known statement [17] that linear perceptrons cannot compute XOR without hidden
neurons. The case of odd n 3 provides an interesting solution. Given two inputs, x and y, one can
choose the output z that minimizes the energy. This leads to the update rule
?
?
?
?
z = Sign En (x, y, 1) En (x, y, +1) = Sign
xy

Thus, in this simple case the network is capable of solving the problem for higher odd values of n,
while it cannot do so for n = 1 and n = 2. In case of rectified polynomials, a similar construction
solves the problem for any n 2. The network works well in spite of the fact that K > N .

4

An example of a pattern recognition problem, the case of MNIST

The MNIST data set is a collection of handwritten digits, which has 60000 training examples and
10000 test images. The goal is to classify the digits into 10 classes. The visible neurons, one for each
pixel, are combined together with 10 classification neurons in one vector that defines the state of
the network. The visible part of this vector is treated as an ?incomplete? pattern and the associative
memory is allowed to calculate a completion of that pattern, which is the label of the image.
Dense associative memory (2) is a recurrent network in which every neuron can be updated multiple
times. For the purposes of digit classification, however, this model will be used in a very limited
4

capacity, allowing it to perform only one update of the classification neurons. The network is
initialized in the state when the visible units vi are clamped to the intensities of a given image and the
classification neurons are in the off state x? = 1 (see Fig.1A). The network is allowed to make
one update of the classification neurons, while keeping the visible units clamped, to produce the
output c? . The update rule is similar to (4) except that the sign is replaced by the continuous function
g(x) = tanh(x)
? X
K ? ?
N
N
?
?
??
X
X
X
X
c? = g
F
??? x? +
??x +
?i? vi
F ??? x? +
??x +
?i? vi
, (9)
?=1

i=1

6=?

i=1

6=?

where parameter regulates the slope of g(x). The proposed digit class is given by the number
of a classification neuron producing the maximal output. Throughout this section the rectified
polynomials (3) are used as functions F . To learn effective memories for use in pattern classification,
an objective function is defined (see Appendix A in Supplemental), which penalizes the discrepancy

vi

vi

c?

x?

179-312 epochs

2

n=2

1.9
1.8
1.7
1.6
1.5

n=3

1.9
1.8
1.7
1.6
1.5

1.4
0

158-262 epochs

2

test
error,error,
test set %

B
test
error,error,
test set %

A

1.4
500

1000

1500
Epochs

2000

2500

number of epochs

3000

0

500

1000

1500

2000

2500

numberEpochs
of epochs

3000

Figure 1: (A) The network has N = 28 ? 28 = 784 visible neurons and Nc = 10 classification neurons.
The visible units are clamped to intensities of pixels (which is mapped on the segment [ 1, 1]), while the
classification neurons are initialized in the state x? and then updated once to the state c? . (B) Behavior of the
error on the test set as training progresses. Each curve corresponds to a different combination of hyperparameters
from the optimal window, which was determined on the validation set. The arrows show the first time when the
error falls below a 2% threshold. All models have K = 2000 memories (hidden units).
between the output c? and the target output. This objective function is then minimized using a
backpropagation algorithm. The learning starts with random memories drawn from a Gaussian
?
distribution. The backpropagation algorithm then finds a collection of K memories ?i,?
, which
minimize the classification error on the training set. The memories are normalized to stay within the
?
1 ? ?i,?
? 1 range, absorbing their overall scale into the definition of the parameter .

The performance of the proposed classification framework is studied as a function of the power n.
The next section shows that a rectified polynomial of power n in the energy function is equivalent
to the rectified polynomial of power n 1 used as an activation function in a feedforward neural
network with one hidden layer of neurons. Currently, the most common choice of activation functions
for training deep neural networks is the ReLU, which in our language corresponds to n = 2 for
the energy function. Although not currently used to train deep networks, the case n = 3 would
correspond to a rectified parabola as an activation function. We start by comparing the performances
of the dense memories in these two cases.
The performance of the network depends on n and on the remaining hyperparameters, thus the hyperparameters should be optimized for each value of n. In order to test the variability of performances
for various choices of hyperparameters at a given n, a window of hyperparameters for which the
network works well on the validation set (see the Appendix A in Supplemental) was determined.
Then many networks were trained for various choices of the hyperparameters from this window to
evaluate the performance on the test set. The test errors as training progresses are shown in Fig.1B.
While there is substantial variability among these samples, on average the cluster of trajectories for
n = 3 achieves better results on the test set than that for n = 2. These error rates should be compared
with error rates for backpropagation alone without the use of generative pretraining, various kinds
of regularizations (for example dropout) or adversarial training, all of which could be added to our
construction if necessary. In this class of models the best published results are all2 in the 1.6% range
[18], see also controls in [19, 20]. This agrees with our results for n = 2. The n = 3 case does
slightly better than that as is clear from Fig.1B, with all the samples performing better than 1.6%.
2

Although there are better results on pixel permutation invariant task, see for example [19, 20, 21, 22].

5

Higher rectified polynomials are also faster in training compared to ReLU. For the n = 2 case, the
error crosses the 2% threshold for the first time during training in the range of 179-312 epochs. For
the n = 3 case, this happens earlier on average, between 158-262 epochs. For higher powers n
this speed-up is larger. This is not a huge effect for a small dataset such as MNIST. However, this
speed-up might be very helpful for training large networks on large datasets, such as ImageNet. A
similar effect was reported earlier for the transition between saturating units, such as logistics or
hyperbolic tangents, to ReLU [7]. In our family of models that result corresponds to moving from
n = 1 to n = 2.
Feature to prototype transition
How does the computation performed by the neural network change as n varies? There are two
extreme classes of theories of pattern recognition: feature-matching and formation of a prototype.
According to the former, an input is decomposed into a set of features, which are compared with
those stored in the memory. The subset of the stored features activated by the presented input is then
interpreted as an object. One object has many features; features can also appear in more than one
object. The prototype theory provides an alternative approach, in which objects are recognized as a
whole. The prototypes do not necessarily match the object exactly, but rather are blurred abstract
n=3
n = 20
n = 30
n=2
1

256

256

256

256

256

192

192

192

192

192

128

128

128

128

128

64

64

64

64

64

0.5
0
0.5

1
n=2
n=3
n=20
n=30

40

30

20

10

0

0

1
2
3
4
5
6
7
8
9
number of strongly positively driven RU

number of RU with

???

10000

number
testimages
images
numberof
of test

percent of active memories

percent of memories, %

50

8000

6000

4000

2000

n=2
n=3
n=20
n=30

errortest = 1.51%
errortest = 1.44%
errortest = 1.61%
errortest = 1.80%

0
1 2 3 4 5 6 7 8 9 10 11 12
number of memories strongly contributing to the correct RU

10

number of memories making the decision

> 0.99

Figure 2: We show 25 randomly selected memories (feature detectors) for four networks, which use rectified polynomials of degrees n = 2, 3, 20, 30 as the energy function. The magnitude of a memory element
corresponding to each pixel is plotted in the location of that pixel, the color bar explains the color code. The
histograms at the bottom are explained in the text. The error rates refer to the particular four samples used in this
figure. RU stands for recognition unit.

representations which include all the features that an object has. We argue that the computational
models proposed here describe feature-matching mode of pattern recognition for small n and the
prototype regime for large n. This can be anticipated from the sharpness of contributions that each
memory makes to the total energy (2). For large n the function F (x) peaks much more sharply
around each memory compared to the case of small n. Thus, at large n all the information about a
digit must be written in only one memory, while at small n this information can be distributed among
several memories. In the case of intermediate n some learned memories behave like features while
others behave like prototypes. These two classes of memories work together to model the data in an
efficient way.
The feature to prototype transition is clearly seen in memories shown in Fig.2. For n = 2 or 3
each memory does not look like a digit, but resembles a pattern of activity that might be useful for
recognizing several different digits. For n = 20 many of the memories can be recognized as digits,
which are surrounded by white margins representing elements of memories having approximately
zero values. These margins describe the variability of thicknesses of lines of different training
examples and mathematically mean that the energy (2) does not depend on whether the corresponding
pixel is on or off. For n = 30 most of the memories represent prototypes of whole digits or large
portions of digits, with a small admixture of feature memories that do not resemble any digit.
6

The feature to prototype transition can be visualized by showing the feature detectors in situations
when there is a natural ordering of pixels. Such ordering exists in images, for example. In general
situations, however, there is no preferred permutation of visible neurons that would reveal this
structure (e.g. in the case of genomic data). It is therefore useful to develop a measure that permits a
distinction to be made between features and prototypes in the absence of such visual space. Towards
the end of training most of the recognition connections ??? are approximately equal to ?1. One
can choose an arbitrary cutoff, and count the number of recognition connections that are in the ?on?
state (??? = +1) for each memory. The distribution function of this number is shown on the left
histogram in Fig.2. Intuitively, this quantity corresponds to the number of different digit classes
that a particular memory votes for. At small n, most of the memories vote for three to five different
digit classes, a behavior characteristic of features. As n increases, each memory specializes and
votes for only a single class. In the case n = 30, for example, more than 40% of memories vote for
only one class, a behavior characteristic of prototypes. A second way to see the feature to prototype
transition is to look at the number of memories which make large contributions to the classification
decision (right histogram in Fig.2). For each test image one can find the memory that makes the
largest contribution to the energy gap, which is the sum over ? in (9). Then one can count the number
of memories that contribute to the gap by more than 0.9 of this largest contribution. For small n,
there are many memories that satisfy this criterion and the distribution function has a long tail. In
this regime several memories are cooperating with each other to make a classification decision. For
n = 30, however, more than 8000 of 10000 test images do not have a single other memory that would
make a contribution comparable with the largest one. This result is not sensitive to the arbitrary choice
(0.9) of the cutoff. Interestingly, the performance remains competitive even for very large n ? 20
(see Fig.2) in spite of the fact that these networks are doing a very different kind of computation
compared with that at small n.

5

Relationship to a neural network with one hidden layer

In this section we derive a simple duality between the dense associative memory and a feedforward
neural network with one layer of hidden neurons. In other words, we show that the same computational
model has two very different descriptions: one in terms of associative memory, the other one in terms
c?

vi

c?

vi

x? =

g
h?

f

"

vi

Figure 3: On the left a feedforward neural network with one layer of hidden neurons. The states of the visible

units are transformed to the hidden neurons using a non-linear function f , the states of the hidden units are
transformed to the output layer using a non-linear function g. On the right the model of dense associative
memory with one step update (9). The two models are equivalent.

of a network with one layer of hidden units. Using this correspondence one can transform the family
of dense memories, constructed for different values of power n, to the language of models used in
deep learning. The resulting neural networks are guaranteed to inherit computational properties of
the dense memories such as the feature to prototype transition.
The construction is very similar to (9), except that the classification neurons are initialized in the state
when all of them are equal to ", see Fig.3. In the limit " ! 0 one can expand the function F in (9)
so that the dominant contribution comes from the term linear in ". Then
K
N
K
K
h X
?X
?
i
hX
i
hX
i
c? ? g
F0
?i? vi ( 2??? x? ) = g
??? F 0 ?i? vi = g
??? f ?i? vi , (10)
?=1

?=1

i=1

?=1

where the parameter is set to = 1/(2") (summation over the visible index i is assumed). Thus,
the model of associative memory with one step update is equivalent to a conventional feedforward
neural network with one hidden layer provided that the activation function from the visible layer to
the hidden layer is equal to the derivative of the energy function
f (x) = F 0 (x)
(11)
7

The visible part of each memory serves as an incoming weight to the hidden layer, and the recognition
part of the memory serves as an outgoing weight from the hidden layer. The expansion used in (10)
Nc
N
P
P
is justified by a condition
?i? vi
??? x? , which is satisfied for most common problems, and
i=1

?=1

is simply a statement that labels contain far less information than the data itself.

From the point of view of associative memory, the dominant contribution shaping the basins of
attraction comes from the low energy states. Therefore mathematically it is determined by the
asymptotics of the activation function f (x), or the energy function F (x), at x ! 1. Thus different
activation functions having similar asymptotics at x ! 1 should fall into the same universality class
and should have similar computational properties. In the table below we list some common activation
activation function
energy function
n
f (x) = tanh(x)
F (x) = ln cosh(x) ? x, at x ! 1 1
f (x) = logistic function
F (x) = ln 1 + ex ? x, at x ! 1
1
f (x) =ReLU
F (x) ? x2 , at x ! 1
2
f (x) = RePn 1
F (x) = RePn
n
functions used in models of deep learning, their associative memory counterparts and the power n
which determines the asymptotic behavior of the energy function at x ! 1.The results of section 4
suggest that for not too large n the speed of learning should improve as n increases. This is consistent
with the previous observation that ReLU are faster in training than hyperbolic tangents and logistics
[5, 6, 7]. The last row of the table corresponds to rectified polynomials of higher degrees. To the
best of our knowledge these activation functions have not been used in neural networks. Our results
suggest that for some problems these higher power activation functions should have even better
computational properties than the rectified liner units.

6

Discussion and conclusions

What is the relationship between the capacity of the dense associative memory, calculated in section 2,
and the neural network with one step update that is used for digit classification? Consider the limit of
very large in (9), so that the hyperbolic tangent is approximately equal to the sign function, as in (4).
In the limit of sufficiently large n the network is operating in the prototype regime. The presented
image places the initial state of the network close to a local minimum of energy, which corresponds
to one of the prototypes. In most cases the one step update of the classification neurons is sufficient
to bring this initial state to the nearest local minimum, thus completing the memory recovery. This is
true, however, only if the stored patterns are stable and have basins of attraction around them of at
least the size of one neuron flip, which is exactly (in the case of random patterns) the condition given
by (6). For correlated patterns the maximal number of stored memories might be different from (6),
however it still rapidly increases with increase of n. The associative memory with one step update (or
the feedforward neural network) is exactly equivalent to the full associative memory with multiple
updates in this limit. The calculation with random patterns thus theoretically justifies the expectation
of a good performance in the prototype regime.
To summarize, this paper contains three main results. First, it is shown how to use the general
framework of associative memory for pattern recognition. Second, a family of models is constructed
that can learn representations of the data in terms of features or in terms of prototypes, and that
smoothly interpolates between these two extreme regimes by varying the power of interaction vertex.
Third, there exists a simple duality between a one step update version of the associative memory
model and a feedforward neural network with one layer of hidden units and an unusual activation
function. This duality makes it possible to propose a class of activation functions that encourages
the network to learn representations of the data with various proportions of features and prototypes.
These activation functions can be used in models of deep learning and should be more effective than
the standard choices. They allow the networks to train faster. We have also observed an improvement
of generalization ability in networks trained with the rectified parabola activation function compared
to the ReLU for the case of MNIST. While these ideas were illustrated using the simplest architecture
of the neural network with one layer of hidden units, the proposed activation functions can also be
used in multilayer architectures. We did not study various regularizations (weight decay, dropout,
etc), which can be added to our construction. The performance of the model supplemented with these
regularizations, as well as performance on other common benchmarks, will be reported elsewhere.
8

References
[1] Hopfield, J.J., 1982. Neural networks and physical systems with emergent collective computational
abilities. Proceedings of the national academy of sciences, 79(8), pp.2554-2558.
[2] LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M. and Huang, F., 2006. A tutorial on energy-based learning.
Predicting structured data, 1, p.0.
[3] Hinton, G.E., Osindero, S. and Teh, Y.W., 2006. A fast learning algorithm for deep belief nets. Neural
computation, 18(7), pp.1527-1554.
[4] Hinton, G.E. and Salakhutdinov, R.R., 2006. Reducing the dimensionality of data with neural networks.
Science, 313(5786), pp.504-507.
[5] Nair, V. and Hinton, G.E., 2010. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning (ICML-10) (pp. 807-814).
[6] Glorot, X., Bordes, A. and Bengio, Y., 2011. Deep sparse rectifier neural networks. In International
Conference on Artificial Intelligence and Statistics (pp. 315-323).
[7] Krizhevsky, A., Sutskever, I. and Hinton, G.E., 2012. ImageNet classification with deep convolutional
neural networks. In Advances in neural information processing systems (pp. 1097-1105).
[8] Amit, D.J., Gutfreund, H. and Sompolinsky, H., 1985. Storing infinite numbers of patterns in a spin-glass
model of neural networks. Physical Review Letters, 55(14), p.1530.
[9] McEliece, R.J., Posner, E.C., Rodemich, E.R. and Venkatesh, S.S., 1987. The capacity of the Hopfield
associative memory. Information Theory, IEEE Transactions on, 33(4), pp.461-482.
[10] Kanter, I. and Sompolinsky, H., 1987. Associative recall of memory without errors. Physical Review A,
35(1), p.380.
[11] Chen, H.H., Lee, Y.C., Sun, G.Z., Lee, H.Y., Maxwell, T. and Giles, C.L., 1986. High order correlation
model for associative memory. In Neural Networks for Computing (Vol. 151, No. 1, pp. 86-99). AIP
Publishing.
[12] Psaltis, D. and Park, C.H., 1986. Nonlinear discriminant functions and associative memories. In Neural
networks for computing (Vol. 151, No. 1, pp. 370-375). AIP Publishing.
[13] Baldi, P. and Venkatesh, S.S., 1987. Number of stable points for spin-glasses and neural networks of higher
orders. Physical Review Letters, 58(9), p.913.
[14] Gardner, E., 1987. Multiconnected neural network models. Journal of Physics A: Mathematical and
General, 20(11), p.3453.
[15] Abbott, L.F. and Arian, Y., 1987. Storage capacity of generalized networks. Physical Review A, 36(10),
p.5091.
[16] Horn, D. and Usher, M., 1988. Capacities of multiconnected memory models. Journal de Physique, 49(3),
pp.389-395.
[17] Minsky, M. and Papert, S., 1969. Perceptron: an introduction to computational geometry. The MIT Press,
Cambridge, expanded edition, 19(88), p.2.
[18] Simard, P.Y., Steinkraus, D. and Platt, J.C., 2003, August. Best practices for convolutional neural networks
applied to visual document analysis. In null (p. 958). IEEE.
[19] Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I. and Salakhutdinov, R., 2014. Dropout: A simple
way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1),
pp.1929-1958.
[20] Wan, L., Zeiler, M., Zhang, S., LeCun, Y. and Fergus, R., 2013. Regularization of neural networks using
dropconnect. In Proceedings of the 30th International Conference on Machine Learning (ICML-13) (pp.
1058-1066).
[21] Goodfellow, I.J., Shlens, J. and Szegedy, C., 2014. Explaining and harnessing adversarial examples. arXiv
preprint arXiv:1412.6572.
[22] Rasmus, A., Berglund, M., Honkala, M., Valpola, H. and Raiko, T., 2015. Semi-supervised learning with
ladder networks. In Advances in Neural Information Processing Systems (pp. 3546-3554).

9


----------------------------------------------------------------

title: 968-capacity-and-information-efficiency-of-a-brain-like-associative-net.pdf

Capacity and Information Efficiency of a
Brain-like Associative Net
Bruce Graham and David Willshaw
Centre for Cognitive Science, University of Edinburgh
2 Buccleuch Place, Edinburgh, EH8 9LW, UK
Email: bruce@cns.ed.ac.uk&david@cns.ed.ac.uk

Abstract
We have determined the capacity and information efficiency of an
associative net configured in a brain-like way with partial connectivity and noisy input cues. Recall theory was used to calculate
the capacity when pattern recall is achieved using a winners-takeall strategy. Transforming the dendritic sum according to input
activity and unit usage can greatly increase the capacity of the
associative net under these conditions. For moderately sparse patterns, maximum information efficiency is achieved with very low
connectivity levels (~ 10%). This corresponds to the level of connectivity commonly seen in the brain and invites speculation that
the brain is connected in the most information efficient way.

1

INTRODUCTION

Standard network associative memories become more plausible as models of associative memory in the brain if they incorporate (1) partial connectivity, (2) sparse
activity and (3) recall from noisy cues. In this paper we consider the capacity of
a binary associative net (Willshaw, Buneman, & Longuet-Higgins, 1969; Willshaw,
1971; Buckingham, 1991) containing these features. While the associative net is
a very simple model of associative memory, its behaviour as a storage device is
not trivial and yet it is tractable to theoretical analysis. We are able to calculate

514

Bruce Graham, David Willshaw

the capacity of the net in different configurations and with different pattern recall
strategies. Here we consider the capacity as a function of connectivity level when
winners-take-all recall is used.
The associative net is an heteroassociative memory in which pairs of binary patterns are stored by altering the connection weights between input and output units
via a Hebbian learning rule. After pattern storage, an output pattern is recalled
by presenting a previously stored input pattern on the input units. Which output
units become active during recall is determined by applying a threshold of activation to measurements that each output unit makes of the input cue pattern. The
most commonly used measurement is the weighted sum of the inputs, or dendritic
sum. Amongst the simpler thresholding strategies is the winners-take-all (WTA)
approach, which chooses the required number of output units with the highest dendritic sums to be active. This works well when the net is fully connected (each input
unit is connected to every output unit), and input cues are noise-free. However,
recall performance deteriorates rapidly if the net is partially connected (each input
unit is connected to only some of the output units) and cues are noisy.
Marr (1971) recognised that when an associative net is only partially connected,
another useful measurement for threshold setting is the total input activity (sum of
the inputs, regardless of the connection weights). The ratio of the dendritic sum
to the input activity can be a better discriminator of which output units should be
active than the dendritic sum alone. Buckingham and Willshaw (1993) showed that
differences in unit usage (the number of patterns in which an output unit is active
during storage) causes variations in the dendritic sums that makes accurate recall
difficult when the input cues are noisy. They incorporated both input activity and
unit usage measurements into a recall strategy that minimised the number of errors
in the output pattern by setting the activity threshold on a unit by unit basis. This
is a rather more complex threshold setting strategy than a simple winners-take-all.
We have previously demonstrated via computer simulations (Graham & Wills haw ,
1994) that the WTA threshold strategy can achieve the same recall performance
as this minimisation approach if the dendritic sums are transformed by certain
functions of the input activity and unit usage before a threshold is applied. Here
we calculate the capacity of the associative net when WTA recall is used with three
different functions of the dendritic sums: (1) pure dendritic sums, (2) modified by
input activity and (3) modified by input activity and unit usage. The results show
that up to four times the capacity can be obtained by transforming the dendritic
sums by a function of both input activity and unit usage. This increase in capacity
was obtained without a loss of information efficiency. For the moderately sparse
patterns used, WTA recall is most information efficient at low levels of connectivity
(~ 10%), as is the minimisation approach to threshold setting (Buckingham, 1991).
This connectivity range is similar to that commonly seen in the brain.

Capacity and Infonnation Efficiency of a Brain-Like Associative Net

2

515

NOTATION AND OPERATION

The associative net consists of N B binary output units each connected to a proportion Z of the N A binary input units. Pairs of binary patterns are stored in the net.
Input and output patterns contain MA and MB active units, respectively (activity
level a = M / N ? 1). All connection weights start at zero. On presentation to
the net of a pattern pair for storage, the connection weight between an active input
unit and an active output unit is set to 1. During recall an input cue pattern is
presented on the input units. The input cue is a noisy version of a previously stored
input pattern in which a fraction, s, of the MA active units do not come from the
stored pattern. A thresholding strategy is applied to the output units to determine
which of them should be active. Those that should be active in response to the
input cue will be called high units, and those that should be inactive will be called
low units. We consider winners-take-all (WTA) thresholding strategies that choose
to be active the MB output units with the highest values of three functions of the
dendritic sum, d, the input activity, a, and the unit usage, r. These functions are
listed in Table 1. The normalised strategy deals with partial connectivity. The
transformed strategy reduces variations in the dendritic sums due to differences in
unit usage. This function minimises the variance of the low unit dendritic sums
with respect to the unit usage (Graham & Willshaw, 1994).
Table 1: WTA Strategies
WTA Strategy
Basic
Normalised
Transformed

3

FUnction
d
d' = d/a
d* = 1 - (1 - d/a)l/r

RECALL THEORY

The capacity of the associative net is defined to be the number of pattern pairs that
can be stored before there is one bit in error in a recalled output pattern. This
cannot be calculated analytically for the net configuration under study. However, it
can be determined numerically for the WTA recall strategy by calculating the recall
response for different numbers of stored patterns, R, until the minimum value of R
is found for which a recall error occurs. The WTA recall response can be calculated
theoretically using expressions for the distributions of the dendritic sums of low and
high output units. The probability that the dendritic sum of a low or high output
unit should have a particular value x is, respectively (Buckingham & Willshaw,
1993; Buckingham, 1991)

P(d,

= x) =

t. (~ )

"'8(1- "B)R-,

( "!,A ) (Zp[rJ)"(I- Zp[rJ)MA-"

(I)

516

Bruce Graham. David Willshaw

P(dh = x)

= ~ ( ~ ) a;'(l-aB)R-" ( ~A

) (ZI'[r +1])"(1- ZI'[r+1])MA-"

(2)
where p[r] and /L[r] are the probabilities that an arbitrarily selected active input is
on a connection with weight 1. For a low unit, p[r] = 1- (1- OA)r. For a high unit
a good approximation for /L is /L[r + 1] ~ g + sp[r] = 1 - s(l - OAY where g and
s are the probabilities that a particular active input in the cue pattern is genuine
(belongs to the stored pattern) or spurious, respectively (g + s = 1) (Buckingham
& Willshaw, 1993). The basic WTA response is calculated using these distributions
by finding the threshold, T, that gives
(3)

The number of false positive and false negative errors of the response is given by

(4)
The actual distributions of the normalised dendritic sums are the distributions of
dja. For the purposes of calculating the normalised WTA response, it is possible to use the basic distributions for the situation where every unit has the mean
input activity, am = MAZ. In this case the low and high unit distributions are
approximately

P(til

P(d'h

= x) =

t. (~ )

,,;'(1- "B)R-" ( "; ) (p[r])"(l- p[r])"m-"

= x) = ~ ( ~ ) aB(1- "B )R-" (

(5)

a; ) (I'[r + 1])" (l-l'[r +1])"m -" (6)

Due to the nonlinear transformation used, it is not possible to calculate the transformed distributions as simple sums of binomials, so the following approach is used
to generate the transformed WTA response. For a given transformed threshold, T*,
and for each possible value of unit usage, r, an equivalent normalised threshold is
calculated via
T'[r] = am (l - (1 - T*Y)

(7)

The transformed cumulative probabilities can then be calculated from the normalised distributions:

P(dj :2: TO) =

P( di. :2: T')

t. (~ )

aB(1 - "B )R-" P( til :2: T'[r])

= ~ ( ~ ) ,,;'(1 -

"B )R-" P(d' :2: T'[r + 1])

(8)

(9)

The normalised and transformed WTA responses are calculated in the same manner
as the basic response, using the appropriate probability distributions.

Capacity and Information Efficiency of a Brain-Like Associative Net

(a) 0% noise

5000

g
~

?u

--B

- - N, T

(b) 40% noise

...
.....................
~

.-~

...-.,

3000

--B
?-N

2500

---T

./"

4000

517

2000

3000

1500

ca
Co
ca 2000
U

1000

1000

500
O~---...L

20

40

60

80

Connectivity (%)

100

0

___--'---'-.....L-....o...-..I.-..........J

20

40

60

80

100

Connectivity (%)

Figure 1: Capacity Versus Connectivity

4

RESULTS

Extensive simulations were previously carried out of WTA recall from a large
associative net with the following specifications (Graham & Willshaw, 1994):
NA = 48000, MA = 1440, NB = 6144, MB = 180. Agreement between the simulations and the theoretical recall described above is extremely good, indicating that
the approximations used in the theory are valid. Here we use the theoretical recall
to calculate capacity results for this large associative net that are not easily obtained via simulations. All the results shown have been generated using the theory
described in the previous section.
Figure 1 shows the capacity as a function of connectivity for the different WTA
strategies when there is no noise in the input cue, or 40% noise in the cue (legend: B
= basic WTA, N = normalised WTA, T = transformed WTA; for clarity, individual
data points are omitted). With no noise in the cue the normalised and transformed
methods perform identically, so only the normalised results are shown. Figure l(a)
highlights the effectiveness of normalising the dendritic sums against input activity
when the net is partially connected. Figure 1(b) shows the effect of noise on capacity.
The capacity of each recall strategy at a given connectivity level is much reduced
compared to the noise-free case. However, for connectivities greater than 10% the
capacity of the transformed WTA is now much greater than either the normalised
or basic WTA.
The relative capacities of the different strategies are shown in Figure 2 (legend:
NIB = ratio of normalised to basic capacity, T I B = ratio of transformed to basic,
TIN = ratio of transformed to normalised). In the noise-free case (Figure 2(a)), at
low levels of connectivity the relative capacity is distorted because the basic capacity

518

Bruce Graham, David Willshaw

drops to near zero, so that even low normalised capacities are relatively very large.
For most connectivity levels (10-90%) the normalised WTA provides 2-4 times the
capacity of the basic WTA. In the noisy case (Figure 2(b)), the normalised capacity
is only up to 1.5 times the basic capacity over this range of connectivities. The
transformed WTA, however, provides 3 to nearly 4 times the basic capacity and 2.5
to nearly 3 times the normalised capacity for connectivities greater than 10%.
The capacities can be interpreted in information theoretic terms by considering the
information efficiency of the net. This is the ratio of the amount of information
that can be retrieved from the net to the number of bits of storage available and
is given by "'0 = Ro10jZNANB' where Ro is the capacity, 10 is the amount of
information contained in an output pattern and Z NANB is the number of weights,
or bits of storage required (Willshaw et al., 1969; Buckingham & Willshaw, 1992).
Information efficiency as a function of connectivity is shown in Figure 3. There is
a distinct peak in information efficiency for each of the recall strategies at some
low level of connectivity. The peak information efficiencies and the efficiencies at
full connectivity are summarised in Table 2. The greatest contrast between full
and partial connectivity is seen with the normalised WTA and noise-free cues. At
1% connectivity the normalised WTA is nearly 14 times more efficient than at full
connectivity. In absolute terms, however, the normalised capacity is only 694 at
1% connectivity, compared with 5122 at full connectivity. The peak efficiency of
53% obtained by the normalised WTA is approaching the theoretically approximate
maximum of 69% for a fully connected net (Willshaw et al., 1969).

5

DISCUSSION

Previous simulations (Graham & Willshaw, 1994) have shown that, when the input
cues are noisy, the recall performance of the winners-take-all thresholding strategy
applied to the partially connected associative net is greatly improved if the dendritic sums of the output units are transformed by functions of input activity and
unit usage. We have confirmed and extended these results here by calculating the
theoretical capacity of the associative net as a function of connectivity.
For the moderately sparse patterns used, all of the recall strategies are most information efficient at very low levels of connectivity (~ 10%). However, the optimum
connectivity level is dependent on the pattern coding rate. Extending the analysis
of Willshaw et al. (1969) to a partially connected net using normalised WTA recall
yields that maximum information efficiency is obtained when ZMA = log2(NB).
So for input coding rates higher than log2(NB), a partially connected net is most
information efficient. For the input coding rate used here, this relationship gives an
optimum connectivity level of 0.87%, very close to the 1% obtained from the recall
theory.
Comparing the peak efficiencies across the different strategies for the noisy cue
case, the normalised WTA is about twice as efficient as the basic WTA, while the
transformed WTA is three times as efficient. This comparison does not include the

Capacity and Information Efficiency of a Brain-Like Associative Net

(a) 0% noise

(b) 40% noise

12

4

- - NIB, TIB

10
.~
0
III

a.

I

~
;;

,

3

(

8

III

Q)

~------

6
4

a:

0

40
60
20
80
Connectivity (%)

0

,

I I
\1 '

......

- ..............

1;?-

,,

- -___

I, - - NIB
II

1

-_

' --- T/B
I
TIN

\

100

..... , ,

,..- ...... -------- -~

}{' ...

2

.,

2

""

I

III

u

519

0

20
40
60
80
Connectivity (%)

0

100

Figure 2: Relative Capacity Versus Connectivity
(a) 0% noise

(b) 40% noise

60

-

6

~ 50

g
Q)

40

ffic:

30

.(3

ia 20
E
....

0

1: 10

o

4

\

.Q

~

r-o

3
2

----"::7

20
40
60
80
Connectivity (%)

,,,'

5

--B
._._- N, T

>.

",

100

0

--B
--N
---T

'\ '\
'\
'\

,

f.l
.\
I

'" "

.,
;'\
., ' ...'--

"

............

- ...

---------

L-'--'........- - - L _ - - L _ - - - L . - - ' - - - I

0

20
40
60
80
Connectivity (%)

Figure 3: Information Efficiency Versus Connectivity
Table 2: Information Efficiency
WTA
Strategy
Basic
Normalised
Transformed

at
Peak
(%)
6.1
53.3
53.3

TJo

0% Noise
Z at
TJo at
Peak Z=1
(%)
(%)
4
3.9
1
3.9
1
3.9

at
Peak

T]o

40% Noise
Z at
TJo at
Peak Z=1

(%)

(%)

(%)

2.0
3.6
5.7

7
5
10

0.8
0.8
2.3

100

520

Bruce Graham, David Willshaw

cost of storing input activity and unit usage information. If one bit of storage per
connection is required for the input activity, and another bit for the unit usage, then
the information efficiency of the normalised WTA is halved, and the information
efficiency of the transformed WTA is reduced by two thirds. This results in all the
strategies having about the same peak efficiency. However, the absolute capacities
of the different strategies at their peak efficiencies are 183,237 and 741 for the basic,
normalised and transformed WTA, respectively. So, at the same level of efficiency,
the transformed WTA delivers four times the capacity of the basic WTA.

In conclusion, numerical calculations of the capacity of the associative net show that
it is most information efficient at a very low level of connectivity when moderately
sparse patterns are stored. Including input activity and unit usage information into
the recall calculations results in a four-fold increase in storage capacity without loss
of efficiency.
Acknowledgements

To the Medical Research Council for financial support under Programme Grant PG
9119632

References
Buckingham, J., & Willshaw, D. (1992). Performance characteristics of the associative net. Network, 8, 407-414.
Buckingham, J., & Willshaw, D. (1993). On setting unit thresholds in an incompletely connected associative net. Network, 4, 441-459.
Buckingham, J. (1991). Delicate nets, faint recollections: a study of partially connected associative network memories. Ph.D. thesis, University of Edinburgh.
Graham, B., & Willshaw, D. (1994). Improving recall from an associative memory.
Bioi. Cybem., in press.
Marr, D. (1971). Simple memory: a theory for archicortex. Phil. Trans. Roy. Soc.
Lond. B, 262, 23-81.
Shepherd, G. (Ed.). (1990). The Synaptic Organization of the Brain (Third edition).
Oxford University Press, New York, Oxford.
Willshaw, D. (1971). Models of distributed associative memory. Ph.D. thesis, University of Edinburgh.
Willshaw, D., Buneman, 0., & Longuet-Higgins, H. (1969). Non-holographic associative memory. Nature, 222, 960-962.


----------------------------------------------------------------

