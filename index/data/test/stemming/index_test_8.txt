query sentence: usage of spectograms in astronomy 
---------------------------------------------------------------------
title: 2153-prediction-and-semantic-association.pdf

Prediction and Semantic Association
Thomas L. Griffiths & Mark Steyvers
Department of Psychology
Stanford University, Stanford, CA 94305-2130
{gruffydd,msteyver}@psych.stanford.edu

Abstract
We explore the consequences of viewing semantic association as
the result of attempting to predict the concepts likely to arise in a
particular context. We argue that the success of existing accounts
of semantic representation comes as a result of indirectly addressing
this problem, and show that a closer correspondence to human data
can be obtained by taking a probabilistic approach that explicitly
models the generative structure of language.

1

Introduction

Many cognitive capacities, such as memory and categorization, can be analyzed as
systems for efficiently predicting aspects of an organism's environment [1]. Previously, such analyses have been concerned with memory for facts or the properties
of objects, where the prediction task involves identifying when those facts might
be needed again, or what properties novel objects might possess. However, one of
the most challenging tasks people face is linguistic communication. Engaging in
conversation or reading a passage of text requires retrieval of a variety of concepts
from memory in response to a stream of information. This retrieval task can be
facilitated by predicting which concepts are likely to be needed from their context,
having efficiently abstracted and stored the cues that support these predictions.
In this paper, we examine how understanding the problem of predicting words
from their context can provide insight into human semantic association, exploring
the hypothesis that the association between words is at least partially affected
by their statistical relationships. Several researchers have argued that semantic
association can be captured using high-dimensional spatial representations , with
the most prominent such approach being Latent Semantic Analysis (LSA) [5]. We
will describe this procedure, which indirectly addresses the prediction problem. We
will then suggest an alternative approach which explicitly models the way language
is generated and show that this approach provides a better account of human word
association data than LSA, although the two approaches are closely related. The
great promise of this approach is that it illustrates how we might begin to relax some
of the strong assumptions about language made by many corpus-based methods.
We will provide an example of this, showing results from a generative model that
incorporates both sequential and contextual information.

2

Latent Semantic Analysis

Latent Semantic Analysis addresses the prediction problem by capturing similarity
in word usage: seeing a word suggests that we should expect to see other words
with similar usage patterns. Given a corpus containing W words and D documents,
the input to LSA is a W x D word-document co-occurrence matrix F in which fwd
corresponds to the frequency with which word w occurred in document d. This
matrix is transformed to a matrix G via some function involving the term frequency
fwd and its frequency across documents fw .. Many applications of LSA in cognitive
science use the transformation

gwd = IOg{fwd

+ 1}(1 -

Hw)

H

- _

2:D_
Wlog{W}
d-l f w
f w.
logD

w -

'

(1)

where Hw is the normalized entropy of the distribution over documents for each
word. Singular value decomposition (SVD) is applied to G to extract a lower
dimensional linear subspace that captures much of the variation in usage across
words. The output of LSA is a vector for each word, locating it in the derived
subspace. The association between two words is typically assessed using the cosine of
the angle between their vectors, a measure that appears to produce psychologically
accurate results on a variety of tasks [5] . For the tests presented in this paper,
we ran LSA on a subset of the TASA corpus, which contains excerpts from texts
encountered by children between first grade and the first year of college. Our subset
used all D = 37651 documents, and the W = 26414 words that occurred at least
ten times in the whole corpus, with stop words removed. From this we extracted a
500 dimensional representation, which we will use throughout the paper. 1

3

The topic model

Latent Semantic Analysis gives results that seem consistent with human judgments
and extracts information relevant to predicting words from their contexts, although
it was not explicitly designed with prediction in mind. This relationship suggests
that a closer correspondence to human data might be obtained by directly attempting to solve the prediction task. In this section, we outline an alternative approach
that involves learning a probabilistic model of the way language is generated. One
generative model that has been used to outperform LSA on information retrieval
tasks views documents as being composed of sets of topics [2,4]. If we assume that
the words that occur in different documents are drawn from T topics, where each
topic is a probability distribution over words, then we can model the distribution
over words in anyone document as a mixture of those topics
T

P(Wi)

= LP(Wil zi =j)P(Zi

=j)

(2)

j=l

where Zi is a latent variable indicating the topic from which the ith word was drawn
and P(wilzi = j) is the probability of the ith word under the jth topic. The words
likely to be used in a new context can be determined by estimating the distribution
over topics for that context, corresponding to P(Zi).
Intuitively, P(wlz = j) indicates which words are important to a topic, while P(z)
is the prevalence of those topics within a document. For example, imagine a world
where the only topics of conversation are love and research. We could then express
IThe dimensionality of the representation is an important parameter for both models in
this paper. LSA performed best on the word association task with around 500 dimensions,
so we used the same dimensionality for the topic model.

the probability distribution over words with two topics, one relating to love and the
other to research. The content of the topics would be reflected in P(wlz = j): the
love topic would give high probability to words like JOY, PLEASURE, or HEART, while
the research topic would give high probability to words like SCIENCE, MATHEMATICS,
or EXPERIMENT. Whether a particular conversation concerns love, research, or
the love of research would depend upon its distribution over topics, P(z), which
determines how these topics are mixed together in forming documents.
Having defined a generative model, learning topics becomes a statistical problem.
The data consist of words w = {Wl' ... , w n }, where each Wi belongs to some document di , as in a word-document co-occurrence matrix. For each document we
have a multinomial distribution over the T topics, with parameters ()(d), so for a
word in document d, P(Zi = j) = ();d;). The jth topic is represented by a multinomial distribution over the W words in the vocabulary, with parameters 1/i), so
P(wilzi = j) = 1>W. To make predictions about new documents, we need to assume
a prior distribution on the parameters (). Existing parameter estimation algorithms
make different assumptions about (), with varying results [2,4]. Here, we present a
novel approach to inference in this model, using Markov chain Monte Carlo with a
symmetric Dirichlet(a) prior on ()(di) for all documents and a symmetric Dirichlet(,B)
prior on 1>(j) for all topics. In this approach we do not need to explicitly represent
the model parameters: we can integrate out () and 1>, defining the model simply in
terms of the assignments of words to topics indicated by the Zi'
Markov chain Monte Carlo is a procedure for obtaining samples from complicated
probability distributions, allowing a Markov chain to converge to the taq~et distribution and then drawing samples from the states of that chain (see [3]). We
use Gibbs sampling, where each state is an assignment of values to the variables
being sampled, and the next state is reached by sequentially sampling all variables
from their distribution when conditioned on the current values of all other variables
and the data. We will sample only the assignments of words to topics, Zi. The
conditional posterior distribution for Zi is given by
'1
)
P( Zi=)Zi ,wex

where

Z- i

is the assignment of all

Zk

+ (3 n(di) + a
-',}
-',}
(d ' )
n_i,j + W (3 n_i,. + Ta
n eW;)
(.)

such that k

f:.

(3)

i, and n~~:j is the number

of words assigned to topic j that are the same as w, n~L is the total number of
words assigned to topic j, n~J,j is the number of words from document d assigned
to topic j, and n~J. is the total number of words in document d, all not counting
the assignment of the current word Wi. a,,B are free parameters that determine how
heavily these distributions are smoothed.
We applied this algorithm to our subset of the TASA corpus, which contains n =
5628867 word tokens. Setting a = 0.1,,B = 0.01 we obtained 100 samples of 500
topics, with 10 samples from each of 10 runs with a burn-in of 1000 iterations and
a lag of 100 iterations between samples. 2 Each sample consists of an assignment of
every word token to a topic, giving a value to each Zi. A subset of the 500 topics
found in a single sample are shown in Table 1. For each sample we can compute
2Random numbers were generated with the Mersenne Twister, which has an extremely
deep period [6]. For each run, the initial state of the Markov chain was found using an
on-line version of Equation 3.

FEEL
FEELINGS
FEELING
ANGRY
WAY
THINK
SHOW
FEELS
PEOPLE
FRIENDS
THINGS
MIGHT
HELP
HAPPY
FELT
LOVE
ANGER
BEING
WAYS
FEAR

MUSIC

BALL
GAME
TEAM

PLAY

DANCE
PLAYS
STAGE
PLAYED
BAND
AUDIENCE
MUSICAL
DANCING
RHYTHM
PLAYING
THEATER
DRUM
ACTORS
SHOW
BALLET
ACTOR
DRAMA
SONG

PLAY

BASEBALL
FOOTBALL
PLAYERS
GAMES
PLAYING
FIELD
PLAYED
PLAYER
COACH
BASKETBALL
SPORTS
HIT
BAT
TENNIS
TEAMS
SOCCER

SCIENCE
STUDY
SCIENTISTS
SCIENTIFIC
KNOWLEDGE
WORK
CHEMISTRY
RESEARCH
BIOLOGY
MATHEMATICS
LABORATORY
STUDYING
SCIENTIST
PHYSICS
FIELD
STUDIES
UNDERSTAND
STUDIED
SCIENCES
MANY

WORKERS
WORK
LABOR
JOBS
WORKING
WORKER
WAGES
FACTORY
JOB
WAGE
SKILLED
PAID
CONDITIONS
PAY
FORCE
MANY
HOURS
EMPLOYMENT
EMPLOYED
EMPLOYERS

FORCE
FORCES
MOT IO N
BODY
GRAVITY
MASS
PULL
NEWTON
OBJECT
LAW
DIRECTION
MOVING
REST
FALL
ACTING
MOMENTUM
DISTANCE
GRAVITATIONAL
PUSH
VELOCITY

Table 1: Each column shows the 20 most probable words in one of the 500 topics
obtained from a single sample. The organization of the columns and use of boldface
displays the way in which polysemy is captured by the model.
the posterior predictive distribution (and posterior mean for q/j)) :

P(wl z

4

= j, z, w) =

J

P(wl z

(.) ( Iz,

= j, ? J

0)

)P(? J

w) d?

( 0)
J

+ (3
+ W (3

n (W)

= _(;=,.J)_ _
nj

(4)

Predicting word association

We used both LSA and the topic model to predict the association between pairs
of words, comparing these results with human word association norms collected by
Nelson, McEvoy and Schreiber [7]. These word association norms were established
by presenting a large number of participants with a cue word and asking them to
name an associated word in response. A total of 4544 of the words in these norms
appear in the set of 26414 taken from the TASA corpus.
4.1

Latent Semantic Analysis

In LSA, the association between two words is usually measured using the cosine
of the angle between their vectors. We ordered the associates of each word in the
norms by their frequencies , making the first associate the word most commonly
given as a response to the cue. For example, the first associate of NEURON is BRAIN.
We evaluated the cosine between each word and the other 4543 words in the norms ,
and then computed the rank of the cosine of each of the first ten associates, or
all of the associates for words with less than ten. The results are shown in Figure
1. Small ranks indicate better performance, with a rank of one meaning that the
target word had the highest cosine. The median rank of the first associate was 32,
and LSA correctly predicted the first associate for 507 of the 4544 words.
4.2

The topic model

The probabilistic nature of the topic model makes it easy to predict the words likely
to occur in a particular context. If we have seen word WI in a document, then we
can determine the probability that word W2 occurs in that document by computing
P( w2IwI). The generative model allows documents to contain multiple topics, which

450

400

1_

LSA - cosine
LSA - inner product
Topi c model

D

1

350

300

II

250

200

150

l;r
100

50

o

lin

2

3

4

5

6

7

8

9

10

Associate number

Figure 1: Performance of different methods of prediction on the word association
task. Error bars show one standard error, estimated with 1000 bootstrap samples.
is extremely important to capturing the complexity of large collections of words
and computing the probability of complete documents. However, when comparing
individual words it is more effective to assume that they both come from a single
topic. This assumption gives us

(5)
z

where we use Equation 4 for P(wlz) and P(z) is uniform, consistent with the symmetric prior on e, and the subscript in Pi (w2lwd indicates the restriction to a single
topic. This estimate can be computed for each sample separately, and an overall
estimate obtained by averaging over samples. We computed Pi (w2Iwi) for the 4544
words in the norms, and then assessed the rank of the associates in the resulting
distribution using the same procedure as for LSA. The results are shown in Figure
1. The median rank for the first associate was 32, with 585 of the 4544 first associates exactly correct. The probabilistic model performed better than LSA, with
the improved performance becoming more apparent for the later associates .
4.3

Discussion

The central problem in modeling semantic association is capturing the interaction
between word frequency and similarity of word usage. Word frequency is an important factor in a variety of cognitive tasks, and one reason for its importance is its
predictive utility. A higher observed frequency means that a word should be predicted to occur more often. However, this effect of frequency should be tempered by
the relationship between a word and its semantic context . The success of the topic
model is a consequence of naturally combining frequency information with semantic
similarity: when a word is very diagnostic of a small number of topics, semantic
context is used in prediction. Otherwise, word frequency plays a larger role.

The effect of word frequency in the topic model can be seen in the rank-order
correlation of the predicted ranks of the first associates with the ranks predicted
by word frequency alone , which is p = 0.49. In contrast, the cosine is used in LSA
because it explicitly removes the effect of word frequency, with the corresponding
correlation being p = -0.01. The cosine is purely a measure of semantic similarity,
which is useful in situations where word frequency is misleading, such as in tests of
English fluency or other linguistic tasks, but not necessarily consistent with human
performance. This measure was based in the origins of LSA in information retrieval ,
but other measures that do incorporate word frequency have been used for modeling
psychological data. We consider one such measure in the next section.

5

Relating LSA and the topic model

The decomposition of a word-document co-occurrence matrix provided by the topic
model can be written in a matrix form similar to that of LSA. Given a worddocument co-occurrence matrix F, we can convert the columns into empirical estimates of the distribution over words in each document by dividing each column
by its sum. Calling this matrix P, the topic model approximates it with the nonnegative matrix factorization P ~ ?O, where column j of ? gives 4/j) , and column d
of 0 gives ()(d). The inner product matrix ppT is proportional to the empirical estimate of the joint distribution over words P(WI' W2)' We can write ppT ~ ?OOT ?T,
corresponding to P(WI ,W2) = L z"Z 2 P(wIl zdP(W2Iz2)P(ZI,Z2) , with OOT an empirical estimate of P(ZI , Z2)' The theoretical distribution for P(ZI, Z2) is proportional to 1+ 0::, where I is the identity matrix, so OOT should be close to diagonal.
The single topic assumption removes the off-diagonal elements, replacing OOT with
I to give PI (Wl ' W2) ex: ??T.
By comparison, LSA transforms F to a matrix G via Equation 1, then the SVD
gives G ~ UDV T for some low-rank diagonal D. The locations of the words along
the extracted dimensions are X = UD. If the column sums do not vary extensively,
the empirical estimate of the joint distribution over words specified by the entries in
G will be approximately P(WI,W2) ex: GG T . The properties of the SVD guarantee
that XX T , the matrix of inner products among the word vectors , is the best lowrank approximation to GG T in terms of squared error. The transformations in
Equation 1 are intended to reduce the effects of word frequency in the resulting
representation, making XX T more similar to ??T.
We used the inner product between word vectors to predict the word association
norms, exactly as for the cosine. The results are shown in Figure 1. The inner
product initially shows worse performance than the cosine, with a median rank
of 34 for the first associate and 500 exactly correct, but performs better for later
associates. The rank-order correlation with the predictions of word frequency for
the first associate was p = 0.46, similar to that for the topic model. The rankorder correlation between the ranks given by the inner product and the topic model
was p = 0.81, while the cosine and the topic model correlate at p = 0.69. The
inner product and PI (w2lwd in the topic model seem to give quite similar results,
despite being obtained by very different procedures. This similarity is emphasized
by choosing to assess the models with separate ranks for each cue word, since this
measure does not discriminate between joint and conditional probabilities. While
the inner product is related to the joint probability of WI and W2, PI (w2lwd is a
conditional probability and thus allows reasonable comparisons of the probability
of W2 across choices of WI , as well as having properties like asymmetry that are
exhibited by word association.

"syntax"

HE
YOU
THEY
I

SHE
WE
IT
PEOPLE
EVERYONE
OTHERS
SCIENTISTS
SOMEONE
WHO
NOBODY
ONE
SOMETHING
ANYONE
EVERYBODY
SOME
THEN

ON
AT
INTO
FROM
WITH
THROUGH
OVER
AROUND
AGAINST
ACROSS
UPON
TOWARD
UNDER
ALONG
NEAR
BEHIND
OFF
ABOVE
DOWN
BEFORE

BE
MAKE
GET
HAVE
GO
TAKE
DO
FIND
USE
SEE
HELP
KEEP
GIVE
LOOK
COME
WORK
MOVE
LIVE
EAT
BECOME

"semantics"

SAID
ASKED
THOUGHT
TOLD
SAYS
MEANS
CALLED
CRIED
S HOWS
ANSWERED
TELLS
REPLIED
SHOUTED
EXPLAINED
LAUGHED
MEANT
WROTE
SHOWED
BELIEVED
WHISPERED

MAP
NORTH
EARTH
SOUTH
POLE
MAPS
EQUATOR
WEST
LINES
EAST
AUSTRALIA
GLOBE
POLES
HEMISPHERE
LATITUDE
PLACES
LAND
WORLD
COMPASS
CONTINE NTS

DOCTOR
PATIENT
HEALTH
HOSPITAL
MEDICAL
CARE
PATIENTS
NURSE
DOCTORS
MEDICINE
NURSING
TREATMENT
NURSES
PHYSICIAN
HOSPITALS
DR
S ICK
ASSISTANT
EMERGENCY
PRACTICE

Table 2: Each column shows the 20 most probable words in one of the 48 "syntactic"
states of the hidden Markov model (four columns on the left) or one of the 150
"semantic" topics (two columns on the right) obtained from a single sample.

6

Exploring more complex generative models

The topic model, which explicitly addresses the problem of predicting words from
their contexts, seems to show a closer correspondence to human word association
than LSA. A major consequence of this analysis is the possibility that we may be
able to gain insight into some of the associative aspects of human semantic memory
by exploring statistical solutions to this prediction problem. In particular, it may
be possible to develop more sophisticated generative models of language that can
capture some of the important linguistic distinctions that influence our processing
of words. The close relationship between LSA and the topic model makes the latter
a good starting point for an exploration of semantic association, but perhaps the
greatest potential of the statistical approach is that it illustrates how we might go
about relaxing some of the strong assumptions made by both of these models.
One such assumption is the treatment of a document as a "bag of words" , in which
sequential information is irrelevant. Semantic information is likely to influence only
a small subset of the words used in a particular context, with the majority of the
words playing functional syntactic roles that are consistet across contexts. Syntax is
just as important as semantics for predicting words, and may be an effective means
of deciding if a word is context-dependent. In a preliminary exploration of the
consequences of combining syntax and semantics in a generative model for language,
we applied a simple model combining the syntactic structure of a hidden Markov
model (HMM) with the semantic structure of the topic model. Specifically, we used
a third-order HMM with 50 states in which one state marked the start or end of
a sentence, 48 states each emitted words from a different multinomial distribution,
and one state emitted words from a document-dependent multinomial distribution
corresponding to the topic model with T = 150. We estimated parameters for this
model using Gibbs sampling, integrating out the parameters for both the HMM and
the topic model and sampling a state and a topic for each of the 11821091 word
tokens in the corpus. 3 Some of the state and topic distributions from a single sample
after 1000 iterations are shown in Table 2. The states of the HMM accurately picked
out many of the functional classes of English syntax, while the state corresponding
to the topic model was used to capture the context-specific distributions over nouns.
3This larger number is a result of including low frequency and stop words.

Combining the topic model with the HMM seems to have advantages for both: no
function words are absorbed into the topics, and the HMM does not need to deal
with the context-specific variation in nouns. The model also seems to do a good job
of generating topic-specific text - we can clamp the distribution over topics to pick
out those of interest, and then use the model to generate phrases. For example, we
can generate phrases on the topics of research ( "the chief wicked selection of research
in the big months" , "astronomy peered upon your scientist's door", or "anatomy
established with principles expected in biology") , language ("he expressly wanted
that better vowel"), and the law ("but the crime had been severely polite and
confused" , or "custody on enforcement rights is plentiful"). While these phrases
are somewhat nonsensical , they are certainly topical.

7

Conclusion

Viewing memory and categorization as systems involved in the efficient prediction
of an organism's environment can provide insight into these cognitive capacities.
Likewise, it is possible to learn about human semantic association by considering
the problem of predicting words from their contexts. Latent Semantic Analysis
addresses this problem, and provides a good account of human semantic association.
Here, we have shown that a closer correspondence to human data can be obtained
by taking a probabilistic approach that explicitly models the generative structure
of language, consistent with the hypothesis that the association between words
reflects their probabilistic relationships. The great promise of this approach is the
potential to explore how more sophisticated statistical models of language, such as
those incorporating both syntax and semantics, might help us understand cognition.
Acknowledgments
This work was generously supported by the NTT Communications Sciences Laboratories.
We used Mersenne Twister code written by Shawn Cokus, and are grateful to Touchstone
Applied Science Associates for making available the TASA corpus, and to Josh Tenenbaum
for extensive discussions on this topic.

References
[1] J. R. Anderson. The Adaptive Character of Thought. Erlbaum, Hillsdale, NJ, 1990.
[2] D . M. Blei, A. Y. Ng, and M. 1. Jordan . Latent Dirichlet allocation. In T. G. Dietterich,
S. Becker, and Z. Ghahramani, eds, Advances in Neural Information Processing Systems
14, 2002.
[3] W . R. Gilks, S. Richardson, and D. J . Spiegelhalter, eds. Markov Chain Monte Carlo
in Practice. Chapman and Hall, Suffolk, 1996.
[4] T . Hofmann. Probabilistic Latent Semantic Indexing. In Proceedings of the TwentySecond Annual International SIGIR Conference, 1999.
[5] T. K. Landauer and S. T. Dumais. A solution to Plato's problem: The Latent Semantic
Analysis theory of acquisition, induction, and representation of knowledge. Psychological
Review, 104:211- 240, 1997.
[6] M. Matsumoto and T . Nishimura. Mersenne twister: A 623-dimensionally equidistributed uniform pseudorandom number generator. ACM Transactions on Modeling and
Computer Simulation, 8:3- 30, 1998.
[7] D. L. Nelson , C. L. McEvoy, and T. A. Schreiber. The University of South Florida
word association norms. http://www. usf. edu/FreeAssociation, 1999.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 960-non-linear-prediction-of-acoustic-vectors-using-hierarchical-mixtures-of-experts.pdf

Non-linear Prediction of Acoustic Vectors
Using Hierarchical Mixtures of Experts

S.R.Waterhouse
A.J.Robinson
Cambridge University Engineering Department,
Trumpington St ., Cambridge, CB2 1PZ, England.
Tel: [+44] 223 332800, Fax: [+44] 223 332662,
Email: srwlO01.ajr@eng.cam.ac.uk
URL: http://svr-www.eng.cam.ac.ukr srw1001

Abstract
In this paper we consider speech coding as a problem of speech
modelling. In particular, prediction of parameterised speech over
short time segments is performed using the Hierarchical Mixture of
Experts (HME) (Jordan & Jacobs 1994). The HME gives two advantages over traditional non-linear function approximators such
as the Multi-Layer Percept ron (MLP); a statistical understanding of the operation of the predictor and provision of information
about the performance of the predictor in the form of likelihood
information and local error bars. These two issues are examined
on both toy and real world problems of regression and time series
prediction. In the speech coding context, we extend the principle
of combining local predictions via the HME to a Vector Quantization scheme in which fixed local codebooks are combined on-line
for each observation.

1 INTRODUCTION
We are concerned in this paper with the application of multiple models, specifically the Hierarchical Mixtures of Experts, to time series prediction, specifically the
problem of predicting acoustic vectors for use in speech coding. There have been
a number of applications of multiple models in time series prediction. A classic
example is the Threshold Autoregressive model (TAR) which was used by Tong &

836

S. R. Waterhouse, A. J. Robinson

Lim (1980) to predict sunspot activity. More recently, Lewis, Kay and Stevens
(in Weigend & Gershenfeld (1994)) describe the use of Multivariate and Regression Splines (MARS) to the prediction of future values of currency exchange rates.
Finally, in speech prediction, Cuperman & Gersho (1985) describe the Switched
Inter-frame Vector Prediction (SIVP) method which switches between separate linear predictors trained on different statistical classes of speech. The form of time
series prediction we shall consider in this paper is the single step prediction fI(t) of a
future quantity y(t) , by considering the previous samples. This may be viewed as
a regression problem over input-output pairs {x t), y(t)}~ where x(t) is the lag vector (y(t-I), y(t-2), ... , y(t- p ?. We may perform this regression using standard linear
models such as the Auto-Regressive (AR) model or via nonlinear models such as
connectionist feed-forward or recurrent networks. The HME overcomes a number of
problems associated with traditional connectionist models via its architecture and
statistical framework. Recently, Jordan & Jacobs (1994) and Waterhouse & Robinson (1994) have shown that via the EM algorithm and a 2nd order optimization
scheme known as Iteratively Reweighted Least Squares (IRLS), the HME is faster
than standard Multilayer Perceptrons (MLP) by at least an order of magnitude on
regression and classification tasks respectively. Jordan & Jacobs also describe various methods to visualise the learnt structure of the HME via 'deviance trees' and
histograms of posterior probabilities. In this paper we provide further examples
of the structural relationship of the trained HME and the input-output space in
the form of expert activation plots. In addition we describe how the HME can be
extended to give local error bars or measures of confidence in regression and time
series prediction problems. Finally, we describe the extension of the HME to acoustic vector prediction, and a VQ coding scheme which utilises likelihood information
from the HME.

c:

2 HIERARCHICAL MIXTURES OF EXPERTS
The HME architecture (Figure 1) is based on the principle of 'divide and conquer'
in which a large, hard to solve problem is broken up into many, smaller, easier
to solve problems. It consists of a series of 'expert networks' which are trained
on different parts of the input space. The outputs of the experts are combined
by a 'gating network' which is trained to stochastically select the expert which is
performing best at solving a particular part of the problem. The operation of the
HME is as follows: the gating networks receive the input vectors x(t) and produce
as outputs probabilities P(mi/.x(t), 7'/j) for each local branch mj of assigning the current
input to the different branches, where T/j are the gating network parameters. The
expert networks sit at the leaves of the tree and each output a vector flJt) given
input vector x(t) and parameters Bj . These outputs are combined in a weighted sum
by P(mjlX<t), T/j) to give the overall output vector for this region. This procedure
continues recursively upwards to the root node. In time series prediction, each
expert j is a linear single layer network with the form:
flY) = B; x (t)

where B; is matrix and
form to an AR model.

x(t)

is the lag vector discussed earlier, which is identical in

Non-Linear Prediction of Acoustic Vectors Using Hierarchical Mixtures of Experts

x

x

x

837

x

Figure 1: The Hierarchical Mixture of Experts.

2.1

Error bars via HME

Since each expert is an AR model, it follows that the output of each expert y(t) is
the expected value of the observations y(t) at each time t. The conditional likelihood
of yet) given the input and expert mj is
P(y(t) Ix (t), mj,

Bj)

= 12:Cj I exp ( - ~ (y -

yy?)T Cj(y - yjt)))

where Cj is the covariance matrix for expert mj which is updated during training as:
C = _1_ "'" h(t)(y(r) _ y(t)l (y(t) _ y~t))
J
' " h~t) L..J J
]
]
L.Jt J

t

where hy) are the posterior probabilities I of each expert mj' Taking the moments of
the overall likelihood of the HME gives the output of the HME as the conditional
expected value of the target output yct),
yet) = E(yct)lxct) , 0, M)

=

2: P(mjlxct), l1j)E(y(t)lx ct), ej,mj) =2: gY)iJ/t),
j

j

Where M represents the overall HME model and
Taking the second central moment of yct) gives,
C

=
=

e

the overall set of parameters.

I

E?y(t) - yy?)2 xct), 0, M)

2: P(mJlx(t), l1j)E?y(t) 2: gjt)(Cj + yjt). iJj(t)T),

yjt))2I x (t), ej, mj)

j

=

j

lSee (Jordan & Jacobs 1994) for a fuller discussion of posterior probabilities and likelihoods in the context of the HME.

838

S. R. Waterhouse, A. J. Robinson

which gives, in a direct fashion, the covariance of the output given the input and the
model. If we assume that the observations are generated by an underlying model,
which generates according to some function f(x(t)) and corrupted by zero mean
normally distributed noise n(x) with constant covariance 1:, then the covariance of
y(t) is given by,
V(y(t)) =V(t o) + 1:,
so that the covariance computed by the method above, V(y(t)) , takes into account
the modelling error as well as the uncertainty due to the noise. Weigend & Nix
(1994) also calculate error bars using an MLP consisting of a set of tanh hidden
units to estimate the conditional mean and an auxiliary set of tanh hidden units
to estimate the variance, assuming normally distributed errors. Our work differs
in that there is no assumption of normality in the error distribution, rather that
the errors of the terminal experts are distributed normally, with the total error
distribution being a mixture of normal distributions.

3

SIMULATIONS

In order to demonstrate the utility of our approach to variance estimation we consider one toy regression problem and one time series prediction problem.
3.1

Toy Problem: Computer generated data
2,------..--....,

0 . 08,------~-...,

1.5

O. OS

N

~

fO.04

~

-0.02

2

x

~-0.5
-1

-0.2

-1.5

-0.4

0.8
Q)

go.S

-2

-O.S

-2.5

-0.8

-3 '------'------'
o
2

x

.~

~0.4

-1 ' - - - - - - ' - - - - - "

o

2

x

x

2

Figure 2: Performance on the toy data set of a 5 level binary HME. (a) training set
(dots) and underlying function f(x) (solid), (b) underlying function (solid) and prediction
y(x) (dashed), (c) squared deviation of prediction from underlying function, (d) true noise
variance (solid) and variance of prediction (dashed).
By way of comparison, we used the same toy problem as Weigend & Nix (1994)
which consists of 1000 training points and 10000 separate evaluation points from

Non-Linear Prediction of Acoustic Vectors Using Hierarchical Mixtures of Experts

839

the function g(x) where g(x) consists of a known underlying function f(x) corrupted
by normally distributed noise N(O, (J2(X)) ,
f(x)

=

sin(2.5x) x sin(l. 5x),

(J2(x)

= 0.01 + O. 25 x [1 -

sin(2. 5x)f.

As can be seen by Figure 2, the HME has learnt to approximate both the underlying
function and the additive noise variance. The deviation of the estimated variance
from the "true" noise variance may be due to the actual noise variance being lower
than the maximum denoted by the solid line at various points.

3.2

Sunspots

1920

1930

1940

1950

1960

1970

1980

Year

8
6
t

8.4
)(

w

2

. .
-.-- . ?
? ?
?

?

.. ? ...

II

?

l-

?

-

?

I ?

-

?

-

..

-

? -? ? ?

?

OL-------~--------~--------~--------L-------~--------~

1920

1930

1940

1950

1960

1970

1980

Year

Figure 3: Performance on the Sunspots data set. (a) Actual Values (x) and predicted
values (0) with error bars. (b) Activation of the expert networks; bars wide in the vertical
axis indicate strong activation. Notice how expert 7 concentrates on the lulls in the series
while expert 2 deals with the peaks.

I METHOD I
MLP
TAR
HME

NMSE '
Train
1700-1920
0.082
0.097
0.061

Test
1921-1955 1956-1979
0.086
0.35
0.097
0.28
0.089
0.27

Table 1: Results of single step prediction on the Sunspots data set using a mixture of 7
experts (104 parameters) and a lag vector of 12 years. NMSE' is the NMSE normalised
by the variance of the entire record 1700 to 1979.

840

S. R. Waterhouse, A. J. Robinson

The Sunspots2 time series consists of yearly sunspot activity from 1700 to 1979 and
was first tackled using connectionist models by Weigend, Huberman & Rumelhart
(1990) who used a 12-8-1 MLP (113 parameters) . Prior to this work, the TAR was
used by Tong (1990). Our results, which were obtained using a random leave 10%
out cross validation method, are shown in Table 1. We are considering only single
step prediction on this problem, which involves prediction of the next value based
on a set of previous values of the time series. Our results are evaluated in terms of
Normalised Mean Squared Error (NMSE) (Weigend et al. 1990), which is defined
as the ratio of the variance of the prediction on the test set to the variance of the
test set itself.
The HME outperforms both the TAR and the MLP on this problem, and additionally provides both information about the structure of the network after training via
the expert activation plot and error bars of the predictions, as shown in Figure 3.
Further improvements may be possible by using likelihood information during cross
validation so that a joint optimisation of overall error and variance is achieved.

4

SPEECH CODING USING HME

In the standard method of Linear Predictive Coding (LPC) (Makhoul 1975), speech
is parametrised into a set of vectors of duration one frame (around 10 ms). Whilst
simple scalar quantization of the LPC vectors can achieve bit rates of around 2400
bits per second (bps), Yong, Davidson & Gersho (1988) have shown that simple
linear prediction of Line Spectral Pairs (LSP) (Soong & Juang 1984) vectors followed
by Vector Quantization (VQ) (Abut , Gray & Rebolledo 1984) of the error vectors
can yield bit rates of around 800 bps. In this paper we describe a speech coding
framework which uses the HME in two stages. Firstly, the HME is used to perform
prediction of the acoustic vectors. The error vectors are then quantized efficiently
by using a VQ scheme which utilises the likelihood information derived from the
HME .
4.1

Mixing VQ codebooks ia Gating networks

In a VQ scheme using a Euclidean distance measure , there is an implicit assumption
that the inputs follow a Gaussian probability density function (pdf). This is satisfied
if we quantize the residuals from a linear predictor , but not the residuals from an
HME which follow a mixture of Gaussians pdf. A more efficient method is therefore
to generate separate VQ code books for each expert in the HME and combine them
via the priors on each expert from the gating networks. The code book for the
overall residual vectors on the test set is then generated at each time dynamically
by choosing the first D x gjt) codes, where D is the size of the expert codebooks and
gY) is the prior on each expert.

2 Available via anonymous
DataSunspots.Yearly

ftp at

fip.cs.colorado.edu

III

jpub jTime-Series as

Non-Linear Prediction of Acoustic Vectors Using Hierarchical Mixtures of Experts

4.2

841

Results of Speech Coding Evaluations

Initial experiments were performed using 23 Mel scale log energy frequency bins as
acoustic vectors and using single variances Cj = (J;I as expert network covariance
matrices. The results of training over 100 ,000 frames and evaluation over a further
100 ,000 frames on the Resource Management (RM) corpus are shown in Table 2
and Figure 4 which shows the good specialisation of the HME in this problem.
METHOD

Prediction
Train
12.07
18.1
20.20

Linear
1 level HME
2 level HME

Gain (dB)
Test
10.95
15.55
16.39

Table 2: Prediction of Acoustic Vectors using linear prediction and binary branching
HMEs with 1 and 2 levels. Prediction gain (Cuperman & Gersho 1985) is the ratio of the
signal variance to prediction error variance.
(a) Spectogram

20
!!!

::3

c;; 15

Cf
~
10
::3

8

<

.

5

o

10

20

30

40

50

60

70

80

90

100

Frame
(b) Gating Network Decisions

71---

-

-

-

I

- -__.-

I

6~----. . . .~.---~--_ _--------_=--~~--?

~5~~---------1~---.I---1IHI~I~?--. . . .~.~.----~4~~~----------------~~~~----------

w3~~---------~-----. .~__------_.----------------- -

2.-.---- - -- - - -11-1...---11---1- --11-1 ? ? I

II
. ....-

I ?

1~--------------~~~~--------------------------------~

10

20

30

40

50

60

70

80

90

100

Frame

Figure 4: The behaviour of a mixture of 7 experts at predicting Mel-scale log energy
frequency bins over 100 16ms fram es. The top figure is a spectrogram of the speech and
the lower figure is an expert activation plot, showmg the gating network decisions.
We have conducted further experiments using LSPs and cepstrals as acoustic vectors , and using diagonal expert network covariance matrices, on a very large speech
corpus. However, initial experiments show only a small improvement in gain over
a single linear predictor and further investigation is underway. We have also coded
acoustic vectors using 8 bits per frame with frame lengths of 12.5 ms, passing power,
pitch and degree of voicing as side band information , without appreciable distortion over simple LPC coding. A full system will include prediction of all acoustic

842

S. R. Waterhouse, A. J. Robinson

parameters and we anticipate further reductions on this initial figure with future
developments.

5

CONCLUSION

The aim of speech coding is the efficient coding of the speech signal with little
perceptual loss. This paper has described the use of the HME for acoustic vector
prediction. We have shown that the HME can provide improved performance over a
linear predictor and in addition it provides a time varying variance for the prediction
error. The decomposition of the linear prediction problem into a solution via a
mixture of experts also allows us to construct a VQ codebook on the fly by mixing
the codebooks of the various experts.
We expect that the direct computation of the time varying nature of the prediction
accuracy will find many applications. Within the acoustic vector prediction problem
we would like to exploit this information by exploring the continuum between the
fixed bit rate coder described here and a variable bit rate coder that produces
constant spectral distortion.
Acknowledgements
This work was funded in part by Hewlett Packard Laboratories, UK. Steve Waterhouse is supported by an EPSRC Research Students hip and Tony Robinson was
supported by a EPSRC Advanced Research Fellowship.

References
Abut, H., Gray, R. M. & Rebolledo, G. (1984), 'Vector quantization of speech and speechlike waveforms', IEEE Transactions on Acoustics, Speech, and Signal Processing.
Cuperman, V. & Gersho, A. (1985), 'Vector predictive coding of speech at 16 kblt/s',
IEEE Transactions on Communications COM-33, 685-696.
Jordan, M. I. & Jacobs, R. A. (1994), 'Hierarchical Mixtures of Experts and the EM
algorithm', Neural Computation 6, 181-214.
Makhoiil, J. (1975), 'Linear prediction: A tutorial review', Proceedings of the IEEE
63(4) 561-580.
Soong~ F. k. & Juang, B. H. (1984), Line spectrum pair (LSP) and speech data compressIon.
Tong, H. (1990), Non-linear Time Series: a dynamical systems approach, Oxford Universi~y Press.
Tong, H. & Lim, K. (1980), 'Threshold autoregression, limit cycles and cyclical data',
Journal of Royal Statistical Society.
Waterhouse, S. R. & Robinson, A. J. (1994), Classification using hierarchical mixtures of
experts, in 'IEEE Workshop on Neural Networks for SigI!al Processing'.
Weigend, A. S. & Gershenfeld, N. A. (1994), Time Series Prediction: Forecasting the
Future and Understanding the Past) Addison-Wesley.
Weigend, A. S. & Nix, D. A. (1994), Predictions with confidence intervals (local error bars),
Technical Report CU-CS-724-94, Department of Computer Science and Institute of
Coznitive Science, University of Colorado, Boulder, CO 80309-0439.
Weigend, A. S., Huberman, B. A. & Rumelhart, D. E. (1990), 'Predicting the future: a
connectionist approach', International Journal of Neural Systems 1, 193-209.
Yong, M., Davidson, G. & Gersho, A. (1988), Encoding of LPC spectral parameters using
switched-adaptive interframe vector prediction, in 'Proceedings of the IEEE International Conference on Acoustics Speech, and Signal Processing', pp. 402-405.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 5301-dynamic-rank-factor-model-for-text-streams.pdf

Dynamic Rank Factor Model for Text Streams

Shaobo Han?, Lin Du?, Esther Salazar and Lawrence Carin
Duke University, Durham, NC 27708
{shaobo.han, lin.du, esther.salazar, lcarin}@duke.edu

Abstract
We propose a semi-parametric and dynamic rank factor model for topic modeling, capable of (i) discovering topic prevalence over time, and (ii) learning contemporary multi-scale dependence structures, providing topic and word correlations as a byproduct. The high-dimensional and time-evolving ordinal/rank observations (such as word counts), after an arbitrary monotone transformation, are
well accommodated through an underlying dynamic sparse factor model. The
framework naturally admits heavy-tailed innovations, capable of inferring abrupt
temporal jumps in the importance of topics. Posterior inference is performed
through straightforward Gibbs sampling, based on the forward-filtering backwardsampling algorithm. Moreover, an efficient data subsampling scheme is leveraged
to speed up inference on massive datasets. The modeling framework is illustrated
on two real datasets: the US State of the Union Address and the JSTOR collection
from Science.

1

Introduction

Multivariate longitudinal ordinal/count data arise in many areas, including economics, opinion polls,
text mining, and social science research. Due to the lack of discrete multivariate distributions supporting a rich enough correlation structure, one popular choice in modeling correlated categorical
data employs the multivariate normal mixture of independent exponential family distributions, after
appropriate transformations. Examples include the logistic-normal model for compositional data
[1], the Poisson log-normal model for correlated count data [2], and the ordered probit model for
multivariate ordinal data [3]. Moreover, a dynamic Bayesian extension of the generalized linear
model [4] may be considered, for capturing the temporal dependencies of non-Gaussian data (such
as ordinal data). In this general framework, the observations are assumed to follow an exponential family distribution, with natural parameter related to a conditionally Gaussian dynamic model
[5], via a nonlinear transformation. However, these model specifications may still be too restrictive
in practice, for the following reasons: (i) Observations are usually discrete, non-negative and with
a massive number of zero values and, unfortunately, far from any standard parametric distributions
(e.g., multinomial, Poisson, negative binomial and even their zero-inflated variants). (ii) The number
of contemporaneous series can be large, bringing difficulties in sharing/learning statistical strength
and in performing efficient computations. (iii) The linear state evolution is not truly manifested after
a nonlinear transformation, where positive shocks (such as outliers and jumps) are magnified and
negative shocks are suppressed; hence, handling temporal jumps (up and down) is a challenge for
the above models.
We present a flexible semi-parametric Bayesian model, termed dynamic rank factor model (DRFM),
that does not suffer these drawbacks. We first reduce the effect of model misspecification by modeling the sampling distribution non-parametrically. To do so, we fit the observed data only after
some implicit monotone transformation, learned automatically via the extended rank likelihood [6].
Second, instead of treating panels of time series as independent collections of variables, we analyze
them jointly, with the high-dimensional cross-sectional dependencies estimated via a latent factor
?

contributed equally

1

model. Finally, by avoiding nonlinear transformations, both smooth transitions and sudden changes
(?jumps?) are better preserved in the state-space model, using heavy-tailed innovations.
The proposed model offers an alternative to both dynamic and correlated topic models [7, 8, 9],
with additional modeling facility of word dependencies, and improved ability to handle jumps. It
also provides a semi-parametric Bayesian treatment of dynamic sparse factor model. Further, our
proposed framework is applicable in the analysis of multiple ordinal time series, where the innovations follow either stationary Gaussian or heavy-tailed distributions.

2

Dynamic Rank Factor Model

We perform analysis of multivariate ordinal time series. In the most general sense, such ordinal
variables indicate a ranking of responses in the sample space, rather than a cardinal measure [10].
Examples include real continuous variables, discrete ordered variables with or without numerical
scales or, more specially, counts, which can be viewed as discrete variables with integer numeric
scales. Our goal is twofold: (i) discover the common trends that govern variations in observations,
and (ii) extract interpretable patterns from the cross-sectional dependencies.
Dependencies among multivariate non-normal variables may be induced through normally distributed latent variables. Suppose we have P ordinal-valued time series yp,t , p = 1, . . . , P ,
t = 1, . . . , T . The general framework contains three components:
yp,t ? g(zp,t ), zp,t ? p(? t ), ? t ? q(? t?1 ),
(1)
where g(?) is the sampling distribution, or marginal likelihood for the observations, the latent variable zp,t is modeled by p(?) (assumed to be Gaussian) with underlying system parameters ? t , and
q(?) is the system equation representing Markovian dynamics for the time-evolving parameter ? t .
In order to gain more model flexibility and robustness against misspecification, we propose a semiparametric Bayesian dynamic factor model for multiple ordinal time series analysis. The model is
based on the extended rank likelihood [6], allowing the transformation from the latent conditionally
Gaussian dynamic model to the multivariate observations, treated non-parametrically.
Extended rank likelihood (ERL): There exist many approaches for dealing with ordinal data, however, they all have some restrictions. For continuous variables, the underlying normality assumption
could be easily violated without a carefully chosen deterministic transformation. For discrete ordinal variables, an ordered probit model, with cut points, becomes computationally expensive if the
number of categories is large. For count variables, a multinomial model requires finite support on
the integer values. Poisson and negative binomial models lack flexibility from a practical viewpoint,
and often lead to non-conjugacy when employing log-normal priors.
Being aware of these issues, a natural candidate for consideration is the ERL [6]. With appropriate
monotone transformations learned automatically from data, it offers a unified framework for handling both continuous [11] and discrete ordinal variables. The ERL depends only on the ranks of the
observations (zero values in observations are further restricted to have negative latent variables),
zp,t ? D(Y ) ? {z p,t ? R : yp,t < yp0 ,t0 ? zp,t < zp0 ,t0 , and zp,t ? 0 if yp,t = 0}.
(2)
In particular, this offers a distribution-free approach, with relaxed assumptions compared to parametric models, such as Poisson log-normal [12]. It also avoids the burden of computing nuisance
parameters in the ordered probit model (cut points). The ERL has been utilized in Bayesian Gaussian
copula modeling, to characterize the dependence of mixed data [6]. In [13] a low-rank decomposition of the covariance matrix is further employed and efficient posterior sampling is developed in
[14]. The proposed work herein can be viewed as a dynamic extension of that framework.
2.1

Latent sparse dynamic factor model

In the forthcoming text, G(?, ?) denotes a gamma distribution with shape parameter ? and rate
parameter ?, TN(l,u) (?, ? 2 ) denotes a univariate truncated normal distribution within the interval
(l, u), and N+ (0, ? 2 ) is the half-normal distribution that only has non-negative support.
Assume z t ? N (0, ?t ), where ?t is usually a high-dimensional (P ? P ) covariance matrix.
To reduce the number of parameters, we assume a low rank factor model decomposition of the
covariance matrix ?t = ?V t ?T + R such that
z t = ?st + t , t ? N (0, R), R = I P .
(3)
2

Common trends (importance of topics) are captured by a low-dimensional factor score parameter
st . We assume autoregressive dynamics on sk,t ? AR(1|(?k , ?k,t )) with heavy-tailed innovations,
sk,t = ?k sk,t?1 + ?k,t , 0 < ?k < 1, ?k,t ? TPBN(e, f, ?), ? 1/2 ? C + (0, h),
(4)
where ?k,t follows the three-parameter beta mixture of normal TPBN(e, f, ?) distribution [15]. Parameter e controls the peak around zero, f controls the heaviness on the tails, and ? controls the
global sparsity with a half-Cauchy prior [16]. This prior encourages smooth transitions in general,
while jumps are captured by the heavy tails. The conjugate hierarchy may be equivalently represented as
?k,t ? N (0, ?k,t ), ?k,t ? G(e, ?k,t ), ?k,t ? G(f, ?) ? ? G(1/2, ?), ? ? G(1/2, h2 ).
Truncated normal priors are employed on ?k , ?k ? TN(0,1) (?0 , ?02 ), and assume s0,k ? N (0, ?s2 ).
Note that the extended rank likelihood is scale-free; therefore, we do not need to include a redundant
intercept parameter in (3). For the same reason, we set R = I P .
Model identifiability issues: Although
p the covariance matrix ?t is not identifiable [10], the related
correlation matrix C t = ?[i,j],t / ?[i,i],t ?[j,j],t , (i, j = 1, . . . , P ) may be identified, using the
parameter expansion technique [3, 13]. Further, the rank K in the low-rank decomposition of ?t is
also not unique. For the purpose of brevity, we do not explore this uncertainty here, but the tools
developed in the Bayesian factor analysis literature [17, 18, 19] can be easily adopted.
Identifiability is a key concern for factor analysis. Conventionally, for fixed K, a full-rank, lowertriangular structure in ? ensures identifiability [20]. Unfortunately, this assumption depends on the
ordering of variables. As a solution, we add nonnegative and sparseness constraints on the factor
loadings, to alleviate the inherit ambiguity, while also improving interpretability. Also, we add a
Procrustes post-processing step [21] on the posterior samples, to reduce this indeterminacy.
The nonnegative and (near) sparseness constraints are imposed by the following hierarchy,
1/2

?p,k ? N+ (0, lp,k ) lp,k ? G(a, up,k ), up,k ? G(b, ?k ), ?k ? C + (0, d).
(5)
Integrating out lp,k and up,k , we obtain a half-TPBN prior ?p,k ? TPBN+ (a, b, ?k ). The columnwise shrinkage parameters ?k enable factors to be of different sparsity levels [22]. We set hyperparameters a = b = e = f = 0.5, d = P , h = 1, ?s2 = 1. For weakly informative priors, we set
? = ? = 0.01; ?0 = 0.5, ?02 = 10.
2.2

Extension to handle multiple documents

nt
t
At each time point t we may have a corpus of documents {y nt t }N
nt =1 , where y t is a P -dimensional
observation vector, and Nt denotes the number of documents at time t. The model presented in
Section 2.1 is readily extended to handle this situation. Specifically, at each time point t, for each
nt
, is
document nt , the ERL representation for word count p, denoted by yp,t

nt
nt
yp,t
= g zp,t
, p = 1, . . . , P, t = 1, . . . , T, nt = 1, . . . , Nt ,

where z nt t ? RP and P is the vocabulary size. We assume a latent factor model for z nt t such that
z nt t = ?bnt t + nt t , nt t ? N (0, I P ), bnt t ? N (st , ?), ? = diag(?), ?k?1 ? G(?, ?),
?K
where ? ? RP
is the topic-word loading matrix, representing the K topics as columns of ?.
+
The factor score vector bnt t ? RK is the topic usage for each document y nt t , corresponding to locations in a low-dimensional RK space. The other parts of the model remain unchanged. The latent
trajectory s1:T represents the common trends for the K topics. Moreover, through the forward filtering backward sampling (FFBS) algorithm [23, 24], we also obtain time-evolving topic correlation
matrices ?t ? RK?K and word dependencies matrices C t ? RP ?P , offering a multi-scale graph
representation, a useful tool for document visualization.

2.3

Comparison with admixture topic models

Many topic models are unified in the admixture framework [25],

!
K

X

P (y n |w, ?) = P y n ?n =
wk,n ?k ,
Base

Admix

(6)

k=1

where y n is the P -dimensional observation vector of word counts in the n th document, and P denotes the vocabulary size. Traditionally, y n is generated from an admixture of base distributions, wn
is the admixture weight (topic proportion for document n), and ?k is the canonical parameter (word
3

distribution for topic k), which denotes the location of the kth topic on the P -1 dimensional simplex.
For example, latent Dirichlet allocation (LDA) [26] assumes the base distribution to be multinomial,
with ?k ? Dir(?0 ), wn ? Dir(? 0 ). The correlated topic model (CTM) [8] modifies the topic distribution, with wn ? Logistic Normal(?, ?). The dynamic topic model (DTM) [7] analyzes document collections in a known chronological order. In order to incorporate the state space model, both
the topic proportion and the word distribution are changed to logistic normal, with isotropic covariance matrices wt ? Logistic Normal(wt?1 , ? 2 I K ) and ?k,t ? Logistic Normal(?k,t?1 , vI P ),
respectively. To overcome the drawbacks of multinomial base, spherical topic models [27] assume
the von Mises-Fisher (vMF) distribution as its base distribution, with ?k ? vMF(?, ?) lying on a
unit P -1 dimensional sphere. Recently in [25] the base and word distribution are both replaced with
Poisson Markov random fields (MRFs), which characterizes word dependencies.
We present here a semi-parametric factor model formulation,

!
K

X

P(y n |s, ?) , P z n ? D(Y ) ?n =
sk,n ?k ,


(7)

k=1

with y n defined as above, ?k ? RP
+ is a vector of nonnegative weights, indicating the P vocabulary usage in each individual topics k, and sn ? RK is the topic usage. Note that the extended
rank likelihood does not depend on any assumptions about the data marginal distribution, making it
appropriate for a broad class of ordinal-valued observations, e.g., term frequency-inverse document
frequency (tf-idf) or rankings, beyond word counts. However, the proposed model here is not an
admixture model, as the topic usage is allowed to be either positive or negative.
The DRFM framework has some appealing advantages: (i) It is more natural and convenient to incorporate with sparsity, rank selection, and state-space model; (ii) it provides topic-correlations and
word-dependences as a byproduct; and (iii) computationally, this model is tractable and often leads
to locally conjugate posterior inference. DRFM has limitations. Since the marginal distributions
are of unspecified types, objective criteria (e.g. perplexity) is not directly computable. This makes
quantitative comparisons to other parametric baselines developed in the literature very difficult.

3

Conjugate Posterior Inference

Let ? = {?, S, L, U , ?, ?, ?, ? , ?, ?, ?} denote the set of parameters in basic model, and let Z be
the augmented data (from the ERL). We use Gibbs sampling to approximate the joint posterior distribution p(Z, ?|Z ? R(Y )). The algorithm alternates between sampling p(Z|?, Z ? R(Y )) and
p(?|Z, Z ? R(Y )) (reduced to p(?|Z)). The derivation of the Gibbs sampler is straightforward,
and for brevity here we only highlight the sampling steps for Z, and the forward filtering backward
sampling (FFBS) steps for the trajectory s1:T . The Supplementary Material contains further details
for the inference.
PK
? Sampling zp,t : p(zp,t |?, Z ? R(Y ), Z ?p,?t ) ? TN[zp,t ,zp,t ] ( k=1 ?p,k sk,t , 1), where zp,t =
max{zp0 ,t0 : yp0 ,t0 < yp,t } and zp,t = min{zp0 ,t0 : yp0 ,t0 > yp,t }.
This conditional sampling scheme is widely used in [6, 10, 13]. In [14] a novel Hamiltonian Monte
Carlo (HMC) approach has been developed recently, for a Gaussian copula extended rank likelihood
model, where ranking is only within each row of Z. This method simultaneously samples a column
vector of z i conditioned on other columns Z ?i , with higher computation but better mixing.
? Sampling st : we have the state model st |st?1 ? N (Ast?1 , Qt ), and the observation model
z t |st ? N (?st , R),1 where A = diag(?), Qt = diag(? t ), R = I P . for t = 1, . . . , T
1. Forward Filtering: beginning at t = 0 with s0 ? N (0, ?s2 I K ), for all t = 1, . . . , T , we
find the on-line posteriors at t, p(st |z 1:t ) = N (mt , V t ), where mt = V t {?T R?1 z t +
?1
T ?1
H ?1
?]?1 , and H t = Qt + AV t?1 AT .
t Amt?1 }, V t = [H t + ? R
2. Backward Sampling: starting from N (f
mt , Ve t ), the backward smoothing density, i.e., the
e t?1 ), where
conditional distribution of st?1 given st , is p(st?1 |st , z 1:(t?1) ) = N (e
?t?1 , ?
T ?1
?1
?1
T ?1
?1
e
e
e t?1 = ?t?1 {A Qt st + V t?1 mt?1 }, ?t?1 = (V t?1 + A Qt A) .
?
There exist different variants of FFBS schemes (see [28] for a detailed comparison); the method we
choose here enjoys fast decay in autocorrelation and reduced computation time.
1

For brevity, we omit the dependencies on ? in notation

4

3.1

Time-evolving topic and word dependencies

We also have the backward recursion density at t ? 1, p(st?1 |z 1:T ) = N (f
mt?1 , Ve t?1 ), where
T ?1
?1
T ?1 e
e
e
e
e
e
ft?1 = ?t?1 (A Qt m
ft + V t?1 mt?1 ) and V t?1 = ?t?1 + ?t?1 A Qt V t Q?1
m
t A?t?1 .
We perform inference on the K ? K time-evolving topic dependences in s1:T , using the posterior
p
covariances {Ve 1:T } (with topic correlation matrices ?1:T , ?[r,s],t = V[r,s],t / V[r,r],t V[s,s],t , r, s =
1, . . . , K), and further obtain the P ? P time-evolving word dependencies capsuled in {?1:T }
with ?t = ?Ve t ?T + I P . Essentially, this can be viewed as a dynamic Gaussian copula model,
et ? N (0, C t ), where g(?) is a non-decreasing function of a univariate marginal
yp,t = g(e
zp,t ), z
likelihood and C t (t = 1, . . . , T ) is the correlation matrix capturing the multivariate dependence.
We obtain a posterior distribution for C 1:T as a byproduct, without having to estimate the nuisance
parameters in marginal likelihoods g(?). This decoupling strategy resembles the idea of copula
models.
3.2

Accelerated MCMC via document subsampling

For large-scale datasets, recent approaches efficiently reduce the computational load of Monte Carlo
Markov chain (MCMC) by data subsampling [29, 30]. We borrow this idea of subsampling documents when considering a large corpora (e.g., in our experiments, we consider analysis of articles
in the magazine Science, composed of 139379 articles from years 1880 to 2002, and a vocabulary
size 5855). In our model, the augmented data z nt t (nt = 1, . . . , Nt ) for each document is relatively
expensive to sample. One simple method is random document sampling without replacement. However, by treating all likelihood contributions symmetrically, this method leads to a highly inefficient
MCMC chain with poor mixing [29].
Alternatively, we adopt the probability proportional-to-size (PSS) sampling scheme in [30], i.e.,
sampling the documents with inclusion probability proportional to the likelihood contributions. For
each MCMC iteration, the sub-sampling procedure for documents at time t is designed as follows:
? Step 1: Given a small subset Vt ? {1, . . . , Nt } of chosen documents, only sample {z dt } for all
d ? Vt and compute the augment log-likelihood contributions (with B t integrated out) `Vt (z dt ) =
e where R
e = ???T + I P . Note that, only a K-dimensional matrix inversion is
N (?st , R),
e ?1 = I P ? ?(??1 + ?T ?)T ?T .
required, by using the Woodbury matrix inversion formula R
? Step 2: Similar to [30], we use a Gaussian process [31] to predict the log-likelihood for
the remaining documents `Vtc (z dt ) = K(Vtc , Vt )K(Vt , Vt )?1 `Vt (z dt ), where K is a Nt ?
j
i
Nt squared-exponential
kernel,
 which denotes the similarity of documents: K(y t , y t ) =

?f2 exp ?||y it ? y jt ||2 /(2s2 ) , i, j = 1, . . . , Nt , ?f2 = 1, s = 1.

P
ed = wd / d0 wd0 .
? Step 3: Calculate the inclusion probability wd ? exp [`(z dt )], d = 1, . . . , Nt , w
? Step 4: Sampling the next subset Vt of pre-specified size |Vt | with inclusion probability w
ed , and
store it for the use of the next MCMC iteration.
In practice, this adaptive design allows MCMC to run more efficiently on a full dataset of large
scale, often mitigating the need to do parallel MCMC implementation. Future work could also consider nonparametric function estimation subject to monotonicity constraint, e.g. Gaussian process
projections recently developed in [32].

4

Experiments

Different from DTM [7] , the proposed model has the jumps directly at the level of the factor scores
(no exponentiation or normalization needed), and therefore it proved more effective in uncovering
jumps in factor scores over time. Demonstrations of this phenomenon in a synthetic experiment are
detailed in the Supplementary Material. In the following, we present exploratory data analysis on
two real examples, demonstrating the ability of the proposed model to infer temporal jumps in topic
importance, and to infer correlations across topics and words.
4.1

Case Study I: State of the Union dataset

The State of the Union dataset contains the transcripts of T = 225 US State of the Union addresses,
from 1790 to 2014. We take each transcript as a document, i.e., we have one document per year.
5

After removing stop words, and removing terms that occur fewer than 3 times in one document and
less than 10 times overall, we have P = 7518 unique words. The observation yp,t corresponds to
the frequency of word p of the State of the Union transcript from year t.
We apply the proposed DRFM setting and learned K = 25 topics. To better understand the temporal
dynamic per topic, six topics are selected and the posterior mean of their latent trajectories sk,1:T
are shown in Figure 1 (with also the top 12 most probable words associated with each of the topics).
A complete table with all 25 learned topics and top 12 words is provided in the Supplementary
Material. The learned trajectory associated with every topic indicates different temporal patterns
across all the topics. Clearly, we can identify jumps associated with some key historical events. For
instance, for Topic 10, we observe a positive jump in 1846 associated with the Mexican-American
war. Topic 13 is related with the Spanish-American war of 1898, with a positive jump in that year.
In Topic 24, we observe a positive jump in 1914, when the Panama Canal was officially opened
(words Panana and canal are included). In Topic 18, the positive jumps observed from 1997 to
1999 seem to be associated with the creation of the State Children?s Health Insurance Program in
1997. We note that the words for this topic are explicitly related with this issue. Topic 25 appears to
be related to banking; the significant spike around 1836 appears to correspond to the Second Bank
of the United States, which was allowed to go out of existence, and end national banking that year.
In 1863 Congress passed the National Banking Act, which ended the ?free-banking? period from
1836-1863; note the spike around 1863 in Topic 25.
Topic 10

4

4
Topic 17
2

2
0

0

6

Topic 13

4

6

Topic 18

4

2

2

0

0
10

6
Topic 24

4

0

0

-5

1800

1850

Topic#10
Mexico
Government
Texas
United
War
Mexican
Army
Territory
Country
Peace
Policy
Lands

1900

Topic#13
Government
United
Islands
Commission
Island
Cuba
Spain
Act
General
Military
International
Officiers

1950

Topic 25

5

2

1800

2000

Topic#24
United
Treaty
Isthmus
Public
Panama
Law
Territory
America
Canal
Service
Banks
Colombia

Topic#17
Jobs
Country
Tax
American
Economy
Deficit
Americans
Energy
Businesses
Health
Plan
Care

1850

1900

Topic#18
Children
America
Americans
Care
Tonight
Support
Century
Health
Working
Challenge
Security
Families

1950

2000

Topic#25
Government
Public
Banks
Bank
Currency
Money
United
Federal
American
National
Duty
Institutions

Figure 1: (State of the Union dataset) Above: Time evolving from 1790 to 2014 for six selected
topics. The plotted values represent the posterior means. Below: Top 12 most probable words
associated with the above topics.
Our modeling framework is able to capture dynamic patterns of topics and word correlations. To
illustrate this, we select three years (associated with some meaningful historical events) and analyze
their corresponding topic and word correlations. Figure 2 (first row) shows graphs of the topic
correlation matrices, in which the nodes represent topics and the edges indicate positive (green) and
negative (red) correlations (we show correlations with absolute value larger than 0.01). We notice
that Topics 11 and 22 are positively correlated with those years. Some of the most probable words
associated with each of them are: increase, united, law and legislation (for Topic 11) and war,
Mexico, peace, army, enemy and military (for Topic 22). We also are interested in understanding
the time-varying correlation between words. To do so, and for the same years as before, in Figure 2
(second row) we plot the dendrogram associated with the learned correlation matrix for words. In
the plots, different colors indicate highly correlated word clusters defined by cutting the branches off
the dendrogram. Those figures reveal different sets of highly correlated words for different years. By
6

1846
Mexican-American War

T6

T5
T2
T22

T11

T5

T14

T13

T7

T14
T18

T18

T20

T12

T10

T25

T9
T18
T21

T19

T16

T10

T23

T21

T21

T1

tre
ex cu b asu
pe rr an r
nd en k y
itu cys
re
dofisca s
milllliars l
o
n
b
illion
constitut
ion
union
president

n
tio om
nareeed s
f re ion
f at rgy
nene
nt
lth
heeavelopmse
d ogram
pr
ic
nom
eco
program
country
general
pow
er
auth
pub ority
gfoederlic
v al
cict onv ernm
ize en e
ns tio nt
n

1.0

0.5

0.0

?0.5

?1.0

n
ai
ry
sp ct ritoy
ater at
tre ited l
n
unationaistratio
n dmin
a olicy
p bject
su sent
pre

r
n
we ns tio
po itizneven
cco ain
sp
y
actrritor
teeaty
tr ited
unexican
m s
texa

mi pea
for litar ce
ce y
arm
ys
islacnuba
islandds
mexico
texas
mexican
treasury
s
bank
ncy
curreident
prespolicyn
t
a io al
istr tion on
min natitutinion
ad
s
n u
co

pr
am subese
n
foer ricaject t
eig n
t
dep orrdaden
ar tm er
e
canant
cour l
americat
americans

service
increase
nu
ju mber
ta ne
carx
job e
t ch s
amoniigldre
am
h n
erericat
ica n
s

om
ed
frereetions
f a rt
n ou l
c ana r tment
c epa
d er
ordde
tra ign
fore
american
country
general
autho
po
ri
pubwer ty
gfoederlic
v al
cocitiz ernm
nv ens e
nt
en
tio
n

n
ai
ry
sp ctrritoy
ate at
tre ited
unx
taare
c bs
jo ren
childght
toni

pm
enhea ent
erglth
w
pe a y
milit acer
a
forcery
armys
islands
cuba
island
mexxicaos
te an
ic
mex gold
tes
noilvers
s ndve
boser
re

secretary
n
attentpioort
re ne
ju er
b se
num
rea aw
inc l essor r
sin ab a
bu l w

servicey
secrettiaron
atterneporet
jun er
mb e
nu rearsamic
incrog omms
p n ra
og
ec ro
p

secretary
n
attentpio
rt
re o
ent
pres ject
sub laws
es r
sin bo al
bu lation ion
t
na na

de
ve
lo

om
ed
frereetions
f a rt
n ou l
c ana r tment
c epa
d er
ordde
tra ign
fore
american
country
general
autho
pub rity
li
fe
g de c
p ov ral
u re ern
co nio side men
ns n nt t
tit
ut
io
n

T1

T9

T24

nation
labor
bu
lawsiness
ta
c x
jobare
tocnhilds
a
i
m
am e ghren
er ric t
ic an
a s

T1

m
bil illio
p wlio n
mil eacearn
forcitary
armes
island y
cubas
island
mexico

T23
T12

T2
T16

n go
silotesld
v
resbonder
trea erves
sury
b
anks
curren
cy
policy
administration
ent
pm
develo ra
m
og
pr
omic
ecoongramsh
pr earltgy
h e
en turesal
i c
nd fis llars
pe
do
ex

T16

T15

T15

T15

T24

T9

T20

T17

T23
T8

T11
T7

T20
T4

T22
T2

T24

T25

T11

T22

T14

T3
T17

T10

T12

T4

T8

T19

T3

T8
T3

T5

T13

T13
T6

T6
T4

T19

T17

2003
Iraq War

service
billion
fis
ex cal
millipenditure
s
o
d
o
r lla n
b es rs
si on erve
gonotlverds
e
ld s

T25 T7

1929
Economic Depression

Figure 2: (State of the Union dataset) First row: Inferred correlations between topics for some
specific years associated with some meaningful historical events. Green edges indicate positive
correlations and red edges indicate negative correlations. Second row: Learned dendrogram based
upon the correlation matrix between the top 10 words associated with each topic (we display 80
unique words in total).
inspecting all the words correlation, we noticed that the set of words {government, federal, public,
power, authority, general, country} are highly correlated across the whole period.
4.2

Case Study II: Analysis of Science dataset

We analyze a collection of scientific documents from the JSTOR Science journal [7]. This dataset
contains a collection of 139379 documents from 1880 to 2002 (T = 123), with approximately 1100
documents per year. After removing terms that occurred fewer than 25 times, the total vocabulary
size is P = 5855. We learn K = 50 topics from the inferred posterior distribution, for brevity and
simplicity, we only show 20 of them. We handle about 2700 documents per iteration (subsampling
rate: 2%). Table 1 shows the 20 selected topics and the top 10 most probable words associated with
each of them. By inspection, we notice that those topics are related with specific fields in science.
For instance, Topic 2 is more related to ?scientific research?, Topic 10 to ?natural resources?, and
Topic 15 to ?genetics?. Figure 3 shows the time-varying trend for some specific words, zbp,1:T , which
reveals the importance of those words across time. Finally, Figure 4 shows the correlation between
the selected 20 topics. For instance, in 1950 and 2000, topic 9 (related to mouse, cells, human,
transgenic) and topic 17 (related to virus, rna, tumor, infection) are highly correlated.
DNA
RNA
Gene

2.5
2

1

Cancer
Patients
Nuclear

0.7
0.6

0.8

1.5

Astronomy
Psychology
Brain

0.5
0.6

1

0.4

0.5

0.4

0

0.2

0.3
0.2

?0.5

0.1

0
?1
1880

1900

1920

1940

1960

1980

2000

1880

1900

1920

1940

1960

1980

2000

0
1880

1900

1920

1940

1960

1980

2000

Figure 3: (Science dataset) the inferred latent trend for variable zbp,1:T associated with words.

7

1900

1950

T14

T14
T8

T15

T12

T9

2000
T11

T11

T13
T20

T19

T18

T20

T17

T19

T20

T13

T3

T7

T15

T16
T15

T17

T2

T6

T5

T9
T4
T10

T16

T10

T5

T18

T3

T10
T4

T19

T12

T8

T9

T1

T1

T17

T6

T5

T2
T4

T2
T11

T16
T13

T12

T7

T3

T6

T14

T18

T8

T1
T7

1.0

0.5

0.0

?0.5

?1.0

Figure 4: (Science dataset) Inferred correlations between topics for some specific years. Green
edges indicate positive correlations and red edges indicate negative correlations.

Table 1: Selected 20 topics associated with the analysis of the Science dataset and top 10 most
probable words.
Topic#1
cells
cell
normal
two
growth
development
tissue
body
egg
blood
Topic#11
system
nuclear
new
systems
power
cost
computer
fuel
coal
plant

5

Topic#2
research
national
government
support
federal
development
new
program
scientific
basic
Topic#12
energy
theory
temperature
radiation
atoms
surface
atomic
mass
atom
time

Topic#3
field
magnetic
solar
energy
spin
state
electron
quantum
temperature
current
Topic#13
association
science
meeting
university
american
society
section
president
committee
secretary

Topic#4
animals
brain
neurons
activity
response
rats
control
fig
effects
days
Topic#14
protein
proteins
cell
membrane
amino
sequence
binding
acid
residues
sequences

Topic#5
energy
oil
percent
production
fuel
total
growth
states
electricity
coal
Topic#15
human
genome
sequence
chromosome
gene
genes
map
data
sequences
genetic

Topic#6
university
professor
college
president
department
research
institute
director
society
school
Topic#16
professor
university
society
department
college
president
director
american
appointed
medical

Topic#7
science
scientific
new
scientists
human
men
sciences
knowledge
meeting
work
Topic#17
virus
rna
viruses
particles
tumor
mice
disease
viral
human
infection

Topic#8
work
research
scientific
laboratory
made
university
results
science
survey
department
Topic#18
energy
electron
state
fig
two
structure
reaction
laser
high
temperature

Topic#9
mice
mouse
type
wild
fig
cells
human
transgenic
animals
mutant
Topic#19
stars
mass
star
temperature
solar
gas
data
density
surface
galaxies

Topic#10
water
surface
temperature
soil
pressure
sea
plants
solution
plant
air
Topic#20
rna
fig
mrna
protein
site
sequence
splicing
synthesis
trna
rnas

Discussion

We have proposed a DRFM framework that could be applied to a broad class of applications such
as: (i) dynamic topic model for the analysis of time-stamped document collections; (ii) joint analysis of multiple time series, with ordinal valued observations; and (iii) multivariate ordinal dynamic
factor analysis or dynamic copula analysis for mixed type of data. The proposed model is a semiparametric methodology, which offers modeling flexibilities and reduces the effect of model misspecification. However, as the marginal likelihood is distribution-free, we could not calculate the
model evidence or other evaluation metrics based on it (e.g. held-out likelihood). As a consequence,
we are lack of objective evaluation criteria, which allow us to perform formal model comparisons.
In our proposed setting, we are able to perform either retrospective analysis or multi-step ahead
forecasting (using the recursive equations derived in the FFBS algorithm). Finally, our inference
framework is easily adaptable for using sequential Monte Carlo (SMC) methods [33] allowing online learning.
Acknowledgments
The research reported here was funded in part by ARO, DARPA, DOE, NGA and ONR. The authors
are grateful to Jonas Wallin, Lund University, Sweden, for providing efficient package on simulation
of the GIG distribution.

8

References
[1] J. Aitchison. The statistical analysis of compositional data. J. Roy. Stat. Soc. Ser. B, 44(2):139?177, 1982.
[2] S. Chib and R. Winkelmann. Markov chain Monte Carlo analysis of correlated count data. Journal of
Business & Economic Statistics, 19(4), 2001.
[3] E. Lawrence, D. Bingham, C. Liu, and V. N. Nair. Bayesian inference for multivariate ordinal data using
parameter expansion. Technometrics, 50(2), 2008.
[4] M. West, P. J. Harrison, and H. S. Migon. Dynamic generalized linear models and Bayesian forecasting.
J. Am. Statist. Assoc., 80(389):73?83, 1985.
[5] C. Cargnoni, P. M?uller, and M. West. Bayesian forecasting of multinomial time series through conditionally Gaussian dynamic models. J. Am. Statist. Assoc., 92(438):640?647, 1997.
[6] P. D. Hoff. Extending the rank likelihood for semiparametric copula estimation. Ann. Appl. Statist.,
1(1):265?283, 2007.
[7] D. M. Blei and J. D. Lafferty. Dynamic topic models. In Int. Conf. Machine Learning, 2006.
[8] D. M. Blei and J. D. Lafferty. Correlated topic models. In Adv. Neural Inform. Processing Systems, 2006.
[9] A. Ahmed and E. P. Xing. Timeline: A dynamic hierarchical dirichlet process model for recovering
birth/death and evolution of topics in text stream. 2010.
[10] P. D. Hoff. A first course in Bayesian statistical methods. Springer, 2009.
[11] A. N. Pettitt. Inference for the linear model using a likelihood based on ranks. J. Roy. Stat. Soc. Ser. B,
44(2):234?243, 1982.
[12] J. Aitchison and C. H. Ho. The multivariate Poisson-log normal distribution. Biometrika, 76(4):643?653,
1989.
[13] J. S. Murray, D. B. Dunson, L. Carin, and J. E. Lucas. Bayesian Gaussian copula factor models for mixed
data. J. Am. Statist. Assoc., 108(502):656?665, 2013.
[14] A. Kalaitzis and R. Silva. Flexible sampling of discrete data correlations without the marginal distributions. In Adv. Neural Inform. Processing Systems, 2013.
[15] A. Armagan, M. Clyde, and D. B. Dunson. Generalized Beta mixtures of Gaussians. In Adv. Neural
Inform. Processing Systems, 2011.
[16] N. G. Polson and J. G. Scott. On the half-Cauchy prior for a global scale parameter. Bayesian Analysis,
7(4):887?902, 2012.
[17] H. F. Lopes and M. West. Bayesian model assessment in factor analysis. Statistica Sinica, 14(1):41?68,
2004.
[18] J. Ghosh and D. B. Dunson. Default prior distributions and efficient posterior computation in Bayesian
factor analysis. Journal of Computational and Graphical Statistics, 18(2):306?320, 2009.
[19] A. Bhattacharya and D. B. Dunson. Sparse Bayesian infinite factor models. Biometrika, 98(2):291?306,
2011.
[20] J. Geweke and G. Zhou. Measuring the pricing error of the arbitrage pricing theory. Review of Financial
Studies, 9(2):557?587, 1996.
[21] A. Christian, B. Jens, and P. Markus. Bayesian analysis of dynamic factor models: an ex-post approach
towards the rotation problem. Kiel Working Papers 1902, Kiel Institute for the World Economy, 2014.
[22] C. Gao and B. E. Engelhardt. A sparse factor analysis model for high dimensional latent spaces. In NIPS:
Workshop on Analysis Operator Learning vs. Dictionary Learning: Fraternal Twins in Sparse Modeling,
2012.
[23] C. K. Carter and R. Kohn. On Gibbs sampling for state space models. Biometrika, 81:541?553, 1994.
[24] S. Fr?uhwirth-Schnatter. Data augmentation and dynamic linear models. Journal of Times Series Analysis,
15:183?202, 1994.
[25] D. Inouye, P. Ravikumar, and I. Dhillon. Admixture of Poisson MRFs: A topic model with word dependencies. In Int. Conf. Machine Learning, 2014.
[26] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet allocation. J. Machine Learn. Res., 3:993?1022,
2003.
[27] J. Reisinger, A. Waters, B. Silverthorn, and R. J. Mooney. Spherical topic models. In Int. Conf. Machine
Learning, 2010.
[28] E. A. Reis, E. Salazar, and D. Gamerman. Comparison of sampling schemes for dynamic linear models.
International Statistical Review, 74(2):203?214, 2006.
[29] A. Korattikara, Y. Chen, and M. Welling. Austerity in MCMC land: cutting the Metropolis-Hastings
budget. In Int. Conf. Machine Learning, pages 181?189, 2014.
[30] M. Quiroz, M. Villani, and R. Kohn.
Speeding up MCMC by efficient data subsampling.
arXiv:1404.4178, 2014.
[31] C. E. Rasmussen. Gaussian processes in machine learning. Springer, 2004.
[32] L. Lin and D. B. Dunson. Bayesian monotone regression using Gaussian process projection. Biometrika,
101(2):303?317, 2014.
[33] A. Doucet, D. F. Nando, and N. Gordon. Sequential Monte Carlo methods in practice. Springer, 2001.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 4618-cprl-an-extension-of-compressive-sensing-to-the-phase-retrieval-problem.pdf

CPRL ? An Extension of Compressive Sensing to the
Phase Retrieval Problem
Henrik Ohlsson
Division of Automatic Control, Department of Electrical Engineering,
Link?oping University, Sweden.
Department of Electrical Engineering and Computer Sciences
University of California at Berkeley, CA, USA
ohlsson@eecs.berkeley.edu
Allen Y. Yang
Department of Electrical Engineering and Computer Sciences
University of California at Berkeley, CA, USA
Roy Dong
Department of Electrical Engineering and Computer Sciences
University of California at Berkeley, CA, USA
S. Shankar Sastry
Department of Electrical Engineering and Computer Sciences
University of California at Berkeley, CA, USA

Abstract
While compressive sensing (CS) has been one of the most vibrant research fields in
the past few years, most development only applies to linear models. This limits its
application in many areas where CS could make a difference. This paper presents
a novel extension of CS to the phase retrieval problem, where intensity measurements of a linear system are used to recover a complex sparse signal. We propose
a novel solution using a lifting technique ? CPRL, which relaxes the NP-hard
problem to a nonsmooth semidefinite program. Our analysis shows that CPRL
inherits many desirable properties from CS, such as guarantees for exact recovery.
We further provide scalable numerical solvers to accelerate its implementation.

1

Introduction

In the area of X-ray imaging, phase retrieval (PR) refers to the problem of recovering a complex
multivariate signal from the squared magnitude of its Fourier transform. Existing sensor devices for
collecting X-ray images are only sensitive to signal intensities but not the phases. However, it is
very important to be able to recover the missing phase information as it reveals finer structures of
the subjects than using the intensities alone. The PR problem also has broader applications and has
been studied extensively in biology, physics, chemistry, astronomy, and more recent nanosciences
[29, 20, 18, 24, 23].
Mathematically, PR can be formulated using a linear system y = Ax ? CN , where the matrix
A may represent the Fourier transform or other more general linear transforms. If the complex
measurements y are available and the matrix A is assumed given, it is well known that the leastsquares (LS) solution recovers the model parameter x that minimizes the squared estimation error:
1

ky ? Axk22 . In PR, we assume that the phase of the coefficients of y is omitted and only the squared
magnitude of the output is observed:
bi = |yi |2 = |hx, ai i|2 ,

i = 1, ? ? ? , N,

(1)

where AH = [a1 , ? ? ? , aN ] ? Cn?N , y T = [y1 , ? ? ? , yN ] ? CN , and AH denotes the Hermitian
transpose of A.
Inspired by the emerging theory of compressive sensing [17, 8] and a lifting technique recently
proposed for PR [13, 10], we study the PR problem with a more restricted assumption that the
model parameter x is sparse and the number of observations N are too few for (1) to have a unique
solution, and in some cases even fewer measurements than the number of unknowns n. The problem
is known as compressive phase retrieval (CPR) [25, 27, 28]. In many X-ray imaging applications,
for instance, if the complex source signal is indeed sparse under a proper basis, CPR provides a
viable solution to exactly recover the signal while collecting much fewer measurements than the
traditional non-compressive solutions.
Clearly, the PR problem and its CPR extension are much more challenging than the LS problem, as
the phase of y is lost while only its squared magnitude is available. For starters, it is important to note
that the setup naturally leads to ambiguous solutions regardless whether the original linear model is
overdetermined or not. For example, if x0 ? Cn is a solution to y = Ax, then any multiplication of
x and a scalar c ? C, |c| = 1, leads to the same squared output b. As mentioned in [10], when the
dictionary A represents the unitary discrete Fourier transform (DFT), the ambiguities may represent
time-reversed or time-shifted solutions of the ground-truth signal. Hence, these global ambiguities
are considered acceptable in PR applications. In this paper, when we talk about a unique solution to
PR, it is indeed a representative of a family of solutions up to a global phase ambiguity.
1.1

Contributions

The main contribution of the paper is a convex formulation of the CPR problem. Using the lifting technique, the NP-hard problem is relaxed as a semidefinite program (SDP). We will briefly
summarize several theoretical bounds for guaranteed recovery of the complex input signal, which
is presented in full detail in our technical report [26]. Built on the assurance of the guaranteed
recovery, we will focus on the development of a novel scalable implementation of CPR based on
the alternating direction method of multipliers (ADMM) approach. The ADMM implementation
provides a means to apply CS ideas to PR applications e.g., high-impact nanoscale X-ray imaging.
In the experiment, we will present a comprehensive comparison of the new algorithm with the traditional interior-point method, other state-of-the-art sparse optimization techniques, and a greedy
algorithm proposed in [26]. In high-dimensional complex domain, the ADMM algorithm demonstrates superior performance in our simulated examples and real images. Finally, the paper also
provides practical guidelines to practitioners at large working on other similar nonsmooth SDP applications. To aid peer evaluation, the source code of all the algorithms have been made available at:
http://www.rt.isy.liu.se/?ohlsson/.

2

Compressive Phase Retrieval via Lifting (CPRL)

Since (1) is nonlinear in the unknown x, N  n measurements are in general needed for a unique
solution. When the number of measurements N are fewer than necessary for such a unique solution,
additional assumptions are needed as regularization to select one of the solutions. In classical CS, the
ability to find the sparsest solution to a linear equation system enables reconstruction of signals from
far fewer measurements than previously thought possible. Classical CS is however only applicable
to systems with linear relations between measurements and unknowns. To extend classical CS to the
nonlinear PR problem, we seek the sparsest solution satisfying (1):
min kxk0 ,
x

H
subj. to b = |Ax|2 = {aH
i xx ai }1?i?N ,

(2)

with the square acting element-wise and b = [b1 , ? ? ? , bN ]T ? RN . As the counting norm k ? k0 is
not a convex function, following the `1 -norm relaxation in CS, (2) can be relaxed as
min kxk1 ,
x

H
subj. to b = |Ax|2 = {aH
i xx ai }1?i?N .

2

(3)

Note that (3) is still not a convex program, as its equality constraint is not a linear equation. In the
literature, a lifting technique has been extensively used to reframe problems such as (3) to a standard
form in SDP, such as in Sparse PCA [15]. More specifically, given the ground-truth signal x0 ? Cn ,
n?n
let X0 , x0 xH
be an induced rank-1 semidefinite matrix. Then (3) can be reformulated
0 ? C
1
into
minX0 kXk1 ,

subj. to

rank(X) = 1, bi = aH
i Xai , i = 1, ? ? ? , N.

(4)

This is of course still a nonconvex problem due to the rank constraint. The lifting approach addresses
this issue by replacing rank(X) with Tr(X). For a positive-semidefinite matrix, Tr(X) is equal to
the sum of the eigenvalues of X (or the `1 -norm on a vector containing all eigenvalues of X). This
leads to the nonsmooth SDP
minX0 Tr(X) + ?kXk1 ,

subj. to bi = Tr(?i X), i = 1, ? ? ? , N,

(5)

n?n
where we further denote ?i , ai aH
and ? ? 0 is a design parameter. Finally, the estimate
i ?C
of x can be found by computing the rank-1 decomposition of X via singular value decomposition.
We refer to the approach as compressive phase retrieval via lifting (CPRL).

Consider now the case that the measurements are contaminated by data noise. In a linear model,
bounded random noise typically affects the output of the system as y = Ax + e, where e ? CN is a
noise term with bounded `2 -norm: kek2 ? . However, in phase retrieval, we follow closely a more
special noise model used in [13]:
bi = |hx, ai i|2 + ei .
(6)
This nonstandard model avoids the need to calculate the squared magnitude output |y|2 with the
added noise term. More importantly, in most practical phase retrieval applications, measurement
noise is introduced when the squared magnitudes or intensities of the linear system are measured on
the sensing device, but not y itself. Accordingly, we denote a linear operator B of X as
B : X ? Cn?n 7? {Tr(?i X)}1?i?N ? RN ,

(7)

which measures the noise-free squared output. Then the approximate CPR problem with bounded
`2 -norm error model can be solved by the following nonsmooth SDP program:
minX0 Tr(X) + ?kXk1 ,

subj. to kB(X) ? bk2 ? ?.

(8)

Due to the machine rounding error, in general a nonzero ? should be always assumed and in its
termination condition during the optimization. The estimate of x, just as in noise free case, can
finally be found by computing the rank-1 decomposition of X via singular value decomposition.
We refer to the method as approximate CPRL.

3

Theoretical Analysis

This section highlights some of the analysis results derived for CPRL. The proofs of these results are
available in the technical report [26]. The analysis follows that of CS and is inspired by derivations
given in [13, 12, 16, 9, 3, 7]. In order to state some theoretical properties for CPRL, we need a
generalization of the restricted isometry property (RIP).
2

2 ? 1| <  for all
Definition 1 (RIP) A linear operator B(?) as defined in (7) is (, k)-RIP if | kB(X)k
kXk2
2
kXk0 ? k and X 6= 0.

We can now state the following theorem:
Theorem 2 (Recoverability/Uniqueness) Let B(?) be a (, 2kX ? k0 )-RIP linear operator with  <
? be the sparsest solution to (1). If X ? satisfies b = B(X ? ), X ?  0, rank{X ? } = 1,
1 and let x
?
?x
?H .
then X is unique and X ? = x
?:
We can also give a bound on the sparsity of x
? be the
? H k0 from above) Let x
? be the sparsest solution to (1) and let X
Theorem 3 (Bound on k?
xx
H
?
?
? k0 .
solution of CPRL (5). If X has rank 1 then kXk0 ? k?
xx
The following result now holds trivially:
1

In this paper, kXk1 for a matrix X denotes the entry-wise `1 -norm, and kXk2 denotes the Frobenius norm.

3

? be the sparsest solution to (1). The solution
Corollary 4 (Guaranteed recovery using RIP) Let x
H
?
? 0 )-RIP with  < 1.
?
?
of CPRL X is equal to xx if it has rank 1 and B(?) is (, 2kXk
? can not be guaranteed, the following bound becomes useful:
?x
?H = X
If x
? 1 ) Let  < 1? and assume B(?) to be a (, 2k)-RIP linear
Theorem 5 (Bound on kX ? ? Xk
1+ 2
?
operator. Let X be any matrix (sparse or dense) satisfying b = B(X ? ), X ?  0, rank{X ? } = 1,
? be the CPRL solution, (5), and form Xs from X ? by setting all but the k largest elements to
let X
zero. Then,
?
2 ?
? ? X ? k1 ?
(1 ? ( 21??k + 1) ?1 )kX
kX ? ? Xs k1 ,
(9)
(1??) k
?
with ? = 2/(1 ? ).
Given the RIP analysis, it may be the case that the linear operator B(?) does not well satisfy the RIP
property defined in Definition 1, as pointed out in [13]. In these cases, RIP-1 maybe considered:
1 ? 1| <  for all matrices
Definition 6 (RIP-1) A linear operator B(?) is (, k)-RIP-1 if | kB(X)k
kXk1
X 6= 0 and kXk0 ? k.
Theorems 2?3 and Corollary 4 all hold with RIP replaced by RIP-1 and are not restated in detail
here. Instead we summarize the most important property in the following theorem:
? be the sparsest solution to (1). The
Theorem 7 (Upper bound & recoverability through `1 ) Let x
? is equal to x
? 0 )-RIP-1 with  < 1.
?x
? H if it has rank 1 and B(?) is (, 2kXk
solution of CPRL (5), X,
The RIP type of argument may be difficult to check for a given matrix and are more useful for
claiming results for classes of matrices/linear operators. For instance, it has been shown that random Gaussian matrices satisfy the RIP with high probability. However, given realization of a random Gaussian matrix, it is indeed difficult to check if it actually satisfies the RIP. Two alternative
arguments are spark [14] and mutual coherence [17, 11]. The spark condition usually gives tighter
bounds but is known to be difficult to compute as well. On the other hand, mutual coherence may
give less tight bounds, but is more tractable. We will focus on mutual coherence, which is defined as:
Definition 8 (Mutual coherence) For a matrix A, define the mutual coherence as ?(A) =
Ha |
j
i
max1?i,j?n,i6=j ka|a
.
i k2 kaj k2
By an abuse of notation, let B be the matrix satisfying b = BX s with X s being the vectorized
version of X. We are now ready to state the following theorem:
? be the sparsest solution to (1). The solution
Theorem 9 (Recovery using mutual coherence) Let x
? is equal to x
? 0 < 0.5(1 + 1/?(B)).
?x
? H if it has rank 1 and kXk
of CPRL (5), X,

4

Numerical Implementation via ADMM

In addition to the above analysis of guaranteed recovery properties, a critical issue for practitioners is
the availability of efficient numerical solvers. Several numerical solvers used in CS may be applied
to solve nonsmooth SDPs, which include interior-point methods (e.g., used in CVX [19]), gradient
projection methods [4], and augmented Lagrangian methods (ALM) [4]. However, interior-point
methods are known to scale badly to moderate-sized convex problems in general. Gradient projection methods also fail to meaningfully accelerate the CPRL implementation due to the complexity
of the projection operator. Alternatively, nonsmooth SDPs can be solved by ALM. However, the
augmented primal and dual objective functions are still complex SDPs, which are equally expensive
to solve in each iteration. In summary, as we will demonstrate in Section 5, CPRL as a nonsmooth
complex SDP is categorically more expensive to solve compared to the linear programs underlying
CS, and the task exceeds the capability of many popular sparse optimization techniques.
In this paper, we propose a novel solver to the nonsmooth SDP underlying CPRL via the alternating
directions method of multipliers (ADMM, see for instance [6] and [5, Sec. 3.4]) technique. The
motivation to use ADMM are two-fold: 1. It scales well to large data sets. 2. It is known for its fast
convergence. There are also a number of strong convergence results [6] which further motivates the
choice.
To set the stage for ADMM, rewrite (5) to the equivalent SDP
minX1 ,X2 ,Z f1 (X1 ) + f2 (X2 ) + g(Z),

subj. to X1 ? Z = 0,
4

X2 ? Z = 0,

(10)

where

0 if X  0
Tr(X) if bi = T r(?i X), i = 1, . . . , N
, g(Z) , ?kZk1 .
, f2 (X) ,
f1 (X) ,
? otherwise
?
otherwise


The update rules of ADMM now lead to the following:
Xil+1
Z l+1
Yil+1

= arg minX fi (X) + Tr(Yil (X ? Z l )) + ?2 kX ? Z l k22 ,
P2
= arg minZ g(Z) + i=1 ?Tr(Yil Z) + ?2 kXil+1 ? Zk22 ,
= Yil + ?(Xil+1 ? Z l+1 ),

(11)

where Xi , Yi , Z are constrained to stay in the domain of Hermitian matrices. Each of these steps has
a tractable calculation. However, the Xi , Yi , and Z variables are complex-valued, and, as most of
the optimization literature deals with real-valued vectors and symmetric matrices, we will emphasize
differences between the real case and complex case. After some simple manipulations, we have:
X1l+1 = argminX kX ? (Z l ?

I+Y1l
? )k2 ,

subj. to bi = Tr(?i X), i = 1, ? ? ? , N.

(12)

Assuming that a feasible solution exists, and defining ?A as the projection onto the convex set given
l
1 ). This optimization problem has a
by the linear constraints, the solution is: X1l+1 = ?A (Z l ? I+Y
?
closed-form solution; converting the matrix optimization problem in (12) into an equivalent vector
optimization problem yields a problem of the form: minx ||x?z||2 subj. to b = Ax. The answer
is given by the pseudo-inverse of A, which can be precomputed. This complex-valued problem can
be solved by converting the linear constraint in Hermitian matrices into an equivalent constraint on
real-valued vectors. This conversion is done by noting that for n ? n Hermitian matrices A, B:
Pn
Pn Pn
Pn Pn
hA, Bi = P
Tr(AB) = i=1P j=1PAij Bij = i=1 Aii Bii + i=1 j=i+1 Aij Bij + Aij Bij
n
n
n
= i=1 Aii Bii + i=1 j=i+1 2 real(Aij ) real(Bij ) + 2 imag(Aij ) imag(Bij )
v
2
So
? if we define the vector A as an n vector such that
? its elements are Aii for i = 1, ? ? ? , n,
2 real(Aij ) for i = 1, ? ? ? , n, j = i + 1, ? ? ? , n, and 2 imag(Aij ) for i = 1, ? ? ? , n, j = i +
1, ? ? ? , n, and similarly define B v , then we can see that hA, Bi = hAv , B v i. This turns the constraint
2
bi = Tr(?i X), i = 1, ? ? ? , N, into one of the form: b = [?v1 ? ? ? ?vN ]T X v , where each ?vi is in Rn .
Thus, for this subproblem, the memory usage scales linearly with N , the number of measurements,
l
and quadratically with n, the dimension of the data. Next, X2l+1 = argminX0 kX ?(Z l ? Y?2 )k2 =
l
?P SD (Z l ? Y?2 ), where ?P SD denotes the projection onto the positive-semidefinite cone, which
can easily be obtained via eigenvalue decomposition. This holds for real-valued and complex-valued
P2
l+1
l
= 21 i=1 Xil+1 and similarly Y . Then, the Z update rule
Hermitian matrices. Finally, let X
can be written:

Z l+1 = argminZ ?kZk1 +

2?
2 kZ

? (X

l+1

l

+

Y
?

)k22 = soft(X

l+1

l

+

Y
?

?
, 2?
).

(13)

We note that the soft operator in the complex domain must be coded with care. One does not simply
check the sign of the difference, as in the real case, but rather the magnitude of the complex number:
(
0
if |x| ? q,
soft(x, q) = |x|?q
(14)
x
otherwise,
|x|
where q is a positive real number. Setting l = 0, the Hermitian matrices Xil , Zil , Yil can now be
iteratively computed using the ADMM iterations (11). The stopping criterion of the algorithm is
given by:
l

krl k2 ? nabs + rel max(kX k2 , kZ l k2 ),
abs

rel

?3

l

ksl k2 ? nabs + rel kY k2 ,
l

(15)

l

where  ,  are algorithm parameters set to 10 and r and s are the primal and dual residuals
given by: rl = (X1l ? Z l , X2l ? Z l ), sl = ??(Z l ? Z l?1 , Z l ? Z l?1 ). We also update ? according
to the rule discussed in [6]:
?
l
if krl k2 > ?ksl k2 ,
??incr ?
l+1
l
?
=
(16)
? /?
if ksl k2 > ?krl k2 ,
? l decr
?
otherwise,
where ?incr , ?decr , and ? are algorithm parameters. Values commonly used are ? = 10 and ?incr =
?decr = 2.
5

5

Experiment

The experiments in this section are chosen to illustrate the computational performance and scalability of CPRL. Being one of the first papers addressing the CPR problem, existing methods available
for comparison are limited. For the CPR problem, to the authors? best knowledge, the only methods
developed are the greedy algorithms presented in [25, 27, 28], and GCPRL [26]. The method proposed in [25] handles CPR but is only tailored to random 2D Fourier samples from a 2D array and it
is extremely sensitive to initialization. In fact, it would fail to converge in our scenarios of interest.
[27] formulates the CPR problem as a nonconvex optimization problem that can be solved by solving a series of convex problems. [28] proposes to alternate between fit the estimate to measurements
and thresholding. GCPRL, which stands for greedy CPRL, is a new greedy approximate algorithm
tailored to the lifting technique in (5). The algorithm draws inspiration from the matching-pursuit algorithm [22, 1]. In each iteration, the algorithm adds a new nonzero component of x that minimizes
the CPRL objective function the most. We have observed that if the number of nonzero elements in
x is expected to be low, the algorithm can successfully recover the ground-truth sparse signal while
consuming less time compared to interior-point methods for the original SDP.2 In general, greedy
algorithms for solving CPR problems work well when a good guess for the true solution is available,
are often computationally efficient but lack theoretical recovery guarantees. We also want to point
out that CPRL becomes a special case in a more general framework that extends CS to nonlinear
systems (see [1]). In general, nonlinear CS can be solved locally by greedy simplex pursuit algorithms. Its instantiation in PR is the GCPRL algorithm. However, the key benefit of developing the
SDP solution for PR in this paper is that the global convergence can be guaranteed.
In this section, we will compare implementations of CPRL using the interior-point method used by
CVX [19] and ADMM with the design parameter choice recommended in [6] (?incr = ?decr = 2).
? = 10 will be used in all experiments. We will also compare the results to GCPRL and the PR
algorithm PhaseLift [13]. The former is a greedy approximate solution, while the latter does not
enforce sparsity and is obtained by setting ? = 0 in CPRL.
In terms of the scale of the problem, the largest problem we have tested is on a 30 ? 30 image and is
100-sparse in the Fourier domain with 2400 measurements. Our experiment is conducted on an IBM
x3558 M3 server with two Xeon X5690 processors, 6 cores each at 3.46GHz, 12MB L3 cache, and
96GB of RAM. The execution for recovering one instance takes approximately 36 hours to finish in
MATLAB environment, comprising of several tens of thousands of iterations. The average memory
usage is 3.5 GB.
5.1

A simple simulation

In this example we consider a simple CPR problem to illustrate the differences between CPRL,
GCPRL, and PhaseLift. We also compare computational speed for solving the CPR problem and
illustrate the theoretical bounds derived in Section 3. Let x ? C64 be a 2-sparse complex signal,
A , RF where F ? C64?64 is the Fourier transform matrix and R ? C32?64 a random projection
matrix (generated by sampling a unit complex Gaussian), and let the measurements b satisfy the
PR relation (1). The left plot of Figure 1 gives the recovered signal x using CPRL, GCPRL and
PhaseLift. As seen, CPRL and GCPRL correctly identify the two nonzero elements in x while
PhaseLift fails to identify the true signal and gives a dense estimate. These results are rather typical
(see the MCMC simulation in [26]). For very sparse examples, like this one, CPRL and GCPRL
often both succeed in finding the ground truth (even though we have twice as many unknowns
as measurements). PhaseLift, on the other side, does not favor sparse solutions and would need
considerably more measurements to recover the 2-sparse signal. The middle plot of Figure 1 shows
the computational time needed to solve the nonsmooth SDP of CPRL using CVX, ADMM, and
GCPRL. It shows that ADMM is the fastest and that GCPRL outperforms CVX. The right plot of
Figure 1 shows the mutual coherence bound 0.5(1 + 1/?(B)) for a number of different N ?s and
n?s, A , RF , F ? Cn?n the Fourier transform matrix and R ? CN ?n a random projection
? satisfies kXk
? 0<
matrix. This is of interest since Theorem 9 states that when the CRPL solution X
H
?
?x
? , where x
? is the sparsest solution to (1). From
0.5(1 + 1/?(B)) and has rank 1, then X = x
2
We have also tested an off-the-shelf toolbox that solves convex cone problems, called TFOCS [2]. Unfortunately, TFOCS cannot be applied directly to solving the nonsmooth SDP in CPRL.

6

? has rank 1 and only a single nonzero
the plot it can be concluded that if the CPRL solution X
? = x
?x
? H . We also
component for a choice of 125 ? n, N ? 5, Theorem 9 guarantees that X
observe that Theorem 9 is conservative, since we previously saw that 2 nonzero components could
be recovered correctly for n = 64 and N = 32. In fact, numerical simulation can be used to show
that N = 30 suffices to recover the ground truth in 95 out of 100 runs [26].
1

10

PhaseLift
CPRL/GCPRL

0.9
0.8

8

0.7

1.22

110
100

1.2

90

~
__

0.5

1.18

80

0.4

1.16

70

0.3

1.14

6

N

|| xxH ? X ||

i

120

7

0.6

|x |

1.24

CPRL (CVX)
GCPRL
CPRL (ADMM)

9

5
4

60
1.12

3

50
0.2

1.1

2

40
0.1

1

1.08

30
0
0

10

20

30

40

50

60

0
0

i

20

40

60

time [s]

80

100

40

60

80

100

120

n

Figure 1: Left: The magnitude of the estimated signal provided by CPRL, GCPRL and PhaseLift.
? 2 plotted against time for ADMM (gray line), GCPRL (solid
? H ? Xk
Middle: The residual k?
xx
black line) and CVX (dashed black line). Right: A contour plot of the quantity 0.5(1 + 1/?(B)). ?
is taken as the average over 10 realizations of the data.
5.2

Compressive sampling and PR

One of the motivations of presented work and CPRL is that it enables compressive sensing for PR
problems. To illustrate this, consider the 20 ? 20 complex image in Figure 2 Left. To measure the
image, we could measure each pixel one-by-one. This would require us to sample 400 times. What
CS proposes is to measure linear combinations of samples rather than individual pixels. It has been
shown that the original image can be recovered from far fewer samples than the total number of
pixels in the image. The gain using CS is hence that fewer samples are needed. However, traditional
CS only discuss linear relations between measurements and unknowns.
To extend CS to PR applications, consider again the complex image in Figure 2 Left and assume that
we only can measure intensities or intensities of linear combinations of pixels. Let R ? CN ?400
capture how intensity measurements b are formed from linear combinations of pixels in the image,
b = |Rz|2 (z is a vectorized version of the image). An essential part in CS is also to find a dictionary
(possibly overcomplete) in which the image can be represented using only a few basis images. For
classical CS applications, dictionaries have been derived. For applying CS to the PR applications,
dictionaries are needed and a topic for future research. We will use a 2D inverse Fourier transform
dictionary in our example and arrange the basis vectors as columns in F ? C400?400 .
If we choose N = 400 and generate R by sampling from a unit Gaussian distribution and set
A = RF , CPRL recovers exactly the true image. This is rather remarkable since the PR relation
(1) is nonlinear in the unknown x and N  n measurements are in general needed for a unique
solution. If we instead sample the intensity of each pixel, one-by-one, neither CPRL or PhaseLift
recover the true image. If we set A = R and do not care about finding a dictionary, we can use
a classical PR algorithm to recover the true image. If PhaseLift is used, N = 1600 measurements
are sufficient to recover the true image. The main reasons for the low number of samples needed in
CPRL is that we managed to find a good dictionary (20 basis images were needed to recover the true
image) and CPRL?s ability to recover the sparsest solution. In fact, setting A = RF , PhaseLift still
needs 1600 measurements to recover the true solution.
5.3

The Shepp-Logan phantom

In this last example, we again consider the recovery of complex valued images from random samples. The motivation is twofold: Firstly, it illustrates the scalability of the ADMM implementation.
In fact, ADMM has to be used in this experiment as CVX cannot handle the CPRL problem in this
scale. Secondly, it illustrates that CPRL can provide approximate solutions that are visually close
to the ground-truth images. Consider now the image in Figure 2 Middle Left. This 30 ? 30 SheppLogan phantom has a 2D Fourier transform with 100 nonzero coefficients. We generate N linear
combinations of pixels as in the previous example and square the measurements, and then apply
7

CPRL and PhaseLift with a 2D Fourier dictionary. The middel image in Figure 2 shows the recovered result using PhaseLift with N = 2400, the second image from the right shows the recovered
result using CPRL with the same number N = 2400 and the right image is the recovered result using
CPRL with N = 1500. The number of measurements with respect to the sparsity in x is too low for
both CPRL and PhaseLift to perfectly recover z. However, CPRL provides a much better approximation and outperforms PhaseLift visually even though it uses considerably fewer measurements.
2
5

5

5

5

10

10

10

10

15

15

15

15

20

20

20

20

25

25

25

4

6

8

10

12

14

16
25

18

20

30
2

4

6

8

10

12

14

16

18

20

30
5

10

15

20

25

30

30
5

10

15

20

25

30

30
5

10

15

20

25

30

5

10

15

20

25

30

Figure 2: Left: Absolute value of the 2D inverse Fourier transform of x, |F x|, used in the experiment in Section 5.2. Middle Left: Ground truth for the experiment in Section 5.3. Middle:
Recovered result using PhaseLift with N = 2400. Middle Right: CPRL with N = 2400. Right:
CPRL with N = 1500.

6

Future Directions

The SDP underlying CPRL scales badly with the number of unknowns or basis vectors in the dictionary. Therefore, learning a suitable dictionary for a specific application becomes even more critical
than that in traditional linear CS setting. We also want to point out that when classical CS was first
studied, many of today?s accelerated numerical algorithms were not available. We are very excited
about the new problem to improve the speed of SDP algorithms in sparse optimization, and hope
our paper would foster the community?s interest to address this challenge collaboratively. One interesting direction might be to use ADMM to solve the dual of (5), see for instance [30, 31]. Another
possible direction is the outer approximation methods [21].

7

Acknowledgement

Ohlsson is partially supported by the Swedish foundation for strategic research in the center MOVIII,
the Swedish Research Council in the Linnaeus center CADICS, the European Research Council
under the advanced grant LEARN, contract 267381, and a postdoctoral grant from the SwedenAmerica Foundation, donated by ASEA?s Fellowship Fund, and by a postdoctoral grant from the
Swedish Research Council. Yang is supported by ARO 63092-MA-II. Dong is supported by the
NSF Graduate Research Fellowship under grant DGE 1106400, and by the Team for Research in
Ubiquitous Secure Technology (TRUST), which receives support from NSF (award number CCF0424422). The authors also want to acknowledge useful input from Stephen Boyd and Yonina Eldar.

References
[1] A. Beck and Y. C. Eldar. Sparsity constrained nonlinear optimization: Optimality conditions and algorithms. Technical Report arXiv:1203.4580, 2012.
[2] S. Becker, E. Cand`es, and M. Grant. Templates for convex cone problems with applications to sparse
signal recovery. Mathematical Programming Computation, 3(3), 2011.
[3] R. Berinde, A. Gilbert, P. Indyk, H. Karloff, and M. Strauss. Combining geometry and combinatorics:
A unified approach to sparse signal recovery. In Communication, Control, and Computing, 2008 46th
Annual Allerton Conference on, pages 798?805, September 2008.
[4] D. P. Bertsekas. Nonlinear Programming. Athena Scientific, 1999.
[5] D. P. Bertsekas and J. N. Tsitsiklis. Parallel and Distributed Computation: Numerical Methods. Athena
Scientific, 1997.
[6] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical learning
via the alternating direction method of multipliers. Foundations and Trends in Machine Learning, 2011.
[7] A. Bruckstein, D. Donoho, and M. Elad. From sparse solutions of systems of equations to sparse modeling
of signals and images. SIAM Review, 51(1):34?81, 2009.

8

[8] E. Cand`es. Compressive sampling. In Proceedings of the International Congress of Mathematicians,
volume 3, pages 1433?1452, Madrid, Spain, 2006.
[9] E. Cand`es. The restricted isometry property and its implications for compressed sensing. Comptes Rendus
Mathematique, 346(9?10):589?592, 2008.
[10] E. Cand`es, Y. Eldar, T. Strohmer, and V. Voroninski. Phase retrieval via matrix completion. Technical
Report arXiv:1109.0573, Stanford University, September 2011.
[11] E. Cand`es, X. Li, Y. Ma, and J. Wright. Robust Principal Component Analysis? Journal of the ACM,
58(3), 2011.
[12] E. Cand`es, J. Romberg, and T. Tao. Robust uncertainty principles: Exact signal reconstruction from highly
incomplete frequency information. IEEE Transactions on Information Theory, 52:489?509, February
2006.
[13] E. Cand`es, T. Strohmer, and V. Voroninski. PhaseLift: Exact and stable signal recovery from magnitude measurements via convex programming. Technical Report arXiv:1109.4499, Stanford University,
September 2011.
[14] S. Chen, D. Donoho, and M. Saunders. Atomic decomposition by basis pursuit. SIAM Journal on
Scientific Computing, 20(1):33?61, 1998.
[15] A. d?Aspremont, L. El Ghaoui, M. Jordan, and G. Lanckriet. A direct formulation for Sparse PCA using
semidefinite programming. SIAM Review, 49(3):434?448, 2007.
[16] D. Donoho. Compressed sensing. IEEE Transactions on Information Theory, 52(4):1289?1306, April
2006.
[17] D. Donoho and M. Elad. Optimally sparse representation in general (nonorthogonal) dictionaries via
`1 -minimization. PNAS, 100(5):2197?2202, March 2003.
[18] J. Fienup. Reconstruction of a complex-valued object from the modulus of its Fourier transform using a
support constraint. Journal of Optical Society of America A, 4(1):118?123, 1987.
[19] M. Grant and S. Boyd. CVX: Matlab software for disciplined convex programming, version 1.21. http:
//cvxr.com/cvx, August 2010.
[20] D. Kohler and L. Mandel. Source reconstruction from the modulus of the correlation function: a practical
approach to the phase problem of optical coherence theory. Journal of the Optical Society of America,
63(2):126?134, 1973.
[21] H. Konno, J. Gotoh, T. Uno, and A. Yuki. A cutting plane algorithm for semi-definite programming
problems with applications to failure discriminant analysis. Journal of Computational and Applied Mathematics, 146(1):141?154, 2002.
[22] S. Mallat and Z. Zhang. Matching pursuits with time-frequency dictionaries. IEEE Transactions on Signal
Processing, 41(12):3397?3415, December 1993.
[23] S. Marchesini. Phase retrieval and saddle-point optimization. Journal of the Optical Society of America
A, 24(10):3289?3296, 2007.
[24] R. Millane. Phase retrieval in crystallography and optics. Journal of the Optical Society of America A,
7:394?411, 1990.
[25] M. Moravec, J. Romberg, and R. Baraniuk. Compressive phase retrieval. In SPIE International Symposium on Optical Science and Technology, 2007.
[26] H. Ohlsson, A. Y. Yang, R. Dong, and S. Sastry. Compressive Phase Retrieval From Squared Output Measurements Via Semidefinite Programming. Technical Report arXiv:1111.6323, University of California,
Berkeley, November 2011.
[27] Y. Shechtman, Y. C. Eldar, A. Szameit, and M. Segev. Sparsity based sub-wavelength imaging with
partially incoherent light via quadratic compressed sensing. Opt. Express, 19(16):14807?14822, Aug
2011.
[28] A. Szameit, Y. Shechtman, E. Osherovich, E. Bullkich, P. Sidorenko, H. Dana, S. Steiner, E. B. Kley,
S. Gazit, T. Cohen-Hyams, S. Shoham, M. Zibulevsky, I. Yavneh, Y. C. Eldar, O. Cohen, and M. Segev.
Sparsity-based single-shot subwavelength coherent diffractive imaging. Nature Materials, 11(5):455?
459, May 2012.
[29] A. Walther. The question of phase retrieval in optics. Optica Acta, 10:41?49, 1963.
[30] Z. Wen, D. Goldfarb, and W. Yin. Alternating direction augmented lagrangian methods for semidefinite
programming. Mathematical Programming Computation, 2:203?230, 2010.
[31] Z. Wen, C. Yang, X. Liu, and S. Marchesini. Alternating direction methods for classical and ptychographic
phase retrieval. Inverse Problems, 28(11):115010, 2012.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 2915-variable-kd-tree-algorithms-for-spatial-pattern-search-and-discovery.pdf

Variable KD-Tree Algorithms for Spatial Pattern
Search and Discovery

Jeremy Kubica
Robotics Institute
Carnegie Mellon University
Pittsburgh, PA 15213

Joseph Masiero
Institute for Astronomy
University of Hawaii
Honolulu, HI 96822

Andrew Moore
Robotics Institute
Carnegie Mellon University
Pittsburgh, PA 15213

jkubica@ri.cmu.edu

masiero@ifa.hawaii.edu

awm@cs.cmu.edu

Robert Jedicke
Institute for Astronomy
University of Hawaii
Honolulu, HI 96822

Andrew Connolly
Physics & Astronomy Department
University of Pittsburgh
Pittsburgh, PA 15213

jedicke@ifa.hawaii.edu

ajc@phyast.pitt.edu

Abstract
In this paper we consider the problem of finding sets of points that conform to a given underlying model from within a dense, noisy set of observations. This problem is motivated by the task of efficiently linking
faint asteroid detections, but is applicable to a range of spatial queries.
We survey current tree-based approaches, showing a trade-off exists between single tree and multiple tree algorithms. To this end, we present a
new type of multiple tree algorithm that uses a variable number of trees
to exploit the advantages of both approaches. We empirically show that
this algorithm performs well using both simulated and astronomical data.

1 Introduction
Consider the problem of detecting faint asteroids from a series of images collected on a
single night. Inherently, the problem is simply one of connect-the-dots. Over a single
night we can treat the asteroid?s motion as linear, so we want to find detections that, up
to observational errors, lie along a line. However, as we consider very faint objects, several difficulties arise. First, objects near our brightness threshold may oscillate around this
threshold, blinking into and out-of our images and providing only a small number of actual
detections. Second, as we lower our detection threshold we will begin to pick up more spurious noise points. As we look for really dim objects, the number of noise points increases
greatly and swamps the number of detections of real objects.
The above problem is one example of a model based spatial search. The goal is to identify
sets of points that fit some given underlying model. This general task encompasses a wide
range of real-world problems and spatial models. For example, we may want to detect
a specific configuration of corner points in an image or search for multi-way structure in
scientific data. We focus our discussion on problems that have a high density of both true

and noise points, but which may have only a few points actually from the model of interest.
Returning to the asteroid linking example, this corresponds to finding a handful of points
that lie along a line within a data set of millions of detections.
Below we survey several tree-based approaches for efficiently solving this problem. We
show that both single tree and conventional multiple tree algorithms can be inefficient and
that a trade-off exists between these approaches. To this end, we propose a new type of
multiple tree algorithm that uses a variable number of tree nodes. We empirically show
that this new algorithm performs well using both simulated and real-world data.

2 Problem Definition
Our problem consists of finding sets of points that fit a given underlying spatial model. In
doing so, we are effectively looking for known types of structure buried within the data. In
general, we are interested in finding sets with k or more points, thus providing a sufficient
amount of support to confirm the discovery. Finding this structure within the data may
either be our end goal, such as in asteroid linkage, or may just be a preprocessor for a more
sophisticated statistical test, such as renewal strings [1]. We are particularly interested in
high-density, low-support domains where there may be many hundreds of thousands of
points, but only a handful actually support our model.
Formally, the data consists of N unique D-dimensional points. We assume that the underlying model can be estimated from c unique points. Since k ? c, the model may overconstrained. In these cases we divide the points into two sets: Model Points and Support
Points. Model points are the c points used to fully define the underlying model. Support
points are the remaining points used to confirm the model. For example, if we are searching for sets of k linear points, we could use a set?s endpoints as model points and treat the
middle k ? 2 as support points. Or we could allow any two points to serve as model points,
providing an exhaustive variant of the RANSAC algorithm [2].
The prototypical example used in this paper is the (linear) asteroid linkage problem:
For each pair of points find the k ? 2 best support points for the line that
they define (such that we use at most one point at each time step).
In addition, we place restrictions on the validity of the initial pairs by providing velocity
bounds. It is important to note that although we use this problem as a running example, the
techniques described can be applied to a range of spatial problems.

3 Overview of Previous Approaches
3.1 Constructive Algorithms
Constructive algorithms ?build up? valid sets of points by repeatedly finding additional
points that are compatible with the current set. Perhaps the simplest approach is to perform
a two-tiered brute force search. First, we exhaustively test all sets of c points to determine
if they define a valid model. Then, for each valid set we test all of the remaining points for
support. For example in the asteroid linkage problem, we can initially search over all O(N 2 )
pairs of points and for each of the resulting lines test all O(N) points to determine if they
support that line. A similar approach within the domain of target tracking is sequential
tracking (for a good introduction see [3]), where points at early time steps are used to
estimate a track that is then projected to later time steps to find additional support points.
In large-scale domains, these approaches can often be made tractable by using spatial structure in the data. Again returning to our asteroid example, we can place the points in a

KD-tree [4]. We can then limit the number of initial pairs examined by using this tree to
find points compatible with our velocity constraints. Further, we can use the KD-tree to
only search for support points in localized regions around the line, ignoring large numbers
of obviously infeasible points. Similarly, trees have been used in tracking algorithms to
efficiently find points near predicted track positions [5]. We call these adaptations single
tree algorithms, because at any given time the algorithm is searching at most one tree.
3.2 Parameter Space Methods
Another approach is to search for valid sets of points by searching the model?s parameter
space, such as in the Hough transform [6]. The idea behind these approaches is that we can
test whether each point is compatible with a small set of model parameters, allowing us to
search parameter space to find the valid sets. However, this method can be expensive in
terms of both computation and memory, especially for high dimensional parameter spaces.
Further, if the model?s total support is low, the true model occurrences may be effectively
washed out by the noise. For these reasons we do not consider parameter space methods.
3.3 Multiple Tree Algorithms
The primary benefit of tree-based algorithms is that they are able to use spatial structure
within the data to limit the cost of the search. However, there is a clear potential to push
further and use structure from multiple aspects of the search at the same time. In doing
so we can hopefully avoid many of the dead ends and wrong turns that may result from
exploring bad initial associations in the first few points in our model. For example, in the
domain of asteroid linkage we may be able to limit the number of short, initial associations
that we have to consider by using information from later time steps. This idea forms the
basis of multiple tree search algorithms [7, 8, 9].
Multiple tree methods explicitly search for the entire set of points at once by searching
over combinations of tree nodes. In standard single tree algorithms, the search tries to find
individual points satisfying some criteria (e.g. the next point to add) and the search state
is represented by a single node that could contain such a point. In contrast, multiple tree
algorithms represent the current search state with multiple tree nodes that could contain
points that together conform to the model. Initially, the algorithm begins with k root nodes
from either the same or different tree data structures, representing the k different points that
must be found. At each step in the search, it narrows in on a set of mutually compatible
spatial regions and thus a set of individual points that fit the model by picking one of the
model nodes and recursively exploring its children. As with a standard ?single tree? search,
we constantly check for opportunities to prune the search.
There are several important drawbacks to multiple tree algorithms. First, additional trees
introduce a higher branching factor in the search and increase the potential for taking deep
?wrong turns.? Second, care must be taken in order to deal with missing or a variable
number of support points. Kubica et. al. discuss the use of an additional ?missing? tree
node to handle these cases [9]. However, this approach can effectively make repeated
searches over subsets of trees, making it more expensive both in theory and practice.

4 Variable Tree Algorithms
In general we would like to exploit structural information from all aspects of our search
problem, but do so while branching the search on just the parameters of interest. To this
end we propose a new type of search that uses a variable number of tree nodes. Like a
standard multiple tree algorithm, the variable tree algorithm searches combinations of tree
nodes to find valid sets of points. However, we limit this search to just those points required

(A)

(B)

Figure 1: The model nodes? bounds (1 and 2) define a region of feasible support (shaded)
for any combination of model points from those nodes (A). As shown in (B), we can classify
entire support tree nodes as feasible (node b) or infeasible (nodes a and c).
to define, and thus bound, the models currently under consideration. Specifically, we use M
model tree nodes,1 which guide the recursion and thus the search. In addition, throughout
the search we maintain information about other potential supporting points that can be used
to confirm the final track or prune the search due to a lack of support.
For example in the asteroid linking problem each line is defined by only 2 points, thus we
can efficiently search through the models using a multiple tree search with 2 model trees.
As shown in Figure 1.A, the spatial bounds of our current model nodes immediately limit
the set of feasible support points for all line segments compatible with these nodes. If we
track which support points are feasible, we can use this information to prune the search due
to a lack of support for any model defined by the points in those nodes.
The key idea behind the variable tree search is that we can use a dynamic representation of
the potential support. Specifically, we can place the support points in trees and maintain
a dynamic list of currently valid support nodes. As shown in Figure 1.B, by only testing
entire nodes (instead of individual points), we are using spatial coherence of the support
points to remove the expense of testing each support point at each step in the search. And
by maintaining a list of support tree nodes, we are no longer branching the search over
these trees. Thus we remove the need to make a hard ?left or right? decision. Further, using
a combination of a list and a tree for our representation allows us to refine our support
representation on the fly. If we reach a point in the search where a support node is no
longer valid, we can simply drop it off the list. And if we reach a point where a support
node provides too coarse a representation of the current support space, we can simply
remove it and add both of its children to the list.
This leaves the question of when to split support nodes. If we split them too soon, we may
end up with many support nodes in our list and mitigate the benefits of the nodes? spatial
coherence. If we wait too long to split them, then we may have a few large support nodes
that cannot efficiently be pruned. Although we are still investigating splitting strategies, the
experiments in this paper use a heuristic that seeks to provide a small number of support
nodes that are a reasonable fit to the feasible region. We effectively split a support node
if doing so would allow one of its two children to be pruned. For KD-trees this roughly
means checking whether the split value lies outside the feasible region.
The full variable tree algorithm is given in Figure 2. A simple example of finding linear
tracks while using the track?s endpoints (earliest and latest in time) as model points and
1 Typically M = c, although in some cases it may be beneficial to use a different number of model
nodes.

1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.

Variable Tree Model Detection
Input: A set of M current model tree nodes M
A set of current support tree nodes S
Output: A list Z of feasible sets of points
S? ? {} and Scurr ? S
IF we cannot prune based on the mutual compatibility of M:
FOR each s ? Scurr
IF s is compatible with M:
IF s is ?too wide?:
Add s?s left and right child to the end of Scurr .
ELSE
Add s to S? .
IF we have enough valid support points:
IF all of m ? M are leaves:
Test all combinations of points owned by the model nodes, using
the support nodes? points as potential support.
Add valid sets to Z.
ELSE
Let m? be the non-leaf model tree node that owns the most points.
Search using m? ?s left child in place of m? and S? instead of S.
Search using m? ?s right child in place of m? and S? instead of S.

Figure 2: A simple variable tree algorithm for spatial structure search. This algorithm
shown uses simple heuristics such as: searching the model node with the most points and
splitting a support node if it is too wide. These heuristics can be replaced by more accurate,
problem-specific ones.

using all other points for support is illustrated in Figure 3. The first column shows all
the tree nodes that are currently part of the search. The second and third columns show
the search?s position on the two model trees and the current set of valid support nodes
respectively. Unlike the pure multiple tree search, the variable tree search does not ?branch
off? on the support trees, allowing us to consider multiple support nodes from the same
time step at any point in the search. Again, it is important to note that by testing the
support points as we search, we are both incorporating support information into the pruning
decisions and ?pruning? the support points for entire sets of models at once.

5 Results on the Asteroid Linking Domain
The goal of the single-night asteroid linkage problem is to find sets of 2-dimensional point
detections that correspond to a roughly linear motion model. In the below experiments we
are interested in finding sets of at least 7 detections from a sequence of 8 images. The
movements were constrained to have a speed between 0.05 and 0.5 degrees per day and
were allowed an observational error threshold of 0.0003 degrees. All experiments were run
on a dual 2.5 GHz Apple G5 with 4 GB of RAM.
The asteroid detection data consists of detections from 8 images of the night sky separated
by half-hour intervals. The images were obtained with the MegaCam instrument on the
3.6-meter Canada-France-Hawaii Telescope. The detections, along with confidence levels,
were automatically extracted from the images. We can pre-filter the data to pull out only
those observations above a given confidence threshold ?. This allows us to examine how
the algorithms perform as we begin to look for increasingly faint asteroids. It should be
noted that only limited preprocessing was done to the data, resulting in a very high level

Search Step 1:

Search Step 2:

Search Step 5:

Figure 3: The variable tree algorithm performs a depth first search over the model nodes.
At each level of the search the model nodes are checked for mutual compatibility and each
support node on the list is check for compatibility with the set of model nodes. Since we
are not branching on the support nodes, we can split a support node and add both children
to our list. This figure shows the current model and support nodes and their spatial regions.

Table 1: The running times (in seconds) for the asteroid linkers with different detection
thresholds ? and thus different numbers N and density of observations.
?
N
Single Tree
Multiple Tree
Variable Tree

10.0
3531
2
1
<1

8.0
5818
7
3
1

6.0
12911
61
30
4

5.0
24068
488
607
40

4.0
48646
2442
4306
205

of false detections. While future data sets will contain significantly reduced noise, it is
interesting to examine the performance of the algorithms on this real-world high noise,
high density data.
The results on the intra-night asteroid tracking domain, shown in Table 1, illustrate a clear
advantage to using a variable tree approach. As the significance threshold ? decreases,
the number and density of detections increases, allowing the support tree nodes to capture
feasibility information for a large number of support points. In contrast, neither the full
multiple tree algorithm nor the single-tree algorithm performed well. For the multiple tree
algorithm, this decrease in performance is likely due to a combination of the high number
of time steps, the allowance of a missing observation, and the high density. In particular,
the increased density can reduce opportunities for pruning, causing the algorithm to explore
deeper before backtracking.

Table 2: Average running times (in seconds) for a 2-dimensional rectangle search with
different numbers of points N. The brute force algorithm was only run to N = 2500.
N
Brute Force
Single Tree
Multi-Tree
Variable-Tree

500
0.37
0.02
0.01
0.01

1000
2.73
0.07
0.02
0.02

2000
21.12
0.30
0.06
0.05

2500
41.03
0.51
0.09
0.07

5000
n/a
2.15
0.30
0.22

10000
n/a
10.05
1.11
0.80

25000
n/a
66.24
6.61
4.27

50000
n/a
293.10
27.79
16.30

Table 3: Average running times (in seconds) for a rectangle search with different numbers
of required corners k. For this experiment N = 10000 and D = 3.
k
Single Tree
Multi-Tree
Variable-Tree

8
4.71
3.96
0.65

7
4.72
19.45
0.75

6
4.71
45.02
0.85

5
4.71
67.50
0.92

4
4.71
78.81
1.02

6 Experiments on the Simulated Rectangle Domain
We can apply the above techniques to a range of other model-based spatial search problems.
In this section we consider a toy template matching problem, finding axis-aligned hyperrectangles in D-dimensional space by finding k or more corners that fit a rectangle. We
use this simple, albeit artificial, problem both to demonstrate potential pattern recognition
applications and to analyze the algorithms as we vary the properties of the data.
Formally, we restrict the model to use the upper and lower corners as the two model points.
Potential support points are those points that fall within some threshold of the other 2D ? 2
corners. In addition, we restrict the allowable bounds of the rectangles by providing a
maximum width.
To evaluate the algorithms? relative performance, we used random data generated from a
uniform distribution on a unit hyper-cube. The threshold and maximum width were fixed
for all experiments at 0.0001 and 0.2 respectively. All experiments were run on a dual 2.5
GHz Apple G5 with 4 GB of RAM.
The first factor that we examined was how each algorithm scales with the number of points.
We generated random data with 5 known rectangles and N additional random points and
computed the average wall-clock running time (over ten trials) for each algorithm. The
results, shown in Table 2, show a graceful scaling of all of the multiple tree algorithms. In
contrast, the brute force and single tree algorithms run into trouble as the number of points
becomes moderately large. The variable tree algorithm consistently performs the best, as it
is able to avoid significant amounts of redundant computation.
One potential drawback of the full multiple tree algorithm is that since it branches on all
points, it may become inefficient as the allowable number of missing support points grows.
To test this we looked at 3-dimensional data and varied the minimum number of required
support points k. As shown in Table 3, all multiple tree methods become more expensive
as the number of required support points decreases. This is especially the case for the
multi-tree algorithm, which has to perform several almost identical searches to account for
missing points. However, the variable-tree algorithm?s performance degrades gracefully
and is the best for all trials.

7 Conclusions
Tree-based spatial algorithms provide the potential for significant computational savings
with multiple tree algorithms providing further opportunities to exploit structure in the
data. However, a distinct trade-off exists between ignoring structure from all aspects of
the problem and increasing the combinatorics of the search. We presented a variable tree
approach that exploits the advantages of both single tree and multiple tree algorithms. A
combinatorial search is carried out over just the minimum number of model points, while
still tracking the feasibility of the various support points. As shown in the above experiments, this approach provides significant computational savings over both the traditional
single tree and and multiple tree searches. Finally, it is interesting to note that the dynamic
support technique described in this paper is general and may be applied to a range of other
algorithms, such as the Fast Hough Transform [10], that maintain information on which
points support a given model.
Acknowledgments
Jeremy Kubica is supported by a grant from the Fannie and John Hertz Foundation. Andrew
Moore and Andrew Connolly are supported by a National Science Foundation ITR grant
(CCF-0121671).

References
[1] A.J. Storkey, N.C. Hambly, C.K.I. Williams, and R.G. Mann. Renewal Strings for
Cleaning Astronomical Databases. In UAI 19, 559-566, 2003.
[2] M.A. Fischler and R.C. Bolles. Random Sample Consensus: A Paradigm for Model
Fitting with Applications to Image Analysis and Automated Cartography. Comm. of
the ACM, 24:381?395, 1981.
[3] S. Blackman and R. Popoli. Design and Analysis of Modern Tracking Systems. Artech
House, 1999.
[4] J.L. Bentley . Multidimensional Binary Search Trees Used for Associative Searching.
Comm. of the ACM, 18 (9), 1975.
[5] J. K. Uhlmann. Algorithms for multiple-target tracking.
80(2):128?141, 1992.

American Scientist,

[6] P. V. C. Hough. Machine analysis of bubble chamber pictures. In International Conference on High Energy Accelerators and Instrumentation. CERN, 1959.
[7] A. Gray and A. Moore. N-body problems in statistical learning. In T. K. Leen and
T. G. Dietterich, editors, Advances in Neural Information Processing Systems. MIT
Press, 2001.
[8] G. R. Hjaltason and H. Samet. Incremental distance join algorithms for spatial
databases. In Proc. of the 1998 ACM-SIGMOD Conference, 237?248, 1998.
[9] J. Kubica, A. Moore, A. Connolly, and R. Jedicke. A Multiple Tree Algorithm for the
Efficient Association of Asteroid Observations. In KDD?05. August 2005.
[10] H. Li, M.A. Lavin, and R.J. Le Master. Fast Hough Transform: A Hierarchical
Approach. In Computer Vision, Graphics, and Image Processing, 36(2-3):139?161,
November 1986.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 555-extracting-and-learning-an-unknown-grammar-with-recurrent-neural-networks.pdf

Extracting and Learning an Unknown Grammar with
Recurrent Neural Networks
C.L.Gnes?, C.B. Miller
NEC Research Institute
4 Independence Way
Princeton. NJ. 08540
giles@research.nj.nec.COOl

D. Chen, G.Z. Sun, B.H. Chen, V.C. Lee
*Institute for Advanced Computer Studies
Dept of Physics and Astronomy
University of Maryland
College pm, Mel 20742

Abstract
Simple secood-order recurrent netwoIts are shown to readily learn sman brown
regular grammars when trained with positive and negative strings examples. We
show that similar methods are appropriate for learning unknown grammars from
examples of their strings. TIle training algorithm is an incremental real-time, recurrent learning (RTRL) method that computes the complete gradient and updates
the weights at the end of each string. After or during training. a dynamic clustering
algorithm extracts the production rules that the neural network has learned.. TIle
methods are illustrated by extracting rules from unknown deterministic regular
grammars. For many cases the extracted grammar outperforms the neural net from
which it was extracted in correctly classifying unseen strings.

1 INTRODUCTION
For many reasons, there has been a long interest in "language" models of neural netwoIts;
see [Elman 1991] for an excellent discussion. TIle orientation of this work is somewhat different TIle focus here is on what are good measures of the computational capabilities of
recurrent neural networks. Since currently there is little theoretical knowledge, what problems would be "good" experimental benchmarks? For discrete i.q>uts, a natural choice
would be the problem of learning fonnal grammars - a "hard" problem even for regular
grammars [Angluin, Smith 1982]. Strings of grammars can be presented one charncter at a
time and strings can be of arbitrary length. However, the strings themselves would be, for
the most part, feature independent Thus, the learning capabilities would be, for the most
part, feature independent and, therefore insensitive to feature extraction choice.
TIle learning of known grammars by recurrent neural networks has sbown promise, for ex-

ample [Qeeresman, et al1989], [Giles, et al199O, 1991, 1992], [pollack 1991], [Sun, et al
1990], [Watrous, Kuhn 1992a,b], [Williams, Zipser 1988]. But what about learning Ml!~ grammars? We demonstrate in this paper that not only can unknown grammars be
learned, but it is possible to extract the grammar from the neural network, both during and
after training. Furthennore, the extraction process requires no a priori knowledge about the

317

318

Giles, Miller, Chen, Sun, Chen, and Lee

grammar, except that the grammar's representation can be regular, which is always true for
a grammar of bounded string length; which is the grammatical "training sample."

2 FORMAL GRAMMARS
We give a brief introduction to grammars; for a more detailed explanation see [Hopcroft &
Ullman, 1979]. We define a grammar as a 4-mple (N, V, P, S) where N and V are DOOlerminal and tenninal vocabularies, P is a finite set of production rules and S is the start symbol. All grammars we discuss are detelUlinistic and regular. For every grammar there exists
a language - the set of strings the grammar generates - and an automaton - the machine that

recognizes (classifies) the grammar's strings. For regular grammars, the recognizing machine is a deterministic finite automaton (DFA). There exists a one-ta-one mapping between a DFA and its grammar. Once the DFA is known, the production rules are the
ordered triples (notk, arc, 1Wde).
Grammatical inference [Fu 1982] is defined as the problem of finding (learning) a grammar
from a finite set of strings, often called the training sample. One can interpret this problem
as devising an inference engine that learm and extracts the grammar, see Figure I.

UNKNOWN

LabeBed
striDgs

....

-

GRAMMAR

INFERENCE

.

Extraction
Process

ENGINE
(NEURAL
NETWQRKl

INFERRED
GRAMMAR

Figure I: Grammatical inference

For a training sample of positive and negative strings and no knowledge of the unknown
regular grammar, the problem is NP..complete (for a summary, see [Angluin, Smith 1982]).
It is possible to construct an inference engine that consists of a recurrent neural network and
a rule extraction process that yields an inferred grammar.

3
3.1

RECURRENT NEURAL NETWORK
ARCHITEcruRE

Our recmrent neural network is quite simple and can be considered as a simplified version

of the model by [Elman 1991]. For an excellent discussion of recurrent networks full of references that we don't have room for here, see [Hertz, et all99I].
A fairly general expression for a recunent network (which has the same computational
power as a DFA) is:
s~+ I
r

= F(Stj' I?W)
,

where F is a nonlinearity that maps the stale neuron Sl and the input neuron 1 at time t to
the next state S'+ 1at time t+ 1. The weight matrix W parameterizes the mapping and is usually leamed (however, it can be totally or partially programmed). A DFA has an analogous
mapping but does not use W. For a recurrent neural network we define the mapping F and
order of the mapping in the following manner [Lee, et aI 1986]. For a first-order recmrent
net:
where N is the number of hidden state neurons and L the number of input neurons; W ij and
Y ij are the real-valued weights for respectively the stale and input neurons; and (J is a stan-

Extracting and Learning an Unknown Grammar with Recurrent Neural Networks

N

L

S:+1 = a (7WilJ + Pi/!)
dard sigmoid discriminant function. The values of the hidden state neurons Sl are defined
in the finite N-dimensional space [O,I]N. Assuming all weights are connected and the net
is fully recurrent, the weight space complexity is bounded by O(N2+NL). Note that the input and state neurons are not the same neurons. This representation has the capability. assuming sufficiently large N and L, to represent any state machine. Note that there are nontrainable unit weights on the recurrent feedback connections.
TIle natural second-order extension of this recurrent net is:

where certain state neurons become input neurons. Note that the weights W ijk modify a
product of the hidden Sj and input Ik neurons. This quadratic fonn directly represents the
state transition diagrams of a state automata process -- (input, state) ::::) (next-state) and thus
makes the state transition mapping very easy to learn. It also pennits the net to be directly
programmed to be a particular DFA. Unpublished experiments comparing first and second
order recurrent nets confirm this ease-in-Iearning hypothesis. The space complexity (number of weights) is O(LN2). For L?N, both first- and second-order are of the same complexity,O(N2).

3.2

SUPERVISED TRAINING & ERROR FUNCTION

The error function is defined by a special recurrent output neuron which is checked at the

end of each string presentation to see if it is on or off. By convention this output neuron
should be on if the string is a positive example of the grammar and off if negative. In practice an error tolerance decides the on and off criteria; see [Giles, et all991] for detail. [If a
multiclass recognition is desired, another error scheme using many output neurons can be
constructed.] We define two error cases: (1) the networl.c fails to reject a negative string (the
output neuron is on); (2) the network fails to accept a positive string (the output neuron is
oft). This accept or reject occurs at the end of each string - we define this problem as inference versus prediction.There is no prediction of the next character in the string sequence.
As such, inference is a more difficult problem than prediction. If knowledge of the classification of every substring of every string exists and alphabetical training order is preserved, then the prediction and inference problems are equivalent.
The training method is real-time recurrent training (RTRL). For more details see [Williams,

Zipser 1988]. The error function is defined as:

E

(1/2) (Target-S~)

=

2

where Sf is the output neuron value at the final time step t=fwhen the final character is
presented and Target is the desired value of (1.0) for (positive. negative) examples. Using
gradient descent training, the weight update rule for a second-order recurrent net becomes:

W 1mn

= -aV

E

{

= a(Target-S o )

d~

. dW

lmn

where a is the learning rate. From the recursive network state equation we obtain the relationship between the derivatives of and St+l:

st

319

320

Giles, Miller, Chen, Sun, Chen, and Lee

~;

= a'?

[f>US~-lr.-l + l:W;jtt.-l~~-l J

1m"

jk

1m"

where a' is the derivative of the discriminant function. This pennits on-line learning with
partial derivatives calculated iteratively at each time step. Let "dS'=OIdWlmn = O. Note that
the space complexity is O(L2~) which can be prohibitive for large N and full connectivity.
It is important to note that for all training discussed here, the full gradient is calculated as
given above.

3.3

PRESENTATION OF TRAINING SAMPLES

The training data consists of a series of stimulus-response pairs, where the stimulus is a

string ofO's and 1's, and the response is either "I" for positive examples or "0" for negative
examples. The positive and negative strings are generated by an unknown source grammar
(created by a program that creates random grammars) prior to training. At each discrete
time step, one symbol from the string activates one input neuron, the other input neurons
are zero (one-hot encoding). Training is on-line and occurs after each string presentation;
there is no total error accumulation as in batch learning; contrast this to the batch method
of [Watrous, Kuhn 1992]. An extra end symbol is added to the string alphabet to give the
network more power in deciding the best final neuron state configuration. This requires another input neuron and does not increase the complexity of the DFA (only N 2 more
weights). The sequence of strings presented during training is very important and certainly
gives a bias in learning. We have perfonned many experiments that indicate that training
with alphabetical order with an equal distribution of positive and negative examples is
much faster and converges more often than random order presentation.
TIle training algorithm is on-line, incremental. A small portion of the training set is preselected and presented to the network. The net is trained at the end of each string presentation. Once the net has learned this small set or reaches a maximum number of epochs (set
before training, 1000 for experiments reported), a small number of strings (10) classified
incorrectly are chosen from the rest of the training set and added to the pre-selected set. This
small string increment prevents the training procedure from driving the network too far towards any local minima that the misclassified strings may represent. Another cycle of epoch training begins with the augmented training set. If the net correctly classifies all the
training data, the net is said to converge. The total number of cycles that the network is permitted to run is also limited, usually to about 20.

4 RULE EXTRACTION (DFA GENERATION)
As the network is training (or after training), we apply a procedure we call dynamic state
partitioning (dsp) for extracting the network's current conception of the DFA it is learning
or has learned. The rule extraction process has the following steps: 1) clustering of DFA
states, 2) constructing a transition diagram by connecting these states together with the alphabet-labelled transitions, 3) putting these transitions together to make the full digraph fonning cycles, and 4) reducing the digraph to a minimal representation. The hypothesis is
that during training, the network begins to partition (or quantize) its state space into fairly
well-separated, distinct regions or clusters, which represent corresponding states in some
DFA. See [Cleeremans, et al1989] and [Watrous and Kuhn 1992a] for other clustering
methods. A simple way of finding these clusters is to divide each neuron's range [0,1] into
q partitions of equal size. For N state neurons, qN partitions. For example, for q=2, the values of S'~.5 are 1 and S'<.0.5 are 0 and there are 2N regions with 2N possible values. Thus
for N hidden neurons, there exist possible regions. The DFA is constructed by generating

I'

Extracting and Learning an Unknown Grammar with Recurrent Neural Networks

a state transition diagram -- associating an input symbol with a set of hidden neuron partitions that it is currently in and the set of neuron partitions it activates. This ordered triple
is also a production rule. The initial partition, or start state of the DFA, is detennined from
the initial value of St=O. If the next input symbol maps to the same partition we assume a
loop in the DFA. Otherwise, a new state in the DFA is fonned.This constructed DFA may
contain a maximum of states; in practice it is usually much less, since not all neuron partition sets are ever reached. This is basically a tree pruning method and different DFA could
be generated based on the choice of branching order. TIle extracted DFA can then be reduced to its minimal size using standard minimization algorithms (an 0(N2) algorithm
where N is the number of DFA states) [Hopcroft, Ullman 1979]. [This minimization procedure does not change the grammar of the DFA; the unminimized DFA has same time
complexity as the minimized DFA. TIle process just rids the DFA of redundant, unnecessary states and reduces the space complexity.] Once the DFA is known, the production rules
are easily extracted.

cf

Since many partition values of q are available, many DFA can be extracted. How is the q
that gives the best DFA chosen? Or viewed in another way, using different q, what DFA
gives the best representation of the grammar of the training set? One approach is to use different q's (starting with q=2), different branching order, different runs with different numbers of neurons and different initial conditions, and see if any similar sets of DFA emerge.
Choose the DFA whose similarity set has the smallest number of states and appears most
often - an Occam's razor assumption. Define the guess of the DFA as DFAg.This method
seems to woIk fairly well. Another is to see which of the DFA give the best perfonnance
on the training set, assuming that the training set is not perfectly learned. We have little experience with this method since we usually train to perfection on the training set It should
be noted that this DFA extraction method may be applied to any discrete-time recurrent
net, regardless of network order or number of hidden layers. Preliminary results on firstorder recurrent networks show that the same DFA are extracted as second-order, but the
first-order nets are less likely to converge and take longer to converge than second-order.

5 SIMULATIONS - GRAMMARS LEARNED
Many different small ? 15 states) regular known grammars have been learned successfully
with both first-order [Cleeremans, et al1989] and second-order recurrent models [Giles, et
al 91] and [Watrous, Kuhn 1992a]. In addition [Giles, et al1990 & 1991] and [Watrous,
Kuhn 1992b] show how corresponding DFA and production rules can be extracted. However for all of the above work, the grammars to be learned were alreatb known. What is
more interesting is the learning of unknown grammars.
In figure 2b is a randomly generated minimallO-state regular grammar created by a program in which the only inputs are the number of states of the umninimized DFA and the
alphabet size p. (A good estimate of the number of possible unique DFA is (n2lln1'"/n!)
[Aton, et al1991] where n is number ofDFA states) TIle shaded state is the start state, filled
and dashed arcs represent 1 and 0 transitions and all final states have a shaded outer circle.
This unknown (honestly, we didn't look) DFA was learned with both 6 and 10 hidden state
neuron second-order recurrent nets using the first 1000 strings in alphabetical training order
(we could ask the unknown grammar for strings). Of two runs for both 10 and 6 neurons,
both of the 10 and one of the 6 converged in less than 1000 epochs. (TIle initial weights
were all randomly chosen between [1,-1] and the learning rate and momentum were both
0.5.) Figure 2a shows one of the unminimized DFA that was extracted for a partition parameter of q=2. The minimized 10-state DFA, figure 3b, appeared for q=2 for one 10 neuron net and for q=2,3,4 of the converged 6 neuron net Consequently, using our previous
criteria, we chose this DFA as DFAg, our guess at the unknown grammar. We then asked

321

322

Giles, Miller, Chen, Sun, Chen, and Lee

Figures 2a & 2b. Unminimized and minimized 100state random grammar.
the program what the grammar was and discovered we were correct in our guess. The other
minimized DFA for different q's were all unique and usually very large (number of states
> 1(0).
The trained recurrent nets were then checked for generalization errors on all strings up to
length 15. All made a small number of errors, usually less than 1% of the total of 65,535
strings. However, the correct extracted DFA was perfect and, of course, makes no errors on
strings of any length. Again, [Giles, et a11991, 1992], the extracted DFA outperforms the
trained neural net from which the DFA was extracted.
Figures 3a and 3b, we see the dynamics ofDFA extraction as a 4 bidden neuron neural network is leaming as a function of epoch and partition size. This is for grammar Tomita-4
[Giles, et al 1991, 1992]] - a 4-state grammar that rejects any string which has more than
three 0' s in a row. The number of states of the extracted DFA starts out small, then increases, and finally decreases to a constant value as the grammar is learned As the partition q of
the neuron space increases, the number of minimized and unminimized states increases.
When the grammar is learned, the number of minimized states becomes constant and, as
expected, the number of minimized states, independent of q, becomes the number of states
in the grammar's DFA - 4.

6 CONCLUSIONS
Simple recurrent neural networks are capable ofleaming small regular unknown grammars
rather easily and generalize fairly well on unseen grammatical strings. The training results
are fairly independent of the initial values of the weights and numbers of neurons. For a
well-trained neural net, the generalization perfonnance on long unseen strings can be perfect.

Extracting and Learning an Unknown Grammar with Recurrent Neural Networks
Unminbnlzed

Minimized

3S
30

J
fIJ

~
~
]
Col
II

~

r;iI

3S

triangles q=4

30

11

25

25

fIJ

~
]

20

~

15

..
Col

e

10

~

r;iI

5

o

20
15
10
5

o 10 20 30 40 SO 60 70
E~b

04-~--~-r~r-'-~--~

0

10

20

30

40

SO

60

70

E~b

Figures 3a & 3b. Size of number of states (unmioimized and minimized) ofDFA
versus training epoch for different partition parameter q. The correct state size is 4.
A heuristic algorithm called dynamic state partitioning was created to extract detenninistic
finite state automata (DFA) from the neural network, both during and after training. Using
a standard DFA minimization algorithm, the extracted DFA can be reduced to an equivalent
minimal-state DFA which has reduced space (not time) complexity. When the source or
generating grammar is unknown, a good guess of the unknown grammar DFAg can be obtained from the minimal DFA that is most often extracted from different runs WIth different
numbers of neurons and initial conditions. From the extracted DFA, minimal or not, the
production rules of the learned grammar are evident.
There are some interesting aspects of the extracted DFA. Each of the unminimized DFA
seems to be unique, even those with the same number of states. For recunent nets that converge, it is often possible to extract DFA that are perfect, i.e. the grammar of the unknown
source grammar. For these cases all unminimized DFA whose minimal sizes have the same
number of states constitute a large equivalence class of neural-net-generated DFA. and
have the same performance on string classification.This equivalence class extends across
neural networks which vary both in size (number of neurons) and initial conditions. Thus.
the extracted DFA gives a good indication of how well the neural network learns the grammar.

In fact, for most of the trained neural nets, the extracted DF~ outperforms the
trained neural networks in classification of unseen strings. (By aefinition, a perfect
DFA will correctly classify all unseen strings). This is not surprising due to the possibility
of error accumulation as the neural network classifies long unseen strings [pollack 1991].
However, when the neural network has leamed the grammar well, its generalization performance can be perfect on all strings tested [Giles, et al1991, 1992]. Thus, the neural network
can be considered as a tool for extracting a DFA that is representative of the unknown
grammar. Once the DFAg is obtained, it can be used independently of the trained neural
network.
The learning of small DFA using second-order techniques and the full gradient computation reported here and elsewhere [Giles, et all991, 1992], [Watrous, Kuhn 1992a, 1992b]
give a strong impetus to using these techniques for learning DFA. The question of DFA
state capacity and scalability is unresolved. Further work must show how well these ap-

323

324

Giles, Miller, Chen, Sun, Chen, and Lee

proaches can model grammars with large numbers of states and establish a theoretical and
experimental relationship between DFA state capacity and neural net size.
Acknowledgments
TIle authors acknowledge useful and helpful discussions with E. Baum, M. Goudreau, G.
Kuhn, K. Lang, L. Valiant, and R. Watrous. The University of Maryland authors gratefully

acknowledge partial support from AFOSR and DARPA.

References
N. Alon, A.K. Dewdney, and T.J.Ott, 'Efficient Simulation of Fmite Automata by Neural
Nets, Journal of the ACM, Vol 38,p. 495 (1991).
D. Angluin, C.H. Smith, Inductive Inference: Theory and Methods, ACM Computing Surveys, Vol 15, No 3, p. 237, (1983).
A. Cleeremans, D. Servan-Scbreiber, J. McClelland, Finite State Automata and Simple Recurrent Recurrent Networks, Neural Computation, Vol 1, No 3, p. 372 (1989).
lL. Elman, Distributed Representations, Simple Recurrent Networks, and Grammatical
Structure, Machine Learning, Vol 7, No 2{3, p. 91 (1991).
K.S. Fu, Syntactic Panern Recognition and Applications, Prentice-Hall, Englewood Cliffs,
NJ. Ch10 (1982).
C.L. Giles, G.Z. Sun, H.H. Chen, Y.C. Lee, D. Olen, Higher Order Recurrent Networks &
Grammatical Inference, Advances in Neural Information Systems 2, D.S. Touretzky (ed),
Morgan Kaufmann, San Mateo, Ca, p.380 (1990).
C.L. Giles, D. Chen, C.B. Miller, H.H. Chen, G.Z. Sun, Y.C. Lee, Grammatical Inference
Using Second-Order Recurrent Neural Networks, Proceedings of the International Joint
Conference on Neural Networks, IEEE91CH3049-4, Vol 2, p.357 (1991).
C.L. Giles, C.B. Miller, D. Chen, H.H. Chen, G.Z. Sun, Y.C. Lee, Learning and Extracting
Finite State Automata with Second-Order Recurrent Neural Networks, Neural Computation, accepted for publication (1992).
J. Hertz, A. Krogh, R.G. Palmer, Introduction to the Theory of Neural Computation, Addison-Wesley, Redwood City, Ca., Ch. 7 (1991).
J.E. Hopcroft, J.D. Ullman, Introduction to Automata Theory, Languages, and Computation, Addison Wesley, Reading, Ma. (1979).
Y.C. Lee, G. Doolen, H.H. Olen, G.Z. Sun, T. Maxwell, H.Y. Lee, C.L. Giles, Machine
Learning Using a Higher Order Correlational Network, PhysicaD, Vol 22-D, Nol-3, p. 276
(1986).
J .B. Pollack, The Induction of Dynamical Recognizers, Machine Learning, Vol 7, No 2/3,
p. 227 (1991).
G.Z. Sun, H.H. Chen, C.L. Giles, Y.C. Lee, D. Chen, Connectionist Pushdown Automata
that Learn Context-Free Grammars, Proceedings of the International Joint Conference on
Neural, Washington D.C., Lawrence Erlbaum Pub., Vol It p. 577 (1990).
R.L. Watrous, G.M. Kuhn, Induction of Finite-State Languages Using Second-Order Recurrent Networks, Neural Computation, accepted for publication (l992a) and these proceedings, (1992b).
RJ. Williams, D. Zipser, A Learning Algorithm for Continually Running Fully Recurrent
Neural Networks, Neural Computation, Vol 1, No 2, p. 270, (1989).


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 2271-dynamic-structure-super-resolution.pdf

Dynamic Structure Super-Resolution

Amos J Storkey
Institute of Adaptive and Neural Computation
Division of Informatics and Institute of Astronomy
University of Edinburgh
5 Forrest Hill, Edinburgh UK
a.storkey@ed.ac.uk

Abstract
The problem of super-resolution involves generating feasible higher
resolution images, which are pleasing to the eye and realistic, from
a given low resolution image. This might be attempted by using simple filters for smoothing out the high resolution blocks or
through applications where substantial prior information is used
to imply the textures and shapes which will occur in the images.
In this paper we describe an approach which lies between the two
extremes. It is a generic unsupervised method which is usable in
all domains, but goes beyond simple smoothing methods in what it
achieves. We use a dynamic tree-like architecture to model the high
resolution data. Approximate conditioning on the low resolution
image is achieved through a mean field approach.

1

Introduction

Good techniques for super-resolution are especially useful where physical limitations
exist preventing higher resolution images from being obtained. For example, in
astronomy where public presentation of images is of significant importance, superresolution techniques have been suggested. Whenever dynamic image enlargement
is needed, such as on some web pages, super-resolution techniques can be utilised.
This paper focuses on the issue of how to increase the resolution of a single image
using only prior information about images in general, and not relying on a specific
training set or the use of multiple images.
The methods for achieving super-resolution are as varied as the applications. They
range from simple use of Gaussian or preferably median filtering, to supervised
learning methods based on learning image patches corresponding to low resolution
regions from training data, and effectively sewing these patches together in a consistent manner. What method is appropriate depends on how easy it is to get suitable
training data, how fast the method needs to be and so on. There is a demand for
methods which are reasonably fast, which are generic in that they do not rely on
having suitable training data, but which do better than standard linear filters or
interpolation methods.
This paper describes an approach to resolution doubling which achieves this. The

method is structurally related to one layer of the dynamic tree model [9, 8, 1] except
that it uses real valued variables.

2

Related work

Simple approaches to resolution enhancement have been around for some time.
Gaussian and Wiener filters (and a host of other linear filters) have been used for
smoothing the blockiness created by the low resolution image. Median filters tend
to fare better, producing less blurry images. Interpolation methods such as cubicspline interpolation tend to be the most common image enhancement approach.
In the super-resolution literature there are many papers which do not deal with the
simple case of reconstruction based on a single image. Many authors are interested
in reconstruction based on multiple slightly perturbed subsamples from an image [3,
2] . This is useful for photographic scanners for example. In a similar manner other
authors utilise the information from a number of frames in a temporal sequence [4].
In other situations highly substantial prior information is given, such as the ground
truth for a part of the image. Sometimes restrictions on the type of processing
might be made in order to keep calculations in real time or deal with sequential
transmission.
One important paper which deals specifically with the problem tackled here is by
Freeman, Jones and Pasztor [5]. They follow a supervised approach, learning a
low to high resolution patch model (or rather storing examples of such maps),
and utilising a Markov random field for combining them and loopy propagation
for inference. Later work [6] simplifies and improves on this approach. Earlier
work tackling the same problem includes that of Schultz and Stevenson [7], which
performed an MAP estimation using a Gibbs prior.
There are two primary difficulties with smoothing (eg Gaussian, Wiener, Median
filters) or interpolation (bicubic, cubic spline) methods. First smoothing is indiscriminate. It occurs both within the gradual change in colour of the sky, say, as well
as across the horizon, producing blurring problems. Second these approaches are
inconsistent: subsampling the super-resolution image will not return the original
low-resolution one. Hence we need a model which maintains consistency but also
tries to ensure that smoothing does not occur across region boundaries (except as
much is as needed for anti-aliasing).

3

The model

Here the high-resolution image is described by a series of very small patches with
varying shapes. Pixel values within these patches can vary, but will have a common
mean value. Pixel values across patches are independent. Apriori exactly where
these patches should be is uncertain, and so the pixel to patch mapping is allowed
to be a dynamic one.
The model is best represented by a belief network. It consists of three layers. The
lowest layer consists of the visible low-resolution pixels. The intermediate layer is a
high-resolution image (4 ? 4 the size of the low-resolution image). The top layer is
a latent layer which is a little more than 2 ? 2 the size of the low resolution image.
The latent variables are ?positioned? at the corners, centres and edge centres of
the pixels of the low resolution image. The values of the pixel colour of the high
resolution nodes are each a single sample from a Gaussian mixture (in colour space),
where each mixture centre is given by the pixel colour of a particular parent latent

Latent

Hi Res

Low Res

Figure 1: The three layers of the model. The small boxes in the left figure (64 of
them) give the position of the high resolution pixels relative to the low resolution
pixels (the 4 boxes with a thick outline). The positions of the latent variable nodes
are given by the black circles. The colour of each high resolution pixel is generated
from a mixture of Gaussians (right figure), each Gaussian centred at its latent
parent pixel value. The closer the parent is, the higher the prior probability of
being generated by that mixture is.
variable node. The prior mixing coefficients decay with distance in image space
between the high-resolution node and the corresponding latent node.
Another way of viewing this is that a further indicator variable can be introduced
which selects which mixture is responsible for a given high-resolution node. We say
a high resolution node ?chooses? to connect to the parent that is responsible for it,
with a connection probability given by the corresponding mixing coefficient. These
connection probabilities can be specified in terms of positions (see figure 2).
The motivation for this model comes from the possibility of explaining away. In
linear filtering methods each high-resolution node is determined by a fixed relationship to its neighbouring low-resolution nodes. Here if one of the latent variables
provides an explanation for a high-resolution node which fits well with it neighbours
to form the low-resolution data, then the posterior responsibility of the other latent
nodes for that high-resolution pixel is reduced, and they are free to be used to model
other nearby pixels. The high-resolution pixels corresponding to a visible node can
be separated into two (or more) independent regions, corresponding to pixels on
different sides of an edge (or edges). A different latent variable is responsible for
each region. In other words each mixture component effectively corresponds to a
small image patch which can vary in size depending on what pixels it is responsible
for.
Let vj ? L denote a latent variable at site j in the latent space L. Let xi ? S
denote the value of pixel i in high resolution image space S, and let yk denote the
value of the visible pixel k. Each of these is a 3-vector representing colour. Let V
denote the ordered set of all vj . Likewise X denotes the ordered set of all xi and Y
the set of all yi . In all the work described here a transformed colorspace of (gray,
red-green, blue-yellow) is used. In other words the data is a linear transformation
on the RGB colour values using the matrix
!
0.66 1 0.5
0.66 ?1 0.5 .
0.66 0 ?1
The remaining component is the connectivity (i.e. the indicator for the responsibility) between the high-resolution nodes and the nodes in the latent layer. Let zij

denote this connectivity with zij an indicator variable taking value 1 when vj is a
parent of xi in the belief network. Every high resolution pixel has one and only one
parent in the latent layer. Let Z denote the ordered set of all zij .
3.1

Distributions

A uniform distribution over the range of pixel values is presumed for the latent
variables. The high resolution pixels are given by Gaussian distributions centred
on the pixel values of the parental latent variable. This Gaussian is presumed
independent in each pixel component. Finally the low resolution pixels are given
by the average of the sixteen high resolution pixels covering the site of the low
resolution pixel. This pixel value can also be subject to some additional Gaussian
noise if necessary (zero noise is assumed in this paper).
It is presumed that each high resolution pixel is allowed to ?choose? its parent from
the set of latent variables in an independent manner. A pixel has a higher probability
of choosing a nearby parent than a far away one.
For this we use a Gaussian integral form so that :


Z
Y z
(rj ? r)2
ij
P (Z) =
pij where pij ?
dr exp ?
,
2?
Bi
ij

(1)

The equations for the other distributions are given here. First we have
!
m 2
Y
(xm
1
i ? vj )
P (X|Z, V ) =
exp ?zij
.
2?m
(2??m )1/2
ijm

(2)

where r is a position in the high resolution picture space, rj is the position of the
jth latent variable in the high resolution image space (where these are located at
the corners of every second pixel in each direction as described above). The integral
is over Bi defined as the region in image space corresponding to pixel xi . ? gives
the width (squared) over which the probability decays. The larger ? the more
possible parents with non-negligible probability. The connection probabilities can
be illustrated by the picture in figure 2.

where ?m is a variance which determines how much each pixel must be like its
latent parent. Here the indicator zij ensures the only contribution for each i comes
from the parent j of i. Second
!
P
2
Y
(ykm ? d1 i?P a(k) xm
1
i )
exp ?
(3)
P (Y |X) =
2?
(2??)1/2
km

Figure 2: An illustration of the connection probabilities from a high resolution pixel
in the position of the smaller checkered square to the latent variables centred at each
of the larger squares. The probability is proportional to the intensity of the shading:
darker is higher probability.

with P a(k) denoting the set of all the d = 16 high resolution pixels which go to
make up the low resolution pixel yk . In this work we let the variance ? ? 0.
? determines the additive Gaussian noise which is in the low resolution image.
Last, P (V ) is simply uniform over the whole of the possible values of V . Hence
P (V ) = 1/C for C the volume of V space being considered.
3.2

Inference

The belief network defined above is not tree structured (rather it is a mixture of tree
structures) and so we have to resort to approximation methods for inference. In this
paper a variational approach is followed. The posterior distribution is approximated
using a factorised distribution over the latent space and over the connectivity. Only
in the high resolution space X do we consider joint distributions: we use a joint
Gaussian for all the nodes corresponding to one low resolution pixel. The full
distribution can be written as Q(Z, V, X) = Q(Z)Q(V )Q(X) where
!
Y z
Y
(vjm ? ?jm )2
1
ij
Q(Z) =
qij ,
Q(V ) =
exp ?
and (4)
1/2
2(?m
(2??m
j )
j )
ij
jm


Y (2?)?d/2
1 ? m
? m T
m ?1
? m
? m
Q(X) =
exp ? [(x )k ? (? )k ] (?k ) [(x )k ? (? )k ]
(5)
1/2
2
|?m
k |
km

m
where (x? )m
k is the vector (xi |i ? P a(k)), the joint of all d high resolution pixel
values corresponding to a given low resolution pixel k (for a given colour component
m
m
m
m). Here qij , ?m
i , ?j , ?j and ?i are variational parameters to be optimised.

As usual, a local minima the KL divergence between the approximate distribution
and the true posterior distribution is computed. This is equivalent to maximising
the negative variational free energy (or variational log likelihood)


Q(Z, V, X)
(6)
L(Q||P ) = log
P (Z, V, X, Y ) Q(Z,V,X)

where Y is given by the low resolution image. In this case we obtain
L(Q||P ) = hlog Q(Z) ? log P (Z)iQ(Z) + hlog Q(V ) ? log p(V )iQ(V )

+ hlog Q(X)iQ(X) ? hlog P (X|Z, V )iQ(X,Z,V ) ? hlog P (Y |X)iQ(Y,X) . (7)
Taking expectations and derivatives with respect to each of the parameters in the
approximation gives a set of self-consistent mean field equations which we can solve
by repeated iteration. Here for simplicity we only solve for qij and for the means ?m
i
and ?jm which turn out to be independent of the variational variance parameters.
We obtain
P
m
X
m
m
i qij xi
?jm = P
and ?m
qij vim
(8)
i = ?i + Dc(i) where ?i =
q
ij
i
j

where c(i) is the child of i, i.e. the low level pixel which i is part of. Dk is
a Lagrange multiplier, and is obtained through constraining the high level pixel
values to average to the low level pixels:
1 X m
1 X
?
m
m
?m
?i
(9)
i = y k ? Dk ? Dk = y k ?
d
d
i?P a(k)

i?P a(k)

In the case where ? is non-zero, this constraint is softened and Dk is given by
Dk = ?Dk? /(? + ?). The update for the qij is given by
!
X (xm ? v m )2
i
k
qij ? pij exp ?
(10)
2?m
m

where the constant of proportionality is given by normalisation:

P

j qij

= 1.

Optimising the KL divergence involves iterating these equations. For each Q(Z)
optimisation (10), equations (8a) and (8b) are iterated a number of times. Each
optimisation loop is either done a preset number of times, or until a suitable convergence criterion is met. The former approach is generally used, as the basic criterion
is a limit on the time available for the optimisation to be done.

4

Setting parameters

The prior variance parameters need to be set. The variance ? corresponds to the
additive noise. If this is not known to be zero, then it will vary from image to image,
and needs to be found for each image. This can be done using variational maximum
likelihood, where ? is set to maximise the variational log likelihood. ? is presumed
to be independent of the images presented, and is set by hand by visualising changes
on a test set. The ?m might depend on the intensity levels in the image: very dark
images will need a smaller value of ?1 for example. However for simplicity ?m = ?
is treated as global and set by hand. Because the primary criterion for optimal
parameters is subjective, this is the most sensible approach, and is reasonable when
there are only two parameters to determine. To optimise automatically based on
the variational log likelihood is possible but does not produce as good results due to
the complicated nature of a true prior or error-measure for images. For example, a
highly elaborate texture offset by one pixel will give a large mean square error, but
look almost identical, whereas a blurred version of the texture would give a smaller
mean square error, but look much worse.

5

Implementation

The basic implementation involves setting the parameters, running the mean field
optimisation and then looking at the result. The final result is a downsampled
version of the 4 ? 4 image to 2 ? 2 size: the larger image is used to get reasonable
anti-aliasing.
To initialise the mean field optimisation, X is set equal to the bi-cubic interpolated
image with added Gaussian noise. The Q(Z) is initialised to P (Z). Although in
the examples here we used 25 optimisations Q(Z), each of which involves 10 cycles
through the mean field equations for Q(X) and Q(V ), it is possible to get reasonable
results with only three Q(Z) optimisation cycles each doing 2 iterations through
the mean field equations. In the runs shown here, ? is set to zero, the variance ?
is set to 0.008, and ? is set to 3.3.

6

Demonstrations and assessment

The method described in this paper is compared with a number of simple filtering and interpolation methods, and also with the methods of Freeman et al.
The image from Freeman?s website is used for comparison with that work (figure 3). Full colour comparisons for these and other images can be found at
http://www.anc.ed.ac.uk/~amos/superresolution.html. First two linear filtering approaches are considered, the Wiener filter and a Gaussian filter. The third
method is a median filter. Bi-cubic interpolation is also given.
Quantitative assessment of the quality of super-resolution results is always something of a difficulty because the basic criterion is human subjectivity. Even so we

(a)

(b)

(c)

(d)

(e)

(f)

Figure 3: Comparison with approach of Freeman et al. (a) gives the 70x70 low resolution image, (b) the true image, (c) a bi-cubic interpolation (d) Freeman et al result
(taken from website and downsampled), (e) dynamic structure super-resolution, (f)
median filter.

compare the results of this approach with standard filtering methods using a root
mean squared pixel error on a set of 8, 128 by 96 colour images, giving 0.0486, 0.0467,
0.0510 and 0.0452 for the original low resolution image, bicubic interpolation, the
median filter and dynamic structure super-resolution respectively. Unfortunately
the unavailability of code prevents representative calculations for the Freeman et al
approach. Dynamic structure resolution requires approximately 30 ? 60 flops per
2 ? 2 high resolution pixel per optimisation cycle, compared with, say, 16 flops for
a linear filter, so it is more costly. Trials have been done working directly with
2 ? 2 grids rather than with 4 ? 4 and then averaging up. This is much faster and
the results, though not quite as good, were still an improvement on the simpler
methods.
Qualitatively, the results for dynamic structure super-resolution are significantly
better than most standard filtering approaches. The texture is better represented
because it maintains consistency, and the edges are sharper, although there is still
some significant difference from the true image. The method of Freeman et al
is perhaps comparable at this resolution, although it should be noted that their
result has been downsampled here to half the size of their enhanced image. Their
method can produce 4 ? 4 the resolution of the original, and so this does not
accurately represent the full power of their technique. Furthermore this image is
representative of early results from their work. However their approach does require
learning large numbers of patches from a training set. Fundamentally the dynamic
structure super-resolution approach does a good job at resolution doubling without
the need for representative training data. The edges are not blurred and much of
the blockiness is removed.
Dynamic structure super-resolution provides a technique for resolution enhancement, and provides an interesting starting model which is different from the Markov
random field approaches. Future directions could incorporate hierarchical frequency
information at each node rather than just a single value.

References
[1] N. J. Adams. Dynamic Trees: A Hierarchical Probabilistic Approach to Image Modelling. PhD thesis, Division of Informatics, University of Edinburgh, 5 Forrest Hill,
Edinburgh, EH1 2QL, UK, 2001.
[2] S. Baker and T. Kanade. Limits on super-resolution and how to break them. In
Proceedings of CVPR 00, pages 372?379, 2000.
[3] P. Cheeseman, B. Kanefsky, R. Kraft, and J. Stutz. Super-resolved surface reconstruction from multiple images. Technical Report FIA-94-12, NASA Ames, 1994.
[4] M. Elad and A. Feuer. Super-resolution reconstruction of image sequences. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 21(9):817?834, 1999.
[5] W. T. Freeman, T. R. Jones, and E. C. Pasztor. Markov networks for super-resolution.
Technical Report TR-2000-08, MERL, 2000.
[6] W. T. Freeman, T. R. Jones, and E. C. Pasztor. Example-based super-resolution.
IEEE Computer Graphics and Applications, 2002.
[7] R. R. Schultz and R. L. Stevenson. A Bayesian approach to image expansion for
improved definition. IEEE Transactions on Image Processing, 3:233?242, 1994.
[8] A. J. Storkey. Dynamic trees: A structured variational method giving efficient propagation rules. In C. Boutilier and M. Goldszmidt, editors, Uncertainty in Artificial
Intelligence, pages 566?573. Morgan Kauffmann, 2000.
[9] C. K. I. Williams and N. J. Adams. DTs: Dynamic trees. In M. J. Kearns, S. A. Solla,
and D. A. Cohn, editors, Advances in Neural Information Processing Systems 11. MIT
Press, 1999.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 6475-a-scalable-end-to-end-gaussian-process-adapter-for-irregularly-sampled-time-series-classification.pdf

A scalable end-to-end Gaussian process adapter for
irregularly sampled time series classification

Steven Cheng-Xian Li
Benjamin Marlin
College of Information and Computer Sciences
University of Massachusetts Amherst
Amherst, MA 01003
{cxl,marlin}@cs.umass.edu

Abstract
We present a general framework for classification of sparse and irregularly-sampled
time series. The properties of such time series can result in substantial uncertainty
about the values of the underlying temporal processes, while making the data
difficult to deal with using standard classification methods that assume fixeddimensional feature spaces. To address these challenges, we propose an uncertaintyaware classification framework based on a special computational layer we refer to
as the Gaussian process adapter that can connect irregularly sampled time series
data to any black-box classifier learnable using gradient descent. We show how
to scale up the required computations based on combining the structured kernel
interpolation framework and the Lanczos approximation method, and how to
discriminatively train the Gaussian process adapter in combination with a number
of classifiers end-to-end using backpropagation.

1

Introduction

In this paper, we propose a general framework for classification of sparse and irregularly-sampled
time series. An irregularly-sampled time series is a sequence of samples with irregular intervals
between their observation times. These intervals can be large when the time series are also sparsely
sampled. Such time series data are studied in various areas including climate science [22], ecology
[4], biology [18], medicine [15] and astronomy [21]. Classification in this setting is challenging both
because the data cases are not naturally defined in a fixed-dimensional feature space due to irregular
sampling and variable numbers of samples, and because there can be substantial uncertainty about
the underlying temporal processes due to the sparsity of observations.
Recently, Li and Marlin [13] introduced the mixture of expected Gaussian kernels (MEG) framework,
an uncertainty-aware kernel for classifying sparse and irregularly sampled time series. Classification
with MEG kernels is shown to outperform models that ignore uncertainty due to sparse and irregular
sampling. On the other hand, various deep learning models including convolutional neural networks
[12] have been successfully applied to fields such as computer vision and natural language processing,
and have been shown to achieve state-of-the-art results on various tasks. Some of these models
have desirable properties for time series classification, but cannot be directly applied to sparse and
irregularly sampled time series.
Inspired by the MEG kernel, we propose an uncertainty-aware classification framework that enables
learning black-box classification models from sparse and irregularly sampled time series data. This
framework is based on the use of a computational layer that we refer to as the Gaussian process
(GP) adapter. The GP adapter uses Gaussian process regression to transform the irregular time series
data into a uniform representation, allowing sparse and irregularly sampled data to be fed into any
black-box classifier learnable using gradient descent while preserving uncertainty. However, the
30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

O(n3 ) time and O(n2 ) space of exact GP regression makes the GP adapter prohibitively expensive
when scaling up to large time series.
To address this problem, we show how to speed up the key computation of sampling from a GP
posterior based on combining the structured kernel interpolation (SKI) framework that was recently
proposed by Wilson and Nickisch [25] with Lanczos methods for approximating matrix functions [3].
Using the proposed sampling algorithm, the GP adapter can run in linear time and space in terms of
the length of the time series, and O(m log m) time when m inducing points are used.
We also show that GP adapter can be trained end-to-end together with the parameters of the chosen
classifier by backpropagation through the iterative Lanczos method. We present results using logistic
regression, fully-connected feedforward networks, convolutional neural networks and the MEG kernel.
We show that end-to-end discriminative training of the GP adapter outperforms a variety of baselines
in terms of classification performance, including models based only on GP mean interpolation, or
with GP regression trained separately using marginal likelihood.

2

Gaussian processes for sparse and irregularly-sampled time series

Our focus in this paper is on time series classification in the presence of sparse and irregular sampling.
In this problem, the data D contain N independent tuples consisting of a time series Si and a label
yi . Thus, D = {(S1 , y1 ), . . . , (SN , yN )}. Each time series Si is represented as a list of time points
ti = [ti1 , . . . , ti|Si | ]> , and a list of corresponding values vi = [vi1 , . . . , vi|Si | ]> . We assume that
each time series is observed over a common time interval [0, T ]. However, different time series
are not necessarily observed at the same time points (i.e. ti 6= tj in general). This implies that the
number of observations in different time series is not necessary the same (i.e. |Si | 6= |Sj | in general).
Furthermore, the time intervals between observation within a single time series are not assumed to be
uniform.
Learning in this setting is challenging because the data cases are not naturally defined in a fixeddimensional feature space due to the irregular sampling. This means that commonly used classifiers
that take fixed-length feature vectors as input are not applicable. In addition, there can be substantial
uncertainty about the underlying temporal processes due to the sparsity of observations.
To address these challenges, we build on ideas from the MEG kernel [13] by using GP regression
[17] to provide an uncertainty-aware representation of sparse and irregularly sampled time series. We
fix a set of reference time points x = [x1 , . . . , xd ]> and represent a time series S = (t, v) in terms
of its posterior marginal distribution at these time points. We use GP regression with a zero-mean
GP prior and a covariance function k(?, ?) parameterized by kernel hyperparameters ?. Let ? 2 be the
independent noise variance of the GP regression model. The GP parameters are ? = (?, ? 2 ).
Under this model, the marginal posterior GP at x is Gaussian distributed with the mean and covariance
given by
? = Kx,t (Kt,t + ? 2 I)?1 v,

(1)
2

? = Kx,x ? Kx,t (Kt,t + ? I)

?1

Kt,x

(2)

where Kx,t denotes the covariance matrix with [Kx,t ]ij = k(xi , tj ). We note that it takes O(n3 +nd)
time to exactly compute the posterior mean ?, and O(n3 + n2 d + nd2 ) time to exactly compute the
full posterior covariance matrix ?, where n = |t| and d = |x|.

3

The GP adapter and uncertainty-aware time series classification

In this section we describe our framework for time series classification in the presence of sparse
and irregular sampling. Our framework enables any black-box classifier learnable by gradient-based
methods to be applied to the problem of classifying sparse and irregularly sampled time series.
3.1

Classification frameworks and the Gaussian process adapter

In Section 2 we described how we can represent a time series through the marginal posterior it induces
under a Gaussian process regression model at any set of reference time points x. By fixing a common
2

set of reference time points x for all time series in a data set, every time series can be transformed
into a common representation in the form of a multivariate Gaussian N (z|?, ?; ?) with z being the
random vector distributed according to the posterior GP marginalized over the time points x.1 Here
we assume that the GP parameters ? are shared across the entire data set.
If the z values were observed, we could simply apply a black-box classifier. A classifier can be
generally defined by a mapping function f (z; w) parameterized by w, associated with a loss function
`(f (z; w), y) where y is a label value from the output space Y. However, in our case z is a Gaussian
random variable, which means `(f (z; w), y) is nowitself a random variable given a label y. Therefore,
we use the expectation Ez?N (?,?;?) `(f (z; w), y) as the overall loss between the label y and a time
series S given its Gaussian representation N (?, ?; ?). The learning problem becomes minimizing
the expected loss over the entire data set:
w? , ? ? = argmin
w,?

N
X



Ezi ?N (?i ,?i ;?) `(f (zi ; w), yi ) .

(3)

i=1

Once we have the optimal parameters w? and ? ? , we can make predictions on unseen data. In
general, given an unseen time series S and its Gaussian representation N (?, ?; ? ? ), we can predict
its label using (4), although in many cases this can be simplified into a function of f (z; w? ) with the
expectation taken on or inside of f (z; w? ).


y ? = argmin Ez?N (?,?;?? ) `(f (z; w? ), y)
(4)
y?Y

We name the above approach the Uncertainty-Aware Classification (UAC) framework. Importantly,
this framework propagates the uncertainty in the GP posterior induced by each time series all the way
through to the loss function. Besides, we call the transformation S 7? (?, ?) the Gaussian process
adapter, since it provides a uniform representation to connect the raw irregularly sampled time series
data to a black-box classifier.
Variations of the UAC framework can be derived by taking the expectation at various position of
f (z; w) where z ? N (?, ?; ?). Taking the expectation at an earlier stage simplifies the computation,
but the uncertainty information will be integrated out earlier as well.2 In the extreme case, if the
expectation is computed immediately followed by the GP adapter transformation, it is equivalent to
using a plug-in estimate ? for z in the loss function, `(f (Ez?N (?,?;?) [z]; w), y) = `(f (?; w), y).
We refer to this as the IMPutation (IMP) framework. The IMP framework discards the uncertainty
information completely, which further simplifies the computation. This simplified variation may be
useful when the time series are more densely sampled, where the uncertainty is less of a concern.
In practice, we can train the model using the UAC objective (3) and predict instead by IMP. In that
case, the predictions would be deterministic and can be computed efficiently without drawing samples
from the posterior GP as described later in Section 4.
3.2

Learning with the GP adapter

In the previous section, we showed that the UAC framework can be trained using (3). In this paper,
we use stochastic gradient descent to scalably optimize (3) by updating the model using a single time
series at a time, although it can be easily modified for batch or mini-batch updates.
From now on,

we will focus on the optimization problem minw,? Ez?N (?,?;?) `(f (z; w), y) where ?, ? are the
output of the GP adapter given
a time series

 S = (t, v) and its label y. For many classifiers, the
expected loss Ez?N (?,?;?) `(f (z; w), y) cannot be analytically computed. In such cases, we use
the Monte Carlo average to approximate the expected loss:
S


1X
Ez?N (?,?;?) `(f (z; w), y) ?
`(f (zs ; w), y),
S s=1

where zs ? N (?, ?; ?).

(5)

To learn the parameters of both the classifier w and the Gaussian process regression model ? jointly
under the expected loss, we need to be able to compute the gradient of the expectation given in (5).
1

The notation N (?, ?; ?) explicitly expresses that both ? and ? are functions of the GP parameters ?.
Besides, they are also functions of S = (t, v) as shown in (1) and (2).
2
For example, the loss of the expected output of the classifier `(Ez?N (?,?;?) [f (z; w)], y).

3

To achieve this, we reparameterize the Gaussian random variable using the identity z = ? + R?
where ? ? N (0, I) and R satisfies ? = RR> [11]. The gradients under this reparameterization
are given below, both of which can be approximated using Monte Carlo sampling as in (5). We will
focus on efficiently computing the gradient shown in (7) since we assume that the gradient of the
base classifier f (z; w) can be computed efficiently.




?
?
Ez?N (?,?;?) `(f (z; w), y) = E??N (0,I)
`(f (z; w), y)
(6)
?w
?w
"
#
X ?`(f (z; w), y) ?zi


?
Ez?N (?,?;?) `(f (z; w), y) = E??N (0,I)
(7)
??
?zi
??
i
There are several choices for R that satisfy ? = RR> . One common choice of R is the Cholesky
factor, a lower triangular matrix, which can be computed using Cholesky decomposition in O(d3 ) for
1
a d ? d covariance matrix ? [7]. We instead use the symmetric matrix square root R = ? /2 . We
will show that this particular choice of R leads to an efficient and scalable approximation algorithm
in Section 4.2.

4

Fast sampling from posterior Gaussian processes

The computation required by the GP adapter is dominated by the time needed to draw samples from
1
the marginal GP posterior using z = ? + ? /2 ?. In Section 2 we noted that the time complexity of
exactly computing the posterior mean ? and covariance ? is O(n3 + nd) and O(n3 + n2 d + nd2 ),
respectively. Once we have both ? and ? we still need to compute the square root of ?, which
requires an additional O(d3 ) time to compute exactly. In this section, we show how to efficiently
generate samples of z.
4.1

Structured kernel interpolation for approximating GP posterior means

The main idea of the structured kernel interpolation (SKI) framework recently proposed by Wilson
e a,b
and Nickisch [25] is to approximate a stationary kernel matrix Ka,b by the approximate kernel K
>
defined below where u = [u1 , . . . , um ] is a collection of evenly-spaced inducing points.
e a,b = Wa Ku,u W> .
Ka,b ? K
b

(8)

Letting p = |a| and q = |b|, Wa ? Rp?m is a sparse interpolation matrix where each row
contains only a small number of non-zero entries. We use local cubic convolution interpolation
(cubic interpolation for short) [10] as suggested in Wilson and Nickisch [25]. Each row of the
interpolation matrices Wa , Wb has at most four non-zero entries. Wilson and Nickisch [25] showed
that when the kernel is locally smooth (under the resolution of u), cubic interpolation results in
accurate approximation. This can be justified as follows: with cubic interpolation, the SKI kernel is
essentially the two-dimensional cubic interpolation of Ka,b using the exact regularly spaced samples
e a,b
stored in Ku,u , which corresponds to classical bicubic convolution. In fact, we can show that K
asymptotically converges to Ka,b as m increases by following the derivation in Keys [10].
Plugging the SKI kernel into (1), the posterior GP mean evaluated at x can be approximated by
?1

>
2 ?1
? = Kx,t Kt,t + ? 2 I
v ? Wx Ku,u Wt> Wt K?1
v.
(9)
u,u Wt + ? I
The inducing points u are chosen to be evenly-spaced because Ku,u forms a symmetric Toeplitz
matrix under a stationary covariance function. A symmetric Toeplitz matrix can be embedded into a
circulant matrix to perform matrix vector multiplication using fast Fourier transforms [7].
>
2 ?1
Further, one can use the conjugate gradient method to solve for (Wt K?1
v which only
u,u Wt +? I)
?1
>
2
involves computing the matrix-vector product (Wt Ku,u Wt + ? I)v. In practice, the conjugate
gradient method converges within only a few iterations. Therefore, approximating the posterior mean
? using SKI takes only O(n + d + m log m) time to compute. In addition, since a symmetric Toeplitz
matrix Ku,u can be uniquely characterized by its first column, and Wt can be stored as a sparse
matrix, approximating ? requires only O(n + d + m) space.

4

Algorithm 1: Lanczos method for approximating ? /2 ?
Input: covariance matrix ?, dimension of the Krylov subspace k, random vector ?
?1 = 0 and d0 = 0
d1 = ?/k?k
for j = 1 to k do
d = ?dj ? ?j dj?1
?
?
? 1 ?2
?j = d>
j d
? ?2 ?2 ?3
?
d = d ? ?j dj
?
?
..
?
?
.
?3 ?3
?j+1 = kdk
H = tridiagonal(?, ?, ?) = ?
?
.. ..
?
dj+1 = d/?j+1
.
. ?k ?
D = [d1 , . . . , dk ]
?k ?k
H = tridiagonal(?, ?, ?)
return k?kDH1/2 e1
// e1 = [1, 0, . . . , 0]>
1

4.2

The Lanczos method for covariance square root-vector products

With the SKI techniques, although we can efficiently approximate the posterior mean ?, computing
1
? /2 ? is still challenging. If computed exactly, it takes O(n3 + n2 d + nd2 ) time to compute ? and
O(d3 ) time to take the square root. To overcome the bottleneck, we apply the SKI kernel to the
Lanczos method, one of the Krylov subspace approximation methods, to speed up the computation
1
1
of ? /2 ? as shown in Algorithm 1. The advantage of the Lanczos method is that neither ? nor ? /2
needs to be computed explicitly. Like the conjugate gradient method, another example of the Krylov
subspace method, it only requires the computation of matrix-vector products with ? as the matrix.
The idea of the Lanczos method is to approximate ? /2 ? in the Krylov subspace Kk (?, ?) =
span{?, ??, . . . , ?k?1 ?}. The iteration in Algorithm 1, usually referred to the Lanczos process,
essentially performs the Gram-Schmidt process to transform the basis {?, ??, . . . , ?k?1 ?} into an
orthonormal basis {d1 , . . . , dk } for the subspace Kk (?, ?).
1

The optimal approximation of ? /2 ? in the Krylov subspace Kk (?, ?) that minimizes the `2 -norm
1
1
of the error is the orthogonal projection of ? /2 ? onto Kk (?, ?) as y? = DD> ? /2 ?. Since we
?
> 1/2
choose d1 = ?/k?k, the optimal projection can be written as y = k?kDD ? De1 where
e1 = [1, 0, . . . , 0]> is the first column of the identify matrix.
1

One can show that the tridiagonal matrix H defined in Algorithm 1 satisfies D> ?D = H [20]. Also,
1
we have D> ? /2 D ? (D> ?D)1/2 since the eigenvalues of H approximate the extremal eigenvalues
1
of ? [19]. Therefore we have y? = k?kDD> ? /2 De1 ? k?kDH1/2 e1 .
The error bound of the Lanczos method is analyzed in Ili?c et al. [9]. Alternatively one can show that
the Lanczos approximation converges superlinearly [16]. In practice, for a d ? d covariance matrix
?, the approximation is sufficient for our sampling purpose with k  d. As H is now a k ? k matrix,
we can use any standard method to compute its square root in O(k 3 ) time [2], which is considered
O(1) when k is chosen to be a small constant. Now the computation of the Lanczos method for
1
approximating ? /2 ? is dominated by the matrix-vector product ?d during the Lanczos process.
Here we apply the SKI kernel trick again to efficiently approximate ?d by
?1
?d ? Wx Ku,u Wx> d ? Wx Ku,u Wt> Wt Ku,u Wt> + ? 2 I
Wt Ku,u Wx> d.

(10)

Similar to the posterior mean, ?d can be approximated in O(n + d + m log m) time and linear space.
Therefore, for k = O(1) basis vectors, the entire Algorithm 1 takes O(n + d + m log m) time and
O(n + d + m) space, which is also the complexity to draw a sample from the posterior GP.
To reduce the variance when estimating the expected loss (5), we can draw multiple samples from the
1
posterior GP: {? /2 ? s }s=1,...,S where ? s ? N (0, I). Since all of the samples are associated with the
same covariance matrix ?, we can use the block Lanczos process [8], an extension to the single-vector
1
Lanczos method presented in Algorithm 1, to simultaneously approximate ? /2 ? for all S random
5

vectors ? = [? 1 , . . . , ? S ]. Similarly, during the block Lanczos process, we use the block conjugate
gradient method [6, 5] to simultaneously solve the linear equation (Wt Ku,u Wt> + ? 2 I)?1 ? for
multiple ?.

5

End-to-end learning with the GP adapter

The most common way to train GP parameters is through maximizing the marginal likelihood [17]

 n
?1
1
1
(11)
v ? log Kt,t + ? 2 I ? log 2?.
log p(v|t, ?) = ? v> Kt,t + ? 2 I
2
2
2
If we follow this criterion, training the UAC framework becomes a two-stage procedure: first we
learn GP parameters by maximizing the marginal likelihood. We then compute ? and ? given each
time series S and the learned GP parameters ? ? . Both ? and ? are then fixed and used to train the
classifier using (6).
In this section, we describe how to instead train the GP parameters discriminatively end-to-end using
backpropagation. As mentioned in Section 3, we train the UAC framework by jointly optimizing the
GP parameters ? and the parameters of the classifier w according to (6) and (7).
The most challenging part in (7) is to compute ?z = ?? + ?(? /2 ?).3 For ??, we can derive the
gradient of the approximating posterior mean (9) as given in Appendix A. Note that the gradient ??
can be approximated efficiently by repeatedly applying fast Fourier transforms and the conjugate
gradient method in the same time and space complexity as computing (9).
1

On the other hand, ?(? /2 ?) can be approximated by backpropagating through the Lanczos method
described in Algorithm 1. To carry out backpropagation, all operations in the Lanczos method must
be differentiable. For the approximation of ?d during the Lanczos process, we can similarly compute
the gradient of (10) efficiently using the SKI techniques as in computing ?? (see Appendix A).
1

The gradient ?H1/2 for the last step of Algorithm 1 can be derived as follows. From H = H1/2 H1/2 ,
we have ?H = (?H1/2 )H1/2 + H1/2 (?H1/2 ). This is known as the Sylvester equation, which has
the form of AX + XB = C where A, B, C are matrices and X is the unknown matrix to solve
for. We can compute the gradient ?H1/2 by solving the Sylvester equation using the Bartels-Stewart
algorithm [1] in O(k 3 ) time for a k ? k matrix H, which is considered O(1) for a small constant k.
Overall, training the GP adapter using stochastic optimization with the aforementioned approach
takes O(n + d + m log m) time and O(n + d + m) space for m inducing points, n observations in
the time series, and d features generated by the GP adapter.

6

Related work

The recently proposed mixtures of expected Gaussian kernels (MEG) [13] for classification of
irregular time series is probably
pthe closest work to
 ours. The random
 feature representation of the
MEG kernel is in the form of 2/m Ez?N (?,?) cos(wi> z + bi ) , which the algorithm described
in Section 4 can be applied to directly. However, by exploiting the spectral property of Gaussian
kernels,
the expected random feature of the MEG kernel is shown to be analytically computable by
p
2/m exp(?wi> ?wi /2) cos(wi> ? + bi ). With the SKI techniques, we can efficiently approximate
both wi> ?wi and wi> ? in the same time and space complexity as the GP adapter. Moreover, the
random features of the MEG kernel can be viewed as a stochastic layer in the classification network,
with no trainable parameters. All {wi , bi }i=1,...,m are randomly initialized once in the beginning and
associated with the output of the GP adapter in a nonlinear way described above.
Moreover, the MEG kernel classification is originally a two-stage method: one first estimates the
GP parameters by maximizing the marginal likelihood and then uses the optimized GP parameters
to compute the MEG kernel for classification. Since the random feature is differentiable, with the
approximation of ?? and ?(?d) described in Section 5, we can form a similar classification network
that can be efficiently trained end-to-end using the GP adapter. In Section 7.2, we will show that
training the MEG kernel end-to-end leads to better classification performance.
3

For brevity, we drop 1/?? from the gradient notation in this section.

6

time (log scale)

error

error

length of time series:
1000
2000
3000
101
100
100
10?1
10?1
10?2
10?2
10?3
10?3
3 4 5 6 7 8 9 10
0
5 10 15 20
# Lanczos iterations
log2 (# inducing points)

103
102
101
100

exact
exact BP

Lanczos
Lanczos BP

5
10 15 20 25 30
length of time series (?100)

Figure 1: Left: Sample approximation error versus the number of inducing points. Middle: Sample
approximation error versus the number of Lanczos iterations. Right: Running time comparisons (in
seconds). BP denotes computing the gradient of the sample using backpropagation.

7

Experiments

In this section, we present experiments and results exploring several facets of the GP adapter
framework including the quality of the approximations and the classification performance of the
framework when combined with different base classifiers.
7.1

Quality of GP sampling approximations

The key to scalable learning with the GP adapter relies on both fast and accurate approximation
for drawing samples from the posterior GP. To assess the approximation quality, we first generate
a synthetic sparse and irregularly-sampled time series S by sampling from a zero-mean Gaussian
process at random time points. We use the squared exponential kernel k(ti , tj ) = a exp(?b(ti ? tj )2 )
with randomly chosen hyperparameters. We then infer ? and ? at some reference x given S. Let e
z
1
denote our approximation of z = ? + ? /2 ?. In this experiment, we set the output size z to be |S|,
that is, d = n. We evaluate the approximation quality by assessing the error ke
z ? zk computed with
a fixed random vector ?.
The leftmost plot in Figure 1 shows the approximation error under different numbers of inducing
points m with k = 10 Lanczos iterations. The middle plot compares the approximation error as the
number of Lanczos iterations k varies, with m = 256 inducing points. These two plots show that the
approximation error drops as more inducing points and Lanczos iterations are used. In both plots,
the three lines correspond to different sizes for z: 1000 (bottom line), 2000 (middle line), 3000 (top
line). The separation between the curves is due to the fact that the errors are compared under the
same number of inducing points. Longer time series leads to lower resolution of the inducing points
and hence the higher approximation error.
Note that the approximation error comes from both the cubic interpolation and the Lanczos method.
Therefore, to achieve a certain normalized approximation error across different data sizes, we should
simultaneously use more inducing points and Lanczos iterations as the data grows. In practice, we
find that k ? 3 is sufficient for estimating the expected loss for classification.
The rightmost plot in Figure 1 compares the time to draw a sample using exact computation versus
the approximation method described in Section 4 (exact and Lanczos in the figure). We also compare
the time to compute the gradient with respect to the GP parameters by both the exact method and
the proposed approximation (exact BP and Lanczos BP in the figure) because this is the actual
computation carried out during training. In this part of the experiment, we use k = 10 and m = 256.
The plot shows that Lanczos approximation with the SKI kernel yields speed-ups of between 1 and
3 orders of magnitude. Interestingly, for the exact approach, the time for computing the gradient
roughly doubles the time of drawing samples. (Note that time is plotted in log scale.) This is because
computing gradients requires both forward and backward propagation, whereas drawing samples
corresponds to only the forward pass. Both the forward and backward passes take roughly the same
computation in the exact case. However, the gap is relatively larger for the approximation approach
due to the recursive relationship of the variables in the Lanczos process. In particular, dj is defined
recursively in terms of all of d1 , . . . , dj?1 , which makes the backpropagation computation more
complicated than the forward pass.
7

Table 1: Comparison of classification accuracy (in percent). IMP and UAC refer to the loss functions
for training described in Section 3.1, and we use IMP predictions throughout. Although not belonging
to the UAC framework, we put the MEG kernel in UAC since it is also uncertainty-aware.

7.2

LogReg

MLP

ConvNet

MEG kernel

Marginal likelihood

IMP
UAC

77.90
78.23

85.49
87.05

87.61
88.17

?
84.82

End-to-end

IMP
UAC

79.12
79.24

86.49
87.95

89.84
91.41

?
86.61

Classification with GP adapter

In this section, we evaluate the performance of classifying sparse and irregularly-sampled time series
using the UAC framework. We test the framework on the uWave data set,4 a collection of gesture
samples categorized into eight gesture patterns [14]. The data set has been split into 3582 training
instances and 896 test instances. Each time series contains 945 fully observed samples. Following
the data preparation procedure in the MEG kernel work [13], we randomly sample 10% of the
observations from each time series to simulate the sparse and irregular sampling scenario. In this
experiment, we use the squared exponential covariance function k(ti , tj ) = a exp(?b(ti ? tj )2 ) for
a, b > 0. Together with the independent noise parameter ? 2 > 0, the GP parameters are {a, b, ? 2 }.
To bypass the positive constraints on the GP parameters, we reparameterize them by {?, ?, ?} such
that a = e? , b = e? , and ? 2 = e? .
To demonstrate that the GP adapter is capable of working with various classifiers, we use the UAC
framework to train three different classifiers: a multi-class logistic regression (LogReg), a fullyconnected feedforward network (MLP), and a convolutional neural network (ConvNet). The detailed
architecture of each model is described in Appendix C.
We use m = 256 inducing points, d = 254 features output by the GP adapter, k = 5 Lanczos
iterations, and S = 10 samples. We split the training set into two partitions: 70% for training and
30% for validation. We jointly train the classifier with the GP adapter using stochastic gradient
descent with Nesterov momentum. We apply early stopping based on the validation set. We also
compare to classification with the MEG kernel implemented using our GP adapter as described in
Section 6. We use 1000 random features trained with multi-class logistic regression.
Table 1 shows that among all three classifiers, training GP parameters discriminatively always leads
to better accuracy than maximizing the marginal likelihood. This claim also holds for the results
using the MEG kernel. Further, taking the uncertainty into account by sampling from the posterior
GP always outperforms training using only the posterior means. Finally, we can also see that the
classification accuracy improves as the model gets deeper.

8

Conclusions and future work

We have presented a general framework for classifying sparse and irregularly-sampled time series
and have shown how to scale up the required computations using a new approach to generating
approximate samples. We have validated the approximation quality, the computational speed-ups,
and the benefit of the proposed approach relative to existing baselines.
There are many promising directions for future work including investigating more complicated
covariance functions like the spectral mixture kernel [24], different classifiers including the encoder
LSTM [23], and extending the framework to multi-dimensional time series and GPs with multidimensional index sets (e.g., for spatial data). Lastly, the GP adapter can also be applied to other
problems such as dimensionality reduction by combining it with an autoencoder.
Acknowledgements
This work was supported by the National Science Foundation under Grant No. 1350522.
4

The data set UWaveGestureLibraryAll is available at http://timeseriesclassification.com.

8

References
[1] Richard H. Bartels and GW Stewart. Solution of the matrix equation AX + XB = C. Communications
of the ACM, 15(9):820?826, 1972.
[2] ?ke Bj?rck and Sven Hammarling. A Schur method for the square root of a matrix. Linear algebra and its
applications, 52:127?140, 1983.
[3] Edmond Chow and Yousef Saad. Preconditioned krylov subspace methods for sampling multivariate
gaussian distributions. SIAM Journal on Scientific Computing, 36(2):A588?A608, 2014.
[4] J.S. Clark and O.N. Bj?rnstad. Population time series: process variability, observation errors, missing
values, lags, and hidden states. Ecology, 85(11):3140?3150, 2004.
[5] Augustin A Dubrulle. Retooling the method of block conjugate gradients. Electronic Transactions on
Numerical Analysis, 12:216?233, 2001.
[6] YT Feng, DRJ Owen, and D Peri?c. A block conjugate gradient method applied to linear systems with
multiple right-hand sides. Computer methods in applied mechanics and engineering, 1995.
[7] Gene H Golub and Charles F Van Loan. Matrix computations, volume 3. JHU Press, 2012.
[8] Gene Howard Golub and Richard Underwood. The block Lanczos method for computing eigenvalues.
Mathematical software, 3:361?377, 1977.
[9] M Ili?c, Ian W Turner, and Daniel P Simpson. A restarted Lanczos approximation to functions of a
symmetric matrix. IMA journal of numerical analysis, page drp003, 2009.
[10] Robert G Keys. Cubic convolution interpolation for digital image processing. Acoustics, Speech and Signal
Processing, IEEE Transactions on, 29(6):1153?1160, 1981.
[11] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. Proceedings of the 2nd International Conference on Learning Representations (ICLR), 2014.
[12] Yann LeCun, Fu Jie Huang, and Leon Bottou. Learning methods for generic object recognition with
invariance to pose and lighting. In Proceedings of Computer Vision and Pattern Recognition (CVPR), 2004.
[13] Steven Cheng-Xian Li and Benjmain M. Marlin. Classification of sparse and irregularly sampled time
series with mixtures of expected Gaussian kernels and random features. In 31st Conference on Uncertainty
in Artificial Intelligence, 2015.
[14] Jiayang Liu, Lin Zhong, Jehan Wickramasuriya, and Venu Vasudevan. uwave: Accelerometer-based
personalized gesture recognition and its applications. Pervasive and Mobile Computing, 2009.
[15] Benjamin M. Marlin, David C. Kale, Robinder G. Khemani, and Randall C. Wetzel. Unsupervised pattern
discovery in electronic health care data using probabilistic clustering models. In Proceedings of the 2nd
ACM SIGHIT International Health Informatics Symposium, pages 389?398, 2012.
[16] Beresford N Parlett. The symmetric eigenvalue problem, volume 7. SIAM, 1980.
[17] Carl Edward Rasmussen. Gaussian processes for machine learning. 2006.
[18] T. Ruf. The lomb-scargle periodogram in biological rhythm research: analysis of incomplete and unequally
spaced time-series. Biological Rhythm Research, 30(2):178?201, 1999.
[19] Yousef Saad. On the rates of convergence of the Lanczos and the block-Lanczos methods. SIAM Journal
on Numerical Analysis, 17(5):687?706, 1980.
[20] Yousef Saad. Iterative methods for sparse linear systems. Siam, 2003.
[21] Jeffrey D Scargle. Studies in astronomical time series analysis. ii-statistical aspects of spectral analysis of
unevenly spaced data. The Astrophysical Journal, 263:835?853, 1982.
[22] M. Schulz and K. Stattegger. Spectrum: Spectral analysis of unevenly spaced paleoclimatic time series.
Computers & Geosciences, 23(9):929?945, 1997.
[23] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In
Advances in neural information processing systems, pages 3104?3112, 2014.
[24] Andrew Gordon Wilson and Ryan Prescott Adams. Gaussian process kernels for pattern discovery and
extrapolation. In Proceedings of the 30th International Conference on Machine Learning, 2013.
[25] Andrew Gordon Wilson and Hannes Nickisch. Kernel interpolation for scalable structured Gaussian
processes (KISS-GP). In Proceedings of the 32nd International Conference on Machine Learning, 2015.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 2197-learning-to-classify-galaxy-shapes-using-the-em-algorithm.pdf

Learning to Classify Galaxy Shapes Using the
EM Algorithm

Sergey Kirshner
Information and Computer Science
University of California
Irvine, CA 92697-3425
skirshne@ics.uci.edu

Igor V. Cadez
Sparta Inc.,
23382 Mill Creek Drive #100,
Laguna Hills, CA 92653
igor cadez@sparta.com

Padhraic Smyth
Information and Computer Science
University of California
Irvine, CA 92697-3425
smyth@ics.uci.edu

Chandrika Kamath
Center for Applied Scienti?c Computing
Lawrence Livermore National Laboratory
Livermore, CA 94551
kamath2@llnl.gov

Abstract
We describe the application of probabilistic model-based learning to the
problem of automatically identifying classes of galaxies, based on both
morphological and pixel intensity characteristics. The EM algorithm can
be used to learn how to spatially orient a set of galaxies so that they
are geometrically aligned. We augment this ?ordering-model? with a
mixture model on objects, and demonstrate how classes of galaxies can
be learned in an unsupervised manner using a two-level EM algorithm.
The resulting models provide highly accurate classi?cation of galaxies in
cross-validation experiments.

1

Introduction and Background

The ?eld of astronomy is increasingly data-driven as new observing instruments permit the
rapid collection of massive archives of sky image data. In this paper we investigate the
problem of identifying bent-double radio galaxies in the FIRST (Faint Images of the Radio
Sky at Twenty-cm) Survey data set [1]. FIRST produces large numbers of radio images of
the deep sky using the Very Large Array at the National Radio Astronomy Observatory. It
is scheduled to cover more that 10,000 square degrees of the northern and southern caps
(skies). Of particular scienti?c interest to astronomers is the identi?cation and cataloging
of sky objects with a ?bent-double? morphology, indicating clusters of galaxies ([8], see
Figure 1). Due to the very large number of observed deep-sky radio sources, (on the order
of 106 so far) it is infeasible for the astronomers to label all of them manually.
The data from the FIRST Survey (http://sundog.stsci.edu/) is available in both raw image
format and in the form of a catalog of features that have been automatically derived from
the raw images by an image analysis program [8]. Each entry corresponds to a single
detectable ?blob? of bright intensity relative to the sky background: these entries are called

Figure 1: 4 examples of radio-source galaxy images. The two on the left are labelled as
?bent-doubles? and the two on the right are not. The con?gurations on the left have more
?bend? and symmetry than the two non-bent-doubles on the right.

components. The ?blob? of intensities for each component is ?tted with an ellipse. The
ellipses and intensities for each component are described by a set of estimated features such
as sky position of the centers (RA (right ascension) and Dec (declination)), peak density
?ux and integrated ?ux, root mean square noise in pixel intensities, lengths of the major and
minor axes, and the position angle of the major axis of the ellipse counterclockwise from
the north. The goal is to ?nd sets of components that are spatially close and that resemble
a bent-double. In the results in this paper we focus on candidate sets of components that
have been detected by an existing spatial clustering algorithm [3] where each set consists
of three components from the catalog (three ellipses). As of the year 2000, the catalog
contained over 15,000 three-component con?gurations and over 600,000 con?gurations
total. The set which we use to build and evaluate our models consists of a total of 128
examples of bent-double galaxies and 22 examples of non-bent-double con?gurations. A
con?guration is labelled as a bent-double if two out of three astronomers agree to label it
as such. Note that the visual identi?cation process is the bottleneck in the process since it
requires signi?cant time and effort from the scientists, and is subjective and error-prone,
motivating the creation of automated methods for identifying bent-doubles.
Three-component bent-double con?gurations typically consist of a center or ?core? component and two other side components called ?lobes?. Previous work on automated classi?cation of three-component candidate sets has focused on the use of decision-tree classi?ers
using a variety of geometric and image intensity features [3]. One of the limitations of the
decision-tree approach is its relative in?exibility in handling uncertainty about the object
being classi?ed, e.g., the identi?cation of which of the three components should be treated
as the core of a candidate object. A bigger limitation is the ?xed size of the feature vector. A primary motivation for the development of a probabilistic approach is to provide a
framework that can handle uncertainties in a ?exible coherent manner.

2

Learning to Match Orderings using the EM Algorithm

We denote a three-component con?guration by C = (c 1 , c2 , c3 ), where the ci ?s are the
components (or ?blobs?) described in the previous section. Each component c x is represented as a feature vector, where the speci?c features will be de?ned later. Our approach
focuses on building a probabilistic model for bent-doubles: p (C) = p (c1 , c2 , c3 ), the likelihood of the observed ci under a bent-double model where we implicitly condition (for
now) on the class ?bent-double.?
By looking at examples of bent-double galaxies and by talking to the scientists studying them, we have been able to establish a number of potentially useful characteristics
of the components, the primary one being geometric symmetry. In bent-doubles, two of
the components will look close to being mirror images of one another with respect to a
line through the third component. We will call mirror-image components lobe compo-

core

core

1

lobe 2

2

3

lobe 2

lobe 1

lobe 1

component 2

lobe 1
component 3

lobe 2

lobe 1

4

core

lobe 1

5
lobe 2

lobe 2

6
core

core

component 1

core

lobe 2

lobe 1

Figure 2: Possible orderings for a hypothetical bent-double. A good choice of ordering
would be either 1 or 2.
nents, and the other one the core component. It also appears that non-bent-doubles either
don?t exhibit such symmetry, or the angle formed at the core component is too straight?
the con?guration is not ?bent? enough. Once the core component is identi?ed, we can
calculate symmetry-based features. However, identifying the most plausible core component requires either an additional algorithm or human expertise. In our approach we use a
probabilistic framework that averages over different possible orderings weighted by their
probability given the data.
In order to de?ne the features, we ?rst need to determine the mapping of the components to
labels ?core?, ?lobe 1?, and ?lobe 2? (c, l1 , and l2 for short). We will call such a mapping
an ordering. Figure 2 shows an example of possible orderings for a con?guration. We can
number the orderings 1, . . . , 6. We can then write
p (C)

=

6
X

p (cc , cl1 , cl2 |? = k) p (? = k) ,

(1)

k=1

i.e., a mixture over all possible orientations. Each ordering is assumed a priori to be equally
likely, i.e., p(? = k) = 61 . Intuitively, for a con?guration that clearly looks like a bentdouble the terms in the mixture corresponding to the correct ordering would dominate,
while the other orderings would have much lower probability.
We represent each component cx by M features (we used M = 3). Note that the features
can only be calculated conditioned on a particular mapping since they rely on properties of
the (assumed) core and lobe components. We denote by fmk (C) the values corresponding
to the mth feature for con?guration C under the ordering ? = k, and by f mkj (C) we
denote the feature value of component j: fmk (C) = (fmk1 (C) , . . . , fmkBm (C)) (in our
case, Bm = 3 is the number of components). Conditioned on a particular mapping ? = k,
where x ? {c, l1 , l2 } and c,l1 ,l2 are de?ned in a cyclical order, our features are de?ned as:
? f1k (C) : Log-transformed angle, the angle formed at the center of the component
(a vertex of the con?guration) mapped to label x;
|center of x to center of next(x)|
? f2k (C) : Logarithms of side ratios, center of x to center of prev(x) ;
|
|
peak ?ux of next(x)
? f3k (C) : Logarithms of intensity ratios, peak ?ux of prev(x) ,
and so (C|? = k) = (f1k (C) , f2k (C) f3k (C)) for a 9-dimensional feature vector in total.
Other features are of course also possible. For our purposes in this paper this particular set
appears to capture the more obvious visual properties of bent-double galaxies.

For a set D = {d1 , . . . , dN } of con?gurations, under an i.i.d. assumption for con?gurations, we can write the likelihood as
P (D) =

N X
K
Y

P (?i = k) P (f1k (di ) , . . . , fM k (di )) ,

i=1 k=1

where ?i is the ordering for con?guration d i . While in the general case one can model
P (f1k (di ) , . . . , fM k (di )) as a full joint distribution, for the results reported in this paper
we make a number of simplifying assumptions, motivated by the fact that we have relatively little labelled training data available for model building. First, we assume that the
fmk (di ) are conditionally independent. Second, we are also able to reduce the number
of components for each fmk (di ) by noting functional dependencies. For example, given
two angles of a triangle, we can uniquely determine the third one. We also assume that
the remaining components for each feature are conditionally independent. Under these
assumptions the multivariate joint distribution P (f1k (di ) , . . . , fM k (di )) is factored into
a product of simple distributions, which (for the purposes of this paper) we model using
Gaussians. If we know for every training example which component should be mapped to
label c, we can then unambiguously estimate the parameters for each of these distributions.
In practice, however, the identity of the core component is unknown for each object. Thus,
we use the EM algorithm to automatically estimate the parameters of the above model.
We begin by randomly assigning an ordering to each object. For each subsequent iteration
the E-step consists of estimating a probability distribution over possible orderings for each
object, and the M-step estimates the parameters of the feature-distributions using the probabilistic ordering information from the E-step. In practice we have found that the algorithm
converges relatively quickly (in 20 to 30 iterations) on both simulated and real data. It is
somewhat surprising that this algorithm can reliably ?learn? how to align a set of objects,
without using any explicit objective function for alignment, but instead based on the fact
that feature values for certain orderings exhibit a certain self-consistency relative to the
model. Intuitively it is this self-consistency that leads to higher-likelihood solutions and
that allows EM to effectively align the objects by maximizing the likelihood.
After the model has been estimated, the likelihood of new objects can also be calculated
under the model, where the likelihood now averages over all possible orderings weighted
by their probability given the observed features.
The problem described above is a speci?c instance of a more general feature unscrambling
problem. In our case, we assume that con?gurations of three 3-dimensional components
(i.e. 3 features) each are generated by some distribution. Once the objects are generated, the
orders of their components are permuted or scrambled. The task is then to simultaneously
learn the parameters of the original distributions and the scrambling for each object. In the
more general form, each con?guration consists of L M -dimensional con?gurations. Since
there are L! possible orderings of L components, the problem becomes computationally
intractable if L is large. One solution is to restrict the types of possible scrambles (to cyclic
shifts for example).

3

Automatic Galaxy Classi?cation

We used the algorithm described in the previous section to estimate the parameters of features and orderings of the bent-double class from labelled training data and then to rank
candidate objects according to their likelihood under the model. We used leave-one-out
cross-validation to test the classi?cation ability of this supervised model, where for each
of the 150 examples we build a model using the positive examples from the set of 149
?other? examples, and then score the ?left-out? example with this model. The examples are
then sorted in decreasing order by their likelihood score (averaging over different possi-

1
0.9

True positive rate

0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

False positive rate

Figure 3: ROC plot for a model using angle, ratio of sides, and ratio of intensities, as
features, and learned using ordering-EM with labelled data.

ble orderings) and the results are analyzed using a receiver operating characteristic (ROC)
methodology. We use AROC , the area under the curve, as a measure of goodness of the
model, where a perfect model would have AROC = 1 and random performance corresponds to AROC = 0.5. The supervised model, using EM for learning ordering models,
has a cross-validated AROC score of 0.9336 (Figure 3) and appears to be quite useful at
detecting bent-double galaxies.

4

Model-Based Galaxy Clustering

A useful technique in understanding astronomical image data is to cluster image objects
based on their morphological and intensity properties. For example, consider how one
might cluster the image objects in Figure 1 into clusters, where we have features on angles,
intensities, and so forth. Just as with classi?cation, clustering of the objects is impeded by
not knowing which of the ?blobs? corresponds to the true ?core? component.
From a probabilistic viewpoint, clustering can be treated as introducing another level of
hidden variables, namely the unknown class (or cluster) identity of each object. We can
generalize the EM algorithm for orderings (Section 2) to handle this additional hidden
level. The model is now a mixture of clusters where each cluster is modelled as a mixture
of orderings. This leads to a more complex two-level EM algorithm than that presented
in Section 2, where at the inner-level the algorithm is learning how to orient the objects,
and at the outer level the algorithm is learning how to group the objects into C classes.
Space does not permit a detailed presentation of this algorithm?however, the derivation is
straightforward and produces intuitive update rules such as:
?
?cmj =

1
N P? (cl = c|?)

N X
K
X

P (cli = c|?i = k, D, ?) P (?i = k|D, ?) fmkj (di )

i=1 k=1

where ?cmj is the mean for the cth cluster (1 ? c ? C), the mth feature (1 ? m ? M ),
and the jth component of fmk (di ), and ?i = k corresponds to ordering k for the ith
object.
We applied this algorithm to the data set of 150 sky objects, where unlike the results in
Section 3, the algorithm now had no access to the class labels. We used the Gaussian
conditional-independence model as before, and grouped the data into K = 2 clusters.
Figures 4 and 5 show the highest likelihood objects, out of 150 total objects, under the

Bent?double

Bent?double

Bent?double

Bent?double

Bent?double

Bent?double

Bent?double

Bent?double

Figure 4: The 8 objects with the highest likelihood conditioned on the model for the larger
of the two clusters learned by the unsupervised algorithm.

Bent?double

Non?bent?double

Non?bent?double

Non?bent?double

Non?bent?double

Non?bent?double

Bent?double

Non?bent?double

Figure 5: The 8 objects with the highest likelihood conditioned on the model for the
smaller of the two clusters learned by the unsupervised algorithm.

150

Unsupervised Rank

bent?doubles
non?bent?doubles

100

50

0
0

50

100

150

Supervised Rank

Figure 6: A scatter plot of the ranking from the unsupervised model versus that of the
supervised model.

models for the larger cluster and smaller cluster respectively. The larger cluster is clearly a
bent-double cluster: 89 of the 150 objects are more likely to belong to this cluster under the
model and 88 out of the 89 objects in this cluster have the bent-double label. In other words,
the unsupervised algorithm has discovered a cluster that corresponds to ?strong examples?
of bent-doubles relative to the particular feature-space and model. In fact the non-bentdouble that is assigned to this group may well have been mislabelled (image not shown
here). The objects in Figure 5 are clearly inconsistent with the general visual pattern of
bent-doubles and this cluster consists of a mixture of non-bent-double and ?weaker? bentdouble galaxies. The objects in Figures 5 that are labelled as bent-doubles seem quite
atypical compared to the bent-doubles in Figure 4.
A natural hypothesis is that cluster 1 (88 bent-doubles) in the unsupervised model is in fact
very similar to the supervised model learned using the labelled set of 128 bent-doubles in
Section 3. Indeed the parameters of the two Gaussian models agree quite closely and the
similarity of the two models is illustrated clearly in Figure 6 where we plot the likelihoodbased ranks of the unsupervised model versus those of the supervised model. Both models
are in close agreement and both are clearly performing well in terms of separating the
objects in terms of their class labels.

5

Related Work and Future Directions

A related earlier paper is Kirshner et al [6] where we presented a heuristic algorithm for
solving the orientation problem for galaxies. The generalization to an EM framework in
this paper is new, as is the two-level EM algorithm for clustering objects in an unsupervised
manner.
There is a substantial body of work in computer vision on solving a variety of different
object matching problems using probabilistic techniques?see Mjolsness [7] for early ideas
and Chui et al. [2] for a recent application in medical imaging. Our work here differs in
a number of respects. One important difference is that we use EM to learn a model for
the simultaneous correspondence of N objects, using both geometric and intensity-based
features, whereas prior work in vision has primarily focused on matching one object to

another (essentially the N = 2 case). An exception is the recent work of Frey and Jojic
[4, 5] who used a similar EM-based approach to simultaneously cluster images and estimate
a variety of local spatial deformations. The work described in this paper can be viewed as
an extension and application of this general methodology to a real-world problem in galaxy
classi?cation.
Earlier work on bent-double galaxy classi?cation used decision tree classi?ers based on a
variety of geometric and intensity-based features [3]. In future work we plan to compare
the performance of this decision tree approach with the probabilistic model-based approach
proposed in this paper. The model-based approach has some inherent advantages over
a decision-tree model for these types of problems. For example, it can directly handle
objects in the catalog with only 2 blobs or with 4 or more blobs by integrating over missing
intensities and over missing correspondence information using mixture models that allow
for missing or extra ?blobs?. Being able to classify such con?gurations automatically is of
signi?cant interest to the astronomers.
Acknowledgments
This work was performed under a sub-contract from the ASCI Scienti?c Data Management Project of the Lawrence Livermore National Laboratory. The work of S. Kirshner
and P. Smyth was also supported by research grants from NSF (award IRI-9703120), the
Jet Propulsion Laboratory, IBM Research, and Microsoft Research. I. Cadez was supported
by a Microsoft Graduate Fellowship. The work of C. Kamath was performed under the auspices of the U.S. Department of Energy by University of California Lawrence Livermore
National Laboratory under contract No. W-7405-Eng-48. We gratefully acknowledge our
FIRST collaborators, in particular, Robert H. Becker for sharing his expertise on the subject.

References
[1] R. H. Becker, R. L. White, and D. J. Helfand. The FIRST Survey: Faint Images of the
Radio Sky at Twenty-cm. Astrophysical Journal, 450:559, 1995.
[2] H. Chui, L. Win, R. Schultz, J. S. Duncan, and A. Rangarajan. A uni?ed feature
registration method for brain mapping. In Proceedings of Information Processing in
Medical Imaging, pages 300?314. Springer-Verlag, 2001.
[3] I. K. Fodor, E. Cant?u-Paz, C. Kamath, and N. A. Tang. Finding bent-double radio
galaxies: A case study in data mining. In Proceedings of the Interface: Computer
Science and Statistics Symposium, volume 33, 2000.
[4] B. J. Frey and N. Jojic. Estimating mixture models of images and inferring spatial
transformations using the EM algorithm. In Proceedings of IEEE Computer Society
Conference on Computer Vision and Pattern Recognition, 1999.
[5] N. Jojic and B. J. Frey. Topographic transformation as a discrete latent variable. In
Advances in Neural Information Processing Systems 12. MIT Press, 2000.
[6] S. Kirshner, I. V. Cadez, P. Smyth, C. Kamath, and E. Cantu? -Paz. Probabilistic modelbased detection of bent-double radio galaxies. In Proceedings 16th International Conference on Pattern Recognition, volume 2, pages 499?502, 2002.
[7] E. Mjolsness. Bayesian inference on visual grammars by neural networks that optimize. Technical Report YALEU/DCS/TR-854, Department of Computer Science, Yale
University, May 1991.
[8] R. L. White, R. H. Becker, D. J. Helfand, and M. D. Gregg. A catalog of 1.4 GHz radio
sources from the FIRST Survey. Astrophysical Journal, 475:479, 1997.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 2906-correlated-topic-models.pdf

Correlated Topic Models

David M. Blei
Department of Computer Science
Princeton University

John D. Lafferty
School of Computer Science
Carnegie Mellon University

Abstract
Topic models, such as latent Dirichlet allocation (LDA), can be useful
tools for the statistical analysis of document collections and other discrete data. The LDA model assumes that the words of each document
arise from a mixture of topics, each of which is a distribution over the vocabulary. A limitation of LDA is the inability to model topic correlation
even though, for example, a document about genetics is more likely to
also be about disease than x-ray astronomy. This limitation stems from
the use of the Dirichlet distribution to model the variability among the
topic proportions. In this paper we develop the correlated topic model
(CTM), where the topic proportions exhibit correlation via the logistic
normal distribution [1]. We derive a mean-field variational inference algorithm for approximate posterior inference in this model, which is complicated by the fact that the logistic normal is not conjugate to the multinomial. The CTM gives a better fit than LDA on a collection of OCRed
articles from the journal Science. Furthermore, the CTM provides a natural way of visualizing and exploring this and other unstructured data
sets.

1

Introduction

The availability and use of unstructured historical collections of documents is rapidly growing. As one example, JSTOR (www.jstor.org) is a not-for-profit organization that maintains a large online scholarly journal archive obtained by running an optical character recognition engine over the original printed journals. JSTOR indexes the resulting text and provides online access to the scanned images of the original content through keyword search.
This provides an extremely useful service to the scholarly community, with the collection
comprising nearly three million published articles in a variety of fields.
The sheer size of this unstructured and noisy archive naturally suggests opportunities for
the use of statistical modeling. For instance, a scholar in a narrow subdiscipline, searching
for a particular research article, would certainly be interested to learn that the topic of
that article is highly correlated with another topic that the researcher may not have known
about, and that is not explicitly contained in the article. Alerted to the existence of this new
related topic, the researcher could browse the collection in a topic-guided manner to begin
to investigate connections to a previously unrecognized body of work. Since the archive
comprises millions of articles spanning centuries of scholarly work, automated analysis is
essential.

Several statistical models have recently been developed for automatically extracting the
topical structure of large document collections. In technical terms, a topic model is a
generative probabilistic model that uses a small number of distributions over a vocabulary
to describe a document collection. When fit from data, these distributions often correspond
to intuitive notions of topicality. In this work, we build upon the latent Dirichlet allocation
(LDA) [4] model. LDA assumes that the words of each document arise from a mixture
of topics. The topics are shared by all documents in the collection; the topic proportions
are document-specific and randomly drawn from a Dirichlet distribution. LDA allows each
document to exhibit multiple topics with different proportions, and it can thus capture the
heterogeneity in grouped data that exhibit multiple latent patterns. Recent work has used
LDA in more complicated document models [9, 11, 7], and in a variety of settings such
as image processing [12], collaborative filtering [8], and the modeling of sequential data
and user profiles [6]. Similar models were independently developed for disability survey
data [5] and population genetics [10].
Our goal in this paper is to address a limitation of the topic models proposed to date: they
fail to directly model correlation between topics. In many?indeed most?text corpora, it
is natural to expect that subsets of the underlying latent topics will be highly correlated. In
a corpus of scientific articles, for instance, an article about genetics may be likely to also
be about health and disease, but unlikely to also be about x-ray astronomy. For the LDA
model, this limitation stems from the independence assumptions implicit in the Dirichlet
distribution on the topic proportions. Under a Dirichlet, the components of the proportions
vector are nearly independent; this leads to the strong and unrealistic modeling assumption
that the presence of one topic is not correlated with the presence of another.
In this paper we present the correlated topic model (CTM). The CTM uses an alternative, more flexible distribution for the topic proportions that allows for covariance structure
among the components. This gives a more realistic model of latent topic structure where
the presence of one latent topic may be correlated with the presence of another. In the
following sections we develop the technical aspects of this model, and then demonstrate its
potential for the applications envisioned above. We fit the model to a portion of the JSTOR
archive of the journal Science. We demonstrate that the model gives a better fit than LDA,
as measured by the accuracy of the predictive distributions over held out documents. Furthermore, we demonstrate qualitatively that the correlated topic model provides a natural
way of visualizing and exploring such an unstructured collection of textual data.

2

The Correlated Topic Model

The key to the correlated topic model we propose is the logistic normal distribution [1]. The
logistic normal is a distribution on the simplex that allows for a general pattern of variability
between the components by transforming a multivariate normal random variable. Consider
the natural parameterization of a K-dimensional multinomial distribution:
p(z | ?) = exp{? T z ? a(?)}.

(1)

The random variable Z can take on K values; it can be represented by a K-vector with
exactly one component equal to one, denoting a value in {1, . . . , K}. The cumulant generating function of the distribution is
P

K
a(?) = log
(2)
i=1 exp{?i } .
The mapping between the mean parameterization (i.e., the simplex) and the natural parameterization is given by
?i = log ?i /?K .
(3)
Notice that this is not the minimal exponential family representation of the multinomial
because multiple values of ? can yield the same mean parameter.

?

?k

?d

Zd,n

?

Wd,n

N

D

K

Figure 1: Top: Graphical model representation of the correlated topic model. The logistic
normal distribution, used to model the latent topic proportions of a document, can represent
correlations between topics that are impossible to capture using a single Dirichlet. Bottom:
Example densities of the logistic normal on the 2-simplex. From left: diagonal covariance
and nonzero-mean, negative correlation between components 1 and 2, positive correlation
between components 1 and 2.
The logistic normal distribution assumes that ? is normally distributed and then mapped
to the simplex
with the inverse of the mapping given in equation (3); that is, f (?i ) =
P
exp ?i / j exp ?j . The logistic normal models correlations between components of the
simplicial random variable through the covariance matrix of the normal distribution. The
logistic normal was originally studied in the context of analyzing observed compositional
data such as the proportions of minerals in geological samples. In this work, we extend its
use to a hierarchical model where it describes the latent composition of topics associated
with each document.
Let {?, ?} be a K-dimensional mean and covariance matrix, and let topics ?1:K be K
multinomials over a fixed word vocabulary. The correlated topic model assumes that an
N -word document arises from the following generative process:
1. Draw ? | {?, ?} ? N (?, ?).
2. For n ? {1, . . . , N }:
(a) Draw topic assignment Zn | ? from Mult(f (?)).
(b) Draw word Wn | {zn , ?1:K } from Mult(?zn ).
This process is identical to the generative process of LDA except that the topic proportions
are drawn from a logistic normal rather than a Dirichlet. The model is shown as a directed
graphical model in Figure 1.
The CTM is more expressive than LDA. The strong independence assumption imposed
by the Dirichlet in LDA is not realistic when analyzing document collections, where one
may find strong correlations between topics. The covariance matrix of the logistic normal
in the CTM is introduced to model such correlations. In Section 3, we illustrate how the
higher order structure given by the covariance can be used as an exploratory tool for better
understanding and navigating a large corpus of documents. Moreover, modeling correlation
can lead to better predictive distributions. In some settings, such as collaborative filtering,

the goal is to predict unseen items conditional on a set of observations. An LDA model
will predict words based on the latent topics that the observations suggest, but the CTM
has the ability to predict items associated with additional topics that are correlated with the
conditionally probable topics.
2.1

Posterior inference and parameter estimation

Posterior inference is the central challenge to using the CTM. The posterior distribution of
the latent variables conditional on a document, p(?, z1:N | w1:N ), is intractable to compute;
once conditioned on some observations, the topic assignments z1:N and log proportions
? are dependent. We make use of mean-field variational methods to efficiently obtain an
approximation of this posterior distribution.
In brief, the strategy employed by mean-field variational methods is to form a factorized
distribution of the latent variables, parameterized by free variables which are called the variational parameters. These parameters are fit so that the Kullback-Leibler (KL) divergence
between the approximate and true posterior is small. For many problems this optimization
problem is computationally manageable, while standard methods, such as Markov Chain
Monte Carlo, are impractical. The tradeoff is that variational methods do not come with
the same theoretical guarantees as simulation methods. See [13] for a modern review of
variational methods for statistical inference.
In graphical models composed of conjugate-exponential family pairs and mixtures, the
variational inference algorithm can be automatically derived from general principles [2,
14]. In the CTM, however, the logistic normal is not conjugate to the multinomial. We
will therefore derive a variational inference algorithm by taking into account the special
structure and distributions used by our model.
We begin by using Jensen?s inequality to bound the log probability of a document:
log p(w1:N | ?, ?, ?) ?
Eq [log p(? | ?, ?)] +

(4)
PN

n=1 (Eq

[log p(zn | ?)] + Eq [log p(wn | zn , ?)]) + H (q) ,

where the expectation is taken with respect to a variational distribution of the latent variables, and H (q) denotes the entropy of that distribution. We use a factorized distribution:
QK
QN
2
q(?1:K , z1:N | ?1:K , ?1:K
, ?1:N ) = i=1 q(?i | ?i , ?i2 ) n=1 q(zn | ?n ).
(5)
The variational distributions of the discrete variables z1:N are specified by the Kdimensional multinomial parameters ?1:N . The variational distribution of the continuous
variables ?1:K are K independent univariate Gaussians {?i , ?i }. Since the variational parameters are fit using a single observed document w1:N , there is no advantage in introducing a non-diagonal variational covariance matrix.
The nonconjugacy of the logistic normal leads to difficulty in computing the expected log
probability of a topic assignment:
h
i


PK
Eq [log p(zn | ?)] = Eq ? T zn ? Eq log( i=1 exp{?i }) .
(6)
To preserve the lower bound on the log probability, we upper bound the log normalizer
with a Taylor expansion,
h P
i
PK
K
Eq log
exp{?
}
? ? ?1 ( i=1 Eq [exp{?i }]) ? 1 + log(?),
(7)
i
i=1
where we have introduced a new variational parameter ?. The expectation Eq [exp{?i }] is
the mean of a log normal distribution with mean and variance obtained from the variational
parameters {?i , ?i2 }; thus, Eq [exp{?i }] = exp{?i + ?i2 /2} for i ? {1, . . . , K}.

fossil record
birds
fossils
dinosaurs
fossil
evolution
taxa
species
specimens
evolutionary

ancient
found
impact
million years ago
africa
site
bones
years ago
date
rock

mantle
crust
upper mantle
meteorites
ratios
rocks
grains
isotopic
isotopic composition
depth

climate
ocean
ice
changes
climate change
north atlantic
record
warming
temperature
past

earthquake
earthquakes
fault
images
data
observations
features
venus
surface
faults

brain
memory
subjects
left
task
brains
cognitive
language
human brain
learning
co2
carbon
carbon dioxide
methane
water
energy
gas
fuel
production
organic matter

ozone
atmospheric
measurements
stratosphere
concentrations
atmosphere
air
aerosols
troposphere
measured

neurons
stimulus
motor
visual
cortical
axons
stimuli
movement
cortex
eye

ca2
calcium
release
ca2 release
concentration
ip3
intracellular calcium
intracellular
intracellular ca2
ca2 i

ras
atp
camp
gtp
adenylyl cyclase
cftr
adenosine triphosphate atp
guanosine triphosphate gtp
gap
gdp

synapses
ltp
glutamate
synaptic
neurons
long term potentiation ltp
synaptic transmission
postsynaptic
nmda receptors
hippocampus

males
male
females
female
sperm
sex
offspring
eggs
species
egg

gene
disease
mutations
families
mutation
alzheimers disease
patients
human
breast cancer
normal

genetic
population
populations
differences
variation
evolution
loci
mtdna
data
evolutionary

p53
cell cycle
activity
cyclin
regulation
protein
phosphorylation
kinase
regulated
cell cycle progression

amino acids
cdna
sequence
isolated
protein
amino acid
mrna
amino acid sequence
actin
clone

development
embryos
drosophila
genes
expression
embryo
developmental
embryonic
developmental biology
vertebrate

wild type
mutant
mutations
mutants
mutation
gene
yeast
recombination
phenotype
genes

Figure 2: A portion of the topic graph learned from 16,351 OCR articles from Science.
Each node represents a topic, and is labeled with the five most probable phrases from its
distribution (phrases are found by the ?turbo topics? method [3]). The interested reader can
browse the full model at http://www.cs.cmu.edu/?lemur/science/.
Given a model {?1:K , ?, ?} and a document w1:N , the variational inference algorithm optimizes equation (4) with respect to the variational parameters {?1:K , ?1:K , ?1:N , ?}. We
use coordinate ascent, repeatedly optimizing with respect to each parameter while holding
the others fixed. In variational inference for LDA, each coordinate can be optimized analytically. However, iterative methods are required for the CTM when optimizing for ?i and
?i2 . The details are given in Appendix A.
Given a collection of documents, we carry out parameter estimation in the correlated topic
model by attempting to maximize the likelihood of a corpus of documents as a function
of the topics ?1:K and the multivariate Gaussian parameters {?, ?}. We use variational
expectation-maximization (EM), where we maximize the bound on the log probability of a
collection given by summing equation (4) over the documents.
In the E-step, we maximize the bound with respect to the variational parameters by performing variational inference for each document. In the M-step, we maximize the bound
with respect to the model parameters. This is maximum likelihood estimation of the topics and multivariate Gaussian using expected sufficient statistics, where the expectation
is taken with respect to the variational distributions computed in the E-step. The E-step
and M-step are repeated until the bound on the likelihood converges. In the experiments
reported below, we run variational inference until the relative change in the probability
bound of equation (4) is less than 10?6 , and run variational EM until the relative change in
the likelihood bound is less than 10?5 .

3

Examples and Empirical Results: Modeling Science

In order to test and illustrate the correlated topic model, we estimated a 100-topic CTM
on 16,351 Science articles spanning 1990 to 1999. We constructed a graph of the latent topics and the connections among them by examining the most probable words from
each topic and the between-topic correlations. Part of this graph is illustrated in Figure 2. In this subgraph, there are three densely connected collections of topics: material
science, geology, and cell biology. Furthermore, an estimated CTM can be used to explore otherwise unstructured observed documents. In Figure 4, we list articles that are
assigned to the cognitive science topic and articles that are assigned to both the cog-

2200

?112800

?

?

?

?

?

?113600

?

1600

?
?
?

?

?114800

?
?
?
?

?
?

1400
1200

?

?

400

?

?

1000

?

800

L(CTM) ? L(LDA)

?114400

?

600

?114000

?
?

?
?

?115200

Held?out log likelihood

?

?

?

1800

?113200

?
?

?115600

?

2000

CTM
LDA

?
?

?116000

200

?116400

0

?

?

5

10

20

30

40

50

60

70

80

Number of topics

90

100

110

120

?

?

20

30

?

10

40

50

60

70

80

90

100

110

120

Number of topics

Figure 3: (L) The average held-out probability; CTM supports more topics than LDA. See
figure at right for the standard error of the difference. (R) The log odds ratio of the held-out
probability. Positive numbers indicate a better fit by the correlated topic model.
nitive science and visual neuroscience topics. The interested reader is invited to visit
http://www.cs.cmu.edu/?lemur/science/ to interactively explore this model, including the topics, their connections, and the articles that exhibit them.
We compared the CTM to LDA by fitting a smaller collection of articles to models of varying numbers of topics. This collection contains the 1,452 documents from 1960; we used
a vocabulary of 5,612 words after pruning common function words and terms that occur
once in the collection. Using ten-fold cross validation, we computed the log probability of
the held-out data given a model estimated from the remaining data. A better model of the
document collection will assign higher probability to the held out data. To avoid comparing
bounds, we used importance sampling to compute the log probability of a document where
the fitted variational distribution is the proposal.
Figure 3 illustrates the average held out log probability for each model and the average
difference between them. The CTM provides a better fit than LDA and supports more
topics; the likelihood for LDA peaks near 30 topics while the likelihood for the CTM peaks
close to 90 topics. The means and standard errors of the difference in log-likelihood of the
models is shown at right; this indicates that the CTM always gives a better fit.
Another quantitative evaluation of the relative strengths of LDA and the CTM is how well
the models predict the remaining words after observing a portion of the document. Suppose we observe words w1:P from a document and are interested in which model provides
a better predictive distribution p(w | w1:P ) of the remaining words. To compare these distributions, we use perplexity, which can be thought of as the effective number of equally
likely words according to the model. Mathematically, the perplexity of a word distribution is defined as the inverse of the per-word geometric average of the probability of the
observations,
 PD ?1
Q
D QNd
d=1 (Nd ?P ) ,
p(w
|
?,
w
)
Perp(?) =
i
1:P
d=1
i=P +1
where ? denotes the model parameters of an LDA or CTM model. Note that lower numbers
denote more predictive power.
The plot in Figure 4 compares the predictive perplexity under LDA and the CTM. When a

(1) A Head for Figures
(2) Sources of Mathematical Thinking: Behavioral and Brain
Imaging Evidence
(3) Natural Language Processing
(4) A Romance Blossoms Between Gray Matter and Silicon
(5) Computer Vision

2400
2200

Predictive perplexity

?

?

?

?

?

2000

Top Articles with
{brain, memory, human, visual, cognitive} and
{computer, data, information, problem, systems}

CTM
LDA
?

?
?

?
?
?
?

?
?

?
?
?
?

1800

(1) Separate Neural Bases of Two Fundamental Memory
Processes in the Human Medial Temporal Lobe
(2) Inattentional Blindness Versus Inattentional Amnesia for
Fixated but Ignored Words
(3) Making Memories: Brain Activity that Predicts How Well
Visual Experience Will be Remembered
(4) The Learning of Categories: Parallel Brain Systems for
Item Memory and Category Knowledge
(5) Brain Activation Modulated by Sentence Comprehension

2600

Top Articles with
{brain, memory, human, visual, cognitive}

10

20

30

40

50

60

70

80

90

% observed words

Figure 4: (Left) Exploring a collection through its topics. (Right) Predictive perplexity for
partially observed held-out documents from the 1960 Science corpus.
small number of words have been observed, there is less uncertainty about the remaining
words under the CTM than under LDA?the perplexity is reduced by nearly 200 words, or
roughly 10%. The reason is that after seeing a few words in one topic, the CTM uses topic
correlation to infer that words in a related topic may also be probable. In contrast, LDA
cannot predict the remaining words as well until a large portion of the document as been
observed so that all of its topics are represented.
Acknowledgments Research supported in part by NSF grants IIS-0312814 and IIS0427206 and by the DARPA CALO project.

References
[1] J. Aitchison. The statistical analysis of compositional data. Journal of the Royal
Statistical Society, Series B, 44(2):139?177, 1982.
[2] C. Bishop, D. Spiegelhalter, and J. Winn. VIBES: A variational inference engine for
Bayesian networks. In NIPS 15, pages 777?784. Cambridge, MA, 2003.
[3] D. Blei, J. Lafferty, C. Genovese, and L. Wasserman. Turbo topics. In progress, 2006.
[4] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet allocation. Journal of Machine
Learning Research, 3:993?1022, January 2003.
[5] E. Erosheva. Grade of membership and latent structure models with application to
disability survey data. PhD thesis, Carnegie Mellon University, Department of Statistics, 2002.
[6] M. Girolami and A. Kaban. Simplicial mixtures of Markov chains: Distributed modelling of dynamic user profiles. In NIPS 16, pages 9?16, 2004.
[7] T. Griffiths, M. Steyvers, D. Blei, and J. Tenenbaum. Integrating topics and syntax.
In Advances in Neural Information Processing Systems 17, 2005.
[8] B. Marlin. Collaborative filtering: A machine learning perspective. Master?s thesis,
University of Toronto, 2004.
[9] A. McCallum, A. Corrada-Emmanuel, and X. Wang. The author-recipient-topic
model for topic and role discovery in social networks. 2004.

[10] J. Pritchard, M. Stephens, and P. Donnelly. Inference of population structure using
multilocus genotype data. Genetics, 155:945?959, June 2000.
[11] M. Rosen-Zvi, T. Griffiths, M. Steyvers, and P. Smith. In UAI ?04: Proceedings of
the 20th Conference on Uncertainty in Artificial Intelligence, pages 487?494.
[12] J. Sivic, B. Rusell, A. Efros, A. Zisserman, and W. Freeman. Discovering object
categories in image collections. Technical report, CSAIL, MIT, 2005.
[13] M. Wainwright and M. Jordan. A variational principle for graphical models. In New
Directions in Statistical Signal Processing, chapter 11. MIT Press, 2005.
[14] E. Xing, M. Jordan, and S. Russell. A generalized mean field algorithm for variational
inference in exponential families. In Proceedings of UAI, 2003.

A

Variational Inference

We describe a coordinate ascent optimization algorithm for the likelihood bound in equation (4) with respect to the variational parameters.
The first term of equation (4) is


Eq [log p(? | ?, ?)] = (1/2) log |??1 | ? (K/2) log 2? ? (1/2)Eq (? ? ?)T ??1 (? ? ?) ,
(8)
where


Eq (? ? ?)T ??1 (? ? ?) = Tr(diag(? 2 )??1 ) + (? ? ?)T ??1 (? ? ?).
(9)
The second term of equation (4), using the additional bound in equation (7), is
P

PK
K
2
Eq [log p(zn | ?)] = i=1 ?i ?n,i ? ? ?1
i=1 exp{?i + ?i /2} + 1 ? log ?.

(10)

The third term of equation (4) is
Eq [log p(wn | zn , ?)] =

PK

i=1

?n,i log ?i,wn .

Finally, the fourth term is the entropy of the variational distribution:
PK 1
PN Pk
2
i=1 2 (log ?i + log 2? + 1) ?
n=1
i=1 ?n,i log ?n,i .

(11)

(12)

We maximize the bound in equation (4) with respect to the variational parameters ?1:K ,
?1:K , ?1:N , and ?. We use a coordinate ascent algorithm, iteratively maximizing the bound
with respect to each parameter.
First, we maximize equation (4) with respect to ?, using the second bound in equation (7).
The derivative with respect to ? is

P


K
2
?1
f 0 (?) = N ? ?2
,
(13)
i=1 exp{?i + ?i /2} ? ?
which has a maximum at

PK
?? = i=1 exp{?i + ?i2 /2}.
Second, we maximize with respect to ?n . This yields a maximum at
??n,i ? exp{?i }?i,w , i ? {1, . . . , K}.

(14)

(15)
n
Third, we maximize with respect to ?i . Since equation (4) is not amenable to analytic
maximization, we use a conjugate gradient algorithm with derivative
PN
dL/d? = ???1 (? ? ?) + n=1 ?n,1:K ? (N/?) exp{? + ? 2 /2} .
(16)
Finally, we maximize with respect to ?i2 . Again, there is no analytic solution. We use
Newton?s method for each coordinate, constrained such that ?i > 0:
2
2
dL/d?i2 = ???1
(17)
ii /2 ? N/2? exp{? + ?i /2} + 1/(2?i ).
Iterating between these optimizations defines a coordinate ascent algorithm on equation (4).


<<----------------------------------------------------------------------------------------------------------------------------------------->>

