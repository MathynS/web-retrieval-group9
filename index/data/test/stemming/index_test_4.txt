query sentence: state-of-art algorithms in theano
---------------------------------------------------------------------
title: 6573-binarized-neural-networks.pdf

Binarized Neural Networks
Itay Hubara1 *
itayh@technion.ac.il

Matthieu Courbariaux2 *
matthieu.courbariaux@gmail.com

Ran El-Yaniv1
rani@cs.technion.ac.il

Daniel Soudry3
daniel.soudry@gmail.com

Yoshua Bengio2,4
yoshua.umontreal@gmail.com

(1) Technion, Israel Institute of Technology.
(3) Columbia University.
(*) Indicates equal contribution.

(2) Universit? de Montr?al.
(4) CIFAR Senior Fellow.

Abstract
We introduce a method to train Binarized Neural Networks (BNNs) - neural
networks with binary weights and activations at run-time. At train-time the binary
weights and activations are used for computing the parameter gradients. During the
forward pass, BNNs drastically reduce memory size and accesses, and replace most
arithmetic operations with bit-wise operations, which is expected to substantially
improve power-efficiency. To validate the effectiveness of BNNs, we conducted
two sets of experiments on the Torch7 and Theano frameworks. On both, BNNs
achieved nearly state-of-the-art results over the MNIST, CIFAR-10 and SVHN
datasets. We also report our preliminary results on the challenging ImageNet
dataset. Last but not least, we wrote a binary matrix multiplication GPU kernel
with which it is possible to run our MNIST BNN 7 times faster than with an
unoptimized GPU kernel, without suffering any loss in classification accuracy. The
code for training and running our BNNs is available on-line.

Introduction
Deep Neural Networks (DNNs) have substantially pushed Artificial Intelligence (AI) limits in a wide
range of tasks (LeCun et al., 2015). Today, DNNs are almost exclusively trained on one or many very
fast and power-hungry Graphic Processing Units (GPUs) (Coates et al., 2013). As a result, it is often
a challenge to run DNNs on target low-power devices, and substantial research efforts are invested in
speeding up DNNs at run-time on both general-purpose (Gong et al., 2014; Han et al., 2015b) and
specialized computer hardware (Chen et al., 2014; Esser et al., 2015).
This paper makes the following contributions:
? We introduce a method to train Binarized-Neural-Networks (BNNs), neural networks with binary
weights and activations, at run-time, and when computing the parameter gradients at train-time
(see Section 1).
? We conduct two sets of experiments, each implemented on a different framework, namely Torch7
and Theano, which show that it is possible to train BNNs on MNIST, CIFAR-10 and SVHN and
achieve near state-of-the-art results (see Section 2). Moreover, we report preliminary results on the
challenging ImageNet dataset
? We show that during the forward pass (both at run-time and train-time), BNNs drastically reduce
memory consumption (size and number of accesses), and replace most arithmetic operations with
bit-wise operations, which potentially lead to a substantial increase in power-efficiency (see Section
30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

3). Moreover, a binarized CNN can lead to binary convolution kernel repetitions; we argue that
dedicated hardware could reduce the time complexity by 60% .
? Last but not least, we programed a binary matrix multiplication GPU kernel with which it is
possible to run our MNIST BNN 7 times faster than with an unoptimized GPU kernel, without
suffering any loss in classification accuracy (see Section 4).
The code for training and running our BNNs is available on-line (both Theano1 and Torch framework2 ).

1

Binarized Neural Networks

In this section, we detail our binarization function, show how we use it to compute the parameter
gradients,and how we backpropagate through it.
Deterministic vs Stochastic Binarization When training a BNN, we constrain both the weights
and the activations to either +1 or ?1. Those two values are very advantageous from a hardware
perspective, as we explain in Section 4. In order to transform the real-valued variables into those
two values, we use two different binarization functions, as in (Courbariaux et al., 2015). Our first
binarization function is deterministic:

+1 if x ? 0,
xb = Sign(x) =
(1)
?1 otherwise,
where xb is the binarized variable (weight or activation) and x the real-valued variable. It is very
straightforward to implement and works quite well in practice. Our second binarization function is
stochastic:

+1 with probability p = ?(x),
b
x =
(2)
?1 with probability 1 ? p,
where ? is the ?hard sigmoid? function:
x+1
x+1
, 0, 1) = max(0, min(1,
)).
(3)
2
2
The stochastic binarization is more appealing than the sign function, but harder to implement as
it requires the hardware to generate random bits when quantizing. As a result, we mostly use the
deterministic binarization function (i.e., the sign function), with the exception of activations at
train-time in some of our experiments.
?(x) = clip(

Gradient Computation and Accumulation Although our BNN training method uses binary
weights and activation to compute the parameter gradients, the real-valued gradients of the weights
are accumulated in real-valued variables, as per Algorithm 1. Real-valued weights are likely required
for Stochasic Gradient Descent (SGD) to work at all. SGD explores the space of parameters in small
and noisy steps, and that noise is averaged out by the stochastic gradient contributions accumulated
in each weight. Therefore, it is important to maintain sufficient resolution for these accumulators,
which at first glance suggests that high precision is absolutely required.
Moreover, adding noise to weights and activations when computing the parameter gradients provide
a form of regularization that can help to generalize better, as previously shown with variational
weight noise (Graves, 2011), Dropout (Srivastava et al., 2014) and DropConnect (Wan et al., 2013).
Our method of training BNNs can be seen as a variant of Dropout, in which instead of randomly
setting half of the activations to zero when computing the parameter gradients, we binarize both the
activations and the weights.
Propagating Gradients Through Discretization The derivative of the sign function is zero almost
everywhere, making it apparently incompatible with back-propagation, since the exact gradient of
the cost with respect to the quantities before the discretization (pre-activations or weights) would
1
2

https://github.com/MatthieuCourbariaux/BinaryNet
https://github.com/itayhubara/BinaryNet

2

be zero. Note that this remains true even if stochastic quantization is used. Bengio (2013) studied
the question of estimating or propagating gradients through stochastic discrete neurons. He found in
his experiments that the fastest training was obtained when using the ?straight-through estimator,?
previously introduced in Hinton?s lectures (Hinton, 2012). We follow a similar approach but use the
version of the straight-through estimator that takes into account the saturation effect, and does use
deterministic rather than stochastic sampling of the bit. Consider the sign function quantization
q = Sign(r),
and assume that an estimator gq of the gradient ?C
?q has been obtained (with the straight-through
estimator when needed).
Algorithm 1: Training a BNN. C is the cost function Algorithm 2: Shift based AdaMax learning
for minibatch, ? the learning rate decay factor and L rule (Kingma & Ba, 2014). gt2 indicates the
the number of layers. ? indicates element-wise mul- element-wise square gt ?gt and  stands for
tiplication. The function Binarize() specifies how to both left and right bit-shift. Good default
(stochastically or deterministically) binarize the activa- settings are ? = 2?10 , 1 ? ?1 = 2?3 , 1 ?
tions and weights, and Clip() specifies how to clip the ?2 = 2?10 . All operations on vectors are
weights. BatchNorm() specifies how to batch-normalize element-wise. With ?1t and ?2t we denote
the activations, using either batch normalization (Ioffe & ?1 and ?2 to the power t.
Szegedy, 2015) or its shift-based variant we describe in
Algorithm 3. BackBatchNorm() specifies how to back- Require: Previous parameters ?t?1 and
their gradient gt , and learning rate ?.
propagate through the normalization. Update() specifies
Ensure:
Updated parameters ?t .
how to update the parameters when their gradients are
{Biased 1st and 2nd moment estimates:}
known, using either ADAM (Kingma & Ba, 2014) or
the shift-based AdaMax we describe in Algorithm 2.
mt ? ?1 ? mt?1 + (1 ? ?1 ) ? gt
vt ? max(?2 ? vt?1 , |gt |)
Require: a minibatch of inputs and targets (a0 , a? ),
{Updated parameters:}
previous weights W , previous BatchNorm parameters ?, weight initialization coefficients from (Glorot
?t ? ?t?1 ? (?  (1 ? ?1 )) ? m
?  vt?1 )
& Bengio, 2010) ?, and previous learning rate ?.
Ensure: updated weights W t+1 , updated BatchNorm
parameters ?t+1 and updated learning rate ? t+1 .
Algorithm 3: Shift based Batch Normaliz{1. Computing the gradients:}
ing Transform, applied to activation x over
{1.1. Forward propagation:}
a mini-batch. The approximate power-offor k = 1 to L do
2 is3 AP 2(x) = sign(x)2round(log2|x|) , and
Wkb ? Binarize(Wk ), sk ? abk?1 Wkb
 stands for both left and right binary shift.
ak ? BatchNorm(sk , ?k )
Require: Values of x over a mini-batch:
if k < L then abk ? Binarize(ak )
B = {x1...m }; parameters to learn: ?, ?.
{1.2. Backward propagation:}
Ensure:
{yi = BN(xi ,?, ?)}
{Please note that the gradients are not binary.}
{1.
Mini-batch
?C
?
Pm mean:}
Compute gaL = ?aL knowing aL and a
1
?B ? m
i=1 xi
for k = L to 1 do
{2. Centered input: }
if k < L then gak ? gabk ? 1|ak |?1
C(xi ) ? (xi ? ?B )
(gsk , g?k ) ? BackBatchNorm(gak , sk , ?k )
{3. Approximate
variance:}
Pm
1
2
?B
?m
gabk?1 ? gsk Wkb , gWkb ? gs>k abk?1
i=1(C(xi )AP 2(C(xi )))
{4. Normalize:}
{2. Accumulating the gradients:}
p
2 + )?1 )
x?i ? C(xi )  AP 2(( ?B
for k = 1 to L do
t+1
t
t+1
t
{5.
Scale
and
shift:}
?k ? Update(?k , ? , g?k ), ?
? ??
yi ? AP 2(?)  x?i
Wkt+1 ? Clip(Update(Wk , ?k ? t , gWkb ), ?1, 1)
Then, our straight-through estimator of

?C
?r

is simply
gr = gq 1|r|?1 .
(4)
Note that this preserves the gradient?s information and cancels the gradient when r is too large.
Not cancelling the gradient when r is too large significantly worsens the performance. The use of
this straight-through estimator is illustrated in Algorithm 1. The derivative 1|r|?1 can also be seen
as propagating the gradient through hard tanh, which is the following piece-wise linear activation
function:
Htanh(x) = Clip(x, ?1, 1).
(5)
3

For hidden units, we use the sign function nonAlgorithm 4: Running a BNN. L = layers.
linearity to obtain binary activations, and for
Require: a vector of 8-bit inputs a0 , the binary
weights we combine two ingredients:
weights W b , and the BatchNorm parameters ?.
? Constrain each real-valued weight between -1 Ensure: the MLP output aL .
and 1, by projecting wr to -1 or 1 when the
{1. First layer:}
weight update brings wr outside of [?1, 1],
a1 ? 0
i.e., clipping the weights during training, as
for n = 1 to 8 do
per Algorithm 1. The real-valued weights
a1 ? a1 +2n?1 ?XnorDotProduct(an0 , W1b )
would otherwise grow very large without any
ab1 ? Sign(BatchNorm(a1 , ?1 ))
impact on the binary weights.
{2. Remaining hidden layers:}
for k = 2 to L ? 1 do
? When using a weight wr , quantize it using
ak ? XnorDotProduct(abk?1 , Wkb )
wb = Sign(wr ).
abk ? Sign(BatchNorm(ak , ?k ))
This is consistent with the gradient canceling
{3. Output layer:}
when |wr | > 1, according to Eq. 4.
aL ? XnorDotProduct(abL?1 , WLb )
aL ? BatchNorm(aL , ?L )
Shift-based Batch Normalization Batch
Normalization (BN) (Ioffe & Szegedy, 2015), accelerates the training and also seems to reduces
the overall impact of the weight scale. The normalization noise may also help to regularize the
model. However, at train-time, BN requires many multiplications (calculating the standard deviation
and dividing by it), namely, dividing by the running variance (the weighted mean of the training
set activation variance). Although the number of scaling calculations is the same as the number of
neurons, in the case of ConvNets this number is quite large. For example, in the CIFAR-10 dataset
(using our architecture), the first convolution layer, consisting of only 128 ? 3 ? 3 filter masks,
converts an image of size 3 ? 32 ? 32 to size 3 ? 128 ? 28 ? 28, which is two orders of magnitude
larger than the number of weights. To achieve the results that BN would obtain, we use a shift-based
batch normalization (SBN) technique. detailed in Algorithm 3. SBN approximates BN almost
without multiplications. In the experiment we conducted we did not observe accuracy loss when
using the shift based BN algorithm instead of the vanilla BN algorithm.
Shift based AdaMax The ADAM learning rule (Kingma & Ba, 2014) also seems to reduce the
impact of the weight scale. Since ADAM requires many multiplications, we suggest using instead the
shift-based AdaMax we detail in Algorithm 2. In the experiment we conducted we did not observe
accuracy loss when using the shift-based AdaMax algorithm instead of the vanilla ADAM algorithm.
First Layer In a BNN, only the binarized values of the weights and activations are used in all
calculations. As the output of one layer is the input of the next, all the layers inputs are binary,
with the exception of the first layer. However, we do not believe this to be a major issue. First, in
computer vision, the input representation typically has far fewer channels (e.g, red, green and blue)
than internal representations (e.g, 512). As a result, the first layer of a ConvNet is often the smallest
convolution layer, both in terms of parameters and computations (Szegedy et al., 2014). Second, it is
relatively easy to handle continuous-valued inputs as fixed point numbers, with m bits of precision.
For example, in the common case of 8-bit fixed point inputs:
s = x ? wb

;

s=

8
X

2n?1 (xn ? wb ),

(6)

n=1

where x is a vector of 1024 8-bit inputs, x81 is the most significant bit of the first input, wb is a vector
of 1024 1-bit weights, and s is the resulting weighted sum. This trick is used in Algorithm 4.

2

Benchmark Results

We conduct two sets of experiments, each based on a different framework, namely Torch7 and Theano.
Implementation details are reported in Appendix A and code for both frameworks is available online.
Results are reported in Table 1.
3

Hardware implementation of AP2 is as simple as extracting the index of the most significant bit from the
number?s binary representation.

4

Table 1: Classification test error rates of DNNs trained on MNIST (fully connected architecture),
CIFAR-10 and SVHN (convnet). No unsupervised pre-training or data augmentation was used.
Data set

MNIST
SVHN
Binarized activations+weights, during training and test
BNN (Torch7)
1.40%
2.53%
BNN (Theano)
0.96%
2.80%
Committee Machines? Array (Baldassi et al., 2015)
1.35%
Binarized weights, during training and test
BinaryConnect (Courbariaux et al., 2015)
1.29? 0.08% 2.30%
Binarized activations+weights, during test
EBP (Cheng et al., 2015)
2.2? 0.1%
Bitwise DNNs (Kim & Smaragdis, 2016)
1.33%
Ternary weights, binary activations, during test
(Hwang & Sung, 2014)
1.45%
No binarization (standard results)
No regularization
1.3? 0.2%
2.44%
Gated pooling (Lee et al., 2015)
1.69%

CIFAR-10
10.15%
11.40%
9.90%
10.94%
7.62%

Preliminary Results on ImageNet To Figure 1: Training curves for different methods on
test the strength of our method, we applied CIFAR-10 dataset. The dotted lines represent the trainit to the challenging ImageNet classifica- ing costs (square hinge losses) and the continuous lines
tion task. Considerable research has been the corresponding validation error rates. Although
concerned with compressing ImageNet ar- BNNs are slower to train, they are nearly as accurate as
chitectures while preserving high accuracy 32-bit float DNNs.
performance (e.g., Han et al. (2015a)). Previous approaches that have been tried include pruning near zero weights using matrix factorization techniques, quantizing
the weights and applying Huffman codes
among others. To the best of the our knowledge, so far there are no reports on successfully quantizing the network?s activations.
Moreover, a recent work Han et al. (2015a)
showed that accuracy significantly deteriorates when trying to quantize convolutional
layers? weights below 4 bits (FC layers are
more robust to quantization and can operate
quite well with only 2 bits). In the present
work we attempted to tackle the difficult task of binarizing both weights and activations. Employing
the well known AlexNet and GoogleNet architectures, we applied our techniques and achieved
36.1% top-1 and 60.1% top-5 accuracies using AlexNet and 47.1% top-1 and 69.1% top-5 accuracies
using GoogleNet. While this performance leaves room for improvement (relative to full precision
nets), they are by far better than all previous attempts to compress ImageNet architectures using less
than 4 bits precision for the weights. Moreover, this advantage is achieved while also binarizing
neuron activations. Detailed descriptions of these results as well as full implementation details
of our experiments are reported in the supplementary material (Appendix B). In our latest work
(Hubara et al., 2016) we relaxed the binary constrains and allowed more than 1-bit per weight and
activations. The resulting QNNs achieve prediction accuracy comparable to their 32-bit counterparts.
For example, our quantized version of AlexNet with 1-bit weights and 2-bit activations achieves
51% top-1 accuracy and GoogleNet with 4-bits weighs and activation achived 66.6%. Moreover, we
quantize the parameter gradients to 6-bits as well which enables gradients computation using only
bit-wise operation. Full details can be found in (Hubara et al., 2016)
5

Table 2: Energy consumption of multiplyaccumulations in pico-joules (Horowitz, 2014)
Operation
MUL ADD
8bit Integer
0.2pJ 0.03pJ
32bit Integer
3.1pJ
0.1pJ
16bit Floating Point 1.1pJ
0.4pJ
32tbit Floating Point 3.7pJ
0.9pJ

3

Table 3: Energy consumption of memory accesses
in pico-joules (Horowitz, 2014)
Memory size 64-bit memory access
8K
10pJ
32K
20pJ
1M
100pJ
DRAM
1.3-2.6nJ

High Power Efficiency during the Forward Pass

Computer hardware, be it general-purpose or specialized, is composed of memories, arithmetic
operators and control logic. During the forward pass (both at run-time and train-time), BNNs
drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise
operations, which might lead to a great increase in power-efficiency. Moreover, a binarized CNN can
lead to binary convolution kernel repetitions, and we argue that dedicated hardware could reduce the
time complexity by 60% .
Memory Size and Accesses Improving computing performance has always been and remains a
challenge. Over the last decade, power has been the main constraint on performance (Horowitz, 2014).
This is why much research effort has been devoted to reducing the energy consumption of neural
networks. Horowitz (2014) provides rough numbers for the energy consumed by the computation (the
given numbers are for 45nm technology), as summarized in Tables 2 and 3. Importantly, we can see
that memory accesses typically consume more energy than arithmetic operations, and memory access
cost augments with memory size. In comparison with 32-bit DNNs, BNNs require 32 times smaller
memory size and 32 times fewer memory accesses. This is expected to reduce energy consumption
drastically (i.e., more than 32 times).
XNOR-Count Applying a DNN mainly consists of convolutions and matrix multiplications. The
key arithmetic operation of deep learning is thus the multiply-accumulate operation. Artificial neurons
are basically multiply-accumulators computing weighted sums of their inputs. In BNNs, both the
activations and the weights are constrained to either ?1 or +1. As a result, most of the 32-bit floating
point multiply-accumulations are replaced by 1-bit XNOR-count operations. This could have a big
impact on dedicated deep learning hardware. For instance, a 32-bit floating point multiplier costs
about 200 Xilinx FPGA slices (Govindu et al., 2004; Beauchamp et al., 2006), whereas a 1-bit XNOR
gate only costs a single slice.
Exploiting Filter Repetitions When using a ConvNet architecture with binary weights, the number
of unique filters is bounded by the filter size. For example, in our implementation we use filters of
size 3 ? 3, so the maximum number of unique 2D filters is 29 = 512. Since we now have binary
filters, many 2D filters of size k ? k repeat themselves. By using dedicated hardware/software, we
can apply only the unique 2D filters on each feature map and sum the results to receive each 3D
filter?s convolutional result. For example, in our ConvNet architecture trained on the CIFAR-10
benchmark, there are only 42% unique filters per layer on average. Hence we can reduce the number
of the XNOR-popcount operations by 3.

4

Seven Times Faster on GPU at Run-Time

It is possible to speed up GPU implementations of BNNs, by using a method sometimes called
SIMD (single instruction, multiple data) within a register (SWAR). The basic idea of SWAR is to
concatenate groups of 32 binary variables into 32-bit registers, and thus obtain a 32-times speed-up
on bitwise operations (e.g, XNOR). Using SWAR, it is possible to evaluate 32 connections with only
3 instructions:
32b
a1 + = popcount(xnor(a32b
0 , w1 )),

(7)

where a1 is the resulting weighted sum, and a32b
and w132b are the concatenated inputs and weights.
0
Those 3 instructions (accumulation, popcount, xnor) take 1 + 4 + 1 = 6 clock cycles on recent
6

Nvidia GPUs (and if they were to become a fused instruction, it would only take a single clock cycle).
Consequently, we obtain a theoretical Nvidia GPU speed-up of factor of 32/6 ? 5.3. In practice, this
speed-up is quite easy to obtain as the memory bandwidth to computation ratio is also increased by 6
times.
In order to validate those theoretical results, we
programed two GPU kernels:

Figure 2: The first three columns represent the
time it takes to perform a 8192 ? 8192 ? 8192 (bi? The first kernel (baseline) is an unoptimized nary) matrix multiplication on a GTX750 Nvidia
matrix multiplication kernel.
GPU, depending on which kernel is used. We
? The second kernel (XNOR) is nearly identical can see that our XNOR kernel is 23 times faster
to the baseline kernel, except that it uses the than our baseline kernel and 3.4 times faster than
cuBLAS. The next three columns represent the
SWAR method, as in Equation (7).
time it takes to run the MLP from Section 2 on the
The two GPU kernels return identical outputs full MNIST test set. As MNIST?s images are not
when their inputs are constrained to ?1 or +1 binary, the first layer?s computations are always
(but not otherwise). The XNOR kernel is about performed by the baseline kernel. The last three
23 times faster than the baseline kernel and 3.4 columns show that the MLP accuracy does not
times faster than cuBLAS, as shown in Figure 2. depend on which kernel is used.
Last but not least, the MLP from Section 2 runs
7 times faster with the XNOR kernel than with
the baseline kernel, without suffering any loss
in classification accuracy (see Figure 2).

5

Discussion and Related Work

Until recently, the use of extremely lowprecision networks (binary in the extreme case)
was believed to be highly destructive to the network performance (Courbariaux et al., 2014).
Soudry et al. (2014) and Cheng et al. (2015)
proved the contrary by showing that good performance could be achieved even if all neurons
and weights are binarized to ?1 . This was done
using Expectation BackPropagation (EBP), a
variational Bayesian approach, which infers networks with binary weights and neurons by updating the posterior distributions over the weights.
These distributions are updated by differentiating their parameters (e.g., mean values) via the back
propagation (BP) algorithm. Esser et al. (2015) implemented a fully binary network at run time using
a very similar approach to EBP, showing significant improvement in energy efficiency. The drawback
of EBP is that the binarized parameters are only used during inference.
The probabilistic idea behind EBP was extended in the BinaryConnect algorithm of Courbariaux et al.
(2015). In BinaryConnect, the real-valued version of the weights is saved and used as a key reference
for the binarization process. The binarization noise is independent between different weights, either
by construction (by using stochastic quantization) or by assumption (a common simplification; see
Spang (1962). The noise would have little effect on the next neuron?s input because the input is
a summation over many weighted neurons. Thus, the real-valued version could be updated by the
back propagated error by simply ignoring the binarization noise in the update. Using this method,
Courbariaux et al. (2015) were the first to binarize weights in CNNs and achieved near state-of-the-art
performance on several datasets. They also argued that noisy weights provide a form of regularization,
which could help to improve generalization, as previously shown in (Wan et al., 2013). This method
binarized weights while still maintaining full precision neurons.
Lin et al. (2015) carried over the work of Courbariaux et al. (2015) to the back-propagation process
by quantizing the representations at each layer of the network, to convert some of the remaining
multiplications into bit-shifts by restricting the neurons values to be power-of-two integers. Lin et al.
(2015)?s work and ours seem to share similar characteristics . However, their approach continues to
use full precision weights during the test phase. Moreover, Lin et al. (2015) quantize the neurons
only during the back propagation process, and not during forward propagation.
7

Other research Baldassi et al. (2015) showed that full binary training and testing is possible in an
array of committee machines with randomized input, where only one weight layer is being adjusted.
Gong et al. (2014) aimed to compress a fully trained high precision network by using a quantization
or matrix factorization methods. These methods required training the network with full precision
weights and neurons, thus requiring numerous MAC operations the proposed BNN algorithm avoids.
Hwang & Sung (2014) focused on a fixed-point neural network design and achieved performance
almost identical to that of the floating-point architecture. Kim & Smaragdis (2016) retrained neural
networks with binary weights and activations.
So far, to the best of our knowledge, no work has succeeded in binarizing weights and neurons, at the
inference phase and the entire training phase of a deep network. This was achieved in the present
work. We relied on the idea that binarization can be done stochastically, or be approximated as
random noise. This was previously done for the weights by Courbariaux et al. (2015), but our BNNs
extend this to the activations. Note that the binary activations are especially important for ConvNets,
where there are typically many more neurons than free weights. This allows highly efficient operation
of the binarized DNN at run time, and at the forward-propagation phase during training. Moreover,
our training method has almost no multiplications, and therefore might be implemented efficiently
in dedicated hardware. However, we have to save the value of the full precision weights. This is a
remaining computational bottleneck during training, since it is an energy-consuming operation.

Conclusion
We have introduced BNNs, which binarize deep neural networks and can lead to dramatic improvements in both power consumption and computation speed. During the forward pass (both at run-time
and train-time), BNNs drastically reduce memory size and accesses, and replace most arithmetic
operations with bit-wise operations. Our estimates indicate that power efficiency can be improved by
more than one order of magnitude (see Section 3). In terms of speed, we programed a binary matrix
multiplication GPU kernel that enabled running MLP over the MNIST datset 7 times faster (than
with an unoptimized GPU kernel) without suffering any accuracy degradation (see Section 4).
We have shown that BNNs can handle MNIST, CIFAR-10 and SVHN while achieving nearly stateof-the-art accuracy performance. While our preliminary results for the challenging ImageNet are
not on par with the best results achievable with full precision networks, they significantly improve
all previous attempts to compress ImageNet-capable architectures (see Section 2 and supplementary
material - Appendix B). Moreover by relaxing the binary constrains and allowed more than 1-bit per
weight and activations we have been able to achieve prediction accuracy comparable to their 32-bit
counterparts. Full details can be found in our latest work (Hubara et al., 2016) A major open question
would be to further improve our results on ImageNet. A substantial progress in this direction might
lead to huge impact on DNN usability in low power instruments such as mobile phones.

Acknowledgments
We would like to express our appreciation to Elad Hoffer, for his technical assistance and constructive
comments. We thank our fellow MILA lab members who took the time to read the article and give us
some feedback. We thank the developers of Torch, Collobert et al. (2011) a Lua based environment,
and Theano (Bergstra et al., 2010; Bastien et al., 2012), a Python library which allowed us to easily
develop a fast and optimized code for GPU. We also thank the developers of Pylearn2 (Goodfellow
et al., 2013) and Lasagne (Dieleman et al., 2015), two Deep Learning libraries built on the top of
Theano. We thank Yuxin Wu for helping us compare our GPU kernels with cuBLAS. We are also
grateful for funding from NSERC, the Canada Research Chairs, Compute Canada, and CIFAR. We
are also grateful for funding from CIFAR, NSERC, IBM, Samsung. This research was also supported
by The Israel Science Foundation (grant No. 1890/14).

References
Baldassi, C., Ingrosso, A., Lucibello, C., Saglietti, L., and Zecchina, R. Subdominant Dense Clusters Allow for
Simple Learning and High Computational Performance in Neural Networks with Discrete Synapses. Physical
Review Letters, 115(12):1?5, 2015.

8

Bastien, F., Lamblin, P., Pascanu, R., et al. Theano: new features and speed improvements. Deep Learning and
Unsupervised Feature Learning NIPS 2012 Workshop, 2012.
Beauchamp, M. J., Hauck, S., Underwood, K. D., and Hemmert, K. S. Embedded floating-point units in FPGAs.
In Proceedings of the 2006 ACM/SIGDA 14th international symposium on Field programmable gate arrays,
pp. 12?20. ACM, 2006.
Bengio, Y. Estimating or propagating gradients through stochastic neurons. Technical Report arXiv:1305.2982,
Universite de Montreal, 2013.
Bergstra, J., Breuleux, O., Bastien, F., et al. Theano: a CPU and GPU math expression compiler. In Proceedings
of the Python for Scientific Computing Conference (SciPy), June 2010. Oral Presentation.
Chen, T., Du, Z., Sun, N., et al. Diannao: A small-footprint high-throughput accelerator for ubiquitous machinelearning. In Proceedings of the 19th international conference on Architectural support for programming
languages and operating systems, pp. 269?284. ACM, 2014.
Cheng, Z., Soudry, D., Mao, Z., and Lan, Z. Training binary multilayer neural networks for image classification
using expectation backpropgation. arXiv preprint arXiv:1503.03562, 2015.
Coates, A., Huval, B., Wang, T., et al. Deep learning with COTS HPC systems. In Proceedings of the 30th
international conference on machine learning, pp. 1337?1345, 2013.
Collobert, R., Kavukcuoglu, K., and Farabet, C. Torch7: A matlab-like environment for machine learning. In
BigLearn, NIPS Workshop, 2011.
Courbariaux, M., Bengio, Y., and David, J.-P. Training deep neural networks with low precision multiplications.
ArXiv e-prints, abs/1412.7024, December 2014.
Courbariaux, M., Bengio, Y., and David, J.-P. Binaryconnect: Training deep neural networks with binary weights
during propagations. ArXiv e-prints, abs/1511.00363, November 2015.
Dieleman, S., Schl?ter, J., Raffel, C., et al. Lasagne: First release., August 2015.
Esser, S. K., Appuswamy, R., Merolla, P., Arthur, J. V., and Modha, D. S. Backpropagation for energy-efficient
neuromorphic computing. In Advances in Neural Information Processing Systems, pp. 1117?1125, 2015.
Glorot, X. and Bengio, Y. Understanding the difficulty of training deep feedforward neural networks. In
AISTATS?2010, 2010.
Gong, Y., Liu, L., Yang, M., and Bourdev, L. Compressing deep convolutional networks using vector quantization.
arXiv preprint arXiv:1412.6115, 2014.
Goodfellow, I. J., Warde-Farley, D., Lamblin, P., et al. Pylearn2: a machine learning research library. arXiv
preprint arXiv:1308.4214, 2013.
Govindu, G., Zhuo, L., Choi, S., and Prasanna, V. Analysis of high-performance floating-point arithmetic on
FPGAs. In Parallel and Distributed Processing Symposium, 2004. Proceedings. 18th International, pp. 149.
IEEE, 2004.
Graves, A. Practical variational inference for neural networks. In Advances in Neural Information Processing
Systems, pp. 2348?2356, 2011.
Han, S., Mao, H., and Dally, W. J. Deep compression: Compressing deep neural networks with pruning, trained
quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015a.
Han, S., Pool, J., Tran, J., and Dally, W. Learning both weights and connections for efficient neural network. In
Advances in Neural Information Processing Systems, pp. 1135?1143, 2015b.
Hinton, G. Neural networks for machine learning. Coursera, video lectures, 2012.
Horowitz, M. Computing?s Energy Problem (and what we can do about it). IEEE Interational Solid State
Circuits Conference, pp. 10?14, 2014.
Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., and Bengio, Y. Quantized neural networks: Training
neural networks with low precision weights and activations. arXiv preprint arXiv:1609.07061, 2016.
Hwang, K. and Sung, W. Fixed-point feedforward deep neural network design using weights+ 1, 0, and- 1. In
Signal Processing Systems (SiPS), 2014 IEEE Workshop on, pp. 1?6. IEEE, 2014.
Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate
shift. 2015.
Kim, M. and Smaragdis, P. Bitwise Neural Networks. ArXiv e-prints, January 2016.
Kingma, D. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
LeCun, Y., Bengio, Y., and Hinton, G. Deep learning. Nature, 521(7553):436?444, 2015.
Lee, C.-Y., Gallagher, P. W., and Tu, Z. Generalizing pooling functions in convolutional neural networks: Mixed,
gated, and tree. arXiv preprint arXiv:1509.08985, 2015.
Lin, Z., Courbariaux, M., Memisevic, R., and Bengio, Y. Neural networks with few multiplications. ArXiv
e-prints, abs/1510.03009, October 2015.
Soudry, D., Hubara, I., and Meir, R. Expectation backpropagation: Parameter-free training of multilayer neural
networks with continuous or discrete weights. In NIPS?2014, 2014.
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. Dropout: A simple way to
prevent neural networks from overfitting. Journal of Machine Learning Research, 15:1929?1958, 2014.
Szegedy, C., Liu, W., Jia, Y., et al. Going deeper with convolutions. Technical report, arXiv:1409.4842, 2014.
Wan, L., Zeiler, M., Zhang, S., LeCun, Y., and Fergus, R. Regularization of neural networks using dropconnect.
In ICML?2013, 2013.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 5647-binaryconnect-training-deep-neural-networks-with-binary-weights-during-propagations.pdf

BinaryConnect: Training Deep Neural Networks with
binary weights during propagations

Matthieu Courbariaux
?
Ecole
Polytechnique de Montr?eal
matthieu.courbariaux@polymtl.ca

Yoshua Bengio
Universit?e de Montr?eal, CIFAR Senior Fellow
yoshua.bengio@gmail.com

Jean-Pierre David
?
Ecole
Polytechnique de Montr?eal
jean-pierre.david@polymtl.ca

Abstract
Deep Neural Networks (DNN) have achieved state-of-the-art results in a wide
range of tasks, with the best results obtained with large training sets and large
models. In the past, GPUs enabled these breakthroughs because of their greater
computational speed. In the future, faster computation at both training and test
time is likely to be crucial for further progress and for consumer applications on
low-power devices. As a result, there is much interest in research and development of dedicated hardware for Deep Learning (DL). Binary weights, i.e., weights
which are constrained to only two possible values (e.g. -1 or 1), would bring great
benefits to specialized DL hardware by replacing many multiply-accumulate operations by simple accumulations, as multipliers are the most space and powerhungry components of the digital implementation of neural networks. We introduce BinaryConnect, a method which consists in training a DNN with binary
weights during the forward and backward propagations, while retaining precision
of the stored weights in which gradients are accumulated. Like other dropout
schemes, we show that BinaryConnect acts as regularizer and we obtain near
state-of-the-art results with BinaryConnect on the permutation-invariant MNIST,
CIFAR-10 and SVHN.

1

Introduction

Deep Neural Networks (DNN) have substantially pushed the state-of-the-art in a wide range of tasks,
especially in speech recognition [1, 2] and computer vision, notably object recognition from images [3, 4]. More recently, deep learning is making important strides in natural language processing,
especially statistical machine translation [5, 6, 7]. Interestingly, one of the key factors that enabled
this major progress has been the advent of Graphics Processing Units (GPUs), with speed-ups on the
order of 10 to 30-fold, starting with [8], and similar improvements with distributed training [9, 10].
Indeed, the ability to train larger models on more data has enabled the kind of breakthroughs observed in the last few years. Today, researchers and developers designing new deep learning algorithms and applications often find themselves limited by computational capability. This along, with
the drive to put deep learning systems on low-power devices (unlike GPUs) is greatly increasing the
interest in research and development of specialized hardware for deep networks [11, 12, 13].
Most of the computation performed during training and application of deep networks regards the
multiplication of a real-valued weight by a real-valued activation (in the recognition or forward
propagation phase of the back-propagation algorithm) or gradient (in the backward propagation
phase of the back-propagation algorithm). This paper proposes an approach called BinaryConnect
1

to eliminate the need for these multiplications by forcing the weights used in these forward and
backward propagations to be binary, i.e. constrained to only two values (not necessarily 0 and 1). We
show that state-of-the-art results can be achieved with BinaryConnect on the permutation-invariant
MNIST, CIFAR-10 and SVHN.
What makes this workable are two ingredients:
1. Sufficient precision is necessary to accumulate and average a large number of stochastic
gradients, but noisy weights (and we can view discretization into a small number of values
as a form of noise, especially if we make this discretization stochastic) are quite compatible
with Stochastic Gradient Descent (SGD), the main type of optimization algorithm for deep
learning. SGD explores the space of parameters by making small and noisy steps and
that noise is averaged out by the stochastic gradient contributions accumulated in each
weight. Therefore, it is important to keep sufficient resolution for these accumulators,
which at first sight suggests that high precision is absolutely required. [14] and [15] show
that randomized or stochastic rounding can be used to provide unbiased discretization.
[14] have shown that SGD requires weights with a precision of at least 6 to 8 bits and
[16] successfully train DNNs with 12 bits dynamic fixed-point computation. Besides, the
estimated precision of the brain synapses varies between 6 and 12 bits [17].
2. Noisy weights actually provide a form of regularization which can help to generalize better,
as previously shown with variational weight noise [18], Dropout [19, 20] and DropConnect [21], which add noise to the activations or to the weights. For instance, DropConnect
[21], which is closest to BinaryConnect, is a very efficient regularizer that randomly substitutes half of the weights with zeros during propagations. What these previous works show
is that only the expected value of the weight needs to have high precision, and that noise
can actually be beneficial.
The main contributions of this article are the following.
? We introduce BinaryConnect, a method which consists in training a DNN with binary
weights during the forward and backward propagations (Section 2).
? We show that BinaryConnect is a regularizer and we obtain near state-of-the-art results on
the permutation-invariant MNIST, CIFAR-10 and SVHN (Section 3).
? We make the code for BinaryConnect available 1 .

2

BinaryConnect

In this section we give a more detailed view of BinaryConnect, considering which two values to
choose, how to discretize, how to train and how to perform inference.
+1 or ?1

2.1

Applying a DNN mainly consists in convolutions and matrix multiplications. The key arithmetic
operation of DL is thus the multiply-accumulate operation. Artificial neurons are basically multiplyaccumulators computing weighted sums of their inputs.
BinaryConnect constraints the weights to either +1 or ?1 during propagations. As a result, many
multiply-accumulate operations are replaced by simple additions (and subtractions). This is a huge
gain, as fixed-point adders are much less expensive both in terms of area and energy than fixed-point
multiply-accumulators [22].
2.2

Deterministic vs stochastic binarization

The binarization operation transforms the real-valued weights into the two possible values. A very
straightforward binarization operation would be based on the sign function:

+1 if w ? 0,
wb =
(1)
?1 otherwise.
1

https://github.com/MatthieuCourbariaux/BinaryConnect

2

Where wb is the binarized weight and w the real-valued weight. Although this is a deterministic operation, averaging this discretization over the many input weights of a hidden unit could compensate
for the loss of information. An alternative that allows a finer and more correct averaging process to
take place is to binarize stochastically:

+1 with probability p = ?(w),
(2)
wb =
?1 with probability 1 ? p.
where ? is the ?hard sigmoid? function:
?(x) = clip(

x+1
x+1
, 0, 1) = max(0, min(1,
))
2
2

(3)

We use such a hard sigmoid rather than the soft version because it is far less computationally expensive (both in software and specialized hardware implementations) and yielded excellent results in our
experiments. It is similar to the ?hard tanh? non-linearity introduced by [23]. It is also piece-wise
linear and corresponds to a bounded form of the rectifier [24].
2.3

Propagations vs updates

Let us consider the different steps of back-propagation with SGD udpates and whether it makes
sense, or not, to discretize the weights, at each of these steps.
1. Given the DNN input, compute the unit activations layer by layer, leading to the top layer
which is the output of the DNN, given its input. This step is referred as the forward propagation.
2. Given the DNN target, compute the training objective?s gradient w.r.t. each layer?s activations, starting from the top layer and going down layer by layer until the first hidden
layer. This step is referred to as the backward propagation or backward phase of backpropagation.
3. Compute the gradient w.r.t. each layer?s parameters and then update the parameters using
their computed gradients and their previous values. This step is referred to as the parameter
update.
Algorithm 1 SGD training with BinaryConnect. C is the cost function for minibatch and the functions binarize(w) and clip(w) specify how to binarize and clip weights. L is the number of layers.
Require: a minibatch of (inputs, targets), previous parameters wt?1 (weights) and bt?1 (biases),
and learning rate ?.
Ensure: updated parameters wt and bt .
1. Forward propagation:
wb ? binarize(wt?1 )
For k = 1 to L, compute ak knowing ak?1 , wb and bt?1
2. Backward propagation:
?C
Initialize output layer?s activations gradient ?a
L
?C
?C
For k = L to 2, compute ?ak?1 knowing ?ak and wb
3. Parameter update:
?C
?C
Compute ?w
and db?C
knowing ?a
and ak?1
t?1
b
k
?C
wt ? clip(wt?1 ? ? ?w
)
b
?C
bt ? bt?1 ? ? ?bt?1

A key point to understand with BinaryConnect is that we only binarize the weights during the forward and backward propagations (steps 1 and 2) but not during the parameter update (step 3), as
illustrated in Algorithm 1. Keeping good precision weights during the updates is necessary for SGD
to work at all. These parameter changes are tiny by virtue of being obtained by gradient descent, i.e.,
SGD performs a large number of almost infinitesimal changes in the direction that most improves
the training objective (plus noise). One way to picture all this is to hypothesize that what matters
3

most at the end of training is the sign of the weights, w? , but that in order to figure it out, we perform
a lot of small changes to a continuous-valued quantity w, and only at the end consider its sign:
X
w? = sign(
gt )
(4)
t
t?1 ,bt?1 ),yt )
where gt is a noisy estimator of ?C(f (xt ,w
, where C(f (xt , wt?1 , bt?1 ), yt ) is the value
?wt?1
of the objective function on (input,target) example (xt , yt ), when wt?1 are the previous weights and
w? is its final discretized value of the weights.

Another way to conceive of this discretization is as a form of corruption, and hence as a regularizer,
and our empirical results confirm this hypothesis. In addition, we can make the discretization errors
on different weights approximately cancel each other while keeping a lot of precision by randomizing
the discretization appropriately. We propose a form of randomized discretization that preserves the
expected value of the discretized weight.
Hence, at training time, BinaryConnect randomly picks one of two values for each weight, for each
minibatch, for both the forward and backward propagation phases of backprop. However, the SGD
update is accumulated in a real-valued variable storing the parameter.
An interesting analogy to understand BinaryConnect is the DropConnect algorithm [21]. Just like
BinaryConnect, DropConnect only injects noise to the weights during the propagations. Whereas
DropConnect?s noise is added Gaussian noise, BinaryConnect?s noise is a binary sampling process.
In both cases the corrupted value has as expected value the clean original value.
2.4

Clipping

Since the binarization operation is not influenced by variations of the real-valued weights w when its
magnitude is beyond the binary values ?1, and since it is a common practice to bound weights (usually the weight vector) in order to regularize them, we have chosen to clip the real-valued weights
within the [?1, 1] interval right after the weight updates, as per Algorithm 1. The real-valued weights
would otherwise grow very large without any impact on the binary weights.
2.5

A few more tricks
Optimization

No learning rate scaling

Learning rate scaling

SGD
Nesterov momentum
ADAM

15.65%
12.81%

11.45%
11.30%
10.47%

Table 1: Test error rates of a (small) CNN trained on CIFAR-10 depending on optimization method
and on whether the learning rate is scaled with the weights initialization coefficients from [25].
We use Batch Normalization (BN) [26] in all of our experiments, not only because it accelerates
the training by reducing internal covariate shift, but also because it reduces the overall impact of
the weights scale. Moreover, we use the ADAM learning rule [27] in all of our CNN experiments.
Last but not least, we scale the weights learning rates respectively with the weights initialization
coefficients from [25] when optimizing with ADAM, and with the squares of those coefficients
when optimizing with SGD or Nesterov momentum [28]. Table 1 illustrates the effectiveness of
those tricks.
2.6

Test-Time Inference

Up to now we have introduced different ways of training a DNN with on-the-fly weight binarization.
What are reasonable ways of using such a trained network, i.e., performing test-time inference on
new examples? We have considered three reasonable alternatives:
1. Use the resulting binary weights wb (this makes most sense with the deterministic form of
BinaryConnect).
4

2. Use the real-valued weights w, i.e., the binarization only helps to achieve faster training but
not faster test-time performance.
3. In the stochastic case, many different networks can be sampled by sampling a wb for each
weight according to Eq. 2. The ensemble output of these networks can then be obtained by
averaging the outputs from individual networks.
We use the first method with the deterministic form of BinaryConnect. As for the stochastic form
of BinaryConnect, we focused on the training advantage and used the second method in the experiments, i.e., test-time inference using the real-valued weights. This follows the practice of Dropout
methods, where at test-time the ?noise? is removed.
Method

MNIST

CIFAR-10

SVHN

No regularizer
BinaryConnect (det.)
BinaryConnect (stoch.)
50% Dropout

1.30 ? 0.04%
1.29 ? 0.08%
1.18 ? 0.04%
1.01 ? 0.04%

10.64%
9.90%
8.27%

2.44%
2.30%
2.15%

Maxout Networks [29]
Deep L2-SVM [30]
Network in Network [31]
DropConnect [21]
Deeply-Supervised Nets [32]

0.94%
0.87%

11.68%

2.47%

10.41%

2.35%
1.94%
1.92%

9.78%

Table 2: Test error rates of DNNs trained on the MNIST (no convolution and no unsupervised
pretraining), CIFAR-10 (no data augmentation) and SVHN, depending on the method. We see
that in spite of using only a single bit per weight during propagation, performance is not worse
than ordinary (no regularizer) DNNs, it is actually better, especially with the stochastic version,
suggesting that BinaryConnect acts as a regularizer.

Figure 1: Features of the first layer of an MLP trained on MNIST depending on the regularizer. From left to right: no regularizer, deterministic BinaryConnect, stochastic BinaryConnect
and Dropout.

3

Benchmark results

In this section, we show that BinaryConnect acts as regularizer and we obtain near state-of-the-art
results with BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN.
3.1

Permutation-invariant MNIST

MNIST is a benchmark image classification dataset [33]. It consists in a training set of 60000 and
a test set of 10000 28 ? 28 gray-scale images representing digits ranging from 0 to 9. Permutationinvariance means that the model must be unaware of the image (2-D) structure of the data (in other
words, CNNs are forbidden). Besides, we do not use any data-augmentation, preprocessing or unsupervised pretraining. The MLP we train on MNIST consists in 3 hidden layers of 1024 Rectifier
Linear Units (ReLU) [34, 24, 3] and a L2-SVM output layer (L2-SVM has been shown to perform
better than Softmax on several classification benchmarks [30, 32]). The square hinge loss is minimized with SGD without momentum. We use an exponentially decaying learning rate. We use Batch
5

Figure 2: Histogram of the weights of the first layer of an MLP trained on MNIST depending on
the regularizer. In both cases, it seems that the weights are trying to become deterministic to reduce
the training error. It also seems that some of the weights of deterministic BinaryConnect are stuck
around 0, hesitating between ?1 and 1.

Figure 3: Training curves of a CNN on CIFAR-10 depending on the regularizer. The dotted lines
represent the training costs (square hinge losses) and the continuous lines the corresponding validation error rates. Both versions of BinaryConnect significantly augment the training cost, slow down
the training and lower the validation error rate, which is what we would expect from a Dropout
scheme.
Normalization with a minibatch of size 200 to speed up the training. As typically done, we use the
last 10000 samples of the training set as a validation set for early stopping and model selection. We
report the test error rate associated with the best validation error rate after 1000 epochs (we do not
retrain on the validation set). We repeat each experiment 6 times with different initializations. The
results are in Table 2. They suggest that the stochastic version of BinaryConnect can be considered
a regularizer, although a slightly less powerful one than Dropout, in this context.
3.2

CIFAR-10

CIFAR-10 is a benchmark image classification dataset. It consists in a training set of 50000 and
a test set of 10000 32 ? 32 color images representing airplanes, automobiles, birds, cats, deers,
dogs, frogs, horses, ships and trucks. We preprocess the data using global contrast normalization
and ZCA whitening. We do not use any data-augmentation (which can really be a game changer for
this dataset [35]). The architecture of our CNN is:
(2?128C3)?M P 2?(2?256C3)?M P 2?(2?512C3)?M P 2?(2?1024F C)?10SV M (5)
Where C3 is a 3 ? 3 ReLU convolution layer, M P 2 is a 2 ? 2 max-pooling layer, F C a fully
connected layer, and SVM a L2-SVM output layer. This architecture is greatly inspired from VGG
[36]. The square hinge loss is minimized with ADAM. We use an exponentially decaying learning
6

rate. We use Batch Normalization with a minibatch of size 50 to speed up the training. We use the
last 5000 samples of the training set as a validation set. We report the test error rate associated with
the best validation error rate after 500 training epochs (we do not retrain on the validation set). The
results are in Table 2 and Figure 3.
3.3

SVHN

SVHN is a benchmark image classification dataset. It consists in a training set of 604K and a test set
of 26K 32 ? 32 color images representing digits ranging from 0 to 9. We follow the same procedure
that we used for CIFAR-10, with a few notable exceptions: we use half the number of hidden units
and we train for 200 epochs instead of 500 (because SVHN is quite a big dataset). The results are in
Table 2.

4

Related works

Training DNNs with binary weights has been the subject of very recent works [37, 38, 39, 40]. Even
though we share the same objective, our approaches are quite different. [37, 38] do not train their
DNN with Backpropagation (BP) but with a variant called Expectation Backpropagation (EBP).
EBP is based on Expectation Propagation (EP) [41], which is a variational Bayes method used to do
inference in probabilistic graphical models. Let us compare their method to ours:
? It optimizes the weights posterior distribution (which is not binary). In this regard, our
method is quite similar as we keep a real-valued version of the weights.
? It binarizes both the neurons outputs and weights, which is more hardware friendly than
just binarizing the weights.
? It yields a good classification accuracy for fully connected networks (on MNIST) but not
(yet) for ConvNets.
[39, 40] retrain neural networks with ternary weights during forward and backward propagations,
i.e.:
? They train a neural network with high-precision,
? After training, they ternarize the weights to three possible values ?H, 0 and +H and adjust
H to minimize the output error,
? And eventually, they retrain with ternary weights during propagations and high-precision
weights during updates.
By comparison, we train all the way with binary weights during propagations, i.e., our training procedure could be implemented with efficient specialized hardware avoiding the forward and backward
propagations multiplications, which amounts to about 2/3 of the multiplications (cf. Algorithm 1).

5

Conclusion and future works

We have introduced a novel binarization scheme for weights during forward and backward propagations called BinaryConnect. We have shown that it is possible to train DNNs with BinaryConnect on
the permutation invariant MNIST, CIFAR-10 and SVHN datasets and achieve nearly state-of-the-art
results. The impact of such a method on specialized hardware implementations of deep networks
could be major, by removing the need for about 2/3 of the multiplications, and thus potentially allowing to speed-up by a factor of 3 at training time. With the deterministic version of BinaryConnect
the impact at test time could be even more important, getting rid of the multiplications altogether
and reducing by a factor of at least 16 (from 16 bits single-float precision to single bit precision)
the memory requirement of deep networks, which has an impact on the memory to computation
bandwidth and on the size of the models that can be run. Future works should extend those results to
other models and datasets, and explore getting rid of the multiplications altogether during training,
by removing their need from the weight update computation.
7

6

Acknowledgments

We thank the reviewers for their many constructive comments. We also thank Roland Memisevic for
helpful discussions. We thank the developers of Theano [42, 43], a Python library which allowed
us to easily develop a fast and optimized code for GPU. We also thank the developers of Pylearn2
[44] and Lasagne, two Deep Learning libraries built on the top of Theano. We are also grateful for
funding from NSERC, the Canada Research Chairs, Compute Canada, and CIFAR.

References
[1] Geoffrey Hinton, Li Deng, George E. Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior,
Vincent Vanhoucke, Patrick Nguyen, Tara Sainath, and Brian Kingsbury. Deep neural networks for acoustic modeling in speech recognition. IEEE Signal Processing Magazine, 29(6):82?97, Nov. 2012.
[2] Tara Sainath, Abdel rahman Mohamed, Brian Kingsbury, and Bhuvana Ramabhadran. Deep convolutional
neural networks for LVCSR. In ICASSP 2013, 2013.
[3] A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet classification with deep convolutional neural
networks. In NIPS?2012. 2012.
[4] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru
Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. Technical report,
arXiv:1409.4842, 2014.
[5] Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul.
Fast and robust neural network joint models for statistical machine translation. In Proc. ACL?2014, 2014.
[6] Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. In
NIPS?2014, 2014.
[7] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning
to align and translate. In ICLR?2015, arXiv:1409.0473, 2015.
[8] Rajat Raina, Anand Madhavan, and Andrew Y. Ng. Large-scale deep unsupervised learning using graphics
processors. In ICML?2009, 2009.
[9] Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic language
model. Journal of Machine Learning Research, 3:1137?1155, 2003.
[10] J. Dean, G.S Corrado, R. Monga, K. Chen, M. Devin, Q.V. Le, M.Z. Mao, M.A. Ranzato, A. Senior,
P. Tucker, K. Yang, and A. Y. Ng. Large scale distributed deep networks. In NIPS?2012, 2012.
[11] Sang Kyun Kim, Lawrence C McAfee, Peter Leonard McMahon, and Kunle Olukotun. A highly scalable
restricted Boltzmann machine FPGA implementation. In Field Programmable Logic and Applications,
2009. FPL 2009. International Conference on, pages 367?372. IEEE, 2009.
[12] Tianshi Chen, Zidong Du, Ninghui Sun, Jia Wang, Chengyong Wu, Yunji Chen, and Olivier Temam.
Diannao: A small-footprint high-throughput accelerator for ubiquitous machine-learning. In Proceedings
of the 19th international conference on Architectural support for programming languages and operating
systems, pages 269?284. ACM, 2014.
[13] Yunji Chen, Tao Luo, Shaoli Liu, Shijin Zhang, Liqiang He, Jia Wang, Ling Li, Tianshi Chen, Zhiwei
Xu, Ninghui Sun, et al. Dadiannao: A machine-learning supercomputer. In Microarchitecture (MICRO),
2014 47th Annual IEEE/ACM International Symposium on, pages 609?622. IEEE, 2014.
[14] Lorenz K Muller and Giacomo Indiveri. Rounding methods for neural networks with low resolution
synaptic weights. arXiv preprint arXiv:1504.05767, 2015.
[15] Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with limited numerical precision. In ICML?2015, 2015.
[16] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Low precision arithmetic for deep learning. In Arxiv:1412.7024, ICLR?2015 Workshop, 2015.
[17] Thomas M Bartol, Cailey Bromer, Justin P Kinney, Michael A Chirillo, Jennifer N Bourne, Kristen M
Harris, and Terrence J Sejnowski. Hippocampal spine head sizes are highly precise. bioRxiv, 2015.
[18] Alex Graves. Practical variational inference for neural networks. In J. Shawe-Taylor, R.S. Zemel, P.L.
Bartlett, F. Pereira, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems
24, pages 2348?2356. Curran Associates, Inc., 2011.
[19] Nitish Srivastava. Improving neural networks with dropout. Master?s thesis, U. Toronto, 2013.
[20] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout:
A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research,
15:1929?1958, 2014.

8

[21] Li Wan, Matthew Zeiler, Sixin Zhang, Yann LeCun, and Rob Fergus. Regularization of neural networks
using dropconnect. In ICML?2013, 2013.
[22] J.P. David, K. Kalach, and N. Tittley. Hardware complexity of modular multiplication and exponentiation.
Computers, IEEE Transactions on, 56(10):1308?1319, Oct 2007.
[23] R. Collobert. Large Scale Machine Learning. PhD thesis, Universit?e de Paris VI, LIP6, 2004.
[24] X. Glorot, A. Bordes, and Y. Bengio. Deep sparse rectifier neural networks. In AISTATS?2011, 2011.
[25] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In AISTATS?2010, 2010.
[26] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. 2015.
[27] Diederik Kingma and Jimmy Ba.
arXiv:1412.6980, 2014.

Adam: A method for stochastic optimization.

arXiv preprint

[28] Yu Nesterov. A method for unconstrained convex minimization problem with the rate of convergence
o(1/k2 ). Doklady AN SSSR (translated as Soviet. Math. Docl.), 269:543?547, 1983.
[29] Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout
networks. Technical Report Arxiv report 1302.4389, Universit?e de Montr?eal, February 2013.
[30] Yichuan Tang. Deep learning using linear support vector machines. Workshop on Challenges in Representation Learning, ICML, 2013.
[31] Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400, 2013.
[32] Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen Tu. Deeply-supervised
nets. arXiv preprint arXiv:1409.5185, 2014.
[33] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278?2324, November 1998.
[34] V. Nair and G.E. Hinton. Rectified linear units improve restricted Boltzmann machines. In ICML?2010,
2010.
[35] Benjamin Graham. Spatially-sparse convolutional neural networks. arXiv preprint arXiv:1409.6070,
2014.
[36] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.
[37] Daniel Soudry, Itay Hubara, and Ron Meir. Expectation backpropagation: Parameter-free training of
multilayer neural networks with continuous or discrete weights. In NIPS?2014, 2014.
[38] Zhiyong Cheng, Daniel Soudry, Zexi Mao, and Zhenzhong Lan. Training binary multilayer neural networks for image classification using expectation backpropgation. arXiv preprint arXiv:1503.03562, 2015.
[39] Kyuyeon Hwang and Wonyong Sung. Fixed-point feedforward deep neural network design using
weights+ 1, 0, and- 1. In Signal Processing Systems (SiPS), 2014 IEEE Workshop on, pages 1?6. IEEE,
2014.
[40] Jonghong Kim, Kyuyeon Hwang, and Wonyong Sung. X1000 real-time phoneme recognition vlsi using
feed-forward deep neural networks. In Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE
International Conference on, pages 7510?7514. IEEE, 2014.
[41] Thomas P Minka. Expectation propagation for approximate bayesian inference. In UAI?2001, 2001.
[42] James Bergstra, Olivier Breuleux, Fr?ed?eric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientific Computing Conference (SciPy), June 2010.
Oral Presentation.
[43] Fr?ed?eric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian J. Goodfellow, Arnaud Bergeron,
Nicolas Bouchard, and Yoshua Bengio. Theano: new features and speed improvements. Deep Learning
and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.
[44] Ian J. Goodfellow, David Warde-Farley, Pascal Lamblin, Vincent Dumoulin, Mehdi Mirza, Razvan Pascanu, James Bergstra, Fr?ed?eric Bastien, and Yoshua Bengio. Pylearn2: a machine learning research
library. arXiv preprint arXiv:1308.4214, 2013.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 6310-phased-lstm-accelerating-recurrent-network-training-for-long-or-event-based-sequences.pdf

Phased LSTM: Accelerating Recurrent Network
Training for Long or Event-based Sequences

Daniel Neil, Michael Pfeiffer, and Shih-Chii Liu
Institute of Neuroinformatics
University of Zurich and ETH Zurich
Zurich, Switzerland 8057
{dneil, pfeiffer, shih}@ini.uzh.ch

Abstract
Recurrent Neural Networks (RNNs) have become the state-of-the-art choice for
extracting patterns from temporal sequences. However, current RNN models are
ill-suited to process irregularly sampled data triggered by events generated in
continuous time by sensors or other neurons. Such data can occur, for example,
when the input comes from novel event-driven artificial sensors that generate
sparse, asynchronous streams of events or from multiple conventional sensors with
different update intervals. In this work, we introduce the Phased LSTM model,
which extends the LSTM unit by adding a new time gate. This gate is controlled
by a parametrized oscillation with a frequency range that produces updates of the
memory cell only during a small percentage of the cycle. Even with the sparse
updates imposed by the oscillation, the Phased LSTM network achieves faster
convergence than regular LSTMs on tasks which require learning of long sequences.
The model naturally integrates inputs from sensors of arbitrary sampling rates,
thereby opening new areas of investigation for processing asynchronous sensory
events that carry timing information. It also greatly improves the performance of
LSTMs in standard RNN applications, and does so with an order-of-magnitude
fewer computes at runtime.

1

Introduction

Interest in recurrent neural networks (RNNs) has greatly increased in recent years, since larger
training databases, more powerful computing resources, and better training algorithms have enabled
breakthroughs in both processing and modeling of temporal sequences. Applications include speech
recognition [13], natural language processing [1, 20], and attention-based models for structured
prediction [5, 29]. RNNs are attractive because they equip neural networks with memories, and
the introduction of gating units such as LSTM and GRU [16, 6] has greatly helped in making the
learning of these networks manageable. RNNs are typically modeled as discrete-time dynamical
systems, thereby implicitly assuming a constant sampling rate of input signals, which also becomes
the update frequency of recurrent and feed-forward units. Although early work such as [25, 10, 4]
has realized the resulting limitations and suggested continuous-time dynamical systems approaches
towards RNNs, the great majority of modern RNN implementations uses fixed time steps.
Although fixed time steps are perfectly suitable for many RNN applications, there are several
important scenarios in which constant update rates impose constraints that affect the precision and
efficiency of RNNs. Many real-world tasks for autonomous vehicles or robots need to integrate input
from a variety of sensors, e.g. for vision, audition, distance measurements, or gyroscopes. Each sensor
may have its own data sampling rate, and short time steps are necessary to deal with sensors with
high sampling frequencies. However, this leads to an unnecessarily higher computational load and
30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

xt

xt

xt

Input it
Gate

Output Gate ot

xt

xt

t
Output
Gate

it Input
Gate

ct

ht

xt

c~t

ct

kt

Forget Gate ft

Forget Gate ft

xt

xt

(a)

t
ot

kt

ht

(b)

Figure 1: Model architecture. (a) Standard LSTM model. (b) Phased LSTM model, with time gate kt
controlled by timestamp t. In the Phased LSTM formulation, the cell value ct and the hidden output
ht can only be updated during an ?open? phase; otherwise, the previous values are maintained.

power consumption so that all units in the network can be updated with one time step. An interesting
new application area is processing of event-based sensors, which are data-driven, and record stimulus
changes in the world with short latencies and accurate timing. Processing the asynchronous outputs of
such sensors with time-stepped models would require high update frequencies, thereby counteracting
the potential power savings of event-based sensors. And finally there is an interest coming from
computational neuroscience, since brains can be viewed loosely as very large RNNs. However,
biological neurons communicate with spikes, and therefore perform asynchronous, event-triggered
updates in continuous time. This work presents a novel RNN model which can process inputs sampled
at asynchronous times and is described further in the following sections.

2

Model Description

Long short-term memory (LSTM) units [16] (Fig. 1(a)) are an important ingredient for modern deep
RNN architectures. We first define their update equations in the commonly-used version from [12]:
it
ft
ct
ot
ht

= ?i (xt Wxi + ht?1 Whi + wci  ct?1 + bi )
= ?f (xt Wxf + ht?1 Whf + wcf  ct?1 + bf )
= ft  ct?1 + it  ?c (xt Wxc + ht?1 Whc + bc )
= ?o (xt Wxo + ht?1 Who + wco  ct + bo )
= ot  ?h (ct )

(1)
(2)
(3)
(4)
(5)

The main difference to classical RNNs is the use of the gating functions it , ft , ot , which represent
the input, forget, and output gate at time t respectively. ct is the cell activation vector, whereas xt
and ht represent the input feature vector and the hidden output vector respectively. The gates use the
typical sigmoidal nonlinearities ?i , ?f , ?o and tanh nonlinearities ?c , and ?h with weight parameters
Whi , Whf , Who , Wxi , Wxf , and Wxo , which connect the different inputs and gates with the memory
cells and outputs, as well as biases bi , bf , and bo . The cell state ct itself is updated with a fraction of
the previous cell state that is controlled by ft , and a new input state created from the element-wise
(Hadamard) product, denoted by , of it and the output of the cell state nonlinearity ?c . Optional
peephole [11] connection weights wci , wcf , wco further influence the operation of the input, forget,
and output gates.
The Phased LSTM model extends the LSTM model by adding a new time gate, kt (Fig. 1(b)). The
opening and closing of this gate is controlled by an independent rhythmic oscillation specified by
three parameters; updates to the cell state ct and ht are permitted only when the gate is open. The
first parameter, ? , controls the real-time period of the oscillation. The second, ron , controls the ratio
of the duration of the ?open? phase to the full period. The third, s, controls the phase shift of the
oscillation to each Phased LSTM cell. All parameters can be learned during the training process.
Though other variants are possible, we propose here a particularly successful linearized formulation
2

t

j-2

j-1

j

Output

Output

Output

...

Layer 2

...

Layer 2

Layer 1
Input

...

Layer 2

Layer 1

Layer 1
tj-2

Input

kt Openness

closed

ct State

open

Input

tj-1

Input

1

tj

(a)

2
Time

3

4

(b)

Figure 2: Diagram of Phased LSTM behaviour. (a) Top: The rhythmic oscillations to the time gates of
3 different neurons; the period ? and the phase shift s is shown for the lowest neuron. The parameter
ron is the ratio of the open period to the total period ? . Bottom: Note that in a multilayer scenario,
the timestamp is distributed to all layers which are updated at the same time point. (b) Illustration of
Phased LSTM operation. A simple linearly increasing function is used as an input. The time gate
kt of each neuron has a different ? , identical phase shift s, and an open ratio ron of 0.05. Note that
the input (top panel) flows through the time gate kt (middle panel) to be held as the new cell state ct
(bottom panel) only when kt is open.
of the time gate, with analogy to the rectified linear unit that propagates gradients well:
?
2?t
1
?
?
,
if ?t < ron
?
?
2
? ron
(t ? s) mod ?
2?t
1
(6)
,
kt =
?t =
2?
, if ron < ?t < ron
?
?
?
?
r
2
on
?
?
??t ,
otherwise
?t is an auxiliary variable, which represents the phase inside the rhythmic cycle. The gate kt has three
phases (see Fig. 2a): in the first two phases, the "openness" of the gate rises from 0 to 1 (first phase)
and drops from 1 to 0 (second phase). During the third phase, the gate is closed and the previous cell
state is maintained. The leak with rate ? is active in the closed phase, and plays a similar role as the
leak in a parametric ?leaky? rectified linear unit [15] by propagating important gradient information
even when the gate is closed. Note that the linear slopes of kt during the open phases of the time gate
allow effective transmission of error gradients.
In contrast to traditional RNNs, and even sparser variants of RNNs [19], updates in Phased LSTM
can optionally be performed at irregularly sampled time points tj . This allows the RNNs to work with
event-driven, asynchronously sampled input data. We use the shorthand notation cj = ctj for cell
states at time tj (analogously for other gates and units), and let cj?1 denote the state at the previous
update time tj?1 . We can then rewrite the regular LSTM cell update equations for cj and hj (from
Eq. 3 and Eq. 5), using proposed cell updates cej and hej mediated by the time gate kj :
cej = fj  cj?1 + ij  ?c (xj Wxc + hj?1 Whc + bc )
(7)
cj = kj  cej + (1 ? kj )  cj?1
(8)
e
hj = oj  ?h (cej )
(9)
hj = kj  hej + (1 ? kj )  hj?1

(10)

A schematic of Phased LSTM with its parameters can be found in Fig. 2a, accompanied by an
illustration of the relationship between the time, the input, the time gate kt , and the state ct in Fig. 2b.
One key advantage of this Phased LSTM formulation lies in the rate of memory decay. For the simple
task of keeping an initial memory state c0 as long as possible without receiving additional inputs (i.e.
ij = 0 at all time steps tj ), a standard LSTM with a nearly fully-opened forget gate (i.e. fj = 1 ? )
after n update steps would contain
cn = fn  cn?1 = (1 ? )  (fn?1  cn?2 ) = . . . = (1 ? )n  c0 .
(11)
3

1.0

1.0

0.5

0.5

0.5

0.0

0.0

0.0

?0.5

?0.5

?0.5

?1.0

?1.0
16 18 20 22 24 26 28 30
Time [ms]

(a)

Accuracy at 70 Epochs [%]

100

1.0

?1.0
16 18 20 22 24 26 28 30
Time [ms]

(b)

90

Phased LSTM
BN LSTM
LSTM

80

70

60

50

16 18 20 22 24 26 28 30
Time [ms]

(c)

High
Standard resolution Async.
sampling sampling sampling

(d)

Figure 3: Frequency discrimination task. The network is trained to discriminate waves of different
frequency sets (shown in blue and gray); every circle is an input point. (a) Standard condition: the
data is regularly sampled every 1 ms. (b) High resolution sampling condition: new input points
are gathered every 0.1ms. (c) Asynchronous sampling condition: new input points are presented at
intervals of 0.02 ms to 10 ms. (d) The accuracy of Phased LSTM under the three sampling conditions
is maintained, but the accuracy of the BN-LSTM and standard LSTM drops significantly in the
sampling conditions (b) and (c). Error bars indicate standard deviation over 5 runs.

This means the memory for  < 1 decays exponentially with every time step. Conversely, the Phased
LSTM state only decays during the open periods of the time gate, but maintains a perfect memory
during its closed phase, i.e. cj = cj?? if kt = 0 for tj?? ? t ? tj . Thus, during a single oscillation
period of length ? , the units only update during a duration of ron ? ? , which will result in substantially
fewer than n update steps. Because of this cyclic memory, Phased LSTM can have much longer and
adjustable memory length via the parameter ? .
The oscillations impose sparse updates of the units, therefore substantially decreasing the total number
of updates during network operation. During training, this sparseness ensures that the gradient is
required to backpropagate through fewer updating timesteps, allowing an undecayed gradient to be
backpropagated through time and allowing faster learning convergence. Similar to the shielding of
the cell state ct (and its gradient) by the input gates and forget gates of the LSTM, the time gate
prevents external inputs and time steps from dispersing and mixing the gradient of the cell state.

3

Results

In the following sections, we investigate the advantages of the Phased LSTM model in a variety
of scenarios that require either precise timing of updates or learning from a long sequence. For all
the results presented here, the networks were trained with Adam [18] set to default learning rate
parameters, using Theano [2] with Lasagne [9]. Unless otherwise specified, the leak rate was set to
? = 0.001 during training and ? = 0 during test. The phase shift, s, for each neuron was uniformly
chosen from the interval [0, ? ]. The parameters ? and s were learned during training, while the open
ratio ron was fixed at 0.05 and not adjusted during training, except in the first task to demonstrate
that the model can train successfully while learning all parameters.
3.1

Frequency Discrimination Task

In this first experiment, the network is trained to distinguish two classes of sine waves from different
frequency sets: those with a period in a target range T ? U(5, 6), and those outside the range, i.e.
T ? {U(1, 5) ? U(6, 100)}, using U(a, b) for the uniform distribution on the interval (a, b). This
task illustrates the advantages of Phased LSTM, since it involves a periodic stimulus and requires
fine timing discrimination. The inputs are presented as pairs hy, ti, where y is the amplitude and t
the timestamp of the sample from the input sine wave.
Figure 3 illustrates the task: the blue curves must be separated from the lighter curves based on
the samples shown as circles. We evaluate three conditions for sampling the input signals: In the
standard condition (Fig. 3a), the sine waves are regularly sampled every 1 ms; in the oversampled
4

90

10 0

80

10 -1

Phased LSTM
BN LSTM
LSTM

75
70

MSE

Accuracy [%]

85

65
60
55

10 -3
10

50
45
0

10 -2

50

100

150 200
Epoch

250

-4

10 -5
0

300

(a)

LSTM
PLSTM (? ? e U(0;
PLSTM (? ? e U(2;
PLSTM (? ? e U(4;
PLSTM (? ? e U(6;

20

)
)
6)
)
8)
)
2)
4)

40
60
Epoch

80

100

(b)

Figure 4: (a) Accuracy during training for the superimposed frequencies task. The Phased LSTM
outperforms both LSTM and BN-LSTM while exhibiting lower variance. Shading shows maximum
and minimum over 5 runs, while dark lines indicate the mean. (b) Mean-squared error over training
on the addition task, with an input length of 500. Note that longer periods accelerate learning
convergence.

condition (Fig. 3b), the sine waves are regularly sampled every 0.1 ms, resulting in ten times
as many data points. Finally, in the asynchronously sampled condition (Fig. 3c), samples are
collected at asynchronous times over the duration of the input. Additionally, the sine waves have
a uniformly drawn random phase shift from all possible shifts, random numbers of samples drawn
from U(15, 125), a random duration drawn from U(15, 125), and a start time drawn from U(0, 125 ?
duration). The number of samples in the asynchronous and standard sampling condition is equal.
The classes were approximately balanced, yielding a 50% chance success rate.
Single-layer RNNs are trained on this data, each repeated with five random initial seeds. We compare
our Phased LSTM configuration to regular LSTM, and batch-normalized (BN) LSTM which has
found success in certain applications [14]. For the regular LSTM and the BN-LSTM, the timestamp
is used as an additional input feature dimension; for the Phased LSTM, the time input controls
the time gates kt . The architecture consists of 2-110-2 neurons for the LSTM and BN-LSTM, and
1-110-2 for the Phased LSTM. The oscillation periods of the Phased LSTMs are drawn uniformly in
the exponential space to give a wide variety of applicable frequencies, i.e., ? ? exp(U(0, 3)). All
other parameters match between models where applicable. The default LSTM parameters are given
in the Lasagne Theano implementation, and were kept for LSTM, BN-LSTM, and Phased LSTM.
Appropriate gate biasing was investigated but did not resolve the discrepancies between the models.
All three networks excel under standard sampling conditions as expected, as seen in Fig. 3d (left).
However, for the same number of epochs, increasing the data sampling by a factor of ten has
devastating effects for both LSTM and BN-LSTM, dropping their accuracy down to near chance
(Fig. 3d, middle). Presumably, if given enough training iterations, their accuracies would return to
the normal baseline. However, for the oversampled condition, Phased LSTM actually increases in
accuracy, as it receives more information about the underlying waveform. Finally, if the updates are
not evenly spaced and are instead sampled at asynchronous times, even when controlled to have the
same number of points as the standard sampling condition, it appears to make the problem rather
challenging for traditional state-of-the-art models (Fig. 3d, right). However, the Phased LSTM has
no difficulty with the asynchronously sampled data, because the time gates kt do not need regular
updates and can be correctly sampled at any continuous time within the period.
We extend the previous task by training the same RNN architectures on signals composed of two
sine waves. The goal is to distinguish signals composed of sine waves with periods T1 ? U(5, 6)
and T2 ? U(13, 15), each with independent phase, from signals composed of sine waves with
periods T1 ? {U(1, 5) ? U(6, 100)} and T2 ? {U(1, 13) ? U(15, 100)}, again with independent
phase. Despite being significantly more challenging, Fig. 4a demonstrates how quickly the Phased
LSTM converges to the correct solution compared to the standard approaches, using exactly the same
parameters. Additionally, the Phased LSTM appears to exhibit very low variance during training.
5

3
1

2

0
?10 5

1
2

Time [us]

(a)

(b)

3

(c)

Figure 5: N-MNIST experiment. (a) Sketch of digit movement seen by the image sensor. (b)
Frame-based representation of an ?8? digit from the N-MNIST dataset [24] obtained by integrating all
input spikes for each pixel. (c) Spatio-temporal representation of the digit, presented in three saccades
as in (a). Note that this representation shows the digit more clearly than the blurred frame-based one.

3.2

Adding Task

To investigate how introducing time gates helps learning when long memory is required, we revisit
an original LSTM task called the adding task [16]. In this task, a sequence of random numbers
is presented along with an indicator input stream. When there is a 0 in the indicator input stream,
the presented value should be ignored; a 1 indicates that the value should be added. At the end of
presentation the network produces a sum of all indicated values. Unlike the previous tasks, there is no
inherent periodicity in the input, and it is one of the original tasks that LSTM was designed to solve
well. This would seem to work against the advantages of Phased LSTM, but using a longer period for
the time gate kt could allow more effective training as a unit opens only a for a few timesteps during
training.
In this task, a sequence of numbers (of length 490 to 510) was drawn from U(?0.5, 0.5). Two
numbers in this stream of numbers are marked for addition: one from the first 10% of numbers
(drawn with uniform probability) and one in the last half (drawn with uniform probability), producing
a model of a long and noisy stream of data with only few significant points. Importantly, this should
challenge the Phased LSTM model because there is no inherent periodicity and every timestep could
contain the important marked points.
The same network architecture is used as before. The period ? was drawn uniformly in the exponential domain, comparing four sampling intervals exp(U(0, 2)), exp(U(2, 4)), exp(U(4, 6)), and
exp(U(6, 8)). Note that despite different ? values, the total number of LSTM updates remains approximately the same, since the overall sparseness is set by ron . However, a longer period ? provides
a longer jump through the past timesteps for the gradient during backpropagation-through-time.
Moreover, we investigate whether the model can learn longer sequences more effectively when longer
periods are used. By varying the period ? , the results in Fig. 4b show longer ? accelerates training of
the network to learn much longer sequences faster.
3.3

N-MNIST Event-Based Visual Recognition

To test performance on real-world asynchronously sampled data, we make use of the publiclyavailable N-MNIST [24] dataset for neuromorphic vision. The recordings come from an event-based
vision sensor that is sensitive to local temporal contrast changes [26]. An event is generated from
a pixel when its local contrast change exceeds a threshold. Every event is encoded as a 4-tuple
hx, y, p, ti with position x, y of the pixel, a polarity bit p (indicating a contrast increase or decrease),
and a timestamp t indicating the time when the event is generated. The recordings consist of events
generated by the vision sensor while the sensor undergoes three saccadic movements facing a static
digit from the MNIST dataset (Fig. 5a). An example of the event responses can be seen in Fig. 5c).
In previous work using event-based input data [21, 23], the timing information was sometimes
removed and instead a frame-based representation was generated by computing the pixel-wise
event-rate over some time period (as shown in Fig. 5(b)). Note that the spatio-temporal surface of
6

Table 1: Accuracy on N-MNIST
CNN
BN-LSTM
Phased LSTM (? = 100ms)
Accuracy at Epoch 1
Train/test ? = 0.75

73.81% ? 3.5
95.02% ? 0.3

40.87% ? 13.3
96.93% ? 0.12

90.32% ? 2.3
97.28% ? 0.1

Test with ? = 0.4
Test with ? = 1.0

90.67% ? 0.3
94.99% ? 0.3

94.79% ? 0.03
96.55% ? 0.63

95.11% ? 0.2
97.27% ? 0.1

3153 per neuron

159 ? 2.8 per neuron

LSTM Updates

?

events in Fig. 5(c) reveals details of the digit much more clearly than in the blurred frame-based
representation.The Phased LSTM allows us to operate directly on such spatio-temporal event streams.
Table 1 summarizes classification results for three different network types: a CNN trained on framebased representations of N-MNIST digits and two RNNs, a BN-LSTM and a Phased LSTM, trained
directly on the event streams. Regular LSTM is not shown, as it was found to perform worse. The
CNN was comprised of three alternating layers of 8 kernels of 5x5 convolution with a leaky ReLU
nonlinearity and 2x2 max-pooling, which were then fully-connected to 256 neurons, and finally fullyconnected to the 10 output classes. The event pixel address was used to produce a 40-dimensional
embedding via a learned embedding matrix [9], and combined with the polarity to produce the input.
Therefore, the network architecture was 41-110-10 for the Phased LSTM and 42-110-10 for the
BN-LSTM, with the time given as an extra input dimension to the BN-LSTM.
Table 1 shows that Phased LSTM trains faster than alternative models and achieves much higher
accuracy with a lower variance even within the first epoch of training. We further define a factor, ?,
which represents the probability that an event is included, i.e. ? = 1.0 means all events are included.
The RNN models are trained with ? = 0.75, and again the Phased LSTM achieves slightly higher
performance than the BN-LSTM model. When testing with ? = 0.4 (fewer events) and ? = 1.0 (more
events) without retraining, both RNN models perform well and greatly outperform the CNN. This is
because the accumulated statistics of the frame-based input to the CNN change drastically when the
overall spike rates are altered. The Phased LSTM RNNs seem to have learned a stable spatio-temporal
surface on the input and are only slightly altered by sampling it more or less frequently.
Finally, as each neuron of the Phased LSTM only updates about 5% of the time, on average, 159
updates are needed in comparison to the 3153 updates needed per neuron of the BN-LSTM, leading
to an approximate twenty-fold reduction in run time compute cost. It is also worth noting that these
results form a new state-of-the-art accuracy for this dataset [24, 7].
3.4

Visual-Auditory Sensor Fusion for Lip Reading

Finally, we demonstrate the use of Phased LSTM on a task involving sensors with different sampling
rates. Few RNN models ever attempt to merge sensors of different input frequencies, although the
sampling rates can vary substantially. For this task, we use the GRID dataset [8]. This corpus contains
video and audio of 30 speakers each uttering 1000 sentences composed of a fixed grammar and a
constrained vocabulary of 51 words. The data was randomly divided into a 90%/10% train-test set.
An OpenCV [17] implementation of a face detector was used on the video stream to extract the face
which was then resized to grayscale 48x48 pixels. The goal here is to obtain a model that can use
audio alone, video alone, or both inputs to robustly classify the sentence. However, since the audio
alone is sufficient to achieve greater than 99% accuracy, sensor modalities were randomly masked to
zero during training to encourage robustness towards sensory noise and loss.
The network architecture first separately processes video and audio data before merging them in
two RNN layers that receive both modalities. The video stream uses three alternating layers of 16
kernels of 5x5 convolution and 2x2 subsampling to reduce the input of 1x48x48 to 16x2x2, which is
then used as the input to 110 recurrent units. The audio stream connects the 39-dimensional MFCCs
(13 MFCCs with first and second derivatives) to 150 recurrent units. Both streams converge into
the Merged-1 layer with 250 recurrent units, and is connected to a second hidden layer with 250
recurrent units named Merged-2. The output of the Merged-2 layer is fully-connected to 51 output
nodes, which represent the vocabulary of GRID. For the Phased LSTM network, all recurrent units
are Phased LSTM units.
7

Low Res.
Loss

10 -1

MFCCs
Video
Frames

Phased LSTM
BN LSTM
LSTM

10 -2

Video
PLSTM
Merged-1
PLSTM
Merged-2
PLSTM

220

260 300
Time [ms]

(a)

340

0
5
10
15
20
25
30
35

10 -1

High Res.
Loss

Audio
PLSTM

MFCC

kj Openness

Inputs

Time

500
1500 2500
Time [ms]

(b)

10 -2
0

10

20 30
Epoch

40

50

(c)

Figure 6: Lip reading experiment. (a) Inputs and openness of time gates for the lip reading experiment.
Note that the 25fps video frame rate is a multiple of the audio input frequency (100 Hz). Phased
LSTM timing parameters are configured to align to the sampling time of their inputs. (b) Example
input of video (top) and audio (bottom). (c) Test loss using the video stream alone. Video frame rate
is 40ms. Top: low resolution condition, MFCCs computed every 40ms with a network update every
40 ms; Bottom: high resolution condition, MFCCs every 10 ms with a network update every 10 ms.
In the audio and video Phased LSTM layers, we manually align the open periods of the time gates
to the sampling times of the inputs and disable learning of the ? and s parameters (see Fig. 6a).
This prevents presenting zeros or artificial interpolations to the network when data is not present.
In the merged layers, however, the parameters of the time gate are learned, with the period ? of the
first merged layer drawn from U(10, 1000) and the second from U(500, 3000). Fig. 6b shows a
visualization of one frame of video and the complete duration of an audio sample.
During evaluation, all networks achieve greater than 98% accuracy on audio-only and combined
audio-video inputs. However, video-only evaluation with an audio-video capable network proved
the most challenging, so the results in Fig. 6c focus on these results (though result rankings are
representative of all conditions). Two differently-sampled versions of the data were used: In the first
?low resolution? version (Fig. 6c, top), the sampling rate of the MFCCs was matched to the sampling
rate of the 25 fps video. In the second ?high-resolution? condition, the sampling rate was set to the
more common value of 100 Hz sampling frequency (Fig. 6c, bottom and shown in Fig. 6a). The
higher audio sampling rate did not increase accuracy, but allows for a faster latency (10ms instead of
40ms). The Phased LSTM again converges substantially faster than both LSTM and batch-normalized
LSTM. The peak accuracy of 81.15% compares favorably against lipreading-focused state-of-the-art
approaches [28] while avoiding manually-crafted features.

4

Discussion

The Phased LSTM has many surprising advantages. With its rhythmic periodicity, it acts like a
learnable, gated Fourier transform on its input, permitting very fine timing discrimination. Alternatively, the rhythmic periodicity can be viewed as a kind of persistent dropout that preserves state [27],
enhancing model diversity. The rhythmic inactivation can even be viewed as a shortcut to the past
for gradient backpropagation, accelerating training. The presented results support these interpretations, demonstrating the ability to discriminate rhythmic signals and to learn long memory traces.
Importantly, in all experiments, Phased LSTM converges more quickly and theoretically requires
only 5% of the computes at runtime, while often improving in accuracy compared to standard LSTM.
The presented methods can also easily be extended to GRUs [6], and it is likely that even simpler
models, such as ones that use a square-wave-like oscillation, will perform well, thereby making even
more efficient and encouraging alternative Phased LSTM formulations. An inspiration for using
oscillations in recurrent networks comes from computational neuroscience [3], where rhythms have
been shown to play important roles for synchronization and plasticity [22]. Phased LSTMs were
not designed as biologically plausible models, but may help explain some of the advantages and
robustness of learning in large spiking recurrent networks.
8

References
[1] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate.
arXiv preprint arXiv:1409.0473, 2014.
[2] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins, J. Turian, D. Warde-Farley,
and Y. Bengio. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for
scientific computing conference (SciPy), volume 4, page 3, 2010.
[3] G. Buzsaki. Rhythms of the Brain. Oxford University Press, 2006.
[4] G. Cauwenberghs. An analog VLSI recurrent neural network learning a continuous-time trajectory. IEEE
Transactions on Neural Networks, 7(2):346?361, 1996.
[5] K. Cho, A. Courville, and Y. Bengio. Describing multimedia content using attention-based encoder-decoder
networks. IEEE Transactions on Multimedia, 17(11):1875?1886, 2015.
[6] K. Cho, B. van Merrienboer, C. Gulcehre, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078,
2014.
[7] G. K. Cohen, G. Orchard, S. H. Ieng, J. Tapson, R. B. Benosman, and A. van Schaik. Skimming digits:
Neuromorphic classification of spike-encoded images. Frontiers in Neuroscience, 10(184), 2016.
[8] M. Cooke, J. Barker, S. Cunningham, and X. Shao. An audio-visual corpus for speech perception and
automatic speech recognition. The Journal of the Acoustical Society of America, 120(5):2421?2424, 2006.
[9] S. Dieleman et al. Lasagne: First release., Aug. 2015.
[10] K.-I. Funahashi and Y. Nakamura. Approximation of dynamical systems by continuous time recurrent
neural networks. Neural Networks, 6(6):801?806, 1993.
[11] F. A. Gers and J. Schmidhuber. Recurrent nets that time and count. In Neural Networks, 2000. IJCNN
2000, Proceedings of the IEEE-INNS-ENNS International Joint Conference on, volume 3, pages 189?194.
IEEE, 2000.
[12] A. Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.
[13] A. Graves, A.-R. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural networks.
In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages
6645?6649, 2013.
[14] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen, R. Prenger, S. Satheesh, S. Sengupta,
A. Coates, et al. Deep speech: Scaling up end-to-end speech recognition. arXiv preprint arXiv:1412.5567,
2014.
[15] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level performance on
imagenet classification. In The IEEE International Conference on Computer Vision (ICCV), 2015.
[16] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735?1780, 1997.
[17] Itseez. Open source computer vision library. https://github.com/itseez/opencv, 2015.
[18] D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
[19] J. Koutnik, K. Greff, F. Gomez, and J. Schmidhuber. A clockwork rnn. arXiv preprint arXiv:1402.3511,
2014.
[20] T. Mikolov, M. Karafi?t, L. Burget, J. Cernock`y, and S. Khudanpur. Recurrent neural network based
language model. Interspeech, 2:3, 2010.
[21] D. Neil and S.-C. Liu. Effective sensor fusion with event-based sensors and deep network architectures. In
IEEE Int. Symposium on Circuits and Systems (ISCAS), 2016.
[22] B. Nessler, M. Pfeiffer, L. Buesing, and W. Maass. Bayesian computation emerges in generic cortical
microcircuits through spike-timing-dependent plasticity. PLoS Comput Biol, 9(4):e1003037, 2013.
[23] P. O?Connor, D. Neil, S.-C. Liu, T. Delbruck, and M. Pfeiffer. Real-time classification and sensor fusion
with a spiking Deep Belief Network. Frontiers in Neuroscience, 7, 2013.
[24] G. Orchard, A. Jayawant, G. Cohen, and N. Thakor. Converting static image datasets to spiking neuromorphic datasets using saccades. arXiv: 1507.07629, 2015.
[25] B. A. Pearlmutter. Learning state space trajectories in recurrent neural networks. Neural Computation,
1(2):263?269, 1989.
[26] C. Posch, T. Serrano-Gotarredona, B. Linares-Barranco, and T. Delbruck. Retinomorphic event-based
vision sensors: bioinspired cameras with spiking outputs. Proceedings of the IEEE, 102(10):1470?1484,
2014.
[27] S. Semeniuta, A. Severyn, and E. Barth. Recurrent dropout without memory loss. arXiv, arXiv:1603.05118,
2016.
[28] M. Wand, J. Koutn?k, and J. Schmidhuber. Lipreading with long short-term memory. arXiv preprint
arXiv:1601.08188, 2016.
[29] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdinov, R. Zemel, and Y. Bengio. Show, attend
and tell: Neural image caption generation with visual attention. In International Conference on Machine
Learning, 2015.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 6215-on-multiplicative-integration-with-recurrent-neural-networks.pdf

On Multiplicative Integration with
Recurrent Neural Networks
Yuhuai Wu1,? , Saizheng Zhang2,? , Ying Zhang2 , Yoshua Bengio2,4 and Ruslan Salakhutdinov3,4
1
University of Toronto, 2 MILA, Universit? de Montr?al, 3 Carnegie Mellon University, 4 CIFAR
ywu@cs.toronto.edu,2 {firstname.lastname}@umontreal.ca,rsalakhu@cs.cmu.edu

Abstract
We introduce a general and simple structural design called ?Multiplicative Integration? (MI) to improve recurrent neural networks (RNNs). MI changes the way in
which information from difference sources flows and is integrated in the computational building block of an RNN, while introducing almost no extra parameters.
The new structure can be easily embedded into many popular RNN models, including LSTMs and GRUs. We empirically analyze its learning behaviour and conduct
evaluations on several tasks using different RNN models. Our experimental results
demonstrate that Multiplicative Integration can provide a substantial performance
boost over many of the existing RNN models.

1

Introduction

Recently there has been a resurgence of new structural designs for recurrent neural networks (RNNs)
[1, 2, 3]. Most of these designs are derived from popular structures including vanilla RNNs, Long
Short Term Memory networks (LSTMs) [4] and Gated Recurrent Units (GRUs) [5]. Despite of their
varying characteristics, most of them share a common computational building block, described by the
following equation:
?(Wx + Uz + b),
(1)
where x ? Rn and z ? Rm are state vectors coming from different information sources, W ? Rd?n
and U ? Rd?m are state-to-state transition matrices, and b is a bias vector. This computational
building block serves as a combinator for integrating information flow from the x and z by a sum
operation ?+?, followed by a nonlinearity ?. We refer to it as the additive building block. Additive
building blocks are widely implemented in various state computations in RNNs (e.g. hidden state
computations for vanilla-RNNs, gate/cell computations of LSTMs and GRUs.
In this work, we propose an alternative design for constructing the computational building block by
changing the procedure of information integration. Specifically, instead of utilizing sum operation
?+", we propose to use the Hadamard product ?? to fuse Wx and Uz:
?(Wx  Uz + b)

(2)

The result of this modification changes the RNN from first order to second order [6], while introducing
no extra parameters. We call this kind of information integration design a form of Multiplicative
Integration. The effect of multiplication naturally results in a gating type structure, in which Wx
and Uz are the gates of each other. More specifically, one can think of the state-to-state computation
Uz (where for example z represents the previous state) as dynamically rescaled by Wx (where
for example x represents the input). Such rescaling does not exist in the additive building block, in
which Uz is independent of x. This relatively simple modification brings about advantages over the
additive building block as it alters RNN?s gradient properties, which we discuss in detail in the next
section, as well as verify through extensive experiments.
?

Equal contribution.

30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

In the following sections, we first introduce a general formulation of Multiplicative Integration. We
then compare it to the additive building block on several sequence learning tasks, including character
level language modelling, speech recognition, large scale sentence representation learning using a
Skip-Thought model, and teaching a machine to read and comprehend for a question answering
task. The experimental results (together with several existing state-of-the-art models) show that
various RNN structures (including vanilla RNNs, LSTMs, and GRUs) equipped with Multiplicative
Integration provide better generalization and easier optimization. Its main advantages include: (1) it
enjoys better gradient properties due to the gating effect. Most of the hidden units are non-saturated;
(2) the general formulation of Multiplicative Integration naturally includes the regular additive
building block as a special case, and introduces almost no extra parameters compared to the additive
building block; and (3) it is a drop-in replacement for the additive building block in most of the
popular RNN models, including LSTMs and GRUs. It can also be combined with other RNN training
techniques such as Recurrent Batch Normalization [7]. We further discuss its relationship to existing
models, including Hidden Markov Models (HMMs) [8], second order RNNs [6] and Multiplicative
RNNs [9].

2

Structure Description and Analysis

2.1

General Formulation of Multiplicative Integration

The key idea behind Multiplicative Integration is to integrate different information flows Wx and Uz,
by the Hadamard product ??. A more general formulation of Multiplicative Integration includes
two more bias vectors ?1 and ?2 added to Wx and Uz:
?((Wx + ?1 )  (Uz + ?2 ) + b)

(3)

d

where ?1 , ?2 ? R are bias vectors. Notice that such formulation contains the first order terms as
in a additive building block, i.e., ?1  Uht?1 + ?2  Wxt . In order to make the Multiplicative
Integration more flexible, we introduce another bias vector ? ? Rd to gate2 the term Wx  Uz,
obtaining the following formulation:
?(?  Wx  Uz + ?1  Uz + ?2  Wx + b),

(4)

Note that the number of parameters of the Multiplicative Integration is about the same as that of the
additive building block, since the number of new parameters (?, ?1 and ?2 ) are negligible compared
to total number of parameters. Also, Multiplicative Integration can be easily extended to LSTMs
and GRUs3 , that adopt vanilla building blocks for computing gates and output states, where one can
directly replace them with the Multiplicative Integration. More generally, in any kind of structure
where k information flows (k ? 2) are involved (e.g. residual networks [10]), one can implement
pairwise Multiplicative Integration for integrating all k information sources.
2.2 Gradient Properties
The Multiplicative Integration has different gradient properties compared to the additive building
block. For clarity of presentation, we first look at vanilla-RNN and RNN with Multiplicative
Integration embedded, referred to as MI-RNN. That is, ht = ?(Wxt + Uht?1 + b) versus
?ht
ht = ?(Wxt  Uht?1 + b). In a vanilla-RNN, the gradient ?h
can be computed as follows:
t?n
t
Y
?ht
=
UT diag(?0k ),
(5)
?ht?n
k=t?n+1

?0k

0

where
= ? (Wxk + Uhk?1 + b). The equation above shows that the gradient flow through time
heavily depends on the hidden-to-hidden matrix U, but W and xk appear to play a limited role: they
?ht
only come in the derivative of ?0 mixed with Uhk?1 . On the other hand, the gradient ?h
of a
t?n
MI-RNN is4 :
t
Y
?ht
=
UT diag(Wxk )diag(?0k ),
(6)
?ht?n
k=t?n+1

2

If ? = 0, the Multiplicative Integration will degenerate to the vanilla additive building block.
See exact formulations in the Appendix.
4
Here we adopt the simplest formulation of Multiplicative Integration for illustration. In the more general
case (Eq. 4), diag(Wxk ) in Eq. 6 will become diag(?  Wxk + ?1 ).
3

2

where ?0k = ?0 (Wxk  Uhk?1 + b). By looking at the gradient, we see that the matrix W and
the current input xk is directly involved in the gradient computation by gating the matrix U, hence
more capable of altering the updates of the learning system. As we show in our experiments, with
Wxk directly gating the gradient, the vanishing/exploding problem is alleviated: Wxk dynamically
reconciles U, making the gradient propagation easier compared to the regular RNNs. For LSTMs
and GRUs with Multiplicative Integration, the gradient propagation properties are more complicated.
But in principle, the benefits of the gating effect also persists in these models.

3

Experiments

In all of our experiments, we use the general form of Multiplicative Integration (Eq. 4) for any hidden
state/gate computations, unless otherwise specified.
3.1 Exploratory Experiments
To further understand the functionality of Multiplicative Integration, we take a simple RNN for
illustration, and perform several exploratory experiments on the character level language modeling
task using Penn-Treebank dataset [11], following the data partition in [12]. The length of the
training sequence is 50. All models have a single hidden layer of size 2048, and we use Adam
optimization algorithm [13] with learning rate 1e?4 . Weights are initialized to samples drawn from
uniform[?0.02, 0.02]. Performance is evaluated by the bits-per-character (BPC) metric, which is
log2 of perplexity.
3.1.1 Gradient Properties
To analyze the gradient flow of the model, we divide the gradient in Eq. 6 into two parts: 1. the
gated matrix products: UT diag(Wxk ), and 2. the derivative of the nonlinearity ?0 , We separately
analyze the properties of each term compared to the additive building block. We first focus on the
gating effect brought by diag(Wxk ). In order to separate out the effect of nonlinearity, we chose ?
to be the identity map, hence both vanilla-RNN and MI-RNN reduce to linear models, referred to as
lin-RNN and lin-MI-RNN.
For each model we monitor the log-L2-norm of the gradient log ||?C/?ht ||2 (averaged over the
training set) after every training epoch, where ht is the hidden state at time step t, and C is the
negative log-likelihood of the single character prediction at the final time step (t = 50). Figure. 1
shows the evolution of the gradient norms for small t, i.e., 0, 5, 10, as they better reflect the gradient
propagation behaviour. Observe that the norms of lin-MI-RNN (orange) increase rapidly and soon
exceed the corresponding norms of lin-RNN by a large margin. The norms of lin-RNN stay close to
zero (? 10?4 ) and their changes over time are almost negligible. This observation implies that with
the help of diag(Wxk ) term, the gradient vanishing of lin-MI-RNN can be alleviated compared to
lin-RNN. The final test BPC (bits-per-character) of lin-MI-RNN is 1.48, which is comparable to a
vanilla-RNN with stabilizing regularizer [14], while lin-RNN performs rather poorly, achieving a test
BPC of over 2.
Next we look into the nonlinearity ?. We chose ? = tanh for both vanilla-RNN and MI-RNN.
Figure 1 (c) and (d) shows a comparison of histograms of hidden activations over all time steps on
the validation set after training. Interestingly, in (c) for vanilla-RNN, most activations are saturated
with values around ?1, whereas in (d) for MI-RNN, most activations are non-saturated with values
around 0. This has a direct consequence in gradient propagation: non-saturated activations imply
that diag(?0k ) ? 1 for ? = tanh, which can help gradients propagate, whereas saturated activations
imply that diag(?0k ) ? 0, resulting in gradients vanishing.
3.1.2 Scaling Problem
When adding two numbers at different order of magnitude, the smaller one might be negligible for the
sum. However, when multiplying two numbers, the value of the product depends on both regardless
of the scales. This principle also applies when comparing Multiplicative Integration to the additive
building blocks. In this experiment, we test whether Multiplicative Integration is more robust to the
scales of weight values. Following the same models as in Section 3.1.1, we first calculated the norms
of Wxk and Uhk?1 for both vanilla-RNN and MI-RNN for different k after training. We found that
in both structures, Wxk is a lot smaller than Uhk?1 in magnitude. This might be due to the fact that
xk is a one-hot vector, making the number of updates for (columns of) W be smaller than U. As a
result, in vanilla-RNN, the pre-activation term Wxk + Uhk?1 is largely controlled by the value of
Uhk?1 , while Wxk becomes rather small. In MI-RNN, on the other hand, the pre-activation term
Wxk  Uhk?1 still depends on the values of both Wxk and Uhk?1 , due to multiplication.
3

validation BPC

?4

?7
5

0.5

lin-RNN, t=0
lin-RNN, t=5
lin-RNN, t=10

lin-MI-RNN, t=0
lin-MI-RNN, t=5
lin-MI-RNN, t=10

10
15
20
number of epochs

2.1
1.8

0

(d)

0.12

0.3
0.2
0.1

?0.5
0.0
0.5
activation values of h_t

2.4

1.5

25

0.4

0.0
?1.0

vanilla-RNN
MI-RNN-simple
MI-RNN-general

2.7

?3

?5

(b)

3.0

?2

?6

normalized fequency

(a)

normalized fequency

log||dC / dh_t||_2

?1

10
15
20
number of epochs

25

(d)

0.10
0.08
0.06
0.04
0.02
0.00
?1.0

1.0

5

?0.5
0.0
0.5
activation values of h_t

1.0

Figure 1: (a) Curves of log-L2-norm of gradients for lin-RNN (blue) and lin-MI-RNN (orange). Time gradually
changes from {1, 5, 10}. (b) Validation BPC curves for vanilla-RNN, MI-RNN-simple using Eq. 2, and MIRNN-general using Eq. 4. (c) Histogram of vanilla-RNN?s hidden activations over the validation set, most
activations are saturated. (d) Histogram of MI-RNN?s hidden activations over the validation set, most activations
are not saturated.

We next tried different initialization of W and U to test their sensitivities to the scaling. For each
model, we fix the initialization of U to uniform[?0.02, 0.02] and initialize W to uniform[?rW , rW ]
where rW varies in {0.02, 0.1, 0.3, 0.6}. Table 1, top left panel, shows results. As we increase
the scale of W, performance of the vanilla-RNN improves, suggesting that the model is able to
better utilize the input information. On the other hand, MI-RNN is much more robust to different
initializations, where the scaling has almost no effect on the final performance.
3.1.3 On different choices of the formulation
In our third experiment, we evaluated the performance of different computational building blocks,
which are Eq. 1 (vanilla-RNN), Eq. 2 (MI-RNN-simple) and Eq. 4 (MI-RNN-general)5 . From the
validation curves in Figure 1 (b), we see that both MI-RNN, simple and MI-RNN-general yield much
better performance compared to vanilla-RNN, and MI-RNN-general has a faster convergence speed
compared to MI-RNN-simple. We also compared our results to the previously published models
in Table 1, bottom left panel, where MI-RNN-general achieves a test BPC of 1.39, which is to our
knowledge the best result for RNNs on this task without complex gating/cell mechanisms.
3.2

Character Level Language Modeling

In addition to the Penn-Treebank dataset, we also perform character level language modeling on two
larger datasets: text86 and Hutter Challenge Wikipedia7 . Both of them contain 100M characters from
Wikipedia while text8 has an alphabet size of 27 and Hutter Challenge Wikipedia has an alphabet
size of 205. For both datasets, we follow the training protocols in [12] and [1] respectively. We use
Adam for optimization with the starting learning rate grid-searched in {0.002, 0.001, 0.0005}. If the
validation BPC (bits-per-character) does not decrease for 2 epochs, we half the learning rate.
We implemented Multiplicative Integration on both vanilla-RNN and LSTM, referred to as MIRNN and MI-LSTM. The results for the text8 dataset are shown in Table 1, bottom middle panel.
All five models, including some of the previously published models, have the same number of
5

We perform hyper-parameter search for the initialization of {?, ?1 , ?2 , b} in MI-RNN-general.
http://mattmahoney.net/dc/textdata
7
http://prize.hutter1.net/
6

4

rW =

0.02 0.1 0.3 0.6

std

RNN 1.69 1.65 1.57 1.54 0.06
MI-RNN 1.39 1.40 1.40 1.41 0.008

WSJ Corpus

CER WER

DRNN+CTCbeamsearch [15]
Encoder-Decoder [16]
LSTM+CTCbeamsearch [17]
Eesen [18]
LSTM+CTC+WFST (ours)
MI-LSTM+CTC+WFST (ours)

10.0 14.1
6.4 9.3
9.2 8.7
7.3
6.5 8.7
6.0 8.2

Penn-Treebank

BPC

text8

BPC

RNN [12]
HF-MRNN [12]
RNN+stabalization [14]
MI-RNN (ours)
linear MI-RNN (ours)

1.42
1.41
1.48
1.39
1.48

RNN+smoothReLu [19]
HF-MRNN [12]
MI-RNN (ours)
LSTM (ours)
MI-LSTM(ours)

1.55
1.54
1.52
1.51
1.44

HutterWikipedia

BPC

stacked-LSTM [20]
GF-LSTM [1]
grid-LSTM [2]
MI-LSTM (ours)

1.67
1.58
1.47
1.44

Table 1: Top: test BPCs and the standard deviation of models with different scales of weight initializations. Top
right: test CERs and WERs on WSJ corpus. Bottom left: test BPCs on character level Penn-Treebank dataset.
Bottom middle: test BPCs on character level text8 dataset. Bottom right: test BPCs on character level Hutter
Prize Wikipedia dataset.

parameters (?4M). For RNNs without complex gating/cell mechanisms (the first three results), our
MI-RNN (with {?, ?1 , ?2 , b} initialized as {2, 0.5, 0.5, 0}) performs the best, our MI-LSTM (with
{?, ?1 , ?2 , b} initialized as {1, 0.5, 0.5, 0}) outperforms all other models by a large margin8 .
On Hutter Challenge Wikipedia dataset, we compare our MI-LSTM (single layer with 2048 unit,
?17M, with {?, ?1 , ?2 , b} initialized as {1, 1, 1, 0}) to the previous stacked LSTM (7 layers,
?27M) [20], GF-LSTM (5 layers, ?20M) [1], and grid-LSTM (6 layers, ?17M) [2]. Table 1, bottom
right panel, shows results. Despite the simple structure compared to the sophisticated connection
designs in GF-LSTM and grid-LSTM, our MI-LSTM outperforms all other models and achieves the
new state-of-the-art on this task.
3.3

Speech Recognition

We next evaluate our models on Wall Street Journal (WSJ) corpus (available as LDC corpus
LDC93S6B and LDC94S13B), where we use the full 81 hour set ?si284? for training, set ?dev93? for
validation and set ?eval92? for test. We follow the same data preparation process and model setting
as in [18], and we use 59 characters as the targets for the acoustic modelling. Decoding is done with
the CTC [21] based weighted finite-state transducers (WFSTs) [22] as proposed by [18].
Our model (referred to as MI-LSTM+CTC+WFST) consists of 4 bidirectional MI-LSTM layers, each with 320 units for each direction. CTC is performed on top to resolve the alignment
issue in speech transcription. For comparison, we also train a baseline model (referred to as
LSTM+CTC+WFST) with the same size but using vanilla LSTM. Adam with learning rate 0.0001
is used for optimization and Gaussian weight noise with zero mean and 0.05 standard deviation
is injected for regularization. We evaluate our models on the character error rate (CER) without
language model and the word error rate (WER) with extended trigram language model.
Table 1, top right panel, shows that MI-LSTM+CTC+WFST achieves quite good results on both CER
and WER compared to recent works, and it has a clear improvement over the baseline model. Note
that we did not conduct a careful hyper-parameter search on this task, hence one could potentially
obtain better results with better decoding schemes and regularization techniques.
3.4

Learning Skip-Thought Vectors

Next, we evaluate our Multiplicative Integration on the Skip-Thought model of [23]. Skip-Thought is
an encoder-decoder model that attempts to learn generic, distributed sentence representations. The
model produces sentence representation that are robust and perform well in practice, as it achieves
excellent results across many different NLP tasks. The model was trained on the BookCorpus dataset
that consists of 11,038 books with 74,004,228 sentences. Not surprisingly, a single pass through
8

[7] reports better results but they use much larger models (?16M) which is not directly comparable.

5

Semantic-Relatedness

r

?

MSE

Paraphrase detection Acc F1

uni-skip [23]
bi-skip [23]
combine-skip [23]

0.8477 0.7780 0.2872
0.8405 0.7696 0.2995
0.8584 0.7916 0.2687

uni-skip [23]
bi-skip [23]
combine-skip [23]

73.0 81.9
71.2 81.2
73.0 82.0

uni-skip (ours)
MI-uni-skip (ours)

0.8436 0.7735 0.2946
0.8588 0.7952 0.2679

uni-skip (ours)
MI-uni-skip (ours)

74.0 81.9
74.0 82.1

Classification

MR CR SUBJ MPQA

uni-skip [23]
75.5 79.3 92.1
bi-skip [23]
73.9 77.9 92.5
combine-skip [23] 76.5 80.1 93.6

86.9
83.3
87.1

uni-skip (ours)
75.9 80.1 93.0
MI-uni-skip (ours) 77.9 82.3 93.3

87.0
88.1

Attentive Reader

Val. Err.

LSTM [7]
BN-LSTM [7]
BN-everywhere [7]
LSTM (ours)
MI-LSTM (ours)
MI-LSTM+BN (ours)
MI-LSTM+BN-everywhere (ours)

0.5033
0.4951
0.5000
0.5053
0.4721
0.4685
0.4644

Table 2: Top left: skip-thought+MI on Semantic-Relatedness task. Top Right: skip-thought+MI on Paraphrase
Detection task. Bottom left: skip-thought+MI on four different classification tasks. Bottom right: Multiplicative
Integration (with batch normalization) on Teaching Machines to Read and Comprehend task.

the training data can take up to a week on a high-end GPU (as reported in [23]). Such training
speed largely limits one to perform careful hyper-parameter search. However, with Multiplicative
Integration, not only the training time is shortened by a factor of two, but the final performance is
also significantly improved.
We exactly follow the authors? Theano implementation of the skip-thought model9 : Encoder and
decoder are single-layer GRUs with hidden-layer size of 2400; all recurrent matrices adopt orthogonal
initialization while non-recurrent weights are initialized from uniform distribution. Adam is used
for optimization. We implemented Multiplicative Integration only for the encoder GRU (embedding
MI into decoder did not provide any substantial gains). We refer our model as MI-uni-skip, with
{?, ?1 , ?2 , b} initialized as {1, 1, 1, 0}. We also train a baseline model with the same size, referred
to as uni-skip(ours), which essentially reproduces the original model of [23].
During the course of training, we evaluated the skip-thought vectors on the semantic relatedness
task, using SICK dataset, every 2500 updates for both MI-uni-skip and the baseline model (each
iteration processes a mini-batch of size 64). The results are shown in Figure 2a. Note that MI-uni-skip
significantly outperforms the baseline, not only in terms of speed of convergence, but also in terms
of final performance. At around 125k updates, MI-uni-skip already exceeds the best performance
achieved by the baseline, which takes about twice the number of updates.
We also evaluated both models after one week of training, with the best results being reported on six
out of eight tasks reported in [23]: semantic relatedness task on SICK dataset, paraphrase detection
task on Microsoft Research Paraphrase Corpus, and four classification benchmarks: movie review
sentiment (MR), customer product reviews (CR), subjectivity/objectivity classification (SUBJ), and
opinion polarity (MPQA). We also compared our results with the results reported on three models in
the original skip-thought paper: uni-skip, bi-skip, combine-skip. Uni-skip is the same model as our
baseline, bi-skip is a bidirectional model of the same size, and combine-skip takes the concatenation
of the vectors from uni-skip and bi-skip to form a 4800 dimension vector for task evaluation. Table
2 shows that MI-uni-skip dominates across all the tasks. Not only it achieves higher performance
than the baseline model, but in many cases, it also outperforms the combine-skip model, which has
twice the number of dimensions. Clearly, Multiplicative Integration provides a faster and better way
to train a large-scale Skip-Thought model.
3.5

Teaching Machines to Read and Comprehend

In our last experiment, we show that the use of Multiplicative Integration can be combined with
other techniques for training RNNs, and the advantages of using MI still persist. Recently, [7]
introduced Recurrent Batch-Normalization. They evaluated their proposed technique on a uni9

https://github.com/ryankiros/skip-thoughts

6

MSE

0.34

(a)

0.32
0.30
0.28
0.26
0

(b)

0.70
uni-skip (ours)
MI-uni-skip (ours)
validation error

0.36

50
100
150
200
number of iterations (2.5k)

0.60
0.55
0.50
0.45
0

250

LSTM [7]
BN-LSTM [7]
MI-LSTM (ours)
MI-LSTM+BN (ours)

0.65

200
400
600
number of iterations (1k)

800

Figure 2: (a) MSE curves of uni-skip (ours) and MI-uni-skip (ours) on semantic relatedness task on SICK
dataset. MI-uni-skip significantly outperforms baseline uni-skip. (b) Validation error curves on attentive reader
models. There is a clear margin between models with and without MI.

directional Attentive Reader Model [24] for the question answering task using the CNN corpus10 . To
test our approach, we evaluated the following four models: 1. A vanilla LSTM attentive reader model
with a single hidden layer size 240 (same as [7]) as our baseline, referred to as LSTM (ours), 2. A
multiplicative integration LSTM with a single hidden size 240, referred to as MI-LSTM, 3. MILSTM with Batch-Norm, referred to as MI-LSTM+BN, 4. MI-LSTM with Batch-Norm everywhere
(as detailed in [7]), referred to as MI-LSTM+BN-everywhere. We compared our models to results
reported in [7] (referred to as LSTM, BN-LSTM and BN-LSTM everywhere) 11 .
For all MI models, {?, ?1 , ?2 , b} were initialized to {1, 1, 1, 0}. We follow the experimental
protocol of [7]12 and use exactly the same settings as theirs, except we remove the gradient clipping
for MI-LSTMs. Figure. 2b shows validation curves of the baseline (LSTM), MI-LSTM, BN-LSTM,
and MI-LSTM+BN, and the final validation errors of all models are reported in Table 2, bottom right
panel. Clearly, using Multiplicative Integration results in improved model performance regardless
of whether Batch-Norm is used. However, the combination of MI and Batch-Norm provides the
best performance and the fastest speed of convergence. This shows the general applicability of
Multiplication Integration when combining it with other optimization techniques.

4

Relationship to Previous Models

4.1

Relationship to Hidden Markov Models

One can show that under certain constraints, MI-RNN is effectively implementing the forward
algorithm of the Hidden Markov Model(HMM). A direct mapping can be constructed as follows (see
[25] for a similar derivation). Let U ? Rm?m be the state transition probability matrix with Uij =
Pr[ht+1 = i|ht = j], W ? Rm?n be the observation probability matrix with Wij = Pr[xt =
i|ht = j]. When xt is a one-hot vector (e.g., in many of the language modelling tasks), multiplying
it by W is effectively choosing a column of the observation matrix. Namely, if the j th entry of xt
is one, then Wxt = Pr[xt |ht = j]. Let h0 be the initial state distribution with h0 = Pr[h0 ] and
{ht }t?1 be the alpha values in the forward algorithm of HMM, i.e., ht = Pr[x1 , ..., xt , ht ]. Then
Uht = Pr[x1 , ..., xt , ht+1 ]. Thus ht+1 = Wxt+1  Uht = Pr[xt+1 |ht+1 ] ? Pr[x1 , ..., xt , ht+1 ] =
Pr[x1 , ..., xt+1 , ht+1 ]. To exactly implement the forward algorithm using Multiplicative Integration,
the matrices W and U have to be probability matrices, and xt needs to be a one-hot vector. The
function ? needs to be linear, and we drop all the bias terms. Therefore, RNN with Multiplicative
Integration can be seen as a nonlinear extension of HMMs. The extra freedom in parameter values
and nonlinearity makes the model more flexible compared to HMMs.
4.2

Relations to Second Order RNNs and Multiplicative RNNs

MI-RNN is related to the second order RNN [6] and the multiplicative RNN (MRNN) [9]. We first
describe the similarities with these two models:
The second order RNN involves a second order term st in a vanilla-RNN, where the ith element
st,i is computed by the bilinear form: st,i = xTt T (i) ht?1 , where T (i) ? Rn?m (1 ? i ? m) is
10

Note that [7] used a truncated version of the original dataset in order to save computation.
Learning curves and the final result number are obtained by emails correspondence with authors of [7].
12
https://github.com/cooijmanstim/recurrent-batch-normalization.git.
11

7

the ith slice of a tensor T ? Rm?n?m . Multiplicative Integration also involve a second order term
st = ?  Wxt  Uht?1 , but in our case st,i = ?i (wi ? xt )(ui ? ht?1 ) = xTt (?wi ? ui )ht?1 ,
where wi and ui are ith row in W and U, and ?i is the ith element of ?. Note that the outer product
?i wi ? ui is a rank-1 matrix. The Multiplicative RNN is also a second order RNN, but which
P (i) (i)
xt T
= Pdiag(Vxt )Q. For MI-RNN, we can
approximates T by a tensor decomposition
also think of the second order term as a tensor decomposition: ?  Wxt  Uht?1 = U(xt )ht?1 =
[diag(?)diag(Wxt )U]ht?1 .
There are however several differences that make MI a favourable model: (1) Simpler Parametrization:
MI uses a rank-1 approximation compared to the second order RNNs, and a diagonal approximation
compared to Multiplicative RNN. Moreover, MI-RNN shares parameters across the first and second
order terms, whereas the other two models do not. As a result, the number of parameters are largely
reduced, which makes our model more practical for large scale problems, while avoiding overfitting.
(2) Easier Optimization: In tensor decomposition methods, the products of three different (low-rank)
matrices generally makes it hard to optimize [9]. However, the optimization problem becomes
easier in MI, as discussed in section 2 and 3. (3) General structural design vs. vanilla-RNN design:
Multiplicative Integration can be easily embedded in many other RNN structures, e.g. LSTMs and
GRUs, whereas the second order RNN and MRNN present a very specific design for modifying
vanilla-RNNs.
Moreover, we also compared MI-RNN?s performance to the previous HF-MRNN?s results (Multiplicative RNN trained by Hessian-free method) in Table 1, bottom left and bottom middle panels, on
Penn-Treebank and text8 datasets. One can see that MI-RNN outperforms HF-MRNN on both tasks.
4.3

General Multiplicative Integration

Multiplicative Integration can be viewed as a general way of combining information flows from
two different sources. In particular, [26] proposed the ladder network that achieves promising
results on semi-supervised learning. In their model, they combine the lateral connections and the
backward connections via the ?combinator? function by a Hadamard product. The performance would
severely degrade without this product as empirically shown by [27]. [28] explored neural embedding
approaches in knowledge bases by formulating relations as bilinear and/or linear mapping functions,
and compared a variety of embedding models on the link prediction task. Surprisingly, the best
results among all bilinear functions is the simple weighted Hadamard product. They further carefully
compare the multiplicative and additive interactions and show that the multiplicative interaction
dominates the additive one.

5

Conclusion

In this paper we proposed to use Multiplicative Integration (MI), a simple Hadamard product to
combine information flow in recurrent neural networks. MI can be easily integrated into many popular
RNN models, including LSTMs and GRUs, while introducing almost no extra parameters. Indeed,
the implementation of MI requires almost no extra work beyond implementing RNN models. We also
show that MI achieves state-of-the-art performance on four different tasks or 11 datasets of varying
sizes and scales. We believe that the Multiplicative Integration can become a default building block
for training various types of RNN models.

Acknowledgments
The authors acknowledge the following agencies for funding and support: NSERC, Canada Research
Chairs, CIFAR, Calcul Quebec, Compute Canada, Disney research and ONR Grant N000141310721.
The authors thank the developers of Theano [29] and Keras [30], and also thank Jimmy Ba for many
thought-provoking discussions.

References
[1] Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. Gated feedback recurrent neural
networks. arXiv preprint arXiv:1502.02367, 2015.
[2] Nal Kalchbrenner, Ivo Danihelka, and Alex Graves. Grid long short-term memory. arXiv preprint
arXiv:1507.01526, 2015.

8

[3] Saizheng Zhang, Yuhuai Wu, Tong Che, Zhouhan Lin, Roland Memisevic, Ruslan Salakhutdinov,
and Yoshua Bengio. Architectural complexity measures of recurrent neural networks. arXiv preprint
arXiv:1602.08210, 2016.
[4] Sepp Hochreiter and J?rgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735?1780,
1997.
[5] Kyunghyun Cho, Bart Van Merri?nboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical
machine translation. arXiv preprint arXiv:1406.1078, 2014.
[6] Mark W Goudreau, C Lee Giles, Srimat T Chakradhar, and D Chen. First-order versus second-order
single-layer recurrent neural networks. Neural Networks, IEEE Transactions on, 5(3):511?513, 1994.
[7] Tim Cooijmans, Nicolas Ballas, C?sar Laurent, and Aaron Courville. Recurrent batch normalization.
http://arxiv.org/pdf/1603.09025v4.pdf, 2016.
[8] LE Baum and JA Eagon. An inequality with application to statistical estimation for probabilistic functions
of markov processes and to a model for ecology. Bulletin of the American Mathematical Society, 73:360?
363, 1967.
[9] Ilya Sutskever, James Martens, and Geoffrey E Hinton. Generating text with recurrent neural networks. In
Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 1017?1024,
2011.
[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
arXiv preprint arXiv:1512.03385, 2015.
[11] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus
of english: The penn treebank. Computational linguistics, 19(2):313?330, 1993.
[12] Tom?? Mikolov, Ilya Sutskever, Anoop Deoras, Hai-Son Le, and Stefan Kombrink. Subword language
modeling with neural networks. preprint, (http://www.fit.vutbr.cz/imikolov/rnnlm/char.pdf), 2012.
[13] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
[14] David Krueger and Roland Memisevic. Regularizing rnns by stabilizing activations. arXiv preprint
arXiv:1511.08400, 2015.
[15] Awni Y Hannun, Andrew L Maas, Daniel Jurafsky, and Andrew Y Ng. First-pass large vocabulary
continuous speech recognition using bi-directional recurrent dnns. arXiv preprint arXiv:1408.2873, 2014.
[16] Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Philemon Brakel, and Yoshua Bengio. End-to-end
attention-based large vocabulary speech recognition. arXiv preprint arXiv:1508.04395, 2015.
[17] Alex Graves and Navdeep Jaitly. Towards end-to-end speech recognition with recurrent neural networks.
In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1764?1772,
2014.
[18] Yajie Miao, Mohammad Gowayyed, and Florian Metze. Eesen: End-to-end speech recognition using deep
rnn models and wfst-based decoding. arXiv preprint arXiv:1507.08240, 2015.
[19] Marius Pachitariu and Maneesh Sahani. Regularization and nonlinearities for neural language models:
when are they needed? arXiv preprint arXiv:1301.5650, 2013.
[20] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.
[21] Alex Graves, Santiago Fern?ndez, Faustino Gomez, and J?rgen Schmidhuber. Connectionist temporal
classification: labelling unsegmented sequence data with recurrent neural networks. In Proceedings of the
23rd international conference on Machine learning, pages 369?376. ACM, 2006.
[22] Mehryar Mohri, Fernando Pereira, and Michael Riley. Weighted finite-state transducers in speech recognition. Computer Speech & Language, 16(1):69?88, 2002.
[23] Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. Skip-thought vectors. In Advances in Neural Information Processing Systems, pages
3276?3284, 2015.
[24] Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,
and Phil Blunsom. Teaching machines to read and comprehend. In Advances in Neural Information
Processing Systems, pages 1684?1692, 2015.
[25] T. Wessels and C. W. Omlin. Refining hidden markov models with recurrent neural networks. In Neural
Networks, 2000. IJCNN 2000, Proceedings of the IEEE-INNS-ENNS International Joint Conference on,
volume 2, pages 271?276 vol.2, 2000.
[26] Antti Rasmus, Harri Valpola, Mikko Honkala, Mathias Berglund, and Tapani Raiko. Semi-supervised
learning with ladder network. arXiv preprint arXiv:1507.02672, 2015.
[27] Mohammad Pezeshki, Linxi Fan, Philemon Brakel, Aaron Courville, and Yoshua Bengio. Deconstructing
the ladder network architecture. arXiv preprint arXiv:1511.06430, 2015.
[28] Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding entities and relations
for learning and inference in knowledge bases. arXiv preprint arXiv:1412.6575, 2014.
[29] Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, and et al. Theano: A python framework for fast
computation of mathematical expressions, 2016.
[30] Fran?ois Chollet. Keras. GitHub repository: https://github.com/fchollet/keras, 2015.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 5271-pre-training-of-recurrent-neural-networks-via-linear-autoencoders.pdf

Pre-training of Recurrent Neural Networks via
Linear Autoencoders
Luca Pasa, Alessandro Sperduti
Department of Mathematics
University of Padova, Italy
{pasa,sperduti}@math.unipd.it

Abstract
We propose a pre-training technique for recurrent neural networks based on linear
autoencoder networks for sequences, i.e. linear dynamical systems modelling the
target sequences. We start by giving a closed form solution for the definition of
the optimal weights of a linear autoencoder given a training set of sequences. This
solution, however, is computationally very demanding, so we suggest a procedure
to get an approximate solution for a given number of hidden units. The weights
obtained for the linear autoencoder are then used as initial weights for the inputto-hidden connections of a recurrent neural network, which is then trained on the
desired task. Using four well known datasets of sequences of polyphonic music,
we show that the proposed pre-training approach is highly effective, since it allows
to largely improve the state of the art results on all the considered datasets.

1

Introduction

Recurrent Neural Networks (RNN) constitute a powerful computational tool for sequences modelling and prediction [1]. However, training a RNN is not an easy task, mainly because of the well
known vanishing gradient problem which makes difficult to learn long-term dependencies [2]. Although alternative architectures, e.g. LSTM networks [3], and more efficient training procedures,
such as Hessian Free Optimization [4], have been proposed to circumvent this problem, reliable and
effective training of RNNs is still an open problem.
The vanishing gradient problem is also an obstacle to Deep Learning, e.g., [5, 6, 7]. In that context,
there is a growing evidence that effective learning should be based on relevant and robust internal
representations developed in autonomy by the learning system. This is usually achieved in vectorial
spaces by exploiting nonlinear autoencoder networks to learn rich internal representations of input
data which are then used as input to shallow neural classifiers or predictors (see, for example, [8]).
The importance to start gradient-based learning from a good initial point in the parameter space has
also been pointed out in [9]. Relationship between autoencoder networks and Principal Component
Analysis (PCA) [10] is well known since late ?80s, especially in the case of linear hidden units [11,
12]. More recently, linear autoencoder networks for structured data have been studied in [13, 14, 15],
where an exact closed-form solution for the weights is given in the case of a number of hidden units
equal to the rank of the full data matrix.
In this paper, we borrow the conceptual framework presented in [13, 16] to devise an effective pretraining approach, based on linear autoencoder networks for sequences, to get a good starting point
into the weight space of a RNN, which can then be successfully trained even in presence of longterm dependencies. Specifically, we revise the theoretical approach presented in [13] by: i) giving
a simpler and direct solution to the problem of devising an exact closed-form solution (full rank
case) for the weights of a linear autoencoder network for sequences, highlighting the relationship
between the proposed solution and PCA of the input data; ii) introducing a new formulation of
1

the autoencoder learning problem able to return an optimal solution also in the case of a number
of hidden units which is less than the rank of the full data matrix; iii) proposing a procedure for
approximate learning of the autoencoder network weights under the scenario of very large sequence
datasets. More importantly, we show how to use the linear autoencoder network solution to derive a
good initial point into a RNN weight space, and how the proposed approach is able to return quite
impressive results when applied to prediction tasks involving long sequences of polyphonic music.

2

Linear Autoencoder Networks for Sequences

In [11, 12] it is shown that principal directions of a set of vectors xi ? Rk are related to solutions
obtained by training linear autoencoder networks
oi = Woutput Whidden xi , i = 1, . . . , n,
(1)
where Whidden ? Rp?k , Woutput ? Rk?p , p  k, and the network is trained so to get oi = xi , ?i.
When considering a temporal sequence x1 , x2 , . . . , xt , . . . of input vectors, where t is a discrete time
index, a linear autoencoder can be defined by considering the coupled linear dynamical systems


xt
= Cyt
(3)
yt = Axt + Byt?1 (2)
yt?1
It should be noticed that eqs. (2) and (3) extend the linear transformation defined in eq. (1) by
introducing a memory term involving matrix B ? Rp?p . In fact, yt?1 is inserted in the right part
of equation (2) to keep track of the input history through time: this is done exploiting a state space
representation. Eq. (3) represents the decoding part of the autoencoder: when a state yt is multiplied
by C, the observed input xt at time t and state at time t ? 1, i.e. yt?1 , are generated. Decoding
can then continue from yt?1 . This formulation has been proposed, for example, in [17] where an
iterative procedure to learn weight matrices A and B, based on Oja?s rule, is presented. No proof
of convergence for the proposed procedure is however given. More recently, an exact closed-form
solution for the weights has been given in the case of a number of hidden units equal to the rank of
the full data matrix (full rank case) [13, 16]. In this section, we revise this result. In addition, we
give an exact solution also for the case in which the number of hidden units is strictly less than the
rank of the full data matrix.
The basic idea of [13, 16] is to look for directions of high variance into the state space of the
dynamical linear system (2). Let start by considering a single sequence x1 , x2 , . . . , xt , . . . , xn and
the state vectors of the corresponding induced state sequence collected as rows of a matrix Y =
T
[y1 , y2 , y3 , ? ? ? , yn ] . By using the initial condition y0 = 0 (the null vector), and the dynamical
linear system (2), we can rewrite the Y matrix as
? T
?? T
?
A
x1 0
0
0
??? 0
? xT xT
?
? AT BT
0
0
??? 0 ?
2
1
? T
? ? T 2T
?
T
T
?
?
? x3 x2
?
x
0
?
?
?
0
A
B
1
Y=?
??
?
? ..
?
..
..
..
..
.. ? ?
..
? .
?
?
?
.
.
.
.
.
.
T
T
T
T
T
T
T
n?1
xn xn?1 xn?2 ? ? ? x2 x1
A B
|
{z
}|
{z
}
?

?

n?s

where, given s = kn, ? ? R
is a data matrix collecting all the (inverted) input subsequences
(including the whole sequence) as rows, and ? is the parameter matrix of the dynamical system.
Now, we are interested in using a state space of dimension p  n, i.e. yt ? Rp , such that as
much information as contained in ? is preserved. We start by factorizing ? using SVD, obtaining
? = V?UT where V ? Rn?n is an unitary matrix, ? ? Rn?s is a rectangular diagonal matrix
with nonnegative real numbers on the diagonal with ?1,1 ? ?2,2 ? ? ? ? ? ?n,n (the singular values),
and UT ? Rs?n is a unitary matrix.
It is important to notice that columns of UT which correspond to nonzero singular values, apart
some mathematical technicalities, basically correspond to the principal directions of data, i.e. PCA.
If the rank of ? is p, then only the first p elements of the diagonal of ? are not null, and the
T
above decomposition can be reduced to ? = V(p) ?(p) U(p) where V(p) ? Rn?p , ?(p) ? Rp?p ,
2

T

T

and U(p) ? Rp?n . Now we can observe that U(p) U(p) = I (where I is the identity matrix of
dimension p), since by definition the columns of U(p) are orthogonal, and by imposing ? = U(p) ,
we can derive ?optimal? matrices A ? Rp?k and B ? Rp?p for our dynamical system, which will
T
have corresponding state space matrix Y(p) = ?? = ?U(p) = V(p) ?(p) U(p) U(p) = V(p) ?(p) .
(p)
Thus, if we represent U(p) as composed of n submatrices Ui , each of size k ? p, the problem
reduces to find matrices A and B such that
? T
? ? (p) ?
A
U1
(p) ?
? AT BT
? ?
U2 ?
? T 2T
? ?
(p) ?
? A B
? ?
(p)
U3 ?
(4)
?=?
?=?
?=U .
?
? ?
..
.
?
?
? ?
?
.. ?
.
T
(p)
AT Bn?1
Un
The reason to impose ? = U(p) is to get a state space where the coordinates are uncorrelated so
to diagonalise the empirical sample covariance matrix of the states. Please, note that in this way
each state (i.e., row of the Y matrix) corresponds to a row of the data matrix ?, i.e. the unrolled
(sub)sequence read up to a given time t. If the rows of ? were vectors, this would correspond to
compute PCA, keeping only the fist p principal directions.
In the following, we demonstrate that there exists a solution to the above equation. We start
by observing that ? owns a special structure, i.e. given? = [?1 ?2 ? ? ? ?n ], where
 ?i ?
0
0
1?1
1?(n?1)
Rn?k , then for i = 1, . . . , n ? 1, ?i+1 = Rn ?i =
?i , and
I(n?1)?(n?1) 0(n?1)?1
Rn ?n = 0, i.e. the null matrix of size n ? k. Moreover, by singular value decomposition, we
(p) T

T

have ?i = V(p) ?(p) Ui , for i = 1, . . . , n. Using the fact that V(p) V(p) = I, and
(p)
(p)
combining the above equations, we get Ui+t = Ui Qt , for i = 1, . . . , n ? 1, and t =
?1

T

(p)

(p) (p)
1, . . . , n ? i, where Q = ?(p) V(p) RT
?
. Moreover, we have that Un Q = 0 since
nV
?1
(p) (p) (p) T T (p) (p) ?1
(p)
= (Rn ?n )T V(p) ?(p) . Thus, eq. (4) is satisfied by
Rn V ?
Un Q = Un ? V
| {z }
=0

(p) T
U1

T

A =
and B = Q . It is interesting to note that the original data ? can be recovered by
T
T
computing Y(p) U(p) = V(p) ?(p) U(p) = ?, which can be achieved by running the system




xt
AT
yt
=
yt?1
BT


AT
starting from yn , i.e.
is the matrix C defined in eq. (3).
BT
Finally, it is important to remark that the above construction works not only for a single sequence,
but also for a set of sequences of different length. For example, let consider the two sequences
(xa1 , xa2 , xa3 ) and (xb 1 , xb 2 ). Then, we have
? aT
?
"
#
x1
0
0
bT
x
0
1
? and ?b =
?a = ? xa2 T xa1 T 0
T
bT
x
xb1
aT
aT
aT
2
x3
x2
x1




?a
R4
, and R =
.
which can be collected together to obtain ? =
?b 02?1
R2 02?1
As a final remark, it should be stressed that the above construction only works if p is equal to the
rank of ?. In the next section, we treat the case in which p < rank(?).
2.1

Optimal solution for low dimensional autoencoders
T

? i = V(p) L(p) U(p) 6= ?i , and
When p < rank(?) the solution given above breaks down because ?
i
? i+1 6= Rn ?
? i . So the question is whether the proposed solutions for A and B still
consequently ?
hold the best reconstruction error when p < rank(?).
3

In this paper, we answer in negative terms to this question by resorting to a new formulation of our
(p)
problem where we introduce slack-like matrices Ei ? Rk?p , i = 1, . . . , n + 1 collecting the
reconstruction errors, which need to be minimised:
n+1
X

min
(p)
Q?Rp?p ,Ei

?

subject to :

?
?
?
?
?
?
?

(p)

i=1
(p)

U1 + E 1
(p)
(p)
U2 + E 2
(p)
(p)
U3 + E 3
..
.
(p)

(p)

kEi k2F

(p)

Un + E n

(p)

(p)

?

?
?
?
?
?
?
?Q = ?
?
?
? (p)
?
? Un + En(p)
?
(p)
En+1

?
?
?
?
?
?
?

?

?

U2 + E 2
(p)
(p)
U3 + E 3
..
.

(5)

Notice that the problem above is convex both in the objective function and in the constraints; thus
(p)
it only has global optimal solutions E?i and Q? , from which we can derive AT = U1 + E?1 and
T
?
T
(p)
(p)
B = Q . Specifically, when p = rank(?), Rs,k U is in the span of U and the optimal
T

(p)
solution is given by E?i = 0k?p ?i, and Q? = U(p) RT
, i.e. the solution we have already
s,k U
described. If p < rank(?), the optimal solution cannot have ?i, E?i = 0k?p . However, it is not
difficult to devise an iterative procedure to reach the minimum. Since in the experimental section we
do not exploit the solution to this problem for reasons that we will explain later, here we just sketch
(p)
such procedure. It helps to observe that, given a fixed Q, the optimal solution for Ei is given by
(p)

(p)

(p)

(p)

(p)

(p)

(p)

(p)

(p)

+
2
3
? ,E
? ,...,E
?
[E
1
2
n+1 ] = [U1 Q ? U2 , U1 Q ? U3 , U1 Q ? U4 , . . .] MQ
?
?
?Q ?Q2 ?Q3 ? ? ?
0
0
??? ?
? I
? 0
+
I
0
??? ?
?
?.
where MQ is the pseudo inverse of MQ = ?
0
I
??? ?
? 0
?
..
..
..
..
.
.
.
.

h
i
T
T
T
T T
? (p) = E
? (p) , E
? (p) , E
? (p) , ? ? ? , E
? n(p)
In general, E
can be decomposed into a component in the
1
2
3
?

?

span of U(p) and a component E(p) orthogonal to it. Notice that E(p) cannot be reduced, while
? (p) = U(p) + E(p) ? and taking
(part of) the other component can be absorbed into Q by defining U
h
i
T
T
T T
? = (U
? (p) )+ U
? (p) , U
? (p) , ? ? ? , U
? (p)T , E(p)
Q
.
n
2
3
n+1
? the new optimal values for E(p) are obtained and the process iterated till convergence.
Given Q,
i

3

Pre-training of Recurrent Neural Networks

Here we define our pre-training procedure for recurrent neural networks with one hidden layer of p
units, and O output units:
ot = ?(Woutput h(xt )) ? RO , h(xt ) = ?(Winput xt + Whidden h(xt?1 )) ? Rp

(6)
T

where Woutput ? RO?p , Whidden ? Rp?k , for a vector z ? Rm , ?(z) = [?(z1 ), . . . , ?(zm )] ,
?zi
.
and here we consider the symmetric sigmoid function ?(zi ) = 1?e
1+e?zi
The idea is to exploit the hidden state representation obtained by eqs. (2) as initial hidden state representation for the RNN described by eqs. (6). This is implemented by initialising the weight matrices
Winput and Whidden of (6) by using the matrices that jointly solve eqs. (2) and eqs. (3), i.e. A and
B (since C is function of A and B). Specifically, we initialize Winput with A, and Whidden with
B. Moreover, the use of symmetrical sigmoidal functions, which do give a very good approximation
of the identity function around the origin, allows a good transferring of the linear dynamics inside
4

RNN. For what concerns Woutput , we initialise it by using the best possible solution, i.e. the pseudoinverse of H times the target matrix T, which does minimise the output squared error. Learning
is then used to introduce nonlinear components that allow to improve the performance of the model.
More formally, let consider a prediction task where for each sequence sq ? (xq1 , xq2 , . . . , xqlq )
of length lq in the training set, a sequence tq of target vectors is defined, i.e. a training sequence is given by hsq , tq i ? h(xq1 , tq1 ), (xq2 , tq2 ), . . . , (xqlq , tqlq )i, where tqi ? RO . Given a trainPN
ing set with N sequences, let define the target matrix T ? RL?O , where L =
q=1 lq , as
 1 1

1
2
?
N T
T = t1 , t2 , . . . , tl1 , t1 , . . . , tlN . The input matrix ? will have size L ? k. Let p be the desired number of hidden units for the recurrent neural network (RNN). Then the pre-training procedure can be defined as follows: i) compute the linear autoencoder for ? using p? principal direc?
?
?
tions, obtaining the optimal matrices A? ? Rp ?k and B? ? Rp ?p ; i) set Winput = A? and
?
Whidden = B ; iii) run the RNN over the training sequences, collecting the hidden activities vec?
tors (computed using symmetrical sigmoidal functions) over time as rows of matrix H ? RL?p ;
+
+
iv) set Woutput = H T, where H is the (left) pseudoinverse of H.

3.1

Computing an approximate solution for large datasets

In real world scenarios the application of our approach may turn difficult because of the size of
the data matrix. In fact, stable computation of principal directions is usually obtained by SVD decomposition of the data matrix ?, that in typical application domains involves a number of rows
and columns which is easily of the order of hundreds of thousands. Unfortunately, the computational complexity of SVD decomposition is basically cubic in the smallest of the matrix dimensions.
Memory consumption is also an important issue. Algorithms for approximate computation of SVD
have been suggested (e.g., [18]), however, since for our purposes we just need matrices V and ?
with a predefined number of columns (i.e. p), here we present an ad-hoc algorithm for approximate
computation of these matrices. Our solution is based on the following four main ideas: i) divide ?
in slices of k (i.e., size of input at time t) columns, so to exploit SVD decomposition at each slice
separately; ii) compute approximate V and ? matrices, with p columns, incrementally via truncated
SVD of temporary matrices obtained by concatenating the current approximation of V? with a new
slice; iii) compute the SVD decomposition of a temporary matrix via either its kernel or covariance
matrix, depending on the smallest between the number of rows and the number of columns of the
temporary matrix; iv) exploit QR decomposition to compute SVD decomposition.
Algorithm 1 shows in pseudo-code the main steps of our procedure. It maintains a temporary matrix
T which is used to collect incrementally an approximation of the principal subspace of dimension p
of ?. Initially (line 4) T is set equal to the last slices of ?, in a number sufficient to get a number
of columns larger than p (line 2). Matrices V and ? from the p-truncated SVD decomposition of
T are computed (line 5) via the K E C O procedure, described in Algorithm 2, and used to define a
new T matrix by concatenation with the last unused slice of ?. When all slices are processed, the
current V and ? matrices are returned. The K E C O procedure, described in Algorithm 2 , reduces
the computational burden by computing the p-truncated SVD decomposition of the input matrix
M via its kernel matrix (lines 3-4) if the number of rows of M is no larger than the number of
columns, otherwise the covariance matrix is used (lines 6-8). In both cases, the p-truncated SVD
decomposition is implemented via QR decomposition by the INDIRECT SVD procedure described in
Algorithm 3. This allows to reduce computation time when large matrices must be processed [19].
1
Finally, matrices V and S 2 (both kernel and covariance matrices have squared singular values of
M) are returned.
We use the strategy to process slices of ? in reverse order since, moving versus columns with larger
indices, the rank as well as the norm of slices become smaller and smaller, thus giving less and less
contribution to the principal subspace of dimension p. This should reduce the approximation error
cumulated by dropping the components from p + 1 to p + k during computation [20]. As a final
remark, we stress that since we compute an approximate solution for the principal directions of ?,
it makes no much sense to solve the problem given in eq. (5): learning will quickly compensate
for the approximations and/or sub-optimality of A and B obtained by matrices V and ? returned
by Algorithm 1. Thus, these are the matrices we have used for the experiments described in next
section.
5

Algorithm 1 Approximated V and ? with p components
1: function SVF OR B IG DATA(?, k, p)
2:
nStart = dp/ke
. Number of starting slices
3:
nSlice = (?.columns/k) ? nStart
. Number of remaining slices
4:
T = ?[:, k ? nSlice : ?.columns]
5:
V, ? =K E C O(T, p)
. Computation of V and ? for starting slices
6:
for i in REVERSED(range(nSlice)) do
. Computation of V and ? for remaining slices
7:
T = [?[:, i ? k:(i + 1) ? k], V?]
8:
V, ? =K E C O(T, p)
9:
end for
10:
return V, ?
11: end function
Algorithm 2 Kernel vs covariance computation Algorithm 3 Truncated SVD by QR
1: function K E C O(M, p)
1: function INDIRECT SVD(M, p)
2:
if M.rows <= ?.columns then
2:
Q, R =QR(M)
3:
K = MMT
3:
Vr , S, UT =SVD(R)
T
4:
V, Ssqr , U =INDIRECT SVD(K, p) 4:
V = QVr
5:
else
5:
S = S[1 : p, 1 : p]
6:
C = MT M
6:
V = V[1 : p, :]
7:
V, Ssqr , UT =INDIRECT SVD(C, p) 7:
UT = UT [:, 1 : p]
1
?
8:
return V, S, UT
8:
V = MUT Ssqr2
9: end function
9:
end if
1

2
10:
return V, Ssqr
11: end function

4

Experiments

In order to evaluate our pre-training approach, we decided to use the four polyphonic music sequences datasets used in [21] for assessing the prediction abilities of the RNN-RBM model. The
prediction task consists in predicting the notes played at time t given the sequence of notes played
till time t ? 1. The RNN-RBM model achieves state-of-the-art in such demanding prediction task.
As performance measure we adopted the accuracy measure used in [21] and described in [22]. Each
dataset is split in training set, validation set, and test set. Statistics on the datasets, including largest
sequence length, are given in columns 2-4 of Table 1. Each sequence in the dataset represents a song
having a maximum polyphony of 15 notes (average 3.9); each time step input spans the whole range
of piano from A0 to C8 and it is represented by using 88 binary values (i.e. k = 88).
Our pre-training approach (PreT-RNN) has been assessed by using a different number of hidden
units (i.e., p is set in turn to 50, 100, 150, 200, 250) and 5000 epochs of RNN training1 using the
Theano-based stochastic gradient descent software available at [23].
Random initialisation (Rnd) has also been used for networks with the same number of hidden units.
Specifically, for networks with 50 hidden units, we have evaluated the performance of 6 different
random initialisations. Finally, in order to verify that the nonlinearity introduced by the RNN is
actually useful to solve the prediction task, we have also evaluated the performance of a network
with linear units (250 hidden units) initialised with our pre-training procedure (PreT-Lin250).
To give an idea of the time performance of pre-training with respect to the training of a RNN, in
column 5 of Table 1 we have reported the time in seconds needed to compute pre-training matrices
c
c
(Pre-) (on Intel
Xeon
CPU E5-2670 @2.60GHz with 128 GB) and to perform training of a
RNN with p = 50 for 5000 epochs (on GPU NVidia K20). Please, note that for larger values of p,
the increase in computation time of pre-training is smaller than the increment in computation time
needed for training a RNN.
1

Due to early overfitting, for the Muse dataset we used 1000 epochs.

6

Dataset
Nottingham

Piano-midi.de

MuseData

JSB Chorales

Set
Training
(39165 ? 56408)
Test
Validation
Training
(70672 ? 387640)
Test
Validation
Training
(248479 ? 214192)
Test
Validation
Training
(27674 ? 22792)
Test
Validation

# Samples
195

Max length
641

170
173
87

1495
1229
4405

25
12
524

2305
1740
2434

25
135
229

2305
2523
259

77
76

320
289

(Pre-)Training Time
seconds
(226) 5837
p = 50
5000 epochs
seconds
(2971) 4147
p = 50
5000 epochs
seconds
(7338) 4190
p = 50
5000 epochs
seconds
(79) 6411
p = 50
5000 epochs

Model
RNN (w. HF)
RNN-RBM
PreT-RNN
PreT-Lin250
RNN (w. HF)
RNN-RBM
PreT-RNN
PreT-Lin250
RNN (w. HF)
RNN-RBM
PreT-RNN
PreT-Lin250
RNN (w. HF)
RNN-RBM
PreT-RNN
PreT-Lin250

ACC% [21]
62.93 (66.64)
75.40
75.23 (p = 250)
73.19
19.33 (23.34)
28.92
37.74 (p = 250)
16.87
23.25 (30.49)
34.02
57.57 (p = 200)
3.56
28.46 (29.41)
33.12
65.67 (p = 250)
38.32

Table 1: Datasets statistics including data matrix size for the training set (columns 2-4), computational times in seconds to perform pre-training and training for 5000 epochs with p = 50 (column
5), and accuracy results for state-of-the-art models [21] vs our pre-training approach (columns 6-7).
The acronym (w. HF) is used to identify an RNN trained by Hessian Free Optimization [4].

Training and test curves for all the models described above are reported in Figure 1. It is evident that
random initialisation does not allow the RNN to improve its performance in a reasonable amount of
epochs. Specifically, for random initialisation with p = 50 (Rnd 50), we have reported the average
and range of variation over the 6 different trails: different initial points do not change substantially
the performance of RNN. Increasing the number of hidden units allows the RNN to slightly increase
its performance. Using pre-training, on the other hand, allows the RNN to start training from a quite
favourable point, as demonstrated by an early sharp improvement of performances. Moreover, the
more hidden units are used, the more the improvement in performance is obtained, till overfitting is
observed. In particular, early overfitting occurs for the Muse dataset. It can be noticed that the linear
model (Linear) reaches performances which are in some cases better than RNN without pre-training.
However, it is important to notice that while it achieves good results on the training set (e.g. JSB and
Piano-midi), the corresponding performance on the test set is poor, showing a clear evidence of overfitting. Finally, in column 7 of Table 1, we have reported the accuracy obtained after validation on
the number of hidden units and number of epochs for our approaches (PreT-RNN and PreT-Lin250)
versus the results reported in [21] for RNN (also using Hessian Free Optimization) and RNN-RBM.
In any case, the use of pre-training largely improves the performances over standard RNN (with
or without Hessian Free Optimization). Moreover, with the exception of the Nottingham dataset,
the proposed approach outperforms the state-of-the-art results achieved by RNN-RBM. Large improvements are observed for the Muse and JSB datasets. Performance for the Nottingham dataset
is basically equivalent to the one obtained by RNN-RBM. For this dataset, also the linear model
with pre-training achieves quite good results, which seems to suggest that the prediction task for
this dataset is much easier than for the other datasets. The linear model outperforms RNN without
pre-training on Nottingham and JSB datasets, but shows problems with the Muse dataset.

5

Conclusions

We have proposed a pre-training technique for RNN based on linear autoencoders for sequences.
For this kind of autoencoders it is possible to give a closed form solution for the definition of the
?optimal? weights, which however, entails the computation of the SVD decomposition of the full
data matrix. For large data matrices exact SVD decomposition cannot be achieved, so we proposed
a computationally efficient procedure to get an approximation that turned to be effective for our
goals. Experimental results for a prediction task on datasets of sequences of polyphonic music
show the usefulness of the proposed pre-training approach, since it allows to largely improve the
state of the art results on all the considered datasets by using simple stochastic gradient descend for
learning. Even if the results are very encouraging the method needs to be assessed on data from
other application domains. Moreover, it is interesting to understand whether the analysis performed
in [24] on linear deep networks for vectors can be extended to recurrent architectures for sequences
and, in particular, to our method.
7

0.4

0.2
0.1
0
-0.1
Rnd 50 (6 trials)
Linear 250
Rnd 100
0

200

600

PreT 200
PreT 250
800
Nottingham
Test Set

1000

0.8

Epoch

0.7

0.7

0.6

0.6

0.5

0.5

Accuracy

Accuracy

PreT 50
PreT 150
PreT 100

400
Nottingham Training
Set

0.8

0.4
0.3

0.4
0.3

0.2

0.2

0.1

0.1

0

0
0

500 1000 1500 2000 2500 3000 3500 4000 4500 5000
Epoch

0

500 1000 1500 2000 2500 3000 3500 4000 4500 5000
Epoch

Piano-Midi.de Training Set

Piano-Midi.de Test Set

0.55

0.4

0.5

0.35

Accuracy

0.45
0.4

0.3

0.35

0.25

0.3
0.25
0.2
0.15

0.2
0.15
0.1

0.1
0.05

0.05
0

0
0

500 1000 1500 2000 2500 3000 3500 4000 4500 5000
Epoch

0

500 1000 1500 2000 2500 3000 3500 4000 4500 5000
Epoch

Muse Dataset Training Set

Muse Dataset Test Set

0.7

0.6

0.6

0.5

0.5

0.4
Accuracy

Accuracy

0.4
0.3

0.3
0.2

0.2

0.1

0.1
0

0
0

200

400

600

800

1000

0

Epoch

200

400

600

800

1000

Epoch

JSB Chorales Training Set

JSB Chorales Test Set

0.8

0.7

0.7

0.6

0.6

0.5

0.5

Accuracy

-0.3

Rnd 150
Rnd 200
Rnd 250

Accuracy

-0.2

Accuracy

Accuracy

0.3

0.4
0.3

0.4
0.3
0.2

0.2

0.1

0.1
0

0
0

500 1000 1500 2000 2500 3000 3500 4000 4500 5000
Epoch

0

500 1000 1500 2000 2500 3000 3500 4000 4500 5000
Epoch

Figure 1: Training (left column) and test (right column) curves for the assessed approaches on the
four datasets. Curves are sampled at each epoch till epoch 100, and at steps of 100 epochs afterwards.
8

References
[1] S. C. Kremer. Field Guide to Dynamical Recurrent Networks. Wiley-IEEE Press, 2001.
[2] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent
is difficult. IEEE Transactions on Neural Networks, 5(2):157?166, 1994.
[3] S. Hochreiter and J. Schmidhuber. Lstm can solve hard long time lag problems. In NIPS, pages
473?479, 1996.
[4] J. Martens and I. Sutskever. Learning recurrent neural networks with hessian-free optimization.
In ICML, pages 1033?1040, 2011.
[5] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504?507, July 2006.
[6] G. E. Hinton, S. Osindero, and Y. W. Teh. A fast learning algorithm for deep belief nets. Neural
Computation, 18(7):1527?1554, 2006.
[7] P. di Lena, K. Nagata, and P. Baldi. Deep architectures for protein contact map prediction.
Bioinformatics, 28(19):2449?2457, 2012.
[8] Y. Bengio. Learning deep architectures for ai. Foundations and Trends in Machine Learning,
2(1):1?127, 2009.
[9] I. Sutskever, J. Martens, G. E. Dahl, and G. E. Hinton. On the importance of initialization and
momentum in deep learning. In ICML (3), pages 1139?1147, 2013.
[10] I.T. Jolliffe. Principal Component Analysis. Springer-Verlag New York, Inc., 2002.
[11] H. Bourlard and Y. Kamp. Auto-association by multilayer perceptrons and singular value
decomposition. Biological Cybernetics, 59(4-5):291?294, 1988.
[12] P. Baldi and K. Hornik. Neural networks and principal component analysis: Learning from
examples without local minima. Neural Networks, 2(1):53?58, 1989.
[13] A. Sperduti. Exact solutions for recursive principal components analysis of sequences and
trees. In ICANN (1), pages 349?356, 2006.
[14] A. Micheli and A. Sperduti. Recursive principal component analysis of graphs. In ICANN (2),
pages 826?835, 2007.
[15] A. Sperduti. Efficient computation of recursive principal component analysis for structured
input. In ECML, pages 335?346, 2007.
[16] A. Sperduti. Linear autoencoder networks for structured data. In NeSy?13:Ninth International
Workshop onNeural-Symbolic Learning and Reasoning, 2013.
[17] T. Voegtlin. Recursive principal components analysis. Neural Netw., 18(8):1051?1063, 2005.
[18] G. Martinsson et al. Randomized methods for computing the singular value decomposition
(svd) of very large matrices. In Works. on Alg. for Modern Mass. Data Sets, Palo Alto, 2010.
[19] E. Rabani and S. Toledo. Out-of-core svd and qr decompositions. In PPSC, 2001.
[20] Z. Zhang and H. Zha. Structure and perturbation analysis of truncated svds for columnpartitioned matrices. SIAM J. on Mat. Anal. and Appl., 22(4):1245?1262, 2001.
[21] N. Boulanger-Lewandowski, Y. Bengio, and P. Vincent. Modeling temporal dependencies in
high-dimensional sequences: Application to polyphonic music generation and transcription.
In ICML, 2012.
[22] M. Bay, A. F. Ehmann, and J. S. Downie. Evaluation of multiple-f0 estimation and tracking
systems. ISMIR, pages 315?320, 2009.
[23] https://github.com/gwtaylor/theano-rnn.
[24] A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of
learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 6279-natural-parameter-networks-a-class-of-probabilistic-neural-networks.pdf

Natural-Parameter Networks:
A Class of Probabilistic Neural Networks
Hao Wang, Xingjian Shi, Dit-Yan Yeung
Hong Kong University of Science and Technology
{hwangaz,xshiab,dyyeung}@cse.ust.hk

Abstract
Neural networks (NN) have achieved state-of-the-art performance in various applications. Unfortunately in applications where training data is insufficient, they are
often prone to overfitting. One effective way to alleviate this problem is to exploit
the Bayesian approach by using Bayesian neural networks (BNN). Another shortcoming of NN is the lack of flexibility to customize different distributions for the
weights and neurons according to the data, as is often done in probabilistic graphical models. To address these problems, we propose a class of probabilistic neural
networks, dubbed natural-parameter networks (NPN), as a novel and lightweight
Bayesian treatment of NN. NPN allows the usage of arbitrary exponential-family
distributions to model the weights and neurons. Different from traditional NN
and BNN, NPN takes distributions as input and goes through layers of transformation before producing distributions to match the target output distributions. As
a Bayesian treatment, efficient backpropagation (BP) is performed to learn the
natural parameters for the distributions over both the weights and neurons. The
output distributions of each layer, as byproducts, may be used as second-order
representations for the associated tasks such as link prediction. Experiments on
real-world datasets show that NPN can achieve state-of-the-art performance.

1

Introduction

Recently neural networks (NN) have achieved state-of-the-art performance in various applications
ranging from computer vision [12] to natural language processing [20]. However, NN trained by
stochastic gradient descent (SGD) or its variants is known to suffer from overfitting especially
when training data is insufficient. Besides overfitting, another problem of NN comes from the
underestimated uncertainty, which could lead to poor performance in applications like active learning.
Bayesian neural networks (BNN) offer the promise of tackling these problems in a principled way.
Early BNN works include methods based on Laplace approximation [16], variational inference (VI)
[11], and Monte Carlo sampling [18], but they have not been widely adopted due to their lack of
scalability. Some recent advances in this direction seem to shed light on the practical adoption of
BNN. [8] proposed a method based on VI in which a Monte Carlo estimate of a lower bound on the
marginal likelihood is used to infer the weights. Recently, [10] used an online version of expectation
propagation (EP), called ?probabilistic back propagation? (PBP), for the Bayesian learning of NN,
and [4] proposed ?Bayes by Backprop? (BBB), which can be viewed as an extension of [8] based on
the ?reparameterization trick? [13]. More recently, an interesting Bayesian treatment called ?Bayesian
dark knowledge? (BDK) was designed to approximate a teacher network with a simpler student
network based on stochastic gradient Langevin dynamics (SGLD) [1].
Although these recent methods are more practical than earlier ones, several outstanding problems
remain to be addressed: (1) most of these methods require sampling either at training time [8, 4, 1] or
at test time [4], incurring much higher cost than a ?vanilla? NN; (2) as mentioned in [1], methods
30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

based on online EP or VI do not involve sampling, but they need to compute the predictive density
by integrating out the parameters, which is computationally inefficient; (3) these methods assume
Gaussian distributions for the weights and neurons, allowing no flexibility to customize different
distributions according to the data as is done in probabilistic graphical models (PGM).
To address the problems, we propose natural-parameter networks (NPN) as a class of probabilistic
neural networks where the input, target output, weights, and neurons can all be modeled by arbitrary
exponential-family distributions (e.g., Poisson distributions for word counts) instead of being limited
to Gaussian distributions. Input distributions go through layers of linear and nonlinear transformation
deterministically before producing distributions to match the target output distributions (previous
work [21] shows that providing distributions as input by corrupting the data with noise plays the
role of regularization). As byproducts, output distributions of intermediate layers may be used as
second-order representations for the associated tasks. Thanks to the properties of the exponential
family [3, 19], distributions in NPN are defined by the corresponding natural parameters which can
be learned efficiently by backpropagation. Unlike [4, 1], NPN explicitly propagates the estimates of
uncertainty back and forth in deep networks. This way the uncertainty estimates for each layer of
neurons are readily available for the associated tasks. Our experiments show that such information is
helpful when neurons of intermediate layers are used as representations like in autoencoders (AE). In
summary, our main contributions are:
? We propose NPN as a class of probabilistic neural networks. Our model combines the merits
of NN and PGM in terms of computational efficiency and flexibility to customize the types
of distributions for different types of data.
? Leveraging the properties of the exponential family, some sampling-free backpropagationcompatible algorithms are designed to efficiently learn the distributions over weights by
learning the natural parameters.
? Unlike most probabilistic NN models, NPN obtains the uncertainty of intermediate-layer
neurons as byproducts, which provide valuable information to the learned representations.
Experiments on real-world datasets show that NPN can achieve state-of-the-art performance
on classification, regression, and unsupervised representation learning tasks.

2

Natural-Parameter Networks

The exponential family refers to an important class of distributions with useful algebraic properties.
Distributions in the exponential family have the form p(x|?) = h(x)g(?) exp{? T u(x)}, where x is
the random variable, ? denotes the natural parameters, u(x) is a vector of sufficient statistics, and
g(?) is the normalizer. For a given type of distributions, different choices of ? lead to different shapes.
c
1
For example, a univariate Gaussian distribution with ? = (c, d)T corresponds to N (? 2d
, ? 2d
).
Motivated by this observation, in NPN, only the natural parameters need to be learned to model the
distributions over the weights and neurons. Consider an NPN which takes a vector random distribution
(e.g., a multivariate Gaussian distribution) as input, multiplies it by a matrix random distribution,
goes through nonlinear transformation, and outputs another distribution. Since all three distributions
in the process can be specified by their natural parameters (given the types of distributions), learning
and prediction of the network can actually operate in the space of natural parameters. For example, if
we use element-wise (factorized) gamma distributions for both the weights and neurons, the NPN
counterpart of a vanilla network only needs twice the number of free parameters (weights) and
neurons since there are two natural parameters for each univariate gamma distribution.
2.1

Notation and Conventions

We use boldface uppercase letters like W to denote matrices and boldface lowercase letters like
b for vectors. Similarly, a boldface number (e.g., 1 or 0) represents a row vector or a matrix with
identical entries. In NPN, o(l) is used to denote the values of neurons in layer l before nonlinear
transformation and a(l) is for the values after nonlinear transformation. As mentioned above, NPN
tries to learn distributions over variables rather than variables themselves. Hence we use letters
without subscripts c, d, m, and s (e.g., o(l) and a(l) ) to denote ?random variables? with corresponding
distributions. Subscripts c and d are used to denote natural parameter pairs, such as Wc and Wd .
Similarly, subscripts m and s are for mean-variance pairs. Note that for clarity, many operations used
?z
below are implicitly element-wise, for example, the square z2 , division bz , partial derivative ?b
, the
2

gamma function ?(z), logarithm log z, factorial z!, 1 + z, and z1 . For the data D = {(xi , yi )}N
i=1 ,
(0)
(0)
(0)
we set am = xi , as = 0 (Input distributions with as 6= 0 resemble AE?s denoising effect.) as
input of the network and yi denotes the output targets (e.g., labels and word counts). In the following
text we drop the subscript i (and sometimes the superscript (l)) for clarity. The bracket (?, ?) denotes
concatenation or pairs of vectors.
2.2

Linear Transformation in NPN

Here we first introduce the linear form of a general NPN. For simplicity, we assume distributions
with two natural parameters (e.g., gamma distributions, beta distributions, and Gaussian distributions), ? = (c, d)T , in this section. Specifically, we have factorized distributions on the weight
Q
(l)
(l)
(l)
(l)
(l)
(l)
(l)
matrices, p(W(l) |Wc , Wd ) = i,j p(Wij |Wc,ij , Wd,ij ), where the pair (Wc,ij , Wd,ij ) is the
corresponding natural parameters. For b(l) , o(l) , and a(l) we assume similar factorized distributions.
In a traditional NN, the linear transformation follows o(l) = a(l?1) W(l) + b(l) where a(l?1) is the
output from the previous layer. In NN a(l?1) , W(l) , and b(l) are deterministic variables while in
NPN they are exponential-family distributions, meaning that the result o(l) is also a distribution. For
convenience of subsequent computation it is desirable to approximate o(l) using another exponentialfamily distribution. We can do this by matching the mean and variance. Specifically, after computing
(l)
(l)
(l)
(l)
(l)
(l)
(l)
(l)
(l)
(l)
(Wm , Ws ) = f (Wc , Wd ) and (bm , bs ) = f (bc , bd ), we can get oc and od through
(l)
(l)
the mean om and variance os of o(l) as follows:
(l?1)

(a(l?1)
, a(l?1)
) = f (a(l?1)
, ad
m
s
c
o(l)
s
(l)
(o(l)
,
o
c
d )

=

a(l?1)
Ws(l)
s

=f

?1

+

(l?1)
(l)
), o(l)
Wm
+ b(l)
m = am
m,

(l)
a(l?1)
(Wm
s

?

(l)
Wm
)

+

(a(l?1)
m

(1)
?

a(l?1)
)Ws(l)
m

+

b(l)
s ,

(2)

(l)
(o(l)
m , os ),

(3)

where ? denotes the element-wise product and the bijective function f (?, ?) maps the natural paramec+1
ters of a distribution into its mean and variance (e.g., f (c, d) = ( c+1
?d , d2 ) in gamma distributions).
(l)

(l)

(l)

(l)

Similarly we use f ?1 (?, ?) to denote the inverse transformation. Wm , Ws , bm , and bs are the
(l)
mean and variance of W(l) and b(l) obtained from the natural parameters. The computed om and
(l)
(l)
(l)
os can then be used to recover oc and od , which will subsequently facilitate the feedforward
computation of the nonlinear transformation described in Section 2.3.
2.3

Nonlinear Transformation in NPN
(l)

After we obtain the linearly transformed distribution over o(l) defined by natural parameters oc and
(l)
od , an element-wise nonlinear transformation v(?) (with a well defined inverse function v ?1 (?)) will
0
be imposed. The resulting activation distribution is pa (a(l) ) = po (v ?1 (a(l) ))|v ?1 (a(l) )|, where po
(l)
(l)
is the factorized distribution over o(l) defined by (oc , od ).
Though pa (a(l) ) may not be an exponential-family distribution, we can approximate it with one,
(l)
(l)
(l)
(l)
p(a(l) |ac , ad ), by matching the first two moments. Once the mean am and variance as of pa (a(l) )
?1
are obtained, we can compute corresponding natural parameters with f (?, ?) (approximation
accuracy is sufficient according to preliminary experiments). The feedforward computation is:
Z
am =

Z
po (o|oc , od )v(o)do, as =

po (o|oc , od )v(o)2 do ? a2m , (ac , ad ) = f ?1 (am , as ).

(4)

Here the key computational challenge is computing the integrals in Equation (4). Closed-form
solutions are needed for their efficient computation. If po (o|oc , od ) is a Gaussian distribution, closedform solutions exist for common activation functions like tanh(x) and max(0, x) (details are in
Section 3.2). Unfortunately this is not the case for other distributions. Leveraging the convenient
form of the exponential family, we find that it is possible to design activation functions so that the
integrals for non-Gaussian distributions can also be expressed in closed form.
Theorem 1. Assume an exponential-family distribution po (x|?) = h(x)g(?) exp{? T u(x)}, where
the vector u(x) = (u1 (x), u2 (x), . . . , uM (x))T (M is the number of natural parameters).
If activaR
tion function v(x) = r ? q exp(?? ui (x)) is used, the first two moments of v(x), po (x|?)v(x)dx
3

Table 1: Activation Functions for Exponential-Family Distributions
Distribution

Probability Density Function

Beta Distribution

p(x) =

Rayleigh Distribution
Gamma Distribution

p(x) =
p(x) =

Poisson Distribution

p(x) =

Gaussian Distribution

p(x) =

R

?(c+d)
xc?1 (1 ? x)d?1
?(c)?(d)
x
x2
exp{?
}
?2
2? 2
c c?1
1
d
x
exp{?dx}
?(c)
cx exp{?c}
x!
1
(2?? 2 )? 2 exp{? 2?12 (x ?

?)2 }

Activation Function

Support

qx? , ? ? (0, 1)

[0, 1]

r ? q exp{?? x2 }
r ? q exp{?? x}

(0, +?)
(0, +?)

r ? q exp{?? x}

Nonnegative interger

ReLU, tanh, and sigmoid

(??, +?)

2

and po (x|?)v(x) dx, can be expressed in closed form. Here i ? {1, 2, . . . , M } (different ui (x)
corresponds to a different set of activation functions) and r, q, and ? are constants.
e = (?1 , ?2 , . . . , ?i ? ?, . . . , ?M ), and ?
b =
Proof. We first let ? = (?1 , ?2 , . . . , ?M ), ?
(?1 , ?2 , . . . , ?i ? 2?, . . . , ?M ). The first moment of v(x) is
Z
E(v(x)) = r ? q h(x)g(?) exp{? T u(x) ? ? ui (x)} dx
Z
g(?)
g(?)
g(e
? ) exp{e
? T u(x)} dx = r ? q
.
= r ? q h(x)
g(e
?)
g(e
?)
g(? )
g(? )
Similarly the second moment can be computed as E(v(x)2 ) = r2 + q 2 g(?
b ) ? 2rq g(?
e) .
A more detailed proof is provided in the supplementary material. With Theorem 1, what remains is to
find the constants that make v(x) strictly increasing and bounded (Table 1 shows some exponentialfamily distributions and their possible activation functions). For example in Equation (4), if v(x) =
d
r ? q exp(?? x), am = r ? q( odo+?
)oc for the gamma distribution.
In the backpropagation, for distributions with two natural parameters the gradient consists of two
?as
?E
?E
?E
m
terms. For example, ?o
= ?a
? ?a
?oc + ?as ? ?oc , where E is the error term of the network.
c
m
Algorithm 1 Deep Nonlinear NPN
1: Input: Data D = {(xi , yi )}N
i=1 , number of iterations T , learning rate ?t , number of layers L.
2: for t = 1 : T do
3:
for l = 1 : L do
4:
Apply Equation (1)-(4) to compute the linear and nonlinear transformation in layer l.
5:
end for
(L)
(L)
(L)
(L)
6:
Compute the error E from (oc , od ) or (ac , ad ).
7:
for l = L : 1 do
8:
Compute ?E(l) , ?E(l) , ?E(l) , and ?E(l) . Compute ?E(l) , ?E(l) , ?E(l) , and ?E(l) .
?Wm

?Ws

?bm

?bs

?Wc

?Wd

?bc

?bd

9:
end for
(l)
(l)
(l)
(l)
10:
Update Wc , Wd , bc , and bd in all layers.
11: end for

2.4

Deep Nonlinear NPN

Naturally layers of nonlinear NPN can be stacked to form a deep NPN1 , as shown in Algorithm 12 . A
deep NPN is in some sense similar to a PGM with a chain structure. Unlike PGM in general, however,
NPN does not need costly inference algorithms like variational inference or Markov chain Monte
Carlo. For some chain-structured PGM (e.g, hidden Markov models), efficient inference algorithms
also exist due to their special structure. Similarly, the Markov property enables NPN to be efficiently
trained in an end-to-end backpropagation learning fashion in the space of natural parameters.
PGM is known to be more flexible than NN in the sense that it can choose different distributions to
depict different relationships among variables. A major drawback of PGM is its scalability especially
1

Although the approximation accuracy may decrease as NPN gets deeper during feedforward computation, it
can be automatically adjusted according to data during backpropagation.
2
Note that since the first part of Equation (1) and the last part of Equation (4) are canceled out, we can
(l)
(l)
(l)
(l)
directly use (am , as ) without computing (ac , ad ) here.

4

when the PGM is deep. Different from PGM, NN stacks relatively simple computational layers and
learns the parameters using backpropagation, which is computationally more efficient than most
algorithms for PGM. NPN has the potential to get the best of both worlds. In terms of flexibility,
different types of exponential-family distributions can be chosen for the weights and neurons. Using
gamma distributions for both the weights and neurons in NPN leads to a deep and nonlinear version
of nonnegative matrix factorization [14] while an NPN with the Bernoulli distribution and sigmoid
activation resembles a Bayesian treatment of sigmoid belief networks [17]. If Poisson distributions
are chosen for the neurons, NPN becomes a neural analogue of deep Poisson factor analysis [26, 9].
Note that similar to the weight decay in NN, we may add the KL divergence between the prior
distributions and the learned distributions on the weights to the error E for regularization (we use
isotropic Gaussian priors in the experiments). In NPN, the chosen prior distributions correspond to
priors in Bayesian models and the learned distributions correspond to the approximation of posterior
distributions on weights. Note that the generative story assumed here is that weights are sampled
from the prior, and then output is generated (given all data) from these weights.

3

Variants of NPN

In this section, we introduce three NPN variants with different properties to demonstrate the flexibility
and effectiveness of NPN. Note that in practice we use a transformed version of the natural parameters,
referred to as proxy natural parameters here, instead of the original ones for computational efficiency.
For example, in gamma distributions p(x|c, d) = ?(c)?1 dc xc?1 exp(?dx), we use proxy natural
parameters (c, d) during computation rather than the natural parameters (c ? 1, ?d).
3.1

Gamma NPN

The gamma distribution with support over positive values is an important member of the exponential
family. The corresponding probability density function is p(x|c, d) = ?(c)?1 dc xc?1 exp(?dx) with
(c ? 1, ?d) as its natural parameters (we use (c, d) as proxy natural parameters). If we assume
gamma distributions for W(l) , b(l) , o(l) , and a(l) , an AE formed by NPN becomes a deep and
nonlinear version of nonnegative matrix factorization [14]. To see this, note that this AE with
activation v(x) = x and zero biases b(l) is equivalent to finding a factorization of matrix X such that
QL
X = H l= L W(l) where H denotes the middle-layer neurons and W(l) has nonnegative entries
2

(l)

(l)

(l)

(l)

from gamma distributions. In this gamma NPN, parameters Wc , Wd , bc , and bd can be learned
following Algorithm 1. We detail the algorithm as follows:
Linear Transformation: Since gamma distributions are assumed here, we can use the function
(l)
(l)
(l)
(l)
(l)
(l)
(l)
(l)
f (c, d) = ( dc , dc2 ) to compute (Wm , Ws ) = f (Wc , Wd ), (bm , bs ) = f (bc , bd ), and
(l)
(l)
(l)
(l)
(oc , od ) = f ?1 (om , os ) during the probabilistic linear transformation in Equation (1)-(3).
Nonlinear Transformation: With the proxy natural parameters for the gamma distributions over
(l)
(l)
o(l) , the mean am and variance as for the nonlinearly transformed distribution over a(l) would
be obtained with Equation (4). Following Theorem 1, closed-form solutions are possible with
v(x) = r(1 ? exp(?? x)) (r = q and ui (x) = x) where r and ? are constants. Using this new
activation function, we have (see Section 2.1 and 6.1 of the supplementary material for details on the
function and derivation):
Z
ooc
od oc
am = po (o|oc , od )v(o)do = r(1 ? d ? ?(oc ) ? (od + ? )?oc ) = r(1 ? (
) ),
?(oc )
od + ?
od
od 2oc
as = r2 ((
)oc ? (
) ).
od + 2?
od + ?
(L)

Error: With oc

(L)

and od , we can compute the regression error E as the negative log-likelihood:
(L)

(L)
E = (log ?(o(L)
? log od
c ) ? oc

(L)

? (o(L)
? 1) ? log y + od
c

? y)1T ,

where y is the observed output corresponding to x. For classification, cross-entropy loss can be used
(l)
(l)
(l)
(l)
as E. Following the computation flow above, BP can be used to learn Wc , Wd , bc , and bd .
5

100
80
60
40

Y

20
0
?20
?40
?60
?80
?100
?6

?4

?2

0
X

2

4

6

Figure 1: Predictive distributions for PBP, BDK, dropout NN, and NPN. The shaded regions correspond to ?3 standard deviations. The black curve is the data-generating function and blue curves
show the mean of the predictive distributions. Red stars are the training data.
3.2 Gaussian NPN
Different from the gamma distribution which has support over positive values only, the Gaussian
distribution, also an exponential-family distribution, can describe real-valued random variables. This
makes it a natural choice for NPN. We refer to this NPN variant with Gaussian distributions over both
the weights and neurons as Gaussian NPN. Details of Algorithm 1 for Gaussian NPN are as follows:
Linear Transformation: Besides support over real values, another property of Gaussian distributions
is that the mean and variance can be used as proxy natural parameters, leading to an identity mapping
function f (c, d) = (c, d) which cuts the computation cost. We can use this function to compute
(l)
(l)
(l)
(l)
(l)
(l)
(l)
(l)
(l)
(l)
(l)
(l)
(Wm , Ws ) = f (Wc , Wd ), (bm , bs ) = f (bc , bd ), and (oc , od ) = f ?1 (om , os )
during the probabilistic linear transformation in Equation (1)-(3).
1
is used, am in
Nonlinear Transformation: If the sigmoid activation v(x) = ?(x) = 1+exp(?x)
Equation (4) would be (convolution of Gaussian with sigmoid is approximated by another sigmoid):
Z
oc
(5)
am = N (o|oc , diag(od )) ? ?(o)do ? ?(
1 ),
(1 + ? 2 od ) 2
Z
?(oc + ?)
as = N (o|oc , diag(od )) ? ?(o)2 do ? a2m ? ?(
) ? a2m ,
(6)
(1 + ? 2 ?2 od )1/2
?
?
where ? = 4 ? 2 2, ? = ? log( 2 + 1), and ? 2 = ?/8. Similar approximation can be applied for
activation v(x) = tanh(x) since tanh(x) = 2?(2x) ? 1.

If the ReLU activation v(x) = max(0, x) is used, we can use the techniques in [6] to obtain the first
two moments of max(z1 , z2 ) where z1 and z2 are Gaussian random variables. Full derivation for
v(x) = ?(x), v(x) = tanh(x), and v(x) = max(0, x) is left to the supplementary material.
(L)

(L)

Error: With oc and od in the last layer, we can then compute the error E as the KL divergence
(L)
(L)
KL(N (oc , diag(od )) k N (ym , diag())), where  is a vector with all entries equal to a small
 1T + ( 1 )(o(L) ? y)T ? K + (log o(L) )1T ? K log ). For
value . Hence the error E = 12 ( (L)
c
(L)
d
od

od

classification tasks, cross-entropy loss is used. Following the computation flow above, BP can be
(l)
(l)
(l)
(l)
used to learn Wc , Wd , bc , and bd .
3.3

Poisson NPN

The Poisson distribution, as another member of the exponential family, is often used to model counts
(e.g., counts of words, topics, or super topics in documents). Hence for text modeling, it is natural to
assume Poisson distributions for neurons in NPN. Interestingly, this design of Poisson NPN can be
seen as a neural analogue of some Poisson factor analysis models [26].
Besides closed-form nonlinear transformation, another challenge of Poisson NPN is to map the pair
(l)
(l)
(l)
(om , os ) to the single parameter
q oc of Poisson distributions. According to the central limit theorem,
(l)

(l)

(l)

(l)

we have oc = 14 (2om ? 1 + (2om ? 1)2 + 8os ) (see Section 3 and 6.3 of the supplementary
material for proofs, justifications, and detailed derivation of Poisson NPN).

4

Experiments

In this section we evaluate variants of NPN and other state-of-the-art methods on four real-world
datasets. We use Matlab (with GPU) to implement NPN, AE variants, and the ?vanilla? NN trained
with dropout SGD (dropout NN). For other baselines, we use the Theano library [2] and MXNet [5].
6

Table 2: Test Error Rates on MNIST
Method
Error

BDK
1.38%

BBB
1.34%

Dropout1
1.33%

Dropout2
1.40%

gamma NPN
1.27%

Gaussian NPN
1.25%

Table 3: Test Error Rates for Different Size of Training Data
Size
NPN
Dropout
BDK

4.1

100
29.97%
32.58%
30.08%

500
13.79%
15.39%
14.34%

2,000
7.89%
8.78%
8.31%

10,000
3.28%
3.53%
3.55%

Toy Regression Task

To gain some insights into NPN, we start with a toy 1d regression task so that the predicted mean and
variance can be visualized. Following [1], we generate 20 points in one dimension from a uniform
distribution in the interval [?4, 4]. The target outputs are sampled from the function y = x3 + n ,
where n ? N (0, 9). We fit the data with the Gaussian NPN, BDK, and PBP (see the supplementary
material for detailed hyperparameters). Figure 1 shows the predicted mean and variance of NPN,
BDK, and PBP along with the mean provided by the dropout NN (for larger versions of figures please
refer to the end of the supplementary materials). As we can see, the variance of PBP, BDK, and NPN
diverges as x is farther away from the training data. Both NPN?s and BDK?s predictive distributions
are accurate enough to keep most of the y = x3 curve inside the shaded regions with relatively low
variance. An interesting observation is that the training data points become more scattered when
x > 0. Ideally, the variance should start diverging from x = 0, which is what happens in NPN.
However, PBP and BDK are not sensitive enough to capture this dispersion change. In another dataset,
Boston Housing, the root mean square error for PBP, BDK, and NPN is 3.01, 2.82, and 2.57.
4.2

MNIST Classification

The MNIST digit dataset consists of 60,000 training images and 10,000 test images. All images
are labeled as one of the 10 digits. We train the models with 50,000 images and use 10,000 images
for validation. Networks with a structure of 784-800-800-10 are used for all methods, since 800
works best for the dropout NN (denoted as Dropout1 in Table 2) and BDK (BDK with a structure of
784-400-400-10 achieves an error rate of 1.41%). We also try the dropout NN with twice the number
of hidden neurons (Dropout2 in Table 2) for fair comparison. For BBB, we directly quote their results
from [4]. We implement BDK and NPN using the same hyperparameters as in [1] whenever possible.
Gaussian priors are used for NPN (see the supplementary material for detailed hyperparameters).
1
0.8
Accuracy

As shown in Table 2, BDK and BBB achieve comparable performance
with dropout NN (similar to [1], PBP is not included in the comparison
since it supports regression only), and gamma NPN slightly outperforms
dropout NN. Gaussian NPN is able to achieve a lower error rate of
1.25%. Note that BBB with Gaussian priors can only achieve an error
rate of 1.82%; 1.34% is the result of using Gaussian mixture priors. For
reference, the error rate for dropout NN with 1600 neurons in each hidden
layer is 1.40%. The time cost per epoch is 18.3s, 16.2s, and 6.4s for NPN,
BDK, NN respectively. Note that BDK is in C++ and NPN is in Matlab.

0.6
0.4
0.2
0

1

2

3

4 5 6
Variance

7

8

9

Figure 2: Classification accuracy
for different variance (uncertainty).
Note that ?1? in the x-axis means
as(L) 1T ? [0, 0.04), ?2? means
as(L) 1T ? [0.04, 0.08), etc.

To evaluate NPN?s ability as a Bayesian treatment to avoid overfitting,
we vary the size of the training set (from 100 to 10,000 data points) and compare the test error rates.
As shown in Table 3, the margin between the Gaussian NPN and dropout NN increases as the training
set shrinks. Besides, to verify the effectiveness of the estimated uncertainty, we split the test set into
(L)
9 subsets according NPN?s estimated variance (uncertainty) as 1T for each sample and show the
accuracy for each subset in Figure 2. We can find that the more uncertain NPN is, the lower the
accuracy, indicating that the estimated uncertainty is well calibrated.
4.3

Second-Order Representation Learning

Besides classification and regression, we also consider the problem of unsupervised representation
learning with a subsequent link prediction task. Three real-world datasets, Citeulike-a, Citeulike-t,
and arXiv, are used. The first two datasets are from [22, 23], collected separately from CiteULike in
different ways to mimic different real-world settings. The third one is from arXiv as one of the SNAP
datasets [15]. Citeulike-a consists of 16,980 documents, 8,000 terms, and 44,709 links (citations).
7

Table 4: Link Rank on Three Datasets
Method
Citeulike-a
Citeulike-t
arXiv

SAE
1104.7
2109.8
4232.7

SDAE
992.4
1356.8
2916.1

VAE
980.8
1599.6
3367.2

gamma NPN
851.7 (935.8)
1342.3 (1400.7)
2796.4 (3038.8)

Gaussian NPN
750.6 (823.9)
1280.4 (1330.7)
2687.9 (2923.8)

Poisson NPN
690.9 (5389.7)
1354.1 (9117.2)
2684.1 (10791.3)

Citeulike-t consists of 25,975 documents, 20,000 terms, and 32,565 links. The last dataset, arXiv,
consists of 27,770 documents, 8,000 terms, and 352,807 links.
The task is to perform unsupervised representation learning before feeding the extracted representations (middle-layer neurons) into a Bayesian LR algorithm [3]. We use the stacked autoencoder (SAE)
[7], stacked denoising autoencoder (SDAE) [21], variational autoencoder (VAE) [13] as baselines
(hyperparameters like weight decay and dropout rate are chosen by cross validation). As in SAE,
we use different variants of NPN to form autoencoders where both the input and output targets are
bag-of-words (BOW) vectors for the documents. The network structure for all models is B-100-50
(B is the number of terms). Please refer to the supplementary material for detailed hyperparameters.
350
300
Reconstruction error

One major advantage of NPN over SAE and SDAE is that the learned representations are distributions instead of point estimates. Since representations
from NPN contain both the mean and variance, we call them secondorder representations. Note that although VAE also produces second-order
representations, the variance part is simply parameterized by multilayer
perceptrons while NPN?s variance is naturally computed through propagation of distributions. These 50-dimensional representations with both mean
and variance are fed into a Bayesian LR algorithm for link prediction (for
deterministic AE the variance is set to 0).

[!h]

250
200
150
100
50
0
0

5

10
Variance

15

20

Figure 3: Reconstruction error
and estimated uncertainty for
each data point in Citeulike-a.

We use links among 80% of the nodes (documents) to train the Bayesian LR and use other links as
the test set. link rank and AUC (area under the ROC curve) are used as evaluation metrics. The link
rank is the average rank of the observed links from test nodes to training nodes. We compute the
AUC for every test node and report the average values. By definition, lower link rank and higher
AUC indicate better predictive performance and imply more powerful representations.
Table 4 shows the link rank for different models. For fair comparison we also try all baselines with
double budget (a structure of B-200-50) and report whichever has higher accuracy. As we can see, by
treating representations as distributions rather than points in a vector space, NPN is able to achieve
much lower link rank than all baselines, including VAE with variance information. The numbers in
the brackets show the link rank of NPN if we discard the variance information. The performance
gain from variance information verifies the effectiveness of the variance (uncertainty) estimated by
NPN. Among different variants of NPN, the Gaussian NPN seems to perform better in datasets with
fewer words like Citeulike-t (only 18.8 words per document). The Poisson NPN, as a more natural
choice to model text, achieves the best performance in datasets with more words (Citeulike-a and
arXiv). The performance in AUC is consistent with that in terms of the link rank (see Section 4 of the
supplementary material). To further verify the effectiveness of the estimated uncertainty, we plot the
(L)
reconstruction error and the variance os 1T for each data point of Citeulike-a in Figure 3. As we
can see, higher uncertainty often indicates not only higher reconstruction error E but also higher
variance in E.

5

Conclusion

We have introduced a family of models, called natural-parameter networks, as a novel class of probabilistic NN to combine the merits of NN and PGM. NPN regards the weights and neurons as arbitrary
exponential-family distributions rather than just point estimates or factorized Gaussian distributions.
Such flexibility enables richer descriptions of hierarchical relationships among latent variables and
adds another degree of freedom to customize NN for different types of data. Efficient sampling-free
backpropagation-compatible algorithms are designed for the learning of NPN. Experiments show that
NPN achieves state-of-the-art performance on classification, regression, and representation learning
tasks. As possible extensions of NPN, it would be interesting to connect NPN to arbitrary PGM to
form fully Bayesian deep learning models [24, 25], allowing even richer descriptions of relationships
among latent variables. It is also worth noting that NPN cannot be defined as generative models
and, unlike PGM, the same NPN model cannot be used to support multiple types of inference (with
different observed and hidden variables). We will try to address these limitations in our future work.
8

References
[1] A. K. Balan, V. Rathod, K. P. Murphy, and M. Welling. Bayesian dark knowledge. In NIPS, 2015.
[2] F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. J. Goodfellow, A. Bergeron, N. Bouchard, and Y. Bengio.
Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS
2012 Workshop, 2012.
[3] C. M. Bishop. Pattern Recognition and Machine Learning. Springer-Verlag New York, Inc., Secaucus, NJ,
USA, 2006.
[4] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra. Weight uncertainty in neural network. In
ICML, 2015.
[5] T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao, B. Xu, C. Zhang, and Z. Zhang. Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems. CoRR, abs/1512.01274,
2015.
[6] C. E. Clark. The greatest of a finite set of random variables. Operations Research, 9(2):145?162, 1961.
[7] I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Book in preparation for MIT Press, 2016.
[8] A. Graves. Practical variational inference for neural networks. In NIPS, 2011.
[9] R. Henao, Z. Gan, J. Lu, and L. Carin. Deep poisson factor modeling. In NIPS, 2015.
[10] J. M. Hern?ndez-Lobato and R. Adams. Probabilistic backpropagation for scalable learning of Bayesian
neural networks. In ICML, 2015.
[11] G. E. Hinton and D. Van Camp. Keeping the neural networks simple by minimizing the description length
of the weights. In COLT, 1993.
[12] A. Karpathy and F. Li. Deep visual-semantic alignments for generating image descriptions. In CVPR,
2015.
[13] D. P. Kingma and M. Welling. Auto-encoding variational Bayes. CoRR, abs/1312.6114, 2013.
[14] D. D. Lee and H. S. Seung. Algorithms for non-negative matrix factorization. In NIPS, 2001.
[15] J. Leskovec and A. Krevl. SNAP Datasets: Stanford large network dataset collection. http://snap.
stanford.edu/data, June 2014.
[16] J. MacKay David. A practical Bayesian framework for backprop networks. Neural computation, 1992.
[17] R. M. Neal. Learning stochastic feedforward networks. Department of Computer Science, University of
Toronto, 1990.
[18] R. M. Neal. Bayesian learning for neural networks. PhD thesis, University of Toronto, 1995.
[19] R. Ranganath, L. Tang, L. Charlin, and D. M. Blei. Deep exponential families. In AISTATS, 2015.
[20] R. Salakhutdinov and G. E. Hinton. Semantic hashing. Int. J. Approx. Reasoning, 50(7):969?978, 2009.
[21] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol. Stacked denoising autoencoders:
Learning useful representations in a deep network with a local denoising criterion. JMLR, 11:3371?3408,
2010.
[22] C. Wang and D. M. Blei. Collaborative topic modeling for recommending scientific articles. In KDD,
2011.
[23] H. Wang, B. Chen, and W.-J. Li. Collaborative topic regression with social regularization for tag recommendation. In IJCAI, 2013.
[24] H. Wang, N. Wang, and D. Yeung. Collaborative deep learning for recommender systems. In KDD, 2015.
[25] H. Wang and D. Yeung. Towards Bayesian deep learning: A framework and some existing methods. TKDE,
2016, to appear.
[26] M. Zhou, L. Hannah, D. B. Dunson, and L. Carin. Beta-negative binomial process and poisson factor
analysis. In AISTATS, 2012.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 5278-general-stochastic-networks-for-classification.pdf

General Stochastic Networks for Classification
Matthias Z?ohrer and Franz Pernkopf
Signal Processing and Speech Communication Laboratory
Graz University of Technology
matthias.zoehrer@tugraz.at, pernkopf@tugraz.at

Abstract
We extend generative stochastic networks to supervised learning of representations. In particular, we introduce a hybrid training objective considering a generative and discriminative cost function governed by a trade-off parameter ?. We use
a new variant of network training involving noise injection, i.e. walkback training, to jointly optimize multiple network layers. Neither additional regularization
constraints, such as `1, `2 norms or dropout variants, nor pooling- or convolutional layers were added. Nevertheless, we are able to obtain state-of-the-art performance on the MNIST dataset, without using permutation invariant digits and
outperform baseline models on sub-variants of the MNIST and rectangles dataset
significantly.

1

Introduction

Since 2006 there has been a boost in machine learning due to improvements in the field of unsupervised learning of representations. Most accomplishments originate from variants of restricted
Boltzmann machines (RBMs) [1], auto-encoders (AE) [2, 3] and sparse-coding [4, 5, 6]. Deep models in representation learning, also obtain impressive results in supervised learning problems, such
as speech recognition, e.g. [7, 8, 9] and computer vision tasks [10].
If no a-priori knowledge is modeled in the architecture, cf. convolutional layers or pooling layers
[11], generatively pre-trained networks are among the best when applied to supervised learning tasks
[12]. Usually, a generative representation is obtained through a greedy-layerwise training procedure
called contrastive divergence (CD) [1]. In this case, the network layer learns the representation from
the layer below by treating the latter as static input. Despite of the impressive results achieved with
CD, we identify two (minor) drawbacks when used for supervised learning: Firstly, after obtaining
a representation by pre-training a network, a new discriminative model is initialized with the trained
weights, splitting the training into two separate models. This seems to be neither biologically plausible, nor optimal when it comes to optimization, as carefully designed early stopping criteria have to
be implemented to prevent over- or under-fitting. Secondly, generative and discriminative objectives
might influence each other beneficially when combined during training. CD does not take this into
account.
In this work, we introduce a new training procedure for supervised learning of representations. In
particular we define a hybrid training objective for general stochastic networks (GSN), dividing the
cost function into a generative and discriminative part, controlled by a trade-off parameter ?. It turns
out that by annealing ?, when solving this unconstrained non-convex multi-objective optimization
problem, we do not suffer from the shortcomings described above. We are able to obtain stateof-the-art performance on the MNIST [13] dataset, without using permutation invariant digits and
significantly outperform baseline models on sub-variants of the MNIST and rectangle database [14].
Our approach is related to the generative-discriminative training approach of RBMs [15]. However
a different model and a new variant of network training involving noise injection, i.e. walkback
training [16, 17], is used to jointly optimize multiple network layers. Most notably, we did not
1

apply any additional regularization constraints, such as `1, `2 norms or dropout variants [12], [18],
unlocking further potential for possible optimizations. The model can be extended to learn multiple
tasks at the same time using jointly trained weights and by introducing multiple objectives. This
might also open a new prospect in the field of transfer learning [19] and multi-task learning [20]
beyond classification.
This paper is organized as follows: Section 2 presents mathematical background material i.e. the
GSN and a hybrid learning criterion. In Section 3 we empirically study the influence of hyper
parameters of GSNs and present experimental results. Section 4 concludes the paper and provides a
perspective on future work.

2

General Stochastic Networks

Recently, a new supervised learning algorithm called walkback training for generalized autoencoders (GAE) was introduced [16]. A follow-up study [17] defined a new network model ?
generative stochastic networks, extending the idea of walkback training to multiple layers. When
applied to image reconstruction, they were able to outperform various baseline systems, due to its
ability to learn multi-modal representations [17, 21]. In this paper, we extend the work of [17].
First, we provide mathematical background material for generative stochastic networks. Then, we
introduce modifications to make the model suitable for supervised learning. In particular we present
a hybrid training objective, dividing the cost into a generative and discriminative part. This paves
the way for any multi-objective learning of GSNs. We also introduce a new terminology, i.e. general stochastic networks, a model class including generative-, discriminative- and hybrid stochastic
network variants.
General Stochastic Networks for Unsupervised Learning
Restricted Boltzmann machines (RBM) [22] and denoising autoencoders (DAE) [3] share the following commonality; The input distribution P (X) is sampled to convergence in a Markov chain.
In the case of the DAE, the transition operator first samples the hidden state Ht from a corruption
distribution C(H|X), and generates a reconstruction from the parametrized model, i.e the density
P?2 (X|H).
Ht+1
P? 1

Ht+2
P?1

P?2

Xt+0

Ht+3
P?1

P?2

Xt+1

Ht+4

P?2

Xt+2

P? 1

P?1
P?2

Xt+3

Xt+4

Figure 1: DAE Markov chain.
The resulting DAE Markov chain, shown in Figure 1, is defined as
Ht+1 ? P?1 (H|Xt+0 ) and Xt+1 ? P?2 (X|Ht+1 ),

(1)

where Xt+0 is the input sample X, fed into the chain at time step 0 and Xt+1 is the reconstruction
of X at time step 1. In the case of a GSN, an additional dependency between the latent variables Ht
over time is introduced to the network graph. The GSN Markov chain is defined as follows:
Ht+1 ? P?1 (H|Ht+0 , Xt+0 ) and Xt+1 ? P?2 (X|Ht+1 ).

(2)

Figure 2 shows the corresponding network graph.
This chain can be expressed with deterministic functions of random variables f? ? {f?? , f?? }. In
particular, the density f? is used to model Ht+1 = f? (Xt+0 , Zt+0 , Ht+0 ), specified for some independent noise source Zt+0 , with the condition that Xt+0 cannot be recovered exactly from Ht+1 .
2

Ht+0

Ht+1

Ht+2

P?1

Ht+3

P?1
P?2

Xt+0

Ht+4

P?1
P?2

Xt+1

P?1

P?1
P?2

Xt+2

P? 2

Xt+3

Xt+4

Figure 2: GSN Markov chain.
We introduce f??i as a back-probable stochastic non-linearity of the form f??i = ?out + g(?in + a
?i )
with noise processes Zt ? {?in , ?out } for layer i. The variable a
?i is the activation for unit i, where
a
?i = W i Iti + bi with a weight matrix W i and bias bi , representing the parametric distribution. It is
embedded in a non-linear activation function g. The input Iti is either the realization xit of observed
sample Xti or the hidden realization hit of Hti . In general, f??i (Iti ) specifies an upward path in a GSN
i
for a specific layer i. In the case of Xt+1
= f??i (Zt+0 , Ht+1 ) we define f??i (Hti ) = ?out + g(?in + a
?i )
i
i
i T
i
as a downward path in the network i.e. a
? = (W ) Ht + b , using the transpose of the weight
matrix W i and the bias bi . This formulation allows to directly back-propagate the reconstruction log-likelihood P (X|H) for all parameters ? ? {W 0 , ..., W d , b0 , ..., bd } where d is the
number of hidden layers. In Figure 2 the GSN includes a simple hidden layer. This can be
extended to multiple hidden layers requiring multiple deterministic functions of random variables
f? ? {f??0 , ..., f??d , f??0 , ...f??d }.
Figure 3 visualizes the Markov chain for a multi-layer GSN, inspired by the unfolded computational
graph of a deep Boltzmann machine Gibbs sampling process.
3
Ht+3

3
Ht+4

f??2

f??2

2
Ht+2

f??1

f??1

f??1

f??1

1
Ht+3

f??0

f??2

2
Ht+4

f??1

1
Ht+2

f??0

f??0

f??2

2
Ht+3

f??1

1
Ht+1

f??0

f??2

f??0

f??1

1
Ht+4

f??0

f??0

f??0

f??0

0
Xt+0

0
Xt+1

0
Xt+2

0
Xt+3

0
Xt+4

Xt+0

0
Lt {Xt+1
, Xt+0 }

0
Lt {Xt+2
, Xt+0 }

0
Lt {Xt+3
, Xt+0 }

0
Lt {Xt+4
, Xt+0 }

Figure 3: GSN Markov chain with multiple layers and backprop-able stochastic units.
In the training case, alternatively even or odd layers are updated at the same time. The information
is propagated both upwards and downwards for K steps allowing the network to build higher order
representations. An example for this update process is given in Figure 3. In the even update (marked
2
1
0
0
1
= f??0 (Ht+1
) and Ht+2
=
in red) Ht+1
= f??0 (Xt+0
). In the odd update (marked in blue) Xt+1
?
?
?
?
2
1
1
0
2
3
0
1
2
1
f? (Ht+1 ) for k = 0. In the case of k = 1, Ht+2 = f? (Xt+1 ) + f? (Ht+2 ) and Ht+3 = f? (Ht+2 )
1
2
1
3
0
= f??0 (Ht+2
) and Ht+3
= f??1 (Ht+2
) + f??2 (Ht+3
) in the odd update.
in the even update and Xt+2
?
?
?
1
0
2
3
2
0
1
2
In case of k = 2, Ht+3 = f? (Xt+2 ) + f? (Ht+3 ) and Ht+4 = f? (Ht+3 ) in the even update and
0
1
2
1
3
Xt+3
= f??0 (Ht+3
) and Ht+4
= f??1 (Ht+3
) + f??2 (Ht+4
) in the odd update.
The cost function of a generative GSN can be written as:

C=

K
X

0
Lt {Xt+k
, Xt+0 },

k=1

3

(3)

Lt is a specific loss-function such as the mean squared error (MSE) at time step t. In general any
0
arbitrary loss function could be used (as long as they can be seen as a log-likelihood) [16]. Xt+k
0
is the reconstruction of the input Xt+0 at layer 0 after k steps. Optimizing the loss function by
building the sum over the costs of multiple corrupted reconstructions is called walkback training
[16, 17]. This form of network training leads to a significant performance boost when used for input
reconstruction. The network is able to handle multi-modal input representations and is therefore
considerably more favorable than standard generative models [16].
General Stochastic Networks for Supervised Learning
In order to make a GSN suitable for a supervised learning task we introduce the output Y to the
network graph. In this case L = log P (X) + log P (Y |X). Although the target Y is not fed into the
network, it is introduced as an additional cost term. The layer update-process stays the same.
3
Lt {Ht+1
, Yt+0 }

3
Lt {Ht+2
, Yt+0 }

3
Ht+3

3
Ht+4

f??2

f??2

2
Ht+2

f??1

f??1

f??0

f??1

f??1

1
Ht+3

f??0

f??2

2
Ht+4

f??1

1
Ht+2

f??0

f??2

2
Ht+3

f??1

1
Ht+1

f??0

f??2

f??0

f??1

1
Ht+4

f??0

f??0

f??0

f??0

0
Xt+0

0
Xt+1

0
Xt+2

0
Xt+3

0
Xt+4

Xt+0

0
Lt {Xt+1
, Xt+0 }

0
Lt {Xt+2
, Xt+0 }

0
Lt {Xt+3
, Xt+0 }

0
Lt {Xt+4
, Xt+0 }

Figure 4: GSN Markov chain for input Xt+0 and target Yt+0 with backprop-able stochastic units.
We define the following cost function for a 3-layer GSN:

C=

K
K
1?? X
? X
3
Lt {Xt+k , Xt+0 } +
Lt {Ht+k
, Yt+0 }
K
K ?d+1
k=1
k=d
|
{z
} |
{z
}
generative
discriminative

(4)

This is a non-convex multi-objective optimization problem, where ? weights the generative and
discriminative part of C. The parameter d specifies the number of network layers i.e. depth of the
network. Scaling the mean loss in (4) is not mandatory, but allows to equally balance both loss terms
with ? = 0.5 for input Xt+0 and target Yt+0 scaled to the same range. Again Figure 4 shows the
corresponding network graph for supervised learning with red and blue edges denoting the even and
odd network updates.
In general the hybrid objective optimization criterion is not restricted to hX, Y i, as additional input
and output terms could be introduced to the network. This setup might be useful for transfer-learning
[19] or multi-task scenarios [20], which is not discussed in this paper.

3

Experimental Results

In order to evaluate the capabilities of GSNs for supervised learning, we studied MNIST digits
[13], variants of MNIST digits [14] and the rectangle datasets [14]. The first database consists of
60.000 labeled training and 10.000 labeled test images of handwritten digits. The second dataset includes 6 variants of MNIST digits, i.e. { mnist-basic, mnist-rot, mnist-back-rand, mnist-back-image,
mnist-rot-back-image }, with additional factors of variation added to the original data. Each variant
includes 10.000 labeled training, 2000 labeled validation, and 50.000 labeled test images. The third
dataset involves two subsets, i.e. { rectangle, rectangle-image }. The dataset rectangle consists of
4

1000 labeled training, 200 labeled validation, and 50.000 labeled test images. The dataset rectangleimage includes 10.000 labeled train, 2000 labeled validation and 50.000 labeled test images.
In a first experiment we focused on the multi-objective optimization problem defined in (4). Next we
evaluated the number of walkback steps in a GSN, necessary for convergence. In a third experiment
we analyzed the influence of different Gaussian noise settings during walkback training, improving
the generalization capabilities of the network. Finally we summarize classification results for all
datasets and compare to baseline systems [14].
3.1

Multi-Objective Optimization in a Hybrid Learning Setup

In order to solve the non-convex multi-objective optimization problem, variants of stochastic gradient descent (SGD) can be used. We applied a search over fixed ? values on all problems. Furthermore, we show that the use of an annealed ? factor, during training works best in practice.
In all experiments a three layer GSN, i.e. GSN-3, with 2000 neurons in each layer, randomly initialized with small Gaussian noise, i.e. 0.01 ? N (0, 1), and an MSE loss function for both inputs and
targets was used. Regarding optimization we applied SGD with a learning rate ? = 0.1, a momentum term of 0.9 and a multiplicative annealing factor ?n+1 = ?n ? 0.99 per epoch n for the learning
rate. A rectifier unit [23] was chosen as activation function. Following the ideas of [24] no explicit
sampling was applied at the input and output layer. In the test case the zero-one loss was computed
averaging the network?s output over k walkback steps.
Analysis of the Hybrid Learning Parameter ?
Concerning the influence of the trade-off parameter ?, we tested fixed ? values in the range
? ? {0.01, 0.1, 0.2, ..., 0.9, 0.99}, where low values emphasize the discriminative part in the objective and vice versa. Walkback training with K = 6 steps using zero-mean pre- and postactivation Gaussian noise with zero mean and variance ? = 0.1 was performed for 500 training epochs. In a more dynamic scenario ?n=1 = 1 was annealed by ?n+1 = ?n ? ? to reach
?n=500 ? {0.01, 0.1, 0.2, ..., 0.9, 0.99} within 500 epochs, simulating generative pre-training to a
certain extend.

Figure 5: Influence of dynamic and static ? on MNIST variants basic (left), rotated (middle) and
background (right) where ? denotes the training-, 4 the validation- and 5 the test-set. The dashed
line denotes the static setup, the bold line the dynamic setup.

Figure 5 compares the results of both GSNs, using static and dynamic ? setups on the MNIST
variants basic, rotated and background. The use of a dynamic i.e. annealed ?n=500 = 0.01, achieved
the best validation and test error in all experiments. In this case, more attention was given to the
generative proportion P (X) of the objective (4) in the early stage of training. After approximately
400 epochs discriminative training i.e. fine-tuning, dominates. This setup is closely related to DBN
training, where emphasis is on optimizing P (X) at the beginning of the optimization, whereas
P (Y |X) is important at the last stages. In case of the GSN, the annealed ? achieves a more smooth
transition by shifting the weight in the optimization criterion from P (X) to P (Y |X) within one
model.
5

Analysis of Walkback Steps K
In a next experiment we tested the influence of K walkback steps for GSNs. Figure 6 shows the
results for different GSNs, trained with K ? {6, 7, 8, 9, 10} walkback steps and annealed ? with
? = 0.99. In all cases the information was at least propagated once up and once downwards in the
d = 3 layer network using fixed Gaussian pre- and post-activation noise with ? = 0 and ? = 0.1.

Figure 6: Evaluating the number of walkback steps on MNIST variants basic (left), rotated (middle)
and background (right) where ? denotes the training-, 4 the validation- and 5 the test-set.
Figure 6 shows that increasing the walkback steps, does not improve the generalization capabilities
of the used GSNs. The setup K = 2 ? d is sufficient for convergence and achieves the best validation
and test result in all experiments.
Analysis of Pre- and Post-Activation Noise
Injecting noise during the training process of GSNs serves as a regularizer and improves the generalization capabilities of the model [17]. In this experiment the influence of Gaussian pre- and
post-activation noise with ? = 0 and ? ? {0.05, 0.1, 0.15, 0.2, 0.25, 0.3} and deactivated noise
during training, was tested on a GSN-3 trained for K = 6 walkback steps. The trade-off factor
? was annealed with ? = 0.99. Figure 7 summarizes the results of the different GSNs for the
MNIST variants basic, rotated and background. Setting ? = 0.1 achieved the best overall result
on the validation- and test-set for all three experiments. In all other cases the GSNs either over- or
underfitted the data.

Figure 7: Evaluating noise injections during training on MNIST variants basic (left), rotated (middle)
and background (right) where ? denotes the training-, 4 the validation- and 5 the test-set.

3.2

MNIST results

Table 1 presents the average classification error of three runs of all MNIST variation datasets obtained by a GSN-3, using fixed Gaussian pre- and post-activation noise with ? = 0, ? = 0.1 and
K = 6 walkback steps. The hybrid learning parameter ? was annealed with ? = 0.99 and ?n=1 = 1.
A small grid test was performed in the range of N ? d with N ? {1000, 2000, 3000} neurons per
layer for d ? {1, 2, 3} layers to find the optimal network configuration.
6

Dataset

SVMrbf

SVMpoly NNet

DBN-1

SAA-3

DBN-3

GSN-3

mnist-basic

3.03
?0.15

3.69
?0.17

4.69
?0.19

3.94
?0.17

3.46
?0.16

3.11
?0.15

2.40
?0.04

mnist-rot*

11.11
?0.28

15.42
?0.32

18.11
?0.34

10.30
?0.27

10.30
?0.27

14.69
?0.31

8.66
?0.08

mnist-back-rand

14.58
?0.31

16.62
?0.33

20.04
?0.35

9.80
?0.26

11.28
?0.28

6.73
?0.22

9.38
?0.03

mnist-back-image

22.61
?0.37

24.01
?0.37

27.41
?0.39

16.15
?0.32

23.00
?0.37

16.31
?0.32

16.04
?0.04

mnist-rot-back-image*

55.18
?0.44
2.15
?0.13

56.41
?0.43
2.15
?0.13

62.16
?0.43
7.16
?0.23

47.39
?0.44
4.71
?0.19

51.93
?0.44
2.41
?0.13

52.21
?0.44
2.60
?0.14

43.86
?0.05
2.04
?0.04

24.04
?0.37

24.05
?0.37

33.20
?0.41

23.69
?0.37

24.05
?0.37

22.50
?0.37

22.10
?0.03

rectangles
rectangles-image

Table 1: MNIST variations and recangle results [14]; For datasets marked by (*) updated results are
shown [25].

Table 1 shows that a three layer GSN clearly outperforms all other models, except for the MNIST
random-background dataset. In particular, when comparing the GSN-3 to the radial basis function
support vector machine (SVMrbf), i.e. the second best model on MNIST basic, the GSN-3 achieved
an relative improvement of 20.79% on the test set. On the MNIST rotated dataset the GSN-3 was
able to beat the second best model i.e. DBN-1, by 15.92% on the test set. On the MNIST rotatedbackground there is an relative improvement of 7.25% on the test set between the second best model,
i.e. DBN-1, and the GSN-3. All results are statistically significant. Regarding the number of model
parameters, although we cannot directly compare the models in terms of network parameters, it is
worth to mention that a far smaller grid test was used to generate the results for all GSNs, cf. [14].
When comparing the classification error of the GSN-3 trained without noise, obtained in the previous
experiments (7) with Table 1, the GSN-3 achieved the test error of 2.72% on the MNIST variant
basic, outperforming all other models on this task. On the MNIST variant rotated, the GSN-3 also
outperformed the DBN-3, obtaining a test error of 11.2%. This indicates that not only the Gaussian
regularizer in the walkback training improves the generalization capabilities of the network, but also
the hybrid training criterion of the GSN.
Table 2 lists the results for the MNIST dataset without additional affine transformations applied to
the data i.e. permutation invariant digits. A three layer GSN achieved the state-of-the-art test error
of 0.80%.
Network

Result

Rectifier MLP + dropout [12]
DBM [26]
Maxout MLP + dropout [27]
MP-DBM [28]
Deep Convex Network [29]
Manifold Tangent Classifier [30]
DBM + dropout [12]
GSN-3

1.05%
0.95%
0.94%
0.91%
0.83%
0.81%
0.79%
0.80%

Table 2: MNIST results.

7

It might be worth noting that in addition to the noise process in walkback training, no other regularizers, such as `1, `2 norms and dropout variants [12], [18] were used in the GSNs. In general ? 800
training epochs with early-stopping are necessary for GSN training.
All simulations1 were executed on a GPU with the help of the mathematical expression compiler
Theano [31].

4

Conclusions and Future Work

We have extended GSNs for classification problems. In particular we defined an hybrid multiobjective training criterion for GSNs, dividing the cost function into a generative and discriminative
part. This renders the need for generative pre-training unnecessary. We analyzed the influence of
the objective?s trade-off parameter ? empirically, showing that by annealing ? we outperform a
static choice of ?. Furthermore, we discussed effects of noise injections and sampling steps during
walkback training. As a conservative starting point we restricted the model to use only rectifier
units. Neither additional regularization constraints, such as `1, `2 norms or dropout variants [12],
[18], nor pooling- [11, 32] or convolutional layers [11] were added. Nevertheless, the GSN was
able to outperform various baseline systems, in particular a deep belief network (DBN), a multi
layer perceptron (MLP), a support vector machine (SVM) and a stacked auto-associator (SSA), on
variants of the MNIST dataset. Furthermore, we also achieved state-of-the-art performance on the
original MNIST dataset without permutation invariant digits. The model not only converges faster
in terms of training iterations, but also show better generalization behavior in most cases. Our
approach opens a wide field of new applications for GSNs. In future research we explore adaptive
noise injection methods for GSNs and non-convex multi-objective optimization strategies.

References
[1] G. E. Hinton, S. Osindero, and Y. Teh, ?A fast learning algorithm for deep belief nets.? Neural computation, vol. 18, no. 7, pp. 1527?1554, 2006.
[2] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle, ?Greedy layer-wise training of deep networks,? in
Advances in Neural Information Processing Systems (NIPS), 2007, pp. 153?160.
[3] P. Vincent, H. Larochelle, Y. Bengio, and P. Manzagol, ?Extracting and composing robust features with
denoising autoencoders,? in International Conference on Machine Learning (ICML), 2008, pp. 1096?
1103.
[4] H. Lee, A. Battle, R. Raina, and A. Y. Ng, ?Efficient sparse coding algorithms,? in Advances in Neural
Information Processing Systems (NIPS), 2007, pp. 801?808.
[5] J. Ngiam, Z. Chen, S. A. Bhaskar, P. W. Koh, and A. Y. Ng, ?Sparse filtering,? in Advances in Neural
Information Processing Systems (NIPS), 2011, pp. 1125?1133.
[6] M. Ranzato, M. Poultney, S. Chopra, and Y. LeCun, ?Efficient learning of sparse representations with an
energy-based model,? in Advances in Neural Information Processing Systems (NIPS), 2006, pp. 1137?
1144.
[7] G. E. Dahl, M. Ranzato, A. Mohamed, and G. E. Hinton, ?Phone recognition with the mean-covariance
restricted Boltzmann machine,? in Advances in Neural Information Processing Systems (NIPS), 2010, pp.
469?477.
[8] L. Deng, M. L. Seltzer, D. Yu, A. Acero, A. Mohamed, and G. E. Hinton, ?Binary coding of speech
spectrograms using a deep auto-encoder.? in Interspeech, 2010, pp. 1692?1695.
[9] F. Seide, G. Li, and D. Yu, ?Conversational speech transcription using context-dependent deep neural
networks.? in Interspeech, 2011, pp. 437?440.
[10] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ?Imagenet classification with deep convolutional neural
networks,? in Advances in Neural Information Processing Systems (NIPS), 2012, pp. 1097?1105.
[11] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, ?Gradient-based learning applied to document recognition,? Proceedings of the IEEE, vol. 86, no. 11, 1998.
[12] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, ?Improving neural networks by preventing co-adaptation of feature detectors,? CoRR, vol. abs/1207.0580, 2012.
1

The code will be made publicly available for reproducing the results.

8

[13] Y. Lecun and C. Cortes, ?The MNIST database of handwritten digits,? 2014. [Online]. Available:
http://yann.lecun.com/exdb/mnist/
[14] H. Larochelle, D. Erhan, A. Courville, J. Bergstra, and Y. Bengio, ?An empirical evaluation of deep architectures on problems with many factors of variation,? in International Conference on Machine Learning
(ICML), 2007, pp. 473?480.
[15] H. Larochelle, M. Mandel, R. Pascanu, and Y. Bengio, ?Learning algorithms for the classification restricted Boltzmann machine,? Journal of Machine Learning Research (JMLR), vol. 13, pp. 643?669,
2012.
[16] Y. Bengio, L. Yao, G. Alain, and P. Vincent, ?Generalized denoising auto-encoders as generative models,?
in Advances in Neural Information Processing Systems (NIPS), 2013, pp. 899?907.
[17] Y. Bengio, E. Thibodeau-Laufer, and J. Yosinski, ?Deep generative stochastic networks trainable by backprop,? CoRR, vol. abs/1306.1091, 2013.
[18] L. Wan and M. Zeiler, ?Regularization of neural networks using dropconnect,? in International Conference on Machine Learning (ICML), 2013, pp. 109?111.
[19] G. Mesnil, Y. Dauphin, X. Glorot, S. Rifai, Y. Bengio, I. J. Goodfellow, E. Lavoie, X. Muller, G. Desjardins, D. Warde-Farley, P. Vincent, A. Courville, and J. Bergstra, ?Unsupervised and transfer learning
challenge: a deep learning approach,? in Unsupervised and Transfer Learning challenge and workshop
(JMLR W& CP), 2012, pp. 97?110.
[20] K. Abhishek and D. Hal, ?Learning task grouping and overlap in multi-task learning,? in International
Conference on Machine Learning (ICML), 2012.
[21] S. Ozair, L. Yao, and Y. Bengio, ?Multimodal transitions for generative stochastic networks.? CoRR, vol.
abs/1312.5578, 2013.
[22] P. Smolensky, Information processing in dynamical systems: Foundations of harmony theory.
Press, 1986, vol. 1, no. 1, pp. 194?281.

MIT

[23] X. Glorot, A. Bordes, and Y. Bengio, ?Deep sparse rectifier neural networks,? in International Conference
on Artificial Intelligence and Statisitics (AISTATS), 2011, pp. 315?323.
[24] G. E. Hinton, ?A practical guide to training restricted boltzmann machines,? in Neural Networks: Tricks
of the Trade (2nd ed.), ser. Lecture Notes in Computer Science. Springer, 2012, pp. 599?619.
[25] H. Larochelle, D. Erhan, A. Courville, J. Bergstra, and Y. Bengio, ?Online companion for
the paper an empirical evaluation of deep architectures on problems with many factors of
variation,? 2014. [Online]. Available: http://www.iro.umontreal.ca/?lisa/twiki/bin/view.cgi/Public/
DeepVsShallowComparisonICML2007
[26] R. Salakhutdinov and G. E. Hinton, ?Deep boltzmann machines,? in International Conference on Artificial
Intelligence and Statistics (AISTATS), 2009, pp. 448?455.
[27] I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and Y. Bengio, ?Maxout networks,? in International Conference on Machine Learning (ICML), 2013, pp. 1319?1327.
[28] I. J. Goodfellow, A. C. Courville, and Y. Bengio, ?Joint training deep boltzmann machines for classification,? CoRR, vol. abs/1301.3568, 2013.
[29] D. Yu and L. Deng, ?Deep convex net: A scalable architecture for speech pattern classification.? in Interspeech, 2011, pp. 2285?2288.
[30] S. Rifai, Y. Dauphin, P. Vincent, Y. Bengio, and X. Muller, ?The manifold tangent classifier,? in Advances
in Neural Information Processing Systems (NIPS), 2012, pp. 2294?2302.
[31] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins, J. Turian, D. Warde-Farley,
and Y. Bengio, ?Theano: a CPU and GPU math expression compiler,? in Python for Scientific Computing
Conference (SciPy), 2010.
[32] M. Zeiler and R. Fergus, ?Stochastic pooling for regularization of deep convolutional neural networks,?
CoRR, vol. abs/1301.3557, 2013.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 5166-training-and-analysing-deep-recurrent-neural-networks.pdf

Training and Analyzing Deep Recurrent Neural
Networks
Michiel Hermans, Benjamin Schrauwen
Ghent University, ELIS departement
Sint Pietersnieuwstraat 41,
9000 Ghent, Belgium
michiel.hermans@ugent.be

Abstract
Time series often have a temporal hierarchy, with information that is spread out
over multiple time scales. Common recurrent neural networks, however, do not
explicitly accommodate such a hierarchy, and most research on them has been
focusing on training algorithms rather than on their basic architecture. In this paper we study the effect of a hierarchy of recurrent neural networks on processing
time series. Here, each layer is a recurrent network which receives the hidden
state of the previous layer as input. This architecture allows us to perform hierarchical processing on difficult temporal tasks, and more naturally capture the
structure of time series. We show that they reach state-of-the-art performance for
recurrent networks in character-level language modeling when trained with simple stochastic gradient descent. We also offer an analysis of the different emergent
time scales.

1

Introduction

The last decade, machine learning has seen the rise of neural networks composed of multiple layers,
which are often termed deep neural networks (DNN). In a multitude of forms, DNNs have shown to
be powerful models for tasks such as speech recognition [17] and handwritten digit recognition [4].
Their success is commonly attributed to the hierarchy that is introduced due to the several layers.
Each layer processes some part of the task we wish to solve, and passes it on to the next. In this
sense, the DNN can be seen as a processing pipeline, in which each layer solves a part of the task
before passing it on to the next, until finally the last layer provides the output.
One type of network that debatably falls into the category of deep networks is the recurrent neural
network (RNN). When folded out in time, it can be considered as a DNN with indefinitely many
layers. The comparison to common deep networks falls short, however, when we consider the functionality of the network architecture. For RNNs, the primary function of the layers is to introduce
memory, not hierarchical processing. New information is added in every ?layer? (every network iteration), and the network can pass this information on for an indefinite number of network updates,
essentially providing the RNN with unlimited memory depth. Whereas in DNNs input is only presented at the bottom layer, and output is only produced at the highest layer, RNNs generally receive
input and produce output at each time step. As such, the network updates do not provide hierarchical processing of the information per se, only in the respect that older data (provided several time
steps ago) passes through the recursion more often. There is no compelling reason why older data
would require more processing steps (network iterations) than newly received data. More likely, the
recurrent weights in an RNN learn during the training phase to select what information they need to
pass onwards, and what they need to discard. Indeed, this quality forms the core motivation of the
so-called Long Short-term memory (LSTM) architecture [11], a special form of RNN.
1

DRNN-AO

DRNN-1O

3-layer RNN

1-layer RNN

time

time

Figure 1: Schematic illustration of a DRNN. Arrows represent connection matrices, and white,
black and grey circles represent input frames, hidden states, and output frames respectively. Left:
Standard RNN, folded out in time. Middle: DRNN of 3 layers folded out in time. Each layer can
be interpreted as an RNN that receives the time series of the previous layer as input. Right: The two
alternative architectures that we study in this paper, where the looped arrows represent the recurrent
weights. Either only the top layer connects to the output (DRNN-1O), or all layers do (DRNN-AO).

One potential weakness of a common RNN is that we may need complex, hierarchical processing of
the current network input, but this information only passes through one layer of processing before
going to the output. Secondly, we may need to process the time series at several time scales. If
we consider for example speech, at the lowest level it is built up of phonemes, which exist on a
very short time-scale. Next, on increasingly longer time scales, there are syllables, words, phrases,
clauses, sentences, and at the highest level for instance a full conversation. Common RNNs do not
explicitly support multiple time scales, and any temporal hierarchy that is present in the input signal
needs to be embedded implicitly in the network dynamics.
In past research, some hierarchical architectures employing RNNs have been proposed [3, 5, 6].
Especially [5] is interesting in the sense that they construct a hierarchy of RNNs, which all operate on different time-scales (using subsampling). The authors limit themselves to artificial tasks,
however. The architecture we study in this paper has been used in [8]. Here, the authors employ
stacked bi-directional LSTM networks, and train it on the TIMIT phoneme dataset [7] in which they
obtain state-of-the-art performance. Their paper is strongly focused on reaching good performance,
however, and little analysis on the actual contribution of the network architecture is provided.
The architecture we study in this paper is essentially a common DNN (a multilayer perceptron) with
temporal feedback loops in each layer, which we call a deep recurrent neural network (DRNN).
Each network update, new information travels up the hierarchy, and temporal context is added in
each layer (see Figure 1). This basically combines the concept of DNNs with RNNs. Each layer
in the hierarchy is a recurrent neural network, and each subsequent layer receives the hidden state
of the previous layer as input time series. As we will show, stacking RNNs automatically creates
different time scales at different levels, and therefore a temporal hierarchy.
In this paper we will study character-based language modelling and provide a more in-depth analysis
of how the network architecture relates to the nature of the task. We suspect that DRNNs are wellsuited to capture temporal hierarchies, and character-based language modeling is an excellent realworld task to validate this claim, as the distribution of characters is highly nonlinear and covers
both short- and long-term dependencies. As we will show, DRNNs embed these different timescales
directly in their structure, and they are able to model long-term dependencies. Using only stochastic
gradient descent (SGD) we are able to get state-of-the-art performance for recurrent networks on
a Wikipedia-based text corpus, which was previously only obtained using the far more advanced
Hessian-free training algorithm [19].

2
2.1

Deep RNNs
Hidden state evolution

We define a DRNN with L layers, and N neurons per layer. Suppose we have an input time series
s(t) of dimensionality Nin , and a target time series y? (t). In order to simplify notation we will not
explicitly write out bias terms, but augment the corresponding variables with an element equal to
2

one. We use the notation x
? = [x; 1].
We denote the hidden state of the i-th layer with ai (t). Its update equation is given by:
ai (t) = tanh (Wi ai (t ? 1) + Zi ?
ai?1 (t)) if i > 1
ai (t) = tanh (Wi ai (t ? 1) + Zi?
s(t)) if i = 1.
Here, Wi and Zi are the recurrent connections and the connections from the lower layer or input
time series, respectively. A schematic drawing of the DRNN is presented in Figure 1.
Note that the network structure inherently offers different time scales. The bottom layer has fading
memory of the input signal. The next layer has fading memory of the hidden state of the bottom
layer, and consequently a fading memory of the input which reaches further in the past, and so on
for each additional layer.
2.2

Generating output

The task we consider in this paper is a classification task, and we use a softmax function to generate
output. The DRNN generates an output which we denote by y(t). We will consider two scenarios:
that where only the highest layer in the hierarchy couples to the output (DRNN-1O), and that where
all layers do (DRNN-AO). In the two respective cases, y(t) is given by:
y(t) = softmax (U?
aL (t)) ,

(1)

where U is the matrix with the output weights, and
L
X

y(t) = softmax

!
Ui ?
ai (t) ,

(2)

i=1

such that Ui corresponds to the output weights of the i-th layer. The two resulting architectures are
depicted in the right part of Figure 1.
The reason that we use output connections at each layer is twofold. First, like any deep architecture,
DRNNs suffer from a pathological curvature in the cost function. If we use backpropagation through
time, the error will propagate from the top layer down the hierarchy, but it will have diminished in
magnitude once it reaches the lower layers, such that they are not trained effectively. Adding output
connections at each layer amends this problem to some degree as the training error reaches all layers
directly.
Secondly, having output connections at each layer provides us with a crude measure of its role in
solving the task. We can for instance measure the decay of performance by leaving out an individual layer?s contribution, or study which layer contributes most to predicting characters in specific
instances.
2.3

Training setup

In all experiments we used stochastic gradient descent. To avoid extremely large gradients near
bifurcations, we applied the often-used trick of normalizing the gradient before using it for weight
updates. This simple heuristic seems to be effective to prevent gradient explosions and sudden jumps
of the parameters, while not diminishing the end performance. We write the number of batches we
train on as T . The learning rate is set at an initial value ?0 , and drops linearly with each subsequent
weight update. Suppose ?(j) is the set of all trainable parameters after j updates, and ?? (j) is the
gradient of a cost function w.r.t. this parameter set, as computed on a randomly sampled part of the
training set. Parameter updates are given by:


j
?? (j)
?(j + 1) = ?(j) ? ?0 1 ?
.
(3)
T ||?? (j)||
In the case where we use output connections at the top layer only, we use an incremental layer-wise
method to train the network, which was necessary to reach good performance. We add layers one
by one and at all times an output layer only exists at the current top layer. When adding a layer, the
previous output weights are discarded and new output weights are initialised connecting from the
new top layer. In this way each layer has at least some time during training in which it is directly
3

coupled to the output, and as such can be trained effectively. Over the course of each of these training
stages we used the same training strategy as described before: training the full network with BPTT
and linearly reducing the learning rate to zero before a new layer is added. Notice the difference to
common layer-wise training schemes where only a single layer is trained at a time. We always train
the full network after each layer is added.

3

Text prediction

In this paper we consider next character prediction on a Wikipedia text-corpus [19] which was
made publicly available1 . The total set is about 1.4 billion characters long, of which the final 10
million is used for testing. Each character is represented by one-out-of-N coding. We used 95 of
the most common characters2 (including small letters, capitals, numbers and punctuation), and one
?unknown? character, used to map any character not part of the 95 common ones, e.g. Cyrillic and
Chinese characters. We need time in the order of 10 days to train a single network, largely due to
the difficulty of exploiting massively parallel computing for SGD. Therefore we only tested three
network instantiations3 . Each experiment was run on a single GPU (NVIDIA GeForce GTX 680,
4GB RAM).
The task is as follows: given a sequence of text, predict the probability distribution of the next
character. The used performance metric is the average number of bits-per-character (BPC), given
by BPC = ? hlog2 pc i, where pc is the probability as predicted by the network of the correct next
character.
3.1

Network setups

The challenge in character-level language modelling lies in the great diversity and sheer number of
words that are used. In the case of Wikipedia this difficulty is exacerbated due to the large number
of names of persons and places, scientific jargon, etc. In order to capture this diversity we need large
models with many trainable parameters.
All our networks have a number of neurons selected such that in total they each had approximately
4.9 million trainable parameters, which allowed us to make a comparison to other published work
[19]. We considered three networks: a common RNN (2119 units), a 5-layer DRNN-1O (727 units
per layer), and a 5-layer DRNN-AO (706 units per layer)4 . Initial learning rates ?0 were chosen at
0.5, except for the the top layer of the DRNN-1O, where we picked ?0 = 0.25 (as we observed that
the nodes started to saturate if we used a too high learning rate).
The RNN and the DRNN-AO were trained over T = 5 ? 105 parameter updates. The network with
output connections only at the top layer had a different number of parameter updates per training
stage, T = {0.5, 1, 1.5, 2, 2.5} ? 105 , for the 5 layers respectively. As such, for each additional
layer the network is trained for more iterations. All gradients are computed using backpropagation
through time (BPTT) on 75 randomly sampled sequences in parallel, drawn from the training set.
All sequences were 250 characters long, and the first 50 characters were disregarded during the
backwards pass, as they may have insufficient temporal context. In the end the DRNN-AO sees the
full training set about 7 times in total, and the DRNN-1O about 10 times.
The matrices Wi and Zi>1 were initialised with elements drawn from N (0, N ?1/2 ). The input
weights Z1 were drawn from N (0, 1). We chose to have the same number of neurons for every
layer, mostly to reduce the number of parameters that need to be optimised. Output weights were
always initialised on zero.
1

http://www.cs.toronto.edu/?ilya/mrnns.tar.gz
In [19] only 86 character are used, but most of the additional characters in our set are exceedingly rare,
such that cross-entropy is not affected meaningfully by this difference.
3
In our experience the networks are so large that there is very little difference in performance for different
initialisations
4
The decision for 5 layers is based on a previous set of experiments (results not shown).
2

4

BPC test
1.610
1.557
1.541
1.55
1.51
1.276
0.6 ? 1.3

2
Increase in BPC test

Model
RNN
DRNN-AO
DRNN-1O
MRNN
PAQ
Hutter Prize (current record) [12]
Human level (estimated) [18]

1.5
1
0.5
0

Table 1: Results on the Wikipedia character prediction task. The first three numbers are our
measurements, the next two the results on the
same dataset found in [19]. The bottom two
numbers were not measured on the same text
corpus.

3.2

1

2

3
4
Removed layer

5

Figure 2: Increase in BPC on the test set from
removing the output contribution of a single
layer of the DRNN-AO.

Results

Performance and text generation
The resulting BPCs for our models and comparative results in literature are shown in Table 1. The
common RNN performs worst, and the DRNN-1O the best, with the DRNN-AO slightly worse. Both
DRNNs perform well and are roughly similar to the state-of-the-art for recurrent networks with the
same number of trainable parameters5 , which was established with a multiplicative RNN (MRNN),
trained with Hessian-free optimization in the course of 5 days on a cluster of 8 GPUs6 . The same
authors also used the PAQ compression algorithm [14] as a comparison, which we included in the
list. In the table we also included two results which were not measured on the same dataset (or even
using the same criteria), but which give an estimation of the true number of BPC for natural text.
To check how each layer influences performance in the case of the DRNN-AO, we performed tests
in which the output of a single layer is set to zero. This can serve as a sanity check to ensure
that the model is efficiently trained. If for instance removing the top layer output contribution
does not significantly harm performance, this essentially means that it is redundant (as it does no
preprocessing for higher layers). Furthermore we can use this test to get an overall indication of
which role a particular layer has in producing output. Note that these experiments only have a limited
interpretability, as the individual layer contributions are likely not independent. Perhaps some layers
provide strong negative output bias which compensates for strong positive bias of another, or strong
synergies might exists between them.
First we measure the increase in test BPC by removing a single layer?s output contribution, which
can then be used as an indicator for the importance of this layer for directly generating output. In
Figure 2 we show the result. The contribution of the top layer is the most important, and that of the
bottom layer second important. The intermediate layers contribute less to the direct output and seem
to be more important in preprocessing the data for the top layer.
As in [19], we also used the networks in a generative mode, where we use the output probabilities
of the DRNN-AO to recursively sample a new input character in order to complete a given sentence.
We too used the phrase ?The meaning of life is ?. We performed three tests: first we generated
text with an intact network, next we see how the text quality deteriorates when we leave out the
contributions of the bottom and top layer respectively7 (by setting it equal to zero before adding up
5
This similarity might reflect limitations caused by the network size. We also performed a long-term experiment with a DRNN-AO with 9.6 million trainable parameters, which resulted in a test BPC of 1.472 after
1,000,000 weight updates (training for over a month). More parameters offer more raw storage power, and
hence provide a straightforward manner in which to increase performance.
6
This would suggest a computational cost of roughly 4 times ours, but an honest comparison is hard to make
as the authors did not specify explicitly how much data their training algorithm went through in total. Likely
the cost ratio is smaller than 4, as we use a more modern GPU.
7
Leaving out the contributions of intermediate layers only has a minimal effect on the subjective quality of
the produced text.

5

The meaning of life is the ?decorator of
Rose?. The Ju along with its perspective character survive, which coincides
with his eromine, water and colorful
art called ?Charles VIII?.??In ?Inferno?
(also 220: ?The second Note Game
Magazine?, a comic at the Old Boys
at the Earl of Jerusalem for two years)
focused on expanded domestic differences from 60 mm Oregon launching,
and are permitted to exchange guidance.

The meaning of life is man sistasteredsteris bus and nuster eril?n ton nis our
ousNmachiselle here hereds?d toppstes impedred wisv.?-hor ens htls betwez rese, and Intantored wren in
thoug and elit toren on the marcel,
gos infand foldedsamps que help sasecre hon Roser and ens in respoted
we frequen enctuivat herde pitched
pitchugismissedre and loseflowered

The meaning of life is impossible
to
unprecede
?Pok.{*
PRER)!?KGOREMFHEAZ CTX=R M
?S=6 5?&+??=7xp*= 5FJ4?13/TxI
JX=?b28O=&4+E9F=&Z26 ?R&N==
Z8&A=58=84&T=RESTZINA=L&95Y
2O59&FP85=&&#=&H=S=Z IO =T
@?CBOM=6&9Y1= 9 5

Table 2: Three examples of text, generated by the DRNN-AO. The left one is generated by the intact
network, the middle one by leaving out the contribution of the first layer, and the right one by leaving
out the contribution of the top layer.
0

RNN
layer 1
layer 2
layer 3
layer 4
layer 5

?1

10

RNN
DRNN?1O
DRNN?AO
layer 1
layer 2
layer 3
layer 4
layer 5

1

10

average increase in BPC

normalised average distance

10

?2

10

0

10

?1

10

?2

10

?3

10

?3

20

40
60
80
nr. of presented characters

10

100

20

40
60
80
nr. of presented characters

100

Figure 3: Left panel: normalised average distance between hidden states of a perturbed and unperturbed network as a function of presented characters. The perturbation is a single typo at the first
character. The coloured full lines are for the individual layers of the DRNN-1O, and the coloured
dashed lines are those of the layers of the DRNN-AO. Distances are normalised on the distance of
the occurrence of the typo. Right panel: Average increase in BPC between a perturbed and unperturbed network as a function of presented characters. The perturbation is by replacing the initial
context (see text), and the result is shown for the text having switched back to the correct context.
Coloured lines correspond to the individual contributions of the layers in the DRNN-AO.

layer contributions and applying the softmax function). Resulting text samples are shown in Table
2. The text sample of the intact network shows short-term correct grammar, phrases, punctuation
and mostly existing words. The text sample with the bottom layer output contribution disabled very
rapidly becomes ?unstable?, and starts to produce long strings of rare characters, indicating that the
contribution of the bottom layer is essential in modeling some of the most basic statistics of the
Wikipedia text corpus. We verified this further by using such a random string of characters as initialization of the intact network, and observed that it consistently fell back to producing ?normal?
text. The text sample with the top layer disabled is interesting in the sense that it produces roughly
word-length strings of common characters (letters and spaces), of which substrings resemble common syllables. This suggests that the top layer output contribution captures text statistics longer than
word-length sequences.
Time scales
In order to gauge at what time scale each individual layer operates, we have performed several
experiments on the models. First of all we considered an experiment in which we run the DRNN
on two identical text sequences from the test set, but after 100 characters we introduce a typo in
one of them (by replacing it by a character randomly sampled from the full set). We record the
hidden states after the typo as a function of time for both the perturbed and unperturbed network
6

output

15
10
5
0
?5

prob.

0.4
0.2
0

50

100

150

200
250
300
nr. presented characters

350

400

450

500

Figure 4: Network output example for a particularly long phrase between parentheses (296 characters), sampled from the test set. The vertical dashed lines indicate the opening and closing parentheses in the input text sequence. Top panel: output traces for the closing parenthesis character for
each layer in the DRNN-AO. Coloring is identical to that of Figure 3. Bottom panel: total predicted
output probability of the closing parenthesis sign of the DRNN-AO.

and measure the Euclidean distance between them as a function of time, to see how long the effect
of the typo remains present in each layer.
Next we measured what the length of the context is the DRNNs effectively employ. In order to do so
we measured the average difference in BPC between normal text and a perturbed copy, in which we
replaced the first 100 characters by text randomly sampled from elsewhere in the test set. This will
give an indication of how long the lack of correct context lingers after the text sequence switched.
All measurements were averaged over 50,000 instances. Results are shown in Figure 3. The left
panel shows how fast each individual layer in the DRNNs forgets the typo-perturbation. It appears
that the layer-wise time scales behave quite differently in the case of the DRNN-1O and the DRNNAO. The DRNN-AO has very short time-scales in the three bottom layers and longer memory only
appears for the two top ones, whereas in the DRNN-1O, the bottom two layers have relatively short
time scales, but the top three layers have virtually the same, very long time scale. This is almost
certainly caused by the way in which we trained the DRNN-1O, such that intermediate layers already
assumed long memory when they were at the top of the hierarchy. The effect of the perturbation of
the normal RNN is also shown. Even though it decays faster at the start, the effect of the perturbation
remains present in the network for a long period as well.
The right panel of Figure 3 depicts the effect on switching the context on the actual prediction
accuracy, which gives some insight in what the actual length of the context used by the networks
is. Both DRNNs seem to recover more slowly from the context switch than the RNN, indicating
that they employ a longer context for prediction. The time scales of the individual layers of the
DRNN-AO are also depicted (by using the perturbed hidden states of an individual layer and the
unperturbed states for the other layers for generating output), which largely confirms the result from
the typo-perturbation test.
The results shown here verify that a temporal hierarchy develops when training a DRNN. We have
also performed a test to see what the time scales of an untrained DRNN are (by performing the typo
test), which showed that here the differences in time-scales for each layer were far smaller (results
not shown). The big differences we see in the trained DRNNs are hence a learned property.
Long-term interactions: parentheses
In order to get a clearer picture on some of the long-term dependencies the DRNNs have learned we
look at their capability of closing parentheses, even when the phrase between parentheses is long.
To see how well the networks remember the opening of a parenthesis, we observe the DRNN-AO
output for the closing parenthesis-character8 . In Figure 4 we show an example for an especially long
phrase between parentheses. We both show the output probability and the individual layers? output
8

Results on the DRNN-1O are qualitatively similar.

7

contribution for the closing parenthesis (before they are added up and sent to the softmax function).
The output of the top layer for the closing parenthesis is increased strongly for the whole duration
of the phrase, and is reduced immediately after it is closed.
The total output probability shows a similar pattern, showing momentary high probabilities for the
closing parenthesis only during the parenthesized phrase, and extremely low probabilities elsewhere.
These results are quite consistent over the test set, with some notable exceptions. When several sentences appear between parentheses (which occasionally happens in the text corpus), the network
reduces the closing bracket probability (i.e., essentially ?forgets? it) as soon as a full stop appears9 .
Similarly, if a sentence starts with an opening bracket it will not increase closing parenthesis probability at all, essentially ignoring it. Furthermore, the model seems not able to cope with nested
parentheses (perhaps because they are quite rare). The fact that the DRNN is able to remember the
opening parenthesis for sequences longer than it has been trained on indicates that it has learned
to model parentheses as a pseudo-stable attractor-like state, rather than memorizing parenthesized
phrases of different lengths.
In order to see how well the networks can close parentheses when they operate in the generative
mode, we performed a test in which we initialize it with a 100-character phrase drawn from the test
set ending in an opening bracket and observe in how many cases the network generates a closing
bracket. A test is deemed unsuccessful if the closing parenthesis doesn?t appear in 500 characters,
or if it produces a second opening parenthesis. We averaged the results over 2000 initializations.
The DRNN-AO performs best in this test; only failing in 12% of the cases. The DRNN-1O fails in
16%, and the RNN in 28%.
The results presented in this section hint at the fact that DRNNs might find it easier to learn longterm relations between input characters than common RNNs. This could lead to test DRNNs on the
tasks introduced in [11]. These tasks are challenging in the sense that they require to retain very
long memory of past input, while being driven by so-called distractor input. It has been shown that
LSTMs and later common RNNs trained with Hessian-free methods [16] and Echo State Networks
[13] are able to model such long-term dependencies. These tasks, however, purely focus on memory
depth, and very little additional processing is required, let alone hierarchical processing. Therefore
we do not suspect that DRNNs pose a strong advantage over common RNNs for these tasks in
particular.

4

Conclusions and Future Work

We have shown that using a deep recurrent neural network (DRNN) is beneficial for characterlevel language modeling, reaching state-of-the-art performance for recurrent neural networks on a
Wikipedia text corpus, confirming the observation that deep recurrent architectures can boost performance [8]. We also present experimental evidence for the appearance of a hierarchy of time-scales
present in the layers of the DRNNs. Finally we have demonstrated that in certain cases the DRNNs
can have extensive memory of several hundred characters long.
The training method we obtained on the DRNN-1O indicates that supervised pre-training for deep
architectures is helpful, which on its own can provide an interesting line of future research. Another
one is to extend common pre-training schemes, such as the deep belief network approach [9] and
deep auto-encoders [10, 20] for DRNNs. The results in this paper can potentially contribute to the
ongoing debate on training algorithms, especially whether SGD or second order methods are more
suited for large-scale machine learning problems [2]. Therefore, applying second order techniques
such as Hessian-free training [15] on DRNNs seems an attractive line of future research in order to
obtain a solid comparison.
Acknowledgments
This work is partially supported by the interuniversity attraction pole (IAP) Photonics@be of the
Belgian Science Policy Office and the ERC NaResCo Starting grant. We would like to thank Sander
Dieleman and Philemon Brakel for helping with implementations. All experiments were performed
using Theano [1].
9

It is consistently resilient against points appearing in abbreviations such as ?e.g.,? and ?dr.? though.

8

References
[1] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins, J. Turian, D. Warde-Farley,
and Y. Bengio. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for
Scientific Computing Conference (SciPy), June 2010.
[2] L. Bottou and O. Bousquet. The tradeoffs of large-scale learning. Optimization for Machine Learning,
page 351, 2011.
[3] W.-Y. Chen, Y.-F. Liao, and S.-H. Chen. Speech recognition with hierarchical recurrent neural networks.
Pattern Recognition, 28(6):795 ? 805, 1995.
[4] D. Ciresan, U. Meier, L. Gambardella, and J. Schmidhuber. Deep, big, simple neural nets for handwritten
digit recognition. Neural computation, 22(12):3207?3220, 2010.
[5] S. El Hihi and Y. Bengio. Hierarchical recurrent neural networks for long-term dependencies. Advances
in Neural Information Processing Systems, 8:493?499, 1996.
[6] S. Fern?andez, A. Graves, and J. Schmidhuber. Sequence labelling in structured domains with hierarchical recurrent neural networks. In Proceedings of the 20th International Joint Conference on Artificial
Intelligence, IJCAI 2007, Hyderabad, India, January 2007.
[7] J. Garofolo, N. I. of Standards, T. (US, L. D. Consortium, I. Science, T. Office, U. States, and D. A. R. P.
Agency. TIMIT Acoustic-phonetic Continuous Speech Corpus. Linguistic Data Consortium, 1993.
[8] A. Graves, A. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural networks. In To
appear in ICASSP 2013, 2013.
[9] G. Hinton, S. Osindero, and Y. Teh. A fast learning algorithm for deep belief nets. Neural computation,
18(7):1527?1554, 2006.
[10] G. E. Hinton. Reducing the dimensionality of data with neural networks. Science, 313:504?507, 2006.
[11] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735?1780, 1997.
[12] M. Hutter. The human knowledge compression prize, 2006.
[13] H. Jaeger. Long short-term memory in echo state networks: Details of a simulation study. Technical
report, Jacobs University, 2012.
[14] M. Mahoney. Adaptive weighing of context models for lossless data compression. Florida Tech., Melbourne, USA, Tech. Rep, 2005.
[15] J. Martens. Deep learning via hessian-free optimization. In Proceedings of the 27th International Conference on Machine Learning, pages 735?742, 2010.
[16] J. Martens and I. Sutskever. Learning recurrent neural networks with hessian-free optimization. In Proceedings of the 28th International Conference on Machine Learning, volume 46, page 68. Omnipress
Madison, WI, 2011.
[17] A. Mohamed, G. Dahl, and G. Hinton. Acoustic modeling using deep belief networks. Audio, Speech,
and Language Processing, IEEE Transactions on, 20(1):14?22, 2012.
[18] C. E. Shannon. Prediction and entropy of printed english. Bell system technical journal, 30(1):50?64,
1951.
[19] I. Sutskever, J. Martens, and G. Hinton. Generating text with recurrent neural networks. In Proceedings
of the 28th International Conference on Machine Learning, pages 1017?1024, 2011.
[20] P. Vincent, H. Larochelle, Y. Bengio, and P. Manzagol. Extracting and composing robust features with
denoising autoencoders. In Proceedings of the 25th International Conference on Machine learning, pages
1096?1103, 2008.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 5958-max-margin-deep-generative-models.pdf

Max-Margin Deep Generative Models

Chongxuan Li? , Jun Zhu? , Tianlin Shi? , Bo Zhang?
?
Dept. of Comp. Sci. & Tech., State Key Lab of Intell. Tech. & Sys., TNList Lab,
Center for Bio-Inspired Computing Research, Tsinghua University, Beijing, 100084, China
?
Dept. of Comp. Sci., Stanford University, Stanford, CA 94305, USA
{licx14@mails., dcszj@, dcszb@}tsinghua.edu.cn; stl501@gmail.com

Abstract
Deep generative models (DGMs) are effective on learning multilayered representations of complex data and performing inference of input data by exploring the
generative ability. However, little work has been done on examining or empowering the discriminative ability of DGMs on making accurate predictions. This paper presents max-margin deep generative models (mmDGMs), which explore the
strongly discriminative principle of max-margin learning to improve the discriminative power of DGMs, while retaining the generative capability. We develop an
efficient doubly stochastic subgradient algorithm for the piecewise linear objective. Empirical results on MNIST and SVHN datasets demonstrate that (1) maxmargin learning can significantly improve the prediction performance of DGMs
and meanwhile retain the generative ability; and (2) mmDGMs are competitive to
the state-of-the-art fully discriminative networks by employing deep convolutional
neural networks (CNNs) as both recognition and generative models.

1

Introduction

Max-margin learning has been effective on learning discriminative models, with many examples
such as univariate-output support vector machines (SVMs) [5] and multivariate-output max-margin
Markov networks (or structured SVMs) [30, 1, 31]. However, the ever-increasing size of complex
data makes it hard to construct such a fully discriminative model, which has only single layer of
adjustable weights, due to the facts that: (1) the manually constructed features may not well capture
the underlying high-order statistics; and (2) a fully discriminative approach cannot reconstruct the
input data when noise or missing values are present.
To address the first challenge, previous work has considered incorporating latent variables into
a max-margin model, including partially observed maximum entropy discrimination Markov networks [37], structured latent SVMs [32] and max-margin min-entropy models [20]. All this work
has primarily focused on a shallow structure of latent variables. To improve the flexibility, learning SVMs with a deep latent structure has been presented in [29]. However, these methods do not
address the second challenge, which requires a generative model to describe the inputs. The recent work on learning max-margin generative models includes max-margin Harmoniums [4], maxmargin topic models [34, 35], and nonparametric Bayesian latent SVMs [36] which can infer the
dimension of latent features from data. However, these methods only consider the shallow structure
of latent variables, which may not be flexible enough to describe complex data.
Much work has been done on learning generative models with a deep structure of nonlinear hidden
variables, including deep belief networks [25, 16, 23], autoregressive models [13, 9], and stochastic
variations of neural networks [3]. For such models, inference is a challenging problem, but fortunately there exists much recent progress on stochastic variational inference algorithms [12, 24].
However, the primary focus of deep generative models (DGMs) has been on unsupervised learning,
1

with the goals of learning latent representations and generating input samples. Though the latent
representations can be used with a downstream classifier to make predictions, it is often beneficial
to learn a joint model that considers both input and response variables. One recent attempt is the
conditional generative models [11], which treat labels as conditions of a DGM to describe input
data. This conditional DGM is learned in a semi-supervised setting, which is not exclusive to ours.
In this paper, we revisit the max-margin principle and present a max-margin deep generative model
(mmDGM), which learns multi-layer representations that are good for both classification and input inference. Our mmDGM conjoins the flexibility of DGMs on describing input data and the
strong discriminative ability of max-margin learning on making accurate predictions. We formulate
mmDGM as solving a variational inference problem of a DGM regularized by a set of max-margin
posterior constraints, which bias the model to learn representations that are good for prediction. We
define the max-margin posterior constraints as a linear functional of the target variational distribution of the latent presentations. Then, we develop a doubly stochastic subgradient descent algorithm,
which generalizes the Pagesos algorithm [28] to consider nontrivial latent variables. For the variational distribution, we build a recognition model to capture the nonlinearity, similar as in [12, 24].
We consider two types of networks used as our recognition and generative models: multiple layer
perceptrons (MLPs) as in [12, 24] and convolutional neural networks (CNNs) [14]. Though CNNs
have shown promising results in various domains, especially for image classification, little work has
been done to take advantage of CNN to generate images. The recent work [6] presents a type of
CNN to map manual features including class labels to RBG chair images by applying unpooling,
convolution and rectification sequentially; but it is a deterministic mapping and there is no random
generation. Generative Adversarial Nets [7] employs a single such layer together with MLPs in a
minimax two-player game framework with primary goal of generating images. We propose to stack
this structure to form a highly non-trivial deep generative network to generate images from latent
variables learned automatically by a recognition model using standard CNN. We present the detailed
network structures in experiments part. Empirical results on MNIST [14] and SVHN [22] datasets
demonstrate that mmDGM can significantly improve the prediction performance, which is competitive to the state-of-the-art methods [33, 17, 8, 15], while retaining the capability of generating input
samples and completing their missing values.

2

Basics of Deep Generative Models

We start from a general setting, where we have N i.i.d. data X = {xn }N
n=1 . A deep generative
model (DGM) assumes that each xn ? RD is generated from a vector of latent variables zn ? RK ,
which itself follows some distribution. The joint probability of a DGM is as follows:
p(X, Z|?, ?) =

N
Y

p(zn |?)p(xn |zn , ?),

(1)

n=1

where p(zn |?) is the prior of the latent variables and p(xn |zn , ?) is the likelihood model for generating observations. For notation simplicity, we define ? = (?, ?). Depending on the structure
of z, various DGMs have been developed, such as the deep belief networks [25, 16], deep sigmoid
networks [21], deep latent Gaussian models [24], and deep autoregressive models [9]. In this paper,
we focus on the directed DGMs, which can be easily sampled from via an ancestral sampler.
However, in most cases learning DGMs is challenging due to the intractability of posterior inference.
The state-of-the-art methods resort to stochastic variational methods under the maximum likelihood
? = argmax log p(X|?). Specifically, let q(Z) be the variational
estimation (MLE) framework, ?
?
distribution that approximates the true posterior p(Z|X, ?). A variational upper bound of the per
sample negative log-likelihood (NLL) ? log p(xn |?, ?) is:
L(?, q(zn ); xn ) , KL(q(zn )||p(zn |?)) ? Eq(zn ) [log p(xn |zn , ?)],

(2)

where KL(q||p) is the Kullback-Leibler (KL) divergence between distributions q and p. Then,
P
L(?, q(Z); X) , n L(?, q(zn ); xn ) upper bounds the full negative log-likelihood ? log p(X|?).
It is important to notice that if we do not make restricting assumption on the variational distribution
q, the lower bound is tight by simply setting q(Z) = p(Z|X, ?). That is, the MLE is equivalent to
solving the variational problem: min?,q(Z) L(?, q(Z); X). However, since the true posterior is intractable except a handful of special cases, we must resort to approximation methods. One common
2

assumption is that the variational distribution is of some parametric form, q? (Z), and then we optimize the variational bound w.r.t the variational parameters ?. For DGMs, another challenge arises
that the variational bound is often intractable to compute analytically. To address this challenge, the
early work further bounds the intractable parts with tractable ones by introducing more variational
parameters [26]. However, this technique increases the gap between the bound being optimized and
the log-likelihood, potentially resulting in poorer estimates. Much recent progress [12, 24, 21] has
been made on hybrid Monte Carlo and variational methods, which approximates the intractable expectations and their gradients over the parameters (?, ?) via some unbiased Monte Carlo estimates.
Furthermore, to handle large-scale datasets, stochastic optimization of the variational objective can
be used with a suitable learning rate annealing scheme. It is important to notice that variance reduction is a key part of these methods in order to have fast and stable convergence.
Most work on directed DGMs has been focusing on the generative capability on inferring the observations, such as filling in missing values [12, 24, 21], while little work has been done on investigating
the predictive power, except the semi-supervised DGMs [11] which builds a DGM conditioned on
the class labels and learns the parameters via MLE. Below, we present max-margin deep generative
models, which explore the discriminative max-margin principle to improve the predictive ability of
the latent representations, while retaining the generative capability.

3

Max-margin Deep Generative Models

We consider supervised learning, where the training data is a pair (x, y) with input features x ? RD
and the ground truth label y. Without loss of generality, we consider the multi-class classification,
where y ? C = {1, . . . , M }. A max-margin deep generative model (mmDGM) consists of two
components: (1) a deep generative model to describe input features; and (2) a max-margin classifier
to consider supervision. For the generative model, we can in theory adopt any DGM that defines a
joint distribution over (X, Z) as in Eq. (1). For the max-margin classifier, instead of fitting the input
features into a conventional SVM, we define the linear classifier on the latent representations, whose
learning will be regularized by the supervision signal as we shall see. Specifically, if the latent
representation z is given, we define the latent discriminant function F (y, z, ?; x) = ? > f (y, z),
where f (y, z) is an M K-dimensional vector that concatenates M subvectors, with the yth being z
and all others being zero, and ? is the corresponding weight vector.
We consider the case that ? is a random vector, following some prior distribution p0 (?). Then
our goal is to infer the posterior distribution p(?, Z|X, Y), which is typically approximated by a
variational distribution q(?, Z) for computational tractability. Notice that this posterior is different
from the one in the vanilla DGM. We expect that the supervision information will bias the learned
representations to be more powerful on predicting the labels at testing. To account for the
 uncertainty

of (?, Z), we take the expectation and define the discriminant function F (y; x) = Eq ? > f (y, z) ,
and the final prediction rule that maps inputs to outputs is:
y? = argmax F (y; x).

(3)

y?C

Note that different from the conditional DGM [11], which puts the class labels upstream, the above
classifier is a downstream model, in the sense that the supervision signal is determined by conditioning on the latent representations.
3.1

The Learning Problem

We want to jointly learn the parameters ? and infer the posterior distribution q(?, Z). Based on the
equivalent variational formulation of MLE, we define the joint learning problem as solving:
min

L(?, q(?, Z); X) + C

?,q(?,Z),?

?n

(4)

n=1


?n, y ? C, s.t. :

N
X

Eq [? > ?fn (y)] ? ?ln (y) ? ?n
?n ? 0,

where ?fn (y) = f (yn , zn ) ? f (y, zn ) is the difference of the feature vectors; ?ln (y) is the loss
function that measures the cost to predict y if the true label is yn ; and C is a nonnegative regularization parameter balancing the two components. In the objective, the variational bound is defined
3

as L(?, q(?, Z); X) = KL(q(?, Z)||p0 (?, Z|?)) ? Eq [log p(X|Z, ?)], and the margin constraints
are from the classifier (3). If we ignore the constraints (e.g., setting C at 0), the solution of q(?, Z)
will be exactly the Bayesian posterior, and the problem is equivalent to do MLE for ?.
By absorbing the slack variables, we can rewrite the problem in an unconstrained form:
min L(?, q(?, Z); X) + CR(q(?, Z; X)),

(5)

?,q(?,Z)

PN
where the hinge loss is: R(q(?, Z); X) = n=1 maxy?C (?ln (y) ? Eq [? > ?fn (y)]). Due to the
convexity of max function, it is easy to verify P
that the hinge loss is an upper bound of the training error of classifier (3), that is, R(q(?, Z); X) ? n ?ln (?
yn ). Furthermore, the hinge loss is a convex
functional over the variational distribution because of the linearity of the expectation operator. These
properties render the hinge loss as a good surrogate to optimize over. Previous work has explored
this idea to learn discriminative topic models [34], but with a restriction on the shallow structure of
hidden variables. Our work presents a significant extension to learn deep generative models, which
pose new challenges on the learning and inference.
3.2

The Doubly Stochastic Subgradient Algorithm

The variational formulation of problem (5) naturally suggests that we can develop a variational
algorithm to address the intractability of the true posterior. We now present a new algorithm to
solve problem (5). Our method is a doubly stochastic generalization of the Pegasos (i.e., Primal
Estimated sub-GrAdient SOlver for SVM) algorithm [28] for the classic SVMs with fully observed
input features, with the new extension of dealing with a highly nontrivial structure of latent variables.
First, we make the structured mean-field (SMF) assumption that q(?, Z) = q(?)q? (Z). Under the
assumption, we have the discriminant function as Eq [? > ?fn (y)] = Eq(?) [? > ]Eq? (z(n) ) [?fn (y)].
Moreover, we can solve for the optimal solution of q(?) in some analytical form. In fact,
by the calculus
of variations, we can
 show that given the other parts the solution is q(?) ?
 P
>
y
p0 (?) exp ?
n,y ?n Eq? [?fn (y)] , where ? are the Lagrange multipliers (See [34] for de2
tails). If the prior is normal,
P p0 (?) = N (0, ? I), we have the normal posterior: q(?) =
N (?, ? 2 I), where ? = ? 2 n,y ?ny Eq? [?fn (y)]. Therefore, even though we did not make a parametric form assumption of q(?), the above results show that the optimal posterior distribution of ?
is Gaussian. Since we only use the expectation in the optimization problem and in prediction, we
can directly solve for the mean parameter ? instead of q(?). Further, in this case we can verify that
2
KL(q(?)||p0 (?)) = ||?||
2? 2 and then the equivalent objective function in terms of ? can be written
as:
||?||2
min L(?, ?; X) +
+ CR(?, ?; X),
(6)
?,?,?
2? 2
PN
where R(?, ?; X) =
n=1 `(?, ?; xn ) is the total hinge loss, and the per-sample hinge-loss is
`(?, ?; xn ) = maxy?C (?ln (y) ? ?> Eq? [?fn (y)]). Below, we present a doubly stochastic subgradient descent algorithm to solve this problem.

The first stochasticity arises from a stochastic estimate of the objective by random mini-batches.
Specifically, the batch learning needs to scan the full dataset to compute subgradients, which is
often too expensive to deal with large-scale datasets. One effective technique is to do stochastic
subgradient descent [28], where at each iteration we randomly draw a mini-batch of the training
data and then do the variational updates over the small mini-batch. Formally, given a mini batch of
size m, we get an unbiased estimate of the objective:
m
m
N X
||?||2
NC X
L?m :=
L(?, ?; xn ) +
+
`(?, ?; xn ).
m n=1
2? 2
m n=1

The second stochasticity arises from a stochastic estimate of the per-sample variational bound
and its subgradient, whose intractability calls for another Monte Carlo estimator. Formally, let
zln ? q? (z|xn , yn ) be a set of samples from the variational distribution, where we explicitly put the
conditions. Then, an estimate of the per-sample variational bound and the per-sample hinge-loss is


X
X
? ?; xn )=max ?ln (y)? 1 ?>?fn (y, zl ) ,
? ?; xn )= 1 log p(xn , zln |?)?log q? (zln ); `(?,
L(?,
n
y
L
L
l

l

4

where ?fn (y, zln ) = f (yn , zln ) ? f (y, zln ). Note that L? is an unbiased estimate of L, while `? is a
biased estimate of `. Nevertheless, we can still show that `? is an upper bound estimate of ` under
expectation. Furthermore, this biasedness does not affect our estimate of the gradient. In fact,
by using the equality ?? q? (z) = q? (z)?? log q? (z), we can construct an unbiased Monte Carlo
estimate of ?? (L(?, ?; xn ) + `(?, ?; xn )) as:
L

g? =


1 X
log p(zln , xn ) ? log q? (zln ) + C?> ?fn (?
yn , zln ) ?? log q? (zln ),
L

(7)

l=1

where the last term roots
from the hinge loss with the loss-augmented prediction y?n =
P
argmaxy (?ln (y) + L1 l ?> f (y, zln )). For ? and ?, the estimates of the gradient ?? L(?, ?; xn )
and the subgradient ?? `(?, ?; xn ) are easier, which are:

1X
1X
g? =
?? log p(xn , zln |?), g? =
f (?
yn , zln ) ? f (yn , zln ) .
L
L
l

l

Notice that the sampling and the gradient
not the underlying model.

?? log q? (zln )

only depend on the variational distribution,

The above estimates consider the gen- Algorithm 1 Doubly Stochastic Subgradient Algorithm
Initialize ?, ?, and ?
eral case where the variational bound is
repeat
intractable. In some cases, we can comdraw a random mini-batch of m data points
pute the KL-divergence term analytidraw random samples from noise distribution p()
cally, e.g., when the prior and the vari? ?, ?; Xm , )
ational distribution are both Gaussian.
compute subgradient g = ??,?,? L(?,
In such cases, we only need to estimate
update parameters (?, ?, ?) using subgradient g.
the rest intractable part by sampling,
until Converge
which often reduces the variance [12].
return ?, ?, and ?
Similarly, we could use the expectation
of the features directly, if it can be computed analytically, in the computation of subgradients (e.g.,
g? and g? ) instead of sampling, which again can lead to variance reduction.
With the above estimates of subgradients, we can use stochastic optimization methods such as
SGD [28] and AdaM [10] to update the parameters, as outlined in Alg. 1. Overall, our algorithm is
a doubly stochastic generalization of Pegasos to deal with the highly nontrivial latent variables.
Now, the remaining question is how to define an appropriate variational distribution q? (z) to obtain
a robust estimate of the subgradients as well as the objective. Two types of methods have been developed for unsupervised DGMs, namely, variance reduction [21] and auto-encoding variational Bayes
(AVB) [12]. Though both methods can be used for our models, we focus on the AVB approach. For
continuous variables Z, under certain mild conditions we can reparameterize the variational distribution q? (z) using some simple variables . Specifically, we can draw samples  from some simple
distribution p() and do the transformation z = g? (, x, y) to get the sample of the distribution
q(z|x, y). We refer the readers to [12] for more details. In our experiments, we consider the special
Gaussian case, where we assume that the variational distribution is a multivariate Gaussian with a
diagonal covariance matrix:
q? (z|x, y) = N (?(x, y; ?), ? 2 (x, y; ?)),

(8)

whose mean and variance are functions of the input data. This defines our recognition model. Then,
the reparameterization trick is as follows: we first draw standard normal variables l ? N (0, I) and
then do the transformation zln = ?(xn , yn ; ?) + ?(xn , yn ; ?)  l to get a sample. For simplicity,
we assume that both the mean and variance are function of x only. However, it is worth to emphasize
that although the recognition model is unsupervised, the parameters ? are learned in a supervised
manner because the subgradient (7) depends on the hinge loss. Further details of the experimental
settings are presented in Sec. 4.1.

4

Experiments

We now present experimental results on the widely adopted MNIST [14] and SVHN [22] datasets.
Though mmDGMs are applicable to any DGMs that define a joint distribution of X and Z, we
5

concentrate on the Variational Auto-encoder (VA) [12], which is unsupervised. We denote our
mmDGM with VA by MMVA. In our experiments, we consider two types of recognition models:
multiple layer perceptrons (MLPs) and convolutional neural networks (CNNs). We implement all
experiments based on Theano [2]. 1
4.1

Architectures and Settings

In the MLP case, we follow the settings in [11] to compare both generative and discriminative
capacity of VA and MMVA. In the CNN case, we use standard convolutional nets [14] with convolution and max-pooling operation as the recognition model to obtain more competitive classification
results. For the generative model, we use unconvnets [6] with a ?symmetric? structure as the recognition model, to reconstruct the input images approximately. More specifically, the top-down generative model has the same structure as the bottom-up recognition model but replacing max-pooling
with unpooling operation [6] and applies unpooling, convolution and rectification in order. The total
number of parameters in the convolutional network is comparable with previous work [8, 17, 15].
For simplicity, we do not involve mlpconv layers [17, 15] and contrast normalization layers in our
recognition model, but they are not exclusive to our model. We illustrate details of the network
architectures in appendix A.
In both settings, the mean and variance of the latent z are transformed from the last layer of the
recognition model through a linear operation. It should be noticed that we could use not only the
expectation of z but also the activation of any layer in the recognition model as features. The only
theoretical difference is from where we add a hinge loss regularization to the gradient and backpropagate it to previous layers. In all of the experiments, the mean of z has the same nonlinearity
but typically much lower dimension than the activation of the last layer in the recognition model,
and hence often leads to a worse performance. In the MLP case, we concatenate the activations of
2 layers as the features used in the supervised tasks. In the CNN case, we use the activations of the
last layer as the features. We use AdaM [10] to optimize parameters in all of the models. Although it
is an adaptive gradient-based optimization method, we decay the global learning rate by factor three
periodically after sufficient number of epochs to ensure a stable convergence.
We denote our mmDGM with MLPs by MMVA. To perform classification using VA, we first learn
the feature representations by VA, and then build a linear SVM classifier on these features using the
Pegasos stochastic subgradient algorithm [28]. This baseline will be denoted by VA+Pegasos. The
corresponding models with CNNs are denoted by CMMVA and CVA+Pegasos respectively.
4.2

Results on the MNIST dataset

We present both the prediction performance and the results on generating samples of MMVA and
VA+Pegasos with both kinds of recognition models on the MNIST [14] dataset, which consists of
images of 10 different classes (0 to 9) of size 28?28 with 50,000 training samples, 10,000 validating
samples and 10,000 testing samples.
Table 1: Error rates (%) on MNIST dataset.
M ODEL
E RROR R ATE
4.2.1 Predictive Performance
VA+Pegasos
1.04
In the MLP case, we only use 50,000 trainVA+Class-conditionVA
0.96
ing data, and the parameters for classification are
MMVA
0.90
optimized according to the validation set. We
CVA+Pegasos
1.35
CMMVA
0.45
choose C = 15 for MMVA and initialize it with
Stochastic Pooling [33]
0.47
an unsupervised pre-training procedure in classiNetwork in Network [17]
0.47
fication. First three rows in Table 1 compare
Maxout Network [8]
0.45
VA+Pegasos, VA+Class-condtionVA and MMVA,
DSN
[15]
0.39
where VA+Class-condtionVA refers to the best fully
supervised model in [11]. Our model outperforms the baseline significantly. We further use the
t-SNE algorithm [19] to embed the features learned by VA and MMVA on 2D plane, which again
demonstrates the stronger discriminative ability of MMVA (See Appendix B for details).
In the CNN case, we use 60,000 training data. Table 2 shows the effect of C on classification error
rate and variational lower bound. Typically, as C gets lager, CMMVA learns more discriminative
features and leads to a worse estimation of data likelihood. However, if C is too small, the supervision is not enough to lead to predictive features. Nevertheless, C = 103 is quite a good trade-off
1

The source code is available at https://github.com/zhenxuan00/mmdgm.

6

(a) VA

(b) MMVA

(c) CVA

(d) CMMVA

Figure 1: (a-b): randomly generated images by VA and MMVA, 3000 epochs; (c-d): randomly
generated images by CVA and CMMVA, 600 epochs.
between the classification performance and generative performance and this is the default setting
of CMMVA on MNIST throughout this paper. In this setting, the classification performance of our
CMMVA model is comparable to the recent state-of-the-art fully discriminative networks (without
data augmentation), shown in the last four rows of Table 1.
Table 2: Effects of C on MNIST dataset
4.2.2 Generative Performance
with a CNN recognition model.
We further investigate the generative capability of MMVA C E RROR R ATE (%) L OWER B OUND
on generating samples. Fig. 1 illustrates the images ran- 0
1.35
-93.17
domly sampled from VA and MMVA models where we 1
1.86
-95.86
output the expectation of the gray value at each pixel to 10
0.88
-95.90
get a smooth visualization. We do not pre-train our model 102
0.54
-96.35
in all settings when generating data to prove that MMVA 103
0.45
-99.62
(CMMVA) remains the generative capability of DGMs.
104
0.43
-112.12
4.3

Results on the SVHN (Street View House Numbers) dataset

SVHN [22] is a large dataset consisting of color images of size 32 ? 32. The task is to recognize
center digits in natural scene images, which is significantly harder than classification of hand-written
digits. We follow the work [27, 8] to split the dataset into 598,388 training data, 6000 validating
data and 26, 032 testing data and preprocess the data by Local Contrast Normalization (LCN).
We only consider the CNN recognition model here. The network structure is similar to that in
MNIST. We set C = 104 for our CMMVA model on SVHN by default.
Table 3 shows the predictive performance.
In
this more challenging problem, we observe a
larger improvement by CMMVA as compared to
CVA+Pegasos, suggesting that DGMs benefit a lot
from max-margin learning on image classification.
We also compare CMMVA with state-of-the-art results. To the best of our knowledge, there is no competitive generative models to classify digits on SVHN
dataset with full labels.

Table 3: Error rates (%) on SVHN dataset.
M ODEL
E RROR R ATE
CVA+Pegasos
25.3
CMMVA
3.09
CNN [27]
4.9
Stochastic Pooling [33]
2.80
Maxout Network [8]
2.47
Network in Network [17]
2.35
DSN [15]
1.92

We further compare the generative capability of CMMVA and CVA to examine the benefits from
jointly training of DGMs and max-margin classifiers. Though CVA gives a tighter lower bound
of data likelihood and reconstructs data more elaborately, it fails to learn the pattern of digits in a
complex scenario and could not generate meaningful images. Visualization of random samples from
CVA and CMMVA is shown in Fig. 2. In this scenario, the hinge loss regularization on recognition
model is useful for generating main objects to be classified in images.
4.4

Missing Data Imputation and Classification

Finally, we test all models on the task of missing data imputation. For MNIST, we consider two types
of missing values [18]: (1) Rand-Drop: each pixel is missing randomly with a pre-fixed probability;
and (2) Rect: a rectangle located at the center of the image is missing. Given the perturbed images,
we uniformly initialize the missing values between 0 and 1, and then iteratively do the following
steps: (1) using the recognition model to sample the hidden variables; (2) predicting the missing
values to generate images; and (3) using the refined images as the input of the next round. For
SVHN, we do the same procedure as in MNIST but initialize the missing values with Guassian
7

(a) Training data

(b) CVA

(c) CMMVA (C = 103 ) (d) CMMVA (C = 104 )

Figure 2: (a): training data after LCN preprocessing; (b): random samples from CVA; (c-d):
random samples from CMMVA when C = 103 and C = 104 respectively.
random variables as the input distribution changes. Visualization results on MNIST and SVHN are
presented in Appendix C and Appendix D respectively.
Intuitively, generative models with CNNs
Table 4: MSE on MNIST data with missing values in
could be more powerful on learning patthe testing procedure.
terns and high-level structures, while
VA
MMVA CVA CMMVA
generative models with MLPs lean more N OISE T YPE
R
AND -D ROP (0.2) 0.0109 0.0110 0.0111 0.0147
to reconstruct the pixels in detail. This
R AND -D ROP (0.4) 0.0127 0.0127 0.0127 0.0161
conforms to the MSE results shown in R AND -D ROP (0.6) 0.0168 0.0165 0.0175 0.0203
Table 4: CVA and CMMVA outperform R AND -D ROP (0.8) 0.0379 0.0358 0.0453 0.0449
VA and MMVA with a missing rectan- R ECT (6 ? 6)
0.0637 0.0645 0.0585 0.0597
gle, while VA and MMVA outperform R ECT (8 ? 8)
0.0850 0.0841 0.0754 0.0724
CVA and CMMVA with random miss- R ECT (10 ? 10) 0.1100 0.1079 0.0978 0.0884
ing values. Compared with the baseline, R ECT (12 ? 12) 0.1450 0.1342 0.1299 0.1090
mmDGMs also make more accurate completion when large patches are missing. All of the models infer missing values for 100 iterations.
We also compare the classification performance of CVA, CNN and CMMVA with Rect missing
values in testing procedure in Appendix E. CMMVA outperforms both CVA and CNN.
Overall, mmDGMs have comparable capability of inferring missing values and prefer to learn highlevel patterns instead of local details.

5

Conclusions

We propose max-margin deep generative models (mmDGMs), which conjoin the predictive power
of max-margin principle and the generative ability of deep generative models. We develop a doubly
stochastic subgradient algorithm to learn all parameters jointly and consider two types of recognition
models with MLPs and CNNs respectively. In both cases, we present extensive results to demonstrate that mmDGMs can significantly improve the prediction performance of deep generative models, while retaining the strong generative ability on generating input samples as well as completing
missing values. In fact, by employing CNNs in both recognition and generative models, we achieve
low error rates on MNIST and SVHN datasets, which are competitive to the state-of-the-art fully
discriminative networks.
Acknowledgments
The work was supported by the National Basic Research Program (973 Program) of China (Nos.
2013CB329403, 2012CB316301), National NSF of China (Nos. 61322308, 61332007), Tsinghua TNList Lab
Big Data Initiative, and Tsinghua Initiative Scientific Research Program (Nos. 20121088071, 20141080934).

References
[1] Y. Altun, I. Tsochantaridis, and T. Hofmann. Hidden Markov support vector machines. In ICML, 2003.
[2] F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. Goodfellow, A. Bergeron, N. Bouchard, D. WardeFarley, and Y. Bengio. Theano: new features and speed improvements. In Deep Learning and Unsupervised Feature Learning NIPS Workshop, 2012.
[3] Y. Bengio, E. Laufer, G. Alain, and J. Yosinski. Deep generative stochastic networks trainable by backprop. In ICML, 2014.
[4] N. Chen, J. Zhu, F. Sun, and E. P. Xing. Large-margin predictive latent subspace learning for multi-view
data analysis. IEEE Trans. on PAMI, 34(12):2365?2378, 2012.

8

[5] C. Cortes and V. Vapnik. Support-vector networks. Journal of Machine Learning, 20(3):273?297, 1995.
[6] A. Dosovitskiy, J. T. Springenberg, and T. Brox. Learning to generate chairs with convolutional neural
networks. arXiv:1411.5928, 2014.
[7] I. J. Goodfellow, J. P. Abadie, M. Mirza, B. Xu, D. W. Farley, S.ozair, A. Courville, and Y. Bengio.
Generative adversarial nets. In NIPS, 2014.
[8] I. J. Goodfellow, D.Warde-Farley, M. Mirza, A. C. Courville, and Y. Bengio. Maxout networks. In ICML,
2013.
[9] K. Gregor, I. Danihelka, A. Mnih, C. Blundell, and D. Wierstra. Deep autoregressive networks. In ICML,
2014.
[10] D. P. Kingma and J. L. Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
[11] D. P. Kingma, D. J. Rezende, S. Mohamed, and M. Welling. Semi-supervised learning with deep generative models. In NIPS, 2014.
[12] D. P. Kingma and M. Welling. Auto-encoding variational Bayes. In ICLR, 2014.
[13] H. Larochelle and I. Murray. The neural autoregressive distribution estimator. In AISTATS, 2011.
[14] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.
In Proceedings of the IEEE, 1998.
[15] C. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply-supervised nets. In AISTATS, 2015.
[16] H. Lee, R. Grosse, R. Ranganath, and A. Y. Ng. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In ICML, 2009.
[17] M. Lin, Q. Chen, and S. Yan. Network in network. In ICLR, 2014.
[18] R. J. Little and D. B. Rubin. Statistical analysis with missing data. JMLR, 539, 1987.
[19] L. V. Matten and G. Hinton. Visualizing data using t-SNE. JMLR, 9:2579?2605, 2008.
[20] K. Miller, M. P. Kumar, B. Packer, D. Goodman, and D. Koller. Max-margin min-entropy models. In
AISTATS, 2012.
[21] A. Mnih and K. Gregor. Neural variational inference and learning in belief networks. In ICML, 2014.
[22] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural images with
unsupervised feature learning. NIPS Workshop on Deep Learning and Unsupervised Feature Learning,
2011.
[23] M. Ranzato, J. Susskind, V. Mnih, and G. E. Hinton. On deep generative models with applications to
recognition. In CVPR, 2011.
[24] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in
deep generative models. In ICML, 2014.
[25] R. Salakhutdinov and G. E. Hinton. Deep Boltzmann machines. In AISTATS, 2009.
[26] L. Saul, T. Jaakkola, and M. Jordan. Mean field theory for sigmoid belief networks. Journal of AI
Research, 4:61?76, 1996.
[27] P. Sermanet, S. Chintala, and Y. Lecun. Convolutional neural networks applied to house numbers digit
classification. In ICPR, 2012.
[28] S. Shalev-Shwartz, Y. Singer, N. Srebro, and A. Cotter. Pegasos: Primal estimated sub-gradient solver for
SVM. Mathematical Programming, Series B, 2011.
[29] Y. Tang. Deep learning using linear support vector machines. In Challenges on Representation Learning
Workshop, ICML, 2013.
[30] B. Taskar, C. Guestrin, and D. Koller. Max-margin Markov networks. In NIPS, 2003.
[31] I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. Support vector machine learning for interdependent and structured output spaces. In ICML, 2004.
[32] C. J. Yu and T. Joachims. Learning structural SVMs with latent variables. In ICML, 2009.
[33] M. D. Zeiler and R. Fergus. Stochastic pooling for regularization of deep convolutional neural networks.
In ICLR, 2013.
[34] J. Zhu, A. Ahmed, and E. P. Xing. MedLDA: Maximum margin supervised topic models. JMLR, 13:2237?
2278, 2012.
[35] J. Zhu, N. Chen, H. Perkins, and B. Zhang. Gibbs max-margin topic models with data augmentation.
JMLR, 15:1073?1110, 2014.
[36] J. Zhu, N. Chen, and E. P. Xing. Bayesian inference with posterior regularization and applications to
infinite latent SVMs. JMLR, 15:1799?1847, 2014.
[37] J. Zhu, E.P. Xing, and B. Zhang. Partially observed maximum entropy discrimination Markov networks.
In NIPS, 2008.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 5954-convolutional-networks-on-graphs-for-learning-molecular-fingerprints.pdf

Convolutional Networks on Graphs
for Learning Molecular Fingerprints

David Duvenaud? , Dougal Maclaurin?, Jorge Aguilera-Iparraguirre
Rafael G?omez-Bombarelli, Timothy Hirzel, Al?an Aspuru-Guzik, Ryan P. Adams
Harvard University

Abstract
We introduce a convolutional neural network that operates directly on graphs.
These networks allow end-to-end learning of prediction pipelines whose inputs
are graphs of arbitrary size and shape. The architecture we present generalizes
standard molecular feature extraction methods based on circular fingerprints. We
show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.

1

Introduction

Recent work in materials design used neural networks to predict the properties of novel molecules
by generalizing from examples. One difficulty with this task is that the input to the predictor, a
molecule, can be of arbitrary size and shape. Currently, most machine learning pipelines can only
handle inputs of a fixed size. The current state of the art is to use off-the-shelf fingerprint software
to compute fixed-dimensional feature vectors, and use those features as inputs to a fully-connected
deep neural network or other standard machine learning method. This formula was followed by
[28, 3, 19]. During training, the molecular fingerprint vectors were treated as fixed.
In this paper, we replace the bottom layer of this stack ? the function that computes molecular
fingerprint vectors ? with a differentiable neural network whose input is a graph representing the
original molecule. In this graph, vertices represent individual atoms and edges represent bonds. The
lower layers of this network is convolutional in the sense that the same local filter is applied to each
atom and its neighborhood. After several such layers, a global pooling step combines features from
all the atoms in the molecule.
These neural graph fingerprints offer several advantages over fixed fingerprints:
? Predictive performance. By using data adapting to the task at hand, machine-optimized
fingerprints can provide substantially better predictive performance than fixed fingerprints.
We show that neural graph fingerprints match or beat the predictive performance of standard fingerprints on solubility, drug efficacy, and organic photovoltaic efficiency datasets.
? Parsimony. Fixed fingerprints must be extremely large to encode all possible substructures
without overlap. For example, [28] used a fingerprint vector of size 43,000, after having
removed rarely-occurring features. Differentiable fingerprints can be optimized to encode
only relevant features, reducing downstream computation and regularization requirements.
? Interpretability. Standard fingerprints encode each possible fragment completely distinctly, with no notion of similarity between fragments. In contrast, each feature of a neural
graph fingerprint can be activated by similar but distinct molecular fragments, making the
feature representation more meaningful.
?

Equal contribution.

1

Figure 1: Left: A visual representation of the computational graph of both standard circular fingerprints and neural graph fingerprints. First, a graph is constructed matching the topology of the
molecule being fingerprinted, in which nodes represent atoms, and edges represent bonds. At each
layer, information flows between neighbors in the graph. Finally, each node in the graph turns on
one bit in the fixed-length fingerprint vector. Right: A more detailed sketch including the bond
information used in each operation.

2

Circular fingerprints

The state of the art in molecular fingerprints are extended-connectivity circular fingerprints
(ECFP) [21]. Circular fingerprints [6] are a refinement of the Morgan algorithm [17], designed
to encode which substructures are present in a molecule in a way that is invariant to atom-relabeling.
Circular fingerprints generate each layer?s features by applying a fixed hash function to the concatenated features of the neighborhood in the previous layer. The results of these hashes are then treated
as integer indices, where a 1 is written to the fingerprint vector at the index given by the feature
vector at each node in the graph. Figure 1(left) shows a sketch of this computational architecture.
Ignoring collisions, each index of the fingerprint denotes the presence of a particular substructure.
The size of the substructures represented by each index depends on the depth of the network. Thus
the number of layers is referred to as the ?radius? of the fingerprints.
Circular fingerprints are analogous to convolutional networks in that they apply the same operation
locally everywhere, and combine information in a global pooling step.

3

Creating a differentiable fingerprint

The space of possible network architectures is large. In the spirit of starting from a known-good configuration, we designed a differentiable generalization of circular fingerprints. This section describes
our replacement of each discrete operation in circular fingerprints with a differentiable analog.
Hashing The purpose of the hash functions applied at each layer of circular fingerprints is to
combine information about each atom and its neighboring substructures. This ensures that any
change in a fragment, no matter how small, will lead to a different fingerprint index being activated.
We replace the hash operation with a single layer of a neural network. Using a smooth function
allows the activations to be similar when the local molecular structure varies in unimportant ways.
Indexing Circular fingerprints use an indexing operation to combine all the nodes? feature vectors
into a single fingerprint of the whole molecule. Each node sets a single bit of the fingerprint to one,
at an index determined by the hash of its feature vector. This pooling-like operation converts an
arbitrary-sized graph into a fixed-sized vector. For small molecules and a large fingerprint length,
the fingerprints are always sparse. We use the softmax operation as a differentiable analog of
indexing. In essence, each atom is asked to classify itself as belonging to a single category. The sum
of all these classification label vectors produces the final fingerprint. This operation is analogous to
the pooling operation in standard convolutional neural networks.
2

Algorithm 1 Circular fingerprints
1: Input: molecule, radius R, fingerprint
length S
2: Initialize: fingerprint vector f ? 0S
3: for each atom a in molecule
4:
ra ? g(a)
. lookup atom features
5: for L = 1 to R
. for each layer
6:
for each atom a in molecule
7:
r1 . . . rN = neighbors(a)
8:
v ? [ra , r1 , . . . , rN ] . concatenate
9:
ra ? hash(v)
. hash function
10:
i ? mod(ra , S) . convert to index
11:
fi ? 1
. Write 1 at index
12: Return: binary vector f

Algorithm 2 Neural graph fingerprints
1: Input: molecule, radius R, hidden weights
5
H11 . . . HR
, output weights W1 . . . WR
2: Initialize: fingerprint vector f ? 0S
3: for each atom a in molecule
4:
ra ? g(a)
. lookup atom features
5: for L = 1 to R
. for each layer
6:
for each atom a in molecule
7:
r1 . . . rN =Pneighbors(a)
8:
v ? ra + N
. sum
i=1 ri
9:
ra ? ?(vHLN )
. smooth function
10:
i ? softmax(ra WL )
. sparsify
11:
f ?f +i
. add to fingerprint
12: Return: real-valued vector f

Figure 2: Pseudocode of circular fingerprints (left) and neural graph fingerprints (right). Differences
are highlighted in blue. Every non-differentiable operation is replaced with a differentiable analog.
Canonicalization Circular fingerprints are identical regardless of the ordering of atoms in each
neighborhood. This invariance is achieved by sorting the neighboring atoms according to their
features, and bond features. We experimented with this sorting scheme, and also with applying the
local feature transform on all possible permutations of the local neighborhood. An alternative to
canonicalization is to apply a permutation-invariant function, such as summation. In the interests of
simplicity and scalability, we chose summation.
Circular fingerprints can be interpreted as a special case of neural graph fingerprints having large
random weights. This is because, in the limit of large input weights, tanh nonlinearities approach
step functions, which when concatenated form a simple hash function. Also, in the limit of large
input weights, the softmax operator approaches a one-hot-coded argmax operator, which is analogous to an indexing operation.
Algorithms 1 and 2 summarize these two algorithms and highlight their differences. Given a fingerprint length L, and F features at each layer, the parameters of neural graph fingerprints consist of
a separate output weight matrix of size F ? L for each layer, as well as a set of hidden-to-hidden
weight matrices of size F ? F at each layer, one for each possible number of bonds an atom can
have (up to 5 in organic molecules).

4

Experiments

We ran two experiments to demonstrate that neural fingerprints with large random weights behave
similarly to circular fingerprints. First, we examined whether distances between circular fingerprints
were similar to distances between neural fingerprint-based distances. Figure 3 (left) shows a scatterplot of pairwise distances between circular vs. neural fingerprints. Fingerprints had length 2048,
and were calculated on pairs of molecules from the solubility dataset [4]. Distance was measured
using a continuous generalization of the Tanimoto (a.k.a. Jaccard) similarity measure, given by
.X
X
distance(x, y) = 1 ?
min(xi , yi )
max(xi , yi )
(1)
There is a correlation of r = 0.823 between the distances. The line of points on the right of the plot
shows that for some pairs of molecules, binary ECFP fingerprints have exactly zero overlap.
Second, we examined the predictive performance of neural fingerprints with large random weights
vs. that of circular fingerprints. Figure 3 (right) shows average predictive performance on the solubility dataset, using linear regression on top of fingerprints. The performances of both methods
follow similar curves. In contrast, the performance of neural fingerprints with small random weights
follows a different curve, and is substantially better. This suggests that even with random weights,
the relatively smooth activation of neural fingerprints helps generalization performance.
3

2.0
1.8

0.9

RMSE (log Mol/L)

Neural fingerprint distances

Neural vs Circular distances, r =0:823
1.0

0.8
0.7
0.6
0.5
0.5

Circular fingerprints
Random conv with large parameters
Random conv with small parameters

1.6
1.4
1.2
1.0
0.8
0

0.6
0.7
0.8
0.9
1.0
Circular fingerprint distances

1

2
3
4
Fingerprint radius

5

6

Figure 3: Left: Comparison of pairwise distances between molecules, measured using circular fingerprints and neural graph fingerprints with large random weights. Right: Predictive performance
of circular fingerprints (red), neural graph fingerprints with fixed large random weights (green) and
neural graph fingerprints with fixed small random weights (blue). The performance of neural graph
fingerprints with large random weights closely matches the performance of circular fingerprints.
4.1

Examining learned features

To demonstrate that neural graph fingerprints are interpretable, we show substructures which most
activate individual features in a fingerprint vector. Each feature of a circular fingerprint vector can
each only be activated by a single fragment of a single radius, except for accidental collisions.
In contrast, neural graph fingerprint features can be activated by variations of the same structure,
making them more interpretable, and allowing shorter feature vectors.
Solubility features Figure 4 shows the fragments that maximally activate the most predictive features of a fingerprint. The fingerprint network was trained as inputs to a linear model predicting
solubility, as measured in [4]. The feature shown in the top row has a positive predictive relationship
with solubility, and is most activated by fragments containing a hydrophilic R-OH group, a standard
indicator of solubility. The feature shown in the bottom row, strongly predictive of insolubility, is
activated by non-polar repeated ring structures.
Fragments most
activated by
pro-solubility
feature

O

OH

O
NH
O

OH

OH

Fragments most
activated by
anti-solubility
feature

Figure 4: Examining fingerprints optimized for predicting solubility. Shown here are representative
examples of molecular fragments (highlighted in blue) which most activate different features of the
fingerprint. Top row: The feature most predictive of solubility. Bottom row: The feature most
predictive of insolubility.

4

Toxicity features We trained the same model architecture to predict toxicity, as measured in two
different datasets in [26]. Figure 5 shows fragments which maximally activate the feature most
predictive of toxicity, in two separate datasets.
Fragments most
activated by
toxicity feature
on SR-MMP
dataset
Fragments most
activated by
toxicity feature
on NR-AHR
dataset
Figure 5: Visualizing fingerprints optimized for predicting toxicity. Shown here are representative
samples of molecular fragments (highlighted in red) which most activate the feature most predictive
of toxicity. Top row: the most predictive feature identifies groups containing a sulphur atom attached
to an aromatic ring. Bottom row: the most predictive feature identifies fused aromatic rings, also
known as polycyclic aromatic hydrocarbons, a well-known carcinogen.
[27] constructed similar visualizations, but in a semi-manual way: to determine which toxic fragments activated a given neuron, they searched over a hand-made list of toxic substructures and chose
the one most correlated with a given neuron. In contrast, our visualizations are generated automatically, without the need to restrict the range of possible answers beforehand.
4.2

Predictive Performance

We ran several experiments to compare the predictive performance of neural graph fingerprints to
that of the standard state-of-the-art setup: circular fingerprints fed into a fully-connected neural
network.
Experimental setup Our pipeline takes as input the SMILES [30] string encoding of each
molecule, which is then converted into a graph using RDKit [20]. We also used RDKit to produce
the extended circular fingerprints used in the baseline. Hydrogen atoms were treated implicitly.
In our convolutional networks, the initial atom and bond features were chosen to be similar to those
used by ECFP: Initial atom features concatenated a one-hot encoding of the atom?s element, its
degree, the number of attached hydrogen atoms, and the implicit valence, and an aromaticity indicator. The bond features were a concatenation of whether the bond type was single, double, triple,
or aromatic, whether the bond was conjugated, and whether the bond was part of a ring.
Training and Architecture Training used batch normalization [11]. We also experimented with
tanh vs relu activation functions for both the neural fingerprint network layers and the fullyconnected network layers. relu had a slight but consistent performance advantage on the validation set. We also experimented with dropconnect [29], a variant of dropout in which weights are
randomly set to zero instead of hidden units, but found that it led to worse validation error in general. Each experiment optimized for 10000 minibatches of size 100 using the Adam algorithm [13],
a variant of RMSprop that includes momentum.
Hyperparameter Optimization To optimize hyperparameters, we used random search. The hyperparameters of all methods were optimized using 50 trials for each cross-validation fold. The
following hyperparameters were optimized: log learning rate, log of the initial weight scale, the log
L2 penalty, fingerprint length, fingerprint depth (up to 6), and the size of the hidden layer in the
fully-connected network. Additionally, the size of the hidden feature vector in the convolutional
neural fingerprint networks was optimized.
5

Dataset
Units
Predict mean
Circular FPs + linear layer
Circular FPs + neural net
Neural FPs + linear layer
Neural FPs + neural net

Solubility [4]
log Mol/L

Drug efficacy [5]
EC50 in nM

Photovoltaic efficiency [8]
percent

4.29 ? 0.40
1.71 ? 0.13
1.40 ? 0.13
0.77 ? 0.11
0.52 ? 0.07

1.47 ? 0.07
1.13 ? 0.03
1.36 ? 0.10
1.15 ? 0.02
1.16 ? 0.03

6.40 ? 0.09
2.63 ? 0.09
2.00 ? 0.09
2.58 ? 0.18
1.43 ? 0.09

Table 1: Mean predictive accuracy of neural fingerprints compared to standard circular fingerprints.

Datasets We compared the performance of standard circular fingerprints against neural graph fingerprints on a variety of domains:
? Solubility: The aqueous solubility of 1144 molecules as measured by [4].
? Drug efficacy: The half-maximal effective concentration (EC50 ) in vitro of 10,000
molecules against a sulfide-resistant strain of P. falciparum, the parasite that causes malaria,
as measured by [5].
? Organic photovoltaic efficiency: The Harvard Clean Energy Project [8] uses expensive
DFT simulations to estimate the photovoltaic efficiency of organic molecules. We used a
subset of 20,000 molecules from this dataset.
Predictive accuracy We compared the performance of circular fingerprints and neural graph fingerprints under two conditions: In the first condition, predictions were made by a linear layer using
the fingerprints as input. In the second condition, predictions were made by a one-hidden-layer
neural network using the fingerprints as input. In all settings, all differentiable parameters in the
composed models were optimized simultaneously. Results are summarized in Table 4.2.
In all experiments, the neural graph fingerprints matched or beat the accuracy of circular fingerprints,
and the methods with a neural network on top of the fingerprints typically outperformed the linear
layers.
Software Automatic differentiation (AD) software packages such as Theano [1] significantly
speed up development time by providing gradients automatically, but can only handle limited control
structures and indexing. Since we required relatively complex control flow and indexing in order
to implement variants of Algorithm 2, we used a more flexible automatic differentiation package
for Python called Autograd (github.com/HIPS/autograd). This package handles standard
Numpy [18] code, and can differentiate code containing while loops, branches, and indexing.
Code for computing neural fingerprints and producing visualizations is available at
github.com/HIPS/neural-fingerprint.

5

Limitations

Computational cost Neural fingerprints have the same asymptotic complexity in the number of
atoms and the depth of the network as circular fingerprints, but have additional terms due to the
matrix multiplies necessary to transform the feature vector at each step. To be precise, computing
the neural fingerprint of depth R, fingerprint length L of a molecule with N atoms using a molecular
convolutional net having F features at each layer costs O(RN F L + RN F 2 ). In practice, training
neural networks on top of circular fingerprints usually took several minutes, while training both the
fingerprints and the network on top took on the order of an hour on the larger datasets.
Limited computation at each layer How complicated should we make the function that goes
from one layer of the network to the next? In this paper we chose the simplest feasible architecture:
a single layer of a neural network. However, it may be fruitful to apply multiple layers of nonlinearities between each message-passing step (as in [22]), or to make information preservation easier by
adapting the Long Short-Term Memory [10] architecture to pass information upwards.
6

Limited information propagation across the graph The local message-passing architecture developed in this paper scales well in the size of the graph (due to the low degree of organic molecules),
but its ability to propagate information across the graph is limited by the depth of the network. This
may be appropriate for small graphs such as those representing the small organic molecules used in
this paper. However, in the worst case, it can take a depth N2 network to distinguish between graphs
of size N . To avoid this problem, [2] proposed a hierarchical clustering of graph substructures. A
tree-structured network could examine the structure of the entire graph using only log(N ) layers,
but would require learning to parse molecules. Techniques from natural language processing [25]
might be fruitfully adapted to this domain.
Inability to distinguish stereoisomers Special bookkeeping is required to distinguish between
stereoisomers, including enantomers (mirror images of molecules) and cis/trans isomers (rotation
around double bonds). Most circular fingerprint implementations have the option to make these
distinctions. Neural fingerprints could be extended to be sensitive to stereoisomers, but this remains
a task for future work.

6

Related work

This work is similar in spirit to the neural Turing machine [7], in the sense that we take an existing
discrete computational architecture, and make each part differentiable in order to do gradient-based
optimization.
Neural nets for quantitative structure-activity relationship (QSAR) The modern standard for
predicting properties of novel molecules is to compose circular fingerprints with fully-connected
neural networks or other regression methods. [3] used circular fingerprints as inputs to an ensemble
of neural networks, Gaussian processes, and random forests. [19] used circular fingerprints (of depth
2) as inputs to a multitask neural network, showing that multiple tasks helped performance.
Neural graph fingerprints The most closely related work is [15], who build a neural network
having graph-valued inputs. Their approach is to remove all cycles and build the graph into a tree
structure, choosing one atom to be the root. A recursive neural network [23, 24] is then run from
the leaves to the root to produce a fixed-size representation. Because a graph having N nodes
has N possible roots, all N possible graphs are constructed. The final descriptor is a sum of the
representations computed by all distinct graphs. There are as many distinct graphs as there are
atoms in the network. The computational cost of this method thus grows as O(F 2 N 2 ), where F
is the size of the feature vector and N is the number of atoms, making it less suitable for large
molecules.
Convolutional neural networks Convolutional neural networks have been used to model images,
speech, and time series [14]. However, standard convolutional architectures use a fixed computational graph, making them difficult to apply to objects of varying size or structure, such as molecules.
More recently, [12] and others have developed a convolutional neural network architecture for modeling sentences of varying length.
Neural networks on fixed graphs [2] introduce convolutional networks on graphs in the regime
where the graph structure is fixed, and each training example differs only in having different features
at the vertices of the same graph. In contrast, our networks address the situation where each training
input is a different graph.
Neural networks on input-dependent graphs [22] propose a neural network model for graphs
having an interesting training procedure. The forward pass consists of running a message-passing
scheme to equilibrium, a fact which allows the reverse-mode gradient to be computed without storing
the entire forward computation. They apply their network to predicting mutagenesis of molecular
compounds as well as web page rankings. [16] also propose a neural network model for graphs
with a learning scheme whose inner loop optimizes not the training loss, but rather the correlation
between each newly-proposed vector and the training error residual. They apply their model to a
dataset of boiling points of 150 molecular compounds. Our paper builds on these ideas, with the
7

following differences: Our method replaces their complex training algorithms with simple gradientbased optimization, generalizes existing circular fingerprint computations, and applies these networks in the context of modern QSAR pipelines which use neural networks on top of the fingerprints
to increase model capacity.
Unrolled inference algorithms [9] and others have noted that iterative inference procedures
sometimes resemble the feedforward computation of a recurrent neural network. One natural extension of these ideas is to parameterize each inference step, and train a neural network to approximately
match the output of exact inference using only a small number of iterations. The neural fingerprint,
when viewed in this light, resembles an unrolled message-passing algorithm on the original graph.

7

Conclusion

We generalized existing hand-crafted molecular features to allow their optimization for diverse tasks.
By making each operation in the feature pipeline differentiable, we can use standard neural-network
training methods to scalably optimize the parameters of these neural molecular fingerprints end-toend. We demonstrated the interpretability and predictive performance of these new fingerprints.
Data-driven features have already replaced hand-crafted features in speech recognition, machine
vision, and natural-language processing. Carrying out the same task for virtual screening, drug
design, and materials design is a natural next step.
Acknowledgments
We thank Edward Pyzer-Knapp, Jennifer Wei, and Samsung Advanced Institute of Technology for
their support. This work was partially funded by NSF IIS-1421780.

References
[1] Fr?ed?eric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian J. Goodfellow, Arnaud
Bergeron, Nicolas Bouchard, and Yoshua Bengio. Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.
[2] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally
connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013.
[3] George E. Dahl, Navdeep Jaitly, and Ruslan Salakhutdinov. Multi-task neural networks for
QSAR predictions. arXiv preprint arXiv:1406.1231, 2014.
[4] John S. Delaney. ESOL: Estimating aqueous solubility directly from molecular structure. Journal of Chemical Information and Computer Sciences, 44(3):1000?1005, 2004.
[5] Francisco-Javier Gamo, Laura M Sanz, Jaume Vidal, Cristina de Cozar, Emilio Alvarez,
Jose-Luis Lavandera, Dana E Vanderwall, Darren VS Green, Vinod Kumar, Samiul Hasan,
et al. Thousands of chemical starting points for antimalarial lead identification. Nature,
465(7296):305?310, 2010.
[6] Robert C. Glem, Andreas Bender, Catrin H. Arnby, Lars Carlsson, Scott Boyer, and James
Smith. Circular fingerprints: flexible molecular descriptors with applications from physical
chemistry to ADME. IDrugs: the investigational drugs journal, 9(3):199?204, 2006.
[7] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural Turing machines. arXiv preprint
arXiv:1410.5401, 2014.
[8] Johannes Hachmann, Roberto Olivares-Amaya, Sule Atahan-Evrenk, Carlos Amador-Bedolla,
Roel S S?anchez-Carrera, Aryeh Gold-Parker, Leslie Vogt, Anna M Brockway, and Al?an
Aspuru-Guzik. The Harvard clean energy project: large-scale computational screening and
design of organic photovoltaics on the world community grid. The Journal of Physical Chemistry Letters, 2(17):2241?2251, 2011.
[9] John R Hershey, Jonathan Le Roux, and Felix Weninger. Deep unfolding: Model-based inspiration of novel deep architectures. arXiv preprint arXiv:1409.2574, 2014.
8

[10] Sepp Hochreiter and J?urgen Schmidhuber. Long short-term memory. Neural computation,
9(8):1735?1780, 1997.
[11] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
[12] Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. A convolutional neural network
for modelling sentences. Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, June 2014.
[13] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
[14] Yann LeCun and Yoshua Bengio. Convolutional networks for images, speech, and time series.
The handbook of brain theory and neural networks, 3361, 1995.
[15] Alessandro Lusci, Gianluca Pollastri, and Pierre Baldi. Deep architectures and deep learning
in chemoinformatics: the prediction of aqueous solubility for drug-like molecules. Journal of
chemical information and modeling, 53(7):1563?1575, 2013.
[16] Alessio Micheli. Neural network for graphs: A contextual constructive approach. Neural
Networks, IEEE Transactions on, 20(3):498?511, 2009.
[17] H.L. Morgan. The generation of a unique machine description for chemical structure. Journal
of Chemical Documentation, 5(2):107?113, 1965.
[18] Travis E Oliphant. Python for scientific computing. Computing in Science & Engineering,
9(3):10?20, 2007.
[19] Bharath Ramsundar, Steven Kearnes, Patrick Riley, Dale Webster, David Konerding, and Vijay
Pande. Massively multitask networks for drug discovery. arXiv:1502.02072, 2015.
[20] RDKit: Open-source cheminformatics. www.rdkit.org. [accessed 11-April-2013].
[21] David Rogers and Mathew Hahn. Extended-connectivity fingerprints. Journal of Chemical
Information and Modeling, 50(5):742?754, 2010.
[22] F. Scarselli, M. Gori, Ah Chung Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural
network model. Neural Networks, IEEE Transactions on, 20(1):61?80, Jan 2009.
[23] Richard Socher, Eric H Huang, Jeffrey Pennin, Christopher D Manning, and Andrew Y Ng.
Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In Advances
in Neural Information Processing Systems, pages 801?809, 2011.
[24] Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages
151?161. Association for Computational Linguistics, 2011.
[25] Kai Sheng Tai, Richard Socher, and Christopher D Manning.
Improved semantic
representations from tree-structured long short-term memory networks. arXiv preprint
arXiv:1503.00075, 2015.
[26] Tox21 Challenge. National center for advancing translational sciences. http://tripod.
nih.gov/tox21/challenge, 2014. [Online; accessed 2-June-2015].
[27] Thomas Unterthiner, Andreas Mayr, G?unter Klambauer, and Sepp Hochreiter. Toxicity prediction using deep learning. arXiv preprint arXiv:1503.01445, 2015.
[28] Thomas Unterthiner, Andreas Mayr, G u? nter Klambauer, Marvin Steijaert, J?org Wenger, Hugo
Ceulemans, and Sepp Hochreiter. Deep learning as an opportunity in virtual screening. In
Advances in Neural Information Processing Systems, 2014.
[29] Li Wan, Matthew Zeiler, Sixin Zhang, Yann L. Cun, and Rob Fergus. Regularization of neural
networks using dropconnect. In International Conference on Machine Learning, 2013.
[30] David Weininger. SMILES, a chemical language and information system. Journal of chemical
information and computer sciences, 28(1):31?36, 1988.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

