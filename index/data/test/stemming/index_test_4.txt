query sentence: state-of-art algorithms in theano
---------------------------------------------------------------------
title: 5647-binaryconnect-training-deep-neural-networks-with-binary-weights-during-propagations.pdf

BinaryConnect: Training Deep Neural Networks with
binary weights during propagations

Matthieu Courbariaux
?
Ecole
Polytechnique de Montr?eal
matthieu.courbariaux@polymtl.ca

Yoshua Bengio
Universit?e de Montr?eal, CIFAR Senior Fellow
yoshua.bengio@gmail.com

Jean-Pierre David
?
Ecole
Polytechnique de Montr?eal
jean-pierre.david@polymtl.ca

Abstract
Deep Neural Networks (DNN) have achieved state-of-the-art results in a wide
range of tasks, with the best results obtained with large training sets and large
models. In the past, GPUs enabled these breakthroughs because of their greater
computational speed. In the future, faster computation at both training and test
time is likely to be crucial for further progress and for consumer applications on
low-power devices. As a result, there is much interest in research and development of dedicated hardware for Deep Learning (DL). Binary weights, i.e., weights
which are constrained to only two possible values (e.g. -1 or 1), would bring great
benefits to specialized DL hardware by replacing many multiply-accumulate operations by simple accumulations, as multipliers are the most space and powerhungry components of the digital implementation of neural networks. We introduce BinaryConnect, a method which consists in training a DNN with binary
weights during the forward and backward propagations, while retaining precision
of the stored weights in which gradients are accumulated. Like other dropout
schemes, we show that BinaryConnect acts as regularizer and we obtain near
state-of-the-art results with BinaryConnect on the permutation-invariant MNIST,
CIFAR-10 and SVHN.

1

Introduction

Deep Neural Networks (DNN) have substantially pushed the state-of-the-art in a wide range of tasks,
especially in speech recognition [1, 2] and computer vision, notably object recognition from images [3, 4]. More recently, deep learning is making important strides in natural language processing,
especially statistical machine translation [5, 6, 7]. Interestingly, one of the key factors that enabled
this major progress has been the advent of Graphics Processing Units (GPUs), with speed-ups on the
order of 10 to 30-fold, starting with [8], and similar improvements with distributed training [9, 10].
Indeed, the ability to train larger models on more data has enabled the kind of breakthroughs observed in the last few years. Today, researchers and developers designing new deep learning algorithms and applications often find themselves limited by computational capability. This along, with
the drive to put deep learning systems on low-power devices (unlike GPUs) is greatly increasing the
interest in research and development of specialized hardware for deep networks [11, 12, 13].
Most of the computation performed during training and application of deep networks regards the
multiplication of a real-valued weight by a real-valued activation (in the recognition or forward
propagation phase of the back-propagation algorithm) or gradient (in the backward propagation
phase of the back-propagation algorithm). This paper proposes an approach called BinaryConnect
1

to eliminate the need for these multiplications by forcing the weights used in these forward and
backward propagations to be binary, i.e. constrained to only two values (not necessarily 0 and 1). We
show that state-of-the-art results can be achieved with BinaryConnect on the permutation-invariant
MNIST, CIFAR-10 and SVHN.
What makes this workable are two ingredients:
1. Sufficient precision is necessary to accumulate and average a large number of stochastic
gradients, but noisy weights (and we can view discretization into a small number of values
as a form of noise, especially if we make this discretization stochastic) are quite compatible
with Stochastic Gradient Descent (SGD), the main type of optimization algorithm for deep
learning. SGD explores the space of parameters by making small and noisy steps and
that noise is averaged out by the stochastic gradient contributions accumulated in each
weight. Therefore, it is important to keep sufficient resolution for these accumulators,
which at first sight suggests that high precision is absolutely required. [14] and [15] show
that randomized or stochastic rounding can be used to provide unbiased discretization.
[14] have shown that SGD requires weights with a precision of at least 6 to 8 bits and
[16] successfully train DNNs with 12 bits dynamic fixed-point computation. Besides, the
estimated precision of the brain synapses varies between 6 and 12 bits [17].
2. Noisy weights actually provide a form of regularization which can help to generalize better,
as previously shown with variational weight noise [18], Dropout [19, 20] and DropConnect [21], which add noise to the activations or to the weights. For instance, DropConnect
[21], which is closest to BinaryConnect, is a very efficient regularizer that randomly substitutes half of the weights with zeros during propagations. What these previous works show
is that only the expected value of the weight needs to have high precision, and that noise
can actually be beneficial.
The main contributions of this article are the following.
? We introduce BinaryConnect, a method which consists in training a DNN with binary
weights during the forward and backward propagations (Section 2).
? We show that BinaryConnect is a regularizer and we obtain near state-of-the-art results on
the permutation-invariant MNIST, CIFAR-10 and SVHN (Section 3).
? We make the code for BinaryConnect available 1 .

2

BinaryConnect

In this section we give a more detailed view of BinaryConnect, considering which two values to
choose, how to discretize, how to train and how to perform inference.
+1 or ?1

2.1

Applying a DNN mainly consists in convolutions and matrix multiplications. The key arithmetic
operation of DL is thus the multiply-accumulate operation. Artificial neurons are basically multiplyaccumulators computing weighted sums of their inputs.
BinaryConnect constraints the weights to either +1 or ?1 during propagations. As a result, many
multiply-accumulate operations are replaced by simple additions (and subtractions). This is a huge
gain, as fixed-point adders are much less expensive both in terms of area and energy than fixed-point
multiply-accumulators [22].
2.2

Deterministic vs stochastic binarization

The binarization operation transforms the real-valued weights into the two possible values. A very
straightforward binarization operation would be based on the sign function:

+1 if w ? 0,
wb =
(1)
?1 otherwise.
1

https://github.com/MatthieuCourbariaux/BinaryConnect

2

Where wb is the binarized weight and w the real-valued weight. Although this is a deterministic operation, averaging this discretization over the many input weights of a hidden unit could compensate
for the loss of information. An alternative that allows a finer and more correct averaging process to
take place is to binarize stochastically:

+1 with probability p = ?(w),
(2)
wb =
?1 with probability 1 ? p.
where ? is the ?hard sigmoid? function:
?(x) = clip(

x+1
x+1
, 0, 1) = max(0, min(1,
))
2
2

(3)

We use such a hard sigmoid rather than the soft version because it is far less computationally expensive (both in software and specialized hardware implementations) and yielded excellent results in our
experiments. It is similar to the ?hard tanh? non-linearity introduced by [23]. It is also piece-wise
linear and corresponds to a bounded form of the rectifier [24].
2.3

Propagations vs updates

Let us consider the different steps of back-propagation with SGD udpates and whether it makes
sense, or not, to discretize the weights, at each of these steps.
1. Given the DNN input, compute the unit activations layer by layer, leading to the top layer
which is the output of the DNN, given its input. This step is referred as the forward propagation.
2. Given the DNN target, compute the training objective?s gradient w.r.t. each layer?s activations, starting from the top layer and going down layer by layer until the first hidden
layer. This step is referred to as the backward propagation or backward phase of backpropagation.
3. Compute the gradient w.r.t. each layer?s parameters and then update the parameters using
their computed gradients and their previous values. This step is referred to as the parameter
update.
Algorithm 1 SGD training with BinaryConnect. C is the cost function for minibatch and the functions binarize(w) and clip(w) specify how to binarize and clip weights. L is the number of layers.
Require: a minibatch of (inputs, targets), previous parameters wt?1 (weights) and bt?1 (biases),
and learning rate ?.
Ensure: updated parameters wt and bt .
1. Forward propagation:
wb ? binarize(wt?1 )
For k = 1 to L, compute ak knowing ak?1 , wb and bt?1
2. Backward propagation:
?C
Initialize output layer?s activations gradient ?a
L
?C
?C
For k = L to 2, compute ?ak?1 knowing ?ak and wb
3. Parameter update:
?C
?C
Compute ?w
and db?C
knowing ?a
and ak?1
t?1
b
k
?C
wt ? clip(wt?1 ? ? ?w
)
b
?C
bt ? bt?1 ? ? ?bt?1

A key point to understand with BinaryConnect is that we only binarize the weights during the forward and backward propagations (steps 1 and 2) but not during the parameter update (step 3), as
illustrated in Algorithm 1. Keeping good precision weights during the updates is necessary for SGD
to work at all. These parameter changes are tiny by virtue of being obtained by gradient descent, i.e.,
SGD performs a large number of almost infinitesimal changes in the direction that most improves
the training objective (plus noise). One way to picture all this is to hypothesize that what matters
3

most at the end of training is the sign of the weights, w? , but that in order to figure it out, we perform
a lot of small changes to a continuous-valued quantity w, and only at the end consider its sign:
X
w? = sign(
gt )
(4)
t
t?1 ,bt?1 ),yt )
where gt is a noisy estimator of ?C(f (xt ,w
, where C(f (xt , wt?1 , bt?1 ), yt ) is the value
?wt?1
of the objective function on (input,target) example (xt , yt ), when wt?1 are the previous weights and
w? is its final discretized value of the weights.

Another way to conceive of this discretization is as a form of corruption, and hence as a regularizer,
and our empirical results confirm this hypothesis. In addition, we can make the discretization errors
on different weights approximately cancel each other while keeping a lot of precision by randomizing
the discretization appropriately. We propose a form of randomized discretization that preserves the
expected value of the discretized weight.
Hence, at training time, BinaryConnect randomly picks one of two values for each weight, for each
minibatch, for both the forward and backward propagation phases of backprop. However, the SGD
update is accumulated in a real-valued variable storing the parameter.
An interesting analogy to understand BinaryConnect is the DropConnect algorithm [21]. Just like
BinaryConnect, DropConnect only injects noise to the weights during the propagations. Whereas
DropConnect?s noise is added Gaussian noise, BinaryConnect?s noise is a binary sampling process.
In both cases the corrupted value has as expected value the clean original value.
2.4

Clipping

Since the binarization operation is not influenced by variations of the real-valued weights w when its
magnitude is beyond the binary values ?1, and since it is a common practice to bound weights (usually the weight vector) in order to regularize them, we have chosen to clip the real-valued weights
within the [?1, 1] interval right after the weight updates, as per Algorithm 1. The real-valued weights
would otherwise grow very large without any impact on the binary weights.
2.5

A few more tricks
Optimization

No learning rate scaling

Learning rate scaling

SGD
Nesterov momentum
ADAM

15.65%
12.81%

11.45%
11.30%
10.47%

Table 1: Test error rates of a (small) CNN trained on CIFAR-10 depending on optimization method
and on whether the learning rate is scaled with the weights initialization coefficients from [25].
We use Batch Normalization (BN) [26] in all of our experiments, not only because it accelerates
the training by reducing internal covariate shift, but also because it reduces the overall impact of
the weights scale. Moreover, we use the ADAM learning rule [27] in all of our CNN experiments.
Last but not least, we scale the weights learning rates respectively with the weights initialization
coefficients from [25] when optimizing with ADAM, and with the squares of those coefficients
when optimizing with SGD or Nesterov momentum [28]. Table 1 illustrates the effectiveness of
those tricks.
2.6

Test-Time Inference

Up to now we have introduced different ways of training a DNN with on-the-fly weight binarization.
What are reasonable ways of using such a trained network, i.e., performing test-time inference on
new examples? We have considered three reasonable alternatives:
1. Use the resulting binary weights wb (this makes most sense with the deterministic form of
BinaryConnect).
4

2. Use the real-valued weights w, i.e., the binarization only helps to achieve faster training but
not faster test-time performance.
3. In the stochastic case, many different networks can be sampled by sampling a wb for each
weight according to Eq. 2. The ensemble output of these networks can then be obtained by
averaging the outputs from individual networks.
We use the first method with the deterministic form of BinaryConnect. As for the stochastic form
of BinaryConnect, we focused on the training advantage and used the second method in the experiments, i.e., test-time inference using the real-valued weights. This follows the practice of Dropout
methods, where at test-time the ?noise? is removed.
Method

MNIST

CIFAR-10

SVHN

No regularizer
BinaryConnect (det.)
BinaryConnect (stoch.)
50% Dropout

1.30 ? 0.04%
1.29 ? 0.08%
1.18 ? 0.04%
1.01 ? 0.04%

10.64%
9.90%
8.27%

2.44%
2.30%
2.15%

Maxout Networks [29]
Deep L2-SVM [30]
Network in Network [31]
DropConnect [21]
Deeply-Supervised Nets [32]

0.94%
0.87%

11.68%

2.47%

10.41%

2.35%
1.94%
1.92%

9.78%

Table 2: Test error rates of DNNs trained on the MNIST (no convolution and no unsupervised
pretraining), CIFAR-10 (no data augmentation) and SVHN, depending on the method. We see
that in spite of using only a single bit per weight during propagation, performance is not worse
than ordinary (no regularizer) DNNs, it is actually better, especially with the stochastic version,
suggesting that BinaryConnect acts as a regularizer.

Figure 1: Features of the first layer of an MLP trained on MNIST depending on the regularizer. From left to right: no regularizer, deterministic BinaryConnect, stochastic BinaryConnect
and Dropout.

3

Benchmark results

In this section, we show that BinaryConnect acts as regularizer and we obtain near state-of-the-art
results with BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN.
3.1

Permutation-invariant MNIST

MNIST is a benchmark image classification dataset [33]. It consists in a training set of 60000 and
a test set of 10000 28 ? 28 gray-scale images representing digits ranging from 0 to 9. Permutationinvariance means that the model must be unaware of the image (2-D) structure of the data (in other
words, CNNs are forbidden). Besides, we do not use any data-augmentation, preprocessing or unsupervised pretraining. The MLP we train on MNIST consists in 3 hidden layers of 1024 Rectifier
Linear Units (ReLU) [34, 24, 3] and a L2-SVM output layer (L2-SVM has been shown to perform
better than Softmax on several classification benchmarks [30, 32]). The square hinge loss is minimized with SGD without momentum. We use an exponentially decaying learning rate. We use Batch
5

Figure 2: Histogram of the weights of the first layer of an MLP trained on MNIST depending on
the regularizer. In both cases, it seems that the weights are trying to become deterministic to reduce
the training error. It also seems that some of the weights of deterministic BinaryConnect are stuck
around 0, hesitating between ?1 and 1.

Figure 3: Training curves of a CNN on CIFAR-10 depending on the regularizer. The dotted lines
represent the training costs (square hinge losses) and the continuous lines the corresponding validation error rates. Both versions of BinaryConnect significantly augment the training cost, slow down
the training and lower the validation error rate, which is what we would expect from a Dropout
scheme.
Normalization with a minibatch of size 200 to speed up the training. As typically done, we use the
last 10000 samples of the training set as a validation set for early stopping and model selection. We
report the test error rate associated with the best validation error rate after 1000 epochs (we do not
retrain on the validation set). We repeat each experiment 6 times with different initializations. The
results are in Table 2. They suggest that the stochastic version of BinaryConnect can be considered
a regularizer, although a slightly less powerful one than Dropout, in this context.
3.2

CIFAR-10

CIFAR-10 is a benchmark image classification dataset. It consists in a training set of 50000 and
a test set of 10000 32 ? 32 color images representing airplanes, automobiles, birds, cats, deers,
dogs, frogs, horses, ships and trucks. We preprocess the data using global contrast normalization
and ZCA whitening. We do not use any data-augmentation (which can really be a game changer for
this dataset [35]). The architecture of our CNN is:
(2?128C3)?M P 2?(2?256C3)?M P 2?(2?512C3)?M P 2?(2?1024F C)?10SV M (5)
Where C3 is a 3 ? 3 ReLU convolution layer, M P 2 is a 2 ? 2 max-pooling layer, F C a fully
connected layer, and SVM a L2-SVM output layer. This architecture is greatly inspired from VGG
[36]. The square hinge loss is minimized with ADAM. We use an exponentially decaying learning
6

rate. We use Batch Normalization with a minibatch of size 50 to speed up the training. We use the
last 5000 samples of the training set as a validation set. We report the test error rate associated with
the best validation error rate after 500 training epochs (we do not retrain on the validation set). The
results are in Table 2 and Figure 3.
3.3

SVHN

SVHN is a benchmark image classification dataset. It consists in a training set of 604K and a test set
of 26K 32 ? 32 color images representing digits ranging from 0 to 9. We follow the same procedure
that we used for CIFAR-10, with a few notable exceptions: we use half the number of hidden units
and we train for 200 epochs instead of 500 (because SVHN is quite a big dataset). The results are in
Table 2.

4

Related works

Training DNNs with binary weights has been the subject of very recent works [37, 38, 39, 40]. Even
though we share the same objective, our approaches are quite different. [37, 38] do not train their
DNN with Backpropagation (BP) but with a variant called Expectation Backpropagation (EBP).
EBP is based on Expectation Propagation (EP) [41], which is a variational Bayes method used to do
inference in probabilistic graphical models. Let us compare their method to ours:
? It optimizes the weights posterior distribution (which is not binary). In this regard, our
method is quite similar as we keep a real-valued version of the weights.
? It binarizes both the neurons outputs and weights, which is more hardware friendly than
just binarizing the weights.
? It yields a good classification accuracy for fully connected networks (on MNIST) but not
(yet) for ConvNets.
[39, 40] retrain neural networks with ternary weights during forward and backward propagations,
i.e.:
? They train a neural network with high-precision,
? After training, they ternarize the weights to three possible values ?H, 0 and +H and adjust
H to minimize the output error,
? And eventually, they retrain with ternary weights during propagations and high-precision
weights during updates.
By comparison, we train all the way with binary weights during propagations, i.e., our training procedure could be implemented with efficient specialized hardware avoiding the forward and backward
propagations multiplications, which amounts to about 2/3 of the multiplications (cf. Algorithm 1).

5

Conclusion and future works

We have introduced a novel binarization scheme for weights during forward and backward propagations called BinaryConnect. We have shown that it is possible to train DNNs with BinaryConnect on
the permutation invariant MNIST, CIFAR-10 and SVHN datasets and achieve nearly state-of-the-art
results. The impact of such a method on specialized hardware implementations of deep networks
could be major, by removing the need for about 2/3 of the multiplications, and thus potentially allowing to speed-up by a factor of 3 at training time. With the deterministic version of BinaryConnect
the impact at test time could be even more important, getting rid of the multiplications altogether
and reducing by a factor of at least 16 (from 16 bits single-float precision to single bit precision)
the memory requirement of deep networks, which has an impact on the memory to computation
bandwidth and on the size of the models that can be run. Future works should extend those results to
other models and datasets, and explore getting rid of the multiplications altogether during training,
by removing their need from the weight update computation.
7

6

Acknowledgments

We thank the reviewers for their many constructive comments. We also thank Roland Memisevic for
helpful discussions. We thank the developers of Theano [42, 43], a Python library which allowed
us to easily develop a fast and optimized code for GPU. We also thank the developers of Pylearn2
[44] and Lasagne, two Deep Learning libraries built on the top of Theano. We are also grateful for
funding from NSERC, the Canada Research Chairs, Compute Canada, and CIFAR.

References
[1] Geoffrey Hinton, Li Deng, George E. Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior,
Vincent Vanhoucke, Patrick Nguyen, Tara Sainath, and Brian Kingsbury. Deep neural networks for acoustic modeling in speech recognition. IEEE Signal Processing Magazine, 29(6):82?97, Nov. 2012.
[2] Tara Sainath, Abdel rahman Mohamed, Brian Kingsbury, and Bhuvana Ramabhadran. Deep convolutional
neural networks for LVCSR. In ICASSP 2013, 2013.
[3] A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet classification with deep convolutional neural
networks. In NIPS?2012. 2012.
[4] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru
Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. Technical report,
arXiv:1409.4842, 2014.
[5] Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul.
Fast and robust neural network joint models for statistical machine translation. In Proc. ACL?2014, 2014.
[6] Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. In
NIPS?2014, 2014.
[7] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning
to align and translate. In ICLR?2015, arXiv:1409.0473, 2015.
[8] Rajat Raina, Anand Madhavan, and Andrew Y. Ng. Large-scale deep unsupervised learning using graphics
processors. In ICML?2009, 2009.
[9] Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic language
model. Journal of Machine Learning Research, 3:1137?1155, 2003.
[10] J. Dean, G.S Corrado, R. Monga, K. Chen, M. Devin, Q.V. Le, M.Z. Mao, M.A. Ranzato, A. Senior,
P. Tucker, K. Yang, and A. Y. Ng. Large scale distributed deep networks. In NIPS?2012, 2012.
[11] Sang Kyun Kim, Lawrence C McAfee, Peter Leonard McMahon, and Kunle Olukotun. A highly scalable
restricted Boltzmann machine FPGA implementation. In Field Programmable Logic and Applications,
2009. FPL 2009. International Conference on, pages 367?372. IEEE, 2009.
[12] Tianshi Chen, Zidong Du, Ninghui Sun, Jia Wang, Chengyong Wu, Yunji Chen, and Olivier Temam.
Diannao: A small-footprint high-throughput accelerator for ubiquitous machine-learning. In Proceedings
of the 19th international conference on Architectural support for programming languages and operating
systems, pages 269?284. ACM, 2014.
[13] Yunji Chen, Tao Luo, Shaoli Liu, Shijin Zhang, Liqiang He, Jia Wang, Ling Li, Tianshi Chen, Zhiwei
Xu, Ninghui Sun, et al. Dadiannao: A machine-learning supercomputer. In Microarchitecture (MICRO),
2014 47th Annual IEEE/ACM International Symposium on, pages 609?622. IEEE, 2014.
[14] Lorenz K Muller and Giacomo Indiveri. Rounding methods for neural networks with low resolution
synaptic weights. arXiv preprint arXiv:1504.05767, 2015.
[15] Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with limited numerical precision. In ICML?2015, 2015.
[16] Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Low precision arithmetic for deep learning. In Arxiv:1412.7024, ICLR?2015 Workshop, 2015.
[17] Thomas M Bartol, Cailey Bromer, Justin P Kinney, Michael A Chirillo, Jennifer N Bourne, Kristen M
Harris, and Terrence J Sejnowski. Hippocampal spine head sizes are highly precise. bioRxiv, 2015.
[18] Alex Graves. Practical variational inference for neural networks. In J. Shawe-Taylor, R.S. Zemel, P.L.
Bartlett, F. Pereira, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems
24, pages 2348?2356. Curran Associates, Inc., 2011.
[19] Nitish Srivastava. Improving neural networks with dropout. Master?s thesis, U. Toronto, 2013.
[20] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout:
A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research,
15:1929?1958, 2014.

8

[21] Li Wan, Matthew Zeiler, Sixin Zhang, Yann LeCun, and Rob Fergus. Regularization of neural networks
using dropconnect. In ICML?2013, 2013.
[22] J.P. David, K. Kalach, and N. Tittley. Hardware complexity of modular multiplication and exponentiation.
Computers, IEEE Transactions on, 56(10):1308?1319, Oct 2007.
[23] R. Collobert. Large Scale Machine Learning. PhD thesis, Universit?e de Paris VI, LIP6, 2004.
[24] X. Glorot, A. Bordes, and Y. Bengio. Deep sparse rectifier neural networks. In AISTATS?2011, 2011.
[25] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In AISTATS?2010, 2010.
[26] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. 2015.
[27] Diederik Kingma and Jimmy Ba.
arXiv:1412.6980, 2014.

Adam: A method for stochastic optimization.

arXiv preprint

[28] Yu Nesterov. A method for unconstrained convex minimization problem with the rate of convergence
o(1/k2 ). Doklady AN SSSR (translated as Soviet. Math. Docl.), 269:543?547, 1983.
[29] Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout
networks. Technical Report Arxiv report 1302.4389, Universit?e de Montr?eal, February 2013.
[30] Yichuan Tang. Deep learning using linear support vector machines. Workshop on Challenges in Representation Learning, ICML, 2013.
[31] Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400, 2013.
[32] Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen Tu. Deeply-supervised
nets. arXiv preprint arXiv:1409.5185, 2014.
[33] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278?2324, November 1998.
[34] V. Nair and G.E. Hinton. Rectified linear units improve restricted Boltzmann machines. In ICML?2010,
2010.
[35] Benjamin Graham. Spatially-sparse convolutional neural networks. arXiv preprint arXiv:1409.6070,
2014.
[36] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.
[37] Daniel Soudry, Itay Hubara, and Ron Meir. Expectation backpropagation: Parameter-free training of
multilayer neural networks with continuous or discrete weights. In NIPS?2014, 2014.
[38] Zhiyong Cheng, Daniel Soudry, Zexi Mao, and Zhenzhong Lan. Training binary multilayer neural networks for image classification using expectation backpropgation. arXiv preprint arXiv:1503.03562, 2015.
[39] Kyuyeon Hwang and Wonyong Sung. Fixed-point feedforward deep neural network design using
weights+ 1, 0, and- 1. In Signal Processing Systems (SiPS), 2014 IEEE Workshop on, pages 1?6. IEEE,
2014.
[40] Jonghong Kim, Kyuyeon Hwang, and Wonyong Sung. X1000 real-time phoneme recognition vlsi using
feed-forward deep neural networks. In Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE
International Conference on, pages 7510?7514. IEEE, 2014.
[41] Thomas P Minka. Expectation propagation for approximate bayesian inference. In UAI?2001, 2001.
[42] James Bergstra, Olivier Breuleux, Fr?ed?eric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientific Computing Conference (SciPy), June 2010.
Oral Presentation.
[43] Fr?ed?eric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian J. Goodfellow, Arnaud Bergeron,
Nicolas Bouchard, and Yoshua Bengio. Theano: new features and speed improvements. Deep Learning
and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.
[44] Ian J. Goodfellow, David Warde-Farley, Pascal Lamblin, Vincent Dumoulin, Mehdi Mirza, Razvan Pascanu, James Bergstra, Fr?ed?eric Bastien, and Yoshua Bengio. Pylearn2: a machine learning research
library. arXiv preprint arXiv:1308.4214, 2013.

9


----------------------------------------------------------------

title: 6573-binarized-neural-networks.pdf

Binarized Neural Networks
Itay Hubara1 *
itayh@technion.ac.il

Matthieu Courbariaux2 *
matthieu.courbariaux@gmail.com

Ran El-Yaniv1
rani@cs.technion.ac.il

Daniel Soudry3
daniel.soudry@gmail.com

Yoshua Bengio2,4
yoshua.umontreal@gmail.com

(1) Technion, Israel Institute of Technology.
(3) Columbia University.
(*) Indicates equal contribution.

(2) Universit? de Montr?al.
(4) CIFAR Senior Fellow.

Abstract
We introduce a method to train Binarized Neural Networks (BNNs) - neural
networks with binary weights and activations at run-time. At train-time the binary
weights and activations are used for computing the parameter gradients. During the
forward pass, BNNs drastically reduce memory size and accesses, and replace most
arithmetic operations with bit-wise operations, which is expected to substantially
improve power-efficiency. To validate the effectiveness of BNNs, we conducted
two sets of experiments on the Torch7 and Theano frameworks. On both, BNNs
achieved nearly state-of-the-art results over the MNIST, CIFAR-10 and SVHN
datasets. We also report our preliminary results on the challenging ImageNet
dataset. Last but not least, we wrote a binary matrix multiplication GPU kernel
with which it is possible to run our MNIST BNN 7 times faster than with an
unoptimized GPU kernel, without suffering any loss in classification accuracy. The
code for training and running our BNNs is available on-line.

Introduction
Deep Neural Networks (DNNs) have substantially pushed Artificial Intelligence (AI) limits in a wide
range of tasks (LeCun et al., 2015). Today, DNNs are almost exclusively trained on one or many very
fast and power-hungry Graphic Processing Units (GPUs) (Coates et al., 2013). As a result, it is often
a challenge to run DNNs on target low-power devices, and substantial research efforts are invested in
speeding up DNNs at run-time on both general-purpose (Gong et al., 2014; Han et al., 2015b) and
specialized computer hardware (Chen et al., 2014; Esser et al., 2015).
This paper makes the following contributions:
? We introduce a method to train Binarized-Neural-Networks (BNNs), neural networks with binary
weights and activations, at run-time, and when computing the parameter gradients at train-time
(see Section 1).
? We conduct two sets of experiments, each implemented on a different framework, namely Torch7
and Theano, which show that it is possible to train BNNs on MNIST, CIFAR-10 and SVHN and
achieve near state-of-the-art results (see Section 2). Moreover, we report preliminary results on the
challenging ImageNet dataset
? We show that during the forward pass (both at run-time and train-time), BNNs drastically reduce
memory consumption (size and number of accesses), and replace most arithmetic operations with
bit-wise operations, which potentially lead to a substantial increase in power-efficiency (see Section
30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

3). Moreover, a binarized CNN can lead to binary convolution kernel repetitions; we argue that
dedicated hardware could reduce the time complexity by 60% .
? Last but not least, we programed a binary matrix multiplication GPU kernel with which it is
possible to run our MNIST BNN 7 times faster than with an unoptimized GPU kernel, without
suffering any loss in classification accuracy (see Section 4).
The code for training and running our BNNs is available on-line (both Theano1 and Torch framework2 ).

1

Binarized Neural Networks

In this section, we detail our binarization function, show how we use it to compute the parameter
gradients,and how we backpropagate through it.
Deterministic vs Stochastic Binarization When training a BNN, we constrain both the weights
and the activations to either +1 or ?1. Those two values are very advantageous from a hardware
perspective, as we explain in Section 4. In order to transform the real-valued variables into those
two values, we use two different binarization functions, as in (Courbariaux et al., 2015). Our first
binarization function is deterministic:

+1 if x ? 0,
xb = Sign(x) =
(1)
?1 otherwise,
where xb is the binarized variable (weight or activation) and x the real-valued variable. It is very
straightforward to implement and works quite well in practice. Our second binarization function is
stochastic:

+1 with probability p = ?(x),
b
x =
(2)
?1 with probability 1 ? p,
where ? is the ?hard sigmoid? function:
x+1
x+1
, 0, 1) = max(0, min(1,
)).
(3)
2
2
The stochastic binarization is more appealing than the sign function, but harder to implement as
it requires the hardware to generate random bits when quantizing. As a result, we mostly use the
deterministic binarization function (i.e., the sign function), with the exception of activations at
train-time in some of our experiments.
?(x) = clip(

Gradient Computation and Accumulation Although our BNN training method uses binary
weights and activation to compute the parameter gradients, the real-valued gradients of the weights
are accumulated in real-valued variables, as per Algorithm 1. Real-valued weights are likely required
for Stochasic Gradient Descent (SGD) to work at all. SGD explores the space of parameters in small
and noisy steps, and that noise is averaged out by the stochastic gradient contributions accumulated
in each weight. Therefore, it is important to maintain sufficient resolution for these accumulators,
which at first glance suggests that high precision is absolutely required.
Moreover, adding noise to weights and activations when computing the parameter gradients provide
a form of regularization that can help to generalize better, as previously shown with variational
weight noise (Graves, 2011), Dropout (Srivastava et al., 2014) and DropConnect (Wan et al., 2013).
Our method of training BNNs can be seen as a variant of Dropout, in which instead of randomly
setting half of the activations to zero when computing the parameter gradients, we binarize both the
activations and the weights.
Propagating Gradients Through Discretization The derivative of the sign function is zero almost
everywhere, making it apparently incompatible with back-propagation, since the exact gradient of
the cost with respect to the quantities before the discretization (pre-activations or weights) would
1
2

https://github.com/MatthieuCourbariaux/BinaryNet
https://github.com/itayhubara/BinaryNet

2

be zero. Note that this remains true even if stochastic quantization is used. Bengio (2013) studied
the question of estimating or propagating gradients through stochastic discrete neurons. He found in
his experiments that the fastest training was obtained when using the ?straight-through estimator,?
previously introduced in Hinton?s lectures (Hinton, 2012). We follow a similar approach but use the
version of the straight-through estimator that takes into account the saturation effect, and does use
deterministic rather than stochastic sampling of the bit. Consider the sign function quantization
q = Sign(r),
and assume that an estimator gq of the gradient ?C
?q has been obtained (with the straight-through
estimator when needed).
Algorithm 1: Training a BNN. C is the cost function Algorithm 2: Shift based AdaMax learning
for minibatch, ? the learning rate decay factor and L rule (Kingma & Ba, 2014). gt2 indicates the
the number of layers. ? indicates element-wise mul- element-wise square gt ?gt and  stands for
tiplication. The function Binarize() specifies how to both left and right bit-shift. Good default
(stochastically or deterministically) binarize the activa- settings are ? = 2?10 , 1 ? ?1 = 2?3 , 1 ?
tions and weights, and Clip() specifies how to clip the ?2 = 2?10 . All operations on vectors are
weights. BatchNorm() specifies how to batch-normalize element-wise. With ?1t and ?2t we denote
the activations, using either batch normalization (Ioffe & ?1 and ?2 to the power t.
Szegedy, 2015) or its shift-based variant we describe in
Algorithm 3. BackBatchNorm() specifies how to back- Require: Previous parameters ?t?1 and
their gradient gt , and learning rate ?.
propagate through the normalization. Update() specifies
Ensure:
Updated parameters ?t .
how to update the parameters when their gradients are
{Biased 1st and 2nd moment estimates:}
known, using either ADAM (Kingma & Ba, 2014) or
the shift-based AdaMax we describe in Algorithm 2.
mt ? ?1 ? mt?1 + (1 ? ?1 ) ? gt
vt ? max(?2 ? vt?1 , |gt |)
Require: a minibatch of inputs and targets (a0 , a? ),
{Updated parameters:}
previous weights W , previous BatchNorm parameters ?, weight initialization coefficients from (Glorot
?t ? ?t?1 ? (?  (1 ? ?1 )) ? m
?  vt?1 )
& Bengio, 2010) ?, and previous learning rate ?.
Ensure: updated weights W t+1 , updated BatchNorm
parameters ?t+1 and updated learning rate ? t+1 .
Algorithm 3: Shift based Batch Normaliz{1. Computing the gradients:}
ing Transform, applied to activation x over
{1.1. Forward propagation:}
a mini-batch. The approximate power-offor k = 1 to L do
2 is3 AP 2(x) = sign(x)2round(log2|x|) , and
Wkb ? Binarize(Wk ), sk ? abk?1 Wkb
 stands for both left and right binary shift.
ak ? BatchNorm(sk , ?k )
Require: Values of x over a mini-batch:
if k < L then abk ? Binarize(ak )
B = {x1...m }; parameters to learn: ?, ?.
{1.2. Backward propagation:}
Ensure:
{yi = BN(xi ,?, ?)}
{Please note that the gradients are not binary.}
{1.
Mini-batch
?C
?
Pm mean:}
Compute gaL = ?aL knowing aL and a
1
?B ? m
i=1 xi
for k = L to 1 do
{2. Centered input: }
if k < L then gak ? gabk ? 1|ak |?1
C(xi ) ? (xi ? ?B )
(gsk , g?k ) ? BackBatchNorm(gak , sk , ?k )
{3. Approximate
variance:}
Pm
1
2
?B
?m
gabk?1 ? gsk Wkb , gWkb ? gs>k abk?1
i=1(C(xi )AP 2(C(xi )))
{4. Normalize:}
{2. Accumulating the gradients:}
p
2 + )?1 )
x?i ? C(xi )  AP 2(( ?B
for k = 1 to L do
t+1
t
t+1
t
{5.
Scale
and
shift:}
?k ? Update(?k , ? , g?k ), ?
? ??
yi ? AP 2(?)  x?i
Wkt+1 ? Clip(Update(Wk , ?k ? t , gWkb ), ?1, 1)
Then, our straight-through estimator of

?C
?r

is simply
gr = gq 1|r|?1 .
(4)
Note that this preserves the gradient?s information and cancels the gradient when r is too large.
Not cancelling the gradient when r is too large significantly worsens the performance. The use of
this straight-through estimator is illustrated in Algorithm 1. The derivative 1|r|?1 can also be seen
as propagating the gradient through hard tanh, which is the following piece-wise linear activation
function:
Htanh(x) = Clip(x, ?1, 1).
(5)
3

For hidden units, we use the sign function nonAlgorithm 4: Running a BNN. L = layers.
linearity to obtain binary activations, and for
Require: a vector of 8-bit inputs a0 , the binary
weights we combine two ingredients:
weights W b , and the BatchNorm parameters ?.
? Constrain each real-valued weight between -1 Ensure: the MLP output aL .
and 1, by projecting wr to -1 or 1 when the
{1. First layer:}
weight update brings wr outside of [?1, 1],
a1 ? 0
i.e., clipping the weights during training, as
for n = 1 to 8 do
per Algorithm 1. The real-valued weights
a1 ? a1 +2n?1 ?XnorDotProduct(an0 , W1b )
would otherwise grow very large without any
ab1 ? Sign(BatchNorm(a1 , ?1 ))
impact on the binary weights.
{2. Remaining hidden layers:}
for k = 2 to L ? 1 do
? When using a weight wr , quantize it using
ak ? XnorDotProduct(abk?1 , Wkb )
wb = Sign(wr ).
abk ? Sign(BatchNorm(ak , ?k ))
This is consistent with the gradient canceling
{3. Output layer:}
when |wr | > 1, according to Eq. 4.
aL ? XnorDotProduct(abL?1 , WLb )
aL ? BatchNorm(aL , ?L )
Shift-based Batch Normalization Batch
Normalization (BN) (Ioffe & Szegedy, 2015), accelerates the training and also seems to reduces
the overall impact of the weight scale. The normalization noise may also help to regularize the
model. However, at train-time, BN requires many multiplications (calculating the standard deviation
and dividing by it), namely, dividing by the running variance (the weighted mean of the training
set activation variance). Although the number of scaling calculations is the same as the number of
neurons, in the case of ConvNets this number is quite large. For example, in the CIFAR-10 dataset
(using our architecture), the first convolution layer, consisting of only 128 ? 3 ? 3 filter masks,
converts an image of size 3 ? 32 ? 32 to size 3 ? 128 ? 28 ? 28, which is two orders of magnitude
larger than the number of weights. To achieve the results that BN would obtain, we use a shift-based
batch normalization (SBN) technique. detailed in Algorithm 3. SBN approximates BN almost
without multiplications. In the experiment we conducted we did not observe accuracy loss when
using the shift based BN algorithm instead of the vanilla BN algorithm.
Shift based AdaMax The ADAM learning rule (Kingma & Ba, 2014) also seems to reduce the
impact of the weight scale. Since ADAM requires many multiplications, we suggest using instead the
shift-based AdaMax we detail in Algorithm 2. In the experiment we conducted we did not observe
accuracy loss when using the shift-based AdaMax algorithm instead of the vanilla ADAM algorithm.
First Layer In a BNN, only the binarized values of the weights and activations are used in all
calculations. As the output of one layer is the input of the next, all the layers inputs are binary,
with the exception of the first layer. However, we do not believe this to be a major issue. First, in
computer vision, the input representation typically has far fewer channels (e.g, red, green and blue)
than internal representations (e.g, 512). As a result, the first layer of a ConvNet is often the smallest
convolution layer, both in terms of parameters and computations (Szegedy et al., 2014). Second, it is
relatively easy to handle continuous-valued inputs as fixed point numbers, with m bits of precision.
For example, in the common case of 8-bit fixed point inputs:
s = x ? wb

;

s=

8
X

2n?1 (xn ? wb ),

(6)

n=1

where x is a vector of 1024 8-bit inputs, x81 is the most significant bit of the first input, wb is a vector
of 1024 1-bit weights, and s is the resulting weighted sum. This trick is used in Algorithm 4.

2

Benchmark Results

We conduct two sets of experiments, each based on a different framework, namely Torch7 and Theano.
Implementation details are reported in Appendix A and code for both frameworks is available online.
Results are reported in Table 1.
3

Hardware implementation of AP2 is as simple as extracting the index of the most significant bit from the
number?s binary representation.

4

Table 1: Classification test error rates of DNNs trained on MNIST (fully connected architecture),
CIFAR-10 and SVHN (convnet). No unsupervised pre-training or data augmentation was used.
Data set

MNIST
SVHN
Binarized activations+weights, during training and test
BNN (Torch7)
1.40%
2.53%
BNN (Theano)
0.96%
2.80%
Committee Machines? Array (Baldassi et al., 2015)
1.35%
Binarized weights, during training and test
BinaryConnect (Courbariaux et al., 2015)
1.29? 0.08% 2.30%
Binarized activations+weights, during test
EBP (Cheng et al., 2015)
2.2? 0.1%
Bitwise DNNs (Kim & Smaragdis, 2016)
1.33%
Ternary weights, binary activations, during test
(Hwang & Sung, 2014)
1.45%
No binarization (standard results)
No regularization
1.3? 0.2%
2.44%
Gated pooling (Lee et al., 2015)
1.69%

CIFAR-10
10.15%
11.40%
9.90%
10.94%
7.62%

Preliminary Results on ImageNet To Figure 1: Training curves for different methods on
test the strength of our method, we applied CIFAR-10 dataset. The dotted lines represent the trainit to the challenging ImageNet classifica- ing costs (square hinge losses) and the continuous lines
tion task. Considerable research has been the corresponding validation error rates. Although
concerned with compressing ImageNet ar- BNNs are slower to train, they are nearly as accurate as
chitectures while preserving high accuracy 32-bit float DNNs.
performance (e.g., Han et al. (2015a)). Previous approaches that have been tried include pruning near zero weights using matrix factorization techniques, quantizing
the weights and applying Huffman codes
among others. To the best of the our knowledge, so far there are no reports on successfully quantizing the network?s activations.
Moreover, a recent work Han et al. (2015a)
showed that accuracy significantly deteriorates when trying to quantize convolutional
layers? weights below 4 bits (FC layers are
more robust to quantization and can operate
quite well with only 2 bits). In the present
work we attempted to tackle the difficult task of binarizing both weights and activations. Employing
the well known AlexNet and GoogleNet architectures, we applied our techniques and achieved
36.1% top-1 and 60.1% top-5 accuracies using AlexNet and 47.1% top-1 and 69.1% top-5 accuracies
using GoogleNet. While this performance leaves room for improvement (relative to full precision
nets), they are by far better than all previous attempts to compress ImageNet architectures using less
than 4 bits precision for the weights. Moreover, this advantage is achieved while also binarizing
neuron activations. Detailed descriptions of these results as well as full implementation details
of our experiments are reported in the supplementary material (Appendix B). In our latest work
(Hubara et al., 2016) we relaxed the binary constrains and allowed more than 1-bit per weight and
activations. The resulting QNNs achieve prediction accuracy comparable to their 32-bit counterparts.
For example, our quantized version of AlexNet with 1-bit weights and 2-bit activations achieves
51% top-1 accuracy and GoogleNet with 4-bits weighs and activation achived 66.6%. Moreover, we
quantize the parameter gradients to 6-bits as well which enables gradients computation using only
bit-wise operation. Full details can be found in (Hubara et al., 2016)
5

Table 2: Energy consumption of multiplyaccumulations in pico-joules (Horowitz, 2014)
Operation
MUL ADD
8bit Integer
0.2pJ 0.03pJ
32bit Integer
3.1pJ
0.1pJ
16bit Floating Point 1.1pJ
0.4pJ
32tbit Floating Point 3.7pJ
0.9pJ

3

Table 3: Energy consumption of memory accesses
in pico-joules (Horowitz, 2014)
Memory size 64-bit memory access
8K
10pJ
32K
20pJ
1M
100pJ
DRAM
1.3-2.6nJ

High Power Efficiency during the Forward Pass

Computer hardware, be it general-purpose or specialized, is composed of memories, arithmetic
operators and control logic. During the forward pass (both at run-time and train-time), BNNs
drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise
operations, which might lead to a great increase in power-efficiency. Moreover, a binarized CNN can
lead to binary convolution kernel repetitions, and we argue that dedicated hardware could reduce the
time complexity by 60% .
Memory Size and Accesses Improving computing performance has always been and remains a
challenge. Over the last decade, power has been the main constraint on performance (Horowitz, 2014).
This is why much research effort has been devoted to reducing the energy consumption of neural
networks. Horowitz (2014) provides rough numbers for the energy consumed by the computation (the
given numbers are for 45nm technology), as summarized in Tables 2 and 3. Importantly, we can see
that memory accesses typically consume more energy than arithmetic operations, and memory access
cost augments with memory size. In comparison with 32-bit DNNs, BNNs require 32 times smaller
memory size and 32 times fewer memory accesses. This is expected to reduce energy consumption
drastically (i.e., more than 32 times).
XNOR-Count Applying a DNN mainly consists of convolutions and matrix multiplications. The
key arithmetic operation of deep learning is thus the multiply-accumulate operation. Artificial neurons
are basically multiply-accumulators computing weighted sums of their inputs. In BNNs, both the
activations and the weights are constrained to either ?1 or +1. As a result, most of the 32-bit floating
point multiply-accumulations are replaced by 1-bit XNOR-count operations. This could have a big
impact on dedicated deep learning hardware. For instance, a 32-bit floating point multiplier costs
about 200 Xilinx FPGA slices (Govindu et al., 2004; Beauchamp et al., 2006), whereas a 1-bit XNOR
gate only costs a single slice.
Exploiting Filter Repetitions When using a ConvNet architecture with binary weights, the number
of unique filters is bounded by the filter size. For example, in our implementation we use filters of
size 3 ? 3, so the maximum number of unique 2D filters is 29 = 512. Since we now have binary
filters, many 2D filters of size k ? k repeat themselves. By using dedicated hardware/software, we
can apply only the unique 2D filters on each feature map and sum the results to receive each 3D
filter?s convolutional result. For example, in our ConvNet architecture trained on the CIFAR-10
benchmark, there are only 42% unique filters per layer on average. Hence we can reduce the number
of the XNOR-popcount operations by 3.

4

Seven Times Faster on GPU at Run-Time

It is possible to speed up GPU implementations of BNNs, by using a method sometimes called
SIMD (single instruction, multiple data) within a register (SWAR). The basic idea of SWAR is to
concatenate groups of 32 binary variables into 32-bit registers, and thus obtain a 32-times speed-up
on bitwise operations (e.g, XNOR). Using SWAR, it is possible to evaluate 32 connections with only
3 instructions:
32b
a1 + = popcount(xnor(a32b
0 , w1 )),

(7)

where a1 is the resulting weighted sum, and a32b
and w132b are the concatenated inputs and weights.
0
Those 3 instructions (accumulation, popcount, xnor) take 1 + 4 + 1 = 6 clock cycles on recent
6

Nvidia GPUs (and if they were to become a fused instruction, it would only take a single clock cycle).
Consequently, we obtain a theoretical Nvidia GPU speed-up of factor of 32/6 ? 5.3. In practice, this
speed-up is quite easy to obtain as the memory bandwidth to computation ratio is also increased by 6
times.
In order to validate those theoretical results, we
programed two GPU kernels:

Figure 2: The first three columns represent the
time it takes to perform a 8192 ? 8192 ? 8192 (bi? The first kernel (baseline) is an unoptimized nary) matrix multiplication on a GTX750 Nvidia
matrix multiplication kernel.
GPU, depending on which kernel is used. We
? The second kernel (XNOR) is nearly identical can see that our XNOR kernel is 23 times faster
to the baseline kernel, except that it uses the than our baseline kernel and 3.4 times faster than
cuBLAS. The next three columns represent the
SWAR method, as in Equation (7).
time it takes to run the MLP from Section 2 on the
The two GPU kernels return identical outputs full MNIST test set. As MNIST?s images are not
when their inputs are constrained to ?1 or +1 binary, the first layer?s computations are always
(but not otherwise). The XNOR kernel is about performed by the baseline kernel. The last three
23 times faster than the baseline kernel and 3.4 columns show that the MLP accuracy does not
times faster than cuBLAS, as shown in Figure 2. depend on which kernel is used.
Last but not least, the MLP from Section 2 runs
7 times faster with the XNOR kernel than with
the baseline kernel, without suffering any loss
in classification accuracy (see Figure 2).

5

Discussion and Related Work

Until recently, the use of extremely lowprecision networks (binary in the extreme case)
was believed to be highly destructive to the network performance (Courbariaux et al., 2014).
Soudry et al. (2014) and Cheng et al. (2015)
proved the contrary by showing that good performance could be achieved even if all neurons
and weights are binarized to ?1 . This was done
using Expectation BackPropagation (EBP), a
variational Bayesian approach, which infers networks with binary weights and neurons by updating the posterior distributions over the weights.
These distributions are updated by differentiating their parameters (e.g., mean values) via the back
propagation (BP) algorithm. Esser et al. (2015) implemented a fully binary network at run time using
a very similar approach to EBP, showing significant improvement in energy efficiency. The drawback
of EBP is that the binarized parameters are only used during inference.
The probabilistic idea behind EBP was extended in the BinaryConnect algorithm of Courbariaux et al.
(2015). In BinaryConnect, the real-valued version of the weights is saved and used as a key reference
for the binarization process. The binarization noise is independent between different weights, either
by construction (by using stochastic quantization) or by assumption (a common simplification; see
Spang (1962). The noise would have little effect on the next neuron?s input because the input is
a summation over many weighted neurons. Thus, the real-valued version could be updated by the
back propagated error by simply ignoring the binarization noise in the update. Using this method,
Courbariaux et al. (2015) were the first to binarize weights in CNNs and achieved near state-of-the-art
performance on several datasets. They also argued that noisy weights provide a form of regularization,
which could help to improve generalization, as previously shown in (Wan et al., 2013). This method
binarized weights while still maintaining full precision neurons.
Lin et al. (2015) carried over the work of Courbariaux et al. (2015) to the back-propagation process
by quantizing the representations at each layer of the network, to convert some of the remaining
multiplications into bit-shifts by restricting the neurons values to be power-of-two integers. Lin et al.
(2015)?s work and ours seem to share similar characteristics . However, their approach continues to
use full precision weights during the test phase. Moreover, Lin et al. (2015) quantize the neurons
only during the back propagation process, and not during forward propagation.
7

Other research Baldassi et al. (2015) showed that full binary training and testing is possible in an
array of committee machines with randomized input, where only one weight layer is being adjusted.
Gong et al. (2014) aimed to compress a fully trained high precision network by using a quantization
or matrix factorization methods. These methods required training the network with full precision
weights and neurons, thus requiring numerous MAC operations the proposed BNN algorithm avoids.
Hwang & Sung (2014) focused on a fixed-point neural network design and achieved performance
almost identical to that of the floating-point architecture. Kim & Smaragdis (2016) retrained neural
networks with binary weights and activations.
So far, to the best of our knowledge, no work has succeeded in binarizing weights and neurons, at the
inference phase and the entire training phase of a deep network. This was achieved in the present
work. We relied on the idea that binarization can be done stochastically, or be approximated as
random noise. This was previously done for the weights by Courbariaux et al. (2015), but our BNNs
extend this to the activations. Note that the binary activations are especially important for ConvNets,
where there are typically many more neurons than free weights. This allows highly efficient operation
of the binarized DNN at run time, and at the forward-propagation phase during training. Moreover,
our training method has almost no multiplications, and therefore might be implemented efficiently
in dedicated hardware. However, we have to save the value of the full precision weights. This is a
remaining computational bottleneck during training, since it is an energy-consuming operation.

Conclusion
We have introduced BNNs, which binarize deep neural networks and can lead to dramatic improvements in both power consumption and computation speed. During the forward pass (both at run-time
and train-time), BNNs drastically reduce memory size and accesses, and replace most arithmetic
operations with bit-wise operations. Our estimates indicate that power efficiency can be improved by
more than one order of magnitude (see Section 3). In terms of speed, we programed a binary matrix
multiplication GPU kernel that enabled running MLP over the MNIST datset 7 times faster (than
with an unoptimized GPU kernel) without suffering any accuracy degradation (see Section 4).
We have shown that BNNs can handle MNIST, CIFAR-10 and SVHN while achieving nearly stateof-the-art accuracy performance. While our preliminary results for the challenging ImageNet are
not on par with the best results achievable with full precision networks, they significantly improve
all previous attempts to compress ImageNet-capable architectures (see Section 2 and supplementary
material - Appendix B). Moreover by relaxing the binary constrains and allowed more than 1-bit per
weight and activations we have been able to achieve prediction accuracy comparable to their 32-bit
counterparts. Full details can be found in our latest work (Hubara et al., 2016) A major open question
would be to further improve our results on ImageNet. A substantial progress in this direction might
lead to huge impact on DNN usability in low power instruments such as mobile phones.

Acknowledgments
We would like to express our appreciation to Elad Hoffer, for his technical assistance and constructive
comments. We thank our fellow MILA lab members who took the time to read the article and give us
some feedback. We thank the developers of Torch, Collobert et al. (2011) a Lua based environment,
and Theano (Bergstra et al., 2010; Bastien et al., 2012), a Python library which allowed us to easily
develop a fast and optimized code for GPU. We also thank the developers of Pylearn2 (Goodfellow
et al., 2013) and Lasagne (Dieleman et al., 2015), two Deep Learning libraries built on the top of
Theano. We thank Yuxin Wu for helping us compare our GPU kernels with cuBLAS. We are also
grateful for funding from NSERC, the Canada Research Chairs, Compute Canada, and CIFAR. We
are also grateful for funding from CIFAR, NSERC, IBM, Samsung. This research was also supported
by The Israel Science Foundation (grant No. 1890/14).

References
Baldassi, C., Ingrosso, A., Lucibello, C., Saglietti, L., and Zecchina, R. Subdominant Dense Clusters Allow for
Simple Learning and High Computational Performance in Neural Networks with Discrete Synapses. Physical
Review Letters, 115(12):1?5, 2015.

8

Bastien, F., Lamblin, P., Pascanu, R., et al. Theano: new features and speed improvements. Deep Learning and
Unsupervised Feature Learning NIPS 2012 Workshop, 2012.
Beauchamp, M. J., Hauck, S., Underwood, K. D., and Hemmert, K. S. Embedded floating-point units in FPGAs.
In Proceedings of the 2006 ACM/SIGDA 14th international symposium on Field programmable gate arrays,
pp. 12?20. ACM, 2006.
Bengio, Y. Estimating or propagating gradients through stochastic neurons. Technical Report arXiv:1305.2982,
Universite de Montreal, 2013.
Bergstra, J., Breuleux, O., Bastien, F., et al. Theano: a CPU and GPU math expression compiler. In Proceedings
of the Python for Scientific Computing Conference (SciPy), June 2010. Oral Presentation.
Chen, T., Du, Z., Sun, N., et al. Diannao: A small-footprint high-throughput accelerator for ubiquitous machinelearning. In Proceedings of the 19th international conference on Architectural support for programming
languages and operating systems, pp. 269?284. ACM, 2014.
Cheng, Z., Soudry, D., Mao, Z., and Lan, Z. Training binary multilayer neural networks for image classification
using expectation backpropgation. arXiv preprint arXiv:1503.03562, 2015.
Coates, A., Huval, B., Wang, T., et al. Deep learning with COTS HPC systems. In Proceedings of the 30th
international conference on machine learning, pp. 1337?1345, 2013.
Collobert, R., Kavukcuoglu, K., and Farabet, C. Torch7: A matlab-like environment for machine learning. In
BigLearn, NIPS Workshop, 2011.
Courbariaux, M., Bengio, Y., and David, J.-P. Training deep neural networks with low precision multiplications.
ArXiv e-prints, abs/1412.7024, December 2014.
Courbariaux, M., Bengio, Y., and David, J.-P. Binaryconnect: Training deep neural networks with binary weights
during propagations. ArXiv e-prints, abs/1511.00363, November 2015.
Dieleman, S., Schl?ter, J., Raffel, C., et al. Lasagne: First release., August 2015.
Esser, S. K., Appuswamy, R., Merolla, P., Arthur, J. V., and Modha, D. S. Backpropagation for energy-efficient
neuromorphic computing. In Advances in Neural Information Processing Systems, pp. 1117?1125, 2015.
Glorot, X. and Bengio, Y. Understanding the difficulty of training deep feedforward neural networks. In
AISTATS?2010, 2010.
Gong, Y., Liu, L., Yang, M., and Bourdev, L. Compressing deep convolutional networks using vector quantization.
arXiv preprint arXiv:1412.6115, 2014.
Goodfellow, I. J., Warde-Farley, D., Lamblin, P., et al. Pylearn2: a machine learning research library. arXiv
preprint arXiv:1308.4214, 2013.
Govindu, G., Zhuo, L., Choi, S., and Prasanna, V. Analysis of high-performance floating-point arithmetic on
FPGAs. In Parallel and Distributed Processing Symposium, 2004. Proceedings. 18th International, pp. 149.
IEEE, 2004.
Graves, A. Practical variational inference for neural networks. In Advances in Neural Information Processing
Systems, pp. 2348?2356, 2011.
Han, S., Mao, H., and Dally, W. J. Deep compression: Compressing deep neural networks with pruning, trained
quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015a.
Han, S., Pool, J., Tran, J., and Dally, W. Learning both weights and connections for efficient neural network. In
Advances in Neural Information Processing Systems, pp. 1135?1143, 2015b.
Hinton, G. Neural networks for machine learning. Coursera, video lectures, 2012.
Horowitz, M. Computing?s Energy Problem (and what we can do about it). IEEE Interational Solid State
Circuits Conference, pp. 10?14, 2014.
Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., and Bengio, Y. Quantized neural networks: Training
neural networks with low precision weights and activations. arXiv preprint arXiv:1609.07061, 2016.
Hwang, K. and Sung, W. Fixed-point feedforward deep neural network design using weights+ 1, 0, and- 1. In
Signal Processing Systems (SiPS), 2014 IEEE Workshop on, pp. 1?6. IEEE, 2014.
Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate
shift. 2015.
Kim, M. and Smaragdis, P. Bitwise Neural Networks. ArXiv e-prints, January 2016.
Kingma, D. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
LeCun, Y., Bengio, Y., and Hinton, G. Deep learning. Nature, 521(7553):436?444, 2015.
Lee, C.-Y., Gallagher, P. W., and Tu, Z. Generalizing pooling functions in convolutional neural networks: Mixed,
gated, and tree. arXiv preprint arXiv:1509.08985, 2015.
Lin, Z., Courbariaux, M., Memisevic, R., and Bengio, Y. Neural networks with few multiplications. ArXiv
e-prints, abs/1510.03009, October 2015.
Soudry, D., Hubara, I., and Meir, R. Expectation backpropagation: Parameter-free training of multilayer neural
networks with continuous or discrete weights. In NIPS?2014, 2014.
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. Dropout: A simple way to
prevent neural networks from overfitting. Journal of Machine Learning Research, 15:1929?1958, 2014.
Szegedy, C., Liu, W., Jia, Y., et al. Going deeper with convolutions. Technical report, arXiv:1409.4842, 2014.
Wan, L., Zeiler, M., Zhang, S., LeCun, Y., and Fergus, R. Regularization of neural networks using dropconnect.
In ICML?2013, 2013.

9


----------------------------------------------------------------

title: 6310-phased-lstm-accelerating-recurrent-network-training-for-long-or-event-based-sequences.pdf

Phased LSTM: Accelerating Recurrent Network
Training for Long or Event-based Sequences

Daniel Neil, Michael Pfeiffer, and Shih-Chii Liu
Institute of Neuroinformatics
University of Zurich and ETH Zurich
Zurich, Switzerland 8057
{dneil, pfeiffer, shih}@ini.uzh.ch

Abstract
Recurrent Neural Networks (RNNs) have become the state-of-the-art choice for
extracting patterns from temporal sequences. However, current RNN models are
ill-suited to process irregularly sampled data triggered by events generated in
continuous time by sensors or other neurons. Such data can occur, for example,
when the input comes from novel event-driven artificial sensors that generate
sparse, asynchronous streams of events or from multiple conventional sensors with
different update intervals. In this work, we introduce the Phased LSTM model,
which extends the LSTM unit by adding a new time gate. This gate is controlled
by a parametrized oscillation with a frequency range that produces updates of the
memory cell only during a small percentage of the cycle. Even with the sparse
updates imposed by the oscillation, the Phased LSTM network achieves faster
convergence than regular LSTMs on tasks which require learning of long sequences.
The model naturally integrates inputs from sensors of arbitrary sampling rates,
thereby opening new areas of investigation for processing asynchronous sensory
events that carry timing information. It also greatly improves the performance of
LSTMs in standard RNN applications, and does so with an order-of-magnitude
fewer computes at runtime.

1

Introduction

Interest in recurrent neural networks (RNNs) has greatly increased in recent years, since larger
training databases, more powerful computing resources, and better training algorithms have enabled
breakthroughs in both processing and modeling of temporal sequences. Applications include speech
recognition [13], natural language processing [1, 20], and attention-based models for structured
prediction [5, 29]. RNNs are attractive because they equip neural networks with memories, and
the introduction of gating units such as LSTM and GRU [16, 6] has greatly helped in making the
learning of these networks manageable. RNNs are typically modeled as discrete-time dynamical
systems, thereby implicitly assuming a constant sampling rate of input signals, which also becomes
the update frequency of recurrent and feed-forward units. Although early work such as [25, 10, 4]
has realized the resulting limitations and suggested continuous-time dynamical systems approaches
towards RNNs, the great majority of modern RNN implementations uses fixed time steps.
Although fixed time steps are perfectly suitable for many RNN applications, there are several
important scenarios in which constant update rates impose constraints that affect the precision and
efficiency of RNNs. Many real-world tasks for autonomous vehicles or robots need to integrate input
from a variety of sensors, e.g. for vision, audition, distance measurements, or gyroscopes. Each sensor
may have its own data sampling rate, and short time steps are necessary to deal with sensors with
high sampling frequencies. However, this leads to an unnecessarily higher computational load and
30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

xt

xt

xt

Input it
Gate

Output Gate ot

xt

xt

t
Output
Gate

it Input
Gate

ct

ht

xt

c~t

ct

kt

Forget Gate ft

Forget Gate ft

xt

xt

(a)

t
ot

kt

ht

(b)

Figure 1: Model architecture. (a) Standard LSTM model. (b) Phased LSTM model, with time gate kt
controlled by timestamp t. In the Phased LSTM formulation, the cell value ct and the hidden output
ht can only be updated during an ?open? phase; otherwise, the previous values are maintained.

power consumption so that all units in the network can be updated with one time step. An interesting
new application area is processing of event-based sensors, which are data-driven, and record stimulus
changes in the world with short latencies and accurate timing. Processing the asynchronous outputs of
such sensors with time-stepped models would require high update frequencies, thereby counteracting
the potential power savings of event-based sensors. And finally there is an interest coming from
computational neuroscience, since brains can be viewed loosely as very large RNNs. However,
biological neurons communicate with spikes, and therefore perform asynchronous, event-triggered
updates in continuous time. This work presents a novel RNN model which can process inputs sampled
at asynchronous times and is described further in the following sections.

2

Model Description

Long short-term memory (LSTM) units [16] (Fig. 1(a)) are an important ingredient for modern deep
RNN architectures. We first define their update equations in the commonly-used version from [12]:
it
ft
ct
ot
ht

= ?i (xt Wxi + ht?1 Whi + wci  ct?1 + bi )
= ?f (xt Wxf + ht?1 Whf + wcf  ct?1 + bf )
= ft  ct?1 + it  ?c (xt Wxc + ht?1 Whc + bc )
= ?o (xt Wxo + ht?1 Who + wco  ct + bo )
= ot  ?h (ct )

(1)
(2)
(3)
(4)
(5)

The main difference to classical RNNs is the use of the gating functions it , ft , ot , which represent
the input, forget, and output gate at time t respectively. ct is the cell activation vector, whereas xt
and ht represent the input feature vector and the hidden output vector respectively. The gates use the
typical sigmoidal nonlinearities ?i , ?f , ?o and tanh nonlinearities ?c , and ?h with weight parameters
Whi , Whf , Who , Wxi , Wxf , and Wxo , which connect the different inputs and gates with the memory
cells and outputs, as well as biases bi , bf , and bo . The cell state ct itself is updated with a fraction of
the previous cell state that is controlled by ft , and a new input state created from the element-wise
(Hadamard) product, denoted by , of it and the output of the cell state nonlinearity ?c . Optional
peephole [11] connection weights wci , wcf , wco further influence the operation of the input, forget,
and output gates.
The Phased LSTM model extends the LSTM model by adding a new time gate, kt (Fig. 1(b)). The
opening and closing of this gate is controlled by an independent rhythmic oscillation specified by
three parameters; updates to the cell state ct and ht are permitted only when the gate is open. The
first parameter, ? , controls the real-time period of the oscillation. The second, ron , controls the ratio
of the duration of the ?open? phase to the full period. The third, s, controls the phase shift of the
oscillation to each Phased LSTM cell. All parameters can be learned during the training process.
Though other variants are possible, we propose here a particularly successful linearized formulation
2

t

j-2

j-1

j

Output

Output

Output

...

Layer 2

...

Layer 2

Layer 1
Input

...

Layer 2

Layer 1

Layer 1
tj-2

Input

kt Openness

closed

ct State

open

Input

tj-1

Input

1

tj

(a)

2
Time

3

4

(b)

Figure 2: Diagram of Phased LSTM behaviour. (a) Top: The rhythmic oscillations to the time gates of
3 different neurons; the period ? and the phase shift s is shown for the lowest neuron. The parameter
ron is the ratio of the open period to the total period ? . Bottom: Note that in a multilayer scenario,
the timestamp is distributed to all layers which are updated at the same time point. (b) Illustration of
Phased LSTM operation. A simple linearly increasing function is used as an input. The time gate
kt of each neuron has a different ? , identical phase shift s, and an open ratio ron of 0.05. Note that
the input (top panel) flows through the time gate kt (middle panel) to be held as the new cell state ct
(bottom panel) only when kt is open.
of the time gate, with analogy to the rectified linear unit that propagates gradients well:
?
2?t
1
?
?
,
if ?t < ron
?
?
2
? ron
(t ? s) mod ?
2?t
1
(6)
,
kt =
?t =
2?
, if ron < ?t < ron
?
?
?
?
r
2
on
?
?
??t ,
otherwise
?t is an auxiliary variable, which represents the phase inside the rhythmic cycle. The gate kt has three
phases (see Fig. 2a): in the first two phases, the "openness" of the gate rises from 0 to 1 (first phase)
and drops from 1 to 0 (second phase). During the third phase, the gate is closed and the previous cell
state is maintained. The leak with rate ? is active in the closed phase, and plays a similar role as the
leak in a parametric ?leaky? rectified linear unit [15] by propagating important gradient information
even when the gate is closed. Note that the linear slopes of kt during the open phases of the time gate
allow effective transmission of error gradients.
In contrast to traditional RNNs, and even sparser variants of RNNs [19], updates in Phased LSTM
can optionally be performed at irregularly sampled time points tj . This allows the RNNs to work with
event-driven, asynchronously sampled input data. We use the shorthand notation cj = ctj for cell
states at time tj (analogously for other gates and units), and let cj?1 denote the state at the previous
update time tj?1 . We can then rewrite the regular LSTM cell update equations for cj and hj (from
Eq. 3 and Eq. 5), using proposed cell updates cej and hej mediated by the time gate kj :
cej = fj  cj?1 + ij  ?c (xj Wxc + hj?1 Whc + bc )
(7)
cj = kj  cej + (1 ? kj )  cj?1
(8)
e
hj = oj  ?h (cej )
(9)
hj = kj  hej + (1 ? kj )  hj?1

(10)

A schematic of Phased LSTM with its parameters can be found in Fig. 2a, accompanied by an
illustration of the relationship between the time, the input, the time gate kt , and the state ct in Fig. 2b.
One key advantage of this Phased LSTM formulation lies in the rate of memory decay. For the simple
task of keeping an initial memory state c0 as long as possible without receiving additional inputs (i.e.
ij = 0 at all time steps tj ), a standard LSTM with a nearly fully-opened forget gate (i.e. fj = 1 ? )
after n update steps would contain
cn = fn  cn?1 = (1 ? )  (fn?1  cn?2 ) = . . . = (1 ? )n  c0 .
(11)
3

1.0

1.0

0.5

0.5

0.5

0.0

0.0

0.0

?0.5

?0.5

?0.5

?1.0

?1.0
16 18 20 22 24 26 28 30
Time [ms]

(a)

Accuracy at 70 Epochs [%]

100

1.0

?1.0
16 18 20 22 24 26 28 30
Time [ms]

(b)

90

Phased LSTM
BN LSTM
LSTM

80

70

60

50

16 18 20 22 24 26 28 30
Time [ms]

(c)

High
Standard resolution Async.
sampling sampling sampling

(d)

Figure 3: Frequency discrimination task. The network is trained to discriminate waves of different
frequency sets (shown in blue and gray); every circle is an input point. (a) Standard condition: the
data is regularly sampled every 1 ms. (b) High resolution sampling condition: new input points
are gathered every 0.1ms. (c) Asynchronous sampling condition: new input points are presented at
intervals of 0.02 ms to 10 ms. (d) The accuracy of Phased LSTM under the three sampling conditions
is maintained, but the accuracy of the BN-LSTM and standard LSTM drops significantly in the
sampling conditions (b) and (c). Error bars indicate standard deviation over 5 runs.

This means the memory for  < 1 decays exponentially with every time step. Conversely, the Phased
LSTM state only decays during the open periods of the time gate, but maintains a perfect memory
during its closed phase, i.e. cj = cj?? if kt = 0 for tj?? ? t ? tj . Thus, during a single oscillation
period of length ? , the units only update during a duration of ron ? ? , which will result in substantially
fewer than n update steps. Because of this cyclic memory, Phased LSTM can have much longer and
adjustable memory length via the parameter ? .
The oscillations impose sparse updates of the units, therefore substantially decreasing the total number
of updates during network operation. During training, this sparseness ensures that the gradient is
required to backpropagate through fewer updating timesteps, allowing an undecayed gradient to be
backpropagated through time and allowing faster learning convergence. Similar to the shielding of
the cell state ct (and its gradient) by the input gates and forget gates of the LSTM, the time gate
prevents external inputs and time steps from dispersing and mixing the gradient of the cell state.

3

Results

In the following sections, we investigate the advantages of the Phased LSTM model in a variety
of scenarios that require either precise timing of updates or learning from a long sequence. For all
the results presented here, the networks were trained with Adam [18] set to default learning rate
parameters, using Theano [2] with Lasagne [9]. Unless otherwise specified, the leak rate was set to
? = 0.001 during training and ? = 0 during test. The phase shift, s, for each neuron was uniformly
chosen from the interval [0, ? ]. The parameters ? and s were learned during training, while the open
ratio ron was fixed at 0.05 and not adjusted during training, except in the first task to demonstrate
that the model can train successfully while learning all parameters.
3.1

Frequency Discrimination Task

In this first experiment, the network is trained to distinguish two classes of sine waves from different
frequency sets: those with a period in a target range T ? U(5, 6), and those outside the range, i.e.
T ? {U(1, 5) ? U(6, 100)}, using U(a, b) for the uniform distribution on the interval (a, b). This
task illustrates the advantages of Phased LSTM, since it involves a periodic stimulus and requires
fine timing discrimination. The inputs are presented as pairs hy, ti, where y is the amplitude and t
the timestamp of the sample from the input sine wave.
Figure 3 illustrates the task: the blue curves must be separated from the lighter curves based on
the samples shown as circles. We evaluate three conditions for sampling the input signals: In the
standard condition (Fig. 3a), the sine waves are regularly sampled every 1 ms; in the oversampled
4

90

10 0

80

10 -1

Phased LSTM
BN LSTM
LSTM

75
70

MSE

Accuracy [%]

85

65
60
55

10 -3
10

50
45
0

10 -2

50

100

150 200
Epoch

250

-4

10 -5
0

300

(a)

LSTM
PLSTM (? ? e U(0;
PLSTM (? ? e U(2;
PLSTM (? ? e U(4;
PLSTM (? ? e U(6;

20

)
)
6)
)
8)
)
2)
4)

40
60
Epoch

80

100

(b)

Figure 4: (a) Accuracy during training for the superimposed frequencies task. The Phased LSTM
outperforms both LSTM and BN-LSTM while exhibiting lower variance. Shading shows maximum
and minimum over 5 runs, while dark lines indicate the mean. (b) Mean-squared error over training
on the addition task, with an input length of 500. Note that longer periods accelerate learning
convergence.

condition (Fig. 3b), the sine waves are regularly sampled every 0.1 ms, resulting in ten times
as many data points. Finally, in the asynchronously sampled condition (Fig. 3c), samples are
collected at asynchronous times over the duration of the input. Additionally, the sine waves have
a uniformly drawn random phase shift from all possible shifts, random numbers of samples drawn
from U(15, 125), a random duration drawn from U(15, 125), and a start time drawn from U(0, 125 ?
duration). The number of samples in the asynchronous and standard sampling condition is equal.
The classes were approximately balanced, yielding a 50% chance success rate.
Single-layer RNNs are trained on this data, each repeated with five random initial seeds. We compare
our Phased LSTM configuration to regular LSTM, and batch-normalized (BN) LSTM which has
found success in certain applications [14]. For the regular LSTM and the BN-LSTM, the timestamp
is used as an additional input feature dimension; for the Phased LSTM, the time input controls
the time gates kt . The architecture consists of 2-110-2 neurons for the LSTM and BN-LSTM, and
1-110-2 for the Phased LSTM. The oscillation periods of the Phased LSTMs are drawn uniformly in
the exponential space to give a wide variety of applicable frequencies, i.e., ? ? exp(U(0, 3)). All
other parameters match between models where applicable. The default LSTM parameters are given
in the Lasagne Theano implementation, and were kept for LSTM, BN-LSTM, and Phased LSTM.
Appropriate gate biasing was investigated but did not resolve the discrepancies between the models.
All three networks excel under standard sampling conditions as expected, as seen in Fig. 3d (left).
However, for the same number of epochs, increasing the data sampling by a factor of ten has
devastating effects for both LSTM and BN-LSTM, dropping their accuracy down to near chance
(Fig. 3d, middle). Presumably, if given enough training iterations, their accuracies would return to
the normal baseline. However, for the oversampled condition, Phased LSTM actually increases in
accuracy, as it receives more information about the underlying waveform. Finally, if the updates are
not evenly spaced and are instead sampled at asynchronous times, even when controlled to have the
same number of points as the standard sampling condition, it appears to make the problem rather
challenging for traditional state-of-the-art models (Fig. 3d, right). However, the Phased LSTM has
no difficulty with the asynchronously sampled data, because the time gates kt do not need regular
updates and can be correctly sampled at any continuous time within the period.
We extend the previous task by training the same RNN architectures on signals composed of two
sine waves. The goal is to distinguish signals composed of sine waves with periods T1 ? U(5, 6)
and T2 ? U(13, 15), each with independent phase, from signals composed of sine waves with
periods T1 ? {U(1, 5) ? U(6, 100)} and T2 ? {U(1, 13) ? U(15, 100)}, again with independent
phase. Despite being significantly more challenging, Fig. 4a demonstrates how quickly the Phased
LSTM converges to the correct solution compared to the standard approaches, using exactly the same
parameters. Additionally, the Phased LSTM appears to exhibit very low variance during training.
5

3
1

2

0
?10 5

1
2

Time [us]

(a)

(b)

3

(c)

Figure 5: N-MNIST experiment. (a) Sketch of digit movement seen by the image sensor. (b)
Frame-based representation of an ?8? digit from the N-MNIST dataset [24] obtained by integrating all
input spikes for each pixel. (c) Spatio-temporal representation of the digit, presented in three saccades
as in (a). Note that this representation shows the digit more clearly than the blurred frame-based one.

3.2

Adding Task

To investigate how introducing time gates helps learning when long memory is required, we revisit
an original LSTM task called the adding task [16]. In this task, a sequence of random numbers
is presented along with an indicator input stream. When there is a 0 in the indicator input stream,
the presented value should be ignored; a 1 indicates that the value should be added. At the end of
presentation the network produces a sum of all indicated values. Unlike the previous tasks, there is no
inherent periodicity in the input, and it is one of the original tasks that LSTM was designed to solve
well. This would seem to work against the advantages of Phased LSTM, but using a longer period for
the time gate kt could allow more effective training as a unit opens only a for a few timesteps during
training.
In this task, a sequence of numbers (of length 490 to 510) was drawn from U(?0.5, 0.5). Two
numbers in this stream of numbers are marked for addition: one from the first 10% of numbers
(drawn with uniform probability) and one in the last half (drawn with uniform probability), producing
a model of a long and noisy stream of data with only few significant points. Importantly, this should
challenge the Phased LSTM model because there is no inherent periodicity and every timestep could
contain the important marked points.
The same network architecture is used as before. The period ? was drawn uniformly in the exponential domain, comparing four sampling intervals exp(U(0, 2)), exp(U(2, 4)), exp(U(4, 6)), and
exp(U(6, 8)). Note that despite different ? values, the total number of LSTM updates remains approximately the same, since the overall sparseness is set by ron . However, a longer period ? provides
a longer jump through the past timesteps for the gradient during backpropagation-through-time.
Moreover, we investigate whether the model can learn longer sequences more effectively when longer
periods are used. By varying the period ? , the results in Fig. 4b show longer ? accelerates training of
the network to learn much longer sequences faster.
3.3

N-MNIST Event-Based Visual Recognition

To test performance on real-world asynchronously sampled data, we make use of the publiclyavailable N-MNIST [24] dataset for neuromorphic vision. The recordings come from an event-based
vision sensor that is sensitive to local temporal contrast changes [26]. An event is generated from
a pixel when its local contrast change exceeds a threshold. Every event is encoded as a 4-tuple
hx, y, p, ti with position x, y of the pixel, a polarity bit p (indicating a contrast increase or decrease),
and a timestamp t indicating the time when the event is generated. The recordings consist of events
generated by the vision sensor while the sensor undergoes three saccadic movements facing a static
digit from the MNIST dataset (Fig. 5a). An example of the event responses can be seen in Fig. 5c).
In previous work using event-based input data [21, 23], the timing information was sometimes
removed and instead a frame-based representation was generated by computing the pixel-wise
event-rate over some time period (as shown in Fig. 5(b)). Note that the spatio-temporal surface of
6

Table 1: Accuracy on N-MNIST
CNN
BN-LSTM
Phased LSTM (? = 100ms)
Accuracy at Epoch 1
Train/test ? = 0.75

73.81% ? 3.5
95.02% ? 0.3

40.87% ? 13.3
96.93% ? 0.12

90.32% ? 2.3
97.28% ? 0.1

Test with ? = 0.4
Test with ? = 1.0

90.67% ? 0.3
94.99% ? 0.3

94.79% ? 0.03
96.55% ? 0.63

95.11% ? 0.2
97.27% ? 0.1

3153 per neuron

159 ? 2.8 per neuron

LSTM Updates

?

events in Fig. 5(c) reveals details of the digit much more clearly than in the blurred frame-based
representation.The Phased LSTM allows us to operate directly on such spatio-temporal event streams.
Table 1 summarizes classification results for three different network types: a CNN trained on framebased representations of N-MNIST digits and two RNNs, a BN-LSTM and a Phased LSTM, trained
directly on the event streams. Regular LSTM is not shown, as it was found to perform worse. The
CNN was comprised of three alternating layers of 8 kernels of 5x5 convolution with a leaky ReLU
nonlinearity and 2x2 max-pooling, which were then fully-connected to 256 neurons, and finally fullyconnected to the 10 output classes. The event pixel address was used to produce a 40-dimensional
embedding via a learned embedding matrix [9], and combined with the polarity to produce the input.
Therefore, the network architecture was 41-110-10 for the Phased LSTM and 42-110-10 for the
BN-LSTM, with the time given as an extra input dimension to the BN-LSTM.
Table 1 shows that Phased LSTM trains faster than alternative models and achieves much higher
accuracy with a lower variance even within the first epoch of training. We further define a factor, ?,
which represents the probability that an event is included, i.e. ? = 1.0 means all events are included.
The RNN models are trained with ? = 0.75, and again the Phased LSTM achieves slightly higher
performance than the BN-LSTM model. When testing with ? = 0.4 (fewer events) and ? = 1.0 (more
events) without retraining, both RNN models perform well and greatly outperform the CNN. This is
because the accumulated statistics of the frame-based input to the CNN change drastically when the
overall spike rates are altered. The Phased LSTM RNNs seem to have learned a stable spatio-temporal
surface on the input and are only slightly altered by sampling it more or less frequently.
Finally, as each neuron of the Phased LSTM only updates about 5% of the time, on average, 159
updates are needed in comparison to the 3153 updates needed per neuron of the BN-LSTM, leading
to an approximate twenty-fold reduction in run time compute cost. It is also worth noting that these
results form a new state-of-the-art accuracy for this dataset [24, 7].
3.4

Visual-Auditory Sensor Fusion for Lip Reading

Finally, we demonstrate the use of Phased LSTM on a task involving sensors with different sampling
rates. Few RNN models ever attempt to merge sensors of different input frequencies, although the
sampling rates can vary substantially. For this task, we use the GRID dataset [8]. This corpus contains
video and audio of 30 speakers each uttering 1000 sentences composed of a fixed grammar and a
constrained vocabulary of 51 words. The data was randomly divided into a 90%/10% train-test set.
An OpenCV [17] implementation of a face detector was used on the video stream to extract the face
which was then resized to grayscale 48x48 pixels. The goal here is to obtain a model that can use
audio alone, video alone, or both inputs to robustly classify the sentence. However, since the audio
alone is sufficient to achieve greater than 99% accuracy, sensor modalities were randomly masked to
zero during training to encourage robustness towards sensory noise and loss.
The network architecture first separately processes video and audio data before merging them in
two RNN layers that receive both modalities. The video stream uses three alternating layers of 16
kernels of 5x5 convolution and 2x2 subsampling to reduce the input of 1x48x48 to 16x2x2, which is
then used as the input to 110 recurrent units. The audio stream connects the 39-dimensional MFCCs
(13 MFCCs with first and second derivatives) to 150 recurrent units. Both streams converge into
the Merged-1 layer with 250 recurrent units, and is connected to a second hidden layer with 250
recurrent units named Merged-2. The output of the Merged-2 layer is fully-connected to 51 output
nodes, which represent the vocabulary of GRID. For the Phased LSTM network, all recurrent units
are Phased LSTM units.
7

Low Res.
Loss

10 -1

MFCCs
Video
Frames

Phased LSTM
BN LSTM
LSTM

10 -2

Video
PLSTM
Merged-1
PLSTM
Merged-2
PLSTM

220

260 300
Time [ms]

(a)

340

0
5
10
15
20
25
30
35

10 -1

High Res.
Loss

Audio
PLSTM

MFCC

kj Openness

Inputs

Time

500
1500 2500
Time [ms]

(b)

10 -2
0

10

20 30
Epoch

40

50

(c)

Figure 6: Lip reading experiment. (a) Inputs and openness of time gates for the lip reading experiment.
Note that the 25fps video frame rate is a multiple of the audio input frequency (100 Hz). Phased
LSTM timing parameters are configured to align to the sampling time of their inputs. (b) Example
input of video (top) and audio (bottom). (c) Test loss using the video stream alone. Video frame rate
is 40ms. Top: low resolution condition, MFCCs computed every 40ms with a network update every
40 ms; Bottom: high resolution condition, MFCCs every 10 ms with a network update every 10 ms.
In the audio and video Phased LSTM layers, we manually align the open periods of the time gates
to the sampling times of the inputs and disable learning of the ? and s parameters (see Fig. 6a).
This prevents presenting zeros or artificial interpolations to the network when data is not present.
In the merged layers, however, the parameters of the time gate are learned, with the period ? of the
first merged layer drawn from U(10, 1000) and the second from U(500, 3000). Fig. 6b shows a
visualization of one frame of video and the complete duration of an audio sample.
During evaluation, all networks achieve greater than 98% accuracy on audio-only and combined
audio-video inputs. However, video-only evaluation with an audio-video capable network proved
the most challenging, so the results in Fig. 6c focus on these results (though result rankings are
representative of all conditions). Two differently-sampled versions of the data were used: In the first
?low resolution? version (Fig. 6c, top), the sampling rate of the MFCCs was matched to the sampling
rate of the 25 fps video. In the second ?high-resolution? condition, the sampling rate was set to the
more common value of 100 Hz sampling frequency (Fig. 6c, bottom and shown in Fig. 6a). The
higher audio sampling rate did not increase accuracy, but allows for a faster latency (10ms instead of
40ms). The Phased LSTM again converges substantially faster than both LSTM and batch-normalized
LSTM. The peak accuracy of 81.15% compares favorably against lipreading-focused state-of-the-art
approaches [28] while avoiding manually-crafted features.

4

Discussion

The Phased LSTM has many surprising advantages. With its rhythmic periodicity, it acts like a
learnable, gated Fourier transform on its input, permitting very fine timing discrimination. Alternatively, the rhythmic periodicity can be viewed as a kind of persistent dropout that preserves state [27],
enhancing model diversity. The rhythmic inactivation can even be viewed as a shortcut to the past
for gradient backpropagation, accelerating training. The presented results support these interpretations, demonstrating the ability to discriminate rhythmic signals and to learn long memory traces.
Importantly, in all experiments, Phased LSTM converges more quickly and theoretically requires
only 5% of the computes at runtime, while often improving in accuracy compared to standard LSTM.
The presented methods can also easily be extended to GRUs [6], and it is likely that even simpler
models, such as ones that use a square-wave-like oscillation, will perform well, thereby making even
more efficient and encouraging alternative Phased LSTM formulations. An inspiration for using
oscillations in recurrent networks comes from computational neuroscience [3], where rhythms have
been shown to play important roles for synchronization and plasticity [22]. Phased LSTMs were
not designed as biologically plausible models, but may help explain some of the advantages and
robustness of learning in large spiking recurrent networks.
8

References
[1] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate.
arXiv preprint arXiv:1409.0473, 2014.
[2] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins, J. Turian, D. Warde-Farley,
and Y. Bengio. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for
scientific computing conference (SciPy), volume 4, page 3, 2010.
[3] G. Buzsaki. Rhythms of the Brain. Oxford University Press, 2006.
[4] G. Cauwenberghs. An analog VLSI recurrent neural network learning a continuous-time trajectory. IEEE
Transactions on Neural Networks, 7(2):346?361, 1996.
[5] K. Cho, A. Courville, and Y. Bengio. Describing multimedia content using attention-based encoder-decoder
networks. IEEE Transactions on Multimedia, 17(11):1875?1886, 2015.
[6] K. Cho, B. van Merrienboer, C. Gulcehre, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078,
2014.
[7] G. K. Cohen, G. Orchard, S. H. Ieng, J. Tapson, R. B. Benosman, and A. van Schaik. Skimming digits:
Neuromorphic classification of spike-encoded images. Frontiers in Neuroscience, 10(184), 2016.
[8] M. Cooke, J. Barker, S. Cunningham, and X. Shao. An audio-visual corpus for speech perception and
automatic speech recognition. The Journal of the Acoustical Society of America, 120(5):2421?2424, 2006.
[9] S. Dieleman et al. Lasagne: First release., Aug. 2015.
[10] K.-I. Funahashi and Y. Nakamura. Approximation of dynamical systems by continuous time recurrent
neural networks. Neural Networks, 6(6):801?806, 1993.
[11] F. A. Gers and J. Schmidhuber. Recurrent nets that time and count. In Neural Networks, 2000. IJCNN
2000, Proceedings of the IEEE-INNS-ENNS International Joint Conference on, volume 3, pages 189?194.
IEEE, 2000.
[12] A. Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.
[13] A. Graves, A.-R. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural networks.
In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages
6645?6649, 2013.
[14] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen, R. Prenger, S. Satheesh, S. Sengupta,
A. Coates, et al. Deep speech: Scaling up end-to-end speech recognition. arXiv preprint arXiv:1412.5567,
2014.
[15] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level performance on
imagenet classification. In The IEEE International Conference on Computer Vision (ICCV), 2015.
[16] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735?1780, 1997.
[17] Itseez. Open source computer vision library. https://github.com/itseez/opencv, 2015.
[18] D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.
[19] J. Koutnik, K. Greff, F. Gomez, and J. Schmidhuber. A clockwork rnn. arXiv preprint arXiv:1402.3511,
2014.
[20] T. Mikolov, M. Karafi?t, L. Burget, J. Cernock`y, and S. Khudanpur. Recurrent neural network based
language model. Interspeech, 2:3, 2010.
[21] D. Neil and S.-C. Liu. Effective sensor fusion with event-based sensors and deep network architectures. In
IEEE Int. Symposium on Circuits and Systems (ISCAS), 2016.
[22] B. Nessler, M. Pfeiffer, L. Buesing, and W. Maass. Bayesian computation emerges in generic cortical
microcircuits through spike-timing-dependent plasticity. PLoS Comput Biol, 9(4):e1003037, 2013.
[23] P. O?Connor, D. Neil, S.-C. Liu, T. Delbruck, and M. Pfeiffer. Real-time classification and sensor fusion
with a spiking Deep Belief Network. Frontiers in Neuroscience, 7, 2013.
[24] G. Orchard, A. Jayawant, G. Cohen, and N. Thakor. Converting static image datasets to spiking neuromorphic datasets using saccades. arXiv: 1507.07629, 2015.
[25] B. A. Pearlmutter. Learning state space trajectories in recurrent neural networks. Neural Computation,
1(2):263?269, 1989.
[26] C. Posch, T. Serrano-Gotarredona, B. Linares-Barranco, and T. Delbruck. Retinomorphic event-based
vision sensors: bioinspired cameras with spiking outputs. Proceedings of the IEEE, 102(10):1470?1484,
2014.
[27] S. Semeniuta, A. Severyn, and E. Barth. Recurrent dropout without memory loss. arXiv, arXiv:1603.05118,
2016.
[28] M. Wand, J. Koutn?k, and J. Schmidhuber. Lipreading with long short-term memory. arXiv preprint
arXiv:1601.08188, 2016.
[29] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdinov, R. Zemel, and Y. Bengio. Show, attend
and tell: Neural image caption generation with visual attention. In International Conference on Machine
Learning, 2015.

9


----------------------------------------------------------------

title: 6215-on-multiplicative-integration-with-recurrent-neural-networks.pdf

On Multiplicative Integration with
Recurrent Neural Networks
Yuhuai Wu1,? , Saizheng Zhang2,? , Ying Zhang2 , Yoshua Bengio2,4 and Ruslan Salakhutdinov3,4
1
University of Toronto, 2 MILA, Universit? de Montr?al, 3 Carnegie Mellon University, 4 CIFAR
ywu@cs.toronto.edu,2 {firstname.lastname}@umontreal.ca,rsalakhu@cs.cmu.edu

Abstract
We introduce a general and simple structural design called ?Multiplicative Integration? (MI) to improve recurrent neural networks (RNNs). MI changes the way in
which information from difference sources flows and is integrated in the computational building block of an RNN, while introducing almost no extra parameters.
The new structure can be easily embedded into many popular RNN models, including LSTMs and GRUs. We empirically analyze its learning behaviour and conduct
evaluations on several tasks using different RNN models. Our experimental results
demonstrate that Multiplicative Integration can provide a substantial performance
boost over many of the existing RNN models.

1

Introduction

Recently there has been a resurgence of new structural designs for recurrent neural networks (RNNs)
[1, 2, 3]. Most of these designs are derived from popular structures including vanilla RNNs, Long
Short Term Memory networks (LSTMs) [4] and Gated Recurrent Units (GRUs) [5]. Despite of their
varying characteristics, most of them share a common computational building block, described by the
following equation:
?(Wx + Uz + b),
(1)
where x ? Rn and z ? Rm are state vectors coming from different information sources, W ? Rd?n
and U ? Rd?m are state-to-state transition matrices, and b is a bias vector. This computational
building block serves as a combinator for integrating information flow from the x and z by a sum
operation ?+?, followed by a nonlinearity ?. We refer to it as the additive building block. Additive
building blocks are widely implemented in various state computations in RNNs (e.g. hidden state
computations for vanilla-RNNs, gate/cell computations of LSTMs and GRUs.
In this work, we propose an alternative design for constructing the computational building block by
changing the procedure of information integration. Specifically, instead of utilizing sum operation
?+", we propose to use the Hadamard product ?? to fuse Wx and Uz:
?(Wx  Uz + b)

(2)

The result of this modification changes the RNN from first order to second order [6], while introducing
no extra parameters. We call this kind of information integration design a form of Multiplicative
Integration. The effect of multiplication naturally results in a gating type structure, in which Wx
and Uz are the gates of each other. More specifically, one can think of the state-to-state computation
Uz (where for example z represents the previous state) as dynamically rescaled by Wx (where
for example x represents the input). Such rescaling does not exist in the additive building block, in
which Uz is independent of x. This relatively simple modification brings about advantages over the
additive building block as it alters RNN?s gradient properties, which we discuss in detail in the next
section, as well as verify through extensive experiments.
?

Equal contribution.

30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

In the following sections, we first introduce a general formulation of Multiplicative Integration. We
then compare it to the additive building block on several sequence learning tasks, including character
level language modelling, speech recognition, large scale sentence representation learning using a
Skip-Thought model, and teaching a machine to read and comprehend for a question answering
task. The experimental results (together with several existing state-of-the-art models) show that
various RNN structures (including vanilla RNNs, LSTMs, and GRUs) equipped with Multiplicative
Integration provide better generalization and easier optimization. Its main advantages include: (1) it
enjoys better gradient properties due to the gating effect. Most of the hidden units are non-saturated;
(2) the general formulation of Multiplicative Integration naturally includes the regular additive
building block as a special case, and introduces almost no extra parameters compared to the additive
building block; and (3) it is a drop-in replacement for the additive building block in most of the
popular RNN models, including LSTMs and GRUs. It can also be combined with other RNN training
techniques such as Recurrent Batch Normalization [7]. We further discuss its relationship to existing
models, including Hidden Markov Models (HMMs) [8], second order RNNs [6] and Multiplicative
RNNs [9].

2

Structure Description and Analysis

2.1

General Formulation of Multiplicative Integration

The key idea behind Multiplicative Integration is to integrate different information flows Wx and Uz,
by the Hadamard product ??. A more general formulation of Multiplicative Integration includes
two more bias vectors ?1 and ?2 added to Wx and Uz:
?((Wx + ?1 )  (Uz + ?2 ) + b)

(3)

d

where ?1 , ?2 ? R are bias vectors. Notice that such formulation contains the first order terms as
in a additive building block, i.e., ?1  Uht?1 + ?2  Wxt . In order to make the Multiplicative
Integration more flexible, we introduce another bias vector ? ? Rd to gate2 the term Wx  Uz,
obtaining the following formulation:
?(?  Wx  Uz + ?1  Uz + ?2  Wx + b),

(4)

Note that the number of parameters of the Multiplicative Integration is about the same as that of the
additive building block, since the number of new parameters (?, ?1 and ?2 ) are negligible compared
to total number of parameters. Also, Multiplicative Integration can be easily extended to LSTMs
and GRUs3 , that adopt vanilla building blocks for computing gates and output states, where one can
directly replace them with the Multiplicative Integration. More generally, in any kind of structure
where k information flows (k ? 2) are involved (e.g. residual networks [10]), one can implement
pairwise Multiplicative Integration for integrating all k information sources.
2.2 Gradient Properties
The Multiplicative Integration has different gradient properties compared to the additive building
block. For clarity of presentation, we first look at vanilla-RNN and RNN with Multiplicative
Integration embedded, referred to as MI-RNN. That is, ht = ?(Wxt + Uht?1 + b) versus
?ht
ht = ?(Wxt  Uht?1 + b). In a vanilla-RNN, the gradient ?h
can be computed as follows:
t?n
t
Y
?ht
=
UT diag(?0k ),
(5)
?ht?n
k=t?n+1

?0k

0

where
= ? (Wxk + Uhk?1 + b). The equation above shows that the gradient flow through time
heavily depends on the hidden-to-hidden matrix U, but W and xk appear to play a limited role: they
?ht
only come in the derivative of ?0 mixed with Uhk?1 . On the other hand, the gradient ?h
of a
t?n
MI-RNN is4 :
t
Y
?ht
=
UT diag(Wxk )diag(?0k ),
(6)
?ht?n
k=t?n+1

2

If ? = 0, the Multiplicative Integration will degenerate to the vanilla additive building block.
See exact formulations in the Appendix.
4
Here we adopt the simplest formulation of Multiplicative Integration for illustration. In the more general
case (Eq. 4), diag(Wxk ) in Eq. 6 will become diag(?  Wxk + ?1 ).
3

2

where ?0k = ?0 (Wxk  Uhk?1 + b). By looking at the gradient, we see that the matrix W and
the current input xk is directly involved in the gradient computation by gating the matrix U, hence
more capable of altering the updates of the learning system. As we show in our experiments, with
Wxk directly gating the gradient, the vanishing/exploding problem is alleviated: Wxk dynamically
reconciles U, making the gradient propagation easier compared to the regular RNNs. For LSTMs
and GRUs with Multiplicative Integration, the gradient propagation properties are more complicated.
But in principle, the benefits of the gating effect also persists in these models.

3

Experiments

In all of our experiments, we use the general form of Multiplicative Integration (Eq. 4) for any hidden
state/gate computations, unless otherwise specified.
3.1 Exploratory Experiments
To further understand the functionality of Multiplicative Integration, we take a simple RNN for
illustration, and perform several exploratory experiments on the character level language modeling
task using Penn-Treebank dataset [11], following the data partition in [12]. The length of the
training sequence is 50. All models have a single hidden layer of size 2048, and we use Adam
optimization algorithm [13] with learning rate 1e?4 . Weights are initialized to samples drawn from
uniform[?0.02, 0.02]. Performance is evaluated by the bits-per-character (BPC) metric, which is
log2 of perplexity.
3.1.1 Gradient Properties
To analyze the gradient flow of the model, we divide the gradient in Eq. 6 into two parts: 1. the
gated matrix products: UT diag(Wxk ), and 2. the derivative of the nonlinearity ?0 , We separately
analyze the properties of each term compared to the additive building block. We first focus on the
gating effect brought by diag(Wxk ). In order to separate out the effect of nonlinearity, we chose ?
to be the identity map, hence both vanilla-RNN and MI-RNN reduce to linear models, referred to as
lin-RNN and lin-MI-RNN.
For each model we monitor the log-L2-norm of the gradient log ||?C/?ht ||2 (averaged over the
training set) after every training epoch, where ht is the hidden state at time step t, and C is the
negative log-likelihood of the single character prediction at the final time step (t = 50). Figure. 1
shows the evolution of the gradient norms for small t, i.e., 0, 5, 10, as they better reflect the gradient
propagation behaviour. Observe that the norms of lin-MI-RNN (orange) increase rapidly and soon
exceed the corresponding norms of lin-RNN by a large margin. The norms of lin-RNN stay close to
zero (? 10?4 ) and their changes over time are almost negligible. This observation implies that with
the help of diag(Wxk ) term, the gradient vanishing of lin-MI-RNN can be alleviated compared to
lin-RNN. The final test BPC (bits-per-character) of lin-MI-RNN is 1.48, which is comparable to a
vanilla-RNN with stabilizing regularizer [14], while lin-RNN performs rather poorly, achieving a test
BPC of over 2.
Next we look into the nonlinearity ?. We chose ? = tanh for both vanilla-RNN and MI-RNN.
Figure 1 (c) and (d) shows a comparison of histograms of hidden activations over all time steps on
the validation set after training. Interestingly, in (c) for vanilla-RNN, most activations are saturated
with values around ?1, whereas in (d) for MI-RNN, most activations are non-saturated with values
around 0. This has a direct consequence in gradient propagation: non-saturated activations imply
that diag(?0k ) ? 1 for ? = tanh, which can help gradients propagate, whereas saturated activations
imply that diag(?0k ) ? 0, resulting in gradients vanishing.
3.1.2 Scaling Problem
When adding two numbers at different order of magnitude, the smaller one might be negligible for the
sum. However, when multiplying two numbers, the value of the product depends on both regardless
of the scales. This principle also applies when comparing Multiplicative Integration to the additive
building blocks. In this experiment, we test whether Multiplicative Integration is more robust to the
scales of weight values. Following the same models as in Section 3.1.1, we first calculated the norms
of Wxk and Uhk?1 for both vanilla-RNN and MI-RNN for different k after training. We found that
in both structures, Wxk is a lot smaller than Uhk?1 in magnitude. This might be due to the fact that
xk is a one-hot vector, making the number of updates for (columns of) W be smaller than U. As a
result, in vanilla-RNN, the pre-activation term Wxk + Uhk?1 is largely controlled by the value of
Uhk?1 , while Wxk becomes rather small. In MI-RNN, on the other hand, the pre-activation term
Wxk  Uhk?1 still depends on the values of both Wxk and Uhk?1 , due to multiplication.
3

validation BPC

?4

?7
5

0.5

lin-RNN, t=0
lin-RNN, t=5
lin-RNN, t=10

lin-MI-RNN, t=0
lin-MI-RNN, t=5
lin-MI-RNN, t=10

10
15
20
number of epochs

2.1
1.8

0

(d)

0.12

0.3
0.2
0.1

?0.5
0.0
0.5
activation values of h_t

2.4

1.5

25

0.4

0.0
?1.0

vanilla-RNN
MI-RNN-simple
MI-RNN-general

2.7

?3

?5

(b)

3.0

?2

?6

normalized fequency

(a)

normalized fequency

log||dC / dh_t||_2

?1

10
15
20
number of epochs

25

(d)

0.10
0.08
0.06
0.04
0.02
0.00
?1.0

1.0

5

?0.5
0.0
0.5
activation values of h_t

1.0

Figure 1: (a) Curves of log-L2-norm of gradients for lin-RNN (blue) and lin-MI-RNN (orange). Time gradually
changes from {1, 5, 10}. (b) Validation BPC curves for vanilla-RNN, MI-RNN-simple using Eq. 2, and MIRNN-general using Eq. 4. (c) Histogram of vanilla-RNN?s hidden activations over the validation set, most
activations are saturated. (d) Histogram of MI-RNN?s hidden activations over the validation set, most activations
are not saturated.

We next tried different initialization of W and U to test their sensitivities to the scaling. For each
model, we fix the initialization of U to uniform[?0.02, 0.02] and initialize W to uniform[?rW , rW ]
where rW varies in {0.02, 0.1, 0.3, 0.6}. Table 1, top left panel, shows results. As we increase
the scale of W, performance of the vanilla-RNN improves, suggesting that the model is able to
better utilize the input information. On the other hand, MI-RNN is much more robust to different
initializations, where the scaling has almost no effect on the final performance.
3.1.3 On different choices of the formulation
In our third experiment, we evaluated the performance of different computational building blocks,
which are Eq. 1 (vanilla-RNN), Eq. 2 (MI-RNN-simple) and Eq. 4 (MI-RNN-general)5 . From the
validation curves in Figure 1 (b), we see that both MI-RNN, simple and MI-RNN-general yield much
better performance compared to vanilla-RNN, and MI-RNN-general has a faster convergence speed
compared to MI-RNN-simple. We also compared our results to the previously published models
in Table 1, bottom left panel, where MI-RNN-general achieves a test BPC of 1.39, which is to our
knowledge the best result for RNNs on this task without complex gating/cell mechanisms.
3.2

Character Level Language Modeling

In addition to the Penn-Treebank dataset, we also perform character level language modeling on two
larger datasets: text86 and Hutter Challenge Wikipedia7 . Both of them contain 100M characters from
Wikipedia while text8 has an alphabet size of 27 and Hutter Challenge Wikipedia has an alphabet
size of 205. For both datasets, we follow the training protocols in [12] and [1] respectively. We use
Adam for optimization with the starting learning rate grid-searched in {0.002, 0.001, 0.0005}. If the
validation BPC (bits-per-character) does not decrease for 2 epochs, we half the learning rate.
We implemented Multiplicative Integration on both vanilla-RNN and LSTM, referred to as MIRNN and MI-LSTM. The results for the text8 dataset are shown in Table 1, bottom middle panel.
All five models, including some of the previously published models, have the same number of
5

We perform hyper-parameter search for the initialization of {?, ?1 , ?2 , b} in MI-RNN-general.
http://mattmahoney.net/dc/textdata
7
http://prize.hutter1.net/
6

4

rW =

0.02 0.1 0.3 0.6

std

RNN 1.69 1.65 1.57 1.54 0.06
MI-RNN 1.39 1.40 1.40 1.41 0.008

WSJ Corpus

CER WER

DRNN+CTCbeamsearch [15]
Encoder-Decoder [16]
LSTM+CTCbeamsearch [17]
Eesen [18]
LSTM+CTC+WFST (ours)
MI-LSTM+CTC+WFST (ours)

10.0 14.1
6.4 9.3
9.2 8.7
7.3
6.5 8.7
6.0 8.2

Penn-Treebank

BPC

text8

BPC

RNN [12]
HF-MRNN [12]
RNN+stabalization [14]
MI-RNN (ours)
linear MI-RNN (ours)

1.42
1.41
1.48
1.39
1.48

RNN+smoothReLu [19]
HF-MRNN [12]
MI-RNN (ours)
LSTM (ours)
MI-LSTM(ours)

1.55
1.54
1.52
1.51
1.44

HutterWikipedia

BPC

stacked-LSTM [20]
GF-LSTM [1]
grid-LSTM [2]
MI-LSTM (ours)

1.67
1.58
1.47
1.44

Table 1: Top: test BPCs and the standard deviation of models with different scales of weight initializations. Top
right: test CERs and WERs on WSJ corpus. Bottom left: test BPCs on character level Penn-Treebank dataset.
Bottom middle: test BPCs on character level text8 dataset. Bottom right: test BPCs on character level Hutter
Prize Wikipedia dataset.

parameters (?4M). For RNNs without complex gating/cell mechanisms (the first three results), our
MI-RNN (with {?, ?1 , ?2 , b} initialized as {2, 0.5, 0.5, 0}) performs the best, our MI-LSTM (with
{?, ?1 , ?2 , b} initialized as {1, 0.5, 0.5, 0}) outperforms all other models by a large margin8 .
On Hutter Challenge Wikipedia dataset, we compare our MI-LSTM (single layer with 2048 unit,
?17M, with {?, ?1 , ?2 , b} initialized as {1, 1, 1, 0}) to the previous stacked LSTM (7 layers,
?27M) [20], GF-LSTM (5 layers, ?20M) [1], and grid-LSTM (6 layers, ?17M) [2]. Table 1, bottom
right panel, shows results. Despite the simple structure compared to the sophisticated connection
designs in GF-LSTM and grid-LSTM, our MI-LSTM outperforms all other models and achieves the
new state-of-the-art on this task.
3.3

Speech Recognition

We next evaluate our models on Wall Street Journal (WSJ) corpus (available as LDC corpus
LDC93S6B and LDC94S13B), where we use the full 81 hour set ?si284? for training, set ?dev93? for
validation and set ?eval92? for test. We follow the same data preparation process and model setting
as in [18], and we use 59 characters as the targets for the acoustic modelling. Decoding is done with
the CTC [21] based weighted finite-state transducers (WFSTs) [22] as proposed by [18].
Our model (referred to as MI-LSTM+CTC+WFST) consists of 4 bidirectional MI-LSTM layers, each with 320 units for each direction. CTC is performed on top to resolve the alignment
issue in speech transcription. For comparison, we also train a baseline model (referred to as
LSTM+CTC+WFST) with the same size but using vanilla LSTM. Adam with learning rate 0.0001
is used for optimization and Gaussian weight noise with zero mean and 0.05 standard deviation
is injected for regularization. We evaluate our models on the character error rate (CER) without
language model and the word error rate (WER) with extended trigram language model.
Table 1, top right panel, shows that MI-LSTM+CTC+WFST achieves quite good results on both CER
and WER compared to recent works, and it has a clear improvement over the baseline model. Note
that we did not conduct a careful hyper-parameter search on this task, hence one could potentially
obtain better results with better decoding schemes and regularization techniques.
3.4

Learning Skip-Thought Vectors

Next, we evaluate our Multiplicative Integration on the Skip-Thought model of [23]. Skip-Thought is
an encoder-decoder model that attempts to learn generic, distributed sentence representations. The
model produces sentence representation that are robust and perform well in practice, as it achieves
excellent results across many different NLP tasks. The model was trained on the BookCorpus dataset
that consists of 11,038 books with 74,004,228 sentences. Not surprisingly, a single pass through
8

[7] reports better results but they use much larger models (?16M) which is not directly comparable.

5

Semantic-Relatedness

r

?

MSE

Paraphrase detection Acc F1

uni-skip [23]
bi-skip [23]
combine-skip [23]

0.8477 0.7780 0.2872
0.8405 0.7696 0.2995
0.8584 0.7916 0.2687

uni-skip [23]
bi-skip [23]
combine-skip [23]

73.0 81.9
71.2 81.2
73.0 82.0

uni-skip (ours)
MI-uni-skip (ours)

0.8436 0.7735 0.2946
0.8588 0.7952 0.2679

uni-skip (ours)
MI-uni-skip (ours)

74.0 81.9
74.0 82.1

Classification

MR CR SUBJ MPQA

uni-skip [23]
75.5 79.3 92.1
bi-skip [23]
73.9 77.9 92.5
combine-skip [23] 76.5 80.1 93.6

86.9
83.3
87.1

uni-skip (ours)
75.9 80.1 93.0
MI-uni-skip (ours) 77.9 82.3 93.3

87.0
88.1

Attentive Reader

Val. Err.

LSTM [7]
BN-LSTM [7]
BN-everywhere [7]
LSTM (ours)
MI-LSTM (ours)
MI-LSTM+BN (ours)
MI-LSTM+BN-everywhere (ours)

0.5033
0.4951
0.5000
0.5053
0.4721
0.4685
0.4644

Table 2: Top left: skip-thought+MI on Semantic-Relatedness task. Top Right: skip-thought+MI on Paraphrase
Detection task. Bottom left: skip-thought+MI on four different classification tasks. Bottom right: Multiplicative
Integration (with batch normalization) on Teaching Machines to Read and Comprehend task.

the training data can take up to a week on a high-end GPU (as reported in [23]). Such training
speed largely limits one to perform careful hyper-parameter search. However, with Multiplicative
Integration, not only the training time is shortened by a factor of two, but the final performance is
also significantly improved.
We exactly follow the authors? Theano implementation of the skip-thought model9 : Encoder and
decoder are single-layer GRUs with hidden-layer size of 2400; all recurrent matrices adopt orthogonal
initialization while non-recurrent weights are initialized from uniform distribution. Adam is used
for optimization. We implemented Multiplicative Integration only for the encoder GRU (embedding
MI into decoder did not provide any substantial gains). We refer our model as MI-uni-skip, with
{?, ?1 , ?2 , b} initialized as {1, 1, 1, 0}. We also train a baseline model with the same size, referred
to as uni-skip(ours), which essentially reproduces the original model of [23].
During the course of training, we evaluated the skip-thought vectors on the semantic relatedness
task, using SICK dataset, every 2500 updates for both MI-uni-skip and the baseline model (each
iteration processes a mini-batch of size 64). The results are shown in Figure 2a. Note that MI-uni-skip
significantly outperforms the baseline, not only in terms of speed of convergence, but also in terms
of final performance. At around 125k updates, MI-uni-skip already exceeds the best performance
achieved by the baseline, which takes about twice the number of updates.
We also evaluated both models after one week of training, with the best results being reported on six
out of eight tasks reported in [23]: semantic relatedness task on SICK dataset, paraphrase detection
task on Microsoft Research Paraphrase Corpus, and four classification benchmarks: movie review
sentiment (MR), customer product reviews (CR), subjectivity/objectivity classification (SUBJ), and
opinion polarity (MPQA). We also compared our results with the results reported on three models in
the original skip-thought paper: uni-skip, bi-skip, combine-skip. Uni-skip is the same model as our
baseline, bi-skip is a bidirectional model of the same size, and combine-skip takes the concatenation
of the vectors from uni-skip and bi-skip to form a 4800 dimension vector for task evaluation. Table
2 shows that MI-uni-skip dominates across all the tasks. Not only it achieves higher performance
than the baseline model, but in many cases, it also outperforms the combine-skip model, which has
twice the number of dimensions. Clearly, Multiplicative Integration provides a faster and better way
to train a large-scale Skip-Thought model.
3.5

Teaching Machines to Read and Comprehend

In our last experiment, we show that the use of Multiplicative Integration can be combined with
other techniques for training RNNs, and the advantages of using MI still persist. Recently, [7]
introduced Recurrent Batch-Normalization. They evaluated their proposed technique on a uni9

https://github.com/ryankiros/skip-thoughts

6

MSE

0.34

(a)

0.32
0.30
0.28
0.26
0

(b)

0.70
uni-skip (ours)
MI-uni-skip (ours)
validation error

0.36

50
100
150
200
number of iterations (2.5k)

0.60
0.55
0.50
0.45
0

250

LSTM [7]
BN-LSTM [7]
MI-LSTM (ours)
MI-LSTM+BN (ours)

0.65

200
400
600
number of iterations (1k)

800

Figure 2: (a) MSE curves of uni-skip (ours) and MI-uni-skip (ours) on semantic relatedness task on SICK
dataset. MI-uni-skip significantly outperforms baseline uni-skip. (b) Validation error curves on attentive reader
models. There is a clear margin between models with and without MI.

directional Attentive Reader Model [24] for the question answering task using the CNN corpus10 . To
test our approach, we evaluated the following four models: 1. A vanilla LSTM attentive reader model
with a single hidden layer size 240 (same as [7]) as our baseline, referred to as LSTM (ours), 2. A
multiplicative integration LSTM with a single hidden size 240, referred to as MI-LSTM, 3. MILSTM with Batch-Norm, referred to as MI-LSTM+BN, 4. MI-LSTM with Batch-Norm everywhere
(as detailed in [7]), referred to as MI-LSTM+BN-everywhere. We compared our models to results
reported in [7] (referred to as LSTM, BN-LSTM and BN-LSTM everywhere) 11 .
For all MI models, {?, ?1 , ?2 , b} were initialized to {1, 1, 1, 0}. We follow the experimental
protocol of [7]12 and use exactly the same settings as theirs, except we remove the gradient clipping
for MI-LSTMs. Figure. 2b shows validation curves of the baseline (LSTM), MI-LSTM, BN-LSTM,
and MI-LSTM+BN, and the final validation errors of all models are reported in Table 2, bottom right
panel. Clearly, using Multiplicative Integration results in improved model performance regardless
of whether Batch-Norm is used. However, the combination of MI and Batch-Norm provides the
best performance and the fastest speed of convergence. This shows the general applicability of
Multiplication Integration when combining it with other optimization techniques.

4

Relationship to Previous Models

4.1

Relationship to Hidden Markov Models

One can show that under certain constraints, MI-RNN is effectively implementing the forward
algorithm of the Hidden Markov Model(HMM). A direct mapping can be constructed as follows (see
[25] for a similar derivation). Let U ? Rm?m be the state transition probability matrix with Uij =
Pr[ht+1 = i|ht = j], W ? Rm?n be the observation probability matrix with Wij = Pr[xt =
i|ht = j]. When xt is a one-hot vector (e.g., in many of the language modelling tasks), multiplying
it by W is effectively choosing a column of the observation matrix. Namely, if the j th entry of xt
is one, then Wxt = Pr[xt |ht = j]. Let h0 be the initial state distribution with h0 = Pr[h0 ] and
{ht }t?1 be the alpha values in the forward algorithm of HMM, i.e., ht = Pr[x1 , ..., xt , ht ]. Then
Uht = Pr[x1 , ..., xt , ht+1 ]. Thus ht+1 = Wxt+1  Uht = Pr[xt+1 |ht+1 ] ? Pr[x1 , ..., xt , ht+1 ] =
Pr[x1 , ..., xt+1 , ht+1 ]. To exactly implement the forward algorithm using Multiplicative Integration,
the matrices W and U have to be probability matrices, and xt needs to be a one-hot vector. The
function ? needs to be linear, and we drop all the bias terms. Therefore, RNN with Multiplicative
Integration can be seen as a nonlinear extension of HMMs. The extra freedom in parameter values
and nonlinearity makes the model more flexible compared to HMMs.
4.2

Relations to Second Order RNNs and Multiplicative RNNs

MI-RNN is related to the second order RNN [6] and the multiplicative RNN (MRNN) [9]. We first
describe the similarities with these two models:
The second order RNN involves a second order term st in a vanilla-RNN, where the ith element
st,i is computed by the bilinear form: st,i = xTt T (i) ht?1 , where T (i) ? Rn?m (1 ? i ? m) is
10

Note that [7] used a truncated version of the original dataset in order to save computation.
Learning curves and the final result number are obtained by emails correspondence with authors of [7].
12
https://github.com/cooijmanstim/recurrent-batch-normalization.git.
11

7

the ith slice of a tensor T ? Rm?n?m . Multiplicative Integration also involve a second order term
st = ?  Wxt  Uht?1 , but in our case st,i = ?i (wi ? xt )(ui ? ht?1 ) = xTt (?wi ? ui )ht?1 ,
where wi and ui are ith row in W and U, and ?i is the ith element of ?. Note that the outer product
?i wi ? ui is a rank-1 matrix. The Multiplicative RNN is also a second order RNN, but which
P (i) (i)
xt T
= Pdiag(Vxt )Q. For MI-RNN, we can
approximates T by a tensor decomposition
also think of the second order term as a tensor decomposition: ?  Wxt  Uht?1 = U(xt )ht?1 =
[diag(?)diag(Wxt )U]ht?1 .
There are however several differences that make MI a favourable model: (1) Simpler Parametrization:
MI uses a rank-1 approximation compared to the second order RNNs, and a diagonal approximation
compared to Multiplicative RNN. Moreover, MI-RNN shares parameters across the first and second
order terms, whereas the other two models do not. As a result, the number of parameters are largely
reduced, which makes our model more practical for large scale problems, while avoiding overfitting.
(2) Easier Optimization: In tensor decomposition methods, the products of three different (low-rank)
matrices generally makes it hard to optimize [9]. However, the optimization problem becomes
easier in MI, as discussed in section 2 and 3. (3) General structural design vs. vanilla-RNN design:
Multiplicative Integration can be easily embedded in many other RNN structures, e.g. LSTMs and
GRUs, whereas the second order RNN and MRNN present a very specific design for modifying
vanilla-RNNs.
Moreover, we also compared MI-RNN?s performance to the previous HF-MRNN?s results (Multiplicative RNN trained by Hessian-free method) in Table 1, bottom left and bottom middle panels, on
Penn-Treebank and text8 datasets. One can see that MI-RNN outperforms HF-MRNN on both tasks.
4.3

General Multiplicative Integration

Multiplicative Integration can be viewed as a general way of combining information flows from
two different sources. In particular, [26] proposed the ladder network that achieves promising
results on semi-supervised learning. In their model, they combine the lateral connections and the
backward connections via the ?combinator? function by a Hadamard product. The performance would
severely degrade without this product as empirically shown by [27]. [28] explored neural embedding
approaches in knowledge bases by formulating relations as bilinear and/or linear mapping functions,
and compared a variety of embedding models on the link prediction task. Surprisingly, the best
results among all bilinear functions is the simple weighted Hadamard product. They further carefully
compare the multiplicative and additive interactions and show that the multiplicative interaction
dominates the additive one.

5

Conclusion

In this paper we proposed to use Multiplicative Integration (MI), a simple Hadamard product to
combine information flow in recurrent neural networks. MI can be easily integrated into many popular
RNN models, including LSTMs and GRUs, while introducing almost no extra parameters. Indeed,
the implementation of MI requires almost no extra work beyond implementing RNN models. We also
show that MI achieves state-of-the-art performance on four different tasks or 11 datasets of varying
sizes and scales. We believe that the Multiplicative Integration can become a default building block
for training various types of RNN models.

Acknowledgments
The authors acknowledge the following agencies for funding and support: NSERC, Canada Research
Chairs, CIFAR, Calcul Quebec, Compute Canada, Disney research and ONR Grant N000141310721.
The authors thank the developers of Theano [29] and Keras [30], and also thank Jimmy Ba for many
thought-provoking discussions.

References
[1] Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. Gated feedback recurrent neural
networks. arXiv preprint arXiv:1502.02367, 2015.
[2] Nal Kalchbrenner, Ivo Danihelka, and Alex Graves. Grid long short-term memory. arXiv preprint
arXiv:1507.01526, 2015.

8

[3] Saizheng Zhang, Yuhuai Wu, Tong Che, Zhouhan Lin, Roland Memisevic, Ruslan Salakhutdinov,
and Yoshua Bengio. Architectural complexity measures of recurrent neural networks. arXiv preprint
arXiv:1602.08210, 2016.
[4] Sepp Hochreiter and J?rgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735?1780,
1997.
[5] Kyunghyun Cho, Bart Van Merri?nboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical
machine translation. arXiv preprint arXiv:1406.1078, 2014.
[6] Mark W Goudreau, C Lee Giles, Srimat T Chakradhar, and D Chen. First-order versus second-order
single-layer recurrent neural networks. Neural Networks, IEEE Transactions on, 5(3):511?513, 1994.
[7] Tim Cooijmans, Nicolas Ballas, C?sar Laurent, and Aaron Courville. Recurrent batch normalization.
http://arxiv.org/pdf/1603.09025v4.pdf, 2016.
[8] LE Baum and JA Eagon. An inequality with application to statistical estimation for probabilistic functions
of markov processes and to a model for ecology. Bulletin of the American Mathematical Society, 73:360?
363, 1967.
[9] Ilya Sutskever, James Martens, and Geoffrey E Hinton. Generating text with recurrent neural networks. In
Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 1017?1024,
2011.
[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
arXiv preprint arXiv:1512.03385, 2015.
[11] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated corpus
of english: The penn treebank. Computational linguistics, 19(2):313?330, 1993.
[12] Tom?? Mikolov, Ilya Sutskever, Anoop Deoras, Hai-Son Le, and Stefan Kombrink. Subword language
modeling with neural networks. preprint, (http://www.fit.vutbr.cz/imikolov/rnnlm/char.pdf), 2012.
[13] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
[14] David Krueger and Roland Memisevic. Regularizing rnns by stabilizing activations. arXiv preprint
arXiv:1511.08400, 2015.
[15] Awni Y Hannun, Andrew L Maas, Daniel Jurafsky, and Andrew Y Ng. First-pass large vocabulary
continuous speech recognition using bi-directional recurrent dnns. arXiv preprint arXiv:1408.2873, 2014.
[16] Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Philemon Brakel, and Yoshua Bengio. End-to-end
attention-based large vocabulary speech recognition. arXiv preprint arXiv:1508.04395, 2015.
[17] Alex Graves and Navdeep Jaitly. Towards end-to-end speech recognition with recurrent neural networks.
In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1764?1772,
2014.
[18] Yajie Miao, Mohammad Gowayyed, and Florian Metze. Eesen: End-to-end speech recognition using deep
rnn models and wfst-based decoding. arXiv preprint arXiv:1507.08240, 2015.
[19] Marius Pachitariu and Maneesh Sahani. Regularization and nonlinearities for neural language models:
when are they needed? arXiv preprint arXiv:1301.5650, 2013.
[20] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.
[21] Alex Graves, Santiago Fern?ndez, Faustino Gomez, and J?rgen Schmidhuber. Connectionist temporal
classification: labelling unsegmented sequence data with recurrent neural networks. In Proceedings of the
23rd international conference on Machine learning, pages 369?376. ACM, 2006.
[22] Mehryar Mohri, Fernando Pereira, and Michael Riley. Weighted finite-state transducers in speech recognition. Computer Speech & Language, 16(1):69?88, 2002.
[23] Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba,
and Sanja Fidler. Skip-thought vectors. In Advances in Neural Information Processing Systems, pages
3276?3284, 2015.
[24] Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,
and Phil Blunsom. Teaching machines to read and comprehend. In Advances in Neural Information
Processing Systems, pages 1684?1692, 2015.
[25] T. Wessels and C. W. Omlin. Refining hidden markov models with recurrent neural networks. In Neural
Networks, 2000. IJCNN 2000, Proceedings of the IEEE-INNS-ENNS International Joint Conference on,
volume 2, pages 271?276 vol.2, 2000.
[26] Antti Rasmus, Harri Valpola, Mikko Honkala, Mathias Berglund, and Tapani Raiko. Semi-supervised
learning with ladder network. arXiv preprint arXiv:1507.02672, 2015.
[27] Mohammad Pezeshki, Linxi Fan, Philemon Brakel, Aaron Courville, and Yoshua Bengio. Deconstructing
the ladder network architecture. arXiv preprint arXiv:1511.06430, 2015.
[28] Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding entities and relations
for learning and inference in knowledge bases. arXiv preprint arXiv:1412.6575, 2014.
[29] Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, and et al. Theano: A python framework for fast
computation of mathematical expressions, 2016.
[30] Fran?ois Chollet. Keras. GitHub repository: https://github.com/fchollet/keras, 2015.

9


----------------------------------------------------------------

title: 6279-natural-parameter-networks-a-class-of-probabilistic-neural-networks.pdf

Natural-Parameter Networks:
A Class of Probabilistic Neural Networks
Hao Wang, Xingjian Shi, Dit-Yan Yeung
Hong Kong University of Science and Technology
{hwangaz,xshiab,dyyeung}@cse.ust.hk

Abstract
Neural networks (NN) have achieved state-of-the-art performance in various applications. Unfortunately in applications where training data is insufficient, they are
often prone to overfitting. One effective way to alleviate this problem is to exploit
the Bayesian approach by using Bayesian neural networks (BNN). Another shortcoming of NN is the lack of flexibility to customize different distributions for the
weights and neurons according to the data, as is often done in probabilistic graphical models. To address these problems, we propose a class of probabilistic neural
networks, dubbed natural-parameter networks (NPN), as a novel and lightweight
Bayesian treatment of NN. NPN allows the usage of arbitrary exponential-family
distributions to model the weights and neurons. Different from traditional NN
and BNN, NPN takes distributions as input and goes through layers of transformation before producing distributions to match the target output distributions. As
a Bayesian treatment, efficient backpropagation (BP) is performed to learn the
natural parameters for the distributions over both the weights and neurons. The
output distributions of each layer, as byproducts, may be used as second-order
representations for the associated tasks such as link prediction. Experiments on
real-world datasets show that NPN can achieve state-of-the-art performance.

1

Introduction

Recently neural networks (NN) have achieved state-of-the-art performance in various applications
ranging from computer vision [12] to natural language processing [20]. However, NN trained by
stochastic gradient descent (SGD) or its variants is known to suffer from overfitting especially
when training data is insufficient. Besides overfitting, another problem of NN comes from the
underestimated uncertainty, which could lead to poor performance in applications like active learning.
Bayesian neural networks (BNN) offer the promise of tackling these problems in a principled way.
Early BNN works include methods based on Laplace approximation [16], variational inference (VI)
[11], and Monte Carlo sampling [18], but they have not been widely adopted due to their lack of
scalability. Some recent advances in this direction seem to shed light on the practical adoption of
BNN. [8] proposed a method based on VI in which a Monte Carlo estimate of a lower bound on the
marginal likelihood is used to infer the weights. Recently, [10] used an online version of expectation
propagation (EP), called ?probabilistic back propagation? (PBP), for the Bayesian learning of NN,
and [4] proposed ?Bayes by Backprop? (BBB), which can be viewed as an extension of [8] based on
the ?reparameterization trick? [13]. More recently, an interesting Bayesian treatment called ?Bayesian
dark knowledge? (BDK) was designed to approximate a teacher network with a simpler student
network based on stochastic gradient Langevin dynamics (SGLD) [1].
Although these recent methods are more practical than earlier ones, several outstanding problems
remain to be addressed: (1) most of these methods require sampling either at training time [8, 4, 1] or
at test time [4], incurring much higher cost than a ?vanilla? NN; (2) as mentioned in [1], methods
30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

based on online EP or VI do not involve sampling, but they need to compute the predictive density
by integrating out the parameters, which is computationally inefficient; (3) these methods assume
Gaussian distributions for the weights and neurons, allowing no flexibility to customize different
distributions according to the data as is done in probabilistic graphical models (PGM).
To address the problems, we propose natural-parameter networks (NPN) as a class of probabilistic
neural networks where the input, target output, weights, and neurons can all be modeled by arbitrary
exponential-family distributions (e.g., Poisson distributions for word counts) instead of being limited
to Gaussian distributions. Input distributions go through layers of linear and nonlinear transformation
deterministically before producing distributions to match the target output distributions (previous
work [21] shows that providing distributions as input by corrupting the data with noise plays the
role of regularization). As byproducts, output distributions of intermediate layers may be used as
second-order representations for the associated tasks. Thanks to the properties of the exponential
family [3, 19], distributions in NPN are defined by the corresponding natural parameters which can
be learned efficiently by backpropagation. Unlike [4, 1], NPN explicitly propagates the estimates of
uncertainty back and forth in deep networks. This way the uncertainty estimates for each layer of
neurons are readily available for the associated tasks. Our experiments show that such information is
helpful when neurons of intermediate layers are used as representations like in autoencoders (AE). In
summary, our main contributions are:
? We propose NPN as a class of probabilistic neural networks. Our model combines the merits
of NN and PGM in terms of computational efficiency and flexibility to customize the types
of distributions for different types of data.
? Leveraging the properties of the exponential family, some sampling-free backpropagationcompatible algorithms are designed to efficiently learn the distributions over weights by
learning the natural parameters.
? Unlike most probabilistic NN models, NPN obtains the uncertainty of intermediate-layer
neurons as byproducts, which provide valuable information to the learned representations.
Experiments on real-world datasets show that NPN can achieve state-of-the-art performance
on classification, regression, and unsupervised representation learning tasks.

2

Natural-Parameter Networks

The exponential family refers to an important class of distributions with useful algebraic properties.
Distributions in the exponential family have the form p(x|?) = h(x)g(?) exp{? T u(x)}, where x is
the random variable, ? denotes the natural parameters, u(x) is a vector of sufficient statistics, and
g(?) is the normalizer. For a given type of distributions, different choices of ? lead to different shapes.
c
1
For example, a univariate Gaussian distribution with ? = (c, d)T corresponds to N (? 2d
, ? 2d
).
Motivated by this observation, in NPN, only the natural parameters need to be learned to model the
distributions over the weights and neurons. Consider an NPN which takes a vector random distribution
(e.g., a multivariate Gaussian distribution) as input, multiplies it by a matrix random distribution,
goes through nonlinear transformation, and outputs another distribution. Since all three distributions
in the process can be specified by their natural parameters (given the types of distributions), learning
and prediction of the network can actually operate in the space of natural parameters. For example, if
we use element-wise (factorized) gamma distributions for both the weights and neurons, the NPN
counterpart of a vanilla network only needs twice the number of free parameters (weights) and
neurons since there are two natural parameters for each univariate gamma distribution.
2.1

Notation and Conventions

We use boldface uppercase letters like W to denote matrices and boldface lowercase letters like
b for vectors. Similarly, a boldface number (e.g., 1 or 0) represents a row vector or a matrix with
identical entries. In NPN, o(l) is used to denote the values of neurons in layer l before nonlinear
transformation and a(l) is for the values after nonlinear transformation. As mentioned above, NPN
tries to learn distributions over variables rather than variables themselves. Hence we use letters
without subscripts c, d, m, and s (e.g., o(l) and a(l) ) to denote ?random variables? with corresponding
distributions. Subscripts c and d are used to denote natural parameter pairs, such as Wc and Wd .
Similarly, subscripts m and s are for mean-variance pairs. Note that for clarity, many operations used
?z
below are implicitly element-wise, for example, the square z2 , division bz , partial derivative ?b
, the
2

gamma function ?(z), logarithm log z, factorial z!, 1 + z, and z1 . For the data D = {(xi , yi )}N
i=1 ,
(0)
(0)
(0)
we set am = xi , as = 0 (Input distributions with as 6= 0 resemble AE?s denoising effect.) as
input of the network and yi denotes the output targets (e.g., labels and word counts). In the following
text we drop the subscript i (and sometimes the superscript (l)) for clarity. The bracket (?, ?) denotes
concatenation or pairs of vectors.
2.2

Linear Transformation in NPN

Here we first introduce the linear form of a general NPN. For simplicity, we assume distributions
with two natural parameters (e.g., gamma distributions, beta distributions, and Gaussian distributions), ? = (c, d)T , in this section. Specifically, we have factorized distributions on the weight
Q
(l)
(l)
(l)
(l)
(l)
(l)
(l)
matrices, p(W(l) |Wc , Wd ) = i,j p(Wij |Wc,ij , Wd,ij ), where the pair (Wc,ij , Wd,ij ) is the
corresponding natural parameters. For b(l) , o(l) , and a(l) we assume similar factorized distributions.
In a traditional NN, the linear transformation follows o(l) = a(l?1) W(l) + b(l) where a(l?1) is the
output from the previous layer. In NN a(l?1) , W(l) , and b(l) are deterministic variables while in
NPN they are exponential-family distributions, meaning that the result o(l) is also a distribution. For
convenience of subsequent computation it is desirable to approximate o(l) using another exponentialfamily distribution. We can do this by matching the mean and variance. Specifically, after computing
(l)
(l)
(l)
(l)
(l)
(l)
(l)
(l)
(l)
(l)
(Wm , Ws ) = f (Wc , Wd ) and (bm , bs ) = f (bc , bd ), we can get oc and od through
(l)
(l)
the mean om and variance os of o(l) as follows:
(l?1)

(a(l?1)
, a(l?1)
) = f (a(l?1)
, ad
m
s
c
o(l)
s
(l)
(o(l)
,
o
c
d )

=

a(l?1)
Ws(l)
s

=f

?1

+

(l?1)
(l)
), o(l)
Wm
+ b(l)
m = am
m,

(l)
a(l?1)
(Wm
s

?

(l)
Wm
)

+

(a(l?1)
m

(1)
?

a(l?1)
)Ws(l)
m

+

b(l)
s ,

(2)

(l)
(o(l)
m , os ),

(3)

where ? denotes the element-wise product and the bijective function f (?, ?) maps the natural paramec+1
ters of a distribution into its mean and variance (e.g., f (c, d) = ( c+1
?d , d2 ) in gamma distributions).
(l)

(l)

(l)

(l)

Similarly we use f ?1 (?, ?) to denote the inverse transformation. Wm , Ws , bm , and bs are the
(l)
mean and variance of W(l) and b(l) obtained from the natural parameters. The computed om and
(l)
(l)
(l)
os can then be used to recover oc and od , which will subsequently facilitate the feedforward
computation of the nonlinear transformation described in Section 2.3.
2.3

Nonlinear Transformation in NPN
(l)

After we obtain the linearly transformed distribution over o(l) defined by natural parameters oc and
(l)
od , an element-wise nonlinear transformation v(?) (with a well defined inverse function v ?1 (?)) will
0
be imposed. The resulting activation distribution is pa (a(l) ) = po (v ?1 (a(l) ))|v ?1 (a(l) )|, where po
(l)
(l)
is the factorized distribution over o(l) defined by (oc , od ).
Though pa (a(l) ) may not be an exponential-family distribution, we can approximate it with one,
(l)
(l)
(l)
(l)
p(a(l) |ac , ad ), by matching the first two moments. Once the mean am and variance as of pa (a(l) )
?1
are obtained, we can compute corresponding natural parameters with f (?, ?) (approximation
accuracy is sufficient according to preliminary experiments). The feedforward computation is:
Z
am =

Z
po (o|oc , od )v(o)do, as =

po (o|oc , od )v(o)2 do ? a2m , (ac , ad ) = f ?1 (am , as ).

(4)

Here the key computational challenge is computing the integrals in Equation (4). Closed-form
solutions are needed for their efficient computation. If po (o|oc , od ) is a Gaussian distribution, closedform solutions exist for common activation functions like tanh(x) and max(0, x) (details are in
Section 3.2). Unfortunately this is not the case for other distributions. Leveraging the convenient
form of the exponential family, we find that it is possible to design activation functions so that the
integrals for non-Gaussian distributions can also be expressed in closed form.
Theorem 1. Assume an exponential-family distribution po (x|?) = h(x)g(?) exp{? T u(x)}, where
the vector u(x) = (u1 (x), u2 (x), . . . , uM (x))T (M is the number of natural parameters).
If activaR
tion function v(x) = r ? q exp(?? ui (x)) is used, the first two moments of v(x), po (x|?)v(x)dx
3

Table 1: Activation Functions for Exponential-Family Distributions
Distribution

Probability Density Function

Beta Distribution

p(x) =

Rayleigh Distribution
Gamma Distribution

p(x) =
p(x) =

Poisson Distribution

p(x) =

Gaussian Distribution

p(x) =

R

?(c+d)
xc?1 (1 ? x)d?1
?(c)?(d)
x
x2
exp{?
}
?2
2? 2
c c?1
1
d
x
exp{?dx}
?(c)
cx exp{?c}
x!
1
(2?? 2 )? 2 exp{? 2?12 (x ?

?)2 }

Activation Function

Support

qx? , ? ? (0, 1)

[0, 1]

r ? q exp{?? x2 }
r ? q exp{?? x}

(0, +?)
(0, +?)

r ? q exp{?? x}

Nonnegative interger

ReLU, tanh, and sigmoid

(??, +?)

2

and po (x|?)v(x) dx, can be expressed in closed form. Here i ? {1, 2, . . . , M } (different ui (x)
corresponds to a different set of activation functions) and r, q, and ? are constants.
e = (?1 , ?2 , . . . , ?i ? ?, . . . , ?M ), and ?
b =
Proof. We first let ? = (?1 , ?2 , . . . , ?M ), ?
(?1 , ?2 , . . . , ?i ? 2?, . . . , ?M ). The first moment of v(x) is
Z
E(v(x)) = r ? q h(x)g(?) exp{? T u(x) ? ? ui (x)} dx
Z
g(?)
g(?)
g(e
? ) exp{e
? T u(x)} dx = r ? q
.
= r ? q h(x)
g(e
?)
g(e
?)
g(? )
g(? )
Similarly the second moment can be computed as E(v(x)2 ) = r2 + q 2 g(?
b ) ? 2rq g(?
e) .
A more detailed proof is provided in the supplementary material. With Theorem 1, what remains is to
find the constants that make v(x) strictly increasing and bounded (Table 1 shows some exponentialfamily distributions and their possible activation functions). For example in Equation (4), if v(x) =
d
r ? q exp(?? x), am = r ? q( odo+?
)oc for the gamma distribution.
In the backpropagation, for distributions with two natural parameters the gradient consists of two
?as
?E
?E
?E
m
terms. For example, ?o
= ?a
? ?a
?oc + ?as ? ?oc , where E is the error term of the network.
c
m
Algorithm 1 Deep Nonlinear NPN
1: Input: Data D = {(xi , yi )}N
i=1 , number of iterations T , learning rate ?t , number of layers L.
2: for t = 1 : T do
3:
for l = 1 : L do
4:
Apply Equation (1)-(4) to compute the linear and nonlinear transformation in layer l.
5:
end for
(L)
(L)
(L)
(L)
6:
Compute the error E from (oc , od ) or (ac , ad ).
7:
for l = L : 1 do
8:
Compute ?E(l) , ?E(l) , ?E(l) , and ?E(l) . Compute ?E(l) , ?E(l) , ?E(l) , and ?E(l) .
?Wm

?Ws

?bm

?bs

?Wc

?Wd

?bc

?bd

9:
end for
(l)
(l)
(l)
(l)
10:
Update Wc , Wd , bc , and bd in all layers.
11: end for

2.4

Deep Nonlinear NPN

Naturally layers of nonlinear NPN can be stacked to form a deep NPN1 , as shown in Algorithm 12 . A
deep NPN is in some sense similar to a PGM with a chain structure. Unlike PGM in general, however,
NPN does not need costly inference algorithms like variational inference or Markov chain Monte
Carlo. For some chain-structured PGM (e.g, hidden Markov models), efficient inference algorithms
also exist due to their special structure. Similarly, the Markov property enables NPN to be efficiently
trained in an end-to-end backpropagation learning fashion in the space of natural parameters.
PGM is known to be more flexible than NN in the sense that it can choose different distributions to
depict different relationships among variables. A major drawback of PGM is its scalability especially
1

Although the approximation accuracy may decrease as NPN gets deeper during feedforward computation, it
can be automatically adjusted according to data during backpropagation.
2
Note that since the first part of Equation (1) and the last part of Equation (4) are canceled out, we can
(l)
(l)
(l)
(l)
directly use (am , as ) without computing (ac , ad ) here.

4

when the PGM is deep. Different from PGM, NN stacks relatively simple computational layers and
learns the parameters using backpropagation, which is computationally more efficient than most
algorithms for PGM. NPN has the potential to get the best of both worlds. In terms of flexibility,
different types of exponential-family distributions can be chosen for the weights and neurons. Using
gamma distributions for both the weights and neurons in NPN leads to a deep and nonlinear version
of nonnegative matrix factorization [14] while an NPN with the Bernoulli distribution and sigmoid
activation resembles a Bayesian treatment of sigmoid belief networks [17]. If Poisson distributions
are chosen for the neurons, NPN becomes a neural analogue of deep Poisson factor analysis [26, 9].
Note that similar to the weight decay in NN, we may add the KL divergence between the prior
distributions and the learned distributions on the weights to the error E for regularization (we use
isotropic Gaussian priors in the experiments). In NPN, the chosen prior distributions correspond to
priors in Bayesian models and the learned distributions correspond to the approximation of posterior
distributions on weights. Note that the generative story assumed here is that weights are sampled
from the prior, and then output is generated (given all data) from these weights.

3

Variants of NPN

In this section, we introduce three NPN variants with different properties to demonstrate the flexibility
and effectiveness of NPN. Note that in practice we use a transformed version of the natural parameters,
referred to as proxy natural parameters here, instead of the original ones for computational efficiency.
For example, in gamma distributions p(x|c, d) = ?(c)?1 dc xc?1 exp(?dx), we use proxy natural
parameters (c, d) during computation rather than the natural parameters (c ? 1, ?d).
3.1

Gamma NPN

The gamma distribution with support over positive values is an important member of the exponential
family. The corresponding probability density function is p(x|c, d) = ?(c)?1 dc xc?1 exp(?dx) with
(c ? 1, ?d) as its natural parameters (we use (c, d) as proxy natural parameters). If we assume
gamma distributions for W(l) , b(l) , o(l) , and a(l) , an AE formed by NPN becomes a deep and
nonlinear version of nonnegative matrix factorization [14]. To see this, note that this AE with
activation v(x) = x and zero biases b(l) is equivalent to finding a factorization of matrix X such that
QL
X = H l= L W(l) where H denotes the middle-layer neurons and W(l) has nonnegative entries
2

(l)

(l)

(l)

(l)

from gamma distributions. In this gamma NPN, parameters Wc , Wd , bc , and bd can be learned
following Algorithm 1. We detail the algorithm as follows:
Linear Transformation: Since gamma distributions are assumed here, we can use the function
(l)
(l)
(l)
(l)
(l)
(l)
(l)
(l)
f (c, d) = ( dc , dc2 ) to compute (Wm , Ws ) = f (Wc , Wd ), (bm , bs ) = f (bc , bd ), and
(l)
(l)
(l)
(l)
(oc , od ) = f ?1 (om , os ) during the probabilistic linear transformation in Equation (1)-(3).
Nonlinear Transformation: With the proxy natural parameters for the gamma distributions over
(l)
(l)
o(l) , the mean am and variance as for the nonlinearly transformed distribution over a(l) would
be obtained with Equation (4). Following Theorem 1, closed-form solutions are possible with
v(x) = r(1 ? exp(?? x)) (r = q and ui (x) = x) where r and ? are constants. Using this new
activation function, we have (see Section 2.1 and 6.1 of the supplementary material for details on the
function and derivation):
Z
ooc
od oc
am = po (o|oc , od )v(o)do = r(1 ? d ? ?(oc ) ? (od + ? )?oc ) = r(1 ? (
) ),
?(oc )
od + ?
od
od 2oc
as = r2 ((
)oc ? (
) ).
od + 2?
od + ?
(L)

Error: With oc

(L)

and od , we can compute the regression error E as the negative log-likelihood:
(L)

(L)
E = (log ?(o(L)
? log od
c ) ? oc

(L)

? (o(L)
? 1) ? log y + od
c

? y)1T ,

where y is the observed output corresponding to x. For classification, cross-entropy loss can be used
(l)
(l)
(l)
(l)
as E. Following the computation flow above, BP can be used to learn Wc , Wd , bc , and bd .
5

100
80
60
40

Y

20
0
?20
?40
?60
?80
?100
?6

?4

?2

0
X

2

4

6

Figure 1: Predictive distributions for PBP, BDK, dropout NN, and NPN. The shaded regions correspond to ?3 standard deviations. The black curve is the data-generating function and blue curves
show the mean of the predictive distributions. Red stars are the training data.
3.2 Gaussian NPN
Different from the gamma distribution which has support over positive values only, the Gaussian
distribution, also an exponential-family distribution, can describe real-valued random variables. This
makes it a natural choice for NPN. We refer to this NPN variant with Gaussian distributions over both
the weights and neurons as Gaussian NPN. Details of Algorithm 1 for Gaussian NPN are as follows:
Linear Transformation: Besides support over real values, another property of Gaussian distributions
is that the mean and variance can be used as proxy natural parameters, leading to an identity mapping
function f (c, d) = (c, d) which cuts the computation cost. We can use this function to compute
(l)
(l)
(l)
(l)
(l)
(l)
(l)
(l)
(l)
(l)
(l)
(l)
(Wm , Ws ) = f (Wc , Wd ), (bm , bs ) = f (bc , bd ), and (oc , od ) = f ?1 (om , os )
during the probabilistic linear transformation in Equation (1)-(3).
1
is used, am in
Nonlinear Transformation: If the sigmoid activation v(x) = ?(x) = 1+exp(?x)
Equation (4) would be (convolution of Gaussian with sigmoid is approximated by another sigmoid):
Z
oc
(5)
am = N (o|oc , diag(od )) ? ?(o)do ? ?(
1 ),
(1 + ? 2 od ) 2
Z
?(oc + ?)
as = N (o|oc , diag(od )) ? ?(o)2 do ? a2m ? ?(
) ? a2m ,
(6)
(1 + ? 2 ?2 od )1/2
?
?
where ? = 4 ? 2 2, ? = ? log( 2 + 1), and ? 2 = ?/8. Similar approximation can be applied for
activation v(x) = tanh(x) since tanh(x) = 2?(2x) ? 1.

If the ReLU activation v(x) = max(0, x) is used, we can use the techniques in [6] to obtain the first
two moments of max(z1 , z2 ) where z1 and z2 are Gaussian random variables. Full derivation for
v(x) = ?(x), v(x) = tanh(x), and v(x) = max(0, x) is left to the supplementary material.
(L)

(L)

Error: With oc and od in the last layer, we can then compute the error E as the KL divergence
(L)
(L)
KL(N (oc , diag(od )) k N (ym , diag())), where  is a vector with all entries equal to a small
 1T + ( 1 )(o(L) ? y)T ? K + (log o(L) )1T ? K log ). For
value . Hence the error E = 12 ( (L)
c
(L)
d
od

od

classification tasks, cross-entropy loss is used. Following the computation flow above, BP can be
(l)
(l)
(l)
(l)
used to learn Wc , Wd , bc , and bd .
3.3

Poisson NPN

The Poisson distribution, as another member of the exponential family, is often used to model counts
(e.g., counts of words, topics, or super topics in documents). Hence for text modeling, it is natural to
assume Poisson distributions for neurons in NPN. Interestingly, this design of Poisson NPN can be
seen as a neural analogue of some Poisson factor analysis models [26].
Besides closed-form nonlinear transformation, another challenge of Poisson NPN is to map the pair
(l)
(l)
(l)
(om , os ) to the single parameter
q oc of Poisson distributions. According to the central limit theorem,
(l)

(l)

(l)

(l)

we have oc = 14 (2om ? 1 + (2om ? 1)2 + 8os ) (see Section 3 and 6.3 of the supplementary
material for proofs, justifications, and detailed derivation of Poisson NPN).

4

Experiments

In this section we evaluate variants of NPN and other state-of-the-art methods on four real-world
datasets. We use Matlab (with GPU) to implement NPN, AE variants, and the ?vanilla? NN trained
with dropout SGD (dropout NN). For other baselines, we use the Theano library [2] and MXNet [5].
6

Table 2: Test Error Rates on MNIST
Method
Error

BDK
1.38%

BBB
1.34%

Dropout1
1.33%

Dropout2
1.40%

gamma NPN
1.27%

Gaussian NPN
1.25%

Table 3: Test Error Rates for Different Size of Training Data
Size
NPN
Dropout
BDK

4.1

100
29.97%
32.58%
30.08%

500
13.79%
15.39%
14.34%

2,000
7.89%
8.78%
8.31%

10,000
3.28%
3.53%
3.55%

Toy Regression Task

To gain some insights into NPN, we start with a toy 1d regression task so that the predicted mean and
variance can be visualized. Following [1], we generate 20 points in one dimension from a uniform
distribution in the interval [?4, 4]. The target outputs are sampled from the function y = x3 + n ,
where n ? N (0, 9). We fit the data with the Gaussian NPN, BDK, and PBP (see the supplementary
material for detailed hyperparameters). Figure 1 shows the predicted mean and variance of NPN,
BDK, and PBP along with the mean provided by the dropout NN (for larger versions of figures please
refer to the end of the supplementary materials). As we can see, the variance of PBP, BDK, and NPN
diverges as x is farther away from the training data. Both NPN?s and BDK?s predictive distributions
are accurate enough to keep most of the y = x3 curve inside the shaded regions with relatively low
variance. An interesting observation is that the training data points become more scattered when
x > 0. Ideally, the variance should start diverging from x = 0, which is what happens in NPN.
However, PBP and BDK are not sensitive enough to capture this dispersion change. In another dataset,
Boston Housing, the root mean square error for PBP, BDK, and NPN is 3.01, 2.82, and 2.57.
4.2

MNIST Classification

The MNIST digit dataset consists of 60,000 training images and 10,000 test images. All images
are labeled as one of the 10 digits. We train the models with 50,000 images and use 10,000 images
for validation. Networks with a structure of 784-800-800-10 are used for all methods, since 800
works best for the dropout NN (denoted as Dropout1 in Table 2) and BDK (BDK with a structure of
784-400-400-10 achieves an error rate of 1.41%). We also try the dropout NN with twice the number
of hidden neurons (Dropout2 in Table 2) for fair comparison. For BBB, we directly quote their results
from [4]. We implement BDK and NPN using the same hyperparameters as in [1] whenever possible.
Gaussian priors are used for NPN (see the supplementary material for detailed hyperparameters).
1
0.8
Accuracy

As shown in Table 2, BDK and BBB achieve comparable performance
with dropout NN (similar to [1], PBP is not included in the comparison
since it supports regression only), and gamma NPN slightly outperforms
dropout NN. Gaussian NPN is able to achieve a lower error rate of
1.25%. Note that BBB with Gaussian priors can only achieve an error
rate of 1.82%; 1.34% is the result of using Gaussian mixture priors. For
reference, the error rate for dropout NN with 1600 neurons in each hidden
layer is 1.40%. The time cost per epoch is 18.3s, 16.2s, and 6.4s for NPN,
BDK, NN respectively. Note that BDK is in C++ and NPN is in Matlab.

0.6
0.4
0.2
0

1

2

3

4 5 6
Variance

7

8

9

Figure 2: Classification accuracy
for different variance (uncertainty).
Note that ?1? in the x-axis means
as(L) 1T ? [0, 0.04), ?2? means
as(L) 1T ? [0.04, 0.08), etc.

To evaluate NPN?s ability as a Bayesian treatment to avoid overfitting,
we vary the size of the training set (from 100 to 10,000 data points) and compare the test error rates.
As shown in Table 3, the margin between the Gaussian NPN and dropout NN increases as the training
set shrinks. Besides, to verify the effectiveness of the estimated uncertainty, we split the test set into
(L)
9 subsets according NPN?s estimated variance (uncertainty) as 1T for each sample and show the
accuracy for each subset in Figure 2. We can find that the more uncertain NPN is, the lower the
accuracy, indicating that the estimated uncertainty is well calibrated.
4.3

Second-Order Representation Learning

Besides classification and regression, we also consider the problem of unsupervised representation
learning with a subsequent link prediction task. Three real-world datasets, Citeulike-a, Citeulike-t,
and arXiv, are used. The first two datasets are from [22, 23], collected separately from CiteULike in
different ways to mimic different real-world settings. The third one is from arXiv as one of the SNAP
datasets [15]. Citeulike-a consists of 16,980 documents, 8,000 terms, and 44,709 links (citations).
7

Table 4: Link Rank on Three Datasets
Method
Citeulike-a
Citeulike-t
arXiv

SAE
1104.7
2109.8
4232.7

SDAE
992.4
1356.8
2916.1

VAE
980.8
1599.6
3367.2

gamma NPN
851.7 (935.8)
1342.3 (1400.7)
2796.4 (3038.8)

Gaussian NPN
750.6 (823.9)
1280.4 (1330.7)
2687.9 (2923.8)

Poisson NPN
690.9 (5389.7)
1354.1 (9117.2)
2684.1 (10791.3)

Citeulike-t consists of 25,975 documents, 20,000 terms, and 32,565 links. The last dataset, arXiv,
consists of 27,770 documents, 8,000 terms, and 352,807 links.
The task is to perform unsupervised representation learning before feeding the extracted representations (middle-layer neurons) into a Bayesian LR algorithm [3]. We use the stacked autoencoder (SAE)
[7], stacked denoising autoencoder (SDAE) [21], variational autoencoder (VAE) [13] as baselines
(hyperparameters like weight decay and dropout rate are chosen by cross validation). As in SAE,
we use different variants of NPN to form autoencoders where both the input and output targets are
bag-of-words (BOW) vectors for the documents. The network structure for all models is B-100-50
(B is the number of terms). Please refer to the supplementary material for detailed hyperparameters.
350
300
Reconstruction error

One major advantage of NPN over SAE and SDAE is that the learned representations are distributions instead of point estimates. Since representations
from NPN contain both the mean and variance, we call them secondorder representations. Note that although VAE also produces second-order
representations, the variance part is simply parameterized by multilayer
perceptrons while NPN?s variance is naturally computed through propagation of distributions. These 50-dimensional representations with both mean
and variance are fed into a Bayesian LR algorithm for link prediction (for
deterministic AE the variance is set to 0).

[!h]

250
200
150
100
50
0
0

5

10
Variance

15

20

Figure 3: Reconstruction error
and estimated uncertainty for
each data point in Citeulike-a.

We use links among 80% of the nodes (documents) to train the Bayesian LR and use other links as
the test set. link rank and AUC (area under the ROC curve) are used as evaluation metrics. The link
rank is the average rank of the observed links from test nodes to training nodes. We compute the
AUC for every test node and report the average values. By definition, lower link rank and higher
AUC indicate better predictive performance and imply more powerful representations.
Table 4 shows the link rank for different models. For fair comparison we also try all baselines with
double budget (a structure of B-200-50) and report whichever has higher accuracy. As we can see, by
treating representations as distributions rather than points in a vector space, NPN is able to achieve
much lower link rank than all baselines, including VAE with variance information. The numbers in
the brackets show the link rank of NPN if we discard the variance information. The performance
gain from variance information verifies the effectiveness of the variance (uncertainty) estimated by
NPN. Among different variants of NPN, the Gaussian NPN seems to perform better in datasets with
fewer words like Citeulike-t (only 18.8 words per document). The Poisson NPN, as a more natural
choice to model text, achieves the best performance in datasets with more words (Citeulike-a and
arXiv). The performance in AUC is consistent with that in terms of the link rank (see Section 4 of the
supplementary material). To further verify the effectiveness of the estimated uncertainty, we plot the
(L)
reconstruction error and the variance os 1T for each data point of Citeulike-a in Figure 3. As we
can see, higher uncertainty often indicates not only higher reconstruction error E but also higher
variance in E.

5

Conclusion

We have introduced a family of models, called natural-parameter networks, as a novel class of probabilistic NN to combine the merits of NN and PGM. NPN regards the weights and neurons as arbitrary
exponential-family distributions rather than just point estimates or factorized Gaussian distributions.
Such flexibility enables richer descriptions of hierarchical relationships among latent variables and
adds another degree of freedom to customize NN for different types of data. Efficient sampling-free
backpropagation-compatible algorithms are designed for the learning of NPN. Experiments show that
NPN achieves state-of-the-art performance on classification, regression, and representation learning
tasks. As possible extensions of NPN, it would be interesting to connect NPN to arbitrary PGM to
form fully Bayesian deep learning models [24, 25], allowing even richer descriptions of relationships
among latent variables. It is also worth noting that NPN cannot be defined as generative models
and, unlike PGM, the same NPN model cannot be used to support multiple types of inference (with
different observed and hidden variables). We will try to address these limitations in our future work.
8

References
[1] A. K. Balan, V. Rathod, K. P. Murphy, and M. Welling. Bayesian dark knowledge. In NIPS, 2015.
[2] F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. J. Goodfellow, A. Bergeron, N. Bouchard, and Y. Bengio.
Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS
2012 Workshop, 2012.
[3] C. M. Bishop. Pattern Recognition and Machine Learning. Springer-Verlag New York, Inc., Secaucus, NJ,
USA, 2006.
[4] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra. Weight uncertainty in neural network. In
ICML, 2015.
[5] T. Chen, M. Li, Y. Li, M. Lin, N. Wang, M. Wang, T. Xiao, B. Xu, C. Zhang, and Z. Zhang. Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems. CoRR, abs/1512.01274,
2015.
[6] C. E. Clark. The greatest of a finite set of random variables. Operations Research, 9(2):145?162, 1961.
[7] I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. Book in preparation for MIT Press, 2016.
[8] A. Graves. Practical variational inference for neural networks. In NIPS, 2011.
[9] R. Henao, Z. Gan, J. Lu, and L. Carin. Deep poisson factor modeling. In NIPS, 2015.
[10] J. M. Hern?ndez-Lobato and R. Adams. Probabilistic backpropagation for scalable learning of Bayesian
neural networks. In ICML, 2015.
[11] G. E. Hinton and D. Van Camp. Keeping the neural networks simple by minimizing the description length
of the weights. In COLT, 1993.
[12] A. Karpathy and F. Li. Deep visual-semantic alignments for generating image descriptions. In CVPR,
2015.
[13] D. P. Kingma and M. Welling. Auto-encoding variational Bayes. CoRR, abs/1312.6114, 2013.
[14] D. D. Lee and H. S. Seung. Algorithms for non-negative matrix factorization. In NIPS, 2001.
[15] J. Leskovec and A. Krevl. SNAP Datasets: Stanford large network dataset collection. http://snap.
stanford.edu/data, June 2014.
[16] J. MacKay David. A practical Bayesian framework for backprop networks. Neural computation, 1992.
[17] R. M. Neal. Learning stochastic feedforward networks. Department of Computer Science, University of
Toronto, 1990.
[18] R. M. Neal. Bayesian learning for neural networks. PhD thesis, University of Toronto, 1995.
[19] R. Ranganath, L. Tang, L. Charlin, and D. M. Blei. Deep exponential families. In AISTATS, 2015.
[20] R. Salakhutdinov and G. E. Hinton. Semantic hashing. Int. J. Approx. Reasoning, 50(7):969?978, 2009.
[21] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol. Stacked denoising autoencoders:
Learning useful representations in a deep network with a local denoising criterion. JMLR, 11:3371?3408,
2010.
[22] C. Wang and D. M. Blei. Collaborative topic modeling for recommending scientific articles. In KDD,
2011.
[23] H. Wang, B. Chen, and W.-J. Li. Collaborative topic regression with social regularization for tag recommendation. In IJCAI, 2013.
[24] H. Wang, N. Wang, and D. Yeung. Collaborative deep learning for recommender systems. In KDD, 2015.
[25] H. Wang and D. Yeung. Towards Bayesian deep learning: A framework and some existing methods. TKDE,
2016, to appear.
[26] M. Zhou, L. Hannah, D. B. Dunson, and L. Carin. Beta-negative binomial process and poisson factor
analysis. In AISTATS, 2012.

9


----------------------------------------------------------------

title: 5271-pre-training-of-recurrent-neural-networks-via-linear-autoencoders.pdf

Pre-training of Recurrent Neural Networks via
Linear Autoencoders
Luca Pasa, Alessandro Sperduti
Department of Mathematics
University of Padova, Italy
{pasa,sperduti}@math.unipd.it

Abstract
We propose a pre-training technique for recurrent neural networks based on linear
autoencoder networks for sequences, i.e. linear dynamical systems modelling the
target sequences. We start by giving a closed form solution for the definition of
the optimal weights of a linear autoencoder given a training set of sequences. This
solution, however, is computationally very demanding, so we suggest a procedure
to get an approximate solution for a given number of hidden units. The weights
obtained for the linear autoencoder are then used as initial weights for the inputto-hidden connections of a recurrent neural network, which is then trained on the
desired task. Using four well known datasets of sequences of polyphonic music,
we show that the proposed pre-training approach is highly effective, since it allows
to largely improve the state of the art results on all the considered datasets.

1

Introduction

Recurrent Neural Networks (RNN) constitute a powerful computational tool for sequences modelling and prediction [1]. However, training a RNN is not an easy task, mainly because of the well
known vanishing gradient problem which makes difficult to learn long-term dependencies [2]. Although alternative architectures, e.g. LSTM networks [3], and more efficient training procedures,
such as Hessian Free Optimization [4], have been proposed to circumvent this problem, reliable and
effective training of RNNs is still an open problem.
The vanishing gradient problem is also an obstacle to Deep Learning, e.g., [5, 6, 7]. In that context,
there is a growing evidence that effective learning should be based on relevant and robust internal
representations developed in autonomy by the learning system. This is usually achieved in vectorial
spaces by exploiting nonlinear autoencoder networks to learn rich internal representations of input
data which are then used as input to shallow neural classifiers or predictors (see, for example, [8]).
The importance to start gradient-based learning from a good initial point in the parameter space has
also been pointed out in [9]. Relationship between autoencoder networks and Principal Component
Analysis (PCA) [10] is well known since late ?80s, especially in the case of linear hidden units [11,
12]. More recently, linear autoencoder networks for structured data have been studied in [13, 14, 15],
where an exact closed-form solution for the weights is given in the case of a number of hidden units
equal to the rank of the full data matrix.
In this paper, we borrow the conceptual framework presented in [13, 16] to devise an effective pretraining approach, based on linear autoencoder networks for sequences, to get a good starting point
into the weight space of a RNN, which can then be successfully trained even in presence of longterm dependencies. Specifically, we revise the theoretical approach presented in [13] by: i) giving
a simpler and direct solution to the problem of devising an exact closed-form solution (full rank
case) for the weights of a linear autoencoder network for sequences, highlighting the relationship
between the proposed solution and PCA of the input data; ii) introducing a new formulation of
1

the autoencoder learning problem able to return an optimal solution also in the case of a number
of hidden units which is less than the rank of the full data matrix; iii) proposing a procedure for
approximate learning of the autoencoder network weights under the scenario of very large sequence
datasets. More importantly, we show how to use the linear autoencoder network solution to derive a
good initial point into a RNN weight space, and how the proposed approach is able to return quite
impressive results when applied to prediction tasks involving long sequences of polyphonic music.

2

Linear Autoencoder Networks for Sequences

In [11, 12] it is shown that principal directions of a set of vectors xi ? Rk are related to solutions
obtained by training linear autoencoder networks
oi = Woutput Whidden xi , i = 1, . . . , n,
(1)
where Whidden ? Rp?k , Woutput ? Rk?p , p  k, and the network is trained so to get oi = xi , ?i.
When considering a temporal sequence x1 , x2 , . . . , xt , . . . of input vectors, where t is a discrete time
index, a linear autoencoder can be defined by considering the coupled linear dynamical systems


xt
= Cyt
(3)
yt = Axt + Byt?1 (2)
yt?1
It should be noticed that eqs. (2) and (3) extend the linear transformation defined in eq. (1) by
introducing a memory term involving matrix B ? Rp?p . In fact, yt?1 is inserted in the right part
of equation (2) to keep track of the input history through time: this is done exploiting a state space
representation. Eq. (3) represents the decoding part of the autoencoder: when a state yt is multiplied
by C, the observed input xt at time t and state at time t ? 1, i.e. yt?1 , are generated. Decoding
can then continue from yt?1 . This formulation has been proposed, for example, in [17] where an
iterative procedure to learn weight matrices A and B, based on Oja?s rule, is presented. No proof
of convergence for the proposed procedure is however given. More recently, an exact closed-form
solution for the weights has been given in the case of a number of hidden units equal to the rank of
the full data matrix (full rank case) [13, 16]. In this section, we revise this result. In addition, we
give an exact solution also for the case in which the number of hidden units is strictly less than the
rank of the full data matrix.
The basic idea of [13, 16] is to look for directions of high variance into the state space of the
dynamical linear system (2). Let start by considering a single sequence x1 , x2 , . . . , xt , . . . , xn and
the state vectors of the corresponding induced state sequence collected as rows of a matrix Y =
T
[y1 , y2 , y3 , ? ? ? , yn ] . By using the initial condition y0 = 0 (the null vector), and the dynamical
linear system (2), we can rewrite the Y matrix as
? T
?? T
?
A
x1 0
0
0
??? 0
? xT xT
?
? AT BT
0
0
??? 0 ?
2
1
? T
? ? T 2T
?
T
T
?
?
? x3 x2
?
x
0
?
?
?
0
A
B
1
Y=?
??
?
? ..
?
..
..
..
..
.. ? ?
..
? .
?
?
?
.
.
.
.
.
.
T
T
T
T
T
T
T
n?1
xn xn?1 xn?2 ? ? ? x2 x1
A B
|
{z
}|
{z
}
?

?

n?s

where, given s = kn, ? ? R
is a data matrix collecting all the (inverted) input subsequences
(including the whole sequence) as rows, and ? is the parameter matrix of the dynamical system.
Now, we are interested in using a state space of dimension p  n, i.e. yt ? Rp , such that as
much information as contained in ? is preserved. We start by factorizing ? using SVD, obtaining
? = V?UT where V ? Rn?n is an unitary matrix, ? ? Rn?s is a rectangular diagonal matrix
with nonnegative real numbers on the diagonal with ?1,1 ? ?2,2 ? ? ? ? ? ?n,n (the singular values),
and UT ? Rs?n is a unitary matrix.
It is important to notice that columns of UT which correspond to nonzero singular values, apart
some mathematical technicalities, basically correspond to the principal directions of data, i.e. PCA.
If the rank of ? is p, then only the first p elements of the diagonal of ? are not null, and the
T
above decomposition can be reduced to ? = V(p) ?(p) U(p) where V(p) ? Rn?p , ?(p) ? Rp?p ,
2

T

T

and U(p) ? Rp?n . Now we can observe that U(p) U(p) = I (where I is the identity matrix of
dimension p), since by definition the columns of U(p) are orthogonal, and by imposing ? = U(p) ,
we can derive ?optimal? matrices A ? Rp?k and B ? Rp?p for our dynamical system, which will
T
have corresponding state space matrix Y(p) = ?? = ?U(p) = V(p) ?(p) U(p) U(p) = V(p) ?(p) .
(p)
Thus, if we represent U(p) as composed of n submatrices Ui , each of size k ? p, the problem
reduces to find matrices A and B such that
? T
? ? (p) ?
A
U1
(p) ?
? AT BT
? ?
U2 ?
? T 2T
? ?
(p) ?
? A B
? ?
(p)
U3 ?
(4)
?=?
?=?
?=U .
?
? ?
..
.
?
?
? ?
?
.. ?
.
T
(p)
AT Bn?1
Un
The reason to impose ? = U(p) is to get a state space where the coordinates are uncorrelated so
to diagonalise the empirical sample covariance matrix of the states. Please, note that in this way
each state (i.e., row of the Y matrix) corresponds to a row of the data matrix ?, i.e. the unrolled
(sub)sequence read up to a given time t. If the rows of ? were vectors, this would correspond to
compute PCA, keeping only the fist p principal directions.
In the following, we demonstrate that there exists a solution to the above equation. We start
by observing that ? owns a special structure, i.e. given? = [?1 ?2 ? ? ? ?n ], where
 ?i ?
0
0
1?1
1?(n?1)
Rn?k , then for i = 1, . . . , n ? 1, ?i+1 = Rn ?i =
?i , and
I(n?1)?(n?1) 0(n?1)?1
Rn ?n = 0, i.e. the null matrix of size n ? k. Moreover, by singular value decomposition, we
(p) T

T

have ?i = V(p) ?(p) Ui , for i = 1, . . . , n. Using the fact that V(p) V(p) = I, and
(p)
(p)
combining the above equations, we get Ui+t = Ui Qt , for i = 1, . . . , n ? 1, and t =
?1

T

(p)

(p) (p)
1, . . . , n ? i, where Q = ?(p) V(p) RT
?
. Moreover, we have that Un Q = 0 since
nV
?1
(p) (p) (p) T T (p) (p) ?1
(p)
= (Rn ?n )T V(p) ?(p) . Thus, eq. (4) is satisfied by
Rn V ?
Un Q = Un ? V
| {z }
=0

(p) T
U1

T

A =
and B = Q . It is interesting to note that the original data ? can be recovered by
T
T
computing Y(p) U(p) = V(p) ?(p) U(p) = ?, which can be achieved by running the system




xt
AT
yt
=
yt?1
BT


AT
starting from yn , i.e.
is the matrix C defined in eq. (3).
BT
Finally, it is important to remark that the above construction works not only for a single sequence,
but also for a set of sequences of different length. For example, let consider the two sequences
(xa1 , xa2 , xa3 ) and (xb 1 , xb 2 ). Then, we have
? aT
?
"
#
x1
0
0
bT
x
0
1
? and ?b =
?a = ? xa2 T xa1 T 0
T
bT
x
xb1
aT
aT
aT
2
x3
x2
x1




?a
R4
, and R =
.
which can be collected together to obtain ? =
?b 02?1
R2 02?1
As a final remark, it should be stressed that the above construction only works if p is equal to the
rank of ?. In the next section, we treat the case in which p < rank(?).
2.1

Optimal solution for low dimensional autoencoders
T

? i = V(p) L(p) U(p) 6= ?i , and
When p < rank(?) the solution given above breaks down because ?
i
? i+1 6= Rn ?
? i . So the question is whether the proposed solutions for A and B still
consequently ?
hold the best reconstruction error when p < rank(?).
3

In this paper, we answer in negative terms to this question by resorting to a new formulation of our
(p)
problem where we introduce slack-like matrices Ei ? Rk?p , i = 1, . . . , n + 1 collecting the
reconstruction errors, which need to be minimised:
n+1
X

min
(p)
Q?Rp?p ,Ei

?

subject to :

?
?
?
?
?
?
?

(p)

i=1
(p)

U1 + E 1
(p)
(p)
U2 + E 2
(p)
(p)
U3 + E 3
..
.
(p)

(p)

kEi k2F

(p)

Un + E n

(p)

(p)

?

?
?
?
?
?
?
?Q = ?
?
?
? (p)
?
? Un + En(p)
?
(p)
En+1

?
?
?
?
?
?
?

?

?

U2 + E 2
(p)
(p)
U3 + E 3
..
.

(5)

Notice that the problem above is convex both in the objective function and in the constraints; thus
(p)
it only has global optimal solutions E?i and Q? , from which we can derive AT = U1 + E?1 and
T
?
T
(p)
(p)
B = Q . Specifically, when p = rank(?), Rs,k U is in the span of U and the optimal
T

(p)
solution is given by E?i = 0k?p ?i, and Q? = U(p) RT
, i.e. the solution we have already
s,k U
described. If p < rank(?), the optimal solution cannot have ?i, E?i = 0k?p . However, it is not
difficult to devise an iterative procedure to reach the minimum. Since in the experimental section we
do not exploit the solution to this problem for reasons that we will explain later, here we just sketch
(p)
such procedure. It helps to observe that, given a fixed Q, the optimal solution for Ei is given by
(p)

(p)

(p)

(p)

(p)

(p)

(p)

(p)

(p)

+
2
3
? ,E
? ,...,E
?
[E
1
2
n+1 ] = [U1 Q ? U2 , U1 Q ? U3 , U1 Q ? U4 , . . .] MQ
?
?
?Q ?Q2 ?Q3 ? ? ?
0
0
??? ?
? I
? 0
+
I
0
??? ?
?
?.
where MQ is the pseudo inverse of MQ = ?
0
I
??? ?
? 0
?
..
..
..
..
.
.
.
.

h
i
T
T
T
T T
? (p) = E
? (p) , E
? (p) , E
? (p) , ? ? ? , E
? n(p)
In general, E
can be decomposed into a component in the
1
2
3
?

?

span of U(p) and a component E(p) orthogonal to it. Notice that E(p) cannot be reduced, while
? (p) = U(p) + E(p) ? and taking
(part of) the other component can be absorbed into Q by defining U
h
i
T
T
T T
? = (U
? (p) )+ U
? (p) , U
? (p) , ? ? ? , U
? (p)T , E(p)
Q
.
n
2
3
n+1
? the new optimal values for E(p) are obtained and the process iterated till convergence.
Given Q,
i

3

Pre-training of Recurrent Neural Networks

Here we define our pre-training procedure for recurrent neural networks with one hidden layer of p
units, and O output units:
ot = ?(Woutput h(xt )) ? RO , h(xt ) = ?(Winput xt + Whidden h(xt?1 )) ? Rp

(6)
T

where Woutput ? RO?p , Whidden ? Rp?k , for a vector z ? Rm , ?(z) = [?(z1 ), . . . , ?(zm )] ,
?zi
.
and here we consider the symmetric sigmoid function ?(zi ) = 1?e
1+e?zi
The idea is to exploit the hidden state representation obtained by eqs. (2) as initial hidden state representation for the RNN described by eqs. (6). This is implemented by initialising the weight matrices
Winput and Whidden of (6) by using the matrices that jointly solve eqs. (2) and eqs. (3), i.e. A and
B (since C is function of A and B). Specifically, we initialize Winput with A, and Whidden with
B. Moreover, the use of symmetrical sigmoidal functions, which do give a very good approximation
of the identity function around the origin, allows a good transferring of the linear dynamics inside
4

RNN. For what concerns Woutput , we initialise it by using the best possible solution, i.e. the pseudoinverse of H times the target matrix T, which does minimise the output squared error. Learning
is then used to introduce nonlinear components that allow to improve the performance of the model.
More formally, let consider a prediction task where for each sequence sq ? (xq1 , xq2 , . . . , xqlq )
of length lq in the training set, a sequence tq of target vectors is defined, i.e. a training sequence is given by hsq , tq i ? h(xq1 , tq1 ), (xq2 , tq2 ), . . . , (xqlq , tqlq )i, where tqi ? RO . Given a trainPN
ing set with N sequences, let define the target matrix T ? RL?O , where L =
q=1 lq , as
 1 1

1
2
?
N T
T = t1 , t2 , . . . , tl1 , t1 , . . . , tlN . The input matrix ? will have size L ? k. Let p be the desired number of hidden units for the recurrent neural network (RNN). Then the pre-training procedure can be defined as follows: i) compute the linear autoencoder for ? using p? principal direc?
?
?
tions, obtaining the optimal matrices A? ? Rp ?k and B? ? Rp ?p ; i) set Winput = A? and
?
Whidden = B ; iii) run the RNN over the training sequences, collecting the hidden activities vec?
tors (computed using symmetrical sigmoidal functions) over time as rows of matrix H ? RL?p ;
+
+
iv) set Woutput = H T, where H is the (left) pseudoinverse of H.

3.1

Computing an approximate solution for large datasets

In real world scenarios the application of our approach may turn difficult because of the size of
the data matrix. In fact, stable computation of principal directions is usually obtained by SVD decomposition of the data matrix ?, that in typical application domains involves a number of rows
and columns which is easily of the order of hundreds of thousands. Unfortunately, the computational complexity of SVD decomposition is basically cubic in the smallest of the matrix dimensions.
Memory consumption is also an important issue. Algorithms for approximate computation of SVD
have been suggested (e.g., [18]), however, since for our purposes we just need matrices V and ?
with a predefined number of columns (i.e. p), here we present an ad-hoc algorithm for approximate
computation of these matrices. Our solution is based on the following four main ideas: i) divide ?
in slices of k (i.e., size of input at time t) columns, so to exploit SVD decomposition at each slice
separately; ii) compute approximate V and ? matrices, with p columns, incrementally via truncated
SVD of temporary matrices obtained by concatenating the current approximation of V? with a new
slice; iii) compute the SVD decomposition of a temporary matrix via either its kernel or covariance
matrix, depending on the smallest between the number of rows and the number of columns of the
temporary matrix; iv) exploit QR decomposition to compute SVD decomposition.
Algorithm 1 shows in pseudo-code the main steps of our procedure. It maintains a temporary matrix
T which is used to collect incrementally an approximation of the principal subspace of dimension p
of ?. Initially (line 4) T is set equal to the last slices of ?, in a number sufficient to get a number
of columns larger than p (line 2). Matrices V and ? from the p-truncated SVD decomposition of
T are computed (line 5) via the K E C O procedure, described in Algorithm 2, and used to define a
new T matrix by concatenation with the last unused slice of ?. When all slices are processed, the
current V and ? matrices are returned. The K E C O procedure, described in Algorithm 2 , reduces
the computational burden by computing the p-truncated SVD decomposition of the input matrix
M via its kernel matrix (lines 3-4) if the number of rows of M is no larger than the number of
columns, otherwise the covariance matrix is used (lines 6-8). In both cases, the p-truncated SVD
decomposition is implemented via QR decomposition by the INDIRECT SVD procedure described in
Algorithm 3. This allows to reduce computation time when large matrices must be processed [19].
1
Finally, matrices V and S 2 (both kernel and covariance matrices have squared singular values of
M) are returned.
We use the strategy to process slices of ? in reverse order since, moving versus columns with larger
indices, the rank as well as the norm of slices become smaller and smaller, thus giving less and less
contribution to the principal subspace of dimension p. This should reduce the approximation error
cumulated by dropping the components from p + 1 to p + k during computation [20]. As a final
remark, we stress that since we compute an approximate solution for the principal directions of ?,
it makes no much sense to solve the problem given in eq. (5): learning will quickly compensate
for the approximations and/or sub-optimality of A and B obtained by matrices V and ? returned
by Algorithm 1. Thus, these are the matrices we have used for the experiments described in next
section.
5

Algorithm 1 Approximated V and ? with p components
1: function SVF OR B IG DATA(?, k, p)
2:
nStart = dp/ke
. Number of starting slices
3:
nSlice = (?.columns/k) ? nStart
. Number of remaining slices
4:
T = ?[:, k ? nSlice : ?.columns]
5:
V, ? =K E C O(T, p)
. Computation of V and ? for starting slices
6:
for i in REVERSED(range(nSlice)) do
. Computation of V and ? for remaining slices
7:
T = [?[:, i ? k:(i + 1) ? k], V?]
8:
V, ? =K E C O(T, p)
9:
end for
10:
return V, ?
11: end function
Algorithm 2 Kernel vs covariance computation Algorithm 3 Truncated SVD by QR
1: function K E C O(M, p)
1: function INDIRECT SVD(M, p)
2:
if M.rows <= ?.columns then
2:
Q, R =QR(M)
3:
K = MMT
3:
Vr , S, UT =SVD(R)
T
4:
V, Ssqr , U =INDIRECT SVD(K, p) 4:
V = QVr
5:
else
5:
S = S[1 : p, 1 : p]
6:
C = MT M
6:
V = V[1 : p, :]
7:
V, Ssqr , UT =INDIRECT SVD(C, p) 7:
UT = UT [:, 1 : p]
1
?
8:
return V, S, UT
8:
V = MUT Ssqr2
9: end function
9:
end if
1

2
10:
return V, Ssqr
11: end function

4

Experiments

In order to evaluate our pre-training approach, we decided to use the four polyphonic music sequences datasets used in [21] for assessing the prediction abilities of the RNN-RBM model. The
prediction task consists in predicting the notes played at time t given the sequence of notes played
till time t ? 1. The RNN-RBM model achieves state-of-the-art in such demanding prediction task.
As performance measure we adopted the accuracy measure used in [21] and described in [22]. Each
dataset is split in training set, validation set, and test set. Statistics on the datasets, including largest
sequence length, are given in columns 2-4 of Table 1. Each sequence in the dataset represents a song
having a maximum polyphony of 15 notes (average 3.9); each time step input spans the whole range
of piano from A0 to C8 and it is represented by using 88 binary values (i.e. k = 88).
Our pre-training approach (PreT-RNN) has been assessed by using a different number of hidden
units (i.e., p is set in turn to 50, 100, 150, 200, 250) and 5000 epochs of RNN training1 using the
Theano-based stochastic gradient descent software available at [23].
Random initialisation (Rnd) has also been used for networks with the same number of hidden units.
Specifically, for networks with 50 hidden units, we have evaluated the performance of 6 different
random initialisations. Finally, in order to verify that the nonlinearity introduced by the RNN is
actually useful to solve the prediction task, we have also evaluated the performance of a network
with linear units (250 hidden units) initialised with our pre-training procedure (PreT-Lin250).
To give an idea of the time performance of pre-training with respect to the training of a RNN, in
column 5 of Table 1 we have reported the time in seconds needed to compute pre-training matrices
c
c
(Pre-) (on Intel
Xeon
CPU E5-2670 @2.60GHz with 128 GB) and to perform training of a
RNN with p = 50 for 5000 epochs (on GPU NVidia K20). Please, note that for larger values of p,
the increase in computation time of pre-training is smaller than the increment in computation time
needed for training a RNN.
1

Due to early overfitting, for the Muse dataset we used 1000 epochs.

6

Dataset
Nottingham

Piano-midi.de

MuseData

JSB Chorales

Set
Training
(39165 ? 56408)
Test
Validation
Training
(70672 ? 387640)
Test
Validation
Training
(248479 ? 214192)
Test
Validation
Training
(27674 ? 22792)
Test
Validation

# Samples
195

Max length
641

170
173
87

1495
1229
4405

25
12
524

2305
1740
2434

25
135
229

2305
2523
259

77
76

320
289

(Pre-)Training Time
seconds
(226) 5837
p = 50
5000 epochs
seconds
(2971) 4147
p = 50
5000 epochs
seconds
(7338) 4190
p = 50
5000 epochs
seconds
(79) 6411
p = 50
5000 epochs

Model
RNN (w. HF)
RNN-RBM
PreT-RNN
PreT-Lin250
RNN (w. HF)
RNN-RBM
PreT-RNN
PreT-Lin250
RNN (w. HF)
RNN-RBM
PreT-RNN
PreT-Lin250
RNN (w. HF)
RNN-RBM
PreT-RNN
PreT-Lin250

ACC% [21]
62.93 (66.64)
75.40
75.23 (p = 250)
73.19
19.33 (23.34)
28.92
37.74 (p = 250)
16.87
23.25 (30.49)
34.02
57.57 (p = 200)
3.56
28.46 (29.41)
33.12
65.67 (p = 250)
38.32

Table 1: Datasets statistics including data matrix size for the training set (columns 2-4), computational times in seconds to perform pre-training and training for 5000 epochs with p = 50 (column
5), and accuracy results for state-of-the-art models [21] vs our pre-training approach (columns 6-7).
The acronym (w. HF) is used to identify an RNN trained by Hessian Free Optimization [4].

Training and test curves for all the models described above are reported in Figure 1. It is evident that
random initialisation does not allow the RNN to improve its performance in a reasonable amount of
epochs. Specifically, for random initialisation with p = 50 (Rnd 50), we have reported the average
and range of variation over the 6 different trails: different initial points do not change substantially
the performance of RNN. Increasing the number of hidden units allows the RNN to slightly increase
its performance. Using pre-training, on the other hand, allows the RNN to start training from a quite
favourable point, as demonstrated by an early sharp improvement of performances. Moreover, the
more hidden units are used, the more the improvement in performance is obtained, till overfitting is
observed. In particular, early overfitting occurs for the Muse dataset. It can be noticed that the linear
model (Linear) reaches performances which are in some cases better than RNN without pre-training.
However, it is important to notice that while it achieves good results on the training set (e.g. JSB and
Piano-midi), the corresponding performance on the test set is poor, showing a clear evidence of overfitting. Finally, in column 7 of Table 1, we have reported the accuracy obtained after validation on
the number of hidden units and number of epochs for our approaches (PreT-RNN and PreT-Lin250)
versus the results reported in [21] for RNN (also using Hessian Free Optimization) and RNN-RBM.
In any case, the use of pre-training largely improves the performances over standard RNN (with
or without Hessian Free Optimization). Moreover, with the exception of the Nottingham dataset,
the proposed approach outperforms the state-of-the-art results achieved by RNN-RBM. Large improvements are observed for the Muse and JSB datasets. Performance for the Nottingham dataset
is basically equivalent to the one obtained by RNN-RBM. For this dataset, also the linear model
with pre-training achieves quite good results, which seems to suggest that the prediction task for
this dataset is much easier than for the other datasets. The linear model outperforms RNN without
pre-training on Nottingham and JSB datasets, but shows problems with the Muse dataset.

5

Conclusions

We have proposed a pre-training technique for RNN based on linear autoencoders for sequences.
For this kind of autoencoders it is possible to give a closed form solution for the definition of the
?optimal? weights, which however, entails the computation of the SVD decomposition of the full
data matrix. For large data matrices exact SVD decomposition cannot be achieved, so we proposed
a computationally efficient procedure to get an approximation that turned to be effective for our
goals. Experimental results for a prediction task on datasets of sequences of polyphonic music
show the usefulness of the proposed pre-training approach, since it allows to largely improve the
state of the art results on all the considered datasets by using simple stochastic gradient descend for
learning. Even if the results are very encouraging the method needs to be assessed on data from
other application domains. Moreover, it is interesting to understand whether the analysis performed
in [24] on linear deep networks for vectors can be extended to recurrent architectures for sequences
and, in particular, to our method.
7

0.4

0.2
0.1
0
-0.1
Rnd 50 (6 trials)
Linear 250
Rnd 100
0

200

600

PreT 200
PreT 250
800
Nottingham
Test Set

1000

0.8

Epoch

0.7

0.7

0.6

0.6

0.5

0.5

Accuracy

Accuracy

PreT 50
PreT 150
PreT 100

400
Nottingham Training
Set

0.8

0.4
0.3

0.4
0.3

0.2

0.2

0.1

0.1

0

0
0

500 1000 1500 2000 2500 3000 3500 4000 4500 5000
Epoch

0

500 1000 1500 2000 2500 3000 3500 4000 4500 5000
Epoch

Piano-Midi.de Training Set

Piano-Midi.de Test Set

0.55

0.4

0.5

0.35

Accuracy

0.45
0.4

0.3

0.35

0.25

0.3
0.25
0.2
0.15

0.2
0.15
0.1

0.1
0.05

0.05
0

0
0

500 1000 1500 2000 2500 3000 3500 4000 4500 5000
Epoch

0

500 1000 1500 2000 2500 3000 3500 4000 4500 5000
Epoch

Muse Dataset Training Set

Muse Dataset Test Set

0.7

0.6

0.6

0.5

0.5

0.4
Accuracy

Accuracy

0.4
0.3

0.3
0.2

0.2

0.1

0.1
0

0
0

200

400

600

800

1000

0

Epoch

200

400

600

800

1000

Epoch

JSB Chorales Training Set

JSB Chorales Test Set

0.8

0.7

0.7

0.6

0.6

0.5

0.5

Accuracy

-0.3

Rnd 150
Rnd 200
Rnd 250

Accuracy

-0.2

Accuracy

Accuracy

0.3

0.4
0.3

0.4
0.3
0.2

0.2

0.1

0.1
0

0
0

500 1000 1500 2000 2500 3000 3500 4000 4500 5000
Epoch

0

500 1000 1500 2000 2500 3000 3500 4000 4500 5000
Epoch

Figure 1: Training (left column) and test (right column) curves for the assessed approaches on the
four datasets. Curves are sampled at each epoch till epoch 100, and at steps of 100 epochs afterwards.
8

References
[1] S. C. Kremer. Field Guide to Dynamical Recurrent Networks. Wiley-IEEE Press, 2001.
[2] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent
is difficult. IEEE Transactions on Neural Networks, 5(2):157?166, 1994.
[3] S. Hochreiter and J. Schmidhuber. Lstm can solve hard long time lag problems. In NIPS, pages
473?479, 1996.
[4] J. Martens and I. Sutskever. Learning recurrent neural networks with hessian-free optimization.
In ICML, pages 1033?1040, 2011.
[5] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504?507, July 2006.
[6] G. E. Hinton, S. Osindero, and Y. W. Teh. A fast learning algorithm for deep belief nets. Neural
Computation, 18(7):1527?1554, 2006.
[7] P. di Lena, K. Nagata, and P. Baldi. Deep architectures for protein contact map prediction.
Bioinformatics, 28(19):2449?2457, 2012.
[8] Y. Bengio. Learning deep architectures for ai. Foundations and Trends in Machine Learning,
2(1):1?127, 2009.
[9] I. Sutskever, J. Martens, G. E. Dahl, and G. E. Hinton. On the importance of initialization and
momentum in deep learning. In ICML (3), pages 1139?1147, 2013.
[10] I.T. Jolliffe. Principal Component Analysis. Springer-Verlag New York, Inc., 2002.
[11] H. Bourlard and Y. Kamp. Auto-association by multilayer perceptrons and singular value
decomposition. Biological Cybernetics, 59(4-5):291?294, 1988.
[12] P. Baldi and K. Hornik. Neural networks and principal component analysis: Learning from
examples without local minima. Neural Networks, 2(1):53?58, 1989.
[13] A. Sperduti. Exact solutions for recursive principal components analysis of sequences and
trees. In ICANN (1), pages 349?356, 2006.
[14] A. Micheli and A. Sperduti. Recursive principal component analysis of graphs. In ICANN (2),
pages 826?835, 2007.
[15] A. Sperduti. Efficient computation of recursive principal component analysis for structured
input. In ECML, pages 335?346, 2007.
[16] A. Sperduti. Linear autoencoder networks for structured data. In NeSy?13:Ninth International
Workshop onNeural-Symbolic Learning and Reasoning, 2013.
[17] T. Voegtlin. Recursive principal components analysis. Neural Netw., 18(8):1051?1063, 2005.
[18] G. Martinsson et al. Randomized methods for computing the singular value decomposition
(svd) of very large matrices. In Works. on Alg. for Modern Mass. Data Sets, Palo Alto, 2010.
[19] E. Rabani and S. Toledo. Out-of-core svd and qr decompositions. In PPSC, 2001.
[20] Z. Zhang and H. Zha. Structure and perturbation analysis of truncated svds for columnpartitioned matrices. SIAM J. on Mat. Anal. and Appl., 22(4):1245?1262, 2001.
[21] N. Boulanger-Lewandowski, Y. Bengio, and P. Vincent. Modeling temporal dependencies in
high-dimensional sequences: Application to polyphonic music generation and transcription.
In ICML, 2012.
[22] M. Bay, A. F. Ehmann, and J. S. Downie. Evaluation of multiple-f0 estimation and tracking
systems. ISMIR, pages 315?320, 2009.
[23] https://github.com/gwtaylor/theano-rnn.
[24] A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of
learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.

9


----------------------------------------------------------------

title: 5954-convolutional-networks-on-graphs-for-learning-molecular-fingerprints.pdf

Convolutional Networks on Graphs
for Learning Molecular Fingerprints

David Duvenaud? , Dougal Maclaurin?, Jorge Aguilera-Iparraguirre
Rafael G?omez-Bombarelli, Timothy Hirzel, Al?an Aspuru-Guzik, Ryan P. Adams
Harvard University

Abstract
We introduce a convolutional neural network that operates directly on graphs.
These networks allow end-to-end learning of prediction pipelines whose inputs
are graphs of arbitrary size and shape. The architecture we present generalizes
standard molecular feature extraction methods based on circular fingerprints. We
show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.

1

Introduction

Recent work in materials design used neural networks to predict the properties of novel molecules
by generalizing from examples. One difficulty with this task is that the input to the predictor, a
molecule, can be of arbitrary size and shape. Currently, most machine learning pipelines can only
handle inputs of a fixed size. The current state of the art is to use off-the-shelf fingerprint software
to compute fixed-dimensional feature vectors, and use those features as inputs to a fully-connected
deep neural network or other standard machine learning method. This formula was followed by
[28, 3, 19]. During training, the molecular fingerprint vectors were treated as fixed.
In this paper, we replace the bottom layer of this stack ? the function that computes molecular
fingerprint vectors ? with a differentiable neural network whose input is a graph representing the
original molecule. In this graph, vertices represent individual atoms and edges represent bonds. The
lower layers of this network is convolutional in the sense that the same local filter is applied to each
atom and its neighborhood. After several such layers, a global pooling step combines features from
all the atoms in the molecule.
These neural graph fingerprints offer several advantages over fixed fingerprints:
? Predictive performance. By using data adapting to the task at hand, machine-optimized
fingerprints can provide substantially better predictive performance than fixed fingerprints.
We show that neural graph fingerprints match or beat the predictive performance of standard fingerprints on solubility, drug efficacy, and organic photovoltaic efficiency datasets.
? Parsimony. Fixed fingerprints must be extremely large to encode all possible substructures
without overlap. For example, [28] used a fingerprint vector of size 43,000, after having
removed rarely-occurring features. Differentiable fingerprints can be optimized to encode
only relevant features, reducing downstream computation and regularization requirements.
? Interpretability. Standard fingerprints encode each possible fragment completely distinctly, with no notion of similarity between fragments. In contrast, each feature of a neural
graph fingerprint can be activated by similar but distinct molecular fragments, making the
feature representation more meaningful.
?

Equal contribution.

1

Figure 1: Left: A visual representation of the computational graph of both standard circular fingerprints and neural graph fingerprints. First, a graph is constructed matching the topology of the
molecule being fingerprinted, in which nodes represent atoms, and edges represent bonds. At each
layer, information flows between neighbors in the graph. Finally, each node in the graph turns on
one bit in the fixed-length fingerprint vector. Right: A more detailed sketch including the bond
information used in each operation.

2

Circular fingerprints

The state of the art in molecular fingerprints are extended-connectivity circular fingerprints
(ECFP) [21]. Circular fingerprints [6] are a refinement of the Morgan algorithm [17], designed
to encode which substructures are present in a molecule in a way that is invariant to atom-relabeling.
Circular fingerprints generate each layer?s features by applying a fixed hash function to the concatenated features of the neighborhood in the previous layer. The results of these hashes are then treated
as integer indices, where a 1 is written to the fingerprint vector at the index given by the feature
vector at each node in the graph. Figure 1(left) shows a sketch of this computational architecture.
Ignoring collisions, each index of the fingerprint denotes the presence of a particular substructure.
The size of the substructures represented by each index depends on the depth of the network. Thus
the number of layers is referred to as the ?radius? of the fingerprints.
Circular fingerprints are analogous to convolutional networks in that they apply the same operation
locally everywhere, and combine information in a global pooling step.

3

Creating a differentiable fingerprint

The space of possible network architectures is large. In the spirit of starting from a known-good configuration, we designed a differentiable generalization of circular fingerprints. This section describes
our replacement of each discrete operation in circular fingerprints with a differentiable analog.
Hashing The purpose of the hash functions applied at each layer of circular fingerprints is to
combine information about each atom and its neighboring substructures. This ensures that any
change in a fragment, no matter how small, will lead to a different fingerprint index being activated.
We replace the hash operation with a single layer of a neural network. Using a smooth function
allows the activations to be similar when the local molecular structure varies in unimportant ways.
Indexing Circular fingerprints use an indexing operation to combine all the nodes? feature vectors
into a single fingerprint of the whole molecule. Each node sets a single bit of the fingerprint to one,
at an index determined by the hash of its feature vector. This pooling-like operation converts an
arbitrary-sized graph into a fixed-sized vector. For small molecules and a large fingerprint length,
the fingerprints are always sparse. We use the softmax operation as a differentiable analog of
indexing. In essence, each atom is asked to classify itself as belonging to a single category. The sum
of all these classification label vectors produces the final fingerprint. This operation is analogous to
the pooling operation in standard convolutional neural networks.
2

Algorithm 1 Circular fingerprints
1: Input: molecule, radius R, fingerprint
length S
2: Initialize: fingerprint vector f ? 0S
3: for each atom a in molecule
4:
ra ? g(a)
. lookup atom features
5: for L = 1 to R
. for each layer
6:
for each atom a in molecule
7:
r1 . . . rN = neighbors(a)
8:
v ? [ra , r1 , . . . , rN ] . concatenate
9:
ra ? hash(v)
. hash function
10:
i ? mod(ra , S) . convert to index
11:
fi ? 1
. Write 1 at index
12: Return: binary vector f

Algorithm 2 Neural graph fingerprints
1: Input: molecule, radius R, hidden weights
5
H11 . . . HR
, output weights W1 . . . WR
2: Initialize: fingerprint vector f ? 0S
3: for each atom a in molecule
4:
ra ? g(a)
. lookup atom features
5: for L = 1 to R
. for each layer
6:
for each atom a in molecule
7:
r1 . . . rN =Pneighbors(a)
8:
v ? ra + N
. sum
i=1 ri
9:
ra ? ?(vHLN )
. smooth function
10:
i ? softmax(ra WL )
. sparsify
11:
f ?f +i
. add to fingerprint
12: Return: real-valued vector f

Figure 2: Pseudocode of circular fingerprints (left) and neural graph fingerprints (right). Differences
are highlighted in blue. Every non-differentiable operation is replaced with a differentiable analog.
Canonicalization Circular fingerprints are identical regardless of the ordering of atoms in each
neighborhood. This invariance is achieved by sorting the neighboring atoms according to their
features, and bond features. We experimented with this sorting scheme, and also with applying the
local feature transform on all possible permutations of the local neighborhood. An alternative to
canonicalization is to apply a permutation-invariant function, such as summation. In the interests of
simplicity and scalability, we chose summation.
Circular fingerprints can be interpreted as a special case of neural graph fingerprints having large
random weights. This is because, in the limit of large input weights, tanh nonlinearities approach
step functions, which when concatenated form a simple hash function. Also, in the limit of large
input weights, the softmax operator approaches a one-hot-coded argmax operator, which is analogous to an indexing operation.
Algorithms 1 and 2 summarize these two algorithms and highlight their differences. Given a fingerprint length L, and F features at each layer, the parameters of neural graph fingerprints consist of
a separate output weight matrix of size F ? L for each layer, as well as a set of hidden-to-hidden
weight matrices of size F ? F at each layer, one for each possible number of bonds an atom can
have (up to 5 in organic molecules).

4

Experiments

We ran two experiments to demonstrate that neural fingerprints with large random weights behave
similarly to circular fingerprints. First, we examined whether distances between circular fingerprints
were similar to distances between neural fingerprint-based distances. Figure 3 (left) shows a scatterplot of pairwise distances between circular vs. neural fingerprints. Fingerprints had length 2048,
and were calculated on pairs of molecules from the solubility dataset [4]. Distance was measured
using a continuous generalization of the Tanimoto (a.k.a. Jaccard) similarity measure, given by
.X
X
distance(x, y) = 1 ?
min(xi , yi )
max(xi , yi )
(1)
There is a correlation of r = 0.823 between the distances. The line of points on the right of the plot
shows that for some pairs of molecules, binary ECFP fingerprints have exactly zero overlap.
Second, we examined the predictive performance of neural fingerprints with large random weights
vs. that of circular fingerprints. Figure 3 (right) shows average predictive performance on the solubility dataset, using linear regression on top of fingerprints. The performances of both methods
follow similar curves. In contrast, the performance of neural fingerprints with small random weights
follows a different curve, and is substantially better. This suggests that even with random weights,
the relatively smooth activation of neural fingerprints helps generalization performance.
3

2.0
1.8

0.9

RMSE (log Mol/L)

Neural fingerprint distances

Neural vs Circular distances, r =0:823
1.0

0.8
0.7
0.6
0.5
0.5

Circular fingerprints
Random conv with large parameters
Random conv with small parameters

1.6
1.4
1.2
1.0
0.8
0

0.6
0.7
0.8
0.9
1.0
Circular fingerprint distances

1

2
3
4
Fingerprint radius

5

6

Figure 3: Left: Comparison of pairwise distances between molecules, measured using circular fingerprints and neural graph fingerprints with large random weights. Right: Predictive performance
of circular fingerprints (red), neural graph fingerprints with fixed large random weights (green) and
neural graph fingerprints with fixed small random weights (blue). The performance of neural graph
fingerprints with large random weights closely matches the performance of circular fingerprints.
4.1

Examining learned features

To demonstrate that neural graph fingerprints are interpretable, we show substructures which most
activate individual features in a fingerprint vector. Each feature of a circular fingerprint vector can
each only be activated by a single fragment of a single radius, except for accidental collisions.
In contrast, neural graph fingerprint features can be activated by variations of the same structure,
making them more interpretable, and allowing shorter feature vectors.
Solubility features Figure 4 shows the fragments that maximally activate the most predictive features of a fingerprint. The fingerprint network was trained as inputs to a linear model predicting
solubility, as measured in [4]. The feature shown in the top row has a positive predictive relationship
with solubility, and is most activated by fragments containing a hydrophilic R-OH group, a standard
indicator of solubility. The feature shown in the bottom row, strongly predictive of insolubility, is
activated by non-polar repeated ring structures.
Fragments most
activated by
pro-solubility
feature

O

OH

O
NH
O

OH

OH

Fragments most
activated by
anti-solubility
feature

Figure 4: Examining fingerprints optimized for predicting solubility. Shown here are representative
examples of molecular fragments (highlighted in blue) which most activate different features of the
fingerprint. Top row: The feature most predictive of solubility. Bottom row: The feature most
predictive of insolubility.

4

Toxicity features We trained the same model architecture to predict toxicity, as measured in two
different datasets in [26]. Figure 5 shows fragments which maximally activate the feature most
predictive of toxicity, in two separate datasets.
Fragments most
activated by
toxicity feature
on SR-MMP
dataset
Fragments most
activated by
toxicity feature
on NR-AHR
dataset
Figure 5: Visualizing fingerprints optimized for predicting toxicity. Shown here are representative
samples of molecular fragments (highlighted in red) which most activate the feature most predictive
of toxicity. Top row: the most predictive feature identifies groups containing a sulphur atom attached
to an aromatic ring. Bottom row: the most predictive feature identifies fused aromatic rings, also
known as polycyclic aromatic hydrocarbons, a well-known carcinogen.
[27] constructed similar visualizations, but in a semi-manual way: to determine which toxic fragments activated a given neuron, they searched over a hand-made list of toxic substructures and chose
the one most correlated with a given neuron. In contrast, our visualizations are generated automatically, without the need to restrict the range of possible answers beforehand.
4.2

Predictive Performance

We ran several experiments to compare the predictive performance of neural graph fingerprints to
that of the standard state-of-the-art setup: circular fingerprints fed into a fully-connected neural
network.
Experimental setup Our pipeline takes as input the SMILES [30] string encoding of each
molecule, which is then converted into a graph using RDKit [20]. We also used RDKit to produce
the extended circular fingerprints used in the baseline. Hydrogen atoms were treated implicitly.
In our convolutional networks, the initial atom and bond features were chosen to be similar to those
used by ECFP: Initial atom features concatenated a one-hot encoding of the atom?s element, its
degree, the number of attached hydrogen atoms, and the implicit valence, and an aromaticity indicator. The bond features were a concatenation of whether the bond type was single, double, triple,
or aromatic, whether the bond was conjugated, and whether the bond was part of a ring.
Training and Architecture Training used batch normalization [11]. We also experimented with
tanh vs relu activation functions for both the neural fingerprint network layers and the fullyconnected network layers. relu had a slight but consistent performance advantage on the validation set. We also experimented with dropconnect [29], a variant of dropout in which weights are
randomly set to zero instead of hidden units, but found that it led to worse validation error in general. Each experiment optimized for 10000 minibatches of size 100 using the Adam algorithm [13],
a variant of RMSprop that includes momentum.
Hyperparameter Optimization To optimize hyperparameters, we used random search. The hyperparameters of all methods were optimized using 50 trials for each cross-validation fold. The
following hyperparameters were optimized: log learning rate, log of the initial weight scale, the log
L2 penalty, fingerprint length, fingerprint depth (up to 6), and the size of the hidden layer in the
fully-connected network. Additionally, the size of the hidden feature vector in the convolutional
neural fingerprint networks was optimized.
5

Dataset
Units
Predict mean
Circular FPs + linear layer
Circular FPs + neural net
Neural FPs + linear layer
Neural FPs + neural net

Solubility [4]
log Mol/L

Drug efficacy [5]
EC50 in nM

Photovoltaic efficiency [8]
percent

4.29 ? 0.40
1.71 ? 0.13
1.40 ? 0.13
0.77 ? 0.11
0.52 ? 0.07

1.47 ? 0.07
1.13 ? 0.03
1.36 ? 0.10
1.15 ? 0.02
1.16 ? 0.03

6.40 ? 0.09
2.63 ? 0.09
2.00 ? 0.09
2.58 ? 0.18
1.43 ? 0.09

Table 1: Mean predictive accuracy of neural fingerprints compared to standard circular fingerprints.

Datasets We compared the performance of standard circular fingerprints against neural graph fingerprints on a variety of domains:
? Solubility: The aqueous solubility of 1144 molecules as measured by [4].
? Drug efficacy: The half-maximal effective concentration (EC50 ) in vitro of 10,000
molecules against a sulfide-resistant strain of P. falciparum, the parasite that causes malaria,
as measured by [5].
? Organic photovoltaic efficiency: The Harvard Clean Energy Project [8] uses expensive
DFT simulations to estimate the photovoltaic efficiency of organic molecules. We used a
subset of 20,000 molecules from this dataset.
Predictive accuracy We compared the performance of circular fingerprints and neural graph fingerprints under two conditions: In the first condition, predictions were made by a linear layer using
the fingerprints as input. In the second condition, predictions were made by a one-hidden-layer
neural network using the fingerprints as input. In all settings, all differentiable parameters in the
composed models were optimized simultaneously. Results are summarized in Table 4.2.
In all experiments, the neural graph fingerprints matched or beat the accuracy of circular fingerprints,
and the methods with a neural network on top of the fingerprints typically outperformed the linear
layers.
Software Automatic differentiation (AD) software packages such as Theano [1] significantly
speed up development time by providing gradients automatically, but can only handle limited control
structures and indexing. Since we required relatively complex control flow and indexing in order
to implement variants of Algorithm 2, we used a more flexible automatic differentiation package
for Python called Autograd (github.com/HIPS/autograd). This package handles standard
Numpy [18] code, and can differentiate code containing while loops, branches, and indexing.
Code for computing neural fingerprints and producing visualizations is available at
github.com/HIPS/neural-fingerprint.

5

Limitations

Computational cost Neural fingerprints have the same asymptotic complexity in the number of
atoms and the depth of the network as circular fingerprints, but have additional terms due to the
matrix multiplies necessary to transform the feature vector at each step. To be precise, computing
the neural fingerprint of depth R, fingerprint length L of a molecule with N atoms using a molecular
convolutional net having F features at each layer costs O(RN F L + RN F 2 ). In practice, training
neural networks on top of circular fingerprints usually took several minutes, while training both the
fingerprints and the network on top took on the order of an hour on the larger datasets.
Limited computation at each layer How complicated should we make the function that goes
from one layer of the network to the next? In this paper we chose the simplest feasible architecture:
a single layer of a neural network. However, it may be fruitful to apply multiple layers of nonlinearities between each message-passing step (as in [22]), or to make information preservation easier by
adapting the Long Short-Term Memory [10] architecture to pass information upwards.
6

Limited information propagation across the graph The local message-passing architecture developed in this paper scales well in the size of the graph (due to the low degree of organic molecules),
but its ability to propagate information across the graph is limited by the depth of the network. This
may be appropriate for small graphs such as those representing the small organic molecules used in
this paper. However, in the worst case, it can take a depth N2 network to distinguish between graphs
of size N . To avoid this problem, [2] proposed a hierarchical clustering of graph substructures. A
tree-structured network could examine the structure of the entire graph using only log(N ) layers,
but would require learning to parse molecules. Techniques from natural language processing [25]
might be fruitfully adapted to this domain.
Inability to distinguish stereoisomers Special bookkeeping is required to distinguish between
stereoisomers, including enantomers (mirror images of molecules) and cis/trans isomers (rotation
around double bonds). Most circular fingerprint implementations have the option to make these
distinctions. Neural fingerprints could be extended to be sensitive to stereoisomers, but this remains
a task for future work.

6

Related work

This work is similar in spirit to the neural Turing machine [7], in the sense that we take an existing
discrete computational architecture, and make each part differentiable in order to do gradient-based
optimization.
Neural nets for quantitative structure-activity relationship (QSAR) The modern standard for
predicting properties of novel molecules is to compose circular fingerprints with fully-connected
neural networks or other regression methods. [3] used circular fingerprints as inputs to an ensemble
of neural networks, Gaussian processes, and random forests. [19] used circular fingerprints (of depth
2) as inputs to a multitask neural network, showing that multiple tasks helped performance.
Neural graph fingerprints The most closely related work is [15], who build a neural network
having graph-valued inputs. Their approach is to remove all cycles and build the graph into a tree
structure, choosing one atom to be the root. A recursive neural network [23, 24] is then run from
the leaves to the root to produce a fixed-size representation. Because a graph having N nodes
has N possible roots, all N possible graphs are constructed. The final descriptor is a sum of the
representations computed by all distinct graphs. There are as many distinct graphs as there are
atoms in the network. The computational cost of this method thus grows as O(F 2 N 2 ), where F
is the size of the feature vector and N is the number of atoms, making it less suitable for large
molecules.
Convolutional neural networks Convolutional neural networks have been used to model images,
speech, and time series [14]. However, standard convolutional architectures use a fixed computational graph, making them difficult to apply to objects of varying size or structure, such as molecules.
More recently, [12] and others have developed a convolutional neural network architecture for modeling sentences of varying length.
Neural networks on fixed graphs [2] introduce convolutional networks on graphs in the regime
where the graph structure is fixed, and each training example differs only in having different features
at the vertices of the same graph. In contrast, our networks address the situation where each training
input is a different graph.
Neural networks on input-dependent graphs [22] propose a neural network model for graphs
having an interesting training procedure. The forward pass consists of running a message-passing
scheme to equilibrium, a fact which allows the reverse-mode gradient to be computed without storing
the entire forward computation. They apply their network to predicting mutagenesis of molecular
compounds as well as web page rankings. [16] also propose a neural network model for graphs
with a learning scheme whose inner loop optimizes not the training loss, but rather the correlation
between each newly-proposed vector and the training error residual. They apply their model to a
dataset of boiling points of 150 molecular compounds. Our paper builds on these ideas, with the
7

following differences: Our method replaces their complex training algorithms with simple gradientbased optimization, generalizes existing circular fingerprint computations, and applies these networks in the context of modern QSAR pipelines which use neural networks on top of the fingerprints
to increase model capacity.
Unrolled inference algorithms [9] and others have noted that iterative inference procedures
sometimes resemble the feedforward computation of a recurrent neural network. One natural extension of these ideas is to parameterize each inference step, and train a neural network to approximately
match the output of exact inference using only a small number of iterations. The neural fingerprint,
when viewed in this light, resembles an unrolled message-passing algorithm on the original graph.

7

Conclusion

We generalized existing hand-crafted molecular features to allow their optimization for diverse tasks.
By making each operation in the feature pipeline differentiable, we can use standard neural-network
training methods to scalably optimize the parameters of these neural molecular fingerprints end-toend. We demonstrated the interpretability and predictive performance of these new fingerprints.
Data-driven features have already replaced hand-crafted features in speech recognition, machine
vision, and natural-language processing. Carrying out the same task for virtual screening, drug
design, and materials design is a natural next step.
Acknowledgments
We thank Edward Pyzer-Knapp, Jennifer Wei, and Samsung Advanced Institute of Technology for
their support. This work was partially funded by NSF IIS-1421780.

References
[1] Fr?ed?eric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian J. Goodfellow, Arnaud
Bergeron, Nicolas Bouchard, and Yoshua Bengio. Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.
[2] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally
connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013.
[3] George E. Dahl, Navdeep Jaitly, and Ruslan Salakhutdinov. Multi-task neural networks for
QSAR predictions. arXiv preprint arXiv:1406.1231, 2014.
[4] John S. Delaney. ESOL: Estimating aqueous solubility directly from molecular structure. Journal of Chemical Information and Computer Sciences, 44(3):1000?1005, 2004.
[5] Francisco-Javier Gamo, Laura M Sanz, Jaume Vidal, Cristina de Cozar, Emilio Alvarez,
Jose-Luis Lavandera, Dana E Vanderwall, Darren VS Green, Vinod Kumar, Samiul Hasan,
et al. Thousands of chemical starting points for antimalarial lead identification. Nature,
465(7296):305?310, 2010.
[6] Robert C. Glem, Andreas Bender, Catrin H. Arnby, Lars Carlsson, Scott Boyer, and James
Smith. Circular fingerprints: flexible molecular descriptors with applications from physical
chemistry to ADME. IDrugs: the investigational drugs journal, 9(3):199?204, 2006.
[7] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural Turing machines. arXiv preprint
arXiv:1410.5401, 2014.
[8] Johannes Hachmann, Roberto Olivares-Amaya, Sule Atahan-Evrenk, Carlos Amador-Bedolla,
Roel S S?anchez-Carrera, Aryeh Gold-Parker, Leslie Vogt, Anna M Brockway, and Al?an
Aspuru-Guzik. The Harvard clean energy project: large-scale computational screening and
design of organic photovoltaics on the world community grid. The Journal of Physical Chemistry Letters, 2(17):2241?2251, 2011.
[9] John R Hershey, Jonathan Le Roux, and Felix Weninger. Deep unfolding: Model-based inspiration of novel deep architectures. arXiv preprint arXiv:1409.2574, 2014.
8

[10] Sepp Hochreiter and J?urgen Schmidhuber. Long short-term memory. Neural computation,
9(8):1735?1780, 1997.
[11] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
[12] Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. A convolutional neural network
for modelling sentences. Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, June 2014.
[13] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
[14] Yann LeCun and Yoshua Bengio. Convolutional networks for images, speech, and time series.
The handbook of brain theory and neural networks, 3361, 1995.
[15] Alessandro Lusci, Gianluca Pollastri, and Pierre Baldi. Deep architectures and deep learning
in chemoinformatics: the prediction of aqueous solubility for drug-like molecules. Journal of
chemical information and modeling, 53(7):1563?1575, 2013.
[16] Alessio Micheli. Neural network for graphs: A contextual constructive approach. Neural
Networks, IEEE Transactions on, 20(3):498?511, 2009.
[17] H.L. Morgan. The generation of a unique machine description for chemical structure. Journal
of Chemical Documentation, 5(2):107?113, 1965.
[18] Travis E Oliphant. Python for scientific computing. Computing in Science & Engineering,
9(3):10?20, 2007.
[19] Bharath Ramsundar, Steven Kearnes, Patrick Riley, Dale Webster, David Konerding, and Vijay
Pande. Massively multitask networks for drug discovery. arXiv:1502.02072, 2015.
[20] RDKit: Open-source cheminformatics. www.rdkit.org. [accessed 11-April-2013].
[21] David Rogers and Mathew Hahn. Extended-connectivity fingerprints. Journal of Chemical
Information and Modeling, 50(5):742?754, 2010.
[22] F. Scarselli, M. Gori, Ah Chung Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural
network model. Neural Networks, IEEE Transactions on, 20(1):61?80, Jan 2009.
[23] Richard Socher, Eric H Huang, Jeffrey Pennin, Christopher D Manning, and Andrew Y Ng.
Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In Advances
in Neural Information Processing Systems, pages 801?809, 2011.
[24] Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages
151?161. Association for Computational Linguistics, 2011.
[25] Kai Sheng Tai, Richard Socher, and Christopher D Manning.
Improved semantic
representations from tree-structured long short-term memory networks. arXiv preprint
arXiv:1503.00075, 2015.
[26] Tox21 Challenge. National center for advancing translational sciences. http://tripod.
nih.gov/tox21/challenge, 2014. [Online; accessed 2-June-2015].
[27] Thomas Unterthiner, Andreas Mayr, G?unter Klambauer, and Sepp Hochreiter. Toxicity prediction using deep learning. arXiv preprint arXiv:1503.01445, 2015.
[28] Thomas Unterthiner, Andreas Mayr, G u? nter Klambauer, Marvin Steijaert, J?org Wenger, Hugo
Ceulemans, and Sepp Hochreiter. Deep learning as an opportunity in virtual screening. In
Advances in Neural Information Processing Systems, 2014.
[29] Li Wan, Matthew Zeiler, Sixin Zhang, Yann L. Cun, and Rob Fergus. Regularization of neural
networks using dropconnect. In International Conference on Machine Learning, 2013.
[30] David Weininger. SMILES, a chemical language and information system. Journal of chemical
information and computer sciences, 28(1):31?36, 1988.

9


----------------------------------------------------------------

title: 5961-neural-adaptive-sequential-monte-carlo.pdf

Neural Adaptive Sequential Monte Carlo

Shixiang Gu??
Zoubin Ghahramani?
Richard E. Turner?
University of Cambridge, Department of Engineering, Cambridge UK
?
MPI for Intelligent Systems, T?ubingen, Germany
sg717@cam.ac.uk, zoubin@eng.cam.ac.uk, ret26@cam.ac.uk
?

Abstract
Sequential Monte Carlo (SMC), or particle filtering, is a popular class of methods for sampling from an intractable target distribution using a sequence of simpler intermediate distributions. Like other importance sampling-based methods,
performance is critically dependent on the proposal distribution: a bad proposal
can lead to arbitrarily inaccurate estimates of the target distribution. This paper
presents a new method for automatically adapting the proposal using an approximation of the Kullback-Leibler divergence between the true posterior and the
proposal distribution. The method is very flexible, applicable to any parameterized proposal distribution and it supports online and batch variants. We use the
new framework to adapt powerful proposal distributions with rich parameterizations based upon neural networks leading to Neural Adaptive Sequential Monte
Carlo (NASMC). Experiments indicate that NASMC significantly improves inference in a non-linear state space model outperforming adaptive proposal methods
including the Extended Kalman and Unscented Particle Filters. Experiments also
indicate that improved inference translates into improved parameter learning when
NASMC is used as a subroutine of Particle Marginal Metropolis Hastings. Finally
we show that NASMC is able to train a latent variable recurrent neural network
(LV-RNN) achieving results that compete with the state-of-the-art for polymorphic music modelling. NASMC can be seen as bridging the gap between adaptive
SMC methods and the recent work in scalable, black-box variational inference.

1

Introduction

Sequential Monte Carlo (SMC) is a class of algorithms that draw samples from a target distribution
of interest by sampling from a series of simpler intermediate distributions. More specifically, the sequence constructs a proposal for importance sampling (IS) [1, 2]. SMC is particularly well-suited for
performing inference in non-linear dynamical models with hidden variables, since filtering naturally
decomposes into a sequence, and in many such cases it is the state-of-the-art inference method [2, 3].
Generally speaking, inference methods can be used as modules in parameter learning systems. SMC
has been used in such a way for both approximate maximum-likelihood parameter learning [4] and
in Bayesian approaches such as the recently developed Particle MCMC methods [3].
Critically, in common with any importance sampling method, the performance of SMC is strongly
dependent on the choice of the proposal distribution. If the proposal is not well-matched to the target distribution, then the method can produce samples that have low effective sample size and this
leads to Monte Carlo estimates that have pathologically high variance [1]. The SMC community
has developed approaches to mitigate these limitations such as resampling to improve particle diversity when the effective sample size is low [1] and applying MCMC transition kernels to improve
particle diversity [5, 2, 3]. A complementary line of research leverages distributional approximate
inference methods, such as the extended Kalman Filter and Unscented Kalman Filter, to construct
better proposals, leading to the Extended Kalman Particle Filter (EKPF) and Unscented Particle Fil1

ter (UPF) [5]. In general, however, the construction of good proposal distributions is still an open
question that severely limits the applicability of SMC methods.
This paper proposes a new gradient-based black-box adaptive SMC method that automatically tunes
flexible proposal distributions. The quality of a proposal distribution can be assessed using the (intractable) Kullback-Leibler (KL) divergence between the target distribution and the parametrized
proposal distribution. We approximate the derivatives of this objective using samples derived from
SMC. The framework is very general and tractably handles complex parametric proposal distributions. For example, here we use neural networks to carry out the parameterization thereby leveraging
the large literature and efficient computational tools developed by this community. We demonstrate
that the method can efficiently learn good proposal distributions that significantly outperform existing adaptive proposal methods including the EKPF and UPF on standard benchmark models used
in the particle filter community. We show that improved performance of the SMC algorithm translates into improved mixing of the Particle Marginal Metropolis-Hasting (PMMH) [3]. Finally, we
show that the method allows higher-dimensional and more complicated models to be accurately handled using SMC, such as those parametrized using neural networks (NN), that are challenging for
traditional particle filtering methods .
The focus of this work is on improving SMC, but many of the ideas are inspired by the burgeoning
literature on approximate inference for unsupervised neural network models. These connections are
explored in section 6.

2

Sequential Monte Carlo

We begin by briefly reviewing two fundamental SMC algorithms, sequential importance sampling
(SIS) and sequential importance resampling (SIR). Consider a probabilistic model comprising (possibly multi-dimensional) hidden and observed states z1:T and x1:T respectively, whose joint disQT
tribution factorizes as p(z1:T , x1:T ) = p(z1 )p(x1 |z1 ) t=2 p(zt |z1:t?1 )p(xt |z1:t , x1:t?1 ). This
general form subsumes common state-space models, such as Hidden Markov Models (HMMs), as
well as non-Markovian models for the hidden state, such as Gaussian processes.
The goal of the sequential importance sampler is to approximate the posterior distribution over
PN
(n)
(n)
the hidden state sequence, p(z1:T |x1:T ) ? n=1 w
?t ?(z1:T ? z1:T ), through a weighted set of
(n)
N sampled trajectories drawn from a simpler proposal distribution {z1:T }n=1:N ? q(z1:T |x1:T ).
Any form of proposal distribution can be used in principle, but a particularly convenient one takes
QT
the same factorisation as the true posterior q(z1:T |x1:T ) = q(z1 |x1 ) t=2 q(zt |z1:t?1 , x1:t ), with
filtering dependence on x. A short derivation (see supplementary material) then shows that the
normalized importance weights are defined by a recursion:
(n)

(n)

(n)

w(z1:T ) =

(n)

(n)

(n)
p(zT |z1:T ?1 )p(xT |z1:T , x1:T ?1 )
w(z1:T )
(n)
(n)
,
w(z
?
)
=
? w(z
? 1:T ?1 )
P
1:T
(n)
(n)
(n) (n)
q(z1:T |x1:T )
q(zT |z1:T ?1 , x1:T )
n w(z1:T )

p(z1:T , x1:T )

SIS is elegant as the samples and weights can be computed in sequential fashion using a single
forward pass. However, na??ve implementation suffers from a severe pathology: the distribution
of importance weights often become highly skewed as t increases, with many samples attaining
very low weight. To alleviate the problem, the Sequential Importance Resampling (SIR) algorithm
(n)
[1] adds an additional step that resamples zt at time t from a multinomial distribution given by
(n)
w(z
? 1:t ) and gives the new particles equal weight.1 This replaces degenerated particles that have low
weight with samples that have more substantial importance weights without violating the validity of
the method. SIR requires knowledge of the full trajectory of previous samples at each stage to draw
the samples and compute the importance weights. For this reason, when carrying out resampling,
(n)
each new particle needs to update its ancestry information. Letting a?,t represent the ancestral
index of particle n at time t for state z? , where 1 ? ? ? t, and collecting these into the set
(n)

At

A

(n)

(n)

(i)

(n)

(n)

A

(i)

t?1
{z1:t?1
, zt } where z1:tt

1

(i)

(a

)

(n)

?,t
= {a1,t , ..., at,t }, where a? ?1,t = a? ?1,?
?1 , the resampled trajectory can be denoted z1:t =
(i)

a

(i)

a

= {z1 1,t , ..., zt t,t }. Finally, to lighten notation, we use the shorthand

More advanced implementations resample only when the effective sample size falls below a threshold [2].

2

(n)

(n)

wt = w(z1:t ) for the weights. Note that, when employing resampling, these do not depend on
(n)
the previous weights wt?1 since resampling has given the previous particles uniform weight. The
implementation of SMC is given by Algorithm 1 in the supplementary material.
2.1

The Critical Role of Proposal Distributions in Sequential Monte Carlo

The choice of the proposal distribution in SMC is critical. Even when employing the resampling
step, a poor proposal distribution will produce trajectories that, when traced backwards, quickly
collapse onto a single ancestor. Clearly this represents a poor approximation to the true posterior
p(z1:T |x1:T ). These effects can be mitigated by increasing the number of particles and/or applying
more complex additional MCMC moves [5, 2], but these strategies increase the computational cost.
The conclusion is that the proposal should be chosen with care. The optimal choice for an unconstrained proposal that has access to all of the observed data at all times is the intractable posterior
distribution q? (z1:T |x1:T ) = p? (z1:T |x1:T ). Given the restrictions imposed by the factorization,
this becomes q(zt |z1:t?1 , x1:t ) = p(zt |z1:t?1 , x1:t ), which is still typically intractable. The bootstrap filter instead uses the prior q(zt |z1:t?1 , x1:t ) = p(zt |z1:t?1 , x1:t?1 ) which is often tractable,
but fails to incorporate information from the current observation xt . A halfway-house employs
distributional approximate inference techniques to approximate p(zt |z1:t?1 , x1:t ). Examples include the EKPF and UPF [5]. However, these methods suffer from three main problems. First,
the extended and unscented Kalman Filter from which these methods are derived are known to be
inaccurate and poorly behaved for many problems outside of the SMC setting [6]. Second, these
approximations must be applied on a sample by sample basis, leading to significant additional computational overhead. Third, neither approximation is tuned using an SMC-relevant criterion. In the
next section we introduce a new method for adapting the proposal that addresses these limitations.

3

Adapting Proposals by Descending the Inclusive KL Divergence

In this work the quality of the proposal distribution will be optimized using the
inclusive KL-divergence between the true posterior distribution and the proposal,
KL[p? (z1:T |x1:T )||q? (z1:T |x1:T )]. (Parameters are made explicit since we will shortly be
interested in both adapting the proposal ? and learning the model ?.) This objective is chosen for
four main reasons. First, this is a direct measure of the quality of the proposal, unlike those typically
used such as effective sample size. Second, if the true posterior lies in the class of distributions
attainable by the proposal family then the objective has a global optimum at this point. Third, if
the true posterior does not lie within this class, then this KL divergence tends to find proposal
distributions that have higher entropy than the original which is advantageous for importance
sampling (the exclusive KL is unsuitable for this reason [7]). Fourth, the derivative of the objective
can be approximated efficiently using a sample based approximation that will now be described.
The gradient of the negative KL divergence with respect to the parameters of the proposal distribution takes a simple form,
Z
?
?
?
KL[p? (z1:T |x1:T )||q? (z1:T |x1:T )] = p? (z1:T |x1:T )
log q? (z1:T |x1:T )dz1:T .
??
??
The expectation over the posterior can be approximated using samples from SMC. One option would
use the weighted sample trajectories at the final time-step of SMC, but although asymptotically
unbiased such an estimator would have high variance due to the collapse of the trajectories. An
alternative, that reduces variance at the cost of introducing some bias, uses the intermediate ancestral
trees i.e. a filtering approximation (see the supplementary material for details),
(n)
X X (n) ?
?
At?1
(n)
KL[p? (z1:T |x1:T )||q? (z1:T |x1:T )] ?
w
?t
log q? (zt |x1:t , z1:t?1
). (1)
?
??
??
t
n
The simplicity of the proposed approach brings with it several advantages and opportunities.
Online and batch variants. Since the derivatives distribute over time, it is trivial to apply this
update in an online way e.g. updating the proposal distribution every time-step. Alternatively, when
learning parameters in a batch setting, it might be more appropriate to update the proposal parameters after making a full forward pass of SMC. Conveniently, when performing approximate
3

maximum-likelihood learning the gradient update for the model parameters ? can be efficiently
approximated using the same sample particles from SMC (see supplementary material and Algorithm 1). A similar derivation for maximum likelihood learning is also discussed in [4].
(n)
X X (n) ?
?
At?1
(n)
log[p? (x1:T )] ?
w
?t
log p? (xt , zt |x1:t?1 , z1:t?1
).
(2)
??
??
t
n
Algorithm 1 Stochastic Gradient Adaptive SMC (batch inference and learning variants)
Require: proposal: q? , model: p? , observations: X = {x1:Tj }j=1:M , number of particles: N
repeat
(j)
{x1:Tj }j=1:m ? NextMiniBatch(X)
(i,j)

(i,j)

(j)

{z1:t , w
?t }i=1:N,j=1:m,t=1:Tj ? SMC(?, ?, N, {x1:Tj }j=1:m )
(i,j)
P PTj P (i,j) ?
At?1
(i,j) (j)
4? = j t=1 i w
?t ?? log q? (zt |x1:t , z1:t?1
)
(i,j)
P PTj P (i,j) ?
At?1
(j)
(i,j) (j)
4? = j t=1 i w
?t ?? log p? (xt , zt |x1:t?1 , z1:t?1
) (optional)
? ? Optimize(?, 4?)
? ? Optimize(?, 4?) (optional)
until convergence

Efficiency of the adaptive proposal. In contrast to the EPF and UPF, the new method employs an
analytic function for propagation and does not require costly particle-specific distributional approximation as an inner-loop. Similarly, although the method bears similarity to the assumed-density filter
(ADF) [8] which minimizes a (local) inclusive KL, the new method has the advantage of minimizing
a global cost and does not require particle-specific moment matching.
Training complex proposal models. The adaptation method described above can be applied to any
parametric proposal distribution. Special cases have been previously treated by [9]. We propose
a related, but arguably more straightforward and general approach to proposal adaptation. In the
next section, we describe a rich family of proposal distributions, that go beyond previous work,
based upon neural networks. This approach enables adaptive SMC methods to make use of the rich
literature and optimization tools available from supervised learning.
Flexibility of training. One option is to train the proposal distribution using samples from SMC
derived from the observed data. However, this is not the only approach. For example, the proposal
could be trained using data sampled from the generative model instead, which might mitigate overfitting effects for small datasets. Similarly, the trained proposal does not need to be the one used to
generate the samples in the first place. The bootstrap filter or more complex variants can be used.

4

Flexible and Trainable Proposal Distributions Using Neural Networks

The proposed adaption method can be applied to any parametric proposal distribution. Here we
briefly describe how to utilize this flexibility to employ powerful neural network-based parameterizations that have recently shown excellent performance in supervised sequence learning tasks [10, 11].
Generally speaking, applications of these techniques to unsupervised sequence modeling settings is
an active research area that is still in its infancy [12] and this work opens a new avenue in this wider
research effort.
In a nutshell, the goal is to parameterize q? (zt |z1:t?1 , x1:t ) ? the proposal?s stochastic mapping from
all previous hidden states z1:t?1 and all observations (up to and including the current observation)
x1:t , to the current hidden state, zt ? in a flexible, computationally efficient and trainable way. Here
we use a class of functions called Long Short-Term Memory (LSTM) that define a deterministic
mapping from an input sequence to an output sequence using parameter-efficient recurrent dynamics, and alleviate the common vanishing gradient problem in recurrent neural networks [13, 10, 11].
The distributions q? (zt |ht ) can be a mixture of Gaussians (a mixture density network (MDN) [14])
in which the mixing proportions, means and covariances are parameterised through another neural
network (see the supplementary for details on LSTM, MDN, and neural network architectures).
4

5

Experiments

The goal of the experiments is three fold. First, to evaluate the performance of the adaptive method
for inference on standard benchmarks used by the SMC community with known ground truth. Second, to evaluate the performance when SMC is used as an inner loop of a learning algorithm. Again
we use an example with known ground truth. Third, to apply SMC learning to complex models that
would normally be challenging for SMC comparing to the state-of-the-art in approximate inference.
One way of assessing the success of the proposed method would be to evaluate
KL[p(z1:T |x1:T )||q(z1:T |x1:T )]. However, this quantity is hard to accurately compute. Instead
we use a number of other metrics. For the experiments where ground truth states z1:T are known
we can evaluate the root mean square error (RMSE) between the approximate
posterior mean of the
P
latent variables (z?t ) and the true value RMSE(z1:T , z?1:T ) = ( T1 t (zt ?P
z?t )2 )1/2 . More generally, the estimate of the log-marginal likelihood (LML = log p(x1:T ) = t log p(xt |x1:t?1 ) =
P
P (n)
1
t log( N
n wt )) and its variance is also indicative of performance. Finally, we also employ a
common metric called the effective sample size (ESS) to measure the effectiveness of our SMC
P
(n)
method. ESS of particles at time t is given by ESSt = ( n (w
?t )2 )?1 . If q(z1:T |x1:T ) =
p(z1:T |x1:T ), expected ESS is maximized and equals the number of particles (equivalently, the
normalized importance weights are uniform). Note that ESS alone is not a sufficient metric, since it
does not measure the absolute quality of samples, but rather the relative quality.
5.1

Inference in a Benchmark Nonlinear State-Space Model

In order to evaluate the effectiveness of our adaptive SMC method, we tested our method on a
standard nonlinear state-space model often used to benchmark SMC algorithms [2, 3]. The model is
given by Eq. 3, where ? = (?v , ?w ). The posterior distribution p? (z1:T |x1:T ) is highly multi-modal
due to uncertainty about the signs of the latent states.
p(zt |zt?1 ) = N (zt ; f (zt?1 , t), ?v2 ), p(z1 ) = N (z1 ; 0, 5),
2
p(xt |zt ) = N (xt ; g(zt?1 ), ?w
),

f (zt?1 , t) = zt?1 /2 + 25zt?1 /(1 +

2
zt?1
)

+ 8 cos(1.2t),

(3)
g(zt ) =

zt2 /20

The experiments investigated how the new proposal adaptation method performed in comparison to
standard methods including the bootstrap filter, EKPF, and UKPF. In particular, we were interested
in the following questions: Do rich multi-modal proposals improve inference? For this we compared
a Gaussian proposal with a diagonal Gaussian to a mixture density network with three components (MD-). Does a recurrent parameterization of the proposal help? For this we compared a non-recurrent
neural network with 100 hidden units (-NN-) to a recurrent neural network with 50 LSTM units (RNN-). Can injecting information about the prior dynamics into the proposal improve performance
(similar in spirit to [15] for variational methods)? To assess this, we parameterized proposals for vt
(process noise) instead of zt (-f-), and let the proposal have access to the prior dynamics f (zt?1 , t) .
For
? all experiments, the parameters in the non-linear state-space model were fixed to (?v , ?w ) =
( 10, 1). Adaptation of the proposal was performed on 1000 samples from the generative process
at each iteration. Results are summarized in Fig. 1 and Table 1 (see supplementary material for
additional results). Average run times for the algorithms over a sequence of length 1000 were:
0.782s bootstrap, 12.1s EKPF, 41.4s UPF, 1.70s NN-NASMC, and 2.67s RNN-NASMC, where
EKPF and UPF implementations are provided by [5]. Although these numbers should only be taken
as a guide as the implementations had differing levels of acceleration.
The new adaptive proposal methods significantly outperform the bootstrap, EKPF, and UPF methods, in terms of ESS, RMSE and the variance in the LML estimates. The multi-modal proposal
outperforms a simple Gaussian proposal (compare RNN-MD-f to RNN-f) indicating multi-modal
proposals can improve performance. Moreover, the RNN outperforms the non-recurrent NN (compare RNN to NN). Although the proposal models can effectively learn the transition function, injecting information about the prior dynamics into the proposal does help (compare RNN-f to RNN).
Interestingly, there is no clear cut winner between the EKPF and UPF, although the UPF does return
LML estimates that have lower variance [5]. All methods converged to similar LMLs that were close
to the values computed using large numbers of particles indicating the implementations are correct.
5

80
?2600

70
effective sample size (/100)

log marginal likelihood

?2800
?3000
?3200
?3400
?3600

EKPF
NN-MD
prior
RNN-f
RNN-MD-f
RNN-MD
RNN
UPF

50
40
30
20

?3800
?4000

60

F
EKP

D
NN-M

r
prio

-f
RNN

-f
D
-MD NN-M
R
RNN

10
0

UPF

RNN

200

400

600

800

1000

iteration

Figure 1: Left: Box plots for LML estimates from iteration 200 to 1000. Right: Average ESS over
the first 1000 iterations.

ESS (iter)
mean std
36.66 0.25
60.15 0.83
50.58 0.63
69.64 0.60
73.88 0.71
69.25 1.04
76.71 0.68
69.39 1.08

prior
EKPF
UPF
RNN
RNN-f
RNN-MD
RNN-MD-f
NN-MD

LML
mean std
-2957 148
-2829 407
-2696 79
-2774 34
-2633 36
-2636 40
-2622 32
-2634 36

RMSE
mean std
3.266 0.578
3.578 0.694
2.956 0.629
3.505 0.977
2.568 0.430
2.612 0.472
2.509 0.409
2.731 0.608

Table 1: Left, Middle: Average ESS and log marginal likelihood estimates over the last 400 iterations. Right: The RMSE over 100 new sequences with no further adaptation.

5.2

Inference in the Cart and Pole System

As a second and more physically meaningful system we considered a cart-pole system that consists
of an inverted pendulum that rests on a movable base [16]. The system was driven by a white noise
input. An ODE solver was used to simulate the system from its equations of motion. We considered
the problem of inferring the true position of the cart and orientation of the pendulum (along with
their derivatives and the input noise) from noisy measurements of the location of the tip of the pole.
The results are presented in Fig. 2. The system is significantly more intricate than the model in
Sec. 5.1, and does not directly admit the usage of EKPF or UPF. Our RNN-MD proposal model
successfully learns good proposals without any direct access to the prior dynamics.

ESS

0.45
0.40
0.35

x

2.0

RNN-MD
prior-?
prior-(? + 1?)
prior-(? ? 1?)

1.5

0.5

1.0
0.0

0.5

0.25

?? (rad)

x(m)

ESS

0.30

?0.5

0.0
?0.5

0.20

?1.5
500

1000

1500
2000
iteration

2500

3000

?2.0
0

?1.0

prior
RNN-MD
ground-truth

?1.0

0.15
0.10
0

??

1.0

2

4

?1.5
6

time (s)

8

10

?2.0
0

2

4

6

8

10

time (s)

Figure 2: Left: Normalized ESS over iterations. Middle, Right: Posterior mean vs. ground-truth
for x, the horizontal location of the cart, and 4?, the change in relative angle of the pole. RNN-MD
learns to have higher ESS than the prior and more accurately estimates the latent states.
6

?w (N=100)

6
5

5

4

4

3

3

2

2

1

1

0
0

100

200

300

?w (N=10)

6

400

0
0

500

prior
RNN-MD-f-pre
RNN-MD-f
RNN-MD-pre
RNN-MD

200

400

600

800

1000

iteration

iteration

Figure 3: PMMH samples of ?w values for N = {100, 10} particles. For small numbers of particles
(right) PMMH is very slow to burn in and mix when proposing from the prior distribution due to the
large variance in the marginal likelihood estimates it returns.

5.3

Bayesian learning in a Nonlinear SSM

SMC is often employed as an inner loop of a more complex algorithm. One prominent example
is Particle Markov Chain Monte Carlo [3], a class of methods that sample from the joint posterior
over model parameters ? and latent state trajectories, p(?, z1:T |x1:T ). Here we consider the Particle
Marginal Metropolis-Hasting sampler (PMMH). In this context SMC is used to construct a proposal
distribution for a Metropolis-Hasting (MH) accept/reject step. The proposal is formed by sampling a
proposed set of parameters e.g. by perturbing the current parameters using a Gaussian random walk,
then SMC is used to sample a proposed set of latent state variables, resulting in a joint proposal
?
?
|x1:T ). The MH step uses the SMC marginal likelihood
|?, z1:T ) = q(?? |?)p?? (z1:T
q(?? , z1:T
estimates to determine acceptance. Full details are given in the supplementary material.
In this experiment, we evaluate our method in a PMMH sampler on the same model from Section 5.1 following [3].2 A random walk proposal is used to sample ? = (?v , ?w ), q(?? |?) =
N (?? |?, diag([0.15, 0.08])). The prior over ? is set as IG(0.01, 0.01). ? is initialized as (10, 10),
and the PMMH is run for 500 iterations.
Two of the adaptive models considered section 5.1 are used for comparison (RNN-MD and RNNMD-f) , where ?-pre-? models are pre-trained for 500 iterations using samples from the initial ? =
(10, 10). The results are shown in Fig. 3 and were typical for a range of parameter settings. Given a
sufficient number of particles (N = 100), there is almost no difference between the prior proposal
and our method. However, when the number of particles gets smaller (N = 10), NASMC enables
significantly faster burn-in to the posterior, particularly on the measurement noise ?w and, for similar
reasons, NASMC mixes more quickly. The limitation with the NASMC-PMMH is that the model
needs to continuously adapt as the global parameter is sampled, but note this is still not as costly as
adapting on a particle-by-particle basis as is the case for the EKPF and UPF.
5.4

Polyphonic Music Generation

Finally, the new method is used to train a latent variable recurrent neural network (LV-RNN) for
modelling four polymorphic music datasets of varying complexity [17]. These datasets are often
used to benchmark RNN models because of their high dimensionality and the complex temporal
dependencies involved at different time scales [17, 18, 19]. Each dataset contains at least 7 hours of
polyphonic music with an average polyphony (number of simultaneous notes) of 3.9 out of 88. LVRNN contains a recurrent neural network with LSTM layers that is driven by i.i.d. stochastic latent
variables (zt ) at each time-point and stochastic outputs (xt ) that are fed back into the dynamics (full
details in the supplementary material). Both the LSTM layers in the generative and proposal models
are set as 1000 units and Adam [20] is used as the optimizer. The bootstrap filter is compared to
the new adaptive method (NASMC). 10 particles are used in the training. The hyperparameters
are tuned using the validation set [17]. A diagonal Gaussian output is used in the proposal model,
with an additional hidden layer of size 200. The log likelihood on the test set, a standard metric
for comparison in generative models [18, 21, 19], is approximated using SMC with 500 particles.
2

Only the prior proposal is compared, since Sec. 5.1 shows the advantage of our method over EKPF/UPF.

7

The results are reported in Table 2.3 The adaptive method significantly outperforms the bootstrap
filter on three of the four datasets. On the piano dataset the bootstrap method performs marginally
better. In general, the NLLs for the new methods are comparable to the state-of-the-art although
detailed comparison is difficult as the methods with stochastic latent states require approximate
marginalization using importance sampling or SMC.
Dataset
Piano-midi-de
Nottingham
MuseData
JSBChorales

LV-RNN
(NASMC)
7.61
2.72
6.89
3.99

LV-RNN
(Bootstrap)
7.50
3.33
7.21
4.26

STORN
(SGVB)
7.13
2.85
6.16
6.91

FD-RNN

sRNN

RNN-NADE

7.39
3.09
6.75
8.01

7.58
3.43
6.99
8.58

7.03
2.31
5.60
5.19

Table 2: Estimated negative log likelihood on test data. ?FD-RNN? and ?STORN? are from [19],
and ?sRNN? and ?RNN-NADE? are results from [18].

6

Comparison of Variational Inference to the NASMC approach

There are several similarities between NASMC and Variational Free-energy methods that employ recognition models. Variational Free-energy methods refine an approximation q? (z|x) to
the posterior distribution p? (z|x) by optimising the exclusive (or variational) KL-divergence
KL[q? (z|x)||p? (z|x)]. It is common to approximate this integral using samples from the approximate posterior [21, 22, 23]. This general approach is similar in spirit to the way that the proposal is
adapted in NASMC, except that the inclusive KL-divergence is employed KL[p? (z|x)||q? (z|x)] and
this entails that sample based approximation requires simulation from the true posterior. Critically,
NASMC uses the approximate posterior as a proposal distribution to construct a more accurate posterior approximation. The SMC algorithm therefore can be seen as correcting for the deficiencies in
the proposal approximation. We believe that this can lead to significant advantages over variational
free-energy methods, especially in the time-series setting where variational methods are known to
have severe biases [24]. Moreover, using the inclusive KL avoids having to compute the entropy
of the approximating distribution which can prove problematic when using complex approximating
distributions (e.g. mixtures and heavy tailed distributions) in the variational framework. There is a
close connection between NASMC and the wake-sleep algorithm [25] . The wake-sleep algorithm
also employs the inclusive KL divergence to refine a posterior approximation and recent generalizations have shown how to incorporate this idea into importance sampling [26]. In this context, the
NASMC algorithm extends this work to SMC.

7

Conclusion

This paper developed a powerful method for adapting proposal distributions within general SMC
algorithms. The method parameterises a proposal distribution using a recurrent neural network
to model long-range contextual information, allows flexible distributional forms including mixture
density networks, and enables efficient training by stochastic gradient descent. The method was
found to outperform existing adaptive proposal mechanisms including the EKPF and UPF on a standard SMC benchmark, it improves burn in and mixing of the PMMH sampler, and allows effective
training of latent variable recurrent neural networks using SMC. We hope that the connection between SMC and neural network technologies will inspire further research into adaptive SMC methods. In particular, application of the methods developed in this paper to adaptive particle smoothing,
high-dimensional latent models and adaptive PMCMC for probabilistic programming are particular
exciting avenues.
Acknowledgments
SG is generously supported by Cambridge-T?ubingen Fellowship, the ALTA Institute, and Jesus
College, Cambridge. RET thanks the EPSRC (grants EP/G050821/1 and EP/L000776/1). We thank
Theano developers for their toolkit, the authors of [5] for releasing the source code, and Roger
Frigola, Sumeet Singh, Fredrik Lindsten, and Thomas Sch?on for helpful suggestions on experiments.
3

Results for RNN-NADE are separately provided for reference, since this is a different model class.

8

References
[1] N. J. Gordon, D. J. Salmond, and A. F. Smith, ?Novel approach to nonlinear/non-gaussian bayesian state
estimation,? in IEE Proceedings F (Radar and Signal Processing), vol. 140, pp. 107?113, IET, 1993.
[2] A. Doucet, N. De Freitas, and N. Gordon, Sequential monte carlo methods in practice. Springer-Verlag,
2001.
[3] C. Andrieu, A. Doucet, and R. Holenstein, ?Particle markov chain monte carlo methods,? Journal of the
Royal Statistical Society: Series B (Statistical Methodology), vol. 72, no. 3, pp. 269?342, 2010.
[4] G. Poyiadjis, A. Doucet, and S. S. Singh, ?Particle approximations of the score and observed information
matrix in state space models with application to parameter estimation,? Biometrika, vol. 98, no. 1, pp. 65?
80, 2011.
[5] R. Van Der Merwe, A. Doucet, N. De Freitas, and E. Wan, ?The unscented particle filter,? in Advances in
Neural Information Processing Systems, pp. 584?590, 2000.
[6] R. Frigola, Y. Chen, and C. Rasmussen, ?Variational gaussian process state-space models,? in Advances
in Neural Information Processing Systems, pp. 3680?3688, 2014.
[7] D. J. MacKay, Information theory, inference, and learning algorithms, vol. 7. Cambridge university press
Cambridge, 2003.
[8] T. P. Minka, ?Expectation propagation for approximate bayesian inference,? in Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence, pp. 362?369, Morgan Kaufmann Publishers
Inc., 2001.
[9] J. Cornebise, Adaptive Sequential Monte Carlo Methods. PhD thesis, Ph. D. thesis, University Pierre and
Marie Curie?Paris 6, 2009.
[10] A. Graves, Supervised sequence labelling with recurrent neural networks, vol. 385. Springer, 2012.
[11] I. Sutskever, O. Vinyals, and Q. V. Le, ?Sequence to sequence learning with neural networks,? in Advances
in Neural Information Processing Systems, pp. 3104?3112, 2014.
[12] A. Graves, ?Generating sequences with recurrent neural networks,? CoRR, vol. abs/1308.0850, 2013.
[13] S. Hochreiter and J. Schmidhuber, ?Long short-term memory,? Neural computation, vol. 9, no. 8,
pp. 1735?1780, 1997.
[14] C. M. Bishop, ?Mixture density networks,? 1994.
[15] K. Gregor, I. Danihelka, A. Graves, D. J. Rezende, and D. Wierstra, ?DRAW: A recurrent neural network
for image generation,? in Proceedings of the 32nd International Conference on Machine Learning, ICML
2015, Lille, France, 6-11 July 2015, pp. 1462?1471, 2015.
[16] A. McHutchon, Nonlinear modelling and control using Gaussian processes. PhD thesis, University of
Cambridge UK, Department of Engineering, 2014.
[17] N. Boulanger-Lewandowski, Y. Bengio, and P. Vincent, ?Modeling temporal dependencies in highdimensional sequences: Application to polyphonic music generation and transcription,? in International
Conference on Machine Learning (ICML), 2012.
[18] Y. Bengio, N. Boulanger-Lewandowski, and R. Pascanu, ?Advances in optimizing recurrent networks,? in
Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pp. 8624?
8628, IEEE, 2013.
[19] J. Bayer and C. Osendorfer, ?Learning stochastic recurrent networks,? arXiv preprint arXiv:1411.7610,
2014.
[20] D. P. Kingma and J. Ba, ?Adam: A method for stochastic optimization,? The International Conference on
Learning Representations (ICLR), 2015.
[21] D. P. Kingma and M. Welling, ?Auto-encoding variational bayes,? The International Conference on
Learning Representations (ICLR), 2014.
[22] D. J. Rezende, S. Mohamed, and D. Wierstra, ?Stochastic backpropagation and approximate inference in
deep generative models,? International Conference on Machine Learning (ICML), 2014.
[23] A. Mnih and K. Gregor, ?Neural variational inference and learning in belief networks,? International
Conference on Machine Learning (ICML), 2014.
[24] R. E. Turner and M. Sahani, ?Two problems with variational expectation maximisation for time-series
models,? in Bayesian Time series models (D. Barber, T. Cemgil, and S. Chiappa, eds.), ch. 5, pp. 109?
130, Cambridge University Press, 2011.
[25] G. E. Hinton, P. Dayan, B. J. Frey, and R. M. Neal, ?The? wake-sleep? algorithm for unsupervised neural
networks,? Science, vol. 268, no. 5214, pp. 1158?1161, 1995.
[26] J. Bornschein and Y. Bengio, ?Reweighted wake-sleep,? The International Conference on Learning Representations (ICLR), 2015.

9


----------------------------------------------------------------

title: 5278-general-stochastic-networks-for-classification.pdf

General Stochastic Networks for Classification
Matthias Z?ohrer and Franz Pernkopf
Signal Processing and Speech Communication Laboratory
Graz University of Technology
matthias.zoehrer@tugraz.at, pernkopf@tugraz.at

Abstract
We extend generative stochastic networks to supervised learning of representations. In particular, we introduce a hybrid training objective considering a generative and discriminative cost function governed by a trade-off parameter ?. We use
a new variant of network training involving noise injection, i.e. walkback training, to jointly optimize multiple network layers. Neither additional regularization
constraints, such as `1, `2 norms or dropout variants, nor pooling- or convolutional layers were added. Nevertheless, we are able to obtain state-of-the-art performance on the MNIST dataset, without using permutation invariant digits and
outperform baseline models on sub-variants of the MNIST and rectangles dataset
significantly.

1

Introduction

Since 2006 there has been a boost in machine learning due to improvements in the field of unsupervised learning of representations. Most accomplishments originate from variants of restricted
Boltzmann machines (RBMs) [1], auto-encoders (AE) [2, 3] and sparse-coding [4, 5, 6]. Deep models in representation learning, also obtain impressive results in supervised learning problems, such
as speech recognition, e.g. [7, 8, 9] and computer vision tasks [10].
If no a-priori knowledge is modeled in the architecture, cf. convolutional layers or pooling layers
[11], generatively pre-trained networks are among the best when applied to supervised learning tasks
[12]. Usually, a generative representation is obtained through a greedy-layerwise training procedure
called contrastive divergence (CD) [1]. In this case, the network layer learns the representation from
the layer below by treating the latter as static input. Despite of the impressive results achieved with
CD, we identify two (minor) drawbacks when used for supervised learning: Firstly, after obtaining
a representation by pre-training a network, a new discriminative model is initialized with the trained
weights, splitting the training into two separate models. This seems to be neither biologically plausible, nor optimal when it comes to optimization, as carefully designed early stopping criteria have to
be implemented to prevent over- or under-fitting. Secondly, generative and discriminative objectives
might influence each other beneficially when combined during training. CD does not take this into
account.
In this work, we introduce a new training procedure for supervised learning of representations. In
particular we define a hybrid training objective for general stochastic networks (GSN), dividing the
cost function into a generative and discriminative part, controlled by a trade-off parameter ?. It turns
out that by annealing ?, when solving this unconstrained non-convex multi-objective optimization
problem, we do not suffer from the shortcomings described above. We are able to obtain stateof-the-art performance on the MNIST [13] dataset, without using permutation invariant digits and
significantly outperform baseline models on sub-variants of the MNIST and rectangle database [14].
Our approach is related to the generative-discriminative training approach of RBMs [15]. However
a different model and a new variant of network training involving noise injection, i.e. walkback
training [16, 17], is used to jointly optimize multiple network layers. Most notably, we did not
1

apply any additional regularization constraints, such as `1, `2 norms or dropout variants [12], [18],
unlocking further potential for possible optimizations. The model can be extended to learn multiple
tasks at the same time using jointly trained weights and by introducing multiple objectives. This
might also open a new prospect in the field of transfer learning [19] and multi-task learning [20]
beyond classification.
This paper is organized as follows: Section 2 presents mathematical background material i.e. the
GSN and a hybrid learning criterion. In Section 3 we empirically study the influence of hyper
parameters of GSNs and present experimental results. Section 4 concludes the paper and provides a
perspective on future work.

2

General Stochastic Networks

Recently, a new supervised learning algorithm called walkback training for generalized autoencoders (GAE) was introduced [16]. A follow-up study [17] defined a new network model ?
generative stochastic networks, extending the idea of walkback training to multiple layers. When
applied to image reconstruction, they were able to outperform various baseline systems, due to its
ability to learn multi-modal representations [17, 21]. In this paper, we extend the work of [17].
First, we provide mathematical background material for generative stochastic networks. Then, we
introduce modifications to make the model suitable for supervised learning. In particular we present
a hybrid training objective, dividing the cost into a generative and discriminative part. This paves
the way for any multi-objective learning of GSNs. We also introduce a new terminology, i.e. general stochastic networks, a model class including generative-, discriminative- and hybrid stochastic
network variants.
General Stochastic Networks for Unsupervised Learning
Restricted Boltzmann machines (RBM) [22] and denoising autoencoders (DAE) [3] share the following commonality; The input distribution P (X) is sampled to convergence in a Markov chain.
In the case of the DAE, the transition operator first samples the hidden state Ht from a corruption
distribution C(H|X), and generates a reconstruction from the parametrized model, i.e the density
P?2 (X|H).
Ht+1
P? 1

Ht+2
P?1

P?2

Xt+0

Ht+3
P?1

P?2

Xt+1

Ht+4

P?2

Xt+2

P? 1

P?1
P?2

Xt+3

Xt+4

Figure 1: DAE Markov chain.
The resulting DAE Markov chain, shown in Figure 1, is defined as
Ht+1 ? P?1 (H|Xt+0 ) and Xt+1 ? P?2 (X|Ht+1 ),

(1)

where Xt+0 is the input sample X, fed into the chain at time step 0 and Xt+1 is the reconstruction
of X at time step 1. In the case of a GSN, an additional dependency between the latent variables Ht
over time is introduced to the network graph. The GSN Markov chain is defined as follows:
Ht+1 ? P?1 (H|Ht+0 , Xt+0 ) and Xt+1 ? P?2 (X|Ht+1 ).

(2)

Figure 2 shows the corresponding network graph.
This chain can be expressed with deterministic functions of random variables f? ? {f?? , f?? }. In
particular, the density f? is used to model Ht+1 = f? (Xt+0 , Zt+0 , Ht+0 ), specified for some independent noise source Zt+0 , with the condition that Xt+0 cannot be recovered exactly from Ht+1 .
2

Ht+0

Ht+1

Ht+2

P?1

Ht+3

P?1
P?2

Xt+0

Ht+4

P?1
P?2

Xt+1

P?1

P?1
P?2

Xt+2

P? 2

Xt+3

Xt+4

Figure 2: GSN Markov chain.
We introduce f??i as a back-probable stochastic non-linearity of the form f??i = ?out + g(?in + a
?i )
with noise processes Zt ? {?in , ?out } for layer i. The variable a
?i is the activation for unit i, where
a
?i = W i Iti + bi with a weight matrix W i and bias bi , representing the parametric distribution. It is
embedded in a non-linear activation function g. The input Iti is either the realization xit of observed
sample Xti or the hidden realization hit of Hti . In general, f??i (Iti ) specifies an upward path in a GSN
i
for a specific layer i. In the case of Xt+1
= f??i (Zt+0 , Ht+1 ) we define f??i (Hti ) = ?out + g(?in + a
?i )
i
i
i T
i
as a downward path in the network i.e. a
? = (W ) Ht + b , using the transpose of the weight
matrix W i and the bias bi . This formulation allows to directly back-propagate the reconstruction log-likelihood P (X|H) for all parameters ? ? {W 0 , ..., W d , b0 , ..., bd } where d is the
number of hidden layers. In Figure 2 the GSN includes a simple hidden layer. This can be
extended to multiple hidden layers requiring multiple deterministic functions of random variables
f? ? {f??0 , ..., f??d , f??0 , ...f??d }.
Figure 3 visualizes the Markov chain for a multi-layer GSN, inspired by the unfolded computational
graph of a deep Boltzmann machine Gibbs sampling process.
3
Ht+3

3
Ht+4

f??2

f??2

2
Ht+2

f??1

f??1

f??1

f??1

1
Ht+3

f??0

f??2

2
Ht+4

f??1

1
Ht+2

f??0

f??0

f??2

2
Ht+3

f??1

1
Ht+1

f??0

f??2

f??0

f??1

1
Ht+4

f??0

f??0

f??0

f??0

0
Xt+0

0
Xt+1

0
Xt+2

0
Xt+3

0
Xt+4

Xt+0

0
Lt {Xt+1
, Xt+0 }

0
Lt {Xt+2
, Xt+0 }

0
Lt {Xt+3
, Xt+0 }

0
Lt {Xt+4
, Xt+0 }

Figure 3: GSN Markov chain with multiple layers and backprop-able stochastic units.
In the training case, alternatively even or odd layers are updated at the same time. The information
is propagated both upwards and downwards for K steps allowing the network to build higher order
representations. An example for this update process is given in Figure 3. In the even update (marked
2
1
0
0
1
= f??0 (Ht+1
) and Ht+2
=
in red) Ht+1
= f??0 (Xt+0
). In the odd update (marked in blue) Xt+1
?
?
?
?
2
1
1
0
2
3
0
1
2
1
f? (Ht+1 ) for k = 0. In the case of k = 1, Ht+2 = f? (Xt+1 ) + f? (Ht+2 ) and Ht+3 = f? (Ht+2 )
1
2
1
3
0
= f??0 (Ht+2
) and Ht+3
= f??1 (Ht+2
) + f??2 (Ht+3
) in the odd update.
in the even update and Xt+2
?
?
?
1
0
2
3
2
0
1
2
In case of k = 2, Ht+3 = f? (Xt+2 ) + f? (Ht+3 ) and Ht+4 = f? (Ht+3 ) in the even update and
0
1
2
1
3
Xt+3
= f??0 (Ht+3
) and Ht+4
= f??1 (Ht+3
) + f??2 (Ht+4
) in the odd update.
The cost function of a generative GSN can be written as:

C=

K
X

0
Lt {Xt+k
, Xt+0 },

k=1

3

(3)

Lt is a specific loss-function such as the mean squared error (MSE) at time step t. In general any
0
arbitrary loss function could be used (as long as they can be seen as a log-likelihood) [16]. Xt+k
0
is the reconstruction of the input Xt+0 at layer 0 after k steps. Optimizing the loss function by
building the sum over the costs of multiple corrupted reconstructions is called walkback training
[16, 17]. This form of network training leads to a significant performance boost when used for input
reconstruction. The network is able to handle multi-modal input representations and is therefore
considerably more favorable than standard generative models [16].
General Stochastic Networks for Supervised Learning
In order to make a GSN suitable for a supervised learning task we introduce the output Y to the
network graph. In this case L = log P (X) + log P (Y |X). Although the target Y is not fed into the
network, it is introduced as an additional cost term. The layer update-process stays the same.
3
Lt {Ht+1
, Yt+0 }

3
Lt {Ht+2
, Yt+0 }

3
Ht+3

3
Ht+4

f??2

f??2

2
Ht+2

f??1

f??1

f??0

f??1

f??1

1
Ht+3

f??0

f??2

2
Ht+4

f??1

1
Ht+2

f??0

f??2

2
Ht+3

f??1

1
Ht+1

f??0

f??2

f??0

f??1

1
Ht+4

f??0

f??0

f??0

f??0

0
Xt+0

0
Xt+1

0
Xt+2

0
Xt+3

0
Xt+4

Xt+0

0
Lt {Xt+1
, Xt+0 }

0
Lt {Xt+2
, Xt+0 }

0
Lt {Xt+3
, Xt+0 }

0
Lt {Xt+4
, Xt+0 }

Figure 4: GSN Markov chain for input Xt+0 and target Yt+0 with backprop-able stochastic units.
We define the following cost function for a 3-layer GSN:

C=

K
K
1?? X
? X
3
Lt {Xt+k , Xt+0 } +
Lt {Ht+k
, Yt+0 }
K
K ?d+1
k=1
k=d
|
{z
} |
{z
}
generative
discriminative

(4)

This is a non-convex multi-objective optimization problem, where ? weights the generative and
discriminative part of C. The parameter d specifies the number of network layers i.e. depth of the
network. Scaling the mean loss in (4) is not mandatory, but allows to equally balance both loss terms
with ? = 0.5 for input Xt+0 and target Yt+0 scaled to the same range. Again Figure 4 shows the
corresponding network graph for supervised learning with red and blue edges denoting the even and
odd network updates.
In general the hybrid objective optimization criterion is not restricted to hX, Y i, as additional input
and output terms could be introduced to the network. This setup might be useful for transfer-learning
[19] or multi-task scenarios [20], which is not discussed in this paper.

3

Experimental Results

In order to evaluate the capabilities of GSNs for supervised learning, we studied MNIST digits
[13], variants of MNIST digits [14] and the rectangle datasets [14]. The first database consists of
60.000 labeled training and 10.000 labeled test images of handwritten digits. The second dataset includes 6 variants of MNIST digits, i.e. { mnist-basic, mnist-rot, mnist-back-rand, mnist-back-image,
mnist-rot-back-image }, with additional factors of variation added to the original data. Each variant
includes 10.000 labeled training, 2000 labeled validation, and 50.000 labeled test images. The third
dataset involves two subsets, i.e. { rectangle, rectangle-image }. The dataset rectangle consists of
4

1000 labeled training, 200 labeled validation, and 50.000 labeled test images. The dataset rectangleimage includes 10.000 labeled train, 2000 labeled validation and 50.000 labeled test images.
In a first experiment we focused on the multi-objective optimization problem defined in (4). Next we
evaluated the number of walkback steps in a GSN, necessary for convergence. In a third experiment
we analyzed the influence of different Gaussian noise settings during walkback training, improving
the generalization capabilities of the network. Finally we summarize classification results for all
datasets and compare to baseline systems [14].
3.1

Multi-Objective Optimization in a Hybrid Learning Setup

In order to solve the non-convex multi-objective optimization problem, variants of stochastic gradient descent (SGD) can be used. We applied a search over fixed ? values on all problems. Furthermore, we show that the use of an annealed ? factor, during training works best in practice.
In all experiments a three layer GSN, i.e. GSN-3, with 2000 neurons in each layer, randomly initialized with small Gaussian noise, i.e. 0.01 ? N (0, 1), and an MSE loss function for both inputs and
targets was used. Regarding optimization we applied SGD with a learning rate ? = 0.1, a momentum term of 0.9 and a multiplicative annealing factor ?n+1 = ?n ? 0.99 per epoch n for the learning
rate. A rectifier unit [23] was chosen as activation function. Following the ideas of [24] no explicit
sampling was applied at the input and output layer. In the test case the zero-one loss was computed
averaging the network?s output over k walkback steps.
Analysis of the Hybrid Learning Parameter ?
Concerning the influence of the trade-off parameter ?, we tested fixed ? values in the range
? ? {0.01, 0.1, 0.2, ..., 0.9, 0.99}, where low values emphasize the discriminative part in the objective and vice versa. Walkback training with K = 6 steps using zero-mean pre- and postactivation Gaussian noise with zero mean and variance ? = 0.1 was performed for 500 training epochs. In a more dynamic scenario ?n=1 = 1 was annealed by ?n+1 = ?n ? ? to reach
?n=500 ? {0.01, 0.1, 0.2, ..., 0.9, 0.99} within 500 epochs, simulating generative pre-training to a
certain extend.

Figure 5: Influence of dynamic and static ? on MNIST variants basic (left), rotated (middle) and
background (right) where ? denotes the training-, 4 the validation- and 5 the test-set. The dashed
line denotes the static setup, the bold line the dynamic setup.

Figure 5 compares the results of both GSNs, using static and dynamic ? setups on the MNIST
variants basic, rotated and background. The use of a dynamic i.e. annealed ?n=500 = 0.01, achieved
the best validation and test error in all experiments. In this case, more attention was given to the
generative proportion P (X) of the objective (4) in the early stage of training. After approximately
400 epochs discriminative training i.e. fine-tuning, dominates. This setup is closely related to DBN
training, where emphasis is on optimizing P (X) at the beginning of the optimization, whereas
P (Y |X) is important at the last stages. In case of the GSN, the annealed ? achieves a more smooth
transition by shifting the weight in the optimization criterion from P (X) to P (Y |X) within one
model.
5

Analysis of Walkback Steps K
In a next experiment we tested the influence of K walkback steps for GSNs. Figure 6 shows the
results for different GSNs, trained with K ? {6, 7, 8, 9, 10} walkback steps and annealed ? with
? = 0.99. In all cases the information was at least propagated once up and once downwards in the
d = 3 layer network using fixed Gaussian pre- and post-activation noise with ? = 0 and ? = 0.1.

Figure 6: Evaluating the number of walkback steps on MNIST variants basic (left), rotated (middle)
and background (right) where ? denotes the training-, 4 the validation- and 5 the test-set.
Figure 6 shows that increasing the walkback steps, does not improve the generalization capabilities
of the used GSNs. The setup K = 2 ? d is sufficient for convergence and achieves the best validation
and test result in all experiments.
Analysis of Pre- and Post-Activation Noise
Injecting noise during the training process of GSNs serves as a regularizer and improves the generalization capabilities of the model [17]. In this experiment the influence of Gaussian pre- and
post-activation noise with ? = 0 and ? ? {0.05, 0.1, 0.15, 0.2, 0.25, 0.3} and deactivated noise
during training, was tested on a GSN-3 trained for K = 6 walkback steps. The trade-off factor
? was annealed with ? = 0.99. Figure 7 summarizes the results of the different GSNs for the
MNIST variants basic, rotated and background. Setting ? = 0.1 achieved the best overall result
on the validation- and test-set for all three experiments. In all other cases the GSNs either over- or
underfitted the data.

Figure 7: Evaluating noise injections during training on MNIST variants basic (left), rotated (middle)
and background (right) where ? denotes the training-, 4 the validation- and 5 the test-set.

3.2

MNIST results

Table 1 presents the average classification error of three runs of all MNIST variation datasets obtained by a GSN-3, using fixed Gaussian pre- and post-activation noise with ? = 0, ? = 0.1 and
K = 6 walkback steps. The hybrid learning parameter ? was annealed with ? = 0.99 and ?n=1 = 1.
A small grid test was performed in the range of N ? d with N ? {1000, 2000, 3000} neurons per
layer for d ? {1, 2, 3} layers to find the optimal network configuration.
6

Dataset

SVMrbf

SVMpoly NNet

DBN-1

SAA-3

DBN-3

GSN-3

mnist-basic

3.03
?0.15

3.69
?0.17

4.69
?0.19

3.94
?0.17

3.46
?0.16

3.11
?0.15

2.40
?0.04

mnist-rot*

11.11
?0.28

15.42
?0.32

18.11
?0.34

10.30
?0.27

10.30
?0.27

14.69
?0.31

8.66
?0.08

mnist-back-rand

14.58
?0.31

16.62
?0.33

20.04
?0.35

9.80
?0.26

11.28
?0.28

6.73
?0.22

9.38
?0.03

mnist-back-image

22.61
?0.37

24.01
?0.37

27.41
?0.39

16.15
?0.32

23.00
?0.37

16.31
?0.32

16.04
?0.04

mnist-rot-back-image*

55.18
?0.44
2.15
?0.13

56.41
?0.43
2.15
?0.13

62.16
?0.43
7.16
?0.23

47.39
?0.44
4.71
?0.19

51.93
?0.44
2.41
?0.13

52.21
?0.44
2.60
?0.14

43.86
?0.05
2.04
?0.04

24.04
?0.37

24.05
?0.37

33.20
?0.41

23.69
?0.37

24.05
?0.37

22.50
?0.37

22.10
?0.03

rectangles
rectangles-image

Table 1: MNIST variations and recangle results [14]; For datasets marked by (*) updated results are
shown [25].

Table 1 shows that a three layer GSN clearly outperforms all other models, except for the MNIST
random-background dataset. In particular, when comparing the GSN-3 to the radial basis function
support vector machine (SVMrbf), i.e. the second best model on MNIST basic, the GSN-3 achieved
an relative improvement of 20.79% on the test set. On the MNIST rotated dataset the GSN-3 was
able to beat the second best model i.e. DBN-1, by 15.92% on the test set. On the MNIST rotatedbackground there is an relative improvement of 7.25% on the test set between the second best model,
i.e. DBN-1, and the GSN-3. All results are statistically significant. Regarding the number of model
parameters, although we cannot directly compare the models in terms of network parameters, it is
worth to mention that a far smaller grid test was used to generate the results for all GSNs, cf. [14].
When comparing the classification error of the GSN-3 trained without noise, obtained in the previous
experiments (7) with Table 1, the GSN-3 achieved the test error of 2.72% on the MNIST variant
basic, outperforming all other models on this task. On the MNIST variant rotated, the GSN-3 also
outperformed the DBN-3, obtaining a test error of 11.2%. This indicates that not only the Gaussian
regularizer in the walkback training improves the generalization capabilities of the network, but also
the hybrid training criterion of the GSN.
Table 2 lists the results for the MNIST dataset without additional affine transformations applied to
the data i.e. permutation invariant digits. A three layer GSN achieved the state-of-the-art test error
of 0.80%.
Network

Result

Rectifier MLP + dropout [12]
DBM [26]
Maxout MLP + dropout [27]
MP-DBM [28]
Deep Convex Network [29]
Manifold Tangent Classifier [30]
DBM + dropout [12]
GSN-3

1.05%
0.95%
0.94%
0.91%
0.83%
0.81%
0.79%
0.80%

Table 2: MNIST results.

7

It might be worth noting that in addition to the noise process in walkback training, no other regularizers, such as `1, `2 norms and dropout variants [12], [18] were used in the GSNs. In general ? 800
training epochs with early-stopping are necessary for GSN training.
All simulations1 were executed on a GPU with the help of the mathematical expression compiler
Theano [31].

4

Conclusions and Future Work

We have extended GSNs for classification problems. In particular we defined an hybrid multiobjective training criterion for GSNs, dividing the cost function into a generative and discriminative
part. This renders the need for generative pre-training unnecessary. We analyzed the influence of
the objective?s trade-off parameter ? empirically, showing that by annealing ? we outperform a
static choice of ?. Furthermore, we discussed effects of noise injections and sampling steps during
walkback training. As a conservative starting point we restricted the model to use only rectifier
units. Neither additional regularization constraints, such as `1, `2 norms or dropout variants [12],
[18], nor pooling- [11, 32] or convolutional layers [11] were added. Nevertheless, the GSN was
able to outperform various baseline systems, in particular a deep belief network (DBN), a multi
layer perceptron (MLP), a support vector machine (SVM) and a stacked auto-associator (SSA), on
variants of the MNIST dataset. Furthermore, we also achieved state-of-the-art performance on the
original MNIST dataset without permutation invariant digits. The model not only converges faster
in terms of training iterations, but also show better generalization behavior in most cases. Our
approach opens a wide field of new applications for GSNs. In future research we explore adaptive
noise injection methods for GSNs and non-convex multi-objective optimization strategies.

References
[1] G. E. Hinton, S. Osindero, and Y. Teh, ?A fast learning algorithm for deep belief nets.? Neural computation, vol. 18, no. 7, pp. 1527?1554, 2006.
[2] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle, ?Greedy layer-wise training of deep networks,? in
Advances in Neural Information Processing Systems (NIPS), 2007, pp. 153?160.
[3] P. Vincent, H. Larochelle, Y. Bengio, and P. Manzagol, ?Extracting and composing robust features with
denoising autoencoders,? in International Conference on Machine Learning (ICML), 2008, pp. 1096?
1103.
[4] H. Lee, A. Battle, R. Raina, and A. Y. Ng, ?Efficient sparse coding algorithms,? in Advances in Neural
Information Processing Systems (NIPS), 2007, pp. 801?808.
[5] J. Ngiam, Z. Chen, S. A. Bhaskar, P. W. Koh, and A. Y. Ng, ?Sparse filtering,? in Advances in Neural
Information Processing Systems (NIPS), 2011, pp. 1125?1133.
[6] M. Ranzato, M. Poultney, S. Chopra, and Y. LeCun, ?Efficient learning of sparse representations with an
energy-based model,? in Advances in Neural Information Processing Systems (NIPS), 2006, pp. 1137?
1144.
[7] G. E. Dahl, M. Ranzato, A. Mohamed, and G. E. Hinton, ?Phone recognition with the mean-covariance
restricted Boltzmann machine,? in Advances in Neural Information Processing Systems (NIPS), 2010, pp.
469?477.
[8] L. Deng, M. L. Seltzer, D. Yu, A. Acero, A. Mohamed, and G. E. Hinton, ?Binary coding of speech
spectrograms using a deep auto-encoder.? in Interspeech, 2010, pp. 1692?1695.
[9] F. Seide, G. Li, and D. Yu, ?Conversational speech transcription using context-dependent deep neural
networks.? in Interspeech, 2011, pp. 437?440.
[10] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ?Imagenet classification with deep convolutional neural
networks,? in Advances in Neural Information Processing Systems (NIPS), 2012, pp. 1097?1105.
[11] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, ?Gradient-based learning applied to document recognition,? Proceedings of the IEEE, vol. 86, no. 11, 1998.
[12] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, ?Improving neural networks by preventing co-adaptation of feature detectors,? CoRR, vol. abs/1207.0580, 2012.
1

The code will be made publicly available for reproducing the results.

8

[13] Y. Lecun and C. Cortes, ?The MNIST database of handwritten digits,? 2014. [Online]. Available:
http://yann.lecun.com/exdb/mnist/
[14] H. Larochelle, D. Erhan, A. Courville, J. Bergstra, and Y. Bengio, ?An empirical evaluation of deep architectures on problems with many factors of variation,? in International Conference on Machine Learning
(ICML), 2007, pp. 473?480.
[15] H. Larochelle, M. Mandel, R. Pascanu, and Y. Bengio, ?Learning algorithms for the classification restricted Boltzmann machine,? Journal of Machine Learning Research (JMLR), vol. 13, pp. 643?669,
2012.
[16] Y. Bengio, L. Yao, G. Alain, and P. Vincent, ?Generalized denoising auto-encoders as generative models,?
in Advances in Neural Information Processing Systems (NIPS), 2013, pp. 899?907.
[17] Y. Bengio, E. Thibodeau-Laufer, and J. Yosinski, ?Deep generative stochastic networks trainable by backprop,? CoRR, vol. abs/1306.1091, 2013.
[18] L. Wan and M. Zeiler, ?Regularization of neural networks using dropconnect,? in International Conference on Machine Learning (ICML), 2013, pp. 109?111.
[19] G. Mesnil, Y. Dauphin, X. Glorot, S. Rifai, Y. Bengio, I. J. Goodfellow, E. Lavoie, X. Muller, G. Desjardins, D. Warde-Farley, P. Vincent, A. Courville, and J. Bergstra, ?Unsupervised and transfer learning
challenge: a deep learning approach,? in Unsupervised and Transfer Learning challenge and workshop
(JMLR W& CP), 2012, pp. 97?110.
[20] K. Abhishek and D. Hal, ?Learning task grouping and overlap in multi-task learning,? in International
Conference on Machine Learning (ICML), 2012.
[21] S. Ozair, L. Yao, and Y. Bengio, ?Multimodal transitions for generative stochastic networks.? CoRR, vol.
abs/1312.5578, 2013.
[22] P. Smolensky, Information processing in dynamical systems: Foundations of harmony theory.
Press, 1986, vol. 1, no. 1, pp. 194?281.

MIT

[23] X. Glorot, A. Bordes, and Y. Bengio, ?Deep sparse rectifier neural networks,? in International Conference
on Artificial Intelligence and Statisitics (AISTATS), 2011, pp. 315?323.
[24] G. E. Hinton, ?A practical guide to training restricted boltzmann machines,? in Neural Networks: Tricks
of the Trade (2nd ed.), ser. Lecture Notes in Computer Science. Springer, 2012, pp. 599?619.
[25] H. Larochelle, D. Erhan, A. Courville, J. Bergstra, and Y. Bengio, ?Online companion for
the paper an empirical evaluation of deep architectures on problems with many factors of
variation,? 2014. [Online]. Available: http://www.iro.umontreal.ca/?lisa/twiki/bin/view.cgi/Public/
DeepVsShallowComparisonICML2007
[26] R. Salakhutdinov and G. E. Hinton, ?Deep boltzmann machines,? in International Conference on Artificial
Intelligence and Statistics (AISTATS), 2009, pp. 448?455.
[27] I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and Y. Bengio, ?Maxout networks,? in International Conference on Machine Learning (ICML), 2013, pp. 1319?1327.
[28] I. J. Goodfellow, A. C. Courville, and Y. Bengio, ?Joint training deep boltzmann machines for classification,? CoRR, vol. abs/1301.3568, 2013.
[29] D. Yu and L. Deng, ?Deep convex net: A scalable architecture for speech pattern classification.? in Interspeech, 2011, pp. 2285?2288.
[30] S. Rifai, Y. Dauphin, P. Vincent, Y. Bengio, and X. Muller, ?The manifold tangent classifier,? in Advances
in Neural Information Processing Systems (NIPS), 2012, pp. 2294?2302.
[31] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins, J. Turian, D. Warde-Farley,
and Y. Bengio, ?Theano: a CPU and GPU math expression compiler,? in Python for Scientific Computing
Conference (SciPy), 2010.
[32] M. Zeiler and R. Fergus, ?Stochastic pooling for regularization of deep convolutional neural networks,?
CoRR, vol. abs/1301.3557, 2013.

9


----------------------------------------------------------------

title: 4443-algorithms-for-hyper-parameter-optimization.pdf

Algorithms for Hyper-Parameter Optimization

R?emi Bardenet
Laboratoire de Recherche en Informatique
Universit?e Paris-Sud
bardenet@lri.fr

James Bergstra
The Rowland Institute
Harvard University
bergstra@rowland.harvard.edu

Yoshua Bengio
D?ept. d?Informatique et Recherche Op?erationelle
Universit?e de Montr?eal
yoshua.bengio@umontreal.ca

Bal?azs K?egl
Linear Accelerator Laboratory
Universit?e Paris-Sud, CNRS
balazs.kegl@gmail.com

Abstract
Several recent advances to the state of the art in image classification benchmarks
have come from better configurations of existing techniques rather than novel approaches to feature learning. Traditionally, hyper-parameter optimization has been
the job of humans because they can be very efficient in regimes where only a few
trials are possible. Presently, computer clusters and GPU processors make it possible to run more trials and we show that algorithmic approaches can find better
results. We present hyper-parameter optimization results on tasks of training neural networks and deep belief networks (DBNs). We optimize hyper-parameters
using random search and two new greedy sequential methods based on the expected improvement criterion. Random search has been shown to be sufficiently
efficient for learning neural networks for several datasets, but we show it is unreliable for training DBNs. The sequential algorithms are applied to the most difficult
DBN learning problems from [1] and find significantly better results than the best
previously reported. This work contributes novel techniques for making response
surface models P (y|x) in which many elements of hyper-parameter assignment
(x) are known to be irrelevant given particular values of other elements.

1

Introduction

Models such as Deep Belief Networks (DBNs) [2], stacked denoising autoencoders [3], convolutional networks [4], as well as classifiers based on sophisticated feature extraction techniques
have from ten to perhaps fifty hyper-parameters, depending on how the experimenter chooses to
parametrize the model, and how many hyper-parameters the experimenter chooses to fix at a reasonable default. The difficulty of tuning these models makes published results difficult to reproduce
and extend, and makes even the original investigation of such methods more of an art than a science.
Recent results such as [5], [6], and [7] demonstrate that the challenge of hyper-parameter optimization in large and multilayer models is a direct impediment to scientific progress. These works
have advanced state of the art performance on image classification problems by more concerted
hyper-parameter optimization in simple algorithms, rather than by innovative modeling or machine
learning strategies. It would be wrong to conclude from a result such as [5] that feature learning
is useless. Instead, hyper-parameter optimization should be regarded as a formal outer loop in the
learning process. A learning algorithm, as a functional from data to classifier (taking classification
problems as an example), includes a budgeting choice of how many CPU cycles are to be spent
on hyper-parameter exploration, and how many CPU cycles are to be spent evaluating each hyperparameter choice (i.e. by tuning the regular parameters). The results of [5] and [7] suggest that
with current generation hardware such as large computer clusters and GPUs, the optimal alloca1

tion of CPU cycles includes more hyper-parameter exploration than has been typical in the machine
learning literature.
Hyper-parameter optimization is the problem of optimizing a loss function over a graph-structured
configuration space. In this work we restrict ourselves to tree-structured configuration spaces. Configuration spaces are tree-structured in the sense that some leaf variables (e.g. the number of hidden
units in the 2nd layer of a DBN) are only well-defined when node variables (e.g. a discrete choice of
how many layers to use) take particular values. Not only must a hyper-parameter optimization algorithm optimize over variables which are discrete, ordinal, and continuous, but it must simultaneously
choose which variables to optimize.
In this work we define a configuration space by a generative process for drawing valid samples.
Random search is the algorithm of drawing hyper-parameter assignments from that process and
evaluating them. Optimization algorithms work by identifying hyper-parameter assignments that
could have been drawn, and that appear promising on the basis of the loss function?s value at other
points. This paper makes two contributions: 1) Random search is competitive with the manual
optimization of DBNs in [1], and 2) Automatic sequential optimization outperforms both manual
and random search.
Section 2 covers sequential model-based optimization, and the expected improvement criterion. Section 3 introduces a Gaussian Process based hyper-parameter optimization algorithm. Section 4 introduces a second approach based on adaptive Parzen windows. Section 5 describes the problem of
DBN hyper-parameter optimization, and shows the efficiency of random search. Section 6 shows
the efficiency of sequential optimization on the two hardest datasets according to random search.
The paper concludes with discussion of results and concluding remarks in Section 7 and Section 8.

2

Sequential Model-based Global Optimization

Sequential Model-Based Global Optimization (SMBO) algorithms have been used in many applications where evaluation of the fitness function is expensive [8, 9]. In an application where the true
fitness function f : X ? R is costly to evaluate, model-based algorithms approximate f with a surrogate that is cheaper to evaluate. Typically the inner loop in an SMBO algorithm is the numerical
optimization of this surrogate, or some transformation of the surrogate. The point x? that maximizes
the surrogate (or its transformation) becomes the proposal for where the true function f should be
evaluated. This active-learning-like algorithm template is summarized in Figure 1. SMBO algorithms differ in what criterion they optimize to obtain x? given a model (or surrogate) of f , and in
they model f via observation history H.

SMBO f, M0 , T, S
1
2
3
4
5
6
7

H ? ?,
For t ? 1 to T ,
x? ? argminx S(x, Mt?1 ),
Evaluate f (x? ),
. Expensive step
H ? H ? (x? , f (x? )),
Fit a new model Mt to H.
return H

Figure 1: The pseudo-code of generic Sequential Model-Based Optimization.
The algorithms in this work optimize the criterion of Expected Improvement (EI) [10]. Other criteria have been suggested, such as Probability of Improvement and Expected Improvement [10],
minimizing the Conditional Entropy of the Minimizer [11], and the bandit-based criterion described
in [12]. We chose to use the EI criterion in our work because it is intuitive, and has been shown to
work well in a variety of settings. We leave the systematic exploration of improvement criteria for
future work. Expected improvement is the expectation under some model M of f : X ? RN that
f (x) will exceed (negatively) some threshold y ? :
Z ?
EIy? (x) :=
max(y ? ? y, 0)pM (y|x)dy.
(1)
??

2

The contribution of this work is two novel strategies for approximating f by modeling H: a hierarchical Gaussian Process and a tree-structured Parzen estimator. These are described in Section 3
and Section 4 respectively.

3

The Gaussian Process Approach (GP)

Gaussian Processes have long been recognized as a good method for modeling loss functions in
model-based optimization literature [13]. Gaussian Processes (GPs, [14]) are priors over functions
that are closed under sampling, which means that if the prior distribution of f is believed to be a GP
with mean 0 and kernel k, the conditional distribution of f knowing a sample H = (xi , f (xi ))ni=1
of its values is also a GP, whose mean and covariance function are analytically derivable. GPs with
generic mean functions can in principle be used, but it is simpler and sufficient for our purposes
to only consider zero mean processes. We do this by centering the function values in the considered data sets. Modelling e.g. linear trends in the GP mean leads to undesirable extrapolation in
unexplored regions during SMBO [15].
The above mentioned closedness property, along with the fact that GPs provide an assessment of
prediction uncertainty incorporating the effect of data scarcity, make the GP an elegant candidate
for both finding candidate x? (Figure 1, step 3) and fitting a model Mt (Figure 1, step 6). The runtime
of each iteration of the GP approach scales cubically in |H| and linearly in the number of variables
being optimized, however the expense of the function evaluations f (x? ) typically dominate even
this cubic cost.
3.1

Optimizing EI in the GP

We model f with a GP and set y ? to the best value found after observing H: y ? = min{f (xi ), 1 ?
i ? n}. The model pM in (1) is then the posterior GP knowing H. The EI function in (1) encapsulates a compromise between regions where the mean function is close to or better than y ? and
under-explored regions where the uncertainty is high.
EI functions are usually optimized with an exhaustive grid search over the input space, or a Latin
Hypercube search in higher dimensions. However, some information on the landscape of the EI criterion can be derived from simple computations [16]: 1) it is always non-negative and zero at training
points from D, 2) it inherits the smoothness of the kernel k, which is in practice often at least once
differentiable, and noticeably, 3) the EI criterion is likely to be highly multi-modal, especially as
the number of training points increases. The authors of [16] used the preceding remarks on the
landscape of EI to design an evolutionary algorithm with mixture search, specifically aimed at optimizing EI, that is shown to outperform exhaustive search for a given budget in EI evaluations. We
borrow here their approach and go one step further. We keep the Estimation of Distribution (EDA,
[17]) approach on the discrete part of our input space (categorical and discrete hyper-parameters),
where we sample candidate points according to binomial distributions, while we use the Covariance
Matrix Adaptation - Evolution Strategy (CMA-ES, [18]) for the remaining part of our input space
(continuous hyper-parameters). CMA-ES is a state-of-the-art gradient-free evolutionary algorithm
for optimization on continuous domains, which has been shown to outperform the Gaussian search
EDA. Notice that such a gradient-free approach allows non-differentiable kernels for the GP regression. We do not take on the use of mixtures in [16], but rather restart the local searches several times,
starting from promising places. The use of tesselations suggested by [16] is prohibitive here, as our
task often means working in more than 10 dimensions, thus we start each local search at the center
of mass of a simplex with vertices randomly picked among the training points.
Finally, we remark that all hyper-parameters are not relevant for each point. For example, a DBN
with only one hidden layer does not have parameters associated to a second or third layer. Thus it
is not enough to place one GP over the entire space of hyper-parameters. We chose to group the
hyper-parameters by common use in a tree-like fashion and place different independent GPs over
each group. As an example, for DBNs, this means placing one GP over common hyper-parameters,
including categorical parameters that indicate what are the conditional groups to consider, three
GPs on the parameters corresponding to each of the three layers, and a few 1-dimensional GPs over
individual conditional hyper-parameters, like ZCA energy (see Table 1 for DBN parameters).
3

4

Tree-structured Parzen Estimator Approach (TPE)

Anticipating that our hyper-parameter optimization tasks will mean high dimensions and small fitness evaluation budgets, we now turn to another modeling strategy and EI optimization scheme for
the SMBO algorithm. Whereas the Gaussian-process based approach modeled p(y|x) directly, this
strategy models p(x|y) and p(y).
Recall from the introduction that the configuration space X is described by a graph-structured generative process (e.g. first choose a number of DBN layers, then choose the parameters for each).
The tree-structured Parzen estimator (TPE) models p(x|y) by transforming that generative process,
replacing the distributions of the configuration prior with non-parametric densities. In the experimental section, we will see that the configuation space is described using uniform, log-uniform,
quantized log-uniform, and categorical variables. In these cases, the TPE algorithm makes the
following replacements: uniform ? truncated Gaussian mixture, log-uniform ? exponentiated
truncated Gaussian mixture, categorical ? re-weighted categorical. Using different observations
{x(1) , ..., x(k) } in the non-parametric densities, these substitutions represent a learning algorithm
that can produce a variety of densities over the configuration space X . The TPE defines p(x|y)
using two such densities:

`(x) if y < y ?
p(x|y) =
(2)
g(x) if y ? y ? ,
where `(x) is the density formed by using the observations {x(i) } such that corresponding loss
f (x(i) ) was less than y ? and g(x) is the density formed by using the remaining observations.
Whereas the GP-based approach favoured quite an aggressive y ? (typically less than the best observed loss), the TPE algorithm depends on a y ? that is larger than the best observed f (x) so that
some points can be used to form `(x). The TPE algorithm chooses y ? to be some quantile ? of the
observed y values, so that p(y < y ? ) = ?, but no specific model for p(y) is necessary. By maintaining sorted lists of observed variables in H, the runtime of each iteration of the TPE algorithm can
scale linearly in |H| and linearly in the number of variables (dimensions) being optimized.
4.1

Optimizing EI in the TPE algorithm

The parametrization of p(x, y) as p(y)p(x|y) in the TPE algorithm was chosen to facilitate the
optimization of EI.
Z y?
Z y?
p(x|y)p(y)
EIy? (x) =
(y ? ? y)p(y|x)dy =
(y ? ? y)
dy
(3)
p(x)
??
??
R
By construction, ? = p(y < y ? ) and p(x) = R p(x|y)p(y)dy = ?`(x) + (1 ? ?)g(x). Therefore
Z y?
Z y?
Z y?
?
?
?
(y ? y)p(x|y)p(y)dy = `(x)
(y ? y)p(y)dy = ?y `(x) ? `(x)
p(y)dy,
??

so that finally EIy? (x) =

??
R y?
?y ? `(x)?`(x) ??
p(y)dy
?`(x)+(1??)g(x)

??

?



?+

g(x)
`(x) (1

? ?)

?1

. This last expression

shows that to maximize improvement we would like points x with high probability under `(x)
and low probability under g(x). The tree-structured form of ` and g makes it easy to draw many
candidates according to ` and evaluate them according to g(x)/`(x). On each iteration, the algorithm
returns the candidate x? with the greatest EI.
4.2

Details of the Parzen Estimator

The models `(x) and g(x) are hierarchical processes involving discrete-valued and continuousvalued variables. The Adaptive Parzen Estimator yields a model over X by placing density in
the vicinity of K observations B = {x(1) , ..., x(K) } ? H. Each continuous hyper-parameter was
specified by a uniform prior over some interval (a, b), or a Gaussian, or a log-uniform distribution.
The TPE substitutes an equally-weighted mixture of that prior with Gaussians centered at each of
the x(i) ? B. The standard deviation of each Gaussian was set to the greater of the distances to the
left and right neighbor, but clipped to remain in a reasonable range. In the case of the uniform, the
points a and b were considered to be potential neighbors. For discrete variables, supposing the prior
was a vector of N probabilities pi , the posterior vector elements were proportional to N pi + Ci
where Ci counts the occurrences of choice i in B. The log-uniform hyper-parameters were treated
as uniforms in the log domain.
4

Table 1: Distribution over DBN hyper-parameters for random sampling. Options separated by ?or?
such as pre-processing (and including the random seed) are weighted equally. Symbol U means
uniform, N means Gaussian-distributed, and log U means uniformly distributed in the log-domain.
CD (also known as CD-1) stands for contrastive divergence, the algorithm used to initialize the layer
parameters of the DBN.
Whole model
Per-layer
Parameter
Prior
Parameter
Prior
pre-processing
raw or ZCA
n. hidden units
log U (128, 4096)
ZCA energy
U (.5, 1)
W init
U (?a, a) or N (0, a2 )
random seed
5 choices
a
algo A or B (see text)
classifier learn rate
log U (0.001, 10)
algo A coef
U (.2, 2)
classifier anneal start log U (100, 104 )
CD epochs
log U (1, 104 )
?7
?4
classifier `2 -penalty
0 or log U (10 , 10 )
CD learn rate
log U (10?4 , 1)
n. layers
1 to 3
CD anneal start log U (10, 104 )
batch size
20 or 100
CD sample data yes or no

5

Random Search for Hyper-Parameter Optimization in DBNs

One simple, but recent step toward formalizing hyper-parameter optimization is the use of random
search [5]. [19] showed that random search was much more efficient than grid search for optimizing
the parameters of one-layer neural network classifiers. In this section, we evaluate random search
for DBN optimization, compared with the sequential grid-assisted manual search carried out in [1].
We chose the prior listed in Table 1 to define the search space over DBN configurations. The details
of the datasets, the DBN model, and the greedy layer-wise training procedure based on CD are
provided in [1]. This prior corresponds to the search space of [1] except for the following differences:
(a) we allowed for ZCA pre-processing [20], (b) we allowed for each layer to have a different size,
(c) we allowed for each layer to have its own training parameters for CD, (d) we allowed for the
possibility of treating the continuous-valued data as either as Bernoulli means (more theoretically
correct) or Bernoulli samples (more typical) in the CD algorithm, and (e) we did not discretize the
possible values of real-valued hyper-parameters. These changes expand the hyper-parameter search
problem, while maintaining the original hyper-parameter search space as a subset of the expanded
search space.
The results of this preliminary random search are in Figure 2. Perhaps surprisingly, the result of
manual search can be reliably matched with 32 random trials for several datasets. The efficiency
of random search in this setting is explored further in [21]. Where random search results match
human performance, it is not clear from Figure 2 whether the reason is that it searched the original
space as efficiently, or that it searched a larger space where good performance is easier to find. But
the objection that random search is somehow cheating by searching a larger space is backward ?
the search space outlined in Table 1 is a natural description of the hyper-parameter optimization
problem, and the restrictions to that space by [1] were presumably made to simplify the search
problem and make it tractable for grid-search assisted manual search. Critically, both methods train
DBNs on the same datasets.
The results in Figure 2 indicate that hyper-parameter optimization is harder for some datasets. For
example, in the case of the ?MNIST rotated background images? dataset (MRBI), random sampling
appears to converge to a maximum relatively quickly (best models among experiments of 32 trials
show little variance in performance), but this plateau is lower than what was found by manual search.
In another dataset (convex), the random sampling procedure exceeds the performance of manual
search, but is slow to converge to any sort of plateau. There is considerable variance in generalization
when the best of 32 models is selected. This slow convergence indicates that better performance is
probably available, but we need to search the configuration space more efficiently to find it. The
remainder of this paper explores sequential optimization strategies for hyper-parameter optimization
for these two datasets: convex and MRBI.

6

Sequential Search for Hyper-Parameter Optimization in DBNs

We validated our GP approach of Section 3.1 by comparing with random sampling on the Boston
Housing dataset, a regression task with 506 points made of 13 scaled input variables and a scalar
5

mnist basic

1.0

mnist background images

0.9

mnist rotated background images

0.6

0.8

0.5

0.7

0.6

0.4

0.6

accuracy

accuracy

accuracy

0.8

0.5
0.4
0.3

0.1

0.1
0.0

1

2

4

8

16

32

64

0.0

128

1

experiment size (# trials)

2

4

8

16

32

64

0.0

128

0.7
0.6
0.5

1

2

4

8

16

32

64

128

0.4

experiment size (# trials)

32

64

128

0.65
0.60
0.55

0.55
0.50

16

0.70

0.8

accuracy

accuracy

accuracy

0.60

8

0.75

0.9

0.65

4

rectangles images

0.80

0.75
0.70

2

experiment size (# trials)

rectangles

1.0

0.80

0.45

1

experiment size (# trials)

convex

0.85

0.3
0.2

0.2

0.2

0.4

0.50
1

2

4

8

16

32

64

experiment size (# trials)

128

0.45

1

2

4

8

16

32

64

128

experiment size (# trials)

Figure 2: Deep Belief Network (DBN) performance according to random search. Random
search is used to explore up to 32 hyper-parameters (see Table 1). Results found using a
grid-search-assisted manual search over a similar domain with an average 41 trials are
given in green (1-layer DBN) and red (3-layer DBN). Each box-plot (for N = 1, 2, 4, ...)
shows the distribution of test set performance when the best model among N random trials
is selected. The datasets ?convex? and ?mnist rotated background images? are used for
more thorough hyper-parameter optimization.
regressed output. We trained a Multi-Layer Perceptron (MLP) with 10 hyper-parameters, including
learning rate, `1 and `2 penalties, size of hidden layer, number of iterations, whether a PCA preprocessing was to be applied, whose energy was the only conditional hyper-parameter [22]. Our
results are depicted in Figure 3. The first 30 iterations were made using random sampling, while
from the 30th on, we differentiated the random samples from the GP approach trained on the updated
history. The experiment was repeated 20 times. Although the number of points is particularly small
compared to the dimensionality, the surrogate modelling approach finds noticeably better points than
random, which supports the application of SMBO approaches to more ambitious tasks and datasets.
Applying the GP to the problem of optimizing DBN performance, we allowed 3 random restarts to
the CMA+ES algorithm per proposal x? , and up to 500 iterations of conjugate gradient method in
fitting the length scales of the GP. The squared exponential kernel [14] was used for every node.
The CMA-ES part of GPs dealt with boundaries using a penalty method, the binomial sampling part
dealt with it by nature. The GP algorithm was initialized with 30 randomly sampled points in H.
After 200 trials, the prediction of a point x? using this GP took around 150 seconds.
For the TPE-based algorithm, we chose ? = 0.15 and picked the best among 100 candidates drawn
from `(x) on each iteration as the proposal x? . After 200 trials, the prediction of a point x? using
this TPE algorithm took around 10 seconds. TPE was allowed to grow past the initial bounds used
with for random sampling in the course of optimization, whereas the GP and random search were
restricted to stay within the initial bounds throughout the course of optimization. The TPE algorithm
was also initialized with the same 30 randomly sampled points as were used to seed the GP.
6.1

Parallelizing Sequential Search

Both the GP and TPE approaches were actually run asynchronously in order to make use of multiple
compute nodes and to avoid wasting time waiting for trial evaluations to complete. For the GP approach, the so-called constant liar approach was used: each time a candidate point x? was proposed,
a fake fitness evaluation equal to the mean of the y?s within the training set D was assigned temporarily, until the evaluation completed and reported the actual loss f (x? ). For the TPE approach,
we simply ignored recently proposed points and relied on the stochasticity of draws from `(x) to
provide different candidates from one iteration to the next. The consequence of parallelization is
that each proposal x? is based on less feedback. This makes search less efficient, though faster in
terms of wall time.
6

26
Best value so far

24
22

TPE
GP
Manual
Random

20
18
16
14
0

10

20

30

40

convex
14.13 ?0.30 %
16.70 ? 0.32%
18.63 ? 0.34%
18.97 ? 0.34 %

MRBI
44.55 ?0.44%
47.08 ? 0.44%
47.39 ? 0.44%
50.52 ? 0.44%

50

Table 2: The test set classification error of
the best model found by each search algorithm on each problem. Each search algorithm was allowed up to 200 trials. The manual searches used 82 trials for convex and 27
trials MRBI.

Time

Figure 3: After time 30, GP optimizing
the MLP hyper-parameters on the Boston
Housing regression task. Best minimum
found so far every 5 iterations, against
time. Red = GP, Blue = Random. Shaded
areas = one-sigma error bars.

Runtime per trial was limited to 1 hour of GPU computation regardless of whether execution was on
a GTX 285, 470, 480, or 580. The difference in speed between the slowest and fastest machine was
roughly two-fold in theory, but the actual efficiency of computation depended also on the load of the
machine and the configuration of the problem (the relative speed of the different cards is different in
different hyper-parameter configurations). With the parallel evaluation of up to five proposals from
the GP and TPE algorithms, each experiment took about 24 hours of wall time using five GPUs.

7

Discussion

The trajectories (H) constructed by each algorithm up to 200 steps are illustrated in Figure 4, and
compared with random search and the manual search carried out in [1]. The generalization scores
of the best models found using these algorithms and others are listed in Table 2. On the convex
dataset (2-way classification), both algorithms converged to a validation score of 13% error. In
generalization, TPE?s best model had 14.1% error and GP?s best had 16.7%. TPE?s best was significantly better than both manual search (19%) and random search with 200 trials (17%). On the
MRBI dataset (10-way classification), random search was the worst performer (50% error), the GP
approach and manual search approximately tied (47% error), while the TPE algorithm found a new
best result (44% error). The models found by the TPE algorithm in particular are better than previously found ones on both datasets. The GP and TPE algorithms were slightly less efficient than
manual search: GP and EI identified performance on par with manual search within 80 trials, the
manual search of [1] used 82 trials for convex and 27 trials for MRBI.
There are several possible reasons for why the TPE approach outperformed the GP approach in
these two datasets. Perhaps the inverse factorization of p(x|y) is more accurate than the p(y|x) in
the Gaussian process. Perhaps, conversely, the exploration induced by the TPE?s lack of accuracy
turned out to be a good heuristic for search. Perhaps the hyper-parameters of the GP approach itself
were not set to correctly trade off exploitation and exploration in the DBN configuration space. More
empirical work is required to test these hypotheses. Critically though, all four SMBO runs matched
or exceeded both random search and a careful human-guided search, which are currently the state
of the art methods for hyper-parameter optimization.
The GP and TPE algorithms work well in both of these settings, but there are certainly settings
in which these algorithms, and in fact SMBO algorithm in general, would not be expected to do
well. Sequential optimization algorithms work by leveraging structure in observed (x, y) pairs. It is
possible for SMBO to be arbitrarily bad with a bad choice of p(y|x). It is also possible to be slower
than random sampling at finding a global optimum with a apparently good p(y|x), if it extracts
structure in H that leads only to a local optimum.

8

Conclusion

This paper has introduced two sequential hyper-parameter optimization algorithms, and shown them
to meet or exceed human performance and the performance of a brute-force random search in two
difficult hyper-parameter optimization tasks involving DBNs. We have relaxed standard constraints
(e.g. equal layer sizes at all layers) on the search space, and fall back on a more natural hyperparameter space of 32 variables (including both discrete and continuous variables) in which many
7

Dataset: convex

Dataset: mnist rotated background images

0.50

manual
99.5?th q.
GP
TPE

error (fraction incorrect)

error (fraction incorrect)

0.45

manual
99.5?th q.
GP
TPE

0.9

0.40

0.35

0.30

0.25

0.20

0.8

0.7

0.6

0.5

0.15
0

50

100

time (trials)

150

200

0

50

100

time (trials)

150

200

Figure 4: Efficiency of Gaussian Process-based (GP) and graphical model-based (TPE) sequential optimization algorithms on the task of optimizing the validation set performance
of a DBN of up to three layers on the convex task (left) and the MRBI task (right). The
dots are the elements of the trajectory H produced by each SMBO algorithm. The solid
coloured lines are the validation set accuracy of the best trial found before each point in
time. Both the TPE and GP algorithms make significant advances from their random initial conditions, and substantially outperform the manual and random search methods. A
95% confidence interval about the best validation means on the convex task extends 0.018
above and below each point, and on the MRBI task extends 0.021 above and below each
point. The solid black line is the test set accuracy obtained by domain experts using a
combination of grid search and manual search [1]. The dashed line is the 99.5% quantile of validation performance found among trials sampled from our prior distribution (see
Table 1), estimated from 457 and 361 random trials on the two datasets respectively.
variables are sometimes irrelevant, depending on the value of other parameters (e.g. the number of
layers). In this 32-dimensional search problem, the TPE algorithm presented here has uncovered new
best results on both of these datasets that are significantly better than what DBNs were previously
believed to achieve. Moreover, the GP and TPE algorithms are practical: the optimization for each
dataset was done in just 24 hours using five GPU processors. Although our results are only for
DBNs, our methods are quite general, and extend naturally to any hyper-parameter optimization
problem in which the hyper-parameters are drawn from a measurable set.
We hope that our work may spur researchers in the machine learning community to treat the hyperparameter optimization strategy as an interesting and important component of all learning algorithms. The question of ?How well does a DBN do on the convex task?? is not a fully specified,
empirically answerable question ? different approaches to hyper-parameter optimization will give
different answers. Algorithmic approaches to hyper-parameter optimization make machine learning
results easier to disseminate, reproduce, and transfer to other domains. The specific algorithms we
have presented here are also capable, at least in some cases, of finding better results than were previously known. Finally, powerful hyper-parameter optimization algorithms broaden the horizon of
models that can realistically be studied; researchers need not restrict themselves to systems of a few
variables that can readily be tuned by hand.
The TPE algorithm presented in this work, as well as parallel evaluation infrastructure, is available
as BSD-licensed free open-source software, which has been designed not only to reproduce the
results in this work, but also to facilitate the application of these and similar algorithms to other
hyper-parameter optimization problems.1
Acknowledgements
This work was supported by the National Science and Engineering Research Council of Canada,
Compute Canada, and by the ANR-2010-COSI-002 grant of the French National Research Agency.
GPU implementations of the DBN model were provided by Theano [23].

1

?Hyperopt? software package: https://github.com/jaberg/hyperopt

8

References
[1] H. Larochelle, D. Erhan, A. Courville, J. Bergstra, and Y. Bengio. An empirical evaluation of deep
architectures on problems with many factors of variation. In ICML 2007, pages 473?480, 2007.
[2] G. E. Hinton, S. Osindero, and Y. Teh. A fast learning algorithm for deep belief nets. Neural Computation,
18:1527?1554, 2006.
[3] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P. A. Manzagol. Stacked denoising autoencoders:
Learning useful representations in a deep network with a local denoising criterion. Machine Learning
Research, 11:3371?3408, 2010.
[4] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.
Proceedings of the IEEE, 86(11):2278?2324, November 1998.
[5] Nicolas Pinto, David Doukhan, James J. DiCarlo, and David D. Cox. A high-throughput screening approach to discovering good forms of biologically inspired visual representation. PLoS Comput Biol,
5(11):e1000579, 11 2009.
[6] A. Coates, H. Lee, and A. Ng. An analysis of single-layer networks in unsupervised feature learning.
NIPS Deep Learning and Unsupervised Feature Learning Workshop, 2010.
[7] A. Coates and A. Y. Ng. The importance of encoding versus training with sparse coding and vector
quantization. In Proceedings of the Twenty-eighth International Conference on Machine Learning (ICML11), 2010.
[8] F. Hutter. Automated Configuration of Algorithms for Solving Hard Computational Problems. PhD thesis,
University of British Columbia, 2009.
[9] F. Hutter, H. Hoos, and K. Leyton-Brown. Sequential model-based optimization for general algorithm
configuration. In LION-5, 2011. Extended version as UBC Tech report TR-2010-10.
[10] D.R. Jones. A taxonomy of global optimization methods based on response surfaces. Journal of Global
Optimization, 21:345?383, 2001.
[11] J. Villemonteix, E. Vazquez, and E. Walter. An informational approach to the global optimization of
expensive-to-evaluate functions. Journal of Global Optimization, 2006.
[12] N. Srinivas, A. Krause, S. Kakade, and M. Seeger. Gaussian process optimization in the bandit setting:
No regret and experimental design. In ICML, 2010.
[13] J. Mockus, V. Tiesis, and A. Zilinskas. The application of Bayesian methods for seeking the extremum.
In L.C.W. Dixon and G.P. Szego, editors, Towards Global Optimization, volume 2, pages 117?129. North
Holland, New York, 1978.
[14] C.E. Rasmussen and C. Williams. Gaussian Processes for Machine Learning.
[15] D. Ginsbourger, D. Dupuy, A. Badea, L. Carraro, and O. Roustant. A note on the choice and the estimation
of kriging models for the analysis of deterministic computer experiments. 25:115?131, 2009.
[16] R. Bardenet and B. K?egl. Surrogating the surrogate: accelerating Gaussian Process optimization with
mixtures. In ICML, 2010.
[17] P. Larra?naga and J. Lozano, editors. Estimation of Distribution Algorithms: A New Tool for Evolutionary
Computation. Springer, 2001.
[18] N. Hansen. The CMA evolution strategy: a comparing review. In J.A. Lozano, P. Larranaga, I. Inza, and
E. Bengoetxea, editors, Towards a new evolutionary computation. Advances on estimation of distribution
algorithms, pages 75?102. Springer, 2006.
[19] J. Bergstra and Y. Bengio. Random search for hyper-parameter optimization. The Learning Workshop
(Snowbird), 2011.
[20] A. Hyv?arinen and E. Oja. Independent component analysis: Algorithms and applications. Neural Networks, 13(4?5):411?430, 2000.
[21] J. Bergstra and Y. Bengio. Random search for hyper-parameter optimization. JMLR, 2012. Accepted.
[22] C. Bishop. Neural networks for pattern recognition. 1995.
[23] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins, J. Turian, and Y. Bengio.
Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientific Computing Conference (SciPy), June 2010.

9


----------------------------------------------------------------

