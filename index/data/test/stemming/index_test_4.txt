query sentence: state-of-art algorithms in theano
---------------------------------------------------------------------
title: 5647-binaryconnect-training-deep-neural-networks-with-binary-weights-during-propagations.pdf

binaryconnect train deep neural network binari weight propag matthieu courbariaux ecol polytechniqu de montr eal matthieu.courbariaux polymtl.ca yoshua bengio universit e de montr eal cifar senior fellow yoshua.bengio gmail.com jean-pierr david ecol polytechniqu de montr eal jean-pierre.david polymtl.ca abstract deep neural network dnn achiev state-of-the-art result wide rang task best result obtain larg train set larg model past gpus enabl breakthrough greater comput speed futur faster comput train test time like crucial progress consum applic low-pow devic result much interest research develop dedic hardwar deep learn binari weight weight constrain two possibl valu would bring great benefit special dl hardwar replac mani multiply-accumul oper simpl accumul multipli space powerhungri compon digit implement neural network introduc binaryconnect method consist train dnn binari weight forward backward propag retain precis store weight gradient accumul like dropout scheme show binaryconnect act regular obtain near state-of-the-art result binaryconnect permutation-invari mnist cifar-10 svhn introduct deep neural network dnn substanti push state-of-the-art wide rang task especi speech recognit comput vision notabl object recognit imag recent deep learn make import stride natur languag process especi statist machin translat interest one key factor enabl major progress advent graphic process unit gpus speed-up order start similar improv distribut train inde abil train larger model data enabl kind breakthrough observ last year today research develop design new deep learn algorithm applic often find limit comput capabl along drive put deep learn system low-pow devic unlik gpus great increas interest research develop special hardwar deep network comput perform train applic deep network regard multipl real-valu weight real-valu activ recognit forward propag phase back-propag algorithm gradient backward propag phase back-propag algorithm paper propos approach call binaryconnect elimin need multipl forc weight use forward backward propag binari constrain two valu necessarili show state-of-the-art result achiev binaryconnect permutation-invari mnist cifar-10 svhn make workabl two ingredi suffici precis necessari accumul averag larg number stochast gradient noisi weight view discret small number valu form nois especi make discret stochast quit compat stochast gradient descent main type optim algorithm deep learn sgd explor space paramet make small noisi step nois averag stochast gradient contribut accumul weight therefor import keep suffici resolut accumul first sight suggest high precis absolut requir show random stochast round use provid unbias discret shown sgd requir weight precis least bit success train dnns bit dynam fixed-point comput besid estim precis brain synaps vari bit noisi weight actual provid form regular help general better previous shown variat weight nois dropout dropconnect add nois activ weight instanc dropconnect closest binaryconnect effici regular random substitut half weight zero propag previous work show expect valu weight need high precis nois actual benefici main contribut articl follow introduc binaryconnect method consist train dnn binari weight forward backward propag section show binaryconnect regular obtain near state-of-the-art result permutation-invari mnist cifar-10 svhn section make code binaryconnect avail binaryconnect section give detail view binaryconnect consid two valu choos discret train perform infer appli dnn main consist convolut matrix multipl key arithmet oper dl thus multiply-accumul oper artifici neuron basic multiplyaccumul comput weight sum input binaryconnect constraint weight either propag result mani multiply-accumul oper replac simpl addit subtract huge gain fixed-point adder much less expens term area energi fixed-point multiply-accumul determinist vs stochast binar binar oper transform real-valu weight two possibl valu straightforward binar oper would base sign function wb otherwis https //github.com/matthieucourbariaux/binaryconnect wb binar weight real-valu weight although determinist oper averag discret mani input weight hidden unit could compens loss inform altern allow finer correct averag process take place binar stochast probabl wb probabl hard sigmoid function clip use hard sigmoid rather soft version far less comput expens softwar special hardwar implement yield excel result experi similar hard tanh non-linear introduc also piece-wis linear correspond bound form rectifi propag vs updat let us consid differ step back-propag sgd udpat whether make sens discret weight step given dnn input comput unit activ layer layer lead top layer output dnn given input step refer forward propag given dnn target comput train object gradient layer activ start top layer go layer layer first hidden layer step refer backward propag backward phase backpropag comput gradient layer paramet updat paramet use comput gradient previous valu step refer paramet updat algorithm sgd train binaryconnect cost function minibatch function binar w clip w specifi binar clip weight number layer requir minibatch input target previous paramet weight bias learn rate ensur updat paramet wt bt forward propag wb binar wt comput ak know wb backward propag initi output layer activ gradient comput know ak wb paramet updat comput db c know wt clip wt bt key point understand binaryconnect binar weight forward backward propag step paramet updat step illustr algorithm keep good precis weight updat necessari sgd work paramet chang tini virtu obtain gradient descent sgd perform larg number almost infinitesim chang direct improv train object plus nois one way pictur hypothes what matter end train sign weight order figur perform lot small chang continuous-valu quantiti end consid sign sign gt gt noisi estim c f valu object function input target exampl previous weight final discret valu weight anoth way conceiv discret form corrupt henc regular empir result confirm this hypothesi addit make discret error differ weight approxim cancel keep lot precis random discret appropri propos form random discret preserv expect valu discret weight henc train time binaryconnect random pick one two valu weight minibatch forward backward propag phase backprop howev sgd updat accumul real-valu variabl store paramet interest analog understand binaryconnect dropconnect algorithm like binaryconnect dropconnect inject nois weight propag wherea dropconnect nois ad gaussian nois binaryconnect nois binari sampl process case corrupt valu expect valu clean origin valu clip sinc binar oper influenc variat real-valu weight magnitud beyond binari valu sinc common practic bound weight usual weight vector order regular chosen clip real-valu weight within interv right weight updat per algorithm real-valu weight would otherwis grow larg without impact binari weight trick optim learn rate scale learn rate scale sgd nesterov momentum adam tabl test error rate small cnn train cifar-10 depend optim method whether learn rate scale weight initi coeffici use batch normal experi acceler train reduc intern covari shift also reduc overal impact weight scale moreov use adam learn rule cnn experi last least scale weight learn rate respect weight initi coeffici optim adam squar coeffici optim sgd nesterov momentum tabl illustr effect trick test-tim infer introduc differ way train dnn on-the-fli weight binar what reason way use train network perform test-tim infer new exampl consid three reason altern use result binari weight wb this make most sens determinist form binaryconnect use real-valu weight binar help achiev faster train faster test-tim perform stochast case mani differ network sampl sampl wb weight accord ensembl output these network obtain averag output individu network use first method determinist form binaryconnect stochast form binaryconnect focus train advantag use second method experi test-tim infer use real-valu weight this follow practic dropout method where test-tim nois remov method mnist cifar-10 svhn regular binaryconnect binaryconnect stoch dropout maxout network deep l2-svm network network dropconnect deeply-supervis net tabl test error rate dnns train mnist convolut unsupervis pretrain cifar-10 data augment svhn depend method see spite use singl bit per weight propag perform wors ordinari no regular dnns actual better especi stochast version suggest binaryconnect act regular figur featur first layer mlp train mnist depend regular left right no regular determinist binaryconnect stochast binaryconnect dropout benchmark result this section show binaryconnect act regular obtain near state-of-the-art result binaryconnect permutation-invari mnist cifar-10 svhn permutation-invari mnist mnist benchmark imag classif dataset it consist train set test set gray-scal imag repres digit rang permutationinvari mean model must unawar imag structur data word cnns forbidden besid use data-augment preprocess unsupervis pretrain mlp we train mnist consist hidden layer rectifi linear unit relu l2-svm output layer l2-svm shown perform better softmax sever classif benchmark squar hing loss minim sgd without momentum we use exponenti decay learn rate we use batch figur histogram weight first layer mlp train mnist depend regular in case it seem weight tri becom determinist reduc train error it also seem weight determinist binaryconnect stuck around hesit figur train curv cnn cifar-10 depend regular dot line repres train cost squar hing loss continu line correspond valid error rate version binaryconnect signific augment train cost slow train lower valid error rate what we would expect dropout scheme normal minibatch size speed train as typic done we use last sampl train set as valid set earli stop model select we report test error rate associ best valid error rate epoch we retrain valid set we repeat experi time differ initi result in tabl suggest stochast version binaryconnect consid regular although slight less power one dropout in this context cifar-10 cifar-10 benchmark imag classif dataset it consist in train set test set color imag repres airplan automobil bird cat deer dog frog hors ship truck we preprocess data use global contrast normal zca whiten we use data-augment realli game changer this dataset architectur cnn where c3 relu convolut layer max-pool layer fulli connect layer svm l2-svm output layer this architectur great inspir vgg squar hing loss minim adam we use an exponenti decay learn rate we use batch normal minibatch size speed up train we use last sampl train set as valid set we report test error rate associ best valid error rate train epoch we retrain valid set result in tabl figur svhn svhn benchmark imag classif dataset it consist in train set test set color imag repres digit rang we follow procedur we use cifar-10 notabl except we use half number hidden unit we train epoch instead svhn quit big dataset result in tabl relat work train dnns binari weight subject recent work even though we share object approach quit differ train dnn backpropag variant call expect backpropag ebp base expect propag variat bay method use infer in probabilist graphic model let us compar method it optim weight posterior distribut binari in this regard method quit similar as we keep real-valu version weight it binar both neuron output weight hardwar friend just binar weight it yield good classif accuraci fulli connect network mnist yet convnet retrain neural network ternari weight forward backward propag train neural network high-precis after train they ternar weight three possibl valu adjust minim output error eventu they retrain ternari weight propag high-precis weight updat comparison we train way binari weight propag train procedur could implement effici special hardwar avoid forward backward propag multipl amount multipl algorithm conclus futur work we introduc novel binar scheme weight forward backward propag call binaryconnect we shown it possibl train dnns binaryconnect permut invari mnist cifar-10 svhn dataset achiev near state-of-the-art result impact method special hardwar implement deep network could major remov need multipl thus potenti allow speed-up factor train time with the determinist version binaryconnect the impact test time could even more import get rid the multipl altogeth reduc a factor least bit single-float precis singl bit precis the memori requir deep network an impact the memori comput bandwidth the size the model run futur work extend result model dataset explor get rid the multipl altogeth train by remov need the weight updat comput acknowledg we thank the review mani construct comment we also thank roland memisev for help discuss we thank the develop theano a python librari allow us easili develop a fast optim code for gpu we also thank the develop pylearn2 lasagn two deep learn librari built the top theano we also grate for fund from nserc the canada research chair comput canada and cifar
----------------------------------------------------------------

title: 6573-binarized-neural-networks.pdf

binar neural network itay hubara1 itayh technion.ac.il matthieu courbariaux2 matthieu.courbariaux gmail.com ran el-yaniv1 rani cs.technion.ac.il daniel soudry3 daniel.soudri gmail.com yoshua bengio2,4 yoshua.umontr gmail.com technion israel institut technolog columbia univers indic equal contribut universit de montr al cifar senior fellow abstract introduc method train binar neural network bnns neural network binari weight activ run-tim train-tim binari weight activ use comput paramet gradient forward pass bnns drastic reduc memori size access replac arithmet oper bit-wis oper expect substanti improv power-effici valid effect bnns conduct two set experi torch7 theano framework bnns achiev near state-of-the-art result mnist cifar-10 svhn dataset also report preliminari result challeng imagenet dataset last least wrote binari matrix multipl gpu kernel possibl run mnist bnn time faster unoptim gpu kernel without suffer loss classif accuraci code train run bnns avail on-lin introduct deep neural network dnns substanti push artifici intellig limit wide rang task lecun today dnns almost exclus train one mani fast power-hungri graphic process unit gpus coat result often challeng run dnns target low-pow devic substanti research effort invest speed dnns run-tim general-purpos gong han special comput hardwar chen esser paper make follow contribut introduc method train binarized-neural-network bnns neural network binari weight activ run-tim comput paramet gradient train-tim section conduct two set experi implement differ framework name torch7 theano show possibl train bnns mnist cifar-10 svhn achiev near state-of-the-art result section moreov report preliminari result challeng imagenet dataset show forward pass run-tim train-tim bnns drastic reduc memori consumpt size number access replac arithmet oper bit-wis oper potenti lead substanti increas power-effici section confer neural inform process system nip barcelona spain moreov binar cnn lead binari convolut kernel repetit argu dedic hardwar could reduc time complex last least program binari matrix multipl gpu kernel possibl run mnist bnn time faster unoptim gpu kernel without suffer loss classif accuraci section code train run bnns avail on-lin theano1 torch framework2 binar neural network section detail binar function show use comput paramet gradient backpropag determinist vs stochast binar train bnn constrain weight activ either two valu advantag hardwar perspect explain section order transform real-valu variabl two valu use two differ binar function courbariaux first binar function determinist xb sign x otherwis xb binar variabl weight activ real-valu variabl straightforward implement work quit well practic second binar function stochast probabl probabl hard sigmoid function stochast binar appeal sign function harder implement requir hardwar generat random bit quantiz result most use determinist binar function sign function except activ train-tim experi clip gradient comput accumul although bnn train method use binari weight activ comput paramet gradient real-valu gradient weight accumul real-valu variabl per algorithm real-valu weight like requir stochas gradient descent sgd work sgd explor space paramet small noisi step nois averag stochast gradient contribut accumul weight therefor import maintain suffici resolut accumul first glanc suggest high precis absolut requir moreov ad nois weight activ comput paramet gradient provid form regular help general better previous shown variat weight nois grave dropout srivastava dropconnect wan method train bnns seen variant dropout instead random set half activ zero comput paramet gradient binar activ weight propag gradient discret deriv sign function zero almost everywher make appar incompat back-propag sinc exact gradient cost respect quantiti discret pre-activ weight would https //github.com/matthieucourbariaux/binarynet https //github.com/itayhubara/binarynet zero note remain true even stochast quantize use bengio studi question estim propag gradient stochast discret neuron he found experi fastest train obtain use straight-through estim previous introduc hinton lectur hinton follow similar approach use version straight-through estim take account satur effect use determinist rather stochast sampl bit consid sign function quantize sign r assum estim gq gradient obtain straight-through estim need algorithm train bnn cost function algorithm shift base adamax learn minibatch learn rate decay factor rule kingma ba gt2 indic number layer indic element-wis mul element-wis squar gt gt stand tiplic function binar specifi left right bit-shift good default stochast determinist binar activa set tion weight clip specifi clip oper vector weight batchnorm specifi batch-norm element-wis denot activ use either batch normal ioff power szegedi shift-bas variant describ algorithm backbatchnorm specifi back requir previous paramet gradient gt learn rate propag normal updat specifi ensur updat paramet updat paramet gradient bias 1st 2nd moment estim known use either adam kingma ba shift-bas adamax describ algorithm mt gt vt gt requir minibatch input target updat paramet previous weight previous batchnorm paramet weight initi coeffici glorot bengio previous learn rate ensur updat weight updat batchnorm paramet updat learn rate algorithm shift base batch normaliz comput gradient ing transform appli activ forward propag mini-batch approxim power-offor is3 ap sign x 2round log2|x wkb binar wk sk abk wkb stand left right binari shift ak batchnorm sk requir valu mini-batch abk binar ak paramet learn backward propag ensur bn xi pleas note gradient binari mini-batch pm mean comput gal al know al center input gak gabk c xi gsk g k backbatchnorm gak sk approxim varianc pm gabk gsk wkb gwkb gs k abk ap normal accumul gradient x c xi ap scale shift updat k g k ap x wkt+1 clip updat wk gwkb then straight-through estim simpli gr gq note preserv gradient inform cancel gradient larg cancel gradient larg signific worsen perform use straight-through estim illustr algorithm deriv also seen propag gradient through hard tanh follow piece-wis linear activ function htanh x clip x hidden unit use sign function nonalgorithm run bnn layer linear obtain binari activ requir vector 8-bit input a0 binari weight combin two ingredi weight batchnorm paramet constrain real-valu weight ensur mlp output al project wr first layer weight updat bring wr outsid a1 clip weight train per algorithm real-valu weight a1 a1 xnordotproduct an0 w1b would otherwis grow larg without ab1 sign batchnorm a1 impact binari weight remain hidden layer use weight wr quantiz use ak xnordotproduct abk wkb wb sign wr abk sign batchnorm ak consist gradient cancel output layer wr accord al xnordotproduct abl wlb al batchnorm al shift-bas batch normal batch normal ioff szegedi acceler train also seem reduc overal impact weight scale normal nois may also help regular model howev train-tim bn requir mani multipl calcul standard deviat divid name divid run varianc weight mean train set activ varianc although number scale calcul number neuron case convnet number quit larg exampl cifar-10 dataset use architectur first convolut layer consist filter mask convert imag size size two order magnitud larger number weight achiev result bn would obtain use shift-bas batch normal sbn techniqu detail algorithm sbn approxim bn almost without multipl experi conduct observ accuraci loss use shift base bn algorithm instead vanilla bn algorithm shift base adamax adam learn rule kingma ba also seem reduc impact weight scale sinc adam requir mani multipl suggest use instead shift-bas adamax detail algorithm experi conduct observ accuraci loss use shift-bas adamax algorithm instead vanilla adam algorithm first layer bnn binar valu weight activ use calcul output one layer input next layer input binari except first layer howev believ major issu first comput vision input represent typic far fewer channel red green blue intern represent result first layer convnet often smallest convolut layer term paramet comput szegedi second relat easi handl continuous-valu input fix point number bit precis exampl common case 8-bit fix point input wb wb vector 8-bit input signific bit first input wb vector 1-bit weight result weight sum trick use algorithm benchmark result conduct two set experi base differ framework name torch7 theano implement detail report appendix code framework avail onlin result report tabl hardwar implement ap2 simpl extract index signific bit number binari represent tabl classif test error rate dnns train mnist fulli connect architectur cifar-10 svhn convnet unsupervis pre-train data augment use data set mnist svhn binar activations+weight train test bnn torch7 bnn theano committe machin array baldassi binar weight train test binaryconnect courbariaux binar activations+weight test ebp cheng bitwis dnns kim smaragdi ternari weight binari activ test hwang sung no binar standard result no regular gate pool lee cifar-10 preliminari result imagenet figur train curv differ method test strength method appli cifar-10 dataset dot line repres trainit challeng imagenet classifica ing cost squar hing loss continu line tion task consider research correspond valid error rate although concern compress imagenet ar bnns slower train near accur chitectur preserv high accuraci float dnns perform han previous approach tri includ prune near zero weight use matrix factor techniqu quantiz weight appli huffman code among other best knowledg far no report success quantiz network activ moreov recent work han show accuraci signific deterior when tri quantiz convolut layer weight bit fc layer robust quantize oper quit well bit present work attempt tackl difficult task binar weight activ employ well known alexnet googlenet architectur appli techniqu achiev top-1 top-5 accuraci use alexnet top-1 top-5 accuraci use googlenet this perform leav room improv relat full precis net far better previous attempt compress imagenet architectur use less bit precis for weight moreov this advantag achiev while also binar neuron activ detail descript result well full implement detail experi report supplementari materi appendix b latest work hubara relax binari constrain allow 1-bit per weight activ result qnns achiev predict accuraci compar counterpart for exampl quantiz version alexnet 1-bit weight 2-bit activ achiev top-1 accuraci googlenet 4-bit weigh activ achiv moreov quantiz paramet gradient 6-bit well enabl gradient comput use bit-wis oper full detail found hubara tabl energi consumpt multiplyaccumul pico-joul horowitz oper mul add 8bit integ 32bit integ 16bit float point 32tbit float point tabl energi consumpt memori access pico-joul horowitz memori size memori access 8k 1m dram high power effici forward pass comput hardwar general-purpos special compos memori arithmet oper control logic dure forward pass run-tim train-tim bnns drastic reduc memori size access replac arithmet oper bit-wis oper might lead great increas power-effici moreov binar cnn lead binari convolut kernel repetit argu dedic hardwar could reduc time complex memori size access improv comput perform alway remain challeng last decad power main constraint perform horowitz this much research effort devot reduc energi consumpt neural network horowitz provid rough number for energi consum comput given number for technolog summar tabl import we see memori access typic consum energi arithmet oper memori access cost augment memori size comparison dnns bnns requir time smaller memori size time fewer memori access this expect reduc energi consumpt drastic time xnor-count appli dnn main consist convolut matrix multipl key arithmet oper deep learn thus multiply-accumul oper artifici neuron basic multiply-accumul comput weight sum input in bnns activ weight constrain either as result float point multiply-accumul replac 1-bit xnor-count oper this could big impact dedic deep learn hardwar for instanc float point multipli cost xilinx fpga slice govindu beauchamp wherea 1-bit xnor gate cost singl slice exploit filter repetit when use convnet architectur binari weight number uniqu filter bound filter size for exampl in implement we use filter size maximum number uniqu 2d filter sinc we binari filter mani 2d filter size repeat use dedic hardware/softwar we appli uniqu 2d filter featur map sum result receiv 3d filter convolut result for exampl in our convnet architectur train cifar-10 benchmark uniqu filter per layer averag henc we reduc number xnor-popcount oper seven time faster gpu run-tim possibl speed gpu implement bnns use method sometim call simd singl instruct multipl data within regist swar basic idea swar concaten group binari variabl regist thus obtain 32-time speed-up bitwis oper xnor use swar possibl evalu connect instruct a1 popcount xnor a32b w1 a1 result weight sum concaten input weight those instruct accumul popcount xnor take clock cycl recent nvidia gpus becom fuse instruct would take singl clock cycl consequ we obtain theoret nvidia gpu speed-up factor in practic this speed-up quit easi obtain as memori bandwidth comput ratio also increas time in order valid those theoret result we program two gpu kernel figur first three column repres time take perform first kernel baselin unoptim nari matrix multipl nvidia matrix multipl kernel gpu depend kernel use we second kernel xnor near ident see our xnor kernel time faster baselin kernel except it use our baselin kernel time faster cubla next three column repres swar method as in equat time it take run mlp section the the two gpu kernel return ident output full mnist test set as mnist imag when input constrain binari the first layer comput alway otherwis the xnor kernel perform the baselin kernel the last three time faster the baselin kernel column show the mlp accuraci time faster cubla as shown in figur depend on kernel use last not least the mlp section run time faster the xnor kernel the baselin kernel without suffer loss in classif accuraci figur discuss relat work until recent the use extrem lowprecis network binari in the extrem case believ to high destruct to the network perform courbariaux soudri cheng prove the contrari show good perform could achiev even all neuron weight binar to this done use expect backpropag variat bayesian approach infer network with binari weight neuron updat the posterior distribut over the weight these distribut updat by differenti paramet mean valu via the back propag algorithm esser implement fulli binari network at run time use similar approach to ebp show signific improv in energi effici the drawback ebp the binar paramet use dure infer the probabilist idea behind ebp extend in the binaryconnect algorithm courbariaux in binaryconnect the real-valu version the weight save use as a key
----------------------------------------------------------------

title: 6310-phased-lstm-accelerating-recurrent-network-training-for-long-or-event-based-sequences.pdf

phase lstm acceler recurr network train long event-bas sequenc daniel neil michael pfeiffer shih-chii liu institut neuroinformat univers zurich eth zurich zurich switzerland dneil pfeiffer shih ini.uzh.ch abstract recurr neural network rnns becom state-of-the-art choic extract pattern tempor sequenc howev current rnn model ill-suit process irregular sampl data trigger event generat continu time sensor neuron data occur exampl input come novel event-driven artifici sensor generat spars asynchron stream event multipl convent sensor differ updat interv work introduc phase lstm model extend lstm unit ad new time gate gate control parametr oscil frequenc rang produc updat memori cell small percentag cycl even spars updat impos oscil phase lstm network achiev faster converg regular lstms task requir learn long sequenc model natur integr input sensor arbitrari sampl rate therebi open new area investig process asynchron sensori event carri time inform also great improv perform lstms standard rnn applic order-of-magnitud fewer comput runtim introduct interest recurr neural network rnns great increas recent year sinc larger train databas power comput resourc better train algorithm enabl breakthrough process model tempor sequenc applic includ speech recognit natur languag process attention-bas model structur predict rnns attract equip neural network memori introduct gate unit lstm gru great help make learn network manag rnns typic model discrete-tim dynam system therebi implicit assum constant sampl rate input signal also becom updat frequenc recurr feed-forward unit although earli work realiz result limit suggest continuous-tim dynam system approach toward rnns great major modern rnn implement use fix time step although fix time step perfect suitabl mani rnn applic sever import scenario constant updat rate impos constraint affect precis effici rnns mani real-world task autonom vehicl robot need integr input varieti sensor vision audit distanc measur gyroscop sensor may data sampl rate short time step necessari deal sensor high sampl frequenc howev lead unnecessarili higher comput load confer neural inform process system nip barcelona spain input gate output gate ot output gate input gate ct ht c~t ct kt forget gate ft forget gate ft ot kt ht figur model architectur standard lstm model phase lstm model time gate kt control timestamp phase lstm formul cell valu ct hidden output ht updat open phase otherwis previous valu maintain power consumpt unit network updat one time step interest new applic area process event-bas sensor data-driven record stimulus chang world short latenc accur time process asynchron output sensor time-step model would requir high updat frequenc therebi counteract potenti power save event-bas sensor final interest come comput neurosci sinc brain view loos larg rnns howev biolog neuron communic spike therefor perform asynchron event-trigg updat continu time work present novel rnn model process input sampl asynchron time describ follow section model descript long short-term memori lstm unit import ingredi modern deep rnn architectur first defin updat equat commonly-us version ft ct ot ht wxi whi wci bi wxf whf wcf bf ft wxc whc bc wxo who wco ct bo ot ct main differ classic rnns use gate function ft ot repres input forget output gate time respect ct cell activ vector wherea ht repres input featur vector hidden output vector respect gate use typic sigmoid nonlinear tanh nonlinear weight paramet whi whf who wxi wxf wxo connect differ input gate memori cell output well bias bi bf bo cell state ct updat fraction previous cell state control ft new input state creat element-wis hadamard product denot output cell state nonlinear option peephol connect weight wci wcf wco influenc oper input forget output gate phase lstm model extend lstm model ad new time gate kt open close gate control independ rhythmic oscil specifi three paramet updat cell state ct ht permit gate open first paramet control real-tim period oscil second ron control ratio durat open phase full period third control phase shift oscil phase lstm cell paramet learn train process though variant possibl propos particular success linear formul output output output layer layer layer input layer layer layer input kt open close ct state open input input tj time figur diagram phase lstm behaviour top rhythmic oscil time gate differ neuron period phase shift shown lowest neuron paramet ron ratio open period total period bottom note multilay scenario timestamp distribut layer updat time point illustr phase lstm oper simpl linear increas function use input time gate kt neuron differ ident phase shift open ratio ron note input top panel flow time gate kt middl panel held new cell state ct bottom panel kt open time gate analog rectifi linear unit propag gradient well ron ron mod kt ron ron otherwis auxiliari variabl repres phase insid rhythmic cycl gate kt three phase first two phase open gate rise first phase drop second phase third phase gate close previous cell state maintain leak rate activ close phase play similar role leak parametr leaki rectifi linear unit propag import gradient inform even gate close note linear slope kt open phase time gate allow effect transmiss error gradient contrast tradit rnns even sparser variant rnns updat phase lstm option perform irregular sampl time point tj allow rnns work event-driven asynchron sampl input data use shorthand notat cj ctj cell state time tj analog gate unit let denot state previous updat time rewrit regular lstm cell updat equat cj hj use propos cell updat cej hej mediat time gate kj cej fj ij wxc whc bc cj kj cej kj hj oj cej hj kj hej kj schemat phase lstm paramet found accompani illustr relationship time input time gate kt state ct one key advantag phase lstm formul lie rate memori decay simpl task keep initi memori state c0 long possibl without receiv addit input ij time step tj standard lstm near fully-open forget gate fj updat step would contain cn fn c0 time accuraci epoch time phase lstm bn lstm lstm time high standard resolut async sampl sampl sampl figur frequenc discrimin task network train discrimin wave differ frequenc set shown blue gray everi circl input point standard condit data regular sampl everi ms high resolut sampl condit new input point gather everi asynchron sampl condit new input point present interv ms ms accuraci phase lstm three sampl condit maintain accuraci bn-lstm standard lstm drop signific sampl condit error bar indic standard deviat run mean memori decay exponenti everi time step convers phase lstm state decay open period time gate maintain perfect memori close phase cj kt tj thus singl oscil period length unit updat durat ron result substanti fewer updat step cyclic memori phase lstm much longer adjust memori length via paramet oscil impos spars updat unit therefor substanti decreas total number updat network oper train spars ensur gradient requir backpropag fewer updat timestep allow undecay gradient backpropag time allow faster learn converg similar shield cell state ct gradient input gate forget gate lstm time gate prevent extern input time step dispers mix gradient cell state result follow section investig advantag phase lstm model varieti scenario requir either precis time updat learn long sequenc result present network train adam set default learn rate paramet use theano lasagn unless otherwis specifi leak rate set train test phase shift neuron uniform chosen interv paramet learn train open ratio ron fix adjust train except first task demonstr model train success learn paramet frequenc discrimin task first experi network train distinguish two class sine wave differ frequenc set period target rang outsid rang use uniform distribut interv task illustr advantag phase lstm sinc involv period stimulus requir fine time discrimin input present pair hy ti amplitud timestamp sampl input sine wave figur illustr task blue curv must separ lighter curv base sampl shown circl evalu three condit sampl input signal standard condit sine wave regular sampl everi ms oversampl phase lstm bn lstm lstm mse accuraci epoch lstm plstm plstm plstm plstm epoch figur accuraci train superimpos frequenc task phase lstm outperform lstm bn-lstm exhibit lower varianc shade show maximum minimum run dark line indic mean mean-squar error train addit task input length note longer period acceler learn converg condit sine wave regular sampl everi ms result ten time mani data point final asynchron sampl condit sampl collect asynchron time durat input addit sine wave uniform drawn random phase shift possibl shift random number sampl drawn random durat drawn start time drawn durat number sampl asynchron standard sampl condit equal class approxim balanc yield chanc success rate single-lay rnns train data repeat five random initi seed compar phase lstm configur regular lstm batch-norm lstm found success certain applic regular lstm bn-lstm timestamp use addit input featur dimens phase lstm time input control time gate kt architectur consist neuron lstm bn-lstm phase lstm oscil period phase lstms drawn uniform exponenti space give wide varieti applic frequenc paramet match model applic default lstm paramet given lasagn theano implement kept lstm bn-lstm phase lstm appropri gate bias investig resolv discrep model three network excel standard sampl condit expect seen 3d left howev number epoch increas data sampl factor ten devast effect lstm bn-lstm drop accuraci near chanc middl presum given enough train iter accuraci would return normal baselin howev oversampl condit phase lstm actual increas accuraci receiv inform under waveform final updat even space instead sampl asynchron time even control number point standard sampl condit appear make problem rather challeng tradit state-of-the-art model right howev phase lstm difficulti asynchron sampl data time gate kt need regular updat correct sampl continu time within period extend previous task train rnn architectur signal compos two sine wave goal distinguish signal compos sine wave period t1 t2 independ phase signal compos sine wave period t1 t2 independ phase despit signific challeng 4a demonstr quick phase lstm converg correct solut compar standard approach use exact paramet addit phase lstm appear exhibit low varianc train time figur n-mnist experi sketch digit movement seen imag sensor frame-bas represent digit n-mnist dataset obtain integr input spike pixel spatio-tempor represent digit present three saccad note represent show digit clear blur frame-bas one ad task investig introduc time gate help learn long memori requir we revisit origin lstm task call ad task task sequenc random number present along indic input stream indic input stream present valu ignor indic valu ad end present network produc sum indic valu unlik previous task inher period input one origin task lstm design solv well this would seem work advantag phase lstm use longer period time gate kt could allow effect train unit open timestep train this task sequenc number length drawn two number this stream number mark addit one first number drawn uniform probabl one last half drawn uniform probabl produc model long noisi stream data signific point import this challeng phase lstm model inher period everi timestep could contain import mark point network architectur use period drawn uniform exponenti domain compar four sampl interv note despit differ valu total number lstm updat remain approxim sinc overal spars set ron howev longer period provid longer jump past timestep gradient dure backpropagation-through-tim moreov we investig whether model learn longer sequenc effect longer period use vari period result 4b show longer acceler train network learn much longer sequenc faster n-mnist event-bas visual recognit test perform real-world asynchron sampl data we make use publiclyavail n-mnist dataset neuromorph vision record come event-bas vision sensor sensit local tempor contrast chang event generat pixel local contrast chang exceed threshold everi event encod 4-tupl hx ti posit pixel polar bit indic contrast increas decreas timestamp indic time event generat record consist event generat vision sensor sensor undergo three saccad movement face static digit mnist dataset exampl event respons seen previous work use event-bas input data time inform sometim remov instead frame-bas represent generat comput pixel-wis event-r time period shown note spatio-tempor surfac tabl accuraci n-mnist cnn bn-lstm phase lstm accuraci epoch train/test test test per neuron per neuron lstm updat event reveal detail digit much clear blur frame-bas representation.th phase lstm allow us oper direct spatio-tempor event stream tabl summar classif result three differ network type cnn train framebas represent n-mnist digit two rnns bn-lstm phase lstm train direct event stream regular lstm shown found perform wors cnn compris three altern layer kernel convolut leaki relu nonlinear max-pool fully-connect neuron final fullyconnect output class event pixel address use produc 40-dimension embed via learn embed matrix combin polar produc input therefor network architectur phase lstm bn-lstm time given extra input dimens bn-lstm tabl show phase lstm train faster altern model achiev much higher accuraci lower varianc even within first epoch train we defin factor repres probabl event includ mean event includ rnn model train phase lstm achiev slight higher perform bn-lstm model test fewer event event without retrain rnn model perform well great outperform cnn this becaus accumul statist frame-bas input cnn chang drastic when overal spike rate alter phase lstm rnns seem learn stabl spatio-tempor surfac input slight alter by sampl less frequent final neuron phase lstm updat time averag updat need in comparison updat need per neuron bn-lstm lead an approxim twenty-fold reduct in run time comput cost also worth note result form new state-of-the-art accuraci this dataset visual-auditori sensor fusion lip read final we demonstr use phase lstm task involv sensor differ sampl rate few rnn model ever attempt merg sensor differ input frequenc although sampl rate vari substanti this task we use grid dataset this corpus contain video audio speaker each utter sentenc compos fix grammar constrain vocabulari word data random divid train-test set an opencv implement face detector use video stream extract face resiz grayscal pixel goal obtain model use audio alon video alon input robust classifi sentenc howev sinc audio alon suffici achiev greater accuraci sensor modal random mask zero dure train encourag robust toward sensori nois loss network architectur first separ process video audio data merg in two rnn layer receiv modal video stream use three altern layer kernel convolut subsampl reduc input use input recurr unit audio stream connect the 39-dimension mfccs mfccs first second deriv recurr unit stream converg the merged-1 layer recurr unit connect second hidden layer recurr unit name merged-2 the output the merged-2 layer fully-connect output node repres the vocabulari grid the phase lstm network recurr unit phase lstm unit low res loss mfccs video frame phase lstm bn lstm lstm video plstm merged-1 plstm merged-2 plstm time high res loss audio plstm mfcc kj open input time time epoch figur lip read experi input open time gate for the lip read experi note the 25fps video frame rate multipl the audio input frequenc hz phase lstm time paramet configur align the sampl time input exampl input video top audio bottom test loss use the video stream alon video frame rate top low resolut condit mfccs comput everi network updat everi ms bottom high resolut condit mfccs everi ms network updat everi ms in the audio video phase lstm layer we manual align the open period the time gate the sampl time the input disabl learn the paramet this prevent present zero artifici interpol the network when data present in the merg layer howev the paramet the time gate learn the period the first merg layer drawn the second 6b show visual one frame video the complet durat an audio sampl dure evalu all network achiev greater accuraci audio-on combin audio-video input howev video-on evalu an audio-video capabl network prove the challeng the result in 6c focus result though result rank repres all condit two differently-sampl version the data use in the first low resolut version top the sampl rate the mfccs match the sampl rate the fps video in the second high-resolut condit the sampl rate set the common valu hz sampl frequenc bottom shown in the higher audio sampl rate increas accuraci allow for faster latenc instead the phase lstm converg substanti faster both lstm batch-norm lstm the peak accuraci compar favor lipreading-focus state-of-the-art approach avoid manually-craft featur discuss the phase lstm mani surpris advantag with rhythmic period it act like learnabl gate fourier transform input permit fine time discrimin altern the rhythmic period view kind persist dropout preserv state enhanc model divers the rhythmic inactiv even view a shortcut the past for gradient backpropag acceler train the present result support interpret demonstr the abil discrimin rhythmic signal learn long memori trace import in all experi phase lstm converg quick theoret requir the comput at runtim often improv in accuraci compar standard lstm the present method also easili extend to grus it like even simpler model such one use a square-wave-lik oscil perform well therebi make even effici encourag altern phase lstm formul an inspir for use oscil in recurr network come comput neurosci rhythm shown to play import role for synchron plastic phase lstms design biolog plausibl model may help explain the advantag and robust learn in larg spike recurr network
----------------------------------------------------------------

title: 6215-on-multiplicative-integration-with-recurrent-neural-networks.pdf

multipl integr recurr neural network yuhuai saizheng zhang2 ying zhang2 yoshua bengio2,4 ruslan salakhutdinov3,4 univers toronto mila universit de montr al carnegi mellon univers cifar ywu cs.toronto.edu,2 firstname.lastnam umontreal.ca rsalakhu cs.cmu.edu abstract introduc general simpl structur design call multipl integr improv recurr neural network rnns mi chang way inform differ sourc flow integr comput build block rnn introduc almost extra paramet new structur easili embed mani popular rnn model includ lstms grus empir analyz learn behaviour conduct evalu sever task use differ rnn model experiment result demonstr multipl integr provid substanti perform boost mani exist rnn model introduct recent resurg new structur design recurr neural network rnns design deriv popular structur includ vanilla rnns long short term memori network lstms gate recurr unit grus despit vari characterist share common comput build block describ follow equat uz rn rm state vector come differ inform sourc rd n rd state-to-st transit matric bias vector comput build block serv combin integr inform flow sum oper follow nonlinear refer addit build block addit build block wide implement various state comput rnns hidden state comput vanilla-rnn gate/cel comput lstms grus work propos altern design construct comput build block chang procedur inform integr specif instead util sum oper propos use hadamard product fuse wx uz uz result modif chang rnn first order second order introduc extra paramet call kind inform integr design form multipl integr effect multipl natur result gate type structur wx uz gate specif one think state-to-st comput uz exampl repres previous state dynam rescal wx exampl repres input rescal exist addit build block uz independ relat simpl modif bring advantag addit build block alter rnn gradient properti discuss detail next section well verifi extens experi equal contribut confer neural inform process system nip barcelona spain follow section first introduc general formul multipl integr compar addit build block sever sequenc learn task includ charact level languag model speech recognit larg scale sentenc represent learn use skip-thought model teach machin read comprehend question answer task experiment result togeth sever exist state-of-the-art model show various rnn structur includ vanilla rnns lstms grus equip multipl integr provid better general easier optim main advantag includ enjoy better gradient properti due gate effect hidden unit non-satur general formul multipl integr natur includ regular addit build block special case introduc almost extra paramet compar addit build block drop-in replac addit build block popular rnn model includ lstms grus also combin rnn train techniqu recurr batch normal discuss it relationship exist model includ hidden markov model hmms second order rnns multipl rnns structur descript analysi general formul multipl integr key idea behind multipl integr integr differ inform flow wx uz hadamard product general formul multipl integr includ two bias vector ad wx uz uz bias vector notic such formul contain first order term addit build block uht wxt order make multipl integr flexibl introduc anoth bias vector rd gate2 term wx uz obtain follow formul wx uz uz wx note number paramet multipl integr addit build block sinc number new paramet neglig compar total number paramet also multipl integr easili extend lstms grus3 adopt vanilla build block comput gate output state one direct replac multipl integr general kind structur inform flow involv residu network one implement pairwis multipl integr integr inform sourc gradient properti multipl integr differ gradient properti compar addit build block clariti present first look vanilla-rnn rnn multipl integr embed refer mi-rnn ht wxt uht versus ht ht wxt uht vanilla-rnn gradient comput follow n ht ut diag 0k ht n wxk uhk equat show gradient flow time heavili depend hidden-to-hidden matrix xk appear play limit role ht come deriv mix uhk hand gradient n mi-rnn is4 ht ut diag wxk diag 0k ht n multipl integr degener vanilla addit build block see exact formul appendix here adopt simplest formul multipl integr illustr general case diag wxk becom diag wxk wxk uhk look gradient see matrix current input xk direct involv gradient comput gate matrix henc capabl alter updat learn system show experi wxk direct gate gradient vanishing/explod problem allevi wxk dynam reconcil make gradient propag easier compar regular rnns lstms grus multipl integr gradient propag properti complic principl benefit gate effect also persist model experi experi use general form multipl integr hidden state/g comput unless otherwis specifi exploratori experi understand function multipl integr take simpl rnn illustr perform sever exploratori experi charact level languag model task use penn-treebank dataset follow data partit length train sequenc model singl hidden layer size use adam optim algorithm learn rate weight initi sampl drawn perform evalu bits-per-charact bpc metric log2 perplex gradient properti analyz gradient flow model divid gradient two part gate matrix product ut diag wxk deriv nonlinear separ analyz properti term compar addit build block first focus gate effect brought diag wxk order separ effect nonlinear chose ident map henc vanilla-rnn mi-rnn reduc linear model refer lin-rnn lin-mi-rnn model monitor log-l2-norm gradient log averag train set everi train epoch ht hidden state time step negat log-likelihood singl charact predict final time step figur show evolut gradient norm small better reflect gradient propag behaviour observ norm lin-mi-rnn orang increas rapid soon exceed correspond norm lin-rnn larg margin norm lin-rnn stay close zero chang time almost neglig observ impli help diag wxk term gradient vanish lin-mi-rnn allevi compar lin-rnn final test bpc bits-per-charact lin-mi-rnn compar vanilla-rnn stabil regular lin-rnn perform rather poor achiev test bpc next look nonlinear chose tanh vanilla-rnn mi-rnn figur show comparison histogram hidden activ time step valid set train interest vanilla-rnn activ satur valu around wherea mi-rnn activ non-satur valu around direct consequ gradient propag non-satur activ impli diag 0k tanh help gradient propag wherea satur activ impli diag 0k result gradient vanish scale problem ad two number differ order magnitud smaller one might neglig sum howev multipli two number valu product depend regardless scale principl also appli compar multipl integr addit build block experi test whether multipl integr robust scale weight valu follow model section first calcul norm wxk uhk vanilla-rnn mi-rnn differ train found structur wxk lot smaller uhk magnitud this might due fact xk one-hot vector make number updat column smaller u result vanilla-rnn pre-activ term wxk uhk larg control valu uhk wxk becom rather small mi-rnn hand pre-activ term wxk uhk still depend valu wxk uhk due multipl valid bpc lin-rnn lin-rnn lin-rnn lin-mi-rnn lin-mi-rnn lin-mi-rnn number epoch activ valu h_t vanilla-rnn mi-rnn-simpl mi-rnn-gener normal fequenc normal fequenc log||dc number epoch activ valu h_t figur curv log-l2-norm gradient lin-rnn blue lin-mi-rnn orang time gradual chang valid bpc curv vanilla-rnn mi-rnn-simpl use mirnn-gener use histogram vanilla-rnn hidden activ valid set most activ satur histogram mi-rnn hidden activ valid set most activ satur next tri differ initi test sensit scale model fix initi initi uniform rw rw rw vari tabl top left panel show result increas scale perform vanilla-rnn improv suggest model abl better util input inform hand mi-rnn much robust differ initi scale almost effect final perform differ choic formul third experi evalu perform differ comput build block vanilla-rnn mi-rnn-simpl mi-rnn-gener valid curv figur see mi-rnn simpl mi-rnn-gener yield much better perform compar vanilla-rnn mi-rnn-gener faster converg speed compar mi-rnn-simpl also compar result previous publish model tabl bottom left panel mi-rnn-gener achiev test bpc knowledg best result rnns this task without complex gating/cel mechan charact level languag model addit penn-treebank dataset also perform charact level languag model two larger dataset text86 hutter challeng wikipedia7 contain charact wikipedia text8 alphabet size hutter challeng wikipedia alphabet size dataset follow train protocol respect use adam optim start learn rate grid-search if valid bpc bits-per-charact decreas epoch half learn rate implement multipl integr vanilla-rnn lstm refer mirnn mi-lstm result text8 dataset shown tabl bottom middl panel five model includ previous publish model number we perform hyper-paramet search initi mi-rnn-gener http //mattmahoney.net/dc/textdata http //prize.hutter1.net rw std rnn mi-rnn wsj corpus cer wer drnn+ctcbeamsearch encoder-decod lstm+ctcbeamsearch eesen lstm+ctc+wfst mi-lstm+ctc+wfst penn-treebank bpc text8 bpc rnn hf-mrnn rnn+stabal mi-rnn linear mi-rnn rnn+smoothrelu hf-mrnn mi-rnn lstm mi-lstm hutterwikipedia bpc stacked-lstm gf-lstm grid-lstm mi-lstm tabl top test bpcs standard deviat model differ scale weight initi top right test cer wer wsj corpus bottom left test bpcs charact level penn-treebank dataset bottom middl test bpcs charact level text8 dataset bottom right test bpcs charact level hutter prize wikipedia dataset paramet rnns without complex gating/cel mechan first three result mi-rnn initi perform best mi-lstm initi outperform model larg margin8 hutter challeng wikipedia dataset we compar mi-lstm singl layer unit initi previous stack lstm layer gf-lstm layer grid-lstm layer tabl bottom right panel show result despit simpl structur compar sophist connect design gf-lstm grid-lstm mi-lstm outperform model achiev new state-of-the-art this task speech recognit we next evalu model wall street journal wsj corpus avail ldc corpus ldc93s6b we use full hour set train set valid set test we follow data prepar process model set we use charact target acoust model decod done ctc base weight finite-st transduc wfsts propos model refer mi-lstm+ctc+wfst consist bidirect mi-lstm layer unit direct ctc perform top resolv align issu speech transcript comparison we also train baselin model refer lstm+ctc+wfst size use vanilla lstm adam learn rate use optim gaussian weight nois zero mean standard deviat inject regular we evalu model charact error rate cer without languag model word error rate wer extend trigram languag model tabl top right panel show mi-lstm+ctc+wfst achiev quit good result cer wer compar recent work clear improv baselin model note we conduct care hyper-paramet search this task henc one could potenti obtain better result better decod scheme regular techniqu learn skip-thought vector next we evalu multipl integr skip-thought model skip-thought encoder-decod model attempt learn generic distribut sentenc represent model produc sentenc represent robust perform well practic achiev excel result across mani differ nlp task model train bookcorpus dataset consist book sentenc surpris singl pass report better result use much larger model direct compar semantic-related mse paraphras detect acc f1 uni-skip bi-skip combine-skip uni-skip bi-skip combine-skip uni-skip mi-uni-skip uni-skip mi-uni-skip classif mr cr subj mpqa uni-skip bi-skip combine-skip uni-skip mi-uni-skip attent reader val err lstm bn-lstm bn-everywher lstm mi-lstm mi-lstm+bn mi-lstm+bn-everywher tabl top left skip-thought+mi semantic-related task top right skip-thought+mi paraphras detect task bottom left skip-thought+mi four differ classif task bottom right multipl integr batch normal teach machin read comprehend task train data take week high-end gpu report such train speed larg limit one perform care hyper-paramet search howev multipl integr train time shorten factor two final perform also signific improv we exact follow author theano implement skip-thought model9 encod decod single-lay grus hidden-lay size recurr matric adopt orthogon initi non-recurr weight initi uniform distribut adam use optim we implement multipl integr encod gru embed mi decod provid substanti gain we refer model mi-uni-skip initi we also train baselin model size refer uni-skip essenti reproduc origin model dure cours train we evalu skip-thought vector semant related task use sick dataset everi updat mi-uni-skip baselin model iter process mini-batch size result shown figur note mi-uni-skip signific outperform baselin term speed converg also term final perform at around updat mi-uni-skip alreadi exceed best perform achiev baselin take twice number updat we also evalu model one week train best result report six eight task report semant related task sick dataset paraphras detect task microsoft research paraphras corpus four classif benchmark movi review sentiment custom product review subjectivity/object classif subj opinion polar mpqa we also compar result result report three model origin skip-thought paper uni-skip bi-skip combine-skip uni-skip model baselin bi-skip bidirect model size combine-skip take concaten vector uni-skip bi-skip form dimens vector task evalu tabl show mi-uni-skip domin across task not achiev higher perform baselin model mani case also outperform combine-skip model twice number dimens clear multipl integr provid faster better way train large-scal skip-thought model teach machin read comprehend last experi we show use multipl integr combin techniqu train rnns advantag use mi still persist recent introduc recurr batch-norm evalu propos techniqu uni9 https //github.com/ryankiros/skip-thought mse uni-skip mi-uni-skip valid error number iter lstm bn-lstm mi-lstm mi-lstm+bn number iter figur mse curv uni-skip mi-uni-skip semant related task sick dataset mi-uni-skip signific outperform baselin uni-skip valid error curv attent reader model there clear margin model without mi direct attent reader model question answer task use cnn corpus10 test approach we evalu follow four model vanilla lstm attent reader model singl hidden layer size baselin refer lstm multipl integr lstm singl hidden size refer mi-lstm milstm batch-norm refer mi-lstm+bn mi-lstm batch-norm everywher detail refer mi-lstm+bn-everywher we compar model result report refer lstm bn-lstm bn-lstm everywher for mi model initi we follow experiment protocol use exact set except we remov gradient clip for mi-lstm figur 2b show valid curv baselin lstm mi-lstm bn-lstm mi-lstm+bn final valid error model report tabl bottom right panel clear use multipl integr result improv model perform regardless whether batch-norm use howev combin mi batch-norm provid best perform fastest speed converg this show general applic multipl integr when combin optim techniqu relationship previous model relationship hidden markov model one show certain constraint mi-rnn effect implement forward algorithm hidden markov model hmm direct map construct follow for similar deriv let rm state transit probabl matrix uij pr ht+1 i|ht rm n observ probabl matrix wij pr xt i|ht when one-hot vector mani languag model task multipli effect choos column observ matrix name if th entri one wxt pr xt ht let h0 initi state distribut h0 pr h0 ht alpha valu in forward algorithm hmm ht pr x1 ht then uht pr x1 thus wxt+1 uht pr xt+1 pr x1 pr x1 exact implement forward algorithm use multipl integr matric probabl matric need one-hot vector function need linear we drop all bias term therefor rnn multipl integr seen nonlinear extens hmms extra freedom in paramet valu nonlinear make model more flexibl compar hmms relat second order rnns multipl rnns mi-rnn relat second order rnn multipl rnn mrnn we first describ similar two model second order rnn involv second order term st in vanilla-rnn ith element st comput bilinear form st xtt rn note use truncat version origin dataset in order save comput learn curv final result number obtain email correspond author https //github.com/cooijmanstim/recurrent-batch-normalization.git ith slice tensor rm n multipl integr also involv second order term st wxt uht in our case st xtt ui ui ith row in ith element note outer product ui rank-1 matrix multipl rnn also second order rnn but pdiag vxt for mi-rnn we approxim tensor decomposit also think second order term tensor decomposit wxt uht u xt diag diag wxt there howev sever differ make mi favour model simpler parametr mi use rank-1 approxim compar second order rnns diagon approxim compar multipl rnn moreov mi-rnn share paramet across first second order term wherea two model not result number paramet larg reduc make our model more practic for larg scale problem avoid overfit easier optim in tensor decomposit method the product three differ low-rank matric general make it hard to optim howev the optim problem becom easier in mi as discuss in section general structur design vanilla-rnn design multipl integr easili embed in mani rnn structur lstms grus wherea the second order rnn mrnn present specif design for modifi vanilla-rnn moreov we also compar mi-rnn perform to the previous hf-mrnn result multipl rnn train hessian-fre method in tabl bottom left bottom middl panel on penn-treebank text8 dataset one see mi-rnn outperform hf-mrnn on both task general multipl integr multipl integr view as general way combin inform flow from two differ sourc in particular propos the ladder network achiev promis result on semi-supervis learn in model they combin the later connect the backward connect via the combin function a hadamard product the perform would sever degrad without this product as empir shown explor neural embed approach in knowledg base by formul relat as bilinear and/or linear map function compar a varieti embed model on the link predict task surpris the best result among all bilinear function the simpl weight hadamard product they care compar the multipl addit interact show the multipl interact domin the addit one conclus in this paper we propos to use multipl integr a simpl hadamard product to combin inform flow in recurr neural network mi easili integr mani popular rnn model includ lstms grus introduc almost extra paramet inde the implement mi requir almost extra work beyond implement rnn model we also show mi achiev state-of-the-art perform on four differ task dataset vari size scale we believ that the multipl integr becom a default build block for train various type rnn model acknowledg the author acknowledg the follow agenc for fund support nserc canada research chair cifar calcul quebec comput canada disney research onr grant the author thank the develop theano kera also thank jimmi ba for mani thought-provok discuss
----------------------------------------------------------------

title: 6279-natural-parameter-networks-a-class-of-probabilistic-neural-networks.pdf

natural-paramet network class probabilist neural network hao wang xingjian shi dit-yan yeung hong kong univers scienc technolog hwangaz xshiab dyyeung cse.ust.hk abstract neural network achiev state-of-the-art perform various applic unfortun applic train data insuffici often prone overfit one effect way allevi problem exploit bayesian approach use bayesian neural network anoth shortcom nn lack flexibl custom differ distribut weight neuron accord data often done probabilist graphic model address problem propos class probabilist neural network dub natural-paramet network novel lightweight bayesian treatment nn npn allow usag arbitrari exponential-famili distribut model weight neuron differ tradit nn bnn npn take distribut input goe layer transform produc distribut match target output distribut bayesian treatment effici backpropag perform learn natur paramet distribut weight neuron output distribut layer byproduct may use second-ord represent associ task link predict experi real-world dataset show npn achiev state-of-the-art perform introduct recent neural network achiev state-of-the-art perform various applic rang comput vision natur languag process howev nn train stochast gradient descent sgd variant known suffer overfit especi train data insuffici besid overfit anoth problem nn come underestim uncertainti could lead poor perform applic like activ learn bayesian neural network bnn offer promis tackl problem principl way earli bnn work includ method base laplac approxim variat infer mont carlo sampl wide adopt due lack scalabl recent advanc direct seem shed light practic adopt bnn propos method base vi mont carlo estim lower bound margin likelihood use infer weight recent use onlin version expect propag call probabilist back propag bayesian learn nn propos bay backprop view extens base reparameter trick recent interest bayesian treatment call bayesian dark knowledg bdk design approxim teacher network simpler student network base stochast gradient langevin dynam sgld although recent method practic earlier one sever outstand problem remain address method requir sampl either train time test time incur much higher cost vanilla nn mention method confer neural inform process system nip barcelona spain base onlin ep vi involv sampl need comput predict densiti integr paramet comput ineffici method assum gaussian distribut weight neuron allow flexibl custom differ distribut accord data done probabilist graphic model address problem propos natural-paramet network npn class probabilist neural network input target output weight neuron model arbitrari exponential-famili distribut poisson distribut word count instead limit gaussian distribut input distribut go layer linear nonlinear transform determinist produc distribut match target output distribut previous work show provid distribut input corrupt data nois play role regular byproduct output distribut intermedi layer may use second-ord represent associ task thank properti exponenti famili distribut npn defin correspond natur paramet learn effici backpropag unlik npn explicit propag estim uncertainti back forth deep network way uncertainti estim layer neuron readili avail associ task experi show inform help neuron intermedi layer use represent like autoencod summari our main contribut propos npn class probabilist neural network our model combin merit nn pgm term comput effici flexibl custom type distribut differ type data leverag properti exponenti famili sampling-fre backpropagationcompat algorithm design effici learn distribut weight learn natur paramet unlik probabilist nn model npn obtain uncertainti intermediate-lay neuron byproduct provid valuabl inform learn represent experi real-world dataset show npn achiev state-of-the-art perform classif regress unsupervis represent learn task natural-paramet network exponenti famili refer import class distribut use algebra properti distribut exponenti famili form exp random variabl denot natur paramet vector suffici statist normal given type distribut differ choic lead differ shape exampl univari gaussian distribut t correspond 2d 2d motiv observ npn natur paramet need learn model distribut weight neuron consid npn take vector random distribut multivari gaussian distribut input multipli matrix random distribut goe nonlinear transform output anoth distribut sinc three distribut process specifi natur paramet given type distribut learn predict network actual oper space natur paramet exampl use element-wis factor gamma distribut weight neuron npn counterpart vanilla network need twice number free paramet weight neuron sinc two natur paramet univari gamma distribut notat convent use boldfac uppercas letter like denot matric boldfac lowercas letter like vector similar boldfac number repres row vector matrix ident entri npn use denot valu neuron layer nonlinear transform valu nonlinear transform mention npn tri learn distribut variabl rather variabl henc use letter without subscript denot random variabl correspond distribut subscript use denot natur paramet pair wc wd similar subscript mean-vari pair note clariti mani oper use implicit element-wis exampl squar z2 divis bz partial deriv gamma function logarithm log factori z1 data set input distribut resembl ae denois effect input network denot output target label word count follow text drop subscript sometim superscript clariti bracket denot concaten pair vector linear transform npn first introduc linear form general npn simplic assum distribut two natur paramet gamma distribut beta distribut gaussian distribut t section specif factor distribut weight matric wc wd j p wij wc ij wd ij pair wc ij wd ij correspond natur paramet assum similar factor distribut tradit nn linear transform follow output previous layer nn determinist variabl npn exponential-famili distribut mean result also distribut conveni subsequ comput desir approxim use anoth exponentialfamili distribut match mean varianc specif comput wm ws wc wd bm bs bc bd get oc od mean om varianc os follow ad ws l wm wm wm os denot element-wis product biject function map natur paramec+1 ter distribut mean varianc d2 gamma distribut similar use denot invers transform wm ws bm bs mean varianc obtain natur paramet comput om os use recov oc od subsequ facilit feedforward comput nonlinear transform describ section nonlinear transform npn after obtain linear transform distribut defin natur paramet oc od element-wis nonlinear transform well defin invers function impos result activ distribut pa po po factor distribut defin oc od though pa may exponential-famili distribut approxim one ac ad match first two moment onc mean varianc pa obtain comput correspond natur paramet approxim accuraci suffici accord preliminari experi feedforward comput po o|oc od po o|oc od a2m ac ad key comput challeng comput integr equat closed-form solut need effici comput po o|oc od gaussian distribut closedform solut exist common activ function like tanh x detail section unfortun case distribut leverag conveni form exponenti famili find possibl design activ function integr non-gaussian distribut also express close form theorem assum exponential-famili distribut po exp vector u2 um number natur paramet if activar tion function ui use first two moment po tabl activ function exponential-famili distribut distribut probabl densiti function beta distribut rayleigh distribut gamma distribut poisson distribut gaussian distribut exp exp dx cx exp c exp activ function support qx nonneg interg relu tanh sigmoid po dx express close form differ ui correspond differ set activ function constant proof first let first moment exp ui dx g e exp e dx g e g e similar second moment comput r2 2rq detail proof provid supplementari materi theorem remain find constant make strict increas bound tabl show exponentialfamili distribut possibl activ function exampl equat if odo oc gamma distribut backpropag distribut two natur paramet gradient consist two term exampl oc oc error term network algorithm deep nonlinear npn input data number iter learn rate number layer l. appli equat comput linear nonlinear transform layer end comput error oc od ac ad comput comput wm ws bm bs wc wd bc bd end updat wc wd bc bd layer end deep nonlinear npn natur layer nonlinear npn stack form deep npn1 shown algorithm deep npn sens similar pgm chain structur unlik pgm general howev npn need cost infer algorithm like variat infer markov chain mont carlo chain-structur pgm hidden markov model effici infer algorithm also exist due special structur similar markov properti enabl npn effici train end-to-end backpropag learn fashion space natur paramet pgm known flexibl nn sens choos differ distribut depict differ relationship among variabl major drawback pgm scalabl especi although approxim accuraci may decreas npn get deeper feedforward comput automat adjust accord data backpropag note sinc first part equat last part equat cancel direct use without comput ac ad pgm deep differ pgm nn stack relat simpl comput layer learn paramet use backpropag comput effici algorithm pgm npn potenti get best world term flexibl differ type exponential-famili distribut chosen weight neuron use gamma distribut weight neuron npn lead deep nonlinear version nonneg matrix factor npn bernoulli distribut sigmoid activ resembl bayesian treatment sigmoid belief network if poisson distribut chosen neuron npn becom neural analogu deep poisson factor analysi note similar weight decay nn may add kl diverg prior distribut learn distribut weight error regular use isotrop gaussian prior experi npn chosen prior distribut correspond prior bayesian model learn distribut correspond approxim posterior distribut weight note generat stori assum here weight sampl prior output generat given data weight variant npn section introduc three npn variant differ properti demonstr flexibl effect npn note practic use transform version natur paramet refer proxi natur paramet here instead origin one comput effici exampl gamma distribut dc exp dx use proxi natur paramet comput rather natur paramet gamma npn gamma distribut support posit valu import member exponenti famili correspond probabl densiti function dc exp dx natur paramet use proxi natur paramet if assum gamma distribut ae form npn becom deep nonlinear version nonneg matrix factor see note ae activ zero bias equival find factor matrix ql denot middle-lay neuron nonneg entri gamma distribut gamma npn paramet wc wd bc bd learn follow algorithm detail algorithm follow linear transform sinc gamma distribut assum here use function dc dc2 comput wm ws wc wd bm bs bc bd oc od om os probabilist linear transform equat nonlinear transform proxi natur paramet gamma distribut mean varianc nonlinear transform distribut would obtain equat follow theorem closed-form solut possibl ui constant use new activ function section supplementari materi detail function deriv ooc od oc po o|oc od v od od od od 2oc r2 oc od od error oc od comput regress error negat log-likelihood log log od oc log od observ output correspond classif cross-entropi loss use e. follow comput flow bp use learn wc wd bc bd figur predict distribut pbp bdk dropout nn npn shade region correspond standard deviat black curv data-gener function blue curv show mean predict distribut red star train data gaussian npn differ gamma distribut support posit valu gaussian distribut also exponential-famili distribut describ real-valu random variabl make natur choic npn refer npn variant gaussian distribut weight neuron gaussian npn detail algorithm gaussian npn follow linear transform besid support real valu anoth properti gaussian distribut mean varianc use proxi natur paramet lead ident map function cut comput cost use function comput wm ws wc wd bm bs bc bd oc od om os probabilist linear transform equat use nonlinear transform if sigmoid activ equat would convolut gaussian sigmoid approxim anoth sigmoid oc o|oc diag od od o|oc diag od a2m a2m od log similar approxim appli activ tanh x sinc tanh x if relu activ use use techniqu obtain first two moment max z1 z2 z1 z2 gaussian random variabl full deriv tanh x left supplementari materi error oc od last layer comput error kl diverg kl n oc diag od ym vector entri equal small 1t t log log valu henc error od od classif task cross-entropi loss use follow comput flow bp use learn wc wd bc bd poisson npn poisson distribut anoth member exponenti famili often use model count count word topic super topic in document henc for text model natur assum poisson distribut for neuron in npn interest design poisson npn seen neural analogu poisson factor analysi model besid closed-form nonlinear transform anoth challeng poisson npn map pair om os singl paramet oc poisson distribut accord central limit theorem we oc 8os section supplementari materi for proof justif detail deriv poisson npn experi in this section we evalu variant npn state-of-the-art method four real-world dataset we use matlab gpu implement npn ae variant vanilla nn train dropout sgd dropout nn for baselin we use theano librari mxnet tabl test error rate mnist method error bdk bbb dropout1 dropout2 gamma npn gaussian npn tabl test error rate for differ size train data size npn dropout bdk toy regress task gain some insight npn we start toy 1d regress task predict mean varianc visual follow we generat point in one dimens a uniform distribut in interv target output sampl function x3 we fit data gaussian npn bdk pbp supplementari materi for detail hyperparamet figur show predict mean varianc npn bdk pbp along mean provid dropout nn for larger version figur pleas refer end supplementari materi we see varianc pbp bdk npn diverg as farther away train data both npn bdk predict distribut accur enough to keep x3 curv insid the shade region relat low varianc interest observ the train data point becom more scatter ideal the varianc start diverg happen in npn howev pbp bdk sensit enough to captur this dispers chang in anoth dataset boston hous the root mean squar error for pbp bdk npn mnist classif the mnist digit dataset consist train imag test imag imag label as one the digit we train the model imag use imag for valid network a structur use for all method sinc work best for the dropout nn denot as dropout1 in tabl bdk bdk with a structur achiev error rate we also tri the dropout nn with twice the number hidden neuron dropout2 in tabl for fair comparison for bbb we direct quot result we implement bdk npn use the hyperparamet as in whenev possibl gaussian prior use for npn the supplementari materi for detail hyperparamet accuraci as shown in tabl bdk bbb achiev compar perform with dropout nn similar to pbp includ in the comparison sinc support regress gamma npn slight outperform dropout nn gaussian npn abl to achiev a lower error rate note bbb with gaussian prior achiev an error rate the result use gaussian mixtur prior for
----------------------------------------------------------------

title: 5271-pre-training-of-recurrent-neural-networks-via-linear-autoencoders.pdf

pre-train recurr neural network via linear autoencod luca pasa alessandro sperduti depart mathemat univers padova itali pasa sperduti math.unipd.it abstract propos pre-train techniqu recurr neural network base linear autoencod network sequenc linear dynam system model target sequenc start give close form solut definit optim weight linear autoencod given train set sequenc solut howev comput demand suggest procedur get approxim solut given number hidden unit weight obtain linear autoencod use initi weight inputto-hidden connect recurr neural network train desir task use four well known dataset sequenc polyphon music show propos pre-train approach high effect sinc allow larg improv state art result consid dataset introduct recurr neural network rnn constitut power comput tool sequenc model predict howev train rnn easi task main well known vanish gradient problem make difficult learn long-term depend although altern architectur lstm network effici train procedur hessian free optim propos circumv problem reliabl effect train rnns still open problem vanish gradient problem also obstacl deep learn context grow evid effect learn base relev robust intern represent develop autonomi learn system usual achiev vectori space exploit nonlinear autoencod network learn rich intern represent input data use input shallow neural classifi predictor see exampl import start gradient-bas learn good initi point paramet space also point relationship autoencod network princip compon analysi pca well known sinc late especi case linear hidden unit recent linear autoencod network structur data studi exact closed-form solut weight given case number hidden unit equal rank full data matrix paper borrow conceptu framework present devis effect pretrain approach base linear autoencod network sequenc get good start point weight space rnn success train even presenc longterm depend specif revis theoret approach present give simpler direct solut problem devis exact closed-form solut full rank case weight linear autoencod network sequenc highlight relationship propos solut pca input data ii introduc new formul autoencod learn problem abl return optim solut also case number hidden unit less rank full data matrix iii propos procedur approxim learn autoencod network weight scenario larg sequenc dataset import show use linear autoencod network solut deriv good initi point rnn weight space propos approach abl return quit impress result appli predict task involv long sequenc polyphon music linear autoencod network sequenc shown princip direct set vector rk relat solut obtain train linear autoencod network oi woutput whidden whidden rp k woutput rk p network train get oi consid tempor sequenc input vector discret time index linear autoencod defin consid coupl linear dynam system cyt axt byt notic eq extend linear transform defin eq introduc memori term involv matrix rp p fact insert right part equat keep track input histori time done exploit state space represent repres decod part autoencod state multipli observ input time state time generat decod continu formul propos exampl iter procedur learn weight matric base oja rule present proof converg propos procedur howev given more recent exact closed-form solut weight given case number hidden unit equal rank full data matrix full rank case section revis result addit give exact solut also case number hidden unit strict less rank full data matrix basic idea look direct high varianc state space dynam linear system let start consid singl sequenc state vector correspond induc state sequenc collect row matrix y2 y3 yn use initi condit y0 null vector dynam linear system rewrit matrix xt xt bt 2t x3 n given kn data matrix collect invert input subsequ includ whole sequenc row paramet matrix dynam system now interest use state space dimens rp much inform contain preserv start factor use svd obtain v ut rn n unitari matrix rn rectangular diagon matrix nonneg real number diagon singular valu ut rs n unitari matrix import notic column ut correspond nonzero singular valu apart mathemat technic basic correspond princip direct data pca rank first element diagon null decomposit reduc rn p rp p rp n now observ ident matrix dimens sinc definit column orthogon impos deriv optim matric rp k rp p dynam system correspond state space matrix thus repres compos submatric ui size problem reduc find matric u1 bt u2 2t u3 un reason impos get state space coordin uncorrel diagonalis empir sampl covari matrix state pleas note way state row matrix correspond row data matrix unrol sub sequenc read given time row vector would correspond comput pca keep fist princip direct follow demonstr exist solut equat start observ own special structur given rn k rn rn null matrix size moreov singular valu decomposit ui use fact combin equat get ui+t ui qt rt moreov un sinc nv rn thus eq satisfi rn un un u1 interest note origin data recov comput achiev run system bt start yn matrix defin eq bt final import remark construct work singl sequenc also set sequenc differ length exampl let consid two sequenc xa2 xa3 xb xb bt xa2 xa1 bt xb1 at at at x3 r4 collect togeth obtain r2 final remark stress construct work equal rank next section treat case optim solut low dimension autoencod rank solut given break rn question whether propos solut still consequ hold best reconstruct error paper answer negat term question resort new formul problem introduc slack-lik matric ei rk p collect reconstruct error need minimis min q rp p ei subject u1 u2 u3 kei k2f un un en p u2 u3 notic problem convex object function constraint thus global optim solut e i deriv at u1 specif when rs k span optim solut given e i rt solut alreadi k describ optim solut e i howev difficult devis iter procedur reach minimum sinc experiment section exploit solut problem reason explain later sketch procedur help observ given fix optim solut ei given u2 u1 u3 u1 u4 mq i i mq pseudo invers mq i general decompos compon span compon orthogon notic reduc take part compon absorb defin new optim valu obtain process iter till converg given pre-train recurr neural network defin pre-train procedur recurr neural network one hidden layer unit output unit ot woutput h xt ro h xt winput whidden rp woutput ro p whidden rp k vector rm zi consid symmetr sigmoid function idea exploit hidden state represent obtain eq initi hidden state represent rnn describ eq this implement initialis weight matric winput whidden use matric joint solv eq eq sinc function b specif initi winput whidden b moreov use symmetr sigmoid function give good approxim ident function around origin allow good transfer linear dynam insid rnn concern woutput initialis use best possibl solut pseudoinvers time target matrix minimis output squar error learn use introduc nonlinear compon allow improv perform model more formal let consid predict task sequenc sq xq2 xqlq length lq train set sequenc tq target vector defin train sequenc given hsq tq h xq1 tq1 tq2 xqlq tqlq tqi ro given trainpn ing set sequenc let defin target matrix rl o lq t1 t2 tl1 t1 tln input matrix size let desir number hidden unit recurr neural network then pre-train procedur defin follow comput linear autoencod use princip direc tion obtain optim matric rp rp set winput whidden iii run rnn train sequenc collect hidden activ vec tor comput use symmetr sigmoid function time row matrix rl p iv set woutput left pseudoinvers h. comput approxim solut larg dataset real world scenario applic approach may turn difficult size data matrix fact stabl comput princip direct usual obtain svd decomposit data matrix typic applic domain involv number row column easili order hundr thousand unfortun comput complex svd decomposit basic cubic smallest matrix dimens memori consumpt also import issu algorithm approxim comput svd suggest howev sinc purpos need matric predefin number column here present ad-hoc algorithm approxim comput matric solut base follow four main idea divid slice size input at time column exploit svd decomposit at slice separ ii comput approxim matric column increment via truncat svd temporari matric obtain concaten current approxim new slice iii comput svd decomposit temporari matrix via either kernel covari matrix depend smallest number row number column temporari matrix iv exploit qr decomposit comput svd decomposit algorithm show pseudo-cod main step procedur maintain temporari matrix use collect increment approxim princip subspac dimens initi line set equal last slice number suffici get number column larger line matric p-truncat svd decomposit comput line via procedur describ algorithm use defin new matrix concaten last unus slice when slice process current matric return procedur describ algorithm reduc comput burden comput p-truncat svd decomposit input matrix via kernel matrix line number row larger number column otherwis covari matrix use line case p-truncat svd decomposit implement via qr decomposit indirect svd procedur describ algorithm this allow reduc comput time when larg matric must process final matric kernel covari matric squar singular valu return use strategi process slice revers order sinc move versus column larger indic rank well norm slice becom smaller smaller thus give less less contribut princip subspac dimens this reduc approxim error cumul drop compon comput as a final remark stress sinc comput approxim solut princip direct make no much sens solv problem given eq learn quick compens approxim and/or sub-optim a obtain matric return algorithm thus matric use experi describ next section algorithm approxim compon function svf ig data nstart dp/ke number start slice nslice columns/k nstart number remain slice nslice column comput start slice revers rang nslice comput remain slice end return end function algorithm kernel vs covari comput algorithm truncat svd qr function function indirect svd m if m.row column then mmt vr ut svd r ssqr indirect svd k qvr els mt ssqr ut indirect svd c ut ut return ut mut ssqr2 end function end if return ssqr end function experi order evalu pre-train approach decid use four polyphon music sequenc dataset use assess predict abil rnn-rbm model predict task consist predict note play at time given sequenc note play till time rnn-rbm model achiev state-of-the-art demand predict task as perform measur adopt accuraci measur use describ dataset split train set valid set test set statist dataset includ largest sequenc length given column tabl sequenc dataset repres a song a maximum polyphoni note averag each time step input span whole rang piano a0 c8 repres use binari valu pre-train approach pret-rnn assess use a differ number hidden unit set turn epoch rnn training1 use theano-bas stochast gradient descent softwar avail at random initialis rnd also use network number hidden unit specif network hidden unit evalu perform differ random initialis final in order verifi nonlinear introduc rnn actual use solv predict task also evalu perform a network linear unit hidden unit initialis pre-train procedur give idea time perform pre-train respect train a rnn in column tabl we report time in second need comput pre-train matric intel xeon cpu gb perform train a rnn epoch gpu nvidia pleas note larger valu increas in comput time pre-train smaller increment in comput time need train a rnn due earli overfit muse dataset we use epoch dataset nottingham piano-midi.d musedata jsb choral set train test valid train test valid train test valid train test valid sampl max length pre- train time second epoch second epoch second epoch second epoch model rnn hf rnn-rbm pret-rnn pret-lin250 rnn hf rnn-rbm pret-rnn pret-lin250 rnn hf rnn-rbm pret-rnn pret-lin250 rnn hf rnn-rbm pret-rnn pret-lin250 acc tabl dataset statist includ data matrix size train set column comput time in second perform pre-train train epoch column accuraci result state-of-the-art model vs pre-train approach column acronym hf use identifi rnn train hessian free optim train test curv model describ report in figur it evid random initialis allow rnn improv perform in a reason amount epoch specif random initialis rnd we report averag rang variat differ trail differ initi point chang substanti perform rnn increas number hidden unit allow rnn slight increas perform use pre-train hand allow rnn start train a quit favour point as demonstr earli sharp improv perform moreov more hidden unit use more improv in perform obtain till overfit observ in particular earli overfit occur muse dataset it notic linear model linear reach perform in case better rnn without pre-train howev it import notic it achiev good result train set jsb piano-midi correspond perform test set poor show a clear evid overfit final in column tabl we report accuraci obtain valid number hidden unit number epoch approach pret-rnn pret-lin250 versus result report in rnn also use hessian free optim rnn-rbm in case use pre-train larg improv perform standard rnn or without hessian free optim moreov except nottingham dataset propos approach outperform state-of-the-art result achiev rnn-rbm larg improv observ muse jsb dataset perform nottingham dataset basic equival one obtain rnn-rbm this dataset also linear model pre-train achiev quit good result seem suggest predict task this dataset much easier dataset linear model outperform rnn without pre-train nottingham jsb dataset show problem muse dataset conclus we propos a pre-train techniqu rnn base linear autoencod sequenc this kind autoencod it possibl give a close form solut definit optim weight howev entail comput the svd decomposit the full data matrix larg data matric exact svd decomposit achiev so we propos a comput effici procedur get approxim turn effect our goal experiment result for a predict task dataset sequenc polyphon music show the use the propos pre-train approach sinc it allow larg improv the state the art result the consid dataset by use simpl stochast gradient descend for learn even if the result encourag the method need assess data applic domain moreov it interest understand whether the analysi perform in linear deep network for vector extend recurr architectur for sequenc in particular to our method rnd trial linear rnd pret pret nottingham test set epoch accuraci accuraci pret pret pret nottingham train set epoch epoch piano-midi.d train set piano-midi.d test set accuraci epoch epoch muse dataset train set muse dataset test set accuraci accuraci epoch epoch jsb choral train set jsb choral test set accuraci rnd rnd rnd accuraci accuraci accuraci epoch epoch figur train left column test right column curv for the assess approach the four dataset curv sampl at each epoch till epoch at step epoch afterward
----------------------------------------------------------------

title: 5954-convolutional-networks-on-graphs-for-learning-molecular-fingerprints.pdf

convolut network graph learn molecular fingerprint david duvenaud dougal maclaurin jorg aguilera-iparraguirr rafael g omez-bombarelli timothi hirzel al aspuru-guzik ryan p. adam harvard univers abstract introduc convolut neural network oper direct graph network allow end-to-end learn predict pipelin whose input graph arbitrari size shape architectur present general standard molecular featur extract method base circular fingerprint show data-driven featur interpret better predict perform varieti task introduct recent work materi design use neural network predict properti novel molecul general exampl one difficulti task input predictor molecul arbitrari size shape current machin learn pipelin handl input fix size current state art use off-the-shelf fingerprint softwar comput fixed-dimension featur vector use featur input fully-connect deep neural network standard machin learn method formula follow dure train molecular fingerprint vector treat fix paper replac bottom layer stack function comput molecular fingerprint vector differenti neural network whose input graph repres origin molecul graph vertic repres individu atom edg repres bond lower layer network convolut sens local filter appli atom neighborhood sever layer global pool step combin featur atom molecul neural graph fingerprint offer sever advantag fix fingerprint predict perform use data adapt task hand machine-optim fingerprint provid substanti better predict perform fix fingerprint show neural graph fingerprint match beat predict perform standard fingerprint solubl drug efficaci organ photovolta effici dataset parsimoni fix fingerprint must extrem larg encod possibl substructur without overlap exampl use fingerprint vector size after remov rarely-occur featur differenti fingerprint optim encod relev featur reduc downstream comput regular requir interpret standard fingerprint encod possibl fragment complet distinct notion similar fragment contrast featur neural graph fingerprint activ similar distinct molecular fragment make featur represent meaning equal contribut figur left visual represent comput graph standard circular fingerprint neural graph fingerprint first graph construct match topolog molecul fingerprint node repres atom edg repres bond layer inform flow neighbor graph final node graph turn one bit fixed-length fingerprint vector right detail sketch includ bond inform use oper circular fingerprint state art molecular fingerprint extended-connect circular fingerprint ecfp circular fingerprint refin morgan algorithm design encod substructur present molecul way invari atom-relabel circular fingerprint generat layer featur appli fix hash function concaten featur neighborhood previous layer result hash treat integ indic written fingerprint vector index given featur vector node graph figur left show sketch comput architectur ignor collis index fingerprint denot presenc particular substructur size substructur repres index depend depth network thus number layer refer radius fingerprint circular fingerprint analog convolut network appli oper local everywher combin inform global pool step creat differenti fingerprint space possibl network architectur larg spirit start known-good configur design differenti general circular fingerprint section describ replac discret oper circular fingerprint differenti analog hash purpos hash function appli layer circular fingerprint combin inform atom neighbor substructur ensur chang fragment matter small lead differ fingerprint index activ replac hash oper singl layer neural network use smooth function allow activ similar local molecular structur vari unimport way index circular fingerprint use index oper combin node featur vector singl fingerprint whole molecul node set singl bit fingerprint one index determin hash featur vector pooling-lik oper convert arbitrary-s graph fixed-s vector small molecul larg fingerprint length fingerprint alway spars use softmax oper differenti analog index essenc atom ask classifi belong singl categori sum classif label vector produc final fingerprint oper analog pool oper standard convolut neural network algorithm circular fingerprint input molecul radius fingerprint length initi fingerprint vector 0s atom molecul ra lookup atom featur layer atom molecul r1 rn neighbor ra r1 rn concaten ra hash v hash function mod ra convert index fi write index return binari vector algorithm neural graph fingerprint input molecul radius hidden weight hr output weight w1 wr initi fingerprint vector 0s atom molecul ra lookup atom featur layer atom molecul r1 rn pneighbor ra sum ri ra vhln smooth function softmax ra wl sparsifi add fingerprint return real-valu vector figur pseudocod circular fingerprint left neural graph fingerprint right differ highlight blue everi non-differenti oper replac differenti analog canonic circular fingerprint ident regardless order atom neighborhood invari achiev sort neighbor atom accord featur bond featur experi sort scheme also appli local featur transform possibl permut local neighborhood altern canonic appli permutation-invari function summat interest simplic scalabl chose summat circular fingerprint interpret special case neural graph fingerprint larg random weight limit larg input weight tanh nonlinear approach step function concaten form simpl hash function also limit larg input weight softmax oper approach one-hot-cod argmax oper analog index oper algorithm summar two algorithm highlight differ given fingerprint length featur layer paramet neural graph fingerprint consist separ output weight matrix size layer well set hidden-to-hidden weight matric size layer one possibl number bond atom organ molecul experi ran two experi demonstr neural fingerprint larg random weight behav similar circular fingerprint first examin whether distanc circular fingerprint similar distanc neural fingerprint-bas distanc figur left show scatterplot pairwis distanc circular neural fingerprint fingerprint length calcul pair molecul solubl dataset distanc measur use continu general tanimoto jaccard similar measur given distanc x min xi max xi correl distanc line point right plot show pair molecul binari ecfp fingerprint exact zero overlap second examin predict perform neural fingerprint larg random weight circular fingerprint figur right show averag predict perform solubl dataset use linear regress top fingerprint perform method follow similar curv contrast perform neural fingerprint small random weight follow differ curv substanti better suggest even random weight relat smooth activ neural fingerprint help general perform rmse log mol/l neural fingerprint distanc neural vs circular distanc circular fingerprint random conv larg paramet random conv small paramet circular fingerprint distanc fingerprint radius figur left comparison pairwis distanc molecul measur use circular fingerprint neural graph fingerprint larg random weight right predict perform circular fingerprint neural graph fingerprint fix larg random weight green neural graph fingerprint fix small random weight blue perform neural graph fingerprint larg random weight close match perform circular fingerprint examin learn featur demonstr neural graph fingerprint interpret show substructur activ individu featur fingerprint vector featur circular fingerprint vector activ singl fragment singl radius except accident collis contrast neural graph fingerprint featur activ variat structur make interpret allow shorter featur vector solubl featur figur show fragment maxim activ predict featur fingerprint fingerprint network train input linear model predict solubl measur featur shown top row posit predict relationship solubl activ fragment contain hydrophil r-oh group standard indic solubl featur shown bottom row strong predict insolubl activ non-polar repeat ring structur fragment activ pro-solubl featur oh nh oh oh fragment activ anti-solubl featur figur examin fingerprint optim predict solubl shown repres exampl molecular fragment highlight blue activ differ featur fingerprint top row featur predict solubl bottom row featur predict insolubl toxic featur we train model architectur predict toxic measur two differ dataset figur show fragment maxim activ featur predict toxic two separ dataset fragment activ toxic featur sr-mmp dataset fragment activ toxic featur nr-ahr dataset figur visual fingerprint optim predict toxic shown repres sampl molecular fragment highlight red activ featur predict toxic top row predict featur identifi group contain sulphur atom attach aromat ring bottom row predict featur identifi fuse aromat ring also known polycycl aromat hydrocarbon well-known carcinogen construct similar visual semi-manu way determin toxic fragment activ given neuron search hand-mad list toxic substructur chose one correl given neuron contrast visual generat automat without need restrict rang possibl answer beforehand predict perform we ran sever experi compar predict perform neural graph fingerprint standard state-of-the-art setup circular fingerprint fed fully-connect neural network experiment setup pipelin take input smile string encod molecul convert graph use rdkit we also use rdkit produc extend circular fingerprint use baselin hydrogen atom treat implicit convolut network initi atom bond featur chosen similar use ecfp initi atom featur concaten one-hot encod atom element degre number attach hydrogen atom implicit valenc aromat indic bond featur concaten whether bond type singl doubl tripl aromat whether bond conjug whether bond part ring train architectur train use batch normal we also experi tanh vs relu activ function neural fingerprint network layer fullyconnect network layer relu slight consist perform advantag valid set we also experi dropconnect variant dropout weight random set zero instead hidden unit found led wors valid error general experi optim minibatch size use adam algorithm variant rmsprop includ momentum hyperparamet optim optim hyperparamet we use random search hyperparamet method optim use trial cross-valid fold follow hyperparamet optim log learn rate log initi weight scale log penalti fingerprint length fingerprint depth size hidden layer fully-connect network addit size hidden featur vector convolut neural fingerprint network optim dataset unit predict mean circular fps linear layer circular fps neural net neural fps linear layer neural fps neural net solubl log mol/l drug efficaci nm photovolta effici percent tabl mean predict accuraci neural fingerprint compar standard circular fingerprint dataset we compar perform standard circular fingerprint neural graph fingerprint varieti domain solubl aqueous solubl molecul measur drug efficaci half-maxim effect concentr vitro molecul sulfide-resist strain p. falciparum parasit caus malaria measur organ photovolta effici harvard clean energi project use expens dft simul estim photovolta effici organ molecul we use subset molecul dataset predict accuraci we compar perform circular fingerprint neural graph fingerprint two condit first condit predict made linear layer use fingerprint input second condit predict made one-hidden-lay neural network use fingerprint input set differenti paramet compos model optim simultan result summar tabl experi neural graph fingerprint match beat accuraci circular fingerprint method neural network top fingerprint typic outperform linear layer softwar automat differenti softwar packag theano signific speed develop time provid gradient automat handl limit control structur index sinc we requir relat complex control flow index order implement variant algorithm we use flexibl automat differenti packag python call autograd github.com/hips/autograd this packag handl standard numpi code differenti code contain loop branch index code comput neural fingerprint produc visual avail github.com/hips/neural-fingerprint limit comput cost neural fingerprint asymptot complex number atom depth network circular fingerprint addit term due matrix multipli necessari transform featur vector step precis comput neural fingerprint depth fingerprint length molecul atom use molecular convolut net featur layer cost o rn rn practic train neural network top circular fingerprint usual took sever minut train fingerprint network top took order hour larger dataset limit comput layer how complic we make function goe one layer network next in this paper we chose simplest feasibl architectur singl layer neural network howev may fruit appli multipl layer nonlinear message-pass step in make inform preserv easier adapt long short-term memori architectur pass inform upward limit inform propag across graph local message-pass architectur develop in this paper scale well in size graph due low degre organ molecul abil propag inform across graph limit depth network this may appropri small graph repres small organ molecul use in this paper howev in worst case take depth n2 network distinguish graph size avoid this problem propos hierarch cluster graph substructur tree-structur network could examin structur entir graph use log n layer would requir learn pars molecul techniqu natur languag process might fruit adapt this domain inabl distinguish stereoisom special bookkeep requir distinguish stereoisom includ enantom mirror imag molecul cis/tran isom rotat around doubl bond circular fingerprint implement option make distinct neural fingerprint could extend sensit stereoisom this remain task futur work relat work this work similar in spirit neural ture machin in sens we take exist discret comput architectur make part differenti in order gradient-bas optim neural net quantit structure-act relationship qsar modern standard predict properti novel molecul compos circular fingerprint fully-connect neural network regress method use circular fingerprint input ensembl neural network gaussian process random forest use circular fingerprint depth input multitask neural network show multipl task help perform neural graph fingerprint most close relat work build neural network graph-valu input approach remov cycl build graph tree structur choos one atom root recurs neural network run leav the root produc fixed-s represent becaus graph node possibl root possibl graph construct the final descriptor sum the represent comput distinct graph there mani distinct graph there atom in the network the comput cost this method thus grow o f the size the featur vector the number atom make less suitabl larg molecul convolut neural network convolut neural network use model imag speech time seri howev standard convolut architectur use fix comput graph make difficult appli object vari size structur molecul more recent other develop convolut neural network architectur model sentenc vari length neural network fix graph introduc convolut network graph in the regim the graph structur fix train exampl differ in differ featur at the vertic the graph in contrast our network address the situat train input differ graph neural network input-depend graph propos neural network model graph interest train procedur the forward pass consist run message-pass scheme equilibrium fact allow the reverse-mod gradient comput without store the entir forward comput they appli network predict mutagenesi molecular compound well web page rank also propos neural network model graph learn scheme whose inner loop optim the train loss rather the correl each newly-propos vector the train error residu they appli model dataset boil point molecular compound our paper build idea the follow differ our method replac complex train algorithm simpl gradientbas optim general exist circular fingerprint comput appli network in the context modern qsar pipelin use neural network top the fingerprint increas model capac unrol infer algorithm other note iter infer procedur sometim resembl the feedforward comput a recurr neural network one natur extens idea to parameter each infer step train a neural network to approxim match the output exact infer use a small number iter the neural fingerprint view in this light resembl an unrol message-pass algorithm the origin graph conclus we general exist hand-craft molecular featur to allow optim divers task by make each oper in the featur pipelin differenti we use standard neural-network train method to scalabl optim the paramet these neural molecular fingerprint end-toend we demonstr the interpret predict perform these new fingerprint data-driven featur alreadi replac hand-craft featur in speech recognit machin vision natural-languag process carri the task for virtual screen drug design materi design a natur next step acknowledg we thank edward pyzer-knapp jennif wei samsung advanc institut technolog for their support this work partial fund by nsf
----------------------------------------------------------------

title: 5961-neural-adaptive-sequential-monte-carlo.pdf

neural adapt sequenti mont carlo shixiang zoubin ghahramani richard e. turner univers cambridg depart engin cambridg uk mpi intellig system ubingen germani sg717 cam.ac.uk zoubin eng.cam.ac.uk ret26 cam.ac.uk abstract sequenti mont carlo particl filter popular class method sampl intract target distribut use sequenc simpler intermedi distribut like import sampling-bas method perform critic depend propos distribut bad propos lead arbitrarili inaccur estim target distribut paper present new method automat adapt propos use approxim kullback-leibl diverg true posterior propos distribut method flexibl applic parameter propos distribut support onlin batch variant use new framework adapt power propos distribut rich parameter base upon neural network lead neural adapt sequenti mont carlo nasmc experi indic nasmc signific improv infer non-linear state space model outperform adapt propos method includ extend kalman unscent particl filter experi also indic improv infer translat improv paramet learn nasmc use subroutin particl margin metropoli hast final show nasmc abl train latent variabl recurr neural network lv-rnn achiev result compet state-of-the-art polymorph music model nasmc seen bridg gap adapt smc method recent work scalabl black-box variat infer introduct sequenti mont carlo smc class algorithm draw sampl target distribut interest sampl seri simpler intermedi distribut specif sequenc construct propos import sampl smc particular well-suit perform infer non-linear dynam model hidden variabl sinc filter natur decompos sequenc mani case state-of-the-art infer method general speak infer method use modul paramet learn system smc use way approxim maximum-likelihood paramet learn bayesian approach recent develop particl mcmc method critic common import sampl method perform smc strong depend choic propos distribut propos well-match target distribut method produc sampl low effect sampl size lead mont carlo estim patholog high varianc smc communiti develop approach mitig limit resampl improv particl divers effect sampl size low appli mcmc transit kernel improv particl divers complementari line research leverag distribut approxim infer method extend kalman filter unscent kalman filter construct better propos lead extend kalman particl filter ekpf unscent particl fil1 ter upf general howev construct good propos distribut still open question sever limit applic smc method paper propos new gradient-bas black-box adapt smc method automat tune flexibl propos distribut qualiti propos distribut assess use intract kullback-leibl diverg target distribut parametr propos distribut approxim deriv object use sampl deriv smc framework general tractabl handl complex parametr propos distribut exampl use neural network carri parameter therebi leverag larg literatur effici comput tool develop communiti demonstr method effici learn good propos distribut signific outperform exist adapt propos method includ ekpf upf standard benchmark model use particl filter communiti show improv perform smc algorithm translat improv mix particl margin metropolis-hast pmmh final show method allow higher-dimension complic model accur handl use smc parametr use neural network challeng tradit particl filter method focus work improv smc mani idea inspir burgeon literatur approxim infer unsupervis neural network model connect explor section sequenti mont carlo begin briefli review two fundament smc algorithm sequenti import sampl sis sequenti import resampl consid probabilist model compris possibl multi-dimension hidden observ state respect whose joint disqt tribut factor p zt p xt general form subsum common state-spac model hidden markov model hmms well non-markovian model hidden state gaussian process goal sequenti import sampler approxim posterior distribut pn hidden state sequenc weight set sampl trajectori drawn simpler propos distribut form propos distribut use principl particular conveni one take qt factoris true posterior q zt filter depend short deriv supplementari materi show normal import weight defin recurs p zt p xt w z w z q zt sis eleg sampl weight comput sequenti fashion use singl forward pass howev na implement suffer sever patholog distribut import weight often becom high skew increas mani sampl attain low weight allevi problem sequenti import resampl sir algorithm add addit step resampl zt time multinomi distribut given w z give new particl equal weight.1 replac degener particl low weight sampl substanti import weight without violat valid method sir requir knowledg full trajectori previous sampl stage draw sampl comput import weight reason carri resampl new particl need updat ancestri inform let repres ancestr index particl time state collect set zt z1 tt resampl trajectori denot zt final lighten notat use shorthand advanc implement resampl effect sampl size fall threshold wt weight note employ resampl depend previous weight sinc resampl given previous particl uniform weight implement smc given algorithm supplementari materi critic role propos distribut sequenti mont carlo choic propos distribut smc critic even employ resampl step poor propos distribut produc trajectori trace backward quick collaps onto singl ancestor clear repres poor approxim true posterior effect mitig increas number particl and/or appli complex addit mcmc move strategi increas comput cost conclus propos chosen care optim choic unconstrain propos access observ data time intract posterior distribut given restrict impos factor becom q zt p zt still typic intract bootstrap filter instead use prior q zt p zt often tractabl fail incorpor inform current observ halfway-hous employ distribut approxim infer techniqu approxim p zt exampl includ ekpf upf howev method suffer three main problem first extend unscent kalman filter method deriv known inaccur poor behav mani problem outsid smc set second approxim must appli sampl sampl basi lead signific addit comput overhead third neither approxim tune use smc-relev criterion next section introduc new method adapt propos address limit adapt propos descend inclus kl diverg work qualiti propos distribut optim use inclus kl-diverg true posterior distribut propos kl p paramet made explicit sinc short interest adapt propos learn model object chosen four main reason first direct measur qualiti propos unlik typic use effect sampl size second true posterior lie class distribut attain propos famili object global optimum point third if true posterior lie within class kl diverg tend find propos distribut higher entropi origin advantag import sampl exclus kl unsuit reason fourth deriv object approxim effici use sampl base approxim describ gradient negat kl diverg respect paramet propos distribut take simpl form kl p log expect posterior approxim use sampl smc one option would use weight sampl trajectori final time-step smc although asymptot unbias estim would high varianc due collaps trajectori altern reduc varianc cost introduc bias use intermedi ancestr tree filter approxim supplementari materi detail kl p log zt simplic propos approach bring sever advantag opportun onlin batch variant sinc deriv distribut time trivial appli updat onlin way updat propos distribut everi time-step altern learn paramet batch set might appropri updat propos paramet make full forward pass smc conveni perform approxim maximum-likelihood learn gradient updat model paramet effici approxim use sampl particl smc supplementari materi algorithm similar deriv maximum likelihood learn also discuss log p log zt algorithm stochast gradient adapt smc batch infer learn variant requir propos model observ number particl repeat nextminibatch x ptj log zt ptj log zt option optim optim option converg effici adapt propos contrast epf upf new method employ analyt function propag requir cost particle-specif distribut approxim inner-loop similar although method bear similar assumed-dens filter adf minim local inclus kl new method advantag minim global cost requir particle-specif moment match train complex propos model adapt method describ appli parametr propos distribut special case previous treat propos relat arguabl straightforward general approach propos adapt next section describ rich famili propos distribut go beyond previous work base upon neural network approach enabl adapt smc method make use rich literatur optim tool avail supervis learn flexibl train one option train propos distribut use sampl smc deriv observ data howev approach exampl propos could train use data sampl generat model instead might mitig overfit effect small dataset similar train propos need one use generat sampl first place bootstrap filter complex variant use flexibl trainabl propos distribut use neural network propos adapt method appli parametr propos distribut here briefli describ util flexibl employ power neural network-bas parameter recent shown excel perform supervis sequenc learn task general speak applic these techniqu unsupervis sequenc model set activ research area still infanc work open new avenu wider research effort nutshel goal parameter zt propos stochast map previous hidden state observ includ current observ current hidden state zt flexibl comput effici trainabl way here use class function call long short-term memori lstm defin determinist map input sequenc output sequenc use parameter-effici recurr dynam allevi common vanish gradient problem recurr neural network distribut zt ht mixtur gaussian mixtur densiti network mdn mix proport mean covari parameteris anoth neural network supplementari detail lstm mdn neural network architectur experi goal experi three fold first evalu perform adapt method infer standard benchmark use smc communiti known ground truth second evalu perform smc use inner loop learn algorithm again use exampl known ground truth third appli smc learn complex model would normal challeng smc compar state-of-the-art approxim infer one way assess success propos method would evalu kl p z1 t howev quantiti hard accur comput instead use number metric experi ground truth state known evalu root mean squar error rmse approxim posterior mean latent variabl true valu rmse z1 t t1 zt z t general estim log-margin likelihood lml log log p xt log wt varianc also indic perform final also employ common metric call effect sampl size ess measur effect smc method ess particl time given esst if expect ess maxim equal number particl equival normal import weight uniform note ess alon suffici metric sinc measur absolut qualiti sampl rather relat qualiti infer benchmark nonlinear state-spac model order evalu effect adapt smc method test method standard nonlinear state-spac model often use benchmark smc algorithm model given posterior distribut high multi-mod due uncertainti sign latent state p zt zt p xt zt g zt zt2 experi investig new propos adapt method perform comparison standard method includ bootstrap filter ekpf ukpf particular interest follow question do rich multi-mod propos improv infer we compar gaussian propos diagon gaussian mixtur densiti network three compon recurr parameter propos help we compar non-recurr neural network hidden unit recurr neural network lstm unit inject inform prior dynam propos improv perform similar spirit variat method assess we parameter propos vt process nois instead zt let propos access prior dynam experi paramet non-linear state-spac model fix adapt propos perform sampl generat process iter result summar tabl supplementari materi addit result averag run time algorithm sequenc length bootstrap ekpf upf nn-nasmc rnn-nasmc ekpf upf implement provid although these number taken guid implement differ level acceler new adapt propos method signific outperform bootstrap ekpf upf method term ess rmse varianc lml estim multi-mod propos outperform simpl gaussian propos compar rnn-md-f rnn-f indic multi-mod propos improv perform moreov rnn outperform non-recurr nn compar rnn nn although propos model effect learn transit function inject inform prior dynam propos help compar rnn-f rnn interest clear cut winner ekpf upf although upf return lml estim lower varianc all method converg similar lmls close valu comput use larg number particl indic implement correct effect sampl size log margin likelihood ekpf nn-md prior rnn-f rnn-md-f rnn-md rnn upf ekp nn-m prio rnn md nn-m rnn upf rnn iter figur left box plot lml estim iter right averag ess first iter ess iter mean std prior ekpf upf rnn rnn-f rnn-md rnn-md-f nn-md lml mean std rmse mean std tabl left middl averag ess log margin likelihood estim last iter right rmse new sequenc adapt infer cart pole system second physic meaning system we consid cart-pol system consist invert pendulum rest movabl base system driven white nois input ode solver use simul system equat motion we consid problem infer true posit cart orient pendulum along deriv input nois noisi measur locat tip pole result present system signific intric model sec doe direct admit usag ekpf upf rnn-md propos model success learn good propos without ani direct access prior dynam ess rnn-md prior prior prior rad ess iter prior rnn-md ground-truth time time figur left normal ess iter middl right posterior mean ground-truth horizont locat cart chang relat angl pole rnn-md learn higher ess prior accur estim latent state prior rnn-md-f-pre rnn-md-f rnn-md-pre rnn-md iter iter figur pmmh sampl valu particl small number particl right pmmh slow burn mix propos prior distribut due larg varianc margin likelihood estim return bayesian learn nonlinear ssm smc often employ inner loop more complex algorithm one promin exampl particl markov chain mont carlo class method sampl joint posterior model paramet latent state trajectori here we consid particl margin metropolis-hast sampler pmmh context smc use construct propos distribut metropolis-hast accept/reject step propos form sampl propos set paramet perturb current paramet use gaussian random walk smc use sampl propos set latent state variabl result joint propos mh step use smc margin likelihood estim determin accept full detail given supplementari materi experi we evalu method pmmh sampler model section follow random walk propos use sampl prior set initi pmmh run iter two adapt model consid section use comparison rnn-md rnnmd-f model pre-train iter use sampl initi result shown typic rang paramet set given suffici number particl almost differ prior propos method howev number particl get smaller nasmc enabl signific faster burn-in posterior particular measur nois similar reason nasmc mix more quick limit nasmc-pmmh model need continu adapt global paramet sampl note still cost adapt particle-by-particl basi case ekpf upf polyphon music generat final new method use train latent variabl recurr neural network lv-rnn model four polymorph music dataset vari complex these dataset often use benchmark rnn model high dimension complex tempor depend involv differ time scale dataset contain least hour polyphon music averag polyphoni number simultan note lvrnn contain recurr neural network lstm layer driven stochast latent variabl zt at each time-point stochast output fed back dynam full detail supplementari materi both lstm layer generat propos model set unit adam use optim bootstrap filter compar new adapt method nasmc particl use train hyperparamet tune use valid set diagon gaussian output use in propos model an addit hidden layer size log likelihood test set standard metric comparison in generat model approxim use smc particl onli the prior propos compar sinc sec show the advantag our method ekpf/upf the result report in tabl the adapt method signific outperform the bootstrap filter three the four dataset the piano dataset the bootstrap method perform margin better in general the nlls the new method compar the state-of-the-art although detail comparison difficult the method stochast latent state requir approxim margin use import sampl smc dataset piano-midi-d nottingham musedata jsbchoral lv-rnn nasmc lv-rnn bootstrap storn sgvb fd-rnn srnn rnn-nade tabl estim negat log likelihood test data fd-rnn storn srnn rnn-nade result comparison variat infer the nasmc approach there sever similar nasmc variat free-energi method employ recognit model variat free-energi method refin an approxim the posterior distribut optimis the exclus variat kl-diverg kl q common approxim integr use sampl the approxim posterior this general approach similar in spirit the way the propos adapt in nasmc except the inclus kl-diverg employ kl p this entail sampl base approxim requir simul the true posterior critic nasmc use the approxim posterior a propos distribut construct a more accur posterior approxim the smc algorithm therefor seen as correct for the defici in the propos approxim we believ this lead signific advantag variat free-energi method especi in the time-seri set variat method known sever bias moreov use the inclus kl avoid comput the entropi the approxim distribut can prove problemat use complex approxim distribut mixtur heavi tail distribut in the variat framework there a close connect nasmc the wake-sleep algorithm the wake-sleep algorithm also employ the inclus kl diverg refin a posterior approxim recent general shown incorpor this idea import sampl in this context the nasmc algorithm extend this work smc conclus this paper develop a power method for adapt propos distribut within general smc algorithm the method parameteris a propos distribut use a recurr neural network model long-rang contextu inform allow flexibl distribut form includ mixtur densiti network enabl effici train stochast gradient descent the method found to outperform exist adapt propos mechan includ the ekpf upf a standard smc benchmark it improv burn in mix the pmmh sampler allow effect train latent variabl recurr neural network use smc we hope the connect smc neural network technolog inspir research adapt smc method in particular applic the method develop in this paper to adapt particl smooth high-dimension latent model adapt pmcmc for probabilist program particular excit avenu acknowledg sg generous support cambridge-t ubingen fellowship the alta institut jesus colleg cambridg ret thank the epsrc grant we thank theano develop for toolkit the author for releas the sourc code roger frigola sumeet singh fredrik lindsten thoma sch for help suggest on experi result for rnn-nade separ provid for
----------------------------------------------------------------

title: 5278-general-stochastic-networks-for-classification.pdf

general stochast network classif matthia z ohrer franz pernkopf signal process speech communic laboratori graz univers technolog matthias.zoehr tugraz.at pernkopf tugraz.at abstract extend generat stochast network supervis learn represent particular introduc hybrid train object consid generat discrimin cost function govern trade-off paramet use new variant network train involv nois inject walkback train joint optim multipl network layer neither addit regular constraint norm dropout variant pool convolut layer ad nevertheless abl obtain state-of-the-art perform mnist dataset without use permut invari digit outperform baselin model sub-vari mnist rectangl dataset signific introduct sinc boost machin learn due improv field unsupervis learn represent accomplish origin variant restrict boltzmann machin rbms auto-encod sparse-cod deep model represent learn also obtain impress result supervis learn problem speech recognit comput vision task if a-priori knowledg model architectur cf convolut layer pool layer generat pre-train network among best appli supervis learn task usual generat represent obtain greedy-layerwis train procedur call contrast diverg case network layer learn represent layer treat latter static input despit impress result achiev cd identifi two minor drawback use supervis learn first obtain represent pre-train network new discrimin model initi train weight split train two separ model seem neither biolog plausibl optim come optim care design earli stop criteria implement prevent under-fit second generat discrimin object might influenc benefici combin train cd take account work introduc new train procedur supervis learn represent particular defin hybrid train object general stochast network divid cost function generat discrimin part control trade-off paramet turn anneal solv unconstrain non-convex multi-object optim problem suffer shortcom describ abl obtain stateof-the-art perform mnist dataset without use permut invari digit signific outperform baselin model sub-vari mnist rectangl databas our approach relat generative-discrimin train approach rbms howev differ model new variant network train involv nois inject walkback train use joint optim multipl network layer most notabl appli addit regular constraint norm dropout variant unlock potenti possibl optim model extend learn multipl task time use joint train weight introduc multipl object might also open new prospect field transfer learn multi-task learn beyond classif paper organ follow section present mathemat background materi gsn hybrid learn criterion section empir studi influenc hyper paramet gsns present experiment result section conclud paper provid perspect futur work general stochast network recent new supervis learn algorithm call walkback train general autoencod gae introduc follow-up studi defin new network model generat stochast network extend idea walkback train multipl layer appli imag reconstruct abl outperform various baselin system due abil learn multi-mod represent paper extend work first provid mathemat background materi generat stochast network then introduc modif make model suitabl supervis learn particular present hybrid train object divid cost generat discrimin part pave way multi-object learn gsns also introduc new terminolog general stochast network model class includ generat discrimin hybrid stochast network variant general stochast network unsupervis learn restrict boltzmann machin rbm denois autoencod dae share follow common input distribut sampl converg markov chain case dae transit oper first sampl hidden state ht corrupt distribut generat reconstruct parametr model i.e densiti figur dae markov chain result dae markov chain shown figur defin input sampl fed chain time step reconstruct time step case gsn addit depend latent variabl ht time introduc network graph gsn markov chain defin follow figur show correspond network graph chain express determinist function random variabl particular densiti use model specifi independ nois sourc condit recov exact figur gsn markov chain introduc back-prob stochast non-linear form g nois process zt layer variabl activ unit iti bi weight matrix bias bi repres parametr distribut embed non-linear activ function input iti either realize xit observ sampl xti hidden realize hit hti general iti specifi upward path gsn specif layer case defin hti g downward path network ht use transpos weight matrix bias bi formul allow direct back-propag reconstruct log-likelihood paramet b0 bd number hidden layer figur gsn includ simpl hidden layer this extend multipl hidden layer requir multipl determinist function random variabl figur visual markov chain multi-lay gsn inspir unfold comput graph deep boltzmann machin gibb sampl process lt lt lt lt figur gsn markov chain multipl layer backprop- stochast unit train case altern even odd layer updat time inform propag upward downward step allow network build higher order represent exampl this updat process given figur even updat mark red odd updat mark blue case odd updat even updat case even updat odd updat cost function generat gsn written lt xt+k lt specif loss-funct mean squar error mse time step general arbitrari loss function could use long seen log-likelihood xt+k reconstruct input layer step optim loss function build sum cost multipl corrupt reconstruct call walkback train this form network train lead signific perform boost use input reconstruct network abl handl multi-mod input represent therefor consider favor standard generat model general stochast network supervis learn order make gsn suitabl supervis learn task introduc output network graph this case log log although target fed network it introduc addit cost term layer update-process stay lt lt lt lt lt lt figur gsn markov chain input target backprop- stochast unit defin follow cost function 3-layer gsn lt xt+k lt ht+k k=d generat discrimin this non-convex multi-object optim problem weight generat discrimin part c. paramet specifi number network layer depth network scale mean loss mandatori allow equal balanc loss term input target scale rang again figur show correspond network graph supervis learn red blue edg denot even odd network updat general hybrid object optim criterion restrict hx addit input output term could introduc network this setup might use transfer-learn multi-task scenario discuss this paper experiment result in order evalu capabl gsns supervis learn studi mnist digit variant mnist digit rectangl dataset first databas consist label train label test imag handwritten digit second dataset includ variant mnist digit mnist-bas mnist-rot mnist-back-rand mnist-back-imag mnist-rot-back-imag addit factor variat ad origin data variant includ label train label valid label test imag third dataset involv two subset rectangl rectangle-imag dataset rectangl consist label train label valid label test imag dataset rectangleimag includ label train label valid label test imag in first experi focus multi-object optim problem defin in next evalu number walkback step in gsn necessari converg in third experi analyz influenc differ gaussian nois set walkback train improv general capabl network final summar classif result dataset compar baselin system multi-object optim in hybrid learn setup in order solv non-convex multi-object optim problem variant stochast gradient descent sgd use appli search fix valu problem furthermor show use anneal factor train work best in practic in experi three layer gsn neuron in each layer random initi small gaussian nois mse loss function input target use regard optim appli sgd learn rate momentum term multipl anneal factor per epoch learn rate rectifi unit chosen activ function follow idea explicit sampl appli input output layer in test case zero-on loss comput averag network output walkback step analysi hybrid learn paramet concern influenc trade-off paramet test fix valu in rang low valu emphas discrimin part in object vice versa walkback train step use zero-mean pre postactiv gaussian nois zero mean varianc perform train epoch in dynam scenario anneal reach within epoch simul generat pre-train certain extend figur influenc dynam static mnist variant basic left rotat middl background right denot train valid test-set dash line denot static setup bold line dynam setup figur compar result gsns use static dynam setup mnist variant basic rotat background use dynam anneal achiev best valid test error in experi in this case attent given generat proport object in earli stage train after approxim epoch discrimin train fine-tun domin this setup close relat dbn train emphasi optim begin optim wherea import last stage in case gsn anneal achiev smooth transit shift weight in optim criterion within one model analysi walkback step in next experi test influenc walkback step gsns figur show result differ gsns train walkback step anneal in case inform least propag downward in layer network use fix gaussian pre post-activ nois figur evalu number walkback step mnist variant basic left rotat middl background right denot train valid test-set figur show increas walkback step improv general capabl use gsns setup suffici converg achiev best valid test result in experi analysi pre post-activ nois inject nois train process gsns serv regular improv general capabl model in this experi influenc gaussian pre post-activ nois deactiv nois train test gsn-3 train walkback step trade-off factor anneal figur summar result differ gsns mnist variant basic rotat background set achiev best overal result valid test-set three experi in case gsns either underfit data figur evalu nois inject train mnist variant basic left rotat middl background right denot train valid test-set mnist result tabl present averag classif error three run mnist variat dataset obtain use fix gaussian pre post-activ nois walkback step hybrid learn paramet anneal small grid test perform in rang neuron per layer layer find optim network configur dataset svmrbf svmpoli nnet dbn-1 saa-3 dbn-3 gsn-3 mnist-bas mnist-rot mnist-back-rand mnist-back-imag mnist-rot-back-imag rectangl rectangles-imag tabl mnist variat recangl result dataset mark updat result shown tabl show three layer gsn clear outperform model except mnist random-background dataset in particular when compar gsn-3 radial basi function support vector machin svmrbf second best model mnist basic gsn-3 achiev relat improv test set mnist rotat dataset gsn-3 abl beat second best model test set mnist rotatedbackground relat improv test set second best model result statist signific regard number model paramet although we direct compar model in term network paramet it worth mention far smaller grid test use generat result gsns cf when compar classif error gsn-3 train without nois obtain in previous experi tabl gsn-3 achiev test error mnist variant basic outperform all model this task mnist variant rotat gsn-3 also outperform obtain test error this indic gaussian regular in the walkback train improv the general capabl the network also the hybrid train criterion the gsn tabl list the result the mnist dataset without addit affin transform appli the data permut invari digit three layer gsn achiev the state-of-the-art test error network result rectifi mlp dropout dbm maxout mlp dropout mp-dbm deep convex network manifold tangent classifi dbm dropout gsn-3 tabl mnist result it might worth note in addit the nois process in walkback train regular norm dropout variant use in the gsns in general train epoch early-stop necessari gsn train all simulations1 execut on gpu the help the mathemat express compil theano conclus futur work we extend gsns classif problem in particular we defin an hybrid multiobject train criterion gsns divid the cost function generat discrimin part this render the need generat pre-train unnecessari we analyz the influenc the object trade-off paramet empir show anneal we outperform static choic furthermor we discuss effect nois inject sampl step walkback train conserv start point we restrict the model use rectifi unit neither addit regular constraint as norm dropout variant pool convolut layer ad nevertheless the gsn abl outperform various baselin system in particular deep belief network a multi layer perceptron a support vector machin svm a stack auto-associ on variant the mnist dataset furthermor we also achiev state-of-the-art perform on the origin mnist dataset without permut invari digit the model converg faster in term train iter also show better general behavior in most case our approach open a wide field new applic gsns in futur research we explor adapt nois inject method for gsns non-convex multi-object optim strategi
----------------------------------------------------------------

title: 4443-algorithms-for-hyper-parameter-optimization.pdf

algorithm hyper-paramet optim r emi bardenet laboratoir de recherch en informatiqu universit e paris-sud bardenet lri.fr jame bergstra rowland institut harvard univers bergstra rowland.harvard.edu yoshua bengio ept d informatiqu recherch op erationell universit e de montr eal yoshua.bengio umontreal.ca bal az k egl linear acceler laboratori universit e paris-sud cnrs balazs.kegl gmail.com abstract sever recent advanc state art imag classif benchmark come better configur exist techniqu rather novel approach featur learn tradit hyper-paramet optim job human effici regim trial possibl present comput cluster gpu processor make possibl run trial show algorithm approach find better result present hyper-paramet optim result task train neural network deep belief network dbns optim hyper-paramet use random search two new greedi sequenti method base expect improv criterion random search shown suffici effici learn neural network sever dataset show unreli train dbns sequenti algorithm appli difficult dbn learn problem find signific better result best previous report work contribut novel techniqu make respons surfac model mani element hyper-paramet assign known irrelev given particular valu element introduct model deep belief network dbns stack denois autoencod convolut network well classifi base sophist featur extract techniqu ten perhap fifti hyper-paramet depend experiment choos parametr model mani hyper-paramet experiment choos fix reason default difficulti tune model make publish result difficult reproduc extend make even origin investig method art scienc recent result demonstr challeng hyper-paramet optim larg multilay model direct impedi scientif progress work advanc state art perform imag classif problem concert hyper-paramet optim simpl algorithm rather innov model machin learn strategi would wrong conclud result featur learn useless instead hyper-paramet optim regard formal outer loop learn process learn algorithm function data classifi take classif problem exampl includ budget choic mani cpu cycl spent hyper-paramet explor mani cpu cycl spent evalu hyperparamet choic tune regular paramet result suggest current generat hardwar larg comput cluster gpus optim alloca1 tion cpu cycl includ hyper-paramet explor typic machin learn literatur hyper-paramet optim problem optim loss function graph-structur configur space work restrict tree-structur configur space configur space tree-structur sens leaf variabl number hidden unit 2nd layer dbn well-defin node variabl discret choic mani layer use take particular valu must hyper-paramet optim algorithm optim variabl discret ordin continu must simultan choos variabl optim work defin configur space generat process draw valid sampl random search algorithm draw hyper-paramet assign process evalu optim algorithm work identifi hyper-paramet assign could drawn appear promis basi loss function valu point paper make two contribut random search competit manual optim dbns automat sequenti optim outperform manual random search section cover sequenti model-bas optim expect improv criterion section introduc gaussian process base hyper-paramet optim algorithm section introduc second approach base adapt parzen window section describ problem dbn hyper-paramet optim show effici random search section show effici sequenti optim two hardest dataset accord random search paper conclud discuss result conclud remark section section sequenti model-bas global optim sequenti model-bas global optim smbo algorithm use mani applic evalu fit function expens applic true fit function cost evalu model-bas algorithm approxim surrog cheaper evalu typic inner loop smbo algorithm numer optim surrog transform surrog point maxim surrog transform becom propos true function evalu active-learning-lik algorithm templat summar figur smbo algorithm differ criterion optim obtain given model surrog model via observ histori h. smbo m0 argminx evalu expens step fit new model mt h. return figur pseudo-cod generic sequenti model-bas optim algorithm work optim criterion expect improv criteria suggest probabl improv expect improv minim condit entropi minim bandit-bas criterion describ chose use ei criterion work intuit shown work well varieti set leav systemat explor improv criteria futur work expect improv expect model rn exceed negat threshold eiy max contribut work two novel strategi approxim model hierarch gaussian process tree-structur parzen estim describ section section respect gaussian process approach gaussian process long recogn good method model loss function model-bas optim literatur gaussian process gps prior function close sampl mean prior distribut believ gp mean kernel condit distribut know sampl valu also gp whose mean covari function analyt deriv gps generic mean function principl use simpler suffici purpos consid zero mean process center function valu consid data set model linear trend gp mean lead undesir extrapol unexplor region smbo mention closed properti along fact gps provid assess predict uncertainti incorpor effect data scarciti make gp eleg candid find candid figur step fit model mt figur step runtim iter gp approach scale cubic linear number variabl optim howev expens function evalu typic domin even cubic cost optim ei gp model gp set best valu found observ min f model pm posterior gp know h. ei function encapsul compromis region mean function close better under-explor region uncertainti high ei function usual optim exhaust grid search input space latin hypercub search higher dimens howev inform landscap ei criterion deriv simpl comput alway non-neg zero train point inherit smooth kernel practic often least differenti notic ei criterion like high multi-mod especi number train point increas author use preced remark landscap ei design evolutionari algorithm mixtur search specif aim optim ei shown outperform exhaust search given budget ei evalu borrow approach go one step keep estim distribut eda approach discret part input space categor discret hyper-paramet sampl candid point accord binomi distribut use covari matrix adapt evolut strategi cma-e remain part input space continu hyper-paramet cma-e state-of-the-art gradient-fre evolutionari algorithm optim continu domain shown outperform gaussian search eda notic gradient-fre approach allow non-differenti kernel gp regress take use mixtur rather restart local search sever time start promis place use tessel suggest prohibit task often mean work dimens thus start local search center mass simplex vertic random pick among train point final remark hyper-paramet relev point exampl dbn one hidden layer paramet associ second third layer thus enough place one gp entir space hyper-paramet chose group hyper-paramet common use tree-lik fashion place differ independ gps group exampl dbns mean place one gp common hyper-paramet includ categor paramet indic condit group consid three gps paramet correspond three layer 1-dimension gps individu condit hyper-paramet like zca energi tabl dbn paramet tree-structur parzen estim approach tpe anticip hyper-paramet optim task mean high dimens small fit evalu budget turn anoth model strategi ei optim scheme smbo algorithm wherea gaussian-process base approach model direct strategi model recal introduct configur space describ graph-structur generat process first choos number dbn layer choos paramet tree-structur parzen estim tpe model transform generat process replac distribut configur prior non-parametr densiti experiment section see configu space describ use uniform log-uniform quantiz log-uniform categor variabl case tpe algorithm make follow replac uniform truncat gaussian mixtur log-uniform exponenti truncat gaussian mixtur categor re-weight categor use differ observ non-parametr densiti substitut repres learn algorithm produc varieti densiti configur space tpe defin use two densiti densiti form use observ correspond loss less densiti form use remain observ wherea gp-base approach favour quit aggress typic less best observ loss tpe algorithm depend larger best observ point use form tpe algorithm choos quantil observ valu p specif model necessari maintain sort list observ variabl runtim iter tpe algorithm scale linear linear number variabl dimens optim optim ei tpe algorithm parametr tpe algorithm chosen facilit optim ei eiy p y|x dy dy construct p p x|i p dy therefor p x|i p dy p dy p dy final eiy p dy last express show maxim improv would like point high probabl low probabl tree-structur form make easi draw mani candid accord evalu accord iter algorithm return candid greatest ei detail parzen estim model hierarch process involv discrete-valu continuousvalu variabl adapt parzen estim yield model place densiti vicin observ h. continu hyper-paramet specifi uniform prior interv gaussian log-uniform distribut tpe substitut equally-weight mixtur prior gaussian center b standard deviat gaussian set greater distanc left right neighbor clip remain reason rang case uniform point consid potenti neighbor discret variabl suppos prior vector probabl pi posterior vector element proport pi ci ci count occurr choic b log-uniform hyper-paramet treat uniform log domain tabl distribut dbn hyper-paramet random sampl option separ pre-process includ random seed weight equal symbol mean uniform mean gaussian-distribut log mean uniform distribut log-domain cd also known stand contrast diverg algorithm use initi layer paramet dbn whole model per-lay paramet prior paramet prior pre-process raw zca hidden unit log zca energi init a2 random seed choic algo text classifi learn rate log algo coef classifi anneal start log cd epoch log classifi penalti log cd learn rate log layer cd anneal start log batch size cd sampl data yes random search hyper-paramet optim dbns one simpl recent step toward formal hyper-paramet optim use random search show random search much effici grid search optim paramet one-lay neural network classifi section evalu random search dbn optim compar sequenti grid-assist manual search carri chose prior list tabl defin search space dbn configur detail dataset dbn model greedi layer-wis train procedur base cd provid prior correspond search space except follow differ we allow zca pre-process we allow layer differ size we allow layer train paramet cd we allow possibl treat continuous-valu data either bernoulli mean theoret correct bernoulli sampl typic cd algorithm we discret possibl valu real-valu hyper-paramet chang expand hyper-paramet search problem maintain origin hyper-paramet search space subset expand search space result preliminari random search figur perhap surpris result manual search reliabl match random trial sever dataset effici random search set explor random search result match human perform clear figur whether reason search origin space effici search larger space where good perform easier find object random search somehow cheat search larger space backward search space outlin tabl natur descript hyper-paramet optim problem restrict space presum made simplifi search problem make tractabl grid-search assist manual search critic method train dbns dataset result figur indic hyper-paramet optim harder dataset exampl case mnist rotat background imag dataset mrbi random sampl appear converg maximum relat quick best model among experi trial show littl varianc perform plateau lower found manual search anoth dataset convex random sampl procedur exceed perform manual search slow converg sort plateau consider varianc general best model select slow converg indic better perform probabl avail we need search configur space effici find remaind this paper explor sequenti optim strategi hyper-paramet optim two dataset convex mrbi sequenti search hyper-paramet optim dbns we valid gp approach section compar random sampl boston hous dataset regress task point made scale input variabl scalar mnist basic mnist background imag mnist rotat background imag accuraci accuraci accuraci experi size trial experi size trial accuraci accuraci accuraci rectangl imag experi size trial rectangl experi size trial convex experi size trial experi size trial figur deep belief network dbn perform accord random search random search use explor hyper-paramet tabl result found use grid-search-assist manual search similar domain averag trial given green 1-layer dbn red 3-layer dbn box-plot show distribut test set perform best model among random trial select dataset convex mnist rotat background imag use thorough hyper-paramet optim regress output we train multi-lay perceptron mlp hyper-paramet includ learn rate penalti size hidden layer number iter whether pca preprocess appli whose energi condit hyper-paramet result depict figur first iter made use random sampl we differenti random sampl gp approach train updat histori experi repeat time although number point particular small compar dimension surrog model approach find notic better point random support applic smbo approach ambiti task dataset appli gp problem optim dbn perform we allow random restart cma+e algorithm per propos iter conjug gradient method fit length scale gp squar exponenti kernel use everi node cma-e part gps dealt boundari use penalti method binomi sampl part dealt natur gp algorithm initi random sampl point h. after trial predict point use this gp took around second tpe-bas algorithm we chose pick best among candid drawn iter propos after trial predict point use this tpe algorithm took around second tpe allow grow past initi bound use random sampl cours optim wherea gp random search restrict stay within initi bound throughout cours optim tpe algorithm also initi random sampl point use seed gp parallel sequenti search gp tpe approach actual run asynchron order make use multipl comput node avoid wast time wait trial evalu complet gp approach so-cal constant liar approach use time candid point propos fake fit evalu equal mean within train set assign temporarili evalu complet report actual loss tpe approach we simpli ignor recent propos point reli stochast draw provid differ candid one iter next consequ parallel propos base less feedback this make search less effici though faster term wall time best valu far tpe gp manual random convex mrbi tabl test set classif error best model found search algorithm problem search algorithm allow trial manual search use trial convex trial mrbi time figur after time gp optim mlp hyper-paramet boston hous regress task best minimum found far everi iter time red gp blue random shade area one-sigma error bar runtim per trial limit hour gpu comput regardless whether execut gtx differ speed slowest fastest machin rough two-fold theori actual effici comput depend also load machin configur problem relat speed differ card differ differ hyper-paramet configur parallel evalu five propos gp tpe algorithm experi took hour wall time use five gpus discuss trajectori construct algorithm step illustr figur compar random search manual search carri general score best model found use algorithm other list tabl convex dataset classif algorithm converg valid score error general tpe best model error gp best tpe best signific better manual search random search trial mrbi dataset classif random search worst perform error gp approach manual search approxim tie error tpe algorithm found new best result error model found tpe algorithm particular better previous found one dataset gp tpe algorithm slight less effici manual search gp ei identifi perform par manual search within trial manual search use trial convex trial for mrbi there sever possibl reason for the tpe approach outperform the gp approach two dataset perhap the invers factor accur the the gaussian process perhap convers the explor induc the tpe lack accuraci turn good heurist for search perhap the hyper-paramet the gp approach set correct trade exploit explor the dbn configur space empir work requir test hypothes critic though four smbo run match exceed random search care human-guid search current the state the art method for hyper-paramet optim the gp tpe algorithm work well set there certain set these algorithm fact smbo algorithm general would expect well sequenti optim algorithm work leverag structur observ pair it possibl for smbo arbitrarili bad bad choic it also possibl slower random sampl find global optimum with appar good it extract structur lead local optimum conclus this paper introduc two sequenti hyper-paramet optim algorithm shown meet exceed human perform the perform brute-forc random search in two difficult hyper-paramet optim task involv dbns we relax standard constraint equal layer size layer the search space fall back more natur hyperparamet space variabl includ discret continu variabl in mani dataset convex dataset mnist rotat background imag manual gp tpe error fraction incorrect error fraction incorrect manual gp tpe time trial time trial figur effici gaussian process-bas graphic model-bas tpe sequenti optim algorithm the task optim the valid set perform dbn three layer the convex task left the mrbi task right the dot the element the trajectori produc smbo algorithm the solid colour line the valid set accuraci the best trial found point in time both the tpe gp algorithm make signific advanc random initi condit substanti outperform the manual random search method confid interv the best valid mean the convex task extend each point the mrbi task extend each point the solid black line the test set accuraci obtain domain expert use combin grid search manual search the dash line the quantil valid perform found among trial sampl prior distribut tabl estim random trial the two dataset respect variabl sometim irrelev depend on the valu paramet the number layer in this 32-dimension search problem the tpe algorithm present uncov new best result on both these dataset signific better dbns previous believ achiev moreov the gp tpe algorithm practic the optim for each dataset done in hour use five gpu processor although result for dbns method quit general extend natur hyper-paramet optim problem in the hyper-paramet drawn a measur set we hope our work may spur research in the machin learn communiti treat the hyperparamet optim strategi interest import compon learn algorithm the question how well a dbn on the convex task a fulli specifi empir answer question differ approach hyper-paramet optim give differ answer algorithm approach hyper-paramet optim make machin learn result easier dissemin reproduc transfer domain the specif algorithm we present also capabl least in case find better result previous known final power hyper-paramet optim algorithm broaden the horizon model realist studi research need restrict system a variabl readili tune hand the tpe algorithm present in this work well parallel evalu infrastructur avail as bsd-licens free open-sourc softwar design not reproduc the result in this work but also facilit the applic these similar algorithm other hyper-paramet optim problems.1 acknowledg this work support the nation scienc engin research council canada comput canada by the grant the french nation research agenc gpu implement the dbn model provid by theano hyperopt softwar packag https //github.com/jaberg/hyperopt
----------------------------------------------------------------

