query sentence: autonomous vehicles
---------------------------------------------------------------------
title: 95-alvinn-an-autonomous-land-vehicle-in-a-neural-network.pdf

305

ALVINN:
AN AUTONOMOUS LAND VEHICLE IN A
NEURAL NETWORK
Dean A. Pomerleau
Computer Science Department
Carnegie Mellon University
Pittsburgh, PA 15213

ABSTRACT
ALVINN (Autonomous Land Vehicle In a Neural Network) is a 3-layer
back-propagation network designed for the task of road following. Currently ALVINN takes images from a camera and a laser range finder as input
and produces as output the direction the vehicle should travel in order to
follow the road. Training has been conducted using simulated road images.
Successful tests on the Carnegie Mellon autonomous navigation test vehicle
indicate that the network can effectively follow real roads under certain field
conditions. The representation developed to perfOIm the task differs dramatically when the networlc is trained under various conditions, suggesting
the possibility of a novel adaptive autonomous navigation system capable of
tailoring its processing to the conditions at hand.

INTRODUCTION
Autonomous navigation has been a difficult problem for traditional vision and robotic
techniques, primarily because of the noise and variability associated with real world
scenes. Autonomous navigation systems based on traditional image processing and pattern recognition techniques often perform well under certain conditions but have problems
with others. Part of the difficulty stems from the fact that the processing performed by
these systems remains fixed across various driving situations.
Artificial neural networks have displayed promising performance and flexibility in other
domains characterized by high degrees of noise and variability, such as handwritten
character recognition [Jackel et al., 1988] [Pawlicki et al., 1988] and speech recognition
[Waibel et al., 1988]. ALVINN (Autonomous Land Vehicle In a Neural Network) is a
connectionist approach to the navigational task of road following. Specifically, ALVINN
is an artificial neural network designed to control the NAVLAB, the Carnegie Mellon
autonomous navigation test vehicle.

NETWORK ARCmTECTURE
ALVINN's current architecture consists of a single hidden layer back-propagation network

306

Pomerleau

Road Intensity
Feedback Unit

45 Direction
Output Units

8x32 Range Finder
Input Retina

30x32 Video
Input Retina
Figure 1: ALVINN Architecture
(See Figure 1). The input layer is divided into three sets of units: two "retinas" and a
single intensity feedback unit. The two retinas correspond to the two forms of sensory
input available on the NAVLAB vehicle; video and range information. The first retina,
consisting of 3002 units, receives video camera input from a road scene. The activation
level of each unit in this retina is proportional to the intensity in the blue color band of
the corresponding patch of the image. The blue band of the color image is used because
it provides the highest contrast between the road and the non-road. The second retina,
consisting of 8x32 units, receives input from a laser range finder. The activation level of
each unit in this retina is proportional to the proximity of the corresponding area in the
image. The road intensity feedback unit indicates whether the road is lighter or darker
than the non-road in the previous image. Each of these 1217 input units is fully connected
to the hidden layer of 29 units, which is in tum fully connected to the output layer.
The output layer consists of 46 units, divided into two groups. The first set of 45 units
is a linear representation of the tum curvature along which the vehicle should travel in
order to head towards the road center. The middle unit represents the "travel straight
ahead" condition while units to the left and right of the center represent successively
sharper left and right turns. The network is trained with a desired output vector of all
zeros except for a "hill" of activation centered on the unit representing the correct tum
curvature, which is the curvature which would bring the vehicle to the road center 7
meters ahead of its current position. More specifically, the desired activation levels for

ALVlNN: An Autonomous Land Vehicle in a Neural Network

Real Road Image

Simulated Road Image

Figure 2: Real and simulated road images
the nine units centered around the correct tum curvature unit are 0.10, 0.32, 0.61, 0.89,
1.00,0.89,0.61,0.32 and 0.10. During testing, the tum curvature dictated by the network
is taken to be the curvature represented by the output unit with the highest activation
level.
The final output unit is a road intensity feedback unit which indicates whether the road
is lighter or darker than the non-road in the current image. During testing, the activation
of the output road intensity feedback unit is recirculated to the input layer in the style
of Jordan [Jordan, 1988] to aid the network's processing by providing rudimentary infonnation concerning the relative intensities of the road and the non-road in the previous
image.

TRAINING AND PERFORMANCE
Training on actual road images is logistically difficult, because in order to develop a
general representation, the network must be presented with a large number of training
exemplaIS depicting roads under a wide variety of conditions. Collection of such a
data set would be difficult, and changes in parameters such as camera orientation would
require collecting an entirely new set of road images. To avoid these difficulties we
have developed a simulated road generator which creates road images to be used as
training exemplars for the network. Figure 2 depicts the video images of one real and
one artificial road. Although not shown in Figure 2, the road generator also creates
corresponding simulated range finder images. At the relatively low resolution being used
it is difficult to distinguish between real and simulated roads.
NetwoIk: training is performed using these artificial road "snapshots" and the Warp back-

307

? 308

Pomerleau

Figure 3: NAVLAB, the eMU autonomous navigation test vehicle.
propagation simulator described in [Pomerleau et al., 1988]. Training involves first creating a set of 1200 road snapshots depicting roads with a wide variety of retinal orientations and positions, under a variety of lighting conditions and with realistic noise levels.
Back-propagation is then conducted using this set of exemplars until only asymptotic
performance improvements appear likely. During the early stages of training, the input
road intensity unit is given a random activation level. This is done to prevent the network from merely learning to copy the activation level of the input road intensity unit
to the output road intensity unit, since their activation levels should almost always be
identical because the relative intensity of the road and the non-road does not often change
between two successive images. Once the network has developed a representation that
uses image characteristics to determine the activation level for the output road intensity
unit, the network is given as input whether the road would have been darker or lighter
than the non-road in the previous image. Using this extra information concerning the
relative brightness of the road and the non-road, the network is better able to determine
the correct direction for the vehicle to trave1.
After 40 epochs of training on the 1200 simulated road snapshots, the network correctly
dictates a tum curvature within two units of the correct answer approximately 90%
of the time on novel simulated road images. The primary testing of the ALVINN's
performance has been conducted on the NAVLAB (See Figure 3). The NAVLAB is
a modified Chevy van equipped with 3 Sun computers, a Warp, a video camera, and
a laser range finder, which serves as a testbed for the CMU autonomous land vehicle
project [Thorpe et al., 1987]. Performance of the network to date is comparable to that
achieved by the best traditional vision-based autonomous navigation algorithm at CMU
under the limited conditions tested. Specifically, the network can accurately drive the
NAVLAB at a speed of 1/2 meter per second along a 400 meter path through a wooded

ALVINN: An Autonomous Land Vehicle in a Neural Network
Weights to Direction Output Units
t~li'C

jjllil

Weight to Output
Feedback Unit
D

Weights from Video Camera Retina

Road 1
Edges

Weight from Input
Feedback Unit
n
Weight from
Bias Unit

?
Weights from
Range Finder Retina

Excitatory
Periphery Connections
Inhibitory
Central Connections

Figure 4: Diagram of weights projecting to and from a typical hidden unit in a network
trained on roads with a fixed width. The schematics on the right are aids for interpretation.
area of the CMU campus under sunny fall conditions. Under similar conditions on the
same course, the ALV group at CMU has recently achieved similar driving accuracy at
a speed of one meter per second by implementing their image processing autonomous
navigation algorithm on the Watp computer. In contrast, the ALVINN network is currently
simulated using only an on-boani Sun computer, and dramatic speedups are expected
when tests are perfonned using the Watp.

NETWORK REPRESENTATION
The representation developed by the network to perfonn the road following task depends
dramatically on the characteristics of the training set. When trained on examples of roads
with a fixed width, the network develops a representations consisting of overlapping road
filters. Figure 4 is a diagram of the weights projecting to and from a single hidden unit
.
in such a network.
As indicated by the weights to and from the feedback units, this hidden unit expects the
road to be lighter than the non-road in the previous image and supports the road being
lighter than the non-road in the current image. More specifically, the weights from the

309

310

Pomerleau
Weights to Direction Output Units
1::tllII:UIll t k?

Weight to Output
Feedback Unit

-

mDIRoad
~ N on-road
~

Weights from Video Camera Retina
.
' :

' .' :

..

Weight from Input
Feedback Unit

. : ' : .'

11111111

1111111

I[

Weight from
Bias Unit

111111111111

?

.:

.

Weights from
Range Finder Retina

Figure 5: Diagram of weights projecting to and from a typical hidden unit in a network
trained on roads with different widths.
video camera retina support the interpretation that this hidden unit is a filter for two light
roads, one slightly left and the other slightly right or center (See schematic to the right
of the weights from the video retina in Figure 4). This interpretation is also supported by
the weights from the range finder retina, which has a much wider field of view than the
video camera. This hidden unit is excited if there is high range activity (Le. obstacles) in
the periphery and inhibited if there is high range activity in the central region of the scene
where this hidden unit expects the road to be (See schematic to the right of the weights
from the range finder retina in Figure 4). Finally, the two road filter interpretation is
reflected in the weights from this hidden unit to the direction output units. Specifically,
this hidden unit has two groups of excitatory connections to the output units, one group
dictating a slight left turn and the other group dictating a slight right turn. Hidden units
which act as filters for 1 to 3 roads are the representation structures most commonly
developed when the network is trained on roads with a fixed width.
The network develops a very different representation when trained on snapshots with
widely varying road widths. A typical hidden unit from this type of representation is
depicted in figure 5. One important feature to notice from the feedback weights is that
this unit is filtering for a road which is darlcer than the non-road. More importantly, it
is evident from the video camera retina weights that this hidden unit is a filter solely for
the left edge of the road (See schematic to the right of the weights from the range finder

ALVINN: An Autonomous Land Vehicle in a Neural Network

retina in Figure 5). This hidden unit supports a rather wide range of travel directions.
This is to be expected, since the correct travel direction for a road with an edge at a
particular location varies substantially depending on the road's width. This hidden unit
would cooperate with hidden units that detect the right road edge to determine the correct
travel direction in any particular situation.

DISCUSSION AND EXTENSIONS
The distinct representations developed for different circumstances illustrate a key advantage provided by neural networks for autonomous navigation. Namely, in this paradigm
the data, not the programmer, determines the salient image features crucial to accurate
road navigation. From a practical standpoint, this data responsiveness has dramatically
sped ALVINN's development. Once a realistic artificial road generator was developed,
back-propagation producted in half an hour a relatively successful road following system.
It took many months of algorithm development and parameter tuning by the vision and
autonomous navigation groups at CMU to reach a similar level of performance using
traditional image processing and pattern recognition techniques.
More speculatively, the flexibility of neural network representations provides the possibility of a very different type of autonomous navigation system in which the salient
sensory features are determined for specific driving conditions. By interactively training
the network on real road images taken as a human drives the NAVLAB, we hope to
develop a system that adapts its processing to accommodate current circumstances. This
is in contrast with other autonomous navigation systems at CMU [Thorpe et al., 1987]
and elsewhere [Dunlay & Seida, 1988] [Dickmanns & Zapp, 1986] [Kuan et al., 1988].
Each of these implementations has relied on a fixed, highly structured and therefore relatively inflexible algorithm for finding and following the road, regardless of the conditions
at hand.
There are difficulties involved with training "on-the-fly" with real images. If the network
is not presented with sufficient variability in its training exemplars to cover the conditions
it is likely to encounter when it takes over driving from the human operator, it will not
develop a sufficiently robust representation and will perform poorly. In addition, the
network must not solely be shown examples of accurate driving, but also how to recover
(i.e. return to the road center) once a mistake has been made. Partial initial training on
a variety of simulated road images should help eliminate these difficulties and facilitate
better performance.
Another important advantage gained through the use of neural networks for autonomous
navigation is the ease with which they assimilate data from independent sensors. The
current ALVINN implementation processes data from two sources, the video camera and
the laser range finder. During training, the network discovers how information from
each source relates to the task, and weights each accordingly. As an example, range
data is in some sense less important for the task of road following than is the video
data. The range data contains information concerning the position of obstacles in the
scene, but nothing explicit about the location of the road. As a result, the range data
is given less significance in the representation, as is illustrated by the relatively small

311

312

Pomerleau

magnitude weights from the range finder retina in the weight diagrams. Figures 4 and
5 illustrate that the range finder connections do correlate with the connections from the
video camera, and do contribute to choosing the correct travel direction. Specifically, in
both figures, obstacles located outside the area in which the hidden unit expects the road
to be located increase the hidden unit's activation level while obstacles located within the
expected road boundaries inhibit the hidden unit. However the contributions from the
range finger connections aren't necessary for reasonable performance. When ALVINN
was tested with normal video input but an obstacle-free range finder image as constant
input, there was no noticeable degradation in driving performance. Obviously under
off-road driving conditions obstacle avoidance would become much more important and
hence one would expect the range finder retina to playa much more significant role in the
network's representation. We are currently working on an off-road version of ALVINN
to test this hypothesis.
Other current directions for this project include conducting more extensive tests of the
network's performance under a variety of weather and lighting conditions. These will
be crucial for making legitimate performance comparisons between ALVINN and other
autonomous navigation techniques. We are also working to increase driving speed by
implementing the network simulation on the on-board Warp computer.
Additional extensions involve exploring different network architectures for the road following task. These include 1) giving the network additional feedback information by using Elman's [Elman, 1988] technique of recirculating hidden activation levels, 2) adding
a second hidden layer to facilitate better internal representations, and 3) adding local
connectivity to give the network a priori knowledge of the two dimensional nature of the
input
In the area of planning, interesting extensions include stopping for, or planning a path
around, obstacles. One area of planning that clearly needs work is dealing sensibly with
road forks and intersections. Currently upon reaching a fork, the network may output two
widely discrepant travel directions, one for each choice. The result is often an oscillation
in the dictated travel direction and hence inaccurate road following. Beyond dealing with
individual intersections, we would eventually like to integrate a map into the system to
enable global point-to-point path planning.

CONCLUSION
More extensive testing must be performed before definitive conclusions can be drawn concerning the peiformance of ALVINN versus other road followers. We are optimistic concerning the eventual contributions neural networks will make to the area of autonomous
navigation. But perhaps just as interesting are the possibilities of contributions in the
other direction. We hope that exploring autonomous navigation, and in particular some of
the extensions outlined in this paper, will have a significant impact on the field of neural
networks. We certainly believe it is important to begin researching and evaluating neural
networks in real world situations, and we think autonomous navigation is an interesting
application for such an approach.

ALVINN: An Autonomous Land Vehicle in a Neural Network

Acknowledgements
This work would not have been possible without the input and support provided by Dave Touretzky,
Joseph Tebelskis, George Gusciora and the CMU Warp group, and particularly Charles Thorpe, Till
Crisman, Martial Hebert, David Simon, and rest of the CMU ALV group.
This research was supported by the Office of Naval Research under Contracts NOOOI4-87-K-0385,
NOOOI4-87-K-0533 and NOOOI4-86-K-0678, by National Science Foundation Grant EET-8716324,
by the Defense Advanced Research Projects Agency (DOD) monitored by the Space and Naval
Warfare Systems Command under Contract NOOO39-87-C-0251, and by the Strategic Computing
Initiative of DARPA, through ARPA Order 5351, and monitored by the U.S. Army Engineer
Topographic Laboratories under contract DACA76-85-C-0003 titled "Road Following".

References
[Dickmanns & Zapp, 1986] Dickmanns, E.D., Zapp, A. (1986) A curvature-based
scheme for improving road vehicle guidance by computer vision. "Mobile Robots",
SPIE-Proc. Vol. 727, Cambridge, MA.
[Elman, 1988] Elman, J.L, (1988) Finding structure in time. Technical report 8801. Center for Research in Language, University of California, San Diego.
[Dunlay & Seida, 1988] Dunlay, R.T., Seida, S. (1988) Parallel off-road perception processing on the ALV. Proc. SPIE Mobile Robot Conference, Cambridge MA.
[Jackel et al., 1988] Jackel, L.D., Graf, H.P., Hubbard, W., Denker, J.S., Henderson, D.,
Guyon, 1. (1988) An application of neural net chips: Handwritten digit recognition.
Proceedings of IEEE International Conference on Neural Networks, San Diego, CA.
[Jordan, 1988] Jordan, M.l. (1988) Supervised learning and systems with excess degrees
of freedom. COINS Tech. Report 88-27, Computer and Infolll1ation Science, University of Massachusetts, Amherst MA.
[Kuan et al., 1988] Kuan, D., Phipps, G. and Hsueh, A.-C. Autonomous Robotic Vehicle
Road Following. IEEE Trans. on Pattern Analysis and Machine Intelligence, Vol.
10, Sept. 1988.
[pawlicki et al., 1988] Pawlicki, T.E, Lee, D.S., Hull, J.J., Srihari, S.N. (1988) Neural
network models and their application to handwritten digit recognition. Proceedings
of IEEE International Conference on Neural Networks, San Diego, CA.
[pomerleau et al., 1988] Pomerleau, D.A., Gusciora, G.L., Touretzky, D.S., and Kung,
H.T. (1988) Neural network simulation at Waq> speed: How we got 17 million
connections per second. Proceedings of IEEE International Conference on Neural
Networks, San Diego, CA.
[Th0IJle et al., 1987] Thorpe, c., Herbert, M., Kanade, T., Shafer S. and the members of
the Strategic Computing Vision Lab (1987) Vision and navigation for the Carnegie
Mellon NAVLAB. Annual Revi~ of Computer Science Vol. 11, Ed. Joseph Traub,
Annual Reviews Inc., Palo Alto, CA.

[Waibel et al., 1988] Waibel, A, Hanazawa, T., Hinton, G., Shikano, K., Lang, K. (1988)
Phoneme recognition: Neural Networks vs. Hidden Markov Models. Proceedings
from Int. Conf. on Acoustics, Speech and Signal Processing, New York, New York.

313


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 432-rapidly-adapting-artificial-neural-networks-for-autonomous-navigation.pdf

Rapidly Adapting Artificial Neural Networks for
Autonomous Navigation

Dean A. Pomerleau
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213

Abstract
The ALVINN (Autonomous Land Vehicle In a Neural Network) project addresses
the problem of training artificial neural networks in real time to perform difficult
perception tasks. ALVINN ,is a back-propagation network that uses inputs from a
video camera and an imaging laser rangefinder to drive the CMU Navlab, a modified
Chevy van. This paper describes training techniques which allow ALVINN to learn
in under 5 minutes to autonomously control the Navlab by watching a human driver's
response to new situations. Using these techniques, ALVINN has been trained
to drive in a variety of circumstances including single-lane paved and unpaved
roads, multilane lined and unlined roads, and obstacle-ridden on- and off-road
environments, at speeds of up to 20 miles per hour.

1 INTRODUCTION
Previous trainable connectionist perception systems have often ignored important aspects of
the form and content of available sensor data. Because of the assumed impracticality of
training networks to perform realistic high level perception tasks, connectionist researchers
have frequently restricted their task domains to either toy problems (e.g. the T-C identification
problem [11] [6]) or fixed low level operations (e.g. edge detection [8]). While these restricted
domains can provide valuable insight into connectionist architectures and implementation
techniques, they frequently ignore the complexities associated with real world problems.
There are exceptions to this trend towards simplified tasks. Notable successes in high level
domains such as speech recognition [12], character recognition [5] and face recognition [2]
have been achieved using real sensor data. However, the results have come only in very
controlled environments, after careful preprocessing of the input to segment and label the
training exemplars. In addition, these successful connectionist perception systems have
ignored the fact that sensor data normally becomes available gradually and not as a monolithic
training set. In short, artificial neural networks previously have never been successfully trained

429

430

Pomerleau

Road Intensity
Feedback Unit

Sh"'Jl
left

SIJ'aIShl
Ahead

Sh"'Jl
RIghI

Sharp
Left

Stralghl

Sharp

Ahead

Righi

30 Output
Units

3Oxl2 Sensor
Retia

Figure 1: ALVINN's previous (left) and current (right) architectures
using sensor data in real time to perform a real world perception task.
The ALVINN (Autonomous Land Vehicle In a Neural Network) system remedies this shortcoming. ALVINN is a back-propagation network designed to drive the CMU Navlab. a
modified Chevy van. Using real time training techniques, the system quickly learns to autonomously control the Navlab by watching a human driver's reactions. ALVINN has been
trained to drive in a variety of circumstances including single-lane paved and unpaved roads,
multilane lined and unlined roads and obstacle ridden on- and off-road environments, at
speeds of up to 20 miles per hour. This paper will primarily focus on improvements and
extensions made to the ALVINN system since the presentation of this work at the 1988 NIPS
conference [9].

2

NETWORK ARCHITECTURE

The current architecture for an individual ALVINN driving network is significantly simpler
than the previous version (See Figure 1). The input layer now consists of a single 30x32 unit
"retina" onto which a sensor image from either the video camera or the laser rangefinder is
projected. Each of the 960 input units is fully connected to the hidden layer of 5 units, which
is in tum fully connected to the output layer. The 30 unit output layer is a linear representation
of the currently appropriate steering direction which may serve to keep the vehicle on the road
or to prevent it from colliding with nearby obstacles l . The centermost output unit represents
the "travel straight ahead" condition, while units to the left and right of center represent
successively sharper left and right turns.
The reductions in network complexity over previous versions have been made in response
to experience with ALVINN in actual driving situations. I have found that the distributed
nature of the internal representation allows a network of only 5 hidden units to accurately
drive in a variety of situations. I have also learned that multiple sensor inputs to a single
network are redundant and can be eliminated. For instance, when training a network on a
single-lane road, there is sufficient information in the video image alone for accurate driving.
Similarly, for obstacle avoidance, the laser rangefinder image is sufficient and the video image
IThe task a particular driving network perfonns depends on the type of input sensor image and the
driving situation it has been trained to handle.

Rapidly Adapting Artificial Neural Networks for Autonomous Navigation
is superfluous. The road intensity feedback unit has been eliminated on similar grounds. In
the previous architecture, it provided the network with the relative intensity of the road vs.
the non-road in the previous image. This information was unnecessary for accurate road
following, and undefined in new ALVINN domains such as off-road driving.
To drive the Navlab, an image from the appropriate sensor is reduced to 30 x 32 pixels and
projected onto the input layer. After propagating activation through the network, the output
layer's activation prOfile is translated into a vehicle steering command. The steering direction
dictated by the network is taken to be the center of mass of the "hill" of activation surrounding
the output unit with the highest activation leveL Using the center of mass of activation instead
of the most active output unit when determining the direction to steer permits finer steering
corrections, thus improving ALVINN's driving accuracy.

3

TRAINING "ON -THE-FLY"

The most interesting recent improvement to ALVINN is the training teChnique. Originally,
ALVINN was trained with backpropagation using 1200 simulated scenes portraying roads
under a wide variety of weather and lighting conditions [9]. Once trained, the network was
able to drive the Navlab at up to 1.8 meters per second (3.5 mph) along a 400 meter path
through a wooded area of the CMU campus in weather which included snowy, rainy, sunny"
and cloudy situations.
Despite its apparent success, this training paradigm had serious shortcomings. It required
approximately 6 hours of Sun-4 CPU time to generate the synthetic road scenes, and then an
additional 45 minutes of Warp2 computation time to train the network. Furthermore, while
effective at training the network to drive on a single-lane road, extending the synthetic training
paradigm to deal with more complex driving situations like multilane and off-road driving
would have required prohibitively complex artificial scene generators.
I have developed a scheme called training "on-the-fiy" to deal with these problems. Using
this technique, the network learns to imitate a person as he drives. The network is trained
with back-propagation using the latest video camera image as input and the person's current
steering direction as the desired output.
There are two potential problems associated with this simple training on-the-fiy scheme. First,
since the person steers the vehicle down the center of the road during training, the network
will never be presented with situations where it must recover from misalignment errors. When
driving for itself, the network may occasionally stray from the road center, so it must be
prepared to recover by steering the vehicle back to the middle of the road. The second
problem is that naively training the network with only the current video image and steering
direction may cause it to overlearn recent inputs. If the person drives the Navlab down a
stretch of straight road near the end of training, the network will be presented with a long
sequence of similar images. This sustained lack of diversity in the training set will cause the
network to "forget" what it had learned about driving on curved roads and instead learn to
always steer straight ahead.
Both problems associated with training on-the-fiy stem from the fact that back-propagation
requires training data which is representative of the full task to be learned. To provide the
necessary variety of exemplars while still training on real data, the simple training on-the2There was fonnerly a 100 MFLOP Warp systolic array supercomputer onboard the Navlab. It has
been replaced by 3 Sun-4s, further necessitating the streamlined architecture described in the previous
section.

431

432

Pomerleau

Original Image

Shifted and Rotated Images
Figure 2: The single original video image is shifted and rotated to create multiple training
exemplars in which the vehicle appears to be a different locations relative to the road.
fly scheme described' above must be modified. Instead of presenting the network with only
the current video image and steering direction, each original image is shifted and rotated in
software to create 14 additional images in which the vehicle appears to be situated differently
relative to the environment (See Figure 2). The sensor's position and orientation relative to
the ground plane are known, so precise transformations can be achieved using perspective
geometry. The correct steering direction as dictated by the driver for the original image is
altered for each of the transformed images to account for the altered vehicle placement 3 .
Using transformed training patterns allows the network to learn how to recover from driving
errors. Also, overtraining on repetitive images is less of a problem, since the transfonned
training exemplars add variety to the training set. As additional insurance against the effects
of repetitive exemplars, the training set diversity is further increased by maintaining a buffer
of previously encountered training patterns.
In practice, training on-the-fly works as follows. A live sensor image is digitized and reduced
to the low resolution image required by the network. This single original image is shifted and
rotated 14 times to create 14 additional training exemplars4 . Fifteen old exemplars from the
current training set of 200 patterns are chosen and replaced by the 15 new exemplars. The 15
exemplars to be replaced in the training set are chosen on the basis of how closely they match
the steering direction of one of the new tokens. Exchanging a new token for an old token with
a similar steering direction helps maintain diversity in the training buffer during monotonous
stretches of road by preventing novel older patterns from being replaced by recent redundant
ones.
After this replacement process, one forward and one backward pass of the baCk-propagation
algorithm is performed on the 200 exemplars to update the network's weights. The entire
process is then repeated. The network requires approximately 50 iterations through this
digitize-replace-train cycle to learn to drive in the domains that have been tested. Running
3 A simple steering model is used when transforming the driver's original direction. It assumes the
"correct" steering direction is the one that will eliminate the additional vehicle translation and rotation
introduced by the transformation and bringing the vehicle to the point the person was originally steering
towards a fixed distance ahead of the vehicle.
4The shifts are chosen randomly from the range -1.25 to +1.25 meters and the rotations from the
range -6.0 to +6.0 degrees.

Rapidly Adapting Artificial Neural Networks for Autonomous Navigation

Figure 3: Video images taken on three of the test roads ALVINN has been trained to drive on.
They are, from left to right, a single-lane dirt access road, a single-lane paved bicycle path,
and a lined two-lane highway.

on a Sun-4, this takes about five minutes during which a person drives the Navlab at about 4
miles per hour over the training road.

4

RESULTS AND DISCUSSION

Once it has learned, the network can accurately traverse the length of road used for training
and also generalize to drive along parts of the road it has never encountered under a variety
of weather conditions. In addition, since determining the steering direction from the input
image merely involves a forward sweep through the network, the system is able to process 25
images per second, allowing it to drive at up to the Navlab's maximum speed of20 miles per
hou~. This is over twice as fast as any other sensor-based autonomous system has driven the
Navlab [3] [7].
The training on-the-fly scheme gives ALVINN a flexibility which is novel among autonomous
navigation systems. It has allowed me to successfully train individual networks to drive in
a variety of situations, including a single-lane dirt access road, a single-lane paved bicycle
path, a two-lane suburban neighborhood street, and a lined two-lane highway (See Figure 3).
Using other sensor modalities as input, including laser range images and laser reflectance
images, individual ALVINN networks have been trained to follow roads in total darkness,
to avoid collisions in obstacle rich environments, and to follow alongside railroad tracks.
ALVINN networks have driven in each of these situations for up to 1/2 mile, until reaching a
dead end or a difficult intersection. The development of a system for each of these domains
using the "traditional approach" to autonomous navigation would require the programmer to
1) determine what features are important for the particular task, 2) program detectors (using
statistical or symbolic techniques) for finding these important features and 3) develop an
algorithm for determining which direction to steer from the location of the detected features.
In contrast, ALVINN is able to learn for each new domain what image features are important,
how to detect them and how to use their position to steer the vehicle. Analysis of the
hidden unit representations developed in different driving situations shows that the network
forms detectors for the image features which correlate with the correct steering direction.
When trained on multi-lane roads, the network develops hidden unit feature detectors for the
lines painted on the road, while in single-lane driving situations, the detectors developed are
5The Navlab has a hydraulic drive system which allows for very precise speed control, but which
prevents the vehicle from driving over 20 miles per hour.

433

434

Pomerleau
sensitive to road edges and rOad-shaped regions of similar intensity in the image. For a more
detailed analysis of ALVINN's internal representations see [9] [10].
This ability to utilize arbitrary image features can be problematic. This was the case when
ALVINN was trained to drive on a poorly defined dirt road with a distinct ditch on its right side.
The network had no problem learning and then driving autonomously in one direction, but
when driving the other way, the network was erratic, swerving from one side of the road to the
other. After analyzing the network's hidden representation, the reason for its difficulty became
clear. Because of the poor distinction between the road and the non-road, the network had
developed only weak detectors for the road itself and instead relied heavily on the position of
the ditch to determine the direction to steer. When tested in the opposite direction, the network
was able to keep the vehicle on the road using its weak road detectors but was unstable because
the ditch it had learned to look for on the right side was now on the left. Individual ALVINN
networks have a tendency to rely on any image feature consistently correlated with the correct
steering direction. Therefore, it is important to expose them to a wide enough variety of
situations during training so as to minimize the effects of transient image features.
On the other hand, experience has shown that it is more efficient to train several domain
specific networks for circumstances like one-lane vs. two-lane driving, instead training a
single network for all situations. To prevent this network specificity from reducing ALVINN's
generality, I am currently implementing connectionist and non-connectionist techniques for
combining networks trained for different driving situations. Using a simple rule-based priority
system similar to the subsumption architecture [1], I have recently combined a road following
network and an obstacle avoidance network. The road following network uses video camera
input to follow a single-lane road. The obstacle avoidance network uses laser rangefinder
images as input. It is trained to swerve appropriately to prevent a collision when confronted
with obstacles and to drive straight when the terrain ahead is free of obstructions. The
arbitration rule gives priority to the road following network when determining the steering
direction, except when the obstacle avoidance network outputs a sharp steering command. In
this case, the urgency of avoiding an imminent collision takes precedence over road following
and the steering direction is determined by the obstacle avoidance network. Together, the
two networks and the arbitration rule comprise a system capable of staying on the road and
swerving to prevent collisions.
To facilitate other rule-based arbitration teChniques, I am currently adding to ALVINN a
non-connectionist module which maintains the vehicle's position on a map. Knowing its map
position will allow ALVINN to use arbitration rules such as "when on a stretch of two lane
highway, rely primarily on the two lane highway network". This symbolic mapping module
will also allow ALVINN to make high level, goal-oriented decisions such as which way to
tum at intersections and when to stop at a predetermined destination.
Finally, I am experimenting with connectionist techniques, such as the task decomposition
architecture [6] and the meta-pi architecture [4], for combining networks more seamlessly
than is possible with symbolic rules. These connectionist arbitration techniques will enable
ALVINN to combine outputs from networks trained to perform the same task using different
sensor modalities and to decide when a new expert must be trained to handle the current
situation.
Acknowledgements
The principle support for the Navlab has come from DARPA, under contracts DACA76-85-C0019, DACA76-85-C-0003 and DACA76-85-C-0002. This research was also funded in part
by a grant from Fujitsu Corporation.

Rapidly Adapting Artificial Neural Networks for Autonomous Navigation

References
[1] Brooks, R.A. (1986) A robust layered control system for a mobile robot. IEEE Journal
of Robotics and Automation, vol. RA-2, no. 1, pp. 14-23, April 1986.
[2] Cottrell, G.W. (1990) Extracting features from faces using compression networks: Face,
identity, emotion and gender recognition using holons. In Connectionist Models: Proc.
of the 1990 Summer School, David Touretzky (Ed.), Morgan Kaufmann, San Mateo,
CA.
[3] Crisman, J.D. and Thorpe C.E. (1990) Color vision for road following. In Vision and
Navigation: The CMU Navlab Charles Thorpe (Ed.), Kluwer Academic Publishers,
Boston,MA.
[4] Hampshire, J.B., Waibel A.H. (1989) The meta-pi network: Buildingdistributedknowledge representations for robust pattern recognition. Carnegie Mellon Technical Report
CMU-CS-89-l66-R. August, 1989.
[5] LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., and
Jackel, L.D. (1989) Backpropagation applied to handwritten zip code recognition. Neural
Computation} (4).
[6] Jacobs, R.A, Jordan, M.I., Barto, A.G. (1990) Task decomposition through competition
in a modular connectionist architecture: The what and where vision tasks. Univ. of
Massachusetts Computer and Information Science Technical Report 90-27, March 1990.
[7] Kluge, K. and Thorpe C.E. (1990) Explicit models for robot road following. In Vision
and Navigation: The CMU Navlab Charles Thorpe (Ed.), Kluwer Academic Publishers,
Boston,MA.
[8] Koch, c., Bair, W., Harris, J.G., Horiuchi, T., Hsu, A. and Luo, J. (1990) Real-time computervision and robotics using analog VLSI circuits. In Advances in Neural Information
Processing Systems, 2, D.S. Touretzky (Ed.), Morgan Kaufmann, San Mateo, CA
[9] Pomerleau, D.A. (1989) ALVINN: An Autonomous Land Vehicle In a Neural Network,
Advances in Neural Information Processing Systems, }, D.S. Touretzky (Ed.), Morgan
Kaufmann, San Mateo, CA.
[10] Pomerleau, D.A. (1990) Neural network based autonomous navigation. In Vision and
Navigation: The CMU Navlab Charles Thorpe (Ed.), Kluwer Academic Publishers,
Boston,MA.
[11] Rumelhart, D.E., Hinton, G.E., and Williams, R.J. (1986) Learning internal representations by error propagation. In D.E. Rumelhart and J.L. McClelland (Eds.) Parallel
Distributed Processing: Explorations in the Microstructures of Cognition. Vol.}.' Foundations. Bradford Books/MlT Press, Cambridge, MA.
[12] Waibel, A, Hanazawa, T., Hinton, G., Shikano, K., Lang, K. (1988) Phoneme recognition: Neural Networks vs. Hidden Markov Models. Proceedings from Int. Conf. on
Acoustics, Speech and Signal ProceSSing, New York, New York.

435


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 1499-vlsi-implementation-of-motion-centroid-localization-for-autonomous-navigation.pdf

VLSI Implementation of Motion Centroid
Localization for Autonomous Navigation

Ralph Etienne-Cummings
Dept. of ECE,
Johns Hopkins University,
Baltimore, MD

Viktor Gruev
Dept. of ECE,
Johns Hopkins University,
Baltimore, MD

Mohammed Abdel Ghani
Dept. ofEE,
S. Illinois University,
Carbondale, IL

Abstract
A circuit for fast, compact and low-power focal-plane motion centroid
localization is presented. This chip, which uses mixed signal CMOS
components to implement photodetection, edge detection, ON-set
detection and centroid localization, models the retina and superior
colliculus. The centroid localization circuit uses time-windowed
asynchronously triggered row and column address events and two
linear resistive grids to provide the analog coordinates of the motion
centroid. This VLSI chip is used to realize fast lightweight
autonavigating vehicles. The obstacle avoiding line-following
algorithm is discussed.

1 INTRODUCTION
Many neuromorphic chips which mimic the analog and parallel characteristics of visual,
auditory and cortical neural circuits have been designed [Mead, 1989; Koch, 1995] .
Recently researchers have started to combine digital circuits with neuromorphic aVLSI
systems [Boahen, 1996]. The persistent doctrine, however, has been that computation
should be performed in analog, and only communication should use digital circuits. We
have argued that hybrid computational systems are better equipped to handle the high
speed processing required for real-world problem solving , while maintaining
compatibility with the ubiquitous digital computer [Etienne, 1998]. As a further
illustration of this point of view, this paper presents a departure form traditional
approaches for focal plane centroid localization by offering a mixed signal solution that is
simultaneously high-speed, low power and compact. In addition, the chip is interfaced
with an 8-bit microcomputer to implement fast autonomous navigation.
Implementation of centroid localization has been either completely analog or completely
digital. The analog implementations, realized in the early 1990s, used focal plane current
mode circuits to find a global continuos time centroid of the pixels' intensities
[DeWeerth, 1992]. Due to their sub-threshold operation, these circuits are low power,
but slow. On the other hand, the digital solutions do not compute the centroid at the focal

R. Etienne-Cummings, V. Grnev and M A. Ghani

686

plane. They use standard CCO cameras, AID converters and OSP/CPU to compute the
intensity centroid [Mansfield, 1996]. These software approaches offer multiple centroid
localization with complex mathematical processing. However, they suffer from the usual
high power consumption and non-scalability of traditional digital visual processing
systems. Our approach is novel in many aspects. We benefit from the low power,
compactness and parallel organization of focal plane analog circuits and the speed,
robustness and standard architecture of asynchronous digital circuits. Furthermore, it
uses event triggered analog address read-out, which is ideal for the visual centroid
localization problem. Moreover, our chip responds to moving targets only by using the
ON-set of each pixel in the centroid computation. Lastly, our chip models the retina and
two dimensional saccade motor error maps of superior colliculus on a single chip
[Sparks, 1990] . Subsequently, this chip is interfaced with a IlC for autonomous obstacle
avoidance during line-following navigation. The line-following task is similar to target
tracking using the saccadic system, except that the "eye" is fixed and the "head" (the
vehicle) moves to maintain fixation on the target. Control signals provided to the vehicle
based on decisions made by the IlC are used for steering and accelerating/braking. Here
the computational flexibility and programmability of the IlC allows rapid prototyping of
complex and robust algorithms.

2 CENTROID LOCALIZATION
The mathematical computation of the centroid of an object on the focal plane uses
intensity weighted average of the position of the pixels forming the object [OeWeerth,
1992] . Equation (1) shows this formulation. The implementation of this representation
N

N

LI,x,
x=

j =1

N

L,I,

LI,y;
and

y=

;=1
N

(1)

L I,

,=1

1=1

can be quite involved since a product between the intensity and position is implied. To
eliminate this requirement, the intensity of the pixels can be normalized to a single value
within the object. This gives equation (2) since the intensity can be factored out of the
summations. Normalization of the intensity using a simple threshold is not advised since

Ix,

x=~

N

Iy,

and

y=~

N

(2)

Intensity Image

XI

X l+l

X 1+2

X I +3

x 1+4

Edges from pixels

Figure 1: Centroid computation
architecture.

Figure 2: Centroid computation method.

the value of the threshold is dependent on the brightness of the image and number of
pixels forming the object may be altered by the thresholding process. To circumvent
these problems, we take the view that the centroid of the object is defined in relation to its
boundaries. This implies that edge detection (second order spatial derivative of intensity)
can be used to highlight the boundaries, and edge labeling (the zero-crossing of the
edges) can be used to normalize the magnitude of the edges. Subsequently, the centroid

VLSI Implementation ofMotion Centroid Localization for Autonomous Navigation

687

of the zero-crossings is computed. Equation (2) is then realized by projecting the zerocrossing image onto the x- and y-axis and performing two linear centroid determinations.
Figure (1) shows this process.
The determination of the centroid is computed using a resistance grid to associate the
position of a column (row) with a voltage. In figure 2, the positions are given by the
voltages Vi . By activating the column (row) switch when a pixel of the edge image
appears in that column (row), the position voltage is connected to the output line through
the switch impedance, Rs. As more switches are activated, the voltage on the output line
approximates equation (2). Clearly, since no buffers are used to isolate the position
voltages, as more switches are activated, the position voltages will also change. This
does not pose a problem since the switch resistors are design to be larger than the position
resistors (the switch currents are small compared to the grid current). Equation (3) gives
the error between the ideal centroid and the switch loaded centroid in the worst case
when Rs

= on.

In the equation, N is the number of nodes, M is the number of switches

set and Xl and xM are the locations of the first and last set switches, respectively. This
error is improved as Rs gets larger, and vanishes as N

(M~N)

approaches infinity . The

terms Xi represent an ascending ordered list of the activated switches; x I may correspond
to column five, for example. This circuit is compact since it uses only a simple linear
resistive grid and MOS switches. It is low power because the total grid resistance, N x R,
can be large. It can be fast when the parasitic capacitors are kept small . It provides an
analog position value, but it is triggered by fast digital signals that activate the switches.
error = VmOll -Vmin
M(N + 1)

1

~[ X
?...J
.=1

?

xCN+l)
_ _--'-1-'--_-'--_
N + 1 + XI - Xm

(3)

3 MODELING THE RETINA AND SUPERIOR COLLICULUS
3.1 System Overview
The centroid computation approach presented in section 2 is used to isolate the location
of moving targets on a 20 focal plane array. Consequently, a chip which realizes a
neuromorphic visual target acquisition system based on the saccadic generation
mechanism of primates can be implemented. The biological saccade generation process is
mediated by the superior colliculus, which contains a map of the visual field [Sparks,
1990}. In laboratory experiments, cellular recordings suggest that the superior colliculus
provides the spatial location of targets to be foveated. Clearly, a great deal of neural
circuitry exists between the superior colliculus and the eye muscle. Horiuchi has built an
analog system which replicates most of the neural circuits (including the motor system)
which are believed to form the saccadic system [Horiuchi, 1996]. Staying true to the
anatomy forced his implementation to be a complex multi-chip system with many control
parameters. On the other hand, our approach focuses on realizing a compact single chip
solution by only mimicking the behavior of the saccadic system, but not its structure.

3.2 Hardware Implementation
Our approach uses a combination of analog and digital circuits to implement the
functions of the retina and superior colliculus at the focal plane. We use simple digital
control ideas, such as pulse-width modulation and stepper motors, to position the "eye".
The retina portion of this chip uses photodiodes, logarithmic compression, edge detection
and zero-crossing circuits. These circuits mimic the first three layers of cells in the retina

688

R. Etienne-Cummings, V. Grnev and M. A. Ghani

with mixed sub-threshold and strong inversion circuits. The edge detection circuit is
realized with an approximation of the Laplacian operator implemented using the
difference between a smooth (with a resistive grid) and unsmoothed version of the image
[Mead, 1989]. The high gain of the difference circuit creates a binary image of
approximate zero-crossings. After this point, the computation is performed using mixed
analog/digital circuits. The zero-crossings are fed to ON-set detectors (positive temporal
derivatives) which signal the location of moving or flashing targets. These circuits model
the behavior of some of the amacrine and ganglion cells of the primate retina [Barlow,
1982]. These first layers of processing constitute all the "direct" mimicry of the
biological models. Figure 3 shows the schematic of these early processing layers.
The ON-set detectors provide inputs to the model of the superior colliculus circuits. The
ON-set detectors allow us to segment moving targets against textured backgrounds. This
is an improvement on earlier centroid and saccade chips that used pixel intensity. The
essence of the superior colliculus map is to locate the target that is to be foveated. In our
case, the target chosen to be foveated will be moving. Here motion is define simply as
the change in contrast over time. Motion, in this sense, can be seen as being the earliest
measurable attribute of the target which can trigger a saccade without requiring any high
level decision making. Subsequently, the coordinates of the motion must be extracted and
provided to the motor drivers.

X M otion Cenuold

~!
~A
!
Edge Detc:ctlon
ON-set Detecu on

Figure 3: Schematic of
the model of the retina.

Figure 4: Schematic of the model of the superior
collicu Ius.

The circuits for locating the target are implemented entirely with mixed signal, nonneuromorphic circuits. The theoretical foundation for our approach is presented in
section 2. The ON-set detector is triggered when an edge of the target appears at a pixel.
At this time, the pixel broadcasts its location to the edge of the array by activating row
and column lines. This row (column) signal sets a latch at the right (top) of the array. The
latches asynchronously activate switches and the centroid of the activated positions is
provided. The latches remain set until they are cleared by an external control signal. This
control signal provides a time-window over which the centroid output is integrated. This
has the effect of reducing noise by combining the outputs of pixels which are activated at
different instances even if they are triggered by the same motion (an artifact of small fill
factor focal plane image processing). Furthermore, the latches can be masked from the
pixels' output with a second control signal. This signal is used to de-activate the centroid

689

VLSI Implementation of Motion Centroid Localization for Autonomous Navigation

circuit during a saccade (saccadic suppression). A centroid valid signal is also generated
by the chip. Figure 4 shows a portion of the schematic of the superior colliculus model.

3.3 Results
In contrast to previous work, this chip provides the 2-D coordinates of the centroid of a
moving target. Figure 5 shows the oscilloscope trace of the coordinates as a target moves
back and forth, in and out of the chip's field of view. The y-coordinate does change
while the x-coordinate increases and decreases as the target moves to the left and right,
respectively. The chip has been used to track targets in 2-D by making micro-saccades .
In this case, the chip chases the target as it attempts to escape from the center. The eye
movement is performed by converting the analog coordinates into PWM signals, which
are used to drive stepper motors. The system performance is limited by the contrast
sensitivity of the edge detection circuit, and the frequency response of the edge (high
frequency cut-off) and ON-set (low frequency cut-off) detectors. With the appropriate
optics, it can track walking or running persons under indoor or outdoor lighting
conditions at close or far distances. Table I gives a summary of the chip characteristics.
VarYing x?
l'Oordma te

Figure 5: Oscilloscope trace of 20
centroid for a moving target.

Technology

1.2um ORBIT

Chip Size
Array Size

4mm 1
12 x 10

Pixel Size
Fill Factor

llOxllOutn
11%

Intensity
Min . Contrast

0.lu-1OOmW/cm 2

Response Time
Power (chip)

2-10 6 Hz(@1 mW/cml)

10%
5 mW (@l m W/cm~ Vdd

=6V)

Table I: Chip characteristics.

4 APPLICATION: OBSTACLE AVOIDANCE DURING LINEFOLLOWING AUTONA VIGATION
4.1 System Overview
The frenzy of activity towards developing neuromorphic systems over the pass 10 years
has been mainly driven by the promise that one day engineers will develop machines that
can interact with the environment in a similar way as biological organisms. The prospect
of having a robot that can help humans in their daily tasks has been a dream of science
fiction for many decades. As can be expected, the key to success is premised on the
development of compact systems, with large computational capabilities, at low cost (in
terms of hardware and power) . Neuromorphic VLSI systems have closed the gap between
dreams and reality, but we are still very far from the android robot. For all these robots,
autonomous behavior, in the form of auto-navigation in natural environments, must be
one of their primary skills. For miniaturization, neuromorphic vision systems performing
most of the pre-processing, can be coupled with small fast computers to realize these
compact yet powerful sensor/processor modules.

4.2 Navigation Algorithm
The simplest form of data driven auto-navigation is the line-following task. In this task,
the robot must maintain a certain relationship with some visual cues that guide its motion.
In the case of the line-follower, the visual system provides data regarding the state of the

R. Etienne-Cummings, V. Gruev and M A. Ghani

690

line relative to the vehicle, which results in controlling steering and/or speed. If obstacle
avoidance is also required, auto-navigation is considerably more difficult. Our system
handles line-following and obstacle avoidance by using two neuromorphic visual sensors
that provide information to a micro-controller OlC) to steer, accelerate or decelerate the
vehicle. The sensors, which uses the centroid location system outlined above, provides
information on the position of the line and obstacles to the p,C, which provides PWM
signals to the servos for controlling the vehicle. The algorithm implemented in the p,C
places the two sensors in competition with each other to force the line into a blind zone
between the sensors. Simultaneously, if an object enters the visual field from outside, it
is treated as an obstacle and the p,C turns the car away from the object. Obstacle
avoidance is given higher priority than line-following to avoid collisions. The p,C also
keeps track of the direction of avoidance such that the vehicle can be re-oriented towards
the line after the obstacle is pushed out of the field of view. Lastly, for line following,
the position, orientation and velocity of drift, determined from the temporal derivative of
the centroid, are used to track the line. The control strategy is to keep the line in the blind
zone, while slowing down at corners, speeding up on straight aways and avoiding
obstacles. The angle which the line or obstacle form with the x-axis also affects the
speed. The value of the x-centroid relative to the y-centroid provides rudimentary
estimate of the orientation of the line or obstacle to the vehicle. For example, angles less

.

Follow

"

AV~8id~nce
...
,

0I0s1ade

L Zone

/

!

~ne

i

'.)", /

:

AV~na;ce

!

~\?': ~../

;i:~~~~???. .\j ~

\;... ?? ? ?~i=~s..s

Figure 6: Block diagram of the
autonomous line-follower system.

Figure 7: A picture of the vehicle.

(greater) than +/- 45 degrees tend to have small (large) x-coordinates and large (small) ycoordinates and require deceleration (acceleration). Figure 6 shows the organization of
the sensors on the vehicle and control spatial zones. Figure 7 shows the vehicle and
samples of the line and obstacles.
4.3 Hardware Implementation
The coordinates from the centroid localization circuits are presented to the p,C for
analysis. The p,C used is the Microchip PIC16C74. This chip is chosen because of its
five NO inputs and three PWM outputs. The analog coordinates are presented directly to
the NO inputs. Two of the PWM outputs are connected to the steering and speed control
servos. The PIC16C74 runs at 20 MHz and has 35 instructions, 4K by 8-b ROM and 80
by 20-b RAM. The program which runs on the PIC determines the control action to take,
based on the signal provided by the neuromorphic visual sensors. The vehicle used is a
four-wheel drive radio controlled model car (the radio receiver is disconnected) with
Digital Proportional Steering (DPS) .

VLSI Implementation ofMotion Centroid Localization for Autonomous Navigation

691

4.4 Results
The vehicle was tested on a track composed of black tape on a gray linoleum floor with
black and white obstacles. The track formed a closed loop with two sharp turns and some
smooth S-curves. The neuromorphic vision chip was equipped with a 12.5 mm variable
iris lens, which limited its field of view to about 100. Despite the narrow field of view ,
the car was able to navigate the track at an average speed of 1 mls without making any
errors. On less curvy parts of the track, it accelerated to about 2 mls and slowed down at
the corners. When the speed of the vehicle is scaled up, the errors made are mainly due
to over steering.

5 CONCLUSION
A 2D model of the saccade generating components of the superior colliculus is presented .
This model only mimics the functionality the saccadic system using mixed signal focal
plane circuits that realize motion centroid localization. The single chip combines a
silicon retina with the superior colliculus model using compact, low power and fast
circuits. Finally, the centroid chip is interfaced with an 8-bit IlC and vehicle for fast linefollowing auto navigation with obstacle avoidance. Here all of the required computation is
performed at the visual sensor, and a standard IlC is the high-level decision maker.

References
Barlow H., The Senses: Physiology of the Retina, Cambridge University Press,
Cambridge, England, 1982.
Boahen K., "Retinomorphic Vision Systems II: Communication Channel Design," ISCAS
96, Atlanta, GA, 1996.
DeWeerth, S. P., "Analog VLSI Circuits for Stimulus Localization and Centroid
Computation," Int'l 1. Computer Vision, Vol. 8, No.2, pp. 191-202, 1992.
Etienne-Cummings R., J Van der Spiegel and P. Mueller, "Neuromorphic and Digital
Hybrid Systems," Neuromorphic Systems: Engineering Silicon from Neurobiology, L.
Smith and A. Hamilton (Eds.), World Scientific, 1998.
Horiuchi T., T. Morris, C . Koch and S. P . DeWeerth, "Analog VLSI Circuits for
Attention-Based Visual Tracking," Advances in Neural Information Processing
Systems, Vol. 9, Denver, CO, 1996.
Koch C. and H. Li (Eds.), Vision Chips: Implementing Vision Algorithms with Analog
VLSI Circuits, IEEE Computer Press, 1995.
Mansfield, P., "Machine Vision Tackles Star Tracking," Laser Focus World, Vol. 30, No.
26, pp. S21 -S24, 1996.
Mead C. and M. Ismail (Eds.), Analog VLSI Implementation of Neural Networks, Kluwer
Academic Press, Newell, MA, 1989.
Sparks D., C. Lee and W. Rohrer, "Population Coding of the Direction, Amplitude and
Velocity of Saccadic Eye Movements by Neurons in the Superior Colliculus," Proc.
Cold Spring Harbor Symp. Quantitative Biology, Vol. LV, 1990.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 2458-an-autonomous-robotic-system-for-mapping-abandoned-mines.pdf

An Autonomous Robotic System
For Mapping Abandoned Mines
D. Ferguson1 , A. Morris1 , D. H?ahnel2 , C. Baker1 , Z. Omohundro1 , C. Reverte1
S. Thayer1 , C. Whittaker1 , W. Whittaker1 , W. Burgard2 , S. Thrun3
1
The Robotics Institute
Carnegie Mellon University
Pittsburgh, PA

2

Computer Science Department
University of Freiburg
Freiburg, Germany

3

Computer Science Department
Stanford University
Stanford, CA

Abstract
We present the software architecture of a robotic system for mapping
abandoned mines. The software is capable of acquiring consistent 2D
maps of large mines with many cycles, represented as Markov random
?elds. 3D C-space maps are acquired from local 3D range scans, which
are used to identify navigable paths using A* search. Our system has
been deployed in three abandoned mines, two of which inaccessible to
people, where it has acquired maps of unprecedented detail and accuracy.

1

Introduction

This paper describes the navigation software of a deployed robotic system for mapping
subterranean spaces such as abandoned mines. Subsidence of abandoned mines poses a
major problem for society, as do ground water contaminations, mine ?res, and so on. Most
abandoned mines are inaccessible to people, but some are accessible to robots. Autonomy
is a key requirement for robots operating in such environments, due to a lack of wireless
communication technology for subterranean spaces.
Our vehicle, shown in Figure 1 (see [1] for a detailed hardware description) is equipped
with two actuated laser range ?nders. When exploring and mapping unknown mines, it
alternates short phases of motion guided by 2D range scans, with phases in which the
vehicle rests to acquire 3D range scans. An analysis of the 3D scans leads to a path that
is then executed, using rapidly acquired 2D scans to determine the robot?s motion relative
to the 3D map. If no such path is found a high-level control module adjusts the motion
direction accordingly.
Acquiring consistent large-scale maps without external geo-referencing through GPS is
largely considered an open research issue. Our approach relies on ef?cient statistical techniques for generating such maps in real-time. At the lowest level, we employ a fast scan
matching algorithm for registering successive scans, thereby recovering robot odometry.
Groups of scans are then converted into local maps, using Markov random ?eld representations (MRFs) to characterize the residual path uncertainty. Loop closure is attained
by adding constraints into those MRFs, based on a maximum likelihood (ML) estimator.
However, the brittleness of the ML approach is overcome by a ?lazy? data association
mechanism that can undo and redo past associations so as to maximize the overall map
consistency.
To navigate, local 3D scans are mapped into 2 12 D terrain maps, by analyzing surface gradients and vertical clearance in the 3D scans. The result is subsequently transformed into cost

Figure 1: The Groundhog robot is
a 1,500 pound custom-built vehicle
equipped with onboard computing, laser
range sensing, gas and sinkage sensors,
and video recording equipment. Its purpose is to explore and map abandoned
mines.

functions expressed in the robot?s three-dimensional con?guration space, by convolving
the 2 12 D terrain maps with kernels that describe the robot?s footprints in different orientations. Fast A* planning is then employed in con?guration space to generate paths executed
through PD control.
The system has been tested in a number of mines. Some of the results reported here
were obtained via manual control in mines accessible to people. Others involved fully
autonomous exploration, for which our robot operated fully self-guided for several hours
beyond the reach of radio communication.

2

2D Mapping

2.1 Generating Locally Consistent Maps
As in [6, 9], we apply an incremental scan matching technique for registering scans, acquired using a forward-pointed laser range ?nder while the vehicle is in motion. This
algorithm aligns scans by iteratively identifying nearby points in pairs of consecutive range
scans, and then calculating the relative displacement and orientation of these scans by minimizing the quadratic distance of these pairs of points [2]. This approach leads to the
recovery of two quantities: locally consistent maps and an estimate of the robot?s motion.
It is well-understood [3, 6], however, that local scan matching is incapable of achieving
globally consistent maps. This is because of the residual error in scan matching, which
accumulates over time. The limitation is apparent in the map shown in Figure 2a, which is
the result of applying local scan matching in a mine that is approximately 250 meters wide.
Our approach addresses this problem by explicitly representing the uncertainty in the map
and the path using a Markov random ?eld (MRF) [11]. More speci?cally, the data acquired
through every ?ve meters of consecutive robot motion is mapped into a local map [3].
Figure 3a shows such a local map. The absolute location of orientation of the k-th map will
be denoted by ?k = ( xk yk ?k )T ; here x and y are the Cartesian coordinates and ? is
the orientation. From the scan matcher, we can retrieve relative displacement information
of the form ?k,k?1 = ( ?xk,k?1 ?yk,k?1 ??k,k?1 )T which, if scan matching was errorfree, would enable us to recover absolute information via the following recursion (under
the boundary condition ?0 = (0, 0, 0)T )
?
!
xk?1 + ?xk,k?1 cos ?k,k?1 + ?yk,k?1 sin ?k?1
yk?1 ? ?xk,k?1 sin ?k,k?1 + ?yk,k?1 cos ?k?1 (1)
?k = f (?k?1 , ?k,k?1 ) =
?k?1 + ??k,k?1
However, scan matching is not without errors. To account for those errors, our approach
generalizes this recursion into a Markov random ?eld (MRF), in which each variable ? =
?1 , ?2 , . . . is a (three-dimensional) node. This MRF is de?ned through the potentials:
?(?k , ?k?1 )

=

exp ? 21 (?k ? f (?k?1 , ?k,k?1 ))T Rk,k?1 (?k ? f (?k?1 , ?k,k?1 )) (2)

Here Rk,k?1 is the inverse covariance of the uncertainty associated with the transition
?k,k?1 . Since the MRF is a linear chain without cycles, the mode of this MRF is the solution
to the recursion de?ned in (1). Figure 3b shows the MRF for the data collected in the

(a)

(b)

Figure 2: Mine map with incremental ML scan matching (left) and using our lazy data association
approach (right). The map is approximately 250 meters wide.

Bruceton Research Mine, over a distance of more than a mile. We note this representation
generalizes the one in [11], who represent posteriors by a local bank of Kalman ?lters.
2.2

Enforcing Global Consistency

The key advantage of the MRF representation is that it encompasses the residual uncertainty in local scan matching. This enables us to alter the shape of the map in accordance
with global consistency constraints. These constraints are obtained by matching local maps
acquired at different points in time (e.g., when closing a large cycle). In particular, if the
k-th map overlaps with some map j acquired at an earlier point in time, our approach localizes the robot relative to this map using once again local scan matching. As a result, it
recovers a relative constraint ?(?k , ?j ) between the coordinates of non-adjacent maps ?k
and ?j . This constraint is of the same form as the local constraints in (2), hence is represented by a potential. For any ?xed set of such potentials ? = {?(? k , ?j )}, the resulting
MRF is described through the following negative log-likelihood function
X
? log p(?) = const. + 21
(?k ? f (?j , ?k,j ))T Rk,j (?k ? f (?j , ?k,j ))
(3)
k,j

where ? = ?1 , ?2 , . . . is the set of all map poses, and f is de?ned in (1).
Unfortunately, the resulting MRF is not a linear chain any longer. Instead, it contains
cycles. The variables ? = ?1 , ?2 , . . . can be recovered using any of the standard inference
algorithms for inference on graphs with cycles, such as the popular loopy belief propagation
algorithm and related techniques [5, 14, 17]. Our approach solves this problem by matrix
inversion. In particular, we linearize the function f using a Taylor expansion:
f (?j , ?k,j )

? f (??j ) + Fk,j (?j ? ??j )

(4)

where ??j denotes a momentary estimate of the variables ?j (e.g., the solution of the
recursion (1) without the additional data association constraints). The matrix F k,j =

(a)

(b)

Figure 3: (a) Example of a local map.
(b) The Markov random ?eld: Each
node is the center of a local map, acquired when traversing the Bruceton Research Mine near Pittsburgh, PA.

??j f (??j , ?k,j ) is the Jacobean of f (?j , ?k,j ) at ??j :
?
?
1 0 ??xk,j sin ??k + ?yk,j cos ??k
Fk,j x = ? 0 1 ??xk,j cos ??k ? ?yk,j sin ??k ?
0 0
1

(5)

The resulting negative log-likelihood is given by
X
? log p(?) ? const. +

?1
(?k ? f (??j ) ? Fk,j (?j ? ??j ))T ?k,j
(?k ? f (??j ) ? Fk,j (?j ? ??j ))

1
2

k,j

is quadratic in the variables ? of the form const. + (A? ? a)T R (A? ? a), where A is a
diagonal matrix, a is a vector, and R is a sparse matrix that is non-zero for all elements j, k
in the set of potentials. The minimum of this function is attained at (AT RA)?1 AT Ra. This
solution requires the inversion of a sparse matrix. Empirically, we ?nd that this inversion
can be performed very ef?ciently using an inversion algorithm described in [15]; it only
requires a few seconds for matrices composed of hundreds of local map positions (and it
appears to be numerically more stable than the solution in [11, 6]). Iterative application of
this linearized optimization quickly converges to the mode of the MRF, which is the set of
locations and orientations ?. However, we conjecture that recent advances on inference in
loopy graphs can further increase the ef?ciency of our approach.
2.3 Lazy Data Association Search
Unfortunately, the approach described thus far leads only to a consistent map when the
additional constraints ?(?k , ?j ) obtained after loop closure are correct. These constraints
amount to a maximum likelihood solution for the challenging data association problem
that arises when closing a loop. When loops are large, this ML solution might be wrong?a
problem that has been the source of an entire literature on SLAM (simultaneous localization
and mapping) algorithms. Figure 4a depicts such a situation, obtained when operating our
vehicle in a large abandoned mine.
The current best algorithms apply proactive particle ?lter (PF) techniques to solve this
problem [4, 8, 12, 13]. PF techniques sample from the path posterior. When closing a loop,
random variations in these samples lead to different loop closures. As long as the correct
such closure is in the set of surviving particle ?lters, the correct map can be recovered.
In the context of our present system, this approach suffers from two disadvantages: it is
computationally expensive due to its proactive nature, and it provides no mechanism for
recovery should the correct loop closure not be represented in the particle set.
Our approach overcomes both of these limitations. When closing a loop, it always picks the
most likely data association. However, it also provides a mechanism to undo and redo past
data association decisions. The exact data association algorithm involves a step that monitors the likelihood of the most recent sensor measurement given the map. If this likelihood
falls below a threshold, data association constraints are recursively undone and replaced
by other constraints of decreasing likelihood (including the possibility of not generating a
constraint at all). The search terminates if the likelihood of the most recent measurement

(a)

(b)

con?ict

?
?
?

map after adjustment

?
start

Figure 4: Example of our lazy data association technique: When closing a large loop, the robot
?rst erroneously assumes the existence of a second, parallel hallway. However, this model leads to
a gross inconsistency as the robot encounters a corridor at a right angle. At this point, our approach
recursively searches for improved data association decisions, arriving at the map shown on the right.

exceeds the threshold [7]. In practice, the threshold test works well, since global inconsistencies tend to induce gross inconsistencies in the robot?s measurements at some point in
time.
The algorithm is illustrated in Figure 4. The left panel shows the ML association after
traversing a large loop inside a mine: At ?rst, it appears that the existence of two adjacent
corridors is more likely than a single one, according to the estimated robot motion. However, as the robot approaches a turn, a noticeable inconsistency is detected. Inconsistencies
are found by monitoring the measurement likelihood, using a threshold for triggering an
exception. As a result, our data association mechanism recursively removes past data association constraints back to the most recent loop closure, and then ?tries? the second most
likely hypothesis. The result of this backtracking step is shown in the right panel of Figure 4. The backtracking requires a fraction of a second, and with high likelihood leads to
a globally consistent map and, as a side-effect, to an improved estimate of the map coordinates ?. Figure 2b shows a proto-typical corrected map, which is globally consistent.

3

Autonomous Navigation

2D maps are suf?cient for localizing robots inside mines; however, they are insuf?cient to
navigate a robot due to the rugged nature of abandoned mines. Our approach to navigation
is based on 3D maps, acquired in periodic intervals while the vehicle suspends motion to
scan its environment. A typical 3D scan is shown in Figure 5a; others are shown in Figure 7.
3.1

2 21 D Terrain Maps

In a ?rst processing step, the robot projects local 3D maps onto 2 12 D terrain maps, such as
the one shown in Figure 5b. The gray-level in this map illustrates the degree at which the
map is traversable: the brighter a 2D location, the better suited it is for navigation.
The terrain map is obtained by analyzing all measurements hx, y, zi in the 3D scan
(where z is the vertical dimension). For each rectangular surface region {x min ; xmax } ?
{ymin ; ymax }, it identi?es the minimum z-value, denoted z. It then searches for the largest
z value in this region whose distance to z does not exceed the vehicle height (plus a safety
margin); this value will be called z?. The difference z? ? z is the navigational coef?cient:
it loosely corresponds to the ruggedness of the terrain under the height of the robot. If no
measurement is available for the target region {xmin ; xmax } ? {ymin ; ymax }, the region is
marked as unknown. For safety reasons, multiple regions {xmin ; xmax } ? {ymin ; ymax }
overlap when building the terrain map. The terrain map is subsequently convolved with
a narrow radial kernel that serves as a repellent potential ?eld, to keep the robot clear of
obstacles.
3.2 Con?guration Space Maps
The terrain map is used to construct a collection of maps that describe the robot?s con?guration space, or C-space [10]. The C-space is the three-dimensional space of poses that

(a)

(b)

(c)

Figure 5: (a) A local 3D model of the mine corridor, obtained by a scanning laser range ?nder. (b)
The corresponding 2 21 D terrain map extracted from this 3D snapshot: the brighter a location, the
easier it is to navigate. (c) Kernels for generating directional C-space maps from the 2 21 D terrain
map. The two black bars in each kernel correspond to the vehicle?s tires. Planning in these C-space
maps ensures that the terrain under the tires is maximally navigable.

the vehicle can assume; it comprises the x-y location along with the vehicle?s orientation
?. The C-space maps are obtained by convolving the terrain map with oriented kernels that
describe the robot?s footprint. Figure 5c shows some of these kernels: Most value is placed
in the wheel area of the vehicle, with only a small portion assigned to the area in between,
where the vehicle?s clearance is approximately 30 centimeters. The intuition of using such
a kernel is as follows: Abandoned mines often possess railroad tracks, and while it is perfectly acceptable to navigate with a track between the wheels, traversing or riding these
tracks causes unnecessary damage to the tires and will increase the energy consumption.
The result of this transformation is a collection of C-space maps, each of which applies to
a different vehicle orientation.
3.3

Corridor Following

Finally, A* search is employed in C-space to determine a path to an unexplored area. The
A* search is initiated with an array of goal points, which places the highest value at locations at maximum distance straight down a mine corridor. This approach ?nds the best path
to traverse, and then executes it using a PD controller.
If no such path can be found even within a short range (2.5 meters), the robot decides
that the hallway is not navigable and initiates a high-level decision to turn around. This
technique has been suf?cient for our autonomous exploration runs thus far (which involved
straight hallway exploration), but it does not yet provide a viable solution for exploring
multiple hallways connected by intersections (see [16] for recent work on this topic).

4

Results

The approach was tested in multiple experiments, some of which were remotely operated
while in others the robot operated autonomously, outside the reach of radio communication.
On October 27, 2002, Groundhog was driven under manual control into the Florence Mine
near Burgettstown, PA. Figure 6b shows a picture of the tethered and remotely controlled
vehicle inside this mine, which has not been entered by people for many decades. Its
partially ?ooded nature prevented an entry into the mine for more than approximately 40
meters. Maps acquired in this mine are shown in Figure 9.
On May 30, 2003, Groundhog successfully explored an abandoned mine using the fully
autonomous mode. The mine, known as the Mathies Mine near Pittsburgh, is part of a
large mine system near Courtney, PA. Existing maps for this mine are highly inaccurate,
and the conditions inside the mine were unknown to us. Figure 6a shows the robot as it
enters the mine, and Figure 7a depicts a typical 3D scan acquired in the entrance area.

(a)

(b)

Figure 6: (a) The vehicle as it enters the Mathies Mine on May 30, 2003. It autonomously descended
308 meters into the mine before making the correct decision to turn around due to a blockage inside
the mine. (b) The vehicle, as it negotiates acidic mud under manual remote control approximately 30
meters into the Florence Mine near Burgettstown, PA.
(a)

(b)

Figure 7: 3D local maps: (a) a typical corridor map that is highly navigable. (b) a map of a broken
ceiling bar that renders the corridor segment unnavigable. This obstacle was encountered 308 meters
into the abandoned Mathies Mine.

Figure 8: Fraction of the 2D mine map of the Mathies Mine, autonomously explored by the Groundhog vehicle. Also shown is the path of the robot and the locations at which it chose to take 3D scans.
The protruding obstacle shows up as a small dot-like obstacle in the 2D map.
(a)

(b)

(c)

Figure 9: (a) A small 2D map acquired by Groundhog in the Florence Mine near Burgettstown, PA.
This remotely-controlled mission was aborted when the robot?s computer was ?ooded by water and
mud in the mine. (b) View of a local 3D map of the ceiling. (c) Image acquired by Groundhog inside
the Mathies Mine (a dry mine).

After successfully descending 308 meters into the Mathies Mine, negotiating some rough
terrain along the way, the robot encountered a broken ceiling beam that draped diagonally
across the robot?s path. The corresponding 3D scan is shown in Figure 7b: it shows rubble
on the ground, along with the ceiling bar and two ceiling cables dragged down by the bar.
The robot?s A* motion planner failed to identify a navigable path, and the robot made the
appropriate decision to retreat. Figure 8 shows the corresponding 2D map; the entire map
is 308 meters long, but here we only show the ?nal section, along with the path and the
location at which the robot stop to take a 3D scan. An image acquired in this mine is
depicted in Figure 9c.

5

Conclusion

We have described the software architecture of a deployed system for robotic mine mapping. The most important algorithmic innovations of our approach are new, lazy techniques
for data association, and a fast technique for navigating rugged terrain. The system has
been tested under extreme conditions, and generated accurate maps of abandoned mines
inaccessible to people.

Acknowledgements
We acknowledge the contributions of the students of the class 16865 Mobile Robot Development
at CMU who helped build Groundhog. We also acknowledge the assistance provided by Bruceton
Research Mine (Paul Stefko), MSHA, PA-DEP, Workhorse Technologies, and the various people
in the mining industry who supported this work. Finally, we also gratefully acknowledge ?nancial
support by DARPA?s MARS program.

References
[1] C. Baker, Z. Omohundro, S. Thayer, W. Whittaker, M. Montemerlo, and S. Thrun. A case study
in robotic mapping of abandoned mines. FSR-03.
[2] P. Besl and N. McKay. A method for registration of 3d shapes. PAMI 14(2), 1992.
[3] M. Bosse, P. Newman, M. Soika, W. Feiten, J. Leonard, and S. Teller. An atlas framework for
scalable mapping. ICRA-03.
[4] A. Eliazar and R. Parr. DP-SLAM: Fast, robust simultaneous localization and mapping without
predetermined landmarks. IJCAI-03.
[5] Anshul Gupta, George Karypis, and Vipin Kumar. Highly scalable parallel algorithms for sparse
matrix factorization. Trans. Parallel and Distrib. Systems, 8(5), 1997.
[6] J.-S. Gutmann and K. Konolige. Incremental mapping of large cyclic environments. CIRA-00.
[7] D. H?
ahnel, W. Burgard, B. Wegbreit, and S. Thrun. Towards lazy data association in SLAM.
11th International Symposium of Robotics Research, Sienna, 2003.
[8] D. H?
ahnel, D. Fox, W. Burgard, and S. Thrun. A highly ef?cient FastSLAM algorithm for generating cyclic maps of large-scale environments from raw laser range measurements. Submitted
to IROS-03.
[9] D. H?
ahnel, D. Schulz, and W. Burgard. Map building with mobile robots in populated environments. IROS-02.
[10] J.-C. Latombe. Robot Motion Planning. Kluwer, 1991.
[11] F. Lu and E. Milios. Globally consistent range scan alignment for environment mapping. Autonomous Robots: 4, 1997.
[12] M. Montemerlo, S. Thrun, D. Koller, and B. Wegbreit. FastSLAM 2.0: An improved particle
?ltering algorithm for simultaneous localization and mapping that provably converges. IJCAI03.
[13] K. Murphy. Bayesian map learning in dynamic environments. NIPS-99.
[14] K.P. Murphy, Y. Weiss, and M.I. Jordan. Loopy belief propagation for approximate inference:
An empirical study. UAI-99
[15] W. H. Press. Numerical recipes in C: the art of scienti?c computing. Cambridge Univ. Press,
1988.
[16] R. Simmons, D. Apfelbaum, W. Burgard, M. Fox, D. an Moors, S. Thrun, and H. Younes.
Coordination for multi-robot exploration and mapping. AAAI-00.
[17] M. J. Wainwright. Stochastic processes on graphs with cycles: geometric and variational approaches. PhD thesis, MIT, 2002.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 631-input-reconstruction-reliability-estimation.pdf

Input Reconstruction Reliability Estimation

Dean A. Pomerleau
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213

Abstract
This paper describes a technique called Input Reconstruction Reliability Estimation
(IRRE) for determining the response reliability of a restricted class of multi-layer
perceptrons (MLPs). The technique uses a network's ability to accurately encode
the input pattern in its internal representation as a measure of its reliability. The
more accurately a network is able to reconstruct the input pattern from its internal
representation, the more reliable the network is considered to be. IRRE is provides
a good estimate of the reliability of MLPs trained for autonomous driving. Results
are presented in which the reliability estimates provided by IRRE are used to select
between networks trained for different driving situations.

1 Introduction
In many real world domains it is important to know the reliability of a network's response
since a single network cannot be expected to accurately handle all the possible inputs. Ideally,
a network should not only provide a response to a given input pattern, but also an indication of
the likelihood that its response is "correct". This reliability measure could be used to weight
the outputs from multiple networks and to determine when a new network needs to be trained.
This paper describes a technique for estimating a network's reliability called Input Reconstruction Reliability Estimation (IRRE). IRRE relies on the fact that the hidden representation
developed by an artificial neural network can be considered to be a compressed representation
of important input features. For example, when the network shown in Figure 1 is trained to
produce the correct steering direction from images of the road ahead, the hidden units learn to
encode the position and orientation of important features like the road edges and lane markers
(See [pomerleau, 1991] for more details). Because there are many fewer hidden units than
input units in the network, the hidden units cannot accurately represent all the details of an
279

280

Pomerleau
SUp

Ri&hI

30 Output

Units

3Ox32 SelUlor
Input Retina

Figure 1: Original driving network architecture.

arbitrary input pattern. Instead, the hidden units learn to devote their limited representational
capabilities to encoding the position and orientation of consistent, frequently-occurring features from the training set. When presented with an atypical input, such as a road with a
different number of lanes, the feature detectors developed by the hidden units will not be
capable of accurately encode all the actual input features.
Input Reconstruction Reliability Estimation exploits this limitation in representational capacity
to estimate a network's reliability. In IRRE, the network's internal representation is used to
reconstruct in the input pattern being presented. The more closely the reconstructed input
matches the actual input, the more familiar the input and hence the more reliable the network's
response.

2 Reconstructing the Input
IRRE utilized an additional set of output units to perform input reconstruction called the
encoder output array, as depicted in Figure 2. This second set of output units has the same
dimensionality as the input retina. In the experiments described in this paper, the input layer
and encoder output array have 30 rows and 32 columns. The desired activation for each of
these additional output units is identical to the activation of the corresponding input unit. In
essence, these additional output units turn the network into an autoencoder.
The network is trained using backpropagation both to produce the correct steering response
on the steering output units, and to reconstruct the input image as accurately as possible
on the encoder output array. During the training process, the network is presented with
several hundred images taken with a camera onboard our test vehicle as a person drives
(See [pomerleau, 1991] for more details). Training typically requires approximately 3 minutes
during which the person drives over a 1/4 to 1/2 mile stretch of road.

Input Reconstruction Reliability Estimation

Figure 2: Network architecture augmented to include an encoder output array.
During testing on a new stretch of road, images are presented to the network and activation is
propagated forward through the network to produce a steering response and a reconstructed
input image. The reliability of the steering response is estimated by computing the correlation
coefficient p(l , R) between the activation levels of units in the actual input image I and the
reconstructed input image R using the following formula:
p(I ,R)

= iR - I . II
01 OR

where I and R are the mean activation value of the actual and the reconstructed images, IR is
the mean of the set formed by the unit-wise product of the two images, and u/ and UR represent
the standard deviations of the activation values of each image. The higher the correlation
between the two images, the more reliable the network's response is estimated to be. The
reason correlation is used to measure the degrees of match between the two images is that,
unlike Euclidean distance, the correlation measure is invariant to differences in the mean
and variance between the two images. This is important since the mean and variance of the
input and the reconstructed images can sometimes vary, even when the input image depicts a
familiar situation.

3 Results and Applications
The degree of correlation between the actual and the reconstructed input images is an extremely
good indicator of network response accuracy in the domain of autonomous driving, as shown
in Figure 3. It shows a trained network's steering error and reconstruction error as the vehicle
drives down a quarter mile stretch of road that starts out as a single lane path and eventually
becomes a two-lane street. The solid line indicates the network's steering error, as measured
by the difference in turn curvature between the network's steering response and a person's

281

282

Pomerleau

Steering Error

CoJTelation Coeff"u:ienl

=0.92

ii.PU;Reoo~n&ror

1.25

0.04
~

g-;;-

0.03

~~
.....
b.OV

'Sv ........e

-

v.....
en

0.02

Intersection

?

1.00

~

c::~
o~

~l.

'a ..
0.75

~b:

0.50

8--

jg~

0.01
?
L -__________________________________
000

~

~

~

O~

I.....t__---- One Lane Road Im.gel----~..~nll"'..---.r--t..~1
Two Lane Road Images

Figure 3: Reconstruction error obtained using autoencoder reconstruction versus network
steering error over a stretch of one-lane and two-lane road.

steering response at that point along the road. The dashed line represents the network's
"reconstruction error", which is defined to be the degree of statical independence between the
actual and reconstructed images, or 1 - p(I , R}.
The two curves are nearly identical, having a correlation coefficient of 0.92. This close match
between the curves demonstrates that when the network is unable to accurately reconstruct the
input image, it is also probably suggesting an incorrect steering direction. Visual inspection
of the actual and reconstructed input images demonstrates that the degree of resemblance
between them is a good indication of the actual input's familiarity, as shown in Figure 4. It
depicts the input image, network response, and reconstructed input at the three points along the
road,labeled A, Band C in Figure 3. When presented with the image at point A, which closely
resembles patterns from training set, the network's reconstructed image closely resembles the
actual input, as shown by the close correspondence between the images labeled "Input Acts"
and "Reconstructed Input" in the left column of Figure 4. This close correspondence between
the input and reconstructed images suggests that the network can reliably steer in this situation.
It in fact it can steer accurately on this image, as demonstrated by the close match between the
network's steering response labeled "Output Acts" and the desired steering response labeled
''Target Acts" in the upper left corner of Figure 4.
When presented with a situation the network did not encounter during training, such as the
fork image and the two-lane road image shown in the other two columns of Figure 4, the
reconstructed image bears much less resemblance to the original input. This suggests that the
network is confused. This confusion results in an incorrect steering response, illustrated in
the discrepancy between the network's steering response and the target steering response for
the two atypical images.
The reliability prediction provided by IRRE has been used to improve the performance of
the neural network based autonomous driving system in a number of ways. The simplest is

Input Reconstruction Reliability Estimation

Figure 4: The actual input. the reconstructed input and the point-wise absolute difference
between them on a road image similar to those in the training set (labeled A), and on two
atypical images (labeled B and C).

283

284

Pomerleau

One Lane Net Rcconstuction Error
TW~ line-Net Rct;;n;tiiCtion Eir~;

1.00

'i,

t-' :! v?, ita
..
f \ :
~
I

0.60

0.40

\

\

1
.'

~

0.80

i ./
';. 1 'V:
\.J

II

I

\..

f

L i\

I

aI
II

!iJ ??

:!I\

?
I

,,':: \.:1 i !=

"\

."

\ ii
?

.:

\\!

,

~

?

II

? II

1 !i

I

II

y

(\

OJ

.

:: \\.-:
,.
,. ,.

0.20
.

~.

J=

'\i

,

,:
:

....::
\_?? .:

O.OOL.-_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ __

I..

One Lane Images

---11-- Two Lane Images -..j

Figure 5: Reconstruction error of networks trained for one-lane road driving (solid line) and
two-lane road driving (dashed line),

to use IRRE to control vehicle speed. The more accurate the input reconstruction, the more
confident the network, and hence the faster the system drives the vehicle. A second use the
system makes of the reliability estimate provided by IRRE is to update the vehicle's position
on a rough map of the area being driven over. When the map indicates there should be an
intersection or other confusing situation up ahead, a subsequent sharp rise in reconstruction
error is a good indication that the vehicle has actually reached that location, allowing the
system to pinpoint the vehicle's position. Knowing the vehicle's location on a map is useful
for integrating symbolic processing, such as planning, into the neural network driving system
(for more details, see [pomerleau et al., 1991]).
Figure 5 illustrates how IRRE can be used to integrate the outputs from multiple expert
networks. The two lines in this graph illustrate the reconstruction error of two networks, one
trained to steer on one-lane roads (solid line), and the other trained to steer on two-lane roads
(dashed line). The reconstruction error for the one-lane network is low on the one-lane road
images, and high on the two-lane road images. The opposite is true for the network trained
for two-lane road driving. The reliability estimate provided by IRRE allows the system to
determine which network is most reliable for driving in the current situation. By simulating
multiple networks in parallel, and then selecting the one with the highest reliability, the system
have been able to drive the Navlab test vehicle on one, two and four lane roads automatically
at speeds of up to 30 miles per hour.

4 Discussion
The effectiveness of input reconstruction reliable estimation stems from the fact that the network has a small number of hidden units and is only trained in a narrow range of situations.
These constraints prevent the network from faithfully encoding arbitrary input patterns. Instead, the hidden units learn to encode features in the training images that are most important
for the task. Baldi and Hornik [Baldi & Hornik, 1989] have shown that if an autoencoder

Input Reconstruction Reliability Estimation

network with a single layer of N linear hidden units is trained with back-propagation, the activation levels of the hidden units will represent the first N principal components of the training
set. Since the units in the driving network are non-linear, this assertion does not strictly hold in
this case. However, Cottrell and Munro [Cottrell & Munro, 1988] have found empirically that
autoencoder networks with a sigmoidal activation function develop hidden units that span the
principal subspace of the training images, with some noise on the first principal component
due to network non-linearity. Because the principal components represent the dimensions
along which the training examples varies most, it can be shown that using linear combinations
of the principal components to represent the individual training patterns optimally preserves
the information contained in the training set [Linsker, 1989].
However the compressed representation developed by a linear autoencoder network is only
optimal for encoding images from the same distribution as the training set. When presented
with images very different from those in the training set, the image reconstructed from the
internal representation is not as accurate. The results presented in this paper demonstrate that
this reconstruction error can be employed to estimate the likelihood and magnitude of error in
MLPs trained for autonomous driving.
However the input reconstruction technique presented here has a serious potential shortcoming, namely that it forces the network's hidden units to encode all input features, including
potentially irrelevant ones. While this increased representation load on the hidden units has
the potential to degrade network performance, this effect has not been observed in the tests
conducted so far. In support of this finding, Gluck [Gluck, personal communications] has
found that forcing a network to autoencode its input frequently improves its generalization.
In [Gluck & Myers, 1992], Gluck and Myers use the representation developed by an autoencoder network as a model for simple types of learning in biological systems. The model
suggests that the hippocampus acts as an autoencoder, developing internal representations that
are then used to perform other tasks.
But if interference from the autoencoder task proves to be a problem, one way to eliminate
it would be to have separate groups of hidden units connected exclusively to one group of
outputs or the other. Having a separate set of hidden units for the autoencoder task would
ensure that the representation developed for the input reconstruction does not interfere with
representation developed for the "normal" task. It remains to be seen if this decoupling of
internal representations will adversely affect IRRE's ability to predict network errors.
As a technique for multi-network integration, IRRE has several advantages over existing connectionist arbitration methods, such as Hampshire and Waibel's Meta-Pi architecture [Hampshire & Waibel, 1992] and the Adaptive Mixture of Experts Model of Jacobs et
aI. [Jacobs et al., 1991]. It is a more modular approach, since each expert can be trained
entirely in isolation and then later combined with other experts without any additional training
by simply selecting the most reliable network for the current input. Since IRRE provides an
absolute measure of a single network's reliability, and not just a measure of how appropriate
a network is relative to others, IRRE can be also used to determine when none of the experts
is capable of coping with the current situation.
A potentially interesting extension to IRRE is the development of techniques for reasoning
about the difference between the actual input and the reconstructed input. For instance, it
should be possible to recognize when the vehicle has reached a fork in the road by the characteristic mistakes the network makes in reconstructing the input image. Another important
component of future work in is to test the ability of IRRE to estimate network reliability in
domains other than autonomous driving.

285

286

Pomerleau

Acknowledgements
I thank Dave Touretzky, Chuck Thorpe and the entire Unmanned Ground Vehicle Group at
CMU for their support and suggestions. Principle support for this research has come from
DARPA, under contracts "Perception for Outdoor Navigation" (contract number DACA7689-C-OOI4, monitored by the US Army Topographic Engineering Center) and ''Unmanned
Ground Vehicle System" (contract number DAAE07-90-C-R059, monitored by TACOM).

References
[Baldi & Hornik,1989] Baldi, P. and Hornik, K. (1989) Neural networks and principal component analysis: Learning from examples without local minima. Neural Networks, Vol.
2 pp. 53-58.
[Cottrell & Munro, 1988] Cottrell, G.W. and Munro, P. (1988) Principal components analysis
ofimages via back-propagation. Proc. Soc. ofPhoto-Opticallnstr. Eng., Cambridge MA.
[Gluck, personal communications] Gluck, M.A. (1992) Personal Communications. Rutgers
Univ., Newark NJ.
[Gluck & Myers, 1992] Gluck, M.A. and Myers, C.E. (1992) Hippocampal function in representation and generalization: a computational theory. Proc.1992 Cogn. Sci. Soc. Con/.
Hillsdale, NJ: Erlbaum Assoc.
[Hampshire & Waibel, 1992] Hampshire J.B. and Waibel, A.H. (1989) The Meta-Pi network:
building distributed knowledge representations for robust pattern recognition. IEEE
Trans. on Pattern Analysis and Machine Intelligence.
[Jacobs et al., 1991] Jacobs, R.A., Jordan, M.I. Nowlan, SJ. and Hinton, G.E. (1991) Adaptive mixtures of local experts. Neural Computation, 3 :1, Terrence Sejnowski (ed).
[Linsker, 1989] Linsker, R. (1989) Designing a sensory processing system: What can be
learned from principal component analysis? IBM Technical Report RC 14983 (#66896).
[pomerleau, 1991] Pomerleau, D.A. (1991) Efficient Training of Artificial Neural Networks
for Autonomous Navigation. Neural Computation 3:1, Terrence Sejnowski (ed).
[pomerleau et al., 1991] Pomerleau, D.A., Gowdy, J., Thorpe, C.E. (1991) Combining artificial neural networks and symbolic processing for autonomous robot guidance. Engineering Applications of Artificial Intelligence, 4:4 pp. 279-285.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 52-teaching-artificial-neural-systems-to-drive-manual-training-techniques-for-autonomous-systems.pdf

693

Teaching Artificial Neural Systems to Drive:
Manual Training Techniques for Autonomous Systems

J. F. Shepanski and S. A. Macy

TRW, Inc .
One Space Park, 02/1779
Redondo Beach, CA 90278

Abetract
We have developed a methodology for manually training autononlous control systems
based on artificial neural systems (ANS). In applications where the rule set governing an expert's
decisions is difficult to formulate, ANS can be used to ext.ra.c:t rules by associating the information
an expert receives with the actions h~ takes . Properly constructed networks imitate rules of
behavior that permits them to function autonomously when they are trained on the spanning set
of possible situations. This training can be provided manually, either under the direct. supervision
or a system trainer, or indirectly using a background mode where the network assimilates training
data as the expert perrorms his day-to-day tasks. To demonstrate these methods we have trained
an ANS network to drive a vehicle through simulated rreeway traffic.

I ntJooducticn
Computational systems employing fine grained parallelism are revolutionizing the way we
approach a number or long standing problems involving pattern recognition and cognitive processing. The field spans a wide variety or computational networks, rrom constructs emulating neural
runctions, to more crystalline configurations that resemble systolic arrays. Several titles are used
to describe this broad area or research, we use the term artificial neural systems (ANS). Our concern in this work is the use or ANS ror manually training certain types or autonomous systems
where the desired rules of behavior are difficult to rormulate.
Artificial neural systems consist of a number or processing elements interconnected in a
weighted, user-specified fashion, the interconnection weights acting as memory ror the system.
Each processing element calculatE',> an output value based on the weighted sum or its inputs. In
addition, the input data is correlated with the output or desired output (specified by an instructive
agent) in a training rule that is used to adjust the interconnection weights. In this way the ne~
work learns patterns or imitates rules of behavior and decision making.
The partiCUlar ANS architecture we use is a variation of Rummelhart et. al. [lJ multi-layer
perceptron employing the generalized delta rule (GD R). Instead of a single, multi-layer ,structure, our final network has a a multiple component or "block" configuration where one blOt'k'~
output reeds into another (see Figure 3). The training methodology we have developed is not
tied to a particular training rule or architecture and should work well with alternative networks
like Grossberg's adaptive resonance model[2J.

? American Institute of Physics 1988

694

The equations describing the network are derived and described in detail by Rumelhart et.
al.[l]. In summary, they are:
Transfer function:

Sj =

?

E WjiOi;

(1)

i-O

Weight adaptation rule:
Error calculation:

Awl'?? =( 1- a l'..)n., l'??0 J?0??

OJ

+ a l'??Awp.revious
.'
l'

'"

=0j{1- OJ) E0.tW.ti,

( 2)

( 3)

.t=1

where OJ is the output or processing element j or a sensor input, wi is the interconnection weight
leading from element ito i, n is the number of inputs to j, Aw is the adjustment of w, '1 is the
training constant, a is the training "momentum," OJ is the calculated error for element i, and m
is the Canout oC a given element. Element zero is a constant input, equal to one, so that. WjO is
equivalent to the bias threshold of element j. The (1- a) factor in equation (2) differs from standard GDR formulation, but. it is useful for keeping track of the relative magnitudes of the two
terms. For the network's output layer the summation in equation (3) is replaced with the
difference between the desired and actual output value of element j.
These networks are usually trained by presenting the system with sets of input/output data
vectors in cyclic fashion, the entire cycle of database presentation repeated dozens of times . This
method is effective when the training agent is a computer operating in batch mode, but would be
intolerable for a human instructor. There are two developments that will help real-time human
training. The first is a more efficient incorporation of data/response patterns into a network. The
second, which we are addressing in this paper, is a suitable environment wherein a man and ANS
network can interact in training situation with minimum inconvenience or boredom on the
human's part. The ability to systematically train networks in this fashion is extremely useful for
developing certain types of expert systems including automatic signal processors, autopilots,
robots and other autonomous machines. We report a number of techniques aimed at facilitating
this type of training, and we propose a general method for teaching these networks .
System. Development

Our work focuses on the utility of ANS for system control. It began as an application of
Barto and Sutton's associative search network[3]. Although their approach was useful in a
number of ways, it fell short when we tried to use it for capturing the subtleties of human
decision-making. In response we shifted our emphasis rrom constructing goal runctions for
automatic learning, to methods for training networks using direct human instruction. An integral
part or this is the development or suitable interraces between humans, networks and the outside
world or simulator. In this section we will report various approaches to these ends, and describe a
general methodology for manually teaching ANS networks . To demonstrate these techniques we
taught a network to drive a robot vehicle down a simulated highway in traffic. This application
combines binary decision making and control of continuous parameters.
Initially we investigated the use or automatic learning based on goal functions[3] for training control systems. We trained a network-controlled vehicle to maintain acceptable following
distances from cars ahead or it. On a graphics workstation, a one lane circular track was

695

constructed and occupied by two vehicles: a network-controlled robot car and a pace car that
varied its speed at random .. Input data to the network consisted of the separation distance and
the speed of the robot vehicle . The values of a goal function were translated into desired output
for GDR training. Output controls consisted of three binary decision elements : 1) accelerate one
increment of speed, 2) maintain speed, and 3) decelerate one increment of speed. At all times
the desired output vector had exactly one of these three elements active . The goal runction was
quadratic with a minimum corresponding to the optimal following distance. Although it had no
direct control over the simulation, the goal function positively or negatively reinforced the
system's behavior.
The network was given complete control of the robot vehicle, and the human trainer had
no influence except the ability to start and terminate training. This proved unsatisractory because
the initial system behavior--governed by random interconnection weights--was very unstable. The
robot tended to run over the car in rront of it before significant training occurred . By carerully
halting and restarting training we achieved stable system behavior. At first the rollowing distance
maintained by the robot car oscillated as ir the vehicle was attached by a sj)ring to the pace car.
This activity gradually damped. Arter about one thousand training steps the vehicle maintained
the optimal following distance and responded quickly to changes in the pace car's speed.
Constructing composite goal functions to promote more sophisticated abilities proved
difficult, even ill-defined, because there were many unspecified parameters. To generate goal
runctions ror these abilities would be similar to conventional programming--the type or labor we
want to circumvent using ANS. On the other hand, humans are adept at assessing complex situations and making decisions based on qualitative data, but their "goal runctions" are difficult ir not
impossible to capture analytically. One attraction of ANS is that it can imitate behavior based on
these elusive rules without rormally specifying them. At this point we turned our efforts to
manual training techniques.
The initially trained network was grafted into a larger system and augmented with additional inputs: distance and speed inrormation on nearby pace cars in a second traffic lane, and an
output control signal governing lane changes . The original network's ability to maintain a safe
following distance was retained intact. Thts grafting procedure is one of two methods we studied
for adding ne .... abilities to an existin, system. (The second, which employs a block structure, is
described below.) The network remained in direct control of the robot vehicle, but a human
trainer instructed it when and when not to change lanes. His commands were interpreted as the
desired output and used in the GDR training algorithm. This technique, which we call coaching,
proved userul and the network quickly correlated its environmental inputs with the teacher's
instructions. The network became adept at changing lanes and weaving through traffic. We found
that the network took on the behavior pattern or its trainer. A conservative teacher produced a
timid network, while an aggressive tzainer produced a network that tended to cut off other automobiles and squeeze through tight openings . Despite its success, the coaching method of training
did not solve the problem or initial network instability.
The stability problem was solved by giving the trainer direct control over the simulation.
The system configuration (Figure 1), allows the expert to exert control or release it to the n~t?
work. During initial tzaining the expert is in the driver's seat while the network acts the role of

696

apprentice. It receives sensor information, predicts system commands, and compares its predictions. against the desired output (ie. the trainer's commands) . Figure 2 shows the data and command flow in detail. Input data is processed through different channels and presented to the
trainer and network. Where visual and audio formats are effective for humans, the network uses
information in vector form. This differentiation of data presentation is a limitation of the system;
removing it is a cask for future ~search. The trainer issues control commands in accordance with
his assigned ~k while the network takes the trainer's actions as desired system responses and
correlates these with the input. We refer to this procedure as master/apprentice training, network
training proceeds invisibly in the background as the expert proceeds with his day to day work. It
avoids the instability problem because the network is free to make errors without the adverse
consequence of throwing the operating environment into disarray.
I

Input

World (--> sensors)

l+

or

Simulation
~------------------~

~

Actuation

I

Ne',WOrk

~-

I

Expert

Commands
+
~------~---------------------------~
J

Figure 1. A scheme for manually training ANS networks. Input data is received by both
the network and trainer. The trainer issues commands that are actuated (solid command
line). or he coaches the network in how it ought to respond (broken command line).

--+ Commands

Preprocessing
tortunan
Input
data
Preprocessing
for network

N twork
e

t

--+

Predicted
commands

~
9'l. Actuation

.1-r"

'-------------.
Coaching/emphasis

Training
rule

Fegure 2. Data and convnand flow In the training system. Input data is processed and presented

to the trainer and network. In master/appre~ice training (solid command Hne). the trainer's
orders are actuated and the network treats his commands as the system's desired output. In
coaching. the network's predicted oonvnands are actuated (broken command line). and the
trainer influences weight adaptation by specifying the desired system output and controlHng
the values of trailing constants-his -suggestions- are not cirec:tty actuated.
Once initial. bacqround wainmg is complete, the expert proceeds in a more formal
manner to teach the network. He releases control of the command system to the network in
order to evaluate ita behavior and weaknesses. He then resumes control and works through a

697

series of scenarios designed to train t.he network out of its bad behavior. By switching back and
forth. between human and network control, the expert assesses the network's reliability and
teaches correct responses as needed. We find master/apprentice training works well for behavior
involving continuous functions, like steering. On the other hand, coaching is appropriate for decision Cunctions, like when Ule car ought to pass. Our methodology employs both techniques.
The Driving Network
The fully developed freeway simulation consists of a two lane highway that is made of
joined straight and curved segments which vary at. random in length (and curvature). Several
pace cars move at random speeds near the robot vehicle. The network is given the tasks of tracking the road, negotiating curves. returning to the road if placed far afield, maintaining safe distances from the pace cars, and changing lanes when appropriate. Instead of a single multi-layer
structure, the network is composed of two blocks; one controls the steering and the other regulates speed and decides when the vehicle should change lanes (Figure 3). The first block receives
information about the position and speed of the robot vehicle relative to other ears in its vicinity.
Its output is used to determine the automobile's speed and whet.her the robot should change
lanes . The passing signal is converted to a lane assignment based on the car's current lane position. The second block receives the lane assignment and data pertinent to the position and orientation of the vehicle with respect to the road. The output is used to determine the steering angle
of the robot car.

Block 1

Inputs

Outputs

Constant.
Speed.
Disl. Ahead, Pl ?
Disl. Ahead, Ol ?
Dist. Behind, Ol ?
ReI. Speed Ahead, Pl ?
ReI. Speed Ahead, Ol ?
ReI. Speed Behind, Ol ?

I

Speed
Change lanes

?

Steering Angle

Convert lane change to lane number
Constant
Rei. Orientation
-..--t~ lane Nurmer
lateral Dist.
Curvature

?
?
?
?
?

??
?

Figure 3. The two blocks of the driving ANS network. Heavy arrows Indicate total interconnectivity
between layers. PL designates the traffic lane presently oca.apied by the robot vehicle, Ol refers
to the other lane, QJrvature refers to the road, lane nurrber is either 0 or 1, relative orientation and
lateral distance refers to the robot car's direction and podion relative to the road'l direction and
center line. respectively.
.

698

The input data is displayed in pictorial and textual form to the driving instructor. He views
the road and nearby vehicles from the perspective of the driver's seat or overhead. The network
receives information in the form of a vector whose elements have been scaled to unitary order,
O( 1) . Wide ranging input parameters, like distance, are compressed using the hyperbolic tangent
or logarithmic functions . In each block , the input layer is totally interconnected to both the ou~
put and a hidden layer. Our scheme trains in real time, and as we discuss later, it trains more
smoothly with a small modification of the training algorithm .
Output is interpreted in two ways: as a binary decision or as a continuously varying parameter. The first simply compares the sigmoid output against a threshold. The second scales the
output to an appropriate range for its application . For example, on the steering output element, a
0.5 value is interpreted as a zero steering angle. Left and right turns of varying degrees are initiated when this output is above or below 0.5, respectively.
The network is divided into two blocks that can be trained separately. Beside being conceptually easier to understand , we find this component approach is easy to train systematically.
Because each block has a restricted, well-defined set of tasks, the trainer can concentrate
specifically on those functions without being concerned that other aspects of the network behavior
are deteriorating.
"'e trained the system from bottom up, first teaching the network to stay on the road ,
negotiate curves , chan~e lanes, and how to return if the vehicle strayed off the highway. Block 2,
responsible for steering, learned these skills in a few minutes using the master/apprentice mode.
It tended to steer more slowly than a human but further training progressively improved its
responsiveness.
We experimented with different trammg constants and "momentum" values. Large "
values, about 1, caused weights to change too coarsely. " values an order of magnitude smaller
worked well . We found DO advantage in using momentum for this method of training , in fact,
the system responded about three times more slowly when 0 =0.9 than when the momentt:m
term was dropped. Our standard training parameters were" =0.2, and Cl' =00

a)

~

Db)~~

=D-=-~=~~--=~--= ~

Figure 4. Typical behavior of a network-controlled vehicle (dam rectangle) when trained by
a) a conservative miYer, ItI:I b}. reckless driver. Speed Is indicated by the length of the arrows.
After Block 2 "Was trained, we gave steering control to the network and concentrated on
teaching the network to change lanes and adjust speed. Speed control in this ('"asP. was a continuous variable and was best taught using master/apprentice training. On the other hand, the binary
decision to change lanes was best taught by coaching . About ten minutes of training were needed
to teach the network to weave through traffic. We found that the network readily adapts the

699

behavioral pattern of its trainer. A conservative trainer generated a network that hardly ever
passed, while an aggressive trainer produced a network that drove recklessly and tended to cut off
other-cars (Figure 4).
Discussion
One of the strengths of el:pert 5ystf'mS based on ANS is that the use of input data in the
decision making and control proc~ss does not have to be specified . The network adapts its internal weights to conform to input/ output correlat.ions it discovers . It is important, however, that
data used by the human expert is also available to the network. The different processing of sensor data for man and network may have important consequences, key information may be
presented to the man but not. the machine.
This difference in data processing is particularly worrisome for image data where human
ability to extract detail is vastly superior to our au tomatic image processing capabilities. Though
we would not require an image processing system to understand images, it would have to extract
relevant information from cluttered backgrounds. Until we have sufficiently sophisticated algorithms or networks to do this, our efforts at constructing expert systems which halldle image data
are handicapped .
Scaling input data to the unitary order of magnitude is important for training stability. 111is
is evident from equations (1) and (2) . The sigmoid transfer function ranges from 0.1 to 0.9 in
approximat.eiy four units, that is, over an 0(1) domain. If system response must change in reaction to a large, O( n) swing of a given input parameter, the weight associated with that input will
be trained toward an O( n- 1) magnitude. On the other hand, if the same system responds to an
input whose range is O( 1), its associated weight will also be 0(1). The weight adjustment equation does not recognize differences in weight magnitude, therefore relatively small weights will
undergo wild magnitude adjustments and converge weakly. On the other hand, if all input parameters are of the same magnitude their associated weights will reflect this and the training constant
can be adjusted for gentle weight convergence . Because the output of hidden units are constrained between zero and one, O( 1) is a good target range for input parameters. Both the hyperbolic tangent and logarithmic functions are useful for scaling wide ranging inputs . A useful form
of the latter is
.8[I+ln(x/o)]
.8x/o
-.8[I+ln(-%/o)]

if o<x,
if-o::;x::;o,
ifx<-o,

( 4)

where 0>0 and defines the limits of the intermediate linear section, and .8 is a scaling factor.
This symmetric logarithmic function is continuous in its first derivative, and useful when network
behavior should change slowly as a parameter increases without bound. On the othl'r hand, if the
system should approach a limiting behavior, the tanh function is appropriate.
Weight adaptation is also complicated by relaxing the common practice of restricting interconnections to adjacent layers. Equation (3) shows that the calculated error for a hidden layergiven comparable weights, fanouts and output errors-will be one quarter or less than that of the

700

output layer. This is caused by the slope ractor, 0 .. ( 1- oil. The difference in error magnitudes is
not noticeable in networks restricted to adjacent layer interconnectivity. But when this constraint
is released the effect of errors originating directly from an output unit has 4" times the magnitude
and effect of an error originating from a hidden unit removed d layers from the output layer.
Compared to the corrections arising from the output units, those from the hidden units have little
influence on weight adjustment, and the power of a multilayer structure is weakened . The system
will train if we restrict connections to adjacent layers, but it trains slowly. To compensate for this
effect we attenuate the error magnitudes originating from the output layer by the above factor.
This heuristic procedure works well and racilitates smooth learning.
Though we have made progress in real-time learning systems using GDR, compared to
humans-who can learn from a single data presentation-they remain relatively sluggish in learning
and response rates. We are interested in improvements of the GDR algorithm or alternative
architectures that facilitate one-shot or rapid learning. In the latter case we are considering least
squares restoration techniquesl4] and Grossberg and Carpenter's adaptive resonance modelsI3,5].
The construction of automated expert systems by observation of human personnel is
attractive because of its efficient use of the expert's time and effort. Though the classic AI
approach of rule base inference is applicable when such rules are clear cut and well organized, too
often a human expert can not put his decision making process in words or specify the values of
parameters that influence him . The attraction or ANS based systems is that imitations of expert
behavior emerge as a natural consequence of their training.

Referenees
1) D. E. Rumelhart, G . E. Hinton, and R. J. Williams, "Learning Internal Representations by
Error Propagation," in Parallel D~tributed Proceuing: Ezploration~ in the Micro~trvcture 0/ Cognition,
Vol. I, D. E . Rumelhart and J. L. McClelland (Eds.)' chap. 8, (1986), Bradford BooksjMIT Press,
Cambridge

2) S. Grossberg,

Studie~

0/ Mind and Brain, (1982), Reidel, Boston

3) A. Barto and R. Sutton, "Landmark Learning: An Illustration of Associative Search," BiologicaIC,6emetiu,42, (1981), p.l
4) A. Rosenfeld and A . Kak, Digital Pieture Proeming, Vol. 1, chap. 7, (1982), Academic Press,
New York

5) G. A. Carpenter and S. Grossberg, "A Massively Parallel Architecture for a Self-organizing
Neural Pattern Recognition Machine," Computer Vision, Graphiu and Image Procu,ing, 37,
( 1987), p.54


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 2847-off-road-obstacle-avoidance-through-end-to-end-learning.pdf

Off-Road Obstacle Avoidance through
End-to-End Learning

Yann LeCun
Courant Institute of Mathematical Sciences
New York University,
New York, NY 10004, USA
http://yann.lecun.com
Jan Ben
Net-Scale Technologies
Morganville, NJ 07751, USA

Eric Cosatto
NEC Laboratories,
Princeton, NJ 08540

Urs Muller
Net-Scale Technologies
Morganville, NJ 07751, USA
urs@net-scale.com

Beat Flepp
Net-Scale Technologies
Morganville, NJ 07751, USA

Abstract
We describe a vision-based obstacle avoidance system for off-road mobile robots. The system is trained from end to end to map raw input
images to steering angles. It is trained in supervised mode to predict the
steering angles provided by a human driver during training runs collected
in a wide variety of terrains, weather conditions, lighting conditions, and
obstacle types. The robot is a 50cm off-road truck, with two forwardpointing wireless color cameras. A remote computer processes the video
and controls the robot via radio. The learning system is a large 6-layer
convolutional network whose input is a single left/right pair of unprocessed low-resolution images. The robot exhibits an excellent ability to
detect obstacles and navigate around them in real time at speeds of 2 m/s.

1 Introduction
Autonomous off-road vehicles have vast potential applications in a wide spectrum of domains such as exploration, search and rescue, transport of supplies, environmental management, and reconnaissance. Building a fully autonomous off-road vehicle that can reliably
navigate and avoid obstacles at high speed is a major challenge for robotics, and a new
domain of application for machine learning research.
The last few years have seen considerable progress toward that goal, particularly in areas
such as mapping the environment from active range sensors and stereo cameras [11, 7],
simultaneously navigating and building maps [6, 15], and classifying obstacle types.
Among the various sub-problems of off-road vehicle navigation, obstacle detection and
avoidance is a subject of prime importance. The wide diversity of appearance of potential
obstacles, and the variability of the surroundings, lighting conditions, and other factors,
make the problem very challenging.
Many recent efforts have attacked the problem by relying on a multiplicity of sensors,
including laser range finder and radar [11]. While active sensors make the problem considerably simpler, there seems to be an interest from potential users for purely passive
systems that rely exclusively on camera input. Cameras are considerably less expensive,

bulky, power hungry, and detectable than active sensors, allowing levels of miniaturization
that are not otherwise possible. More importantly, active sensors can be slow, limited in
range, and easily confused by vegetation, despite rapid progress in the area [2].
Avoiding obstacles by relying solely on camera input requires solving a highly complex
vision problem. A time-honored approach is to derive range maps from multiple images
through multiple cameras or through motion [6, 5]. Deriving steering angles to avoid obstacles from the range maps is a simple matter. A large number of techniques have been
proposed in the literature to construct range maps from stereo images. Such methods have
been used successfully for many years for navigation in indoor environments where edge
features can be reliably detected and matched [1], but navigation in outdoors environment,
despite a long history, is still a challenge [14, 3]: real-time stereo algorithms are considerably less reliable in unconstrained outdoors environments. The extreme variability of
lighting conditions, and the highly unstructured nature of natural objects such as tall grass,
bushes and other vegetation, water surfaces, and objects with repeating textures, conspire
to limit the reliability of this approach. In addition, stereo-based methods have a rather
limited range, which dramatically limits the maximum driving speed.

2 End-To-End Learning for Obstacle Avoidance
In general, computing depth from stereo images is an ill-posed problem, but the depth map
is only a means to an end. Ultimately, the output of an obstacle avoidance system is a set
of possible steering angles that direct the robot toward traversible regions.
Our approach is to view the entire problem of mapping input stereo images to possible
steering angles as a single indivisible task to be learned from end to end. Our learning
system takes raw color images from two forward-pointing cameras mounted on the robot,
and maps them to a set of possible steering angles through a single trained function.
The training data was collected by recording the actions of a human driver together with the
video data. The human driver remotely drives the robot straight ahead until the robot encounters a non-traversible obstacle. The human driver then avoids the obstacle by steering
the robot in the appropriate direction. The learning system is trained in supervised mode.
It takes a single pair of heavily-subsampled images from the two cameras, and is trained to
predict the steering angle produced by the human driver at that time.
The learning architecture is a 6-layer convolutional network [9]. The network takes the
left and right 149?58 color images and produces two outputs. A large value on the first
output is interpreted as a left steering command while a large value on the second output
indicates a right steering command. Each layer in a convolutional network can be viewed as
a set of trainable, shift-invariant linear filters with local support, followed by a point-wise
non-linear saturation function. All the parameters of all the filters in the various layers
are trained simultaneously. The learning algorithm minimizes the discrepancy between the
desired output vector and the output vector produced by the output layer.
The approach is somewhat reminiscent of the ALVINN and MANIAC systems [13, 4]. The
main differences with ALVINN are: (1) our system uses stereo cameras; (2) it is trained
for off-road obtacle avoidance rather than road following; (3) Our trainable system uses a
convolutional network rather than a traditional fully-connected neural net.
Convolutional networks have two considerable advantages for this applications. Their local and sparse connection scheme allows us to handle images of higher resolution than
ALVINN while keeping the size of the network within reasonnable limits. Convolutional
nets are particularly well suited for our task because local feature detectors that combine
inputs from the left and right images can be useful for estimating distances to obstacles
(possibly by estimating disparities). Furthermore, the local and shift-invariant property of
the filters allows the system to learn relevant local features with a limited amount of training
data.
They key advantage of the approach is that the entire function from raw pixels to steering
angles is trained from data, which completely eliminates the need for feature design and

selection, geometry, camera calibration, and hand-tuning of parameters. The main motivation for the use of end-to-end learning is, in fact, to eliminate the need for hand-crafted
heuristics. Relying on automatic global optimization of an objective function from massive
amounts for data may produce systems that are more robust to the unpredictable variability
of the real world. Another potential benefit of a pure learning-based approach is that the
system may use other cues than stereo disparity to detect obstacles, possibly alleviating the
short-sightedness of methods based purely on stereo matching.

3 Vehicle Hardware
We built a small and light-weight vehicle which can be carried by a single person so as
to facilitate data collection and testing in a wide variety of environments. Using a small,
rugged and low-cost robot allowed us to drive at relatively high speed without fear of causing damage to people, property or the robot itself. The downside of this approach is the
limited payload, too limited for holding the computing power necessary for the visual processing. Therefore, the robot has no significant on-board computing power. It is remotely
controled by an off-board computer. A wireless link is used to transmit video and sensor
readings to the remote computer. Throttle and steering controls are sent from the computer
to the robot through a regular radio control channel.
The robot chassis was built around a customized 1/10-th scale remote-controlled, electricpowered, four-wheel-drive truck which was roughly 50cm in length. The typical speed of
the robot during data collection and testing sessions was roughly 2 meters per second. Two
forward-pointing low-cost 1/3-inch CCD cameras were mounted 110mm apart behind a
clear lexan window. With 2.5mm lenses, the horizontal field of view of each camera was
about 100 degrees.
A pair of 900MHz analog video transmitters was used to send the camera outputs to the
remote computer. The analog video links were subject to high signal noise, color shifts,
frequent interferences, and occasional video drop-outs. But the small size, light weight,
and low cost provided clear advantages. The vehicle is shown in Figure 1. The remote
control station consisted of a 1.4GHz Athlon PC running Linux with video capture cards,
and an interface to an R/C transmitter.

Figure 1: Left: The robot is a modified 50 cm-long truck platform controled by a remote
computer. Middle: sample images images from the training data. Right: poor reception
occasionally caused bad quality images.

4 Data Collection
During a data collection session, the human operator wears video goggles fed with the
video signal from one the robot?s cameras (no stereo), and controls the robot through a
joystick connected to the PC. During each run, the PC records the output of the two video
cameras at 15 frames per second, together with the steering angle and throttle setting from
the operator.

A crucially important requirement of the data collection process was to collect large
amounts of data with enough diversity of terrain, obstacles, and lighting conditions. Tt
was necessary for the human driver to adopt a consistent obstacle avoidance behaviour. To
ensure this, the human driver was to drive the vehicle straight ahead whenever no obstacle
was present within a threatening distance. Whenever the robot approached an obstacle, the
human driver had to steer left or right so as to avoid the obstacle. The general strategy
for collecting training data was as follows: (a) Collecting data from as large a variety of
off-road training grounds as possible. Data was collected from a large number of parks,
playgrounds, frontyards and backyards of a number of suburban homes, and heavily cluttered construction areas; (b) Collecting data with various lighting conditions, i. e., different
weather conditions and different times of day; (c) Collecting sequences where the vehicle
starts driving straight and then is steered left or right as the robot approached an obstacle;
(d) Avoiding turns when no obstacles were present; (e) Including straight runs with no obstacles and no turns as part of the training set; (f) Trying to be consistent in the turning
behavior, i. e., always turning at approximately the same distance from an obstacle.
Even though great care was taken in collecting the highest quality training data, there were
a number of imperfections in the training data that could not be avoided: (a) The smallform-factor, low-cost cameras presented significant differences in their default settings. In
particular, the white balance of the two cameras were somewhat different; (b) To maximize
image quality, the automatic gain control and automatic exposure were activated. Because
of differences in fabrication, the left and right images had slightly different brightness and
contrast characteristics. In particular, the AGC adjustments seem to react at different speeds
and amplitudes; (c) Because of AGC, driving into the sunlight caused the images to become
very dark and obstacles to become hard to detect; (d) The wireless video connection caused
dropouts and distortions of some frames. Approximately 5 % of the frames were affected.
An example is shown in Figures 1; (e) The cameras were mounted rigidly on the vehicle
and were exposed to vibration, despite the suspension. Despite these difficult conditions,
the system managed to learn the task quite well as will be shown later.
The data was recorded and archived at a resolution of 320?240? pixels at 15 frames per
second. The data was collected on 17 different days during the Winter of 2003/2004 (the
sun was very low on the horizon). A total of 1,500 clips were collected with an average
length of about 85 frames each. This resulted in a total of about 127,000 individual pairs of
frames. Segments during which the robot was driven into position in preparation for a run
were edited out. No other manual data cleaning took place. In the end, 95,000 frame pairs
were used for training and 32,000 for validation/testing. The training pairs and testing pairs
came from different sequences (and often different locations).
Figure 1 shows example snapshots from the training data, including an image with poor
reception. Note that only one of the two (stereo) images is shown. High noise and frame
dropouts occurred in approximately 5 % of the frames. It was decided to leave them in the
training set and test set so as to train the system under realistic conditions.

5 The Learning System
The entire processing consists of a single convolutional network. The architecture of convolutional nets is somewhat inspired by the structure of biological visual systems. Convolutional nets have been used successfully in a number of vision applications such as
handwriting recognition [9], object recognition [10], and face detection [12].
The input to the convolutional net consists of 6 planes of size 149?58 pixels. The six
planes respectively contain the Y, U and V components for the left camera and the right
camera. The input images were obtained by cropping the 320 ? 240 images, and through
2? horizontal low-pass filtering and subsampling, and 4? vertical low-pass filtering and
subsampling. The horizontal resolution was set higher so as to preserve more accurate
image disparity information.
Each layer in a convolutional net is composed of units organized in planes called feature
maps. Each unit in a feature map takes inputs from a small neighborhood within the feature

maps of the previous layer. Neighborhing units in a feature map are connected to neighboring (possibly overlapping) windows. Each unit computes a weighted sum of its inputs and
passes the result through a sigmoid saturation function. All units within a feature map share
the same weights. Therefore, each feature map can be seen as convolving the feature maps
of the previous layers with small-size kernels, and passing the sum of those convolutions
through sigmoid functions. Units in a feature map detect local features at all locations on
the previous layer.
The first layer contains 6 feature maps of size 147?56 connected to various combinations
of the input maps through 3?3 kernels. The first feature map is connected to the YUV
planes of the left image, the second feature map to the YUV planes of the right image, and
the other 4 feature maps to all 6 input planes. Those 4 feature maps are binocular, and
can learn filters that compare the location of features in the left and right images. Because
of the weight sharing, the first layer merely has 276 free parameters (30 kernels of size
3?3 plus 6 biases). The next layer is an averaging/subsampling layer of size 49?14 whose
purpose is to reduce the spatial resolution of the feature maps so as to build invariances
to small geometric distortions of the input. The subsampling ratios are 3 horizontally and
4 vertically. The 3-rd layer contains 24 feature maps of size 45?12. Each feature map is
connected to various subsests of maps in the previous layer through a total of 96 kernels of
size 5?3. The 4-th layer is an averaging/subsampling layer of size 9?4 with 5?3 subsampling ratios. The 5-th layer contains 100 feature maps of size 1?1 connected to the 4-th
layer through 2400 kernels of size 9?4 (full connection). finally, the output layer contains
two units fully-connected to the 100 units in the 5-th layer. The two outputs respectively
code for ?turn left? and ?turn right? commands. The network has 3.15 Million connections
and about 72,000 trainable parameters.
The bottom half of figure 2 shows the states of the six layers of the convolutional net. the
size of the input, 149?58, was essentially limited by the computing power of the remote
computer (a 1.4GHz Athlon). The network as shown runs in about 60ms per image pair on
the remote computer. Including all the processing, the driving system ran at a rate of 10
cycles per second.
The system?s output is computed on a frame by frame basis with no memory of the past
and no time window. Using multiple successive frames as input would seem like a good
idea since the multiple views resulting from ego-motion facilitates the segmentation and
detection of nearby obstacles. Unfortunately, the supervised learning approach precludes
the use of multiple frames. The reason is that since the steering is fairly smooth in time
(with long, stable periods), the current rate of turn is an excellent predictor of the next
desired steering angle. But the current rate of turn is easily derived from multiple successive
frames. Hence, a system trained with multiple frames would merely predict a steering
angle equal to the current rate of turn as observed through the camera. This would lead to
catastrophic behavior in test mode. The robot would simply turn in circles.
The system was trained with a stochastic gradient-based method that automatically sets the
relative step sizes of the parameters based on the local curvature of the loss surface [8]. Gradients were computed using the variant of back-propagation appropriate for convolutional
nets.

6 Results
Two performance measurements were recorded, the average loss, and the percentage of
?correctly classified? steering angles. The average loss is the sum of squared differences
between outputs produced by the system and the target outputs, averaged over all samples. The percentage of correctly classified steering angles measures the number of times
the predicted steering angle, quantized into three bins (left, straight, right), agrees with
steering angle provided by the human driver. Since the thresholds for deciding whether an
angle counted as left, center, or right were somewhat arbitrary, the percentages cannot be
intepreted in absolute terms, but merely as a relative figure of merit for comparing runs and
architectures.

Figure 2: Internal state of the convolutional net for two sample frames. The top row shows
left/right image pairs extracted from the test set. The light-blue bars below show the steering angle produced by the system. The bottom halves show the state of the layers of the
network, where each column is a layer (the penultimate layer is not shown). Each rectangular image is a feature map in which each pixel represents a unit activation. The YUV
components of the left and right input images are in the leftmost column.

With 95,000 training image pairs, training took 18 epochs through the training set. No
significant improvements in the error rate occurred thereafter. After training, the error rate
was 25.1% on the training set, and 35.8% on the test set. The average loss (mean-sqaured
error) was 0.88 on the training set and 1.24 on the test set. A complete training session
required about four days of CPU time on a 3.0GHz Pentium/Xeon-based server. Naturally,
a classification error rate of 35.8 % doesn?t mean that the vehicle crashes into obstacles
35.8 % of the time, but merely that the prediction of the system was in a different bin
than that of the human drivers for 35.8 % of the frames. The seemingly high error rate is
not an accurate reflection of the actual effectiveness of the robot in the field. There are
several reasons for this. First, there may be several legitimate steering angles for a given
image pair: turning left or right around an obstacle may both be valid options, but our
performance measure would record one of those options as incorrect. In addition, many
illegitimate errors are recorded when the system starts turning at a different time than the
human driver, or when the precise values of the steering angles are different enough to be
in different bins, but close enough to cause the robot to avoid the obstacle. Perhaps more
informative is diagram in figure 3. It shows the steering angle produced by the system and
the steering angle provided by the human driver for 8000 frames from the test set. It is
clear for the plot that only a small number of obstacles would not have been avoided by the
robot.
The best performance measure is a set of actual runs through representative testing grounds.
Videos of typical test runs are available at
http://www.cs.nyu.edu/?yann/research/dave/index.html.
Figure 2 shows a snapshot of the trained system in action. The network was presented with
a scene that was not present in the training set. This figure shows that the system can detect
obstacles and predict appropriate steering angles in the presence of back-lighting and with
wild difference between the automatics gain settings of the left and right cameras.
Another visualization of the results can be seen in Figures 4. They are snapshots of
video clips recorded from the vehicle?s cameras while the vehicle was driving itself autonomously. Only one of the two camera outputs is shown here. Each picture also shows

Figure 3: The steering angle produced by the system (black) compared to the steering
angle provided by the human operator (red line) for 8000 frames from the test set. Very
few obstacles would not have been avoided by the system.
the steering angle produced by the system for that particular input.

7 Conclusion
We have demonstrate the applicability of end-to-end learning methods to the task of obstacle avoidance for off-road robots.
A 6-layer convolutional network was trained with massive amounts of data to emulate the
obstacle avoidance behavior of a human driver. the architecture of the system allowed it
to learn low-level and high-level features that reliably predicted the bearing of traversible
areas in the visual field.
The main advantage of the system is its robustness to the extreme diversity of situations
in off-road environments. Its main design advantage is that it is trained from raw pixels to
directly produce steering angles. The approach essentially eliminates the need for manual
calibration, adjustments, parameter tuning etc. Furthermore, the method gets around the
need to design and select an appropriate set of feature detectors, as well as the need to
design robust and fast stereo algorithms.
The construction of a fully autonomous driving system for ground robots will require several other components besides the purely-reactive obstacle detection and avoidance system
described here. The present work is merely one component of a future system that will
include map building, visual odometry, spatial reasoning, path finding, and other strategies
for the identification of traversable areas.
Acknowledgment
This project was a preliminary study for the DARPA project ?Learning Applied to Ground Robots?
(LAGR). The material presented is based upon work supported by the Defense Advanced Research
Project Agency Information Processing Technology Office, ARPA Order No. Q458, Program Code
No. 3D10, Issued by DARPA/CMO under Contract #MDA972-03-C-0111.

References
[1] N. Ayache and O. Faugeras. Maintaining representations of the environment of a mobile robot.
IEEE Trans. Robotics and Automation, 5(6):804?819, 1989.
[2] C. Bergh, B. Kennedy, L. Matthies, and Johnson A. A compact, low power two-axis scanning
laser rangefinder for mobile robots. In The 7th Mechatronics Forum International Conference,
2000.
[3] S. B. Goldberg, M. Maimone, and L. Matthies. Stereo vision and rover navigation software for
planetary exploration. In IEEE Aerospace Conference Proceedings, March 2002.
[4] T. Jochem, D. Pomerleau, and C. Thorpe. Vision-based neural network road and intersection
detection and traversal. In Proc. IEEE Conf. Intelligent Robots and Systems, volume 3, pages
344?349, August 1995.

Figure 4: Snapshots from the left camera while the robots drives itself through various environment. The black bar beneath each image indicates the steering angle produced by the system. Top row: four successive snapshots showing the robot navigating
through a narrow passageway between a trailer, a backhoe, and some construction material. Bottom row, left: narrow obstacles such as table legs and poles (left), and solid
obstacles such as fences (center-left) are easily detected and avoided. Higly textured objects on the ground do not detract the system from the correct response (center-right).
One scenario where the vehicle occasionally made wrong decisions is when the sun is
in the field of view: the system seems to systematically drive towards the sun, whenever the sun is low on the horizon (right). Videos of these sequences are available at
http://www.cs.nyu.edu/?yann/research/dave/index.html.
[5] A. Kelly and A. Stentz. Stereo vision enhancements for low-cost outdoor autonomous vehicles. In International Conference on Robotics and Automation, Workshop WS-7, Navigation of
Outdoor Autonomous Vehicles, (ICRA ?98), May 1998.
[6] D.J. Kriegman, E. Triendl, and T.O. Binford. Stereo vision and navigation in buildings for
mobile robots. IEEE Trans. Robotics and Automation, 5(6):792?803, 1989.
[7] E. Krotkov and M. Hebert. Mapping and positioning for a prototype lunar rover. In Proc. IEEE
Int?l Conf. Robotics and Automation, pages 2913?2919, May 1995.
[8] Y. LeCun, L. Bottou, G. Orr, and K. Muller. Efficient backprop. In G. Orr and Muller K.,
editors, Neural Networks: Tricks of the trade. Springer, 1998.
[9] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278?2324, November 1998.
[10] Yann LeCun, Fu-Jie Huang, and Leon Bottou. Learning methods for generic object recognition
with invariance to pose and lighting. In Proceedings of CVPR?04. IEEE Press, 2004.
[11] L. Matthies, E. Gat, R. Harrison, B. Wilcox, R. Volpe, and T. Litwin. Mars microrover navigation: Performance evaluation and enhancement. In Proc. IEEE Int?l Conf. Intelligent Robots
and Systems, volume 1, pages 433?440, August 1995.
[12] R. Osadchy, M. Miller, and Y. LeCun. Synergistic face detection and pose estimation with
energy-based model. In Advances in Neural Information Processing Systems (NIPS 2004).
MIT Press, 2005.
[13] Dean A. Pomerleau. Knowledge-based training of artificial neural netowrks for autonomous
robot driving. In J. Connell and S. Mahadevan, editors, Robot Learning. Kluwer Academic
Publishing, 1993.
[14] C. Thorpe, M. Herbert, T. Kanade, and S Shafer. Vision and navigation for the carnegie-mellon
navlab. IEEE Trans. Pattern Analysis and Machine Intelligence, 10(3):362?372, May 1988.
[15] S. Thrun. Learning metric-topological maps for indoor mobile robot navigation. Artificial
Intelligence, 99(1):21?71, February 1998.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 414-real-time-autonomous-robot-navigation-using-vlsi-neural-networks.pdf

Real-time autonomous robot navigation using
VLSI neural networks

Lionel Tarassenko Michael Brownlow Gillian Marshall?
Department of Engineering Science
Oxford University, Oxford, OXl 3PJ, UK

Jon Tombs

Alan Murray
Department of Electrical Engineering
Edinburgh University, Edinburgh, EH9 3JL, UK

Abstract
We describe a real time robot navigation system based on three VLSI
neural network modules. These are a resistive grid for path planning, a
nearest-neighbour classifier for localization using range data from a timeof-flight infra-red sensor and a sensory-motor associative network for dynamic obstacle avoidance .

1

INTRODUCTION

There have been very few demonstrations ofthe application ofVLSI neural networks
to real world problems. Yet there are many signal processing, pattern recognition
or optimization problems where a large number of competing hypotheses need to
be explored in parallel, most often in real time. The massive parallelism of VLSI
neural network devices, with one multiplier circuit per synapse, is ideally suited to
such problems. In this paper, we present preliminary results from our design for a
real time robot navigation system based on VLSI neural network modules. This is a
? Also: RSRE, Great Malvern, Worcester, WR14 3PS

422

Real-time Autonomous Robot Navigation Using VLSI Neural Networks
real world problem which has not been fully solved by traditional AI methods; even
when partial solutions have been proposed and implemented, these have required
vast computational resources, usually remote from the robot and linked to it via an
umbilical cord.

2

OVERVIEW

The aim of our work is to develop an autonomous vehicle capable of real-time
navigation, including obstacle avoidance, in a known indoor environment. The
obstacles may be permanent (static) or unexpected and dynamic (for example,
in an automated factory environment, the walls and machines are permanent but
people, other moving vehicles and packages are not.) There are three neural network
modules at the heart of our navigation system: a localization module (to determine,
at any time, the robot's position within the environment), an obstacle detection
module and a path planning module (to compute a path to the goal which avoids
obstacles). These modules perform low-level processing in real time which can then
be decoupled from higher level processing to be carried out by a simple controller.
It is our view that such a hybrid system is the best way to realise the computational
potential of artificial neural networks for solving a real world problem such as this
without compromising overall system performance.
A short description of each module is now given. In each case, the general principles
are first outlined and, where applicable, the results of our preliminary work are then
reported.

3

PATH PLANNING

The use ofresistive grids for parallel analog computation was first suggested by Horn
in the mid-seventies (Horn, 1974) and the idea has since been exploited by Mead and
co-workers, for example in a silicon retina (Mead and Mahowald, 1988). Although
these resistive grids cannot be said to be neural networks in the conventional sense,
they also perform parallel analog computation and they have the same advantages,
in terms of speed and fault-tolerance, as any hardware realisation of neural networks.
We have taken the resistive grid concept and applied it to the path planning problem, here taken to be the computation of an obstacle-avoiding path, in a structured
environment, from the robot's initial (or present) position (P) to its goal (G). In our
approach, the robot's working domain is discretized and mapped onto a resistive
grid of hexagonal or rectangular cells - see Figure 1 which shows the test environment for Autonomous Guided Vehicles (AGV's) in the Oxford Robotics Laboratory.
Each resistor in the grid has a value of flo, unless it is part of a region of the grid
corresponding to an obstacle, in which case its resistance is infinite (Roo).
The principle of the method is perhaps best understood by considering a continuous
analog of the resistive grid (for example, a sheet of material of uniform resistivity in
which holes have been cut to represent the obstacles). The current streamlines resulting from the application of an external source between P and G skirt around the
obstacles; if we follow one of these streamlines from P to G, we will obtain a guaranteed collision-free path since current cannot flow into the obstacles (Tarassenko and

423

424

Tarassenko, Brownlow, Marshall, Tombs, and Murray
Blake, 1991). For simple cases such as circularly symmetric conductivity distributions in 2D, Laplace's equation can be solved in order to calculate the value of the
potential V at every point within the workspace. Following a current streamline is
then simply a matter of performing gradient descent in V.

Figure 1: The Oxford test environment for AGV's mapped out as a hexagonal
resistive grid. The resistors corresponding to the four pillars in the middle are open
circuits. Note that the pillars are enlarged in their grid representation in order to
take into account the mobile robot's finite size.
It is not possible, however, to solve Laplace's equation analytically for realistic environments. With the resistive grid, the problem is discretized and mapped onto a
hardware representation which can be implemented in VLSI. As soon as an external
source of power is connected between P and G, the resistive network settles into
the state of least power dissipation and the node voltages can be read out (hardware computation of Kirchhoff's equations). The path from P to G is computed
incrementally from local voltage measurements: for each node, the next move is
identified by measuring the voltage drop ~ Vn between that node and each of its
nearest neighbours (n = 6 for a hexagonal grid) and then selecting the node corresponding to (~Vn)max. This is illustrated by the example of a robot in a maze
(Figure 2). As above, the resistors shown shaded are open circuits whilst all other
resistors are set to be equal to Ro. The robot is initially placed at the centre of the
maze (P) and a path has to be found to the goal in the top left-hand corner (G). The
solid line shows the path resulting from a single application of the voltage between
P and G. The dotted line shows the (optimal) path computed by re-applying the

Real-time Autonomous Robot Navigation Using VLSI Neural Networks
voltage at every node as the robot moves towards the goal. As already indicated,
this is actually how we intend to use the resistive grid planner in practice, since
this approach also allows us to re-compute the robot's path whenever unexpected
obstacles appear in the environment (see Section 5) .

..:~:::.~~::~.::~.~: ::~::~~:::~:::.~.::~::~.~::.::~.::::~~::~::~;;':X
~---;~~~~-*" ~I.: :x.........'?){ .... ) <?.... )<. . ?::x
x ..........
?x.
..'......

)E ...... ? ?~)( .... ???) (?????? ..

,?????.... ?~?? ...... ?x.. ??.. ? ?: ~: .. ???? ? :~::???? ??:~: .. ?? .. ?:x

..>ot~?? ....?:'X':.. ?.... ?';I(...... ?:~~~~'"*-*-'*?.? . . .?.:X: .. ..??

x:?

* .........,>0:: .

x;~?.~:~:~?:::?~:~?:~: ?. ~~: ~?:::~:::~::. . . :~:: .?~:.: ~.: .~:::~:::.~::.:~::~~~ ;~:.: .:~:.: ~: :~:.: ~:::~.:x

Figure 2: Path from middle of maze (P) to top left-hand corner (G)

3.1

VLSI IMPLEMENTATION

The VLSI implementation of the resistive grid method will allow us to solve the path
planning for complex environments in real time. MOS switches are ideal implementations of the binary resistors in the grid. Each transistor can be programmed to
be either open (Roo) or closed (Ro) from a RAM cell connected to its gate. With
the incremental computation of the path described above , the selection of the next
move is a matter of identifying the largest of six voltages. Of course, the nearest
neighbour voltages and that of the P node could be read out through an AID converter and the decision made off-chip. We favour a full hardware solution instead,
whereby the maximum voltage difference is directly identified on-chip.

4

LOCALIZATION

The autonomous robot should at any time be able to work out its position in
the workspace so that the path to the goal can be updated if required. The grid
representation of the environment used for the path planner can also be employed

425

426

Tarassenko, Brownlow, Marshall, Tombs, and Murray
for localization purposes, in which case localization becomes, in the first instance,
a matter of identifying the nearest node in the grid at any time during navigation.
This task can be performed by harnessing the pattern recognition capabilities of
neural networks. The room environment is learnt by recording a 3600 range scan
at every node during a training phase prior to navigation. During navigation, the
nearest node is identified using a minimum-distance classifier implemented on a
single-layer neural network working on dense input data (one range value every 30 ,
say). In order to solve the localization problem in real-time, we have designed a timeof-flight optical rangefinder, which uses near infra-red light, amplitude-modulated
at a frequency of just above 5 MHz, together with a heterodyne mixing technique.
Our design is capable of resolving phase shifts in the received light signal of the
order of 0.10 over a 50 dB dynamic range.
The rotating optical scanner gives a complete 360 0 scan approximately every second
during navigation. The minimum-distance classifier is used to compare this scan x
with the k patterns Uj recorded at each node during training. If we use a Euclidean
metric for the comparison, this is equivalent to identifying the pattern Uj for which:

(1)
is a minimum. The first term in the above equation is the same for all i and can be
ignored. We can therefore write:

= - '12 (- 2wT x +

= WiT +
where gj(x) is a linear discriminant function, Wi = Uj and
= -~u;,
gj

( x)

j

2
Uj)

X

WjQ

(2)

WjQ
Thus each
vector is one of the learnt patterns Ui and the discriminant gi(X) matches the
input x with Uj, point by point. If we let W j
{Iij} and x = {Vj} and assume
that there are n range values in each scan, then we can write:

Wj

=

j=n

gj(x)

= E Iij Vj +

WiO

(3)

j=l

Thus the synaptic weights are an exact copy of the patterns recorded at each grid
point during learning and the neurons can be thought of as processors which compute distances to those patterns. During navigation, the nearest node is identified
with a network of k neurons evaluating k discriminant functions in parallel, followed
by a ''winner-take-all'' network to pick the maximum gj(x). This is the well-known
implementation of the nearest-neighbour classifier on a neural network architecture.
Since the ui's are analog input vectors, then the synaptic weights Iij will also be
analog quantities and this leads to a very efficient use of the pulse-stream analog
VLSI technology which we have recently developed for the implementation of neural
networks (Murray et ai, 1990).
With pulse-stream arithmetic, analog computation is performed under digital control. The neural states are represented by pulse rates and synaptic multiplication is achieved by pulse width modulation. This allows very compact, fully-

Real-time Autonomous Robot Navigation Using VLSI Neural Networks
programmable, synapse circuits to be designed (3 or 4 transistors per synapse).
We have already applied one set of our working chips to the nearest-neighbour classification task described in this Section. They were evaluated on a 24-node test
environment and full results have been reported elsewhere (Brownlow, Tarassenko
and Murray, 1990). It was found that the E Iij Vi scalar products evaluated by our
VLSI chips on this test problem were always within 1.2% of those computed on a
SUN 3/80 workstation.

5

OBSTACLE DETECTION/AVOIDANCE

A more appropriate name for this module may be that of local navigation. The
module will rely on optical flow information derived from a number of fixed optical
sensors mounted on the robot platform. Each sensor will include a pulsed light
source to illuminate the scene locally and the light reflected from nearby objects
will be focussed onto a pair of gratings at right angles to each other, before being
detected by a photodiode array. From the time derivatives of the received signals,
it is possible to compute the relative velocities of nearby objects such as moving
obstacles. We plan to use previous work on structure from motion to pre-process
these velocity vectors and derive from them appropriate feature vectors to be used
as inputs to a low-level neural network for motor control (see Figure 3 below).

sensors~ intended path,...."'"

""

"

"q'

un
Measurement
flow from

of optic
sensors

w
Velocity signal of
approaching objects.

Low level network

Direct motor
control

Figure 3: Sensory-motor associative network for obstacle avoidance

427

428

Tarassenko, Brownlow, Marshall, Tombs, and Murray
The obstacle avoidance network will be taught to associate appropriate motor behaviours with different types of sensory input data, for example the taking of the
correct evasive action when a moving object is approaching the robot from a particular direction. This module will therefore be responsible for path adjustment in
response to dynamic obstacles (with a bandwidth of around 100 Hz), but the path
planner of Section 3 will continue to deal with path reconfiguration at a much lower
data rate (1 Hz), once the dynamic obstacle has been avoided. Our work on this
module has, so far, been mainly concerned with the design of the input sensors and
associated electronics.

6

CONCLUSION

We have implemented the path planning and localization modules described in this
paper on a SUN 4 workstation and used them to control a mobile robot platform
via a radio link. This capability was demonstrated at the NIPS'90 Conference with
a videotape recording of our mobile robot navigating around static obstacles in
a laboratory environment, using real-time infra-red data for localization. It was
possible to run the path planner in near real-time in simulation because no resistor
value need be changed in a static environment; in order to achieve real-time path
planning in a dynamic environment, however, the hardware solution of Section 3
will be mandatory. Our aim remains the implementation of all 3 modules in VLSI
in order to demonstrate a fully autonomous real-time navigation system with all
the sensors and hardware mounted on the robot platform.
Acknowledgements

We gratefully acknowledge the financial support of UK Science and Engineering
Research Council and of the EEC (ESPRIT BRA). We have benefitted greatly
from the help and advice of members of the Robotics Research Group, most notably
Martin Adams, Gabriel Hamid and Jake Reynolds.
References

M.J. Brownlow, L. Tarassenko & A.F. Murray. (1990) Analogue computation using
VLSI neural network devices. Electronics Letters, 26(16):1297-1299.
B.K.P. Horn. (1974) Determining lightness from an image. Computational Graphics
fj Image Processing, 3:277-299.
C.A. Mead & M.A. Mahowald. (1988) A silicon model of early visual processing.
Neural Networks, 1(1 ):91-97.
A.F. Murray, M.J. Brownlow, L. Tarassenko, A. Hamilton, I.S. Han & H.M. Reekie.
(1990) Pulse-Firing Neural Chips for Hundreds of Neurons. In D.S. Touretzky (ed.),
Advances in Neural Information Processing Systems 2, 785-792. San Mateo, CA:
Morgan Kaufmann.
L. Tarassenko & A. Blake. (1991). Analogue computation of collision-free paths. To
be published in: Proceedings of 1991 IEEE Int. Conf. on Robotics fj Automation,
Sacramento, CA:


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 4189-joint-3d-estimation-of-objects-and-scene-layout.pdf

Joint 3D Estimation of Objects and Scene Layout

Andreas Geiger
Karlsruhe Institute of Technology

Christian Wojek
MPI Saarbr?ucken

Raquel Urtasun
TTI Chicago

geiger@kit.edu

cwojek@mpi-inf.mpg.de

rurtasun@ttic.edu

Abstract
We propose a novel generative model that is able to reason jointly about the 3D
scene layout as well as the 3D location and orientation of objects in the scene.
In particular, we infer the scene topology, geometry as well as traffic activities
from a short video sequence acquired with a single camera mounted on a moving
car. Our generative model takes advantage of dynamic information in the form of
vehicle tracklets as well as static information coming from semantic labels and geometry (i.e., vanishing points). Experiments show that our approach outperforms
a discriminative baseline based on multiple kernel learning (MKL) which has access to the same image information. Furthermore, as we reason about objects in
3D, we are able to significantly increase the performance of state-of-the-art object
detectors in their ability to estimate object orientation.

1

Introduction

Visual 3D scene understanding is an important component in applications such as autonomous driving and robot navigation. Existing approaches produce either only qualitative results [11] or a mild
level of understanding, e.g., semantic labels [10, 26], object detection [5] or rough 3D [15, 24]. A
notable exception are approaches that try to infer the scene layout of indoor scenes in the form of
3D bounding boxes [13, 22]. However, these approaches can only cope with limited amounts of
clutter (e.g., beds), and rely on the fact that indoor scenes satisfy very closely the manhattan world
assumption, i.e., walls (and often objects) are aligned with the three dominant vanishing points. In
contrast, outdoor scenarios often show more clutter, vanishing points are not necessarily orthogonal
[25, 2], and objects often do not agree with the dominant vanishing points.
Prior work on 3D urban scene analysis is mostly limited to simple ground plane estimation [4, 29]
or models for which the objects and the scene are inferred separately [6, 7]. In contrast, in this paper
we propose a novel generative model that is able to reason jointly about the 3D scene layout as well
as the 3D location and orientation of objects in the scene. In particular, given a video sequence
of short duration acquired with a single camera mounted on a moving car, we estimate the scene
topology and geometry, as well as the traffic activities and 3D objects present in the scene (see Fig.
1 for an illustration). Towards this goal we propose a novel image likelihood which takes advantage
of dynamic information in the form of vehicle tracklets as well as static information coming from
semantic labels and geometry (i.e., vanishing points). Interestingly, our inference reasons about
whether vehicles are on the road, or parked, in order to get more accurate estimations. Furthermore,
we propose a novel learning-based approach to detecting vanishing points and experimentally show
improved performance in the presence of clutter when compared to existing approaches [19].
We focus our evaluation mainly on estimating the layout of intersections, as this is the most challenging inference task in urban scenes. Our approach proves superior to a discriminative baseline
based on multiple kernel learning (MKL) which has access to the same image information (i.e., 3D
tracklets, segmentation and vanishing points). We evaluate our method on a wide range of metrics
including the accuracy of estimating the topology and geometry of the scene, as well as detecting
1

Vehicle Tracklets

Vanishing Points

?
Scene Labels

Figure 1: Monocular 3D Urban Scene Understanding. (Left) Image cues. (Right) Estimated layout: Detections
belonging to a tracklet are depicted with the same color, traffic activities are depicted with red lines.

activities (i.e., traffic situations). Furthermore, we show that we are able to significantly increase the
performance of state-of-the-art object detectors [5] in terms of estimating object orientation.

2

Related Work

While outdoor scenarios remain fairly unexplored, estimating the 3D layout of indoor scenes has
experienced increased popularity in the past few years [13, 27, 22]. This can be mainly attributed
to the success of novel structured prediction methods as well as the fact that indoor scenes behave
mostly as ?Manhattan worlds?, i.e., edges on the image can be associated with parallel lines defined
in terms of the three dominant vanishing points which are orthonormal. With a moderate degree of
clutter, accurate geometry estimation has been shown for this scenario.
Unfortunately, most urban scenes violate the Manhattan world assumption. Several approaches
have focused on estimating vanishing points in this more adversarial setting [25]. Barinova et al. [2]
proposed to jointly perform line detection as well as vanishing point, azimut and zenith estimation.
However, their approach does not tackle the problem of 3D scene understanding and 3D object
detection. In contrast, we propose a generative model which jointly reasons about these two tasks.
Existing approaches to estimate 3D from single images in outdoor scenarios typically infer popups [14, 24]. Geometric approaches, reminiscent to the blocksworld model, which impose physical
constraints between objects (e.g., object A supports object B) have also been introduced [11]. Unfortunately, all these approaches are mainly qualitative and do not provide the level of accuracy
necessary for real-world applications such as autonomous driving and robot navigation. Prior work
on 3D traffic scene analysis is mostly limited to simple ground plane estimation [4], or models for
which the objects and scene are inferred separately [6]. In contrast, our model offers a much richer
scene description and reasons jointly about 3D objects and the scene layout.
Several methods have tried to infer the 3D locations of objects in outdoor scenarios [15, 1]. The most
successful approaches use tracklets to prune spurious detections by linking consistent evidence in
successive frames [18, 16]. However, these models are either designed for static camera setups in
surveillance applications [16] or do not provide a rich scene description [18]. Notable exceptions
are [3, 29] which jointly infer the camera pose and the location of objects. However, the employed
scene models are rather simplistic containing only a single flat ground plane.
The closest approach to ours is probably the work of Geiger et al. [7], where a generative model is
proposed in order to estimate the scene topology, geometry as well as traffic activities at intersections. Our work differs from theirs in two important aspects. First, they rely on stereo sequences
while we make use of monocular imagery. This makes the inference problem much harder, as the
noise in monocular imagery is strongly correlated with depth. Towards this goal we develop a richer
image likelihood model that takes advantage of vehicle tracklets, vanishing points as well as segmentations of the scene into semantic labels. The second and most important difference is that
Geiger et al. [7] estimate only the scene layout, while we reason jointly about the layout as well as
the 3D location and orientation of objects in the scene (i.e., vehicles).
2

1

2

3

4

5

6

7

(a) Model Geometry (? = 4)
(b) Model Topology ?
Figure 2: (a) Geometric model. In (b), the grey shaded areas illustrate the range of ?.

Finally, non-parametric models have been proposed to perform traffic scene analysis from a stationary camera with a view similar to bird?s eye perspective [20, 28]. In our work we aim to infer similar
activities but use video sequences from a camera mounted on a moving car with a substantially lower
viewpoint. This makes the recognition task much more challenging. Furthermore, those models do
not allow for viewpoint changes, while our model reasons about over 100 unseen scenes.

3

3D Urban Scene Understanding

We tackle the problem of estimating the 3D layout of urban scenes (i.e., road intersections) from
monocular video sequences. In this paper 2D refers to observations in the image plane while 3D
refers to the bird?s eye perspective (in our scenario the height above ground is non-informative). We
assume that the road surface is flat, and model the bird?s eye perspective as the y = 0 plane of the
standard camera coordinate system. The reference coordinate system is given by the position of the
camera in the last frame of the sequence. The intrinsic parameters of the camera are obtained using
camera calibration and the extrinsics using a standard Structure-from-Motion (SfM) pipeline [12].
We take advantage of dynamic and static information in the form of 3D vehicle tracklets, semantic labels (i.e., sky, background, road) and vanishing points. In order to compute 3D tracklets,
we first detect vehicles in each frame independently using a semi-supervised version of the partbased detector of [5] in order to obtain orientation estimates. 2D tracklets are then estimated using
?tracking-by-detection?: First adjacent frames are linked and then short tracklets are associated to
create longer ones via the hungarian method. Finally, 3D vehicle tracklets are obtained by projecting the 2D tracklets into bird?s eye perspective, employing error-propagation to obtain covariance
estimates. This is illustrated in Fig. 1 where detections belonging to the same tracklet are grouped
by color. The observer (i.e., our car) is shown in black. See sec 3.2 for more details on this process.
Since depth estimates in the monocular case are much noisier than in the stereo case, we employ
a more constrained model than the one utilized in [7]. In particular, as depicted in Fig. 2, we
model all intersection arms with the same width and force alternate arms to be collinear. We model
lanes with splines (see red lines for active lanes in Fig. fig:motivation), and place parking spots
at equidistant places along the street boundaries (see Fig. 3(b)). Our model then infers whether
the cars participate in traffic or are parked in order to get more accurate layout estimations. Latent
variables are employed to associate each detected vehicle with positions in one of these lanes or
parking spaces. In the following, we first give an overview of our probabilistic model and then
describe each part in detail.
3.1

Probabilistic Model

As illustrated in Fig. 2(b), we consider a fixed set of road layouts ?, including straight roads, turns,
3- and 4- armed intersections. Each of these layouts is associated with a set of geometric random
variables: The intersection center c, the street width w, the global scene rotation r and the angle of
the crossing street ? with respect to r (see Fig. 2(a)). Note that for ? = 1, ? does not exist.
Joint Distribution: Our goal is to estimate the most likely configuration R = (?, c, w, r, ?) given
the image evidence E = {T, V, S}, which comprises vehicle tracklets T = {t1 , .., tN }, vanish3

(a) Graphical model
(b) Road model
Figure 3: Graphical model and road model with lanes represented as B-splines.

ing points V = {vf , vc } and semantic labels S. We assume that, given R, all observations are
independent. Fig. 3(a) depicts our graphical model which factorizes the joint distribution as
#
"N
YX
p(tn , ln |R, C) p(vf |R, C)p(vc |R, C) p(S|R, C)
(1)
p(E, R|C) = p(R)
|
{z
} | {z }
n=1 ln
Semantic Labels
Vanishing Points
|
{z
}
Vehicle Tracklets

where C are the (known) extrinsic and intrinsic camera parameters for all the frames in the video
sequence, N is the total number of tracklets and {ln } denotes latent variables representing the lane
or parking positions associated with every vehicle tracklet. See Fig. 3(b) for an illustration.
Prior:

Let us first define a scene prior, which factorizes as
p(R) = p(?)p(c, w)p(r)p(?)

(2)

where c and w are modeled jointly to capture their correlation. We model w using a log-Normal
distribution since it takes only positive values. Further, since it is highly multimodal, we model p(?)
in a non-parametric fashion using kernel density estimation (KDE), and define:
r ? N (?r , ?r )

(c, log w)T ? N (?cw , ?cw )

? ? ?(?M AP )

In order to avoid the requirement for trans-dimensional inference procedures, the topology ?M AP
is estimated a priori using joint boosting, and set fixed at inference. To estimate ?M AP , we use the
same feature set employed by the MKL baseline (see Sec. 4 for details).
3.2

Image Likelihood

This section details our image likelihood for tracklets, vanishing points and semantic labels.
Vehicle Tracklets: In the following, we drop the tracklet index n to simplify notation. Let us
define a 3D tracklet as a set of object detections t = {d1 , .., dM }. Here, each object detection
dm = (fm , bm , om ) contains the frame index fm ? N, the object bounding box bm ? R4 defined
as 2D position and size, as well as a normalized orientation histogram om ? R8 with 8 bins. We
compute the bounding box bm and orientation om by supervised training of a part-based object
detector [5], where each component contains examples from a single orientation. Following [5], we
apply the softmax function on the output scores and associate frames using the hungarian algorithm
in order to obtain tracklets.
As illustrated in Fig. 3(b), we represent drivable locations with splines, which connect incoming
and outgoing lanes of the intersection. We also allow cars to be parked on the side of the road, see
Fig. 3(b) for an illustration. Thus, for a K-armed intersection, we have l ? {1, .., K(K ? 1) + 2K}
in total, where K(K ? 1) is the number of lanes and 2K is the number of parking areas. We use the
latent variable l to index the lane or parking position associated with a tracklet. The joint probability
of a tracklet t and its lane index l is given by p(t, l|R, C) = p(t|l, R, C)p(l). We assume a uniform
prior over lanes and parking positions l ? U(1, K(K ? 1) + 2K), and denote the posterior by pl
when l corresponds to a lane, and pp when it is a parking position.
In order to evaluate the tracklet posterior for lanes pl (t|l, R, C), we need to associate all object
detections t = {d1 , .., dM } to locations on the spline. We do this by augmenting the observation
4

Figure 4: Scene Labels: Scene labels obtained from joint boosting (left) and from our model (right).

model with an additional latent variable s per object detection d as illustrated in Fig. 3(b). The
posterior is modeled using a left-to-right Hidden Markov Model (HMM), defined as:
pl (t|l, R, C) =

X

pl (s1 )pl (d1 |s1 , l, R, C)

s1 ,..,sM

M
Y

pl (sm |sm?1 )pl (dm |sm , l, R, C)

(3)

m=2

We constrain all tracklets to move forward in 3D by defining the transition probability p(sm |sm?1 )
as uniform on sm ? sm?1 and 0 otherwise. Further, uniform initial probabilites pl (s1 ) are employed, since no location information is available a priori. We assume that the emission likelihood
pl (dm |sm , l, R, C) factorizes into the object location and its orientation. We impose a multinomial
distribution over the orientation pl (fm , om |sm , l, R, C), where each object orientation votes for its
bin as well as neighboring bins, accounting for the uncertainty of the object detector. The 3D object
location is modeled as a Gaussian with uniform outlier probability cl
pl (fm , bm |sm , l, R, C) ? cl + N (? m |?m , ?m )

(4)

2

where ? m = ? m (fm , bm , C) ? R denotes the object detection mapped into bird?s eye perspective, ?m = ?m (sm , l, R) ? R2 is the coordinate of the spline point sm on lane l and
?m = ?m (fm , bm , C) ? R2?2 is the covariance of the object location in bird?s eye coordinates.
We now describe how we transform the 2D tracklets into 3D tracklets {? 1 , ?1 , .., ? M , ?M }, which
we use in pl (dm |sm , l, R, C): We project the image coordinates into bird?s eye perspective by backprojecting objects into 3D using several complementary cues. Towards this goal we use the 2D
bounding box foot-point in combination with the estimated road plane. Assuming typical vehicle
dimensions obtained from annotated ground truth, we also exploit the width and height of the bounding box. Covariances in bird?s eye perspective are obtained by error-propagation. In order to reduce
noise in the observations we employ a Kalman smoother with constant 3D velocity model.
Our parking posterior model is similar to the lane posterior described above, except that we do not
allow parked vehicles to move; We assume them to have arbitrary orientations and place them at the
sides of the road. Hence, we have
pp (t|l, R, C) =

M
XY
s

pp (dm |s, l, R, C)p(s)

(5)

m=1

with s the index for the parking spot location within a parking area and
pp (dm |s, l, R, C) = pp (fm , bm |s, l, R, C) ? cp + N (? m |?m , ?m )

(6)

Here, cp , ? m and ?m are defined as above, while ?m = ?m (s, l, R) ? R2 is the coordinate of
the parking spot location in bird?s eye perspective (see Fig. 3(b) for an illustration). For inference,
we subsample each tracklet trajectory equidistantly in intervals of 5 meters in order to reduce the
number of detections within a tracklet and keep the total evaluation time of p(R, E|C) low.
Vanishing Points: We detect two types of dominant vanishing points (VP) in the last frame of
each sequence: vf corresponding to the forward facing street and vc corresponding to the crossing
street. While vf is usually in the image, the u-coordinate of the crossing VP is often close to infinity
(see Fig. 1). As a consequence, we represent vf ? R by its image u-coordinate and vc ? [? ?4 , ?4 ]
by the angle of the crossing road, back projected into the image.
Following [19], we employ a line detector to reason about dominant VPs in the scene. We relax
the original model of [19] to allow for non-orthogonal VPs, as intersection arms are often nonorthogonal. Unfortunately, traditional VP detectors tend to fail in the presence of clutter, which
our images exhibit to a large extent, for example generated by shadows. To tackle this problem we
5

1

true positive rate

0.8

Felzenszwalb et al. [5] (raw)
Felzenszwalb et al. [5] (smoothed)
Our method (? unknown)
Our method (? known)

0.6

0.4

0.2
Learning based
Kosecka et al.
0
0

0.2

0.4
0.6
false positive rate

0.8

Error
32.6 ?
31.2 ?
15.7 ?
13.7 ?

1

(a) Detecting Structured Lines
(b) Object Orientation Error
Figure 5: Detecting Structured Lines and Object Orientation Errors: Our approach outperforms [19] in
the task of VP estimation, and [5] in estimating the orientation of objects.

reweight line segments according to their likelihood of carrying structural information. To this end,
we learn a k-nn classifier on an annotated training database where lines are labeled as either structure
or clutter. Here, structure refers to line segments that are aligned with the major orientations of the
road, as well as facade edges of buildings belonging to dominant VPs. Our feature set comprises
geometric information in the form of position, length, orientation and number of lines with the
same orientation as well as perpendicular orientation in a local window. The local appearance is
represented by the mean, standard deviation and entropy of all pixels on both sides of the line.
Finally, we add texton-like features using a Gabor filter bank, as well as 3 principal components of
the scene GIST [23]. The structure k-nn classifier?s confidence is used in the VP voting process to
reweight the lines. The benefit of our learning-based approach is illustrated in Fig. 5.
To avoid estimates from spurious outliers we threshold the dominant VPs and only retain the most
confident ones. We assume that vf and vc are independent given the road parameters. Let ?f =
?f (R, C) be the image u-coordinate (in pixels) of the forward facing street?s VP and let ?c =
?c (R, C) be the orientation (in radians) of the crossing street in the image. We define
p(vf |R, C) ? cf + ?f N (vf |?f , ?f )

p(vc |R, C) ? cc + ?c N (vc |?c , ?c )

where {cf , cc } are small constants capturing outliers, {?f , ?c } take value 1 if the corresponding VP
has been detected in the image and 0 otherwise, and {?f , ?c } are parameters of the VP model.
Semantic Labels: We segment the last frame of the sequence pixelwise into 3 semantic classes,
i.e., road, sky and background. For each patch, we infer a score for each of the 3 labels using the
boosting algorithm of [30] with a combination of Walsh-Hadamard filters [30], as well as multi-scale
features developed for detecting man-made structures [21] on patches of size 16?16, 32?32 and
64?64. We include the latter ones as they help in discriminating buildings from road. For training,
we use a set of 200 hand-labeled images which are not part of the test data.
(i)

Given the softmax normalized label scores Su,v ? R of each class i for the patch located at position
(u, v) in the image, we define the likelihood of a scene labeling S = {S(1) , S(2) , S(3) } as
p(S|R, C) ? exp(?

3
X
X

(i)
Su,v
)

(7)

i=1 (u,v)?Si

where ? is a model parameter and Si is the set of all pixels of class i obtained from the reprojection
of the geometric model into the image. Note that the road boundaries directly define the lower end
of a facade while we assume a typical building height of 4 stories, leading to the upper end. Facades
adjacent to the observers own? street are not considered. Fig. 4 illustrates an example of the scene
labeling returned by boosting (left) as well as the labeling generated from the reprojection of our
model (right). Note that a large overlap corresponds to a large likelihood in Eq. 7
3.3

Learning and Inference

Our goal is to estimate the posterior of R, given the image evidence E and the camera calibration C:
p(R|E, C) ? p(E|R, C)p(R)

(8)

Learning the prior: We estimate the parameters of the prior p(R) using maximum likelihood
leave-one-out cross-validation on the scene database of [7]. This is straightforward as the prior in
Eq. 2 factorizes. We employ KDE with ? = 0.02 to model p(?), as it works well in practice.
6

(Inference with known ?)

(Inference with unknown ?)

Baseline
Ours
Baseline
Ours

Location
6.0 m
5.8 m

?
27.4 %
70.8 %

Orientation
9.6 deg
5.9 deg

Location
6.2 m
6.6 m

Overlap
44.9 %
53.0 %

Orientation
21.7 deg
7.2 deg

Activity
18.4 %
11.5 %

Overlap
39.3 %
48.1 %

Activity
28.1 %
16.6 %

Figure 6: Inference of topology and geometry .
Stereo
Ours

k
92.9 %
71.7 %

Location
4.4 m
6.6 m

Orientation
6.6 deg
7.2 deg

Overlap
62.7 %
48.1 %

Activity
8.0 %
16.6 %

Figure 7: Comparison with stereo when k and ? are unknown.

Learning the 3D tracklet parameters: Eq. 4 requires a function ? : f, b, C ? ?, ? which takes
a frame index f ? N, an object bounding box b ? R4 and the calibration parameters C as input
and maps them to the object location ? ? R2 and uncertainty ? ? R2?2 in bird?s eye perspective.
As cues for this mapping we use the bounding box width and height, as well as the location of the
bounding box foot-point. Scene depth adaptive error propagation is employed for obtaining ?. The
unknown parameters of the mapping are the uncertainty in bounding box location ?u , ?v , width ??u
and height ??v as well as the real-world object dimensions ?x , ?y along with their uncertainties
??x , ??y . We learn these parameters using a separate training dataset, including 1020 images with
3634 manually labeled vehicles and depth information [8].
Inference: Since the posterior in Eq. 8 cannot be computed in closed form, we approximate it
using Metropolis-Hastings sampling [9]. We exploit a combination of local and global moves to
obtain a well-mixing Markov chain. While local moves modify R slightly, global moves sample R
directly from the prior. This ensures quickly traversing the search space, while still exploring local
modes. To avoid trans-dimensional jumps, the road layout ? is estimated separately beforehand using
MAP estimation ?M AP provided by joint boosting [30]. We pick each of the remaining elements of
R at random and select local and global moves with equal probability.

4

Experimental Evaluation

In this section, we first show that learning which line features convey structural information improves
dominant vanishing point detection. Next, we compare our method to a multiple kernel learning
(MKL) baseline in estimating scene topology, geometry and traffic activities on the dataset of [7], but
only employing information from a single camera. Finally, we show that our model can significantly
improve object orientation estimates compared to state-of-the-art part based models [5]. For all
experiments, we set cl = cp = 10?15 , ?f = 0.1, cf = 10?10 , ?c = 0.01, cc = 10?30 and ? = 0.1.
Vanishing Point Estimation: We use a database of 185 manually annotated images to learn a
predictor of which line segments are structured. This is important since cast shadows often mislead
the VP estimation process. Fig. 5(a) shows the ROC curves for the method of [19] relaxed to nonorthogonal VPs (blue) as well as our learning-based approach (red). While the baseline gets easily
disturbed by clutter, our method is more accurate and has significantly less false positives.
3D Urban Scene Inference: We evaluate our method?s ability to infer the scene layout by building
a competitive baseline based on multi-kernel Gaussian process regression [17]. We employ a total of
4 kernels built on GIST [23], tracklet histograms, VPs as well as scene labels. Note that these are the
same features employed by our model to estimate the scene topology, ?M AP . For the tracklets, we
discretize the 50?50 m area in front of the vehicle into bins of size 5?5 m. Each bin consists of four
binary elements, indicating whether forward, backward, left or right motion has been observed at
that location. The VPs are included with their value as well as an indicator variable denoting whether
the VP has been found or not. For each semantic class, we compute histograms at 3 scales, which
divide the image into 3 ? 1, 6 ? 2 and 12 ? 4 bins, and concatenate them. Following [7] we measure
error in terms of the location of the intersection center in meters, the orientation of the intersection
arms in degrees, the overlap of road area with ground truth as well as the percentage of correctly
discovered intersection crossing activities. For details about these metrics we refer the reader to [7].
7

Figure 8: Automatically inferred scene descriptions. (Left) Trackets from all frames superimposed. (Middle)
Inference result with ? known and (Right) ? unknown. The inferred intersection layout is shown in gray, ground
truth labels are given in blue. Detected activities are marked by red lines.

We perform two types of experiments: In the first one we assume that the type of intersection ? is
given, and in the second one we estimate ? as well. As shown in Fig. 6, our method significantly
outperforms the MKL baseline in almost all error measures. Our method particularly excels in
estimating the intersection arm orientations and activities. We also compare our approach to [7] in
Fig. 7. As this approach uses stereo cameras, it can be considered as an oracle, yielding the highest
performance achievable. Our approach is close to the oracle; The difference in performance is due
to the depth uncertainties that arise in the monocular case, which makes the problem much more
ambiguous. Fig. 8 shows qualitative results, with detections belonging to the same tracklet depicted
with the same color. The trajectories of all the trackets are superimposed in the last frame. Note
that, while for the 2-armed and 4-armed case the topology has been estimated correctly, the 3-armed
case has been confused with a 4-armed intersection. This is our most typical failure mode. Despite
this, the orientations are correctly estimated and the vehicles are placed at the correct locations.
Improving Object Orientation Estimation: We also evaluate the performance of our method
in estimating 360 degree object orientations. As cars are mostly aligned with the road surface,
we only focus on the orientation angle in bird?s eye coordinates. As a baseline, we employ the
part-based detector of [5] trained in a supervised fashion to distinguish between 8 canonical views,
where each view is a mixture component. We correct for the ego motion and project the highest
scoring orientation into bird?s eye perspective. For our method, we infer the scene layout R using
our approach and associate every tracklet to its lane by maximizing pl (l|t, R, C) over l using Viterbi
decoding. We then select the tangent angle at the associated spline?s footpoint s on the inferred lane
l as our orientation estimate. Since parked cars are often oriented arbitrarily, our evaluation focuses
on moving vehicles only. Fig. 5(b) shows that we are able to significantly reduce the orientation
error with respect to [5]. This also holds true for the smoothed version of [5], where we average
orientations over temporally neighboring bins within each tracklet.

5

Conclusions

We have proposed a generative model which is able to perform joint 3D inference over the scene
layout as well as the location and orientation of objects. Our approach is able to infer the scene
topology and geometry, as well as traffic activities from a short video sequence acquired with a
single camera mounted on a car driving around a mid-size city. Our generative model proves superior to a discriminative approach based on MKL. Furthermore, our approach is able to outperform
significantly a state-of-the-art detector on its ability to estimate 3D object orientation. In the future, we plan to incorporate more discriminative cues to further boost performance in the monocular
case. We also believe that incorporating traffic sign states and pedestrians into our model will be an
interesting avenue for future research towards fully understanding complex urban scenarios.
8

References
[1] S. Bao, M. Sun, and S. Savarese. Toward coherent object detection and scene layout understanding. In
CVPR, 2010.
[2] O. Barinova, V. Lempitsky, E. Tretyak, and P. Kohli. Geometric image parsing in man-made environments. In ECCV, 2010.
[3] W. Choi and S. Savarese. Multiple target tracking in world coordinate with single, minimally calibrated
camera. In ECCV, 2010.
[4] A. Ess, B. Leibe, K. Schindler, and L. Van Gool. Robust multi-person tracking from a mobile platform.
PAMI, 31:1831?1846, 2009.
[5] P. Felzenszwalb, R.Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively
trained part-based models. PAMI, 32:1627?1645, 2010.
[6] D. Gavrila and S. Munder. Multi-cue pedestrian detection and tracking from a moving vehicle. IJCV,
73:41?59, 2007.
[7] A. Geiger, M. Lauer, and R. Urtasun. A generative model for 3d urban scene understanding from movable
platforms. In Computer Vision and Pattern Recognition, 2011.
[8] A. Geiger, M. Roser, and R. Urtasun. Efficient large-scale stereo matching. In Asian Conference on
Computer Vision, 2010.
[9] W. Gilks and S. Richardson, editors. Markov Chain Monte Carlo in Practice. Chapman & Hall, 1995.
[10] S. Gould, T. Gao, and D. Koller. Region-based segmentation and object detection. In NIPS, 2009.
[11] A. Gupta, A. Efros, and M. Hebert. Blocks world revisited: Image understanding using qualitative geometry and mechanics. In ECCV, 2010.
[12] R. Hartley and A. Zisserman. Multiple View Geometry in Computer Vision. Cambridge, 2004.
[13] V. Hedau, D. Hoiem, and D.A. Forsyth. Recovering the spatial layout of cluttered rooms. In ICCV, 2009.
[14] D. Hoiem, A. Efros, and M. Hebert. Recovering surface layout from an image. IJCV, 75:151?172, 2007.
[15] D. Hoiem, A. Efros, and M. Hebert. Putting objects in perspective. IJCV, 80:3?15, 2008.
[16] C. Huang, B. Wu, and R. Nevatia. Robust object tracking by hierarchical association of detection responses. In ECCV, 2008.
[17] A. Kapoor, K. Grauman, R. Urtasun, and T. Darrell. Gaussian processes for object categorization. IJCV,
88:169?188, 2010.
[18] R. Kaucic, A. Perera, G. Brooksby, J. Kaufhold, and A. Hoogs. A unified framework for tracking through
occlusions and across sensor gaps. In CVPR, 2005.
[19] J. Kosecka and W. Zhang. Video compass. In ECCV, 2002.
[20] D. Kuettel, M. Breitenstein, L. Gool, and V. Ferrari. What?s going on?: Discovering spatio-temporal
dependencies in dynamic scenes. In CVPR, 2010.
[21] S. Kumar and M. Hebert. Man-made structure detection in natural images using a causal multiscale
random field. In CVPR, 2003.
[22] D. Lee, A. Gupta, M. Hebert, and T. Kanade. Estimating spatial layout of rooms using volumetric reasoning about objects and surfaces. In NIPS, 2010.
[23] A. Oliva and A. Torralba. Modeling the shape of the scene: a holistic representation of the spatial envelope. IJCV, 42:145?175, 2001.
[24] A. Saxena, S. H. Chung, and A. Y. Ng. 3-D depth reconstruction from a single still image. IJCV, 76:53?
69, 2008.
[25] G. Schindler and F. Dellaert. Atlanta world: An expectation maximization framework for simultaneous
low-level edge grouping and camera calibration in complex man-made environments. In CVPR, 2004.
[26] J. Shotton, J. Winn, C. Rother, and A. Criminisi. Textonboost for image understanding: Multi-class object
recognition and segmentation by jointly modeling texture, layout, and context. IJCV, 81:2?23, 2009.
[27] H. Wang, S. Gould, and D. Koller. Discriminative learning with latent variables for cluttered indoor scene
understanding. In ECCV, 2010.
[28] X. Wang, X. Ma, and W. Grimson. Unsupervised activity perception in crowded and complicated scenes
using hierarchical bayesian models. PAMI, 2009.
[29] C. Wojek, S. Roth, K. Schindler, and B. Schiele. Monocular 3D Scene Modeling and Inference: Understanding Multi-Object Traffic Scenes. In ECCV, 2010.
[30] C. Wojek and B. Schiele. A dynamic CRF model for joint labeling of object and scene classes. In ECCV,
2008.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 1378-using-expectation-to-guide-processing-a-study-of-three-real-world-applications.pdf

Using Expectation to Guide Processing:
A Study of Three Real-World Applications
Shumeet 8aluja
Justsystem Pittsburgh Research Center &
School of Computer Science, Carnegie Mellon University
baluja@cs.cmu.edu

Abstract
In many real world tasks, only a small fraction of the available inputs are important
at any particular time. This paper presents a method for ascertaining the relevance
of inputs by exploiting temporal coherence and predictability. The method proposed in this paper dynamically allocates relevance to inputs by using expectations
of their future values. As a model of the task is learned, the model is simultaneously extended to create task-specific predictions of the future values of inputs.
Inputs which are either not relevant, and therefore not accounted for in the model,
or those which contain noise, will not be predicted accurately. These inputs can be
de-emphasized, and, in turn, a new, improved, model of the task created. The techniques presented in this paper have yielded significant improvements for the
vision-based autonomous control of a land vehicle, vision-based hand tracking in
cluttered scenes, and the detection of faults in the etching of semiconductor wafers.

1 Introduction
In many real-world tasks, the extraneous information in the input can be easily confused
with the important features, making the specific task much more difficult. One of the
methods in which humans function in the presence of many distracting features is to selectively attend to only portions of the input signal. A means by which humans select where
to focus their attention is through the use of expectations. Once the important features in
the current input are found, an expectation can be formed of what the important features in
the next inputs will be, as well as where they will be. The importance of features must be
determined in the context of a specific task; different tasks can require the processing of
different subsets of the features in the same input.
There are two distinct uses of expectations. Consider Carnegie Mellon's Navlab autonomous navigation system. The road-following module [Pomerleau, 1993] is separate from
the obstacle avoidance modules [Thorpe, 1991]. One role of expectation, in which unexpected features are de-emphasized, is appropriate for the road-following module in which
the features to be tracked, such as lane-markings, appear in predictable locations. This use
of expectation removes distractions from the input scene. The second role of expectation,
to emphasize unexpected features, is appropriate for the obstacle avoidance modules. This
use of expectation emphasizes unanticipated features of the input scene.

2 Architectures for Attention
In many studies of attention, saliency maps (maps which indicate input relevance) have
been constructed in a bottom-up manner. For example, in [Koch & Ullman, 1985], a

s. Baluja

860

saliency map, which is not task-specific, is created by emphasizing inputs which are different from their neighbors. An alternate approach, presented in [Clark & Ferrier, 1992],
places mUltiple different, weighted, task-specific feature detectors around the input image.
The regions of the image which contain high weighted sums of the detected features are
the portion of the scene which are focused upon. Top-down knowledge of which features
are used and the weightings of the features is needed to make the procedure task-specific.
In contrast, the goal of this study is to learn which task-specific features are relevant without requiring top-down knowledge.
In this study, we use a method based on Input Reconstruction Reliability Estimation
(IRRE) [Pomerleau, 1993] to detennine which portions of the input are important for the
task. IRRE uses the hidden units of a neural network (NN) to perfonn the desired task and
to reconstruct the inputs. In its original use, IRRE estimated how confident a network's
outputs were by measuring the similarity between the reconstructed and current inputs.
Figure 1(Left) provides a schematic ofIRRE. Note that the weights between the input and
hidden layers are trained to reduce both task and reconstruction error.
Because the weights between the input and hidden layers are trained to reduce both task
and reconstruction error, a potential drawback of IRRE is the use of the hidden layer to
encode all of the features in the image, rather than only the ones required for solving the
particular task [Pomerleau, 1993]. This can be addressed by noting the following: if a
strictly layered (connections are only between adjacent layers) feed-forward neural network can solve a given task, the activations of the hidden layer contain, in some fonn, the
important infonnation for this task from the input layer. One method of detennining what
is contained in the hidden layer is to attempt to reconstruct the original input image, based
solely upon the representation developed in the hidden layer. Like IRRE, the input image
is reconstructed from the activations of the units in the hidden layer. Unlike IRRE, the hidden units are not trained to reduce reconstruction error, they are only trained to solve the
panicular task. The network's allocation of its limited representation capacity at the hidden layer is an indicator of what it deems relevant to the task. Information which is not relevant to the task will not be encoded in the hidden units. Since the reconstruction of the
inputs is based solely on the hidden units' activations, and the irrelevant portions of the
input are not encoded in the hidden units' activations, the inputs which are irrelevant to the
task cannot be reconstructed. See Figure I(Right).
By measuring which inputs can be reconstructed accurately, we can ascertain which inputs
the hidden units have encoded to solve the task. A synthetic task which demonstrates this
idea is described here. Imagine being given a lOxlO input retina such as shown in
Figure 2a&b. The task is to categorize many such examples into one of four classes.
Because of the random noise in the examples, the simple underlying process, of a cross
being present in one of four locations (see Figure 2c), is not easily discernible, although it
is the feature on which the classifications are to be based. Given enough examples, the NN
will be able to solve this task. However, even after the model of the task is learned, it is
difficult to ascertain to which inputs the network is attending. To detennine this, we can
freeze the weights in the trained network and connect a input-reconstruction layer to the
hidden units, as shown in Figure 1(Right). After training these connections, by measuring
where the reconstruction matches the actual input, we can detennine which inputs the network has encoded in its hidden units, and is therefore attending. See Figure 2d.
weights
trained to
reduce task
error only
weights
trained to reduce
reconstruction
error only.
error.

Figure 1:

weights
trained to
reduce task
error only.

(Left) IRRE. (Right) Modified IRRE.

"-

weights
trained to reduce
reconstruction
error only.

Using Expectation to Guide Processing

B:

861

C:

D:
2

3

4

+

ir+

Figure 2: (A & B): Samples of training data (cross appears in position 4 & 1 respectively). Note the large
amounts of noise. (C): The underlying process puts a cross in one of these four locations. (D): The black
crosses are where the reconstruction matched the inputs; these correspond exactly to the underlying process.
IRRE and this modified IRRE are related to auto-encoding networks [Cottrell, 1990] and
principal components analysis (PeA). The difference between auto-encoding networks
and those employed in this study is that the hidden layers of the networks used here were
trained to perfonn well on the specific task, not to reproduce the inputs accurately.

2.1 Creating Expectations
A notion of time is necessary in order to focus attention in future frames. Instead of reconstructing the current input, the network is trained to predict the next input; this corresponds to changing the subscript in the reconstruction layer of the network shown in
Figure 1(Right) from t to t+ 1. The prediction is trained in a supervised manner, by using
the next set of inputs in the time sequence as the target outputs. The next inputs may contain noise or extraneous features. However, since the hidden units only encode infonnation
to solve the task, the network will be unable to construct the noise or extraneous features
in its prediction.
To this point, a method to create a task-specific expectation of what the next inputs will be
has been described. As described in Section 1, there are two fundamentally different ways
in which to interpret the difference between the expected next inputs and the actual next
inputs. The first interpretation is that the difference between the expected and the actual
inputs is a point of interest because it is a region which was not expected. This has applications in anomaly detection; it will be explored in Section 3.2. In the second interpretation, the difference between the expected and actual inputs is considered noise. Processing
should be de-emphasized from the regions in which the difference is large. This makes the
assumption that there is enough infonnation in the previous inputs to specify what and
where the important portions of the next image will be. As shown in the road-following
and hand-tracking task, this method can remove spurious features and noise.

3 Real-World Applications
1bree real-world tasks are discussed in this section. The first, vision-based road following,
shows how the task-specific expectations developed in the previous section can be used to
eliminate distractions from the input. The second, detection of anomalies in the plasmaetch step of wafer fabrication, shows how expectations can be used to emphasize the unexpected'features in the input. The third, visual hand-tracking, demonstrates how to incorporate a priori domain knowledge about expectations into the NN.

3.1 Application 1: Vision-Based Autonomous Road Following
In the domain of autonomous road following, the goal is to control a robot vehicle by analyzing the image of the road ahead. The direction of travel should be chosen based on the
location of important features like lane markings and r~ad edges. On highways and dirt
roads, simple techniques, such as feed-forward NNs, have worked well for mapping road
images to steering commands [Pomerleau, 1993]. However, on city streets, where there
are distractions like old lane-.narkings, pedestrians, and heavy traffic, these methods fail.
The purpose of using attention in this domain is to eliminate features of the road which the
NN may mistake as lane markings. Approximately 1200 images were gathered from a

862

S. Baluja

E

F

Figure 3: (Top) : Four samples of training images . Left most
shows the position of the lane-marking which was hand-marked.
(Right): In each triplet: Left: raw input imagtt. Middle: the
network's prediction of the inputs at time t; this prediction was
made by a network with input ofimaget_I ' Right: a pixel-by-pixel
filtered image (see text). This image is used as the input to the NN.

G

camera mounted on the left side of the CMU-Navlab 5 test vehicle, pointed downwards
and slightly ahead of the vehicle. The car was driven through city and residential neighborhoods around Pittsburgh, PA. The images were gathered at 4-5 hz. The images were
subsampled to 30x32 pixels. In each of these images, the horizontal position of the lane
marking in the 20th row of the input image was manually identified. The task is to produce
a Gaussian of activation in the outputs centered on the horizontal position of the lane
marking in the 20th row of the image, given the entire input image. Sample images and
target outputs are shown in Figure 3. In this task, the ANN can be confused by road edges
(Figure 3a), by extraneous lane markings (Figure 3b), and reflections on the car itself
(since the camera was positioned on the side of the car), as shown in Figure 3c.
The network architecture shown in Figure 4 was used; this is the same architecture as in
Figure l(right) with the feedback shown. The feedback is used during both training and
simulation. In each time-step, a steering direction and a prediction of the next inputs is
produced. For each time-step, the magnitude of the difference' between the input's
expected value (computed in the previous time-step) and its actual value is computed.
Each input pixel can be moved towards its background value l in proportion to this difference-value. The larger the difference value, the more weight is given to the background
value. If the difference value is small, the actual inputs are used. This has the effect of deemphasizing the unexpected inputs.
The results of using this method were very promising. The lane tracker removed distracting features from the images. In Figure 3G, a distracting lane-marking is removed: the
lane marker on the right was correctly tracked in images before the distractor lane-marker
appeared. In Figure 3F, a passing car is de-emphasized: the network does not have a model
to predict the movement of passing cars, since these are not relevant for the lane-marker
detection task. In Figure 3E, the side of the road appears brighter than expected; therefore
it is de-emphasized. Note that the expectation-images (shown in the middle of each triplet

weights
trained to
reduce task
error only.

Delayed I time step

I ...~~....

__

weights
--- trained to reduce
prediction
error only.

m~tii;:mrt;,._--1 Weight bkgd.

and actual
inputs according to
difference ima e.

Figure 4:
t - - - t Difference

between
inputs t &
predicted in uts

Architecture used
to track the lane
marking in
cluttered scenes.

Signal Transfer
(Connections are not trainable)
I. A simple estimate of the background value for each pixel is its average activation across the training set. For
the road-following domain, it is possible to use a background activation of 0.0 (when the entire image is scaled to
activations of +1.0 to -1.0) since the road often appears as intermediate grays.

Using Expectation to Guide Processing

863

in Figure 3) show that the expected lane-marker and road edge locations are not precisely
defined. This is due to the training method, which attempts to model the many possible
transitions from one time step to the next to account for inter- and intra-driver variability
with a limited training set [Baluja, 1996].
In summary, by eliminating the distractions in the input images, the lane-tracker with the
attention mechanisms improved performance by 20% over the standard lane-tracker, measured on the difference between the estimated and hand-marked position of the lanemarker in each image. This improvement was seen on multiple runs, with random initial
weights in the NN and different random translations chosen for the training images.

3.2 Application 2: Fault Detection in the Plasma-Etch Wafer Fabrication
Plasma etch is one of the many steps in the fabrication of semiconductor wafers. In this
study, the detection of four faults was attempted. Descriptions of the faults can be found in
[Baluja, 1996][Maxion, 1996]. For the experiments conducted here, only a single sensor
was used, which measured the intensity of light emitted from the plasma at the 520nm
wavelength. Each etch was sampled once a second, providing approximately 140 samples
per wafer wavefonn. The data-collection phase of this experiment began on October 25,
1994, and continued until April 4, 1995. The detection of faults is a difficult problem
because the contamination of the etch chamber and the degradation parts keeps the sensor's outputs, even for fault-free wafers, changing over time. Accounting for machine state
should help the detection process.
Expectation is used as follows: Given the waveform signature of waferT_I' an expectation
of waferT can be fonned. The input to the prediction-NN is the wavefonn signature of
waferT_I; the output is the prediction of the signature of waferT. The target output for each
example is the signature of the next wafer in sequence (the full 140 parameters). Detection
of the four faults is done with a separate network which used as input: the expectation of
the wafer's wavefonn, the actual wafer's wavefonn, and the point-by-point difference of
the two. In this task, the input is not filtered as in the driving domain described previously;
the values of the point-by-point difference vector are used as extra inputs.
The perfonnance of many methods and architectures were compared on this task, details
can be found in [Baluja, 1996]. The results using the expectation based methods was a
98.7% detection rate, 100% classification rate on the detected faults (detennining which of
the four types of faults the detected fault was), and a 2.3% false detection rate. For comparison, a simple perceptron had an 80% detection rate, and a 40% false-detection rate. A
fully-connected network which did not consider the state of the machine achieved a 100%
detection rate, but a 53% false detection rate. A network which considered state by using
the last-previous no-fault wafer for comparison with the current wafer (instead of an
expectation for the current wafer) achieved an 87.9% detection rate, and a 1.5% falsedetection rate. A variety of neural and non-neural methods which examined the differences between the expected and current wafer, as well those which examined the differences between the last no-fault wafer and the current wafer, perfonned poorly. In
summary, methods which did not use expectations were unable to obtain the false-positives and detection rates of the expectation-based methods.

3.3 Application 3: Hand-Tracking in Cluttered Scenes
In the tasks described so far, the transition rules were learned by the NN. However, if the
transition rules had been known a priori, processing could have been directed to only the
relevant regions by explicitly manipulating the expectations. The ability to incorporate a
priori rules is important in many vision-based tasks. Often the constraints about the environment in which the tracking is done can be used to limit the portions of the input scene
which need to be processed. For example, consider visually tracking a person's hand.
Given a fast camera sampling rate, the person's hand in the current frame will be close to

864

S. Baluja

B.

A.

Figure 5:
Typical input images used for
the hand-tracking experiments.
The target is to track the
subject's right hand. Without
expectation, in (A) both hands
were found in X outputs, and
the wrong hand was found in
the Y outputs. In (8) Subject's
right hand and face found in
the X outputs.

where it appeared in the previous frame. Although a network can learn this constraint by
developing expectations of future inputs (as with the NN architecture shown in Figure 4),
training the expectations can be avoided by incorporating this rule directly.
In this task, the input layer is a 48*48 image. There are two output layers of 48 units; the
desired outputs are two gaussians centered on the (X,Y) position of the hand to be tracked.
See Figure 5. Rather than creating a saliency map based upon the difference between the
actual and predicted inputs, as was done with autonomous road following, the saliency
map was explicitly created with the available domain knowledge. Given the sampling rate
of the camera and the size of the hand in the image, the salient region for the next timestep was a circular region centered on the estimated location of the hand in the previous
image. The activations of the inputs outside of the salient region were shifted towards the
background image. The activations inside the salient region were not modified. After
applying the saliency map to the inputs, the filtered inputs were fed into the NN.
This system was tested in very difficult situations; the testing set contained images of a
person moving both of his hands and body throughout the sequence (see Figure 5). Therefore, both hands and body are clearly visible in the difference images used as input into
the network. All training was done on much simpler training sets in which only a single
hand was moving. To gauge the perfonnance of an expectation-based system, it was compared to a system which used the following post-processing heuristics to account for temporal coherence. First, before a gaussian was fit to either of the output layers, the
activation of the outputs was inversely scaled with the distance away from the location of
the hand in the previous time step. This reduces the probability of detecting a hand in a
location very different than the previous detection. This helps when both hands are
detected, as shown in Figure 5. The second heuristic was that any predictions which differ
from the previous prediction by more than half of the dimension of the output layer were
ignored, and the previous prediction used instead. See Table I for the results. In summary,
by using the expectation based methods, perfonnance improved from 66% to 90% when
tracking the left hand, and 52% to 91 % when tracking the right hand.
Table I: Performance: Number of frames in which each hand was located (283 total images).

Method
No Heuristics, No Expect.
Heuristics
Expectation
Expectation + Heuristics

Target: Find Left Hand

Target: Find Right Hand

Which Hand Was Found

Which Hand Was Found

% Correct

L

52%
66%
91%
90%

146
187
258
256

R
44
22
3
3

None

% Correct

L

93
74
22
24

16%
52%
90%
91%

143
68
3
2

R
47
147
255
257

None
93
68
25
24

[Nowlan & Platt, 1995] presented a convolutional-NN based hand-tracker which used separate NNs for intensity and differences images with a rule-based integration of the multiple network outputs. The integration of this expectation-based system should improve the
performance of the difference-image NN.

Using Expectation to Guide Processing

865

4 Conclusions
A very closely related procedure to the one described in this paper is the use of Kalman
Filters to predict the locations of objects of interest in the input retina. For example, Dickmanns uses the prediction of the future state to help guide attention by controlling the
direction of a camera to acquire accurate position of landmarks [Dickmanns, 1992].
Strong models of the vehicle motion, the appearance of objects of interest (such as the
road, road-signs, and other vehicles), and the motion of these objects are encoded in the
system. The largest difference in their system and the one presented here is the amount of
a priori knowledge that is used. Many approaches which use Kalman Filters require a
large amount of problem specific information for creating the models. In the approach presented in this paper, the main object is to automatically learn this information from examples. First, the system must learn what the important features are, since no top-down
information is assumed. Second, the system must automatically develop the control strategy from the detected features. Third, the system must also learn a model for the movements of all of the relevant features.
In deciding whether the approaches described in this paper are suitable to a new problem,
two criteria must be considered. First, if expectation is to be used to remove distractions
from the inputs, then given the current inputs, the activations of the relevant inputs in the
next time step must be predictable while the irrelevant inputs are either unrelated to the
task or are unpredictable. In many visual object tracking problems, the relevant inputs are
often predictable while the distractions are not. In the cases in which the distractions are
predictable, if they are unrelated to the main task, these methods can work. When using
expectation to emphasize unexpected or potentially anomalous features, the activations of
the relevant inputs should be unpredictable while the irrelevant ones are predictable. This
is often the case for anomaly/fault detection tasks. Second, when expectations are used as
a filter, it is necessary to explicitly define the role of the expected features. In particular, it
is necessary to define whether the expected features should be considered relevant or irrelevant, and therefore, whether they should be emphasized or de-emphasized, respectively.
We have demonstrated the value of using task-specific expectations to guide processing in
three real-world tasks. In complex, dynamic, environments, such as driving, expectations
are used to quickly and accurately discriminate between the relevant and irrelevant features. For the detection of faults in the plasma-etch step of semiconductor fabrication,
expectations are used to account for the underlying drift of the process. Finally, for visionbased hand-tracking, we have shown that a priori knowledge about expectations can be
easily integrated with a hand-detection model to focus attention on small portions of the
scene, so that distractions in the periphery can be ignored.
Acknowledgments
The author would like to thank Dean Pomerleau, Takeo Kanade, Tom Mitchell and Tomaso Poggio
for their help in shaping this work.
References
Baluja, S. 1996, Expectation-Based Selective Attention. Ph.D. Thesis, School of Computer Science, CMU.
Clark, J. & Ferrier, N (1992), Attentive Visual Servoing, in: Active Vision. Blake & Yuille, (MIT Press) 137-154.
Cottrell, G.W., 1990, Extracting Features from Faces using Compression Network, Connectionist Models, Morgan Kaufmann 328-337.
Dickmanns, 1992, Expectation-based Dynamic Scene Understanding, in: Active Vision. A. Blake & A.Yuille,
MIT Press .
Koch, C. & Ullman, S. (1985) "Shifts in Selective Visual Attention: Towards the Underlying Neural Circuitry",
in: Human Neurobiology 4 (1985) 219-227.
Maxion, R. (1995) The Semiconductor Wafer Plasma-Etch Data Set.
Nowlan, S. & Platt, J., 1995, "A Convolutional Neural Network Hand Tracker". NIPS 7. MIT Press . 901-908.
Pomerleau, D.A., 1993. Neural Network Perception for Mobile Robot Guidance, Kluwer Academic.
Thorpe, C., 1991, Outdoor Visual Navigation for Autonomous Robots, in: Robotics and Autonomous Systems 7.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

