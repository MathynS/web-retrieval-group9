query sentence: autonomous vehicles
---------------------------------------------------------------------
title: 414-real-time-autonomous-robot-navigation-using-vlsi-neural-networks.pdf

Real-time autonomous robot navigation using
VLSI neural networks

Lionel Tarassenko Michael Brownlow Gillian Marshall?
Department of Engineering Science
Oxford University, Oxford, OXl 3PJ, UK

Jon Tombs

Alan Murray
Department of Electrical Engineering
Edinburgh University, Edinburgh, EH9 3JL, UK

Abstract
We describe a real time robot navigation system based on three VLSI
neural network modules. These are a resistive grid for path planning, a
nearest-neighbour classifier for localization using range data from a timeof-flight infra-red sensor and a sensory-motor associative network for dynamic obstacle avoidance .

1

INTRODUCTION

There have been very few demonstrations ofthe application ofVLSI neural networks
to real world problems. Yet there are many signal processing, pattern recognition
or optimization problems where a large number of competing hypotheses need to
be explored in parallel, most often in real time. The massive parallelism of VLSI
neural network devices, with one multiplier circuit per synapse, is ideally suited to
such problems. In this paper, we present preliminary results from our design for a
real time robot navigation system based on VLSI neural network modules. This is a
? Also: RSRE, Great Malvern, Worcester, WR14 3PS

422

Real-time Autonomous Robot Navigation Using VLSI Neural Networks
real world problem which has not been fully solved by traditional AI methods; even
when partial solutions have been proposed and implemented, these have required
vast computational resources, usually remote from the robot and linked to it via an
umbilical cord.

2

OVERVIEW

The aim of our work is to develop an autonomous vehicle capable of real-time
navigation, including obstacle avoidance, in a known indoor environment. The
obstacles may be permanent (static) or unexpected and dynamic (for example,
in an automated factory environment, the walls and machines are permanent but
people, other moving vehicles and packages are not.) There are three neural network
modules at the heart of our navigation system: a localization module (to determine,
at any time, the robot's position within the environment), an obstacle detection
module and a path planning module (to compute a path to the goal which avoids
obstacles). These modules perform low-level processing in real time which can then
be decoupled from higher level processing to be carried out by a simple controller.
It is our view that such a hybrid system is the best way to realise the computational
potential of artificial neural networks for solving a real world problem such as this
without compromising overall system performance.
A short description of each module is now given. In each case, the general principles
are first outlined and, where applicable, the results of our preliminary work are then
reported.

3

PATH PLANNING

The use ofresistive grids for parallel analog computation was first suggested by Horn
in the mid-seventies (Horn, 1974) and the idea has since been exploited by Mead and
co-workers, for example in a silicon retina (Mead and Mahowald, 1988). Although
these resistive grids cannot be said to be neural networks in the conventional sense,
they also perform parallel analog computation and they have the same advantages,
in terms of speed and fault-tolerance, as any hardware realisation of neural networks.
We have taken the resistive grid concept and applied it to the path planning problem, here taken to be the computation of an obstacle-avoiding path, in a structured
environment, from the robot's initial (or present) position (P) to its goal (G). In our
approach, the robot's working domain is discretized and mapped onto a resistive
grid of hexagonal or rectangular cells - see Figure 1 which shows the test environment for Autonomous Guided Vehicles (AGV's) in the Oxford Robotics Laboratory.
Each resistor in the grid has a value of flo, unless it is part of a region of the grid
corresponding to an obstacle, in which case its resistance is infinite (Roo).
The principle of the method is perhaps best understood by considering a continuous
analog of the resistive grid (for example, a sheet of material of uniform resistivity in
which holes have been cut to represent the obstacles). The current streamlines resulting from the application of an external source between P and G skirt around the
obstacles; if we follow one of these streamlines from P to G, we will obtain a guaranteed collision-free path since current cannot flow into the obstacles (Tarassenko and

423

424

Tarassenko, Brownlow, Marshall, Tombs, and Murray
Blake, 1991). For simple cases such as circularly symmetric conductivity distributions in 2D, Laplace's equation can be solved in order to calculate the value of the
potential V at every point within the workspace. Following a current streamline is
then simply a matter of performing gradient descent in V.

Figure 1: The Oxford test environment for AGV's mapped out as a hexagonal
resistive grid. The resistors corresponding to the four pillars in the middle are open
circuits. Note that the pillars are enlarged in their grid representation in order to
take into account the mobile robot's finite size.
It is not possible, however, to solve Laplace's equation analytically for realistic environments. With the resistive grid, the problem is discretized and mapped onto a
hardware representation which can be implemented in VLSI. As soon as an external
source of power is connected between P and G, the resistive network settles into
the state of least power dissipation and the node voltages can be read out (hardware computation of Kirchhoff's equations). The path from P to G is computed
incrementally from local voltage measurements: for each node, the next move is
identified by measuring the voltage drop ~ Vn between that node and each of its
nearest neighbours (n = 6 for a hexagonal grid) and then selecting the node corresponding to (~Vn)max. This is illustrated by the example of a robot in a maze
(Figure 2). As above, the resistors shown shaded are open circuits whilst all other
resistors are set to be equal to Ro. The robot is initially placed at the centre of the
maze (P) and a path has to be found to the goal in the top left-hand corner (G). The
solid line shows the path resulting from a single application of the voltage between
P and G. The dotted line shows the (optimal) path computed by re-applying the

Real-time Autonomous Robot Navigation Using VLSI Neural Networks
voltage at every node as the robot moves towards the goal. As already indicated,
this is actually how we intend to use the resistive grid planner in practice, since
this approach also allows us to re-compute the robot's path whenever unexpected
obstacles appear in the environment (see Section 5) .

..:~:::.~~::~.::~.~: ::~::~~:::~:::.~.::~::~.~::.::~.::::~~::~::~;;':X
~---;~~~~-*" ~I.: :x.........'?){ .... ) <?.... )<. . ?::x
x ..........
?x.
..'......

)E ...... ? ?~)( .... ???) (?????? ..

,?????.... ?~?? ...... ?x.. ??.. ? ?: ~: .. ???? ? :~::???? ??:~: .. ?? .. ?:x

..>ot~?? ....?:'X':.. ?.... ?';I(...... ?:~~~~'"*-*-'*?.? . . .?.:X: .. ..??

x:?

* .........,>0:: .

x;~?.~:~:~?:::?~:~?:~: ?. ~~: ~?:::~:::~::. . . :~:: .?~:.: ~.: .~:::~:::.~::.:~::~~~ ;~:.: .:~:.: ~: :~:.: ~:::~.:x

Figure 2: Path from middle of maze (P) to top left-hand corner (G)

3.1

VLSI IMPLEMENTATION

The VLSI implementation of the resistive grid method will allow us to solve the path
planning for complex environments in real time. MOS switches are ideal implementations of the binary resistors in the grid. Each transistor can be programmed to
be either open (Roo) or closed (Ro) from a RAM cell connected to its gate. With
the incremental computation of the path described above , the selection of the next
move is a matter of identifying the largest of six voltages. Of course, the nearest
neighbour voltages and that of the P node could be read out through an AID converter and the decision made off-chip. We favour a full hardware solution instead,
whereby the maximum voltage difference is directly identified on-chip.

4

LOCALIZATION

The autonomous robot should at any time be able to work out its position in
the workspace so that the path to the goal can be updated if required. The grid
representation of the environment used for the path planner can also be employed

425

426

Tarassenko, Brownlow, Marshall, Tombs, and Murray
for localization purposes, in which case localization becomes, in the first instance,
a matter of identifying the nearest node in the grid at any time during navigation.
This task can be performed by harnessing the pattern recognition capabilities of
neural networks. The room environment is learnt by recording a 3600 range scan
at every node during a training phase prior to navigation. During navigation, the
nearest node is identified using a minimum-distance classifier implemented on a
single-layer neural network working on dense input data (one range value every 30 ,
say). In order to solve the localization problem in real-time, we have designed a timeof-flight optical rangefinder, which uses near infra-red light, amplitude-modulated
at a frequency of just above 5 MHz, together with a heterodyne mixing technique.
Our design is capable of resolving phase shifts in the received light signal of the
order of 0.10 over a 50 dB dynamic range.
The rotating optical scanner gives a complete 360 0 scan approximately every second
during navigation. The minimum-distance classifier is used to compare this scan x
with the k patterns Uj recorded at each node during training. If we use a Euclidean
metric for the comparison, this is equivalent to identifying the pattern Uj for which:

(1)
is a minimum. The first term in the above equation is the same for all i and can be
ignored. We can therefore write:

= - '12 (- 2wT x +

= WiT +
where gj(x) is a linear discriminant function, Wi = Uj and
= -~u;,
gj

( x)

j

2
Uj)

X

WjQ

(2)

WjQ
Thus each
vector is one of the learnt patterns Ui and the discriminant gi(X) matches the
input x with Uj, point by point. If we let W j
{Iij} and x = {Vj} and assume
that there are n range values in each scan, then we can write:

Wj

=

j=n

gj(x)

= E Iij Vj +

WiO

(3)

j=l

Thus the synaptic weights are an exact copy of the patterns recorded at each grid
point during learning and the neurons can be thought of as processors which compute distances to those patterns. During navigation, the nearest node is identified
with a network of k neurons evaluating k discriminant functions in parallel, followed
by a ''winner-take-all'' network to pick the maximum gj(x). This is the well-known
implementation of the nearest-neighbour classifier on a neural network architecture.
Since the ui's are analog input vectors, then the synaptic weights Iij will also be
analog quantities and this leads to a very efficient use of the pulse-stream analog
VLSI technology which we have recently developed for the implementation of neural
networks (Murray et ai, 1990).
With pulse-stream arithmetic, analog computation is performed under digital control. The neural states are represented by pulse rates and synaptic multiplication is achieved by pulse width modulation. This allows very compact, fully-

Real-time Autonomous Robot Navigation Using VLSI Neural Networks
programmable, synapse circuits to be designed (3 or 4 transistors per synapse).
We have already applied one set of our working chips to the nearest-neighbour classification task described in this Section. They were evaluated on a 24-node test
environment and full results have been reported elsewhere (Brownlow, Tarassenko
and Murray, 1990). It was found that the E Iij Vi scalar products evaluated by our
VLSI chips on this test problem were always within 1.2% of those computed on a
SUN 3/80 workstation.

5

OBSTACLE DETECTION/AVOIDANCE

A more appropriate name for this module may be that of local navigation. The
module will rely on optical flow information derived from a number of fixed optical
sensors mounted on the robot platform. Each sensor will include a pulsed light
source to illuminate the scene locally and the light reflected from nearby objects
will be focussed onto a pair of gratings at right angles to each other, before being
detected by a photodiode array. From the time derivatives of the received signals,
it is possible to compute the relative velocities of nearby objects such as moving
obstacles. We plan to use previous work on structure from motion to pre-process
these velocity vectors and derive from them appropriate feature vectors to be used
as inputs to a low-level neural network for motor control (see Figure 3 below).

sensors~ intended path,...."'"

""

"

"q'

un
Measurement
flow from

of optic
sensors

w
Velocity signal of
approaching objects.

Low level network

Direct motor
control

Figure 3: Sensory-motor associative network for obstacle avoidance

427

428

Tarassenko, Brownlow, Marshall, Tombs, and Murray
The obstacle avoidance network will be taught to associate appropriate motor behaviours with different types of sensory input data, for example the taking of the
correct evasive action when a moving object is approaching the robot from a particular direction. This module will therefore be responsible for path adjustment in
response to dynamic obstacles (with a bandwidth of around 100 Hz), but the path
planner of Section 3 will continue to deal with path reconfiguration at a much lower
data rate (1 Hz), once the dynamic obstacle has been avoided. Our work on this
module has, so far, been mainly concerned with the design of the input sensors and
associated electronics.

6

CONCLUSION

We have implemented the path planning and localization modules described in this
paper on a SUN 4 workstation and used them to control a mobile robot platform
via a radio link. This capability was demonstrated at the NIPS'90 Conference with
a videotape recording of our mobile robot navigating around static obstacles in
a laboratory environment, using real-time infra-red data for localization. It was
possible to run the path planner in near real-time in simulation because no resistor
value need be changed in a static environment; in order to achieve real-time path
planning in a dynamic environment, however, the hardware solution of Section 3
will be mandatory. Our aim remains the implementation of all 3 modules in VLSI
in order to demonstrate a fully autonomous real-time navigation system with all
the sensors and hardware mounted on the robot platform.
Acknowledgements

We gratefully acknowledge the financial support of UK Science and Engineering
Research Council and of the EEC (ESPRIT BRA). We have benefitted greatly
from the help and advice of members of the Robotics Research Group, most notably
Martin Adams, Gabriel Hamid and Jake Reynolds.
References

M.J. Brownlow, L. Tarassenko & A.F. Murray. (1990) Analogue computation using
VLSI neural network devices. Electronics Letters, 26(16):1297-1299.
B.K.P. Horn. (1974) Determining lightness from an image. Computational Graphics
fj Image Processing, 3:277-299.
C.A. Mead & M.A. Mahowald. (1988) A silicon model of early visual processing.
Neural Networks, 1(1 ):91-97.
A.F. Murray, M.J. Brownlow, L. Tarassenko, A. Hamilton, I.S. Han & H.M. Reekie.
(1990) Pulse-Firing Neural Chips for Hundreds of Neurons. In D.S. Touretzky (ed.),
Advances in Neural Information Processing Systems 2, 785-792. San Mateo, CA:
Morgan Kaufmann.
L. Tarassenko & A. Blake. (1991). Analogue computation of collision-free paths. To
be published in: Proceedings of 1991 IEEE Int. Conf. on Robotics fj Automation,
Sacramento, CA:


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 4189-joint-3d-estimation-of-objects-and-scene-layout.pdf

Joint 3D Estimation of Objects and Scene Layout

Andreas Geiger
Karlsruhe Institute of Technology

Christian Wojek
MPI Saarbr?ucken

Raquel Urtasun
TTI Chicago

geiger@kit.edu

cwojek@mpi-inf.mpg.de

rurtasun@ttic.edu

Abstract
We propose a novel generative model that is able to reason jointly about the 3D
scene layout as well as the 3D location and orientation of objects in the scene.
In particular, we infer the scene topology, geometry as well as traffic activities
from a short video sequence acquired with a single camera mounted on a moving
car. Our generative model takes advantage of dynamic information in the form of
vehicle tracklets as well as static information coming from semantic labels and geometry (i.e., vanishing points). Experiments show that our approach outperforms
a discriminative baseline based on multiple kernel learning (MKL) which has access to the same image information. Furthermore, as we reason about objects in
3D, we are able to significantly increase the performance of state-of-the-art object
detectors in their ability to estimate object orientation.

1

Introduction

Visual 3D scene understanding is an important component in applications such as autonomous driving and robot navigation. Existing approaches produce either only qualitative results [11] or a mild
level of understanding, e.g., semantic labels [10, 26], object detection [5] or rough 3D [15, 24]. A
notable exception are approaches that try to infer the scene layout of indoor scenes in the form of
3D bounding boxes [13, 22]. However, these approaches can only cope with limited amounts of
clutter (e.g., beds), and rely on the fact that indoor scenes satisfy very closely the manhattan world
assumption, i.e., walls (and often objects) are aligned with the three dominant vanishing points. In
contrast, outdoor scenarios often show more clutter, vanishing points are not necessarily orthogonal
[25, 2], and objects often do not agree with the dominant vanishing points.
Prior work on 3D urban scene analysis is mostly limited to simple ground plane estimation [4, 29]
or models for which the objects and the scene are inferred separately [6, 7]. In contrast, in this paper
we propose a novel generative model that is able to reason jointly about the 3D scene layout as well
as the 3D location and orientation of objects in the scene. In particular, given a video sequence
of short duration acquired with a single camera mounted on a moving car, we estimate the scene
topology and geometry, as well as the traffic activities and 3D objects present in the scene (see Fig.
1 for an illustration). Towards this goal we propose a novel image likelihood which takes advantage
of dynamic information in the form of vehicle tracklets as well as static information coming from
semantic labels and geometry (i.e., vanishing points). Interestingly, our inference reasons about
whether vehicles are on the road, or parked, in order to get more accurate estimations. Furthermore,
we propose a novel learning-based approach to detecting vanishing points and experimentally show
improved performance in the presence of clutter when compared to existing approaches [19].
We focus our evaluation mainly on estimating the layout of intersections, as this is the most challenging inference task in urban scenes. Our approach proves superior to a discriminative baseline
based on multiple kernel learning (MKL) which has access to the same image information (i.e., 3D
tracklets, segmentation and vanishing points). We evaluate our method on a wide range of metrics
including the accuracy of estimating the topology and geometry of the scene, as well as detecting
1

Vehicle Tracklets

Vanishing Points

?
Scene Labels

Figure 1: Monocular 3D Urban Scene Understanding. (Left) Image cues. (Right) Estimated layout: Detections
belonging to a tracklet are depicted with the same color, traffic activities are depicted with red lines.

activities (i.e., traffic situations). Furthermore, we show that we are able to significantly increase the
performance of state-of-the-art object detectors [5] in terms of estimating object orientation.

2

Related Work

While outdoor scenarios remain fairly unexplored, estimating the 3D layout of indoor scenes has
experienced increased popularity in the past few years [13, 27, 22]. This can be mainly attributed
to the success of novel structured prediction methods as well as the fact that indoor scenes behave
mostly as ?Manhattan worlds?, i.e., edges on the image can be associated with parallel lines defined
in terms of the three dominant vanishing points which are orthonormal. With a moderate degree of
clutter, accurate geometry estimation has been shown for this scenario.
Unfortunately, most urban scenes violate the Manhattan world assumption. Several approaches
have focused on estimating vanishing points in this more adversarial setting [25]. Barinova et al. [2]
proposed to jointly perform line detection as well as vanishing point, azimut and zenith estimation.
However, their approach does not tackle the problem of 3D scene understanding and 3D object
detection. In contrast, we propose a generative model which jointly reasons about these two tasks.
Existing approaches to estimate 3D from single images in outdoor scenarios typically infer popups [14, 24]. Geometric approaches, reminiscent to the blocksworld model, which impose physical
constraints between objects (e.g., object A supports object B) have also been introduced [11]. Unfortunately, all these approaches are mainly qualitative and do not provide the level of accuracy
necessary for real-world applications such as autonomous driving and robot navigation. Prior work
on 3D traffic scene analysis is mostly limited to simple ground plane estimation [4], or models for
which the objects and scene are inferred separately [6]. In contrast, our model offers a much richer
scene description and reasons jointly about 3D objects and the scene layout.
Several methods have tried to infer the 3D locations of objects in outdoor scenarios [15, 1]. The most
successful approaches use tracklets to prune spurious detections by linking consistent evidence in
successive frames [18, 16]. However, these models are either designed for static camera setups in
surveillance applications [16] or do not provide a rich scene description [18]. Notable exceptions
are [3, 29] which jointly infer the camera pose and the location of objects. However, the employed
scene models are rather simplistic containing only a single flat ground plane.
The closest approach to ours is probably the work of Geiger et al. [7], where a generative model is
proposed in order to estimate the scene topology, geometry as well as traffic activities at intersections. Our work differs from theirs in two important aspects. First, they rely on stereo sequences
while we make use of monocular imagery. This makes the inference problem much harder, as the
noise in monocular imagery is strongly correlated with depth. Towards this goal we develop a richer
image likelihood model that takes advantage of vehicle tracklets, vanishing points as well as segmentations of the scene into semantic labels. The second and most important difference is that
Geiger et al. [7] estimate only the scene layout, while we reason jointly about the layout as well as
the 3D location and orientation of objects in the scene (i.e., vehicles).
2

1

2

3

4

5

6

7

(a) Model Geometry (? = 4)
(b) Model Topology ?
Figure 2: (a) Geometric model. In (b), the grey shaded areas illustrate the range of ?.

Finally, non-parametric models have been proposed to perform traffic scene analysis from a stationary camera with a view similar to bird?s eye perspective [20, 28]. In our work we aim to infer similar
activities but use video sequences from a camera mounted on a moving car with a substantially lower
viewpoint. This makes the recognition task much more challenging. Furthermore, those models do
not allow for viewpoint changes, while our model reasons about over 100 unseen scenes.

3

3D Urban Scene Understanding

We tackle the problem of estimating the 3D layout of urban scenes (i.e., road intersections) from
monocular video sequences. In this paper 2D refers to observations in the image plane while 3D
refers to the bird?s eye perspective (in our scenario the height above ground is non-informative). We
assume that the road surface is flat, and model the bird?s eye perspective as the y = 0 plane of the
standard camera coordinate system. The reference coordinate system is given by the position of the
camera in the last frame of the sequence. The intrinsic parameters of the camera are obtained using
camera calibration and the extrinsics using a standard Structure-from-Motion (SfM) pipeline [12].
We take advantage of dynamic and static information in the form of 3D vehicle tracklets, semantic labels (i.e., sky, background, road) and vanishing points. In order to compute 3D tracklets,
we first detect vehicles in each frame independently using a semi-supervised version of the partbased detector of [5] in order to obtain orientation estimates. 2D tracklets are then estimated using
?tracking-by-detection?: First adjacent frames are linked and then short tracklets are associated to
create longer ones via the hungarian method. Finally, 3D vehicle tracklets are obtained by projecting the 2D tracklets into bird?s eye perspective, employing error-propagation to obtain covariance
estimates. This is illustrated in Fig. 1 where detections belonging to the same tracklet are grouped
by color. The observer (i.e., our car) is shown in black. See sec 3.2 for more details on this process.
Since depth estimates in the monocular case are much noisier than in the stereo case, we employ
a more constrained model than the one utilized in [7]. In particular, as depicted in Fig. 2, we
model all intersection arms with the same width and force alternate arms to be collinear. We model
lanes with splines (see red lines for active lanes in Fig. fig:motivation), and place parking spots
at equidistant places along the street boundaries (see Fig. 3(b)). Our model then infers whether
the cars participate in traffic or are parked in order to get more accurate layout estimations. Latent
variables are employed to associate each detected vehicle with positions in one of these lanes or
parking spaces. In the following, we first give an overview of our probabilistic model and then
describe each part in detail.
3.1

Probabilistic Model

As illustrated in Fig. 2(b), we consider a fixed set of road layouts ?, including straight roads, turns,
3- and 4- armed intersections. Each of these layouts is associated with a set of geometric random
variables: The intersection center c, the street width w, the global scene rotation r and the angle of
the crossing street ? with respect to r (see Fig. 2(a)). Note that for ? = 1, ? does not exist.
Joint Distribution: Our goal is to estimate the most likely configuration R = (?, c, w, r, ?) given
the image evidence E = {T, V, S}, which comprises vehicle tracklets T = {t1 , .., tN }, vanish3

(a) Graphical model
(b) Road model
Figure 3: Graphical model and road model with lanes represented as B-splines.

ing points V = {vf , vc } and semantic labels S. We assume that, given R, all observations are
independent. Fig. 3(a) depicts our graphical model which factorizes the joint distribution as
#
"N
YX
p(tn , ln |R, C) p(vf |R, C)p(vc |R, C) p(S|R, C)
(1)
p(E, R|C) = p(R)
|
{z
} | {z }
n=1 ln
Semantic Labels
Vanishing Points
|
{z
}
Vehicle Tracklets

where C are the (known) extrinsic and intrinsic camera parameters for all the frames in the video
sequence, N is the total number of tracklets and {ln } denotes latent variables representing the lane
or parking positions associated with every vehicle tracklet. See Fig. 3(b) for an illustration.
Prior:

Let us first define a scene prior, which factorizes as
p(R) = p(?)p(c, w)p(r)p(?)

(2)

where c and w are modeled jointly to capture their correlation. We model w using a log-Normal
distribution since it takes only positive values. Further, since it is highly multimodal, we model p(?)
in a non-parametric fashion using kernel density estimation (KDE), and define:
r ? N (?r , ?r )

(c, log w)T ? N (?cw , ?cw )

? ? ?(?M AP )

In order to avoid the requirement for trans-dimensional inference procedures, the topology ?M AP
is estimated a priori using joint boosting, and set fixed at inference. To estimate ?M AP , we use the
same feature set employed by the MKL baseline (see Sec. 4 for details).
3.2

Image Likelihood

This section details our image likelihood for tracklets, vanishing points and semantic labels.
Vehicle Tracklets: In the following, we drop the tracklet index n to simplify notation. Let us
define a 3D tracklet as a set of object detections t = {d1 , .., dM }. Here, each object detection
dm = (fm , bm , om ) contains the frame index fm ? N, the object bounding box bm ? R4 defined
as 2D position and size, as well as a normalized orientation histogram om ? R8 with 8 bins. We
compute the bounding box bm and orientation om by supervised training of a part-based object
detector [5], where each component contains examples from a single orientation. Following [5], we
apply the softmax function on the output scores and associate frames using the hungarian algorithm
in order to obtain tracklets.
As illustrated in Fig. 3(b), we represent drivable locations with splines, which connect incoming
and outgoing lanes of the intersection. We also allow cars to be parked on the side of the road, see
Fig. 3(b) for an illustration. Thus, for a K-armed intersection, we have l ? {1, .., K(K ? 1) + 2K}
in total, where K(K ? 1) is the number of lanes and 2K is the number of parking areas. We use the
latent variable l to index the lane or parking position associated with a tracklet. The joint probability
of a tracklet t and its lane index l is given by p(t, l|R, C) = p(t|l, R, C)p(l). We assume a uniform
prior over lanes and parking positions l ? U(1, K(K ? 1) + 2K), and denote the posterior by pl
when l corresponds to a lane, and pp when it is a parking position.
In order to evaluate the tracklet posterior for lanes pl (t|l, R, C), we need to associate all object
detections t = {d1 , .., dM } to locations on the spline. We do this by augmenting the observation
4

Figure 4: Scene Labels: Scene labels obtained from joint boosting (left) and from our model (right).

model with an additional latent variable s per object detection d as illustrated in Fig. 3(b). The
posterior is modeled using a left-to-right Hidden Markov Model (HMM), defined as:
pl (t|l, R, C) =

X

pl (s1 )pl (d1 |s1 , l, R, C)

s1 ,..,sM

M
Y

pl (sm |sm?1 )pl (dm |sm , l, R, C)

(3)

m=2

We constrain all tracklets to move forward in 3D by defining the transition probability p(sm |sm?1 )
as uniform on sm ? sm?1 and 0 otherwise. Further, uniform initial probabilites pl (s1 ) are employed, since no location information is available a priori. We assume that the emission likelihood
pl (dm |sm , l, R, C) factorizes into the object location and its orientation. We impose a multinomial
distribution over the orientation pl (fm , om |sm , l, R, C), where each object orientation votes for its
bin as well as neighboring bins, accounting for the uncertainty of the object detector. The 3D object
location is modeled as a Gaussian with uniform outlier probability cl
pl (fm , bm |sm , l, R, C) ? cl + N (? m |?m , ?m )

(4)

2

where ? m = ? m (fm , bm , C) ? R denotes the object detection mapped into bird?s eye perspective, ?m = ?m (sm , l, R) ? R2 is the coordinate of the spline point sm on lane l and
?m = ?m (fm , bm , C) ? R2?2 is the covariance of the object location in bird?s eye coordinates.
We now describe how we transform the 2D tracklets into 3D tracklets {? 1 , ?1 , .., ? M , ?M }, which
we use in pl (dm |sm , l, R, C): We project the image coordinates into bird?s eye perspective by backprojecting objects into 3D using several complementary cues. Towards this goal we use the 2D
bounding box foot-point in combination with the estimated road plane. Assuming typical vehicle
dimensions obtained from annotated ground truth, we also exploit the width and height of the bounding box. Covariances in bird?s eye perspective are obtained by error-propagation. In order to reduce
noise in the observations we employ a Kalman smoother with constant 3D velocity model.
Our parking posterior model is similar to the lane posterior described above, except that we do not
allow parked vehicles to move; We assume them to have arbitrary orientations and place them at the
sides of the road. Hence, we have
pp (t|l, R, C) =

M
XY
s

pp (dm |s, l, R, C)p(s)

(5)

m=1

with s the index for the parking spot location within a parking area and
pp (dm |s, l, R, C) = pp (fm , bm |s, l, R, C) ? cp + N (? m |?m , ?m )

(6)

Here, cp , ? m and ?m are defined as above, while ?m = ?m (s, l, R) ? R2 is the coordinate of
the parking spot location in bird?s eye perspective (see Fig. 3(b) for an illustration). For inference,
we subsample each tracklet trajectory equidistantly in intervals of 5 meters in order to reduce the
number of detections within a tracklet and keep the total evaluation time of p(R, E|C) low.
Vanishing Points: We detect two types of dominant vanishing points (VP) in the last frame of
each sequence: vf corresponding to the forward facing street and vc corresponding to the crossing
street. While vf is usually in the image, the u-coordinate of the crossing VP is often close to infinity
(see Fig. 1). As a consequence, we represent vf ? R by its image u-coordinate and vc ? [? ?4 , ?4 ]
by the angle of the crossing road, back projected into the image.
Following [19], we employ a line detector to reason about dominant VPs in the scene. We relax
the original model of [19] to allow for non-orthogonal VPs, as intersection arms are often nonorthogonal. Unfortunately, traditional VP detectors tend to fail in the presence of clutter, which
our images exhibit to a large extent, for example generated by shadows. To tackle this problem we
5

1

true positive rate

0.8

Felzenszwalb et al. [5] (raw)
Felzenszwalb et al. [5] (smoothed)
Our method (? unknown)
Our method (? known)

0.6

0.4

0.2
Learning based
Kosecka et al.
0
0

0.2

0.4
0.6
false positive rate

0.8

Error
32.6 ?
31.2 ?
15.7 ?
13.7 ?

1

(a) Detecting Structured Lines
(b) Object Orientation Error
Figure 5: Detecting Structured Lines and Object Orientation Errors: Our approach outperforms [19] in
the task of VP estimation, and [5] in estimating the orientation of objects.

reweight line segments according to their likelihood of carrying structural information. To this end,
we learn a k-nn classifier on an annotated training database where lines are labeled as either structure
or clutter. Here, structure refers to line segments that are aligned with the major orientations of the
road, as well as facade edges of buildings belonging to dominant VPs. Our feature set comprises
geometric information in the form of position, length, orientation and number of lines with the
same orientation as well as perpendicular orientation in a local window. The local appearance is
represented by the mean, standard deviation and entropy of all pixels on both sides of the line.
Finally, we add texton-like features using a Gabor filter bank, as well as 3 principal components of
the scene GIST [23]. The structure k-nn classifier?s confidence is used in the VP voting process to
reweight the lines. The benefit of our learning-based approach is illustrated in Fig. 5.
To avoid estimates from spurious outliers we threshold the dominant VPs and only retain the most
confident ones. We assume that vf and vc are independent given the road parameters. Let ?f =
?f (R, C) be the image u-coordinate (in pixels) of the forward facing street?s VP and let ?c =
?c (R, C) be the orientation (in radians) of the crossing street in the image. We define
p(vf |R, C) ? cf + ?f N (vf |?f , ?f )

p(vc |R, C) ? cc + ?c N (vc |?c , ?c )

where {cf , cc } are small constants capturing outliers, {?f , ?c } take value 1 if the corresponding VP
has been detected in the image and 0 otherwise, and {?f , ?c } are parameters of the VP model.
Semantic Labels: We segment the last frame of the sequence pixelwise into 3 semantic classes,
i.e., road, sky and background. For each patch, we infer a score for each of the 3 labels using the
boosting algorithm of [30] with a combination of Walsh-Hadamard filters [30], as well as multi-scale
features developed for detecting man-made structures [21] on patches of size 16?16, 32?32 and
64?64. We include the latter ones as they help in discriminating buildings from road. For training,
we use a set of 200 hand-labeled images which are not part of the test data.
(i)

Given the softmax normalized label scores Su,v ? R of each class i for the patch located at position
(u, v) in the image, we define the likelihood of a scene labeling S = {S(1) , S(2) , S(3) } as
p(S|R, C) ? exp(?

3
X
X

(i)
Su,v
)

(7)

i=1 (u,v)?Si

where ? is a model parameter and Si is the set of all pixels of class i obtained from the reprojection
of the geometric model into the image. Note that the road boundaries directly define the lower end
of a facade while we assume a typical building height of 4 stories, leading to the upper end. Facades
adjacent to the observers own? street are not considered. Fig. 4 illustrates an example of the scene
labeling returned by boosting (left) as well as the labeling generated from the reprojection of our
model (right). Note that a large overlap corresponds to a large likelihood in Eq. 7
3.3

Learning and Inference

Our goal is to estimate the posterior of R, given the image evidence E and the camera calibration C:
p(R|E, C) ? p(E|R, C)p(R)

(8)

Learning the prior: We estimate the parameters of the prior p(R) using maximum likelihood
leave-one-out cross-validation on the scene database of [7]. This is straightforward as the prior in
Eq. 2 factorizes. We employ KDE with ? = 0.02 to model p(?), as it works well in practice.
6

(Inference with known ?)

(Inference with unknown ?)

Baseline
Ours
Baseline
Ours

Location
6.0 m
5.8 m

?
27.4 %
70.8 %

Orientation
9.6 deg
5.9 deg

Location
6.2 m
6.6 m

Overlap
44.9 %
53.0 %

Orientation
21.7 deg
7.2 deg

Activity
18.4 %
11.5 %

Overlap
39.3 %
48.1 %

Activity
28.1 %
16.6 %

Figure 6: Inference of topology and geometry .
Stereo
Ours

k
92.9 %
71.7 %

Location
4.4 m
6.6 m

Orientation
6.6 deg
7.2 deg

Overlap
62.7 %
48.1 %

Activity
8.0 %
16.6 %

Figure 7: Comparison with stereo when k and ? are unknown.

Learning the 3D tracklet parameters: Eq. 4 requires a function ? : f, b, C ? ?, ? which takes
a frame index f ? N, an object bounding box b ? R4 and the calibration parameters C as input
and maps them to the object location ? ? R2 and uncertainty ? ? R2?2 in bird?s eye perspective.
As cues for this mapping we use the bounding box width and height, as well as the location of the
bounding box foot-point. Scene depth adaptive error propagation is employed for obtaining ?. The
unknown parameters of the mapping are the uncertainty in bounding box location ?u , ?v , width ??u
and height ??v as well as the real-world object dimensions ?x , ?y along with their uncertainties
??x , ??y . We learn these parameters using a separate training dataset, including 1020 images with
3634 manually labeled vehicles and depth information [8].
Inference: Since the posterior in Eq. 8 cannot be computed in closed form, we approximate it
using Metropolis-Hastings sampling [9]. We exploit a combination of local and global moves to
obtain a well-mixing Markov chain. While local moves modify R slightly, global moves sample R
directly from the prior. This ensures quickly traversing the search space, while still exploring local
modes. To avoid trans-dimensional jumps, the road layout ? is estimated separately beforehand using
MAP estimation ?M AP provided by joint boosting [30]. We pick each of the remaining elements of
R at random and select local and global moves with equal probability.

4

Experimental Evaluation

In this section, we first show that learning which line features convey structural information improves
dominant vanishing point detection. Next, we compare our method to a multiple kernel learning
(MKL) baseline in estimating scene topology, geometry and traffic activities on the dataset of [7], but
only employing information from a single camera. Finally, we show that our model can significantly
improve object orientation estimates compared to state-of-the-art part based models [5]. For all
experiments, we set cl = cp = 10?15 , ?f = 0.1, cf = 10?10 , ?c = 0.01, cc = 10?30 and ? = 0.1.
Vanishing Point Estimation: We use a database of 185 manually annotated images to learn a
predictor of which line segments are structured. This is important since cast shadows often mislead
the VP estimation process. Fig. 5(a) shows the ROC curves for the method of [19] relaxed to nonorthogonal VPs (blue) as well as our learning-based approach (red). While the baseline gets easily
disturbed by clutter, our method is more accurate and has significantly less false positives.
3D Urban Scene Inference: We evaluate our method?s ability to infer the scene layout by building
a competitive baseline based on multi-kernel Gaussian process regression [17]. We employ a total of
4 kernels built on GIST [23], tracklet histograms, VPs as well as scene labels. Note that these are the
same features employed by our model to estimate the scene topology, ?M AP . For the tracklets, we
discretize the 50?50 m area in front of the vehicle into bins of size 5?5 m. Each bin consists of four
binary elements, indicating whether forward, backward, left or right motion has been observed at
that location. The VPs are included with their value as well as an indicator variable denoting whether
the VP has been found or not. For each semantic class, we compute histograms at 3 scales, which
divide the image into 3 ? 1, 6 ? 2 and 12 ? 4 bins, and concatenate them. Following [7] we measure
error in terms of the location of the intersection center in meters, the orientation of the intersection
arms in degrees, the overlap of road area with ground truth as well as the percentage of correctly
discovered intersection crossing activities. For details about these metrics we refer the reader to [7].
7

Figure 8: Automatically inferred scene descriptions. (Left) Trackets from all frames superimposed. (Middle)
Inference result with ? known and (Right) ? unknown. The inferred intersection layout is shown in gray, ground
truth labels are given in blue. Detected activities are marked by red lines.

We perform two types of experiments: In the first one we assume that the type of intersection ? is
given, and in the second one we estimate ? as well. As shown in Fig. 6, our method significantly
outperforms the MKL baseline in almost all error measures. Our method particularly excels in
estimating the intersection arm orientations and activities. We also compare our approach to [7] in
Fig. 7. As this approach uses stereo cameras, it can be considered as an oracle, yielding the highest
performance achievable. Our approach is close to the oracle; The difference in performance is due
to the depth uncertainties that arise in the monocular case, which makes the problem much more
ambiguous. Fig. 8 shows qualitative results, with detections belonging to the same tracklet depicted
with the same color. The trajectories of all the trackets are superimposed in the last frame. Note
that, while for the 2-armed and 4-armed case the topology has been estimated correctly, the 3-armed
case has been confused with a 4-armed intersection. This is our most typical failure mode. Despite
this, the orientations are correctly estimated and the vehicles are placed at the correct locations.
Improving Object Orientation Estimation: We also evaluate the performance of our method
in estimating 360 degree object orientations. As cars are mostly aligned with the road surface,
we only focus on the orientation angle in bird?s eye coordinates. As a baseline, we employ the
part-based detector of [5] trained in a supervised fashion to distinguish between 8 canonical views,
where each view is a mixture component. We correct for the ego motion and project the highest
scoring orientation into bird?s eye perspective. For our method, we infer the scene layout R using
our approach and associate every tracklet to its lane by maximizing pl (l|t, R, C) over l using Viterbi
decoding. We then select the tangent angle at the associated spline?s footpoint s on the inferred lane
l as our orientation estimate. Since parked cars are often oriented arbitrarily, our evaluation focuses
on moving vehicles only. Fig. 5(b) shows that we are able to significantly reduce the orientation
error with respect to [5]. This also holds true for the smoothed version of [5], where we average
orientations over temporally neighboring bins within each tracklet.

5

Conclusions

We have proposed a generative model which is able to perform joint 3D inference over the scene
layout as well as the location and orientation of objects. Our approach is able to infer the scene
topology and geometry, as well as traffic activities from a short video sequence acquired with a
single camera mounted on a car driving around a mid-size city. Our generative model proves superior to a discriminative approach based on MKL. Furthermore, our approach is able to outperform
significantly a state-of-the-art detector on its ability to estimate 3D object orientation. In the future, we plan to incorporate more discriminative cues to further boost performance in the monocular
case. We also believe that incorporating traffic sign states and pedestrians into our model will be an
interesting avenue for future research towards fully understanding complex urban scenarios.
8

References
[1] S. Bao, M. Sun, and S. Savarese. Toward coherent object detection and scene layout understanding. In
CVPR, 2010.
[2] O. Barinova, V. Lempitsky, E. Tretyak, and P. Kohli. Geometric image parsing in man-made environments. In ECCV, 2010.
[3] W. Choi and S. Savarese. Multiple target tracking in world coordinate with single, minimally calibrated
camera. In ECCV, 2010.
[4] A. Ess, B. Leibe, K. Schindler, and L. Van Gool. Robust multi-person tracking from a mobile platform.
PAMI, 31:1831?1846, 2009.
[5] P. Felzenszwalb, R.Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively
trained part-based models. PAMI, 32:1627?1645, 2010.
[6] D. Gavrila and S. Munder. Multi-cue pedestrian detection and tracking from a moving vehicle. IJCV,
73:41?59, 2007.
[7] A. Geiger, M. Lauer, and R. Urtasun. A generative model for 3d urban scene understanding from movable
platforms. In Computer Vision and Pattern Recognition, 2011.
[8] A. Geiger, M. Roser, and R. Urtasun. Efficient large-scale stereo matching. In Asian Conference on
Computer Vision, 2010.
[9] W. Gilks and S. Richardson, editors. Markov Chain Monte Carlo in Practice. Chapman & Hall, 1995.
[10] S. Gould, T. Gao, and D. Koller. Region-based segmentation and object detection. In NIPS, 2009.
[11] A. Gupta, A. Efros, and M. Hebert. Blocks world revisited: Image understanding using qualitative geometry and mechanics. In ECCV, 2010.
[12] R. Hartley and A. Zisserman. Multiple View Geometry in Computer Vision. Cambridge, 2004.
[13] V. Hedau, D. Hoiem, and D.A. Forsyth. Recovering the spatial layout of cluttered rooms. In ICCV, 2009.
[14] D. Hoiem, A. Efros, and M. Hebert. Recovering surface layout from an image. IJCV, 75:151?172, 2007.
[15] D. Hoiem, A. Efros, and M. Hebert. Putting objects in perspective. IJCV, 80:3?15, 2008.
[16] C. Huang, B. Wu, and R. Nevatia. Robust object tracking by hierarchical association of detection responses. In ECCV, 2008.
[17] A. Kapoor, K. Grauman, R. Urtasun, and T. Darrell. Gaussian processes for object categorization. IJCV,
88:169?188, 2010.
[18] R. Kaucic, A. Perera, G. Brooksby, J. Kaufhold, and A. Hoogs. A unified framework for tracking through
occlusions and across sensor gaps. In CVPR, 2005.
[19] J. Kosecka and W. Zhang. Video compass. In ECCV, 2002.
[20] D. Kuettel, M. Breitenstein, L. Gool, and V. Ferrari. What?s going on?: Discovering spatio-temporal
dependencies in dynamic scenes. In CVPR, 2010.
[21] S. Kumar and M. Hebert. Man-made structure detection in natural images using a causal multiscale
random field. In CVPR, 2003.
[22] D. Lee, A. Gupta, M. Hebert, and T. Kanade. Estimating spatial layout of rooms using volumetric reasoning about objects and surfaces. In NIPS, 2010.
[23] A. Oliva and A. Torralba. Modeling the shape of the scene: a holistic representation of the spatial envelope. IJCV, 42:145?175, 2001.
[24] A. Saxena, S. H. Chung, and A. Y. Ng. 3-D depth reconstruction from a single still image. IJCV, 76:53?
69, 2008.
[25] G. Schindler and F. Dellaert. Atlanta world: An expectation maximization framework for simultaneous
low-level edge grouping and camera calibration in complex man-made environments. In CVPR, 2004.
[26] J. Shotton, J. Winn, C. Rother, and A. Criminisi. Textonboost for image understanding: Multi-class object
recognition and segmentation by jointly modeling texture, layout, and context. IJCV, 81:2?23, 2009.
[27] H. Wang, S. Gould, and D. Koller. Discriminative learning with latent variables for cluttered indoor scene
understanding. In ECCV, 2010.
[28] X. Wang, X. Ma, and W. Grimson. Unsupervised activity perception in crowded and complicated scenes
using hierarchical bayesian models. PAMI, 2009.
[29] C. Wojek, S. Roth, K. Schindler, and B. Schiele. Monocular 3D Scene Modeling and Inference: Understanding Multi-Object Traffic Scenes. In ECCV, 2010.
[30] C. Wojek and B. Schiele. A dynamic CRF model for joint labeling of object and scene classes. In ECCV,
2008.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 218-real-time-computer-vision-and-robotics-using-analog-vlsi-circuits.pdf

750

Koch, Bair, Harris, Horiuchi, Hsu and Luo

Real- Time Computer Vision and Robotics
Using Analog VLSI Circuits
Christof Koch

Wyeth Bair
John G. Harris
Timothy Horiuchi
Andrew Hsu
Jin Luo
Computation and Neural Systems Program
Caltech 216-76
Pasadena, CA 91125

ABSTRACT
The long-term goal of our laboratory is the development of analog
resistive network-based VLSI implementations of early and intermediate vision algorithms. We demonstrate an experimental circuit for smoothing and segmenting noisy and sparse depth data
using the resistive fuse and a 1-D edge-detection circuit for computing zero-crossings using two resistive grids with different spaceconstants. To demonstrate the robustness of our algorithms and
of the fabricated analog CMOS VLSI chips, we are mounting these
circuits onto small mobile vehicles operating in a real-time, laboratory environment.

1

INTRODUCTION

A large number of computer vision algorithms for finding intensity edges, computing motion, depth, and color, and recovering the 3-D shapes of objects have been
developed within the framework of minimizing an associated "energy" functional.
Such a variational formalism is attractive because it allows a priori constraints
to be explicitly stated. The single most important constraint is that the physical
processes underlying image formation, such as depth, orientation and surface reflectance, change slowly in space. For instance, the depths of neighboring points on
a surface are usually very similar. Standard regularization algorithms embody this
smoothness constraint and lead to quadratic variational functionals with a unique
global minimum (Poggio, Torre, and Koch, 1985). These quadratic functionals

Real-Time Computer Vision and Robotics Using Analog VLSI Circuits

G

G

G

G

Rl

G

G

Rl

(a)

3.1V

Node
Voltage

(b)
3.0V

I? I?

?
1

1

Edge
Output

?

2

345

Photoreceptor

6

o
7

Figure 1: (a) shows the schematic of the zero-crossing chip. The phototransistors
logarithmically map light intensity to voltages that are applied via a conductance G
onto the nodes of two linear resistive networks. The network resistances Rl and R2
can be arbitrarily adjusted to achieve different space-constants. Transconductance
amplifiers compute the difference of the smoothed network node voltages and report
a current proportional to that difference. The sign of current then drives exclusive-or
circuitry (not shown) between each pair of neighboring pixels. The final output is
a binary signal indicating the positions of the zero-crossings. The linear network
resistances have been implemented using Mead's saturating resistor circuit (Mead,
1989), and the vertical resistors are implemented with transconductance followers.
(b) shows the measured response of a seven-pixel version of the chip to a bright
background with a shadow cast across the middle three photoreceptors. The circles
and triangles show the node voltages on the resistive networks with the smaller and
larger space-constants, respectively. Edges are indicated by the binary output (bar
chart at bottom) corresponding to the locations of zero-crossings.

751

752

Koch, Bair, Harris, Horiuchi, Hsu and Luo

can be mapped onto linear resistive networks, such that the stationary voltage distribution, corresponding to the state of least power dissipation, is equivalent to
the solution of the variational functional (Horn, 1974; Poggio and Koch , 1985).
Smoothness breaks down, however, at discontinuities caused by occlusions or differences in the physical processes underlying image formation (e.g., different surface
reflectance properties). Detecting these discontinuities becomes crucial, not only
because otherwise smoothness is incorrectly applied but also because the locations
of discontinuities are often required for further image analysis and understanding.
We describe two different approaches for finding discontinuities in early vision: (1)
a 1-D edge-detection circuit for computing zero-crossings using two resistive grids
with different space-constants, and (2) a 20 by 20 pixel circuit for smoothing and
segmenting noisy and sparse depth data using the resistive fuse.
Finally, while successfully demonstrating a highly integrated circuit on a stationary
laboratory bench under controlled conditions is already a tremendous success, this
is not the environment in which we ultimately intend them to be used. The jump
from a sterile, well-controlled, and predictable environment such as that of the
laboratory bench to a noisy and physically demanding environment of a mobile
robot can often spell out the true limits of a circuit's robustness. In order to
demonstrate the robustness and real-time performance of these circuits, we have
mounted two such chips onto small toy vehicles.

2

AN EDGE DETECTION CIRCUIT

The zero-crossings of the Laplacian of the Gaussian, V 2 G, are often used for detecting edges. Marr and Hildreth (1980) discovered that the Mexican-hat shape
of the V2G operator can be approximated by the difference of two Gaussians
(DOG). In this spirit, we have built a chip that takes the difference of two resistivenetwork smoothings of photoreceptor input and finds the resulting zero-crossings.
The Green's function of the resistive network, a decaying exponential, differs from
the Gaussian, but simulations with digitized camera images have shown that the
difference of exponentials (DOE) gives results nearly as good as the DOG. Furthermore, resistive nets have a natural implementation in silicon, while implementing
the Gaussian is cumbersome.
The circuit, Figure la, uses two independent resistive networks to smooth the voltages supplied by logarithmic photoreceptors. The voltages on the two networks are
subtracted and exclusive-or circuitry (not shown) is used to detect zero-crossings. In
order to facilitate thresholding of edges, an additional current is computed at each
node indicating the strength of the zero-crossing. This is particularly important
for robust real-world performance where there will be many small (in magnitude
of slope) zero-crossings due to noise. Figure 1b shows the measured response of a
seven-pixel version of the chip to a bright background with a shadow cast across the
middle three photoreceptors. Subtracting the two network voltage traces shown at
the top, we find two zero-crossings, which the chip correctly identifies in the binary
output shown at the bottom.

Real-Time Computer Vision and Robotics Using Analog VLSI Circuits

~OJ
V

--

00

V- ~ ~,
_

J
./

(a)

........

vr-.....

,-

--

~
c::J-

I~

/,:1
I

...

G-

\' -

-

~f;j

-It\.

I:::j-

~I..

1 ~
2u 2 ~

dij

c::J-

1.

~

~

o0

/

~"""I

0 I Il
HRES

300
I
(nA)

(b)

O+-__~____-~V~T~~~________~____

-30~0.5
~V

0.0
(Volts)

0.5

Figure 2: (a) Schematic diagram for the 20 by 20 pixel surface interpolation and
smoothing chip. A rectangular mesh of resistive fuse elements (shown as rectangles)
provide the smoothing and segmentation ability of the network. The data are given
as battery values dij with the conductance G connecting the battery to the grid set
to G = 1/2u 2 , where u 2 is the variance of the additive Gaussian noise assumed to
corrupt the data. (b) Measured current-voltage relationship for different settings
of the resistive fuse. For a voltage of less than VT across this two-terminal device,
the circuit acts as a resistor with conductance A. Above VT, the current is either
abruptly set to zero (binary fuse) or smoothly goes to zero (analog fuse). We can
continuously vary the I-V curve from the hyperbolic tangent of Mead's saturating
resistor (HRES) to that of an analog fuse (Fig. 2b), effectively implementing a
continuation method for minimizing the non-convex functional. The I-V curve of a
binary fuse is also illustrated.

753

754

Koch, Bair, Harris, Horiuchi, Hsu and Luo

3

A CIRCUIT FOR SMOOTHING AND SEGMENTING

Many researchers have extended regularization theory to include discontinuities.
Let us consider the problem of interpolating noisy and sparse 1-D data (the 2-D
generalization is straightforward), where the depth data di is given on a discrete
grid. Associated with each lattice point is the value of the recovered surface Ii
and a binary line discontinuity Ii. When the surface is expected to be smooth
(with a first-order, membrane-type stabilizer) except at isolated discontinuities, the
functional to be minimized is given by:

J(f, I) = A~(fi+l - 1i)2(1 -Ii)

+ 2!2 ~(di -

I

I

1i)2 + a

~ Ii

(1)

I

where (]'2 is the variance of the additive Gaussian noise process assumed to corrupt
the data di, and A and a are free parameters. The first term implements the
piecewise smooth constraint: if all variables, with the exception of Ii, Ii+l, and Ii,
are held fixed and A(fi+l - h)2 < a, it is "cheaper" to pay the price A(fi+l - h)2
and set Ii = 0 than to pay the larger price a; if the gradient becomes too steep,
Ii = 1, and the surface is segmented at that location. The second term, with the
sum only including those locations i where data exist, forces the surface I to be
close to the measured data d. How close depends on the estimated magnitude of
the noise, in this case on (]'2. The final surface I is the one that best satisfies the
conflicting demands of piecewise smoothness and fidelity on the measured data.
To minimize the 2-D generalization of eq. (1), we map the functional J onto the
circuit shown in Fig. 2a such that the stationary voltage at every gridpoint then
corresponds to hi. The cost functional J is interpreted as electrical co-content,
the generalization of power for nonlinear networks. We designed a two-terminal
nonlinear device, which we call a resistive fuse, to implement piecewise smoothness
(Fig. 2b). If the magnitude of the voltage drop across the device is less than
VT = (a/A)1/2, the fuse acts as a linear resistor with conductance A. If VT is
exceeded, however, the fuse breaks and the current goes to zero. The operation of
the fuse is fully reversible. We built a 20 by 20 pixel fuse network chip and show
its segmentation and smoothing performance in Figure 3.

4

AUTONOMOUS VEHICLES

Our goal-beyond the design and fabrication of analog resistive-network chips-is
to build mobile testbeds for the evaluation of chips as well as to provide a systems
perspective on the usefulness of certain vision algorithms. Due to the small size
and power requirements of these chips, it is possible to utilize the vast resource of
commercially available toy vehicles. The advantages of toy cars over robotic vehicles
built for research are their low cost, ease of modification, high power-to-weight ratio,
availability, and inherent robustness to the real-world. Accordingly, we integrated
two analog resistive-network chips designed and built in Mead's laboratory onto
small toy cars controlled by a digital microprocessor (see Figure 4).

Real-Time Computer Vision and Robotics Using Analog VLSI Circuits

(c)

(b)

<a)

M

M

M

'.1

'.1

?
1.1
1.0

'.1

'.0

~

12 10
"Noel.?Number

(d)

20
( e)

1.0

...1""'' \

.".'

1.1

1.1

0

1.1

12 10
"Noel.? Number

20
( f)

12 10
"Noel.?Number

20

Figure 3: Experimental data from the fuse network chip. We use as input data a
tower (corresponding to dij = 3.0 V) rising from a plane (corresponding to 2.0 V)
with superimposed Gaussian noise. (a) shows the input with the variance of the
noise set to 0.2 V, (b) the voltage output using the fuse configured as a saturating
resistance, and (c) the output when the fuse elements are activated. (d), (e),
and (f) illustrate the same behavior along a horizontal slice across the chip for
(12
0.4 V. We used a hardware deterministic algorithm of varying the fuse I-V
curve of the saturating resistor to that of the analog fuse (following the arroW in
Fig. 2b) as well as increasing the conductance A. This algorithm is closely related
to other deterministic approximations based on continuation methods or a Mean
Field Theory approach (Koch, Marroquin, and Yuille, 1986; Blake and Zisserman,
1987; Geiger and Girosi, 1989). Notice that the amplitude of the noise in the last
case (40% of the amplitude of the voltage step) is so large that a single filtering
step on the input (d) will fail to detect the tower. Cooperativity and hysteresis
are required for optimal performance. Notice the "bad" pixel in the middle of the
tower (in c). Its effect is localized, however, to a single element.

=

755


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 6090-backprop-kf-learning-discriminative-deterministic-state-estimators.pdf

Backprop KF: Learning Discriminative Deterministic
State Estimators
Tuomas Haarnoja, Anurag Ajay, Sergey Levine, Pieter Abbeel
{haarnoja, anuragajay, svlevine, pabbeel}@berkeley.edu
Department of Computer Science, University of California, Berkeley

Abstract
Generative state estimators based on probabilistic filters and smoothers are one
of the most popular classes of state estimators for robots and autonomous vehicles. However, generative models have limited capacity to handle rich sensory
observations, such as camera images, since they must model the entire distribution
over sensor readings. Discriminative models do not suffer from this limitation,
but are typically more complex to train as latent variable models for state estimation. We present an alternative approach where the parameters of the latent state
distribution are directly optimized as a deterministic computation graph, resulting
in a simple and effective gradient descent algorithm for training discriminative
state estimators. We show that this procedure can be used to train state estimators
that use complex input, such as raw camera images, which must be processed
using expressive nonlinear function approximators such as convolutional neural
networks. Our model can be viewed as a type of recurrent neural network, and
the connection to probabilistic filtering allows us to design a network architecture
that is particularly well suited for state estimation. We evaluate our approach on
synthetic tracking task with raw image inputs and on the visual odometry task in
the KITTI dataset. The results show significant improvement over both standard
generative approaches and regular recurrent neural networks.

1

Introduction

State estimation is an important component of mobile robotic applications, including autonomous
driving and flight [22]. Generative state estimators based on probabilistic filters and smoothers are
one of the most popular classes of state estimators. However, generative models are limited in their
ability to handle rich observations, such as camera images, since they must model the full distribution
over sensor readings. This makes it difficult to directly incorporate images, depth maps, and other
high-dimensional observations. Instead, the most popular methods for vision-based state estimation
(such as SLAM [22]) are based on domain knowledge and geometric principles. Discriminative
models do not need to model the distribution over sensor readings, but are more complex to train
for state estimation. Discriminative models such as CRFs [16] typically do not use latent variables,
which means that training data must contain full state observations. Most real-world state estimation
problem settings only provide partial labels. For example, we might observe noisy position readings
from a GPS sensor and need to infer the corresponding velocities. While discriminative models can
be augmented with latent state [18], this typically makes them harder to train.
We propose an efficient and scalable method for discriminative training of state estimators. Instead of
performing inference in a probabilistic latent variable model, we instead construct a deterministic
computation graph with equivalent representational power. This computation graph can then be
optimized end-to-end with simple backpropagation and gradient descent methods. This corresponds
to a type of recurrent neural network model, where the architecture of the network is informed by the
30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

structure of the probabilistic state estimator. Aside from the simplicity of the training procedure, one
of the key advantages of this approach is the ability to incorporate arbitrary nonlinear components
into the observation and transition functions. For example, we can condition the transitions on raw
camera images processed by multiple convolutional layers, which have been shown to be remarkably
effective for interpreting camera images. The entire network, including the observation and transition
functions, is trained end-to-end to optimize its performance on the state estimation task.
The main contribution of this work is to draw a connection between discriminative probabilistic state
estimators and recurrent computation graphs, and thereby derive a new discriminative, deterministic
state estimation method. From the point of view of probabilistic models, we propose a method for
training expressive discriminative state estimators by reframing them as representationally equivalent
deterministic models. From the point of view of recurrent neural networks, we propose an approach
for designing neural network architectures that are well suited for state estimation, informed by
successful probabilistic state estimation models. We evaluate our approach on a visual tracking
problem, which requires processing raw images and handling severe occlusion, and on estimating
vehicle pose from images in the KITTI dataset [8]. The results show significant improvement over
both standard generative methods and standard recurrent neural networks.

2

Related Work

Some of the most successful methods for state estimation have been probabilistic generative state space models (SSMs) based on filtering and smoothing (Figure 1).
Kalman filters are perhaps the best known state estimators,
and can be extended to the case of nonlinear dynamics
through linearization and the unscented transform. Nonparametric filtering methods, such as particle filtering, are
also often used for tasks with multimodal posteriors. For
a more complete review of state estimation, we refer the
reader to standard references on this topic [22].

xt?1

xt

xt+1

ot?1

ot

ot+1

Figure 1: A generative state space model
with hidden states xi and observation ot
generated by the model. ot are observed
at both training and test time.

Generative models aim to estimate the distribution over state observation sequences o1:T as originating
from some underlying hidden state x1:T , which is typically taken to be the state space of the system.
This becomes impractical when the observation space is extremely high dimensional, and when the
observation is a complex, highly nonlinear function of the state, as in the case of vision-based state
estimation, where ot corresponds to an image viewed from a robot?s on-board camera. The challenges
of generative state space estimation can be mitigated by using complex observation models [14] or
approximate inference [15], but building effective generative models of images remains a challenging
open problem.
As an alternative to generative models, discriminative models such as conditional random fields
(CRFs) can directly estimate p(xt |o1:t ) [16]. A number of CRFs and conditional state space models
(CSSMs) have been applied to state estimation [21, 20, 12, 17, 9], typically using a log-linear
representation. More recently, discriminative fine-tuning of generative models with nonlinear neural
network observations [6], as well as direct training of CRFs with neural network factors [7], have
allowed for training of nonlinear discriminative models. However, such models have not been
extensively applied to state estimation. Training CRFs and CSSMs typically requires access to
true state labels, while generative models only require observations, which often makes them more
convenient for physical systems where the true underlying state is unknown. Although CRFs have
also been combined with latent states [18], the difficulty of CRF inference makes latent state CRF
models difficult to train. Prior work has also proposed to optimize SSM parameters with respect to a
discriminative loss [1]. In contrast to this work, our approach incorporates rich sensory observations,
including images, and allows for training of highly expressive discriminative models.
Our method optimizes the state estimator as a deterministic computation graph, analogous to recurrent
neural network (RNN) training. The use of recurrent neural networks (RNNs) for state estimation
has been explored in several prior works [24, 4, 23, 19], but has generally been limited to simple
tasks without complex sensory inputs such as images. Part of the reason for this is the difficulty of
training general-purpose RNNs. Recently, innovative RNN architectures have been successful at
mitigating this problem, through models such as the long short-term memory (LSTM) [10] and the
2

ot?1

ot

ot+1

zt?1

zt

zt+1

xt?1

xt

st?2

ot?1

ot

ot+1

g?

g?

g?

?

zt?1
st?1

yt

?

st?1

xt+1
q

yt?1

zt

yt+1

?yt?1

(a)

zt+1
st

st+1

?
st+1

st
q

q

?yt

?yt+1

(b)

Figure 2: (a) Standard two-step engineering approach for filtering with high-dimensional observations.
The generative part has hidden state xt and two observations, yt and zt , where the latter observation
is actually the output of a second deterministic model zt = g? (ot ), denoted by dashed lines and
trained explicitly to predict zt . (b) Computation graph that jointly optimizes both models in (a),
consisting of the deterministic map g? and a deterministic filter that infers the hidden state given zt .
By viewing the entire model as a single deterministic computation graph, it can be trained end-to-end
using backpropagation as explained in Section 4.

gated recurrent unit (GRU) [5]. LSTMs have been combined with vision for perception tasks such as
activity recognition [3]. However, in the domain of state estimation, such black-box models ignore
the considerable domain knowledge that is available. By drawing a connection between filtering and
recurrent networks, we can design recurrent computation graphs that are particularly well suited to
state estimation and, as shown in our evaluation, can achieve improved performance over standard
LSTM models.

3

Preliminaries

Performing state estimation with a generative model directly using high-dimensional observations
ot , such as camera images, is very difficult, because these observations are typically produced by a
complex and highly nonlinear process. However, in practice, a low-dimensional vector, zt , which can
be extracted from ot , can fully capture the dependence of the observation on the underlying state
of the system. Let xt denote this state, and let yt denote some labeling of the states that we wish
to be able to infer from ot . For example, ot might correspond to pairs of images from a camera
on an automobile, zt to its velocity, and yt to the location of the vehicle. In that case, we can first
train a discriminative model g? (ot ) to predict zt from ot in feedforward manner, and then filter the
predictions to output the desired state labels y1:t . For example, a Kalman filter with hidden state
xt could be trained to use the predicted zt as observations, and then perform inference over xt and
yt at test time. This standard approach for state estimation with high-dimensional observations is
illustrated in Figure 2a.
While this method may be viewed as an engineering solution without a probabilistic interpretation, it
has the advantage that g? (ot ) is trained discriminatively, and the entire model is conditioned on ot ,
with xt acting as an internal latent variable. This is why the model does not need to represent the
distribution over observations explicitly. However, the function g? (ot ) that maps the raw observations
ot to low-dimensional predictions zt is not trained for optimal state estimation. Instead, it is trained
to predict an intermediate variable zt that can be readily integrated into the generative filter.

4

Discriminative Deterministic State Estimation

Our contribution is based on a generalized view of state estimation that subsumes the na?ve, piecewisetrained models discussed in the previous section and allows them to be trained end-to-end using
simple and scalable stochastic gradient descent methods. In the na?ve approach, the observation
function g? (ot ) is trained to directly predict zt , since a standard generative filter model does not
provide for a straightforward way to optimize g? (ot ) with respect to the accuracy of the filter on
the labels y1:T . However, the filter can be viewed as a computation graph unrolled through time, as
shown in Figure 2b. In this graph, the filter has an internal state defined by the posterior over xt . For
3

example, in a Kalman filter with Gaussian posteriors, we can represent the internal state with the
tuple st = (?xt , ?xt ). In general, we will use st to refer to the state of any filter. We also augment
this graph with an output function q(st ) = ?yt that outputs the parameters of a distribution over
labels yt . In the case of a Kalman filter, we would simply have q(st ) = (Cy ?xt , Cy ?xt CT
y ), where
the matrix Cy defines a linear observation function from xt to yt .
Viewing the filter as a computation graph in this way, g? (ot ) can be trained discriminatively
on the entire sequence, rather than individually on single time steps. Let l(?yt ) be a loss function on the output distribution of the computation graph, which might, for example, be given by
l(?yt ) = ? log p?yt (yP
t ), where p?yt is the distribution induced by the parameters ?yt , and yt is
the label. Let L(?) = t l(?yt ) be the loss on an entire sequence with respect to ?. Furthermore,
let ?(st , zt+1 ) denote the operation performed by the filter to compute st+1 based on st and zt+1 .
We can compute the gradient of l(?) with respect to the parameters ? by first recursively computing
the gradient of the loss with respect to the filter state st from the back to the front according to the
following recursion:
d?yt?1 dL
dst dL
dL
=
+
,
dst?1
dst d?yt?1
dst?1 dst

(1)

and then applying the chain rule to obtain
?? L(?) =

T
X
dzt dst dL
.
d? dzt dst
t=1

(2)

All of the derivatives in these equations can be obtained from g? (ot ), ?(st?1 , zt ), q(st ), and l(?yt ):
dst
dst
= ?st?1 ?(st?1 , zt ),
= ?zt ?(st?1 , zt ),
dst?1
dzt
dL
dzt
d?yt
= ?? g? (ot ).
= ??yt l(?yt ),
= ?st q(st ),
d?yt
dst
d?

(3)

The parameters ? can be optimized with gradient descent using these gradients. This is an instance of
backpropagation through time (BPTT), a well known algorithm for training recurrent neural networks.
Recognizing this connection between state-space models and recurrent neural networks allows us to
extend this generic filtering architecture and explore the continuum of models between filters with
a discriminatively trained observation model g? (ot ) all the way to fully general recurrent neural
networks. In our experimental evaluation, we use a standard Kalman filter update as ?(st , zt+1 ), but
we use a nonlinear convolutional neural network observation function g? (ot ). We found that this
provides a good trade-off between incorporating domain knowledge and end-to-end learning for the
task of visual tracking and odometry, but other variants of this model could be explored in future
work.

5

Experimental Evaluation

In this section, we compare our deterministic discriminatively trained state estimator with a set
of alternative methods, including simple feedforward convolutional networks, piecewise-trained
Kalman filter, and fully general LSTM models. We evaluate these models on two tasks that require
processing of raw image input: synthetic task of tracking a red disk in the presence of clutter and
severe occlusion; and the KITTI visual odometry task [8].
5.1

State Estimation Models

Our proposed model, which we call the ?backprop Kalman filter? (BKF), is a computation graph
made up of a Kalman filter (KF) and a feedforward convolutional neural network that distills the
observation ot into a low-dimensional signal zt , which serves as the observation for the KF. The
neural network outputs both a point observation zt and an observation covariance matrix Rt . Since
the network is trained together with the filter, it can learn to use the covariance matrix to communicate
the desired degree of uncertainty about the observation, so as to maximize the accuracy of the final
filter prediction.
4

?xt?1

fc

zt

A?xt?1
reshape
diag exp Lt

h4
fc

fc

h3

ReLU

fc

h2

ReLU

ReLU

max_pool

conv

h1

Kalman filter
resp_norm

ReLU

max_pool

conv

ot

resp_norm

Feedforward network

?t
L

Lt LT
t

?0xt + Kt zt ? Cz ?0xt

Rt
0
T
?0xt CT
z Cz ?xt Cz + Rt

T

A?xt?1 A +

Bw QBT
w

?0xt

yt

?0xt

?1


?xt

Loss
PN PT
i=1

1
t=1 2T N


2


(i)
(i) 

Cy ?xt ? yt 
2

Kt
(I ? Kt Cz ) ?0xt

?xt

?xt?1

Figure 3: Illustration of the computation graph for the BKF. The graph is composed of a feedforward
?t
part, which processes the raw images ot and outputs intermediate observations zt and a matrix L
that is used to form a positive definite observation covariance matrix Rt , and a recurrent part that
integrates zt through time to produce filtered state estimates. See Appendix A for details.
We compare the backprop KF to three alternative state estimators: the ?feedforward model?, the
?piecewise KF?, and the ?LSTM model?. The simplest of the models, the feedforward model, does
not consider the temporal structure in the task at all, and consists only of a feedforward convolutional
network that takes in the observations ot and outputs a point estimate y
?t of the label yt . This approach
is viable only if the label information can be directly inferred from ot , such as when tracking an
object. On the other hand, tasks that require long term memory, such as visual odometry, cannot
be solved with a plain feedforward network. The piecewise KF model corresponds to the simple
generative approach described in Section 3, which combines the feedforward network with a Kalman
? t . The
filter that filters the network predictions zt to produce a distribution over the state estimate x
piecewise model is based on the same computation graph as the BKF, but does not optimize the filter
and network together end-to-end, instead training the two pieces separately. The only difference
between the two graphs is that the piecewise KF does not implement the additional pathway for
propagating the uncertainty from the feedforward network into the filter, but instead, the filter needs
to learn to handle the uncertainty in zt independently. An example instantiation of BKF is depicted
in Figure 3. A detailed overview of the computational blocks shown in the figure is deferred to
Appendix A.
Finally, we compare to a recurrent neural network based on LSTM hidden units [10]. This model
resembles the backprop KF, except that the filter portion of the graph is replaced with a generic
LSTM layer. The LSTM model learns the dynamics from data, without incorporating the domain
knowledge present in the KF.
5.2

Neural Network Design

A special aspect of our network design is a novel response normalization layer that is applied to the
convolutional activations before applying the nonlinearity. The response normalization transforms
the activations such that the activations of layer i have always mean ?i and variance ?i2 regardless
of the input to the layer. The parameters ?i and ?i2 are learned along with other parameters. This
normalization is used in all of the convolutional networks in our evaluation, and resembles batch
normalization [11] in its behavior. However, we found this approach to be substantially more effective
for recurrent models that require backpropagation through time, compared to the more standard
batch normalization approach, which is known to require additional care when applied to recurrent
networks. It has been since proposed independently from our work in [2], which gives an in-depth
analysis of the method. The normalization is followed by a rectified linear unit (ReLU) and a max
pooling layer.
5.3

Synthetic Visual State Estimation Task

Our state estimation task is meant to reflect some of the typical challenges in visual state estimation:
the need for long-term tracking to handle occlusions, the presence of noise, and the need to process
raw pixel data. The task requires tracking a red disk from image observations, as shown in Figure
4. Distractor disks with random colors and radii are added into the scene to occlude the red disk,
and the trajectories of all disks follow linear-Gaussian dynamics, with a linear spring force that pulls
the disks toward the center of the frame and a drag force that prevents high velocities. The disks
can temporally leave the frame since contacts are not modeled. Gaussian noise is added to perturb
the motion. While these model parameters are assumed to be known in the design of the filter, it is
a straightforward to learn also the model parameters. The difficulty of the task can be adjusted by
increasing or decreasing the number of distractor disks, which affects the frequency of occlusions.
5

Figure 4: Illustration of six consecutive frames of two training sequences. The objective is to track
the red disk (circled in the the first frame for illustrative purposes) throughout the 100-frame sequence.
The distractor disks are sampled for each sequence at random and overlaid on top of the target disk.
The upper row illustrates an easy sequence (9 distractors), while the bottom row is a sequence of high
difficulty (99 distractors). Note that the target is very rarely visible in the hardest sequences.
Table 1: Benchmark Results
State Estimation Model # Parameters RMS test error ??
feedforward model
piecewise KF
LSTM model (64 units)
LSTM model (128 units)
BKF (ours)

0.2322 ? 0.1316
0.1160 ? 0.0330
0.1407 ? 0.1154
0.1423 ? 0.1352
0.0537 ? 0.1235

7394
7397
33506
92450
7493

The easiest variants of the task are solvable with a feedforward estimator, while the hardest variants
require long-term tracking through occlusion. To emphasize the sample efficiency of the models, we
trained them using 100 randomly sampled sequences.
The results in Table 1 show that the BKF outperforms both the standard probabilistic KF-based
estimators and the more powerful and expressive LSTM estimators. The tracking error of the simple
feedforward model is significantly larger due to the occlusions, and the model tends to predict the
mean coordinates when the target is occluded. The piecewise model performs better, but because
the observation covariance is not conditioned on ot , the Kalman filter learns to use a very large
observation covariance, which forces it to rely almost entirely on the dynamics model for predictions.
On the other hand, since the BKF learns to output the observation covariances conditioned on ot that
optimize the performance of the filter, it is able to find a compromise between the observations and
the dynamics model. Finally, although the LSTM model is the most general, it performs worse than
the BKF, since it does not incorporate prior knowledge about the structure of the state estimation
problem.

6

feedforward
piecewise
LSTM

10 0

RMS error

To test the robustness of the estimator to occlusions,
we trained each model on a training set of 1000 sequences of varying amounts of clutter and occlusions.
We then evaluated the models on several test sets,
each corresponding to a different level of occlusion
and clutter. The tracking error as the test set difficulty
is varied is shown Figure 5. Note that even in the
absence of distractors, BKF and LSTM models outperform the feedforward model, since the target occasionally leaves the field of view. The performance
of the piecewise KF does not change significantly as
the difficulty increases: due to the high amount of
clutter during training, the piecewise KF learns to
use a large observation covariance and rely primarily
on feedforward estimates for prediction. The BKF
achieves the lowest error in nearly all cases. At the
same time, the BKF also has dramatically fewer parameters than the LSTM models, since the transitions
correspond to simple Kalman filter updates.

10 -1

10 -2

10 -3

0

20

40

60

80

100

# distractors

Figure 5: The RMS error of various models
trained on a single training set that contained
sequences of varying difficulty. The models
were then evaluated on several test sets of
fixed difficulty.

Figure 6: Example image sequence from the KITTI dataset (top row) and the corresponding difference
image that is obtained by subtracting the RGB values of the previous image from the current image
(bottom row). The observation ot is formed by concatenating the two images into a six-channel
feature map which is then treated as an input to a convolutional neural network. The figure shows
every fifth sample from the original sequence for illustrative purpose.
Table 2: KITTI Visual Odometry Results
Test 100
# training trajectories
Translational Error [m/m]
piecewise KF
LSTM model (128 units)
LSTM model (256 units)
BKF (ours)
Rotational Error [deg/m]
piecewise KF
LSTM model (128 units)
LSTM model (256 units)
BKF (ours)

5.4

Test 100/200/400/800

3

6

10

3

6

10

0.3257
0.5022
0.5199
0.3089

0.2452
0.3456
0.3172
0.2346

0.2265
0.2769
0.2630
0.2062

0.3277
0.5491
0.5439
0.2982

0.2313
0.4732
0.4506
0.2031

0.2197
0.4352
0.4228
0.1804

0.1408
0.5484
0.4960
0.1207

0.1028
0.3681
0.3391
0.0901

0.0978
0.3767
0.2933
0.0801

0.1069
0.4123
0.3845
0.0888

0.0768
0. 3573
0.3566
0.0587

0.0754
0.3530
0.3221
0.0556

KITTI Visual Odometry Experiment

Next, we evaluated the state estimation models on visual odometry task in the KITTI dataset [8]
(Figure 6, top row). The publicly available training set contains 11 trajectories of ego-centric video
sequences of a passenger car driving in suburban scenes, along with ground truth position and
orientation. The dataset is challenging since it is relatively small for learning-based algorithms, and
the trajectories are visually very diverse. For training the Kalman filter variants, we used a simplified
state-space model with three of the state variables corresponding to the vehicle?s 2D pose (two spatial
coordinates and heading) and two for the forward and angular velocities. Because the dynamics
model is non-linear, we equipped our model-based state estimators with extended Kalman filters,
which is a straightforward addition to the BKF framework.
The objective of the task is to estimate the relative change in the pose during fixed-length subsequences. However, because inferring the pose requires integration over all past observations, a simple
feedforward model cannot be used directly. Instead, we trained a feedforward network, consisting of
four convolutional and two fully connected layers and having approximately half a million parameters,
to estimate the velocities from pairs of images at consecutive time steps. In practice, we found it better
to use a difference image, corresponding to the change in the pixel intensities between the images,
along with the current image as an input to the feedforward network (Figure 6). The ground truth
velocities, which were used to train the piecewise KF as well as to pretrain the other models, were
computed by finite differencing from the ground truth positions. The recurrent models?piecewise
KF, the BKF, and the LSTM model?were then fine-tuned to predict the vehicle?s pose. Additionally,
for the LSTM model, we found it crucial to pretrain the recurrent layer to predict the pose from the
velocities before fine-tuning.
We evaluated each model using 11-fold cross-validation, and we report the average errors of the
held-out trajectories over the folds. We trained the models by randomly sampling subsequences of
100 time steps. For each fold, we constructed two test sets using the held-out trajectory: the first set
contains all possible subsequences of 100 time steps, and the second all subsequences of lengths
100, 200, 400, and 800.1 We repeated each experiment using 3, 6, or all 10 of the sequences in each
training fold to evaluate the resilience of each method to overfitting.
1
The second test set aims to mimic the official (publicly unavailable) test protocol. Note, however, that
because the methods are not tested on the same sequences as the official test set, they are not directly comparable
to results on the official KITTI benchmark.

7

Table 2 lists the cross-validation results. As expected, the error decreases consistently as the number
of training sequences becomes larger. In each case, BKF outperforms the other variants in both
predicting the position and heading of the vehicle. Because both the piecewise KF and the BKF
incorporate domain knowledge, they are more data-efficient. Indeed, the performance of the LSTM
degrades faster as the number of training sequences is decreased. Although the models were trained
on subsequences of 100 time steps, they were also tested on a set containing a mixture of different
sequence lengths. The LSTM model generally failed to generalize to longer sequences, while the
Kalman filter variants perform slightly better on mixed sequence lengths.

6

Discussion

In this paper, we proposed a discriminative approach to state estimation that consists of reformulating
probabilistic generative state estimation as a deterministic computation graph. This makes it possible
to train our method end-to-end using simple backpropagation through time (BPTT) methods, analogously to a recurrent neural network. In our evaluation, we present an instance of this approach that
we refer to as the backprop KF (BKF), which corresponds to a (extended) Kalman filter combined
with a feedforward convolutional neural network that processes raw image observations. Our approach to state estimation has two key benefits. First, we avoid the need to construct generative state
space models over complex, high-dimensional observation spaces such as raw images. Second, by
reformulating the probabilistic state-estimator as a deterministic computation graph, we can apply
simple and effective backpropagation and stochastic gradient descent optimization methods to learn
the model parameters. This avoids the usual challenges associated with inference in continuous,
nonlinear conditional probabilistic models, while still preserving the same representational power as
the corresponding approximate probabilistic inference method, which in our experiments corresponds
to approximate Gaussian posteriors in a Kalman filter.
Our approach also can be viewed as an application of ideas from probabilistic state-space models to
the design of recurrent neural networks. Since we optimize the state estimator as a deterministic computation graph, it corresponds to a particular type of deterministic neural network model. However,
the architecture of this neural network is informed by principled and well-motivated probabilistic
filtering models, which provides us with a natural avenue for incorporating domain knowledge into
the system.
Our experimental results indicate that end-to-end training of a discriminative state estimators can
improve their performance substantially when compared to a standard piecewise approach, where
a discriminative model is trained to process the raw observations and produce intermediate lowdimensional observations that can then be integrated into a standard generative filter. The results also
indicate that, although the accuracy of the BKF can be matched by a recurrent LSTM network with
a large number of hidden units, BKF outperforms the general-purpose LSTM when the dataset is
limited in size. This is due to the fact that BKF incorporates domain knowledge about the structure of
probabilistic filters into the network architecture, providing it with a better inductive bias when the
training data is limited, which is the case in many real-world robotic applications.
In our experiments, we primarily focused on models based on the Kalman filter. However, our
approach to state estimation can equally well be applied to other probabilistic filters for which the
update equations (approximate or exact) can be written in closed form, including the information
filter, the unscented Kalman filter, and the particle filter, as well as deterministic filters such as state
observers or moving average processes. As long as the filter can be expressed as a differentiable mapping from the observation and previous state to the new state, we can construct and differentiate the
corresponding computation graph. An interesting direction for future work is to extend discriminative
state-estimators with complex nonlinear dynamics and larger latent state. For example, one could
explore the continuum of models that span the space between simple KF-style state estimators and
fully general recurrent networks. The trade-off between these two extremes is between generality and
domain knowledge, and striking the right balance for a given problem could produce substantially
improved results even with relative modest amounts of training data.
Acknowledgments
This research was funded in part by ONR through a Young Investigator Program award, by the Army
Research Office through the MAST program, and by the Berkeley DeepDrive Center.
8

References
[1] P. Abbeel, A. Coates, M. Montemerlo, A. Y. Ng, and S. Thrun. Discriminative training of kalman filters.
In Robotics: Science and Systems (R:SS), 2005.
[2] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.
[3] M. Baccouche, F. Mamalet, C. Wolf, C. Garcia, and A. Baskurt. Sequential deep learning for human
action recognition. In Second International Conference on Human Behavior Unterstanding, pages 29?39,
Berlin, Heidelberg, 2011. Springer-Verlag.
[4] O. Bobrowski, R. Meir, S. Shoham, and Y. C. Eldar. A neural network implementing optimal state
estimation based on dynamic spike train decoding. In Advances in Neural Information Processing Systems
(NIPS), 2007.
[5] K. Cho, B. van Merrienboer, C. Gulcehre, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078,
2014.
[6] G. E. Dahl, D. Yu, L. Deng, and A. Acero. Context-dependent pre-trained deep neural networks for
large-vocabulary speech recognition. Audio, Speech, and Language Processing, IEEE Transactions on,
20(1):30?42, 2012.
[7] T. Do, T. Arti, et al. Neural conditional random fields. In International Conference on Artificial Intelligence
and Statistics, pages 177?184, 2010.
[8] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets robotics: The KITTI dataset. International
Journal of Robotics Research (IJRR), 2013.
[9] R. Hess and A. Fern. Discriminatively trained particle filters for complex multi-object tracking. In
Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 240?247. IEEE,
2009.
[10] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735?1780, 1997.
[11] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal
covariate shift. In International Conference on Machine Learning (ICML), 2015.
[12] M. Kim and V. Pavlovic. Conditional state space models for discriminative motion estimation. In Computer
Vision, 2007. ICCV 2007. IEEE 11th International Conference on, pages 1?8. IEEE, 2007.
[13] D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,
2014.
[14] J. Ko and D. Fox. GP-BayesFilters: Bayesian filtering using Gaussian process prediction and observation
models. Autonomous Robots, 27(1):75?90, 2009.
[15] R. G. Krishnan, U. Shalit, and D. Sontag. Deep kalman filters. arXiv preprint arXiv:1511.05121, 2015.
[16] J. Lafferty, A. McCallum, and F. C. Pereira. Conditional random fields: Probabilistic models for segmenting
and labeling sequence data. 2001.
[17] B. Limketkai, D. Fox, and L. Liao. CRF-filters: Discriminative particle filters for sequential state
estimation. In International Conference on Robotics and Automation (ICRA), 2007.
[18] L.-P. Morency, A. Quattoni, and T. Darrell. Latent-dynamic discriminative models for continuous gesture
recognition. In Computer Vision and Pattern Recognition, 2007. CVPR?07. IEEE Conference on, pages
1?8. IEEE, 2007.
[19] P. Ondruska and I. Posner. Deep tracking: Seeing beyond seeing using recurrent neural networks. arXiv
preprint arXiv:1602.00991, 2016.
[20] D. A. Ross, S. Osindero, and R. S. Zemel. Combining discriminative features to infer complex trajectories.
In Proceedings of the 23rd international conference on Machine learning, pages 761?768. ACM, 2006.
[21] C. Sminchisescu, A. Kanaujia, Z. Li, and D. Metaxas. Discriminative density propagation for 3d human
motion estimation. In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer
Society Conference on, volume 1, pages 390?397. IEEE, 2005.
[22] S. Thrun, W. Burgard, and D. Fox. Probabilistic Robotics. The MIT Press, 2005.
[23] R. Wilson and L. Finkel. A neural implementation of the kalman filter. In Advances in neural information
processing systems, pages 2062?2070, 2009.
[24] N. Yadaiah and G. Sowmya. Neural network based state estimation of dynamical systems. In International
Joint Conference on Neural Networks (IJCNN), 2006.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 1499-vlsi-implementation-of-motion-centroid-localization-for-autonomous-navigation.pdf

VLSI Implementation of Motion Centroid
Localization for Autonomous Navigation

Ralph Etienne-Cummings
Dept. of ECE,
Johns Hopkins University,
Baltimore, MD

Viktor Gruev
Dept. of ECE,
Johns Hopkins University,
Baltimore, MD

Mohammed Abdel Ghani
Dept. ofEE,
S. Illinois University,
Carbondale, IL

Abstract
A circuit for fast, compact and low-power focal-plane motion centroid
localization is presented. This chip, which uses mixed signal CMOS
components to implement photodetection, edge detection, ON-set
detection and centroid localization, models the retina and superior
colliculus. The centroid localization circuit uses time-windowed
asynchronously triggered row and column address events and two
linear resistive grids to provide the analog coordinates of the motion
centroid. This VLSI chip is used to realize fast lightweight
autonavigating vehicles. The obstacle avoiding line-following
algorithm is discussed.

1 INTRODUCTION
Many neuromorphic chips which mimic the analog and parallel characteristics of visual,
auditory and cortical neural circuits have been designed [Mead, 1989; Koch, 1995] .
Recently researchers have started to combine digital circuits with neuromorphic aVLSI
systems [Boahen, 1996]. The persistent doctrine, however, has been that computation
should be performed in analog, and only communication should use digital circuits. We
have argued that hybrid computational systems are better equipped to handle the high
speed processing required for real-world problem solving , while maintaining
compatibility with the ubiquitous digital computer [Etienne, 1998]. As a further
illustration of this point of view, this paper presents a departure form traditional
approaches for focal plane centroid localization by offering a mixed signal solution that is
simultaneously high-speed, low power and compact. In addition, the chip is interfaced
with an 8-bit microcomputer to implement fast autonomous navigation.
Implementation of centroid localization has been either completely analog or completely
digital. The analog implementations, realized in the early 1990s, used focal plane current
mode circuits to find a global continuos time centroid of the pixels' intensities
[DeWeerth, 1992]. Due to their sub-threshold operation, these circuits are low power,
but slow. On the other hand, the digital solutions do not compute the centroid at the focal

R. Etienne-Cummings, V. Grnev and M A. Ghani

686

plane. They use standard CCO cameras, AID converters and OSP/CPU to compute the
intensity centroid [Mansfield, 1996]. These software approaches offer multiple centroid
localization with complex mathematical processing. However, they suffer from the usual
high power consumption and non-scalability of traditional digital visual processing
systems. Our approach is novel in many aspects. We benefit from the low power,
compactness and parallel organization of focal plane analog circuits and the speed,
robustness and standard architecture of asynchronous digital circuits. Furthermore, it
uses event triggered analog address read-out, which is ideal for the visual centroid
localization problem. Moreover, our chip responds to moving targets only by using the
ON-set of each pixel in the centroid computation. Lastly, our chip models the retina and
two dimensional saccade motor error maps of superior colliculus on a single chip
[Sparks, 1990] . Subsequently, this chip is interfaced with a IlC for autonomous obstacle
avoidance during line-following navigation. The line-following task is similar to target
tracking using the saccadic system, except that the "eye" is fixed and the "head" (the
vehicle) moves to maintain fixation on the target. Control signals provided to the vehicle
based on decisions made by the IlC are used for steering and accelerating/braking. Here
the computational flexibility and programmability of the IlC allows rapid prototyping of
complex and robust algorithms.

2 CENTROID LOCALIZATION
The mathematical computation of the centroid of an object on the focal plane uses
intensity weighted average of the position of the pixels forming the object [OeWeerth,
1992] . Equation (1) shows this formulation. The implementation of this representation
N

N

LI,x,
x=

j =1

N

L,I,

LI,y;
and

y=

;=1
N

(1)

L I,

,=1

1=1

can be quite involved since a product between the intensity and position is implied. To
eliminate this requirement, the intensity of the pixels can be normalized to a single value
within the object. This gives equation (2) since the intensity can be factored out of the
summations. Normalization of the intensity using a simple threshold is not advised since

Ix,

x=~

N

Iy,

and

y=~

N

(2)

Intensity Image

XI

X l+l

X 1+2

X I +3

x 1+4

Edges from pixels

Figure 1: Centroid computation
architecture.

Figure 2: Centroid computation method.

the value of the threshold is dependent on the brightness of the image and number of
pixels forming the object may be altered by the thresholding process. To circumvent
these problems, we take the view that the centroid of the object is defined in relation to its
boundaries. This implies that edge detection (second order spatial derivative of intensity)
can be used to highlight the boundaries, and edge labeling (the zero-crossing of the
edges) can be used to normalize the magnitude of the edges. Subsequently, the centroid

VLSI Implementation ofMotion Centroid Localization for Autonomous Navigation

687

of the zero-crossings is computed. Equation (2) is then realized by projecting the zerocrossing image onto the x- and y-axis and performing two linear centroid determinations.
Figure (1) shows this process.
The determination of the centroid is computed using a resistance grid to associate the
position of a column (row) with a voltage. In figure 2, the positions are given by the
voltages Vi . By activating the column (row) switch when a pixel of the edge image
appears in that column (row), the position voltage is connected to the output line through
the switch impedance, Rs. As more switches are activated, the voltage on the output line
approximates equation (2). Clearly, since no buffers are used to isolate the position
voltages, as more switches are activated, the position voltages will also change. This
does not pose a problem since the switch resistors are design to be larger than the position
resistors (the switch currents are small compared to the grid current). Equation (3) gives
the error between the ideal centroid and the switch loaded centroid in the worst case
when Rs

= on.

In the equation, N is the number of nodes, M is the number of switches

set and Xl and xM are the locations of the first and last set switches, respectively. This
error is improved as Rs gets larger, and vanishes as N

(M~N)

approaches infinity . The

terms Xi represent an ascending ordered list of the activated switches; x I may correspond
to column five, for example. This circuit is compact since it uses only a simple linear
resistive grid and MOS switches. It is low power because the total grid resistance, N x R,
can be large. It can be fast when the parasitic capacitors are kept small . It provides an
analog position value, but it is triggered by fast digital signals that activate the switches.
error = VmOll -Vmin
M(N + 1)

1

~[ X
?...J
.=1

?

xCN+l)
_ _--'-1-'--_-'--_
N + 1 + XI - Xm

(3)

3 MODELING THE RETINA AND SUPERIOR COLLICULUS
3.1 System Overview
The centroid computation approach presented in section 2 is used to isolate the location
of moving targets on a 20 focal plane array. Consequently, a chip which realizes a
neuromorphic visual target acquisition system based on the saccadic generation
mechanism of primates can be implemented. The biological saccade generation process is
mediated by the superior colliculus, which contains a map of the visual field [Sparks,
1990}. In laboratory experiments, cellular recordings suggest that the superior colliculus
provides the spatial location of targets to be foveated. Clearly, a great deal of neural
circuitry exists between the superior colliculus and the eye muscle. Horiuchi has built an
analog system which replicates most of the neural circuits (including the motor system)
which are believed to form the saccadic system [Horiuchi, 1996]. Staying true to the
anatomy forced his implementation to be a complex multi-chip system with many control
parameters. On the other hand, our approach focuses on realizing a compact single chip
solution by only mimicking the behavior of the saccadic system, but not its structure.

3.2 Hardware Implementation
Our approach uses a combination of analog and digital circuits to implement the
functions of the retina and superior colliculus at the focal plane. We use simple digital
control ideas, such as pulse-width modulation and stepper motors, to position the "eye".
The retina portion of this chip uses photodiodes, logarithmic compression, edge detection
and zero-crossing circuits. These circuits mimic the first three layers of cells in the retina

688

R. Etienne-Cummings, V. Grnev and M. A. Ghani

with mixed sub-threshold and strong inversion circuits. The edge detection circuit is
realized with an approximation of the Laplacian operator implemented using the
difference between a smooth (with a resistive grid) and unsmoothed version of the image
[Mead, 1989]. The high gain of the difference circuit creates a binary image of
approximate zero-crossings. After this point, the computation is performed using mixed
analog/digital circuits. The zero-crossings are fed to ON-set detectors (positive temporal
derivatives) which signal the location of moving or flashing targets. These circuits model
the behavior of some of the amacrine and ganglion cells of the primate retina [Barlow,
1982]. These first layers of processing constitute all the "direct" mimicry of the
biological models. Figure 3 shows the schematic of these early processing layers.
The ON-set detectors provide inputs to the model of the superior colliculus circuits. The
ON-set detectors allow us to segment moving targets against textured backgrounds. This
is an improvement on earlier centroid and saccade chips that used pixel intensity. The
essence of the superior colliculus map is to locate the target that is to be foveated. In our
case, the target chosen to be foveated will be moving. Here motion is define simply as
the change in contrast over time. Motion, in this sense, can be seen as being the earliest
measurable attribute of the target which can trigger a saccade without requiring any high
level decision making. Subsequently, the coordinates of the motion must be extracted and
provided to the motor drivers.

X M otion Cenuold

~!
~A
!
Edge Detc:ctlon
ON-set Detecu on

Figure 3: Schematic of
the model of the retina.

Figure 4: Schematic of the model of the superior
collicu Ius.

The circuits for locating the target are implemented entirely with mixed signal, nonneuromorphic circuits. The theoretical foundation for our approach is presented in
section 2. The ON-set detector is triggered when an edge of the target appears at a pixel.
At this time, the pixel broadcasts its location to the edge of the array by activating row
and column lines. This row (column) signal sets a latch at the right (top) of the array. The
latches asynchronously activate switches and the centroid of the activated positions is
provided. The latches remain set until they are cleared by an external control signal. This
control signal provides a time-window over which the centroid output is integrated. This
has the effect of reducing noise by combining the outputs of pixels which are activated at
different instances even if they are triggered by the same motion (an artifact of small fill
factor focal plane image processing). Furthermore, the latches can be masked from the
pixels' output with a second control signal. This signal is used to de-activate the centroid

689

VLSI Implementation of Motion Centroid Localization for Autonomous Navigation

circuit during a saccade (saccadic suppression). A centroid valid signal is also generated
by the chip. Figure 4 shows a portion of the schematic of the superior colliculus model.

3.3 Results
In contrast to previous work, this chip provides the 2-D coordinates of the centroid of a
moving target. Figure 5 shows the oscilloscope trace of the coordinates as a target moves
back and forth, in and out of the chip's field of view. The y-coordinate does change
while the x-coordinate increases and decreases as the target moves to the left and right,
respectively. The chip has been used to track targets in 2-D by making micro-saccades .
In this case, the chip chases the target as it attempts to escape from the center. The eye
movement is performed by converting the analog coordinates into PWM signals, which
are used to drive stepper motors. The system performance is limited by the contrast
sensitivity of the edge detection circuit, and the frequency response of the edge (high
frequency cut-off) and ON-set (low frequency cut-off) detectors. With the appropriate
optics, it can track walking or running persons under indoor or outdoor lighting
conditions at close or far distances. Table I gives a summary of the chip characteristics.
VarYing x?
l'Oordma te

Figure 5: Oscilloscope trace of 20
centroid for a moving target.

Technology

1.2um ORBIT

Chip Size
Array Size

4mm 1
12 x 10

Pixel Size
Fill Factor

llOxllOutn
11%

Intensity
Min . Contrast

0.lu-1OOmW/cm 2

Response Time
Power (chip)

2-10 6 Hz(@1 mW/cml)

10%
5 mW (@l m W/cm~ Vdd

=6V)

Table I: Chip characteristics.

4 APPLICATION: OBSTACLE AVOIDANCE DURING LINEFOLLOWING AUTONA VIGATION
4.1 System Overview
The frenzy of activity towards developing neuromorphic systems over the pass 10 years
has been mainly driven by the promise that one day engineers will develop machines that
can interact with the environment in a similar way as biological organisms. The prospect
of having a robot that can help humans in their daily tasks has been a dream of science
fiction for many decades. As can be expected, the key to success is premised on the
development of compact systems, with large computational capabilities, at low cost (in
terms of hardware and power) . Neuromorphic VLSI systems have closed the gap between
dreams and reality, but we are still very far from the android robot. For all these robots,
autonomous behavior, in the form of auto-navigation in natural environments, must be
one of their primary skills. For miniaturization, neuromorphic vision systems performing
most of the pre-processing, can be coupled with small fast computers to realize these
compact yet powerful sensor/processor modules.

4.2 Navigation Algorithm
The simplest form of data driven auto-navigation is the line-following task. In this task,
the robot must maintain a certain relationship with some visual cues that guide its motion.
In the case of the line-follower, the visual system provides data regarding the state of the

R. Etienne-Cummings, V. Gruev and M A. Ghani

690

line relative to the vehicle, which results in controlling steering and/or speed. If obstacle
avoidance is also required, auto-navigation is considerably more difficult. Our system
handles line-following and obstacle avoidance by using two neuromorphic visual sensors
that provide information to a micro-controller OlC) to steer, accelerate or decelerate the
vehicle. The sensors, which uses the centroid location system outlined above, provides
information on the position of the line and obstacles to the p,C, which provides PWM
signals to the servos for controlling the vehicle. The algorithm implemented in the p,C
places the two sensors in competition with each other to force the line into a blind zone
between the sensors. Simultaneously, if an object enters the visual field from outside, it
is treated as an obstacle and the p,C turns the car away from the object. Obstacle
avoidance is given higher priority than line-following to avoid collisions. The p,C also
keeps track of the direction of avoidance such that the vehicle can be re-oriented towards
the line after the obstacle is pushed out of the field of view. Lastly, for line following,
the position, orientation and velocity of drift, determined from the temporal derivative of
the centroid, are used to track the line. The control strategy is to keep the line in the blind
zone, while slowing down at corners, speeding up on straight aways and avoiding
obstacles. The angle which the line or obstacle form with the x-axis also affects the
speed. The value of the x-centroid relative to the y-centroid provides rudimentary
estimate of the orientation of the line or obstacle to the vehicle. For example, angles less

.

Follow

"

AV~8id~nce
...
,

0I0s1ade

L Zone

/

!

~ne

i

'.)", /

:

AV~na;ce

!

~\?': ~../

;i:~~~~???. .\j ~

\;... ?? ? ?~i=~s..s

Figure 6: Block diagram of the
autonomous line-follower system.

Figure 7: A picture of the vehicle.

(greater) than +/- 45 degrees tend to have small (large) x-coordinates and large (small) ycoordinates and require deceleration (acceleration). Figure 6 shows the organization of
the sensors on the vehicle and control spatial zones. Figure 7 shows the vehicle and
samples of the line and obstacles.
4.3 Hardware Implementation
The coordinates from the centroid localization circuits are presented to the p,C for
analysis. The p,C used is the Microchip PIC16C74. This chip is chosen because of its
five NO inputs and three PWM outputs. The analog coordinates are presented directly to
the NO inputs. Two of the PWM outputs are connected to the steering and speed control
servos. The PIC16C74 runs at 20 MHz and has 35 instructions, 4K by 8-b ROM and 80
by 20-b RAM. The program which runs on the PIC determines the control action to take,
based on the signal provided by the neuromorphic visual sensors. The vehicle used is a
four-wheel drive radio controlled model car (the radio receiver is disconnected) with
Digital Proportional Steering (DPS) .

VLSI Implementation ofMotion Centroid Localization for Autonomous Navigation

691

4.4 Results
The vehicle was tested on a track composed of black tape on a gray linoleum floor with
black and white obstacles. The track formed a closed loop with two sharp turns and some
smooth S-curves. The neuromorphic vision chip was equipped with a 12.5 mm variable
iris lens, which limited its field of view to about 100. Despite the narrow field of view ,
the car was able to navigate the track at an average speed of 1 mls without making any
errors. On less curvy parts of the track, it accelerated to about 2 mls and slowed down at
the corners. When the speed of the vehicle is scaled up, the errors made are mainly due
to over steering.

5 CONCLUSION
A 2D model of the saccade generating components of the superior colliculus is presented .
This model only mimics the functionality the saccadic system using mixed signal focal
plane circuits that realize motion centroid localization. The single chip combines a
silicon retina with the superior colliculus model using compact, low power and fast
circuits. Finally, the centroid chip is interfaced with an 8-bit IlC and vehicle for fast linefollowing auto navigation with obstacle avoidance. Here all of the required computation is
performed at the visual sensor, and a standard IlC is the high-level decision maker.

References
Barlow H., The Senses: Physiology of the Retina, Cambridge University Press,
Cambridge, England, 1982.
Boahen K., "Retinomorphic Vision Systems II: Communication Channel Design," ISCAS
96, Atlanta, GA, 1996.
DeWeerth, S. P., "Analog VLSI Circuits for Stimulus Localization and Centroid
Computation," Int'l 1. Computer Vision, Vol. 8, No.2, pp. 191-202, 1992.
Etienne-Cummings R., J Van der Spiegel and P. Mueller, "Neuromorphic and Digital
Hybrid Systems," Neuromorphic Systems: Engineering Silicon from Neurobiology, L.
Smith and A. Hamilton (Eds.), World Scientific, 1998.
Horiuchi T., T. Morris, C . Koch and S. P . DeWeerth, "Analog VLSI Circuits for
Attention-Based Visual Tracking," Advances in Neural Information Processing
Systems, Vol. 9, Denver, CO, 1996.
Koch C. and H. Li (Eds.), Vision Chips: Implementing Vision Algorithms with Analog
VLSI Circuits, IEEE Computer Press, 1995.
Mansfield, P., "Machine Vision Tackles Star Tracking," Laser Focus World, Vol. 30, No.
26, pp. S21 -S24, 1996.
Mead C. and M. Ismail (Eds.), Analog VLSI Implementation of Neural Networks, Kluwer
Academic Press, Newell, MA, 1989.
Sparks D., C. Lee and W. Rohrer, "Population Coding of the Direction, Amplitude and
Velocity of Saccadic Eye Movements by Neurons in the Superior Colliculus," Proc.
Cold Spring Harbor Symp. Quantitative Biology, Vol. LV, 1990.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 52-teaching-artificial-neural-systems-to-drive-manual-training-techniques-for-autonomous-systems.pdf

693

Teaching Artificial Neural Systems to Drive:
Manual Training Techniques for Autonomous Systems

J. F. Shepanski and S. A. Macy

TRW, Inc .
One Space Park, 02/1779
Redondo Beach, CA 90278

Abetract
We have developed a methodology for manually training autononlous control systems
based on artificial neural systems (ANS). In applications where the rule set governing an expert's
decisions is difficult to formulate, ANS can be used to ext.ra.c:t rules by associating the information
an expert receives with the actions h~ takes . Properly constructed networks imitate rules of
behavior that permits them to function autonomously when they are trained on the spanning set
of possible situations. This training can be provided manually, either under the direct. supervision
or a system trainer, or indirectly using a background mode where the network assimilates training
data as the expert perrorms his day-to-day tasks. To demonstrate these methods we have trained
an ANS network to drive a vehicle through simulated rreeway traffic.

I ntJooducticn
Computational systems employing fine grained parallelism are revolutionizing the way we
approach a number or long standing problems involving pattern recognition and cognitive processing. The field spans a wide variety or computational networks, rrom constructs emulating neural
runctions, to more crystalline configurations that resemble systolic arrays. Several titles are used
to describe this broad area or research, we use the term artificial neural systems (ANS). Our concern in this work is the use or ANS ror manually training certain types or autonomous systems
where the desired rules of behavior are difficult to rormulate.
Artificial neural systems consist of a number or processing elements interconnected in a
weighted, user-specified fashion, the interconnection weights acting as memory ror the system.
Each processing element calculatE',> an output value based on the weighted sum or its inputs. In
addition, the input data is correlated with the output or desired output (specified by an instructive
agent) in a training rule that is used to adjust the interconnection weights. In this way the ne~
work learns patterns or imitates rules of behavior and decision making.
The partiCUlar ANS architecture we use is a variation of Rummelhart et. al. [lJ multi-layer
perceptron employing the generalized delta rule (GD R). Instead of a single, multi-layer ,structure, our final network has a a multiple component or "block" configuration where one blOt'k'~
output reeds into another (see Figure 3). The training methodology we have developed is not
tied to a particular training rule or architecture and should work well with alternative networks
like Grossberg's adaptive resonance model[2J.

? American Institute of Physics 1988

694

The equations describing the network are derived and described in detail by Rumelhart et.
al.[l]. In summary, they are:
Transfer function:

Sj =

?

E WjiOi;

(1)

i-O

Weight adaptation rule:
Error calculation:

Awl'?? =( 1- a l'..)n., l'??0 J?0??

OJ

+ a l'??Awp.revious
.'
l'

'"

=0j{1- OJ) E0.tW.ti,

( 2)

( 3)

.t=1

where OJ is the output or processing element j or a sensor input, wi is the interconnection weight
leading from element ito i, n is the number of inputs to j, Aw is the adjustment of w, '1 is the
training constant, a is the training "momentum," OJ is the calculated error for element i, and m
is the Canout oC a given element. Element zero is a constant input, equal to one, so that. WjO is
equivalent to the bias threshold of element j. The (1- a) factor in equation (2) differs from standard GDR formulation, but. it is useful for keeping track of the relative magnitudes of the two
terms. For the network's output layer the summation in equation (3) is replaced with the
difference between the desired and actual output value of element j.
These networks are usually trained by presenting the system with sets of input/output data
vectors in cyclic fashion, the entire cycle of database presentation repeated dozens of times . This
method is effective when the training agent is a computer operating in batch mode, but would be
intolerable for a human instructor. There are two developments that will help real-time human
training. The first is a more efficient incorporation of data/response patterns into a network. The
second, which we are addressing in this paper, is a suitable environment wherein a man and ANS
network can interact in training situation with minimum inconvenience or boredom on the
human's part. The ability to systematically train networks in this fashion is extremely useful for
developing certain types of expert systems including automatic signal processors, autopilots,
robots and other autonomous machines. We report a number of techniques aimed at facilitating
this type of training, and we propose a general method for teaching these networks .
System. Development

Our work focuses on the utility of ANS for system control. It began as an application of
Barto and Sutton's associative search network[3]. Although their approach was useful in a
number of ways, it fell short when we tried to use it for capturing the subtleties of human
decision-making. In response we shifted our emphasis rrom constructing goal runctions for
automatic learning, to methods for training networks using direct human instruction. An integral
part or this is the development or suitable interraces between humans, networks and the outside
world or simulator. In this section we will report various approaches to these ends, and describe a
general methodology for manually teaching ANS networks . To demonstrate these techniques we
taught a network to drive a robot vehicle down a simulated highway in traffic. This application
combines binary decision making and control of continuous parameters.
Initially we investigated the use or automatic learning based on goal functions[3] for training control systems. We trained a network-controlled vehicle to maintain acceptable following
distances from cars ahead or it. On a graphics workstation, a one lane circular track was

695

constructed and occupied by two vehicles: a network-controlled robot car and a pace car that
varied its speed at random .. Input data to the network consisted of the separation distance and
the speed of the robot vehicle . The values of a goal function were translated into desired output
for GDR training. Output controls consisted of three binary decision elements : 1) accelerate one
increment of speed, 2) maintain speed, and 3) decelerate one increment of speed. At all times
the desired output vector had exactly one of these three elements active . The goal runction was
quadratic with a minimum corresponding to the optimal following distance. Although it had no
direct control over the simulation, the goal function positively or negatively reinforced the
system's behavior.
The network was given complete control of the robot vehicle, and the human trainer had
no influence except the ability to start and terminate training. This proved unsatisractory because
the initial system behavior--governed by random interconnection weights--was very unstable. The
robot tended to run over the car in rront of it before significant training occurred . By carerully
halting and restarting training we achieved stable system behavior. At first the rollowing distance
maintained by the robot car oscillated as ir the vehicle was attached by a sj)ring to the pace car.
This activity gradually damped. Arter about one thousand training steps the vehicle maintained
the optimal following distance and responded quickly to changes in the pace car's speed.
Constructing composite goal functions to promote more sophisticated abilities proved
difficult, even ill-defined, because there were many unspecified parameters. To generate goal
runctions ror these abilities would be similar to conventional programming--the type or labor we
want to circumvent using ANS. On the other hand, humans are adept at assessing complex situations and making decisions based on qualitative data, but their "goal runctions" are difficult ir not
impossible to capture analytically. One attraction of ANS is that it can imitate behavior based on
these elusive rules without rormally specifying them. At this point we turned our efforts to
manual training techniques.
The initially trained network was grafted into a larger system and augmented with additional inputs: distance and speed inrormation on nearby pace cars in a second traffic lane, and an
output control signal governing lane changes . The original network's ability to maintain a safe
following distance was retained intact. Thts grafting procedure is one of two methods we studied
for adding ne .... abilities to an existin, system. (The second, which employs a block structure, is
described below.) The network remained in direct control of the robot vehicle, but a human
trainer instructed it when and when not to change lanes. His commands were interpreted as the
desired output and used in the GDR training algorithm. This technique, which we call coaching,
proved userul and the network quickly correlated its environmental inputs with the teacher's
instructions. The network became adept at changing lanes and weaving through traffic. We found
that the network took on the behavior pattern or its trainer. A conservative teacher produced a
timid network, while an aggressive tzainer produced a network that tended to cut off other automobiles and squeeze through tight openings . Despite its success, the coaching method of training
did not solve the problem or initial network instability.
The stability problem was solved by giving the trainer direct control over the simulation.
The system configuration (Figure 1), allows the expert to exert control or release it to the n~t?
work. During initial tzaining the expert is in the driver's seat while the network acts the role of

696

apprentice. It receives sensor information, predicts system commands, and compares its predictions. against the desired output (ie. the trainer's commands) . Figure 2 shows the data and command flow in detail. Input data is processed through different channels and presented to the
trainer and network. Where visual and audio formats are effective for humans, the network uses
information in vector form. This differentiation of data presentation is a limitation of the system;
removing it is a cask for future ~search. The trainer issues control commands in accordance with
his assigned ~k while the network takes the trainer's actions as desired system responses and
correlates these with the input. We refer to this procedure as master/apprentice training, network
training proceeds invisibly in the background as the expert proceeds with his day to day work. It
avoids the instability problem because the network is free to make errors without the adverse
consequence of throwing the operating environment into disarray.
I

Input

World (--> sensors)

l+

or

Simulation
~------------------~

~

Actuation

I

Ne',WOrk

~-

I

Expert

Commands
+
~------~---------------------------~
J

Figure 1. A scheme for manually training ANS networks. Input data is received by both
the network and trainer. The trainer issues commands that are actuated (solid command
line). or he coaches the network in how it ought to respond (broken command line).

--+ Commands

Preprocessing
tortunan
Input
data
Preprocessing
for network

N twork
e

t

--+

Predicted
commands

~
9'l. Actuation

.1-r"

'-------------.
Coaching/emphasis

Training
rule

Fegure 2. Data and convnand flow In the training system. Input data is processed and presented

to the trainer and network. In master/appre~ice training (solid command Hne). the trainer's
orders are actuated and the network treats his commands as the system's desired output. In
coaching. the network's predicted oonvnands are actuated (broken command line). and the
trainer influences weight adaptation by specifying the desired system output and controlHng
the values of trailing constants-his -suggestions- are not cirec:tty actuated.
Once initial. bacqround wainmg is complete, the expert proceeds in a more formal
manner to teach the network. He releases control of the command system to the network in
order to evaluate ita behavior and weaknesses. He then resumes control and works through a

697

series of scenarios designed to train t.he network out of its bad behavior. By switching back and
forth. between human and network control, the expert assesses the network's reliability and
teaches correct responses as needed. We find master/apprentice training works well for behavior
involving continuous functions, like steering. On the other hand, coaching is appropriate for decision Cunctions, like when Ule car ought to pass. Our methodology employs both techniques.
The Driving Network
The fully developed freeway simulation consists of a two lane highway that is made of
joined straight and curved segments which vary at. random in length (and curvature). Several
pace cars move at random speeds near the robot vehicle. The network is given the tasks of tracking the road, negotiating curves. returning to the road if placed far afield, maintaining safe distances from the pace cars, and changing lanes when appropriate. Instead of a single multi-layer
structure, the network is composed of two blocks; one controls the steering and the other regulates speed and decides when the vehicle should change lanes (Figure 3). The first block receives
information about the position and speed of the robot vehicle relative to other ears in its vicinity.
Its output is used to determine the automobile's speed and whet.her the robot should change
lanes . The passing signal is converted to a lane assignment based on the car's current lane position. The second block receives the lane assignment and data pertinent to the position and orientation of the vehicle with respect to the road. The output is used to determine the steering angle
of the robot car.

Block 1

Inputs

Outputs

Constant.
Speed.
Disl. Ahead, Pl ?
Disl. Ahead, Ol ?
Dist. Behind, Ol ?
ReI. Speed Ahead, Pl ?
ReI. Speed Ahead, Ol ?
ReI. Speed Behind, Ol ?

I

Speed
Change lanes

?

Steering Angle

Convert lane change to lane number
Constant
Rei. Orientation
-..--t~ lane Nurmer
lateral Dist.
Curvature

?
?
?
?
?

??
?

Figure 3. The two blocks of the driving ANS network. Heavy arrows Indicate total interconnectivity
between layers. PL designates the traffic lane presently oca.apied by the robot vehicle, Ol refers
to the other lane, QJrvature refers to the road, lane nurrber is either 0 or 1, relative orientation and
lateral distance refers to the robot car's direction and podion relative to the road'l direction and
center line. respectively.
.

698

The input data is displayed in pictorial and textual form to the driving instructor. He views
the road and nearby vehicles from the perspective of the driver's seat or overhead. The network
receives information in the form of a vector whose elements have been scaled to unitary order,
O( 1) . Wide ranging input parameters, like distance, are compressed using the hyperbolic tangent
or logarithmic functions . In each block , the input layer is totally interconnected to both the ou~
put and a hidden layer. Our scheme trains in real time, and as we discuss later, it trains more
smoothly with a small modification of the training algorithm .
Output is interpreted in two ways: as a binary decision or as a continuously varying parameter. The first simply compares the sigmoid output against a threshold. The second scales the
output to an appropriate range for its application . For example, on the steering output element, a
0.5 value is interpreted as a zero steering angle. Left and right turns of varying degrees are initiated when this output is above or below 0.5, respectively.
The network is divided into two blocks that can be trained separately. Beside being conceptually easier to understand , we find this component approach is easy to train systematically.
Because each block has a restricted, well-defined set of tasks, the trainer can concentrate
specifically on those functions without being concerned that other aspects of the network behavior
are deteriorating.
"'e trained the system from bottom up, first teaching the network to stay on the road ,
negotiate curves , chan~e lanes, and how to return if the vehicle strayed off the highway. Block 2,
responsible for steering, learned these skills in a few minutes using the master/apprentice mode.
It tended to steer more slowly than a human but further training progressively improved its
responsiveness.
We experimented with different trammg constants and "momentum" values. Large "
values, about 1, caused weights to change too coarsely. " values an order of magnitude smaller
worked well . We found DO advantage in using momentum for this method of training , in fact,
the system responded about three times more slowly when 0 =0.9 than when the momentt:m
term was dropped. Our standard training parameters were" =0.2, and Cl' =00

a)

~

Db)~~

=D-=-~=~~--=~--= ~

Figure 4. Typical behavior of a network-controlled vehicle (dam rectangle) when trained by
a) a conservative miYer, ItI:I b}. reckless driver. Speed Is indicated by the length of the arrows.
After Block 2 "Was trained, we gave steering control to the network and concentrated on
teaching the network to change lanes and adjust speed. Speed control in this ('"asP. was a continuous variable and was best taught using master/apprentice training. On the other hand, the binary
decision to change lanes was best taught by coaching . About ten minutes of training were needed
to teach the network to weave through traffic. We found that the network readily adapts the

699

behavioral pattern of its trainer. A conservative trainer generated a network that hardly ever
passed, while an aggressive trainer produced a network that drove recklessly and tended to cut off
other-cars (Figure 4).
Discussion
One of the strengths of el:pert 5ystf'mS based on ANS is that the use of input data in the
decision making and control proc~ss does not have to be specified . The network adapts its internal weights to conform to input/ output correlat.ions it discovers . It is important, however, that
data used by the human expert is also available to the network. The different processing of sensor data for man and network may have important consequences, key information may be
presented to the man but not. the machine.
This difference in data processing is particularly worrisome for image data where human
ability to extract detail is vastly superior to our au tomatic image processing capabilities. Though
we would not require an image processing system to understand images, it would have to extract
relevant information from cluttered backgrounds. Until we have sufficiently sophisticated algorithms or networks to do this, our efforts at constructing expert systems which halldle image data
are handicapped .
Scaling input data to the unitary order of magnitude is important for training stability. 111is
is evident from equations (1) and (2) . The sigmoid transfer function ranges from 0.1 to 0.9 in
approximat.eiy four units, that is, over an 0(1) domain. If system response must change in reaction to a large, O( n) swing of a given input parameter, the weight associated with that input will
be trained toward an O( n- 1) magnitude. On the other hand, if the same system responds to an
input whose range is O( 1), its associated weight will also be 0(1). The weight adjustment equation does not recognize differences in weight magnitude, therefore relatively small weights will
undergo wild magnitude adjustments and converge weakly. On the other hand, if all input parameters are of the same magnitude their associated weights will reflect this and the training constant
can be adjusted for gentle weight convergence . Because the output of hidden units are constrained between zero and one, O( 1) is a good target range for input parameters. Both the hyperbolic tangent and logarithmic functions are useful for scaling wide ranging inputs . A useful form
of the latter is
.8[I+ln(x/o)]
.8x/o
-.8[I+ln(-%/o)]

if o<x,
if-o::;x::;o,
ifx<-o,

( 4)

where 0>0 and defines the limits of the intermediate linear section, and .8 is a scaling factor.
This symmetric logarithmic function is continuous in its first derivative, and useful when network
behavior should change slowly as a parameter increases without bound. On the othl'r hand, if the
system should approach a limiting behavior, the tanh function is appropriate.
Weight adaptation is also complicated by relaxing the common practice of restricting interconnections to adjacent layers. Equation (3) shows that the calculated error for a hidden layergiven comparable weights, fanouts and output errors-will be one quarter or less than that of the

700

output layer. This is caused by the slope ractor, 0 .. ( 1- oil. The difference in error magnitudes is
not noticeable in networks restricted to adjacent layer interconnectivity. But when this constraint
is released the effect of errors originating directly from an output unit has 4" times the magnitude
and effect of an error originating from a hidden unit removed d layers from the output layer.
Compared to the corrections arising from the output units, those from the hidden units have little
influence on weight adjustment, and the power of a multilayer structure is weakened . The system
will train if we restrict connections to adjacent layers, but it trains slowly. To compensate for this
effect we attenuate the error magnitudes originating from the output layer by the above factor.
This heuristic procedure works well and racilitates smooth learning.
Though we have made progress in real-time learning systems using GDR, compared to
humans-who can learn from a single data presentation-they remain relatively sluggish in learning
and response rates. We are interested in improvements of the GDR algorithm or alternative
architectures that facilitate one-shot or rapid learning. In the latter case we are considering least
squares restoration techniquesl4] and Grossberg and Carpenter's adaptive resonance modelsI3,5].
The construction of automated expert systems by observation of human personnel is
attractive because of its efficient use of the expert's time and effort. Though the classic AI
approach of rule base inference is applicable when such rules are clear cut and well organized, too
often a human expert can not put his decision making process in words or specify the values of
parameters that influence him . The attraction or ANS based systems is that imitations of expert
behavior emerge as a natural consequence of their training.

Referenees
1) D. E. Rumelhart, G . E. Hinton, and R. J. Williams, "Learning Internal Representations by
Error Propagation," in Parallel D~tributed Proceuing: Ezploration~ in the Micro~trvcture 0/ Cognition,
Vol. I, D. E . Rumelhart and J. L. McClelland (Eds.)' chap. 8, (1986), Bradford BooksjMIT Press,
Cambridge

2) S. Grossberg,

Studie~

0/ Mind and Brain, (1982), Reidel, Boston

3) A. Barto and R. Sutton, "Landmark Learning: An Illustration of Associative Search," BiologicaIC,6emetiu,42, (1981), p.l
4) A. Rosenfeld and A . Kak, Digital Pieture Proeming, Vol. 1, chap. 7, (1982), Academic Press,
New York

5) G. A. Carpenter and S. Grossberg, "A Massively Parallel Architecture for a Self-organizing
Neural Pattern Recognition Machine," Computer Vision, Graphiu and Image Procu,ing, 37,
( 1987), p.54


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 636-remote-sensing-image-analysis-via-a-texture-classification-neural-network.pdf

Remote Sensing Image Analysis via a Texture
Classification Neural Network

Hayit K. Greenspan and Rodney Goodman
Department of Electrical Engineering
California Institute of Technology, 116-81
Pasadena, CA 91125
hayit@electra.micro.caltech.edu

Abstract
In this work we apply a texture classification network to remote sensing image analysis. The goal is to extract the characteristics of the area depicted
in the input image, thus achieving a segmented map of the region. We have
recently proposed a combined neural network and rule-based framework
for texture recognition. The framework uses unsupervised and supervised
learning, and provides probability estimates for the output classes. We
describe the texture classification network and extend it to demonstrate
its application to the Landsat and Aerial image analysis domain .

1

INTRODUCTION

In this work we apply a texture classification network to remote sensing image
analysis. The goal is to segment the input image into homogeneous textured regions
and identify each region as one of a prelearned library of textures, e.g. tree area and
urban area distinction. Classification 0 f remote sensing imagery is of importance in
many applications, such as navigation, surveillance and exploration. It has become
a very complex task spanning a growing number of sensors and application domains.
The applications include: landcover identification (with systems such as the AVIRIS
and SPOT), atmospheric analysis via cloud-coverage mapping (using the AVHRR
sensor), oceanographic exploration for sea/ice type classification (SAR input) and
more.
Much attention has been given to the use of the spectral signature for the identifica425

426

Greenspan and Goodman

tion of region types (Wharton, 1987; Lee and Philpot, 1991). Only recently has the
idea of adding on spatial information been presented (Ton et aI, 1991). In this work
we investigate the possibility of gaining information from textural analysis. We
have recently developed a texture recognition system (Greenspan et aI, 1992) which
achieves state-of-the-art results on natural textures. In this paper we apply the
system to remote sensing imagery and check the system's robustness in this noisy
environment. Texture can playa major role in segmenting the images into homogeneous areas and enhancing other sensors capabilities, such as multispectra analysis,
by indicating areas of interest in which further analysis can be pursued. Fusion of
the spatial information with the spectral signature will enhance the classification
and the overall automated analysis capabilities.
Most of the work in the literature focuses on human expert-based rules with specific
sensor data calibration. Some of the existing problems with this classic approach
are the following (Ton et aI, 1991):
- Experienced photointerpreters are required to spend a considerable amount of
time generating rules.
- The rules need to be updated for different geographical regions.
- No spatial rules exist for the complex Landsat imagery.
An interesting question is if one can automate the rule generation. In this paper we
present a learning framework in which spatial rules are learned by the system from
a given database of examples.
The learning framework and its contribution in a texture-recognition system is the
topic of section 2. Experimental results of the system's application to remote sensing
imagery are presented in section 3.

2

The texture-classification network

We have previously presented a texture classification network which combines a
neural network and rule-based framework (Greenspan et aI, 1992) and enables both
unsupervised and supervised learning. The system consists of three major stages,
as shown in Fig. 1. The first stage performs feature extraction and transforms the
image space into an array of 15-dimensional feature vectors, each vector corresponding to a local window in the original image. There is much evidence in animal visual
systems supporting the use of multi-channel orientation selective band-pass filters
in the feature-extraction phase. An open issue is the decision regarding the appropriate number of frequencies and orientations required for the representation of the
input domain. We define an initial set of 15 filters and achieve a computationally
efficient filtering scheme via the multi-resolution pyramidal approach.
The learning mechanism shown next derives a minimal subset of the above filters
which conveys sufficient information about the visual input for its differentiation
and labeling. In an unsupervised stage a machine-learning clustering algorithm is
used to quantize the continuous input features. A supervised learning stage follows
in which labeling of the input domain is achieved using a rule-based network. Here
an information theoretic measure is utilized to find the most informative correlations
between the attributes and the pattern class specification, while providing probability estimates for the output classes. Ultimately, a minimal representation for a
library of patterns is learned in a training mode, following which the classification

Remote Sensing Image Analysis via a Texture Classification Neural Network

ORENI'AT10N

SELEcrlVE
8PF

SUPERVISED
UNSUPERVISED

a.USTEANi

TEXTURE
LEARNING

CLASSES
vIII

Window
of Input Image

N-Dimensional
Continuous
Feature- Vector

?

?

N-Dimensional
Quantized
Feature-Vector

?

?

FEATURE-EXTRACTION

LEARNING

PHASE

PHASE

Figure 1: System block diagram
of new patterns is achieved.

2.1

The system in more detail

The initial stage for a classification system is the feature extraction phase. In the
texture-analysis task there is both biological a.nd computational evidence supporting the use of Gabor-like filters for the feature-extraction. In this work, we use
the Log Gabor pyramid, or the Gabor wavelet decomposition to define an initial
finite set of filters. A computational efficient. scheme involves using a pyramidal
representation of the image which is convolved with fixed spatial support oriented
Gabor filters (Greenspan at aI, 1993). Three scales are used with 4 orientations per
scale (0,90,45,135 degrees), together with a non-oriented component, to produce a
15-dimensional feature vector as the output of the feature extraction stage. Using
the pyramid representation is computationally efficient as the image is subsampled
in the filtering process. Two such size reduction stages take place in the three scale
pyramid. The feature values thus generated correspond to the average power of the
response, to specific orientation and frequency ranges, in an 8 * 8 window of the
input image. Each such window gets mapped to a 15-dimensional attribute vector
as the output of the feature extraction stage.
The goal of the learning system is to use the feature representation described above
to discriminate between the input patterns, or textures. Both unsupervised and
supervised learning stages are utilized. A minimal set of features are extracted from
the 15-dimensional attribute vector, which convey sufficient information about the
visual input for its differentiation and labeling.
The unsupervised learning stage can be viewed as a preprocessing stage for achieving a more compact representation of the filtered input. The goal is to quantize the
continuous valued features which are the result of the initial filtering, thus shifting
to a more symbolic representation of the input domain . This clustering stage was
found experimentally to be of importance as an initial learning phase in a classification system. The need for discretization becomes evident when trying to learn
associations between attributes in a symbolic representation, such as rules.

427

428

Greenspan and Goodman

The output of the filtering stage consists of N (=15), continuous valued feature
maps; each representing a filtered version of the original input. Thus, each local
area of the input image is represented via an N-dimensional feature vector. An
array of such N-dimensional vectors, viewed across the input image, is the input
to the learning stage. We wish to detect characteristic behavior across the Ndimensional feature space, for the family of textures to be learned. In this work, each
dimension, out of the 15-dimensional attribute vector, is individually clustered. All
training samples are thus projected onto each axis of the space and one-dimensional
clusters are found using the K-means clustering algorithm (Duda and Hart, 1973).
This statistical clustering technique consists of an iterative procedure of finding
K means in the training sample space, following which each new input sample is
associated with the closest mean in Euclidean distance. The means, labeled 0 thru K
minus 1 arbitrarily, correspond to discrete codewords. Each continuous-valued input
sample gets mapped to the discrete codeword representing its associated mean. The
output of this preprocessing stage is a 15-dimensional quantized vector of attributes
which is the result of concatenating the discrete-valued codewords of the individual
dimensions.
In the final, supervised stage, we utilize the existing information in the feature
maps for higher level analysis, such as input labeling and classification. A rule based information theoretic approach is used which is an extension of a first order
Bayesian classifier, because of its ability to output probability estimates for the output classes (Goodman et aI, 1992). The classifier defines correlations between input
features and output classes as probabilistic rules. A data driven supervised learning
approach utilizes an information theoretic measure to learn the most informative
links or rules between features and class labels. The classifier then uses these links
to provide an estimate of the probability of a given output class being true. When
presented with a new input evidence vector, a set of rules R can be considered to
"fire". The classifier estimates the posterior probability of each class given the rules
that fire in the form log(p( x )IR), and the largest estimate is chosen as the initial
class label decision. The probability estimates for the output classes can now be
used for feedback purposes and further higher level processing.
The rule-based classification system can be mapped into a 3 layer feed forward
architecture as shown in Fig. 2 (Greenspan et aI, 1993). The input layer contains
a node for each attribute. The hidden layer contains a node for each rule and the
output layer contains a node for each class. Each rule (second layer node j) is
connected to a class via a multiplicative weight of evidence Wj.

Inputs

Rules

Class
Probability
Estimates

Figure 2: Rule-Based Network

Remote Sensing Image Analysis via a Texture Classification Neural Network

3

Results

The above-described system has achieved state-of-the-art results on both structured
and unstructured natural texture classification [5]. In this work we present initial
results of applying the network to the noisy environment of satellite and air-borne
Imagery.
Fig. 3 presents two such examples. The first example (top) is an image of Pasadena,
California, taken via the AVIRIS system (Airborne Visible/Infrared Imaging Spectrometer). rhe AVIRIS system covers 224 contiguous spectral bands simultaneously, at 20 meters per pixel resolution. The presented example is taken as an
average of several bands in the visual range. In this input image we can see that
a major distinguishing characteristic is urban area vs. hilly surround. These are
the two categories we set forth to learn. The training consists of a 128*128 image
sample for each category. The test input is a 512*512 image which is very noisy
and because of its low resolution, very difficult to segment into the two categories,
even to our own visual perception. In the presented output (top right), the urban area is labeled in white, the hillside in gray and unknown, undetermined areas
are in darker gray. We see that a rough segmentation into the desired regions has
been achieved. The probabilistic network's output allows for the identification of
unknown or unspecified regions, in which more elaborate analysis can be pursued
(Greenspan et aI, 1992). The dark gray areas correspond to such regions; one example is the hill and urban contact (bottom right) in which some urban suburbs
on the hill slopes form a mixture of the classes. Note that in the initial results
presented the blockiness perceived is the result of the analysis resolution chosen.
Fusing into the system additional spectral bands as our input, would enable pixel
resolution as well as enable detecting additional classes (not visually detectable),
such as concrete material, a variety of vegetation etc.
A higher resolution Airborne image is presented at the bottom of Fig. 3. The
classes learned are bush (output label dark gray), ground (output label gray) and a
structured area, such as a field present or the man-made structures (white). Here,
the training was done on 128*128 image examples (1 example per class). The input
image is 800*800. In the result presented (right) we see that the three classes have
been found and a rough segmentation into the three regions is achieved. Note in
particular the detection of the bush areas and the three main structured areas in
the image, including the man-made field, indicated in white.
Our final example relates to an autonomous navigation scenario. Autonomous vehicles require an automated scene analysis system to avoid obstacles and navigate
through rough terrain. Fusion of several visual modalities, such as intensity-based
segmentation, texture, stereo, and color, together with other domain inputs, such
as soil spectral decomposition analysis, will be required for this challenging task. In
Fig. 4. we present preliminary results on outdoor photographed scenes taken by an
autonomous vehicle at JPL (Jet Propulsion Laboratory, Pasadena). The presented
scenes (left) are segmented into bush and gravel regions (right). The training set
consists of 4 64 * 64 image samples from each category. In the top example (a
256*256 pixel image), light gray indicates gravel while black represents bushy regions. We can see that intensity alone can not suffice in this task (for example, top
right corner). The system has learned some textural characteristics which guided

429

430

Greenspan and Goodman

Figure 3: Remote sensing image analysis results. The input test image is shown
(left) followed by the system output classification map (right). In the AVIRIS (top)
input, white indicates urban regions, gray is a hilly area and dark gray reflects
undetermined or different region types. In the Airborne output (bottom), dark
gray indicates a bush area, light gray is a ground cover region and white indicates
man-made structures. Both robustness to noise and generalization are demonstrated
in these two challenging real-world problems.

Remote Sensing Image Analysis via a Texture Classification Neural Network

the segmentation in otherwise similar-intensity regions. Note that this is also probably the cause for identifying the track-like region (e.g., center bottom) as bush
regions. We could learn track-like regions as a third category, or specifically include
such examples as gravel in our training set.
In the second example (a 400*400 input image, bottom) light gray indicates gravel,
dark gray represents a bush-like region, and black represents the unknown category.
Here, the top right region of the sky, is labeled correctly as an unknown, or new
category. Kote that intensity alone would have confused that region as being gravel.
Overall, the texture classification neural-network succeeds in achieving a correct,
yet rough, segmentation of the scene based on textural characteristics alone. These
are encouraging results indicating that the learning system has learned informative
characteristics of the domain.

?

Fig 4: Image Analysis for Autonomous Navigation

431

432

Greenspan and Goodman

4

Summary and Discussion

The presented results demonstrate the network's capability for generalization and
robustness to noise in very challenging real-world problems. In the presented framework a learning mechanism automates the rule generation. This framework can answer some of the current difficulties in using the human expert's knowledge. Further
more, the automation of the rule generation can enhance the expert's knowledge
regarding the task at hand. We have demonstrated that the use of textural spatial information can segment complex scenery into homogeneous regions. Some of
the system's strengths include generalization to new scenes, invariance to intensity,
and the ability to enlarge the feature vector representation to include additional
inputs (such as additional spectral bands) and learn rules characterizing the integrated modalities. Future work includes fusing several modalities within the learning framework for enhanced performance and testing the performance on a large
database.
Acknowledgements

This work is supported in part by Pacific Bell, and in part by DARPA and ONR
under grant no. N00014-92-J-1860. H. Greenspan is supported in part by an Intel
fellowship. The research described in this paper was carried out in part by the
Jet Propulsion Laboratories, California Institute of Technology. We would like to
thank Dr. C. Anderson for his pyramid software support and Dr. 1. Matthies for
the autonomous vehicle images.
References

S. Wharton. (1987) A Spectral-Knowledge-Based Approach for Urban Land-Cover
Discrimination. IEEE Transactions on Geoscience and Remote Sensing, Vol. GE25[3] :272-282.
J. Lee and W. Philpot. (1991) Spectral Texture Pattern Matching: A Classifier
For Digital Imagery. IEEE Transactions on Geoscience and Remote Sensing, Vol.
29[4] :545-554.
J. Ton, J. Sticklen and A. Jain. (1991) Knowledge-Based Segmentation of Landsat
Images. IEEE Transactions on Geoscience and Remote Sensing, Vol. 29[2]:222-232.
H. Greenspan, R. Goodman and R. Chellappa. (1992) Combined Neural Network
and Rule-Based Framework for Probabilistic Pattern Recognition and Discovery. In
J. E. Moody, S. J. Hanson, and R. P. Lippman (eds.), Advances in Neural Information Processing Systems 4.,444-452, San Mateo, CA: Morgan Kaufmann Publishers.
H. Greenspan, R. Goodman, R. Chellappa and C. Anderson. (1993) Learning
Texture Discrimination Rules in a Multiresolution System. Submitted to IEEE
Transactions on Pattern Analysis and Machine Intelligence.
R. O. Duda and P. E. Hart. (1973) Pattern Classification and Scene Analysis. John
Wiley and Sons, Inc.
R. Goodman, C. Higgins, J. Miller and P. Smyth. (1992) Rule-Based Networks for
Classification and Probability Estimation. Neural Computation, [4]:781-804.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 4477-monte-carlo-value-iteration-with-macro-actions.pdf

Monte Carlo Value Iteration with Macro-Actions
Zhan Wei Lim

David Hsu

Wee Sun Lee

Department of Computer Science, National University of Singapore
Singapore, 117417, Singapore

Abstract
POMDP planning faces two major computational challenges: large state spaces
and long planning horizons. The recently introduced Monte Carlo Value Iteration (MCVI) can tackle POMDPs with very large discrete state spaces or continuous state spaces, but its performance degrades when faced with long planning
horizons. This paper presents Macro-MCVI, which extends MCVI by exploiting macro-actions for temporal abstraction. We provide sufficient conditions for
Macro-MCVI to inherit the good theoretical properties of MCVI. Macro-MCVI
does not require explicit construction of probabilistic models for macro-actions
and is thus easy to apply in practice. Experiments show that Macro-MCVI substantially improves the performance of MCVI with suitable macro-actions.

1

Introduction

Partially observable Markov decision process (POMDP) provides a principled general framework for
planning with imperfect state information. In POMDP planning, we represent an agent?s possible
states probabilistically as a belief and systematically reason over the space of all beliefs in order
to derive a policy that is robust under uncertainty. POMDP planning, however, faces two major
computational challenges. The first is the ?curse of dimensionality?. A complex planning task
involves a large number of states, which result in a high-dimensional belief space. The second
obstacle is the ?curse of history?. In applications such as robot motion planning, an agent often
takes many actions before reaching the goal, resulting in a long planning horizon. The complexity
of the planning task grows very fast with the horizon.
Point-based approximate algorithms [10, 14, 9] have brought dramatic progress to POMDP planning. Some of the fastest ones, such as HSVI [14] and SARSOP [9], can solve moderately complex
POMDPs with hundreds of thousands states in reasonable time. The recently introduced Monte
Carlo Value Iteration (MCVI) [2] takes one step further. It can tackle POMDPs with very large discrete state spaces or continuous state spaces. The main idea of MCVI is to sample both an agent?s
state space and the corresponding belief space simultaneously, thus avoiding the prohibitive computational cost of unnecessarily processing these spaces in their entirety. It uses Monte Carlo sampling
in conjunction with dynamic programming to compute a policy represented as a finite state controller. Both theoretical analysis and experiments on several robotic motion planning tasks indicate
that MCVI is a promising approach for plannning under uncertainty with very large state spaces, and
it has already been applied successfully to compute the threat resolution logic for aircraft collision
avoidance systems in 3-D space [1].
However, the performance of MCVI degrades, as the planning horizon increases. Temporal abstraction using macro-actions is effective in mitigating the negative effect and has achieved good
results in earlier work on Markov decision processes (MDPs) and POMDPs (see Section 2). In this
work, we show that macro-actions can be seamlessly integrated into MCVI, leading to the MacroMCVI algorithm. Unfortunately, the theoretical properties of MCVI, such as the approximation error
bounds [2], do not carry over to Macro-MCVI automatically, if arbitrary mapping from belief to actions are allowed as macro-actions. We give sufficient conditions for the good theoretical properties
1

to be retained, tranforming POMDPs into a particular type of partially observable semi-Markov
decision processes (POSMDPs) in which the lengths of macro-actions are not observable.
A major advantage of the new algorithm is its ability to abstract away the lengths of macro-actions in
planning and reduce the effect of long planning horizons. Furthermore, it does not require explicit
probabilistic models for macro-actions and treats them just like primitive actions in MCVI. This
simplifies macro-action construction and is a major benefit in practice. Macro-MCVI can also be
used to construct a hierarchy of macro-actions for planning large spaces. Experiments show that the
algorithm is effective with suitably designed macro-actions.

2

Related Works

Macro-actions have long been used to speed up planning and learning algorithms for MDPs (see,
e.g., [6, 15, 3]). Similarly, they have been used in offline policy computation for POMDPs [16, 8].
Macro-actions can be composed hierarchically to further improve scalability [4, 11]. These earlier
works rely on vector representations for beliefs and value functions, making it difficult to scale up to
large state spaces. Macro-actions have also been used in online search algorithms for POMDPs [7].
Macro-MCVI is related to Hansen and Zhou?s work [5]. The earlier work uses finite state controllers
for policy representation and policy iteration for policy computation, but it has not yet been shown
to work on large state spaces. Expectation-maximization (EM) can be used to train finite state
controllers [17] and potentially handle large state spaces, but it often gets stuck in local optima.

3

Planning with Macro-actions

We would like to generalize POMDPs to handle macro-actions. Ideally, the generalization should
retain properties of POMDPs such as piecewise linear and convex finite horizon value functions. We
would also like the approximation bounds for MCVI [2] to hold with macro-actions.
We would like to allow our macro-actions to be as powerful as possible. A very powerful representation for a macro-action would be to allow it to be an arbitrary mapping from belief to action
that will run until some termination condition is met. Unfortunately, the value function of a process
with such macro-actions need not even be continuous. Consider the following simple finite horizon example, with horizon one. Assume that there are two primitive actions, both with constant
rewards, regardless of state. Consider two macro-actions, one which selects the poorer primitive
action all the time while the other which selects the better primitive action for some beliefs. Clearly,
the second macro-action dominates the first macro-action over the entire belief space. The reward
for the second macro-action takes two possible values depending on which action is selected for the
belief. The reward function also forms the optimal value function of the process and need not even
be continuous as the macro-action can be an arbitrary mapping from belief to action.
Next, we give sufficient conditions for the process to retain piecewise linearity and convexity of
the value function. We do this by constructing a type of partially observable semi-Markov decision
process (POSMDP) with the desired property. The POSMDP does not need to have the length of
the macro-action observed, a property that can be practically very useful as it allows the branching
factor for search to be significantly smaller. Furthermore, the process is a strict generalization of a
POMDP as it reduces to a POMDP when all the macro-actions have length one.
3.1 Partially Observable Semi-Markov Decision Process
Finite-horizon (undiscounted) POSMDP were studied in [18]. Here, we focus on a type of infinitehorizon discounted POSMDPs whose transition intervals are not observable. Our POSMDP is formally defined as a tuple (S, A, O, T, R, ?), where S is a state space, A is a macro-action space,
O is a macro-observation space, T is a joint transition and observation function, R is a reward
function, and ? ? (0, 1) is a discount factor. If we apply a macro-action a with start state si ,
T = p(sj , o, k|si , a) encodes the joint conditional probability of the end state sj , macro-observation
o, and the number of time steps k that it takes for a to reach sj from si . We could decompose T
into a state-transition function and an observation function, but avoid doing so here to remain general and simplify the notation. The reward function
P?R gives the discounted cumulative reward for a
macro-action a that starts at state s: R(s, a) = t=0 ? t E(rt |s, a), where E(rt |s, a) is the expected
reward at step t. Here we assume that the reward is 0 once a macro-action terminates.
For convenience, we will work with reweighted beliefs, instead of beliefs. Assuming that the number
of states is n, a reweighted belief (like a belief) is a vector of n non-negative numbers that sums to
2

one. By assuming that the POSMDP process will stop with probability 1?? at each time step, we can
interpret the reweighted belief as the conditional probability of a state given that the process has not
stopped. This gives an interpretation of the reweighted belief in terms of the discount factor. Given
a reweighted belief, we compute the next reweighted belief given macroaction a and observation o,
b0 = ? (b, a, o), as follows:
P? k?1 Pn
?
p(s, o, k|si , a)b(si )
Pn i=1
Pn
b0 (s) = P? k=1
.
(1)
k?1
?
j=0
i=1 p(sj , o, k|si , a)b(si )
k=1
We
simply
the reweighted belief as a belief from here on. We denote the denominator
P?will k?1
Pnrefer
Pto
n
?
k=1
j=0
i=1 p(sj , o, k|si , a)b(si ) by p? (o|a, b). The value of ?p? (o|a, b) can be interpreted
as
the
probability
that observation o is received and the POSMDP has not stopped. Note that
P
p
(o|a,
b)
may
sum
to
less than 1 due to discounting.
?
o
P
A policy ? is a mapping from a belief to a macro-action. Let R(b, a) = s b(s)R(s, a). The value
of a policy ? can be defined recursively as
X
V? (b) = R(b, ?(b)) + ?
p? (o|?(b), b)V? (? (b, ?(b), o)).
o

Note that the policy operates on the belief and may not know the number of steps taken by the
macro-actions. If knowledge of the number of steps is important, it can be added into the observation
function in the modeling process.
We now define the backup operator H that operates on a value function Vm and returns Vm+1
X

p? (o|a, b)V (? (b, a, o)) .
HV (b) = max R(b, a) + ?
a

(2)

o?O

The backup operator is a contractive mapping1 .
Lemma 1 Given value functions U and V , ||HU ? HV ||? ? ?||U ? V ||? .
Let the value of an optimal policy, ? ? , be V ? . The following theorem is a consequence of the Banach
fixed point theorem and Lemma 1.
Theorem 1 V ? is the unique fixed point of H and satisfies the Bellman equation V ? = HV ? .
We call a policy an m-step policy if the number of times the macro-actions is applied is m. For
m-step policies, V ? can be approximated by a finite set of linear functions; the weight vectors of
these linear functions are called the ?-vectors.
Theorem 2 The value function for an m-step policy is piecewise linear and convex and can be
represented as
X
Vm (b) = max
?(s)b(s)
(3)
???m

s?S

where ?m is a finite collection of ?-vectors.
As Vm is convex and converges to V ? , V ? is also convex.
3.2 Macro-action Construction
We would like to construct macro-actions from primitive actions of a POMDP in order to use temporal abstraction to help solve difficult POMDP problems. A partially observable Markov decision
process (POMDP) is defined by finite state space S, finite action space A, a reward function R(s, a),
an observation space O, and a discount ? ? (0, 1).
In our POSMDP, the probability function p(sj , o, k|si , a) for a macro-action must be independent
of the history given the current state si ; hence the selection of primitive actions and termination
conditions within the macro-action cannot depend on the belief. We examine some allowable dependencies here. Due to partial observability, it is often not possible to allow the primitive action and
the termination condition to be functions of the initial state. Dependence on the portion of history
1

Proofs of the results in this section are included in the supplementary material.

3

that occurs after the macro-action has started is, however, allowed. In some POMDPs, a subset of
the state variables are always observed and can be used to decide the next action. In fact, we may
sometimes explicitly construct observed variables to remember relevant parts of the history prior to
the start of macro-action (see Section 5); these can be considered as parameters that are passed on to
the macro-action. Hence, one way to construct the next action in a macro-action is to make it a function of the history since the macro-action started, xk , ak , ok+1 , . . . , xt?1 , at?1 , ot , xt , where xi is
the fully observable subset of state variables at time i, and k is the starting time of the macro-action.
Similarly, when the termination criterion and the observation function of the macro-action depends
only on the history xk , ak , ok+1 , . . . , xt?1 , at?1 , ot , xt , the macro-action can retain a transition
function that is independent of the history given the initial state. Note that the observation to be
passed on to the POSMDP to create the POSMDP observation space, O, is part of the design tradeoff - usually it is desirable to reduce the number of observations in order to reduce complexity
without degrading the value of the POSMDP too much. In particular, we may not wish to include
the execution length of the macro-action if it does not contribute much towards obtaining a good
policy.

4

Monte Carlo Value Iteration with Macro-Actions

We have shown that if the action space A and the observation space O of a POSMDP are discrete,
then the optimal value function V ? can be approximated arbitrarily closely by a piecewise-linear,
convex function. Unfortunately, when S is very high-dimensional (or continuous), a vector representation is no longer effective. In this section, we show how the Monte Carlo Value Iteration (MCVI)
algorithm [2], which has been designed for POMDPs with very large or infinite state spaces, can be
extended to POSMDP.
Instead of ?-vectors, MCVI uses an alternative policy representation called a policy graph G. A
policy graph is a directed graph with labeled nodes and edges. Each node of G is labeled with an
macro-action a and each edge of G is labeled with an observation o. To execute a policy ?G , it
is treated as a finite state controller whose states are the nodes of G. Given an initial belief b, a
starting node v of G is selected and its associated macro-action av is performed. The controller
then transitions from v to a new node v 0 by following the edge (v, v 0 ) labeled with the observation
received, o. The process then repeats with the new controller node v 0 .
Let ?G,v denote a policy represented by G, when the controller always starts in node v of G. We
define the value ?v (s) to be the expected total reward of executing ?G,v with initial state s. Hence
X
VG (b) = max
?v (s)b(s).
(4)
v?G

s?S

VG is completely determined by the ?-functions associated with the nodes of G.
4.1 MC-Backup
One way to approximate the value function is to repeatedly run the backup operator H starting
from an arbitrary value function until it is close to convergence. This algorithm is called value
iteration (VI). Value iteration can be carried out on policy graphs as well, as it provides an implicit
representation of a value function. Let VG be the value function for a policy graph G. Substituting
(4) into (2), we get
nX
o
X
X
HVG (b) = max
R(s, a)b(s) +
p? (o|a, b) max
?v (s)b0 (s) .
(5)
a?A

s?S

o?O

v?G

s?S

It is possible to then evaluate the right-hand side of (5) via sampling and monte carlo simulation at a
? b VG . This is called MC-backup
belief b. The outcome is a new policy graph G0 with value function H
of G at b (Algorithm 1) [2].
There are |A||G||O| possible ways to generate a new policy graph G0 which has one new node
compared to the old policy graph node. Algorithm 1 computes an estimate of the best new policy
graph at b using only N |A||G| samples. Furthermore, we can show that MC-backup approximates
?
the standard VI backup (equation (5)) well at b, with error decreasing at the rate O(1/ N ). Let
Rmax be the largest absolute value of the reward, |rt |, at any time step.
4

Algorithm 1 MC-Backup of a policy graph G at a belief b ? B with N samples.
MC-BACKUP(G, b, N )
1: For each action a ? A, Ra ? 0.
2: For each action a ? A, each observation o ? O, and each node v ? G, Va,o,v ? 0.
3: for each action a ? A do
4:
for i = 1 to N do
5:
Sample a state si with probability b(si ).
6:
Simulate taking macro-action a in state si . Generate a new state s0i , observation oi , and discounted
reward R0 (si , a) by sampling from p(sj , o, k|si , a).
7:
Ra ? Ra + R0 (si , a).
8:
for each node v ? G do
9:
Set V 0 to be the expected total reward of simulating the policy represented by G, with initial
controller state v and initial state s0i .
10:
Va,oi ,v ? Va,oi ,v + V 0 .
11:
for each observation o ? O do
12:
Va,o ? maxv?G Va,o,v .
13:
va,o ? argmax
Pv?G Va,o,v .
14:
Va ? (Ra + ? o?O Va,o )/N .
15: V ? ? maxa?A Va .
16: a? ? argmaxa?A Va .
17: Create a new policy graph G0 by adding a new node u to G. Label u with a? . For each o ? O, add the
edge (u, va? ,o ) and label it with o.
18: return G0 .

Theorem 3 Given a policy graph G and a point b ? B, MC-BACKUP(G, b, N ) produces an improved policy graph such that
s

2 |O| ln |G| + ln(2|A|) + ln(1/? )
2R
max
?
|Hb VG (b) ? HVG (b)| ?
,
1??
N
with probability at least 1 ? ? .
The proof uses Hoeffding bound together with union bound. Details can be found in [2].
MC-backup can be combined with point-based POMDP planning, which samples the belief space
B. Point-based POMDP algorithms use a set B of points sampled from B as an approximate representation of B. In contrast to the standard VI backup operator H, which performs backup at every
? B applies MC-BACKUP(Gm , b, N ) on a policy graph Gm at every point
point in B, the operator H
? B then produces a new policy graph Gm+1 by
in B. This results in |B| new policy graph nodes. H
adding the new policy graph nodes to the previous policy graph Gm .
Let ?B = supb?B minb0 ?B kb ? b0 k1 be the maximum L1 distance from any point in B to the closest
? B Vm . The theorem
point in B. Let V0 be value function for some initial policy graph and Vm+1 = H
below bounds the approximation error between Vm and the optimal value function V ? .
Theorem 4 For every b ? B,
s
2Rmax
|V ? (b) ? Vm (b)| ?
(1 ? ?)2


2 |O| ln(|B|m) + ln(2|A|) + ln(|B|m/? )
2Rmax
2? m Rmax
+
?
+
,
B
N
(1 ? ?)2
(1 ? ?)

with probability at least 1 ? ? .
The proof requires the contraction property and a Lipschitz property that can be derived from the
piece-wise linearity of the value function. Having established those results in Section 3.1, the rest
of the proof follows from the proof in [2]. The first term in the bound in?Theorem 4 comes from
Theorem 3, showing that the error from sampling decays at the rate O(1/ N ) and can be reduced
by taking a large enough sample size. The second term depends on how well the set B covers B
and can be reduced by sampling a larger number of beliefs. The last term depends on the number of
MC-backup iterations and decays exponentially with m.
5

(a)
(b)
(c)
Figure 1: (a) Underwater Navigation: A reduced map with a 11 ? 12 grid is shown with ?S? marking the
possible initial positions, ?D? marking the destinations, ?R? marking the rocks and ?O? marking the locations
where the robot can localize completely. (b) Collaborative search and capture: Two robotic agents catching 12
escaped crocodiles in a 21 ? 21 grid. (c) Vehicular ad-hoc networking: An UAV maintains ad-hoc network
over four ground vehicles in a 10 ? 10 grid with ?B? marking the base and ?D? the destinations.

4.2 Algorithm
Theorem 4 bounds the performance of the algorithm when given a set of beliefs. Macro-MCVI,
like MCVI, samples beliefs incrementally in practice and performs backup at the sampled beliefs.
Branch and bound is used to avoid sampling unimportant parts of the belief space. See [2] for details.
The other important component in a practical algorithm is the generation of next belief; MacroMCVI uses a particle filter for that. Given the macro-action construction as described in Section 3.2,
a simple particle filter is easily implemented to approximate the next belief function in equation (1):
sample a set of states from the current belief; from each sampled state, simulate the current macroaction until termination, keeping track of its path length, t; if the observation at termination matches
the desired observation, keep the particle; the set of particles that are kept are weighted by ? t and
then renormalized to form the next belief2 . Similarly, MC-backup is performed by simply running
simulations of the macro-actions - there is no need to store additional transition and observation
matrices, allowing the method to run for very large state spaces.

5

Experiments

We now illustrate the use of macro-actions for temporal abstraction in three POMDPs of varying
complexity. Their state spaces range from relatively small to very large. Correspondingly, the
macro-actions range from relatively simple ones to much more complex ones forming a hierarchy.
Underwater Navigation: The underwater navigation task was introduced in [9]. In this task, an
autonomous underwater vehicle (AUV) navigates in an environment modeled as 51 x 52 grid map.
The AUV needs to move from the left border to the right border while avoiding the rocks scattered
near its destination. The AUV has six actions: move north, move south, move east, move north-east,
move south-east or stay in the same location. Due to poor visibility, the AUV can only localize itself
along the top or bottom borders where there are beacon signals.
This problem has several interesting characteristics. First, the relatively small state space size of
2653 means that solvers that use ?-vectors, such as SARSOP [9] can be used. Second, the dynamics
of the robot is actually noiseless, hence the main difficulty is actually localization from the robot?s
initially unknown location.
We use 5 macro-actions that move in a direction (north, south, east, north-east, or south-east) until
either a beacon signal or the destination is reached. We also define an additional macro-action that:
navigates to the nearest goal location if the AUV position is known, or simply stays in the same
location if the AUV position is not known. To enable proper behaviour of the last macro-action,
we augment the state space with a fully observable state variable that indicates the current AUV
location. The variable is initialized to a value denoting ?unknown? but takes the value of the current
AUV location after the beacon signal is received. This gives a simple example where the original
state space is augmented with a fully observable state variable to allow more sophisticated macroaction behaviour.
2
More sophisticated approximation of the belief can be constructed but may require more knowledge of the
underlying POMDP and more computation.

6

Collaborative Search and Capture: In this problem, a group of crocodiles had escaped from its
enclosure into the environment and two robotic agents have to collaborate to hunt down and capture
the crocodiles (see Figure 1). Both agents are centrally controlled and each agent can make a one
step move in one of the four directions (north, south, east and west) or stay still at each time instance.
There are twelve crocodiles in the environment. At every time instance, each crocodile moves to
a location furthest from the agent that is nearest to it with a probability 1 ? p (p = 0.05 in the
experiments). With a probability p, the crocodile moves randomly. A crocodile is captured when
it is at the same location as an agent. The agents do not know the exact location of the crocodiles,
but each agent knows the number of crocodiles in the top left, top right, bottom left and bottom
right quadrants around itself from the noise made by the crocodiles. Each captured crocodile gives
a reward of 10, while movement is free.
We define twenty-five macro actions where each agent moves (north, south, east, west, or stay) along
a passage way until one of them reaches an intersection. In addition, the macro-actions only return
the observation it makes at the point when the macro-action terminates, reducing the complexity
of the problem, possibly at a cost of some sub-optimality. In this problem, the macro-actions are
simple, but the state space is extremely large (approximately 17914 ).
Vehicular Ad-hoc Network: In a post disaster search and rescue scenario, a group of rescue vehicles are deployed for operation work in an area where communication infrastructure has been
destroyed. The rescue units need high-bandwidth network to relay images of ground situations. An
Unmanned Aerial Vehicle (UAV) can be deployed to maintain WiFi network communication between the ground units. The UAV needs to visit each vehicle as often as possible to pick up and
deliver data packets [13].
In this task, 4 rescue vehicles and 1 UAV navigates in a terrain modeled as a 10 x 10 grid map. There
are obstacles on the terrain that are impassable to ground vehicle but passable to UAV. The UAV can
move in one of the four directions (north, south, east, and west) or stay in the same location at every
time step. The vehicles set off from the same base and move along some predefined path towards
their pre-assigned destinations where they will start their operations, randomly stopping along the
way. Upon reaching its destination, the vehicle may roam around the environment randomly while
carrying out its mission. The UAV knows its own location on the map and can observe the location
of a vehicle if they are in the same grid square. To elicit a policy with low network latency, there
is a penalty of ?0.1? number of time steps since last visit of a vehicle for each time step for each
vehicle. There is a reward of 10 for each time a vehicle is visited by the UAV. The state space
consists of the vehicles? locations, UAV location in the grid map and the number of time steps since
each vehicle is last seen (for computing the reward).
We abstract the movements of UAV to search and visit a single vehicle as macro actions. There
are two kinds of search macro actions for each vehicle: search for a vehicle along its predefined
path and search for a vehicle that has started to roam randomly. To enable the macro-actions to
work effectively, the state space is also augmented with the previous seen location of each vehicle.
Each macro-action is in turn hierarchically constructed by solving the simplified POMDP task of
searching for a single vehicle on the same map using basic actions and some simple macro-actions
that move along the paths. This problem has both complex hierarchically constructed macro-actions
and very large state space.
5.1

Experimental setup

We applied Macro-MCVI to the above tasks and compared its performance with the original MCVI
algorithm. We also compared with a state-of-the-art off-line POMDP solver, SARSOP [9], on the
underwater navigation task. SARSOP could not run on the other two tasks, due to their large state
space sizes. For each task, we ran Macro-MCVI until the average total reward stablized. We then ran
the competing algorithms for at least the same amount of time. The exact running times are difficult
to control because of our implementation limitations. To confirm the comparison results, we also
ran the competing algorithms 100 times longer when possible. All experiments were conducted on
a 16 core Intel Xeon 2.4Ghz computer server.
Neither MCVI nor SARSOP uses macro-actions. We are not aware of other efficient off-line macroaction POMDP solvers that have been demonstrated on very large state space problems. Some online
search algorithms, such as PUMA [7], use macro-actions and have shown strong results. Online
search algorithms do not generate a policy, making a fair comparison difficult. Despite that, they
7

are useful as baseline references; we implement a variant of PUMA as a one such reference. In our
experiments, we simply gave the online search algorithms as much or more time than Macro-MCVI
and report the results here. PUMA uses open-loop macro-actions. As a baseline reference for online
solvers with closed-loop macro-actions, we also created an online search variant of Macro-MCVI
by removing the MC-backup component. We refer to this variant as Online-Macro. It is similar to
other recent online POMDP algorithms [12], but uses the same closed-loop macro-actions as MCVI
does.
5.2 Results
The performance of the different algorithms is shown
in Figure 2 with 95% confidence intervals.
The underwater navigation task consist of two phases:
the localization phase and navigate to goal phase.
Macro-MCVI?s policy takes one macro-action, ?moving northeast until reaching the border?, to localize
and another macro-action, ?navigating to the goal?, to
reach the goal. In contrast, both MCVI and SARSOP
fail to match the performance of Macro-MCVI even
when they are run 100 times longer. Online-Macro
does well, as the planning horizon is short with the
use of macro-actions. PUMA, however, does not do
as well, as it uses the less powerful open-loop macroactions, which move in the same direction for a fixed
number of time steps.

Figure 2: Performance comparison.
Reward Time(s)
Underwater Navigation
Macro-MCVI
749.30 ? 0.28
1
MCVI
678.05 ? 0.48
4
725.28 ? 0.38
100
SARSOP
710.71 ? 4.52
1
730.83 ? 0.75
100
PUMA
697.47 ? 4.58
1
Online-Macro
746.10 ? 2.37
1
Collaborative Search & Capture
Macro-MCVI
17.04 ? 0.03
120
MCVI
13.14 ? 0.04
120
16.38 ? 0.05
12000
PUMA
1.04 ? 0.91
144
Online-Macro
0
3657
Vehicular Ad-Hoc Network
Macro-MCVI
-323.55 ? 3.79
29255
MCVI
-1232.57 ? 2.24
29300
Greedy
-422.26 ? 3.98
28800

For the collaborative search & capture task, MCVI
fails to match the performance of Macro-MCVI even
when it is run for 100 times longer. PUMA and
Online-Macro do badly as they fail to search deep
enough and do not have the benefit of reusing sub-policies obtained from the backup operation.
To confirm that it is the backup operation and not the shorter per macro-action time that is responsible for the performance difference, we ran Online-Macro for a much longer time and found the
result unchanged.
The vehicular ad-hoc network task was solved hierarchically in two stages. We first used MacroMCVI to solve for the policy that finds a single vehicle. This stage took roughly 8 hours of computation time. We then used the single-vehicle policy as a macro-action and solved for the higher-level
policy that plans over the macro-actions. Although it took substantial computation time, MacroMCVI generated a reasonable policy in the end. In constrast, MCVI, without macro-actions, fails
badly for this task. Due to the long running time involved, we did not run MCVI 100 times longer.
To confirm that that the policy computed by Macro-MCVI at the higher level of the hierarchy is also
effective, we manually crafted a greedy policy over the single-vehicle macro-actions. This greedy
policy always searches for the vehicle that has not been visited for the longest duration. The experimental results indicate that the higher-level policy computed by Macro-MCVI is more effective than
the greedy policy. We did not apply online algorithms to this task, as we are not aware of any simple
way to hierarchically construct macro-actions online.

6

Conclusions

We have successfully extended MCVI, an algorithm for solving very large state space POMDPs,
to include macro-actions. This allows MCVI to use temporal abstraction to help solve difficult
POMDP problems. The method inherits the good theoretical properties of MCVI and is easy to
apply in practice. Experiments show that it can substantially improve the performance of MCVI
when used with appropriately chosen macro-actions.
Acknowledgement We thank Tom?s Lozano-P?rez and Leslie Kaelbling from MIT for many insightful discussions. This work is supported in part by MoE grant MOE2010-T2-2-071 and MDA
GAMBIT grant R-252-000-398-490.

8

References
[1] H. Bai, D. Hsu, M.J. Kochenderfer, and W. S. Lee. Unmanned aircraft collision avoidance
using continuous-state POMDPs. In Proc. Robotics: Science & Systems, 2011.
[2] H. Bai, D. Hsu, W. S. Lee, and V. Ngo. Monte Carlo Value Iteration for Continuous-State
POMDPs. In Algorithmic Foundations of Robotics IX?Proc. Int. Workshop o n the Algorithmic
Foundations of Robotics (WAFR), pages 175?191. Springer, 2011.
[3] Andrew G. Barto and Sridhar Mahadevan. Recent advances in hierarchical reinforcement
learning. Discrete Event Dynamic Systems, 13:2003, 2003.
[4] T. G. Dietterich. Hierarchical reinforcement learning with the MAXQ value function decomposition. J. Artificial Intelligence Research, 13:227?303, 2000.
[5] E. Hansen and R. Zhou. Synthesis of hierarchical finite-state controllers for POMDPs. In Proc.
Int. Conf. on Automated Planning and Scheduling, 2003.
[6] M. Hauskrecht, N. Meuleau, L.P. Kaelbling, T. Dean, and C. Boutilier. Hierarchical solution
of Markov decision processes using macro-actions. In Proc. Conf. on Uncertainty in Artificial
Intelligence, pages 220?229. Citeseer, 1998.
[7] R. He, E. Brunskill, and N. Roy. PUMA: Planning under uncertainty with macro-actions. In
Proc. AAAI Conf. on Artificial Intelligence, 2010.
[8] H. Kurniawati, Y. Du, D. Hsu, and W. S. Lee. Motion planning under uncertainty for robotic
tasks with long time horizons. Int. J. Robotics Research, 30(3):308?323, 2010.
[9] H. Kurniawati, D. Hsu, and W.S. Lee. SARSOP: Efficient point-based POMDP planning by
approximating optimally reachable belief spaces. In Proc. Robotics: Science & Systems, 2008.
[10] J. Pineau, G. Gordon, and S. Thrun. Point-based value iteration: An anytime algorithm for
POMDPs. In Int. Jnt. Conf. on Artificial Intelligence, volume 18, pages 1025?1032, 2003.
[11] J. Pineau, N. Roy, and S. Thrun. A hierarchical approach to POMDP planning and execution.
In Workshop on Hierarchy & Memory in Reinforcement Learning (ICML), volume 156, 2001.
[12] S. Ross, J. Pineau, S. Paquet, and B. Chaib-Draa. Online planning algorithms for POMDPs.
Journal of Artificial Intelligence Research, 32(1):663?704, 2008.
[13] A. Sivakumar and C.K.Y. Tan. UAV swarm coordination using cooperative control for establishing a wireless communications backbone. In Proc. Int. Conf. on Autonomous Agents &
Multiagent Systems, pages 1157?1164, 2010.
[14] T. Smith and R. Simmons. Heuristic search value iteration for POMDPs. In Proc. Conf. on
Uncertainty in Artificial Intelligence, pages 520?527. AUAI Press, 2004.
[15] R.S. Sutton, D. Precup, and S. Singh. Between MDPs and semi-MDPs: A framework for
temporal abstraction in reinforcement learning. Artificial Intelligence, 112(1):181?211, 1999.
[16] G. Theocharous and L. P. Kaelbling. Approximate planning in POMDPs with macro-actions.
Advances in Neural Processing Information Systems, 17, 2003.
[17] M. Toussaint, L. Charlin, and P. Poupart. Hierarchical POMDP controller optimization by
likelihood maximization. Proc. Conf. on Uncertainty in Artificial Intelligence, 2008.
[18] C.C. White. Procedures for the solution of a finite-horizon, partially observed, semi-Markov
optimization problem. Operations Research, 24(2):348?358, 1976.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 4104-size-matters-metric-visual-search-constraints-from-monocular-metadata.pdf

Size Matters: Metric Visual Search Constraints from
Monocular Metadata
Mario Fritz
UC Berkeley EECS & ICSI

Kate Saenko
UC Berkeley EECS & ICSI

Trevor Darrell
UC Berkeley EECS & ICSI

Abstract
Metric constraints are known to be highly discriminative for many objects, but
if training is limited to data captured from a particular 3-D sensor the quantity of
training data may be severly limited. In this paper, we show how a crucial aspect of
3-D information?object and feature absolute size?can be added to models learned
from commonly available online imagery, without use of any 3-D sensing or reconstruction at training time. Such models can be utilized at test time together
with explicit 3-D sensing to perform robust search. Our model uses a ?2.1D?
local feature, which combines traditional appearance gradient statistics with an
estimate of average absolute depth within the local window. We show how category size information can be obtained from online images by exploiting relatively
unbiquitous metadata fields specifying camera intrinstics. We develop an efficient metric branch-and-bound algorithm for our search task, imposing 3-D size
constraints as part of an optimal search for a set of features which indicate the
presence of a category. Experiments on test scenes captured with a traditional
stereo rig are shown, exploiting training data from from purely monocular sources
with associated EXIF metadata.

1

Introduction

Two themes dominate recent progress towards situated visual object recognition. Most significantly,
the availability of large scale image databases and machine learning methods has driven performance: accuracy on many category detection tasks is a function of the quantity and quality of the
available training data. At the same time, when we consider situated recognition tasks, i.e., as
performed by robots, autonomous vehicles, and interactive physical devices (e.g., mobile phones),
it is apparent that the variety and number of sensors is often what determines performance levels:
e.g., the avaibility of 3-D sensing can significantly improve performance on specific practical tasks,
irrespective of the amount of training data. A rich variety of 3-D sensors are available on modern
robotic systems, yet the training data are few for most 3-D sensor regimes: the vast majority of
available online visual category data are from monocular sources and there are few databases of
real-world 3-D scans from which to train robust visual recognizers. In general it is, however, difficult to reconcile these two trends: while one would like to use all available sensors at test time,
the paucity of 3D training data will mean few categories are well-defined with full 3-D models, and
generalization performance to new categories which lack 3-D training data may be poor. In this
paper, we propose a method to bridge this gap and extract features from typical 2D data sources that
can enhance recognition performance when 3D information is available at test time.

1

Figure 1: Recovery of object size from known camera intrinsics
The paradigm of recognition-by-local-features has been well established in the computer vision
literature in recent years. Existing recognition schemes are designed generally to be invariant to
scale and size. Local shape descriptors based on 3-D sensing have been proposed (e.g., VIP [2]),
as well as local 3-D descriptors (e.g., 3-D shape context and SIFT [4, 3]), but we are somewhat
skeptical of the ability of even the most recent 3-D sensor systems to extract the detailed local
geometry required to reliably detect and describe local 3-D shapes on real world objects.
Instead of extracting full 3D local features, we propose a ?2.1D? local feature model which augments
a traditional 2D local feature (SIFT, GLOH, SURF, etc.) with an estimate of the depth and 3-D size
of an observed patch. Such features could distinguish, for example, the two different keypad patterns
on a mobile device keyboard vs. on a full-size computer keyboard; while the keys might look locally
similar, the absolute patch size would be highly distinctive. We focus on the recognition of realworld objects when additional sensors are available at test time, and show how 2.1D information
can be extracted from monocular metadata already present in many online images. Our model
includes both a representation of the absolute size of local features, and of the overall dimension
of categories. We recover the depth and size of the local features, and thus of the bounding box of
a detected object in 3-D. Efficient search is an important goal, and we show a novel extension to
multi-class branch-and-bound search using explicit metric 3-D constraints.

2

Recognition with ?2.1D? features

The crux of our method is the inference and exploitation of size information; we show that we
can obtain such measurements from non-traditional sources that do not presume a 3-D scanner at
training time, nor rely on multi-view reconstruction / structure-from-motion methods. We instead
exploit cues that are readily available in many monocular camera images.1 We are not interested
in reconstructing the object surface, and only estimate the absolute size of local patches, and the
statistics of the bounding box of instances in the category; from these quantities we can infer the
category size.
We adopt a local-feature based recognition model and augment it with metric size information.
While there are several possible local feature recognition schemes based on sets of such local features, we focus on the Naive Bayes nearest-neighbor model of [1] because of its simplicity and
good empirical results. We assume one or more common local feature descriptors (and associated
detectors or dense sampling grids): SIFT, SURF, GLOH, MSER. Our emphasis in this paper is on
1
There are a number of general paradigms by which estimates of object size can be extracted from a 2D
image data source, e.g., regression from scene context [6]), or inference of depth-from-a-single-image [7, 11,
16]. In addition to such schemes, text associated with the training images extracted from internet merchants
(e.g., Amazon, eBay) typically explicitly defines a bounding volume for the object. While all these are of
interest, we consider here only the use of methods based implicitly on depth-from-focus (e.g., [8]), present
as camera intrinsics stored as metadata in the JPEG EXIF file format. Images collected by many modern
consumer-grade digital SLR cameras automatically store absolute distance-to-subject as metadata in the JPEG
image.

2

Figure 2: Illustration of metric object size derived from image metadata stored in EXIF fields on an
image downloaded from Flickr.com. Absolute size is estimated by projecting bounding box of local
features on object into 3-D using EXIF camera intrinsics stored in image file format.

improving the accuracy of recognizing categories that are at least approximately well modeled with
such-local feature schemes; size information alone cannot help recognize a category that does not
repeatably and reliably produce such features.
2.1

Metric object size from monocular metadata

Absolute pixel size can be infered using a planar object approximation and depth from focus cues.
Today?s digital cameras supplement the image data with rich meta-data provided in the EXIF format.
EXIF stores a wide range of intrinsic camera parameters, which often include the focus distance as
an explicit parameter (in some cameras it is not provided directly, but can be estimated from other
provided parameters). This gives us a workable approximation of the depth of the object, assuming
it is in focus in the scene: with a pinhole camera model, we can derive the metric size of a pixel
in the scene given these assumptions. Using simple trigonometry, the metric pixel size is ? = fsdr ,
where s is the sensor width, d is the focus distance, f is the focal length, and r is the horizontal
resolution of the sensor.
As shown in Figure 2, this method provides a size estimate reference for the visual observation based
on images commonly available on the internet, e.g., Flickr.com. A bounding box can either be estimated from the feature locations, given an uncluttered background, or provided by manual labeling
or by an object discovery technique which clusters local features to discover the segmentation of the
training data.
2.2

Naive Bayes estimation of discriminative feature weights

Our object model is based on a bag-of-words model where an object is encoded by a set of visual
features xi ? X within the circumscribing bounding box. Our size-constrained learning scheme
is applicable to a range of recognition methods; for simplicity we adopt a simple but efficient nonparametric naive Bayes scheme. We denote object appearance with p(X|C); following [1], this
density can be captured and modeled using Parzen window density estimates:
3

Figure 3: Metric object size for ten different categories derived from camera metadata. Bold symbols
depict ground truth obtained by direct physical measurement of category instance.

p?(x|C) =

N
1 X
K(x ? xC
j ),
N j=1

(1)

where K(.) is a Gaussian kernel.
We extend this model in a discriminative fashion similar to [18]. We compute the detection score for
a given bounding box from the log-likelihood ratio computed based on the kernel density estimate
from above. Assuming independence of the features, the class specfic probabilities are factorized to
obtain a sum of individual feature contributions:

log

p(X|C)
?
p(X|C)

Q
p(xi |C)
log Qi
?
i p(xi |C)
X
?
=
log(p(xi |C) ? log(p(xi |C))

=

(2)
(3)

i

As shown in [1], an approximate density based only on the nearest neighbor is accurate for many
recognition tasks. This further simplifies the computation and approximates the class specific feature
probabilities by:
log p(xi |C) ? ||xi ? N NC (xi )||2 ,

(4)

where N NC (xi ) represents the distance of data point xi to the nearest example in the training data
of class C. In the multi-class case, each feature xi is compared to the nearest neighbors in the
training examples of each class, N NC? (xi ) can be simply obtained as the minimum of all retrieved
nearest neighbors except those in C.

3

Efficient search with absolute size

Recently, a class of algorithms for efficient detection based on local features has been proposed
[19, 20, 21]; these search for the highest-scoring bounding box given the observed features X and a
4

scoring function f using an efficient branch-and-bound scheme. These methods can be formulated
as an optimization b = arg maxb f (b), where b = (x1 , y1 , x2 , y2 ) is a bounding box. The core idea
is to structure the search space using a search tree. The top node contains the set of all possible
bounding boxes. The child nodes contain splits of the set of bounding boxes in the parent node. The
leafs contain single bounding boxes. If it is possible to derive lower and upper bounds for rectangle
sets at the nodes, a branch and bound technique can be applied to quickly prune nodes if its upper
bound is lower than the lower bound of a previously visited node.
Bounds can be easily computed for bag-of-words representations, which have been previously used
in this context for object detection. Each feature has a learned weight wj , wherefore the score
function f reads:

f (r) =

X

wj ,

(5)

j?T (b)

where T (b) is the set of all features contained in the bounding box b.
While previous approaches have derived the feature weight from SVM training, we propose to use
likelihood ratios which are derived in a non-parametric fashion.
We further extend this method to search for objects in 3d. Our bounding box hypotheses b =
(x1 , y1 , z1 , x2 , y2 , z2 ) are defined explicitly in 3d and indicate the actual spatial relation of objects
in the scene.
We employ a constraint factor S(b) to the objective that indicates if a bounding box has a valid size
given a particular class or not:

f (r) =

X

wj S(b)

(6)

j?T (b)

S(b) = 1 is a basic rectangle function that takes the value 1 for valid bounding boxes and 0 otherwise.
Most importantly, bounds over bounding box sets can still be efficiently computed. As long as the
bounding box set at a given node in the search tree contains at least one bounding box of valid size,
the score is unaffected. When there is no valid rectangle left, the score evaluates to zero and that
node as well as the associated sub space of the search problem gets pruned.
At test time, it is anticipated that 3D observations are directly available via LIDAR scans or active
or passive stereo estimation. Given these measurements, we constrain the search to leverage the
metric information acquired at training time. The depth for each feature in the image at test time
allows us to infer their 3D location in the test scene. We can thus extend efficient multi-class branchand-bound search to operate in metric 3D space under the constraints imposed by our knowledge of
metric patch size and metric object size.
We also make use of the proposed multi-class branch-and-bound scheme as proposed in [20]. We
not only split bounding box sets along dimensions, but also split the set of object classes. This leads
to a simultaneous search scheme for multiple classes.

4

Related Work

Many methods have been proposed to deal with the problem of establishing feature correspondence
across varying image scales. Lowe et. al. proposed to up/downsample an image at multiple scales
and identify the characterstic scale for each image patch [9]. A histogram of edge orientations is
computed for each patch scaled to its characteristic scale in order to obtain a scale-invariant visual
descriptor. [10] identifies scale invariant regions by interatively expanding consistent regions with
an increasing intensity threshold until they become ?stable?. The size of the stable region is the
charactersitic scale for the feature. With both methods, a feature in one image can be mapped to the
same characteristic scale a feature in another image. Since both features are mapped to the same
5

scale, an ?apple-to-apple? comparison can be performed. In contrast, our method does not require
such a mapping. Instead, it determines the metric size of any image patch and uses it to compare
two features directly.
There have been several works on estimating depth from single images. Some very early work
estimated depth from the degree of the defocus of edges [8]. [6] describes a method to infer scene
depth from structure baesd on global and local histograms of Gabor filter responses for indoor and
outdoor scenes. [11] describes a supervised Markov Random Field method to predict the depth from
local and global features for outdoor images. In our work, we focus on indoor office scenes with
finer granularity. Hardware-based methods for obtaining 3D information from monocular images
include modifying the structure of a conventional camera to enable it to capture 3D geometry. For
example, [12] introduces the coded aperture technique by inserting a patterned occluder within the
aperture of the camera lenses. Images captured by such a camera exhibit depth-dependent patterns
from which a layered depth map can be extracted.
Most methods based on visual feature quantization learn their codebooks using invariant features.
However, the scale of each code word is lost after each image patch is normalized to its invariant region. Thus, it is possible for two features to match because they happen to look similar, even though
in the physical world they actually have two different sizes. For example, an eye of a dinosaur may
be confused with an eye of a fish, because their size difference is lost once they are embedded into
the visual code book. There have been some proposals to deal with this problem. For example,
[13] records the relative position of the object center in the codebook, and at test time each codebook word votes for the possible object center at multiple scales. Moreover, [14] explicitly put the
orientation and scale of each feature in the codebook, so that object center location can be inferred
directly. However, these works treat orientation and scale as independent of the feature descriptor
and use them to post-verify whether a feature found to be consistent in terms of the appearance
desciptor would also be consistent in terms of scale. In contrast, our work directly embeds the scale
attribute into the visual descriptor. A visual word would be matched only if its size is right. In other
words, the visual apperance and the scale are matched simulaneously in our codebook.
Depth information has been used to improve the performance of various image processing tasks,
such as video retrieval, object instance detection, 3D scene recognition, and vehicle navigation.
For example, [15] used depth feature for video retrieval, extracting depth from monocular video
sequences by exploiting the motion parallax of the objects in the video. [16] developed an intergrated
probablistic model for apperance and 3D geometry of object categories. However, their method
does not expliclty assign physical size to each image patch and needs to provide scale-invariance by
explictly calculating the perspective projection of objects in different 3D poposes. In contrast, our
method can infer the real-world sizes of features and can establish feature correspondences at their
true physical scale. [17] proposed a way to use depth estimation for real-time obstacle detection
from a monocular video stream in a vehicle navigation scenario. Their method estimates scene
depth from the scaling of supervised image regions and generates obstacle hypotheses from these
depth estimates.

5

Experiments

In the experiments we show how to improve performance of visual object classifiers by leveraging
richer sensor modalities deployed at test time. We analyze how the different proposed means of
putting visual recognition in metric context improves detection performance.
5.1

Data

For training we explore the camera-based metadata scheme described above, where we derive the
metric pixel size from EXIF data. We downloaded 38 images of 10 object categories taken with
a consumer grade dSLR that stores relevant EXIF fields (e.g., Nikon D90). For test data we have
collected 34 scenes in our laboratory of varying complexity containing 120 object instances in offices
and and a kitchen. Considerable levels of clutter, lighting and occlusion are present in the test set.
Stereo depth observations using a calibrated camera rig are obtained with test imagery, providing an
estimate of the 3-D depth of each feature point at test time.
6

Figure 4: Example detections.

7

object
bike helmet
body wash
juice
kleenex
mug
pasta
phone
pringles
toothpaste
vitamins
average

baseline
89.0
3.3
76.0
60.0
0.0
36.3
80.0
45.8
20.0
0.0
41.0

2.1D
99.1
80.0
100.0
76.53
24.63
65.6
65.7
94.3
100.0
60.0
76.59

Table 1: Average precision for several categories for baseline 2-D branch and bound search and our
2.1D method.
5.2

Evaluation

We start with a baseline, which uses the plain branch and bound detection scheme and 2D features.
We then experiment with augment the representation to 2.1D, adding 3D location to the interest
points, as well as employing the metric size constraint.
Table 1 shows the average precision for each category for baseline 2-D branch and bound search
and our 2.1D method. Adding the metric object constraints (second column) improves the results
significantly. As illustrated in Figure 4, our 2.1D representation allows grouping in 3-D and provides
improved occlusion handling. We see that the baseline branch-and-bound performs poorly on this
data set and is not capable of localizing two of the items at all. For the training data available
for these categories the local evidence was apparently not strong enough to support this detection
scheme, but with size constraints performance improved significantly.

6

Conclusion

Progress on large scale systems for visual categorization has been driven by the abundance of training data available from the web. Much richer and potentially more discriminative measurements can
be acquired and leveraged by additional sensor modalities, e.g. 3D measurements from stereo or
lidar, typically found on contemporary robotic platforms, but there is rarely sufficient training data
to learn robust models using these sensors. In order to reconcile these two trends, we developed a
method for appearance-based visual recognition in metric context, exploiting camera-based metadata to obtain size information regarding a category and local feature models that can be exploited
using 3-D sensors at test time.
We believe that ?size matters?, and that the most informative and robust aspect of 3-D information is
dimensional. We augmented local feature-based visual models with a ?2.1D? object representation
by introducing the notion of a metric patch size. Scene context from 3-D sensing and category-level
dimension estimates provide additional cues to limit search. We presented a fast, multi-class detection scheme based on a metric branch-and-bound formulation. While our method was demonstrated
only on simple 2-D SURF features, we belive these methods will be applicable as well to multikernel schemes with additional feature modalities, as well as object level desriptors (e.g., HOG,
LatentSVM).
Acknowledgements. This work was supported in part by TOYOTA and a Feodor Lynen Fellowship granted by the Alexander von Humboldt Foundation.

8

References
[1] O. Boiman, E. Shechtman, and M. Irani, In defense of Nearest-Neighbor based image classification, In Proceedings of Computer Vision and Pattern Recognition, 2008.
[2] C. Wu, B. Clipp, X. Li, J.-M. Frahm, and M. Pollefeys, 3D model matching with ViewpointInvariant Patches (VIP), In Proceedings of Computer Vision and Pattern Recognition, 2008.
[3] P. Scovanner, S. Ali, M. Shah, A 3-dimensional SIFT descriptor and its application to action
recognition, In Proceedings of the 15th international conference on Multimedia, 2007.
[4] M. Kortgen, G. J. Park, M. Novotni, R. Klein, 3D Shape Matching with 3D Shape Contexts, In
the 7th Central European Seminar on Computer Graphics, 2003.
[5] A. Frome, D. Huber, R. Kolluri, T. Bulow, and J. Malik. Recognizing objects in range data using
regional point descriptors, In Proceedings of the 8th European Conference on Computer Vision,
2004.
[6] A. Oliva, and A. Torralba, Building the Gist of a Scene: The Role of Global Image Features in
Recognition, In Visual Perception, Progress in Brain Research, vol 155, 2006.
[7] D. Hoiem, A. Efros, M. Hebert, Geometric Context from a Single Image, In Proceedings of the
Tenth IEEE International Conference on Computer Vision, 2005.
[8] T. Darrell and K. Wohn, Pyramid based depth from focus, In Proceedings of Computer Vision
and Pattern Recognition, 1988.
[9] D. Lowe, Distinctive Image Features from Scale-Invariant Keypoints, International Journal of
Computer Vision, 2004.
[10] J. Matas, O. Chum, and M. Urban, and T. Pajdla, Robust wide baseline stereo from maximally
stable extremal regions. In British Machine Vision Conference, 2002.
[11] A. Saxena, M. Sun, A. Y. Ng, Make3D: Learning 3-D Scene Structure from a Single Still
Image, In IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 2008.
[12] A. Levin, R. Fergus, F. Durand, Fr?edo, and W.T. Freeman, Image and depth from a conventional camera with a coded aperture, ACM Transactions on Graphics, 2007.
[13] Bastian Leibe and Ales Leonardis and Bernt Schiele, Combined Object Categorization and
Segmentation With An Implicit Shape Model In ECCV workshop on statistical learning in computer vision, 2004
[14] Krystian Mikolajczyk and Cordelia Schmid, A Performance Evaluation of Local Descriptors,
In PAMI, 2005.
[15] R. Ewerth, M. Schwalb, Martin, and B. Freisleben, Using depth features to retrieve monocular video shots, In Proceedings of the 6th ACM international conference on image and video
retrieval, 2007.
[16] E. Sudderth, A. Torralba, W. T. Freeman, and A. Wilsky, Depth from Familiar Objects: A
Hierarchical Model for 3D Scenes, In Proceedings of Computer Vision and Pattern Recognition,
2006.
[17] A. Wedel, U. Franke, J. Klappstein, T. Brox, and D. Cremers, Realtime Depth Estimation and
Obstacle Detection from Monocular Video, DAGM-Symposium, 2006.
[18] Junsong Yuan, Zicheng Liu and Ying Wu, Discriminative Subvolume Search for Efficient Action Detection, In Proceedings of Computer Vision and Pattern Recognition, 2009.
[19] Christoph H. Lampert and Matthew B. Blaschko and Thomas Hofmann, Efficient Subwindow
Search: A Branch and Bound Framework for Object Localization, In Transactions on Pattern
Analysis and Machine Intelligence (PAMI), 2009.
[20] Tom Yeh, John Lee and Trevor Darrell, Fast Concurrent Object Localization and Recognition,
In CVPR 2009.
[21] Junsong Yuan and Zicheng Liu and Ying Wu, Discriminative Subvolume Search for Efficient
Action Detection, In CVPR 2009.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 1378-using-expectation-to-guide-processing-a-study-of-three-real-world-applications.pdf

Using Expectation to Guide Processing:
A Study of Three Real-World Applications
Shumeet 8aluja
Justsystem Pittsburgh Research Center &
School of Computer Science, Carnegie Mellon University
baluja@cs.cmu.edu

Abstract
In many real world tasks, only a small fraction of the available inputs are important
at any particular time. This paper presents a method for ascertaining the relevance
of inputs by exploiting temporal coherence and predictability. The method proposed in this paper dynamically allocates relevance to inputs by using expectations
of their future values. As a model of the task is learned, the model is simultaneously extended to create task-specific predictions of the future values of inputs.
Inputs which are either not relevant, and therefore not accounted for in the model,
or those which contain noise, will not be predicted accurately. These inputs can be
de-emphasized, and, in turn, a new, improved, model of the task created. The techniques presented in this paper have yielded significant improvements for the
vision-based autonomous control of a land vehicle, vision-based hand tracking in
cluttered scenes, and the detection of faults in the etching of semiconductor wafers.

1 Introduction
In many real-world tasks, the extraneous information in the input can be easily confused
with the important features, making the specific task much more difficult. One of the
methods in which humans function in the presence of many distracting features is to selectively attend to only portions of the input signal. A means by which humans select where
to focus their attention is through the use of expectations. Once the important features in
the current input are found, an expectation can be formed of what the important features in
the next inputs will be, as well as where they will be. The importance of features must be
determined in the context of a specific task; different tasks can require the processing of
different subsets of the features in the same input.
There are two distinct uses of expectations. Consider Carnegie Mellon's Navlab autonomous navigation system. The road-following module [Pomerleau, 1993] is separate from
the obstacle avoidance modules [Thorpe, 1991]. One role of expectation, in which unexpected features are de-emphasized, is appropriate for the road-following module in which
the features to be tracked, such as lane-markings, appear in predictable locations. This use
of expectation removes distractions from the input scene. The second role of expectation,
to emphasize unexpected features, is appropriate for the obstacle avoidance modules. This
use of expectation emphasizes unanticipated features of the input scene.

2 Architectures for Attention
In many studies of attention, saliency maps (maps which indicate input relevance) have
been constructed in a bottom-up manner. For example, in [Koch & Ullman, 1985], a

s. Baluja

860

saliency map, which is not task-specific, is created by emphasizing inputs which are different from their neighbors. An alternate approach, presented in [Clark & Ferrier, 1992],
places mUltiple different, weighted, task-specific feature detectors around the input image.
The regions of the image which contain high weighted sums of the detected features are
the portion of the scene which are focused upon. Top-down knowledge of which features
are used and the weightings of the features is needed to make the procedure task-specific.
In contrast, the goal of this study is to learn which task-specific features are relevant without requiring top-down knowledge.
In this study, we use a method based on Input Reconstruction Reliability Estimation
(IRRE) [Pomerleau, 1993] to detennine which portions of the input are important for the
task. IRRE uses the hidden units of a neural network (NN) to perfonn the desired task and
to reconstruct the inputs. In its original use, IRRE estimated how confident a network's
outputs were by measuring the similarity between the reconstructed and current inputs.
Figure 1(Left) provides a schematic ofIRRE. Note that the weights between the input and
hidden layers are trained to reduce both task and reconstruction error.
Because the weights between the input and hidden layers are trained to reduce both task
and reconstruction error, a potential drawback of IRRE is the use of the hidden layer to
encode all of the features in the image, rather than only the ones required for solving the
particular task [Pomerleau, 1993]. This can be addressed by noting the following: if a
strictly layered (connections are only between adjacent layers) feed-forward neural network can solve a given task, the activations of the hidden layer contain, in some fonn, the
important infonnation for this task from the input layer. One method of detennining what
is contained in the hidden layer is to attempt to reconstruct the original input image, based
solely upon the representation developed in the hidden layer. Like IRRE, the input image
is reconstructed from the activations of the units in the hidden layer. Unlike IRRE, the hidden units are not trained to reduce reconstruction error, they are only trained to solve the
panicular task. The network's allocation of its limited representation capacity at the hidden layer is an indicator of what it deems relevant to the task. Information which is not relevant to the task will not be encoded in the hidden units. Since the reconstruction of the
inputs is based solely on the hidden units' activations, and the irrelevant portions of the
input are not encoded in the hidden units' activations, the inputs which are irrelevant to the
task cannot be reconstructed. See Figure I(Right).
By measuring which inputs can be reconstructed accurately, we can ascertain which inputs
the hidden units have encoded to solve the task. A synthetic task which demonstrates this
idea is described here. Imagine being given a lOxlO input retina such as shown in
Figure 2a&b. The task is to categorize many such examples into one of four classes.
Because of the random noise in the examples, the simple underlying process, of a cross
being present in one of four locations (see Figure 2c), is not easily discernible, although it
is the feature on which the classifications are to be based. Given enough examples, the NN
will be able to solve this task. However, even after the model of the task is learned, it is
difficult to ascertain to which inputs the network is attending. To detennine this, we can
freeze the weights in the trained network and connect a input-reconstruction layer to the
hidden units, as shown in Figure 1(Right). After training these connections, by measuring
where the reconstruction matches the actual input, we can detennine which inputs the network has encoded in its hidden units, and is therefore attending. See Figure 2d.
weights
trained to
reduce task
error only
weights
trained to reduce
reconstruction
error only.
error.

Figure 1:

weights
trained to
reduce task
error only.

(Left) IRRE. (Right) Modified IRRE.

"-

weights
trained to reduce
reconstruction
error only.

Using Expectation to Guide Processing

B:

861

C:

D:
2

3

4

+

ir+

Figure 2: (A & B): Samples of training data (cross appears in position 4 & 1 respectively). Note the large
amounts of noise. (C): The underlying process puts a cross in one of these four locations. (D): The black
crosses are where the reconstruction matched the inputs; these correspond exactly to the underlying process.
IRRE and this modified IRRE are related to auto-encoding networks [Cottrell, 1990] and
principal components analysis (PeA). The difference between auto-encoding networks
and those employed in this study is that the hidden layers of the networks used here were
trained to perfonn well on the specific task, not to reproduce the inputs accurately.

2.1 Creating Expectations
A notion of time is necessary in order to focus attention in future frames. Instead of reconstructing the current input, the network is trained to predict the next input; this corresponds to changing the subscript in the reconstruction layer of the network shown in
Figure 1(Right) from t to t+ 1. The prediction is trained in a supervised manner, by using
the next set of inputs in the time sequence as the target outputs. The next inputs may contain noise or extraneous features. However, since the hidden units only encode infonnation
to solve the task, the network will be unable to construct the noise or extraneous features
in its prediction.
To this point, a method to create a task-specific expectation of what the next inputs will be
has been described. As described in Section 1, there are two fundamentally different ways
in which to interpret the difference between the expected next inputs and the actual next
inputs. The first interpretation is that the difference between the expected and the actual
inputs is a point of interest because it is a region which was not expected. This has applications in anomaly detection; it will be explored in Section 3.2. In the second interpretation, the difference between the expected and actual inputs is considered noise. Processing
should be de-emphasized from the regions in which the difference is large. This makes the
assumption that there is enough infonnation in the previous inputs to specify what and
where the important portions of the next image will be. As shown in the road-following
and hand-tracking task, this method can remove spurious features and noise.

3 Real-World Applications
1bree real-world tasks are discussed in this section. The first, vision-based road following,
shows how the task-specific expectations developed in the previous section can be used to
eliminate distractions from the input. The second, detection of anomalies in the plasmaetch step of wafer fabrication, shows how expectations can be used to emphasize the unexpected'features in the input. The third, visual hand-tracking, demonstrates how to incorporate a priori domain knowledge about expectations into the NN.

3.1 Application 1: Vision-Based Autonomous Road Following
In the domain of autonomous road following, the goal is to control a robot vehicle by analyzing the image of the road ahead. The direction of travel should be chosen based on the
location of important features like lane markings and r~ad edges. On highways and dirt
roads, simple techniques, such as feed-forward NNs, have worked well for mapping road
images to steering commands [Pomerleau, 1993]. However, on city streets, where there
are distractions like old lane-.narkings, pedestrians, and heavy traffic, these methods fail.
The purpose of using attention in this domain is to eliminate features of the road which the
NN may mistake as lane markings. Approximately 1200 images were gathered from a

862

S. Baluja

E

F

Figure 3: (Top) : Four samples of training images . Left most
shows the position of the lane-marking which was hand-marked.
(Right): In each triplet: Left: raw input imagtt. Middle: the
network's prediction of the inputs at time t; this prediction was
made by a network with input ofimaget_I ' Right: a pixel-by-pixel
filtered image (see text). This image is used as the input to the NN.

G

camera mounted on the left side of the CMU-Navlab 5 test vehicle, pointed downwards
and slightly ahead of the vehicle. The car was driven through city and residential neighborhoods around Pittsburgh, PA. The images were gathered at 4-5 hz. The images were
subsampled to 30x32 pixels. In each of these images, the horizontal position of the lane
marking in the 20th row of the input image was manually identified. The task is to produce
a Gaussian of activation in the outputs centered on the horizontal position of the lane
marking in the 20th row of the image, given the entire input image. Sample images and
target outputs are shown in Figure 3. In this task, the ANN can be confused by road edges
(Figure 3a), by extraneous lane markings (Figure 3b), and reflections on the car itself
(since the camera was positioned on the side of the car), as shown in Figure 3c.
The network architecture shown in Figure 4 was used; this is the same architecture as in
Figure l(right) with the feedback shown. The feedback is used during both training and
simulation. In each time-step, a steering direction and a prediction of the next inputs is
produced. For each time-step, the magnitude of the difference' between the input's
expected value (computed in the previous time-step) and its actual value is computed.
Each input pixel can be moved towards its background value l in proportion to this difference-value. The larger the difference value, the more weight is given to the background
value. If the difference value is small, the actual inputs are used. This has the effect of deemphasizing the unexpected inputs.
The results of using this method were very promising. The lane tracker removed distracting features from the images. In Figure 3G, a distracting lane-marking is removed: the
lane marker on the right was correctly tracked in images before the distractor lane-marker
appeared. In Figure 3F, a passing car is de-emphasized: the network does not have a model
to predict the movement of passing cars, since these are not relevant for the lane-marker
detection task. In Figure 3E, the side of the road appears brighter than expected; therefore
it is de-emphasized. Note that the expectation-images (shown in the middle of each triplet

weights
trained to
reduce task
error only.

Delayed I time step

I ...~~....

__

weights
--- trained to reduce
prediction
error only.

m~tii;:mrt;,._--1 Weight bkgd.

and actual
inputs according to
difference ima e.

Figure 4:
t - - - t Difference

between
inputs t &
predicted in uts

Architecture used
to track the lane
marking in
cluttered scenes.

Signal Transfer
(Connections are not trainable)
I. A simple estimate of the background value for each pixel is its average activation across the training set. For
the road-following domain, it is possible to use a background activation of 0.0 (when the entire image is scaled to
activations of +1.0 to -1.0) since the road often appears as intermediate grays.

Using Expectation to Guide Processing

863

in Figure 3) show that the expected lane-marker and road edge locations are not precisely
defined. This is due to the training method, which attempts to model the many possible
transitions from one time step to the next to account for inter- and intra-driver variability
with a limited training set [Baluja, 1996].
In summary, by eliminating the distractions in the input images, the lane-tracker with the
attention mechanisms improved performance by 20% over the standard lane-tracker, measured on the difference between the estimated and hand-marked position of the lanemarker in each image. This improvement was seen on multiple runs, with random initial
weights in the NN and different random translations chosen for the training images.

3.2 Application 2: Fault Detection in the Plasma-Etch Wafer Fabrication
Plasma etch is one of the many steps in the fabrication of semiconductor wafers. In this
study, the detection of four faults was attempted. Descriptions of the faults can be found in
[Baluja, 1996][Maxion, 1996]. For the experiments conducted here, only a single sensor
was used, which measured the intensity of light emitted from the plasma at the 520nm
wavelength. Each etch was sampled once a second, providing approximately 140 samples
per wafer wavefonn. The data-collection phase of this experiment began on October 25,
1994, and continued until April 4, 1995. The detection of faults is a difficult problem
because the contamination of the etch chamber and the degradation parts keeps the sensor's outputs, even for fault-free wafers, changing over time. Accounting for machine state
should help the detection process.
Expectation is used as follows: Given the waveform signature of waferT_I' an expectation
of waferT can be fonned. The input to the prediction-NN is the wavefonn signature of
waferT_I; the output is the prediction of the signature of waferT. The target output for each
example is the signature of the next wafer in sequence (the full 140 parameters). Detection
of the four faults is done with a separate network which used as input: the expectation of
the wafer's wavefonn, the actual wafer's wavefonn, and the point-by-point difference of
the two. In this task, the input is not filtered as in the driving domain described previously;
the values of the point-by-point difference vector are used as extra inputs.
The perfonnance of many methods and architectures were compared on this task, details
can be found in [Baluja, 1996]. The results using the expectation based methods was a
98.7% detection rate, 100% classification rate on the detected faults (detennining which of
the four types of faults the detected fault was), and a 2.3% false detection rate. For comparison, a simple perceptron had an 80% detection rate, and a 40% false-detection rate. A
fully-connected network which did not consider the state of the machine achieved a 100%
detection rate, but a 53% false detection rate. A network which considered state by using
the last-previous no-fault wafer for comparison with the current wafer (instead of an
expectation for the current wafer) achieved an 87.9% detection rate, and a 1.5% falsedetection rate. A variety of neural and non-neural methods which examined the differences between the expected and current wafer, as well those which examined the differences between the last no-fault wafer and the current wafer, perfonned poorly. In
summary, methods which did not use expectations were unable to obtain the false-positives and detection rates of the expectation-based methods.

3.3 Application 3: Hand-Tracking in Cluttered Scenes
In the tasks described so far, the transition rules were learned by the NN. However, if the
transition rules had been known a priori, processing could have been directed to only the
relevant regions by explicitly manipulating the expectations. The ability to incorporate a
priori rules is important in many vision-based tasks. Often the constraints about the environment in which the tracking is done can be used to limit the portions of the input scene
which need to be processed. For example, consider visually tracking a person's hand.
Given a fast camera sampling rate, the person's hand in the current frame will be close to

864

S. Baluja

B.

A.

Figure 5:
Typical input images used for
the hand-tracking experiments.
The target is to track the
subject's right hand. Without
expectation, in (A) both hands
were found in X outputs, and
the wrong hand was found in
the Y outputs. In (8) Subject's
right hand and face found in
the X outputs.

where it appeared in the previous frame. Although a network can learn this constraint by
developing expectations of future inputs (as with the NN architecture shown in Figure 4),
training the expectations can be avoided by incorporating this rule directly.
In this task, the input layer is a 48*48 image. There are two output layers of 48 units; the
desired outputs are two gaussians centered on the (X,Y) position of the hand to be tracked.
See Figure 5. Rather than creating a saliency map based upon the difference between the
actual and predicted inputs, as was done with autonomous road following, the saliency
map was explicitly created with the available domain knowledge. Given the sampling rate
of the camera and the size of the hand in the image, the salient region for the next timestep was a circular region centered on the estimated location of the hand in the previous
image. The activations of the inputs outside of the salient region were shifted towards the
background image. The activations inside the salient region were not modified. After
applying the saliency map to the inputs, the filtered inputs were fed into the NN.
This system was tested in very difficult situations; the testing set contained images of a
person moving both of his hands and body throughout the sequence (see Figure 5). Therefore, both hands and body are clearly visible in the difference images used as input into
the network. All training was done on much simpler training sets in which only a single
hand was moving. To gauge the perfonnance of an expectation-based system, it was compared to a system which used the following post-processing heuristics to account for temporal coherence. First, before a gaussian was fit to either of the output layers, the
activation of the outputs was inversely scaled with the distance away from the location of
the hand in the previous time step. This reduces the probability of detecting a hand in a
location very different than the previous detection. This helps when both hands are
detected, as shown in Figure 5. The second heuristic was that any predictions which differ
from the previous prediction by more than half of the dimension of the output layer were
ignored, and the previous prediction used instead. See Table I for the results. In summary,
by using the expectation based methods, perfonnance improved from 66% to 90% when
tracking the left hand, and 52% to 91 % when tracking the right hand.
Table I: Performance: Number of frames in which each hand was located (283 total images).

Method
No Heuristics, No Expect.
Heuristics
Expectation
Expectation + Heuristics

Target: Find Left Hand

Target: Find Right Hand

Which Hand Was Found

Which Hand Was Found

% Correct

L

52%
66%
91%
90%

146
187
258
256

R
44
22
3
3

None

% Correct

L

93
74
22
24

16%
52%
90%
91%

143
68
3
2

R
47
147
255
257

None
93
68
25
24

[Nowlan & Platt, 1995] presented a convolutional-NN based hand-tracker which used separate NNs for intensity and differences images with a rule-based integration of the multiple network outputs. The integration of this expectation-based system should improve the
performance of the difference-image NN.

Using Expectation to Guide Processing

865

4 Conclusions
A very closely related procedure to the one described in this paper is the use of Kalman
Filters to predict the locations of objects of interest in the input retina. For example, Dickmanns uses the prediction of the future state to help guide attention by controlling the
direction of a camera to acquire accurate position of landmarks [Dickmanns, 1992].
Strong models of the vehicle motion, the appearance of objects of interest (such as the
road, road-signs, and other vehicles), and the motion of these objects are encoded in the
system. The largest difference in their system and the one presented here is the amount of
a priori knowledge that is used. Many approaches which use Kalman Filters require a
large amount of problem specific information for creating the models. In the approach presented in this paper, the main object is to automatically learn this information from examples. First, the system must learn what the important features are, since no top-down
information is assumed. Second, the system must automatically develop the control strategy from the detected features. Third, the system must also learn a model for the movements of all of the relevant features.
In deciding whether the approaches described in this paper are suitable to a new problem,
two criteria must be considered. First, if expectation is to be used to remove distractions
from the inputs, then given the current inputs, the activations of the relevant inputs in the
next time step must be predictable while the irrelevant inputs are either unrelated to the
task or are unpredictable. In many visual object tracking problems, the relevant inputs are
often predictable while the distractions are not. In the cases in which the distractions are
predictable, if they are unrelated to the main task, these methods can work. When using
expectation to emphasize unexpected or potentially anomalous features, the activations of
the relevant inputs should be unpredictable while the irrelevant ones are predictable. This
is often the case for anomaly/fault detection tasks. Second, when expectations are used as
a filter, it is necessary to explicitly define the role of the expected features. In particular, it
is necessary to define whether the expected features should be considered relevant or irrelevant, and therefore, whether they should be emphasized or de-emphasized, respectively.
We have demonstrated the value of using task-specific expectations to guide processing in
three real-world tasks. In complex, dynamic, environments, such as driving, expectations
are used to quickly and accurately discriminate between the relevant and irrelevant features. For the detection of faults in the plasma-etch step of semiconductor fabrication,
expectations are used to account for the underlying drift of the process. Finally, for visionbased hand-tracking, we have shown that a priori knowledge about expectations can be
easily integrated with a hand-detection model to focus attention on small portions of the
scene, so that distractions in the periphery can be ignored.
Acknowledgments
The author would like to thank Dean Pomerleau, Takeo Kanade, Tom Mitchell and Tomaso Poggio
for their help in shaping this work.
References
Baluja, S. 1996, Expectation-Based Selective Attention. Ph.D. Thesis, School of Computer Science, CMU.
Clark, J. & Ferrier, N (1992), Attentive Visual Servoing, in: Active Vision. Blake & Yuille, (MIT Press) 137-154.
Cottrell, G.W., 1990, Extracting Features from Faces using Compression Network, Connectionist Models, Morgan Kaufmann 328-337.
Dickmanns, 1992, Expectation-based Dynamic Scene Understanding, in: Active Vision. A. Blake & A.Yuille,
MIT Press .
Koch, C. & Ullman, S. (1985) "Shifts in Selective Visual Attention: Towards the Underlying Neural Circuitry",
in: Human Neurobiology 4 (1985) 219-227.
Maxion, R. (1995) The Semiconductor Wafer Plasma-Etch Data Set.
Nowlan, S. & Platt, J., 1995, "A Convolutional Neural Network Hand Tracker". NIPS 7. MIT Press . 901-908.
Pomerleau, D.A., 1993. Neural Network Perception for Mobile Robot Guidance, Kluwer Academic.
Thorpe, C., 1991, Outdoor Visual Navigation for Autonomous Robots, in: Robotics and Autonomous Systems 7.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

