query sentence: Neural-network
---------------------------------------------------------------------
title: 516-neural-network-routing-for-random-multistage-interconnection-networks.pdf

neural network rout random multistag interconnect network mark w. goudreau princeton univers nee research institut inc independ way princeton nj lee gile nec research institut inc independ way princeton nj abstract rout scheme use neural network develop aid establish point-to-point communic rout multistag interconnect network min neural network network type examin hopfield hopfield work problem establish rout random min rmin shared-memori distribut comput system address perform neural network rout scheme compar two tradit approach exhaust search rout greedi rout result suggest neural network router may competit certain rmin introduct neural network develop aid establish point-topoint communic rout multistag interconnect network min goudreau gile interconnect network wide studi huang siegel rout problem great interest due broad applic although neural network rout scheme accommod mani type communic system work concentr use shared-memori distribut comput system neural network sometim use solv certain interconnect network neural network rout random multistag interconnect network input port output port interconnect network control bit logic1 neural network interconnect logic2 network control i externa control figur communic system neural network router input port processor left output port memori modul right problem find legal rout brown hakim meadow increas throughput interconnect network brown liu marrakchi troudet neural network router subject work howev differ signific router special design handl parallel process system min random interstag connect random min call rmin rmin tend greater fault-toler regular min problem allow set processor access set memori modul rmin pictur communic system neural network router shown figur processor memori modul system assum synchron begin messag cycl set processor may desir access set memori modul job router establish mani desir connect possibl non-conflict manner obtain optim solut critic stymi processor may attempt communic subsequ messag cycl combin speed qualiti solut import object work discov neural network router could competit type router term qualiti solut speed resourc goudreau gile rmin2 rmini rmin3 figur three random multistag interconnect network block shown crossbar switch input may connect output util end neural scheme rout rmin rout so far result router may inde practic larg network rout scheme compar two name exhaust search rout greedi investig suggest neural network altern rout rmin exhaust search rout exhaust search rout method optim term abil router find best solut mani way implement such router one approach describ given interconnect network everi rout input output store databas rmin use test case paper alway least one rout processor memori modul new messag cycl began new messag set present router router would search databas combin rout messag set conflict conflict said occur one rout set rout use singl bus interconnect network case everi combin rout messag set conflict router would find combin rout could establish largest possibl number desir connect possibl rout messag algorithm need memori size mnk worst case take exponenti time respect size neural network rout random multistag interconnect network messag set consequ impract approach rmin provid conveni upper bound perform router greedi rout greedi rout appli messag connect establish one time onc rout establish given messag cycl may remov greedi rout alway provid optim rout solut greedi rout algorithm use requir rout databas exhaust search router howev select combin rout follow manner new messag set present router choos one desir messag look first rout messag 's list rout router establish rout next router examin second messag assum second desir messag request see one rout second messag 's rout list establish without conflict alreadi establish first messag such rout exist router establish rout move next desir messag worst case speed greedi router quadrat respect size messag set neural network rout focal point neural network router neural network type examin hopfield hopfield problem establish set non-conflict rout reduc constraint satisfact problem structur neural network router complet determin rmin when new set rout desir certain bias current network chang neural network rout scheme also certain fault-toler properti describ neural network calcul rout converg legal rout array legal rout array 3-dimension therefor element rout array three indic element ai i k equal messag rout output port stage we say row if i column if i final they rod if legal rout array satisfi follow three constraint one one element column equal element success column equal repres output port connect interconnect network one element rod equal first restrict ensur messag rout one one output port stage interconnect network second restrict ensur messag rout legal path goudreau gile interconnect network third restrict ensur resourc content interconnect network resolv word one messag use certain output port certain stage interconnect network when three constraint met rout array provid legal rout messag messag set like rout array neural network router natur 3-dimension structur ai j k rout array repres output voltag neuron at begin messag cycl neuron random output voltag if neural network settl one global minima problem solv continu time mode network chosen simul digit neural network neuron input neuron ui input bias current ii output vi input ui convert output vi sigmoid function neuron influenc neuron connect repres similar neuron affect neuron connect iij order liapunov function equat construct iij must equal7ji we assum iii synchron updat model also time constant denot t. equat describ output neuron duo ln t dt t=rc g uj e-x equat forc neural net stabl state local minima approxim energi equat inn 2l iij vi v'i ii i=l neural network weight iii 's set bias current it output voltag vari minim e. let number messag messag set let number stage rmin let number port per stage may function stage number below energi function implement three constraint discuss e1 e2 p=l vm i p neural network rout random multistag interconnect network ea s-l m=l p=l tt i=l vm i jim j vm ij pm vm s ij arbitrari posit constant el ea handl first constraint rout array e4 deal second constraint e2 ensur third equat function sl pl p2 repres distanc output port pi stage sl output port p2 from stage if pi connect p2 stage sl distanc may set zero if pi p2 connect stage sl distanc may set one also sourc address messag f3m destin address messag entir energi function solv connect bias current valu shown equat result follow equat oml m2 1m p kroneck delta when otherwis essenti approach promis neural network act parallel comput hope neural network generat solut much faster convent approach rout rmin neural network use standard problem name global minimum alway reach but serious difficulti typic when global minim energi reach neural network desir rout calcul other even local minim solut may partial solv rout problem consequ would seem particular encourag type applic type neural network applic tradit problem reach global minimum may hurt system 's perform much expect speed neural network calcul solut a great asset ifor simul a chosen empir valu a goudreau gile tabl rout result rmin shown figur calcul due comput complex rmin1 rmin2 entri rmin3 eel egr enn eel egr enn eel egr enn neural network router use a larg number neuron if input port output port each stage rmin upper bound number neuron need s. often howev number neuron actual requir much smaller upper bound it shown empir neural network the type use con verg a solut essenti constant time exampl claim made the neural network describ takefuji lee a slight variat the model use simul result figur show three rmin examin the rout result the three rout scheme shown in tabl eel repres the expect number messag rout use exhaust search rout egr greedi rout enn neural network rout valu function the size the messag set m. onli messag set obvious conflict examin exampl messag set could two processor tri communic the memori modul the tabl show at least three rmin the three rout scheme produc solut similar virtu in case the neural network router appear outperform the suppos optim exhaust search router the eel egr valu calcul test everi messag set size enn calcul test random generat messag set size m. the neural network router appear perform best it must gotten messag set easier rout averag in general the perform the neural network router degener the size the rmin increas it felt the neural network router in present form scale well for larg rmin this work shown larg neural network the type use difficulti converg a valid solut hopfield neural network rout for random multistag interconnect network conclus the result show there much differ in term qualiti solut for the three rout methodolog work relat small sampl rmin the exhaust search approach clear a practic approach sinc it time consum but when consid the asymptot analys for these three methodolog one keep in mind the perform degrad the greedi router the neural network router the size the rmin increas greedi rout neural network rout would appear valid approach for rmin moder size but sinc asymptot analysi a limit signific the best way compar the speed these two rout scheme would to build actual implement sinc the neural network router essenti calcul the rout in parallel it reason hope a fast analog implement for the neural network router may find solut faster the exhaust search router even the greedi router thus the neural network router may a viabl altern for rmin that larg
----------------------------------------------------------------

title: 1162-experiments-with-neural-networks-for-real-time-implementation-of-control.pdf

experi neural network real time implement control p. k. campbel m. dale h. l. ferra a. kowalczyk telstra research laboratori blackburn road clayton vic australia p.campbel m.dale h.ferra a.kowalczyk trl.oz.au abstract paper describ neural network base control alloc capac telecommun network system propos order overcom real time respons constraint two basic architectur evalu feedforward network-heurist feedforward network-recurr network architectur compar linear program optimis benchmark lp optimis also use teacher label data sampl feedforward neural network train algorithm found system abl provid traffic throughput respect throughput obtain linear program solut onc train neural network base solut found fraction time requir lp optimis introduct among mani virtu neural network effici term execut time requir memori store structur practic abil approxim complex function typic drawback usual data hungri train algorithm howev train data comput generat line problem may overcom mani applic algorithm use generat solut may impract implement real time case neural network substitut becom crucial feasibl project paper present preliminari result non-linear optim problem use neural network applic question capac alloc optic communic network work area continu far explor possibl applic bandwidth alloc sdh network synchron digit hierarchi sdh new standard digit transmiss optic fibr adopt australia europ equival sonet synchron optic network standard north america architectur particular sdh network research paper shown figur node peripheri sdh network switch handl individu call p. campbel m. dale h. l. ferra a. kowalczyk switch concentr traffic anoth switch number stream stream transfer digit cross-connect dxc switch transmiss destin alloc one sever altern virtual path task hand dynam alloc capac virtual path order maxim sdh network throughput non-linear optim task sinc virtual path capac constraint physic limit capac link dxc 's quantiz object function erlang block depend high non-linear fashion alloc capac demand task solv optim use classic linear program techniqu approach time-consum larg sdh network task could even requir hour complet one major featur sdh network remot reconfigur use softwar control reconfigur sdh network becom necessari traffic demand vari failur occur dxc 's link connect reconfigur case failur must extrem fast need restor time ms output path capac synapt weight hidden unit gate threshold use input link capac dxc digit cross-connect offer traffic switch figur exampl inter-c sdh/sonet network topolog use experi exampl architectur mask perceptron generat experi particular case three virtual path alloc pair switch use differ set link dxc 's sdh network call one switch anoth sent along virtual path lead path total switch switch path path capac normal set give predefin throughput known steadi state link sdh network becom partial damag complet cut oper sdh network move away steadi state path capac must reconfigur satisfi traffic demand subject follow constraint capac integ valu unit correspond mb/s stream erlang total capac virtual path anyon link sdh network experi neural network real time implement control exceed physic capac link neural network train data consist link capac traffic demand valu repres situat oper one link degrad complet partial output data consist integ valu repres differ steadi state path capac final alloc path capac previous work problem optim sdh network reconfigur research alreadi particular gopal propos heurist greedi search algorithm solv nonlinear integ program problem herzberg reformul non-linear integ optim problem linear program task herzberg bye investig applic simplex algorithm solv lp problem whilst bye consid applic hopfield neural network task final lecki use anoth set ai inspir heurist solv optim task approach practic defici linear program slow heurist approach relat inaccur hopfield neural network method simul serial comput suffer problem previous paper campbel investig applic mask perceptron problem reconfigur toy sdh network work present expand work paper idea use second stage mask perceptron recurr mode reduc link violationslunderutil neural control architectur instead use neural network solv optim task substitut simplex algorithm taught replic optim lp solut provid decid use two stage approach experi first stage develop feedforward network abl produc approxim solut precis use collect random exampl linear program solut capac alloc pre-comput develop feedforward neural network abl approxim solut new exampl approxim neural network solut round nearest integ satisfi constraint use seed second stage provid refin enforc constraint second stage experi initi use heurist modul base gopal approach heurist first reduc capac assign path caus physic capac violat link subsequ increas capac assign path across link under-util also investig approach second stage use anoth feedforward neural network teach signal second stage neural network differ output first stage neural network alon combin first stage neural networkiheurist solut time input data consist link usag valu either link violat underutil valu repres amount traffic lost per path current capac alloc second stage neural network output repres correct first stage neural network output second stage neural network run recurr mode adjust small step current alloc link capac therebi attempt iter move closer combin neural-heurist solut remov link violat under-util left behind first stage network setup use simul shown figur each particular instanc test network initialis solut first stage neural network offer traffic demand avail maximum link capac use determin extent link violat underutil well amount lost traffic demand satisfact this data form initi input second stage network output neural network use check qualiti p. campbel m. dale h. l. ferra a. kowalczyk solut iter continu either link violat occur preset maximum number iter perform offer traffic link capac comput constraint-demand satisfact solut solut correct i initi solut stage demand satisfact input link capac violat underutil input figur recurr network use second stage experi comput constraint satisfact output neural network combin round give integ link violations/under-util this mean mani case small correct made network discard improv possibl order overcom this introduc scheme wherebi error link violations/under-util occasion amplifi allow network chanc remov this scheme work follow instanc iter either link violat iter perform link violat still present size error multipli amplif factor maximum iter perform if subsequ link violat persist amplif factor increas procedur repeat either link violat remov amplif factor reach fix valu descript neural network generat first stage feedforward neural network mask perceptron figur each input pass number arbitrarili chosen binari threshold unit there total threshold input task mask perceptron train algorithm select set use threshold hidden unit thousand possibl then set weight minim mean-square-error train set mask perceptron train algorithm automat select unit direct connect output unit hidden unit gate whose experi neural network real time implement control output connect neural network output give connect such neural network rapid simul sinc oper requir comparison addit recurr network use second stage also use mask perceptron train algori thin use recurr network first stage particular note gradual adapt employ input network pass arbitrarili chosen binari threshold unit select train algorithm direct connect output unit via weight link result result present tabl figur valu tabl repres traffic throughput sdh network respect method percentag throughput determin lp solut both neural network train use instanc test differ set instanc howev recurr network approxim case still link violat simul valu tabl valid solut obtain either train test set solut type feedforward net/heurist feedforward net/recurr net gopal- gopal-o train test number train test instanc recurr network achiev solut link violat simul describ section tabl effici solut measur averag fraction optim throughput lp solut comparison implement two sole heurist algorithm we refer gopal- gopal-o both employ scheme describ earlier gopal heurist differ two gopal- use steadi state solut initi start point determin virtual path capac degrad network wherea gopal-o start point all path capac initi set zero refer figur link capac ratio denot total link capac degrad sdh network relat total link capac steadi state sdh network low valu link capac ratio indic heavili degrad network traffic throughput ratio denot ratio throughput obtain method question throughput steadi state solut each dot graph figur repres one test set case it clear figur neural network/heurist approach abl find better solut heavili degrad network each approach overal cluster dot neural network/heurist combin tighter y-direct closer method result recurr network encourag qualit quit close gopal- algorithm all experi run sparcstat neural network train took minut dure simul neural network took averag ms per test case ms heurist total ms averag gopal- algorithm requir ms gopal-o algorithm requir ms per test case recurr network solut requir averag ms per test case optim solut calcul use linear program algorithm took second per case sparcstat p. campbel m. dale h. l. ferra a. kowalczyk neural network/heurist recurr neural network link capac ratio ra cr cr link capac ratio gopal-o gopal- link capac ratio link capacili ratio figur experiment result inter-c sdh network independ test set random case axi we ratio total link capac degrad sdh network steadi state sdh network axi we ratio throughput obtain method question throughput steadi state solut fig show result neural network combin heurist second stage fig show result recurr neural network second stage fig show result heurist initialis steadi state gopal- fig result heurist initialis zero gopal-o discuss conclus combin neural network/heurist approach perform well across whole rang degre sdh network degrad test result obtain this paper consist found the averag accuraci fast solut generat time ffj ms highlight this approach as possibl candid implement real system especi one consid the easili achiev speed increas avail parallel the neural network the mask perceptron use these experi well suit simul on dsp hardwar the oper requir comparison calcul logic the summat synapt weight multipl non-linear transfonn requir the interest thing note the relat good perfonn the recurr network name it abl handl case achiev good perfonn when compar the neural network/heurist solut the qualiti the teacher one thing bear mind the heurist approach high tune produc solut satisfi the constraint chang the capac one link time the desir goal achiev on the hand the recurr network generic target the constraint in such specif manner make quit crude global chang in experi neural network real time implement control one hit yet still abl achiev reason level perform while the speed for the recurr network lower on averag for the heurist solut in experi this a major problem sinc mani improv still possibl the result report preliminari serv show possibl it plan continu the soh network experi in the futur investig on the recurr network for the second stage also more complex sdh architectur acknowledg the research develop report the activ support various section individu within the telstra research laboratori especi dr. c. lecki mr. p. sember dr. m. herzberg mr. a. herschtal dr. l. campbel the permiss the manag director research inform technolog telstra publish this paper acknowledg the research develop report the activ support various section individu within the telstra research laboratori especi dr. c. lecki mr. p. sember respons for the creation triall the program design produc the test train data the soh applic possibl due co-oper a number colleagu in trl in particular dr. l. campbel suggest this particular applic dr. m. herzberg and mr. a. herschtal the permiss of the manag director research and inform technolog telstra publish this paper acknowledg
----------------------------------------------------------------

title: 211-a-large-scale-neural-network-which-recognizes-handwritten-kanji-characters.pdf

large-scal neural network large-scal neural network recogn handwritten kanji charact yoshihiro mori kazuki joe atr auditori visual percept research laboratori sanpeidani inuidani seika-cho soraku-gun kyoto japan abstract propos new way construct large-scal neural network handwritten kanji charact recognit neural network consist part collect small-scal network train individu small number kanji charact network integr output small-scal network process facilit integr nework recognit rate total system compar small-scal network our result indic propos method effect construct large-scal network without loss recognit perform introduct neural network appli recognit task mani field good result denker they perform better convent method howev network current oper categori japanes write system present compos charact network recogn mani charact must given larg number categori maintain level perform train small-scal neural network difficult task therefor explor method integr small-scal neural network import construct large-scal network if method could integr small-scal network without loss perform scale neural network would extend dramat paper propos method construct large-scal network whose object recogn handwritten kanji charact report result part network method limit system charact recognit appli system recogn mani categori strategi large-scal network know current recognit general capac neural network realiz construct large-scal monolith network would effici mori joe effect instead start decid build block approach two strategi mix mani small-scal network select neural network snn strategi large-scal neural network made mani small-scal network train individu small number categori network snn select appropri small-scal network i advantag strategi inform pass select small-scal network alway appropri network therefor train small-scal network easi hand increas number categori substanti increas train time snn may make harder snn retain high perfonn furthennor error rate snn limit perfonn whole system integr neural network inn strategi large-scal neural network made mani small-scal network train individu small number categori network inn integr output small-scal network fig advantag this strategi everi small-scal network get inform contribut find right answer therefor possibl use knowledg distribut among small-scal network but respect various devic need make integr easier common advantag strategi mention size neural network relat small take long time train network small-scal network consid independ part whole system therefor retrain network improv perform whole system take long o utput sub net neural network select type suspend network snn strategi large-scal neural network output neural network integr type inn strategi structur large-scal network whole system construct use three kind neural network ftrst one call subnet ordinari three layer feed forward type neural network train use back propag learn algorithm second kind network call supernet this neural network make decis integr output subnet this network also 3-layer feed-forward net but larger subnet last network we call otherfilt devis improv integr upernet this otherfilt network design use vq algorithm there also chang made bp learn algorithm especi for pattern recognit we decid base time take for learn there categori small-scal network charact separ small group k-mean cluster method allow similar charact group togeth separ occur two stage first group charact form group separ smaller unit this way group charact obtain we choos inn strategi use distribut knowledg full advantag 9-charact unit subnet integr stage first subnet integr a higher level network supernet altogeth supernet need recogn charact supernet in turn integr a higher level network hypernet more precis role structur kind network follow subnet a featur vector extract handwritten pattern use input describ in section number unit in output layer number categori recogn subnet in short role a subnet output the similar the input pattern the categori allot the subnet supernet the output each subnet filter the otherfilt network use the input mori joe the supernet the number unit in output layer the the number subnet belong a supernet in shortt the role of supernet select the subnet which cover the categori correspond to the input pattern output horizont diagon vertic origin pattern ubnet otherfiit
----------------------------------------------------------------

title: 1193-a-new-approach-to-hybrid-hmmann-speech-recognition-using-mutual-information-neural-networks.pdf

new approach hybrid hmmjann speech recognit use mutual inform neural network g. rigol neukirchen gerhard-mercator-univers duisburg faculti electr engin depart comput scienc bismarckstr duisburg germani abstract paper present new approach speech recognit hybrid hmm/ann technolog standard approach hybrid hmmiann system base use neural network posterior probabl estim new approach base use mutual inform neural network train special learn algorithm order maxim mutual inform input class network result sequenc fire output neuron train shown paper neural network optim neural vector quantize discret hidden markov model system train maximum likelihood principl one main advantag approach fact neural network easili combin hmm 's complex context-depend capabl shown result hybrid system achiev high recognit rate alreadi level best convent hmm system continu paramet capabl mutual inform neural network yet entir exploit introduct hybrid hmm/ann system deal optim combin artifici neural network ann hidden markov model especi area automat speech recognit shown hybrid approach lead power effici system combin discrimin capabl neural network superior dynam time warp abil hmm 's popular hybrid approach describ hochberg replac compon model emiss probabl hmm neural net possibl shown mutual in/orm neural networks/or hybrid hmmiann speech recognit bourlard neural network train output m-th neuron approxim posterior probabl p qmix paper altern method construct hybrid system present base use discret hmm 's combin neural vector quantize order form hybrid system speech featur vector present neural network generat fire neuron output layer neuron process vq label hmm follow argument altern hybrid approach neural vector quantize train special inform theori criterion base mutual inform network input result neuron fire sequenc shown network optim acoust processor discret hmm system result profound mathemat theori approach result theori formula deriv joint describ behavior hmm neural acoust processor way system describ unifi manner major compon hybrid system train use unifi learn criterion mention theoret background lead develop new neural network paradigm use novel train algorithm use area neurocomput therefor repres major challeng issu learn train neural system neural network easili combin hmm system arbitrari complex lead combin optim train neural network power hmm 's featur use speech recognit triphon function word crossword triphon etc context-depend desir relat difficult realiz pure neural approach left hmm result hybrid system still basic structur discret system therefor effect featur associ discret system quick easi train well recognit procedur real-tim capabl etc work present paper also success implement demand speech recognit problem word speaker-independ continu resourc manag speech recognit task task hybrid system produc one best recognit result obtain speech recognit system follow section theoret foundat hybrid approach briefli explain unifi probabilist model combin hmmiann system deriv describ interact neural hmm compon furthermor shown optim neural acoust processor obtain special inform theoret network train algorithm inform theori principl neural network train consid neural network arbitrari topolog use neural vector quantize discret hmm system pattern present hybrid system train featur vector result pattern use featur extract method denot featur vector present input layer neural network network generat one fire neuron present henc present generat stream fire neuron length result output layer neural net label stream denot label stream present hmm 's calcul probabl stream observ while pattern certain class present system assum differ class activ g. rigol c. neukirchen system word phonem speech recognit featur vector belong one class class om featur vector belong denot major train issu neural network formul follow how weight network train network produc stream fire neuron use discret hmm 's optim way known hmm 's usual train inform theori method most reli maximum likelihood principl paramet hybrid system transit emiss probabl network weight summar vector probabl denot probabl pattern discret time assumpt it generat model repres class paramet set ml principl tri maxim joint probabl present train pattern accord follow maximum likelihood function fl arg max log i q k j optim paramet vector maxim equat goal feed featur vector neural network present neural network output markov model therefor one introduc neural network output suitabl manner formula vector present network input layer assum there chanc neuron yn network output layer size fire certain probabl output probabl written p kiq p x iq p iq p x iy combin neural compon hmm made obvious typic probabl p yniq describ markov model term emiss probabl hmm instanc continu paramet hmm 's probabl interpret weight gaussian mixtur case semi-continu system discret hmm 's probabl serv discret emiss probabl codebook label probabl p xiyn q describ acoust processor system character relat vector input acoust processor label yn consid n-th output compon acoust processor n-th output compon may character n-th gaussian mixtur compon continu paramet hmm generat n-th label vector quantize discret system probabl often consid independ class express p xiyn it exact probabl model effici neural network case vector serv input neural network yn character n-th neuron output layer network use bay law probabl written p ynik pw p xl yn p i i yield use bay law express mutual inform neural network hybrid hmmiann speech recognit one obtain p .qli p ynlo j we modifi class-depend probabl featur vector way allow incorpor probabl p ynix probabl allow better character behavior neural network it describ probabl various neuron yn vector present network input therefor probabl give good descript input/output behavior neural network therefor consid probabilist model hybrid system neural acoust processor character input/output behavior two case distinguish first case neural network assum probabilist paradigm each neuron fire certain probabl if input vector present case neuron contribut inform forward hmm 's alreadi mention paper second possibl case consid name one neuron output layer fire fed observ label hmm case we determinist decis probabl p ynix describ neuron yn fire if vector present input layer therefor probabl reduc yield class-depend probabl p xln express probabl p niyn involv direct fire neuron featur vector present one turn back recal fact equat describ fact markov model train ml criterion it also recal entir sequenc featur vector result label stream fire neuron fire neuron if k-th vector present neural network now substitut each present yield modifi ml criterion arg ax log p q i arg ax log p x iog usual continu paramet system probabl express lp k ly p therefor depend paramet vector ft case p xiyn interpret probabl provid gaussian distribut paramet g. rigol c. neukirchen gaussian depend ft mention discret system one fire neuron yn surviv result fact n*-th member remain sum would correspond one fire gaussian continu case lead follow express p x iy nj p nj p k nj p n lx consid now fact acoust processor repres gaussian instead vector quantize probabl p yn*ix fire neuron equal then reduc it becom obvious probabl affect distribut depend paramet vector ft would differ if p yn*ix would binari characterist would comput continu function this case would depend paramet vector ft thus without consider remain express maxim reduc arg ax iog arg max log i log log i these expect logarithm probabl also defin entropi therefor also written arg max i this equat interpret follow term right side also known mutual inform probabilist variabl nand h yi.o therefor final inform theory-bas train criterion neural network formul follow synapt weight neural network chosen maxim mutual inform string repres class vector present network input layer train string repres result sequenc fire neuron output layer neural network this also express maximum mutual inform mmi criterion neural network train this conclud proof mmi neural network inde optim acoust processor hmm 's train maximum likelihood principl realize mmi train algorithm neural network train synapt weight neural network order achiev mutual inform maxim easi two differ algorithm develop this task briefli outlin this paper detail descript found rigol neukirchen first experi use single-lay neural network euclidean distanc propag function first implement mmi train paradigm realiz rigol base self-organ procedur start initi weight deriv k-mean cluster train vector follow iter procedur modifi weight mutual inform increas self-organ way low valu start much higher valu sever iter cycl second implement realiz mutual inform neural network hybrid hmmiann speech recognit recent describ detail neukirchen it base idea use gradient method find mmi valu this techniqu use maximum search find fire neuron output layer prevent calcul deriv this maximum search approxim use softmax function denot sn n-th neuron it comput activ zl neuron it sn=e zi it small valu paramet approxim crisp maximum select sinc string alway fix train independ paramet ft function h niy minim this function also express i p i deriv respect weight wlj neural network yield ah jw/j as shown neukirchen requir term comput effect it possibl realiz gradient descend method in order maxim mutual inform train data great advantag this method fact it now possibl general this algorithm use in popular neural network architectur includ multilay recurr neural network result hybrid system new hybrid system develop extens test use resourc manag word speaker-independ continu speech recognit task first baselin discret hmm system built well-known featur context-depend hmm system perform baselin system shown in column tabl 1st column show perform hybrid system neural vector quantize this network special featur mention in previous section it use multipl frame input train contextdepend class mean mutual inform stream fire neuron correspond input stream triphon maxim in this way fire behavior network becom sensit context-depend unit therefor this network may exist context-depend acoust processor carri principl triphon model the hmm structur the acoust front end it seen substanti higher recognit perform obtain the hybrid system compar well the lead continu system htk in column it expect the system improv in the near futur various addit featur includ full exploit multilay neural vq g. rigol c. neukirchen sever convent hmm improv the use crossword triphon recent result the larger wall street journal wsj databas shown a error rate the hybrid system compar a error rate for a standard discret system use the 5k vocabulari test bigram languag model perplex this error rate reduc use crossword triphon a trigram languag model this rate compar alreadi quit favor the best continu system for the task it note this hybrid wsj system still in initi stage the neural compon yet as sophist as in the rm system conclus a new neural network paradigm the result hybrid hmmiann speech recognit system present in this paper the new approach perform alreadi well still perfect it gain good perform the follow fact the use inform theory-bas train algorithm for the neural vector quantize shown optim for the hybrid approach the possibl introduc context-depend the hmm 's also the neural quantize the fact that this hybrid approach allow the combin of optim neural acoust processor the advanc context-depend hmm system we continu implement various possibl improv for our hybrid speech recognit system
----------------------------------------------------------------

title: 1968-not-bounding-the-true-error.pdf

bound true error john langford depart comput scienc carnegie-mellon univers pittsburgh pa jcl+ cs.cmu.edu rich caruana depart comput scienc cornel univers ithaca ny caruana cs.cornell.edu abstract present new approach bound true error rate continu valu classifi base upon pac-bay bound method first construct distribut classifi determin sensit paramet model nois true error rate stochast classifi found sensit analysi tight bound use pac-bay bound paper demonstr method order magnitud artifici neural network result improv best determinist neural net bound introduct machin learn import know true error rate classifi achiev futur test case estim error rate supris difficult exampl known bound true error rate artifici neural network tend extrem loos often result meaningless bound alway err error rate paper bound true error rate neural network instead bound true error rate distribut neural network creat analys one neural network henc titl approach prove much fruit tri bound true error rate individu network best current approach often requir exampl produc nontrivi bound true error rate produc nontrivi bound true error rate stochast neural network less exampl stochast neural network neural network weight perturb gaussian varianc everi time evalu approach use pac-bay bound approach thought redivis work experiment theoretician make experiment work harder theoretician true error bound becom much tighter extra work part experiment signific tractabl result bound much tighter altern viewpoint classif problem find hypothesi low upper bound futur error rate present post-process phase neural network result classifi much lower upper bound futur error rate post-process use artifici neural net train optim method requir learn procedur modifi re-run even threshold function differenti fact post-process step easili adapt learn algorithm david mackay done signific work make approxim bayesian learn tractabl neural network work complimentari rather competit exhibit techniqu like give nontrivi true error rate bound bayesian neural network regardless approxim prior model error verif statement work progress post-process step find larg distribut classifi small averag empir error rate given averag empir error rate straightforward appli pac-bay bound order find bound averag true error rate find larg distribut classifi perform simpl nois sensitivi analysi learn model nois model allow us generat distribut classifi known small averag empir error rate paper refer distribut neural net result nois analysi stochast neural net model whi expect pac-bay bound signific improv standard cover number vc bound approach exist learn problem differ lower bound pac-bay upper bound tight number train exampl superior guarante made typic cover number bound gap best known asymptot constant guarante pac-bay bound sometim quit tight encourag us appli next section describ bound compar describ algorithm construct distribut neural network present experiment result theoret setup work standard supervis batch learn set set start assumpt exampl drawn fix unknown distribut input input output pair output drawn space space arbitrari goal machin learn use sampl set pair find classifi map input space output space small true error sinc distribut unknown true error rate observ howev observ empir error rate now basic quantiti interest defin first present modern neural network bound special pac-bay bound stochast neural network stochast neural network simpli neural network weight neural network drawn distribut whenev use describ techniqu construct distribut stochast neural network neural network bound compar special best current neural network true error rate bound approach neural network bound describ term follow paramet margin arbitrari function unrel neural network sigmoid function defin linear upper bound sum magnitud weight th layer neural network lipschitz constant hold th layer neural network lipschitz constant bound magnitud deriv size input space paramet defin get follow bound theorem layer feed-forward neural network true error bound egf hji l 9m5npqso t8uwvyx z 1b 1b proof given theorem actual given univers constant might right choic educ guess neural network true error bound perhap tightest known bound general feed-forward neural network natur bound compar layer feed-forward bound easili appli tight manner calcul priori weight bound patch use principl structur risk minim particular state bound non-neg integ constant th bound hold probabl bound hold simultan probabl sinc appli approach valu ef get follow theorem theorem layer feed-forward neural network true error bound hji no t8 x9z 1b 1b proof appli union bound possibl valu discuss report valu tightest applic bound practic use stochast neural network bound approach start simpl refin origin pac-bay bound first special bound stochast neural network show use bound conjunct post-process algorithm result much tighter true error rate upper bound first need defin paramet theorem distribut hypothes found exampl depen dent manner distribut hypothes chosen priori without depend exampl true error rate stochast hypothesi evalu draw hypothesi output averag empir error rate stochast hypothesi ba cd ba fcg he now readi state theorem ef kl kl kl kullback-leibl diverg distribut kl coin bias kl diverg coin bias theorem pac-bay relat entropi bound prior ha sr ut nioi jii wx za ioi ha m nioi lk qp proof given need special theorem applic stochast neural network choic prior prior zero neural net structur one train multidimension isotrop gaussian valu weight neural network multidimension gaussian mean varianc dimens choic made conveni happen work optim valu unknown depend learn problem wish parameter exampl depend manner use trick origin neural net bound use sequenc bound constant nonneg number th bound set now union bound impli bound hold simultan probabl least now assum posterior also defin multidimension gaussian mean varianc dimens defin special follow corollari corollari stochast neural network bound let number weight neural net th weight varianc th weight kl 9m5npo jii ha proof analyt calcul kl diverg two multidimension gaussian union bound appli valu choos reason default valu one step necessari order appli bound essenti difficulti evalut quantiti observ although calcul high precis difficult avoid need direct evalu mont carlo evalu bound tail mont carlo evalu let observ rate failur random hypothes drawn accord appli random train exampl follow simpl bound hold theorem sampl converg bound distribut sampl set kl x z ioi ha number evalu stochast hypothesi proof simpli an applic chernoff bound tail binomi head occur an error observ bias order calcul bound expect true error rate first bound expect empir error rate za confid then bound expect true error rate sinc total probabl failur confid use bound practic use bound hold probabl evalu empir error rate stochast neural network distribut construct algorithm one critic step miss descript calcul multidimension gaussian varianc posterior gaussian need depend weight order achiev tight bound sinc want meaningless weight contribut signific overal sampl complex use simpl greedi algorithm find appropri varianc dimens train neural net exampl everi weight search varianc reduc empir accuraci stochast neural network fix target percentag use hold weight fix error error snn bound nn bound snn train error nn train error snn test error nn test error pattern present pattern present figur plot measur error error bound neural network stochast neural network snn synthet problem train set case reduct empir error note true error bound visibl graph left impli least exampl requir order make nonvacu bound graph right expand vertic scale exclud poor true error bound error curv nn snn qualit similar train test set expect snn consist perform wors nn train set easier see graph right surpris snn perform wors nn less test set nn snn exhibit overfit pattern present epoch shape snn bound rough mimic shape empir measur true error this visibl graph right thus might use indic net begin overfit stochast neural network defin general too-larg empir error therefor calcul global multipli stochast neural network defin decreas empir accuraci absolut error rate use step then evalu empir error rate result stochast neural net repeat draw sampl stochast neural network work report use sampl experiment result how well bound true error rate stochast neural network answer much better bound true error rate neural network use two dataset empir evalu qualiti new bound first synthet dataset input dimens one output dimens dimens useless simpli random number drawn gaussian one input dimens depend label first label drawn uniform then special dimens drawn gaussian note this learn problem solv perfect exampl drawn tail gaussian overlap ideal neural net use solv this synthet problem singl node perceptron instead use layer neural net hidden node use sigmoid transfer function this over complex neural net result potenti signific overfit make bound predict problem interest also somewhat realist neural net structur exact match learn problem second dataset adult problem uci machin learn repositori we use layer neural net hidden unit this problem well preliminari experi show net this small overfit adult dataset train sampl small keep thing challeng we use exampl our experi error error snn bound nn bound snn train error nn train error snn test error nn test error pattern present pattern present figur plot measur error error bound neural network stochast neural network snn uci adult dataset graph show result obtain use reduct empir error instead reduct use figur train sampl size this problem case nn snn exhibit overfit approxim pattern present epoch figur true error bound impli least exampl requir order make nonvacu bound graph right expand vertic scale exclud poor true error bound we see figur construct nonvacu bound continu hypothesi space exampl quit difficult convent bound hopeless loos figur show result synthet problem this problem we use train case reduct empir error result adult problem present figur this problem we use train case a reduct empir error experi perform these problem use somewhat smaller larger train sampl yield similar result choic reduct empir error somewhat arbitrari we see qualit similar result if we switch a reduct synthet problem a reduct adult problem there sever thing worth note result two figur snn upper bound order magnitud lower nn upper bound tight might desir snn upper bound order magnitud better vacuous snns perform somewhat better expect particular synthet problem snn true error rate most wors true error rate nn true error rate estim use larg test set this supris consid we fix differ empir error rate synthet problem similar adult problem we observ true error rate snn nn typic half target differ this good suggest we lose much accuraci might expect creat snn test problem shape snn bound somewhat similar shape true error rate particular local minima snn bound occur rough local minima true error rate occur snn bound may weak predict overfit point snn nn net comparison neural network bound stochast neural network bound quit fair due form bound in particular stochast neural network bound never return a valu greater alway err this impli bound near valu difficult judg how rapid extra exampl improv stochast neural network bound we judg sampl complex stochast bound plot valu numer in equat figur plot complex versus number pattern present in train in this figur we complex complex pattern present figur we plot complex stochast network model numer equat train epoch note complex increas train expect stay impli nonvacu bound a train set size observ expect result complex numer equat increas train signific less number exampl stochast bound a radic improv neural network bound yet a perfect tight bound given we a perfect tight bound one import consider aris minimum stochast bound predict minimum true error rate predict a larg holdout dataset in particular we use stochast bound determin we ceas train stochast bound depend upon the complex increas train time the train error decreas train time this depend result in a minima occur approxim pattern present both our test problem the point minim true error the stochast determinist neural network occur approxim pattern present the synthet problem pattern present for the adult problem indic the stochast bound weak predict the point minimum error the neural network bound minimum the choic increas empir error optim in general the optim choic the extra error rate depend upon the learn problem sinc the stochast neural network bound corollari hold for multidimension gaussian distribut we free optim the choic distribut in anyway we desir figur show the result bound for differ choic posterior the bound a minimum extra error indic our initi choic in the right ballpark may unnecessarili larg larger differ in empir error rate easier obtain reliabl fewer sampl the stochast neural net we difficulti use as sampl the snn as small as a increas in empir error also note the complex alway decreas increas entropi in the distribut our stochast neural net the exist a minimum in figur the right behaviour the increas empir error rate signific in the calcul the true error bound conclus we appli a pac-bay bound for the true error rate a stochast network neural the stochast neural network bound result in a radic tighter order mag true error bound complex stochast nn bound complex extra train error figur plot the stochast neural net snn bound for posterior distribut chosen accord the extra empir error introduc nitud bound the true error rate a classifi while increas the empir true error rate a small amount although stochast neural net bound complet tight vacuous theexampl the minima the bound weak predict the point overtrain occur the result with two dataset one synthet one uci extrem promis the bound order magnitud better our next step test the method on dataset use a greater varieti net architectur to insur the bound remain tight in addit there remain mani opportun for improv the applic the bound for exampl it is possibl shift the weight find a maximum accept varianc result in a tighter bound also we not taken account symmetri within the network would allow for a tighter bound calcul
----------------------------------------------------------------

title: 503-refining-pid-controllers-using-neural-networks.pdf

refin pin control use neural network gari m. scott depart chemic engin johnson drive univers wisconsin madison wi jude w. shavlik depart comput scienc w. dayton street univers wisconsin madison wi w. harmon ray depart chemic engin johnson drive univers wisconsin madison wi abstract kbann approach use neural network refin knowledg written form simpl proposit rule extend idea present manncon algorithm mathemat equat govern pid control determin topolog initi weight network train use backpropag appli method task control outflow temperatur water tank produc statistically-signific gain accuraci standard neural network approach non-learn pid control furthermor use pid knowledg initi weight network produc statist less variat testset accuraci compar network initi small random number introduct research design neural network process control larg ignor exist knowledg task hand one form knowledg often call domain theori take embodi tradit control paradigm scott shavlik ray recently-develop kbann knowledge-bas artifici neural network approach towel address issu task domain theori written use simpl proposit rule avail basi approach use exist knowledg determin appropri network topolog initi weight network begin learn process good start point paper describ manncon multivari artifici neural network control algorithm method use tradit control paradigm determin topolog initi weight network use pid control way elimin network-design problem choic network topolog number hidden unit reduc sensit network initi valu weight furthermor initi configur network closer final state would normal randomly-configur network thus manncon network perform better consist standard randomly-initi three-lay approach task examin learn control multiple-input multiple-output mimo system number reason investig task use neural network one usual involv nonlinear input-output relationship match nonlinear natur neural network two number success applic neural network task bhat mcavoy jordan jacob miller final there number exist control paradigm use determin topolog initi weight network control network manncon algorithm use proportional-integral-deriv pid control stephanopoulo one simplest tradit feedback control scheme basi construct initi neural network control basic idea pid control control action vector proport error integr error time tempor deriv error sever tune paramet determin contribut various compon figur depict result network topolog base pid control paradigm first layer network y p desir process output setpoint actual process output past time step calcul simpl error simpl vector differ e=i p-i accomplish second layer calcul actual error pass pid mechan effect layer act steady-st pre-compens ray gie produc current error error signal past two time step compens constant matrix i valu interact steadi state various control loop elimin final layer control output/pl input calcul control action refin pid control use neural network fd td den water tank yen wco who wci whi wc2 wh2 figur manncon network show weight initi use ziegler-nichol tune paramet base veloc form discret pid control uc n uc n-l wcoci n wcici n-l wc2 wca wcb wc2 constant determin tune paramet control loop similar set equat constant who whi exist control loop figur show schemat water tank ray network control figur also show control variabl fc tank output variabl disturb variabl fd td control measur disturb repres nois system mann con initi weight figur network va.lu mimic behavior pid control tune ziegler-nichol paramet stephanopoulo particular oper condit use kbann approach towel add weight network unit layer connect unit subsequ layer initi weight small random number sever order magnitud smaller weight determin pid paramet scale input output network rang initi weight network manner given assum activ function unit network linear scott shavlik ray cold stream fe hot stream th dis urban ce fd td i temperatur flow rate output i i figur stir mix tank requir outflow temperatur control tabl topolog initi network network standard neural network manncon network i manncon network topolog 3-layer hidden unit pid topolog pid topolog weight initi random random z-n tune strength neural network howev lie nonlinear typic sigmoid activ function reason manncon system initi set weight bias unit linear respons dictat pid initi approxim sigmoid output rang unit unit output rang activ function becom exp wji wjioi linear weight describ onc manncon configur initi weight network use set train exampl backpropag improv accuraci network weight initi pid inform well initi small random number chang backpropag train experiment detail compar perform three network differ topolog and/or method initi tabl summar network topolog weight initi method network tabl pid topolog network structur shown figur random weight initi set refin pid control use neural network tabl rang averag durat setpoint experi experi train set instanc instanc instanc test set instanc instanc instanc weight small random number center around zero we also compar network non-learn pid control we train network use backpropag randomly-determin schedul setpoint ysp disturb chang repeat setpoint repres desir output valu control maintain temperatur outflow tank disturb repres nois inflow rate temperatur disturb stream magnitud setpoint disturb form gaussian distribut center number train exampl chang setpoint disturb exponenti distribut we perform three experi characterist train and/or test set differ tabl summar rang setpoint well averag durat data set experi seen experi train set test set qualit similar experi test set longer durat setpoint experi train set restrict subrang test set we period interrupt train test network result averag run scott we use error output tank figur determin network error propag error backward plant psalti method error signal input tank given 8u yi netui 8y oui 8yj repres simpl error output water tank 8ui error signal input tank sinc we use model process real tank we calcul partial deriv process model equat result figur compar perform three network experi seen manncon network show increas correct standard neural network approach statist analysi error use t-test show differ signific confid level furthermor differ perform manncon network i manncon network scott shavlik ray standard neural network manncon network i mann con network pid control non-learn train instanc figur mean squar error network testset function number train instanc present experi signific differ varianc test error differ run signific confid level final manncon network perform signific better confid level non-learn pid control perform standard neural network repres best sever trial vari number hidden unit rang second observ figur manncon network learn much quick standard neural-network approach manncon network requir signific fewer train instanc reach perform level within final error rate experi tabl summar final mean error well number train instanc requir achiev perform within valu experi we see signific gain correct man ncon network standard neural network approach confid level well non-learn pid control confid level experi manncon network initi z-n tune also learn signific quicker confid level standard neural network futur work one question whether introduct extra hidden unit network would improv perform give network room learn concept outsid given domain theori addit extra hidden unit well remov unneed unit area much ongo research refin pid control use neural network tabl comparison network perform i mean squar error i train instanc method experi standard neural network mann con network i mann con network pid control tune fix control action experi standard neural network mann con network i mann con network pid control tune fix con trol action experi standard neural network mann con network i mann con network pid control tune fix control action indic true valu lie within bound confid level the valu given fix control action repres the error result fix the control action level produc output steadi state ring rapid chang control action occur the train network futur enhanc approach would creat network architectur prevent ring perhap limit the chang the control action relat small valu anoth import goal approach the applic real-world process the water tank project illustr the approach quit simpl much difficult problem contain signific time delay exist explor there sever control paradigm could use as a basi for network construct initi there sever differ digit control as deadbeat dahlin 's stephanopoulo could use place the digit pid control use in this project dynam matrix control dmc pratt intern model control imc garcia morari also candid for consider for this approach final neural network general consid black box in inner work complet uninterpret sinc the neural network in this approach initi inform may possibl interpret the weight the network extract use inform the train network scott shavlik ray conclus we describ the manncon algorithm use the inform a pid control determin a relev network topolog without resort trialand-error method in addit the algorithm initi the weight prior knowledg give the backpropagt algorithm appropri direct in continu learn final we shown use the manncon algorithm signific improv the perform the train network in the follow way improv mean testset accuraci less variabl run faster rate learn better general extrapol abil acknowledg this materi base upon work partial support a nation scienc foundat graduat fellowship scott offic naval research grant nation scienc foundat grant
----------------------------------------------------------------

title: 40-neural-networks-for-template-matching-application-to-real-time-classification-of-the-action-potentials-of-real-neurons.pdf

neural network templat match applic real-tim classif action potenti real neuron yiu-fai wongt jashojiban banikt jame m. bower tdivis engin appli scienc divis biolog california institut technolog pasadena ca abstract much experiment studi real neural network reli proper classif extracellulari sampl neural signal action potenti record brain experiment anim neurophysiolog laboratori classif task simplifi limit investig singl electr well-isol neuron record one time howev interest sampl activ mani singl neuron simultan waveform classif becom serious concern paper describ constrast three approach problem design recogn isol neural event also separ classifi tempor overlap event real time first present two formul waveform classif use neural network templat match approach two formul compar simpl templat match implement analysi real neural signal reveal simpl templat match better solut problem either neural network approach introduct mani year neurobiologist studi nervous system use singl electrod serial sampl electr activ singl neuron brain howev physiologist theorist becom awar complex nonlinear dynam network becom appar serial sampl strategi may provid inform necessari understand function organ addit like necessari develop new techniqu sampl activ multipl neuron simultaneouslyl over last sever year develop two differ method acquir multineuron data initi design involv placement mani tini micro electrod individu tight pack pseudo-flo configur within brain recent develop sophist approach util recent advanc silicon technolog fabric multi-port silicon base electrod use electrod expect abl readili record activ pattern larger number neuron research multi-singl neuron record techniqu continu becom clear whatev techniqu use acquir neural signal mani brain locat technic difficulti associ sampl data compress store analyz interpret signal larg dwarf develop sampl devic report specif consid need assur neural action potenti also known spike mani parallel record channel correct classifi one aspect problem post-process multi-singl neuron data tradit singl electrode/singl neuron record task usual american institut physic volv pass analog signal schmidt trigger whose output indic occur event comput time trigger oscilloscop sweep analog data experiment visual monitor oscilloscop verifi accuraci discrimin well-discrimin signal singl neuron overlap success oscilloscop trace ic obvious approach impract larg number channel record time instead necessari autom classif procedur paper describ contrast three approach develop trace upper layer iv trace ume msec lower layer c. record s~e 75sq.jllt1 silicon probe develop lababoratori multi-singl unit record cerebellar cortex complet probe surfac view one record tip sever superimpos neuron action potenti record silicon electrod ill cerebellar cortex princip design object assur neural waveform adequ discrimin multipl channel technic overal object research project sampl mani singl neuron possibl therefor natur extent effort develop neural waveform classif scheme robust enough allow us distinguish activ aris one neuron per record site howev determin particular signal neural origin also sever possibl neuron aros general signal differ neuron differ waveform aid classif neuron record channel fire simultan near simultan produc novel combin waveform also need classifi last complic particular bedevil previous effort classifi neural signal review see also see summari object design circuit would distinguish differ waveform even though neuron discharg tend quit similar shape recogn waveform even though unavoid movement anim respir often result period chang amplitud record signal move brain relat tip electrod consider robust record nois variabl corrupt neural record resolv overlap waveform like particular interest event neurobiolog point view provid real-tim perform allow experiment detect problem discrimin monitor progress experi implement hardwar due need classifi neural signal mani channel simultan simpli duplic software-bas algorithm channel work rather multipl small independ programm hardwar devic need construct i jl.v signal record electrod schemat diagram electrod record two neuron cell bodi actual multi-neuron record note similar two waveform overlap event synthes data differ nois level test classificat.ion algorithm nsr nsr method problem detect classifi multipl neural signal singl voltag record involv two step first waveform present particular signal must identifi templat generat second waveform must detect classifi ongo data record accomplish first step modifi princip compon analysi procedur describ abel goldstein automat extract templat distinct waveform found initi sampl digit analog data discuss mean accomplish second step concern us specif paper compar three new approach ongo waveform classif deal explicit overlap spike variabl meet design criteria outlin approach consist modifi templat match scheme two appli neural network implement first consid neural network approach point nomenclatur avoid confus follow real neuron whose signal want classifi refer neuron while comput element appli neural network call hopon neural network approach overal problem classifi neural waveform best seen optim problem presenc nois much recent work neural-typ network algorithm demonstr network work quit well problem sort particular recent paper hopfield tank describ a/d convert network suggest map problem templat match similar context energi function network propos form i vi tij connect hopon hopon voltag output hopon ii input current hopon hopon sigmoid input-output characterist exp equat motion set du fdt oe/ov jvj ii see de/dt i itijvj ii dv/dt du/dt dv/dt henc go minimum network construct describ correspond propos solut particular waveform classif problem templat match use hopfield-typ neural net taken follow approach templat match use neural network simplic initi restrict classif problem one involv two waveform accord construct neural network made two group hopon concern discrimin one waveform classif procedur work follow first schmidt trigger use detect presenc voltag signal channel set threshold threshold cross impli presenc possibl neural signal msec data around cross store buffer sampl khz note biophys limit assur singl real neuron discharg time period one waveform particular type occur data sampl also action potenti order msec durat msec window includ full signal singl overlap waveform next step explain later data valu correl pass hopfield network design minim mean-squar error actual data linear combin differ delay templat hopon set hopon concern one waveform repres particular tempor delay occurr waveform buffer express network term energi function formul let input waveform amplitud tth time bin sj amplitud ph templat vjk denot sj k j th templat delay time bin present input waveform appropri energi function first term design minim mean-squar error specifi best match sinc second term minim vjk assum valu also set diagon element tij third term creat mutual inhibit among process node evalu neuron signal describ occur per sampl expand simplifi express connect matrix input current seen input correl actual data various delay templat subtract constant term modifi hopfield network document detail full hopfield-typ network work well tempor isol spike moder nois level overlap spike local minima problem sever two waveform network need build network hardwar full hopfield network difficult implement current technolog reason develop modifi neural network approach signific reduc necessari hardwar complex also improv perform understand work let us look inform contain quantiti tij iij 3a 3b make use quantiti calcul pre-process stage load hopfield network calcul quantiti quick rule larg number possibl templat combin signific reduc size problem thus use much smaller henc effici neural network find optim solut make deriv simpl defin slight modifi version iij 4a two-templ case iij si case overlap spike cross-correl si differ delay ii 's cross-correl input weight combin si si overlap first templat time bin delay second templat time bin delay iijl howev presenc nois ij ident zero equal nois simpl algorithm may make unaccept error solut problem overlap spike describ let us consid problem classifi non-overlap spike case compar input cross-correl auto-correl 4c lsi ls~ non-overlap case if si if absenc nois minimum ij repres correct classif howev presenc nois none these quantiti ident zero equal nois input give rise unaccept error solut nois relat problem choos minima three chosen case instead one minimum either known correspond linear combin templat overlap case simpl templat non-overlap case three neuron hopfield-typ network program so neuron correspond case input fed tini network resolv whatev confus remain first step cross-correl comparison note simpl templat match describ also use place tini hopfield type network simpl templat match evalu perform these neural network approach decid implement simpl templat match scheme now describ howev document approach turn accur requir least complex hardwar three approach first step fill buffer data base detect possibl neural signal then calcul differ record waveform possibl combin two previous identifi templat formal consist calcul distanc input possibl case generat combin two templat j ix sl jonl ix sl ix t dmin min dij d~ dn dm n give best fit possibl combin templat actual voltag signal test procedur compar perform three approach devis common set test data use follow procedur first use princip compon method abel goldstein generat two templat digit analog record neural activ record cerebellum rat two actual spike waveform templat decid use peak-to-peak ratio second set analog record made site cerebellum action potenti event evid determin spectral characterist record nois these two compon deriv real neural record then digit combin object construct realist record while also know absolut correct solut templat match problem occur spike shown 2c data set correspond differ nois signal ratio construct also carri simul amplitud templat vari synthes record simul waveform chang due brain movement often seen real record addit two waveform test set also construct three waveform set generat third templat averag first two templat further quantifi comparison three difffer approach describ consid non-overlap overlap spike separ quantifi perform three differ approach two standard classif devis first hardest case judg correct classif precis order time two waveform reconstruct second looser scheme classif judg correct if order two waveform correct time allow vari loo jlsec i.e time bin neurobiolog applic probabl suffici resolut fig compar perform result three approach waveform classif implement digit simul perform comparison two templat non-overlap waveform as shown low noise-to-sign ratio nsrs three approach compar perform reach close accuraci criterion as ratio increas howev neural network implement less less well respect simpl templat match algorithm full hopfield type network consider wors modifi network rang nsr often found real data simpl templat match perform consider better either neural network approach also note simpl templat match give estim good fit betwwen waveform closest templat could use identifi event classifi signal due nois nois level 3a/peak amplitud i nois level 3a/peak amplitud i i i i i tli i degre overlap light line absolut criteria heavi line less stringent criteria simpl templat match hopfield network modifi hopfield network comparison three approach detect two non-overlap overlap waveform compar perform neural network approach differ degre waveform overlap two templat overlap waveform 3b 3c compar perform waveform overlap 3b serious local minima problem encount full neural network demonstr as improv perform modifi network again overal perform physi olog rang nois clear best simpl templat match when nois level low modifi approach bet ter two neural network due reliabl correl number reflect resembl input data templat when nois level high error correl number may exclud right combin smaller network in case perform actual littl wors larger hopfield network 3c document in detail degre overlap produc troubl neural network approach averag nsr level found in real neural data seen neural network serious problem encount when delay two waveform small enough result waveform look like larger waveform perturb three templat overlap non-overlap in shown comparison full hopfield network approach simpl templat match approach nonoverlap waveform perform these two approach much compar two waveform case although simpl templat match still optim method in overlap waveform condit howev neural network approach fail bad 4b particular applic implement neural network approach scale well nois level 3a peak amplitud i nois level 3a peak amplitud hopfield network simpl templat match light line absolut criteria heavi line less stringent criteria varianc nois nois level 3a peak amplitud comparison perform three waveform nonoverlap waveform two waveform overlap three waveform overlap hardwar comparison as describ earlier import design requi~ work abil letect neural signal in analog record in real-tim origin from mani simultan activ sampl electrod feasibl run algorithm in comput in real time channel simultan necessari design build dedic hardwar channel decid design vlsi implement circuitri in regard it well recogn larg modifi neural network need elabor hardwar implement let us consid exampl implement hard ware for two-templ case for comparison let neuron per templat one neuron for each delay templat iter reach stabl state simul discret differenti equat step size sampl in templat then number connect in full hopfield network 4n total synapt calcul 4mn so for two templat thus build full hopfield-typ network digit requir system larg put in singl vlsi chip work in real time if we want build analog system we need mani 4n easili modifi synaps as yet technolog avail for net this size modifi hopfield-typ network hand less technic demand preprocess obtain minimum valu we addit to find possibl iij requir subtract comparison to find three minima cost associ input cross-correl as for full neural network 2nl multipl save modifi approach network use small fast multipl addit to construct modifi synaps synapt calcul in contrast to the neural network simpl temrlat match simpl inde for exampl it must perform addit comparison to find the minimum ij addit consider less cost in time hardwar multipl in fact becaus this method need addit oper our preliminari design work suggest it built singl chip abl to the two-templ classif in as littl as microsecond this actual rais the possibl switch buffer one chip might abl to servic more one channel in essenti real time conclus templat match use full hopfield-typ neural network found to robust to nois chang in signal waveform for the two neural waveform classif problem howev for three-waveform case the network perform well further the network requir mani modifi connect therefor result in an elabor hardwar implement the overal perform of the modifi neural network approach better the full iopfield network approach the comput reduc larg the hardwar requir consider less demand demonstr the valu of design specif network to specifi problem howev even the modifi neural network perform less well simpl template-match algorithm also the simplest hardwar implement use the simpl templat match algorithm our simul suggest it possibl to build two three waveform classifi on a singl vlsi chip use cmos technolog work in real time with excel error characterist further a chip abl to accur classifi variabl overlap neural signal
----------------------------------------------------------------

title: 232-analog-neural-networks-of-limited-precision-i-computing-with-multilinear-threshold-functions.pdf

obradov pclrberri analog neural network limit precis i comput multilinear threshold function preliminari version zoran obradov ian parberri depart comput scienc penn state univers univers park pa. abstract experiment evid shown analog neural network ex~m fault-toler particular perform appear signific impair precis limit analog neuron limit precis essenti comput k-ari weight multilinear threshold function divid region k-l hyperplan behaviour k-ari neural network investig canon set threshold valu although exist binari ternari neural network weight made integ log bit number processor without increas hardwar run time weight made increas run time constant multipl hardwar small polynomi binari neuron use run time allow increas larger constant multipl hardwar allow increas slight larger polynomi symmetr k-ari function comput constant depth size k-ari function comput constant depth size altern neural network olafsson abu-mostafa quantiz neural network fleisher close relat model analog neural network limit precis i introduct neural network typic circuit construct process unit comput simpl function form f wl wli rii-+ ser wier wii xl xli =g lwi output function two choic set current popular literatur first discret model s=b denot boolean set case typic linear threshold function iff call weight linear threshold function second analog model denot case typic monoton increas function sigmoid function constant r. analog neural network model popular easi construct processor requir characterist use transistor digit model popular behaviour easi analyz experiment evid indic analog neural network produc accur comput precis compon limit consid actual happen analog model precis limit suppos neuron take distinct excit valu exampl restrict number digit binari decim expans isomorph show essenti multilinear threshold function hloh2 defin here throughout paper assum conveni defin ho=-oo h/c=oo call k-ari weight multilinear threshold function multilinear threshold function studi neural network construct k-ari multilinear threshold function call k-ari neural network order distinguish standard 2-ari binari neural network particular concern resourc time size number processor weight sum weight k-ari neural network use accord classic comput paradigm reader refer parberri similar result binari neural network companion paper obradov parberri deal learn k-ari neural network detail version paper appear obradov parberri k-ari neural network model k-ari neural network weight graph set processor cvxv set connect processor function w vxv assign weight interconnect h v assign set k-l threshold processor assum ee size defin number processor weight obradov parberri processor k-ari neural network relat limit comput power k-ari function function let denot set n-input k-ari function defin iff hi set k-ari weight multilinear threshold function union n. rang processor k-ari neural network comput k-ari weight multilinear threshold function input processor one state initi input processor place state encod input processor updat interv state time output time state k-ari neural network comput processor chang state stabl configur reach output state output processor stabl state reach neural network said equival iff input everi comput input termin time comput input termin time output neural network said equival iff equival analog neural network let function rang ani limited-precis devic purport comput must actual comput function rang ration valu ken suffici practic purpos provid larg enough sinc isomorph z formal defin limit precis variant function defin f x =round j round r~n natur round function defin round x =n iff theorem letf wlo wier defin lwixi i=l monoton increas invert then f wlo k-ari weight multilinear threshold function proof easi verifi f wlo hi thus see analog neural network limit precis essenti k-ari neural network analog neural network limit precis i canon threshold binari neural network advantag threshold taken equal zero see exampl theorem parberri similar result hold ternari neural network theorem everi n-input ternari weight multilinear threshold function equival i -input ternari weight multilinear threshold function threshold valu equal zero one proof suppos wii hloh2 r. without loss general assum l h defin w= wl rii+i wj=wjl hrh wli demonstr simpl case analysi xll e z l hz x xll choic threshold valu theorem arbitrari unfortun canon set threshold theorem everi 1o hk 1e r. exist n-input k-ari weight multilinear threshold function input k-ari weight multilinear threshold function wii+m hk-l proof sketch suppos i tk-l canon set threshold assum let 1o hk l=h hi 4si hypothesi exist wlo y= ylo xezi let i wi+2yi sinc follow wl+wz+s sinc follow obradov pdrberri inequ impli similar argument conclud but contradict network bound weight although model allow weight take infinit number possibl valu finit number threshold function sinc finit number k-ari function fix number input thus number input threshold function bound function fact someth stronger shown weight made integr log bit suffici describ one theorem everi k-ari neural network size exist equival k-ari neural network m2 size weight integ weight proof sketch suffici prove everi weight threshold function f wlt nen equival we1f.ht threshold function w .hi extend techniqu use muroga toda takasu binari case we see weight bound maximum determin matrix dimens lover z thus bound polynomi we guarante abl describ weight use polynomi number bit threshold circuit k-ari neural network weight drawn said unit weight unit-weight direct acycl k-ari neural network call k-ari threshold circuit k-ari threshold circuit divid layer each layer receiv input layer it depth k-ari threshold circuit defin number layer weight equal number edg bound squar size despit appar handicap limit weight kari threshold circuit surpris power much interest focuss comput symmetr function neural network motiv fact visual system appear abl recogn object regardless posit retina function call symmetr output remain matter input permut analog neural network limit precis i theorem ani symmetr k-ari function input comput k-ari threshold circuit depth size proof omit it note mani time neural network comput ani boolean function constant depth true k-ari neural network although result appear requir exponenti size mani interest function theorem ani k-ari function input comput k-ari threshold circuit size depth proof similar chandra parberri interest problem remain determin function requir exponenti size achiev constant depth comput polynomi size constant depth we consid problem ad integ repres k-ari notat theorem sum two k-ari integ size comput k-ari threshold circuit size depth proof first comput carri luadrat size depth use standard elementari school algorithm then it posit result comput tit posit operand carri propag posit constant size depth theorem sum integ size comput k-ari threshold circuit size constant depth proof similar proof use theorem chandra parberri theorem everi k-ari neural network size exist equival unit-weight k-ari neural network m2 size proof theorem we bound weight size binari notat theorem we replac everi processor non-unit weight threshold circuit size constant depth theorem impli we assum unit weight increas size polynomi run time constant multipl provid number logic level bound polynomi size network number threshold also reduc one size increas larger polynomi theorem everi k-ari neural network size exist equival unit-weight binari neural network size 4k log output binari encod requir result proof similar proof theorem result primarili theoret interest binari neural network appear simpler henc desir analog neural network howev analog neural network actual desir sinc easier build in mind theorem simpli serv limit the function analog neural network obradov parberri expect comput effici we concern construct model the comput abil neural network rather model implement detail nonmonoton multilinear neural network olafsson abu-mostafa studi f wlt w er inform capac function xli =g the altern threshold function monoton increas h er defin if we call altern weight multilinear threshold function neural network construct function form altern multilinear neural network altern multilinear neural network close relat k-ari neural network theorem for everi k-ari neural network size weight there equival altern multilinear neural network size log weight log produc the output the former in binari notat proof sketch each k-ari gate replac log gate togeth essenti perform binari search determin each bit the k-ari gate weight increas exponenti use provid the correct output valu theorem for everi altern multilinear neural network size weight there a 3t-equival k-ari neural network size 4z weight proof sketch without loss general assum odd each altern gate replac a k-ari gate ident weight threshold the output this gate goe weight one a k-ari gate threshold weight minus one a k-ari gate threshold the output gate goe a binari gate threshold both k-ari altern multilinear neural network a special case nonmonoton multilinear neural network the defin by iff hi~ h +lt for monoton increas h er co ck-1ezk nonmonoton neural network correspond analog neural network whose output function necessarili monoton nondecreas mani the result this paper includ theorem also appli nonmonoton neural network the size weight run time mani the upper-bound also improv by a small amount by use nonmonoton neural network instead k-ari one the detail left the interest reader mul tilinear hopfield network a multilinear version the hopfield network call the quantiz neural network studi by fleisher use the terminolog parberri a quantiz neural network a simpl symmetr k-ari neural network interconnect pattern undirect graph without self-loop the addit properti all processor ident set threshold although the latter assumpt analog neural network limit precis i reason for binari neural network see for exampl theorem parberri ternari neural network theorem it necessarili for k-ari neural network theorem howev it easi extend fleisher main result give the follow theorem ani product sequenti comput a simpl symmetr k-ari neural network converg conclus it shown analog neural network with limit precis essenti k-ari neural network if limit a polynomi then polynomi size constant depth k-ari neural network equival polynomi size constant depth binari neural network nonetheless the save in time a constant multipl hardwar a polynomi aris use k-ari neural network rather binari one quit signific we suggest one actual construct binari k-ari neural network analog neural network construct by exploit the analog behaviour transistor rather use extra hardwar inhibit it rather we suggest k-ari neural network a tool for reason the behaviour analog neural network acknowledg the financi support the air forc offic of scientif research air forc ystern command dsaf grant number afosr afosr nsf grant ian parberri grate acknowledg
----------------------------------------------------------------

title: 822-bounds-on-the-complexity-of-recurrent-neural-network-implementations-of-finite-state-machines.pdf

bound complex recurr neural network implement finit state machin bill g. horn nec research institut independ way princeton nj don r. hush eec depart univers new mexico albuquerqu nm abstract paper effici recurr neural network implement m-state finit state machin explor specif shown node complex unrestrict case bound fo also shown node complex log weight threshold restrict set fan-in restrict two match lower bound provid upper bound assum state fsm encod subset node size rlog introduct topic paper understand effici neural network scale larg problem although mani way measur effici shall concern node complex name impli calcul requir number node node complex use measur effici sinc amount resourc requir implement even simul recurr neural network typic relat number node node complex also relat effici learn algorithm network perhap general abil well shall focus node complex recurr neural network implement finit state machin fsms node network restrict threshold logic unit home hush shown recurr neural network capabl implement arbitrari fsms first result area due minski show m-state fsms implement fulli connect recurr neural network although circuit complex focus investig turn construct yield node construct also guarante use weight valu limit set sinc recurr neural network hard-limit node capabl repres mani 2k state one might wonder m-state fsm could implement network log node howev shown node complex standard fulli connect network log also abl improv upon minski 's result provid construct guarante yield node paper lower bound node complex investig network subject restrict possibl rang weight valu fan-in fan-out node network their investig limit fulli connect recurr neural network discov node complex case weight restrict finit size set log altern node network restrict constant fan-in node complex becom howev left open question tight bound appli variat basic architectur recent work includ investig node complex network continu valu nonlinear howev also shown continu nonlinear use recurr neural network far power fsms fact ture equival paper improv upper bound node complex unrestrict case also provid upper bound match lower bound various restrict specif show node complex 'm log achiev weight restrict set i node complex case fan-in node network restrict two final explor possibl implement finit state machin complex model might yield lower node complex specif explor node complex general recurr neural network topolog capabl simul varieti popular recurr neural network architectur except unrestrict case show node complex differ architectur fulli connect case number feedback variabl limit rlog state fsm encod optim subset node leav open question sparser encod lead effici implement background finit state machin fsms may defin sever way paper shall concern meali machin although approach easili extend formul yield equival result bound complex recurr neural network implement definit meali machin quintupl qo finit set state qo initi state input alphabet output alphabet combin transit output function throughout paper input output alphabet binari general number state iqi may arbitrari sinc element encod binari vector whose minimum length pog function implement boolean logic function form pogm1+l pogm1+l number differ minim fsms state use determin lower bound number gate requir implement arbitrari fsm recurr neural network easili shown nm howev conveni reexpress term flog follow recurr neural network fundament process unit model wish consid perceptron bias linear weight sum input follow hard-limit nonlinear whose output zero input negat one otherwis fan-in perceptron defin number non-zero weight valu binari they paper perceptron often refer threshold logic unit tl u count number differ partial specifi threshold logic function threshold logic function whose valu defin vertic unit hypercub need develop lower bound node complex requir implement arbitrari logic function shown number denot 2v point mani popular discrete-tim recurr neural network model implement feedforward network whose output fed back recurr set unit time delay generic version architectur feed forward section lower triangular mean th node node layer i receiv input node previous layer includ input layer lower triangular network threshold logic element general topolog possibl feedforward network sinc other feedforward network view special case network appropri weight set equal zero direct implement model architectur propos howev mani recurr neural network architectur cast framework exampl fulli connect network fit model feedforward network simpli singl layer node even model appear differ cast framework home hush unrestrict case unrestrict case general thus explor inher power recurr neural network unrestrict case also import serv baselin one evalu effect various restrict node complex order deriv upper bound node complex recurr neural network implement fsms shall util follow lemma due lupanov proof lemma involv construct extrem complex beyond scope paper lemma lupanov arbitrari boolean logic function input output implement network perceptron node complex theorem multilay recurr neural network implement fsms state node complex jffi proof sinc m-state fsm implement recurr neural network multilay network perform map form equat use flog appli lemma give upper bound o .jffi theorem multilay recurr neural network implement fsms state node complex number unit time delay flog proof order prove theorem deriv express maximum number function k-node recurr neural network comput compar minimum number finit state machin solv term number state fsm specif wish manipul inequ krr-l left hand side given equat repres total number way choos output feedback variabl network repres total number logic function comput feed forward section network lower triangular part found simpl combinatori argument note last node network must use either output feedback node part obtain follow argument state optim encod flog m1 node flog m1 variabl need bound complex recurr neural network implement fed back togeth extern input give rlog m1 local input feedforward network repeat applic 2n yield express follow seri algebra manipul easili shown exist constant n2n ck sinc flog ml follow f2 restrict weight threshold threshold logic function implement perceptron whose weight threshold valu integ well known threshold logic function variabl requir perceptron weight whose maximum magnitud impli perceptron implement digit number bit requir repres weight threshold worst case super linear function fan-in general undesir would far better requir logarithm number bit per weight even better constant number bit per weight primarili interest extrem case weight limit valu set i order deriv node complex network weight restrict shall util follow lemma prove lemma arbitrari boolean logic function input output implement network ofperceptron whose weight threshold restrict set i node complex lemma difficult prove howev beyond scope paper basic idea involv use decomposit logic function propos specif boolean function may alway decompos disjunct term form xix2 xr fi x one conjunct first variabl xj repres either complement uncompl version input variabl xj ii logic function last variabl express implement direct neural network neglig number addit node construct implement way weight either lor final variabl optim yield minimum number node network theorem multilay recurr neural network node whose weight threshold restrict set i implement fsms state node complex jm log proof sinc m-state fsm implement recurr neural network multilay network perform map form equat use flog appli lemma give upper bound jmlogm home hush theorem multilay recurr neural network node whose weight threshold restrict set size iwi implement fsms state node complex number unit time delay flogml proof proof similar proof theorem gave lower bound node complex requir arbitrari network threshold logic unit here inequ wish manipul given k-l n-i iwln+i i=o left hand side comput repres maximum number way configur node network iwi choic weight threshold follow seri algebra manipul shown exist constant n2n ck log iwi sinc pog follow clear mlogm loglwi i lower bound match upper bound theorem restrict fan-in limit fan-in perceptron anoth import practic restrict network discuss far node unlimit fan-in fact construct describ mani node receiv input polynomi number node term previous layer practic it possibl build devic larg connect restrict fan-in sever restrict primari interest paper onc order deriv node complex restrict fan-in we shall util follow lemma prove in lemma arbitrari boolean logic function input output implement in network perceptron restrict fan-in node complexityof y2x logi this proof this lemma similar proof lemma here shannon decomposit use recurs decompos logic function set tree tree depth all possibl function last variabl implement in invert tree-lik structur feed bottom tree final optim yield minimum number node bound complex recurr neural network implement theorem multilay recurr neural network node whose fan-in restrict two implement fsms state node complex oem proof sinc m-state fsm implement in recurr neural network in multilay network perform map form in equat then use rlog appli lemma give upper bound theorem multilay recurr neural network node whose fan-in restrict two implement fsms state node complex number unit time delay rlog proof onc proof similar theorem gave lower bound node complex requir in arbitrari network threshold logic unit here inequ we need solv given d. left hand side comput repres maximum number way configur node in network term use sinc node in the ith layer possibl input two chosen the constant repres the fourteen possibl threshold logic function two variabl follow seri algebra manipul it shown exist constant ck logk sinc rlog m1 it follow n2n summari in summari we provid new bound the node complex implement fsms recurr neural network upper bound match lower bound develop in fulli connect recurr network the size the weight set the fan-in node finit although one might specul complex network might yield effici construct we show these lower bound chang restrict weight fan-in least the state the fsm encod optim in subset flog m1 node the network unrestrict this lower bound match upper bound we leav it as open question if a sparser encod the state variabl lead a effici implement one interest aspect this studi realli much differ in effici when the network total unrestrict when sever restrict place the weight assum bound tight then home hush a y'log penalti restrict the weight either to get idea margin this differ consid for a finit state machin with state y'log eight a detail version this paper found in
----------------------------------------------------------------

title: 70-on-the-power-of-neural-networks-for-solving-hard-problems.pdf

power neural network solv hard problem ehoshua bruck joseph w. goodman inform system laboratori departmen electr engin stanford univers stanford ca abstract paper deal neural network model neuron perform threshold logic function import properti model alway converg stabl state oper serial mode properti basi potenti applic model associ memori devic combinatori optim one motiv use model solv hard combinatori problem fact implement optic devic thus oper higher speed convent electron main theme work investig power model solv np-hard problem understand relat speed oper size neural network particular shown np-hard problem exist polynomi size network solv impli np=co-np also travel salesman problem even polynomi size network get approxim solut exist unless p=np result great practic interest right possibl build neural network oper fast limit number neuron background neural network model discret time system repres weight undirect graph weight attach edg graph threshold valu attach node neuron graph american institut physic order network number node correspond graph let neural network order uniqu defin symmetr matrix wii equal weight attach edg vector dimens ti denot threshold attach node everi node neuron one two possibl state either state node time denot state neural network time vector next state node comput vi hi wiivj ti i=l next state network v comput current state perform evalu subset node network denot s. mode oper determin method set select time interv comput perform singl node time interv 1s say network oper serial mode 1s say network oper fulli parallel mode case i call parallel mode oper set chosen random accord determinist rule state call stabl iff sgn wv chang state network matter mode oper one import properti model fact alway converg stabl state oper serial mode main idea proof converg properti defin call energi function show energi function nondecreas state network chang energi function import note origin energi function defin nonincreas chang compli known graph problem min cut neural network alway get stabl state correspond local maximum energi function suggest use network devic perform local search algorithm find maxim valu energi function thus network perform local search oper random serial mode also known maxim associ given network equival find minimum cut n. actual mani hard problem formul maxim quadrat form tsp thus map neural network main result set stabl state set possibl final solut one get use approach these final solut correspond local maxima energi function necessarili correspond global optima correspond problem main question suppos allow network oper long time converg better get local optimum possibl design network alway find exact solut guarante approxim problem definit let instanc problem denot size number bit requir repres exampl instanc tsp i number bit need repres matrix distanc citi definit let neural network denot size network n. name number bit need repres wand t. let us start defin desir setup use neural network model solv hard problem consid optim problem would like everi instanc neural network follow properti everi local maximum energi function associ correspond global optimum network small 1x i. i nx bound polynomi moreov would like algorithm denot given instanc generat descript polynomi i i time now defin desir setup use neural network model find approxim solut hard problem definit let eglo global maximum energi function let eloc local maximum energi function say local maximum f-approxim global iff eglo eloc eglo setup find approxim solut similar one find exact solut fo fix number would like network everi local maximum f-approxim global global correspond optimum network small name bound polynomi i also would like algorithm given instanc generat descript polynomi i time note exact case approxim case put restrict time take network converg solut exponenti point reader convinc descript imagin setup use neural network model solv hard problem follow definit definit say neural network solv find fapproxim problem exist algorithm al alj generat descript exist main result paper summar follow two proposit first one deal exact solut np-hard problem second deal approxim solut tsp proposit let np-hard problem exist neural network solv impli np co-np proposit let fix number exist neural network find f-approxim solut tsp impli p=np both np=co-np believ fals statement henc use model way imagin key observ prove proposit fact singl iter neural network take time bound polynomi size instanc correspond problem proof two proposit follow direct known result complex theori consid new result complex theori proof proof proposit proof follow definit the class np co-np lemma the definit the lemma appear chapter also chapter lemma the complement np-complet problem np np=co-np let np-hard problem suppos exist neural network solv l. let np-complet problem definit polynomiali reduc l. thus everi instanc neural network global maxima effici recogn whether yes instanc claim nondeterminist polynomi time algorithm decid given instanc instanc here construct the neural network solv use the reduct l. check everi state the network see local maximum done polynomi time case local maximum check the instanc yes instanc this also done polynomi time thus nondeterminist polynomi time algorithm recogn instanc thus the complement the problem np np-complet problem henc lemma follow np=co-np proof proposit the result corollari the result the reader refer complet present the proof use the fact the restrict hamiltonian circuit rhc np-complet problem definiton rhc given graph hamiltonian path g. the question whether hamiltonian circuit proven rhc np-complet suppos exist polynomi size neural network find f-approxim solut tsp then shown instanc rhc reduc instanc tsp the network the follow hold the hamiltonian path given correspond local maximum then instanc els if correspond local maximum then yes instanc note check local polynomi time henc the exist xe tsp impli we polynomi time algorithm rhc conclud remark proposit we let i i i i arbitrari bound by polynomi the size given instanc problem if we assum i i i i fix for instanc then similar result proposit prove without use complex theori this result appear the network correspond tsp suggest in solv the tsp guarante qualiti howev one note the analysi in this paper worst case type analysi so might there exist network good behavior on the averag proposit general all np-hard problem proposit specif tsp both proposit hold for type network in iter take polynomi time clear everi network algorithm equival it but algorithm necessarili a correspond network thus if we know an algorithm solut a problem we also abl find a network solv the problem if one believ the neural network model a good model it amen implement optic one develop techniqu program the network perform an algorithm known guarante good behavior acknowledg support the u.s. air forc offic scientif research grate acknowledg
----------------------------------------------------------------

