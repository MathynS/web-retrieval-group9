query sentence: duplicated documents detection
---------------------------------------------------------------------
title: 4119-b-bit-minwise-hashing-for-estimating-three-way-similarities.pdf

b-Bit Minwise Hashing for Estimating Three-Way Similarities
Ping Li
Dept. of Statistical Science
Cornell University

Arnd Christian K?onig
Microsoft Research
Microsoft Corporation

Wenhao Gui
Dept. of Statistical Science
Cornell University

Abstract

Computing1 two-way and multi-way set similarities is a fundamental problem.
This study focuses on estimating 3-way resemblance (Jaccard similarity) using
b-bit minwise hashing. While traditional minwise hashing methods store each
hashed value using 64 bits, b-bit minwise hashing only stores the lowest b bits
(where b ? 2 for 3-way). The extension to 3-way similarity from the prior work
on 2-way similarity is technically non-trivial. We develop the precise estimator
which is accurate and very complicated; and we recommend a much simplified
estimator suitable for sparse data. Our analysis shows that b-bit minwise hashing
can normally achieve a 10 to 25-fold improvement in the storage space required
for a given estimator accuracy of the 3-way resemblance.

1

Introduction

The efficient computation of the similarity (or overlap) between sets is a central operation in a variety
of applications, such as word associations (e.g., [13]), data cleaning (e.g., [40, 9]), data mining
(e.g., [14]), selectivity estimation (e.g., [30]) or duplicate document detection [3, 4]. In machine
learning applications, binary (0/1) vectors can be naturally viewed as sets. For scenarios where the
underlying data size is sufficiently large to make storing them (in main memory) or processing them
in their entirety impractical, probabilistic techniques have been proposed for this task.
Word associations (collocations, co-occurrences)
If one inputs a query NIPS machine learning,
all major search engines will report the number of pagehits (e.g., one reports 829,003), in addition to
the top ranked URLs. Although no search engines have revealed how they estimate the numbers of
pagehits, one natural approach is to treat this as a set intersection estimation problem. Each word can
be represented as a set of document IDs; and each set belongs to a very large space ?. It is expected
that |?| > 1010 . Word associations have many other applications in Computational Linguistics [13,
38], and were recently used for Web search query reformulation and query suggestions [42, 12].
Here is another example. Commercial search engines display various form of ?vertical? content
(e.g., images, news, products) as part of Web search. In order to determine from which ?vertical?
to display information, there exist various techniques to select verticals. Some of these (e.g., [29,
15]) use the number of documents the words in a search query occur in for different text corpora
representing various verticals as features. Because this selection is invoked for all search queries
(and the tight latency bounds for search), the computation of these features has to be very fast.
Moreover, the accuracy of vertical selection depends on the number/size of document corpora that
can be processed within the allotted time [29], i.e., the processing speed can directly impact quality.
Now, because of the large number of word-combinations in even medium-sized text corpora (e.g.,
the Wikipedia corpus contains > 107 distinct terms), it is impossible to pre-compute and store the
associations for all possible multi-term combinations (e.g., > 1014 for 2-way and > 1021 for 3-way);
instead the techniques described in this paper can be used for fast estimates of the co-occurrences.
Database query optimization Set intersection is a routine operation in databases, employed for
example during the evaluation of conjunctive selection conditions in the presence of single-column
indexes. Before conducting intersections, a critical task is to (quickly) estimate the sizes of the
intermediate results to plan the optimal intersection order [20, 8, 25]. For example, consider the task
of intersecting four sets of record identifiers: A ? B ? C ? D. Even though the final outcome will
be the same, the order of the join operations, e.g., (A ? B) ? (C ? D) or ((A ? B) ? C) ? D, can
significantly affect the performance, in particular if the intermediate results, e.g., A?B ?C, become
too large for main memory and need to be spilled to disk. A good query plan aims to minimize
1

This work is supported by NSF (DMS-0808864), ONR (YIP-N000140910911) and Microsoft.

the total size of intermediate results. Thus, it is highly desirable to have a mechanism which can
estimate join sizes very efficiently, especially for the lower-order (2-way and 3-way) intersections,
which could potentially result in much larger intermediate results than higher-order intersections.
Duplicate Detection in Data Cleaning: A common task in data cleaning is the identification of
duplicates (e.g., duplicate names, organizations, etc.) among a set of items. Now, despite the fact
that there is considerable evidence (e.g., [10]) that reliable duplicate-detection should be based on
local properties of groups of duplicates, most current approaches base their decisions on pairwise
similarities between items only. This is in part due to the computational overhead associated with
more complex interactions, which our approach may help to overcome.
Clustering Most clustering techniques are based on pair-wise distances between the items to be
clustered. However, there are a number of natural scenarios where the affinity relations are not
pairwise, but rather triadic, tetradic or higher (e.g. [1, 43]). Again, our approach may improve the
performance in these scenarios if the distance measures can be expressed in the form of set-overlap.
Data mining A lot of work in data mining has focused on efficient candidate pruning in the
context of pairwise associations (e.g., [14]), a number of such pruning techniques leverage minwise
hashing to prune pairs of items, but in many contexts (e.g., association rules with more than 2 items)
multi-way associations are relevant; here, pruning based on pairwise interactions may perform much
less well than multi-way pruning.
1.1

Ultra-high dimensional data are often binary

For duplicate detection in the context of Web crawling/search, each document can be represented as
a set of w-shingles (w contiguous words); w = 5 or 7 in several studies [3, 4, 17]. Normally only the
abscence/presence (0/1) information is used, as a w-shingle rarely occurs more than once in a page
if w ? 5. The total number of shingles is commonly set to be |?| = 264 ; and thus the set intersection
corresponds to computing the inner product in binary data vectors of 264 dimensions. Interestingly,
even when the data are not too high-dimensional (e.g., only thousands), empirical studies [6, 23, 26]
achieved good performance using SVM with binary-quantized (text or image) data.
1.2

Minwise Hashing and SimHash

Two of the most widely adopted approaches for estimating set intersections are minwise hashing [3,
4] and sign (1-bit) random projections (also known as simhash) [7, 34], which are both special
instances of the general techniques proposed in the context of locality-sensitive hashing [7, 24].
These techniques have been successfully applied to many tasks in machine learning, databases, data
mining, and information retrieval [18, 36, 11, 22, 16, 39, 28, 41, 27, 5, 2, 37, 7, 24, 21].
Limitations of random projections The method of random projections (including simhash) is
limited to estimating pairwise similarities. Random projections convert any data distributions to
(zero-mean) multivariate normals, whose density functions are determined by the covariance matrix
which contains only the pairwise information of the original data. This is a serious limitation.
1.3

Prior work on b-Bit Minwise Hashing

Instead of storing each hashed value using 64 bits as in prior studies, e.g., [17], [35] suggested to
store only the lowest b bits. [35] demonstrated that using b = 1 reduces the storage space at least
by a factor of 21.3 (for a given accuracy) compared to b = 64, if one is interested in resemblance
? 0.5, the threshold used in prior studies [3, 4]. Moreover, by choosing the value b of bits to be
retained, it becomes possible to systematically adjust the degree to which the estimator is ?tuned?
towards higher similarities as well as the amount of hashing (random permutations) required.
[35] concerned only the pairwise resemblance. To extend it to the multi-way case, we have to solve
new and challenging probability problems. Compared to the pairwise case, our new estimator is
significantly different. In fact, as we will show later, estimating 3-way resemblance requires b ? 2.
1.4 Notation
a

13

f

f

a

1

3

a

a12

23

f2

s13

r

1

r3

s
s

s12

23

r2

Figure 1: Notation for 2-way and 3-way set intersections.

Fig. 1 describes the notation used in 3-way intersections for three sets S1 , S2 , S3 ? ?, |?| = D.
? f1 = |S1 |, f2 = |S2 |, f3 = |S3 |.
? a12 = |S1 ? S2 |, a13 = |S1 ? S3 |, a23 = |S2 ? S3 |, a = a123 = |S1 ? S2 ? S3 |.
f1
,
D

? r1 =

r2 =

f2
,
D

r3 =

f3
.
D

s12 =

a12
,
D

s13 =

a13
,
D

s23 =

a23
,
D

s = s123 =

a
.
D

? u = r1 + r2 + r3 ? s12 ? s13 ? s23 + s.

We define three 2-way resemblances (R12 , R13 , R23 ) and one 3-way resemblance (R) as:
R12 =

|S1 ? S2 |
|S1 ? S3 |
|S2 ? S3 |
, R13 =
, R23 =
,
|S1 ? S2 |
|S1 ? S3 |
|S2 ? S3 |

R = R123 =

|S1 ? S2 ? S3 |
.
|S1 ? S2 ? S3 |

(1)

which, using our notation, can be expressed in various forms:
aij
sij
=
, i 6= j,
fi + fj ? aij
ri + rj ? sij
a
s
s
R=
=
= .
f1 + f2 + f3 ? a12 ? a23 ? a13 + a
r1 + r2 + r3 ? s12 ? s23 ? s13 + s
u

Rij =

(2)
(3)

Note that, instead of a123 , s123 , R123 , we simply use a, s, R. When the set sizes, fi = |Si |, can be
assumed to be known, we can compute resemblances from intersections and vice versa:
aij =

Rij
(fi + fj ),
1 + Rij

a=

R
(f1 + f2 + f3 ? a12 ? a13 ? a23 ) .
1?R

Thus, estimating resemblances and estimating intersection sizes are two closely related problems.
1.5

Our Main Contributions
? We derive the basic probability formula for estimating 3-way resemblance using b-bit hashing. The derivation turns out to be significantly much more complex than the 2-way case.
This basic probability formula naturally leads to a (complicated) estimator of resemblance.
? We leverage the observation that many real applications involve sparse data (i.e., ri = fDi ?
0, but fi /fj = ri /rj may be still significant) to develop a much simplified estimator, which
is desired in practical applications. This assumption of fi /D ? 0 significantly simplifies
the estimator and frees us from having to know the cardinalities fi .
? We analyze the theoretical variance of the simplified estimator and compare it with the
original minwise hashing method (using 64 bits). Our theoretical analysis shows that bbit minwise hashing can normally achieve a 10 to 25-fold improvement in storage space
(for a given estimator accuracy of the 3-way resemblance) when the set similarities are not
extremely low (e.g., when the 3-way resemblance > 0.02). These results are particularly
important for applications in which only detecting high resemblance/overlap is relevant,
such as many data cleaning scenarios or duplicate detection.

The recommended procedure for estimating 3-way resemblances (in sparse data) is shown as Alg. 1.
Algorithm 1 The b-bit minwise hashing algorithm, applied to estimating 3-way resemblances in a
collection of N sets. This procedure is suitable for sparse data, i.e., ri = fi /D ? 0.
Input: Sets Sn ? ? = {0, 1, ..., D ? 1}, n = 1 to N .
Pre-processing phrase:
1) Generate k random permutations ?j : ? ? ?, j = 1 to k.
2) For each set Sn and permutation ?j , store the lowest b bits of min (?j (Sn )), denoted by en,t,?j , t = 1 to b.
Estimation phrase: (Use threensets S1 , S2 , and S3 as an example.)
o
P
Qb
?
?
1) Compute P?12,b = k1 kj=1
t=1 1{e1,t,?j = e2,t,?j } . Similarly, compute P13,b and P23,b .
nQ
o
P
b
2) Compute P?b = k1 kj=1
t=1 1{e1,t,?j = e2,t,?j = e3,t,?j } .
?b =
3) Estimate R by R

?b ?2b (P
?12,b +P
?13,b +P
?23,b )+2
4b P
(2b ?1)(2b ?2)

.

? ij,b =
4) If needed, the 2-way resemblances Rij,b can be estimated as R

?ij,b ?1
2b P
.
2b ?1

2

The Precise Theoretical Probability Analysis

Minwise hashing applies k random permutations ?j : ? ?? ?, ? = {0, 1, ..., D ? 1}, and then
estimates R12 (and similarly other 2-way resemblances) using the following probability:
Pr (min(?j (S1 )) = min(?j (S2 ))) =

|S1 ? S2 |
= R12 .
|S1 ? S2 |

(4)

This method naturally extends to estimating 3-way resemblances for three sets S1 , S2 , S3 ? ?:
Pr (min(?j (S1 )) = min(?j (S2 )) = min(?j (S3 ))) =

|S1 ? S2 ? S3 |
= R.
|S1 ? S2 ? S3 |

(5)

To describe b-bit hashing, we define the minimum values under ? and their lowest b bits to be:
zi = min (? (Si )) ,

ei,t = t-th lowest bit of zi .

To estimate R, we need to computes the empirical estimates of the probabilities Pij,b and Pb , where
?
Pij,b = Pr

b
Y

!

?

1{ei,t = ej,t } = 1 ,

Pb = P123,b = Pr

t=1

b
Y

!
1{e1,t = e2,t = e3,t } = 1 .

t=1

The main theoretical task is to derive Pb . The prior work[35] already derived Pij,b ; see Appendix A.
To simplify the algebra, we assume that D is large, which is virtually always satisfied in practice.
Theorem 1 Assume D is large.
?
Pb = Pr

b
Y

!
1{e1,i = e2,i = e3,i } = 1

i=1

=

Z
Z +s
+R=
,
u
u

(6)

where u = r1 + r2 + r3 ? s12 ? s13 ? s23 + s, and
(r3 ? s13 ? s23 + s)
(r2 ? s12 ? s23 + s)
s12 G12,b + (s13 ? s)A2,b +
s13 G13,b
r1 + r2 ? s12
r1 + r3 ? s13
(r1 ? s12 ? s13 + s)
(r1 ? s12 ? s13 + s)
+(s23 ? s)A1,b +
s23 G23,b + [(r2 ? s23 )A3,b + (r3 ? s23 )A2,b ]
G23,b
r2 + r3 ? s23
r2 + r3 ? s23
(r2 ? s12 ? s23 + s)
+ [(r1 ? s13 )A3,b + (r3 ? s13 )A1,b ]
G13,b
r1 + r3 ? s13
(r3 ? s13 ? s23 + s)
G12,b ,
+ [(r1 ? s12 )A2,b + (r2 ? s12 )A1,b ]
r1 + r2 ? s12

Z =(s12 ? s)A3,b +

Aj,b =

rj (1 ? rj )2

b ?1

1 ? (1 ? rj )2b

,

Gij,b =

(ri + rj ? sij )(1 ? ri ? rj + sij )2

b ?1

1 ? (1 ? ri ? rj + sij )2b

, i, j ? {1, 2, 3}, i 6= j.

Theorem 1 naturally suggests an iterative estimation procedure, by writing Eq. (6) as s = Pb u ? Z.

D = 216

0.56
Pb

0.54

0.58

2 bits
3 bits
4 bits
Theoretical

0.52

0.54
0.52

0.5 b = 2

0.5

0.48 b = 3
0.46 b = 4
0
100

0.48
200 300 400
Sample size k

500

b=2

0.56
Pb

0.58

0.46
0

2 bits
3 bits
4 bits
Theoretical

b=3

b=4
20

D=2
100

200 300 400
Sample size k

500

Figure 2: Pb , for verifying the probability formula in Theorem 1. The empirical estimates and the
theoretical predictions essentially overlap regardless of the sparsity measure ri = fi /D.
A Simulation Study
For the purpose of verifying Theorem 1, we use three sets corresponding
to the occurrences of three common words (?OF?, ?AND?, and ?OR?) in a chunk of real world Web
crawl data. Each (word) set is a set of document (Web page) IDs which contained that word at least
once. The three sets are not too sparse and D = 216 suffices to represent their elements. The ri = fDi
values are 0.5697, 0.5537, and 0.3564, respectively. The true 3-way resemblance is R = 0.47.

We can also increase D by mapping these sets into a larger space using a random mapping, with
D = 216 , 218 , 220 , or 222 . When D = 222 , the ri values are 0.0089, 0.0087, 0.0056.
Fig. 2 presents the empirical estimates of the probability Pb , together with the theoretical predictions
by Theorem 1. The empirical estimates essentially overlap the theoretical predictions. Even though
the proof assumes D ? ?, D does not have to be too large for Theorem 1 to be accurate.

3 The Much Simplified Estimator for Sparse Data
The basic probability formula (Theorem 1) we derive could be too complicated for practical use. To
obtain a simpler formula, we leverage the observation that in practice we often have ri = fDi ? 0,
even though both fi and D can be very large. For example, consider web duplicate detection [17].
Here, D = 264 , which means that even for a web page with fi = 254 shingles (corresponding to the
text of a small novel), we still have fDi ? 0.001. Note that, even when ri ? 0, the ratios, e.g., rr12 ,
can be still large. Recall the resemblances (2) and (3) are only determined by these ratios.
We analyzed the distribution of fDi using two real-life datasets: the UCI dataset containing 3 ? 105
NYTimes articles; and a Microsoft proprietary dataset with 106 news articles [19]. For the UCINYTimes dataset, each document was already processed as a set of single words. For the anonymous
dataset, we report results using three different representations: single words (1-shingle), 2-shingles
(two contiguous words), and 3-shingles. Table 1 reports the summary statistics of the fDi values.
Table 1: Summary statistics of the
Data
3 ? 105 UCI-NYTimes articles
106 Microsoft articles (1-shingle)
106 Microsoft articles (2-shingle)
106 Microsoft articles (3-shingle)

fi
D

values in two datasets

Median
0.0021
0.00027
0.00003
0.00002

Mean
0.0022
0.00032
0.00004
0.00002

Std.
0.0011
0.00023
0.00005
0.00002

For truly large-scale applications, prior studies [3, 4, 17] commonly used 5-shingles. This means
that real world data may be significantly more sparse than the values reported in Table 1.
3.1

The Simplified Probability Formula and the Practical Estimator

Theorem 2 Assume D is large. Let T = R12 + R13 + R23 . As r1 , r2 , r3 ? 0,
?

Pb = Pr

b
Y

i=1

!

1{e1,i = e2,i = e3,i } = 1

=

o
1 n b
(2 ? 1)(2b ? 2)R + (2b ? 1)T + 1 .
b
4

(7)

Interestingly, if b = 1, then P1 = 14 (1 + T ), i.e., no information about the 3-way resemblance R is
contained. Hence, it is necessary to use b ? 2 to estimate 3-way similarities.
Alg. 1 uses P?b and P?ij,b to respectively denote the empirical estimates of the theoretical probabilities
? b , is
Pb and Pij,b . Assuming r1 , r2 , r3 ? 0, the proposed estimator of R, denoted by R
?
?
4b P?b ? 2b P?12,b + P?13,b + P?23,b + 2
?b =
R
.
(8)
(2b ? 1)(2b ? 2)
? b in (8) is unbiased with the variance
Theorem 3 Assume D is large and r1 , r2 , r3 ? 0. Then R

n
?
?
o
? ?
1
b
b
b
b
b
2
?b = 1
1
+
(2
?
3)T
+
4
?
6
?
2
+
10
R
?
(2
?
1)(2
?
2)R
.
V ar R
k (2b ? 1)(2b ? 2)
(9)

It is interesting to examine several special cases:
? 1 ) = ?, i.e., one must use b ? 2.
? b = 1: V ar(R
?
?
? 2 ) = 1 1 + T + 2R ? 6R2 .
? b = 2: V ar(R
6k
? ? ) = 1 R(1 ? R) = V ar(R
? M ). R
? M is the original minwise hashing esti? b = ?: V ar(R
k
? M requires an infinite precision
mator for 3-way resemblance. In principle, the estimator R
? M ) and V ar(R
? 64 ) are indistinguishable.
(i.e., b = ?). Numerically, V ar(R

3.2

Simulations for Validating Theorem 3

We now present a simulation study for verifying Theorem 3, using the same three sets used in Fig. 2.
? b )?Rb . Fig. 4 presents the empirical mean square
Fig. 3 presents the resulting empirical biases: E(R
2
? b ) in Theorem 3.
errors (MSE = bias +variance) together with the theoretical variances V ar(R
M
?3

0.05

0.01

?0.05

0

b=4
?0.02

?0.1
0

100

?0.03
0

500

100

0

M
4

?5

3

?5

b=2

3
b=2

b=3
b=2

200 300 400
Sample size k

D = 222

M

Bias

?0.01

b=2

x 10

b=4

M

b=3

5

D = 220

Bias

Bias

Bias

0

M
b=4

?3

x 10

D=2

D=2
0

5

18

16

200 300 400
Sample size k

?10
0

500

100

200 300 400
Sample size k

?10
0

500

100

200 300 400
Sample size k

500

? b (8). We used 3 (word) sets: ?OF?, ?AND?, and ?OR? and four D values: 216 ,
Figure 3: Bias of R
218 , 220 , and 222 . We conducted experiments using b = 2, 3, and 4 as well as the original minwise
hashing (denoted by ?M?). The plots verify that as ri decreases (to zero), the biases vanish. Note
that the set sizes fi remain the same, but the relative values ri = fDi decrease as D increases.

b=3

?3

10

10

2 bits
3 bits
4 bits
minwise
Theoretical

100
Sample size k

b=4
M
500

D = 218

?2

3

b=2

10

?3

10

10

2 bits
3 bits
4 bits
minwise
Theoretical

100
Sample size k

4

M

3
500

?1

10

D = 220
3

b=2

?2

10

?3

10

10

2 bits
3 bits
4 bits
minwise
Theoretical

100
Sample size k

4

M
500

Mean square error (MSE)

b=2
?2

10

?1

10

Mean square error (MSE)

?1

D = 216

Mean square error (MSE)

Mean square error (MSE)

?1

10

10

D = 222
3

?2

b=2

10

?3

10

10

2 bits
3 bits
4 bits
minwise
Theoretical

4

M

100
Sample size k

500

? b (8). The solid curves are the empirical MSEs (=var+bias2 ) and the dashed
Figure 4: MSE of R
lines are the theoretical variances (9), under the assumption of ri ? 0. Ideally, we would like to see
the solid and dashed lines overlap. When D = 220 and D = 222 , even though the ri values are not
too small, the solid and dashed lines almost overlap. Note that, at the same sample size k, we always
? 2 ) > V ar(R
? 3 ) > V ar(R
? 4 ) > V ar(R
? M ), where R
? M is the original minwise hashing
have V ar(R
?
?
? M ).
estimator. We can see that, V ar(R3 ) and V ar(R4 ) are very close to V ar(R
We can summarize the results in Fig. 3 and Fig. 4 as follows:
? When the ri = fDi values are large (e.g., ri ? 0.5 when D = 216 ), the estimates using
(8) can be noticeably biased. The estimation biases diminish as the ri values decrease. In
fact, even when the ri values are not small (e.g., ri ? 0.05 when D = 220 ), the biases are
already very small (roughly 0.005 when D = 220 ).
? The variance formula (9) becomes accurate when the ri values are not too large. For example, when D = 218 (ri ? 0.1), the empirical MSEs largely overlap the theoretical variances
which assumed ri ? 0, unless the sample size k is large. When D = 220 (and D = 222 ),
the empirical MSEs and theoretical variances overlap.
? For real applications, as we expect D will be very large (e.g., 264 ) and the ri values (fi /D)
will be very small, our proposed simple estimator (8) will be very useful in practice, because it becomes unbiased and the variance can be reliably predicted by (9).

4

Improving Estimates for Dense Data Using Theorem 1

While we believe the simple estimator in (8) and Alg. 1 should suffice in most applications, we
demonstrate here that the sparsity assumption of ri ? 0 is not essential if one is willing to use the
more sophisticated estimation procedure provided by Theorem 1.
By Eq. (6), s = Pb u ? Z, where Z contains s, sij , ri etc. We first estimate sij (from the estimated
Rij ) using the precise formula for the two-way case; see Appendix A. We then iteratively solve for
? b in (8). Usually a few iterations suffice.
s using the initial guess provided by the estimator R
Fig. 5 reports the bias (left most panel, only for D = 216 ) and MSE, corresponding to Fig. 3 and
Fig. 4. In Fig. 5, the solid curves are obtained using the precise estimation procedure by Theorem 1.
? b which assumes ri ? 0.
The dashed curves are the estimates using the simplified estimator R

Even when the data are not sparse, the precise estimation procedure provides unbiased estimates
as verified by the leftmost panel of Fig. 5. Using the precise procedure results in noticeably more
accurate estimates in non-sparse data, as verified by the second panel of Fig. 5. However, as long as
? b in (8) is accurate.
the data are reasonably sparse (the right two panels), the simple estimator R
?1

0.5

b=3

0
b=2
?0.5
?1
0

100

200 300 400
Sample size k

?1

10

Mean square error (MSE)

D = 216

Bias

Bias

Mean square error (MSE)

x 10

D = 216
b=2
?2

10

b=3
b=2
b=3

?3

10

500

10

100
Sample size k

500

?1

10

D = 218

?2

10

b=2
b=3

b=3

?3

10

b=2

10

100
Sample size k

Mean square error (MSE)

?3

1

500

10

D = 220

?2

10

b=2
b=3

?3

10

10

100
Sample size k

500

Figure 5: The bias (leftmost panel) and MSE of the precise estimation procedure, using the same
data used in Fig. 3 and Fig. 4. The dashed curves correspond to the estimates using the simplified
? b in (8) which assumes ri ? 0.
estimator R

5 Quantifying the Improvements Using b-Bit Hashing
This section is devoted to analyzing the improvements of b-bit minwise hashing, compared to using
64 bits for each hashed value. Throughout the paper, we use the terms ?sample? and ?sample size?
(denoted by k). The original minwise hashing stores each ?sample? using 64 bits (as in [17]). For
? 64 ) and V ar(R
?M )
b-bit minwise hashing, we store each ?sample? using b bits only. Note that V ar(R
(the variance of the original minwise hashing) are numerically indistinguishable.
As we decrease b, the space needed for each sample will be smaller; the estimation variance at
the same sample?size?k, however, will increase. This variance-space trade-off can be quantified by
? b ? k, which is called the storage factor. Lower B(b) is more desirable. The
B(b) = b ? Var R
ratio

B(64)
B(b)

precisely characterizes the improvements of b-bit hashing compared to using 64 bits.

20

b=3

15

b=4

10

b=6
T = 3R

5
0
0

0.2

0.4

0.6

0.8

1

20
15
b=2
10

Storage ratio ( B(64) / B(b) )

Storage ratio B(64) / B(b)

b=4
10
8

b=6

b=3

6
4

b=2
T = 10R

2
0
0

0.05

0.1

B(64)
B(b) ,

0.15
R

0.2

0.25

0.3

b=4
b=6

5
T = 4R
0
0

R
12

b=2
b=3

b=6
8
b=4
b=3
4
2
0
0

15

b=2

T = 20R

0.05

0.1
R

0.15

b=2

b=3
4

b=4
b=6

10

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
R

10

6

20
Storage ratio B(64) / B(b)

b=2

25

5

b=2
T = 6R

0
0

0.1

0.2

0.3

0.4

0.5

R
Storage ratio ( B(64) / B(b) )

30

Storage ratio B(64) / B(b)

Storage ratio ( B(64) / B(b) )

Fig. 6 confirms the substantial improvements of b-bit hashing over the original minwise hashing
using 64 bits. The improvements in terms of the storage space are usually 10 (or 15) to 25-fold
when the sets are reasonably similar (i.e., when the 3-way resemblance > 0.1). When the three sets
are very similar (e.g., the top left panel), the improvement will be even 25 to 30-fold.

7
6

b=6

5
4

b=4

3

b=3

2
1
0
0

b=2

T = 50R

0.01 0.02 0.03 0.04 0.05 0.06
R

the relative storage improvement of using b = 2, 3, 4, 6 bits, compared to using 64
Figure 6:
bits. Since the variance (9) contains both R and T = R12 + R13 + R23 , we compare variances using
different T /R ratios. As 3R ? T always, we let T = ?R, for some ? ? 3. Since T ? 3, we know
R ? 3/?. Practical applications are often interested in cases with reasonably large R values.

6

Evaluation of Accuracy

We conducted a duplicate detection experiment on a public (UCI) collection of 300,000 NYTimes
news articles. The task is to identify 3-groups with 3-way resemblance R exceeding a threshold R0 .
We used a subset of the data; the total number of 3-groups is about one billion. We experimented
with b = 2, 4 and the original minwise hashing. Fig. 7 presents the precision curves for a representative set of thresholds R0 ?s. Just like in [35], the recall curves are not shown because they could not
differentiate estimators. These curves confirm the significant improvement of using b-bit minwise
hashing when the threshold R0 is quite high (e.g., 0.3). In fact, when R0 = 0.3, using b = 4 resulted in similar precisions as using the original minwise hashing (i.e., a 64/4=16-fold reduction in
storage). Even when R0 = 0.1, using b = 4 can still achieve similar precisions as using the original
minwise hashing by only slightly increasing the sample size k.
1

1

1
M

0.8
b=4

0.6
b=2
0.4
R0 = 0.1

0.2

0.6

0.8

M
b=4

Precision

M
Precision

Precision

0.8

b=2

0.4
R0 = 0.2

0.2

0.6

b=2

0.4
0.2

R0 = 0.3

4
M

0
0

100

200
300
400
Sample size k

0
0

500

100

200
300
400
Sample size k

0
0

500

100

200
300
400
Sample size k

500

Figure 7: Precision curves on the UCI collection of news data. The task is to retrieve news article
3-groups with resemblance R ? R0 . For example, consider R0 = 0.2. To achieve a precision of
at least 0.8, 2-bit hashing and 4-bit hashing require about k = 500 samples and k = 260 samples
respectively, while the original minwise hashing (denoted by M ) requires about 170 samples.

7 Conclusion
Computing set similarities is fundamental in many applications. In machine learning, highdimensional binary data are common and are equivalent to sets. This study is devoted to simultaneously estimating 2-way and 3-way similarities using b-bit minwise hashing. Compared to the
prior work on estimating 2-way resemblance [35], the extension to 3-way is important for many
application scenarios (as described in Sec. 1) and is technically non-trivial.
For estimating 3-way resemblance, our analysis shows that b-bit minwise hashing can normally
achieve a 10 to 25-fold improvement in the storage space required for a given estimator accuracy,
when the set similarities are not extremely low (e.g., 3-way resemblance > 0.02). Many applications
such as data cleaning and de-duplication are mainly concerned with relatively high set similarities.
For many practical applications, the reductions in storage directly translate to improvements in processing speed as well, especially when memory latency is the main bottleneck, which, with the
advent of many-core processors, is more and more common.
Future work: We are interested in developing a b-bit version for Conditional Random Sampling
(CRS) [31, 32, 33], which requires only one permutation (instead of k permutations) and naturally
extends to non-binary data. CRS is also provably more accurate than minwise hashing for binary
data. However, the analysis for developing the b-bit version of CRS appears to be very difficult.

A Review of b-Bit Minwise Hashing for 2-Way Resemblance
Theorem 4 ([35]) Assume D is large.
?

where

C1,b

!

b
Y

P12,b = Pr
1 {e1,i = e2,i } = 1 = C1,b + (1 ? C2,b ) R12
r2 i=1
r1
r1
r2
= A1,b
+ A2,b
, C2,b = A1,b
+ A2,b
,
r1 + r2
r1 + r2
r1 + r2
r1 + r2
b

A1,b =

r1 [1 ? r1 ]2

b

?1

1 ? [1 ? r1 ]

2b

,

A2,b =

r2 [1 ? r2 ]2

?1
b

1 ? [1 ? r2 ]2

.

2 P?
?1
12
If r1 , r2 ? 0, P12,b = 1+(2 2?1)R
and one can estimate R12 by 212,b
, where P?12,b is the
b
b ?1
?
empirical observation of P12,b . If r1 , r2 are not small, R12 is estimated by (P12,b ?C1,b )/(1?C2,b ).
b

b

References
[1] S. Agarwal, J. Lim, L. Zelnik-Manor, P. Perona, D. Kriegman, and S. Belongie. Beyond pairwise clustering. In CVPR, 2005.
[2] M. Bendersky and W. B. Croft. Finding text reuse on the web. In WSDM, pages 262?271, Barcelona, Spain, 2009.
[3] A. Z. Broder. On the resemblance and containment of documents. In the Compression and Complexity of Sequences, pages 21?29,
Positano, Italy, 1997.
[4] A. Z. Broder, S. C. Glassman, M. S. Manasse, and G. Zweig. Syntactic clustering of the web. In WWW, pages 1157 ? 1166, Santa Clara,
CA, 1997.
[5] G. Buehrer and K. Chellapilla. A scalable pattern mining approach to web graph compression with communities. In WSDM, pages
95?106, Stanford, CA, 2008.
[6] O. Chapelle, P. Haffner, and V. N. Vapnik. Support vector machines for histogram-based image classification. 10(5):1055?1064, 1999.
[7] M. S. Charikar. Similarity estimation techniques from rounding algorithms. In STOC, pages 380?388, Montreal, Quebec, Canada, 2002.
[8] S. Chaudhuri. An Overview of Query Optimization in Relational Systems. In PODS, pages 34?43, 1998.
[9] S. Chaudhuri, V. Ganti, and R. Kaushik. A primitive operatior for similarity joins in data cleaning. In ICDE, 2006.
[10] S. Chaudhuri, V. Ganti, and R. Motwani. Robust identification of fuzzy duplicates. In ICDE, pages 865?876, Tokyo, Japan, 2005.
[11] F. Chierichetti, R. Kumar, S. Lattanzi, M. Mitzenmacher, A. Panconesi, and P. Raghavan. On compressing social networks. In KDD,
pages 219?228, Paris, France, 2009.
[12] K. Church. Approximate lexicography and web search. International Journal of Lexicography, 21(3):325?336, 2008.
[13] K. Church and P. Hanks. Word association norms, mutual information and lexicography. Computational Linguistics, 16(1):22?29, 1991.
[14] E. Cohen, M. Datar, S. Fujiwara, A. Gionis, P. Indyk, R. Motwani, J. D. Ullman, and C. Yang. Finding interesting associations without
support pruning. IEEE Trans. on Knowl. and Data Eng., 13(1), 2001.
[15] F. Diaz. Integration of News Content into Web Results. In WSDM, 2009.
[16] Y. Dourisboure, F. Geraci, and M. Pellegrini. Extraction and classification of dense implicit communities in the web graph. ACM Trans.
Web, 3(2):1?36, 2009.
[17] D. Fetterly, M. Manasse, M. Najork, and J. L. Wiener. A large-scale study of the evolution of web pages. In WWW, pages 669?678,
Budapest, Hungary, 2003.
[18] G. Forman, K. Eshghi, and J. Suermondt. Efficient detection of large-scale redundancy in enterprise file systems. SIGOPS Oper. Syst.
Rev., 43(1):84?91, 2009.
[19] M. Gamon, S. Basu, D. Belenko, D. Fisher, M. Hurst, and A. C. K?onig. Blews: Using blogs to provide context for news articles. In AAAI
Conference on Weblogs and Social Media, 2008.
[20] H. Garcia-Molina, J. D. Ullman, and J. Widom. Database Systems: the Complete Book. Prentice Hall, New York, NY, 2002.
[21] A. Gionis, D. Gunopulos, and N. Koudas. Efficient and tunable similar set retrieval. In SIGMOD, pages 247?258, CA, 2001.
[22] S. Gollapudi and A. Sharma. An axiomatic approach for result diversification. In WWW, pages 381?390, Madrid, Spain, 2009.
[23] M. Hein and O. Bousquet. Hilbertian metrics and positive definite kernels on probability measures. In AISTATS, pages 136?143,
Barbados, 2005.
[24] P. Indyk and R. Motwani. Approximate nearest neighbors: Towards removing the curse of dimensionality. In STOC, pages 604?613,
Dallas, TX, 1998.
[25] Y. E. Ioannidis. The history of histograms (abridged). In VLDB, 2003.
[26] Y. Jiang, C. Ngo, and J. Yang. Towards optimal bag-of-features for object categorization and semantic video retrieval. In CIVR, pages
494?501, Amsterdam, Netherlands, 2007.
[27] N. Jindal and B. Liu. Opinion spam and analysis. In WSDM, pages 219?230, Palo Alto, California, USA, 2008.
[28] K. Kalpakis and S. Tang. Collaborative data gathering in wireless sensor networks using measurement co-occurrence. Computer
Communications, 31(10):1979?1992, 2008.
[29] A. C. K?onig, M. Gamon, and Q. Wu. Click-Through Prediction for News Queries. In SIGIR, 2009.
[30] H. Lee, R. T. Ng, and K. Shim. Power-law based estimation of set similarity join size. In PVLDB, 2009.
[31] P. Li and K. W. Church. A sketch algorithm for estimating two-way and multi-way associations. Computational Linguistics, 33(3):305?
354, 2007 (Preliminary results appeared in HLT/EMNLP 2005).
[32] P. Li, K. W. Church, and T. J. Hastie. Conditional random sampling: A sketch-based sampling technique for sparse data. In NIPS, pages
873?880, Vancouver, BC, Canada, 2006.
[33] P. Li, K. W. Church, and T. J. Hastie. One sketch for all: Theory and applications of conditional random sampling. In NIPS, Vancouver,
BC, Canada, 2008.
[34] P. Li, T. J. Hastie, and K. W. Church. Improving random projections using marginal information. In COLT, pages 635?649, Pittsburgh,
PA, 2006.
[35] P. Li and A. C. K?onig. b-bit minwise hashing. In WWW, pages 671?680, Raleigh, NC, 2010.
[36] Ludmila, K. Eshghi, C. B. M. III, J. Tucek, and A. Veitch. Probabilistic frequent itemset mining in uncertain databases. In KDD, pages
1087?1096, Paris, France, 2009.
[37] G. S. Manku, A. Jain, and A. D. Sarma. Detecting Near-Duplicates for Web-Crawling. In WWW, Banff, Alberta, Canada, 2007.
[38] C. D. Manning and H. Schutze. Foundations of Statistical Natural Language Processing. The MIT Press, Cambridge, MA, 1999.
[39] M. Najork, S. Gollapudi, and R. Panigrahy. Less is more: sampling the neighborhood graph makes salsa better and faster. In WSDM,
pages 242?251, Barcelona, Spain, 2009.
[40] S. Sarawagi and A. Kirpal. Efficient set joins on similarity predicates. In SIGMOD, pages 743?754, 2004.
[41] T. Urvoy, E. Chauveau, P. Filoche, and T. Lavergne. Tracking web spam with html style similarities. ACM Trans. Web, 2(1):1?28, 2008.
[42] X. Wang and C. Zhai. Mining term association patterns from search logs for effective query reformulation. In CIKM, pages 479?488,
Napa Valley, California, USA, 2008.
[43] D. Zhou, J. Huang, and B. Sch?olkopf. Beyond pairwise classification and clustering using hypergraphs. 2006.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 5949-semi-supervised-sequence-learning.pdf

Semi-supervised Sequence Learning

Andrew M. Dai
Google Inc.
adai@google.com

Quoc V. Le
Google Inc.
qvl@google.com

Abstract
We present two approaches to use unlabeled data to improve Sequence Learning
with recurrent networks. The first approach is to predict what comes next in a
sequence, which is a language model in NLP. The second approach is to use a
sequence autoencoder, which reads the input sequence into a vector and predicts
the input sequence again. These two algorithms can be used as a ?pretraining?
algorithm for a later supervised sequence learning algorithm. In other words, the
parameters obtained from the pretraining step can then be used as a starting point
for other supervised training models. In our experiments, we find that long short
term memory recurrent networks after pretrained with the two approaches become more stable to train and generalize better. With pretraining, we were able to
achieve strong performance in many classification tasks, such as text classification
with IMDB, DBpedia or image recognition in CIFAR-10.

1

Introduction

Recurrent neural networks (RNNs) are powerful tools for modeling sequential data, yet training
them by back-propagation through time [37, 27] can be difficult [9]. For that reason, RNNs have
rarely been used for natural language processing tasks such as text classification despite their ability
to preserve word ordering.
On a variety of document classification tasks, we find that it is possible to train an LSTM [10] RNN
to achieve good performance with careful tuning of hyperparameters. We also find that a simple
pretraining step can significantly stabilize the training of LSTMs. A simple pretraining method is
to use a recurrent language model as a starting point of the supervised network. A slightly better
method is to use a sequence autoencoder, which uses a RNN to read a long input sequence into
a single vector. This vector will then be used to reconstruct the original sequence. The weights
obtained from pretraining can then be used as an initialization for the standard LSTM RNNs. We
believe that this semi-supervised approach [1] is superior to other unsupervised sequence learning
methods, e.g., Paragraph Vectors [19], because it can allow for easy fine-tuning.
In our experiments with document classification tasks with 20 Newsgroups [17] and DBpedia [20],
and sentiment analysis with IMDB [22] and Rotten Tomatoes [26], LSTMs pretrained by recurrent
language models or sequence autoencoders are usually better than LSTMs initialized randomly.
Another important result from our experiments is that it is possible to use unlabeled data from related tasks to improve the generalization of a subsequent supervised model. For example, using
unlabeled data from Amazon reviews to pretrain the sequence autoencoders can improve classification accuracy on Rotten Tomatoes from 79.0% to 83.3%, an equivalence of adding substantially
more labeled data. This evidence supports the thesis that it is possible to use unsupervised learning
with more unlabeled data to improve supervised learning. With sequence autoencoders, and outside
unlabeled data, LSTMs are able to match or surpass previously reported results.
1

Our semi-supervised learning approach is related to Skip-Thought vectors [14], with two differences.
The first difference is that Skip-Thought is a harder objective, because it predicts adjacent sentences.
The second is that Skip-Thought is a pure unsupervised learning algorithm, without fine-tuning.

2

Sequence autoencoders and recurrent language models

Our approach to sequence autoencoding is inspired by the work in sequence to sequence learning
(also known as seq2seq) by Sutskever et al. [32], which has been successfully used for machine
translation [21, 11], text parsing [33], image captioning [35], video analysis [31], speech recognition [4] and conversational modeling [28, 34]. Key to their approach is the use of a recurrent
network as an encoder to read in an input sequence into a hidden state, which is the input to a
decoder recurrent network that predicts the output sequence.
The sequence autoencoder is similar to the above concept, except that it is an unsupervised learning
model. The objective is to reconstruct the input sequence itself. That means we replace the output
sequence in the seq2seq framework with the input sequence. In our sequence autoencoders, the
weights for the decoder network and the encoder network are the same (see Figure 1).

Figure 1: The sequence autoencoder for the sequence ?WXYZ?. The sequence autoencoder uses
a recurrent network to read the input sequence in to the hidden state, which can then be used to
reconstruct the original sequence.
We find that the weights obtained from the sequence autoencoder can be used as an initialization
of another supervised network, one which tries to classify the sequence. We hypothesize that this
is because the network can already memorize the input sequence. This reason, and the fact that the
gradients have shortcuts, are our hypothesis of why the sequence autoencoder is a good and stable
approach in initializing recurrent networks.
A significant property of the sequence autoencoder is that it is unsupervised, and thus can be trained
with large quantities of unlabeled data to improve its quality. Our result is that additional unlabeled
data can improve the generalization ability of recurrent networks. This is especially useful for tasks
that have limited labeled data.
We also find that recurrent language models [2, 24] can be used as a pretraining method for LSTMs.
This is equivalent to removing the encoder part of the sequence autoencoder in Figure 1. Our
experimental results show that this approach works better than LSTMs with random initialization.

3

Overview of baselines

In our experiments, we use LSTM recurrent networks [10] because they are generally better than
RNNs. Our LSTM implementation is standard and has input gates, forget gates, and output gates [6,
7, 8]. We compare this basic LSTM against a LSTM initialized with the sequence autoencoder
method. When the LSTM is initialized with a sequence autoencoder, the method is called SA-LSTM
in our experiments. When LSTM is initialized with a language model, the method is called LMLSTM. We also compare our method to other baselines, e.g., bag-of-words methods or paragraph
vectors, previously reported on the same datasets.
In most of our experiments our output layer predicts the document label from the LSTM output
at the last timestep. We also experiment with the approach of putting the label at every timestep
and linearly increasing the weights of the prediction objectives from 0 to 1 [25]. This way we can
inject gradients to earlier steps in the recurrent networks. We call this approach linear label gain.
2

Lastly, we also experiment with the method of jointly training the supervised learning task with the
sequence autoencoder and call this method joint training.

4

Experiments

In our experiments with LSTMs, we follow the basic recipes as described in [7, 32] by clipping the
cell outputs and gradients. The benchmarks of focus are text understanding tasks, with all datasets
being publicly available. The tasks are sentiment analysis (IMDB and Rotten Tomatoes) and text
classification (20 Newsgroups and DBpedia). Commonly used methods on these datasets, such as
bag-of-words or n-grams, typically ignore long-range ordering information (e.g., modifiers and their
objects may be separated by many unrelated words); so one would expect recurrent methods which
preserve ordering information to perform well. Nevertheless, due to the difficulty in optimizing
these networks, recurrent models are not the method of choice for document classification.
In our experiments with the sequence autoencoder, we train it to reproduce the full document after
reading all the input words. In other words, we do not perform any truncation or windowing. We
add an end of sentence marker to the end of each input sequence and train the network to start
reproducing the sequence after that marker. To speed up performance and reduce GPU memory
usage, we perform truncated backpropagation up to 400 timesteps from the end of the sequence. We
preprocess the text so that punctuation is treated as separate tokens and we ignore any non-English
characters and words in the DBpedia text. We also remove words that only appear once in each
dataset and do not perform any term weighting or stemming.
After training the recurrent language model or the sequence autoencoder for roughly 500K steps
with a batch size of 128, we use both the word embedding parameters and the LSTM weights to
initialize the LSTM for the supervised task. We then train on that task while fine tuning both the
embedding parameters and the weights and use early stopping when the validation error starts to
increase. We choose the dropout parameters based on a validation set.
Using SA-LSTMs, we are able to match or surpass reported results for all datasets. It is important
to emphasize that previous best results come from various different methods. So it is significant
that one method achieves strong results for all datasets, presumably because such a method can be
used as a general model for any similar task. A summary of results in the experiments are shown in
Table 1. More details of the experiments are as follows.
Table 1: A summary of the error rates of SA-LSTMs and previous best reported results.
Dataset
SA-LSTM Previous best result
IMDB
Rotten Tomatoes
20 Newsgroups
DBpedia

4.1

7.24%
16.7%
15.6%
1.19%

7.42%
18.5%
17.1%
1.74%

Sentiment analysis experiments with IMDB

In this first set of experiments, we benchmark our methods on the IMDB movie sentiment dataset,
proposed by Maas et al. [22].1 There are 25,000 labeled and 50,000 unlabeled documents in the
training set and 25,000 in the test set. We use 15% of the labeled training documents as a validation
set. The average length of each document is 241 words and the maximum length of a document is
2,526 words. The previous baselines are bag-of-words, ConvNets [13] or Paragraph Vectors [19].
Since the documents are long, one might expect that it is difficult for recurrent networks to learn. We
however find that with tuning, it is possible to train LSTM recurrent networks to fit the training set.
For example, if we set the size of hidden state to be 512 units and truncate the backprop to be 400,
an LSTM can do fairly well. With random embedding dimension dropout [38] and random word
dropout (not published previously), we are able to reach performance of around 86.5% accuracy in
the test set, which is approximately 5% worse than most baselines.
1

http://ai.Stanford.edu/amaas/data/sentiment/index.html

3

Fundamentally, the main problem with this approach is that it is unstable: if we were to increase the
number of hidden units or to increase the number of backprop steps, the training breaks down very
quickly: the objective function explodes even with careful tuning of the gradient clipping. This is
because LSTMs are sensitive to the hyperparameters for long documents. In contrast, we find that
the SA-LSTM works better and is more stable. If we use the sequence autoencoders, changing the
size of the hidden state or the number of backprop steps hardly affects the training of LSTMs. This
is important because the models become more practical to train.
Using sequence autoencoders, we overcome the optimization instability in LSTMs in such a way
that it is fast and easy to achieve perfect classification on the training set. To avoid overfitting, we
again use input dimension dropout, with the dropout rate chosen on a validation set. We find that
dropping out 80% of the input embedding dimensions works well for this dataset. The results of
our experiments are shown in Table 2 together with previous baselines. We also add an additional
baseline where we initialize a LSTM with word2vec embeddings on the training set.

Table 2: Performance of models on the IMDB sentiment classification task.
Model

Test error rate

LSTM with tuning and dropout
LSTM initialized with word2vec embeddings
LM-LSTM (see Section 2)
SA-LSTM (see Figure 1)
SA-LSTM with linear gain (see Section 3)
SA-LSTM with joint training (see Section 3)

13.50%
10.00%
7.64%
7.24%
9.17%
14.70%

Full+Unlabeled+BoW [22]
WRRBM + BoW (bnc) [22]
NBSVM-bi (Na??ve Bayes SVM with bigrams) [36]
seq2-bown-CNN (ConvNet with dynamic pooling) [12]
Paragraph Vectors [19]

11.11%
10.77%
8.78%
7.67%
7.42%

The results confirm that SA-LSTM with input embedding dropout can be as good as previous best
results on this dataset. In contrast, LSTMs without sequence autoencoders have trouble in optimizing the objective because of long range dependencies in the documents.
Using language modeling (LM-LSTM) as an initialization works well, achieving 8.98%, but less
well compared to the SA-LSTM. This is perhaps because language modeling is a short-term objective, so that the hidden state only captures the ability to predict the next few words.
In the above table, we use 1,024 units for memory cells, 512 units for the input embedding layer in
the LM-LSTM and SA-LSTM. We also use a hidden layer 30 units with dropout of 50% between the
last hidden state and the classifier. We continue to use these settings in the following experiments.
In Table 3, we present some examples from the IMDB dataset that are correctly classified by SALSTM but not by a bigram NBSVM model. These examples often have long-term dependencies or
have sarcasm that is difficult to detect by solely looking at short phrases.
4.2

Sentiment analysis experiments with Rotten Tomatoes and the positive effects of
additional unlabeled data

The success on the IMDB dataset convinces us to test our methods on another sentiment analysis
task to see if similar gains can be obtained. The benchmark of focus in this experiment is the Rotten
Tomatoes dataset [26].2 The dataset has 10,662 documents, which are randomly split into 80% for
training, 10% for validation and 10% for test. The average length of each document is 22 words and
the maximum length is 52 words. Thus compared to IMDB, this dataset is smaller both in terms of
the number of documents and the number of words per document.
2

http://www.cs.cornell.edu/people/pabo/movie-review-data/

4

Table 3: IMDB sentiment classification examples that are correctly classified by SA-LSTM and
incorrectly by NBSVM-bi.
Text

Sentiment

Looking for a REAL super bad movie? If you wanna have great fun, don?t hesitate and
check this one! Ferrigno is incredibly bad but is also the best of this mediocrity.

Negative

A professional production with quality actors that simply never touched the heart or the
funny bone no matter how hard it tried. The quality cast, stark setting and excellent
cinemetography made you hope for Fargo or High Plains Drifter but sorry, the soup had
no seasoning...or meat for that matter. A 3 (of 10) for effort.

Negative

The screen-play is very bad, but there are some action sequences that i really liked. I
think the image is good, better than other romanian movies. I liked also how the actors
did their jobs.

Negative

Our first observation is that it is easier to train LSTMs on this dataset than on the IMDB dataset
and the gaps between LSTMs, LM-LSTMs and SA-LSTMs are smaller than before. This is because
movie reviews in Rotten Tomatoes are sentences whereas reviews in IMDB are paragraphs.
As this dataset is small, our methods tend to severely overfit the training set. Combining SA-LSTMs
with 95% input embedding and 50% word dropout improves generalization and allows the model
to achieve 19.3% test set error.Tuning the SA-LSTM further on the validation set can improve the
result to 19.3% error rate on the test set.
To better the performance, we add unlabeled data from the IMDB dataset in the previous experiment
and Amazon movie reviews [23] to the autoencoder training stage.3 We also run a control experiment
where we use the pretrained word vectors trained by word2vec from Google News.

Table 4: Performance of models on the Rotten Tomatoes sentiment classification task.
Model

Test error rate

LSTM with tuning and dropout
LM-LSTM
LSTM with linear gain
SA-LSTM

20.3%
21.9%
22.2%
19.3%

LSTM with word vectors from word2vec Google News
SA-LSTM with unlabeled data from IMDB
SA-LSTM with unlabeled data from Amazon reviews

20.5%
18.6%
16.7%

MV-RNN [29]
NBSVM-bi [36]
CNN-rand [13]
CNN-non-static (ConvNet with word vectors from word2vec Google News) [13]

21.0%
20.6%
23.5%
18.5%

The results for this set of experiments are shown in Table 4. Our observation is that if we use the
word vectors from word2vec, there is only a small gain of 0.5%. This is perhaps because the recurrent weights play an important role in our model and are not initialized properly in this experiment.
However, if we use IMDB to pretrain the sequence autoencoders, the error decreases from 20.5%
to 18.6%, nearly a 2% gain in accuracy; if we use Amazon reviews, a larger unlabeled dataset (7.9
million movie reviews), to pretrain the sequence autoencoders, the error goes down to 16.7% which
is another 2% gain in accuracy.
3
The dataset is available at http://snap.stanford.edu/data/web-Amazon.html, which has
34 million general product reviews, but we only use 7.9 million movie reviews in our experiments.

5

This brings us to the question of how well this method of using unlabeled data fares compared to
adding more labeled data. As argued by Socher et al. [30], a reason of why the methods are not
perfect yet is the lack of labeled training data, they proposed to use more labeled data by labeling an
addition of 215,154 phrases created by the Stanford Parser. The use of more labeled data allowed
their method to achieve around 15% error in the test set, an improvement of approximately 5% over
older methods with less labeled data.
We compare our method to their reported results [30] on sentence-level classification. As our method
does not have access to valuable labeled data, one might expect that our method is severely disadvantaged and should not perform on the same level. However, with unlabeled data and sequence
autoencoders, we are able to obtain 16.7%, ranking second amongst many other methods that have
access to a much larger corpus of labeled data. The fact that unlabeled data can compensate for the
lack of labeled data is very significant as unlabeled data are much cheaper than labeled data. The
results are shown in Table 5.
Table 5: More unlabeled data vs. more labeled data. Performance of SA-LSTM with additional
unlabeled data and previous models with additional labeled data on the Rotten Tomatoes task.
Model

Test error rate

LSTM initialized with word2vec embeddings trained on Amazon reviews
SA-LSTM with unlabeled data from Amazon reviews

21.7%
16.7%

NB [30]
SVM [30]
BiNB [30]
VecAvg [30]
RNN [30]
MV-RNN [30]
RNTN [30]

18.2%
20.6%
16.9%
19.9%
17.6%
17.1%
14.6%

4.3

Text classification experiments with 20 newsgroups

The experiments so far have been done on datasets where the number of tokens in a document is
relatively small, a few hundred words. Our question becomes whether it is possible to use SALSTMs for tasks that have a substantial number of words, such as web articles or emails and where
the content consists of many different topics.
For that purpose, we carry out the next experiments on the 20 newsgroups dataset [17].4 There are
11,293 documents in the training set and 7,528 in the test set. We use 15% of the training documents
as a validation set. Each document is an email with an average length of 267 words and a maximum
length of 11,925 words. Attachments, PGP keys, duplicates and empty messages are removed. As
the newsgroup documents are long, it was previously considered improbable for recurrent networks
to learn anything from the dataset. The best methods are often simple bag-of-words.
We repeat the same experiments with LSTMs and SA-LSTMs on this dataset. Similar to observations made in previous experiments, SA-LSTMs are generally more stable to train than LSTMs.
To improve generalization of the models, we again use input embedding dropout and word dropout
chosen on the validation set. With 70% input embedding dropout and 75% word dropout, SA-LSTM
achieves 15.6% test set error which is much better than previous classifiers in this dataset. Results
are shown in Table 6.
4.4

Character-level document classification experiments with DBpedia

In this set of experiments, we turn our attention to another challenging task of categorizing
Wikipedia pages by reading character-by-character inputs. The dataset of attention is the DBpedia
dataset [20], which was also used to benchmark convolutional neural nets in Zhang and LeCun [39].
4

http://qwone.com/?jason/20Newsgroups/

6

Table 6: Performance of models on the 20 newsgroups classification task.
Model

Test error rate

LSTM
LM-LSTM
LSTM with linear gain
SA-LSTM

18.0%
15.3%
71.6%
15.6%

Hybrid Class RBM [18]
RBM-MLP [5]
SVM + Bag-of-words [3]
Na??ve Bayes [3]

23.8%
20.5%
17.1%
19.0%

Note that unlike other datasets in Zhang and LeCun [39], DBpedia has no duplication or tainting
issues so we assume that their experimental results are valid on this dataset. DBpedia is a crowdsourced effort to extract information from Wikipedia and categorize it into an ontology.
For this experiment, we follow the same procedure suggested in Zhang and LeCun [39]. The task is
to classify DBpedia abstracts into one of 14 categories after reading the character-by-character input.
The dataset is split into 560,000 training examples and 70,000 test examples. A DBpedia document
has an average of 300 characters while the maximum length of all documents is 13,467 characters.
As this dataset is large, overfitting is not an issue and thus we do not perform any dropout on the
input or recurrent layers. For this dataset, we use a two-layered LSTM, each layer has 512 hidden
units and and the input embedding has 128 units.
Table 7: Performance of models on the DBpedia character level classification task.
Model

Test error rate

LSTM
LM-LSTM
LSTM with linear gain
SA-LSTM
SA-LSTM with linear gain
SA-LSTM with 3 layers and linear gain
SA-LSTM (word-level)

13.64%
1.50%
1.32%
2.34%
1.23%
1.19%
1.40%

Bag-of-words
Small ConvNet
Large ConvNet

3.57%
1.98%
1.73%

In this dataset, we find that the linear label gain as described in Section 3 is an effective mechanism to
inject gradients to earlier steps in LSTMs. This linear gain method works well and achieves 1.32%
test set error, which is better than SA-LSTM. Combining SA-LSTM and the linear gain method
achieves 1.19% test set error, a significant improvement from the results of convolutional networks
as shown in Table 7.
4.5

Object classification experiments with CIFAR-10

In these experiments, we attempt to see if our pre-training methods extend to non-textual data. To
do this, we train a LSTM to read the CIFAR-10 image dataset row-by-row (where the input at
each timestep is an entire row of pixels) and output the class of the image at the end. We use the
same method as in [16] to perform data augmentation. We also trained a LSTM to do next row
prediction given the current row (we denote this as LM-LSTM) and a LSTM to predict the image
by rows after reading all its rows (SA-LSTM). We then fine-tune these on the classification task.
We present the results in Table 8. While we do not achieve the results attained by state of the
art convolutional networks, our 2-layer pretrained LM-LSTM is able to exceed the results of the
7

baseline convolutional DBN model [15] despite not using any convolutions and outperforms the non
pre-trained LSTM.
Table 8: Performance of models on the CIFAR-10 object classification task.
Model

5

Test error rate

1-layer LSTM
1-layer LM-LSTM
1-layer SA-LSTM

25.0%
23.1%
25.1%

2-layer LSTM
2-layer LM-LSTM
2-layer SA-LSTM

26.0%
18.7%
26.0%

Convolution DBNs [15]

21.1%

Discussion

In this paper, we found that it is possible to use LSTM recurrent networks for NLP tasks such as
document classification. We also find that a language model or a sequence autoencoder can help
stabilize the learning in recurrent networks. On five benchmarks that we tried, LSTMs can become
a general classifier that reaches or surpasses the performance levels of all previous baselines.
Acknowledgements: We thank Oriol Vinyals, Ilya Sutskever, Greg Corrado, Vijay Vasudevan,
Manjunath Kudlur, Rajat Monga, Matthieu Devin, and the Google Brain team for their help.

References
[1] R. K. Ando and T. Zhang. A framework for learning predictive structures from multiple tasks
and unlabeled data. J. Mach. Learn. Res., 6:1817?1853, December 2005.
[2] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. A neural probabilistic language model. In
JMLR, 2003.
[3] A. Cardoso-Cachopo. Datasets for single-label text categorization. http://web.ist.
utl.pt/acardoso/datasets/, 2015. [Online; accessed 25-May-2015].
[4] William Chan, Navdeep Jaitly, Quoc V Le, and Oriol Vinyals. Listen, attend and spell. arXiv
preprint arXiv:1508.01211, 2015.
[5] Y. Dauphin and Y. Bengio. Stochastic ratio matching of RBMs for sparse high-dimensional
inputs. In NIPS, 2013.
[6] F. A. Gers, J. Schmidhuber, and F. Cummins. Learning to forget: Continual prediction with
LSTM. Neural Computation, 2000.
[7] A. Graves. Generating sequences with recurrent neural networks. In Arxiv, 2013.
[8] K. Greff, R. K. Srivastava, J. Koutn??k, B. R. Steunebrink, and J. Schmidhuber. LSTM: A search
space odyssey. In ICML, 2015.
[9] S. Hochreiter, Y. Bengio, P. Frasconi, and J. Schmidhuber. Gradient flow in recurrent nets: the
difficulty of learning long-term dependencies. A Field Guide to Dynamical Recurrent Neural
Networks, 2001.
[10] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 1997.
[11] S. Jean, K. Cho, R. Memisevic, and Y. Bengio. On using very large target vocabulary for neural
machine translation. In ICML, 2014.
[12] R. Johnson and T. Zhang. Effective use of word order for text categorization with convolutional
neural networks. In NAACL, 2014.
[13] Y. Kim. Convolutional neural networks for sentence classification, 2014.
8

[14] R. Kiros, Y. Zhu, R. Salakhutdinov, R. S. Zemel, A. Torralba, R. Urtasun, and S. Fidler. Skipthought vectors. In NIPS, 2015.
[15] A. Krizhevsky. Convolutional deep belief networks on CIFAR-10. Technical report, University
of Toronto, 2010.
[16] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional
neural networks. In NIPS, 2012.
[17] K. Lang. Newsweeder: Learning to filter netnews. In ICML, 1995.
[18] H. Larochelle, M. Mandel, R. Pascanu, and Y. Bengio. Learning algorithms for the classification restricted boltzmann machine. JMLR, 2012.
[19] Q. V. Le and T. Mikolov. Distributed representations of sentences and documents. In ICML,
2014.
[20] J. Lehmann, R. Isele, M. Jakob, A. Jentzsch, D. Kontokostas, P. N. Mendes, S. Hellmann,
M. Morsey, P. van Kleef, S. Auer, et al. DBpedia ? a large-scale, multilingual knowledge base
extracted from wikipedia. Semantic Web, 2014.
[21] T. Luong, I. Sutskever, Q. V. Le, O. Vinyals, and W. Zaremba. Addressing the rare word
problem in neural machine translation. arXiv preprint arXiv:1410.8206, 2014.
[22] A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts. Learning word vectors
for sentiment analysis. In ACL, 2011.
[23] J. McAuley and J. Leskovec. Hidden factors and hidden topics: understanding rating dimensions with review text. In RecSys, pages 165?172. ACM, 2013.
[24] T. Mikolov, M. Karafi?at, L. Burget, J. Cernock`y, and S. Khudanpur. Recurrent neural network
based language model. In INTERSPEECH, 2010.
[25] J. Y. H. Ng, M. J. Hausknecht, S. Vijayanarasimhan, O. Vinyals, R. Monga, and G. Toderici.
Beyond short snippets: Deep networks for video classification. In CVPR, 2015.
[26] B. Pang and L. Lee. Seeing stars: Exploiting class relationships for sentiment categorization
with respect to rating scales. In ACL, 2005.
[27] D. Rumelhart, G. E. Hinton, and R. J. Williams. Learning representations by back-propagating
errors. Nature, 1986.
[28] L. Shang, Z. Lu, and H. Li. Neural responding machine for short-text conversation. In EMNLP,
2015.
[29] R. Socher, B. Huval, C. D. Manning, and A. Y. Ng. Semantic compositionality through recursive matrix-vector spaces. In EMNLP, 2012.
[30] R. Socher, A. Perelygin, J. Y. Wu, J. Chuang, C. D. Manning, A. Y. Ng, and C. Potts. Recursive
deep models for semantic compositionality over a sentiment treebank. In EMNLP, 2013.
[31] N. Srivastava, E. Mansimov, and R. Salakhutdinov. Unsupervised learning of video representations using LSTMs. In ICML, 2015.
[32] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks.
In NIPS, 2014.
[33] O. Vinyals, L. Kaiser, T. Koo, S. Petrov, I. Sutskever, and G. Hinton. Grammar as a foreign
language. In NIPS, 2015.
[34] O. Vinyals and Q. V. Le. A neural conversational model. In ICML Deep Learning Workshop,
2015.
[35] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and tell: A neural image caption
generator. In CVPR, 2014.
[36] S. I. Wang and C. D. Manning. Baselines and bigrams: Simple, good sentiment and topic
classification. In ACL, 2012.
[37] P. J. Werbos. Beyond regression: New tools for prediction and analysis in the behavioral
sciences. PhD thesis, Harvard, 1974.
[38] W. Zaremba, I. Sutskever, and O. Vinyals. Recurrent neural network regularization. arXiv
preprint arXiv:1409.2329, 2014.
[39] X. Zhang and Y. LeCun. Character-level convolutional networks for text classification. In
NIPS, 2015.
9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 2480-from-algorithmic-to-subjective-randomness.pdf

From Algorithmic to Subjective Randomness

Thomas L. Griffiths & Joshua B. Tenenbaum
{gruffydd,jbt}@mit.edu
Massachusetts Institute of Technology
Cambridge, MA 02139

Abstract
We explore the phenomena of subjective randomness as a case study in
understanding how people discover structure embedded in noise. We
present a rational account of randomness perception based on the statistical problem of model selection: given a stimulus, inferring whether the
process that generated it was random or regular. Inspired by the mathematical definition of randomness given by Kolmogorov complexity, we
characterize regularity in terms of a hierarchy of automata that augment
a finite controller with different forms of memory. We find that the regularities detected in binary sequences depend upon presentation format,
and that the kinds of automata that can identify these regularities are informative about the cognitive processes engaged by different formats.

1

Introduction

People are extremely good at finding structure embedded in noise. This sensitivity to patterns and regularities is at the heart of many of the inductive leaps characteristic of human
cognition, such as identifying the words in a stream of sounds, or discovering the presence
of a common cause underlying a set of events. These acts of everyday induction are quite
different from the kind of inferences normally considered in machine learning and statistics: human cognition usually involves reaching strong conclusions on the basis of limited
data, while many statistical analyses focus on the asymptotics of large samples.
The ability to detect structure embedded in noise has a paradoxical character: while it is
an excellent example of the kind of inference at which people excel but machines fail, it
also seems to be the source of errors in tasks at which machines regularly succeed. For
example, a common demonstration conducted in introductory psychology classes involves
presenting students with two binary sequences of the same length, such as HHTHTHTT and
HHHHHHHH, and asking them to judge which one seems more random. When students
select the former, they are told that their judgments are irrational: the two sequences are
equally random, since they have the same probability of being produced by a fair coin. In
the real world, the sense that some random sequences seem more structured than others
can lead people to a variety of erroneous inferences, whether in a casino or thinking about
patterns of births and deaths in a hospital [1].
Here we show how this paradox can be resolved through a proper understanding of what our
sense of randomness is designed to compute. We will argue that our sense of randomness is
actually extremely well-calibrated with a rational statistical computation ? just not the one
to which it is usually compared. While previous accounts criticize people?s randomness

judgments as poor estimates of the probability of an outcome, we claim that subjective
randomness, together with other everyday inductive leaps, can be understood in terms of the
statistical problem of model selection: given a set of data, evaluating hypotheses about the
process that generated it. Solving this model selection problem for small datasets requires
two ingredients: a set of hypotheses about the processes by which the data could have been
generated, and a rational statistical inference by which these hypotheses are evaluated.
We will model subjective randomness as an inference comparing the probability of a sequence under a random process, P (X|random), with the probability of that sequence
under a regular process, P (X|regular). In previous work we have shown that defining
P (X|regular) using a restricted form of Kolmogorov complexity, in which regularity is
characterized in terms of a simple computing machine, can provide a good account of human randomness judgments for binary sequences [2]. Here, we explore the consequences
of manipulating the conditions under which these sequences are presented. We will show
that the kinds of regularity to which people are sensitive depend upon whether the full sequence is presented simultaneously, or its elements are presented sequentially. By exploring how these regularities can be captured by different kinds of automata, we extend our
rational analysis of the inference involved in subjective randomness to a rational characterization of the processes underlying it: certain regularities can only be detected by automata
with a particular form of memory access, and identifying the conditions under which regularities are detectable provides insight into how characteristics of human memory interact
with rational statistical inference.

2

Kolmogorov complexity and randomness

A natural starting point for a formal account of subjective randomness is Kolmogorov complexity, which provides a mathematical definition of the randomness of a sequence in terms
of the length of the shortest computer program that would produce that sequence. The idea
of using a code based upon the length of computer programs was independently proposed
in [3], [4] and [5], although it has come to be associated with Kolmogorov. A sequence
X has Kolmogorov complexity K(X) equal to the length of the shortest program p for a
(prefix) universal Turing machine U that produces X and then halts,
K(X) =

min

p:U (p)=X

`(p),

(1)

where `(p) is the length of p in bits. Kolmogorov complexity identifies a sequence X
as random if `(X) ? K(X) is small: random sequences are those that are irreducibly
complex [4]. While not necessarily following the form of this definition, psychologists
have preserved its spirit in proposing that the perceived randomness of a sequence increases
with its complexity (eg. [6]). Kolmogorov complexity can also be used to define a variety
of probability distributions, assigning probability to events based upon their complexity.
One such distribution is algorithmic probability, in which the probability of X is
R(X) = 2?K(X) =

max

p:U (p)=X

2?`(p) .

(2)

There is no requirement that R(X) sum to one over all sequences; many probability distributions that correspond to codes are unnormalized, assigning the missing probability to an
undefined sequence.
There are three problems with using Kolmogorov complexity as the basis for a computational model of subjective randomness. Firstly, the Kolmogorov complexity of any particular sequence X is not computable [4], presenting a practical challenge for any modelling
effort. Secondly, while the universality of an encoding scheme based on Turing machines
is attractive, many of the interesting questions in cognition come from the details: issues of
representation and processing are lost in the asymptotic equivalence of coding schemes, but

play a key role in people?s judgments. Finally, Kolmogorov complexity is too permissive in
what it considers a regularity. The set of regularities identified by people are a strict subset
of those that might be expressed in short computer programs. For example, people are very
unlikely to be able to tell the difference between a binary sequence produced by a linear
congruential random number generator (a very short program) and a sequence produced by
flipping a coin, but these sequences should differ significantly in Kolmogorov complexity.
Restricting the set of regularities does not imply that people are worse than machines at
recognizing patterns: reducing the size of the set of hypotheses increases inductive bias,
making it possible to identify the presence of structure from smaller samples.

3

A statistical account of subjective randomness

While there are problems with using Kolmogorov complexity as the basis for a rational
theory of subjective randomness, it provides a clear definition of regularity. In this section
we will present a statistical account of subjective randomness in terms of a comparison between random and regular sources, where regularity is defined by analogues of Kolmogorov
complexity for simpler computing machines.
3.1

Subjective randomness as model selection

One of the most basic problems that arises in statistical inference is identifying the source
of a set of observations, based upon a set of hypotheses. This is the problem of model
selection. Model selection provides a natural basis for a statistical theory of subjective
randomness, viewing these judgments as the consequence of an inference to the process
that produced a set of observations. On seeing a stimulus X, we consider two hypotheses:
X was produced by a random process, or X was produced by a regular process. The
decision about the source of X can be formalized as a Bayesian inference,
P (random|X)
P (X|random) P (random)
=
,
P (regular|X)
P (X|regular) P (regular)

(3)

in which the posterior odds in favor of a random generating process are obtained from the
likelihood ratio and the prior odds. The only part of the right hand side of the equation
affected by X is the likelihood ratio, so we define the subjective randomness of X as
random(X) = log

P (X|random)
,
P (X|regular)

(4)

being the evidence that X provides towards the conclusion that it was produced by a random process.
3.2

The nature of regularity

In order to define random(X), we need to specify P (X|random) and P (X|regular). When
evaluating binary sequences, it is natural to set P (X|random) = ( 21 )`(X) . Taking the
logarithm in base 2, random(X) is ?`(X) ? log 2 P (X|regular), depending entirely on
P (X|regular). We obtain random(X) = K(X) ? `(X), the difference between the complexity of a sequence and its length, if we choose P (X|regular) = R(X), the algorithmic probability defined in Equation 2. This is identical to the mathematical definition of
randomness given by Kolmogorov complexity. However, the key point of this statistical
approach is that we are not restricted to using R(X): we have a measure of the randomness
of X for any choice of P (X|regular).
The choice of P (X|regular) will reflect the stimulus domain, and express the kinds of
regularity which people can detect in that domain. For binary sequences, a good candidate for specifying P (X|regular) is a hidden Markov model (HMM), a probabilistic finite

1

H

2

T

5

4

H

T
T

H
6

3

Figure 1: Finite state automaton used to define P (X|regular) to give random(X) ? DP .
Solid arrows are transitions consistent with repeating a motif, which are taken with probability ?. Dashed arrows are motif changes, using the prior determined by ?.

state automaton. In fact, specifying P (X|regular)in terms of a particular HMM results in
random(X) being equivalent to the ?Difficulty Predictor? (DP) [6] a measure of sequence
complexity that has been extremely successful in modelling subjective randomness judgments. DP measures the complexity of a sequence in terms of the number of repeating (eg.
HHHH) and alternating (eg. HTHT) subsequences it contains, adding one point for each
repeating subsequence and two points for each alternating subsequence. For example, the
sequence TTTHHHTHTH is a run of tails, a run of heads, and an alternating sub-sequence,
DP = 4. If there are several partitions into runs and alternations, DP is calculated on the
partition that results in the lowest score.
In [2], we showed that random(X) ? DP if P (X|regular) is specified by a particular HMM. This HMM produces sequences by motif repetition, using the transition graph
shown in Figure 1. The model emits sequences by choosing a motif, a sequence of symbols
of length k, with probability proportional to ?k , and emitting symbols consistent with that
motif with probability ?, switching to a new motif with probability 1 ? ?. In Figure 1,
state 1 repeats the motif H, state 2 repeats T, and the remaining states repeat the alternating motifs HT and TH. The randomness of a sequence under this definition of regularity
depends on ? and ?, but is generally affected by the number of repeating and alternating
subsequences. The equivalence to DP, in which a sequence scores a single point for each
repeating subsequence
and two points for each alternating subsequence, results from taking
?
? = 0.5 and ? = 3?1
2 , and choosing the the state sequence for the HMM that maximizes
the probability of the sequence.
Just as the algorithmic probability R(X) is a probability distribution defined by the length
of programs for a universal Turing machine, this choice of P (X|regular) can be seen as
specifying the length of ?programs? for a particular finite state automaton. The output of a
finite state automaton is determined by its state sequence, just as the output of a universal
Turing machine is determined by its program. However, since the state sequence is the
same length as the sequence itself, this alone does not provide a meaningful measure of
complexity. In our model, probability imposes a metric on state sequences, dictating a
greater cost for moves between certain states, which translates into a code length through
the logarithm. Since we find the state sequence most likely to have produced X, and thus
the shortest code length, we have an analogue of Kolmogorov complexity defined on a
finite state automaton.
3.3

Regularities and automata

Using a hidden Markov model to specify P (X|regular) provides a measure of complexity
defined in terms of a finite state automaton. However, the kinds of regularities people can
detect in binary sequences go beyond the capacity of a finite state automaton. Here, we
consider three additional regularities: symmetry (eg. THTHHTHT), symmetry in the com-

Finite state automaton
(motif repetition)

Queue automaton
(duplication)
Pushdown automaton
(symmetry)

Stack automaton

Turing machine
(all computable)

Figure 2: Hierarchy of automata used to define measures of complexity. Of the regularities
discussed in this paper, each automaton can identify all regularities identified by those
automata to its left as well as those stated in parentheses beneath its name.

plement (eg. TTTTHHHH), and the perfect duplication of subsequences (eg. HHHTHHHT
vs. HHHTHHHTH). These regularities identify formal languages that cannot be recognized
by a finite state automaton, suggesting that we might be able to develop better models of
subjective randomness by defining P (X|regular) in terms of more sophisticated automata.
The automata we will consider in this paper form a hierarchy, shown in Figure 2. This
hierarchy expresses the same content as Chomsky?s [7] hierarchy of computing machines
? the regularities identifiable by each machine are a strict superset of those identifiable
to the machine to the left ? although it features a different set of automata. The most
restricted set of regularities are those associated with the finite state automaton, and the
least restricted are those associated with the Turing machine. In between are the pushdown
automaton, which augments a finite controller with a stack memory, in which the last item
added is the first to be accessed; the queue automaton,1 in which the memory is a queue, in
which the first item added is the first to be accessed; and the stack automaton, in which the
memory is a stack but any item in the stack can be read by the controller [9, 10]. The key
difference between these kinds of automata is the memory available to the finite controller,
and exploring measures of complexity defined in terms of these automata thus involves
assessing the kind of memory required to identify regularities.
Each of the automata shown in Figure 2 can identify a different set of regularities. The
finite state automaton is only capable of identifying motif repetition, while the pushdown
automaton can identify both kinds of symmetry, and the queue automaton can identify
duplication. The stack automaton can identify all of these regularities, and the Turing
machine can identify all computable regularities. For each of the sub-Turing automata,
we can use these constraints to specify a probabilistic model for P (X|regular). For example, the probabilistic model corresponding to the pushdown automaton generates regular sequences by three methods: repetition, producing sequences with probabilities determined by the HMM introduced above; symmetry, where half of the sequence is produced
by the HMM and the second half is produced by reflection; and complement symmetry,
where the second half is produced by reflection and exchanging H and T. We then take
P (X|regular) = maxZ,M P (X, Z|M )P (M ), where M is the method of production and
Z is the state sequence for the HMM. Similar models can be defined for the queue and
stack automata, with the queue automaton allowing generation by repetition or duplication,
and the stack automaton allowing any of these four methods. Each regularity introduced
into the model requires a further parameter in specifying P (M ), so the hierarchy shown
in Figure 2 also expresses the statistical structure of this set of models: each model is a
special case of the model to its right, in which some regularities are eliminated by setting
P (M ) to zero. We can use this structure to perform model selection with likelihood ratio
tests, determining which model gives the best account of a particular dataset using just the
difference in the log-likelihoods. We apply this method in the next section.
1

An unrestricted queue automaton is equivalent to a Turing machine. We will use the phrase to
refer to an automaton in which the number of queue operations that can be performed for each input
symbol is limited, which is generally termed a quasi real time queue automaton [8].

4

Testing the models

The models introduced in the previous section differ in the memory systems with which
they augment the finite controller. The appropriateness of any one measure of complexity
to a particular task may thus depend upon the memory demands placed upon the participant. To explore this hypothesis, we conducted an experiment in which participants make
randomness judgments after either seeing a sequence in its entirety, or seeing each element
one after another. We then used model selection to determine which measure of complexity gave the best account of each condition, illustrating how the strategy of defining
more restricted forms of complexity can shed light into the cognitive processes underlying
regularity detection.
4.1

Experimental methods

There were two conditions in the experiment, corresponding to Simultaneous and Sequential presentation of stimuli. The stimuli were sequences of heads (H) and tails (T) presented
in 130 point fixed width sans-serif font on a 19? monitor at 1280 ? 1024 pixel resolution.
In the Simultaneous condition, all eight elements of the sequence appeared on the display
simultaneously. In the Sequential condition, the elements appeared one by one, being displayed for 300ms with a 300ms inter-stimulus interval.
The participants were 40 MIT undergraduates, randomly assigned to the two conditions.
Participants were instructed that they were about to see sequences which had either been
produced by a random process (flipping a fair coin) or by other processes in which the
choice of heads and tails was not random, and had to classify these sequences according
to their source. After a practice session, each participant classified all 128 sequences of
length 8, in random order, with each sequence randomly starting with either a head or a
tail. Participants took breaks at intervals of 32 sequences.
4.2

Results and Discussion

We analyzed the results by fitting the models corresponding to the four automata described above, using all motifs up to length 4 to specify the basic model. We computed
random(X) for each stimulus as in Eq. (4), with P (X|regular) specified by the probabilistic model corresponding to each of the automata. We then converted this log-likelihood
ratio into the posterior probability of a random generating process, using
P (random|X) =

1
1 + exp{?? random(X) ? ?}

where ? and ? are parameters weighting the contribution of the likelihoods and the priors respectively. We then optimized ?, ?, ?, ? and the parameters contributing to P (M )
for each model, maximizing the likelihood of the classifications of the sequences by the
20 participants in each of the 2 conditions. The results of the model-fitting are shown in
Figure 3(a) and (b), which indicate the relationship between the posterior probabilities predicted by the model and the proportion of participants who classified a sequence as random.
The correlation coefficients shown in the figure provide a relatively good indicator of the
fit of the models, and each sequence is labelled according to the regularity it expresses,
showing how accommodating particular regularities contributes to the fit.
The log-likelihood scores obtained from fitting the models can be used for model selection, testing whether any of the parameters involved in the models are unnecessary. Since
the models form a nested hierarchy, we can use likelihood ratio tests to evaluate whether
introducing a particular regularity (and the parameters associated with it) results in a statistically significant improvement in fit. Specifically, if model 1 has log-likelihood L1 and
df1 parameters, and model 2 has log-likelihood L2 and df2 > df1 parameters, 2(L2 ? L1 )

1

P(random|x)

Finite state
(a)

0.5

Pushdown

r=0.69

0

0

r=0.79

1

P(random|x)

0.5

0

Repetition
Symmetry
Complement
Duplication
Pushdown
r=0.70

r=0.70

0

Stack
r=0.83

0.5
1
Simultaneous data
Finite state

(b)

Queue
r=0.76

0.5
Sequential data

(c) 57.43 (1df, p < 0.0001)

Stack

r=0.76

r=0.77

1

Queue

Finite state
87.76 (2df, p < 0.0001)

Queue

75.41 (2df, p < 0.0001)
Stack

Pushdown

45.08 (1df, p < 0.0001)

(d) 33.24 (1df, p < 0.0001)

Queue

5.69 (2df, p = 0.0582)

Pushdown

31.42 (1df, p < 0.0001)

Finite state
1.82 (2df, p = 0.4025)

Stack

Figure 3: Experimental results for (a) the Simultaneous and (b) the Sequential condition,
showing the proportion of participants classifying a sequence as ?random? (horizontal axis)
and P (random|X) (vertical axis) as assessed by the four models. Points are labelled according to their parse under the Stack model. (c) and (d) show the model selection results
for the Simultaneous and Sequential conditions respectively, showing the four automata
with edges between them labelled with ?2 score (df, p-value) for improvement in fit.

should have a ?2 (df2 ? df1 ) distribution under the null hypothesis of no improvement in
fit. We evaluated the pairwise likelihood ratio tests for the four models in each condition,
with the results shown in Figure 3(c) and (d). Additional regularities always improved the
fit for the Simultaneous condition, while adding duplication, but not symmetry, resulted in
a statistically significant improvement in the Sequential condition.
The model selection results suggest that the best model for the Simultaneous condition
is the stack automaton, while the best model for the Sequential condition is the queue
automaton. These results indicate the importance of presentation format in determining
subjective randomness, as well as the benefits of exploring measures of complexity defined
in terms of a range of computing machines. The stack automaton can evaluate regularities
that require checking information in arbitrary positions in a sequence, something that is
facilitated by a display in which the entire sequence is available. In contrast, the queue
automaton can only access information in the order that it enters memory, and gives a
better match to the task in which working memory is required. This illustrates an important
fact about cognition ? that human working memory operates like a queue rather than a stack
? that is highlighted by this approach.
The final parameters of the best-fitting models provide some insight into the relative importance of the different kinds of regularities under different presentation conditions. For the
Simultaneous condition, ? = 0.66, ? = 0.12, ? = 0.26, ? = ?1.98 and motif repetition,
symmetry, symmetry in the complement, and duplication were given probabilities of 0.748,
0.208, 0.005, and 0.039 respectively. Symmetry is thus a far stronger characteristic of reg-

ularity than either symmetry in the complement or duplication, when entire sequences are
viewed simultaneously. For the Sequential condition, ? = 0.70, ? = 0.11, ? = 0.38, ? =
?1.24, and motif repetition was given a probability of 0.962 while duplication had a probability of 0.038, with both forms of symmetry being given zero probability since the queue
model provided the best fit. Values of ? > 0.5 for both models indicates that regular sequences tend to repeat motifs, rather than rapidly switching between them, and the low ?
values reflect a preference for short motifs.

5

Conclusion

We have outlined a framework for understanding the rational basis of the human ability to
find structure embedded in noise, viewing this inference in terms of the statistical problem of model selection. Solving this problem for small datasets requires two ingredients:
strong prior beliefs about the hypothetical mechanisms by which the data could have been
generated, and a rational statistical inference by which these hypotheses are evaluated.
When assessing the randomness of binary sequences, which involves comparing random
and regular sources, people?s beliefs about the nature of regularity can be expressed in
terms of probabilistic versions of simple computing machines. Different machines capture
regularity when sequences are presented simultaneously and when their elements are presented sequentially, and the differences between these machines provide insight into the
cognitive processes involved in the task. Analyses of the rational basis of human inference
typically either ignore questions about processing or introduce them as relatively arbitrary
constraints. Here, we are able to give a rational characterization of process as well as inference, evaluating a set of alternatives that all correspond to restrictions of Kolmogorov
complexity to simple general-purpose automata.
Acknowledgments. This work was supported by a Stanford Graduate Fellowship to the first author.
We thank Charles Kemp and Michael Lee for useful comments.

References
[1] D. Kahneman and A. Tversky. Subjective probability: A judgment of representativeness. Cognitive Psychology, 3:430?454, 1972.
[2] T. L. Griffiths and J. B. Tenenbaum. Probability, algorithmic complexity and subjective randomness. In Proceedings of the 25th Annual Conference of the Cognitive Science Society, Hillsdale,
NJ, 2003. Erlbaum.
[3] R. J. Solomonoff. A formal theory of inductive inference. Part I. Information and Control,
7:1?22, 1964.
[4] A. N. Kolmogorov. Three approaches to the quantitative definition of information. Problems of
Information Transmission, 1:1?7, 1965.
[5] G. J. Chaitin. On the length of programs for computing finite binary sequences: statistical
considerations. Journal of the ACM, 16:145?159, 1969.
[6] R. Falk and C. Konold. Making sense of randomness: Implicit encoding as a bias for judgment.
Psychological Review, 104:301?318, 1997.
[7] N. Chomsky. Threee models for the description of language. IRE Transactions on Information
Theory, 2:113?124, 1956.
[8] A. Cherubini, C. Citrini, S. C. Reghizzi, and D. Mandrioli. QRT FIFO automata, breadth-first
grammars and their relations. Theoretical Comptuer Science, 85:171?203, 1991.
[9] S. Ginsburg, S. A. Greibach, and M. A. Harrison. Stack automata and compiling. Journal of
the ACM, 14:172?201, 1967.
[10] A. V. Aho. Indexed grammars ? an extension of context-free grammars. Journal of the ACM,
15:647?671, 1968.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 40-neural-networks-for-template-matching-application-to-real-time-classification-of-the-action-potentials-of-real-neurons.pdf

103

NEURAL NETWORKS FOR TEMPLATE MATCHING:
APPLICATION TO REAL-TIME CLASSIFICATION
OF THE ACTION POTENTIALS OF REAL NEURONS
Yiu-fai Wongt, Jashojiban Banikt and James M. Bower!
tDivision of Engineering and Applied Science
!Division of Biology
California Institute of Technology
Pasadena, CA 91125

ABSTRACT
Much experimental study of real neural networks relies on the proper classification of
extracellulary sampled neural signals (i .e. action potentials) recorded from the brains of experimental animals. In most neurophysiology laboratories this classification task is simplified
by limiting investigations to single, electrically well-isolated neurons recorded one at a time.
However, for those interested in sampling the activities of many single neurons simultaneously,
waveform classification becomes a serious concern. In this paper we describe and constrast
three approaches to this problem each designed not only to recognize isolated neural events,
but also to separately classify temporally overlapping events in real time. First we present two
formulations of waveform classification using a neural network template matching approach.
These two formulations are then compared to a simple template matching implementation.
Analysis with real neural signals reveals that simple template matching is a better solution to
this problem than either neural network approach.

INTRODUCTION
For many years, neurobiologists have been studying the nervous system by
using single electrodes to serially sample the electrical activity of single neurons in the brain. However, as physiologists and theorists have become more
aware of the complex, nonlinear dynamics of these networks, it has become
apparent that serial sampling strategies may not provide all the information
necessary to understand functional organization. In addition, it will likely be
necessary to develop new techniques which sample the activities of multiple
neurons simultaneouslyl. Over the last several years, we have developed two
different methods to acquire multineuron data. Our initial design involved
the placement of many tiny micro electrodes individually in a tightly packed
pseudo-floating configuration within the brain 2 . More recently we have been
developing a more sophisticated approach which utilizes recent advances in
silicon technology to fabricate multi-ported silicon based electrodes (Fig. 1) .
Using these electrodes we expect to be able to readily record the activity patterns of larger number of neurons.
As research in multi-single neuron recording techniques continue, it has become very clear that whatever technique is used to acquire neural signals from
many brain locations, the technical difficulties associated with sampling, data
compressing, storing, analyzing and interpreting these signals largely dwarf the
development of the sampling device itself. In this report we specifically consider
the need to assure that neural action potentials (also known as "spikes") on
each of many parallel recording channels are correctly classified, which is just
one aspect of the problem of post-processing multi-single neuron data. With
more traditional single electrode/single neuron recordings, this task usually in? American Institute or Physics 1988

104

volves passing analog signals through a Schmidt trigger whose output indicates
the occurence of an event to a computer, at the same time as it triggers an
oscilloscope sweep of the analog data. The experimenter visually monitors the
oscilloscope to verify the accuracy of the discrimination as a well-discriminated
signal from a single neuron will overlap on successive oscilloscope traces (Fig.
Ic). Obviously this approach is impractical when large numbers of channels
are recorded at the same time. Instead, it is necessary to automate this classification procedure. In this paper we will describe and contrast three approaches
we have developed to do this .

Traces
on upper
layer

~

'a.
E

IV

1~
0

Traces

4

2
Ume (msec)

on lower
layer

C. ,

&.

Recording s~e

b.

75sq.jllT1

Fig. 1. Silicon probe being developed in our lababoratory for multi-single unit recording
in cerebellar cortex. a) a complete probe; b) surface view of one recording tip; c) several
superimposed neuronal action potentials recorded from such a silicon electrode ill cerebellar
cortex.

While our principal design objective is the assurance that neural waveforms
are adequately discriminated on multiple channels, technically the overall objective of this research project is to sample from as many single neurons as
possible. Therefore, it is a natural extention of our effort to develop a neural
waveform classification scheme robust enough to allow us to distinguish activities arising from more than one neuron per recording site. To do this, however,
we now not only have to determine that a particular signal is neural in origin,
but also from which of several possible neurons it arose (see Fig. 2a). While
in general signals from different neurons have different waveforms aiding in
the classification, neurons recorded on the same channel firing simultaneously
or nearly simultaneously will produce novel combination waveforms (Fig. 2b)
which also need to be classified. It is this last complication which particularly

105

bedevils previous efforts to classify neural signals (For review see 5, also see
3-4). In summary, then, our objective was to design a circuit that would:
1. distinguish different waveforms even though neuronal discharges tend
to be quite similar in shape (Fig. 2a);
2. recognize the same waveform even though unavoidable movements
such as animal respiration often result in periodic changes in the amplitude
of a recorded signal by moving the brain relative to the tip of the electrode;
3. be considerably robust to recording noise which variably corrupts all
neural recordings (Fig. 2);
4. resolve overlapping waveforms, which are likely to be particularly interesting events from a neurobiological point of view;
5. provide real-time performance allowing the experimenter to detect
problems with discrimination and monitor the progress of the experiment;
6. be implementable in hardware due to the need to classify neural signals on many channels simultaneously. Simply duplicating a software-based
algorithm for each channel will not work, but rather, multiple, small, independent, and programmable hardware devices need to be constructed.

I
b.

50 Jl.V

signal recorded

c.

electrode
a.

Fig. 2. a) Schematic diagram of an electrode recording from two neuronal cell bodies b) An
actual multi-neuron recording. Note the similarities in the two waveforms and the overlapping
event. c) and d) Synthesized data with different noise levels for testing classificat.ion algorithms
(c : 0.3 NSR ; d: 1.1 NSR) .

106

METHODS
The problem of detecting and classifying multiple neural signals on single voltage records involves two steps. First, the waveforms that are present
in a particular signal must be identified and the templates be generated;
second, these waveforms must be detected and classified in ongoing data
records. To accomplish the first step we have modified the principal component analysis procedure described by Abeles and Goldstein 3 to automatically extract templates of the distinct waveforms found in an initial sample of the digitized analog data. This will not be discussed further as it is
the means of accomplishing the second step which concerns us here. Specifically, in this paper we compare three new approaches to ongoing waveform classification which deal explicitly with overlapping spikes and variably meet other design criteria outlined above. These approaches consist of
a modified template matching scheme, and two applied neural network implementations. We will first consider the neural network approaches. On
a point of nomenclature, to avoid confusion in what follows, the real neurons whose signals we want to classify will be referred to as "neurons" while
computing elements in the applied neural networks will be called "Hopons."

Neural Network Approach - Overall, the problem of classifying neural
waveforms can best be seen as an optimization problem in the presence of
noise. Much recent work on neural-type network algorithms has demonstrated
that these networks work quite well on problems of this sort 6- 8 . In particular,
in a recent paper Hopfield and Tank describe an A/D converter network and
suggest how to map the problem of template matching into a similar context 8 .
The energy functional for the network they propose has the form:
- 1

E = - 2 ~~
'" '" T.I] v..v.
I]
1

]

- '"
~ VI

II

(1)

1

where Tij = connectivity between Hopon i and Hopon y', V; = voltage output
of Hopon i, Ii = input current to Hopon i and each Hopon has a sigmoid
input-output characteristic V = g(u) = 1/(1 + exp( -au)).
If the equation of motion is set to be:

du;fdt

=

-oE/oV =

L

T;jVj

+ Ii

(la)

j

then we see that dE/dt = -(I:iTijVj + Ii)dV/dt = - (du/dt)(dV/dt) =
-g'{u)(du/dt)2 :s: O. Hence E will go to to a minimum which, in a network
constructed as described below, will correspond to a proposed solution to a
particular waveform classification problem.

Template Matching using a Hopfield-type Neural Net - We have
taken the following approach to template matching using a neural network. For
simplicity, we initially restricted the classification problem to one involving two
waveforms and have accordingly constructed a neural network made up of two
groups of Hopons, each concerned with discriminating one or the other waveform. The classification procedure works as follows: first, a Schmidt trigger

107

is used to detect the presence of a voltage on the signal channel above a set
threshold . When this threshold is crossed, implying the presence of a possible
neural signal, 2 msecs of data around the crossing are stored in a buffer (40
samples at 20 KHz). Note that biophysical limitations assure that a single real
neuron cannot discharge more than once in this time period, so only one waveform of a particular type can occur in this data sample. Also, action potentials
are of the order of 1 msec in duration, so the 2 msec window will include the full
signal for single or overlapped waveforms. In the next step (explained later)
the data values are correlated and passed into a Hopfield network designed to
minimize the mean-square error between the actual data and the linear combination of different delays of the templates. Each Hopon in the set of Hopons
concerned with one waveform represents a particular temporal delay in the
occurrence of that waveform in the buffer. To express the network in terms of
an energy function formulation: Let x(t) = input waveform amplitude in the
tth time bin, Sj(t) = amplitude of the ph template, Vjk denote if Sj(t - k)(J?th
template delayed by k time bins)is present in the input waveform. Then the
appropriate energy function is:

(2)

The first term is designed to minimize the mean-square error and specifies
the best match. Since V E [0,1]' the second term is minimized only when each
Vjk assumes values 0 or 1. It also sets the diagonal elements Tij to o. The
third term creates mutual inhibition among the processing nodes evaluating
the same neuronal signal, which as described above can only occur once per
sample.
Expanding and simplifying expression (2), the connection matrix is :

(3a)
and the input current

(3b)
As it can be seen, the inputs are the correlations between the actual data and
the various delays of the templates subtracting a constant term.

Modified Hopfield Network - As documented in more detail in Fig.
3-4, the above full Hopfield-type network works well for temporally isolated
spikes at moderate noise levels, but for overlapping spikes it has a local minima
problem. This is more severe with more than two waveforms in the network.

108

Further, we need to build our network in hardware and the full Hopfield network is difficult to implement with current technology (see below) . For these
reasons, we developed a modified neural network approach which significantly
reduces the necessary hardware complexity and also has improved performance.
To understand how this works, let us look at the information contained in the
quantities Tij and Iij (eq. 3a and 3b ) and make some use of them. These
quantities have to be calculated at a pre-processing stage before being loaded
into the Hopfield network. If after calculating these quantities, we can quickly
rule out a large number of possible template combinations, then we can significantly reduce the size of the problem and thus use a much smaller (and
hence more efficient) neural network to find the optimal solution. To make the
derivation simple, we define slightly modified versions of 1';j and Iij (eq. 4a
and 4b) for two-template case.

Iij

= L x(t) [~SI(t - i) + ~S2(t - j)] t

~L

si(t - i) -

t

~ L s~(t -

j)

(4b)

t

In the case of overlaping spikes the 1';j'S are the cross-correlations between SI (t)
and S2(t) with different delays and Ii;'s are the cross-correlations between input
x(t) and weighted combination of SI(t) and S2(t). Now if x(t) = SI(t - i) +
S2(t - J') (i.e. the overlap of the first template with i time bin delay and the
second template with j time bin delay), then I:::.ij = l1';j - Iijl = O. However
in the presence of noise, I:::. ij will not be identically zero, but will equal to the
noise, and if I:::.ij > l:::.1';j (where l:::.1';j = l1';j - 1';'j.1 for i =f: i' and j =f: l) this
simple algorithm may make unacceptable errors. A solution to this problem
for overlapping spikes will be described below, but now let us consider the
problem of classifying non-overlapping spikes. In this case, we can compare
the input cross-correlation with the auto-correlations (eq. 4c and 4d).

T! = Lsi(t - i); T!, = Ls~(t - i)

(4c)

t

(4d)
So for non-overlapping cases, if x(t) = SI(t - i), then I:::.~ = IT: - 1:1 = O. If
x(t) = S2(t - i), then 1:::.:' = IT:' - 1:'1 = o.
In the absence of noise, then the minimum of I:::. ij , 1:::.: and I:::.? represents the
correct classification. However, in the presence of noise, none of these quantities
will be identically zero, but will equal the noise in the input x(t) which will
give rise to unacceptible errors. Our solution to this noise related. problem is
to choose a few minima (three have chosen in our case) instead of one. For
each minimum there is either a known corresponding linear combination of
templates for overlapping cases or a simple template for non-overlapping cases.
A three neuron Hopfield-type network is then programmed so that each neuron
corresponds to each of the cases. The input x(t) is fed to this tiny network to
resolve whatever confusion remains after the first step of "cross-correlation"
comparisons. (Note: Simple template matching as described below can also be
used in the place of the tiny Hopfield type network.)

109

Simple Template Matching ~ To evaluate the performances of these
neural network approaches, we decided to implement a simple template matching scheme, which we will now describe. However, as documented below, this
approach turned out to be the most accurate and require the least complex
hardware of any of the three approaches. The first step is, again, to fill a buffer
with data based on the detection of a possible neural signal. Then we calculate
the difference between the recorded waveform and all possible combinations of
the two previously identified templates. Formally, this consists of calculating
the distances between the input x(m) and all possible cases generated by all
the combinations of the two templates.
d,j =

L

Ix(t) - {Sl(t - i)

+ S2(t - Jonl

t

d~

=

L
t

Ix(t) - Sl(t - i)l;

d~'

= L Ix(t) - S2(t - i)1
t

dmin = min(dij,d~,dn
dm,n gives the best fit of all possible combinations of templates to the actual
voltage signal.
TESTING PROCEDURES
To compare the performance of each of the three approaches, we devised a
common set of test data using the following procedures. First, we used the principal component method of Abeles and Goldstein 3 to generate two templates
from a digitized analog record of neural activity recorded in the cerebellum
of the rat. The two actual spike waveform templates we decided to use had
a peak-to-peak ratio of 1.375. From a second set of analog recordings made
from a site in the cerebellum in which no action potential events were evident,
we determined the spectral characteristics of the recording noise. These two
components derived from real neural recordings were then digitally combined,
the objective being to construct realistic records, while also knowing absolutely
what the correct solution to the template matching problem was for each occurring spike. As shown in Fig. 2c and 2d, data sets corresponding to different
noise to signal ratios were constructed. We also carried out simulations with
the amplitudes of the templates themselves varied in the synthesized records to
simulate waveform changes due to brain movements often seen in real recordings. In addition to two waveform test sets, we also constructed three waveform
sets by generating a third template that was the average of the first two templates. To further quantify the comparisons of the three diffferent approaches
described above we considered non-overlapping and overlapping spikes separately. To quantify the performance of the three different approaches, two
standards for classification were devised. In the first and hardest case, to be
judged a correct classification, the precise order and timing of two waveforms
had to be reconstructed. In the second and looser scheme, classification was
judged correct if the order of two waveforms was correct but timing was allowed to vary by ?lOO Jlsecs(i.e. ?2 time bins) which for most neurobiological
applications is probably sufficient resolution . Figs. 3-4 compare the performance results for the three approaches to waveform classification implemented
as digital simulations.

110

PERFORMANCE COMPARISON
Two templates - non-overlapping waveforms: As shown in Fig. 3a, at
low noise-to-signal ratios (NSRs below .2) each of the three approaches were
comparable in performance reaching close to 100% accuracy for each criterion.
As the ratio was increased, however the neural network implementations did
less and less well with respect to the simple template matching algorithm with
the full Hopfield type network doing considerably worse than the modified
network. In the range of NSR most often found in real data (.2 - .4) simple
template matching performed considerably better than either of the neural
network approaches. Also it is to be noted that simple template matching
gives an estimate of the goodness of fit betwwen the waveform and the closest
template which could be used to identify events that should not be classified
(e.g. signals due to noise).
a.

.

b.

,

..
c.

,.

..

. ..

..

..

1.1

noise level: 3a/peak amplitude

.,

,

?

//

\,

.
,

.

1.1

,.-.-..-----------.

/

,
,,

,,

I

,,

,:

.

.

.

noise level: 3a/peak amplitude

I

I

I

:'
I

,I

\,'
-14

-12

-tli

-I

-2

12

degrees of overlap
light line - absolute criteria
heavy line - less stringent criteria

simple template matching
Hopfield network
modified Hopfield network

Fig. 3. Comparisons of the three approaches detecting two non-overlapping (a), and overlapping (b) waveforms, c) compares the performances of the neural network approaches for
different degrees of waveform overlap.

Two' templates - overlapping waveforms: Fig. 3b and 3c compare performances when waveforms overlapped. In Fig. 3b the serious local minima problem encountered in the full neural network is demonstrated as is the improved
performance of the modified network. Again, overall performance in physi-

111

ological ranges of noise is clearly best for simple template matching. When
the noise level is low, the modified approach is the bet ter of the two neural
networks due to the reliability of the correlation number which reflects the
resemblence between the input data and the template. When the noise level
is high, errors in the correlation numbers may exclude the right combination
from the smaller network. In this case its performance is actually a little worse
than the larger Hopfield network. Fig. 3c documents in detail which degrees
of overlap produce the most trouble for the neural network approaches at average NSR levels found in real neural data. It can be seen that for the neural
networks, the most serious problem is encountered when the delays between
the two waveforms are small enough that the resulting waveform looks like the
larger waveform with some perturbation.
Three templates - overlapping and non-overlapping: In Fig. 4 are shown
the comparisons between the full Hopfield network approach and the simple
template matching approach. For nonoverlapping waveforms, the performance
of these two approaches is much more comparable than for the two waveform
case (Fig. 4a), although simple template matching is still the optimal method.
In the overlapping waveform condition, however, the neural network approach
fails badly (Fig. 4b and 4c). For this particular application and implementation, the neural network approach does not scale well.
b.

a.
~

!:!...

.

o ..
v

~

..

.

28

.2

c.
~

......'"
o
V

~

.

1. 1

.S

.2

noise level: 3a /peak amplitude

..
..

.4

..

.S

.. I

noise level: 3a /peak amplitude

Hopfield network
simple template matching
light line - absolute criteria
heavy line - less stringent criteria
a = variance of the noise

50

2.

.2

.6

.8

1. ?

noise level: 3a /peak amplitude
Fig. 4. Comparisons of performance for three waveforms. a) nonoverlapping waveforms; b)
two waveforms overlapping; c) three waveforms overlapping.

HARDWARE COMPARISONS
As described earlier, an important design requi~ement for this work was the
ability to <letect neural signals in analog records in real-time originating from

112

many simultaneously active sampling electrodes. Because it is not feasible to
run the algorithms in a computer in real time for all the channels simultaneously, it is necessary to design and build dedicated hardware for each channel.
To do this, we have decided to design VLSI implementations of our circuitry.
In this regard, it is well recognized that large modifiable neural networks need
very elaborate hardware implementations. Let us consider, for example, implementing hard wares for a two-template case for comparisons. Let n = no.
of neurons per template (one neuron for each delay of the template), m =
no. of iterations to reach the stable state (in simulating the discretized differential equation, with step size = 0.05), [ = no. of samples in a template
tj(m). Then, the number of connections in the full Hopfield network will be
4n 2 ? The total no. of synaptic calculations = 4mn 2 ? So, for two templates
and n = 16, m = 100,4mn 2 = 102,400. Thus building the full Hopfield-type
network digitally requires a system too large to be put in a single VLSI chip
which will work in real time. If we want to build an analog system, we need
to have many (O{ 4n 2 )) easily modifiable synapses. As yet this technology is
not available for nets of this size. The modified Hopfield-type network on the
other hand is less technically demanding . To do the preprocessing to obtain
the minimum values we have to do about n 2 = 256 additions to find all possible
Iijs and require 256 subtractions and comparisons to find three minima. The
costs associated with doing input cross-correlations are the same as for the full
neural network (i.e. 2nl = 768(l = 24) mUltiplications). The saving with the
modified approach is that the network used is small and fast (120 multiplications and 120 additions to construct the modifiable synapses, no. of synaptic
calculations = 90 with m = 10, n = 3).
In contrast to the neural networks, simple temrlate matching is simple
indeed. For example, it must perform about n 2 [ + n = 10,496 additions and
n 2 = 256 comparisons to find the minimum d ij . Additions are considerably less
costly in time and hardware than multiplications. In fact, because this method
needs only addition operations, our preliminary design work suggests it can be
built on a single chip and will be able to do the two-template classification
in as little as 20 microseconds. This actually raises the possibility that with
switching and buffering one chip might be able to service more than one channel
in essentially real time.

CONCLUSIONS
Template matching using a full Hopfield-type neural network is found to
be robust to noise and changes in signal waveform for the two neural waveform
classification problem. However, for a three-waveform case, the network does
not perform well. Further, the network requires many modifiable connections
and therefore results in an elaborate hardware implementation. The overall
performance of the modified neural network approach is better than the full
~Iopfield network approach. The computation has been reduced largly and
the hardware requirements are considerably less demanding demonstrating the
value of designing a specific network to a specified problem. However, even the
modified neural network performs less well than a simple template-matching
algorithm which also has the simplest hardware implementation. Using the
simple template matching algorithm, our simulations suggest it will be possible to build a two or three waveform classifier on a single VLSI chip using
CMOS technology that works in real time with excellent error characteristics.
Further, such a chip will be able to accurately classify variably overlapping

113

neural signals.

REFERENCES
[1] G. L. Gerstein, M. J. Bloom, 1. E. Espinosa, S. Evanczuk & M. R. Turner,
IEEE Trans. Sys. Cyb. Man., SMC-13, 668(1983).
2 J. M. Bower & R . Llinas, Soc. Neurosci. Abst.,~, 607(1983).
3 M. Abeles & M. H. Goldstein, Proc. IEEE, 65, 762(1977).
4 W. M. Roberts & D. K. Hartline, Brain Res., 94, 141(1976).
5 E. M. Schmidt, J. of Neurosci. Methods, 12, 95(1984).
6 J. J. Hopfield, Proc. Natl. Acad. Sci. (USA), 81, 3088(1984).
7 J. J. Hopfield & D. W. Tank, BioI. Cybern., 52, 141(1985).
8 D. W. Tank & J. J. Hopfield, IEEE Trans. Circuits Syst., CAS-33,
533(1986).

ACKNOWLEDGEMENTS
We would like to acknowledge the contribution of Dr. Mark Nelson to the intellectual
development of these projects and the able assistance of Herb Adams, Mike Walshe and John
Powers in designing and constructing support equipment. This work was supported by NIH
grant NS22205, the Whitaker Foundation and the Joseph Drown Foundation.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 4465-online-submodular-set-cover-ranking-and-repeated-active-learning.pdf

Online Submodular Set Cover,
Ranking, and Repeated Active Learning

Jeff Bilmes
Department of Electrical Engineering
University of Washington
bilmes@ee.washington.edu

Andrew Guillory
Department of Computer Science
University of Washington
guillory@cs.washington.edu

Abstract
We propose an online prediction version of submodular set cover with connections
to ranking and repeated active learning. In each round, the learning algorithm
chooses a sequence of items. The algorithm then receives a monotone submodular function and suffers loss equal to the cover time of the function: the number of
items needed, when items are selected in order of the chosen sequence, to achieve
a coverage constraint. We develop an online learning algorithm whose loss converges to approximately that of the best sequence in hindsight. Our proposed
algorithm is readily extended to a setting where multiple functions are revealed at
each round and to bandit and contextual bandit settings.

1

Problem

In an online ranking problem, at each round we choose an ordered list of items and then incur some
loss. Problems with this structure include search result ranking, ranking news articles, and ranking
advertisements. In search result ranking, each round corresponds to a search query and the items
correspond to search results. We consider online ranking problems in which the loss incurred at
each round is the number of items in the list needed to achieve some goal. For example, in search
result ranking a reasonable loss is the number of results the user needs to view before they find the
complete information they need. We are specifically interested in problems where the list of items is
a sequence of questions to ask or tests to perform in order to learn. In this case the ranking problem
becomes a repeated active learning problem. For example, consider a medical diagnosis problem
where at each round we choose a sequence of medical tests to perform on a patient with an unknown
illness. The loss is the number of tests we need to perform in order to make a confident diagnosis.
We propose an approach to these problems using a new online version of submodular set cover.
A set function F (S) defined over a ground set V is called submodular if it satisfies the following
diminishing returns property: for every A ? B ? V \ {v}, F (A + v) ? F (A) ? F (B + v) ? F (B).
Many natural objectives measuring information, influence, and coverage turn out to be submodular
[1, 2, 3]. A set function is called monotone if for every A ? B, F (A) ? F (B) and normalized if
F (?) = 0. Submodular set cover is the problem of selecting an S ? V minimizing |S| under the
constraint that F (S) ? 1 where F is submodular, monotone, and normalized (note we can always
rescale F ). This problem is NP-hard, but a greedy algorithm gives a solution with cost less than
1 + ln 1/ that of the optimal solution where  is the smallest non-zero gain of F [4].
We propose the following online prediction version of submodular set cover, which we simply call
online submodular set cover. At each time step t = 1 . . . T we choose a sequence of elements
S t = (v1t , v2t , . . . vnt ) where each vit is chosen from a ground set V of size n (we use a superscript
for rounds of the online problem and a subscript for other indices). After choosing S t , an adversary
reveals a submodular, monotone, normalized function F t , and we suffer loss `(F t , S t ) where

`(F t , S t ) , min {n} ? {i : F t (Sit ) ? 1}i
(1)
1

S
and Sit , j?i {vjt } is defined to be the set containing the first i elements of S t (let S0t , ?). Note
Pn
` can be equivalently written `(F t , S t ) , i=0 I(F t (Sit ) < 1) where I is the indicator function.
Intuitively, `(F t , S t ) corresponds to a bounded version of cover time: it is the number of items up to
n needed to achieve F t (S) ? 1 when we select items in the order specified by S t . Thus, if coverage
is not achieved, we suffer a loss of n. We assume that F t (V ) ? 1 (therefore coverage is achieved if
S t does not contain duplicates) and that the sequence of functions (F t )t is chosen in advance
(by an
P
oblivious adversary). The goal of our learning algorithm is to minimize the total loss t `(F t , S t ).
To make the problem clear, we present it first in its simplest, full information version. However, we
will later consider more complex variations including (1) a version where we only produce a list of
t
length k ? n instead of n, (2) a multiple objective version where a set of objectives F1t , F2t , . . . Fm
is revealed each round, (3) a bandit (partial information) version where we do not get full access to
F t and instead only observe F t (S1t ), F t (S2t ), . . . F t (Snt ), and (4) a contextual bandit version where
there is some context associated with each round.
We argue that online submodular set cover, as we have defined it, is an interesting and useful model
for ranking and repeated active learning problems. In a search result ranking problem, after presenting search results to a user we can obtain implicit feedback from this user (e.g., clicks, time spent
viewing each result) to determine which results were actually relevant. We can then construct an
objective F t (S) such that F t (S) ? 1 iff S covers or summarizes the relevant results. Alternatively,
we can avoid explicitly constructing an objective by considering the bandit version of the problem
where we only observe the values F t (Sit ). For example, if the user clicked on k total results then
we can let F (Sit ) , ci /k where ci ? k is the number of results in the subset Si which were clicked.
Note that the user may click an arbitrary set of results in an arbitrary order, and the user?s decision
whether or not to click a result may depend on previously viewed and clicked results. All that we
assume is that there is some unknown submodular function explaining the click counts. If the user
clicks on a small number of very early results, then coverage is achieved quickly and the ordering is
desirable. This coverage objective makes sense if we assume that the set of results the user clicked
are of roughly equal importance and together summarize the results of interest to the user.
In the medical diagnosis application, we can define F t (S) to be proportional to the number of
candidate diseases which are eliminated after performing the set of tests S on patient t. If we assume
that a particular test result always eliminates a fixed set of candidate diseases, then this function is
submodular. Specifically, this objective is the reduction in the size of the version space [5, 6]. Other
active learning problems can also be phrased in terms of satisfying a submodular coverage constraint
including problems that allow for noise [7]. Note that, as in the search result ranking problem, F t is
not initially known but can be inferred after we have chosen S t and suffered loss `(F t , S t ).

2

Background and Related Work

Recently, Azar and Gamzu [8] extended the O(ln 1/) greedy approximation algorithm for submodular set cover to the more general problem of minimizing the average cover time of a set of
objectives. Here  is the smallest non-zero gain of all the objectives. Azar and Gamzu [8] call this
problem ranking with submodular valuations. More formally, we have a known set of functions
F1 , F2 , . . . , Fm each with an associated
weight wi . The goal is then to choose a permutation S of
Pm
the ground set V to minimize i=1 wi `(Fi , S). The offline approximation algorithm for ranking
with submodular valuations will be a crucial tool in our analysis of online submodular set cover.
In particular, this offline algorithm can viewed as constructing the best single permutation S for a
sequence of objectives F 1 , F 2 . . . F T in hindsight (i.e., after all the objectives are known). Recently
the ranking with submodular valuations problem was extended to metric costs [9].
Online learning is a well-studied problem [10]. In one standard setting, the online learning algorithm
has a collection of actions A, and at each time step t the algorithm picks an action S t ? A. The
learning algorithm then receives a loss function `t , and the algorithm incurs the loss value for the
action it chose `t (S t ). We assume `t (S t ) ? [0, 1] but make no other assumptions about the form
of loss. The performance of an online learning algorithm is often measured in terms of regret, the
difference between the loss incurred by the algorithm and the loss of the best single fixed action
PT
PT
in hindsight: R = p t=1 `t (S t ) ? minS?A t=1 `t (S). There are randomized algorithms which
guarantee E[R] ? T ln |A| for adversarial sequences of loss functions [11]. Note that because
2

E[R] = o(T ) the per round regret approaches zero. In the bandit version of this problem the learning
algorithm only observes `t (S t ) [12].
Our problem fits in this standard setting with A chosen to be the set of all ground set permutations
(v1 , v2 , . . . vn ) and `t (S t ) , `(F t , S t )/n. However, in this case A is very large so standard online
learning algorithms which keep weight vectors of size |A| cannot be directly applied. Furthermore,
our problem generalizes an NP-hard offline problem which has no polynomial time approximation
scheme, so it is not likely that we will be able to derive any efficient algorithm with o(T ln |A|)
regret. We therefore instead consider ?-regret, the loss incurred by the algorithm as compared to ?
PT
PT
times the best fixed prediction. R? = t=1 `t (S t ) ? ? minS?A t=1 `t (S). ?-regret is a standard
notion of regret for online versions of NP-hard problems. If we can show R? grows sub linearly
with T then we have shown loss converges to that of an offline approximation with ratio ?.
Streeter and Golovin [13] give online algorithms for the closely related problems of submodular
function maximization and min-sum submodular set cover. In online submodular function maximization, theP
learning algorithm selects a set S t with |S t | ? k before F t is revealed, and the goal is
to maximize t F t (S t ). This problem differs from ours in that our problem is a loss minimization
problem as opposed to an objective maximization problem. Online min-sum submodular set cover
is similar to online submodular set cover except the loss is not cover time but rather
? t, St) ,
`(F

n
X

max(1 ? F t (Sit ), 0).

(2)

i=0

Min-sum submodular set cover penalizes 1 ? F t (Sit ) where submodular set cover uses I(F t (Sit ) <
1). We claim that for certain applications the hard
P threshold makes more sense. For example, in
repeated active learning problems minimizing t `(F t , S t ) naturally corresponds to minimizing
P ? t t
the number of questions asked. Minimizing t `(F
, S ) does not have this interpretation as it
charges less for questions asked when F t is closer to 1. One might hope that minimizing ` could
? This is not likely to be the case, as the apbe reduced to or shown equivalent to minimizing `.
proximation algorithm of Streeter and Golovin [13] does not carry over to online submodular set
cover. P
Their online algorithm is based on approximating an offline algorithm which greedily maximizes t min(F t (S), 1). Azar and Gamzu [8] show that this offline algorithm, which they call the
cumulative greedy algorithm, does not achieve a good approximation ratio for average cover time.
Radlinski et al. [14] consider a special case of online submodular function maximization applied
to search result ranking. In their problem the objective function is assumed to be a binary valued
submodular function with 1 indicating the user clicked on at least one document. The goal is then
to maximize the number of queries which receive at least one click. For binary valued functions
`? and ` are the same, so in this setting minimizing the number of documents a user must view
before clicking on a result is a min-sum submodular set cover problem. Our results generalize this
problem to minimizing the number of documents a user must view before some possibly non-binary
submodular objective is met. With non-binary objectives we can incorporate richer implicit feedback
such as multiple clicks and time spent viewing results. Slivkins et al. [15] generalize the results of
Radlinski et al. [14] to a metric space bandit setting.
Our work differs from the online set cover problem of Alon et al. [16]; this problem is a single
set cover problem in which the items that need to be covered are revealed one at a time. Kakade
et al. [17] analyze general online optimization problems with linear loss. If we assume that the
functions F t are all taken from a known finite set of functions F then we have linear loss over a |F|
dimensional space. However, this approach gives poor dependence on |F|.

3

Offline Analysis

In this work we present an algorithm for online submodular set cover which extends the offline
algorithm of Azar and Gamzu [8] for the ranking with submodular valuations problem. Algorithm 1
shows this offline algorithm, called the adaptive residual updates algorithm. Here we use T to denote
the number of objective functions and superscript t to index the set of objectives. This notation is
chosen to make the connection to the proceeding online algorithm clear: our online algorithm will
approximately implement Algorithm 1 in an online setting, and in this case the set of objectives in
3

Algorithm 1 Offline Adaptive Residual
Input: Objectives F 1 , F 2 , . . . F T
Output: Sequence S1 ? S2 ? . . . Sn
S0 ? ?
for i ? 1 . . . n do
P
v ? argmax t ?(F t , Si?1 , v)
v?V

Si ? Si?1 + v
end for

Figure 1: Histograms used in offline analysis

the offline algorithm will be the sequence of objectives in the online problem. The algorithm is a
greedy algorithm similar to the standard algorithm for submodular set cover. The crucial difference
is that instead of a normal gain term of F t (S + v) ? F t (S) it uses a relative gain term
(
t
t
(S)
min( F (S+v)?F
, 1) if F (S) < 1
t
1?F t (S)
?(F , S, v) ,
0
otherwise
The intuition is that (1) a small gain for F t matters more if F t is close to being covered (F t (S) close
to 1) and (2) gains for F t with F t (S) ? 1 do not matter as these functions are already covered. The
main result of Azar and Gamzu [8] is that Algorithm 1 is approximately optimal.
P
t
Theorem 1 ([8]). The loss
t `(F , S) of the sequence produced by Algorithm 1 is within
4(ln(1/) + 2) of that of any other sequence.
We note Azar and Gamzu [8] allow for weights for each F t . We omit weights for simplicity. Also,
Azar and Gamzu [8] do not allow the sequence S to contain duplicates while we do?selecting a
ground set element twice has no benefit but allowing them will be convenient for the online analysis. The proof of Theorem 1 involves representing solutions to the submodular ranking problem as
histograms. Each histogram is defined such that the area of the histogram is equal to the loss of the
corresponding solution. The approximate optimality of Algorithm 1 is shown by proving that the
histogram for the solution it finds is approximately contained within the histogram for the optimal
solution. In order to convert Algorithm 1 into an online algorithm, we will need a stronger version of
Theorem 1. Specifically, we will need to show that when there is some additive error in the greedy
selection rule Algorithm 1 is still approximately optimal.
P
For the optimal solution S ? = argminS?V n t `(F t , S) (V n is the set of all length n sequences of
ground set elements), define a histogram h? with T columns, one for each function F t . Let the tth
column have with width 1 and height equal to `(F t , S ? ). Assume that the columns are ordered by
increasing cover time so that the histogram is monotone non-decreasing. Note that the area of this
histogram is exactly the loss of S ? .
For a sequence of sets ? = S0 ? S1 ? . . . Sn (e.g., those found by Algorithm 1) define a corresponding sequence of truncated objectives
(
t
t
i?1 )?F (Si?1 )
, 1) if F t (Si?1 ) < 1
min( F (S?S
t (S
t
1?F
)
?
i?1
Fi (S) ,
1
otherwise
F?it (S) is essentially F t except with (1) Si?1 given ?for free?, and (2) values rescaled to range
between 0 and 1. We note that F?it is submodular and that if F t (S) ? 1 then F?it (S) ? 1. In this
sense F?it is an easier objective than F t . Also, for any v, F?it ({v}) ? F?it (?) = ?(F t , Si?1 , v). In
other words, the gain of F?it at ? is the normalized gain of F t at Si?1 . This property will be crucial.
? 1, h
? 2, . . . h
? n which correspond to the loss of S ? for the
We next define truncated versions of h? : h
t
? i have T columns of height j
?
easier covering problems involving Fi . For each j ? 1 . . . n, let h
t
?
t
?
with the tth such column of width F?i (Sj ) ? F?i (Sj?1 ) (some of these columns may have 0 width).
? i.
Assume again the columns are ordered by height. Figure 1 shows h? and h
We assume without loss of generality that F t (Sn? ) ? 1 for every t (clearly some choice of S ?
contains no duplicates, so under our assumption that F t (V ) ? 1 we also have F t (Sn? ) ? 1). Note
4

? i is then the number of functions remaining to be covered after Si?1 is given
that the total width of h
? i is
for free (i.e., the number of F t with F t (Si?1 ) < 1). It is not hard to see that the total area of h
P ? ?t ?
?
t `(Fi , S ) where l is the loss function for min-sum submodular set cover (2). From this we know
? i has area less than h? . In fact, Azar and Gamzu [8] show the following.
h
? i is completely contained within h? when h
? i and h? are aligned along their lower
Lemma 1 ([8]). h
right boundaries.
We need
lemma before proving the main result of this section. For a sequence S define
P one final
t
Q
=
?(F
,
S
, vi ) to be the total normalized gain of the ith selected element and let ?i =
i
i?1
t
Pn
t
j=i Qj be the sum of the normalized gains from i to n. Define ?i = |{t : F (Si?1 ) < 1}| to be
the number of functions which are still uncovered before vi is selected (i.e., the loss incurred at step
i). [8] show the following result relating ?i to ?i .
Lemma 2 ([8]). For any i, ?i ? (ln 1/ + 2)?i
We now state and prove the main result of this section, that Algorithm 1 is approximately optimal
even when the ith greedy selection is preformed with some additive error Ri . This theorem shows
that in order to achieve low average cover time it suffices to approximately implement Algorithm 1.
Aside from being useful for converting Algorithm 1 into an online algorithm, this theorem may be
useful for applications in which the ground set V is very large. In these situations it may be possible
to approximate Algorithm 1 (e.g., through sampling). Streeter and Golovin [13] prove similar results
for submodular function maximization and min-sum submodular set cover. Our result is similar, but
the proof is non trivial. The loss function ` is highly non linear with respect to changes in F t (Sit ),
so it is conceivable that small additive errors in the greedy selection could have a large effect. The
analysis of Im and Nagarajan [9] involves a version of Algorithm 1 which is robust to a sort of
multplicative error in each stage of the greedy selection.
Theorem 2. Let S = (v1 , v2 , . . . vn ) be any sequence for which
X
X
?(F t , Si?1 , vi ) + Ri ? max
?(F t , Si?1 , v)
v?V

t

Then

P

t

t

t

`(F , S ) ? 4(ln 1/ + 2)

P

t

t

?

`(F , S ) + n

t

P

i

Ri

Proof. Let h be a histogram with a column for each ?i with ?iP
6= 0. Let ? = (ln 1/ + 2). Let the
ith column have width (Qi + Ri )/(2?) and height max(?i ? j Rj , 0)/(2(Qi + Ri )). Note that
?i 6= 0 iff Qi + Ri 6= 0 as if there are functions not yet covered then there is some set element with
non zero gain (and vice versa). The area of h is
P
X 1
max(?i ? j Rj , 0)
1 X
n X
(Qi + Ri )
?
`(F t , S) ?
Rj
2?
2(Qi + Ri )
4? t
4? j
i:?i 6=0

? i are aligned along their lower right boundaries. We show that if the ith
Assume h and every h
? i . Then, it follows from Lemma 1 that h
column of h has non-zero area then it is contained within h
?
is contained within h , completing the proof.
P
Consider the ith column in h. Assume this column has non zero area so ?i ? j Rj . This column
P
is at most (?i + j?i Rj )/(2?) away from the right hand boundary. To show that this column is in
? i it suffices to show that after selecting the first k = b(?i ? P Rj )/(2(Qi + Ri ))c items in S ? we
h
j
P
P
P
still have t (1 ? F?it (Sk? )) ? (?i + j?i Rj )/(2?) . The most that t F?it can increase through
the addition of one item is Qi + Ri . Therefore, using the submodularity of F?it ,
X
X
X
F?it (Sk? ) ?
F?it (?) ? k(Qi + Ri ) ? ?i /2 ?
Rj /2
t

Therefore

P

t (1

?

t

F?it (Sk? ))

?i /2 +

X

? ?i /2 +

j

P

P

F?it (?))

Rj /2 since t (1 ?
= ?i . Using Lemma 2
X
X
Rj /2 ? ?i /(2?) +
Rj /2 ? (?i +
Rj )/(2?)
j

j

j

5

j?i

Algorithm 2 Online Adaptive Residual
Input: Integer T
Initialize n online learning algorithms
E1 , E2 , . . . En with A = V
for t = 1 ? T do
?i ? 1 . . . n predict vit with Ei
S t ? (v1t , . . . vnt )
Receive F t , pay loss l(F t , S t )
t
For Ei , `t (v) ? (1 ? ?(F t , Si?1
, v))
end for

4

Figure 2: Ei selects the ith element in S t .

Online Analysis

We now show how to convert Algorithm 1 into an online algorithm. We use the same idea used by
Streeter and Golovin [13] and Radlinski et al. [14] for online submodular function maximization: we
run n copies of some low regret online learning algorithm, E1 , E2 , . . . En , each with action space
A = V . We use the ith copy Ei to select the ith item in each predicted sequence S t . In other
words, the predictions of Ei will be vi1 , vi2 , . . . viT . Figure 2 illustrates this. Our algorithm assigns
loss values to each Ei so that, assuming Ei has low regret, Ei approximately implements the ith
greedy selection in Algorithm 1. Algorithm 2 shows this approach. Note that under our assumption
that F 1 , F 2 , . . . F T is chosen by an oblivious adversary, the loss values for the ith copy of the online
algorithm are oblivious to the predictions of that run of the algorithm. Therefore we can use any
algorithm for learning against an oblivious adversary.
Theorem
? algorithm with expected regret
? 3. Assume we use as a subroutine an online prediction
E[R] ? T ln n. Algorithm 2 has expected ?-regret E[R? ] ? n2 T ln n for ? = 4(ln(1/) + 2)
Proof. Define a meta-action v?i for the sequence of actions chosen by Ei , v?i = (vi1 , vi2 , . . . viT ). We
can extend the domain of F t to allow for meta-actions F t (S ? {?
vi }) = F t (S ? {vit }). Let S? be
the sequence of meta actions S? = (v?1 , v?2 , . . . v?n ). Let Ri be the regret of Ei . Note that from the
definition of regret and our choice of loss values we have that
X
X
?(F t , S?i?1 , v) ?
?(F t , S?i?1 , v?i ) = Ri
max
v?V

t

t

Therefore, S? approximates the greedy solution in the sense required by Theorem 2. Theorem 2 did
not require that S be constructed V . From Theorem 2 we then have
X
X
X
X
? ??
`(F t , S t ) =
`(F t , S)
`(F t , S ? ) + n
Ri
t

The expected ?-regret is then E[n

t

P

i

Ri ] ? n

t

?
2

i

T ln n

We describe several variations and extensions of this analysis, some of which mirror those for related
work [13, 14, 15].
Avoiding Duplicate Items Since each run of the online prediction algorithm is independent, Algorithm 2 may select the same ground set element multiple times. This drawback is easy to fix. We
can simply select any arbitrary vi ?
/ Si?1 if Ei selects a vi ? Si?i . This modification does not affect
the regret guarantee as selecting a vi ? Si?1 will always result in a gain of zero (loss of 1).
Truncated Loss In some applications we only care about the first k items in the sequence S t . For
these applications it makes sense to consider a truncated version of l(F t , S t ) with parameter k

`k (F t , S t ) , min {k} ? {|Sit | : F t (Sit ) ? 1}
This is cover time computed up to the kth element in S t . The analysis for Theorem 2 also shows
P k t t
P
Pk
t
?
t ` (F , S ) ? 4(ln 1/ + 2)
t `(F , S ) + k
i=1 Ri . The corresponding regret bound is then
6

?
P k t t
2
k
of untruncated loss
P T lnt n. ?Note here we are bounding truncated loss t ` (F , S ) in terms
2
`(F
,
S
).
In
this
sense
this
bound
is
weaker.
However,
we
replace
n
with
k 2 which may be
t
much smaller. Algorithm 2 achieves this bound simultaneously for all k.
Multiple Objectives per Round Consider a variation of online submodular set cover in which int
stead of receiving
a single objective F t each round we receive a batch of objectives F1t , F2t , . . . Fm
Pm
t
t
and incur loss i=1 `(Fi , S ). In other words, each rounds corresponds to a ranking with submodular
problem. It is easy to extend Algorithm 2 to this?setting by using 1 ?
Pvaluations
m
t
, v) for the loss of action v in Ei . We then get O(k 2 mL? ln n+k 2 m ln n)
(1/m) i=1 ?(Fit , Si?1
PT Pm
?
total regret where L = t=1 i=1 `(Fit , S ? ) (Section 2.6 of [10]).
Bandit Setting Consider a setting where instead of receiving full access to F t we only observe
the sequence of objective function values F t (S1t ), F t (S2t ), . . . F t (Snt ) (or in the case of multiple
objectives per round, Fit (Sjt ) for every i and j). We can extend Algorithm 2 to this setting using a
nonstochastic multiarmed bandits algorithm [12]. We note duplicate removal becomes more subtle
in the bandit setting: should we feedback a gain of zero when a duplicate is selected or the gain of
the non-duplicate replacement? We propose either is valid if replacements are chosen obliviously.
Bandit Setting with Expert Advice We can further generalize the bandit setting to the contextual
bandit setting [18] (e.g., the bandit setting with expert advice [12]). Say that we have access at time
step t to predictions from a set of m experts. Let v?j be the meta action corresponding to the sequence
of predictions from the jth expert and V? be the set of all v?j . Assume that Ei guarantees low regret
with respect to V?
X
X
t
t
?(F t , Si?1
, vit ) + Ri ? max
?(F t , Si?1
, v?)
(3)
v
??V?

t

t

where we have extended the domain of each F t to include meta actions as in the proof
P of Theorem
3. Additionally assume that F t (V? ) ? 1 for every t. In this case we can show t `k (F t , S t ) ?
?
P
Pk
minS ? ?V? m t `m (F t , S ? ) + k i=1 Ri . The Exp4 algorithm [12] has Ri = O( nT ln m) giving
?
total regret O(k 2 nT ln m). Experts may use context in forming recommendations. For example,
in a search ranking problem the context could be the query.

5
5.1

Experimental Results
Synthetic Example

We present a synthetic example for which the online cumulative greedy algorithm [13] fails, based
on the example in Azar and Gamzu [8] for the offline setting. Consider an online ad placement
problem where the ground set V is a set of available ad placement actions (e.g., a v ? V could
correspond to placing an ad on a particular web page for a particular length of time). On round
t, we receive an ad from an advertiser, and our goal is to acquire ? clicks for the ad using as few
advertising actions as possible. Define F t (Sit ) to be min(cti , ?)/? where cti is number of clicks
acquired from the ad placement actions Sit .
Say that we have n advertising actions of two types: 2 broad actions and n ? 2 narrow actions. Say
that the ads we receive are also of two types. Common type ads occur with probability (n ? 1)/n
and receive 1 and ? ? 1 clicks respectively from the two broad actions and 0 clicks from narrow
actions. Uncommon type ads occur with probability 1/n and receive ? clicks from one randomly
chosen narrow action and 0 clicks from all other actions. Assume ? ? n2 . Intuitively broad actions
could correspond to ad placements on sites for which many ads are relevant. The optimal strategy
giving an average cover time O(1) is to first select the two broad actions covering all common ads
then select the narrow actions in any order. However, the offline cumulative greedy algorithm will
pick all narrow actions before picking the broad action with gain 1 giving average cover time O(n).
The left of Figure 3 shows average cover time for our proposed algorithm and the cumulative greedy
algorithm of [13] on the same sequences of random objectives. For this example we use n = 25
and the bandit version of the problem with the Exp3 algorithm [12]. We also plot the average cover
times for offline solutions as baselines. As seen in the figure, the cumulative algorithms converge to
higher average cover times than the adaptive residual algorithms. Interestingly, the online cumulative
algorithm does better than the offline cumulative algorithm: it seems added randomization helps.
7

Figure 3: Average cover time
5.2

Repeated Active Learning for Movie Recommendation

Consider a movie recommendation website which asks users a sequence of questions before they are
given recommendations. We define an online submodular set cover problem for choosing sequences
of questions in order to quickly eliminate a large number of movies from consideration. This is
similar conceptually to the diagnosis problem discussed in the introduction. Define the ground set
V to be a set of questions (for example ?Do you want to watch something released in the past
10 years?? or ?Do you want to watch something from the Drama genre??). Define F t (S) to be
proportional to the number of movies eliminated from consideration after asking the tth user S.
Specifically, let H be the set of all movies in our database and V t (S) be the subset of movies
consistent with the tth user?s responses to S. Define F t (S) , min(|H \ V t (S)|/c, 1) where c is a
constant. F t (S) ? iff after asking the set of questions S we have eliminated at least c movies.
We set H to be a set of 11634 movies available on Netflix?s Watch Instantly service and use 803
questions based on those we used for an offline problem [7]. To simulate user responses to questions,
on round t we randomly select a movie from H and assume the tth user answers questions consistently with this movie. We set c = |H| ? 500 so the goal is to eliminate about 95% of all movies.
We evaluate in the full information setting: this makes sense if we assume we receive as feedback
the movie the user actually selected. As our online prediction subroutine we tried Normal-Hedge
[19], a second order multiplicative weights method [20], and a version of multiplicative weights for
small gains using the doubling trick (Section 2.6 of [10]). We also tried a heuristic modification of
Normal-Hedge which fixes ct = 1 for a fixed, more aggressive learning rate than theoretically justified. The right of Figure 3 shows average cover time for 100 runs of T = 10000 iterations. Note the
different scale in the bottom row?these methods performed significantly worse than Normal-Hedge.
The online cumulative greedy algorithm converges to a average cover time close to but slightly
worse than that of the adaptive greedy method. The differences are more dramatic for prediction
subroutines that converge slowly. The modified Normal-Hedge has no theoretical justification, so it
may not generalize to other problems. For the modified Normal-Hedge the final average cover times
are 7.72 adaptive and 8.22 cumulative. The offline values are 6.78 and 7.15.

6

Open Problems

It is not yet clear what practical value our proposed approach will have for web search result ranking.
A drawback to our approach is that we pick a fixed order in which to ask questions. For some
problems it makes more sense to consider adaptive strategies [5, 6].
Acknowledgments
This material is based upon work supported in part by the National Science Foundation under grant
IIS-0535100, by an Intel research award, a Microsoft research award, and a Google research award.
8

References
[1] H. Lin and J. Bilmes. A class of submodular functions for document summarization. In HLT,
2011.
? Tardos. Maximizing the spread of influence through a social
[2] D. Kempe, J. Kleinberg, and E.
network. In KDD, 2003.
[3] A. Krause, A. Singh, and C. Guestrin. Near-optimal sensor placements in Gaussian processes:
Theory, efficient algorithms and empirical studies. JMLR, 2008.
[4] L.A. Wolsey. An analysis of the greedy algorithm for the submodular set covering problem.
Combinatorica, 2(4), 1982.
[5] D. Golovin and A. Krause. Adaptive submodularity: A new approach to active learning and
stochastic optimization. In COLT, 2010.
[6] Andrew Guillory and Jeff Bilmes. Interactive submodular set cover. In ICML, 2010.
[7] Andrew Guillory and Jeff Bilmes. Simultaneous learning and covering with adversarial noise.
In ICML, 2011.
[8] Yossi Azar and Iftah Gamzu. Ranking with Submodular Valuations. In SODA, 2011.
[9] S. Im and V. Nagarajan. Minimum Latency Submodular Cover in Metrics. ArXiv e-prints,
October 2011.
[10] N. Cesa-Bianchi and G. Lugosi. Prediction, learning, and games. Cambridge University Press,
2006.
[11] Y. Freund and R. Schapire. A desicion-theoretic generalization of on-line learning and an
application to boosting. In Computational learning theory, pages 23?37, 1995.
[12] P. Auer, N. Cesa-Bianchi, Y. Freund, and R.E. Schapire. The nonstochastic multiarmed bandit
problem. SIAM Journal on Computing, 32(1):48?77, 2003.
[13] M. Streeter and D. Golovin. An online algorithm for maximizing submodular functions. In
NIPS, 2008.
[14] F. Radlinski, R. Kleinberg, and T. Joachims. Learning diverse rankings with multi-armed
bandits. In ICML, 2008.
[15] A. Slivkins, F. Radlinski, and S. Gollapudi. Learning optimally diverse rankings over large
document collections. In ICML, 2010.
[16] N. Alon, B. Awerbuch, and Y. Azar. The online set cover problem. In STOC, 2003.
[17] Sham M. Kakade, Adam Tauman Kalai, and Katrina Ligett. Playing games with approximation
algorithms. In STOC, 2007.
[18] J. Langford and T. Zhang. The epoch-greedy algorithm for contextual multi-armed bandits. In
NIPS, 2007.
[19] K. Chaudhuri, Y. Freund, and D. Hsu. A parameter-free hedging algorithm. In NIPS, 2009.
[20] N. Cesa-Bianchi, Y. Mansour, and G. Stoltz. Improved second-order bounds for prediction
with expert advice. Machine Learning, 2007.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 570-a-self-organizing-integrated-segmentation-and-recognition-neural-net.pdf

A Self-Organizing Integrated Segmentation And
Recognition Neural Net

Jim Keeler *
MCC
3500 West Balcones Center Drive
Austin, TX 78729

David E. Rumelhart
Psychology Department
Stanford University
Stanford, CA 94305

Abstract
We present a neural network algorithm that simultaneously performs segmentation and recognition of input patterns that self-organizes to detect
input pattern locations and pattern boundaries. We demonstrate this neural network architecture on character recognition using the NIST database
and report on results herein. The resulting system simultaneously segments and recognizes touching or overlapping characters, broken characters, and noisy images with high accuracy.

1

INTRODUCTION

Standard pattern recognition systems usually involve a segmentation step prior to
the recognition step. For example, it is very common in character recognition to
segment characters in a pre-processing step then normalize the individual characters
and pass them to a recognition engine such as a neural network, as in the work of
LeCun et al. 1988, Martin and Pittman (1988).
This separation between segmentation and recognition becomes unreliable if the
characters are touching each other, touching bounding boxes, broken, or noisy.
Other applications such as scene analysis or continuous speech recognition pose
similar and more severe segmentation problems. The difficulties encountered in
these applications present an apparent dilemma: one cannot recognize the patterns
*keeler@mcc.com Reprint requests: coila@mcc.com or at the above address.

496

A Self-Organizing Integrated Segmentation and Recognition Neural Net

Sz Outputs: pz = - I + Sz

I
I
I
ItfIM.............
I
I
I

5

Summing Units:

Sz =

LXxyz
xy

I~~~

I
I
I
I
I

,

2.AFI

1ABY/

?AW

Grey-scale
Input image

I(.X,y)

Figure 1: The ISR network architecture. The input image may contain several
characters and is presented to the network in a two-dimensional grey-scale image.
The units in the first block, hij", have linked-local receptive field connections to the
input image. Block 2, Hr'JI'z" has a three-dimensional linked-local receptive field
to block 1, and the exponential unit block, block 3, has three-dimensional linkedlocal receptive field connections to block 2. These linked fields insure translational
invariance (except for edge-effects at the boundary). The exponential unit block
has one layer for each output category. These units are the output units in the test
mode, but hidden units during training: the exponential unit activity is summed
over (sz) to project out the positional information, then converted to a probability
Pz. Once trained, the exponential unit layers serve as "smart histograms" giving
sharp peaks of activity directly above the corresponding characters in the input
image, as shown to the left.

497

498

Keeler and Rumelhart

until they are segmented, yet in many cases one cannot segment the patterns until
they are recognized.
A solution to this apparent dilemm is to simultaneously segment and recognize
the patterns. Integration of the segmentation and recognition steps is essential for
further progress in these difficult pattern recognition tasks, and much effort has been
devoted to this topic in speech recognition. For example, Hidden Markov models
integrate the task of segmentation and recognition as a part of the word-recognition
module. Nevertheless, little neural network research in pattern recognition has
focused on the integrated segmentation and recognition (ISR) problem.
There are several ways to achieve ISR in a neural network. The first use of backpropagation ISR neural networks for character recognition was reported by Keeler,
Rumelhart and Leow (1991a). The ISR neural network architecture is similar to
the time-delayed neural network architecture for speech recognition used by Lang,
Hinton, and Waibel (1990).
The following section outlines the neural network algorithm and architecture. Details and rationale for the exact structure and assumptions of the network can be
found in Keeler et al. (1991a,b).

2

NETWORK ARCHITECTURE AND ALGORITHM

The basic organization of the network is illustrated in Figure 2. The input consists
of a twcrdimensional grey-scale image representing the pattern to be processed. We
designate this input pattern by the twcrdimensional field lex, y). In general, we
assume that any pattern can be presented at any location and that the characters
may touch, overlap or be broken or noisy. The input then projects to a linked-Iocalreceptive-field block of sigmoidal hidden units (to enforce translational invariance).
We designate the activation of the sigmoidal units in this block by h ij It.
The second block of hidden units, H 1:'J/' z', is a linked-local receptive field block of
sigmoidal units that receives input from a three-dimensional receptive field in the
hiilt block. In a standard neural network architecture we would normally connect
block H to the output units. However we connect block H to a block of exponential
units X1:J/z, The X block serves as the outputs after the network has been trained;
there is a sheet of exponential units for each output category. These units are
e"''''"?,
connected to block H via a linked-local receptive field structure. X1:J/z
where the net input to the unit is

=

TJ1:J/Z =

L W:,~~z,H1:'J/'z' + /3z,

(1)

1:'J/'

and W:,~~z' is the weight from hidden unit H1:'J/'z' to the exponential unit X1:J/z,
Since we use linked weights in each block, the entire structure is translationally
invariant. We make use of this property in our training algorithm and project out
the positional information by summing over the entire layer, Sz = L1:Y X1:J/z, This
allows us to give non-specific target information in the form of "the input contains
a 5 and a 3, but I will not say where." We do this by converting the summed
information in"to an output probability, pz = 1!5?.

A Self-Organizing Integrated Segmentation and Recognition Neural Net

2.1

The learning Rule

There are two objective functions that we have used to train ISR networks: cross
entropy and total-sum-square-error. I Ez tzlnpz + (1 - t z )ln(l - Pz), where t z
equals 1 if pattern z is presented and 0 otherwise. Computing the gradient with
respect to the net input to a particular exponential unit yields the following term
in our learning rule:

=

~ -- (t z _ pz )
8TJ~yz

X~yz

(2)

E~y X~yz

It should be noted that this is a kind of competitive rule in which the learning is

proportional to the relative strength of the activation at the unit at a particular
location in the X layer to the strength of activation in the entire layer. For example,
suppose that X2,3,5
1000 and X5,3,5= 100. Given the above rules, X2,3,5 would
receive about 10 times more of the output error than the unit X5,3,5. Thus the units
compete with each other for the credit or blame of the output, and the "rich get
richer" until the proper target is achieved. This favors self-organization of highly
localized spikes of activity in the exponential layers directly above the particular
character that the exponential layer detects ("smart histograms" as shown in Figure 1). Note that we never give positional information in the network but that the
network self-organizes the exponential unit activity to discern the positional information. The second function is the total-sum-square error, E = Ez(tz - pz)2. For
the total-sum-square error measure, the gradient term becomes

=

8E
uTJ~yz

-~-

=

(

tz -

pz

)

X~yz

~

(1 + L.~y X~yz

)2 .

(3)

Again this has a competitive term, but the competition is only important for X~yz
large, otherwise the denominator is dominated by 1 for small E~y X~yz. We used
the quadratic error function for the networks reported in the next section.

3
3.1

NIST DATABASE RECOGNITION
Data

We tested this neural network algorithm on the problem of segmenting and recognizing handwritten numerals from the NIST database. This database contains
approximately 273,000 samples of handwritten numerals collected from the Bureau
of Census field staff. There were 50 different forms used in the study, each with
33 fields, 28 of which contain handwritten numerals ranging in length from 2 to 10
digits per field. We only used fields of length 2 to 6 (field numbers 6 to 30). We
used two test sets: a small test set, Test Set A of approximately 4,000 digits, 1,000
fields, from forms labeled f1800 to f1840 and a larger test set, Test Set B, containing
20,000 numerals 5,000 fields and 200 forms from f1800 to f1899 and f2000 to f2199.
We used two different training sets: a hand-segmented training set containing approximately 33,000 digits from forms mooo to m636 (the Segmented Training Set)
and another training set that was never hand-segmented from forms mooo to f1800
(the Unsegmented Training Set. We pre-processed the fields with a simple boxremoval and size-normalization program before they were input to the ISR net.

499

500

Keeler and Rumelhart

The hand segmentation was conventional in the sense that boxes were drawn around
each of the characters, but we the boxes included any other portions of characters
that may be nearby or touching in the natural context. Note that precise labeling of
the characters is not essential at all. We have trained systems where only the center
information the characters was used and found no degradation in performance. This
is due to the fact that the system self-organizes the positional information, so it is
only required that we know whether a character is in a field, not precisely where.

3.2

TRAINING

We trained several nets on the NIST database. The best training procedure was
as follows: Step 1): train the network to an intermediate level of accuracy (96%
or so on single characters, about 12 epochs of training set 1). Note that when we
train on single characters, we do not need isolated characters - there are often
portions of other nearby characters within the input field. Indeed, it helps the ISR
performance to use this natural context. There are two reasons for this step: the
first is speed - training goes much faster with single characters because we can use a
small network. We also found a slight generalization accuracy benefit by including
this training step. Step 2): copy the weights of this small network into a larger
network and start training on 2 and 3 digit fields from the database without hand
segmentation. These are fields numbered 6,7,11,15,19,20,23,24,27, and 28. The
reason that we use these fields is that we do not have to hand-segment them - we
present the fields to the net with the answer that the person was supposed to write
in the field. (There were several cases where the person wrote the wrong numbers
or didn't write anything. These cases were NOT screened from the training set.)
Taking these fields from forms mooo to f1800 gives us another 45,000 characters to
train on without ever segmenting them.
There were several reasons that we use fields of length 2 and 3 and not fields of
4,5,or 6 for training (even though we used these in testing). First, 3 characters
covers the most general case: a character either has no characters on either side,
one to the left, one to the right or one on both sides (3 characters total). If we train
on 3 characters and duplicate the weights, we have covered the most general case for
any number of characters, and it is clearly faster to train on shorter fields. Second,
training with more characters confuses the net. As pointed out in our previous
work (keeler 1991a), the learning algorithm that we use is only valid for one or no
characters of a given type presented in the input field. Thus, the field '39541' is ok
to train on, but the field '288' violates one of the assumptions of the training rule.
In this case the two 8 's would be competing with each other for the answer and
the rule favors only one winner. Even though this problem occurs 1/lth of the
time for two digit fields, it is not serious enough to prevent the net from learning.
(Clearly it would not learn fields of length 10 where all of the target units are
turned on and there would be no chance for discrimination.) This problem could
be avoided by incorporating order information into training and we have proposed
several mechanisms for incorporating order information in training, but do not use
them in the present system. Note that this biases the training toward the a-priori
distribution of characters in the 2 and 3 digit fields, which is a different distribution
from that of the testing set.
The two networks that we used had the following architectures: Net1: Input: 28x24

A Self-Organizing Integrated Segmentation and Recognition Neural Net

receptive fields 6x6 shift 2x2. hidden 1: 12xllx12 receptive fields 4x4x12 shift
2x2x12. hidden 2: 5x4x18 receptive fields 3x3x18 shift lxlxl8. exponentials (block
3): 3x2xlO 10 summing, 10 outputs.
Net2: Input: 28x26 receptive fields 6x6 shift 2x4. hidden 1: 12x6x12 receptive
fields 5x4x12 shift lx2xl2. hidden 2: 8x2x18 receptive fields 5x2x18 shift lxlxl8.
exponentials (block 3): 4xlxlO 10 summing, 10 outputs.
100

0/0

c

99

0

98

r
r

97

e
c
t

A

n1&2

B

96

99. 5 t--+-+--t---:lhr-t:~rI-',

n2

95
94
93
92

91 . . . .______. ._ .....

o

5

10

15

20

25

3C

98 ----'--I-

,6

0/0 Rejected

97.5 .........._ ....~............_ ..............
5

10 15 20 25 30 35 40 45 50 55 60

Figure 2: Average combined network performance on the NIST database. Figure
2A shows the generalization performance of two neural networks on the NIST Test
Set A. The individual nets Netl and Net2 (nl, n2 respectively) and the combined
performance of nets 1 and 2 are shown where fields are rejected when the nets differ.
The curves show results for fields ranging length 2 to 6 averaged over all fields for
1,000 total fields, 4,000 characters. Note that Net2 is not nearly as accurate as Netl
on fields, but that the combination of the two is significantly better than either.
For this test set the rejection rate is 17% (83% acceptance) with an accuracy rate of
99.3% (error rate 0.7%) overall on fields of average length 4. Figure 2B shows the
per-field performance for test-set B (5,000 fields, 20,000 digits) Again both nets are
used for the rejection criterion. For comparison, 99% accuracy on fields of length 4
is achieved at 23% rejection.
Figure 2 shows the generalization performance on the NIST database for Netl, Net2
and their combination. For the combination, we accepted the answer only when the
networks agreed and rejected further based on a simple confidence measure (the
difference of the two highest activations) of each individual net.

501

502

Keeler and Rumelhart

../f.

.!

~./.~;,
I .I

1"'"

I

Figure 3: Examples of correctly recognized fields in the NIST database. This figure
shows examples of fields that were correctly recognized by the ISR network. Note
the cases of touching characters, multiple touching characters, characters touching
in multiple places, fields with extrinsic noise, broken characters and touching, broken
characters with noise. Because of the integrated nature of the segmentation and
recognition, the same system is able to handle all of these cases.

4

DISCUSSION AND CONCLUSIONS

This investigation has demonstrated that the ISR algorithm can be used for integrated segmentation and recognition and achieve high-accuracy results on a large
database of hand-printed numerals. The overall accuracy rates of 83% acceptance
with 99.3% accuracy on fields of average length 4 is competitive with accuracy reported in commercial products. One should be careful making such comparisons.
We found a variance of 7% or more in rejection performance on different test sets
with more than 1,000 fields (a good statistical sample). Perhaps more important
than the high accuracy, we have demonstrated that the ISR system is able to deal
with touching, broken and noisy characters. In other investigations we have demonstrated the ISR system on alphabetic characters with good results, and on speech
recognition (Keeler, Rumelhart, Zand-Biglari, 1991) where the results are slightly
better than Hidden Markov Model results.
There are several attractive aspects about the ISR algorithm: 1) Labeling can be
"sloppy" in the sense that the borders of the characters do not have to be defined.
This reduces the labor burden of getting a system running. 2) The final weights can
be duplicated so that the system can all run in parallel. Even with both networks
running, the number of weights and activations needed to be stored in memory is
quite small - about 30,000 floating point numbers, and the system is quite fast
in the feed-forward mode: peak performance is about 2.5 characters/sec on a Dec
5000 (including everything: both networks running, input pre-processing, parsing
the answers, printing results, etc.). This structure is ideal for VLSI implementation
since it contains a very small number of weights (about 5,000). This is one possible
way around the computational bottleneck facing encountered in processing complex
scenes - the ISR net can do very-fast first-cut scene analysis with good discrimi-

A Self-Organizing Integrated Segmentation and Recognition Neural Net

nation of similar objects - an extremely difficult task. 3) The ISR algorithm and
architecture presents a new and powerful approach of using forward models to convert position-independent training information into position-specific error signals.
4) There is no restriction to one-dimension; The same ISR structure has been used
for two-dimensional parsing.
Nevertheless, there are several aspects of the ISR net that require improvement for
future progress. First, the algorithmic assumption of having one pattern of a given
type in the input field is too restrictive and can cause confusion in some training
examples. Second, we are throwing some information away when we project out
all of the positional information order information could be incorporated into the
training information. This extra information should improve training performance
due to the more-specific error signals. Finally, normalization is still a problem.
We do a crude normalization, and the networks are able to segment and recognize
characters as long as the difference in size is not too large. A factor of two in
size difference is easily handled with the ISR system, but a factor of four decreases
recognition accuracy by about 3-5% on the character recognition rates. This requires a tighter coupling between the segmentation/recognition and normalization.
Just as one must segment and recognize simultaneously, in many cases one can't
properly normalize until segmentation/recognition has occurred. Fortunately, in
most document processing applications, crude normalization to within a factor of
two is simple to achieve, allowing high accuracy networks.
Acknowledgements

We thank Wee-Kheng Leow, Steve O'Hara, John Canfield, for useful discussions
and coding.
References

(1] J.D. Keeler, D.E. Rumelhart, and W.K. Leow (1991a) "Integrated Segmentation and Recognition of Hand-printed Numerals". In: Lippmann, Moody and
Touretzky, Editors, Neural Information Processing Systems 3, 557-563.
[2] J.D. Keeler, D.E. Rumelhart, and S. Zand-Biglari (1991b) "A Neural Network
For Integrated Segmentation and Recognition of Continuous Speech". MCC
Technical Report ACT-NN-359-91.
[3] K. Lang, A. Waibel, G. Hinton. (1990) A time delay Neural Network Architecture for Isolated Word Recognition. Neural Networks, 3 23-44.
[4] Y. Le Cun, B. Boser, J .S. Denker, S. Solla, R. Howard, and L. Jackel.
(1990) "Back-Propagation applied to Handwritten Zipcode Recognition." Neural Computation 1(4):541-551.
[5] G. Martin, J. Pittman (1990) "Recognizing hand-printed letters and digits."
In D. Touretzky (Ed.). Neural Information Processing Systems 2, 405-414,
Morgan Kauffman Publishers, San Mateo, CA.
[6] The NIST database can be obtained by writing to: Standard Reference Data
National Institute of Standards and Technology 221/ A323 Gaithersburg, MD
20899 USA and asking for NIST special database 1 (HWDB).

503


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 5517-spectral-methods-for-supervised-topic-models.pdf

Spectral Methods for Supervised Topic Models

Yining Wang?
Jun Zhu?
Machine Learning Department, Carnegie Mellon University, yiningwa@cs.cmu.edu
?
Dept. of Comp. Sci. & Tech.; Tsinghua National TNList Lab; State Key Lab of Intell. Tech. & Sys.,
Tsinghua University, dcszj@mail.tsinghua.edu.cn
?

Abstract
Supervised topic models simultaneously model the latent topic structure of large
collections of documents and a response variable associated with each document. Existing inference methods are based on either variational approximation or
Monte Carlo sampling. This paper presents a novel spectral decomposition algorithm to recover the parameters of supervised latent Dirichlet allocation (sLDA)
models. The Spectral-sLDA algorithm is provably correct and computationally
efficient. We prove a sample complexity bound and subsequently derive a sufficient condition for the identifiability of sLDA. Thorough experiments on a diverse
range of synthetic and real-world datasets verify the theory and demonstrate the
practical effectiveness of the algorithm.

1

Introduction

Topic modeling offers a suite of useful tools that automatically learn the latent semantic structure of a
large collection of documents. Latent Dirichlet allocation (LDA) [9] represents one of the most popular topic models. The vanilla LDA is an unsupervised model built on input contents of documents.
In many applications side information is available apart from raw contents, e.g., user-provided rating scores of an online review text. Such side signal usually provides additional information to
reveal the underlying structures of the documents in study. There have been extensive studies on
developing topic models that incorporate various side information, e.g., by treating it as supervision.
Some representative models are supervised LDA (sLDA) [8] that captures a real-valued regression
response for each document, multiclass sLDA [21] that learns with discrete classification responses,
discriminative LDA (DiscLDA) [14] that incorporates classification response via discriminative linear transformations on topic mixing vectors, and MedLDA [22, 23] that employs a max-margin
criterion to learn discriminative latent topic representations.
Topic models are typically learned by finding maximum likelihood estimates (MLE) through local
search or sampling methods [12, 18, 19], which may suffer from local optima. Much recent progress
has been made on developing spectral decomposition [1, 2, 3] and nonnegative matrix factorization
(NMF) [4, 5, 6, 7] methods to infer latent topic-word distributions. Instead of finding MLE estimates,
which is a known NP-hard problem [6], these methods assume that the documents are i.i.d. sampled
from a topic model, and attempt to recover the underlying model parameters. Compared to local
search and sampling algorithms, these methods enjoy the advantage of being provably effective. In
fact, sample complexity bounds have been proved to show that given a sufficiently large collection
of documents, these algorithms can recover the model parameters accurately with a high probability.
Although spectral decomposition (as well as NMF) methods have achieved increasing success in
recovering latent variable models, their applicability is quite limited. For example, previous work
has mainly focused on unsupervised latent variable models, leaving the broad family of supervised
models (e.g., sLDA) largely unexplored. The only exception is [10] which presents a spectral method
for mixtures of regression models, quite different from sLDA. Such ignorance is not a coincidence
as supervised models impose new technical challenges. For instance, a direct application of previous
1

techniques [1, 2] on sLDA cannot handle regression models with duplicate entries. In addition, the
sample complexity bound gets much worse if we try to match entries in regression models with their
corresponding topic vectors. On the practical side, few quantitative experimental results (if any at
all) are available for spectral decomposition based methods on LDA models.
In this paper, we extend the applicability of spectral learning methods by presenting a novel spectral decomposition algorithm to recover the parameters of sLDA models from empirical low-order
moments estimated from the data. We provide a sample complexity bound and analyze the identifiability conditions. A key step in our algorithm is a power update step that recovers the regression
model in sLDA. The method uses a newly designed empirical moment to recover regression model
entries directly from the data and reconstructed topic distributions. It is free from making any constraints on the underlying regression model, and does not increase the sample complexity much.
We also provide thorough experiments on both synthetic and real-world datasets to demonstrate the
practical effectiveness of our proposed algorithm. By combining our spectral recovery algorithm
with a Gibbs sampling procedure, we showed superior performance in terms of language modeling,
prediction accuracy and running time compared to traditional inference algorithms.

2

Preliminaries

We first overview the basics of sLDA, orthogonal tensor decomposition and the notations to be used.
2.1

Supervised LDA

Latent Dirichlet allocation (LDA) [9] is a generative model for topic modeling of text documents.
It assumes k different topics with topic-word distributions ?1 , ? ? ? , ?k ? ?V ?1 , where V is the
vocabulary size and ?V ?1 denotes the probability simplex of a V -dimensional random vector. For
a document, LDA models a topic mixing vector h ? ?k?1 as a probability distribution over the
k topics. A conjugate Dirichlet prior with parameter ? is imposed on the topic mixing vectors. A
bag-of-word model is then adopted, which generates each word in the document based on h and
the topic-word vectors ?. Supervised latent Dirichlet allocation (sLDA) [8] incorporates an extra
response variable y ? R for each document. The response variable is modeled by a linear regression
? , where
model ?P
? Rk on either the topic mixing vector h or the averaging topic assignment vector z
1
z?i = m
1
with
m
the
number
of
words
in
a
document.
The
noise
is
assumed
to
be
Gaussian
j [zj =i]
with zero mean and ? 2 variance.
Fig. 1 shows the graph structure of two sLDA variants mentioned above. Although previous work
has mainly focused on model (b) which is convenient for Gibbs sampling and variational inference,
we consider model (a) because it will considerably simplify our spectral algorithm and analysis. One
may assume that whenever a document is not too short, the empirical distribution of its word topic
assignments should be close to the document?s topic mixing vector. Such a scheme was adopted to
learn sparse topic coding models [24], and has demonstrated promising results in practice.
2.2

High-order tensor product and orthogonal tensor decomposition
Np
ni
belongs to the tensor product of Euclidean spaces Rni .
A real p-th order tensor A ?
i=1 R
Generally we assume n1 = n2 = ? ? ? = np = n, and we can identify each coordinate of A by a
p-tuple (i1 , ? ? ? , ip ), where i1 , ? ? ? , ip ? [n]. For instance, a p-th order tensor is a vector when p = 1
and aN
matrix when p = 2. We can also consider a p-th order tensor A as a multilinear mapping. For
p n
A?
R and matrices X1 , ? ? ? , Xp ? Rn?m , the mapping A(X1 , ? ? ? , Xp ) is a p-th order tensor
Np m
P
R , with [A(X1 , ? ? ? , Xp )]i1 ,??? ,ip , j1 ,??? ,jp ?[n] Aj1 ,??? ,jp [X1 ]j1 ,i1 [X2 ]j2 ,i2 ? ? ? [Xp ]jp ,ip .
in
Consider some concrete examples of such a multilinear mapping. When A, X1 , X2 are matrices, we
have A(X1 , X2 ) = X1> AX2 . Similarly, when A is a matrix and x is a vector, A(I, x) = Ax.
Np n
An orthogonal tensor decomposition of a tensor A ?
R is a collection of orthonormal vectors
Pk
?p
k
k
{v i }i=1 and scalars {?i }i=1 such that A = i=1 ?i v i . Without loss of generality, we assume
?i are nonnegative when p is odd. Although orthogonal tensor decomposition in the matrix case
can be done efficiently by singular value decomposition (SVD), it has several delicate issues in
higher order tensor spaces [2]. For instance, tensors may not have unique decompositions, and an
orthogonal decomposition may not exist for every symmetric tensor [2]. Such issues are further
complicated when only noisy estimates of the desired tensors are available. For these reasons, we
need more advanced techniques to handle high-order tensors. In this paper, we will apply the robust
2

?

z

h

?

x
M

?

?

x
M

k

y

?, ?

z

h

?, ?

k

y

?

?

N

N

(a) yd = ? >
d hd + ?d

? d + ?d
(b) yd = ? >
d z

Figure 1: Plate notations for two variants of sLDA
tensor power method [2] to recover robust eigenvalues and eigenvectors of an (estimated) third-order
tensor. The algorithm recovers eigenvalues and eigenvectors up to an absolute error ?, while running
in polynomial time with respect to the tensor dimension and log(1/?). Further details and analysis
of the robust tensor power method are presented in Appendix A.2 and [2].
2.3

Notations

?p
Throughout,pwe
Puse2v , v?v?? ? ??v to denote the p-th order tensor generated by a vector v. We
use kvk =
i vi to denote the Euclidean norm of a vector v, kM k to denote the spectral
qP norm
2
of a matrix M and kT k to denote the operator norm of a high-order tensor. kM kF =
i,j Mij

denotes the Frobenious norm of a matrix. We use an indicator vector x ? RV to represent a word in
a document, e.g., for the i-th word in the vocabulary, xi = 1 and xj = 0 for all j 6= i. We also use
e , (e
e 2, ? ? ? , ?
eK )
O , (?1 , ?2 , ? ? ? , ?k ) ? RV ?k to denote the topicqdistribution matrix, and O
?1 , ?
P
k
?i
e i = ?0 (?0 +1) ? with ?0 = i=1 ?i .
to denote the canonical version of O, where ?

3

Spectral Parameter Recovery

We now present a novel spectral parameter recovery algorithm for sLDA. The algorithm consists of
two key components?the orthogonal tensor decomposition of observable moments to recover the
topic distribution matrix O and a power update method to recover the linear regression model ?. We
elaborate on these techniques and a rigorous theoretical analysis in the following sections.
3.1

Moments of observable variables

Our spectral decomposition methods recover the topic distribution matrix O and the linear regression
model ? by manipulating moments of observable variables. In Definition 1, we define a list of
moments on random variables from the underlying sLDA model.
Definition 1. We define the following moments of observable variables:
M1 = E[x1 ],

M2 = E[x1 ? x2 ] ?

M3 = E[x1 ? x2 ? x3 ] ?

?0
M1 ? M1 ,
?0 + 1

(1)

?0
(E[x1 ? x2 ? M1 ] + E[x1 ? M1 ? x2 ] + E[M1 ? x1 ? x2 ])
?0 + 2

2?02
M1 ? M1 ? M1 ,
(?0 + 1)(?0 + 2)
?0
(E[y]E[x1 ? x2 ] + E[x1 ] ? E[yx2 ] + E[yx1 ] ? E[x2 ])
My = E[yx1 ? x2 ] ?
?0 + 2
2?02
+
E[y]M1 ? M1 .
(?0 + 1)(?0 + 2)
+

(2)

(3)

Note that the moments M1 , M2 and M3 were also defined and used in previous work [1, 2] for the
parameter recovery for LDA models. For the sLDA model, we need to define a new moment My
in order to recover the linear regression model ?. The moments are based on observable variables
in the sense that they can be estimated from i.i.d. sampled documents. For instance, M1 can be
estimated by computing the empirical distribution of all words, and M2 can be estimated using M1
and word co-occurrence frequencies. Though the moments in the above forms look complicated,
we can apply elementary calculations based on the conditional independence structure of sLDA to
significantly simplify them and more importantly to get them connected with the model parameters
to be recovered, as summarized in Proposition 1. The proof is deferred to Appendix B.
3

Proposition 1. The momentsk can be expressed using the model parameters
as:
k

3.2

M2 =

X
X
1
2
?i ?i ? ?i , M3 =
?i ?i ? ?i ? ?i ,
?0 (?0 + 1) i=1
?0 (?0 + 1)(?0 + 2) i=1

(4)

My =

k
X
2
?i ?i ?i ? ?i .
?0 (?0 + 1)(?0 + 2) i=1

(5)

Simultaneous diagonalization

Proposition 1 shows that the moments in Definition 1 are all the weighted sums of tensor products
of {?i }ki=1 from the underlying sLDA model. One idea to reconstruct {?i }ki=1 is to perform simultaneous diagonalization on tensors of different orders. The idea has been used in a number of
recent developments of spectral methods for latent variable models [1, 2, 10]. Specifically, we first
whiten the second-order tensor M2 by finding a matrix W ? RV ?k such that W > M2 W = Ik .
This whitening procedure is possible whenever the topic distribuction vectors {?i }ki=1 are linearly
independent (and hence M2 has rank k). The whitening procedure and the linear independence
assumption also imply that {W ?i }ki=1 are orthogonal vectors (see Appendix A.2 for details), and
can be subsequently recovered by performing an orthogonal tensor decomposition on the simultaneously whitened third-order tensor M3 (W, W, W ). Finally, by multiplying the pseudo-inverse of the
whitening matrix W + we obtain the topic distribution vectors {?i }ki=1 .
It should be noted that Jennrich?s algorithm [13, 15, 17] could recover {?i }ki=1 directly from the 3rd order tensor M3 alone when {?i }ki=1 is linearly independent. However, we still adopt the above
simultaneous diagonalization framework because the intermediate vectors {W ?i }ki=1 play a vital
role in the recovery procedure of the linear regression model ?.
3.3

The power update method

Although the linear regression model ? can be recovered in a similar manner by performing simultaneous diagonalization on M2 and My , such a method has several disadvantages, thereby calling
for novel solutions. First, after obtaining entry values {?i }ki=1 we need to match them to the topic
distributions {?i }ki=1 previously recovered. This can be easily done when we have access to the true
moments, but becomes difficult when only estimates of observable tensors are available because the
estimated moments may not share the same singular vectors due to sampling noise. A more serious problem is that when ? has duplicate entries the orthogonal decomposition of My is no longer
unique. Though a randomized strategy similar to the one used in [1] might solve the problem, it
could substantially increase the sample complexity [2] and render the algorithm impractical.
We develop a power update method to resolve the above difficulties. Specifically, after obtaining the
whitened (orthonormal) vectors {v i } , ci ? W ?i 1 we recover the entry ?i of the linear regression
model directly by computing a power update v >
i My (W, W )v i . In this way, the matching problem
is automatically solved because we know what topic distribution vector ?i is used when recovering
?i . Furthermore, the singular values (corresponding to the entries of ?) do not need to be distinct
because we are not using any unique SVD properties of My (W, W ). As a result, our proposed
algorithm works for any linear model ?.
3.4

Parameter recovery algorithm

An outline of our parameter recovery algorithm for sLDA (Spectral-sLDA) is given in Alg. 1. First,
empirical estimates of the observable moments in Definition 1 are computed from the given documents. The simultaneous diagonalization method is then used to reconstruct the topic distribution
matrix O and its prior parameter ?. After obtaining O = (?1 , ? ? ? , ?k ), we use the power update
method introduced in the previous section to recover the linear regression model ?.
Alg. 1 admits three hyper-parameters ?0 , L and T . ?0 is defined as the sum of all entries in the
prior parameter ?. Following the conventions in [1, 2], we assume that ?0 is known a priori and use
this value to perform parameter estimation. It should be noted that this is a mild assumption, as in
practice usually a homogeneous vector ? is assumed and the entire vector is known [20]. The L and
T parameters are used to control the number of iterations in the robust tensor power method. In general, the robust tensor power method runs in O(k 3 LT ) time. To ensure sufficient recovery accuracy,
1

ci is a scalar coefficient that depends on ?0 and ?i . See Appendix A.2 for details.

4

Algorithm 1 spectral parameter recovery algorithm for sLDA. Input parameters: ?0 , L, T .
c2 , M
c3 and M
cy .
1: Compute empirical moments and obtain M
n?k
c
c
c
c
2: Find W ? R
such that M2 (W , W ) = Ik .
bi , v
c3 (W
c, W
c, W
c ) using the robust tensor
bi ) of M
3: Find robust eigenvalues and eigenvectors (?
power method [2] with parameters L and T .
4: Recover prior parameters: ?
bi ? 4?0 (?0 +1)
.
2 b2
(?0 +2) ?i
?0 +2 b c + >
b
2 ?i (W ) v i .
?0 +2 > c c c
bi My (W , W )b
model: ?bi ? 2 v
vi .

bi ?
5: Recover topic distributions: ?
6: Recover the linear regression
b, ?
b and {b
7: Output: ?
?i }ki=1 .

L should be at least a q
linear function of k and T should be set as T = ?(log(k) + log log(?max /?)),

0 +1)
where ?max = ?02+2 ?0?(?min
and ? is an error tolerance parameter. Appendix A.2 and [2] provide a deeper analysis into the choice of L and T parameters.

3.5

Speeding up moment computation

c3 requires O(N M 3 ) time and
In Alg. 1, a straightforward computation of the third-order tensor M
O(V 3 ) storage, where N is corpus size and M is the number of words per document. Such time
and space complexities are clearly prohibitive for real applications, where the vocabulary usually
contains tens of thousands of terms. However, we can employ a trick similar as in [11] to speed
c3 (W
c, W
c, W
c ) is needed
up the moment computation. We first note that only the whitened tensor M
3
in our algorithm, which only takes O(k ) storage. Another observation is that the most difficult
c3 can be written as Pr ci ui,1 ? ui,2 ? ui,3 , where r is proportional to N and ui,?
term in M
i=1
c3 (W
c, W
c, W
c ) in O(N M k) time
contains at most
M
non-zero
entries.
This allows us to compute M
Pr
by computing i=1 ci (W > ui,1 ) ? (W > ui,2 ) ? (W > ui,3 ). Appendix B.2 provides more details
about this speed-up trick. The overall time complexity is O(N M (M + k 2 ) + V 2 + k 3 LT ) and the
space complexity is O(V 2 + k 3 ).

4

Sample Complexity Analysis

We now analyze the sample complexity of Alg. 1 in order to achieve ?-error with a high probability.
For clarity, we focus on presenting the main results, while deferring the proof details to Appendix A,
including the proofs of important lemmas that are needed for the main theorem.
e and ?k (O)
e be the largest and the smallest singular values of the canonical
Theorem 1. Let ?1 (O)
q
q
?0 (?0 +1)
0 +1)
e Define ?min , 2
topic distribution matrix O.
and ?max , ?02+2 ?0?(?min
with
?0 +2
?max
b, ?
b and ?
b are the outputs of
?max and ?min the largest and the smallest entries of ?. Suppose ?
Algorithm 1, and L is at least a linear function of k. Fix ? ? (0, 1). For any small error-tolerance
parameter ? > 0, if Algorithm 1 is run with parameter T = ?(log(k) + log log(?max /?)) on N
i.i.d. sampled documents (each containing at least 3 words) with N ? max(n1 , n2 , n3 ), where
p



2 ?2 (? + 1)2
p
(1 + log(9/?))2
1 k2
0
n1 = C1 ? 1 + log(6/?) ? 0
, n 3 = C3 ?
? max
,
,
e 10
?min
?2 ?2min
?k (O)
p


(1 + log(15/?))2
2
e 2 ,
n2 = C2 ?
? max (k?k + ??1 (?/60?))2 , ?max
?1 (O)
e 4
?2 ?k (O)

and C1 , C2 and C3 are universal constants, then with probability at least 1 ? ?, there exists a
permutation ? : [k] ? [k] such that for every topic i, the following holds:
1. |?i ? ?
b?(i) | ?

4?0 (?0 +1)(?max +5?)
(?0 +2)2 ?2min (?min ?5?)2



e 8?max +
b ?(i) k ? 3?1 (O)
2. k?i ? ?
?min
3. |?i ? ?b?(i) | ?



k?k
?min

? 5?, if ?min > 5?;
5(?0 +2)
2


+ (?0 + 2) ?.
5




+ 1 ?;

? error (1?norm)

? error (1?norm)
0.6

M=250
M=500

0.4

M=250
M=500

10

5

? error (1?norm)
M=250
M=500

0.4
0.2

0.2
0

300

600 1000 3000 6000 10000

0

300

600 1000 3000 6000 10000

0

300

600 1000 3000 6000 10000

Figure 2: Reconstruction errors of Alg. 1. X axis denotes the training size. Error bars denote the
standard deviations measured on 3 independent trials under each setting.
In brevity, the proof is based on matrix perturbation lemmas (see Appendix A.1) and analysis to
the orthogonal tensor decomposition methods (including SVD and robust tensor power method)
performed on inaccurate tensor estimations (see Appendix A.2). The sample complexity lower
bound consists of three terms, from n1 to n3 . The n3 term comes from the sample complexity
bound for the robust tensor power method [2]; the (k?k + ??1 (?/60?))2 term in n2 characterizes
2
e 2 term arises when
the recovery accuracy for the linear regression model ?, and the ?max
?1 (O)
we try to recover the topic distribution vectors ?; finally, the term n1 is required so that some
e and could be
technical conditions are met. The n1 term does not depend on either k or ?k (O),
largely neglected in practice.
An important implication of Theorem 1 is that it provides a sufficient condition for a supervised
LDA model to be identifiable, as shown in Remark 1. To some extent, Remark 1 is the best identifiability result possible under our inference framework, because it makes no restriction on the linear
regression model ?, and the linear independence assumption is unavoidable without making further
assumptions on the topic distribution matrix O.
Remark 1. Given a sufficiently large number of i.i.d. sampled documents with at least 3 words per
Pk
document, a supervised LDA model M = (?, ?, ?) is identifiable if ?0 = i=1 ?i is known and
{?i }ki=1 are linearly independent.
e and a simplified
We also make remarks on indirected quantities appeared in Theorem 1 (e.g., ?k (O))
sample complexity bound for some specical cases. They can be found in Appendix A.4.

5
5.1

Experiments
Datasets description and Algorithm implementation details

We perform experiments on both synthetic and real-world datasets. The synthetic data are generated
in a similar manner as in [22], with a fixed vocabulary of size V = 500. We generate the topic
distribution matrix O by first sampling each entry from a uniform distribution and then normalize
every column of O. The linear regression model ? is sampled from a standard Gaussian distribution.
The prior parameter ? is assumed to be homogeneous, i.e., ? = (1/k, ? ? ? , 1/k). Documents and
response variables are then generated from the sLDA model specified in Sec. 2.1.
For real-world data, we use the large-scale dataset built on Amazon movie reviews [16] to demonstrate the practical effectiveness of our algorithm. The dataset contains 7,911,684 movie reviews
written by 889,176 users from Aug 1997 to Oct 2012. Each movie review is accompanied with a
score from 1 to 5 indicating how the user likes a particular movie. The median number of words per
review is 101. A vocabulary with V = 5, 000 terms is built by selecting high frequency words. We
also pre-process the dataset by shifting the review scores so that they have zero mean.
Both Gibbs sampling for the sLDA model in Fig. 1 (b) and the proposed spectral recovery algorithm
are implemented in C++. For our spectral algorithm, the hyperparameters L and T are set to 100,
which is sufficiently large for all settings in our experiments. Since Alg. 1 can only recover the
topic model itself, we use Gibbs sampling to iteratively sample topic mixing vectors h and topic
assignments for each word z in order to perform prediction on a held-out dataset.
5.2

Convergence of reconstructed model parameters

We demonstrate how the sLDA model reconstructed by Alg. 1 converges to the underlying true
model when more observations are available. Fig. 2 presents the 1-norm reconstruction errors of ?,
? and ?. The number of topics k is set to 20 and the number of words per document (i.e., M ) is set
6

MSE (k=20)
0.4
0.2

MSE (k=50)

Neg. Log?likeli. (k=20)

ref. model
Spec?sLDA
Gibbs?sLDA

9

0.4

8.9

0.2

Neg. Log?likeli. (k=50)
8.97
8.96
8.95
8.94

0

8.8

0.20.40.60.8 1 2 4 6 8 10

0

0.20.40.60.8 1 2 4 6 8 10

0.20.40.60.8 1 2 4 6 8 10

8.93

0.20.40.60.8 1 2 4 6 8 10

Figure 3: Mean square errors and negative per-word log-likelihood of Alg. 1 and Gibbs sLDA.
Each document contains M = 500 words. The X axis denotes the training size (?103 ).
PR2 (?=0.01)
0.15
0.1

PR2 (?=0.1)

Gibbs?sLDA
Spec?sLDA
Hybrid

0.15
0.1

0.1
0
?0.1

0

0
2

4

6

8

10

?0.05
0

Neg. Log?likeli. (?=0.01)

2

4

7.6

6

8

10

?0.2
0

Gibbs?sLDA
Spec?sLDA
Hybrid

2

4

7.8

7.6

8

10

Gibbs?sLDA
Spec?sLDA
Hybrid

8

7.5

6

Neg. Log?likeli. (?=1.0)

Neg. Log?likeli. (?=0.1)
7.8

Gibbs?sLDA
Spec?sLDA
Hybrid

7.7

7.6
7.4

7.4
0

Gibbs?sLDA
Spec?sLDA
Hybrid

0.05

0.05

0

PR2 (?=1.0)

Gibbs?sLDA
Spec?sLDA
Hybrid

2

4

6

8

10

0

2

4

6

8

10

7.4
0

2

4

6

8

10

Figure 4: pR2 scores and negative per-word log-likelihood. The X axis indicates the number of
topics. Error bars indicate the standard deviation of 5-fold cross-validation.
to 250 and 500. Since Spectral-sLDA can only recover topic distributions up to a permutation over
b to find an optimal permutation.
[k], a minimum weighted graph match was computed on O and O
Fig. 2 shows that the reconstruction errors for all the parameters go down rapidly as we obtain more
documents. Furthermore, though Theorem 1 does not involve the number of words per document,
the simulation results demonstrate a significant improvement when more words are observed in each
document, which is a nice complement for the theoretical analysis.
5.3

Prediction accuracy and per-word likelihood

We compare the prediction accuracy and per-word likelihood of Spectral-sLDA and Gibbs-sLDA
on both synthetic and real-world datasets. On the synthetic dataset, the regression error is measured
by the mean square error (MSE), and the per-word log-likelihood is defined as log2 p(w|h, O) =
PK
log2 k=1 p(w|z = k, O)p(z = k|h). The hyper-parameters used in our Gibbs sampling implementation are the same with the ones used to generate the datasets.
Fig. 3 shows that Spectral-sLDA consistently outperforms Gibbs-sLDA. Our algorithm also enjoys
the advantage of being less variable, as indicated by the curve and error bars. Moreover, when the
number of training documents is sufficiently large, the performance of the reconstructed model is
very close to the underlying true model2 , which implies that Alg. 1 can correctly identify an sLDA
model from its observations, therefore supporting our theory.
We also test both algorithms on the large-scale Amazon movie review dataset. The quality of the
2
2
prediction is assessed
P with predictive
P R (pR 2) [8], a normalized version of MSE, which is defined
2
2
as pR , 1 ? ( i (yi ? ybi ) )/( i (yi ? y?) ), where ybi is the estimate, yi is the truth, and y? is
the average true value. We report the results under various settings of ? and k in Fig. 4, with the
? hyper-parameter of Gibbs-sLDA selected via cross-validation on a smaller subset of documents.
Apart from Gibbs-sLDA and Spectral-sLDA, we also test the performance of a hybrid algorithm
which performs Gibbs sampling using models reconstructed by Spectral-sLDA as initializations.
Fig. 4 shows that in general Spectral-sLDA does not perform as well as Gibbs sampling. One
possible reason is that real-world datasets are not exact i.i.d. samples from an underlying sLDA
model. However, a significant improvement can be observed when the Gibbs sampler is initialized
with models reconstructed by Spectral-sLDA instead of random initializations. This is because
Spectral-sLDA help avoid the local optimum problem of local search methods like Gibbs sampling.
Similar improvements for spectral methods were also observed in previous papers [10].
2

Due to the randomness in the data generating process, the true model has a non-zero prediction error.

7

Table 1: Training time of Gibbs-sLDA and Spectral-sLDA, measured in minutes. k is the number
of topics and n is the number of documents used in training.
n(?104 )
Gibbs-sLDA
Spec-sLDA

1
0.6
1.5

5
3.0
1.6

k = 10
10
50
6.0 30.5
1.7 2.9

100
61.1
4.3

1
2.9
3.1

5
14.3
3.6

k = 50
10
50
28.2 145.4
4.3
9.5

100
281.8
16.2

Table 2: Prediction accuracy and per-word log-likelihood of Gibbs-sLDA and the hybrid algorithm.
The initialization solution is obtained by running Alg. 1 on a collection of 1 million documents,
while n is the number of documents used in Gibbs sampling. k = 8 topics are used.
log10 n
Gibbs-sLDA
Hybrid

3
0.00
(0.01)
0.02
(0.01)

predictive R2
4
5
0.04
0.11
(0.02) (0.02)
0.17
0.18
(0.03) (0.03)

6
0.14
(0.01)
0.18
(0.03)

Negative per-word log-likelihood
3
4
5
6
7.72
7.55
7.45
7.42
(0.01) (0.01) (0.01) (0.01)
7.70
7.49
7.40
7.36
(0.01) (0.02) (0.01) (0.01)

Note that for k > 8 the performance of Spectral-sLDA significantly deteriorates. This phenomenon
can be explained by the nature of Spectral-sLDA itself: one crucial step in Alg. 1 is to whiten the
c2 , which is only possible when the underlying topic matrix O has full rank.
empirical moment M
c2 when the underlying model
For the Amazon movie review dataset, it is impossible to whiten M
contains more than 8 topics. This interesting observation shows that the Spectral-sLDA algorithm
can be used for model selection to avoid overfitting by using too many topics.
5.4

Time efficiency

The proposed spectral recovery algorithm is very time efficient because it avoids time-consuming
iterative steps in traditional inference and sampling methods. Furthermore, empirical moment computation, the most time-consuming part in Alg. 1, consists of only elementary operations and could
be easily optimized. Table 1 compares the training time of Gibbs-sLDA and Spectral-sLDA and
shows that our proposed algorithm is over 15 times faster than Gibbs sampling, especially for large
document collections. Although both algorithms are implemented in a single-threading manner,
Spectral-sLDA is very easy to parallelize because unlike iterative local search methods, the moment
computation step in Alg. 1 does not require much communication or synchronization.
There might be concerns about the claimed time efficiency, however, because significant performance improvements could only be observed when Spectral-sLDA is used together with GibbssLDA, and the Gibbs sampling step might slow down the entire procedure. To see why this is not
the case, we show in Table 2 that in order to obtain high-quality models and predictions, only a
very small collection of documents are needed after model reconstruction of Alg. 1. In contrast,
Gibbs-sLDA with random initialization requires more data to get reasonable performances.
To get a more intuitive idea of how fast our proposed method is, we combine Tables 1 and 2 to see
that by doing Spectral-sLDA on 106 documents and then post-processing the reconstructed models
using Gibbs sampling on only 104 documents, we obtain a pR2 score of 0.17 in 5.8 minutes, while
Gibbs-sLDA takes over an hour to process a million documents with a pR2 score of only 0.14.
Similarly, the hybrid method takes only 10 minutes to get a per-word likelihood comparable to the
Gibbs sampling algorithm that requires more than an hour running time.

6

Conclusion

We propose a novel spectral decomposition based method to reconstruct supervised LDA models
from labeled documents. Although our work has mainly focused on tensor decomposition based
algorithms, it is an interesting problem whether NMF based methods could also be applied to obtain
better sample complexity bound and superior performance in practice for supervised topic models.

Acknowledgement
The work was done when Y.W. was at Tsinghua. The work is supported by the National Basic Research Program of China (No. 2013CB329403), National NSF of China (Nos. 61322308,
61332007), and Tsinghua University Initiative Scientific Research Program (No. 20121088071).
8

References
[1] A. Anandkumar, D. Foster, D. Hsu, S. Kakade, and Y.-K. Liu. Two SVDs suffice: Spectral decompositions for probabilistic topic modeling and latent Dirichlet allocatoin. arXiv:1204.6703,
2012.
[2] A. Anandkumar, R. Ge, D. Hsu, S. Kakade, and M. Telgarsky. Tensor decompositions for
learning latent variable models. arXiv:1210:7559, 2012.
[3] A. Anandkumar, D. Hsu, and S. Kakade. A method of moments for mixture models and hidden
Markov models. arXiv:1203.0683, 2012.
[4] S. Arora, R. Ge, Y. Halpern, D. Mimno, and A. Moitra. A practical algorithm for topic modeling with provable guarantees. In ICML, 2013.
[5] S. Arora, R. Ge, R. Kannan, and A. Moitra. Computing a nonnegative matrix factorization provably. In STOC, 2012.
[6] S. Arora, R. Ge, and A. Moitra. Learning topic models-going beyond SVD. In FOCS, 2012.
[7] V. Bittorf, B. Recht, C. Re, and J. Tropp. Factoring nonnegative matrices with linear programs.
In NIPS, 2012.
[8] D. Blei and J. McAuliffe. Supervised topic models. In NIPS, 2007.
[9] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet allocation. Journal of Machine Learning
Research, (3):993?1022, 2003.
[10] A. Chaganty and P. Liang. Spectral experts for estimating mixtures of linear regressions. In
ICML, 2013.
[11] S. Cohen and M. Collins. Tensor decomposition for fast parsing with latent-variable PCFGs.
In NIPS, 2012.
[12] M. Hoffman, F. R. Bach, and D. M. Blei. Online learning for latent Dirichlet allocation. In
NIPS, 2010.
[13] J. Kruskal. Three-way arrays: Rank and uniqueness of trilinear decompositions, with applications to arithmetic complexity and statistics. Linear Algebra and its Applications, 18(2):95?
138, 1977.
[14] S. Lacoste-Julien, F. Sha, and M. Jordan. DiscLDA: Discriminative learning for dimensionality
reduction and classification. In NIPS, 2008.
[15] S. Leurgans, R. Ross, and R. Abel. A decomposition for three-way arrays. SIAM Journal on
Matrix Analysis and Applications, 14(4):1064?1083, 1993.
[16] J. McAuley and J. Leskovec. From amateurs to connoisseus: Modeling the evolution of user
expertise through online reviews. In WWW, 2013.
[17] A. Moitra. Algorithmic aspects of machine learning. 2014.
[18] I. Porteous, D. Newman, A. Ihler, A. Asuncion, P. Smyth, and M. Welling. Fast collapsed
Gibbs sampling for latent Dirichlet allocation. In SIGKDD, 2008.
[19] R. Redner and H. Walker. Mixture densities, maximum likelihood and the EM algorithm.
SIAM Review, 26(2):195?239, 1984.
[20] M. Steyvers and T. Griffiths. Latent semantic analysis: a road to meaning, chapter Probabilistic
topic models. Laurence Erlbaum, 2007.
[21] C. Wang, D. Blei, and F.-F. Li. Simultaneous image classification and annotation. In CVPR,
2009.
[22] J. Zhu, A. Ahmed, and E. Xing. MedLDA: Maximum margin supervised topic models. Journal
of Machine Learning Research, (13):2237?2278, 2012.
[23] J. Zhu, N. Chen, H. Perkins, and B. Zhang. Gibbs max-margin topic models with data augmentation. Journal of Machine Learning Research, (15):1073?1110, 2014.
[24] J. Zhu and E. Xing. Sparse topic coding. In UAI, 2011.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 1075-improving-committee-diagnosis-with-resampling-techniques.pdf

Improving Committee Diagnosis with
Resampling Techniques

Bambang Parmanto
Department of Information Science
University of Pittsburgh
Pittsburgh, PA 15260
parmanto@li6.pitt. edu

Paul W. Munro
Department of Information Science
University of Pittsburgh
Pittsburgh, PA 15260
munro@li6.pitt. edu

Howard R. Doyle
Pittsburgh Transplantation Institute
3601 Fifth Ave, Pittsburgh, PA 15213
doyle@vesaliw. tu. med. pitt. edu

Abstract
Central to the performance improvement of a committee relative to
individual networks is the error correlation between networks in the
committee. We investigated methods of achieving error independence between the networks by training the networks with different
resampling sets from the original training set. The methods were
tested on the sinwave artificial task and the real-world problems of
hepatoma (liver cancer) and breast cancer diagnoses.

1

INTRODUCTION

The idea of a neural net committee is to combine several neural net predictors
to perform collective decision making, instead of using a single network (Perrone,
1993). The potential of a committee in improving classification performance has
been well documented. Central to this improvement is the extent to which the
errors tend to coincide. Committee errors occur where the misclassification sets of
individual networks overlap. On the one hand, if all errors of committee members
coincide, using a committee does not improve performance. On the other hand, if
errors do not coincide, performance of the committee dramatically increases and
asymptotically approaches perfect performance. Therefore, it is beneficial to make
the errors among the networks in the committee less correlated in order to improve
the committee performance.

Improving Committee Diagnosis with Resampling Techniques

883

One way of making the networks less correlated is to train them with different sets
of data. Decreasing the error correlation by training members of the committee
using different sets of data is intuitively appealing. Networks trained with different
data sets have a higher probability of generalizing differently and tend to make
errors in different places in the problem space.
The idea is to split the data used in the training into several sets. The sets are
not necessarily mutually exclusive, they may share part of the set (overlap). This
idea resembles resampling methods such as cross-validation and bootstrap known
in statistics for estimating the error of a predictor from limited sets of available
data. In the committee framework, these techniques are recast to construct different
training sets from the original training set. David Wolpert (1992) has put forward
a general framework of training the committee using different partitions of the
data known as stacked generalization. This approach has been adopted to the
regression environment and is called stacked regression (Breiman, 1992). Stacked
regression uses cross-validation to construct different sets of regression functions.
A similar idea of using a bootstrap method to construct different training sets has
been proposed by Breiman (1994) for classification and regression trees predictors.

2
2.1

THE ALGORITHMS
BOOTSTRAP COMMITTEE (BOOTC)

Consider a total of N items are available for training. The approach is to generate
K replicates from the original set, each containing the same number of item as the
original set. The replicates are obtained from the original set by drawing at random
with replacement. See Efron & Tibshirani (1993) for background on bootstrapping.
Use each replicate to train each network in the committee.
Using this bootstrap procedure, each replicate is expected to include roughly 36
% duplicates (due to replacement during sampling). Only the distinct fraction is
used for training and the leftover fraction for early stopping, if necessary (notice
slight difference from the standard bootstrapping and from Breiman's bagging).
Early stopping usually requires a fraction of the data to be taken from the original
training set, which might degrade the performance of the neural network. The
advantage of a BOOTC is that the leftover sample is already available.
Algorithm:
1. Generate bootstrap replicates Ll, ... , LK from the original set.

2. For each bootstrap replicate, collect unsampled items into leftover sample
..
l*l , ... , l*K .
set s, gIVIng:
3. For each Lk, train a network. Use the leftover set l*k as validation stopping
criteria if necessary. Giving K neural net predictors: f(~i Lk)
4. Build a committee from the bootstrap networks using a simple averaging
procedure: fcom(~) =
~~=l f(~i Lk)

ic

There is no rule as to how many bootstrap replicates should be used to achieve a
good performance. In error estimation, the number ranges from 20 to 200. It is
beneficial to keep the number of replicates, hence the number of networks, small to
reduce training time. Unless the networks are trained on a parallel machine, training
time increases proportionally to the number of networks in the committee. In this
experiment, 20 bootstrap training replicates were constructed for 20 networks in

884

B. PARMANTO, P. W. MUNRO, H. R. DOYLE

the committee. Twenty replicates were chosen since beyond this number there is
no significant improvement on the performance.

2.2

CROSS-VALIDATION COMMITTEE (CVC)

The algorithm is quite similar to the procedure used in prediction error estimation.
First, generate replicates from the original training set by removing a fraction of
the data. Let D denote the original data, and D- V denote the data with subset
v removed. The procedure revolves so that each item is in the removed fraction
at least once. Generate replicates D11Jl , ??? Di/Ie and train each network in the
committee with one replicate.
An important issue in the eve is the degree of data overlap between the replicates.
The degree of overlap depends on the number of replicates and the size of a removed
fraction from the original sample. For example, if the committee consists of 5
networks and 0.5 of the data are removed for each replicate, the minimum fraction
of overlap is 0 (calculation: (v x 2) - 1.0) and the maximum is ~ (calculation:
1.0 -

k)'

Algorithm:

1. Divide data into v-fractions db . . . , dv
2. Leave one fraction die and train network fie with the rest of the data (D-d le ).
3. Use die as a validation stopping criteria, if necessary.
4. Build a committee from the networks using a simple averaging procedure.
The fraction of data overlap determines the trade-off between the individual network
performance and error correlation between the networks. Lower correlation can be
expected if the networks train with less overlapped data, which means a larger
removed fraction and smaller fraction for training. The smaller the training set
size, the lower the individual network performance that can be expected.
We investigated the effect of data overlap on the error correlations between the
networks and the committee performance. We also studied the effect of training
size on the individual performance. The goal was to find an optimal combination
of data overlap and individual training size.

3

THE BASELINE & PERFORMANCE EVALUATION

To evaluate the improvement of the proposed methods on the committee performance, they should be compared with existing methods as the baseline. The common method for constructing a committee is to train an ensemble of networks
independently. The networks in the committee are initialized with different sets
of weights. This type of committee has been reported as achieving significant improvement over individual network performances in regression (Hashem, 1993) and
classification tasks (Perrone, 1993; Parmanto et al., 1994).
The baseline, BOOTe, and eve were compared using exactly the same architecture
and using the same pair of training-test sets. Performance evaluation was conducted
using 4-fold exhaustive cross-validation where 0.25 fraction of the original data is
used for the test set and the remainder of the data is used for the training set. The
procedure was repeated 4 times so that all items were once on the test set. The
performance was calculated by averaging the results of 4 test sets. The simulations

Improving Committee Diagnosis with Resampling Techniques

885

were conducted several times using different initial weights to exclude the possibility
that the improvement was caused by chance.

4
4.1

EXPERIMENTS
SYNTHETIC DATA: SINWAVE CLASSIFICATION

The sinwave task is a classification problem with two classes, a negative class represented as 0 and a positive class represented as 1. The data consist of two input
variables, x = (Xli X2). The entire space is divided equally into two classes with
the separation line determined by the curve X2 = sin( 2: Xl). The upper half of the
rectangle is the positive class, while the lower half is the negative one (see Fig. 1).
Gaussian noise along the perfect boundary with variance of 0.1 is introduced to
the clean data and is presented in Fig. 1 (middle). Let z be a vector drawn from
the Gaussian distribution with variance TI, then the classification rule is given by
equation:
(1)
A similar artificial problem is used to analyze the bias-variance trade-offs by Geman
et al. (1992).

Figure 1: Complete and clean data/without noise (top), complete data with noise
(middle), and a small fraction used for training (bottom).
The population contains 3030 data items, since a grid of 0.1 is used for both Xl and
X2 . In the real world, we usually have no access to the entire population. To mimic
this situation, the training set contained only a small fraction of the population.
Fig. 1 (bottom) visualizes a training set that contains 200 items with 100 items for
each class. The training set is constructed by randomly sampling the population.
The performance of the predictor is measured with respect to the test set. The
population (3030 items) is used as the test set.

4.2

HEPATOMA DETECTION

Hepatoma is a very important clinical problem in patients who are being considered
for liver transplantation for its high probability of recurrence. Early hepatoma
detection may improve the ultimate outlook of the patients since special treatment
can be carried out. Unfortunately, early detection using non-invasive procedures

886

B. PARMANTO, P. W. MUNRO, H. R. DOYLE

can be difficult, especially in the presence of cirrhosis. We have been developing
neural network classifiers as a detection system with minimum imaging or invasive
studies (Parmanto et al., 1994).
The task is to detect the presence or absence (binary output) of a hepatoma given
variables taken from an individual patient. Each data item consists of 16 variables,
7 of which are continuous variables and the rest are binary variables, primarily
blood measurements.
For this experiment, 1172 data items with their associated diagnoses are available.
Out of 1172 itmes, 693 items are free from missing values, 309 items contain missing
values only on the categorical variables, and 170 items contain missing values on
both types of variables. For this experiment, only the fraction without missing
values and the fraction with missing values on the categorical variables were used,
giving the total item of 1002. Out of the 1002 items, 874 have negative diagnoses
and the remaining 128 have positive diagnoses.

4.3

BREAST CANCER

The task is to diagnose if a breast cytology is benign or malignant based on cytological characteristics. Nine input variables have been established to differentiate
between the benign and malignant samples which include clump thickness, marginal
adhesion, the uniformity of cell size and shape, etc.
The data set was originally obtained from the University of Wisconsin Hospitals
and currently stored at the UCI repository for machine learning (Murphy & Aha,
1994). The current size of the data set is 699 examples.

5

THE RESULTS
Committee Performance

Indiv. Performance

~ ~.::.:.:-:~~~?:
.: : ..::.::---.-.-.........---.. . .

?
? :!

---.... _---

,I; N

~ .....

o

4

6

10

12

14

16

4

8

bas.an.

0

-

?

::-:::.

&10...,,,,

10

12

14

/I hidden units

/I hidden units

Correlation

Percent Improvement

16

0r------------------.

.... -... -

-~-------~-------~
......-. ....-.........
-- ...._-. --_._..._---.
" , --.

Q

o

o~

4

-~. . . .

& :::: li&>mr",
________________
6

10

12

II hidden units

14

~

16

8

10

/I hidden

12

14

16

...,its

Figure 2: Results on the sinwave classif. task. Performances of individual nets
and the committee (top); error correlation and committee improvement (bottom).
Figure 2. (top) and Table 1. show that the performance of the committee is always
better than the average performance of individual networks in all three committees.

Improving Committee Diagnosis with Resampling Techniques

Task

Methods

Smwave
(2 vars )

Baseline
BOOTC
CVC
Baseline
BOOTC
CVC
BaSeline
BOOTC
CVC

Cancer
(9 vars)
Hepatoma
(16 vars)

Indiv. Nets
% error
13.31
12.85
15.72
2.7
3.14
3.2
25.95
26.00
26.90

Error
Corr
.87
.57
.33
.96
.83
.80
.89
.70
.55

Committee
% error
11.8
8.36
9.79
2.5
2.0
1.63
23.25
19.72
19.05

887

Improv.
to Indiv.
11 '70
35 %
38 %
5%
34 %
49 %
10.5 %
24 %
29 %

Improv.
to baseline

29 %
17 %

20 %
35 %

15.2 %
18 %

Table 1: Error rate, correlation, and performance improvement calculated based on
the best architecture for each method. Reduction of misclassification rates compare
to the baseline committee
Correlation vs . Fraction of Data Overlap

0r-----------------------____- .
m

?

.,.,
T

N

o

!

i~ ,,
Fraction 01data overlap

Figure 3: Error correlation and fraction of overlap in training data (results from
the sinwave classification task).

The CVC and BOOTC are always better than the baseline even when the individual
network performance is worse. Figure 2 (bottom) and the table show that the
improvement of a committee over individual networks is proportional to the error
correlation between the networks in the committee. The CVC consistently produces
significant improvement over its individual network performance due to the low error
correlation, while the baseline committee only produces modest improvement. This
result confirms the basic assumption of this research: committee performance can
be improved by decorrelating the errors made by the networks.
The performance of a committee depends on two factors: individual performance of
the networks and error correlation between the networks. The gain of using BOOTC
or CVC depends on how the algorithms can reduce the error correlations while still
maintaining the individual performance as good as the individual performance of the
baseline. The BOOTC produced impressive improvement (29 %) over the baseline
on the sinwave task due to the lower correlation and good individual performance.
The performances of the BOOTC on the other two tasks were not as impressive
due to the modest reduction of error correlation and slight decrease in individual
performance. The performances were still significantly better than the baseline
committee. The CVC, on the other hand, consistently reduced the correlation and

888

B. PARMANTO, P. W. MUNRO, H. R. DOYLE

improved the committee performance. The improvement on the sinwave task was
not as good as the BOOTC due to the low individual performance.
The individual performance of the CVC and BOOTC in general are worse than the
baseline. The individual performance of CVC is 18 % and 19 % lower than the
baseline on the sinwave and cancer tasks respectively, while the BOOTC suffered
significant reduction of individual performance only on the cancer task (16 %). The
degradation of individual performance is due to the smaller training set for each
network on the CVC and the BOOTC. The detrimental effect of a small training
set, however, is compensated by low correlation between the networks. The effect
of a smaller training set depends on the size of the original training set. If the data
size is large, using a smaller set may not be harmful. On the contrary, if the data set
is small, using an even smaller data set can significantly degrade the performance.
Another interesting finding of this experiment is the relationship between the error
correlation and the overlap fraction in the training set. Figure 3 shows that small
data overlap causes the networks to have low correlation to each other.

6

SUMMARY

Training committees of networks using different set of data resampled from the
original training set can improve committee performance by reducing the error correlation among the networks in the committee. Even when the individual network
performances of the BOOTC and CVC degrade from the baseline networks, the
committee performance is still better due to the lower correlation.
Acknowledgement

This study is supported in part by Project Grant DK 29961 from the National
Institutes of Health, Bethesda, MD. We would like to thank the Pittsburgh Transplantation Institute for providing the data for this study.
References

Breiman, L, (1992) Stacked Regressions, TR 367, Dept. of Statistics., UC. Berkeley.
Breiman, L, (1994) Bagging Predictors, TR 421, Dept. of Statistics, UC. Berkeley.
Efron, B., & Tibshirani, R.J. (1993) An Introd. to the Bootstrap. Chapman & Hall.
Hashem, S. (1994). Optimal Linear Combinations of Neural Networks. PhD Thesis,
Purdue University.
Geman, S., Bienenstock, E., and Doursat, R. (1992) Neural networks and the
bias/variance dilemma. Neural Computation, 4(1), 1-58.
Murphy, P. M., &. Aha, D. W. (1994). UCI Repository of machine learning databases
[ftp: ics.uci.edu/pub/machine-Iearning-databases/]
Parmanto, B., Munro, P.W., Doyle, H.R., Doria, C., Aldrighetti, 1., Marino, I.R.,
Mitchel, S., and Fung, J.J. (1994) Neural network classifier for hepatoma detectipn.
Proceedings of the World Congress of Neural Networks 1994 San Diego, June 4-9.
Perrone, M.P. (1993) Improving Regression Estimation: Averaging Methods for
Variance Reduction with Eztension to General Convez Measure Optimization. PhD
Thesis, Department of Physics, Brown University.
Wolpert, D. (1992). Stacked generalization, Neural Networks, 5, 241-259.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 1447-an-improved-policy-iteration-algorithm-for-partially-observable-mdps.pdf

An Improved Policy Iteratioll Algorithm
for Partially Observable MDPs

Eric A. Hansen
Computer Science Department
University of Massachusetts
Amherst, MA 01003
hansen@cs.umass.edu

Abstract
A new policy iteration algorithm for partially observable Markov
decision processes is presented that is simpler and more efficient than
an earlier policy iteration algorithm of Sondik (1971,1978). The key
simplification is representation of a policy as a finite-state controller.
This representation makes policy evaluation straightforward. The paper's contribution is to show that the dynamic-programming update
used in the policy improvement step can be interpreted as the transformation of a finite-state controller into an improved finite-state controller. The new algorithm consistently outperforms value iteration
as an approach to solving infinite-horizon problems.

1

Introduction

A partially observable Markov decision process (POMDP) is a generalization of the
standard completely observable Markov decision process that allows imperfect information about the state of the system. First studied as a model of decision-making in
operations research, it has recently been used as a framework for decision-theoretic
planning and reinforcement learning with hidden state (Monahan, 1982; Cassandra,
Kaelbling, & Littman, 1994; Jaakkola, Singh, & Jordan, 1995).
Value iteration and policy iteration algorithms for POMDPs were first developed by
Sondik and rely on a piecewise linear and convex representation of the value function
(Sondik, 1971; Smallwood & Sondik,1973; Sondik, 1978). Sondik's policy iteration
algorithm has proved to be impractical, however, because its policy evaluation step is
extremely complicated and difficult to implement. As a result, almost all subsequent
work on dynamic programming for POMDPs has used value iteration. In this paper,
we describe an improved policy iteration algorithm for POMDPs that avoids the difficulties of Sondik's algorithm. We show that these difficulties hinge on the choice of
a policy representation and can be avoided by representing a policy as a finite-state

E. A. Hansen

1016

controller. This representation makes the policy evaluation step easy to implement
and efficient. We show that the policy improvement step can be interpreted in a natural way as the transformation of a finite-state controller into an improved finite-state
controller. Although it is not always possible to represent an optimal policy for an
infinite-horizon POMDP as a finite-state controller, it is always possible to do so when
the optimal value function is piecewise linear and convex. Therefore representation of
a poiicy as a finite-state controller is no more limiting than representation of the value
function as piecewise linear and convex. In fact, it is the close relationship between
representation of a policy as a finite-state controller and representation of a value
function as piecewise linear and convex that the new algorithm successfully exploits.
The paper is organized as follows. Section 2 briefly reviews the POMDP model and
Sondik's policy iteration algorithm. Section 3 describes an improved policy iteration
algorithm. Section 4 illustrates the algorithm with a simple example and reports a
comparison of its performance to value iteration. The paper concludes with a discussion of the significance of this work.

2

Background

Consider a discrete-time POMDP with a finite set of states 5, a finite set of actions
A, and a finite set of observations e. Each time period, the system is in some state
i E 5, an agent chooses an action a E A for which it receives a reward with expected
value ri, the system makes a transition to state j E 5 with probability pij' and the
agent observes () E e with probability tje. We assume the performance objective is
to maximize expected total discounted reward over an infinite horizon.
Although the state of the system cannot be directly observed, the probability that it is
in a given state can be calculated. Let 7r denote a vector of state probabilities, called
an information state, where 7ri denotes the probability that the system is in state i. If
action a is taken in information state 7r and () is observed, the successor information
state is determined by revising each state probability using Bayes' theorem: trj =
LiEs 7riPijQje/ Li,jES 7riPijQje' Geometrically, each information state 7r is a point in
the (151 - I)-dimensional unit simplex, denoted II.
It is well-known that an information state 7r is a sufficient statistic that summarizes
all information about the history of a POMDP necessary for optimal action selection.
Therefore a POMDP can be recast as a completely observable MDP with a continuous
state space II and it can be theoretically solved using dynamic programming. The key
to practical implementation of a dynamic-programming algorithm is a piecewise-linear
and convex representation of the value function. Smallwood and Sondik (1973) show
that the dynamic-programming update for POMDPs preserves the piecewise linearity
and convexity of the value function. They also show that an optimal value function fot
a finite-horizon POMDP is always piecewise linear and convex. For infinite-horizon
POMDPs, Sondik (1978) shows that an optimal value function is sometimes piecewise
linear and convex and can be aproximated arbitrarily closely by a piecewise linear and
convex function otherwise.

A piecewise linear and convex value function V can be represented by a finite set
of lSI-dimensional vectors, r = {aO,a i , ?.. }, such that V(7r) = maxkLi s7riaf. A
dynamic-programming update transforms a value function V representedEfiy a set r
of a-vectors into an improved value function V' represented by a set r' of a-vectors.
Each possible a-vector in r' corresponds to choice of an action, and for each possible
observation, choice of a successor vector in r. Given the combinatorial number of
choices that can be made, the maximum n4mber of vectors in r' is IAllfll91. However
most of these potential vectors are not needed to define the updated value function
and can be pruned. Thus the dynamic-programming update problem is to find a

An Improved Policy Iteration Algorithmfor Partially Observable MDPs

J017

minimal set of vectors r' that represents V', given a set of vectors r that represents
V . Several algorithms for performing this dynamic-programming update have been
developed but describing them is beyond the scope of this paper. Any algorithm for
performing the dynamic-programming update can be used in the policy improvement
step of policy iteration. The algorithm that is presently the fastest is described by
(Cassandra, Littman, & Zhang, 1997).
For value iteration, it is sufficient to have a representation of the value function because
a policy is defined implicitly by the value function, as follows,

8(11") = a(arg mF

L

1I"i o

f),

(1)

iES

where a(k) denotes the action associated with vector ok. But for policy iteration,
a policy must be represented independently of the value function because the policy
evaluation step computes the value function of a given policy. Sondik's choice of
a policy representation is influenced by Blackwell's proof that for a continuous-space
infinite-horizon MDP, there is a stationary, deterministic Markov policy that is optimal
(Blackwell, 1965). Based on this result, Sondik restricts policy space to stationary and
deterministic Markov policies that map the continuum of information space II into
action space A. Because it is important for a policy to have a finite representation,
Sondik defines an admissible policy as a mapping from a finite number of polyhedral
regions of II to A . Each region is represented by a set of linear inequalities, where
each linear inequality corresponds to a boundary of the region.
This is Sondik's canonical representation of a policy, but his policy iteration algorithm
makes use of two other representations. In the policy evaluation step, he converts a
policy from this representation to an equivalent, or approximately equivalent, finitestate controller. Although no method is known for computing the value function of
a policy represented as a mapping from II to A, the value function of a finite-state
controller can be computed in a straightforward way. In the policy improvement
step, Sondik converts a policy represented implicitly by the updated value function
and equation (1) back to his canonical representation. The complexity of translating
between these different policy representations - especially in the policy evaluation step
- makes Sondik's policy iteration algorithm difficult to implement and explains why
it is not used in practice.

3

Algorithm

We now show that policy iteration for POMDPs can be simplified - both conceptually
and computationally - by using a single representation of a policy as a finite-state
controller.
3.1

Policy evaluation

As Sondik recognized, policy evaluation is straightforward when a policy is represented
as a finite-state controller. An o-vector representation of the value function of a finitestate controller is computed by solving the system of linear equations,
k _

0i -

a(k)

ri

+ (3'"'
a(k) a(k) s(k ,8)
L.JPij qj8 OJ
,

(2)

j ,8

where k is an index of a state of the finite-state controller, a(k) is the action associated
with machine state k, and s(k,O) is the index of the successor machine state if 0 is
observed. This value function is convex as well as piecewise linear because the expected
value of an information state is determined by assuming the controller is started in
the machine state that optimizes it.

E. A. Hansen

1018

1. Specify an initial finite-state controller, <5, and select f. for detecting conver-

gence to an f.-optimal policy.
2. Policy evaluation: Calculate a set r of a-vectors that represents the value
function for <5 by solving the system of equations given by equation 2.
3. Policy improvement: Perform a dynamic-programming update and use the
new set of vectors r' to transform <5 into a new finite-state controller, <5', as
follows:
(a) For each vector a in r':
l. If the action and successor links associated with a duplicate those of
a machine state of <5, then keep that machine state unchanged in 8'.
ii. Else if a pointwise dominates a vector associated with a machine state
of <5, change the action and successor links of that machine state to
those used to create a. (If it pointwise dominates the vectors of more
than one machine state, they can be combined into a single machine
state.)
iii. Otherwise add a machine state to <5' that has the same action and
successor links used to create a.
(b) Prune any machine state for which there is no corresponding vector in
r', as long as it is not reachable from a machine state to which a vector
in r' does correspond.
4. Termination test. If the Bellman residual is less than or equal to f.(1 - /3)//3,
exit with f.-optimal policy. Otherwise set <5 to <5' and go to step 2.
Figure 1: Policy iteration algorithm.
3.2

Policy improvement

The policy improvement step uses the dynamic-programming update to transform a
value function V represented by a set r of a-vectors into an improved value function
V' represented by a set r' of a-vectors. We now show that the dynamic-programming
update can also be interpreted as the transformation of a finite-state controller 8 into
an improved finite-state controller <5'. The transformation is made based on a simple
comparison of r' and r.
First note that some of the a-vectors in r' are duplicates of a-vectors in r, that is,
their action and successor links match (and their vector values are pointwise equal).
Any machine state of <5 for which there is a duplicate vector in r' is left unchanged.
The vectors in r' that are not duplicates of vectors in r indicate how to change the
finite-state controller. If a non-duplicate vector in r' pointwise dominates a vector
in r, the machine state that corresponds to the pointwise dominated vector in r is
changed so that its action and successor links match those of the dominating vector
in r'. If a non-duplicate vector in r' does not pointwise dominate a vector in r, a
machine state is added to the finite-state controller with the same action and successor
links used to generate the vector. There may be some machine states for which there
is no corresponding vector in r' and they can be pruned, but only if they are not
reachable from a machine state that corresponds to a vector in r'. This last point is
important because it preserves the integrity of the finite-state controller.
A policy iteration algorithm that uses these simple transformations to change a finitestate controller in the policy improvement step is summarized in Figure 1. An algorithm that performs this transformation is easy to implement and runs very efficiently
because it simply compares the a-vectors in r' to the a-vectors in r and modifies the
finite-state controller accordingly. The policy evaluation step is invoked to compute
the value function of the transformed finite-state controller. (This is only necessary

An Improved Policy Iteration Algorithmfor Partially Observable MDPs

1019

if a machine state has been changed, not if machine states have simply been added.)
It is easy to show that the value function of the transformed finite-state controller /j'
dominates the value function of the original finite-state controller, /j, and we omit the
proof which appears in (Hansen, 1998).
Theorem 1 If a finite-state controller is not optimal, policy improvement transforms
it into a finite-state controller with a value function that is as good or better for every
information state and better for some information state.
3.3

Convergence

If a finite-state controller cannot be improved in the policy improvement step (Le., all
the vectors in r' are duplicates of vectors in r), it must be optimal because the value
function satisfies the optimality equation. However policy iteration does not necessarily converge to an optimal finite-state controller after a finite number of iterations
because there is not necessarily an optimal finite-state controller. Therefore we use
the same stopping condition used by Sondik to detect t-optimality: a finite-state controller is t-optimal when the Bellman residual is less than or equal to t(l- {3) / {3, where
{3 denotes the discount factor. Representation of a policy as a finite-state controller
makes the following proof straightforward (Hansen, 1998).

Theorem 2 Policy iteration converges to an t-optimal finite-state controller after a
finite number of iterations.

4

Example and performance

We illustrate the algorithm using the same example used by Sondik: a simple twostate, two-action, two-observation POMDP that models the problem of finding an
optimal marketing strategy given imperfect information about consumer preferences
(Sondik,1971,1978). The two states of the problem represent consumer preference
or lack of preference for the manufacturers brand; let B denote brand preference
and ....,B denote lack of brand preference. Although consumer preferences cannot be
observed, they can be infered based on observed purchasing behavior; let P denote
purchase of the product and let ....,p denote no purchase. There are two marketing
alternatives or actions; the company can market a luxury version of the product (L)
or a standard version (S). The luxury version is more expensive to market but can
bring greater profit. Marketing the luxury version also increases brand preference.
However consumers are more likely to purchase the less expensive, standard product.
The transition probabilities, observation probabilities, and reward function for this
example are shown in Figure 2. The discount factor is 0.9.
Both Sondik's policy iteration algorithm and the new policy iteration algorithm converge in three iterations from a starting policy that is equivalent to the finite-state
AClions

Markel
luxury
producl (L)

Markel
slandard
producl (S)

Transilion
probabililies

B -B
B/O.8/0.2\
-B 0.5 0.5
B -B

-BB~
0.4 o.

Observalion
probabililies

P

-p

B10.81 0.2\
-B 0.60.4
P

Expecled
reward

-BB?j
?4

-p

-BB~
O. 0.

-BBbj
?3

Figure 2: Parameters for marketing example of Sondik (1971,1978) .

E A. Hansen

1020
.

~; " -~ ''''

..

" a=L \
~ 9,96
:
'- 18.86 <,8=-P

'~~9~~_:~~~;~<~._
= S " \\
14.82! \

: ' .1

:
\

18.20 /
\
', __ __ - '~~ P \.,
''',

,.;,\

/""---.. ,9=-p,y:
" a.= S \ .. ,'"

:

14.86

t ____ . .

\_: 8.1~ / 8=P
(.)

(b)

(e)

(d)

(e)

Figure 3: (a) shows the initial finite-state controller, (b) uses dashed circles to show the
vectors in r' generated in the first policy improvement step and (c) shows the transformed
finite-state controller, (d) uses dashed circles to show the vectors in r' generated in the second
policy improvement step and (e) shows the transformed finite-state controller after policy
evaluation. The optimality of this finite-state controller is detected on the third iteration,
which is not shown. Arcs are labeled with one of two possible observations and machine
states are labeled with one of two possible actions and a 2-dimensional vector that contains
a value for each of the two possible system states.

controller shown in Figure 3a. Figure 3 shows how the initial finite-state controller
is transformed into an optimal finite-state controller by the new algorithm. In the
first iteration, the updated set of vectors r' (indicated by dashed circles in Figure 3b)
includes two duplicate vectors and one non-duplicate that results in an added machine
state. Figure 3c shows the improved finite-state controller after the first iteration. In
the second iteration, each of the three vectors in the updated set of vectors r' (indicated by dashed circles in Figure 3d) pointwise dominates a vector that corresponds
to a current machine state. Thus each of these machine states is changed. Figure 4e
shows the improved finite-state controller after the second iteration. The optimality
of this finite-state controller is detected in the third iteration.
This is the only example for which Sondik reports using policy iteration to find an optimal policy. For POMDPs with more than two states, Sondik's algorithm is especially
difficult to implement. Sondik reports that his algorithm finds a suboptimal policy
for an example described in (Smallwood & Sondik, 1973). No further computational
experience with his algorithm has been reported.
The new policy iteration algorithm described in this paper easily finds an optimal
finite-state controller for the example described in (Smallwood & Sondik, 1973) and
has been used to solve many other POMDPs. In fact, it consistently outperforms value
iteration. We compared its performance to the performance of value iteration on a
suite of ten POMDPs that represent a range of problem sizes for which exact dynamicprogramming updates are currently feasible. (Presently, exact dynamic-prorgramming
updates are not feasible for POMDPs with more than about ten or fifteen states,
actions, or observations.) Starting from the same point, we measured how soon each
algorithm converged to f-optimality for f values of 10.0, 1.0, 0.1 , and 0.01. Policy
iteration was consistently faster than value iteration by a factor that ranged from a
low of about 10 times faster to a high of over 120 times faster. On average, its rate
of convergence was between 40 and 50 times faster than value iteration for this set
of examples. The finite-state controllers it found had as many as several hundred
machine states, although optimal finite-state controllers were sometimes found with
just a few machine states.

An Improved Policy Iteration Algorithm for Partially Observable MDPs

5

1021

Discussion

We have demonstrated that the dynamic-programming update for POMDPs can be
interpreted as the improvement of a finite-state controller. This interpretation can
be applied to both value iteration and policy iteration. It provides no computational
speedup for value iteration, but for policy iteration it results in substantial speedup by
making policy evaluation straightforward and easy to implement. This representation
also has the advantage that it makes a policy easier to understand and execute than
representation as a mapping from regions of information space to actions. In particular, a policy can be executed without maintaining an information state at run-time.
It is well-known that policy iteration converges to f-optimality (or optimality) in
fewer iterations than value iteration. For completely observable MDPs, this is not a
clear advantage because the policy evaluation step is more computationally expensive
than the dynamic-programming update. But for POMDPs, policy evaluation has loworder polynomial complexity compared to the worst-case exponential complexity of
the dynamic-programming update (Littman et al., 1995). Therefore, policy iteration
appears to have a clearer advantage over value iteration for POMDPs. Preliminary
testing bears this out and suggests that policy iteration significantly outperforms value
iteration as an approach to solving infinite-horizon POMDPs.
Acknowledgements

Thanks to Shlomo Zilberstein and especially Michael Littman for helpful discussions.
Support for this work was provided in part by the National Science Foundation under
grants IRI-9409827 and IRI-9624992.
References

Blackwell, D. {1965} Discounted dynamic programming. Ann. Math. Stat. 36:226235.
Cassandra, A.; Kaelbling, L.P.; Littman, M.L. {1994} Acting optimally in partially
observable stochastic domains. In Proc. 13th National Conf. on AI, 1023-1028.
Cassandra, A.; Littman, M.L.; & Zhang, N.L. (1997) Incremental pruning: A simple,
fast, exact algorithm for partially observable Markov decision processes. In Proc. 13th
A nnual Con/. on Uncertainty in AI.
Hansen, E.A. (1998). Finite-Memory Control of Partially Observable Systems. PhD
thesis, Department of Computer Science, University of Massachusetts at Amherst.
Jaakkola, T.; Singh, S.P. ; & Jordan, M.I. (1995) Reinforcement learning algorithm for
partially observable Markov decision problems. In NIPS-7.
Littman, M.L.; Cassandra, A.R.; & Kaebling, L.P. (1995) Efficient dynamicprogramming updates in partially observable Markov decision processes. Computer
Science Technical Report CS-95-19, Brown University.
Monahan, G.E. (1982) A survey of partially observable Markov decision processes:
Theory, models, and algorithms. Management Science 28:1-16.
Smallwood, R.D. & Sondik, E.J. (1973) The optimal control of partially observable
Markov processes over a finite horizon. Operations Research 21:1071-1088.
Sondik, E.J. (1971) The Optimal Control of Partially Observable Markov Processes.
PhD thesis, Department of Electrical Engineering, Stanford University.
Sondik, E.J. (1978) The optimal control of partially observable Markov processes over
the infinite horizon: Discounted costs. Operations Research 26:282-304.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 2035-a-bayesian-network-for-real-time-musical-accompaniment.pdf

A Bayesian Network for Real-Time
Musical Accompaniment

Christopher Raphael
Department of Mathematics and Statistics,
University of Massachusetts at Amherst,
Amherst, MA 01003-4515,
raphael~math.umass.edu

Abstract
We describe a computer system that provides a real-time musical accompaniment for a live soloist in a piece of non-improvised
music for soloist and accompaniment. A Bayesian network is developed that represents the joint distribution on the times at which
the solo and accompaniment notes are played, relating the two
parts through a layer of hidden variables. The network is first constructed using the rhythmic information contained in the musical
score. The network is then trained to capture the musical interpretations of the soloist and accompanist in an off-line rehearsal phase.
During live accompaniment the learned distribution of the network
is combined with a real-time analysis of the soloist's acoustic signal, performed with a hidden Markov model, to generate a musically principled accompaniment that respects all available sources
of knowledge. A live demonstration will be provided.

1

Introduction

We discuss our continuing work in developing a computer system that plays the
role of a musical accompanist in a piece of non-improvisatory music for soloist
and accompaniment. The system begins with the musical score to a given piece
of music. Then, using training for the accompaniment part as well as a series of
rehearsals, we learn a performer-specific model for the rhythmic interpretation of the
composition. In performance, the system takes the acoustic signal of the live player
and generates the accompaniment around this signal, in real-time, while respecting
the learned model and the constraints imposed by the score. The accompaniment
played by our system responds both flexibly and expressively to the soloist's musical
interpretation.
Our system is composed of two high level tasks we call "Listen" and "Play." Listen
takes as input the acoustic signal of the soloist and, using a hidden Markov model,
performs a real-time analysis of the signal. The output of Listen is essentially
a running commentary on the acoustic input which identifies note boundaries in
the solo part and communicates these events with variable latency. The HMM
framework is well-suited to the listening task and has several attributes we regard

as indispensable to any workable solution:
1. The HMM allows unsupervised training using the Baum-Welch algorithm.
Thus we can automatically adapt to changes in solo instrument, microphone
placement, ambient noise, room acoustics, and the sound of the accompaniment instrument.

2. Musical accompaniment is inherently a real-time problem. Fast dynamic
programming algorithms provide the computational efficiency necessary to
process the soloist's acoustic signal at a rate consistent with the real-time
demands of our application.
3. Musical signals are occasionally ambiguous locally in time, but become
easier to parse when more context is considered. Our system owes much of
its accuracy to the probabilistic formulation of the HMM. This formulation
allows one to compute the probability that an event is in the past. We delay
the estimation of the precise location of an event until we are reasonably
confident that it is, in fact, past. In this way our system achieves accuracy
while retaining the lowest latency possible in the identification of musical
events.
Our work on the Listen component is documented thoroughly in [1] and we omit a
more detailed discussion here.
The heart of our system, the Play component, develops a Bayesian network consisting of hundreds of Gaussian random variables including both observable quantities,
such as note onset times, and unobservable quantities, such as local tempo. The
network can be trained during a rehearsal phase to model both the soloist's and
accompanist's interpretations of a specific piece of music. This model then forms
the backbone of a principled real-time decision-making engine used in performance.
We focus here on the Play component which is the most challenging part of our
system. A more detailed treatment of various aspects of this work is given in [2- 4].

2

Knowledge Sources

A musical accompaniment requires the synthesis of a number of different knowledge
sources. From a modeling perspective, the fundamental challenge of musical accompaniment is to express these disparate knowledge sources in terms of a common
denominator. We describe here the three knowledge sources we use.
1. We work with non-improvisatory music so naturally the musical score,
which gives the pitches and relative durations of the various notes, as well
as points of synchronization between the soloist and accompaniment, must
figure prominently in our model. The score should not be viewed as a rigid
grid prescribing the precise times at which musical events will occur; rather,
the score gives the basic elastic material which will be stretched in various
ways to to produce the actual performance. The score simply does not
address most interpretive aspects of performance.

2. Since our accompanist must follow the soloist, the output of the Listen
component, which identifies note boundaries in the solo part, constitutes
our second knowledge source. While most musical events, such as changes
between neighboring diatonic pitches, can be detected very shortly after
the change of note, some events, such as rearticulations and octave slurs,
are much less obvious and can only be precisely located with the benefit
of longer term hindsight. With this in mind, we feel that any successful

accompaniment system cannot synchronize in a purely responsive manner.
Rather it must be able to predict the future using the past and base its
synchronization on these predictions, as human musicians do.
3. While the same player's performance of a particular piece will vary from
rendition to rendition, many aspects of musical interpretation are clearly
established with only a few repeated examples. These examples, both of
solo performances and human (MIDI) performances of the accompaniment
part constitute the third knowledge source for our system. The solo data
is used primarily to teach the system how to predict the future evolution
of the solo part. The accompaniment data is used to learn the musicality
necessary to bring the accompaniment to life.
We have developed a probabilistic model, a Bayesian network, that represents all
of these knowledge sources through a jointly Gaussian distribution containing hundreds of random variables. The observable variables in this model are the estimated
soloist note onset times produced by Listen and the directly observable times for
the accompaniment notes. Between these observable variables lies a layer of hidden variables that describe unobservable quantities such as local tempo, change in
tempo, and rhythmic stress.

3

A Model for Rhythmic Interpretation

We begin by describing a model for the sequence of note onset times generated by a
monophonic (single voice) musical instrument playing a known piece of music. For
each of the notes, indexed by n = 0, . . . , N, we define a random vector representing
the time, tn, (in seconds) at which the note begins, and the local "tempo," Sn, (in
secs. per measure) for the note. We model this sequence ofrandom vectors through
a random difference equation:

(1)
n = 0, ... , N - 1, where in is the musical length of the nth note, in measures, and
the {(Tn' CTnY} and (to, so)t are mutually independent Gaussian random vectors.

?

The distributions of the {CT n } will tend concentrate around expressing the notion
that tempo changes are gradual. The means and variances of the {CT n} show where
the soloist is speeding-up (negative mean), slowing-down (positive mean), and tell
us if these tempo changes are nearly deterministic (low variance), or quite variable
(high variance). The {Tn} variables describe stretches (positive mean) or compressions (negative mean) in the music that occur without any actual change in tempo,
as in a tenuto or agogic accent. The addition of the {Tn} variables leads to a more
musically plausible model, since not all variation in note lengths can be explained
through tempo variation. Equally important, however, the {Tn} variables stabilize
the model by not forcing the model to explain, and hence respond to, all note length
variation as tempo variation.
Collectively, the distributions of the (Tn' CTn)t vectors characterize the solo player's
rhythmic interpretation. Both overall tendencies (means) and the repeatability of
these tendencies (covariances) are captured by these distributions.

3.1

Joint Model of Solo and Accompaniment

In modeling the situation of musical accompaniment we begin with the our basic
rhythm model of Eqn. 1, now applied to the composite rhythm. More precisely,

Listen
Update
Composite
Accomp
Figure 1: A graphical description of the dependency structure of our model. The
top layer of the graph corresponds to the solo note onset times detected by Listen.
The 2nd layer of the graph describes the (Tn, 0"n) variables that characterize the
rhythmic interpretation. The 3rd layer of the graph is the time-tempo process
{(Sn, t n )}. The bottom layer is the observed accompaniment event times.
let mo , ... , mivs and mg, ... , m'Na denote the positions, in measures, of the various
solo and accompaniment events. For example, a sequence of quarter notes in 3/ 4
time would lie at measure positions 0, 1/ 3, 2/ 3, etc. We then let mo, ... , mN be
the sorted union of these two sets of positions with duplicate times removed; thus
mo < ml < .. . < mN? We then use the model of Eqn. 1 with In = mn+1 - m n ,
n = 0, . . . , N - 1. A graphical description of this model is given in the middle
two layers of Figure 1. In this figure, the layer labeled "Composite" corresponds
to the time-tempo variables, (tn, sn)t, for the composite rhythm, while the layer
labeled "Update" corresponds to the interpretation variables (Tn, 0"n) t. The directed
arrows of this graph indicate the conditional dependency structure of our model.
Thus, given all variables "upstream" of a variable, x, in the graph, the conditional
distribution of x depends only on the parent variables.
Recall that the Listen component estimates the times at which solo notes begin.
How do these estimates figure into our model? We model the note onset times
estimated by Listen as noisy observations of the true positions {t n }. Thus if m n
is a measure position at which a solo note occurs, then the corresponding estimate
from Listen is modeled as
an = tn + an
2
where an rv N(O, 1I ). Similarly, if m n is the measure position of an accompaniment
event, then we model the observed time at which the event occurs as
bn

= tn + f3n

where f3n rv N(O, ",2). These two collections of observable variables constitute the
top layer of our figure, labeled "Listen," and the bottom layer, labeled "Accomp."
There are, of course, measure positions at which both solo and accompaniment
events should occur. If n indexes such a time then an and bn will both be noisy
observations of the true time tn. The vectors/ variables {(to, so)t, (Tn ' O"n)t, a n , f3n}
are assumed to be mutually independent.

4

Training the Model

Our system learns its rhythmic interpretation by estimating the parameters of the
(Tn,O"n) variables. We begin with a collection of J performances of the accompaniment part played in isolation. We refer to the model learned from this accompaniment data as the "practice room" distribution since it reflects the way the
accompanist plays when the constraint of following the soloist is absent. For each

Listen

Update
Composite
Accomp
Figure 2: Conditioning on the observed accompaniment performance (darkened circles), we use the message passing algorithm to compute the conditional distributions
on the unobservable {Tn' O"n} variables.
such performance, we treat the sequence of times at which accompaniment events
occur as observed variables in our model. These variables are shown with darkened
circles in Figure 2. Given an initial assignment of of means and covariances to the
(Tn , O"n) variables, we use the "message passing" algorithm of Bayesian Networks
[8,9] to compute the conditional distributions (given the observed performance) of
the (Tn,O"n) variables. Several such performances lead to several such estimates,
enabling us to improve our initial estimates by reestimating the (Tn ' O"n) parameters
from these conditional distributions.
More specifically, we estimate the (Tn,O"n) parameters using the EM algorithm, as
follows, as in [7]. We let J-L~, ~~ be our initial mean and covariance matrix for the
vector (Tn, 0"n). The conditional distribution of (Tn, 0"n) given the jth accompaniment performance, and using {J-L~ , ~~} , has a N(m; ,n, S~ ) distribution where the
m;,n and S~ parameters are computed using the message passing algorithm. We
then update our parameter estimates by
1

J

.

} Lmj,n
j=l
~ i+ l

n

The conventional wisdom of musicians is that the accompaniment should follow the
soloist. In past versions of our system we have explicitly modeled the asymmetric
roles of soloist and accompaniment through a rather complicated graph structure
[2- 4] . At present we deal with this asymmetry in a more ad hoc, however, perhaps
more effective, manner , as follows.
Training using the accompaniment performances allows our model to learn some of
the musicality these performances demonstrate. Since the soloist's interpretation
must take precedence, we want to use this accompaniment interpretation only to
the extent that it does not conflict with that of the soloist. We accomplish this
by first beginning with the result of the accompaniment training described above.
We use the practice room distributions , (the distributions on the {(Tn, O"n)} learned
from the accompaniment data) , as the initial distributions , {J-L~ , ~~} . We then run
the EM algorithm as described above now treating the currently available collection
of solo performances as the observed data. During this phase, only those parameters relevant to the soloist's rhythmic interpretation will be modified significantly.
Parameters describing the interpretation of a musical segment in which the soloist
is mostly absent will be largely unaffected by the second training pass.

Listen

Update
Composite
Accomp
Figure 3: At any given point in the performance we will have observed a collection
of solo note times estimated estimated by Listen, and the accompaniment event
times (the darkened circles). We compute the conditional distribution on the next
unplayed accompaniment event, given these observations.
This solo training actually happens over the course of a series of rehearsals. We
first initialize our model to the practice room distribution by training with the
accompaniment data. Then we iterate the process of creating a performance with
our system, (described in the next section), extracting the sequence of solo note
onset times in an off-line estimation process, and then retraining the model using all
currently available solo performances. In our experience, only a few such rehearsals
are necessary to train a system that responds gracefully and anticipates the soloist's
rhythmic nuance where appropriate - generally less than 10.

5

Real Time Accompaniment

The methodological key to our real-time accompaniment algorithm is the computation of (conditional) marginal distributions facilitated by the message-passing machinery of Bayesian networks. At any point during the performance some collection
of solo notes and accompaniment notes will have been observed, as in Fig. 3. Conditioned on this information we can compute the distribution on the next unplayed
accompaniment. The real-time computational requirement is limited by passing
only the messages necessary to compute the marginal distribution on the pending
accompaniment note.
Once the conditional marginal distribution of the pending accompaniment note is
calculated we schedule the note accordingly. Currently we schedule the note to be
played at the conditional mean time, given all observed information, however other
reasonable choices are possible. Note that this conditional distribution depends on
all of the sources of information included in our model: The score information, all
currently observed solo and accompaniment note times, and the rhythmic interpretations demonstrated by both the soloist and accompanist captured during the
training phase.
The initial scheduling of each accompaniment note takes place immediately after
the previous accompaniment note is played. It is possible that a solo note will be
detected before the pending accompaniment is played; in this event the pending
accompaniment event is rescheduled by recomputing the its conditional distribution using the newly available information. The pending accompaniment note is
rescheduled each time an additional solo note is detected until its currently scheduled time arrives, at which time it is finally played. In this way our accompaniment
makes use of all currently available information.
Does our system pass the musical equivalent of the Turing Test? We presume
no more objectivity in answering this question than we would have in judging

the merits of our other children. However, we believe that the level of musicality attained by our system is truly surprising, while the reliability is sufficient for live demonstration. We hope that the interested reader will form
an independent opinion, even if different from ours, and to this end we have
made musical examples demonstrating our progress available on the web page:
http://fafner.math.umass.edu/musicplus_one.
Acknowledgments

This work supported by NSF grants IIS-998789 and IIS-0113496.

References
[1] Raphael C. (1999), "Automatic Segmentation of Acoustic Musical Signals Using Hidden
Markov Models," IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol.
21, No.4, pp. 360-370.
[2] Raphael C. (2001), "A Probabilistic Expert System for Automatic Musical Accompaniment," Journal of Computational and Graphical Statistics, vol. 10 no. 3, 487-512.
[3] Raphael C. (2001), "Can the Computer Learn to Play Expressively?" Proceedings of
Eighth International Workshop on Artificial Intelligence and Statistics, 113-120, Morgan
Kauffman.
[4] Raphael C. (2001), "Synthesizing Musical Accompaniments with Bayesian Belief Networks," Journal of New Music Research, vol. 30, no. 1, 59-67.
[5] Spiegelhalter D., Dawid A. P., Lauritzen S., Cowell R. (1993), "Bayesian Analysis in
Expert Systems," Statistical Science, Vol. 8, No.3, pp. 219-283.
[6] Cowell R., Dawid A. P., Lauritzen S., Spiegelhalter D. (1999), "Probabilistic Networks
and Expert Systems," Springer, New York.
[7] Lauritzen S. L. (1995), "The EM Algorithm for Graphical Association Models with
Missing Data," Computational Statistics and Data Analysis, Vol. 19, pp. 191-20l.
[8] Lauritzen S. L. (1992), "Propagation of Probabilities, Means, and Variances in Mixed
Graphical Association Models," Journal of the American Statistical Association, Vol. 87,
No. 420, (Theory and Methods), pp. 1098-1108.
[9] Lauritzen S. L. and F. Jensen (1999), "Stable Local Computation with Conditional
Gaussian Distributions," Technical Report R-99-2014, Department of Mathematic Sciences, Aalborg University.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

