query sentence: neural network computers
---------------------------------------------------------------------
title: 232-analog-neural-networks-of-limited-precision-i-computing-with-multilinear-threshold-functions.pdf

702

Obradovic and Pclrberry

Analog Neural Networks of Limited Precision I:
Computing with Multilinear Threshold Functions
(Preliminary Version)

Zoran Obradovic and Ian Parberry
Department of Computer Science.
Penn State University.
University Park. Pa. 16802.

ABSTRACT
Experimental evidence has shown analog neural networks to be ex~mely fault-tolerant; in particular. their performance does not appear to be significantly impaired when precision is limited. Analog
neurons with limited precision essentially compute k-ary weighted
multilinear threshold functions. which divide R" into k regions with
k-l hyperplanes. The behaviour of k-ary neural networks is investigated. There is no canonical set of threshold values for k>3.
although they exist for binary and ternary neural networks. The
weights can be made integers of only 0 ?z +k ) log (z +k ? bits. where
z is the number of processors. without increasing hardware or running time. The weights can be made ?1 while increasing running
time by a constant multiple and hardware by a small polynomial in z
and k. Binary neurons can be used if the running time is allowed to
increase by a larger constant multiple and the hardware is allowed to
increase by a slightly larger polynomial in z and k. Any symmetric
k-ary function can be computed in constant depth and size
(n k - 1/(k-2)!). and any k-ary function can be computed in constant
depth and size 0 (nk"). The alternating neural networks of Olafsson
and Abu-Mostafa. and the quantized neural networks of Fleisher are
closely related to this model.

o

Analog Neural Networks of Limited Precision I

1 INTRODUCTION
Neural networks are typically circuits constructed from processing units which compute simple functions of the form f(Wl> ... ,wlI):RII-+S where SeR, wieR for 1~~,
and
II

f (Wl> ... ,WII)(Xl, .?. ,xlI)=g (LWi X;)
i=1

for some output function g :R-+S. There are two choices for the set S which are
currently popular in the literature. The first is the discrete model, with S=B (where B
denotes the Boolean set (0,1)). In this case, g is typically a linear threshold function
g (x)= 1 iff x~. and f is called a weighted linear threshold function. The second is
the analog model, with S=[O,I] (where [0,1] denotes (re RI~~I}). In this case. g
is typically a monotone increasing function, such as the sigmoid function
g (x)=(1 +c -% 1 for some constant c e R. The analog neural network model is popular
because it is easy to construct processors with the required characteristics using a few
transistors. The digital model is popular because its behaviour is easy to analyze.

r

Experimental evidence indicates that analog neural networks can produce accurate
computations when the precision of their components is limited. Consider what actually happens to the analog model when the precision is limited. Suppose the neurons
can take on k distinct excitation values (for example, by restricting the number of digits in their binary or decimal expansions). Then S is isomorphic to Zk={O, ... ,k-l}.
We will show that g is essentially the multilinear threshold function
g (hloh2 ....,hk-l):R-+Zk defined by

Here and throughout this paper, we will assume that hl~h2~ ... ~hk-1> and for convenience define ho=-oo and h/c=oo. We will call f a k-ary weighted multilinear threshold
function when g is a multilinear threshold function.
We will study neural networks constructed from k-ary multilinear threshold functions.
We will call these k-ary neural networks, in order to distinguish them from the standard 2-ary or binary neural network. We are particularly concerned with the resources
of time, size (number of processors), and weight (sum of all the weights) of k-ary
neural networks when used in accordance with the classical computational paradigm.
The reader is referred to (parberry, 1990) for similar results on binary neural networks.
A companion paper (Obradovic & Parberry, 1989b) deals with learning on k-ary neural networks. A more detailed version of this paper appears in (Obradovic & Parberry,
1989a).

2 A K-ARY NEURAL NETWORK MODEL
A k-ary neural network is a weighted graph M =(V ,E ,W ,h), where V is a set of processors and E cVxV is a set of connections between processors. Function
w:VxV -+R assign weights to interconnections and h:V -+Rk - assign a set of k-l
thresholds to each of the processors. We assume that if (u ,v) eE, W (u ,v )=0. The
size of M is defined to be the number of processors, and the weight of M is

703

704

Obradovic and Parberry

The processors of a k-ary neural network are relatively limited in computing power.
A k-ary function is a function f :Z:~Z". Let F; denote the set of all n-input k-ary
functions. Define e::R,,+Ir;-l~F; by e:(w l .....w".h It .???h''_l):R;~Z,,. where

.

e;(w It ???? w" .h h???.h,,-l)(X 1o... ,%.. )=i iff hi ~~Wi xi <h; +1?
i=1

The set of k-ary weighted multilinear threshold functions is the union. over all n e N.
of the range of e;. Each processor of a k-ary neural network can compute a k-ary
weighted multilinear threshold function of its inputs.
Each processor can be in one of k states, 0 through k-l. Initially. the input processors of M are placed into states which encode the input If processor v was updated
during interval t, its state at time t -1 was i and output was j. then at time t its state
will be j. A k-ary neural network computes by having the processors change state until a stable configuration is reached. The output of M are the states of the output processors after a stable state has been reached. A neural network M 2 is said to be f (t )equivalent to M 1 iff for all inputs x. for every computation of M 1 on input x which
terminates in time t there is a computation of M 2 on input x which terminates in time
f (t) with the same output. A neural network M 2 is said to be equivalent to M 1 iff it
is t -equivalent to it.

3 ANALOG NEURAL NETWORKS
Let f be a function with range [0.1]. Any limited-precision device which purports to
compute f must actually compute some function with range the k rational values

R"={ilk-llieZ,,,~<k} (for some keN). This is sufficient for all practical purposes
provided k is large enough. Since R" is isomorphic to Z". we will formally define
the limited precision variant of f to be the function f" :X ~Z" defined by
f,,(x)=round(j (x).(k-l?, where round:R~N is the natural rounding function defined
by round(x)=n iff n-o.5~<n-tO.5.

Theorem 3.1 : Letf(Wlo ... ,w.. ):R"~[O,I] where WieR for

1~~.

be defined by

.
f (w1O.?.,W,,)(X 10 .?? ,x.. )=g (LWiXi)
i=l

where g:R~[O,I] is monotone increasing and invertible. Then f(Wlo ... ,W.. )":R"~Z,,
is a k-ary weighted multilinear threshold function.
Proof: It is easy to verify that f(Wlo ...?W")"=S;(Wl' ... ,w",hl, ...?h,,_l)' where
hi =g-1?2i-l)/2(k-l?. 0
Thus we see that analog neural networks with limited precision are essentially k-ary
neural networks.

Analog Neural Networks of Limited Precision I

4 CANONICAL THRESHOLDS
Binary neural networks have the advantage that all thresholds can be taken equal to
zero (see. for example. Theorem 4.3.1 of Parberry, 1990). A similar result holds for
ternary neural networks.
Theorem 4.1 : For every n-input ternary weighted multilinear threshold function there
is an equivalent (n +I)-input ternary weighted multilinear threshold function with
threshold values equal to zero and one.
Proof: Suppose W=(W1o ??? ,WII )E R", hloh2E R. Without loss of generality assume
h l<h 2.
Define W=(Wl ?...?wlI+l)e RII+I by wj=wjl(hrh 1) for I~!0t, and
wlI +I=-h I/(h2-h 1). It can be demonstrated by a simple case analysis that for all
x =(x 1, ??? ,xll)e

Z;.

8;(w,h l,hz)(x )=8;+I(W ,0,I)(x l,... ,xll ,1).

o
The choice of threshold values in Theorem 4.1 was arbitrary. Unfortunately there is
no canonical set of thresholds for k >3.
Theorem 4.2 : For every k>3, n~2, m~. h 1o ??? ,hk - 1E R. there exists an n-input k-ary
weighted multilinear threshold function

such that for all (n +m )-input k-ary weighted multilinear threshold functions

8 k"+m("
WI.???

)?zm+1I
.WII+m. h 10???. hk-l'
k
~Z k
A

Proof (Sketch): Suppose that t I ?.. . .tk-l e R is a canonical set of thresholds. and w.t.o.g.
assume n =2. Let h =(h 1o ??? ,hk - 1), where h l=h z=2. h j=4, hi =5 for 4Si <k. and
f=8i(1,I.h).
By hypothesis there exist wlo ????wm+2 and y=(ylo ...?ym)eRm such that for all xeZi,

f (x )=8r+2(w 1.? .. ,Wm+2,t 1, ??? ,tk-l)(X ,y).
m

Let S= I:Wi+2Yi. Since f (1.0)=0. f (0.1)=0, f (2,1)=2, f (1,2)=2. it follows that
;=1

2(Wl+Wz+S )<tl+t 3.
Since f (2,0)=2, f (1.1 )=2. and f (0.2)=2, it follows that

(1)

70S

706

Obradovic and Pdrberry

Wl+W2+S~2?

(2)

2t2<ll+13.

(3)

Inequalities (1) and (2) imply that

By similar arguments from g=S;(1,l,l.3.3.4 ?...?4) we can conclude that
(4)

But (4) contradicts (3). 0

S NETWORKS OF BOUNDED WEIGHT
Although our model allows each weight to take on an infinite number of possible
values. there are only a finite number of threshold functions (since there are only a
finite number of k-ary functions) with a fixed number of inputs. Thus the number of
n -input threshold functions is bounded above by some function in n and k. In fact.
something stronger can be shown. All weights can be made integral. and
o ((n +k) log (n +k? bits are sufficient to describe each one.
Theorem 5.1 : For every k-ary neural network M 1 of size z there exists an equivalent
k-ary neural network M2 of size z and weight ((k_l)/2)Z(z+I)(z+k)'2+0(1) with integer
weights.
Proof (Sketch): It is sufficient to prove that for every weighted threshold function
f:(Wlt ...?wll.hh ...?h"-I):Z:~Z,, for some neN. there is an equivalent we1f.hted threshold function g:(w~ ?...? w:.hi ?...? h;-d such that Iwtl~((k-l)/2)I(n+l)'" )12+0(1) for
l~i~. By extending the techniques used by Muroga. Toda and Takasu (1961) in the
binary case. we see that the weights are bounded above by the maximum determinant
of a matrix of dimension n +k -lover Z". 0
Thus if k is bounded above by a polynomial in n. we are guaranteed of being able to
describe the weights using a polynomial number of bits.

6 THRESHOLD CIRCUITS
A k-ary neural network with weights drawn from {?1} is said to have unit weights. A
unit-weight directed acyclic k-ary neural network is called a k-ary threshold circuit.
A k-ary threshold circuit can be divided into layers. with each layer receiving inputs
only from the layers above it. The depth of a k-ary threshold circuit is defined to be
the number of layers. The weight is equal to the number of edges. which is bounded
above by the square of the size. Despite the apparent handicap of limited weights. kary threshold circuits are surprisingly powerful.
Much interest has focussed on the computation of symmetric functions by neural networks. motivated by the fact that the visual system appears to be able to recognize objects regardless of their position on the retina A function f :Z:~Z" is called symmetric if its output remains the same no matter how the input is permuted.

Analog Neural Networks of Limited Precision I

Theorem 6.1 : Any symmetric k-ary function on n inputs can be computed by a k-ary
threshold circuit of depth 6 and size (n+1)k-l/(k-2)!+ o (kn).
Proof: Omitted. 0
It has been noted many times that neural networks can compute any Boolean function
in constant depth. The same is true of k-ary neural networks, although both results
appear to require exponential size for many interesting functions.

Theorem 6.2 : Any k-ary function of n inputs can be computed by a k-ary threshold
circuit with size (2n+1)k"+k+1 and depth 4.
Proof: Similar to that for k=2 (see Chandra et. al., 1984; Parberry, 1990). 0
The interesting problem remaining is to determine which functions require exponential
size to achieve constant depth, and which can be computed in polynomial size and
constant depth. We will now consider the problem of adding integers represented in
k-ary notation.

Theorem 6.3 : The sum of two k-ary integers of size n can be computed by a k-ary
threshold circuit with size 0 (n 2) and depth 5.
Proof: First compute the carry of x and y in 'luadratic size and depth 3 using the standard elementary school algorithm. Then the it position of the result can be computed
from the i tit position of the operands and a carry propagated in that position in constant size and depth 2. 0
Theorem 6.4 : The sum of n k-~ integers of size n can be computed by a k-ary
threshold circuit with size 0 (n 3+kn ) and constant depth.
Proof: Similar to the proof for k=2 using Theorem 6.3 (see Chandra et. al., 1984; Parberry, 1990). 0
Theorem 6.S : For every k-ary neural network M 1 of size z there exists an 0 (t)equivalent unit-weight k-ary neural network M2 of size o ((z+k)410g3(z+k?.
Proof: By Theorem 5.1 we can bound all weights to have size 0 ((z+k)log(z+k? in
binary notation. By Theorem 6.4 we can replace every processor with non-unit
weights by a threshold circuit of size o ((z+k)310g3(z+k? and constant depth. 0
Theorem 6.5 implies that we can assume unit weights by increasing the size by a polynomial and the running time by only a constant multiple provided the number of
logic levels is bounded above by a polynomial in the size of the network. The
number of thresholds can also be reduced to one if the size is increased by a larger
polynomial:

Theorem 6.6 : For every k-ary neural network M 1 of size z there exists an 0 (t )equivalent unit-weight binary neural network M 2 of size 0 (z 4k 4)(log z + log k)3
which outputs the binary encoding of the required result
Proof: Similar to the proof of Theorem 6.5. 0
This result is primarily of theoretical interest. Binary neural networks appear simpler,
and hence more desirable than analog neural networks. However, analog neural networks are actually more desirable since they are easier to build. With this in mind,
Theorem 6.6 simply serves as a limit to the functions that an analog neural network

707

708

Obradovic and Parberry

can be expected to compute efficiently. We are more concerned with constructing a
model of the computational abilities of neural networks, rather than a model of their
implementation details.

7 NONMONOTONE MULTILINEAR NEURAL NETWORKS
Olafsson and Abu-Mostafa (1988) study
f(Wlt ... ,wl):R"-+B for w;ER, 1~~, where

f

information

capacity

of functions

II
(Wlt.. ??WII)(X1 ?... , xlI)=g (~W;X;)
;=1

and g is the alternating threshold function g (h loh2.....hk-1):R-+B for some monotone
increasing h;ER, 1~<k, defined by g(x)=O if h2i~<h2i+1 for some ~5:nI2. We
will call f an alternating weighted multilinear threshold function, and a neural network constructed from functions of this form alternating multilinear neural networks.
Alternating multilinear neural networks are closely related to k-ary neural networks:
Theorem 7.1 : For every k-ary neural network of size z and weight w there is an
equivalent alternating multilinear neural network of size z log k and weight
(k -l)w log (k -1) which produces the output of the former in binary notation.
Proof (Sketch): Each k-ary gate is replaced by log k gates which together essentially
perform a "binary search" to determine each bit of the k-ary gate. Weights which increase exponentially are used to provide the correct output value. 0
Theorem 7.2 : For every alternating multilinear neural network of size z and weight
w there is a 3t-equivalent k-ary neural network of size 4z and weight w+4z.
Proof (Sketch): Without loss of generality. assume k is odd. Each alternating gate is
replaced by a k-ary gate with identical weights and thresholds. The output of this gate
goes with weight one to a k-ary gate with thresholds 1,3,S ?... ,k-1 and with weight
minus one to a k-ary gate with thresholds -(k-1), ... ,-3,-1. The output of these gates
goes to a binary gate with threshold k. 0
Both k-ary and alternating multilinear neural networks are a special case of nonmonotone multilinear neural networks, where g :R-+R is the defined by g (x )=Ci iff
hi~<h;+lt for some monotone increasing h;ER, 1~<k, and co, ... ,Ck-1EZk. Nonmonotone neural networks correspond to analog neural networks whose output function is not necessarily monotone nondecreasing. Many of the result of this paper, including Theorems 5.1, 6.5, and 6.6, also apply to nonmonotone neural networks. The
size, weight and running time of many of the upper-bounds can also be improved by a
small amount by using nonmonotone neural networks instead of k-ary ones. The details are left to the interested reader.

8 MUL TILINEAR HOPFIELD NETWORKS
A multilinear version of the Hopfield network called the quantized neural network has
been studied by Fleisher (1987). Using the terminology of (parberry, 1990), a quantized neural network is a simple symmetric k-ary neural network (that is, its interconnection pattern is an undirected graph without self-loops) with the additional property
that all processors have an identical set of thresholds. Although the latter assumption

Analog Neural Networks of Limited Precision I

is reasonable for binary neural networks (see, for example, Theorem 4.3.1 of Parberry,
1990), and ternary neural networks (Theorem 4.1), it is not necessarily so for k-ary
neural networks with k>3 (Theorem 4.2). However, it is easy to extend Fleisher's
main result to give the following:
Theorem 8.1 : Any productive sequential computation of a simple symmetric k-ary
neural network will converge.

9 CONCLUSION
It has been shown that analog neural networks with limited precision are essentially
k-ary neural networks. If k is limited to a polynomial, then polynomial size, constant
depth k-ary neural networks are equivalent to polynomial size, constant depth binary
neural networks. Nonetheless, the savings in time (at most a constant multiple) and
hardware (at most a polynomial) arising from using k-ary neural networks rather than
binary ones can be quite significant. We do not suggest that one should actually construct binary or k-ary neural networks. Analog neural networks can be constructed by
exploiting the analog behaviour of transistors, rather than using extra hardware to inhibit it Rather, we suggest that k-ary neural networks are a tool for reasoning about the
behaviour of analog neural networks.
Acknowledgements
The financial support of the Air Force Office of Scientific Research, Air Force S ysterns Command, DSAF, under grant numbers AFOSR 87-0400 and AFOSR 89-0168
and NSF grant CCR-8801659 to Ian Parberry is gratefully acknowledged.
References
Chandra A. K., Stockmeyer L. J. and Vishkin D., (1984) "Constant depth reducibility,"
SIAM 1. Comput., vol. 13, no. 2, pp. 423-439.
Fleisher M., (1987) "The Hopfield model with multi-level neurons," Proc. IEEE
Conference on Neural Information Processing Systems, pp. 278-289, Denver, CO.
Muroga S., Toda 1. and Takasu S., (1961) "Theory of majority decision elements," 1.
Franklin Inst., vol. 271., pp. 376-418.
Obradovic Z. and Parberry 1., (1989a) "Analog neural networks of limited precision I:
Computing with multilinear threshold functions (preliminary version)," Technical Report CS-89-14, Dept of Computer Science, Penn. State Dniv.
Obradovic Z. and Parberry I., (1989b) "Analog neural networks of limited precision II:
Learning with multilinear threshold functions (preliminary version)," Technical Report
CS-89-15, Dept. of Computer Science, Penn. State Dniv.
Olafsson S. and Abu-Mostafa Y. S., (1988) "The capacity of multilevel threshold functions," IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 10, no. 2, pp.
277-281.
Parberry I., (To Appear in 1990) "A Primer on the Complexity Theory of Neural Networks," in A Sourcebook of Formal Methods in Artificial Intelligence, ed. R. Banerji,
North-Holland.

709


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 2111-computing-time-lower-bounds-for-recurrent-sigmoidal-neural-networks.pdf

Computing Time Lower Bounds for
Recurrent Sigmoidal Neural Networks

Michael Schmitt
Lehrstuhl Mathematik und Informatik, Fakultat fUr Mathematik
Ruhr-Universitat Bochum, D- 44780 Bochum, Germany
mschmitt@lmi.ruhr-uni-bochum.de

Abstract
Recurrent neural networks of analog units are computers for realvalued functions. We study the time complexity of real computation in general recurrent neural networks. These have sigmoidal,
linear, and product units of unlimited order as nodes and no restrictions on the weights. For networks operating in discrete time,
we exhibit a family of functions with arbitrarily high complexity,
and we derive almost tight bounds on the time required to compute
these functions. Thus, evidence is given of the computational limitations that time-bounded analog recurrent neural networks are
subject to.

1

Introduction

Analog recurrent neural networks are known to have computational capabilities that
exceed those of classical Turing machines (see, e.g., Siegelmann and Sontag, 1995;
Kilian and Siegelmann, 1996; Siegelmann, 1999). Very little, however, is known
about their limitations. Among the rare results in this direction, for instance,
is the one of Sima and Orponen (2001) showing that continuous-time Hopfield
networks may require exponential time before converging to a stable state. This
bound, however, is expressed in terms of the size of the network and, hence, does
not apply to fixed-size networks with a given number of nodes. Other bounds
on the computational power of analog recurrent networks have been established by
Maass and Orponen (1998) and Maass and Sontag (1999). They show that discretetime recurrent neural networks recognize only a subset of the regular languages in
the presence of noise. This model of computation in recurrent networks, however,
receives its inputs as sequences. Therefore, computing time is not an issue since
the network halts when the input sequence terminates. Analog recurrent neural
networks, however, can also be run as "real" computers that get as input a vector
of real numbers and, after computing for a while, yield a real output value. No
results are available thus far regarding the time complexity of analog recurrent
neural networks with given size.
We investigate here the time complexity of discrete-time recurrent neural networks
that compute functions over the reals. As network nodes we allow sigmoidal units,
linear units, and product units- that is, monomials where the exponents are ad-

justable weights (Durbin and Rumelhart, 1989) . We study the complexity of real
computation in the sense of Blum et aI. (1998). That means, we consider real numbers as entities that are represented exactly and processed without restricting their
precision. Moreover, we do not assume that the information content of the network
weights is bounded (as done, e.g., in the works of Balcazar et aI. , 1997; Gavalda and
Siegelmann, 1999). With such a general type of network, the question arises which
functions can be computed with a given number of nodes and a limited amount of
time. In the following , we exhibit a family of real-valued functions ft, l 2: 1, in one
variable that is computed by some fixed size network in time O(l). Our main result
is, then, showing that every recurrent neural network computing the functions ft
requires at least time nW /4). Thus, we obtain almost tight time bounds for real
computation in recurrent neural networks.

2

Analog Computation in Recurrent Neural Networks

We study a very comprehensive type of discrete-time recurrent neural network that
we call general recurrent neural network (see Figure 1). For every k, n E N there is
a recurrent neural architecture consisting of k computation nodes YI , . . . , Yk and n
input nodes Xl , ... , x n . The size of a network is defined to be the number ofits computation nodes. The computation nodes form a fully connected recurrent network.
Every computation node also receives connections from every input node. The input
nodes play the role of the input variables of the system. All connections are parameterized by real-valued adjustable weights. There are three types of computation
nodes: product units, sigmoidal units, and linear units. Assume that computation
node i has connections from computation nodes weighted by Wil, ... ,Wi k and from
input nodes weighted by ViI, .. . ,Vi n. Let YI (t) , . . . ,Yk (t) and Xl (t), ... ,X n (t) be the
values of the computation nodes and input nodes at time t, respectively. If node i
is a product unit, it computes at time t + 1 the value

(1)
that is, after weighting them exponentially, the incoming values are multiplied.
Sigmoidal and linear units have an additional parameter associated with them, the
threshold or bias ()i . A sigmoidal unit computes the value

where (J is the standard sigmoid (J( z ) = 1/ (1
simply outputs the weighted sum

+ e- Z ).

If node i is a linear unit, it

We allow the networks to be heterogeneous, that is, they may contain all three types
of computation nodes simultaneously. Thus, this model encompasses a wide class of
network types considered in research and applications. For instance, architectures
have been proposed that include a second layer of linear computation nodes which
have no recurrent connections to computation nodes but serve as output nodes (see,
e.g. , Koiran and Sontag, 1998; Haykin, 1999; Siegelmann, 1999). It is clear that in
the definition given here, the linear units can function as these output nodes if the
weights of the outgoing connections are set to O. Also very common is the use
of sigmoidal units with higher-order as computation nodes in recurrent networks
(see, e.g., Omlin and Giles, 1996; Gavalda and Siegelmann, 1999; Carrasco et aI.,
2000). Obviously, the model here includes these higher-order networks as a special
case since the computation of a higher-order sigmoidal unit can be simulated by
first computing the higher-order terms using product units and then passing their

.

I

I

sigmoidal, product, and linear units

computation
nodes

.

Yl

Yk

t
input nodes

Xl

Xn

I

Figure 1: A general recurrent neural network of size k. Any computation node may
serve as output node.

outputs to a sigmoidal unit. Product units , however, are even more powerful than
higher-order terms since they allow to perform division operations using negative
weights. Moreover, if a negative input value is weighted by a non-integer weight,
the output of a product unit may be a complex number. We shall ensure here that
all computations are real-valued. Since we are mainly interested in lower bounds,
however, these bounds obviously remain valid if the computations of the networks
are extended to the complex domain.
We now define what it means that a recurrent neural network N computes a function
f : ~n --+ llt Assume that N has n input nodes and let x E ~n. Given tE N,
we say that N computes f(x) in t steps if after initializing at time 0 the input
nodes with x and the computation nodes with some fixed values, and performing t
computation steps as defined in Equations (1) , (2) , and (3) , one of the computation
nodes yields the value f(x). We assume that the input nodes remain unchanged
during the computation. We further say that N computes f in time t if for every
x E ~n , network N computes f in at most t steps. Note that t may depend
on f but must be independent of the input vector. We emphasize that this is
a very general definition of analog computation in recurrent neural networks. In
particular, we do not specify any definite output node but allow the output to occur
at any node. Moreover, it is not even required that the network reaches a stable
state, as with attractor or Hopfield networks. It is sufficient that the output value
appears at some point of the trajectory the network performs. A similar view of
computation in recurrent networks is captured in a model proposed by Maass et al.
(2001). Clearly, the lower bounds remain valid for more restrictive definitions of
analog computation that require output nodes or stable states. Moreover, they
hold for architectures that have no input nodes but receive their inputs as initial
values of the computation nodes. Thus, the bounds serve as lower bounds also for
the transition times between real-valued states of discrete-time dynamical systems
comprising the networks considered here.
Our main tool of investigation is the Vapnik-Chervonenkis dimension of neural
networks. It is defined as follows (see also Anthony and Bartlett, 1999): A dichotomy
of a set S ~ ~n is a partition of S into two disjoint subsets (So , Sd satisfying
So U S1 = S. A class :F of functions mapping ~n to {O, I} is said to shatter S if
for every dichotomy (So , Sd of S there is some f E :F that satisfies f(So) ~ {O}
and f(S1) ~ {I}. The Vapnik-Chervonenkis (VC) dimension of :F is defined as

4"'+4",IL
'I

-1---Y-2----Y-5~1

S~

output

Y5

Y4
Figure 2: A recurrent neural network computing the functions fl in time 2l

+ 1.

the largest number m such that there is a set of m elements shattered by F. A
neural network given in terms of an architecture represents a class of functions
obtained by assigning real numbers to all its adjustable parameters, that is, weights
and thresholds or a subset thereof. The output of the network is assumed to be
thresholded at some fixed constant so that the output values are binary. The VC
dimension of a neural network is then defined as the VC dimension of the class of
functions computed by this network.

In deriving lower bounds in the next section, we make use of the following result
on networks with product and sigmoidal units that has been previously established
(Schmitt, 2002). We emphasize that the only constraint on the parameters of the
product units is that they yield real-valued, that is, not complex-valued, functions.
This means further that the statement holds for networks of arbitrary order, that is,
it does not impose any restrictions on the magnitude of the weights of the product
units.
Proposition 1. (Schmitt, 2002, Theorem 2) Suppose N is a feedforward neural
network consisting of sigmoidal, product, and linear units. Let k be its size and W
the number of adjustable weights. The VC dimension of N restricted to real-valued
functions is at most 4(Wk)2 + 20Wk log(36Wk).

3

Bounds on Computing Time

We establish bounds on the time required by recurrent neural networks for computing a family of functions fl : JR -+ JR, l 2:: 1, where l can be considered as a measure
of the complexity of fl. Specifically, fl is defined in terms of a dynamical system as
the lth iterate of the logistic map ?>(x) = 4x(1 - x), that is,
fl(X)

{

= 1,

?>(x)

l

?>(fl- l (x))

l > 2.

We observe that there is a single recurrent network capable of computing every fl
in time O(l).
Lemma 2. There is a general recurrent neural network that computes fl in time
2l + 1 for every l.
Proof. The network is shown in Figure 2. It consists of linear and second-order
units. All computation nodes are initialized with 0, except Yl, which starts with 1
and outputs 0 during all following steps. The purpose of Yl is to let the input x

output

Figure 3: Network Nt.

enter node Y2 at time 1 and keep it away at later times. Clearly, the value fl (x)
results at node Y5 after 2l + 1 steps.
D
The network used for computing fl requires only linear and second-order units. The
following result shows that the established upper bound is asymptotically almost
tight, with a gap only of order four . Moreover, the lower bound holds for networks
of unrestricted order and with sigmoidal units.
Theorem 3. Every general recurrent neural network of size k requires at least time
cl l / 4 j k to compute function fl' where c> 0 is some constant.

Proof. The idea is to construct higher-order networks Nt of small size that have
comparatively large VC dimension. Such a network will consist of linear and product
units and hypothetical units that compute functions fJ for certain values of j. We
shall derive a lower bound on the VC dimension of these networks. Assuming that
the hypothetical units can be replaced by time-bounded general recurrent networks,
we determine an upper bound on the VC dimension of the resulting networks in
terms of size and computing time using an idea from Koiran and Sontag (1998) and
Proposition 1. The comparison of the lower and upper VC dimension bounds will
give an estimate of the time required for computing k
Network Nt, shown in Figure 3, is a feedforward network composed of three networks
? r(1) , JVI
? r(2) , JVI
.r(3) . E ach networ k JVI
? r(/1) ,J.L = 1, 2, 3 , h as l ?lnput no d es Xl'
(/1) .. . , x I(/1)
JVI
and 2l + 2 computation nodes yb/1), ... , Y~r~l (see Figure 4). There is only one
adjustable parameter in Nt, denoted w, all other weights are fixed. The computation
nodes are defined as follows (omitting time parameter t):
for J.L

= 3,

for J.L = 1,2,

y~/1)

fll'--1 (Y~~)l) for i = 1, ... ,l and J.L = 1,2,3,

y}~{

y~/1) . x~/1), for i = 1, .. . ,l and

(/1)
Y21+l

(/1)
YIH

+ ... + Y21(/1)

J.L = 1,2,3,

c
- 1 2 3
lor
J.L , , ?

The nodes Yb/1) can be considered as additional input nodes for N//1), where N;(3)
gets this input from w, and N;(/1) from N;(/1+l) for J.L = 1,2. Node Y~r~l is the
output node of N;(/1), and node Y~~~l is also the output node of Nt. Thus, the entire
network has 3l + 6 nodes that are linear or product units and 3l nodes that compute
functions h, fl' or f12.

output

8

r - - - - - - - - - - - - '.....L - - - - - - - - - - - ,

I

I

B

B

t

t

I x~p)1

~

----t
input:

w

or

output of N;(P+1)

Figure 4: Network N;(p).

We show that Ni shatters some set of cardinality [3, in particular, the set S = ({ ei :
i = 1, . .. , [})3, where ei E {O, 1}1 is the unit vector with a 1 in position i and
elsewhere. Every dichotomy of S can be programmed into the network parameter
w using the following fact about the logistic function ? (see Koiran and Sontag,
1998, Lemma 2): For every binary vector b E {O, l}m, b = b1 .?. bm , there is some
real number w E [0,1] such that for i = 1, ... , m

?

E

{

[0,1 /2)

if bi = 0,

(1/2,1]

if bi = 1.

Hence, for every dichotomy (So, Sd of S the parameter w can be chosen such that
every (ei1' ei2 , ei3) E S satisfies
1/2 if (eillei2,eis) E So,
1/2 if (eillei2,eiJ E S1.
Since h +i2 H i 3 .12 (w) = ?i1 (?i2'1 (?i3 .1 2(w))), this is the value computed by Ni on
input (eill ei2' ei3), where ei" is the input given to network N;(p). (Input ei" selects
the function li"'I,,-1 in N;(p).) Hence, S is shattered by Ni, implying that Ni has
VC dimension at least [3.

Assume now that Ii can be computed by a general recurrent neural network of size
at most kj in time tj. Using an idea of Koiran and Sontag (1998), we unfold the
network to obtain a feedforward network of size at most kjtj computing fj. Thus we
can replace the nodes computing ft, ft, fl2 in Nz by networks of size k1t1, kltl, k12t12,
respectively, such that we have a feedforward network
consisting of sigmoidal,
product, and linear units. Since there are 3l units in Nl computing ft, ft, or fl2
and at most 3l + 6 product and linear units, the size of Nt is at most c1lkl2tl2
for some constant C1 > O. Using that Nt has one adjustable weight, we get from
Proposition 1 that its VC dimension is at most c2l2kr2tr2 for some constant C2 > o.
On the other hand, since Nz and Nt both shatter S, the VC dimension of Nt is at
least l3. Hence, l3 ~ C2l2 kr2 tr2 holds, which implies that tl2 2: cl 1/ 2/ kl2 for some
c > 0, and hence tl 2: cl 1/ 4/ kl .
D

'!J

Lemma 2 shows that a single recurrent network is capable of computing every
function fl in time O(l). The following consequence of Theorem 3 establishes that
this bound cannot be much improved.
Corollary 4. Every general recurrent neural network requires at least time 0(ll /4 )

to compute the functions fl.

4

Conclusions and Perspectives

We have established bounds on the computing time of analog recurrent neural
networks. The result shows that for every network of given size there are functions
of arbitrarily high time complexity. This fact does not rely on a bound on the
magnitude of weights. We have derived upper and lower bounds that are rather
tight- with a polynomial gap of order four- and hold for the computation of a
specific family of real-valued functions in one variable. Interestingly, the upper
bound is shown using second-order networks without sigmoidal units, whereas the
lower bound is valid even for networks with sigmoidal units and arbitrary product
units. This indicates that adding these units might decrease the computing time
only marginally. The derivation made use of an upper bound on the VC dimension
of higher-order sigmoidal networks. This bound is not known to be optimal. Any
future improvement will therefore lead to a better lower bound on the computing
time.
We have focussed on product and sigmoidal units as nonlinear computing elements.
However, the construction presented here is generic. Thus, it is possible to derive
similar results for radial basis function units, models of spiking neurons, and other
unit types that are known to yield networks with bounded VC dimension. The
questions whether such results can be obtained for continuous-time networks and for
networks operating in the domain of complex numbers, are challenging. A further
assumption made here is that the networks compute the functions exactly. By a
more detailed analysis and using the fact that the shattering of sets requires the
outputs only to lie below or above some threshold, similar results can be obtained
for networks that approximate the functions more or less closely and for networks
that are subject to noise.
Acknowledgment

The author gratefully acknowledges funding from the Deutsche Forschungsgemeinschaft (DFG). This work was also supported in part by the ESPRIT Working Group
in Neural and Computational Learning II, NeuroCOLT2, No. 27150.

References
Anthony, M. and Bartlett, P. L. (1999). Neural Network Learning: Theoretical
Foundations. Cambridge University Press, Cambridge.
Balcazar, J. , Gavalda, R., and Siegelmann, H. T. (1997). Computational power of
neural networks: A characterization in terms of Kolmogorov complexity. IEEE
Transcations on Information Theory, 43: 1175- 1183.
Blum, L., Cucker, F. , Shub, M. , and Smale, S. (1998) . Complexity and Real Computation. Springer-Verlag, New York.
Carrasco, R. C., Forcada, M. L., Valdes-Munoz, M. A. , and Neco, R. P. (2000).
Stable encoding of finite state machines in discrete-time recurrent neural nets
with sigmoid units. Neural Computation, 12:2129- 2174.
Durbin, R. and Rumelhart, D. (1989). Product units: A computationally powerful and biologically plausible extension to backpropagation networks. Neural
Computation, 1:133- 142.
Gavalda, R. and Siegelmann, H. T . (1999) . Discontinuities in recurrent neural
networks. Neural Computation, 11:715- 745.
Haykin, S. (1999). Neural Networks : A Comprehensive Foundation. Prentice Hall,
Upper Saddle River, NJ , second edition.
Kilian, J. and Siegelmann, H. T. (1996). The dynamic universality of sigmoidal
neural networks. Information and Computation, 128:48- 56.
Koiran, P. and Sontag, E . D. (1998). Vapnik-Chervonenkis dimension of recurrent
neural networks. Discrete Applied Mathematics, 86:63- 79.
Maass, W., NatschUiger, T., and Markram, H. (2001). Real-time computing without
stable states: A new framework for neural computation based on perturbations.
Preprint.
Maass, W. and Orponen, P. (1998). On the effect of analog noise in discrete-time
analog computations. Neural Computation, 10:1071- 1095.
Maass, W. and Sontag, E . D. (1999). Analog neural nets with Gaussian or other
common noise distributions cannot recognize arbitrary regular languages. Neural
Computation, 11:771- 782.
amlin, C. W. and Giles, C. L. (1996). Constructing deterministic finite-state automata in recurrent neural networks. Journal of the Association for Computing
Machinery, 43:937- 972.
Schmitt, M. (2002). On the complexity of computing and learning with multiplicative neural networks. Neural Computation, 14. In press.
Siegelmann, H. T . (1999). Neural Networks and Analog Computation: Beyond the
Turing Limit. Progress in Theoretical Computer Science. Birkhiiuser, Boston.
Siegelmann, H. T. and Sontag, E. D. (1995). On the computational power of neural
nets. Journal of Computer and System Sciences, 50:132- 150.
Sima, J. and Orponen, P. (2001). Exponential transients in continuous-time
symmetric Hopfield nets. In Dorffner, G., Bischof, H. , and Hornik, K. , editors , Proceedings of the International Conference on Artificial Neural Networks
ICANN 2001, volume 2130 of Lecture Notes in Computer Science, pages 806- 813,
Springer, Berlin.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 5954-convolutional-networks-on-graphs-for-learning-molecular-fingerprints.pdf

Convolutional Networks on Graphs
for Learning Molecular Fingerprints

David Duvenaud? , Dougal Maclaurin?, Jorge Aguilera-Iparraguirre
Rafael G?omez-Bombarelli, Timothy Hirzel, Al?an Aspuru-Guzik, Ryan P. Adams
Harvard University

Abstract
We introduce a convolutional neural network that operates directly on graphs.
These networks allow end-to-end learning of prediction pipelines whose inputs
are graphs of arbitrary size and shape. The architecture we present generalizes
standard molecular feature extraction methods based on circular fingerprints. We
show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.

1

Introduction

Recent work in materials design used neural networks to predict the properties of novel molecules
by generalizing from examples. One difficulty with this task is that the input to the predictor, a
molecule, can be of arbitrary size and shape. Currently, most machine learning pipelines can only
handle inputs of a fixed size. The current state of the art is to use off-the-shelf fingerprint software
to compute fixed-dimensional feature vectors, and use those features as inputs to a fully-connected
deep neural network or other standard machine learning method. This formula was followed by
[28, 3, 19]. During training, the molecular fingerprint vectors were treated as fixed.
In this paper, we replace the bottom layer of this stack ? the function that computes molecular
fingerprint vectors ? with a differentiable neural network whose input is a graph representing the
original molecule. In this graph, vertices represent individual atoms and edges represent bonds. The
lower layers of this network is convolutional in the sense that the same local filter is applied to each
atom and its neighborhood. After several such layers, a global pooling step combines features from
all the atoms in the molecule.
These neural graph fingerprints offer several advantages over fixed fingerprints:
? Predictive performance. By using data adapting to the task at hand, machine-optimized
fingerprints can provide substantially better predictive performance than fixed fingerprints.
We show that neural graph fingerprints match or beat the predictive performance of standard fingerprints on solubility, drug efficacy, and organic photovoltaic efficiency datasets.
? Parsimony. Fixed fingerprints must be extremely large to encode all possible substructures
without overlap. For example, [28] used a fingerprint vector of size 43,000, after having
removed rarely-occurring features. Differentiable fingerprints can be optimized to encode
only relevant features, reducing downstream computation and regularization requirements.
? Interpretability. Standard fingerprints encode each possible fragment completely distinctly, with no notion of similarity between fragments. In contrast, each feature of a neural
graph fingerprint can be activated by similar but distinct molecular fragments, making the
feature representation more meaningful.
?

Equal contribution.

1

Figure 1: Left: A visual representation of the computational graph of both standard circular fingerprints and neural graph fingerprints. First, a graph is constructed matching the topology of the
molecule being fingerprinted, in which nodes represent atoms, and edges represent bonds. At each
layer, information flows between neighbors in the graph. Finally, each node in the graph turns on
one bit in the fixed-length fingerprint vector. Right: A more detailed sketch including the bond
information used in each operation.

2

Circular fingerprints

The state of the art in molecular fingerprints are extended-connectivity circular fingerprints
(ECFP) [21]. Circular fingerprints [6] are a refinement of the Morgan algorithm [17], designed
to encode which substructures are present in a molecule in a way that is invariant to atom-relabeling.
Circular fingerprints generate each layer?s features by applying a fixed hash function to the concatenated features of the neighborhood in the previous layer. The results of these hashes are then treated
as integer indices, where a 1 is written to the fingerprint vector at the index given by the feature
vector at each node in the graph. Figure 1(left) shows a sketch of this computational architecture.
Ignoring collisions, each index of the fingerprint denotes the presence of a particular substructure.
The size of the substructures represented by each index depends on the depth of the network. Thus
the number of layers is referred to as the ?radius? of the fingerprints.
Circular fingerprints are analogous to convolutional networks in that they apply the same operation
locally everywhere, and combine information in a global pooling step.

3

Creating a differentiable fingerprint

The space of possible network architectures is large. In the spirit of starting from a known-good configuration, we designed a differentiable generalization of circular fingerprints. This section describes
our replacement of each discrete operation in circular fingerprints with a differentiable analog.
Hashing The purpose of the hash functions applied at each layer of circular fingerprints is to
combine information about each atom and its neighboring substructures. This ensures that any
change in a fragment, no matter how small, will lead to a different fingerprint index being activated.
We replace the hash operation with a single layer of a neural network. Using a smooth function
allows the activations to be similar when the local molecular structure varies in unimportant ways.
Indexing Circular fingerprints use an indexing operation to combine all the nodes? feature vectors
into a single fingerprint of the whole molecule. Each node sets a single bit of the fingerprint to one,
at an index determined by the hash of its feature vector. This pooling-like operation converts an
arbitrary-sized graph into a fixed-sized vector. For small molecules and a large fingerprint length,
the fingerprints are always sparse. We use the softmax operation as a differentiable analog of
indexing. In essence, each atom is asked to classify itself as belonging to a single category. The sum
of all these classification label vectors produces the final fingerprint. This operation is analogous to
the pooling operation in standard convolutional neural networks.
2

Algorithm 1 Circular fingerprints
1: Input: molecule, radius R, fingerprint
length S
2: Initialize: fingerprint vector f ? 0S
3: for each atom a in molecule
4:
ra ? g(a)
. lookup atom features
5: for L = 1 to R
. for each layer
6:
for each atom a in molecule
7:
r1 . . . rN = neighbors(a)
8:
v ? [ra , r1 , . . . , rN ] . concatenate
9:
ra ? hash(v)
. hash function
10:
i ? mod(ra , S) . convert to index
11:
fi ? 1
. Write 1 at index
12: Return: binary vector f

Algorithm 2 Neural graph fingerprints
1: Input: molecule, radius R, hidden weights
5
H11 . . . HR
, output weights W1 . . . WR
2: Initialize: fingerprint vector f ? 0S
3: for each atom a in molecule
4:
ra ? g(a)
. lookup atom features
5: for L = 1 to R
. for each layer
6:
for each atom a in molecule
7:
r1 . . . rN =Pneighbors(a)
8:
v ? ra + N
. sum
i=1 ri
9:
ra ? ?(vHLN )
. smooth function
10:
i ? softmax(ra WL )
. sparsify
11:
f ?f +i
. add to fingerprint
12: Return: real-valued vector f

Figure 2: Pseudocode of circular fingerprints (left) and neural graph fingerprints (right). Differences
are highlighted in blue. Every non-differentiable operation is replaced with a differentiable analog.
Canonicalization Circular fingerprints are identical regardless of the ordering of atoms in each
neighborhood. This invariance is achieved by sorting the neighboring atoms according to their
features, and bond features. We experimented with this sorting scheme, and also with applying the
local feature transform on all possible permutations of the local neighborhood. An alternative to
canonicalization is to apply a permutation-invariant function, such as summation. In the interests of
simplicity and scalability, we chose summation.
Circular fingerprints can be interpreted as a special case of neural graph fingerprints having large
random weights. This is because, in the limit of large input weights, tanh nonlinearities approach
step functions, which when concatenated form a simple hash function. Also, in the limit of large
input weights, the softmax operator approaches a one-hot-coded argmax operator, which is analogous to an indexing operation.
Algorithms 1 and 2 summarize these two algorithms and highlight their differences. Given a fingerprint length L, and F features at each layer, the parameters of neural graph fingerprints consist of
a separate output weight matrix of size F ? L for each layer, as well as a set of hidden-to-hidden
weight matrices of size F ? F at each layer, one for each possible number of bonds an atom can
have (up to 5 in organic molecules).

4

Experiments

We ran two experiments to demonstrate that neural fingerprints with large random weights behave
similarly to circular fingerprints. First, we examined whether distances between circular fingerprints
were similar to distances between neural fingerprint-based distances. Figure 3 (left) shows a scatterplot of pairwise distances between circular vs. neural fingerprints. Fingerprints had length 2048,
and were calculated on pairs of molecules from the solubility dataset [4]. Distance was measured
using a continuous generalization of the Tanimoto (a.k.a. Jaccard) similarity measure, given by
.X
X
distance(x, y) = 1 ?
min(xi , yi )
max(xi , yi )
(1)
There is a correlation of r = 0.823 between the distances. The line of points on the right of the plot
shows that for some pairs of molecules, binary ECFP fingerprints have exactly zero overlap.
Second, we examined the predictive performance of neural fingerprints with large random weights
vs. that of circular fingerprints. Figure 3 (right) shows average predictive performance on the solubility dataset, using linear regression on top of fingerprints. The performances of both methods
follow similar curves. In contrast, the performance of neural fingerprints with small random weights
follows a different curve, and is substantially better. This suggests that even with random weights,
the relatively smooth activation of neural fingerprints helps generalization performance.
3

2.0
1.8

0.9

RMSE (log Mol/L)

Neural fingerprint distances

Neural vs Circular distances, r =0:823
1.0

0.8
0.7
0.6
0.5
0.5

Circular fingerprints
Random conv with large parameters
Random conv with small parameters

1.6
1.4
1.2
1.0
0.8
0

0.6
0.7
0.8
0.9
1.0
Circular fingerprint distances

1

2
3
4
Fingerprint radius

5

6

Figure 3: Left: Comparison of pairwise distances between molecules, measured using circular fingerprints and neural graph fingerprints with large random weights. Right: Predictive performance
of circular fingerprints (red), neural graph fingerprints with fixed large random weights (green) and
neural graph fingerprints with fixed small random weights (blue). The performance of neural graph
fingerprints with large random weights closely matches the performance of circular fingerprints.
4.1

Examining learned features

To demonstrate that neural graph fingerprints are interpretable, we show substructures which most
activate individual features in a fingerprint vector. Each feature of a circular fingerprint vector can
each only be activated by a single fragment of a single radius, except for accidental collisions.
In contrast, neural graph fingerprint features can be activated by variations of the same structure,
making them more interpretable, and allowing shorter feature vectors.
Solubility features Figure 4 shows the fragments that maximally activate the most predictive features of a fingerprint. The fingerprint network was trained as inputs to a linear model predicting
solubility, as measured in [4]. The feature shown in the top row has a positive predictive relationship
with solubility, and is most activated by fragments containing a hydrophilic R-OH group, a standard
indicator of solubility. The feature shown in the bottom row, strongly predictive of insolubility, is
activated by non-polar repeated ring structures.
Fragments most
activated by
pro-solubility
feature

O

OH

O
NH
O

OH

OH

Fragments most
activated by
anti-solubility
feature

Figure 4: Examining fingerprints optimized for predicting solubility. Shown here are representative
examples of molecular fragments (highlighted in blue) which most activate different features of the
fingerprint. Top row: The feature most predictive of solubility. Bottom row: The feature most
predictive of insolubility.

4

Toxicity features We trained the same model architecture to predict toxicity, as measured in two
different datasets in [26]. Figure 5 shows fragments which maximally activate the feature most
predictive of toxicity, in two separate datasets.
Fragments most
activated by
toxicity feature
on SR-MMP
dataset
Fragments most
activated by
toxicity feature
on NR-AHR
dataset
Figure 5: Visualizing fingerprints optimized for predicting toxicity. Shown here are representative
samples of molecular fragments (highlighted in red) which most activate the feature most predictive
of toxicity. Top row: the most predictive feature identifies groups containing a sulphur atom attached
to an aromatic ring. Bottom row: the most predictive feature identifies fused aromatic rings, also
known as polycyclic aromatic hydrocarbons, a well-known carcinogen.
[27] constructed similar visualizations, but in a semi-manual way: to determine which toxic fragments activated a given neuron, they searched over a hand-made list of toxic substructures and chose
the one most correlated with a given neuron. In contrast, our visualizations are generated automatically, without the need to restrict the range of possible answers beforehand.
4.2

Predictive Performance

We ran several experiments to compare the predictive performance of neural graph fingerprints to
that of the standard state-of-the-art setup: circular fingerprints fed into a fully-connected neural
network.
Experimental setup Our pipeline takes as input the SMILES [30] string encoding of each
molecule, which is then converted into a graph using RDKit [20]. We also used RDKit to produce
the extended circular fingerprints used in the baseline. Hydrogen atoms were treated implicitly.
In our convolutional networks, the initial atom and bond features were chosen to be similar to those
used by ECFP: Initial atom features concatenated a one-hot encoding of the atom?s element, its
degree, the number of attached hydrogen atoms, and the implicit valence, and an aromaticity indicator. The bond features were a concatenation of whether the bond type was single, double, triple,
or aromatic, whether the bond was conjugated, and whether the bond was part of a ring.
Training and Architecture Training used batch normalization [11]. We also experimented with
tanh vs relu activation functions for both the neural fingerprint network layers and the fullyconnected network layers. relu had a slight but consistent performance advantage on the validation set. We also experimented with dropconnect [29], a variant of dropout in which weights are
randomly set to zero instead of hidden units, but found that it led to worse validation error in general. Each experiment optimized for 10000 minibatches of size 100 using the Adam algorithm [13],
a variant of RMSprop that includes momentum.
Hyperparameter Optimization To optimize hyperparameters, we used random search. The hyperparameters of all methods were optimized using 50 trials for each cross-validation fold. The
following hyperparameters were optimized: log learning rate, log of the initial weight scale, the log
L2 penalty, fingerprint length, fingerprint depth (up to 6), and the size of the hidden layer in the
fully-connected network. Additionally, the size of the hidden feature vector in the convolutional
neural fingerprint networks was optimized.
5

Dataset
Units
Predict mean
Circular FPs + linear layer
Circular FPs + neural net
Neural FPs + linear layer
Neural FPs + neural net

Solubility [4]
log Mol/L

Drug efficacy [5]
EC50 in nM

Photovoltaic efficiency [8]
percent

4.29 ? 0.40
1.71 ? 0.13
1.40 ? 0.13
0.77 ? 0.11
0.52 ? 0.07

1.47 ? 0.07
1.13 ? 0.03
1.36 ? 0.10
1.15 ? 0.02
1.16 ? 0.03

6.40 ? 0.09
2.63 ? 0.09
2.00 ? 0.09
2.58 ? 0.18
1.43 ? 0.09

Table 1: Mean predictive accuracy of neural fingerprints compared to standard circular fingerprints.

Datasets We compared the performance of standard circular fingerprints against neural graph fingerprints on a variety of domains:
? Solubility: The aqueous solubility of 1144 molecules as measured by [4].
? Drug efficacy: The half-maximal effective concentration (EC50 ) in vitro of 10,000
molecules against a sulfide-resistant strain of P. falciparum, the parasite that causes malaria,
as measured by [5].
? Organic photovoltaic efficiency: The Harvard Clean Energy Project [8] uses expensive
DFT simulations to estimate the photovoltaic efficiency of organic molecules. We used a
subset of 20,000 molecules from this dataset.
Predictive accuracy We compared the performance of circular fingerprints and neural graph fingerprints under two conditions: In the first condition, predictions were made by a linear layer using
the fingerprints as input. In the second condition, predictions were made by a one-hidden-layer
neural network using the fingerprints as input. In all settings, all differentiable parameters in the
composed models were optimized simultaneously. Results are summarized in Table 4.2.
In all experiments, the neural graph fingerprints matched or beat the accuracy of circular fingerprints,
and the methods with a neural network on top of the fingerprints typically outperformed the linear
layers.
Software Automatic differentiation (AD) software packages such as Theano [1] significantly
speed up development time by providing gradients automatically, but can only handle limited control
structures and indexing. Since we required relatively complex control flow and indexing in order
to implement variants of Algorithm 2, we used a more flexible automatic differentiation package
for Python called Autograd (github.com/HIPS/autograd). This package handles standard
Numpy [18] code, and can differentiate code containing while loops, branches, and indexing.
Code for computing neural fingerprints and producing visualizations is available at
github.com/HIPS/neural-fingerprint.

5

Limitations

Computational cost Neural fingerprints have the same asymptotic complexity in the number of
atoms and the depth of the network as circular fingerprints, but have additional terms due to the
matrix multiplies necessary to transform the feature vector at each step. To be precise, computing
the neural fingerprint of depth R, fingerprint length L of a molecule with N atoms using a molecular
convolutional net having F features at each layer costs O(RN F L + RN F 2 ). In practice, training
neural networks on top of circular fingerprints usually took several minutes, while training both the
fingerprints and the network on top took on the order of an hour on the larger datasets.
Limited computation at each layer How complicated should we make the function that goes
from one layer of the network to the next? In this paper we chose the simplest feasible architecture:
a single layer of a neural network. However, it may be fruitful to apply multiple layers of nonlinearities between each message-passing step (as in [22]), or to make information preservation easier by
adapting the Long Short-Term Memory [10] architecture to pass information upwards.
6

Limited information propagation across the graph The local message-passing architecture developed in this paper scales well in the size of the graph (due to the low degree of organic molecules),
but its ability to propagate information across the graph is limited by the depth of the network. This
may be appropriate for small graphs such as those representing the small organic molecules used in
this paper. However, in the worst case, it can take a depth N2 network to distinguish between graphs
of size N . To avoid this problem, [2] proposed a hierarchical clustering of graph substructures. A
tree-structured network could examine the structure of the entire graph using only log(N ) layers,
but would require learning to parse molecules. Techniques from natural language processing [25]
might be fruitfully adapted to this domain.
Inability to distinguish stereoisomers Special bookkeeping is required to distinguish between
stereoisomers, including enantomers (mirror images of molecules) and cis/trans isomers (rotation
around double bonds). Most circular fingerprint implementations have the option to make these
distinctions. Neural fingerprints could be extended to be sensitive to stereoisomers, but this remains
a task for future work.

6

Related work

This work is similar in spirit to the neural Turing machine [7], in the sense that we take an existing
discrete computational architecture, and make each part differentiable in order to do gradient-based
optimization.
Neural nets for quantitative structure-activity relationship (QSAR) The modern standard for
predicting properties of novel molecules is to compose circular fingerprints with fully-connected
neural networks or other regression methods. [3] used circular fingerprints as inputs to an ensemble
of neural networks, Gaussian processes, and random forests. [19] used circular fingerprints (of depth
2) as inputs to a multitask neural network, showing that multiple tasks helped performance.
Neural graph fingerprints The most closely related work is [15], who build a neural network
having graph-valued inputs. Their approach is to remove all cycles and build the graph into a tree
structure, choosing one atom to be the root. A recursive neural network [23, 24] is then run from
the leaves to the root to produce a fixed-size representation. Because a graph having N nodes
has N possible roots, all N possible graphs are constructed. The final descriptor is a sum of the
representations computed by all distinct graphs. There are as many distinct graphs as there are
atoms in the network. The computational cost of this method thus grows as O(F 2 N 2 ), where F
is the size of the feature vector and N is the number of atoms, making it less suitable for large
molecules.
Convolutional neural networks Convolutional neural networks have been used to model images,
speech, and time series [14]. However, standard convolutional architectures use a fixed computational graph, making them difficult to apply to objects of varying size or structure, such as molecules.
More recently, [12] and others have developed a convolutional neural network architecture for modeling sentences of varying length.
Neural networks on fixed graphs [2] introduce convolutional networks on graphs in the regime
where the graph structure is fixed, and each training example differs only in having different features
at the vertices of the same graph. In contrast, our networks address the situation where each training
input is a different graph.
Neural networks on input-dependent graphs [22] propose a neural network model for graphs
having an interesting training procedure. The forward pass consists of running a message-passing
scheme to equilibrium, a fact which allows the reverse-mode gradient to be computed without storing
the entire forward computation. They apply their network to predicting mutagenesis of molecular
compounds as well as web page rankings. [16] also propose a neural network model for graphs
with a learning scheme whose inner loop optimizes not the training loss, but rather the correlation
between each newly-proposed vector and the training error residual. They apply their model to a
dataset of boiling points of 150 molecular compounds. Our paper builds on these ideas, with the
7

following differences: Our method replaces their complex training algorithms with simple gradientbased optimization, generalizes existing circular fingerprint computations, and applies these networks in the context of modern QSAR pipelines which use neural networks on top of the fingerprints
to increase model capacity.
Unrolled inference algorithms [9] and others have noted that iterative inference procedures
sometimes resemble the feedforward computation of a recurrent neural network. One natural extension of these ideas is to parameterize each inference step, and train a neural network to approximately
match the output of exact inference using only a small number of iterations. The neural fingerprint,
when viewed in this light, resembles an unrolled message-passing algorithm on the original graph.

7

Conclusion

We generalized existing hand-crafted molecular features to allow their optimization for diverse tasks.
By making each operation in the feature pipeline differentiable, we can use standard neural-network
training methods to scalably optimize the parameters of these neural molecular fingerprints end-toend. We demonstrated the interpretability and predictive performance of these new fingerprints.
Data-driven features have already replaced hand-crafted features in speech recognition, machine
vision, and natural-language processing. Carrying out the same task for virtual screening, drug
design, and materials design is a natural next step.
Acknowledgments
We thank Edward Pyzer-Knapp, Jennifer Wei, and Samsung Advanced Institute of Technology for
their support. This work was partially funded by NSF IIS-1421780.

References
[1] Fr?ed?eric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra, Ian J. Goodfellow, Arnaud
Bergeron, Nicolas Bouchard, and Yoshua Bengio. Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop, 2012.
[2] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally
connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013.
[3] George E. Dahl, Navdeep Jaitly, and Ruslan Salakhutdinov. Multi-task neural networks for
QSAR predictions. arXiv preprint arXiv:1406.1231, 2014.
[4] John S. Delaney. ESOL: Estimating aqueous solubility directly from molecular structure. Journal of Chemical Information and Computer Sciences, 44(3):1000?1005, 2004.
[5] Francisco-Javier Gamo, Laura M Sanz, Jaume Vidal, Cristina de Cozar, Emilio Alvarez,
Jose-Luis Lavandera, Dana E Vanderwall, Darren VS Green, Vinod Kumar, Samiul Hasan,
et al. Thousands of chemical starting points for antimalarial lead identification. Nature,
465(7296):305?310, 2010.
[6] Robert C. Glem, Andreas Bender, Catrin H. Arnby, Lars Carlsson, Scott Boyer, and James
Smith. Circular fingerprints: flexible molecular descriptors with applications from physical
chemistry to ADME. IDrugs: the investigational drugs journal, 9(3):199?204, 2006.
[7] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural Turing machines. arXiv preprint
arXiv:1410.5401, 2014.
[8] Johannes Hachmann, Roberto Olivares-Amaya, Sule Atahan-Evrenk, Carlos Amador-Bedolla,
Roel S S?anchez-Carrera, Aryeh Gold-Parker, Leslie Vogt, Anna M Brockway, and Al?an
Aspuru-Guzik. The Harvard clean energy project: large-scale computational screening and
design of organic photovoltaics on the world community grid. The Journal of Physical Chemistry Letters, 2(17):2241?2251, 2011.
[9] John R Hershey, Jonathan Le Roux, and Felix Weninger. Deep unfolding: Model-based inspiration of novel deep architectures. arXiv preprint arXiv:1409.2574, 2014.
8

[10] Sepp Hochreiter and J?urgen Schmidhuber. Long short-term memory. Neural computation,
9(8):1735?1780, 1997.
[11] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
[12] Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. A convolutional neural network
for modelling sentences. Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, June 2014.
[13] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
[14] Yann LeCun and Yoshua Bengio. Convolutional networks for images, speech, and time series.
The handbook of brain theory and neural networks, 3361, 1995.
[15] Alessandro Lusci, Gianluca Pollastri, and Pierre Baldi. Deep architectures and deep learning
in chemoinformatics: the prediction of aqueous solubility for drug-like molecules. Journal of
chemical information and modeling, 53(7):1563?1575, 2013.
[16] Alessio Micheli. Neural network for graphs: A contextual constructive approach. Neural
Networks, IEEE Transactions on, 20(3):498?511, 2009.
[17] H.L. Morgan. The generation of a unique machine description for chemical structure. Journal
of Chemical Documentation, 5(2):107?113, 1965.
[18] Travis E Oliphant. Python for scientific computing. Computing in Science & Engineering,
9(3):10?20, 2007.
[19] Bharath Ramsundar, Steven Kearnes, Patrick Riley, Dale Webster, David Konerding, and Vijay
Pande. Massively multitask networks for drug discovery. arXiv:1502.02072, 2015.
[20] RDKit: Open-source cheminformatics. www.rdkit.org. [accessed 11-April-2013].
[21] David Rogers and Mathew Hahn. Extended-connectivity fingerprints. Journal of Chemical
Information and Modeling, 50(5):742?754, 2010.
[22] F. Scarselli, M. Gori, Ah Chung Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural
network model. Neural Networks, IEEE Transactions on, 20(1):61?80, Jan 2009.
[23] Richard Socher, Eric H Huang, Jeffrey Pennin, Christopher D Manning, and Andrew Y Ng.
Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In Advances
in Neural Information Processing Systems, pages 801?809, 2011.
[24] Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages
151?161. Association for Computational Linguistics, 2011.
[25] Kai Sheng Tai, Richard Socher, and Christopher D Manning.
Improved semantic
representations from tree-structured long short-term memory networks. arXiv preprint
arXiv:1503.00075, 2015.
[26] Tox21 Challenge. National center for advancing translational sciences. http://tripod.
nih.gov/tox21/challenge, 2014. [Online; accessed 2-June-2015].
[27] Thomas Unterthiner, Andreas Mayr, G?unter Klambauer, and Sepp Hochreiter. Toxicity prediction using deep learning. arXiv preprint arXiv:1503.01445, 2015.
[28] Thomas Unterthiner, Andreas Mayr, G u? nter Klambauer, Marvin Steijaert, J?org Wenger, Hugo
Ceulemans, and Sepp Hochreiter. Deep learning as an opportunity in virtual screening. In
Advances in Neural Information Processing Systems, 2014.
[29] Li Wan, Matthew Zeiler, Sixin Zhang, Yann L. Cun, and Rob Fergus. Regularization of neural
networks using dropconnect. In International Conference on Machine Learning, 2013.
[30] David Weininger. SMILES, a chemical language and information system. Journal of chemical
information and computer sciences, 28(1):31?36, 1988.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 1692-lower-bounds-on-the-complexity-of-approximating-continuous-functions-by-sigmoidal-neural-networks.pdf

Lower Bounds on the Complexity of
Approximating Continuous Functions by
Sigmoidal Neural Networks

Michael Schmitt
Lehrstuhl Mathematik und Informatik
FakuWit ftir Mathematik
Ruhr-Universitat Bochum
D-44780 Bochum, Germany
mschmitt@lmi.ruhr-uni-bochum.de

Abstract
We calculate lower bounds on the size of sigmoidal neural networks
that approximate continuous functions. In particular, we show
that for the approximation of polynomials the network size has
to grow as O((logk)1/4) where k is the degree of the polynomials.
This bound is valid for any input dimension, i.e. independently of
the number of variables. The result is obtained by introducing a
new method employing upper bounds on the Vapnik-Chervonenkis
dimension for proving lower bounds on the size of networks that
approximate continuous functions.

1

Introduction

Sigmoidal neural networks are known to be universal approximators. This is one of
the theoretical results most frequently cited to justify the use of sigmoidal neural
networks in applications. By this statement one refers to the fact that sigmoidal
neural networks have been shown to be able to approximate any continuous function
arbitrarily well. Numerous results in the literature have established variants of
this universal approximation property by considering distinct function classes to be
approximated by network architectures using different types of neural activation
functions with respect to various approximation criteria, see for instance [1, 2, 3, 5,
6, 11, 12, 14, 15]. (See in particular Scarselli and Tsoi [15] for a recent survey and
further references.)
All these results and many others not referenced here, some of them being constructive, some being merely existence proofs, provide upper bounds for the network size
asserting that good approximation is possible if there are sufficiently many network nodes available. This, however, is only a partial answer to the question that
mainly arises in practical applications: "Given some function, how many network
nodes are needed to approximate it?" Not much attention has been focused on
establishing lower bounds on the network size and, in particular, for the approximation of functions over the reals. As far as the computation of binary-valued

Complexity ofApproximating Continuous Functions by Neural Networks

329

functions by sigmoidal networks is concerned (where the output value of a network
is thresholded to yield 0 or 1) there are a few results in this direction. For a specific Boolean function Koiran [9] showed that networks using the standard sigmoid
u(y) = 1/(1 + e- Y ) as activation function must have size O(nl/4) where n is the
number of inputs. (When measuring network size we do not count the input nodes
here and in what follows.) Maass [13] established a larger lower bound by constructing a binary-valued function over IRn and showing that standard sigmoidal networks
require O(n) many network nodes for computing this function. The first work on
the complexity of sigmoidal networks for approximating continuous functions is due
to DasGupta and Schnitger [4]. They showed that the standard sigmoid in network
nodes can be replaced by other types of activation functions without increasing the
size of the network by more than a polynomial. This yields indirect lower bounds
for the size of sigmoidal networks in terms of other network types. DasGupta and
Schnitger [4] also claimed the size bound AO(I/d) for sigmoidal networks with d
layers approximating the function sin(Ax).
In this paper we consider the problem of using the standard sigmoid u(y) =
1/(1 + e- Y ) in neural networks for the approximation of polynomials. We show

that at least O?logk)1/4) network nodes are required to approximate polynomials
of degree k with small error in the loo norm. This bound is valid for arbitrary input
dimension, i.e., it does not depend on the number of variables. (Lower bounds can
also be obtained from the results on binary-valued functions mentioned above by
interpolating the corresponding functions by polynomials. This, however, requires
growing input dimension and does not yield a lower bound in terms of the degree.)
Further, the bound established here holds for networks of any number of layers. As
far as we know this is the first lower bound result for the approximation of polynomials. From the computational point of view this is a very simple class of functions;
they can be computed using the basic operations addition and multiplication only.
Polynomials also play an important role in approximation theory since they are
dense in the class of continuous functions and some approximation results for neural networks rely on the approximability of polynomials by sigmoidal networks (see,
e.g., [2, 15]).
We obtain the result by introducing a new method that employs upper bounds on
the Vapnik-Chervonenkis dimension of neural networks to establish lower bounds
on the network size. The first use of the Vapnik-Chervonenkis dimension to obtain
a lower bound is due to Koiran [9] who calculated the above-mentioned bound
on the size of sigmoidal networks for a Boolean function. Koiran's method was
further developed and extended by Maass [13] using a similar argument but another
combinatorial dimension. Both papers derived lower bounds for the computation
of binary-valued functions (Koiran [9] for inputs from {O, 1}n, Maass [13] for inputs
from IRn). Here, we present a new technique to show that and how lower bounds can
be obtained for networks that approximate continuous functions. It rests on two
fundamental results about the Vapnik-Chervonenkis dimension of neural networks.
On the one hand, we use constructions provided by Koiran and Sontag [10] to build
networks that have large Vapnik-Chervonenkis dimension and consist of gates that
compute certain arithmetic functions. On the other hand, we follow the lines of
reasoning of Karpinski and Macintyre [7] to derive an upper bound for the VapnikChervonenkis dimension of these networks from the estimates of Khovanskil [8] and
a result due to Warren [16].
In the following section we give the definitions of sigmoidal networks and the VapnikChervonenkis dimension. Then we present the lower bound result for function
approximation. Finally, we conclude with some discussion and open questions.

330

2

M Schmitt

Sigmoidal Neural Networks and VC Dimension

We briefly recall the definitions of a sigmoidal neural network and the VapnikChervonenkis dimension (see, e.g., [7, 10]). We consider /eed/orward neural networks
which have a certain number of input nodes and one output node. The nodes
which are not input nodes are called computation nodes and associated with each
of them is a real number t, the threshold. Further, each edge is labelled with a
real number W called weight. Computation in the network takes place as follows:
The input values are assigned to the input nodes. Each computation node applies
the standard sigmoid u(y) = 1/(1 + e- V ) to the sum W1Xl + ... + WrXr - t where
Xl, .?. ,X r are the values computed by the node's predecessors, WI, ??? ,W r are the
weights of the corresponding edges, and t is the threshold. The output value of the
network is defined to be the value computed by the output node. As it is common
for approximation results by means of neural networks, we assume that the output
node is a linear gate, i.e., it just outputs the sum WIXI + ... + WrXr - t. (Clearly,
for computing functions on finite sets with output range [0, 1] the output node
may apply the standard sigmoid as well.) Since u is the only sigmoidal function
that we consider here we will refer to such networks as sigmoidal neural networks.
(Sigmoidal functions in general need to satisfy much weaker assumptions than u
does.) The definition naturally generalizes to networks employing other types of
gates that we will make use of (e.g. linear, multiplication, and division gates).
The Vapnik-Chervonenkis dimension is a combinatorial dimension of a function class
and is defined as follows: A dichotomy of a set S ~ IRn is a partition of S into two
disjoint subsets (So, Sl) such that So U SI = S. Given a set F offunctions mapping
IRn to {O, I} and a dichotomy (So, Sd of S, we say that F induces the dichotomy
(So, Sd on S if there is some f E F such that /(So) ~ {O} and f(Sd ~ {I}.
We say further that F shatters S if F induces all dichotomies on S. The VapnikChervonenkis (VC) dimension of F, denoted VCdim(F), is defined as the largest
number m such that there is a set of m elements that is shattered by F. We refer
to the VC dimension of a neural network, which is given in terms of a "feedforward
architecture", i.e. a directed acyclic graph, as the VC dimension of the class of
functions obtained by assigning real numbers to all its programmable parameters,
which are in general the weights and thresholds of the network or a subset thereof.
Further, we assume that the output value of the network is thresholded at 1/2 to
obtain binary values.

3

Lower Bounds on Network Size

Before we present the lower bound on the size of sigmoidal networks required for
the approximation of polynomials we first give a brief outline of the proof idea.
We will define a sequence of univariate polynomials (Pn)n>l by means of which
we show how to construct neural architectures N n consistmg of various types of
gates such as linear, multiplication, and division gates, and, in particular, gates
that compute some of the polynomials. Further, this architecture has a single
weight as programmable parameter (all other weights and thresholds are fixed).
We then demonstrate that, assuming the gates computing the polynomials can be
approximated by sigmoidal neural networks sufficiently well, the architecture Nn
can shatter a certain set by assigning suitable values to its programmable weight.
The final step is to reason along the lines of Karpinski and Macintyre [7] to obtain
via Khovanskil's estimates [8] and Warren's result [16] an upper bound on the VC
dimension of N n in terms of the number of its computation nodes. (Note that we
cannot directly apply Theorem 7 of [7] since it does not deal with division gates.)
Comparing this bound with the cardinality of the shattered set we will then be able

331

Complexity ofApproximating Continuous Functions by Neural Networks

(3)

W

1

n

P3

(1)

(2)

W1

W1

(3)

Wi

(3)

W1

n

P2

(2)

Wj

(1)

Wk

(1)

(2)

Wn

Wn

n

P1

Wn

j --------------------------------~
k--------------------------------------------------~

Figure 1: The network N n with values k, j, i, 1 assigned to the input nodes
Xl, X2, X3, X4 respectively. The weight W is the only programmable parameter of
the network.

to conclude with a lower bound on the number of computation nodes in N n and
thus in the networks that approximate the polynomials.
Let the sequence (Pn)n2: l of polynomials over IR be inductively defined by
Pn(X) =

{ 4x(1 - x)

P(Pn-dx))

n = 1,
n 2:: 2 .

Clearly, this uniquely defines Pn for every n 2:: 1 and it can readily be seen that
Pn has degree 2n. The main lower bound result is made precise in the following
statement.
Theorem 1 Sigmoidal neural networks that approximate the polynomials (Pn)n >l
on the interval [0,1] with error at most O(2- n ) in the 100 norm must have at least
n(nl/4) computation nodes.
Proof. For each n a neural architecture N n can be constructed as follows: The
network has four input nodes Xl, X2, X3, X4. Figure 1 shows the network with input
values assigned to the input nodes in the order X4 = 1, X3 = i, X2 = j, Xl = k.
There is one weight which we consider as the (only) programmable parameter of
N n . It is associated with the edge outgoing from input node X4 and is denoted
by w. The computation nodes are partitioned into six levels as indicated by the
boxes in Figure 1. Each level is itself a network. Let us first assume, for the sake of
simplicity, that all computations over real numbers are exact. There are three levels
labeled with II, having n + 1 input nodes and one output node each, that compute
so-called projections 7r : IRnH -+ IR where 7r(YI,"" Yn, a) = Ya for a E {I, ... , n}.
The levels labeled P3 , P2 , PI have one input node and n output nodes each. Level
P3 receives the constant 1 as input and thus the value W which is the parameter of
the network. We define the output values of level P A for>. = 3,2, 1 by
(A)

wb

= Pbon"'-l ( v) ,

b= 1, ... ,n

where v denotes the input value to level P A. This value is equal to w for>. = 3 and
(A+l) , .?. , Wn()..+l) ,XA+l ) oth erWlse.
.
OUT
(A) can b
id
vve observe t h at wb+l
e calcu
ate f rom

7r (WI

332

M Schmitt

w~A) as Pn>'_l(W~A?). Therefore, the computations of level P A can be implemented
using n gates each of them computing the function Pn>.-l.
We show now that Nn can shatter a set of cardinality n 3 ? Let S = {I, ... ,n p. It
has been shown in Lemma 2 of [10] that for each (/31 , ... , /3r) E {O, 1Y there exists
some W E [0,1] such that for q = 1, ... ,T
pq(w) E [0,1/2)

if /3q

= 0,

and pq(w) E (1/2,1]

if /3q

= 1.

This implies that, for each dichotomy (So, Sd of S there is some
that for every (i, j, k) E S
Pk (pj.n (Pi.n 2(w)))
Pk(Pj.n(Pi.n2(w)))

< 1/2
> 1/2

if
if

W

E [0,1] such

(i, j, k) E So ,
(i,j,k)ES1'

Note that Pk(Pj.n(Pi.n2 (w))) is the value computed by N n given input values k, j, i, 1.
Therefore, choosing a suitable value for w, which is the parameter of Nn , the network
can induce any dichotomy on S. In other words, S is shattered by Nn .

An such that
for each E > weights can be chosen for An such that the function in,? computed
by this network satisfies lim?~o in,?(Yl, ... ,Yn, a) = Ya. Moreover, this architecture
consists of O(n) computation nodes, which are linear, multiplication, and division
gates. (Note that the size of An does not depend on E.) Therefore, choosing E
sufficiently small, we can implement the projections 1r in N n by networks of O(n)
computation nodes such that the resulting network N~ still shatters S. Now in N~
we have O(n) computation nodes for implementing the three levels labeled II and
we have in each level P A a number of O(n) computation nodes for computing Pn>.-l,
respectively. Assume now that the computation nodes for Pn>.-l can be replaced
by sigmoidal networks such that on inputs from S and with the parameter values
defined above the resulting network N:: computes the same functions as N~. (Note
that the computation nodes for Pn>.-l have no programmable parameters.)

?

It has been shown in Lemma 1 of [10] that there is an architecture

N::.

We estimate the size of
According to Theorem 7 of Karpinski and Macintyre
[7] a sigmoidal neural network with I programmable parameters and m computation
nodes has VC dimension O((ml)2). We have to generalize this result slightly before
being able to apply it. It can readily be seen from the proof of Theorem 7 in [7] that
the result also holds if the network additionally contains linear and multiplication
gates. For division gates we can derive the same bound taking into account that for
a gate computing division, say x/y, we can introduce a defining equality x = z . Y
where z is a new variable. (See [7] for how to proceed.) Thus, we have that a
network with I programmable parameters and m computation nodes, which are
linear, multiplication, division, and sigmoidal gates, has VC dimension O((ml)2).
In particular, if m is the number of computation nodes of N::, the VC dimension
can shatter a set
is O(m 2 ). On the other hand, as we have shown above,
of cardinality n 3 ? Since there are O(n) sigmoidal networks in
computing the
functions Pn>.-l, and since the number of linear, multiplication, and division gates
is bounded by O(n), for some value of A a single network computing Pn>.-l must
have size at least O(fo). This yields a lower bound of O(nl/4) for the size of a
sigmoidal network computing Pn.

N::
N::

Thus far, we have assumed that the polynomials Pn are computed exactly. Since
polynomials are continuous functions and since we require them to be calculated
only on a finite set of input values (those resulting from S and from the parameter
values chosen for w to shatter S) an approximation of these polynomials is sufficient.
A straightforward analysis, based on the fact that the output value of the network
has a "tolerance" close to 1/2, shows that if Pn is approximated with error O(2- n )

Complexity ofApproximating Continuous Functions by Neural Networks

333

in the loo norm, the resulting network still shatters the set S. This completes the
proof of the theorem.
D
The statement of the previous theorem is restricted to the approximation of polynomials on the input domain [0,1]. However, the result immediately generalizes to
any arbitrary interval in llt Moreover, it remains valid for multivariate polynomials
of arbitrary input dimension.
Corollary 2 The approximation of polynomials of degree k by sigmoidal neural
networks with approximation error O(ljk) in the 100 norm requires networks of size
O((log k)1/4). This holds for polynomials over any number of variables.

4

Conclusions and Open Questions

We have established lower bounds on the size of sigmoidal networks for the approximation of continuous functions. In particular, for a concrete class of polynomials
we have calculated a lower bound in terms of the degree of the polynomials. The
main result already holds for the approximation of univariate polynomials. Intuitively, approximation of multivariate polynomials seems to become harder when
the dimension increases. Therefore, it would be interesting to have lower bounds
both in terms of the degree and the input dimension.
Further, in our result the approximation error and the degree are coupled. Naturally,
one would expect that the number of nodes has to grow for each fixed function when
the error decreases. At present we do not know of any such lower bound.
We have not aimed at calculating the constants in the bounds. For practical applications such values are indispensable. Refining our method and using tighter results
it should be straightforward to obtain such numbers. Further, we expect that better
lower bounds can be obtained by considering networks of restricted depth.
To establish the result we have introduced a new method for deriving lower bounds
on network sizes. One of the main arguments is to use the functions to be approximated to construct networks with large VC dimension. The method seems suitable
to obtain bounds also for the approximation of other types of functions as long as
they are computationally powerful enough.
Moreover, the method could be adapted to obtain lower bounds also for networks
using other activation functions (e.g. more general sigmoidal functions, ridge functions, radial basis functions). This may lead to new separation results for the
approximation capabilities of different types of neural networks. In order for this
to be accomplished, however, an essential requirement is that small upper bounds
can be calculated for the VC dimension of such networks.
Acknowledgments
I thank Hans U. Simon for helpful discussions. This work was supported in part
by the ESPRIT Working Group in Neural and Computational Learning II, NeuroCOLT2, No. 27150.

References
[1] A. Barron. Universal approximation bounds for superposition of a sigmoidal
function. IEEE Transactions on Information Theory, 39:930--945, 1993.

334

M Schmitt

[2J C. K. Chui and X. Li. Approximation by ridge functions and neural networks
with one hidden layer. Journal of Approximation Theory, 70:131-141,1992.
[3J G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals, and Systems, 2:303-314, 1989.
[4J B. DasGupta and G. Schnitger. The power of approximating: A comparison
of activation functions. In C. L. Giles, S. J. Hanson, and J. D. Cowan, editors,
Advances in Neural Information Processing Systems 5, pages 615-622, Morgan
Kaufmann, San Mateo, CA, 1993.
[5] K. Hornik. Approximation capabilities of multilayer feedforward networks.
Neural Networks, 4:251-257, 1991.
[6] K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward networks
are universal approximators. Neural Networks, 2:359-366, 1989.
[7] M. Karpinski and A. Macintyre. Polynomial bounds for VC dimension of
sigmoidal and general Pfaffian neural networks. Journal of Computer and
System Sciences, 54:169-176, 1997.
[8] A. G. Khovanskil. Fewnomials, volume 88 of Translations of Mathematical
Monographs. American Mathematical Society, Providence, RI, 1991.
[9] P. Koiran. VC dimension in circuit complexity. In Proceedings of the 11th
Annual IEEE Conference on Computational Complexity CCC'96, pages 81-85,
IEEE Computer Society Press, Los Alamitos, CA, 1996.
[10] P. Koiran and E. D. Sontag. Neural networks with quadratic VC dimension.
Journal of Computer and System Sciences, 54:190-198, 1997.
[11] V. Y. Kreinovich. Arbitrary nonlinearity is sufficient to represent all functions
by neural networks: A theorem. Neural Networks, 4:381-383, 1991.
[12] M. Leshno, V. Y. Lin, A. Pinkus, and S. Schocken. Multilayer feedforward networks with a nonpolynomial activation function can approximate any function.
Neural Networks, 6:861-867, 1993.
[13] W. Maass. Noisy spiking neurons with temporal coding have more computational power than sigmoidal neurons. In M. Mozer, M. 1. Jordan, and
T. Petsche, editors, Advances in Neural Information Processing Systems 9,
pages 211-217. MIT Press, Cambridge, MA, 1997.
[14] H. Mhaskar. Neural networks for optimal approximation of smooth and analytic
functions. Neural Computation, 8:164-177, 1996.
[15J F. Scarselli and A. C. Tsoi. Universal approximation using feedforward neural
networks: A survey of some existing methods and some new results. Neural
Networks, 11:15-37, 1998.
[16] H. E. Warren. Lower bounds for approximation by nonlinear manifolds. Transactions of the American Mathematical Society, 133:167-178, 1968.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 516-neural-network-routing-for-random-multistage-interconnection-networks.pdf

Neural Network Routing for Random Multistage
Interconnection Networks

Mark W. Goudreau
Princeton University
and
NEe Research Institute, Inc.
4 Independence Way
Princeton, NJ 08540

c. Lee Giles
NEC Research Institute, Inc.
4 Independence Way
Princeton, NJ 08540

Abstract
A routing scheme that uses a neural network has been developed that can
aid in establishing point-to-point communication routes through multistage interconnection networks (MINs). The neural network is a network
of the type that was examined by Hopfield (Hopfield, 1984 and 1985).
In this work, the problem of establishing routes through random MINs
(RMINs) in a shared-memory, distributed computing system is addressed.
The performance of the neural network routing scheme is compared to two
more traditional approaches - exhaustive search routing and greedy routing. The results suggest that a neural network router may be competitive
for certain RMIN s.

1

INTRODUCTION

A neural network has been developed that can aid in establishing point-topoint communication routes through multistage interconnection networks (MINs)
(Goudreau and Giles, 1991). Such interconnection networks have been widely studied (Huang, 1984; Siegel, 1990). The routing problem is of great interest due to
its broad applicability. Although the neural network routing scheme can accommodate many types of communication systems, this work concentrates on its use in a
shared-memory, distributed computing system.
Neural networks have sometimes been used to solve certain interconnection network
722

Neural Network Routing for Random Multistage Interconnection Networks

Input
Ports

Output
Ports

Interconnection
Network

Control Bits
-

: Logic1
I
L.: _ _ _ _ _

Neural
Network
_ _ ....
_-_-:--r-_~_

-

:1

Interconnection
Logic2:
Network
I
Controller
I

___'--_-_-_

-_-J:.J

Externa Control
Figure 1: The communication system with a neural network router. The input
ports (processors) are on the left, while the output ports (memory modules) are on
the right.

problems, such as finding legal routes (Brown, 1989; Hakim and Meadows, 1990)
and increasing the throughput of an interconnection network (Brown and Liu, 1990;
Marrakchi and Troudet, 1989). The neural network router that is the subject of
this work, however, differs significantly from these other routers and is specially
designed to handle parallel processing systems that have MINs with random interstage connections. Such random MINs are called RMINs. RMINs tend to have
greater fault-tolerance than regular MINs.
The problem is to allow a set of processors to access a set of memory modules
through the RMIN. A picture of the communication system with the neural network
router is shown in Figure 1. The are m processors and n memory modules. The
system is assumed to be synchronous. At the beginning of a message cycle, some
set of processors may desire to access some set of memory modules. It is the
job of the router to establish as many of these desired connections as possible in
a non-conflicting manner. Obtaining the optimal solution is not critical. Stymied
processors may attempt communication again during the subsequent message cycle.
It is the combination of speed and the quality of the solution that is important.
The object of this work was to discover if the neural network router could be competitive with other types of routers in terms of quality of solution, speed, and resource

723

724

Goudreau and Giles

RMIN2
RMINI
2

1

1
2

3

3
4
5
6

4

5
6

7
8

RMIN3
1

1

2

3

4

1

2
3

2
3

4

4

3

6

7
8

5

7
8
9

10

Figure 2: Three random multistage interconnection networks. The blocks that are
shown are crossbar switches, for which each input may be connected to each output.

utilization. To this end, the neural
other schemes for routing in RMINs
routing. So far, the results of this
router may indeed be a practicable
too large.

2

network routing scheme was compared to two
- namely, exhaustive search routing and greedy
investigation suggest that the neural network
alternative for routing in RMINs that are not

EXHAUSTIVE SEARCH ROUTING

The exhaustive search routing method is optimal in terms of the ability of the router
to find the best solution. There are many ways to implement such a router. One
approach is described here.
For a given interconnection network, every route from each input to each output
was stored in a database. (The RMIN s that were used as test cases in this paper
always had at least one route from each processor to each memory module.) When
a new message cycle began and a new message set was presented to the router,
the router would search through the database for a combination of routes for the
message set that had no conflicts. A conflict was said to occur if more than one
route in the set of routes used a single bus in the interconnection network. In the
case where every combination of routes for the message set had a conflict, the router
would find a combination of routes that could establish the largest possible number
of desired connections.

If there are k possible routes for each message, this algorithm needs a memory of
size 8( mnk) and, in the worst case, takes exponential time with respect to the size

Neural Network Routing for Random Multistage Interconnection Networks

of the message set. Consequently, it is an impractical approach for most RMINs,
but it provides a convenient upper bound for the performance of other routers.

3

GREEDY ROUTING

When greedy routing is applied, message connections are established one at a time.
Once a route is established in a given message cycle, it may not be removed. Greedy
routing does not always provide the optimal routing solution.
The greedy routing algorithm that was used required the same route database as
the exhaustive search router did. However, it selects a combination of routes in
the following manner. When a new message set is present, the router chooses one
desired message and looks at the first route on that message's list of routes. The
router then establishes that route. Next, the router examines a second message
(assuming a second desired message was requested) and sees if one of the routes
in the second message's route list can be established without conflicting with the
already established first message. If such a route does exist, the router establishes
that route and moves on to the next desired message.
In the worst case, the speed of the greedy router is quadratic with respect to the
size of the message set.

4

NEURAL NETWORK ROUTING

The focal point of the neural network router is a neural network of the type that
was examined by Hopfield (Hopfield, 1984 and 1985). The problem of establishing
a set of non-conflicting routes can be reduced to a constraint satisfaction problem.
The structure of the neural network router is completely determined by the RMIN.
When a new set of routes is desired, only certain bias currents in the network change.
The neural network routing scheme also has certain fault-tolerant properties that
will not be described here.
The neural network calculates the routes by converging to a legal routing array. A
legal routing array is 3-dimensional. Therefore, each element of the routing array
will have three indices. If element ai,i,k is equal to 1 then message i is routed
through output port k of stage j. We say ai,;,k and a',m,n are in the same row if
i = I and k = n. They are in the same column if i = I and j = m. Finally, they are
in the same rod if j = m and k = n.
A legal routing array will satisfy the following three constraints:
1. one and only one element in each column is equal to 1.

2. the elements in successive columns that are equal to 1 represent output ports
that can be connected in the interconnection network.
3. no more than one element in each rod is equal to 1.
The first restriction ensures that each message will be routed through one and
only one output port at each stage of the interconnection network. The second
restriction ensures that each message will be routed through a legal path in the

725

726

Goudreau and Giles

interconnection network. The third restriction ensures that any resource contention
in the interconnection network is resolved. In other words, only one message can
use a certain output port at a certain stage in the interconnection network. When
all three of these constraints are met, the routing array will provide a legal route
for each message in the message set.
Like the routing array, the neural network router will naturally have a 3-dimensional
structure. Each ai,j,k of a routing array is represented by the output voltage of a
neuron, V'i,j,k' At the beginning of a message cycle, the neurons have a random
output voltage. If the neural network settles in one of the global minima, the
problem will have been solved.
A continuous time mode network was chosen. It was simulated digitally. The neural
network has N neurons. The input to neuron i is Ui, its input bias current is Ii, and
its output is Vi. The input Ui is converted to the output Vi by a sigmoid function,
g(z). Neuron i influences neuron j by a connection represented by 7ji. Similarly,
neuron j affects neuron i through connection Iij. In order for the Liapunov function
(Equation 5) to be constructed, Iij must equal7ji. We further assume that Iii = O.
For the synchronous updating model, there is also a time constant, denoted by T.
The equations which describe the output of a neuron i are:
duo
LN T... v,. + L?
-'
J
,
dt = --' +
U?

(1)

~

T

.

J=

1

T=RC

(2)

V; = g(Uj)

(3)

1

(4)

g(z) = 1 + e-X

The equations above force the neural net into stable states that are the local minima
of this approximate energy equation
iNN

E = -

2L

N

2: Iij Vi V; - L V'i Ii

i=1j=1

(5)

i=l

For the neural network, the weights (Iii's) are set, as are the bias currents (Ii'S).
It is the output voltages (V'i's) that vary to to minimize E.

Let M be the number of messages in a message set, let S be the number of stages
in the RMIN, and let P be the number of ports per stage (P may be a function
of the stage number). Below are the energy functions that implement the three
constraints discussed above:
A M 8-1 P
P
(6)
E1 = 2'
Vm",p(-Vm",p +
Vm,3,i)
B

E2

2: L 2:

2:

m=1 1=1 p=l

i=1

8-1 P

M

M

= 2' 2: 2: 2: Vm,I,p( - Vm,3,p + L
,=1 p=1 m=1

i=1

V'i,3,p)

(7)

Neural Network Routing for Random Multistage Interconnection Networks

C
Ea =

"2

M

S-l P

P

2: 2: 2:( -2Vm",p + Vm",p(-Vm",p + 2: Vm",i))
m=l

,=1 p=l

f. ~]; tt
+ &,(
M

[S-l P

D

(8)

i=l
P

d(s, p, i)Vm,,-l,p Vm",i

(9)

d( 1, (JIm, j)Vm,IJ + d( S, j, Pm )Vm,S -IJ )]

A, B, C, and D are arbitrary positive constants. l El and Ea handle the first
constraint in the routing array. E4 deals with the second constraint. E2 ensures the
third. From the equation for E4, the function d(sl,pl,p2) represents the "distance"
between output port pI from stage sl - 1 and output port p2 from stage s1. If pI
can connect to p2 through stage sl, then this distance may be set to zero. If pI
and p2 are not connected through stage sl, then the distance may be set to one.
Also, am is the source address of message m, while f3m is the destination address
of message m.

The entire energy function is:

(10)
Solving for the connection and bias current values as shown in Equation 5 results
in the following equations:

(11)
-B031 ,,2 0pl,p2(1 - Oml,m2)
-D8m1,m2[031+1,,2d(s2,pl,p2) + 8,1,,2+1 d(sl,p2,pl)]

=

1m ",p C - D[8"ld(l, am,p) + o"s-ld(S,p,f3m)]
8i,j is a Kronecker delta (8j,j = 1 when i = j, and 0 otherwise).

(12)

Essentially, this approach is promising because the neural network is acting as a
parallel computer. The hope is that the neural network will generate solutions much
faster than conventional approaches for routing in RMINs.
The neural network that is used here has the standard problem - namely, a global
minimum is not always reached. But this is not a serious difficulty. Typically,
when the globally minimal energy is not reached by the neural network, some of
the desired routes will have been calculated while others will not have. Even a
locally minimal solution may partially solve the routing problem. Consequently,
this would seem to be a particularly encouraging type of application for this type
of neural network. For this application, the traditional problem of not reaching
the global minimum may not hurt the system's performance very much, while the
expected speed of the neural network in calculating the solution will be a great
asset.
IFor the simulations, T = 1.0, A
0, and D were chosen empirically.

= 0 = D = 3.0, and B = 6.0.

These values for A, B,

727

728

Goudreau and Giles

Table 1: Routing results for the RMINs shown in Figure 2. The
calculated due to their computational complexity.
RMIN1

M
1
2
3
4
5
6
7
8

RMIN2

* entries were not

RMIN3

Eel

Egr

Enn

Eel

Egr

Enn

Eel

Egr

Enn

1.00
1.86
2.54
3.08
3.53
3.89
4.16
4.33

1.00
1.83
2.48
2.98
3.38
3.67
3.91
4.10

1.00
1.87
2.51
2.98
3.24
3.45
3.66
3.78

1.00
1.97
2.91
3.80
4.65
5.44
6.17
6.86

1.00
1.97
2.91
3.79
4.62
5.39
6.13
6.82

1.00
1.98
2.93
3.80
4.61
5.36
6.13
6.80

1.00
1.99
2.99
3.94

1.00
1.88
2.71
3.49
4.22
4.90
5.52
6.10

1.00
1.94
2.87
3.72
4.54
5.23
5.80
6.06

*
*
*
*

The neural network router uses a large number of neurons. If there are m input
ports, and m output ports for each stage of the RMIN, an upper bound on the
number of neurons needed is m 2 S. Often, however, the number of neurons actually
required is much smaller than this upper bound.
It has been shown empirically that neural networks of the type used here can con-

verge to a solution in essentially constant time. For example, this claim is made for
the neural network described in (Takefuji and Lee, 1991), which is a slight variation
of the model used here.

5

SIMULATION RESULTS

Figure 2 shows three RMINs that were examined. The routing results for the three
routing schemes are shown in Table 1. Eel represents the expected number of
messages to be routed using exhaustive search routing. Egr is for greedy routing
while Enn is for neural network routing. These values are functions of the size
of the message set, M. Only message sets that did not have obvious conflicts
were examined. For example, no message set could have two processors trying to
communicate to the same memory module. The table shows that, for at least these
three RMINs, the three routing schemes produce solutions that are of similar virtue.
In some cases, the neural network router appears to outperform the supposedly
optimal exhaustive search router. That is because the Eel and Egr values were
calculated by testing every message set of size M, while Enn was calculated by
testing 1,000 randomly generated message sets of size M. For the neural network
router to appear to perform best, it must have gotten message sets that were easier
to route than average.
In general, the performance of the neural network router degenerates as the size of
the RMIN increases. It is felt that the neural network router in its present form will
not scale well for large RMINs. This is because other work has shown that large
neural networks of the type used here have difficulty converging to a valid solution
(Hopfield, 1985).

Neural Network Routing for Random Multistage Interconnection Networks

6

CONCLUSIONS

The results show that there is not much difference, in terms of quality of solution, for
the three routing methodologies working on these relatively small sample RMINs.
The exhaustive search approach is clearly not a practical approach since it is too
time consuming. But when considering the asymptotic analyses for these three
methodologies one should keep in mind the performance degradation of the greedy
router and the neural network router as the size of the RMIN increases.
Greedy routing and neural network routing would appear to be valid approaches
for RMINs of moderate size. But since asymptotic analysis has a very limited
significance here, the best way to compare the speeds of these two routing schemes
would be to build actual implementations.
Since the neural network router essentially calculates the routes in parallel, it can
reasonably be hoped that a fast, analog implementation for the neural network
router may find solutions faster than the exhaustive search router and even the
greedy router. Thus, the neural network router may be a viable alternative for
RMIN s that are not too large.
References

Brown, T. X., (1989), "Neural networks for switching," IEEE Commun. Mag., Vol.
27, pp. 72-81, Nov. 1989.
Brown, T. X. and Liu, K. H., (1990), "Neural network design of a banyan network
controller," IEEE J. on Selected Areas of Comm., pp. 1428-1438, Oct. 1990.
Goudreau, M. W. and Giles, C. L., (1991), "Neural network routing for multiple
stage interconnection networks," Proc. IJCNN 91, Vol. II, p. A-885, July 1991.
Hakim, N. Z. and Meadows, H. E., (1990), "A neural network approach to the setup
of the Benes switch," in Infocom 90, pp. 397-402.
Hopfield, J. J., (1984), "Neurons with graded response have collective computational
properties like those of two-state neurons," Proc. Natl. Acad. Sci. USA, Vol. 81,
pp. 3088-3092, May 1984.
Hopfield, J. J ., (1985), "Neural computation on decisions in optimization problems,"
Bioi. Cybern., Vol. 52, pp. 141-152, 1985.
Huang, K. and Briggs, F. A., (1984), Computer Architecture and Parallel Processing,
McGraw-Hill, New York, 1984.
Marrakchi, A. M. and Troudet, T., (1989), "A neural net arbitrator for large crossbar packet-switches," IEEE Trans. on Cire. and Sys., Vol. 36, pp. 1039-1041, July
1989.
Siegel, H. J., (1990), Interconnection Networks for Large Scale Parallel Processing,
McGraw-Hill, New York, 1990.
Takefuji, Y. and Lee, K. C., (1991), "An artificial hysteresis binary neuron: a model
suppressing the oscillatory behaviors of neural dynamics", Biological Cybernetics,
Vol. 64, pp. 353-356, 1991.

729


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 1162-experiments-with-neural-networks-for-real-time-implementation-of-control.pdf

Experiments with Neural Networks for Real
Time Implementation of Control
P. K. Campbell, M. Dale, H. L. Ferra and A. Kowalczyk
Telstra Research Laboratories
770 Blackburn Road Clayton, Vic. 3168, Australia
{p.campbell, m.dale, h.ferra, a.kowalczyk}@trl.oz.au

Abstract
This paper describes a neural network based controller for allocating
capacity in a telecommunications network. This system was proposed in
order to overcome a "real time" response constraint. Two basic
architectures are evaluated: 1) a feedforward network-heuristic and; 2) a
feedforward network-recurrent network. These architectures are
compared against a linear programming (LP) optimiser as a benchmark.
This LP optimiser was also used as a teacher to label the data samples
for the feedforward neural network training algorithm. It is found that
the systems are able to provide a traffic throughput of 99% and 95%,
respectively, of the throughput obtained by the linear programming
solution. Once trained, the neural network based solutions are found in a
fraction of the time required by the LP optimiser.

1 Introduction
Among the many virtues of neural networks are their efficiency, in terms of both execution
time and required memory for storing a structure, and their practical ability to approximate
complex functions. A typical drawback is the usually "data hungry" training algorithm.
However, if training data can be computer generated off line, then this problem may be
overcome. In many applications the algorithm used to generate the solution may be
impractical to implement in real time. In such cases a neural network substitute can
become crucial for the feasibility of the project. This paper presents preliminary results for
a non-linear optimization problem using a neural network. The application in question is
that of capacity allocation in an optical communications network. The work in this area is
continuing and so far we have only explored a few possibilities.

2 Application: Bandwidth Allocation in SDH Networks
Synchronous Digital Hierarchy (SDH) is a new standard for digital transmission over
optical fibres [3] adopted for Australia and Europe equivalent to the SONET
(Synchronous Optical NETwork) standard in North America. The architecture of the
particular SDH network researched in this paper is shown in Figure 1 (a).
1)

Nodes at the periphery of the SDH network are switches that handle individual
calls.

P. CAMPBELL, M. DALE, H. L. FERRA, A. KOWALCZYK

974

2) Each switch concentrates traffic for another switch into a number of streams.
3)

Each stream is then transferred to a Digital Cross-Connect (DXC) for switching and
transmission to its destination by allocating to it one of several alternative virtual
paths.

The task at hand is the dynamic allocation of capacities to these virtual paths in order to
maximize SDH network throughput.
This is a non-linear optimization task since the virtual path capacities and the constraints,
i.e. the physical limit on capacity of links between DXC's, are quantized, and the objective
function (Erlang blocking) depends in a highly non-linear fashion on the allocated
capacities and demands. Such tasks can be solved 'optimally' with the use of classical
linear programming techniques [5], but such an approach is time-consuming - for large
SDH networks the task could even require hours to complete.
One of the major features of an SDH network is that it can be remotely reconfigured using
software controls. Reconfiguration of the SDH network can become necessary when
traffic demands vary, or when failures occur in the DXC's or the links connecting them.
Reconfiguration in the case of failure must be extremely fast, with a need for restoration
times under 60 ms [1].
(b)
output: path
capacities
synaptic weights
(22302)
hidden units:
'AND' gates
(l10)

thresholds
(738,67 used)
input

o

link
capacities

DXC (Digital
Cross-Connect)

?

offered
traffic

Switch

Figure 1
(a) Example of an Inter-City SDH/SONET Network Topology used in experiments.
(b) Example of an architecture of the mask perceptron generated in experiments.

In our particular case, there are three virtual paths allocated between any pair of switches,
each using a different set of links between DXC's of the SDH network. Calls from one
switch to another can be sent along any of the virtual paths, leading to 126 paths in total (7
switches to 6 other switches, each with 3 paths).
The path capacities are normally set to give a predefined throughput. This is known as the
"steady state". If links in the SDH network become partially damaged or completely cut,
the operation of the SDH network moves away from the steady state and the path
capacities must be reconfigured to satisfy the traffic demands subject to the following
constraints:
(i) Capacities have integer values (between 0 and 64 with each unit corresponding to a
2 Mb/s stream, or 30 Erlangs),
(ii) The total capacity of all virtual paths through anyone link of the SDH network

Experiments with Neural Networks for Real Time Implementation of Control

975

cannot exceed the physical capacity of that link.
The neural network training data consisted of 13 link capacities and 42 traffic demand
values, representing situations in which the operation of one or more links is degraded
(completely or partially). The output data consisted of 126 integer values representing the
difference between the steady state path capacities and the final allocated path capacities.

3 Previous Work
The problem of optimal SDH network reconfiguration has been researched already. In
particular Gopal et. al. proposed a heuristic greedy search algorithm [4] to solve this nonlinear integer programming problem. Herzberg in [5] reformulated this non-linear integer
optimization problem as a linear programming (LP) task, Herzberg and Bye in [6]
investigated application of a simplex algorithm to solve the LP problem, whilst Bye [2]
considered an application of a Hopfield neural network for this task, and finally Leckie [8]
used another set of AI inspired heuristics to solve the optimization task.
All of these approaches have practical deficiencies; the linear programming is slow, while
the heuristic approaches are relatively inaccurate and the Hopfield neural network method
(simulated on a serial computer) suffers from both problems.
In a previous paper Campbell et al. [10] investigated application of a mask perceptron to
the problem of reconfiguration for a "toy" SDH network. The work presented here
expands on the work in that paper, with the idea of using a second stage mask perceptron
in a recurrent mode to reduce link violationslunderutilizations.

4 The Neural Controller Architecture
Instead of using the neural network to solve the optimization task, e.g. as a substitute for
the simplex algorithm, it is taught to replicate the optimal LP solution provided by it.
We decided to use a two stage approach in our experiments. For the first stage we
developed a feedforward network able to produce an approximate solution. More
precisely, we used a collection of 2000 random examples for which the linear
programming solution of capacity allocations had been pre-computed to develop a
feedforward neural network able to approximate these solutions.
Then, for a new example, such an "approximate" neural network solution was rounded to
the nearest integer, to satisfy constraint (i), and used to seed the second stage providing
refinement and enforcement of constraint (ii).
For the second stage experiments we initially used a heuristic module based on the Gopal
et al. approach [4]. The heuristic firstly reduces the capacities assigned to all paths which
cause a physical capacity violation on any links, then subsequently increases the capacities
assigned to paths across links which are being under-utilized.
We also investigated an approach for the second stage which uses another feedforward
neural network. The teaching signal for the second stage neural network is the difference
between the outputs from the first stage neural network alone and the combined first stage
neural networkiheuristic solution. This time the input data consisted of 13 link usage
values (either a link violation or underutilization) and 42 values representing the amount
of traffic lost per path for the current capacity allocations. The second stage neural
network had 126 outputs representing the correction to the first stage neural network's
outputs.
The second stage neural network is run in a recurrent mode, adjusting by small steps the
currently allocated link capacities, thereby attempting to iteratively move closer to the
combined neural-heuristic solution by removing the link violations and under-utilizations
left behind by the first stage network.
The setup used during simulation is shown in Figure 2. For each particular instance tested
the network was initialised with the solution from the first stage neural network. The
offered traffic (demand) and the available maximum link capacities were used to
determine the extent of any link violations or underutilizations as well as the amount of
lost traffic (demand satisfaction). This data formed the initial input to the second stage
network. The outputs of the neural network were then used to check the quality of the

976

P. CAMPBELL, M. DALE, H. L. FERRA, A. KOWALCZYK

solution, and iteration continued until either no link violations occurred or a preset
maximum number of iterations had been performed.
offered traffic
link capacities

computation of
constraint-demand
satisfaction

[........ ~ ........-. ....
-----~(+)

!

solution (t-l)

solution (t)
correction (t)

!

I!
initialization:
solution (0)
from stage 1

demand satisfaction (t-l
42 inputs
link capacities
violation!underutilization (t-l)

13 inputs

Figure 2. Recurrent Network used for second stage experiments.
When computing the constraint satisfaction the outputs of the neural network where
combined and rounded to give integer link violations/under-utilizations. This means that
in many cases small corrections made by the network are discarded and no further
improvement is possible. In order to overcome this we introduced a scheme whereby
errors (link violations/under-utilizations) are occasionally amplified to allow the network a
chance of removing them. This scheme works as follows :
1) an instance is iterated until it has either no link violations or until 10 iterations have
been performed;
2) if any link violations are still present then the size of the errors are multiplied by an
amplification factor (> 1);
3)

a further maximum of 10 iterations are performed;

4) if subsequently link violations persist then the amplification factor is increased;
the procedure repeats until either all link violations are removed or the amplification factor
reaches some fixed value.

S Description of Neural Networks Generated
The first stage feedforward neural network is a mask perceptron [7], c.f. Figure 1 (b). Each
input is passed through a number of arbitrarily chosen binary threshold units. There were a
total of 738 thresholds for the 55 inputs. The task for the mask perceptron training
algorithm [7] is to select a set of useful thresholds and hidden units out of thousands of
possibilities and then to set weights to minimize the mean-square-error on the training set.
The mask perceptron training algorithm automatically selected 67 of these units for direct
connection to the output units and a further 110 hidden units ("AND" gates) whose

Experiments with Neural Networks for Real Time Implementation of Control

977

outputs are again connected to the neural network outputs, giving 22,302 connections in
all.
Such neural networks are very rapid to simulate since the only operations required are
comparison and additions.
For the recurrent network used in the second stage we also used a mask perceptron. The
training algori thIn used for the recurrent network was the same as for the first stage, in
particular note that no gradual adaptation was employed. The inputs to the network are
passed through 589 arbitrarily chosen binary threshold units. Of these 35 were selected by
the training algorithm for direct connection to the output units via 4410 weighted links.

6 Results
The results are presented in Table 1 and Figure 3. The values in the table represent the
traffic throughput of the SDH network, for the respective methods, as a percentage of the
throughput determined by the LP solution. Both the neural networks were trained using
2000 instances and tested against a different set of 2000 instances. However for the
recurrent network approximately 20% of these cases still had link violations after
simulation so the values in Table 1 are for the 80% of valid solutions obtained from either
the training or test set.
Solution type
Feedforward Net/Heuristic
Feedforward Net/Recurrent Net
Gopal-S
Gopal-O

Training
99.08%
94.93% (*)
96.38%
85.63%

Test
98 .90%,

94.76%(*)
96.20%
85.43%

(*) these numbers are for the 1635 training and 1608 test instances (out of 2000) for which the
recurrent network achieved a solution with no link violations after simulation as described in
Section 3.

Table 1. Efficiency of solutions measured by average fraction of the ' optimal'
throughput of the LP solution
As a comparison we implemented two solely heuristic algorithms. We refer to these as
Gopal-S and Gopal-O. Both employ the same scheme described earlier for the Gopal et al.
heuristic. The difference between the two is that Gopal-S uses the steady state solution as
an initial starting point to determine virtual path capacities for a degraded network,
whereas Gopal-O starts from a point where all path capacities are initially set to zero.
Referring to Figure 3, link capacity ratio denotes the total link capacity of the degraded
SDH network relative to the total link capacity of the steady state SDH network. A low
value of link capacity ratio indicates a heavily degraded network. The traffic throughput
ratio denotes the ratio between the throughput obtained by the method in question, and the
throughput of the steady state solution.
Each dot in the graphs in Figure 3 represents one of the 2000 test set cases. It is clear from
the figure that the neural network/heuristic approach is able to find better solutions for
heavily degraded networks than each of the other approaches. Overall the clustering of
dots for the neural network/heuristic combination is tighter (in the y-direction) and closer
to 1.00 than for any of the other methods. The results for the recurrent network are very
encouraging being qUalitatively quite close to those for the Gopal-S algorithm.
All experiments were run on a SPARCStation 20. The neural network training took a few
minutes. During simulation the neural network took an average of 9 ms per test case with
a further 36.5 ms for the heuristic, for a total of 45.5 ms. On average the Gopal-S
algorithm required 55.3 ms and the Gopal-O algorithm required 43.7 ms per test case. The
recurrent network solution required an average of 55.9 ms per test case. The optimal
solutions calculated using the linear programming algorithm took between 2 and 60
seconds per case on a SPARCStation 10.

978

P. CAMPBELL, M. DALE, H. L. FERRA, A. KOWALCZYK

Neural Network/Heuristic

Recurrent Neural Network

1.00

.2

~

0.95

8.

0.90

.r:

0>

is

0 .85

.!.!
~

0 .80

.c
t~

?? , _ ._0 ?? _ ? ?? ? ? ? ?? : ? ? ?? :.'???

0.60

0 .10

0 .80

0.90

0.70 0.50

1.00

link Capacity Ratio

1.00

ra
cr
~
.r:

0 .95

6

0.85

,g

0 .80

0 .90

"

" - -' - "~':'

.2
r.;

..... ...~ ...... -... --- -.. ..

0 .95

cr
~ 0.90
.r:

0>

0>

5

0.85

.~

0 .80

.ct~

~

~

1.00

1.00

? ? ? :? ? :? ?:,,~i~ffI~
.-. -,.

0.60 0 .70 0 .80 0.90
Link Capacity Ratio

Gopal-O

Gopal-S

.ct-

_ ?? ? ? ? _????????

0.75
0.70 0 .50

.2

:.~'?? : ? ???? :. '0""

~

0 .75
0.70 0.50

0.60

0.70

0.80

0 .90

link Capacity Ratio

1.00

0 .75

0.70 0.50

0.60 0.70 0.80 0 .90
Link Capacily Ratio

100

Figure 3. Experimental results for the Inter-City SDH network (Fig. 1) on the
independent test set of 2000 random cases. On the x axis we have the ratio
between the total link capacity of the degraded SDH network and the steady state
SDH network. On the y axis we have the ratio between the throughput obtained
by the method in question, and the throughput of the steady state solution.
Fig 3. (a) shows results for the neural network combined with the heuristic
second stage. Fig 3. (b) shows results for the recurrent neural network second
stage. Fig 3. (c) shows results for the heuristic only, initialised by the steady state
(Gopal-S) and Fig 3. (d) has the results for the heuristic initialised by zero
(Gopal-O).

7 Discussion and Conclusions
The combined neural network/heuristic approach performs very well across the whole
range of degrees of SDH network degradation tested. The results obtained in this paper are
consistent with those found in [10]. The average accuracy of -99% and fast solution
generation times ? ffJ ms) highlight this approach as a possible candidate for
implementation in a real system, especially when one considers the easily achievable
speed increase available from parallelizing the neural network. The mask perceptron used
in these experiments is well suited for simulation on a DSP (or other hardware) : the
operations required are only comparisons, calculation of logical "AND" and the
summation of synaptic weights (no multiplications or any non-linear transfonnations are
required).
The interesting thing to note is the relatively good perfonnance of the recurrent network,
namely that it is able to handle over 80% of cases achieving very good perfonnance when
compared against the neural network/heuristic solution (95% of the quality of the teacher).
One thing to bear in mind is that the heuristic approach is highly tuned to producing a
solution which satisfies the constraints, changing the capacity of one link at a time until
the desired goal is achieved. On the other hand the recurrent network is generic and does
not target the constraints in such a specific manner, making quite crude global changes in

Experiments with Neural Networks for Real Time Implementation of Control

979

one hit, and yet is still able to achieve a reasonable level of performance. While the speed
for the recurrent network was lower on average than for the heuristic solution in our
experiments, this is not a major problem since many improvements are still possible and
the results reported here are only preliminary, but serve to show what is possible. It is
planned to continue the SOH network experiment in the future; with more investigation on
the recurrent network for the second stage and also more complex SDH architectures.

Acknowledgments
The research and development reported here has the active support of various sections and
individuals within the Telstra Research Laboratories (TRL), especially Dr. C. Leckie, Mr.
P. Sember, Dr. M. Herzberg, Mr. A. Herschtal and Dr. L. Campbell. The permission of the
Managing Director, Research and Information Technology, Telstra, to publish this paper is
acknowledged.
The research and development reported here has the active support of various sections and
individuals within the Telstra Research Laboratories (TRL), especially Dr. C. Leckie and
Mr. P. Sember who were responsible for the creation and trialling of the programs
designed to produce the testing and training data.
The SOH application was possible due to co-operation of a number of our colleagues in
TRL, in particular Dr. L. Campbell (who suggested this particular application), Dr. M.
Herzberg and Mr. A. Herschtal.
The permission of the Managing Director, Research and Information Technology, Telstra,
to publish this paper is acknowledged.

References
[1]
[2]

[3]
[4]

[5]

[6]
[7]

[8]

[9]

[10]

E. Booker, Cross-connect at a Crossroads, Telephony, Vol. 215, 1988, pp. 63-65.
S. Bye, A Connectionist Approach to SDH Bandwidth Management, Proceedings
of the 19th International Conference on Artificial Neural Networks (ICANN-93),
Brighton Conference Centre, UK, 1993, pp. 286-290.
R. Gillan, Advanced Network Architectures Exploiting the Synchronous Digital
Hierarchy, Telecommunications Journal of Australia 39, 1989, pp. 39-42.
G. Gopal, C. Kim and A. Weinrib, Algorithms for Reconfigurable Networks,
Proceedings of the 13th International Teletraffic Congress (ITC-13), Copenhagen,
Denmark, 1991, pp. 341-347.
M. Herzberg, Network Bandwidth Management - A New Direction in Network
Management, Proceedings of the 6th Australian Teletraffic Research Seminar,
Wollongong, Australia, pp. 218-225.
M. Herzberg and S. Bye, Bandwidth Management in Reconfigurable Networks,
Australian Telecommunications Research 27, 1993, pp 57-70.
A. Kowalczyk and H.L. Ferra, Developing Higher Order Networks with
Empirically Selected Units, IEEE Transactions on Neural Networks, pp. 698-711,
1994.
C. Leckie, A Connectionist Approach to Telecommunication Network
Optimisation, in Complex Systems: Mechanism of Adaptation, R.J. Stonier and
X.H. Yu, eds., lOS Press, Amsterdam, 1994.
M. Schwartz, Telecommunications Networks, Addison-Wesley, Readings,
Massachusetts, 1987.
p. Campbell, H.L. Ferra, A. Kowalczyk, C. Leckie and P. Sember, Neural Networks
in Real Time Decision Making, Proceedings of the International Workshop on
Applications of Neural Networks to Telecommunications 2 (IWANNT-95), Ed. J
Alspector et. al. Lawrence Erlbaum Associates, New Jersey, 1995, pp. 273-280.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 291-dataflow-architectures-flexible-platforms-for-neural-network-simulation.pdf

818

Smotroff

Dataflow Architectures:
Flexible Platforms for
Neural Network Simulation

Ira G. Smotroff
MITRE-Bedford Neural Network Group
The MITRE Corporation
Bedford, MA 01730

ABSTRACT
Dataflow architectures are general computation engines optimized for
the execution of fme-grain parallel algorithms. Neural networks can be
simulated on these systems with certain advantages. In this paper, we
review dataflow architectures, examine neural network simulation
performance on a new generation dataflow machine, compare that
performance to other simulation alternatives, and discuss the benefits
and drawbacks of the dataflow approach.

1 DATAFLOW ARCHITECTURES
Dataflow research has been conducted at MIT (Arvind & Culler, 1986) and elsewhere
(Hiraki, et. aI., 1987) for a number of years. Dataflow architectures are general
computation engines that treat each instruction of a program as a separate task which is
scheduled in an asynchronous, data-driven fashion. Dataflow programs are compiled into
graphs which explicitly describe the data dependencies of the computation. These graphs
are directly executed by the machine. Computations which are not linked by a path in the
graphs can be executed in parallel. Each machine has a large number of processing
elements with hardware that is optimized to reduce task switching overhead to a
minimum. As each computation executes and produces a result, it causes all of the
following computations that require the result to be scheduled. In this manner, fine grain
parallel computation is achieved, with the limit on the amount of possible parallelism
determined by the problem and the number of processing elements in the machine.

Dataflow Architectures: Flexible Platforms for Neural Network Simulation

-1 -1

a
Figure 1: XOR network and its dataflow graph.
1.1 NEURAL NETWORKS & DATAFLOW
The most powerful hardware platforms for neural network simulation were enumerated
in the DARPA Neural Network Study (Lincoln Laboratory, 1988): Supercomputers offer
programming in sequential languages at great cost. Systolic Arrays such as the eMU
WARP (pomerleau, 1988) and "Massively" Parallel machines such as the Connection
Machine (Hillis, 1987), offer power at increasingly reasonable costs, but require
specialized low-level programming to map the algorithm to the hardware. Specialized
VLSI and Optical devices (Alspector, 1989) (Farhat, 1987) (Rudnick & Hammerstrom,
1989) offer fast implementations of fixed algorithms 1.
Although dataflow architectures were not included on the DARPA list, there are good
reasons for using them for neural network simulation. First, there is a natural mapping
between neural networks and the dataflow graphs used to encode dataflow programs (see
Figure 1). By expressing a neural network simulation as a dataflow program, one gains
the data synchronization and the parallel execution efficiencies that the dataflow
architecture provides at an appropriate fine grain of abstraction. The close mapping may
allow simple compilation of neural network specifications into executable programs.
Second, this ease of programming makes the approach extremely flexible, so one can get
good performance on a new algorithm the first time it is run, without having to spend
additional time determining the best way to map it onto the hardware. Thus dataflow
simulations may be particularly appropriate for those who develop new learning
algorithms or architectures. Third, high level languages are being developed for dataflow
machines, providing environments in which neural nets can be combined with standard
calculations; this can't be done with much of the specialized neural network hardware.
Last, there may be ways to optimize dataflow architectures for neural network simulation.

1 Hammerstrom's device (Rudnick & Hammerstrom, 1989) may be micro-programmable.

819

820

Smotroff

wait
match
J

from

netwo(''k

,'"

~

~ ALU ~

'W ,

"""

h

instruction
fetch

~

~
~

form
tag

--'"

form
token

to
network

~

oJ

"'
\
IJ

"'

structure .oJ
......
memory

Figure 2: Schematic of a tagged-token dataflow processor.

2 TAGGED-TOKEN DATAFLOW
The Tagged-token dataflow approach represents each computation product as a token
which is passed to following computations. A schematic view of a tagged-token
processor is shown in Figure 2. Execution proceeds in a Wait-Match-Store cycle which
achieves data synchronization. An instruction to be executed waits in the wait-match
queue for a token with its operand. If a match occurs, the incoming token contains its
operand and one of two things happens: for a monadic operation, the instruction is
executed and the result is passed on; for a dyadic operation, a check is made to see if the
operand is the first or the second one to arrive. If it's the first, the location representing
the instruction is tagged, the operand is stored, and the instruction continues to wait. If
it's the second (Le. the instruction is tagged already) the instruction is executed and a
token containing the result is sent to all computations requiring the result. A schematic
view of the execution of the XOR network of Figure 1 on a tagged-token dataflow
machine is illustrated in Figure 3.
2.1 SPLIT-PHASE TRANSACTIONS
In fine-grain parallel computations distributed over a number of physical devices, the
large number of network transactions represent a potential bottleneck. The tagged-token
dataflow architecture mitigates this problem in a way that enhances the overall parallel
execution time. Each network transaction is split into two phases. A process requests an
external data value and then goes to sleep. When the token bearing the requested value
returns, the process is awakened and the computation proceeds. In standard approaches, a
processor must idle while it waits for a result. This non-blocking approach allows other
computations to proceed while the value is in transit, thus masking memory and network
latencies. Independent threads of computation may be interwoven at each cycle, thus
allowing the maximum amount of parallel execution at each cycle. As long as the amount
of parallelism in the task (Le. the length of each processor's task queue) is larger than the
network latency, the processors never idle. Consequently, massively parallel applications
such as neural simulations benefit most from the split-phase transaction approach.

Dataflow Architectures: Flexible Platforms for Neural Network Simulation

3 NEURAL NETWORK DATAFLOW SIMULATION
To illustrate neural network execution on a dataflow processor, the XOR network in
Figure 1 was coded in the dataflow language ID (Nikhil, 1988) and run on the MIT GITA
(Q.raph Interpreter for Tagged-token Architecture) simulator (Nikhil, 1988). Figures 4-6
are ALU operations profiles with the vertical axis representing the number of processors
that could be simultaneously kept busy (i.e. the amount of parallelism in the task at a
particular instance) and the horizontal axis representing elapsed computation cycles. In
addition, Figures 4 & 5 are ideal simulations with communication latency of zero time
and an infinite number of processors available at all times. The ideal profile width
represents the absolute minimum time in which the dataflow calculation could possibly
be performed, and is termed the critical path. Figure 4 shows the execution profile for a
single linear threshold neuron processing its two inputs. The initial peak: activity of eleven
corresponds to initialization activities, with later peaks corresonding to actual
computation steps. The complexity of the profile may be attributed to various dataflow
synchronization mechanisms. In figure 5, the ideal execution profile for the XOR net,
note the initialization peak: similar to the one appearing in the single neuron profile; the
peak parallelism of fifty-five corresponds to all five neuron initializations occuring
simultaneously. This illustrates the ability of the dataflow approach to automatically
expose the inherent parallelism in the overall computation. Note also that the critical path
of one hundred fifty one is substantially less than five times the single neuron critical path
of eighty-five. Wherever possible, the dataflow approach has performed computation in
parallel, and the lengthening of the critical path can be attributed to those computations
which had to be delayed until prior computations became available.
Figure 6 represents the execution of the same XOR net under more realistic conditions in
which each token operation is subject to a finite network delay. The regular spacing of the
profile corresponds to the effect of the network delays. The interesting thing to observe is
that the overall critical path length has only increased slightly to one hundred seventy
because the average amount of parallelism available as tokens come in from the net is
higher. Dataflow's ability to interleave computations thus compensates for much of the
network latency effects.

I SS

ZQQ

Figure 4: Ideal parallelism profile for dataflow execution - single threshold neuron unit.

821

822

Smotroff

1

Figure 3: Execution of the XOR network of Figure 1 on a tagged-token
dataflow processor. The black dots represent active tokens, the white dots
represent waiting tokens, and the shaded boxes represent enabled operations
executing.

1

Dataflow Architectures: Flexible Platforms for Neural Network Simulation

1

55

s91

I
38

299

Figure 5: Ideal parallelism profile for dataflow execution of XOR network.

"1j

_a

I

39

29

la

9~~~~~~~~~~~~~--

9

19a

_ _ _ _ _ _ _ __
289

Figure 6: Parallelism profile for dataflow execution of XOR with constant
communication latency.
3.1 COST OF THE DATAFLOW APPROACH
The Tagged-Token Dataflow machine executing an ID program performs two to three
times as many instructions as an IBM 370 executing an equivalent FORTRAN program.
The overhead in dataflow programs is attributable to mechanisms which manage the
asynchronous parallel execution. Similar overhead would probably exist in specialized
neural network simulators written for dataflow machines. However, this overhead can be
justified because the maximum amount of parallelism in the computation is exposed in a
straightforward manner, which requires no additional programming effort. On
conventional multiprocessors, parallelism must be selectively tailored for each problem.
As the amount of parallelism increases, the associated costs increase as well; often they
will eventually surpass the cost of dataflow (Arvind ,Culler & Ekanadham, 1988). Thus
the parallel performance on the dataflow machine will often surpass that of alternative
platforms despite the overhead.

823

824

Smotroff

4 THE MONSOON ARCHITECTURE
Early dataflow implementations using a Tagged Token approach had a number of
practical barriers (papadoupoulos, 1988). While useful results were achieved, the cost and
expansion limits of the associative memory used for token matching made them
impractical. However, the systems did prove the utility of the Tagged Token approach ..
Recently, the MONSOON architecture (papadoupoulos, 1988) was developed to remedy
the problems encountered with Tagged Token architectures. The token-matching problem
has been solved by treating each token descriptor as an address in a global memory space
which is partitioned among the processors in the system; matching becomes a simple
RAM operation.
An initial MONSOON prototype has been constructed and a 8 processor machine is
scheduled to be built in 1990. Processor elements for that machine are CMOS gate-array
implementations being fabricated by Motorola. Each processor board will have a 100 ns
cycle time and process at a rate of 7-8 MIPS!2-4 MFLOPS. Total memory for the 8
processor machine is 256 MBytes. Interconnect is provided by a 100 MByte!s packet
switch network. The throughput of the 8 processor machine is estimated at 56-64 MIPS!
16-32 MFLOPs. This translates to 2-3 million connections per second per processor and
16-24 million connections per second for the machine. Monsoon performance is in the
supercomputer class while the projected Monsoon cost is significantly less due to the use
of standard process technologies.
A 256 processor machine with CMOS VLSI processors is envisioned. Estimated
performance is 40 MIPS per processor and 10,240 MIPS for the machine. Aggregate
neural simulation performance is estimated at 2.5-3.8 billion connections per second,
assuming an interconnect network of suitable performance.

5 CONCLUSIONS
i)

Dataflow architectures should be cost effective and flexible
platforms for neural network simulation if they become widely
available.

ii)

As general architectures. their performance will not exceed that of
specialized neural network architectures.

iii) Maximum parallelism is attained simply by using the dataflow
approach: no machine or problem-specific tuning is needed. Thus
dataflow is seen as an excellent tool for empirical simulation.
Excellent performance may be obtained on cost effective hardware,
with no special effort required for performance improvement.

iv) Dataflow architectures optimized for neural network simulation
performance may be possible.
References
Alspector, J .? Gupta, B. and Allen, R. B. (1989) Performance of a Stochastic Learning
Microchip. In D. S. Touretzky (ed.). Advances in Neural Information Processing Systems
1, 748-760. San Mateo, CA: Morgan Kaufmann.

Dataflow Architectures: Flexible Platforms for Neural Network Simulation

Arvind and Culler, D. E .. (1986) Dataflow Architectures, MIT Technical Report
MIT/LCS/fM-294, Cambridge, MA.
Arvind, Culler, D. E., Ekanadham, K. (1988) The Price of Asynchronous Parallelism: An
Analysis of Dataflow Architectures. MIT Laboratory for Computer Science, Computation
Structures Group Memo 278.
DARPA Neural Network Study (1988) Lincoln Laboratory, MIT, Lexington, MA.
Farhat, N.H., and Shai, Z. Y.(1987) Architectures and Methodologies for SelfOrganization and Stochastic Learning in Opto-Electronic Analogs of Neural Nets. In
Proceedings of IEEE First International Conference on Neural Networks, ill:565-576.
Hillis, W. D.(1986) The Connection Machine, Cambridge, MA: The MIT Press.
Hiraki, K., Sekiguchi, S. and Shimada, T. (1987) System Architecture of a Dataflow
Supercomputer. Technical Report, Computer Systems Division, Electrotechnical
Laboratory, 1-1-4 Umezono, Sakura-mura, Niihari-gun, lbaraki, 305, Japan.
Nikhil, R. S. (1988) Id World Reference Manual, Computational Structures Group, MIT
Laboratory for Computer Science, Cambridge, MA.
Pomerleau, D. A., Gusciora, G. L., Touretsky and D. S., Kung, H. T.(1988) Neural
Simulation at Warp Speed: How we got 17 Million Connections per Second. In
Proceedings of the IEEE International Conference on Neural Networks, II: 143-150, San
Diego.
Papadoupoulos, G. M. (1988) Implementation of a General Purpose Dataflow
Multiprocessor, Phd. Thesis, MIT Department of Electrical Engineering and Computer
Science, Cambridge, MA.
Rudnick, M. and Hammerstrom, D.(1989) An Interconnection Structure for Wafer Scale
Neurocomputers. In Proceedings of the 1988 Connectionist Models Summer School. San
Mateo, CA: Morgan Kaufmann.

825

PART X:
HISTORY OF NEURAL NETWORKS


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 1193-a-new-approach-to-hybrid-hmmann-speech-recognition-using-mutual-information-neural-networks.pdf

A New Approach to Hybrid HMMJANN Speech
Recognition Using Mutual Information Neural
Networks
G. Rigoll,

c.

Neukirchen

Gerhard-Mercator-University Duisburg
Faculty of Electrical Engineering
Department of Computer Science
Bismarckstr. 90, Duisburg, Germany

ABSTRACT
This paper presents a new approach to speech recognition with hybrid
HMM/ANN technology. While the standard approach to hybrid
HMMIANN systems is based on the use of neural networks as
posterior probability estimators, the new approach is based on the use
of mutual information neural networks trained with a special learning
algorithm in order to maximize the mutual information between the
input classes of the network and its resulting sequence of firing output
neurons during training. It is shown in this paper that such a neural
network is an optimal neural vector quantizer for a discrete hidden
Markov model system trained on Maximum Likelihood principles.
One of the main advantages of this approach is the fact, that such
neural networks can be easily combined with HMM's of any
complexity with context-dependent capabilities. It is shown that the
resulting hybrid system achieves very high recognition rates, which
are now already on the same level as the best conventional HMM
systems with continuous parameters, and the capabilities of the
mutual information neural networks are not yet entirely exploited.

1 INTRODUCTION
Hybrid HMM/ANN systems deal with the optimal combination of artificial neural
networks (ANN) and hidden Markov models (HMM). Especially in the area of automatic
speech recognition, it has been shown that hybrid approaches can lead to very powerful
and efficient systems, combining the discriminative capabilities of neural networks and
the superior dynamic time warping abilities of HMM's. The most popular hybrid
approach is described in (Hochberg, 1995) and replaces the component modeling the
emission probabilities of the HMM by a neural net. This is possible, because it is shown

Mutual In/ormation Neural Networks/or Hybrid HMMIANN Speech Recognition

773

in (Bourlard, 1994) that neural networks can be trained so that the output of the m-th
neuron approximates the posterior probability p(QmIX). In this paper, an alternative
method for constructing a hybrid system is presented. It is based on the use of discrete
HMM's which are combined with a neural vector quantizer (VQ) in order to form a hybrid
system. Each speech feature vector is presented to the neural network, which generates a
firing neuron in its output layer. This neuron is processed as VQ label by the HMM's.
There are the following arguments for this alternative hybrid approach:
? The neural vector quantizer has to be trained on a special information theory criterion,
based on the mutual information between network input and resulting neuron firing
sequence. It will be shown that such a network is the optimal acoustic processor for a
discrete HMM system, resulting in a profound mathematical theory for this approach.
? Resulting from this theory, a formula can be derived which jointly describes the
behavior of the HMM and the neural acoustic processor. In that way, both systems can
be described in a unified manner and both major components of the hybrid system can
be trained using a unified learning criterion.
? The above mentioned theoretical background leads to the development of new neural
network paradigms using novel training algorithms that have not been used before in
other areas of neurocomputing, and therefore represent major challenges and issues in
learning and training for neural systems.
? The neural networks can be easily combined with any HMM system of arbitrary
complexity. This leads to the combination of optimally trained neural networks with
very powerful HMM's, having all features useful for speech recognition, e.g. triphones,
function words, crossword triphones, etc .. Context-dependency, which is very desirable
but relatively difficult to realize with a pure neural approach, can be left to the HMM's.
? The resulting hybrid system has still the basic structure of a discrete system, and
therefore has all the effective features associated with discrete systems, e.g. quick and
easy training as well as recognition procedures, real-time capabilities, etc ..
? The work presented in this paper has been also successfully implemented for a
demanding speech recognition problem, the 1000 word speaker-independent continuous
Resource Management speech recognition task. For this task, the hybrid system
produces one of the best recognition results obtained by any speech recognition system.
In the following section, the theoretical foundations of the hybrid approach are briefly
explained. A unified probabilistic model for the combined HMMIANN system is derived,
describing the interaction of the neural and the HMM component. Furthermore, it is
shown that the optimal neural acoustic processor can be obtained from a special
information theoretic network training algorithm.

2 INFORMATION THEORY PRINCIPLES FOR NEURAL
NETWORK TRAINING
We are considering now a neural network of arbitrary topology used as neural vector
quantizer for a discrete HMM system. If K patterns are presented to the hybrid system
during training, the feature vectors resulting from these patterns using any feature
extraction method can be denoted as x(k), k=l.. .K. If these feature vectors are presented to
the input layer of a neural network, the network will generate one firing neuron for each
presentation. Hence, all K presentations will generate a stream of firing neurons with
length K resulting from the output layer of the neural net. This label stream is denoted as
Y=y(l) ... y(K). The label stream Y will be presented to the HMM's, which calculate the
probability that this stream has been observed while a pattern of a certain class has been
presented to the system. It is assumed, that M different classes Q m are active in the

G. Rigoll and C. Neukirchen

774

system, e.g. the words or phonemes in speech recognition. Each feature vector ~(k) will
belong to one of these classes. The class Om, to which feature vector ~(k) belongs is
denoted as Q(k). The major training issue for the neural network can be now formulated
as follows : How should the weights of the network be trained, so that the network
produces a stream of firing neurons that can be used by the discrete HMM's in an optimal
way? It is known that HMM's are usually trained with information theory methods which
mostly rely on the Maximum Likelihood (ML) principle. If the parameters of the hybrid
system (i.e. transition and emission probabilities and network weights) are summarized in
the vector !!, the probability P!!(x(k)IQ(k? denotes the probability of the pattern X at
discrete time k, under the assumption that it has been generated by the model representing
class O(k), with parameter set !!. The ML principle will then try to maximize the joint
probability of all presented training patterns ~(k), according to the following Maximum
Likelihood function:
fl* = arg max
~

{~ i

log P!! (K(k) I Q(k?j
(1)

k=1

where !!* is the optimal parameter vector maximizing this equation. Our goal is to feed
the feature vector ~ into a neural network and to present the neural network output to the
Markov model. Therefore, one has to introduce the neural network output in a suitable
manner into the above formula. If the vector ~ is presented to the network input layer, and
we assume that there is a chance that any neuron Yn, n=1...N (with network output layer
size N) can fire with a certain probability, then the output probability p(~IQ) in (1) can
be written as:
N
N
p(KIQ) =
p(x ,Y n IQ) =
p(y n IQ) . p(x Iy n,Q)
(2)
n=1
n=1
Now, the combination of the neural component with the HMM can be made more
obvious: In (2), typically the probability P(YnIQ) will be described by the Markov model,
in terms of the emission probabilities of the HMM . For instance, in continuous
parameter HMM's, these probabilities are interpreted as weights for Gaussian mixtures. In
the case of semi-continuous systems or discrete HMM's, these probabilities will serve as
discrete emission probabilities of the codebook labels. The probability p(xIYn,Q)
describes the acoustic processor of the system and is characterizing the relation between
the vector ~ as input to the acoustic processor and the label Yn, which can be considered
as the n-th output component of the acoustic processor. This n-th output component may
characterize e.g. the n-th Gaussian mixture component in continuous parameter HMM's,
or the generation of the n-th label of a vector quantizer in a discrete system. This
probability is often considered as independent of the class 0 and can then be expressed as
p(xIYn). It is exactly this probability, that can be modeled efficiently by our neural
network. In this case, the vector X serves as input to the neural network and Yn
characterizes the n-th neuron in the output layer of the network. Using Bayes law, this
probability can be written as:
P(YnIK) ' pW
p(xl Yn) =
p(y n)

I

I

(3)

yielding for (2):

(4)
Using again Bayes law to express

Mutual Information Neural Networks for Hybrid HMMIANN Speech Recognition

775

(5)

one obtains from (4):
p(K)
N
p(KI.Q)= -(.Q) . L p(.Qly n ) ?p(ynlo!J
p
n=1
(6)
We have now modified the class-dependent probability of the feature vector X in a way
that allows the incorporation of the probability P(YnIX). This probability allows a better
characterization of the behavior of the neural network, because it describes the probability
of the various neurons Yn, if the vector X is presented to the network input. Therefore,
these probabilities give a good description of the input/output behavior of the neural
network. Eq. (6) can therefore be considered as probabilistic model for the hybrid system,
where the neural acoustic processor is characterized by its input/output behavior. Two
cases can be now distinguished: In the first case, the neural network is assumed to be a
probabilistic paradigm, where each neuron fires with a certain probability, if an input
vector is presented. In this case all neurons contribute to the information forwarded to the
HMM's. As already mentioned, in this paper, the second possible case is considered,
namely that only one neuron in the output layer fires and will be fed as observed label to
the HMM. In this case, we have a deterministic decision, and the probability P(YnIX)
describes what neuron Yn* fires if vector X is presented to the input layer. Therefore, this
probability reduces to
(7)

Then, (6) yields:
(8)

Now, the class-dependent probability p(Xln) is expressed through the probability
p(nIYn*), involving directly the firing neuron Yn*, when feature vector X is presented.
One has now to turn back to (1), recalling the fact, that this equation describes the fact
that the Markov models are trained with the ML criterion. It should also be recalled, that
the entire sequence of feature vectors, x(k), k=l...K, results in a label stream of firing
neurons Yn*(k), k=l...K, where Yn*(k) is the firing neuron if the k-th vector x(k) is
presented to the neural network. Now, (8) can be substituted into (1) for each presentation
k, yielding the modified ML criterion:

1( =

arg;ax {

::1
K

p(x(k))
}
log P(Q (k)) . p(.Q(k) I Y n*,k))

~ arg;ax {~, log p(x (k)) - ~109P(Q(k)) + ~IOg p(Q(k) IYn.(k))}

(9)

Usually, in a continuous parameter system, the probability p(x) can be expressed as:
N

p(K)

=

LP(K,ly n) . p(y n)
n=1

(10)

and is therefore dependent of the parameter vector ft, because in this case, p(xIYn) can be
interpreted as the probability provided by the Gaussian distributions, and the parameters of

G. Rigoll and C. Neukirchen

776

the Gaussians will depend on ft. As just mentioned before, in a discrete system, only one
firing neuron Yn* survives, resulting in the fact that only the n*-th member remains in
the sum in (10). This would correspond to only one "firing Gaussian" in the continuous
case, leading to the following expression for p(x):
p(K)

= p(x Iy nJ? p(y nJ = p(K,y nJ = p(y n"lx)

. p(x)

(11)

Considering now the fact, that the acoustic processor is not represented by a Gaussian but
instead by a vector quantizer, where the probability P(Yn*IX) of the firing neuron is equal
to 1, then (11) reduces to p(~) =p(x) and it becomes obvious that this probability is not
affected by any distribution that depends on the parameter vector ft. This would be
different, if P(Yn*IX) in (11) would not have binary characteristics as in (7), but would be
computed by a continuous function which in this case would depend on the parameter
vector ft. Thus, without consideration of p(X), the remaining expression to be maximized
in (9) reduces to:

,r( =arg;ax

[~ ~IOg p(.Q( k)) +

= arg max [-

!

1

log p(.Q( k) I Y n?(k))

(12)

E {log p(.o)} + E {log p(.o I y n")}]

fJ..
These expectations of logarithmic probabilities are also defined as entropies. Therefore,
(9) can be also written as
fl." = arg max {H (.0) - H(.o I Y)}

fJ..

(13)

This equation can be interpreted as follows: The term on the right side of (13) is also
known as the mutual information I(n,Y~ between the probabilistic variables nand Y,
i.e. :
1(.0, Y) =H(.o) - H (.01 Y) =H (Y) - H(YI.o)
(14)
Therefore, the final information theory-based training criterion for the neural network can
be formulated as follows: The synaptic weights of the neural network should be chosen as
to maximize the mutual information between the string representing the classes of the
vectors presented to the network input layer during training and the string representing the
resulting sequence of firing neurons in the output layer of the neural network. This can be
also expressed as the Maximum Mutual Information (MMI) criterion for neural network
training. This concludes the proof that MMI neural networks are indeed optimal acoustic
processors for HMM's trained with maximum likelihood principles.

3 REALIZATION OF MMI TRAINING ALGORITHMS FOR
NEURAL NETWORKS
Training the synaptic weights of a neural network in order to achieve mutual information
maximization is not easy. Two different algorithms have been developed for this task and
can only be briefly outlined in this paper. A detailed description can be found in (Rigoll,
1994) and (Neukirchen, 1996). The first experiments used a single-layer neural network
with Euclidean distance as propagation function. The first implementation of the MMI
training paradigm has been realized in (Rigoll, 1994) and is based on a self-organizing
procedure, starting with initial weights derived from k-means clustering of the training
vectors, followed by an iterative procedure to modify the weights. The mutual
information increases in a self-organizing way from a low value at the start to a much
higher value after several iteration cycles. The second implementation has been realized

Mutual Information Neural Networks for Hybrid HMMIANN Speech Recognition

777

recently and is described in detail in (Neukirchen, 1996). It is based on the idea of using
gradient methods for finding the MMI value. This technique has not been used before,
because the maximum search for finding the firing neuron in the output layer has
prevented the calculation of derivatives. This maximum search can be approximated using
the softmax function, denoted as sn for the n-th neuron. It can be computed from the
activations Zl of all neurons as:
z IT

Sn=e n

N

""

/ ?..Je

ZI

IT

/=1
(15)
where a small value for parameter T approximates a crisp maximum selection. Since the
string n in (14) is always fixed during training and independent of the parameters in ft,
only the function H(nIY) has to be minimized. This function can also be expressed as

M

H(!2 I Y)

=-

N

L L

p(y n,!2 m ) ?logp(!2 m I Y n)

m=1 n=1

m=1 n=1

(16)

A derivative with respect to a weight Wlj of the neural network yields:
aH (!21 Y)
JW/j

=
(17)

As shown in (Neukirchen, 1996), all the required terms in (17) can be computed
effectively and it is possible to realize a gradient descend method in order to maximize the
mutual information of the training data. The great advantage of this method is the fact
that it is now possible to generalize this algorithm for use in all popular neural network
architectures, including multilayer and recurrent neural networks.

4 RESULTS FOR THE HYBRID SYSTEM
The new hybrid system has been developed and extensively tested using the Resource
Management 1000 word speaker-independent continuous speech recognition task. First, a
baseline discrete HMM system has been built up with all well-known features of a
context-dependent HMM system. The performance of that baseline system is shown in
column 2 of Table 1. The 1st column shows the performance of the hybrid system with
the neural vector quantizer. This network has some special features not mentioned in the
previous sections, e.g. it uses multiple frame input and has been trained on contextdependent classes. That means that the mutual information between the stream of firing
neurons and the corresponding input stream of triphones has been maximized. In this
way, the firing behavior of the network becomes sensitive to context-dependent units.
Therefore, this network may be the only existing context-dependent acoustic processor,
carrying the principle of triphone modeling from the HMM structure to the acoustic front
end. It can be seen, that a substantially higher recognition performance is obtained with
the hybrid system, that compares well with the leading continuous system (HTK, in
column 3). It is expected, that the system will be further improved in the near future
through various additional features, including full exploitation of multilayer neural VQ's

778

G. Rigoll and C. Neukirchen

and several conventional HMM improvements, e.g. the use of crossword triphones.
Recent results on the larger Wall Street Journal (WSJ) database have shown a 10.5% error
rate for the hybrid system compared to a 13.4% error rate for a standard discrete system,
using the 5k vocabulary test with bigram language model of perplexity 110. This error
rate can be further reduced to 8.9% using crossword triphones and 6.6% with a trigram
language model. This rate compares already quite favorably with the best continuous
systems for the same task. It should be noted that this hybrid WSJ system is still in its
initial stage and the neural component is not yet as sophisticated as in the RM system.

5 CONCLUSION
A new neural network paradigm and the resulting hybrid HMMIANN speech recognition
system have been presented in this paper. The new approach performs already very well
and is still perfectible. It gains its good performance from the following facts: (1) The use
of information theory-based training algorithms for the neural vector quantizer, which can
be shown to be optimal for the hybrid approach. (2) The possibility of introducing
context-dependency not only to the HMM's, but also to the neural quantizer. (3) The fact
that this hybrid approach allows the combination of an optimal neural acoustic processor
with the most advanced context-dependent HMM system. We will continue to further
implement various possible improvements for our hybrid speech recognition system.

REFERENCES
Rigoll, G. (1994) Maximum Mutual Information Neural Networks for Hybrid
Connectionist-HMM Speech Recognition Systems, IEEE Transactions on Speech and
Audio Processing, Vol. 2, No.1, Special Issue on Neural Networks for Speech
Processing, pp. 175-184
Neukirchen, C. & Rigoll, G. (1996) Training of MMI Neural Networks as Vector
Quantizers, Internal Report, Gerhard-Mercator-University Duisburg, Faculty of Electrical
Engineering, available via http://www.fb9-tLuni-duisburg.de/veroeffentl.html
Bourlard, H. & Morgan, N. (1994) Connectionist Speech Recognition: A Hybrid
Approach, Kluwer Academic Publishers
Hochberg, M., Renals, S., Robinson, A., Cook, G. (1995) Recent Improvements to the
ABBOT Large Vocabulary CSR System, in Proc. IEEE-ICASSP, Detroit, pp. 69-72
Rigoll, G., Neukirchen, c., Rottland, J. (1996) A New Hybrid System Based on MMINeural Networks for the RM Speech Recognition Task, in Proc. IEEE-ICASSP, Atlanta
Table 1: Comparison of recognition rates for different speech recognition systems
RM SI word recognition rate with word pair grammar: correctness (accuracy)
test set

hybrid MMI-NN
system

baseline k-means
VQ system

continuous pdf system
(HTK)

Feb.'89

96,3 %

(95,6 %)

94,3 % (93,6 %)

96,0 % (95,5 %)

Oct.'89

95,4 %

(94,5 %)

93,5 % (92,0 %)

95,4% (94,9 %)

Feb.'91

96,7 %

(95,9 %)

94,4% (93,5 %)

96,6% (96,0 %)

Sep.'92

93,9 %

(92,5 %)

90,7 % (88,9 %)

93,6 % (92,6 %)

average

95,6 %

(94,6 %)

93,2 % (92,0 %)

95,4% (94,7 %)


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 657-optimal-depth-neural-networks-for-multiplication-and-related-problems.pdf

Optimal Depth Neural Networks for Multiplication
and Related Problems

Kai-Yeung Siu
Dept. of Electrical & Compo Engineering
University of California, Irvine
Irvine, CA 92717

Vwani Roychowdhury
School of Electrical Engineering
Purdue University
West Lafayette, IN 47907

Abstract
An artificial neural network (ANN) is commonly modeled by a threshold
circuit, a network of interconnected processing units called linear threshold
gates. The depth of a network represents the number of unit delays or the
time for parallel computation. The SIze of a circuit is the number of gates
and measures the amount of hardware . It was known that traditional logic
circuits consisting of only unbounded fan-in AND, OR, NOT gates would
require at least O(log n/log log n) depth to compute common arithmetic
functions such as the product or the quotient of two n-bit numbers, unless
we allow the size (and fan-in) to increase exponentially (in n). We show in
this paper that ANNs can be much more powerful than traditional logic
circuits. In particular, we prove that that iterated addition can be computed by depth-2 ANN, and multiplication and division can be computed
by depth-3 ANNs with polynomial size and polynomially bounded integer
weights, respectively. Moreover, it follows from known lower bound results that these ANNs are optimal in depth. We also indicate that these
techniques can be applied to construct polynomial-size depth-3 ANN for
powering, and depth-4 ANN for mUltiple product.

1

Introduction

Recent interest in the application of artificial neural networks [10, 11] has spurred
research interest in the theoretical study of such networks. In most models of neural networks, the basic processing unit is a Boolean gate that computes a linear
59

60

Siu and Roychowdhury
threshold function, or an analog element that computes a sigmoidal function. Artificial neural networks can be viewed as circuits of these processing units which are
massively interconnected together.
While neural networks have found wide application in many areas, the behavior
and the limitation of these networks are far from being understood. One common
model of a neural network is a threshold circuit. Incidentally, the study of threshold
circuits, motivated by some other complexity theoretic issues, has also gained much
interest in the area of computer science. Threshold circuits are Boolean circuits in
which each gate computes a linear threshold function, whereas in the classical model
of unbounded fan-in Boolean circuits only AND, OR, NOT gates are allowed. A
Boolean circuit is usually arranged in layers such that all gates in the same layer are
computed concurrently and the circuit is computed layer by layer in some increasing
depth order. We define the depth as the number of layers in the circuit. Thus each
layer represents a unit delay and the depth represents the overall delay in the
computation of the circuit .

2

Related Work

Theoretical computer scientists have used unbounded fan-in Boolean circuits as
a model to understand fundamental issues of parallel computation. To be more
specific, this computational model should be referred to as unbounded fan-in parallelism, since the number of inputs to each gate in the Boolean circuit is not bounded
by a constant. The theoretical study of unbounded fan-in parallelism may give us
insights into devising faster algorithms for various computational problems than
would be possible with bounded fan-in parallelism. In fact, any nondegenerate
Boolean function of n variables requires at least O(log n) depth to compute in a
bounded fan-in circuit. On the other hand, in some practical situations, (for example large fan-in circuits such as programmable logic arrays (PLAs) or multiple
processors simultaneously accessing a shared bus), unbounded fan-in parallelism
seems to be a natural model. For example, a PLA can be considered as a depth-2
AND/OR circuit.
In the Boolean circuit model, the amount of resources is usually measured by the
number of gates, and is considered to be 'reasonable' as long as it is bounded
by a polynomial (as opposed to exponential) in the number of the inputs. For
example, a Boolean circuit for computing the sum of two n-bit numbers with O(n 3 )
gates is 'reasonable', though circuit designers might consider the size of the circuit
impractical for moderately large n. One of the most important theoretical issues in
parallel computation is the following: Given that the number of gates in the Boolean
circuit is bounded by a polynomial in the size of inputs, what is the minimum depth
(i.e. number of layers) that is needed to compute certain functions?

A first step toward answering this important question was taken by Furst et al. [4]
and independently by Ajtai [2]. It follows from their results that for many basic
functions, such as the parity and the majority of n Boolean variables, or the multiplication of two n-bit numbers, any constant depth (i. e. independent of n) classical
Boolean circuit of unbounded fan-in AND/OR gates computing these functions
must have more than a polynomial (in n) number of gates. This lower bound on
the size was subsequently improved by Yao [18] and Hastad [7]; it was proved that

Optimal Depth Neural Networks for Multiplication and Related Problems

indeed an exponential number of AND/OR gates are needed. So functions such as
parity and majority are computationally 'hard' with respect to constant depth and
polynomial size classical Boolean circuits. Another way of interpreting these results
is that circuits of AND/OR gates computing these 'hard' functions which use polynomial amount of chip area must have unbounded delay (i. e. delay that increases
with n). In fact, the lower bound results imply that the minimum possible delay
for multipliers (with polynomial number of AND/OR gates) is O(logn/loglogn).
These results also give theoretical justification why it is impossible for circuit designers to implement fast parity circuit or multiplier in small chip area using AND,
OR gates as the basic building blocks.
One of the 'hard' functions mentioned above is the majority function, a special case
of a threshold function in which the weights or parameters are restricted. A natural
extension is to study Boolean circuits that contain majority gates. This type of
Boolean circuit is called a threshold circuit and is believed to capture some aspects
of the computation in our brain [12]. In the rest of the paper, the term 'neural
networks' refers to the threshold circuits model.
With the addition of majority gates, the resulting Boolean circuit model seems
much more powerful than the classical one. Indeed, it was first shown by Muroga
[13] three decades ago that any symmetric Boolean function (e.g. parity) can be
computed by a two-layer neural network with (n + 1) gates. Recently, Chandra
et al. [3] showed that multiplication of two n-bit numbers and sorting of n n-bit
numbers can be computed by neural networks with 'constant' depth and polynomial
size. These 'constants' have been significantly reduced by Siu and Bruck [14, 15] to
4 in both cases, whereas a lower bound of depth-3 was proved by Hajnal et al. [6]
in the case of multiplication. It is now known [8] that the size of the depth-4 neural
networks for multiplication can be reduced to O(n 2 ). However, the existence of
depth-3 and polynomial-size neural networks for multiplication was left as an open
problem [6, 5, 15] since the lower bound result in [6]. In [16], some depth-efficient
neural networks were constructed for division and related arithmetic problems; the
networks in [16] do not have optimal depth.
Our main contribution in this paper is to show that small constant depth neural
networks for multiplication, division and related problems can be constructed. For
the problems such as iterated addition, multiplication, and division, the neural networks constructed can be shown to have optimal depth. These results have the
following implication on their practical significance: Suppose we can use analog devices to build threshold gates with a cost (in terms of delay and chip area) that is
comparable to that of AND, OR, logic gates, then we can compute many basic functions much faster than using traditional circuits. Clearly, the particular weighting

of depth, fan-in, and size that gives a realistic measure of a network's cost and speed
depends on the technology used to build it. One case where circuit depth would
seem to be the most important parameter is when the circuit is implemented using
optical devices. We refer those who are interested in the optical implementation of
neural networks to [1].
Due to space limitations, we shall only state some of the important results; further
results and detailed proofs will appear in the journal version of this paper [17].

61

62

Siu and Roychowdhury

3

Main Results

Definition 1
Given n n-bit integers, Zi = Lj~; zi,i2i, i = 1, ... , n, zi,i E {O, I},
We define iterated addition to be the problem of computing the (n + log n )-bit sum
L~=l Zi of the n integers.

=

=

Definition 2
Given 2 n-bit integers, x
Lj==-~ xi2i and Y Lj==-~ Yi2i. We
define multiplication to be the problem of computing the (2n)-bit product of x and
y.

Using the notations of [15], let us denote the class of depth-d polynomial-size neural
networks where the (integer) weights are polynomially bounded by & d and the
corresponding class where the weights are unrestricted by LTd. It is easy to see that
if it~ated addition can be computed in &2, then multiplication can be computed
in LT 3 . We first prove the result on iterated addition. Our result hinges on a
recent striking result of Goldmann, Hcistad and Razborov [5]. The key observation
is that iterated addition can be computed as a sum of polynomially many linear
threshold (LTd functions (with exponential weights). Let us first state the result
of Goldmann, Hastad and Razborov [5].
Lemma 1
[5] Let LTd denote the class of depth-d polynomial-size neural networks where the weights at the output gate are polynomially bounded integers (with
no restriction on the weights of the other gates). Then LTd = & d for any fixed
integer d ~ 1.
The following lemma is a generalization of the result in [13]. Informally, the result
says that if a function is 1 when a weighted sum (possibly exponential) of its inputs
lies in one of polynomially many intervals, and is 0 otherwise, then the function can
be computed as a sum of polynomially many LTI functions.
Lemma 2
Let S = L7=1 WiXi and f(X) be a function such that f = 1 if S E
[Ii, ud for i = 1, ... , Nand f = 0 otherwise, where N is polynomially bounded.
The~ can be computed as a sum of polynomially many LTI functions and thus
f E LT2 ?
Combining the above two lemmas yields a depth-2 neural network for iterated addition.
.-

Theorem 1

Iterated addition is in LT2 ?

It is also easy to see that iterated addition cannot be computed in LTI

Simply
observe that the first bit of the sum is the parity function, which does not belong
to LT1 . Thus the above neural network for iterated addition has minimum possible
depth.
Theorem 2

Multiplication of 2 n-bit integers can be computed in

.

LT3.

It follows from the results in [6] that the depth-3 neural network for multiplication

stated in the above theorem has optimal depth.

Optimal Depth Neural Networks for Multiplication and Related Problems

We can further apply the results in [5] to construct small depth neural networks for
division, powering and multiple product. Let us give a formal definition of these
problems.

Definition 3
Let X be an input n-bit integer
2
n -bit representation of xn.

~

O. We define powering to be the

Definition 4
Given n n-bit integers Zi, i = 1, ... , n, We define multiple product
2
to be the n -bit representation of n~=l Zi.
Suppose we want to compute the quotient of two integers. Some quotient in binary representation might require infinitely many bits, however, a circuit can only
compute the most significant bits of the quotient. If a number has both finite and
infinite binary representation (for example 0.1 = 0.0111 ... ), we shall always express
the number in its finite binary representation. We are interested in computing the
truncated quotient, defined below:

=

Definition 5
Let X and Y ~ 1 be two input n bit integers. Let X /Y
L~;~oo zi 2i be the quotient of X divided by Y. We define DIVk(X/Y) to be
X/Y truncated to the (n + k)-bit number, i.e.

o
In particular, DIVo(X /Y) is l X /Y J, the greatest integer ~ X /Y.

Theorem 3
-.

1. Powering can be computed in LT3 .

2. DIVk(x/y) can be computed in

Lr3 .

3. Multiple Product can be computed in

LT4 .

It can be shown from the lower-bound results in [9] that the neural networks for
division are optimal in depth.

References
[1] Y. S. Abu-Mostafa and D. Psaltis. Optical Neural Computers. Scientific American
, 256(3) :88-95, 1987.

L~ -formulae on finite structures. Annals of Pure and Applied Logic,
24:1-48, 1983.

[2] M. Ajtai.

[3] A. K. Chandra, 1. Stockmeyer, and U. Vishkin. Constant depth reducibility. Siam
J. Comput., 13:423-439, 1984.
[4] M. Furst, J. B. Saxe, and M. Sipser. Parity, Circuits and the Polynomial-Time
Hierarchy. IEEE Symp. Found. Compo Sci., 22:260-270, 1981.
[5] M. Goldmann, J. Hastad, and A. Razborov. Majority Gates vs. General Weighted
Threshold Gates. preprint, 1991.

63

64

Siu and Roychowdhury
[6] A. Hajnal, W. Maass, P. Pudlak, M. Szegedy, and G. Turan. Threshold circuits of
bounded depth. IEEE Symp. Found. Compo Sci., 28:99-110, 1987.

[7] J. H1stad and M. Goldmann.

On the power of small-depth threshold circuits.
InProceedings of the 31st IEEE FOCS, pp. 610-618, 1990.

[8] T. Hofmeister, W. Hohberg and S. Kohling . Some notes on threshold circuits and
multiplication in depth 4. Information Processing Letters, 39:219-225, 1991.
[9] T. Hofmeister and P. PudIa.k, A proof that division is not in TC~. Forschungsbericht
Nr. 447, 1992, Uni Dortmund.
[10] J. J. Hopfield. Neural Networks and physical systems with emergent collective computational abilities. Proceedings of the National Academy of Sciences, 79:2554-2558,
1982.
[11] J. L. McClelland D. E. Rumelhart and the PDP Research Group. Parallel Distributed
Processing: Explorations in the Microstructure of Cognition, vol. 1. MIT Press, 1986.
[12] W. S. McCulloch and W. Pitts. A Logical Calculus of Ideas Immanent in Nervous
Activity. Bulletin of Mathematical Biophysics, 5:115-133, 1943.
[13] S. Muroga. The principle of majority decision logic elements and the complexity of
their circuits. Inti. Con/. on Information Processing, Paris, France, June 1959.
[14] K. Y. Siu and J. Bruck. Neural Computation of Arithmetic Functions. Proc. IEEE,
78, No. 10:1669-1675, October 1990. Special Issue on Neural Networks.
[15] K.-Y. Siu and J. Bruck. On the Power of Threshold Circuits with Small Weights.
SIAM J. Discrete Math., 4(3):423-435, August 1991.
[16] K.-Y. Siu, J. Bruck, T. Kailath, and T. Hofmeister. Depth-Efficient Neural Networks

for Division and Related Problems . to appear in IEEE Trans. Information Theory,
1993.
[17] K.- Y. Siu and V. Roychowdhury.

On Optimal Depth Threshold Circuits for Mulitplication and Related Problems. to appear in SIAM J. Discrete Math.

[18] A. Yao.

Separating the polynomial-time hierarchy by oracles. IEEE Symp. Found.
Compo Sci., pages 1-10, 1985.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 362-a-second-order-translation-rotation-and-scale-invariant-neural-network.pdf

A Second-Order Translation, Rotation and
Scale Invariant Neural Network

Shelly D.D. Goggin
Kristina M. Johnson
Karl E. Gustafson?
Optoelectronic Computing Systems Center and
Department of Electrical and Computer Engineering
University of Colorado at Boulder
Boulder, CO 80309
shellg@boulder.colorado.edu

ABSTRACT
A second-order architecture is presented here for translation, rotation and
scale invariant processing of 2-D images mapped to n input units. This
new architecture has a complexity of O( n) weights as opposed to the O( n 3 )
weights usually required for a third-order, rotation invariant architecture.
The reduction in complexity is due to the use of discrete frequency information. Simulations show favorable comparisons to other neural network
architectures.

1

INTRODUCTION

Multiplicative interactions in neural networks have been proposed (Pitts and McCulloch, 1947; Giles and Maxwell, 1987; McClelland et aI, 1988) both to explain biological neural functions and to provide invariances in pattern recognition. Higherorder neural networks are useful for invariant pattern recognition problems, but
their complexity prohibits their use in mal1Y large image processing applications.
The complexity of the third-order rotation invariant neural network of Reid et aI,
1990 is 0(n 3 ), which will clearly not scale. For example, when 11 is on the order
of 10 6 , as in high definition television (HDTV), 0(10 18 ) weights would be required
in a third-order neural network. Clearly, image processing applications are best
approached with neural networks of lower complexity. \Ve present a translation,
?Department of Mathematics

313

314

Goggin, Johnson, and Gustafson
rotation and scale invariant architecture, which has weight complexity of O( n), and
requires only multiplicative and additive operations in the activation function.

2

HIGHER-ORDER NEURAL NETWORKS

Higher-order neural networks (HONN) have multiplicative terms in their activation
function, such that the output of a unit, Ok, has the form
(n-l)(n-l)

Ok

= f[ E

(n-l)

E ... E

(i=O) (j=0)

Wij .. .lkXiXj ... Xr]

(1)

1=0

where f is a thresholding function, Wij. .. lk is the weight for each term, and Xi is one
of n input values. Some of the Xi could be bias units to give lower order terms. The
order of the multiplications is O(nm) for an m-order network, but the order of the
number of weights can be lower. Since the multiplications of data can be done in
a preprocessing stage, the major factor in the computational burden is the number
of weights. The emphasis on the complexity of the weights is especially relevant for
optical implementations of higher-order networks (Psaltis et aI, 1988, Zhang et aI,
1990), since the multiplications can usually be performed in parallel.
Invariances can be achieved with higher-order neural networks by using the spatial frequencies of the input as a priori information. Wechsler and Zimmerman,
1988, compute the Fourier transform of the data in polar coordinates and use these
data as inputs to a neural network to achieve rotation, scale and translation invarianee. The disadvantage with this approach is that the Fourier transform and the
computation of polar coordinates require more complex operations than addition
and multiplication of inputs. It has been shown that second-order networks can
be constructed to provide either translation and scale invariance or rotation and
scale invariance (Giles et aI, 1988). However, their approach does not consider the
difficulties in defining scale and rotation for images made up of pixels. Our architecture directly addresses the problem of rotation, translation and scale invariance
in pattern recognition for 2-D arrays ofbinal'Y pixels. Restrictions permit structure
to be built into the weights, which reduces their complexity.

3

WEDGE-RING HONN

vVe present a new architecture for a second-order neural network based on the
concept of the wedge-ring detector (Casasent, 1985). When a wedge-ring detector
is used in the Fourier plane of an optical processor, a set of features are obtained
that are invariant to scale, rotation and translation. As shown in figure 1, the lens
performs a spatial Fourier transform on an image, which yields an intensity pattern
that is invariant to translations in the image plane. The ring detectors sum the
amplitudes of the spatial frequencies with the same radial distance from the zero
frequency, to give features that are invariant to rotation and shift changes. The
wedge detectors sum the amplitudes of frequencies within a range of angles with
respect to the zero frequency to produce features that are invariant to scale and
shift changes, assuming the images retain the same zero frequency power as they
are scaled.

A Second-Order Thanslation, Rotation and Scale Invariant Neural Network

Laser

Image

Fourier Wedge-Ring
Transform Detector
Lens

Computer

Figure 1: A Wedge-Ring Detector Optical Processor
In a multi-pixel, binary image, a second-order neural network can perform the same
function as the wedge-ring detector without the need for a Fourier transform. For
an image of dimensions fo x yin, let us define the pixel spatial frequency fi,j as
(v'n-l-Ikl) (v'n- l -Ill)

L

L

(i=O)

(j=O)

h,l =

;ri,j;ri+lkl,j+I'I'

-(vn -1) ~ k,

I

<

vn - 1

(2)

where ;ri,j is a binary valued pixel at location (i, j). Note that the pixel frequencies
have symmetry; /i,j = f -i,-j. The frequency terms can be arranged in a grid
in a manner analogous to the Fourier transform image in the optical wedge-ring
detector. (See figure 2.)
Pixel Wedge Terms

??

B1~lIIll?Zlra.

V lao. V lS3. V 13S' V 117' V 90" V 63. V
(~1 f~. f4,l (I.'
XO.!l Xo

XO?2

x l ?O Xl,l Xl,
X2,O X2,1

~2

4S.

V

27?

f4,1

f' r1 f. r ? f?.- f ?.? f.,2
fa,1 fl;-' f ... f .~ fl,t
f.,4 f ..... f ?.- f .~ f .,2
fl,4 f2,o1 f1,t fl,l f2,3

Image
(Input Units)

Pixel Spatial Frequencies

Pixel Ring Terms

DIml ? ? ?
ro

r1

r2

r3

r4

Figure 2: A Simple Input Image and its Associated Pixel Spatial
Frequencies, Pixel Ring Terms and Pixel Wedge Terms
For all integers p, 0

L

~ p ~

2( fo - 1), the ring pixel terms

vn -

rp

are given by

h,l, 0 ~ k ~
1, 0 ~ I ~ yin - 1, if k = O.
(3)
Ikl+lll=p
-(yin - 1) ~ I ~ yin - 1, if k > O.
as shown in figure 2. This definition of the ring pixel terms works well for
images with a small number of pixels. Larger pixel arrays can use the following
rp

= 2

315

316

Goggin, Johnson, and Gustafson
definition. For 0 ~ p ~ 2( Vii - I?,
rp

L

=2

h: ,f, 0 ~ k ~ Fn - 1, 0 < I < y'n - 1,
-(y'n - 1)

I:l+ll=p

~ 1~

if k =

o.

y'n - 1, if k >

o.

(4)

Note that p will not take on all values less than 2n. The number of ring pixel terms
generated by equation 4 is less than or equal to n/21 + Ly'n/2 J. The number of
ring pixel terms can be reduced by making the rings a fixed width, ~r. Then, for
all integers p, 0 ~ p < rV2(Vii - 1)/~rl

r

rp

L

= 2

fl:,l,

(p-l)~r<~~p~r

o <k ~ Vii -1,
o < I ~ y'n - 1, if k = o.
-( Vii -

1) ~ I ~

Vii -

(5)

1, if k

> o.

As the image size increases, the ring pixel terms will approximate continuous rings.
For 0
V9

< ()

~

180 0 , the wedge pixel terms

=2

V9

are

-(Fn -1) < k < 0, -(y'n - 1) ~ I
-( Vii - 1) < I

fl:,l,
tan- l (I: 11)=9

~
~

1, if k = 0,
y'n - 1, if k < 0,

(6)
as shown in figure 2. The number of wedge pixel terms is less than or equal to
2n - 2y'n + 1. The number of wedge pixel terms can be reduced by using a fixed
wedge width, ~v. Then for all integers q, 1 ~ q ~ P80? / ~v1,

-(Vii -( Vii -(Vii -

(q-l )~tJ< tan- l (I: 11)~q~tJ

1) ~ k < 0,
1) < I ~ 1, if k 0,
1) ~ I ~ Vii - 1, if k

=

(7)

< 0,

For small pixel arrays, the pixel frequencies are not evenly distributed between the
wedges.
All of the operations from the second-order terms to the pixel frequencies and from
the pixel frequencies to the ring and wedge pixel terms are linear. Therefore, the
values of the wedge-ring features can be obtained by directly summing the secondorder terms, without explicitly determining the individual spatial frequencies.
(y"il-l-II:I) (y"il-I-lll)

L

(i=O)

L

(j=O)

o <k ~ y'n - 1,
o ~ I ~ y'n - 1, if k = o.
-(y'n - 1) < I

if k

> o.

~

y'n - 1,

(8)
(y"il-l-Ikl) (fo-l-lll)
V9

=2
(tan-l(1: 11)=9)

L

(i=O)

L

(j=O)

-(y'n-l)<k~O,
~ I ~ 1,

-(y'n - 1)
if k

= o.

-( y'n - 1)
if k

< o.

~ 1<

Vii - 1,

(9)
A mask can be used to sum the second-order terms directly. For an example of the
mask for the 3 x 3 image, see figure 3.

A Second-Order Iranslation, Rotation and Scale Invariant Neural Network

Pixel Wedge Terms

~~~-

x,~

.Blm~[]]]~mlll
V 180" V 153" V 135" V 117? V

90"

V 63" V

45.

V

'1:1?

Pixel Ring Terms

DElm ? ?
fO

f1

f2

f3

f4

Figure 3: A Mask for Summing Second-Order Terms for Ring Features
and "Vedge Features for the Image in Figure 2
The ring and wedge pixel terms can be used as inputs for a multilayer neural
network that can then perform pattern recognition with general combinations of
these features. The output of the first (and possibly only) hidden layer units are
for unit j,
OJ
wj,prp +
Wj,(1V(1],
(10)

= J[L
p

L
(1

where f here is the threshold function. The total number of ring and wedge terms,
which corresponds to the number of weights, is less than or equal to (5/2)n.

4

EXAMPLE RESULTS FOR THE TC PROBLEM

Results have been obtained for the 9 x 9 TC problem (McClelland et aI, 1988) (see
figure 4). Since wedge and ring pixel terms are used, a solution to the problem
is readily seen. Figure 5 shows the final neural network architecture. Equations 4
and 6 are used to calculate the ring and wedge pixel terms, respectively. With two
additional layers, the network can distinguish between the T and the C at any of
the three scales or four rotations. In the hidden layer, the 180 0 wedge pixel term is
subtracted from the 90 0 wedge pixel term and vice-versa with a bias unit weighted
by 0.5 and a hard-limiting threshold function. This computation results ill hidden
units with values (0,1) or (1,0) for the C and (1,1) for the T. The next level then
performs a binary AND, to get a 1 for T and a 0 for C. The weJge features are also
used in a layer to determine whether the image was rotated by ?90? or not. The ring
units are used as input to a layer with an output uuit for each of the three scales.
Due to the reduced complexity of the weights in this second-order neural network,
a solution for the architecture and weights is obtained by inspection, whereas t.he

317

318

Goggin, Johnson, and Gustafson

Scale =2

Scale =3

Figure 4: Examples of Rotated and Scaled Input Images for the
TC Problem

???
???
Figure 5: Multilayer Neural Network for the Wedge-Ring Features
for the TC Problem

A Second-Order Thanslation, Rotation and Scale Invariant Neural Network
same problem required computer simulation when presented to a third-order neural
network (Reid et aI, 1990).

5

CONCLUSIONS

In this paper, we show how the weight complexity in a higher-order neural network
is reduced from O( n 3 ) to O( n) by building into the architecture invariances in
rotation, translation and scale. These invariances were built into the neural network
architecture by analogy to the architecture for feature extraction in the optical
wedge-ring detector system. This neural network architecture has been shown to
greatly simplify the computations required to solve the classic TC problem.
Acknowledgements

We gratefully acknowledge fellowship support from GTE Research Labs and the
NSF Engineering Research Center for Optoelectronic Computing Systems grant
CDR8622236.
References

D. Casasent, "Coherent optical pattern recognition: A review," Optical Engineering,
vol. 24, no. 1, pp. 26-32 (1985).
C.L. Giles, R.D. Griffin, and T. Maxwell, "Encoding geometric invariances in higherorder networks," In: Neural Information Processing Systems, D. Z. Anderson (ed.),
(New York: American Institute of Physics, 1988) pp. 301-309.
C.L. Giles and T. Maxwell, "Lea.rning, invariance and generalization in high-order
neural networks," Applied Optics, vol. 26, no. 23, pp. 4972-4978 (1987).
J.t. McClelland, D.E. Rumelhart.and the PDP Research Group, Parallel Distributed
Processing, Explorations in the ~Microstructure of Cognition, (Cambridge, MA: The
MIT Press, 1988).

vv.

Pitts and \V.S. ~IcCulloch, "How we know universals: The perception of auditory and visual forms," Bulletin of Afathematical Biophysics, vol. 9, pp. 127-147
(1947).
D. Psaltis, C.H. Park and J. Hong, "Higher order associative memories and their
optical implementations," Neural Networks, vol. 1, pp. 149-163 (1988).
M.ll. Reid, L. Spirkovska and E. Ochoa, "Simultaneous position, scale and rotation
invariant pattern classification using third-order neural networks," To appear in:
The International Journal of Neural Networks - Research and Applications.
H. \Vechsler and G.L. Zimmerman, "Invariant object recognition using a distributed
associative memory," In: Neural Information Processing Systems, D. Z. Anderson
(ed.), (New York: American Institute of Physics, 1988) pp. 830-839.
L. Zhang, M.G. Robinson and K.M. Johnson, "Optical implementation of a second
order neural network," International Neural Network Conference, Paris, July, 1990.

319


<<----------------------------------------------------------------------------------------------------------------------------------------->>

