query sentence: neural network computers
---------------------------------------------------------------------
title: 235-computational-efficiency-a-common-organizing-principle-for-parallel-computer-maps-and-brain-maps.pdf

60

Nelson and Bower

Computational Efficiency:
A Common Organizing Principle for
Parallel Computer Maps and Brain Maps?

Mark E. Nelson James M. Bower
Computation and Neural Systems Program
Division of Biology, 216-76
California Institute of Technology
Pasadena, CA 91125

ABSTRACT
It is well-known that neural responses in particular brain regions
are spatially organized, but no general principles have been developed that relate the structure of a brain map to the nature of
the associated computation. On parallel computers, maps of a sort
quite similar to brain maps arise when a computation is distributed
across multiple processors. In this paper we will discuss the relationship between maps and computations on these computers and
suggest how similar considerations might also apply to maps in the
brain.

1

INTRODUCTION

A great deal of effort in experimental and theoretical neuroscience is devoted to
recording and interpreting spatial patterns of neural activity. A variety of map
patterns have been observed in different brain regions and , presumably, these patterns reflect something about the nature of the neural computations being carried
out in these regions. To date, however, there have been no general principles for
interpreting the structure of a brain map in terms of properties of the associated
computation. In the field of parallel computing, analogous maps arise when a computation is distributed across multiple processors and, in this case, the relationship

Computational Eftkiency

between maps and computations is better understood. In this paper, we will attempt to relate some of the mapping principles from the field of parallel computing
to the organization of brain maps.

2

MAPS ON PARALLEL COMPUTERS

The basic idea of parallel computing is to distribute the computational workload
for a single task across a large number of processors (Dongarra, 1987; Fox and
Messina, 1987). In principle, a parallel computer has the potential to deliver computing power equivalent to the total computing power of the processors from which
it is constructed; a 100 processor machine can potentially deliver 100 times the
computing power of a single processor. In practice, however, the performance that
can be achieved is always less efficient than this ideal. A perfectly efficient implementation with N processors would give a factor N speed up in computation time;
the ratio of the actual speedup (1 to the ideal speedup N can serve as a measure of
the efficiency f of a parallel implementation.
(1

(1)

f= -

N

For a given computation, one of the factors that most influences the overall performance is the way in which the computation is mapped onto the available processors.
The efficiency of any particular mapping can be analyzed in terms of two principal
factors: load-balance and communication overhead. Load-balance is a measure of
how uniformly the computational work load is distributed among the available processors. Communication overhead, on the other hand, is related to the cost in time
of communicating information between processors.
On parallel computers, the load imbalance A is defined in terms of the average
calculation time per processor T atJg and the maximum calculation time required by
the busiest processor T maz :
A

= Tmaz -

T atJg

T atJg

(2)

The communication overhead 7] is defined in terms of the maximum calculation time
and the maximum communication time Tcomm:

T maz

Tcomm
7]=-----Tmaz
Tcomm

+

(3)

Assuming that the calculation and communication phases of a computation do not
overlap in time, as is the case for many parallel computers, the relationship between
efficiency f, load-imbalance A, and communicaticn overhead 7] is given by (Fox et
al.,1988):

61

62

Nelson and Bower

1-7]

{=l+A

(4)

When both load-imbalance A and communication overhead 7] are small, the inefficiency is approximately the sum of the contributions from load-imbalance and
communication overhead:
(~l-(7]+A)

(5)

When attempting to achieve maximum performance from a parallel computer, a
programmer tries to find a mapping that minimizes the combined contributions of
load-imbalance and communication overhead. In some cases this is accomplished by
applying simple heuristics (Fox et al., 1988), while in others it requires the explicit
use of optimization techniques like simulated annealing (Kirkpatrick et al., 1983)
or even artificial neural network approaches (Fox and Furmanski, 1988). In any
case, the optimal tradeoff between load imbalance and communication overhead
depends on certain properties of the computation itself. Thus different types of
computations give rise to different kinds of optimal maps on parallel computers.

2.1

AN EXAMPLE

In order to illustrate how different mappings can give rise to different computational
efficiencies, we will consider the simulation of a single neuron using a multicompartment modeling approach (Segev et al., 1989). In such a simulation, the model neuron is divided into a large number of compartments, each of which is assumed to be
isopotential. Each compartment is represented by an equivalent electric circuit that
embodies information about the local membrane properties. In order to update the
voltage of an individual compartment, it is necessary to know the local properties
as well as the membrane voltages of the neighboring compartments. Such a model
gives rise to a system of differential equations of the following form:

(6)
where em is the membrane capacitance, Vi is the membrane voltage of compartment
i, 9k and Ek are the local conductances and their reversal potentials, and 9i?l,i are
coupling conductances to neighboring compartments.
When carrying out such a simulation on a parallel computer, where there are more
compartments than processors, each processor is assigned responsibility for updating
a subset of the compartments (Nelson et al., 1989). If the compartments represent
equivalent computational loads, then the load-imbalance will be proportional to
the difference between the maximum and the average number of compartments per
processor. If the computer processors are fully interconnected by communication
channels, then the communication overhead will be proportional to the number
of interprocessor messages providing the voltages of neighboring compartments. If

Computational Efficiency

c

A

A= 0.26

11 = 0.04
E=

0.76

\' A= 0.01
:,:!' 11 = 0.07

:i~

~

? =

0.92

Figure 1: Tradeoffs between load-imbalance A and communication overhead 7],
giving rise to different efficiencies ? for different mappings of a multicompartment neuron model. (A) a minimum-cut mapping that minimizes communication
overhead but suffers from a significant load-imbalance, (B) a scattered mapping
that minimizes load-imbalance but has a large communication overhead, and (C)
a near-optimal mapping that simultaneously minimizes both load-imbalance and
communication overhead.

neighboring compartments are mapped to the same processor, then this information
is available without any interprocessor communication and thus no communication
overhead is incurred.
Fig. 1 shows three different ways of mapping a 155 compartment neuron model
onto a group of 4 processors. In each case the load-imbalance and communication
overhead are calculated using the assumptions listed above and the computational
efficiency is computed using eq. 4. The map in Fig. 1A minimizes the communication
overhead of the' mapping by making a minimum number of cuts in the dendritic
tree, but is rather inefficient because a significant load-imbalance remains even
after optimizing the location of each cut. The map is Fig. 1B, on the other hand,
minimizes the load-imbalance, by using a scattered mapping technique (Fox et al.,
1988), but is inefficient because of a large communication overhead. The map in
Fig. 1C strikes a balance between load-imbalance and communication overhead that
results in a high computational efficiency. Thus this particular mapping makes the
best use of the available computing resources for this particular computational task.

63

64

Nelson and Bower

A

B

c

Figure 2: Three classes of map topologies found in the brain (of the rat). (A)
continuous map of tactile inputs in somatosensory cortex (B) patchy map of tactile
inputs to cerebellar cortex and (C) scattered mapping of olfactory inputs to olfactory
cortex as represented by the unstructured pattern of 2DG uptake in a single section
of this cortex.

3

MAPS IN THE BRAIN

Since some parallel computer maps are clearly more efficient than others for particular problems, it seems natural to ask whether a similar relationship might hold for
brain maps and neural computations. Namely, for a given computational task, does
one particular brain map topology make more efficient use of the available neural
computing resources than another? If so, does this impose a significant constraint
on the evolution and development of brain map topologies?
It turns out that there are striking similarities between the kinds of maps that
arise on parallel computers and the types of maps that have been observed in
the brain. In both cases, the map patterns can be broadly grouped into three
categories: continuous maps, patchy maps, and scattered (non-topographic) maps.
Fig. 2 shows examples of brain maps that fall into these categories. Fig. 2A shows
an example of a smooth and continuous map representing the pattern of afferent
tactile projections to the primary somatosensory cortex of a rat (Welker, 1971).
The patchy map in Fig. 2B represents the spatial pattern of tactile projections to
the granule cell layer of the rat cerebellar hemispheres (Shambes et aI., 1978; Bower
and Woolston, 1983). Finally, Fig. 2C represents an extreme case in which a brain
region shows no apparent topographic organization. This figure shows the pattern
of metabolic activity in one section of the olfactory (piriform) cortex, as assayed by
2-deoxyglucose (2DG) uptake, in response to the presentation of a particular odor
(Sharp et al., 1977). As suggested by the uniform label in the cortex, no discernible

Computational Eftkiency

odor-specific patterns are found in this region of cortex.
On parallel computers, maps in these different categories arise as optimal solutions
to different classes of computations. Continuous maps are optimal for computations
that are local in the problem space, patchy maps are optimal for computations that
involve a mixture of local and non-local interactions, and scattered maps are optimal or near-optimal for computations characterized by a high degree of interaction
throughout the problem space, especially if the patterns of interaction are dynamic
or cannot be easily predicted. Interestingly, it turns out that the intrinsic neural circuitry associated with different kinds of brain maps also reflects these same
patterns of interaction. Brain regions with continuous maps, like somatosensory
cortex, tend to have predominantly local circuitry; regions with patchy maps, like
cerebellar cortex, tend to have a mixture of local and non-local circuitry; and regions
with scattered maps, like olfactory cortex, tend to be characterized by wide-spread
connectivity.
The apparent correspondence between brain maps and computer maps raises the
general question of whether or not there are correlates of load-imbalance and communication overhead in the nervous system. In general, these factors are much more
difficult to identify and quantify in the brain than on parallel computers. Parallel
computer systems are, after all, human-engineered while the nervous system has
evolved under a set of selection criteria and constraints that we know very little
about. Furthermore, fundamental differences in the organization of digital computers and brains make it difficult to translate ideas from parallel computing directly
into neural equivalents (c.f. Nelson et al., 1989). For example, it is far from clear
what should be taken as the neural equivalent of a single processor. Depending on
the level of analysis, it might be a localized region of a dendrite, an entire neuron, or
an assembly of many neurons. Thus, one must consider multiple levels of processing
in the nervous system when trying to draw analogies with parallel computers.
First we will consider the issue of load-balancing in the brain. The map in Fig. 2A,
while smooth and continuous, is obviously quite distorted. In particular, the regions
representing the lips and whiskers are disproportionately large in comparison to
the rest of the body. It turns out that similar map distortions arise on parallel
computers as a result of load-balancing. If different regions of the problem space
require more computation than other regions, load-balance is achieved by distorting
the map until each processor ends up with an equal share of the workload (Fox et
al., 1988). In brain maps, such distortions are most often explained by variations
in the density of peripheral receptors. However, it has recently been shown in
the monkey, that prolonged increased use of a particular finger is accompanied by
an expansion of the corresponding region of the map in the somatosensory cortex
(Merzenich, 1987). Presumably this is not a consequence of a change in peripheral
receptor density, but instead reflects a use-dependent remapping of some tactile
computation onto available cortical circuitry.
Although such map reorganization phenomena are suggestive of load-balancing effects, we cannot push the analogy too far because we do not know what actually

6S

66

Nelson and Bower
corresponds to "computational load" in the brain. One possibility is that it is associated with the metabolic load that arises in response to neural activity (Yarowsky
and Ingvar, 1981). Since metabolic activity necessitates the delivery of an adequate
supply of oxygen and glucose via a network of small capillaries, the efficient use of
the capillary system might favor mappings that tend to avoid metabolic "hot spots"
which would overload the delivery capabilities of the system.
When discussing communication overhead in the brain, we also run into the problem of not knowing exactly what corresponds to "communication cost". On parallel
computers, communication overhead is usually associated with the time-cost of exchanging information between processors. In the nervous system, the importance of
such time-costs is probably quite dependent on the behavioral context of the computation. There is evidence, for example, that some brain regions actually make use
of transmission delays to process information (Carr and Konishi, 1988). However,
there is another aspect of communication overhead that may be more generally
applicable having to do with the space-costs of physically connecting processors together. In the design of modern parallel computers and in the design of individual
computer processor chips, space-costs associated with interconnections pose a very
serious constraint for the design engineer. In the nervous system, the extremely
large numbers of potential connections combined with rather strict limitations on
cranial capacity are likely to make space-costs a very important factor.

4

CONCLUSIONS

The view that computational efficiency is an important constraint on the organization of brain maps provides a potentially useful new perspective for interpretting
the structure of those maps. Although the available evidence is largely circumstantial, it seems likely that the topology of a brain map affects the efficiency with
which neural resources are utilized. Furthermore, it seems reasonable to assume
that network efficiency would impose a constraint on the evolution and development of map topologies that would tend to favor maps that are near-optimal for
the computational tasks being performed. The very substantial task before us, in
the case of the nervous system, is to carry out further experiments to better understand the detailed relationships between brain maps, neural architectures and
associated computations (Bower, 1990).
Acknowledgements
We would like to acknowledge Wojtek Furmanski and Geoffrey Fox of the Caltech
Concurrent Computation Program (CCCP) for their parallel computing support.
We would also like to thank Geoffrey for his comments on an earlier version of this
manuscript. This effort was supported by the NSF (ECS-8700064), the Lockheed
Corporation, and the Department of Energy (DE-FG03-85ER25009).
References
Bower, J .M. (1990) Reverse engineering the nervous system: An anatomical, physiological, and computer based approach. In: An Introduction to Neural and Electronic

Computational Efficiency

Networks. (S. Zornetzer, J. Davis, and C. Lau, eds), pp. 3-24, Academic Press.

Bower, J .M. and D.C. Woolston (1983) Congruence of Spatial Organization of Tactile Projections to Granule Cell and Purkinje Cell Layers of Cerebellar Hemispheres
of the Albino Rat: Vertical Organization of Cerebellar Cortex. J. Neurophysiol. 49,
745-756.
Carr, C.E. and M. Konishi (1988) Axonal delay lines for time measurement in the
owl's brain stem. Proc Natl Acad Sci USA 85, 8311-8315.
Dongarra, J.J. (1987) Experimental Parallel Computing Architectures, (Dongarra,
J.J., ed.) North-Holland.
Fox, G. C., M. Johnson, G. Lyzenga, S. Otto, J. Salmon, D. Walker (1988) Solving
Problems on Concurrent Processors, Prentice Hall.
Fox, G.C. and W. Furmanski (1988) Load Balancing loosely synchronous problems
with a neural network. In: Proceedings of the Third Conference on Hypercube
Concurrent Computers and Applications, (Fox, G.C., ed.), pp.241-278, ACM.
Fox, G.C. and P. Messina (1987) Advanced Computer Architectures. Scientific
American, October, 66-74.
Kirkpatrick, S., C.D. Gelatt and M.P. Vecchi (1983) Optimization by Simulated
Annealing. Science, 220, 671-680.
Merzenich, M.M. (1987) Dynamic neocortical processes and the origins of higher
brain functions. In: The Neural and Molecular Bases of Learning, (Changeux, J .-P.
and Konishi, M., eds.), pp. 337-358, John Wiley & Sons.
Nelson, M.E., W. Furmanski and J .M. Bower (1989) Modeling Neurons and Networks on Parallel Computers. In: Methods in Neuronal Modeling: From Synapses
to Networks, (Koch, C. and I. Segev, eds.), pp. 397-438, MIT Press.
Segev, I., J.W. Fleshman and R.E. Burke (1989) Compartmental Models of Complex
Neurons. In: Methods in Neuronal Modeling: From Synapses to Networks, (Koch,
C. and I. Segev, eds.), pp. 63-96, MIT Press.
Shambes, G.M., J .M. Gibson and W. Welker (1978) Fractured Somatotopy in Granule Cell Tactile Areas of Rat Cerebellar Hemispheres Revealed by Micromapping.
Brain Behav. Evol. 15, 94-140.
Sharp, F.R., J.S . Kauer and G.M. Shepherd (1977) Laminar Analysis of 2-Deoxyglucose Uptake in Olfactory Bulb and Olfactory Cortex of Rabbit and Rat. J.
Neurophysiol. 40, 800-813.
Welker, C. (1971) Microelectrode delineation of fine grain somatotopic organization
of SMI cerebral neocortex in albino rat. Brain Res. 26, 259-275.
Yarowsky, P.J. and D.H. Ingvar (1981) Neuronal activity and energy metabolism.
Federation Proc. 40, 2353-2263.

67


----------------------------------------------------------------

title: 4718-analog-readout-for-optical-reservoir-computers.pdf

Analog readout for optical reservoir computers

A. Smerieri1 , F. Duport1 , Y. Paquot1 , B. Schrauwen2 , M. Haelterman1 , S. Massar3

1

Service OPERA-photonique, Universit? Libre de Bruxelles (U.L.B.), 50 Avenue F. D.
Roosevelt, CP 194/5, B-1050 Bruxelles, Belgium
2
Department of Electronics and Information Systems (ELIS), Ghent University,
Sint-Pietersnieuwstraat 41, 9000 Ghent, Belgium
3
Laboratoire d?Information Quantique (LIQ), Universit? Libre de Bruxelles (U.L.B.), 50
Avenue F. D. Roosevelt, CP 225, B-1050 Bruxelles, Belgium

Abstract
Reservoir computing is a new, powerful and flexible machine learning technique that is easily implemented in hardware. Recently, by using a timemultiplexed architecture, hardware reservoir computers have reached performance comparable to digital implementations. Operating speeds allowing for real time information operation have been reached using optoelectronic systems. At present the main performance bottleneck is the readout
layer which uses slow, digital postprocessing. We have designed an analog
readout suitable for time-multiplexed optoelectronic reservoir computers,
capable of working in real time. The readout has been built and tested experimentally on a standard benchmark task. Its performance is better than
non-reservoir methods, with ample room for further improvement. The
present work thereby overcomes one of the major limitations for the future
development of hardware reservoir computers.

1

Introduction

The term ?reservoir computing? encompasses a range of similar machine learning techniques,
independently introduced by H. Jaeger [1] and by W. Maass [2]. While these techniques
differ in implementation details, they share the same core idea: that one can leverage the
dynamics of a recurrent nonlinear network to perform computation on a time dependent
signal without having to train the network itself. This is done simply by adding an external,
generally linear readout layer and training it instead. The result is a powerful system that
can outperform other techniques on a range of tasks (see for example the ones reported
in [3, 4]), and is significantly easier to train than recurrent neural networks. Furthermore
it can be quite easily implemented in hardware [5, 6, 7], although it is only recently that
hardware implementations with performance comparable to digital implementations have
been reported [8, 9, 10].
One great advantage of this technique is that it places almost no requirements on the
structure of the recurrent nonlinear network. The topology of the network, as well as
the characteristics of the nonlinear nodes, are left to the user. The only requirements are
that the network should be of sufficiently high dimensionality, and that it should have
suitable rich dynamics. The last requirement essentially means that the dynamics allows
the exploration of a large number of network states when new inputs come in, while at
the same time retaining for a finite time information on the previous inputs [11]. For this
reason, the reservoir computers appearing in literature use widely different nonlinear units,
1

see for example [1, 2, 5, 12] and in particular the time multiplexing architecture proposed
in [7, 8, 9, 10].
Optical reservoir computers are particularly promising, as they can provide an alternative
path to optical computing. They could leverage the inherent high speeds and parallelism
granted by optics, without the need for strong nonlinear interaction needed to mimic traditional electronic components. Very recently, optoelectronic reservoir computers have been
demonstrated by different research teams [10, 9], conjugating good computational performances with the promise of very high operating speeds. However, one major drawback in
these experiments, as well as all preceding ones, was the absence of readout mechanisms:
reservoir states were collected on a computer and post-processed digitally, severely limiting
the processing speeds obtained and hence the applicability.
An analog readout for experimental reservoirs would remove this major bottleneck, as
pointed out in [13]. The modular characteristics of reservoir computing imply that hardware reservoirs and readouts can be optimized independently and in parallel. Moreover,
an analog readout opens the possibility of feeding back the output of the reservoir into the
reservoir itself, which in turn allows the use of different training techniques [14] and to apply
reservoir computing to new categories of tasks, such as pattern generation [15, 16].
In this paper we present a proposal for the readout mechanism for opto-electronic reservoirs,
using an optoelectronic intensity modulator. The design that we propose will drastically
cut down their operation time, specially in the case of long input sequences. Our proposal
is suited to optoelectronic or all-optical reservoirs, but the concept can be easily extended
to any experimental time-multiplexed reservoir computer. The mechanism has been tested
experimentally using the experimental reservoir reported in [10], and compared to a digital
readout. Although the results are preliminary, they are promising: while not as good as
those reported in [10], they are however already better than non-reservoir methods for the
same task [16].

2
2.1

Reservoir computing and time multiplexing
Principles of Reservoir Computing

The main component of a reservoir computer (RC) is a recurrent network of nonlinear
elements, usually called ?nodes? or ?neurons?. The system typically works in discrete time,
and the state of each node at each time step is a function of the input value at that time
step and of the states of neighboring nodes at the previous time step. The network output
is generated by a readout layer - a set of linear nodes that provide a linear combination of
the instantaneous node states with fixed coefficients.
The equation that describes the evolution of the reservoir computer is
xi (n) = f (?mi u(n) + ?

N
X

wij xj (n ? 1))

(1)

j=1

where xi (n) is the state of the i-th node at discrete time n, N is the total number of nodes,
u(n) is the reservoir input at time n, mi and wij are the connection coefficients that describe
the network topology, ? and ? are two parameters that regulate the network?s dynamics,
and f is a nonlinear function. One generally tunes ? and ? to have favorable dynamics
when the input to be treated is injected in the reservoir. The network output y(n) is then
constructed using a set of readout weights Wi and a bias weight Wb , as

y(n) =

N
X

Wi xi (n) + Wb

(2)

i=1

Training a reservoir computer only involves the readout layer, and consists in finding the
best set of readout weights Wi and bias Wb that minimize the error between the desired
output and the actual network output. Unlike conventional recurrent neural networks, the
2

Figure 1: Scheme of the experimental setup, including the optoelectronic reservoir (?Input?
and ?Reservoir? layers) and the analog readout (?Output? layer). The red and green parts
represent respectively the optical and electronic components. ?AWG?: Arbitrary waveform
generator. ?M-Z?: LiN bO3 Mach-Zehnder modulator. ?FPD?: Feedback photodiode. ?AMP?:
Amplifier. ?Scope?: NI PXI acquisition card.

strength of connections mi and wij are left untouched. As the output layer is made only of
linear units, given the full set of reservoir states xi (n) for all the time steps n, the training
procedure is a basic, regularized linear regression.
2.2

Time multiplexing

The number of nodes in a reservoir computer determines an upper limit to the reservoir
performance [17]; this can be an obstacle when designing physical implementations of RCs,
which should contain a high number of interconnected nonlinear units. A solution to this
problem proposed in [7, 8], is time multiplexing: the xi (n) are computed one by one by
a single nonlinear element, which receives a combination of the input u(n) and a previous
state xj (n ? 1). In addition an input mask mi is applied to the input u(n), to enrich the
reservoir dynamics. The value of xi (n) is then stored in a delay line to be used at a later
time step n + 1. The interaction between different neurons can be provided by either having
a slow nonlinear element which couples state xi to the previous states xi?1 , xi?2 , ... [8], or
by using an instantaneous nonlinear element and desynchronizing the input with respect to
the delay line [10].
2.3

Hardware RC with digital readout

The hardware reservoir computer we use in the present work is identical to the one reported
in [10] (see also [9]). It uses the time-multiplexing with desynchronisation technique described in the previous paragraph. We give a brief description of the experimental system,
represented in the left part of Figure 1. It uses a LiN bO3 Mach-Zehnder (MZ) modulator,
operating on a constant power 1560 nm laser, as the nonlinear component. A MZ modulator
is a voltage controlled optoelectronic device; the amount of light that it transmits is a sine
function of the voltage applied to it. The resulting state xi (n) is encoded in a light intensity
level at the MZ output. It is then stored in a spool of optical fiber, acting as delay line of
duration T = 8.5?s, while all the subsequent states xi (n) are being computed by the MZ
modulator. When a state xi (n) reaches the end of the fiber spool it is converted into a
voltage by a photodiode.
The input u(n) is multiplied by the input mask mi and encoded in a voltage level by an
Arbitrary Waveform Generator (AWG). The two voltages corresponding to the state xi (n)
at the end of the fiber spool and the input mi u(n) are added, amplified, and the resulting
voltage is used to drive the MZ modulator, thereby producing the state xj (n + 1), and so
on for all values of n.
3

In the experiment reported in [10] a portion of the light coming out of the MZ is deviated
to a second photodiode (not shown in Figure 1), that converts it into a voltage and sends
it to a digital oscilloscope. The Mach-Zehnder output can be represented as ?steps? of light
intensities of duration ? (see Figure 2a), each one representing the value of a single node
state xi at discrete time n. The value of each xi (n) is recovered by taking an average of the
measured voltage for each state at each time step. The optimal readout weights Wi and bias
Wb are then calculated on a computer from a subset (training set) of the recorded states,
using ridge regression [18], and the output y(n) is then calculated using equation 2 for all
the states collected. The performance of the reservoir is then calculated by comparing the
reservoir output y(n) with the desired output y?(n).

3

Analog readout

Readout scheme
Developing an analog readout for the reservoir computer described in section 2 means designing a device that multiplies the reservoir states shown in Figure 2a by the readout
weights Wi , and that sums them together in such a way that the reservoir output y(n)
can be retrieved directly from its output. However, this is not straightforward to do, since
obtaining good performance requires positive and negative readout weights Wi . In optical
implementations [10, 9] the states xi are encoded as light intensities which are always positive, so they cannot be subtracted one from another. Moreover, the summation over the
states must include only the values of xi pertaining to the same discrete time step n and reject all other values. This is difficult in time-multiplexed reservoirs, where the states xN (n)
and x1 (n + 1) follow seamlessly.
Here we show how to resolve both difficulties using the scheme depicted in the right panel of
Figure 1. Reservoir states encoded as light intensities in the optical reservoir computer and
represented in Figure 2a are fed to the input of a second MZ modulator with two outputs.
A second function generator governs the bias of the second Mach-Zehnder, providing the
modulation voltage V (t). The modulation voltage controls how much of the input light
passing through the readout Mach-Zehnder is sent to each output, keeping constant the
sum of the two output intensities. The two outputs are connected to the two inputs of
a balanced photodiode, which in turn gives as output a voltage level proportional to the
difference of the light intensities received at its two inputs1 . This allows us to multiply the
reservoir states by both positive and negative weights.
The time average of the output voltage of the photodiode is obtained by using a capacitor.
The characteristic time of the analog integrator ? is proportional to the capacity C.2 The
role of this time scale is to include in the readout output all the pertinent contributions and
exclude the others. The final output of the reservoir is the voltage across the capacitor at
the end of each discretized time n.
What follows is a detailed description of the readout design.
Multiplication by arbitrary weights
The multiplication of the reservoir states by arbitrary weights, positive or negative, is realized by the second MZ modulator followed by the balanced photodiode. The modulation
voltage V (t) that drives the second Mach Zehnder is piecewise constant, with a step duration equal to the duration ? of the reservoir states; transitions in voltages and in reservoir
states are synchronized. The modulation voltage is also a periodic function of period ?N ,
so that each reservoir state xi (n) is paired with a voltage level Vi that doesn?t depend on
n. The light intensities O1 (t) and O2 (t) at the two outputs of the Mach-Zehnder modulator
1
A balanced photodiode consists of two photodiodes which convert the two light intensities
into two electric currents, followed by an electronic circuit which produces as output a voltage
proportional to the difference of the two currents
2
In the case where the impedance of the coaxial cable R = 50? is matched with the output
impedance of the photodiode, we have ? = RC
2

4

are
O1 (t) = I(t)

1 + cos((V (t) + Vbias ) V?? + ?)
2

, O2 (t) = I(t)

1 ? cos((V (t) + Vbias ) V?? + ?)
2

,
(3)

where I(t) is the light intensity coming from the reservoir, Vbias is a constant voltage that
drives the modulator, ? is an arbitrary, constant phase value, and V? is the half-wave
voltage of the modulator. Neglecting the effect of any bandpass filter in the photodiode,
and choosing Vbias appropriately, the output P (t) from the photodiode can be written as
P (t) = G(O1 (t) ? O2 (t)) = I(t)(G sin(

V (t)?
)) = I(t)W (t)
V?

(4)

with G a constant gain factor. In other words, by setting the right bias and driving the
modulator with a voltage V (t), we multiply the signal I(t) by an arbitrary coefficient W (t).
Note that, if V (t) is piecewise constant, then W (t) is as well. This allows us to achieve the
multiplication of the states xi (n), encoded in the light intensity I(t), by the weights Wi ,
just by choosing the right voltage V (t), as shown in Figure 2b.
Summation of weighted states
To achieve the summation over all the states pertaining to the same discrete time step n,
which according to equation 2 will give us the reservoir output minus the bias Wb , we use
the capacitor at the right side of the Output layer in Figure 1. The capacitor provides the
integration of the photodiode output given by eq. 4 with an exponential kernel and time
constant ? . If ? is significantly less than the amount of time ?N needed for the system to
process all the nodes relative to a single time step, we can minimize the crosstalk between
node states relative to different time steps.
Let us consider the input I(t) of the readout, and let t = 0 be the instant where the state of
the first node for a given discrete time step n begins to be encoded in I(t) . Using equation
4, we can write the voltage Q(t) on the capacitor at time ?N as
? ?N
?N ?s
? ?N
?
Q(?N ) = Q(0)e
+
(5)
I(s)W (s)e? ? ds
0

For 0 < t < ?N , we have
I(t) = xi (n), W (t) = wi , for ?(i ? 1) < t < ?i

(6)

Integrating equation 5 yields
Q(?N ) = Q(0)e

? ?N
?

+

N
X

xi (n)?i wi , ?i = e?

?(N ?i)
?

?

(1 ? e? ? )?

(7)

i=1

Equation 7 shows that, at time ?N , the voltage on the capacitor is a linear combination of
the reservoir states for the discrete time n, with node-dependent coefficients ?i wi , plus a
?N
residual of the voltage at time 0, multiplied by an extinction coefficient e? ? . At time 2?N
the voltage on the capacitor would be a linear combination of the states for discrete time
n + 1, multiplied by the same coefficients, plus a residual of the voltage at time ?N , and so
on for all values of n and corresponding multiples of ?N .
i
A simple procedure would encode the weights wi = W
?i onto the voltage V (t) that drives the
modulator , provide an external, constant bias Wb , and have the output y(n) of the reservoir,
defined by equation 2, effectively encoded on the capacitor. This simple procedure would
however be unsatisfactory because unavoidably some of the ?i would be very small, and
therefore the wi would be large, spanning several orders of magnitude. This is undesirable,
as it requires a very precise control of the modulation voltage V (t) in order to recreate all
the wi values, leaving the system vulnerable to noise and to any non-ideal behavior of the
modulator itself.

5

0.04

a

?N

0.04
0.03

b

0.02
0

c

10
Readout
Output

?

0.05

Voltage (V)

Voltage (V)

0.06

5
0

?0.02

?

?5
0.02

?0.04
10

12.5

15

17.5

20

22.5

10

12.5

15

17.5

20

22.5

10

12.5

15
17.5
Time (?s)

20

22.5

Figure 2: a) Reservoir output I(t). The gray line represents the output as measured by
a photodiode and an oscilloscope. We indicated for reference the time ? = 130ns used to
process a single node and the duration ?N = 8.36?s of the whole set of states. b) Output
P (t) of the balanced photodiode (see equation 4), with the trace of panel a) as input, before
integration. c) Voltage Q(t) on the capacitor for the same input (see equation 5). The
integration time ? is indicated for reference. The black dots indicate the values at the end
of each discretized time n, taken as the output y(n)of the analog readout.

To mitigate this, we adapt the training algorithm based on ridge regression to our case. We
redefine the reservoir states as ?i (n) = xi (n)?i ; we then calculate the weights ?i that, applied
to the states ?i , give the best approximation to the desired output y?(n). The advantage here
is that ridge regression keeps the norm of the weight vector to a minimum; by redefining
the states, we can take the ?i into account without having big values of wi that force us to
be extremely precise in generating the readout weights.
A sample trace of the voltage on the capacitor is shown in Figure 2c.
Hardware implementation
To implement the analog readout, we started from the experimental architecture described
in Section 2, and we added the components depicted in the right part of Figure 1. For the
weight multiplication, we used a second Mach-Zehnder modulator (Photline model MXDOLN-10 with bandwidth in excess of 10GHz and V? = 5.9V ), driven by a Tabor 2074 Arbitrary
Waveform Generator (maximum sampling rate 200 MSamples/s). The two outputs of the
modulator were fed into a balanced photodiode (Terahertz technologies model 527 InGaAs
balanced photodiode, bandwidth set to 125MHz, response set to 1000V/W), whose output was read by the National Instruments PXI digital acquisition card (sampling rate 200
MSamples/s).
In most of the experimental results described here, the capacitor at the end of the circuit
was simulated and not physically inserted into the circuit: this allowed us to quickly cycle
in our experiments through different values of ? without taking apart the circuit every
time. The external bias Wb to the output, introduced in equation 2, was also provided
after the readout. The reasoning behind these choices is that both these implementations
are straightforward, while the use of a modulator and a balanced photodiode as a weight
generator is more complex: we chose to focus on the latter issue for now, as our goal is to
validate the proposed architecture.

4

Results

As a benchmark for our analog readout, we use a wireless channel equalization task, introduced in 1994 [19] to test adaptive bilinear filtering and subsequently used by Jaeger [16] to
show the capabilities of reservoir computing. This task is becoming a standard benchmark
task in the reservoir computing community, and has been used for example in [20]. It consists in recovering a sequence of symbols transmitted along a wireless channel, in presence
of multiple reflections, noise and nonlinear distortion; a more detailed description of the
task can be found in the Appendix. The performance of the reservoir is usually measured
in Symbol Error Rate (SER), i.e. the rate of misinterpreted symbols, as a function of the
amount of noise in the wireless channel.
6

?1

?1

10

?3

?2

SER

?2

SER

SER

0.1

10

10

10

?3

10

10
12

16

20

24

Input noise [dB]

28

32

0.05

0
12

16

20

24

Input noise [dB]

28

32

0.2

0.3

?/?N

0.4

0.5

Figure 3: Performance of the analog readout. Left: Performance as a function of the input
SNR, for a reservoir of 28 nodes, with ? /?N = 0.18. Middle: Performance for the same
task, for a reservoir of 64 nodes, ? /?N = 0.18. Right: Performance as a function of the
ratio ? /?N , at constant input noise level (28 dB SNR) for a reservoir of 64 nodes. The
performance is measured in Signal Error Rate (SER). Blue triangles: reservoir with digital
readout. Red squares: reservoir with ideal analog readout. Black circles: reservoir with
experimental analog readout (simulated capacitor). Purple stars in the left panel: reservoir
where a physical capacitor has been used.

Figure 3 shows the performance of the experimental setup of [10] for a network of 28 nodes
and one of 64 nodes, for different amounts of noise. For each noise level, three quantities
are presented. The first is the performance of the reservoir with a digital readout (blue
triangles), identical to the one used in [10]. The second is the performance of a simulated,
ideal analog readout, which takes into account the effect of the ?i coefficients introduced in
PN
equation 7, but no other imperfection. It produces as output the discrete sum ?b + i=1 ?i ?i
(red squares). This is, roughly speaking, the goal performance for our experimental readout.
The third and most important is the performance of the reservoir as calculated on real data
taken from the analog reservoir with the analog output, with the effect of the continuous
capacitive integration computed in simulation (black circles).
As can be seen from the figure, the performance of the analog readout is fairly close to its
ideal value, although it is significantly worse than the performance of the digital readout.
However, it is already better than the non-reservoir methods reported in [19] and used by
Jaeger as benchmarks in [16]. It can also handle higher signal-to-noise ratios. As expected,
networks with more nodes have better performance; it should be noted, however, that in
experimental reservoirs the number of nodes cannot be raised over a certain threshold.
The reason is that the total loop time ?N is determined by the experimental hardware
(specifically, the length of the delay line); as N increases, the length ? of each node must
decrease. This leaves the experiment vulnerable to noise and bandpass effect, that may
lead, for example, to an incorrect discretization of the xi (n) values, and an overall worse
performance.
We did test our readout with a 70nF capacitor, with a network of 28 nodes, to prove that the
physical implementation of our concept is feasible: the performance of this setup is shown
in the left panel of Figure 3. The results are comparable to those obtained in simulation,
even if, at low levels of noise in the input, the performance of the physical setup is slightly
worse.
The rightmost panel of figure 3 shows the effects of the choice of the capacitor at the end
of the circuit, and therefore of the value of ? . The plot represents the performance at 28
dB SNR for a network of 64 nodes, for different values of the ratio ? /?N , obtained by
averaging the results of 10 tests. It is clear that the choice of ? has a complicated effect on
the readout performance; however, some general rules may be inferred. Too small values
of ? mean that the contribution from the very first nodes is vanishingly small, effectively
decreasing the reservoir dimensionality, which has a strong impact on the performance both
of the ideal and the experimental reservoir. On the other hand, larger values of ? impact
the performance of the experimental readout, as the residual term in equation 7 gets larger.
A compromise value of ? /?N = 0.222 seems to give the best result, corresponding in our
case to a capacity of about 70 nF.
7

5

Discussion

To our knowledge, the system presented here is the first analog readout for an experimental
reservoir computer. While the results presented here are preliminary, and there is much
optimization of experimental parameters to be done, the system already outperforms nonreservoir methods. We expect to extend easily this approach to different tasks, already
studied in [9, 10], including a spoken digit recognition task on a standard dataset[22].
Further performance improvements can reasonably be expected from fine-tuning of the training parameters: for instance the amount of regularization in the ridge regression procedure,
that here is left constant at 1?10?4 , should be tuned for best performance. Adaptive training
algorithms, such as the ones mentioned in [21], could also take into account nonidealities in
the readout components. Moreover the choice of ?, as Figure 3 shows, is not obvious and a
more extensive investigation could lead to better performance.
The architecture proposed here is simple and quite straightforward to realize; it can be
added at the output of any preexisting time multiplexing reservoir with minimal effort. The
capacitor at the end of the circuit could be substituted with an active electronic circuit
performing the summation of the incoming signal before resetting itself. This would eliminate the problem of residual voltages, and allow better performance at the cost of increased
complexity of the readout.
The main interest of the analog readout is that it allows optoelectronic reservoir computers
to fully leverage their main characteristic, which is the speed of operation. Indeed, removing
the need for slow, offline postprocessing is indicated in [13] as one of the major challenges
in the field. Once the training is finished, optoelectronic reservoirs can process millions of
nonlinear nodes per second [10]; however, in the case of a digital readout, the node states
must be recovered and postprocessed to obtain the reservoir outputs. It takes around 1.6
seconds for the digital readout in our setup to retrieve and digitize the states generated by a
9000 symbol input sequence. The analog readout removes the need for postprocessing, and
can work at a rate of about 8.5 ?s per input symbol, five orders of magnitude faster than
the electronic reservoir reported in [8].
Finally, having an analog readout opens the possibility of feedback - using the output of the
reservoir as input or part of an input for the successive time steps. This opens the way for
different tasks to be performed [15] or different training techniques to be employed [14].

Appendix: Nonlinear Channel Equalization task
What follows is a detailed description of the channel equalization task. The goal is to
reconstruct a sequence d(n) of symbols taken from {?3, ?1, 1, 3}. The symbols in d(n) are
mixed together in a new sequence q(n) given by
q(n) = 0.08d(n + 2) ? 0.12d(n + 1) + d(n) + 0.18d(n ? 1) ? 0.1d(n-2)
(8)
+0.091d(n ? 3)-0.05d(n ? 4) + 0.04d(n ? 5) + 0.03d(n ? 6) + 0.01d(n-7)
which models a wireless signal reaching a receiver through different paths with different
traveling times. A noisy, distorted version u(n) of the mixed signal q(n), simulating the
nonlinearities and the noise sources in the receiver, is created by having u(n) = q(n) +
0.036q(n)2 ? 0.011q(n)3 + ?(n), where ?(n) is an i.i.d. Gaussian noise with zero mean
adjusted in power to yield signal-to-noise ratios ranging from 12 to 32 dB. The sequence
u(n) is then fed to the reservoir as an input; the output of the readout R(n) is rounded off to
the closest value among {?3, ?1, 1, 3}, and then compared to the desired symbol d(n). The
performance is usually measured in Signal Error Rate (SER), or the rate of misinterpreted
symbols.

Acknowledgements
This research was supported by the Interuniversity Attraction Poles program of the Belgian Science Policy Office, under grant IAP P7-35 ?photonics@be? and by the Fonds de la
Recherche Scientifique FRS-FNRS.
8

References
[1] Jaeger, H. The "echo state" approach to analysing and training recurrent neural networks.
Technical report, Technical Report GMD Report 148, German National Research Center for
Information Technology, 2001.
[2] Maass, W., Natschlager, T., and Markram, H. Real-time computing without stable states:
A new framework for neural computation based on perturbations. Neural computation,
14(11):2531?2560, 2002.
[3] Schrauwen, B., Verstraeten, D., and Van Campenhout, J. An overview of reservoir computing:
theory, applications and implementations. In Proceedings of the 15th European Symposium on
Artificial Neural Networks, pages 471?482, 2007.
[4] Lukosevicius, M. and Jaeger, H. Reservoir computing approaches to recurrent neural network
training. Computer Science Review, 3(3):127?149, 2009.
[5] Fernando, C. and Sojakka, S. Pattern recognition in a bucket. Advances in Artificial Life,
pages 588?597, 2003.
[6] Schurmann, F., Meier, K., and Schemmel, J. Edge of chaos computation in mixed-mode vlsi a hard liquid. In In Proc. of NIPS. MIT Press, 2005.
[7] Paquot, Y., Dambre, J., Schrauwen, B., Haelterman, M., and Massar, S. Reservoir computing:
a photonic neural network for information processing. volume 7728, page 77280B. SPIE, 2010.
[8] Appeltant, L., Soriano, M. C., Van der Sande, G., Danckaert, G., Massar, S., Dambre, J.,
Schrauwen, B., Mirasso, C. R., and Fischer, I. Information processing using a single dynamical
node as complex system. Nature Communications, 2:468, 2011.
[9] Larger, L., Soriano, M. C., Brunner, D., Appeltant, L., Gutierrez, J. M., Pesquera, L., Mirasso,
C. R. , and Fischer, I. Photonic information processing beyond Turing: an optoelectronic
implementation of reservoir computing. Optics Express, 20(3):3241, 2012.
[10] Paquot, Y., Duport, F., Smerieri, A., Dambre, J., Schrauwen, B., Haelterman, M., and Massar,
S. Optoelectronic reservoir computing. Scientific reports, 2:287, January 2012.
[11] Legenstein, R. and Maass, W. What makes a dynamical system computationally powerful?
In Simon Haykin, Jos? C. Principe, Terrence J. Sejnowski, and John McWhirter, editors, New
Directions in Statistical Signal Processing: From Systems to Brain. MIT Press, 2005.
[12] Vandoorne, K., Fiers, M., Verstraeten, D., Schrauwen, B., Dambre, J., and Bienstman, P.
Photonic reservoir computing: A new approach to optical information processing. In 2010
12th International Conference on Transparent Optical Networks, pages 1?4. IEEE, 2010.
[13] Woods, D. and Naughton, T. J. Optical computing: Photonic neural networks. Nature Physics,
8(4):257?259, April 2012.
[14] Sussillo, D. and Abbott, L. F. Generating coherent patterns of activity from chaotic neural
networks. Neuron, 63(4):544?57, 2009.
[15] Jaeger, H., Lukosevicius, M., Popovici, D., and Siewert, U. Optimization and applications of
echo state networks with leaky-integrator neurons. Neural networks : the official journal of
the International Neural Network Society, 20(3):335?52, 2007.
[16] Jaeger, H. and Haas, H. Harnessing nonlinearity: predicting chaotic systems and saving energy
in wireless communication. Science, 304(5667):78?80, 2004.
[17] Verstraeten, D., Dambre, J., Dutoit, X., and Schrauwen, B. Memory versus non-linearity in
reservoirs. In The 2010 International Joint Conference on Neural Networks (IJCNN), pages
1?8. IEEE, 2010.
[18] Wyffels, F. and Schrauwen, B. Stable output feedback in reservoir computing using ridge
regression. Artificial Neural Networks-ICANN, pages 808?817, 2008.
[19] Mathews. V. J. Adaptive algorithms for bilinear filtering. Proceedings of SPIE, 2296(1):317?
327, 1994.
[20] Rodan, A., and Tino, P. Minimum complexity echo state network. IEEE transactions on
neural networks, 22(1):131?44, January 2011.
[21] Legenstein, R., Chase, S. M., Schwartz, A. B., and Maass, W. A reward-modulated hebbian learning rule can explain experimentally observed network reorganization in a brain control task. The Journal of neuroscience : the official journal of the Society for Neuroscience,
30(25):8400?10, 2010.
[22] Texas Instruments-Developed 46-Word Speaker-Dependent Isolated Word Corpus (TI46),
September 1991, NIST Speech Disc 7-1.1 (1 disc) (1991).

9


----------------------------------------------------------------

title: 256-performance-of-connectionist-learning-algorithms-on-2-d-simd-processor-arrays.pdf

810

Nunez and Fortes

Performance of Connectionist Learning Algorithms
on 2-D SIMD Processor Arrays

Fernando J. Nunez* and Jose A.B. Fortes
School of Electrical Engineering
Purdue University
West Lafayette, IN 47907

ABSTRACT
The mapping of the back-propagation and mean field theory
learning algorithms onto a generic 2-D SIMD computer is
described. This architecture proves to be very adequate for these
applications since efficiencies close to the optimum can be
attained. Expressions to find the learning rates are given and
then particularized to the DAP array procesor.

1 INTRODUCTION
The digital simulation of connectionist learning algorithms is flexible and
accurate. However, with the exception of very small networks, conventional
computer architectures spend a lot of time in the execution of simulation
software. Parallel computers can be used to reduce the execution time. Vectorpipelined, multiprocessors, and array processors are some of the most important
classes of parallel computers 3 . Connectionist or neural net (NN) learning
algorithms have been mapped onto all of them.
The focus of this contribution is on the mapping of the back-propagation (BP)
and mean field theory (MFT) learning algorithms onto the subclass of SIMD
computers with the processors arranged in a square two-dimensional mesh and
interconnected by nearest-neighbor links.
The material is organized as follows. In section 2, the execution cost of BP and
MFT on sequential computers is found. Two-dimensional SIMD processor arrays
are described in section 3, and the costs of the two dominanting operations in the
simulations are derived. In section 4 the mapping of BP and MFT is I;ommented

* Current address: Motorola Inc., 1301

E Algonquin Rd., Schaumburg, IL 60196

Performance of Connectionist Learning Algorithms

and expressions for the learning rates are obtained. These expressions are
particularized to the DAP computer in section 5. Section 6 concludes this work.

2 BACK-PROPAGATION AND MEAN FIELD THEORY
In this paper, two learning algorithms: Bp 7 and MFT4; and 3-layer nets are
considered. The number of neurons in the input, hidden, and output layer is I, H,
and 0 respectively. BP has been used in many applications. Probably, NETtalk8
is the best known. MFT can also be used to learn arbitrary mappings between
two sets, and remarkably, to find approximate solutions to hard optimization
problems much more efficiently than a Boltzmann Machine does 4,5.
The
Vj

=

output

f ( ~ajjvj

of
-

a

neuron

i will be denoted as

Vi

and

called

value:

OJ). The summation represents the net input received and will

j'l"j

be called activation. The neuron thresold is OJ. A sigmoid-like function f is
applied to find the value. The weight of the link from neuron j to neuron i is ajj.
Since input patterns are the values of the I layer, only neuron values and
activations of the Hand 0 layers must be computed. In BP, the activation error
and the value error of the Hand 0 layers are calculated and used to change the
weights.
In a conventional computer, the execution time of BP is approximately the time
spent in finding the activations, back-propagating the activation error of the 0
layer, and modifying the I-H and H-O weights. The result is: (21 + 30)Htm'
where tm is the time required to perform a multiply/accumulate operation. Since
the net has (I + O)H connections, the learning rate in connections per second is:

1+ 0

fNBP =

(21 + 30)tm

CPS

In the MFT algorithm, only from the neuron values in equilibrium at the end of
the clamped and free annealing phases we can compute the weight increments. It
is assumed that in both phases there are A annealing temperature~ ~nd that E
iterations are enough to reach equilibrium at each temperature 4,5. With these
changes, MFT is now a deterministic algorithm where the anne ling phases are
composed of AE sweeps. The MFT execution time can be apprl?"jmated by the
time spent in computing activations in the annealing loops. T J,ing into account
that in. the clamped phase only the H layer is updated, and tha ', in the free phase
both, the Hand 0 layers change their values, the MFT leaning performance is
found to be:
ft

tMFT =

tBP

AE

CPS

MFT is AE times more expensive than BP. However, the learning qualities of
both algorithms are different and such a direct cOP'tJarison is simplistic.

811

812

Nunez and Fortes

3 2-D SIMD PROCESSOR ARRAYS
Two-dimensional single instruction multiple data stream (2-D SIMD) computers
are very efficient in the simulation of NN learning algorithms. They can provide
massive parallelism at low cost. An SIMD computer is an array of processing
elements (PEs) that execute the same instruction in each cycle. There is a single
control unit that broadcasts instructions to all the PEs. SIMD architectures
operate in a synchronous, lock-step fashion 3 ? They are also called array procesors
because their raison cfetre is to operate on vectors and matrices.
Example SIMD computers are the Illiac-IV, the Massively Parallel Processor
(MPP), the Connection Machine (CM), and the Distributed Array Processor
(DAP). With the exception of the CM, whose PE interconnection topology is a
hypercube, the other three machines are 2-D SThAD arrays because their PEs are
interconnected by a 2-D mesh with wrap-around links (figure 1).

CONTROL

UNIT

1----4

pp

Figure 1: A 2-D SIMD Processor Array
Each PE has its own local memory. The instruction has an address field to access
it. The array memory space can be seen as a 3-D volume. This volume is
generated by the PE plane, and the depth is the number of memory words that
each PE can address. When the control unit issues an address, a plane of the
memory volume is being referenced. Then, square blocks of PxP elements are the
natural addressing unit of 2-D SThAD processor arrays. There is an activity bit
register in each PE to disable the execution of instructions. This is useful to
perform operations with a subset of the PEs. It is assumed that there is no

Performance of Connectionist Learning Algorithms
overlapping between data processing an data moving operations. In other words,
PEs can be either performing some operation on data (this includes accessing the
local memory) or exchanging data with other processors.

3.1

MAPPING THE TWO BASIC OPERATIONS

It is characteristic of array processors that the way data is allocated into the PEs
memories has a very important effect on performance. For our purposes, two
data structures must be considered: vectors and matrices. The storage of vectors
is illustrated in figure 2-a. There are two modes: row and column. A vector is
split into P-element subvectors stored in the same memory plane. Very large
vectors will require two or more planes. The storage of matrices is also very
simple. They must be divided into square PXP blocks (figure 2-b). The shading
in figure 2 indicates that, in general, the sizes of vectors and matrices do not fit
the array dimensions perfectly.

p
(a)

~P

(b)

?
row

[IIJ
column
Figure 2: (a) Vector and (b) Matrix Storage
The execution time of BP and MFT in a 2-D SIMD computer is spent, almost
completely,
in
matrix-vector
multiply
(MVM)
and
vector
outer
multiply/accumulate (VOM) operations. They can be decomposed in the
following simpler operations involving PxP blocks.
a) Addition (+): C = A + B such that eij = aij + bij.
b) Point multiply/accumulate (-):
= C + A-B such that e'ij = eij + aijb ij ?
c) Unit rotation: The result block has the same elements than the original, but
rotated one place in one of the four possible directions (N, E, W, and S).
d) Row (column) broadcast: The result of the row (column) broadcast of a vector
x stored in row (column) mode is a block X such that xii = Xj ( = Xi).

a

The time required to execute a, b, c, and d will be denoted as tll' tm , t,., and t6
respectively. Next, let us see how the operation y = Ax (MVM) is decomposed in
simpler steps using the operations above. Assume that x and yare P-element
vectors, and A is a PXP block.

813

814

Nunez and Fortes

1) Row-broadcast vector x.

2) Point multiply Y = A?X.
3) Row addition of block Y,

t

Yi = f'llij =
aijxj'
j=1
j-l

This requires flOg2pl steps. In

each step multiple rotations and one addition are performed. Figure 3 shows how
eight values in the same row are added using the recursive doubling technique.
Note that the number of rotations doubles in each step. The cost is:
Pt r + log2Pto' Row addition is an inefficient operation because of the large cost
due to communication. Fortunately, for larger data its importance can be
diminished by using the scheduling described nextly.

00000000
....-

....-

....-

....-

+

+

+

+

?

.

+

+

..

+

Figure 3: Recursive Doubling

Suppose that x, y, and A have dimensions m = MP, n = NP, and nxm
respectively. Then, y = Ax must be partitioned into a sequence of nonpartitioned block operations as the one explained above. We can write:
yi

=

M

M

j=1

j=1

~Aijxj = ~(Aij?Xj)u

M

= (~Aij.Xj)u
j=1

In this expression, yi and x j represent the i-th and i-th P-element subvector of y
and x respectively, and A ij is the PxP block of A with indices i and i. Block Xi
is the result of row-broadcasting xj (x is stored in row mode.) Finally, u is a
vector with all its P-elements equal to 1. Note that in the second term M column
additions are implicit, while only one is required in the third term because blocks
instead of vectors are accumulated. Since 'II has N subvectors, and the M
subvectors of x are broadcast only once, the total cost of the MVM operation is:

Mter a similar development, the cost of the YOM ( At = A

+ yx T

)

operation is:

Performance of Connectionist Learning Algorithms

If the number of neurons in each layer is not an integer multiple of P, the storage
and execution efficiencies decrease. This effect is less important in large networks.

4 LEARNING RATES ON 2-D SIMD COMPUTERS
4.1

BACK-PROPAGATION

The neuron val~es, activations, value errors, activation errors, and thresolds of
the Hand 0 layers are organized as vectors. The weights are grouped into two
matrices: I-H and H-O. Then, the scalar operations of the original algorithm are
transformed into matrix-vector operations.
From now on, the size of the input, hidden, and output layers will be IP, HP, and
OP. .A13 commented before, the execution time is mostly spent in computing
activations, values, their errors, and in changing the weights. To compute
activations, and to back-propagate the activation error of the 0 layer MVM
operations are performed. The change of weights requires YOM operations. Alter
substituting the expressions of the previous section, the time required to learn a
pattern simulating BP on a 2-D SIMD computer is:

The time spent in data communication is given by the factors in tr and t,. The
larger they are, the smaller is the efficiency. For array processors with fast
broadcast facilities, and for nets large enough in terms of the array dimensions,
the efficiency grows since a smaller fraction of the total execution time is
dedicated to moving data. Since the net has (I + O)HP2 connections, the
learning rate is p2 times greater than using a single PE:

(I

f..
NSIMD-BP

4.2

= (21

+ O)p2

+ 30)tm

CPS

MEAN FIELD THEORY

The operations outside the annealing loops can be neglected with small error. In
consequence, only the computation of activations in the clamped and free
annealing phases is accounted for:

AE((21 + 30)Htm + {21 + H + 20)t, + (2H + O)(Ptr + log2Pta))
Under the same favorable conditions above mentioned, the learning rate is:

_
!:SIMD-MFT -

(I + O)P2
AE(21 + 30)tm

CPS

815

816

Nunez and Fortes

() LEARNING PERFORMANCE ON THE DAP
The DAP is a commercial 2-D SIMD processor array developed by lCL. It is a
massively parallel computer with bit-level PEs built around a single-bit full
adder. In addition to the 2-D PE interconnection mesh, there are row and column
broadcast buses that allow the direct transfer of data from any processor row or
column to an edge register. Many instructions require a single clock cycle leading
to very efficient codings of loop bodies. The DAP-510 computer features 25 x2 5
PEs with a maximum local memory of 1Mbit per PE. The DAP-610 has 26 x2 6
PEs, and the maximum local memory IS 64Kbit. The clock cycle in both
machines is 100 nsl.
With bit-level processors it is possible to tailor the preCISIon of fixed-point
computations to the minimum required by the application. The costs in cycles
required by several basic operations are given below. These expressions are
function of the number of bits of the operands, that has been assumed to be the
same for all of them: b bits.
The time required by the DAP to perform a block addition, point
multiplication/accumulation, and broadcast is to = 2b, tm = 2b 2 , and t6 = 8b
clock cycles respectively. On the other hand, P + 2b log2P cycles is the duration
of a row addition. Let us take b = 8 bits, and AE = 24. This values have been
found adequate in many applications. Then, the maximum learning rates of the
DAP-610 (P = 64) are:
100-160 MCPS

BP:

MFT: 4.5-6.6 MCPS

where MCPS = 106 CPS. These figures are 4 times smaller for the DAP-510. It is
worth to mention that the performance decreases quadratically with b. The two
learning rates of each algorithm correspond to the worst and best case topology.

6.1

EXAMPLES

Let us consider a one-thousand neuron net with 640, 128, and 256 neurons in the
input, hidden, and output layer. For the DAP-610 we have 1= 10, H = 2, and
o = 4. The other parameters are the same than used above. After substituting,
we see that the communication costs are less than 10% of the total,
demonstrating the efficiency of the DAP in this type of applications. The learning
rates are:
BP:

140 MCPS

MFT: 5.8 MCPS

NETtalk 10 is frequently used as a benchmark in order to compare the
performance achieved on different computers. Here, a network with similar
dimensions is considered: 224 input, 64 hidden, and 32 output neurons. These
dimensions fit perfectly into the DAP-510 since P = 32. ~ before, a data
precision of 8 bits has been taken. However, the fact than the input patterns are
binary has been exploited to obtain some savings.
The performance reached in this case is 50 MCPS. Even though NETtalk is a
relatively small network, only 30% of the total execution time is spent in data
communication. If the DAP-610 were used, somewhat less than 200 MCPS would
be learnt since the output layer is smaller than P what causes some inefficiency.

Performance of Connectionist Learning Algorithms

Finally, BP learning rates of the DAP-610 with 8- and 16-bit operands are
compared to those obtained by other machines below 2,6:

COMPUTER
VAX 780
CRAY-2
CM (65K PEs)
DAP-610 (8 bits)
DAP-610 (16 bits)

MCPS
0.027
7

13
100-160
25-40

6 CONCLUSIONS
Two-dimensional SThfl) array processors are very adequate for the simulation of
connectionist learning algorithms like BP and :MFT. These architectures can
execute them at nearly optimum speed if the network is large enough, and there is
full connectivity between layers. Other much more costly parallel architectures
are outperformed.
The mapping approach described in this paper can be easily extended to any
network topology with dense blocks in its global interconnection matrix.
However, it is obvious that 2-D SIMD arrays are not a good option to simulate
networks with random sparse connectivity.
Acknow ledgements

This work has been supported by the Ministry of Education and Science of Spain.
References

[1] (1988) AMT DAP Series, Technical Overview. Active Memory Technology.
[2] G. Blelloch & C. Rosenberg. (1987) Network Learning on the Connection
Machine. Proc. 10th Joint Coni. on Artificial Intelligence, IJCA Inc.
[3] K. Hwang & F. Briggs. (1984) Computer Architecture and Parallel Processing,
McGraw-Hill.
[4] C. Peterson & J. Anderson. (1987) A Mean Field Theory Learning Algorithm
for Neural Networks. Complex Systems, 1:995-1019.
[5] C. Peterson & B. Soderberg. (1989) A New Method For Mapping Optimization
Problems onto Neural Networks. Int'/ J. 01 Neural Systems, 1(1):3-22.
[6] D. Pomerleau, G. Gusciora, D. Touretzky & H.T. Kung. (1988) Neural
Network Simulation at Warp Speed: How We Got 17 Million Connections per
Second. Proc. IEEE Int'l Coni. on Neural Networks, 11:143-150.
[7] D. Rumelhart, G. Hinton & R. Williams. (1986) Learning Representations by
Back-Propagating Errors. Nature, (323):533-536.
[8] T. Sejnowski & C. Rosenberg. (1987) Parallel Networks that Learn to
Pronounce English Text. Complex Systems, 1:145-168.

817


----------------------------------------------------------------

title: 2571-using-machine-learning-to-break-visual-human-interaction-proofs-hips.pdf

Using Machine Learning to Break Visual
Human Interaction Proofs (HIPs)
Kumar Chellapilla
Microsoft Research
One Microsoft Way
Redmond, WA 98052
kumarc@microsoft.com

Patrice Y. Simard
Microsoft Research
One Microsoft Way
Redmond, WA 98052
patrice@microsoft.com

Abstract
Machine learning is often used to automatically solve human tasks.
In this paper, we look for tasks where machine learning algorithms
are not as good as humans with the hope of gaining insight into
their current limitations. We studied various Human Interactive
Proofs (HIPs) on the market, because they are systems designed to
tell computers and humans apart by posing challenges presumably
too hard for computers. We found that most HIPs are pure
recognition tasks which can easily be broken using machine
learning. The harder HIPs use a combination of segmentation and
recognition tasks. From this observation, we found that building
segmentation tasks is the most effective way to confuse machine
learning algorithms. This has enabled us to build effective HIPs
(which we deployed in MSN Passport), as well as design
challenging segmentation tasks for machine learning algorithms.

1

In t rod u ct i on

The OCR problem for high resolution printed text has virtually been solved 10 years
ago [1]. On the other hand, cursive handwriting recognition today is still too poor
for most people to rely on. Is there a fundamental difference between these two
seemingly similar problems?
To shed more light on this question, we study problems that have been designed to
be difficult for computers. The hope is that we will get some insight on what the
stumbling blocks are for machine learning and devise appropriate tests to further
understand their similarities and differences.
Work on distinguishing computers from humans traces back to the original Turing
Test [2] which asks that a human distinguish between another human and a machine
by asking questions of both. Recent interest has turned to developing systems that
allow a computer to distinguish between another computer and a human. These
systems enable the construction of automatic filters that can be used to prevent
automated scripts from utilizing services intended for humans [4]. Such systems
have been termed Human Interactive Proofs (HIPs) [3] or Completely Automated
Public Turing Tests to Tell Computers and Humans Apart (CAPTCHAs) [4]. An
overview of the work in this area can be found in [5]. Construction of HIPs that are
of practical value is difficult because it is not sufficient to develop challenges at

which humans are somewhat more successful than machines. This is because the
cost of failure for an automatic attacker is minimal compared to the cost of failure
for humans. Ideally a HIP should be solved by humans more than 80% of the time,
while an automatic script with reasonable resource use should succeed less than
0.01% of the time. This latter ratio (1 in 10,000) is a function of the cost of an
automatic trial divided by the cost of having a human perform the attack.
This constraint of generating tasks that are failed 99.99% of the time by all
automated algorithms has generated various solutions which can easily be sampled
on the internet. Seven different HIPs, namely, Mailblocks, MSN (before April 28th,
2004), Ticketmaster, Yahoo, Yahoo v2 (after Sept?04), Register, and Google, will
be given as examples in the next section. We will show in Section 3 that machinelearning-based attacks are far more successful than 1 in 10,000. Yet, some of these
HIPs are harder than others and could be made even harder by identifying the
recognition and segmentation parts, and emphasizing the latter. Section 4 presents
examples of more difficult HIPs which are much more respectable challenges for
machine learning, and yet surprisingly easy for humans. The final section discusses
a (known) weakness of machine learning algorithms and suggests designing simple
artificial datasets for studying this weakness.

2

Exa mp les o f H I Ps

The HIPs explored in this paper are made of characters (or symbols) rendered to an
image and presented to the user. Solving the HIP requires identifying all characters
in the correct order. The following HIPs can be sampled from the web:
Mailblocks: While signing up for free email service with
(www.mailblocks.com), you will find HIP challenges of the type:

mailblocks

MSN: While signing up for free e-mail with MSN Hotmail (www.hotmail.com), you
will find HIP challenges of the type:

Register.com: While requesting a whois lookup for a domain at www.register.com,
you will HIP challenges of the type:

Yahoo!/EZ-Gimpy (CMU): While signing up for free e-mail service with Yahoo!
(www.yahoo.com), you will receive HIP challenges of the type:

Yahoo! (version 2): Starting in August 2004, Yahoo! introduced their second
generation HIP. Three examples are presented below:

Ticketmaster: While looking for concert tickets at www.ticketmaster.com, you
will receive HIP challenges of the type:

Google/Gmail: While signing up for free e-mail with Gmail at www.google.com,
one will receive HIP challenges of the type:

While solutions to Yahoo HIPs are common English words, those for ticketmaster
and Google do not necessarily belong to the English dictionary. They appear to have
been created using a phonetic generator [8].

3

Usi n g ma ch i n e lea rn i n g t o b rea k H IP s

Breaking HIPs is not new. Mori and Malik [7] have successfully broken the EZGimpy (92% success) and Gimpy (33% success) HIPs from CMU. Our approach
aims at an automatic process for solving multiple HIPs with minimum human
intervention, using machine learning. In this paper, our main goal is to learn more
about the common strengths and weaknesses of these HIPs rather than to prove that
we can break any one HIP in particular with the highest possible success rate. We
have results for six different HIPs: EZ-Gimpy/Yahoo, Yahoo v2, mailblocks,
register, ticketmaster, and Google.
To simplify our study, we will not be using language models in our attempt to break
HIPs. For example, there are only about 600 words in the EZ-Gimpy dictionary [7],
which means that a random guess attack would get a success rate of 1 in 600 (more
than enough to break the HIP, i.e., greater than 0.01% success). HIPs become harder
when no language model is used. Similarly, when a HIP uses a language model to
generate challenges, success rate of attacks can be significantly improved by
incorporating the language model. Further, since the language model is not common
to all HIPs studied, it was not used in this paper.
Our generic method for breaking all of these HIPs is to write a custom algorithm to
locate the characters, and then use machine learning for recognition. Surprisingly,
segmentation, or finding the characters, is simple for many HIPs which makes the
process of breaking the HIP particularly easy. Gimpy uses a single constant
predictable color (black) for letters even though the background color changes. We
quickly realized that once the segmentation problem is solved, solving the HIP
becomes a pure recognition problem, and it can trivially be solved using machine
learning. Our recognition engine is based on neural networks [6][9]. It yielded a
0.4% error rate on the MNIST database, uses little memory, and is very fast for
recognition (important for breaking HIPs).
For each HIP, we have a segmentation step, followed by a recognition step. It
should be stressed that we are not trying to solve every HIP of a given type i.e., our
goal is not 100% success rate, but something efficient that can achieve much better
than 0.01%.

In each of the following experiments, 2500 HIPs were hand labeled and used as
follows (a) recognition (1600 for training, 200 for validation, and 200 for testing),
and (b) segmentation (500 for testing segmentation). For each of the five HIPs, a
convolution neural network, identical to the one described in [6], was trained and
tested on gray level character images centered on the guessed character positions
(see below). The trained neural network became the recognizer.
3.1

M a i l b l oc k s

To solve the HIP, we select the red channel, binarize and erode it, extract the largest
connected components (CCs), and breakup CCs that are too large into two or three
adjacent CCs. Further, vertically overlapping half character size CCs are merged.
The resulting rough segmentation works most of the time. Here is an example:

For instance, in the example above, the NN would be trained, and tested on the
following images:

?
The end-to-end success rate is 88.8% for segmentation, 95.9% for recognition
(given correct segmentation), and (0.888)*(0.959)7 = 66.2% total. Note that most of
the errors come from segmentation, even though this is where all the custom
programming was invested.
3.2

Register

The procedure to solve HIPs is very similar. The image was smoothed, binarized,
and the largest 5 connected components were identified. Two examples are
presented below:

The end-to-end success rate is 95.4% for segmentation, 87.1% for recognition
(given correct segmentation), and (0.954)*(0.871)5 = 47.8% total.
3.3

Y a h oo/ E Z - G i mp y

Unlike the mailblocks and register HIPs, the Yahoo/EZ-Gimpy HIPs are richer in
that a variety of backgrounds and clutter are possible. Though some amount of text
warping is present, the text color, size, and font have low variability. Three simple
segmentation algorithms were designed with associated rules to identify which
algorithm to use. The goal was to keep these simple yet effective:
a) No mesh: Convert to grayscale image, threshold to black and white, select
large CCs with sizes close to HIP char sizes. One example:

b) Black mesh: Convert to grayscale image, threshold to black and white,
remove vertical and horizontal line pixels that don?t have neighboring
pixels, select large CCs with sizes close to HIP char sizes. One example:

c) White mesh: Convert to grayscale image, threshold to black and white, add
black pixels (in white line locations) if there exist neighboring pixels, select
large CCs with sizes close to HIP char sizes. One example:

Tests for black and white meshes were performed to determine which segmentation
algorithm to use. The end-to-end success rate was 56.2% for segmentation (38.2%
came from a), 11.8% from b), and 6.2% from c), 90.3% for recognition (given
correct segmentation), and (0.562)*(0.903)4.8 = 34.4% total. The average length of a
Yahoo HIP solution is 4.8 characters.
3.4

T i c k e t ma s t e r

The procedure that solved the Yahoo HIP is fairly successful at solving some of the
ticket master HIPs. These HIPs are characterized by cris-crossing lines at random
angles clustered around 0, 45, 90, and 135 degrees. A multipronged attack as in the
Yahoo case (section 3.3) has potential. In the interests of simplicity, a single attack
was developed: Convert to grayscale, threshold to black and white, up-sample
image, dilate first then erode, select large CCs with sizes close to HIP char sizes.
One example:

The dilate-erode combination causes the lines to be removed (along with any thin
objects) but retains solid thick characters. This single attack is successful in
achieving an end-to-end success rate of 16.6% for segmentation, the recognition rate
was 82.3% (in spite of interfering lines), and (0.166)*(0.823)6.23 = 4.9% total. The
average HIP solution length is 6.23 characters.
3.5

Y a h oo ve r s i on 2

The second generation HIP from Yahoo had several changes: a) it did not use words
from a dictionary or even use a phonetic generator, b) it uses only black and white
colors, c) uses both letters and digits, and d) uses connected lines and arcs as clutter.
The HIP is somewhat similar to the MSN/Passport HIP which does not use a
dictionary, uses two colors, uses letters and digits, and background and foreground
arcs as clutter. Unlike the MSN/Passport HIP, several different fonts are used. A
single segmentation attack was developed: Remove 6 pixel border, up-sample, dilate
first then erode, select large CCs with sizes close to HIP char sizes. The attack is
practically identical to that used for the ticketmaster HIP with different
preprocessing stages and slightly modified parameters. Two examples:

This single attack is successful in achieving an end-to-end success rate of 58.4% for
segmentation, the recognition rate was 95.2%, and (0.584)*(0.952)5 = 45.7% total.
The average HIP solution length is 5 characters.
3.6

G oog l e / G M a i l

The Google HIP is unique in that it uses only image warp as a means of distorting
the characters. Similar to the MSN/Passport and Yahoo version 2 HIPs, it is also
two color. The HIP characters are arranged closed to one another (they often touch)
and follow a curved baseline. The following very simple attack was used to segment
Google HIPs: Convert to grayscale, up-sample, threshold and separate connected
components.

a)

b)

This very simple attack gives an end-to-end success rate of 10.2% for segmentation,
the recognition rate was 89.3%, giving (0.102)*(0.893)6.5 = 4.89% total probability
of breaking a HIP. Average Google HIP solution length is 6.5 characters. This can
be significantly improved upon by judicious use of dilate-erode attack. A direct
application doesn?t do as well as it did on the ticketmaster and yahoo HIPs (because
of the shear and warp of the baseline of the word). More successful and complicated
attacks might estimate and counter the shear and warp of the baseline to achieve
better success rates.

4

Lesso n s lea rn ed f ro m b rea ki n g H IPs

From the previous section, it is clear that most of the errors come from incorrect
segmentations, even though most of the development time is spent devising custom
segmentation schemes. This observation raises the following questions: Why is
segmentation a hard problem? Can we devise harder HIPs and datasets? Can we
build an automatic segmentor? Can we compare classification algorithms based on
how useful they are for segmentation?
4.1

T h e s e g me n t a t i on p r ob l e m

As a review, segmentation is difficult for the following reasons:
1. Segmentation is computationally expensive. In order to find valid patterns, a
recognizer must attempt recognition at many different candidate locations.
2. The segmentation function is complex. To segment successfully, the system
must learn to identify which patterns are valid among the set of all possible
valid and non-valid patterns. This task is intrinsically more difficult than
classification because the space of input is considerably larger. Unlike the space
of valid patterns, the space of non-valid patterns is typically too vast to sample.
This is a problem for many learning algorithms which yield too many false
positives when presented non-valid patterns.
3. Identifying valid characters among a set of valid and invalid candidates is a
combinatorial problem. For example, correctly identifying which 8 characters
among 20 candidates (assuming 12 false positives), has a 1 in 125,970 (20
choose 8) chances of success by random guessing.

4.2

B ui l d i n g b e t te r / h a r de r H I P s

We can use what we have learned to build better HIPs. For instance the HIP below
was designed to make segmentation difficult and a similar version has been
deployed by MSN Passport for hotmail registrations (www.hotmail.com):

The idea is that the additional arcs are themselves good candidates for false
characters. The previous segmentation attacks would fail on this HIP. Furthermore,
simple change of fonts, distortions, or arc types would require extensive work for
the attacker to adjust to. We believe HIPs that emphasize the segmentation problem,
such as the above example, are much stronger than the HIPs we examined in this
paper, which rely on recognition being difficult. Pushing this to the extreme, we can
easily generate the following HIPs:

Despite the apparent difficulty of these HIPs, humans are surprisingly good at
solving these, indicating that humans are far better than computers at segmentation.
This approach of adding several competing false positives can in principle be used
to automatically create difficult segmentation problems or benchmarks to test
classification algorithms.
4.3

B ui l d i n g a n a ut o ma t i c s e g me n t or

To build an automatic segmentor, we could use the
following procedure. Label characters based on
their correct position and train a recognizer. Apply
the trained recognizer at all locations in the HIP
image. Collect all candidate characters identified
with high confidence by the recognizer. Compute
the probability of each combination of candidates
(going from left to right), and output the solution
string with the highest probability. This is better
illustrated with an example.
Consider the following HIP (to the right). The
trained neural network has these maps (warm
colors indicate recognition) that show that K, Y,
and so on are correctly identified. However, the
maps for 7 and 9 show several false positives. In
general, we would get the following color coded
map for all the different candidates:

HIP
K
Y
B
7
9

With a threshold of 0.5 on the network?s outputs, the map obtained is:

We note that there are several false positives for each true positive. The number of
false positives per true positive character was found to be between 1 and 4, giving a
1 in C(16,8) = 12,870 to 1 in C(32,8) = 10,518,300 random chance of guessing the
correct segmentation for the HIP characters. These numbers can be improved upon
by constraining solution strings to flow sequentially from left to right and by
restricting overlap. For each combination, we compute a probability by multiplying
the 8 probabilities of the classifier for each position. The combination with the
highest probability is the one proposed by the classifier. We do not have results for
such an automatic segmentor at this time. It is interesting to note that with such a
method a classifier that is robust to false positives would do far better than one that
is not. This suggests another axis for comparing classifiers.

5

Con clu si on

In this paper, we have successfully applied machine learning to the problem of
solving HIPs. We have learned that decomposing the HIP problem into
segmentation and recognition greatly simplifies analysis. Recognition on even
unprocessed images (given segmentation is a solved) can be done automatically
using neural networks. Segmentation, on the other hand, is the difficulty
differentiator between weaker and stronger HIPs and requires custom intervention
for each HIP. We have used this observation to design new HIPs and new tests for
machine learning algorithms with the hope of improving them.
A c k n ow l e d ge me n t s
We would like to acknowledge Chau Luu and Eric Meltzer for their help with
labeling and segmenting various HIPs. We would also like to acknowledge Josh
Benaloh and Cem Paya for stimulating discussions on HIP security.
References
[1] Baird HS (1992), ?Anatomy of a versatile page reader,? IEEE Pro., v.80, pp. 1059-1065.
[2] Turing AM (1950), ?Computing Machinery and Intelligence,? Mind, 59:236, pp. 433-460.
[3] First Workshop on Human Interactive Proofs, Palo Alto, CA, January 2002.
[4] Von Ahn L, Blum M, and Langford J, The Captcha Project. http://www.captcha.net
[5] Baird HS and Popat K (2002) ?Human Interactive Proofs and Document Image
Analysis,? Proc. IAPR 2002 Workshop on Document Analysis Systerms, Princeton, NJ.
[6] Simard PY, Steinkraus D, and Platt J, (2003) ?Best Practice for Convolutional Neural
Networks Applied to Visual Document Analysis,? in International Conference on Document
Analysis and Recognition (ICDAR), pp. 958-962, IEEE Computer Society, Los Alamitos.
[7] Mori G, Malik J (2003), ?Recognizing Objects in Adversarial Clutter: Breaking a Visual
CAPTCHA,? Proc. of the Computer Vision and Pattern Recognition (CVPR) Conference,
IEEE Computer Society, vol.1, pages:I-134 - I-141, June 18-20, 2003

[8] Chew, M. and Baird, H. S. (2003), ?BaffleText: a Human Interactive Proof,? Proc.,
10th IS&T/SPIE Document Recognition & Retrieval Conf., Santa Clara, CA, Jan. 22.
[9] LeCun Y, Bottou L, Bengio Y, and Haffner P, ?Gradient-based learning applied to
document recognition,? Proceedings of the IEEE, Nov. 1998.


----------------------------------------------------------------

title: 383-back-propagation-implementation-on-the-adaptive-solutions-cnaps-neurocomputer-chip.pdf

Back Propagation Implementation on the
Adaptive Solutions CNAPS Neurocomputer Chip

Hal McCartor
Adaptive Solutions Inc.
1400 N.W. Compton Drive
Suite 340
Beaverton, OR 97006

Abstract
The Adaptive Solutions CN APS architecture chip is a general purpose
neurocomputer chip. It has 64 processors, each with 4 K bytes of local
memory, running at 25 megahertz. It is capable of implementing most
current neural network algorithms with on chip learning. This paper discusses the implementation of the Back Propagation algorithm on an array
of these chips and shows performance figures from a clock accurate hardware simulator. An eight chip configuration on one board can update 2.3
billion connections per second in learning mode and process 9.6 billion
connections per second in feed forward mode.

1

Introduction

The huge computational requirements of neural networks and their natural parallelism have led to a number of interesting hardware innovations for executing such
networks. Most investigators have created large parallel computers or special purpose chips limited to a small subset of algorithms. The Adaptive Solutions CNAPS
architecture describes a general-purpose 64-processor chip which supports on chip
learning and is capable of implementing most current algorithms. Implementation
of the popular Back Propagation (BP) algorithm will demonstrate the speed and
versatility of this new chip.

1028

Back Propagation Implementation

2

The Hardware Resources

The Adaptive Solutions CNAPS architecture is embodied in a single chip digital
neurocomputer with 64 processors running at 25 megahertz. All processors receive
the same instruction which they conditionally execute. Multiplication and addition
are performed in parallel allowing 1.6 billion inner product steps per second per
chip . Each processor has a 32-bit adder, 9-bit by 16-bit multiplier (16 by 16 in two
clock cycles), shifter, logic unit, 32 16-bit registers, and 4096 bytes oflocal memory.
Input and output are accomplished over 8-bit input and output buses common
to all processors. The output bus is tied to the input bus so that output of one
processor can be broadcast to all others. When multiple chips are used, they appear
to the user as one chip with more processors. Special circuits support finding the
maximum of values held in each processor and conserving weight space for sparsely
connected networks. An accompanying sequencer chip controls instruction flow,
input and output.

3

The Back Propagation Algorithm Implementation

Three critical issues must be addressed in the parallel implementation of BP on efficient hardware. These are the availability of weight values for back propagating the
error, the scaling and precision of computations, and the efficient implementation
of the output transfer function.
BP requires weight values at different nodes during the feed forward and back
propagation phases of computation. This problem is solved by having a second set
of weights which is the transpose of the output layer weights. These are located on
hidden node processors. The two matrices are updated identically. The input to the
hidden layer weight matrix is not used for error propagation and is not duplicated.
BP implementations typically use 32-bit floating point math. This largely eliminates
scaling, precision and dynamic range issues. Efficient hardware implementation
dictates integer arithmetic units with precision no greater than required. Baker
[Bak90] has shown 16-bit integer weights are sufficient for BP training and much
lower values adequate for use after training.
With fixed point integer math, the position of the binary point must be chosen. In
this implementation weights are 16 bits and use 12 bits to the right of the binary
point and four to the left including a sign bit. They range from -8 to +8. Input
and output are represented as 8-bit unsigned integers with binary point at the left.
The leaning rate is represented as an 8-bits integer with two bits to the left of the
binary point and values ranging from .016 to 3.98. Error is represented as 8 bit
signed integers at the output layer and with the same representation as the weights
at the hidden layer.
This data representation has been used to train benchmark BP applications with
results comparable to the floating point versions [HB91].
The BP sigmoid output function is implemented as an 8-bit by 256 lookup table.
During the forward pass input values are broadcast to all processors from off chip
via the input bus or from hidden nodes via the output bus to the input bus. During

1029

1030

McCartor
the backward error propagation, error values are broadcast from the output nodes
to hidden nodes.
The typical BP network has two computational layers, the hidden and output layers.
They can be assigned to the same or different processor nodes (PN s) depending on
available memory for weights. PNs used for the hidden layer contain the transpose
weights of the output layer for back propagating error. If momentum or periodic
weight update are used, additional storage space is allocated with each weight.
In this implementation BP can be mapped to any set of contiguous processors
allowing multiple networks in CNAPS memory simultaneously. Thus, the output
of one algorithm can be directly used as input to another. For instance, in speech
recognition, a Fourier transform performed on the PN array could be input to a
series of matched BP networks whose hidden layers run concurrently. Their output
could be directed to an LVQ2 network for final classification. This can all be
accomplished without any intermediate results leaving the chip array.

4

Results

BP networks have been successfully run on a hardware clock accurate simulator
which gives the following timing results. In this example an eight-chip implementation (512 processors) was used. The network had 1900 inputs, 500 hidden nodes
and 12 outputs. Weights were updated after each input and no momentum was
used. The following calculations show BP performance:

TRAINING PHASE
Overhead clock cycles per input vector = 360
Cycles per input vector element = 4
Cycles per hidden node = 4
Cycles per output node = 7
Cycles per vector = 360+(1900*4)+(500*4)+(12*7) = 10,044
Vectors per second = 25,000,000 / 10,044 = 2,489
Total forward weights = (1900*500)+(500*12) = 956,000
Weight updates per second = 956,000*2,489 = 2,3'79,484,000

FEED FORWARD ONLY
Overhead cycles per input vector = 59
Cycles per input vector element
1
Cycles per hidden node = 1
Cycles per output node = 1 (for output of data)
Cycles per vector = 59+1900+500+12 = 2,471
Vectors per second
25,000,000/2,471
10,117

=

=

=

Connections per second = 956,000*10,11'7 = 9,6'71,852,000

Back Propagation Implementation

5

Comparative Performance

An array of eight Adaptive Solutions CN APS chips would execute the preceding BP
network at 2.3 billion training weight updates per second or 9.6 billion feed forward
connections per second. These results can be compared with the results on other
computers shown in table 1.

MACHINE
SUN 3 lD88j
SAle SIGMA-llD88j
WARP [PGTK88]
CRAY 2 lPGTK88J
CRAY X-MP lD88J
CM-2 (65,536) [ZMMW90]
GF-1l1566) lWZ89j
8 ADAPTIVE CN APS chips

MCUPS
.034

MCPS
0.25
8

17
7
40
901
2,379

50
182
9,671

WTS
fp
fp
fp
fp
fp
fp
fp
16 bit int

Table 1. Comparison of BP performance for various computers and 8 Adaptive
Solutions CNAPS chips on one board. MCUPS is Millions of BP connection updates
per second in training mode. MCPS is millions of connections processed per second
in feed forward mode. WTS is representation used for weights.

6

Summary

The Adaptive Solutions CN APS chip is a very fast general purpose digital neurocomputer chip. It is capable of executing the Back Propagation algorithm quite
efficiently. An 8 chip configuration can train 2.3 billion connections per second and
evaluate 9.6 billion BP feed forward connections per second.
References
[Bak90] T Baker. Implementation limits for artificial neural networks. Master's
thesis, Oregon Graduate Institute, 1990.
[D88] DARPA Neural Network Study. pp309-310 AFCEA International Press, Fairfax Virginia. 1988
[HB91] J. Holt and T. Baker. Back Propagation Simulations using Limited Precision
Calculations. Submitted to IJCNN, Seattle WA 1991.
[RM86] D. Rummelhart, J. McClelland. Parallel Distributed Processing. (1986)
MIT Press, Cambridge, MA.
[WZ89] M. Witbrock and M Zagha. An Implementation of Back-Propagation Learning on GFll, a Large SIMD Parallel Computer. 1989. Tech report CMU-CS-89-208
Carnegie Mellon University.
[ZMMW90] X. Zhang, M. Mckenna, J Misirov, D Waltz. An Efficient Implementation of the Back-propagation Algorithm on the Connection Machine CM-2 (1990)
in Adv. in Neural Information Processing Systems 2. Ed. D. Touretzky. Morgan
Kaufmann, San Mateo, CA.

1031


----------------------------------------------------------------

title: 6261-visual-question-answering-with-question-representation-update-qru.pdf

Visual Question Answering with
Question Representation Update (QRU)

Ruiyu Li
Jiaya Jia
The Chinese University of Hong Kong
{ryli,leojia}@cse.cuhk.edu.hk

Abstract
Our method aims at reasoning over natural language questions and visual images.
Given a natural language question about an image, our model updates the question
representation iteratively by selecting image regions relevant to the query and
learns to give the correct answer. Our model contains several reasoning layers,
exploiting complex visual relations in the visual question answering (VQA) task.
The proposed network is end-to-end trainable through back-propagation, where its
weights are initialized using pre-trained convolutional neural network (CNN) and
gated recurrent unit (GRU). Our method is evaluated on challenging datasets of
COCO-QA [19] and VQA [2] and yields state-of-the-art performance.

1

Introduction

Visual question answering (VQA) is a new research direction as intersection of computer vision and
natural language processing. Developing stable systems for VQA attracts increasing interests in
multiple communities. Possible applications include bidirectional image-sentence retrieval, human
computer interaction, blind person assistance, etc. It is now still a difficult problem due to many
challenges in visual object recognition and grounding, natural language representation, and common
sense reasoning.
Most recently proposed VQA models are based on image captioning [10, 24, 28]. These methods
have been advanced by the great success of deep learning on building language models [23], image
classification [12] and on visual object detection [6]. Compared with image captioning, where a
plausible description is produced for a given image, VQA requires algorithms to give the correct
answer to a specific human-raised question regarding the content of a given image. It is a more
complex research problem since the method is required to answer different types of questions.
An example related to image content is ?What is the color of the dog??. There are also
questions requiring extra knowledge or commonsense reasoning, such as ?Does it appear to be
rainy?".
Properly modeling questions is essential for solving the VQA problem. A commonly employed
strategy is to use a CNN or an RNN to extract semantic vectors. The general issue is that the resulting
question representation lacks detailed information from the given image, which however is vital
for understanding visual content. We take the question and image in Figure 1 as an example. To
answer the original question ?What is sitting amongst things have been abandoned?",
one needs to know the target object location. Thus the question can be more specific as ?What is
discarded on the side of a building near an old book shelf?".
In this paper, we propose a neural network based reasoning model that is able to update the question
representation iteratively by inferring image information. With this new system, it is now possible
to make questions more specific than the original ones focusing on important image information
automatically. Our approach is based on neural reasoner [18], which has recently shown remarkable
30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.

Question: What is sitting
amongst things have been
abandoned?
Answer: Toilet.
Before:
What sits in the
room that appears to be
partially abandoned?
Updated: What is discarded
on the side of a building
near an old book shelf?
(a)

(b)

Figure 1: The questions asked by human can be ambiguous given an image containing various objects.
The Before and Updated questions are the most similar ones based on the cosine similarity to the
original Question before and after applying our algorithm to update representation. (b) shows the
attention masks generated by our model.
success in text question answering tasks. Neural reasoner updates the question by interacting it with
supporting facts through multiple reasoning layers. We note applying this model to VQA is nontrivial
since the facts are in the form of an image. Thus image region information is extracted in our
model. To determine the relevance between question and each image region, we employ the attention
mechanism to generate the attention distribution over regions of the image. Our contributions are as
follows.
? We present a reasoning network to iteratively update the question representation after each
time the question interacts with image content.
? Our model utilizes object proposals to obtain candidate image regions and has the ability to
focus on image regions relevant to the question.
We evaluate and compare the performance of our model on two challenging VQA datasets ? i.e.,
COCO-QA [19] and VQA [2]. Experiments demonstrate the ability of our model to infer image
regions relevant to the question.

2

Related Work

Research on visual question answering is mostly driven by text question answering and image
captioning methods. In natural language processing, question answering is a well-studied problem. In
[22], an end-to-end memory network was used with a recurrent attention model over a large external
memory. Compared with the original memory network, it has less supervision and shows comparable
results on the QA task. The neural reasoning system proposed in [18], named neural reasoner, can
utilize multiple supporting facts and find an answer. Decent performance was achieved on positional
reasoning and path finding QA tasks.
VQA is closely related to image captioning [10, 24, 28, 5]. In [5], a set of likely words are detected
in several regions of the image and are combined together using a language model to generate image
description. In [10], a structured max-margin objective was used for deep neural networks. It learns to
embed both visual and language data into a common multi-modal space. Vinyals et al. [24] extracted
high-level image feature vectors from CNN and took them as the first input to the recurrent network
to generate caption. Xu et al. [28] integrated visual attention in the recurrent network. The proposed
algorithm predicts one word at a time by looking at local image regions relevant to the currently
generated word.
Malinowski et al. [15] first introduced a solution addressing the VQA problem. It combines natural
language processing with semantic segmentation in a Bayesian framework for automatic question
answering. Since it, several neural network based models [16, 19, 2] were proposed to solve the
VQA problem. These models use CNN to extract image features and recurrent neural networks to
embed questions. The embedded image and question features are then fused by concatenation [16]
2

Image Understanding
Region 1

Image
Question
What are they playing?

1

Query

1

Query

GRU

Query

1

2

Query

1

SoftMax

Region M

Query

......

CNN

......

Region 2

1
M

0

Reasoning

Question Encoding

Answering

Figure 2: The overall architecture of our model with single reasoning layer for VQA.

or element-wise addition [29] to predict answers. Recently several models integrated the attention
mechanism [29, 27, 3, 20] and showed the ability of their networks to focus on image regions related
to the question.
There also exist other approaches for VQA. For example, Xiong et al. [26] proposed an improved
dynamic memory network to fuse the question and image region representations using bi-directional
GRU. The algorithm of [1] learns to compose a network from a collection of composable modules.
Ma et al. [14] made use of CNN and proposed a model with three CNNs to capture information of
the image, question and multi-modal representation.

3

Our Model

The overall architecture of our model is illustrated in Figure 2. The model is derived from the neural
reasoner [18], which is able to update the representation of question recursively by inferring over
multiple supporting facts. Our model yet contains a few inherently different components. Since
VQA involves only one question and one image each time instead of a set of facts, we use object
proposal to obtain candidate image regions serving as the facts in our model. Moreover, in the
pooling step, we employ an attention mechanism to determine the relevance between representation
of original questions and updated ones. Our network consists of four major components ? i.e., image
understanding, question encoding, reasoning and answering layers.
3.1

Image Understanding Layer

The image understanding layer is designed for modeling image content into semantic vectors. We
build this layer upon the VGG model with 19 weight layers [21]. It is pre-trained on ImageNet [4].
The network has sixteen convolutional layers and five max-pooling layers of kernel size 2 ? 2 with
stride 2, followed by two fully-connected layers with 4,096 neurons.
Using a global representation of the image may fail to capture all necessary information for answering
the question involving multiple objects and spatial configuration. Moreover, since most of the
questions are related to objects [19, 2], we utilize object proposal generator to produce a set of
candidate regions that are most likely to be an object. For each image, we choose candidate regions
by extracting the top 19 detected edge boxes [31]. We choose intersection over union (IoU) value 0.3
when performing non-maximum suppression, which is a common setting in object detection.
Additionally, the whole image region is added to capture the global information in the image
understanding layer, resulting in 20 candidate regions per image. We extract features from each
candidate region through the above mentioned CNN, bringing a dimension of 4,096 image region
features. The extracted features, however, lack spatial information for object location. To remedy this
issue, we follow the method of [8] to include an 8D representation
[xmin , ymin , xmax , ymax , xcenter , ycenter , wbox , hbox ],
3

where wbox and hbox are the width and height of the image region. We set the image center as the
origin. The coordinates are normalized to range from ?1 to 1. Then each image region is represented
as a 4104D feature denoted as fi where i ? [1, 20]. For modeling convenience, we use a single layer
perceptron to transform the image representation into a common latent space shared with the question
feature
vi = ?(Wvf ? fi + bvf ),
(1)
where ? is the rectified activation function ?(x) = max(0, x).
3.2

Question Encoding Layer

To encode the natural language question, we resort to the recurrent neural network, which has
demonstrated great success on sentence embedding. The question encoding layer is composed of a
word embedding layer and GRU cells. Given a question w = [w1 , ..., wT ], where wt is the tth word
in the question and T is the length of the question, we first embed each word wt to a vector space xt
with an embedding matrix xt = We wt . Then for each time step, we feed xt into GRU sequentially.
At each step, the GRU takes one input vector xt , and updates and outputs a hidden state ht . The final
hidden state hT is considered as the question representation. We also embed it into the common
latent space same as image embedding through a single layer perceptron
q = ?(Wqh ? hT + bqh ).

(2)

We utilize the pre-trained network with skip-thought vectors model [11] designed for general sentence
embedding to initialize our question encoding layer as used in [17]. Note that the skip-thought vectors
model is trained in an unsupervised manner on large language corpus. By fine-tuning the GRU, we
transfer knowledge from natural language corpus to the VQA problem.
3.3

Reasoning Layer

The reasoning layer includes question-image interaction and weighted pooling.
Question-Image Interaction Given that multilayer perceptron (MLP) has the ability to determine
the relationship between two input sentences according to supervision [7, 18]. We examine image
region features and question representation to acquire a good understanding of the question. In a
memory network [22], these image region features are akin to the input memory representation,
which can be retrieved for multiple times according to the question.
There are a total of L reasoning layers. In the lth reasoning layer, the ith interaction happens between
q l?1 and vi through an MLP, resulting in updated question representation qil as
qil = M LPl (q l?1 , vi ; ?l ),

(3)

with ?l being the model parameter of interaction at the lth reasoning layer. In the simplest case with
one single layer in M LPl , the updating process is given by
qil = ?(Wl ? (q l?1 ? vi ) + bl ),

(4)

where ? indicates element-wise multiplication, which performs better in our experiments than other
strategies, e.g., concatenation and element-wise addition.
Generally speaking, qil contains update of network focus towards answering the question after its
interaction with image feature vi . This property is important for the reasoning process [18].
Weighted Pooling Pooling aims to fuse components of the question after its interaction with all
image features to update representation. Two common strategies for pooling are max and mean
pooling. However, when answering a specifical question, it is often the case the correct answer is only
related to particular image regions. Therefore, using max pooling may lead to unsatisfying results
since questions may involve interaction between human and object, while mean pooling may also
cause inferior performance due to noise introduced by regions irrelevant to the question.
To determine the relevance between question and each image region, we resort to the attention
mechanism used in [28] to generate the attention distribution over image regions. For each updated
4

question qil after interaction with the ith image region, it is chosen close to the original question
representation q l?1 . Hence, the attention weights take the following forms.
Ci = tanh(WA ? qil ? (WB ? q l?1 + bB )),
P = sof tmax(WP ? C + bP ),

(5)

where C is a matrix and its ith column is Ci . P ? RM is a M dimensional vector representing the
attention weights. M is the number of image regions, set to 20. Based on the attention distribution,
we calculate weighted average of qil , resulting in the updated question representation q l as
X
ql =
Pi qil .
(6)
i
l

The updated question representation q after weighted pooling serves as the question input to the next
reasoning or answering layer.
3.4

Answering Layer

Following [19, 2], we model VQA as a classification problem with pre-defined classes. Given the
updated question representation at last reasoning layer q L , a softmax layer is employed to classify q L
into one of the possible answers as
pans = sof tmax(Wans ? q L + bans ).

(7)

Note instead of the softmax layer for predicting the correct answer, it is also possible to utilize LSTM
or GRU decoder, taking q L as input, to generate free-form answers.

4
4.1

Experiments
Datasets and Evaluation Metrics

We conduct experiments on COCO-QA [19] and VQA [2]. The COCO-QA dataset is based on
Microsoft COCO image data [13]. There are 78,736 training questions and 38,948 test ones, based
on a total of 123,287 images. Four types of questions are provided, including Object, Number, Color
and Location. Each type takes 70%, 7%, 17% and 6% of the whole dataset respectively.
In the VQA dataset, each image from the COCO data is annotated by Amazon Mechanical Turk
(AMT) with three questions. It is the largest for VQA benchmark so far. There are 248,349, 121,512
and 244,302 questions for training, validation and testing, respectively. For each question, ten answers
are provided to take consensus of annotators. Following [2], we choose the top 1,000 most frequent
answers as candidate outputs, which constitutes 82.67% of the train+val answers.
Since we formulate VQA as a classification problem, mean classification accuracy is used to evaluate
the model on the COCO-QA dataset. Besides, Wu-Palmer similarity (WUPS) [25] measure is also
reported on COCO-QA dataset. WUPS calculates similarity between two words based on their
longest common subsequence in the taxonomy tree. Following [19], we use thresholds 0.9 and 0.0 in
our evaluation. VQA dataset provides a different kind of evaluation metric. Since ten ground truth
answers are given, a predicted answer is considered to be correct when three or more ground truth
answers match it. Otherwise, partial score is given.
4.2

Implementation Details

We implement our network using the public Torch computing framework. Before training, all question
sentences are normalized to lower case where question marks are removed. These words are fed into
GRU one by one. The whole answer with one or more words is regarded as a separate class. For
extracting image features, each candidate region is cropped and resized to 224 ? 224 before feeding
into CNN.
For the COCO-QA dataset, we set the dimension of common latent space to 1,024. Since VQA
dataset is larger than COCO-QA, we double the dimension of common latent space to adapt the data
and classes. On each reasoning layer, we use one single layer in MLP. We test up to two reasoning
layers. No further improvement is observed when using three or more layers.
5

Methods
Mean Pooling
Max Pooling
W/O Global
W/O Coord
Full Model

ACC.
58.15
59.37
60.87
61.33
61.99

Object
60.61
62.11
63.32
63.76
64.53

Number
45.34
45.70
46.68
46.24
46.68

Color
55.37
55.91
58.66
59.35
59.81

Location
52.74
53.63
55.49
56.66
56.82

Table 1: Comparison of ablation models. Models are trained and tested on COCO-QA [19] with one
reasoning layer.
Methods
IMG+BOW [19]
2VIS+BLSTM [19]
Ensemble [19]
ABC-CNN [3]
DPPnet [17]
SAN [29]
QRU (1)
QRU (2)

ACC.
55.92
55.09
57.84
58.10
61.19
61.60
61.99
62.50

Object
58.66
58.17
61.08
62.46
64.50
64.53
65.06

Number
44.10
44.79
47.66
45.70
48.60
46.68
46.90

Color
51.96
49.53
51.48
46.81
57.90
59.81
60.50

Location
49.39
47.34
50.28
53.67
54.00
56.82
56.99

WUPS 0.9
66.78
65.34
67.90
68.44
70.84
71.60
71.83
72.58

WUPS 0.0
88.99
88.64
89.52
89.85
90.61
90.90
91.11
91.62

Table 2: Evaluation results on COCO-QA dataset [19]. ?QRU (1)? and ?QRU (2)? refer to 1 and 2
reasoning layers incorporated in the system.

The network is trained in an end-to-end fashion using stochastic gradient descent with mini-batches
of 100 samples and momentum 0.9. The learning rate starts from 10?3 and decreases by a factor of
10 when validation accuracy stops improving. We use dropout and gradient clipping to regularize the
training process. Our model is denoted as QRU in following experiments.
4.3

Ablation Results

We conduct experiments to exam the usefulness of each component in our model. Specifically, we
compare different question representation pooling mechanisms, i.e., mean pooling and max pooling.
We also train two controlled models devoid of global image feature and spatial coordinate, denoted
as W/O Global and W/O Coord. Table 1 shows the results.
The performance of mean and max pooling models are substantially worse than the full model, which
uses weighted pooling. This indicates that our model benefits from the attention mechanism by
looking at several image regions rather than only one or all of them. A drop of 1.12% in accuracy is
observed if the global image feature is not modeled, confirming that inclusion of the whole image
is important for capturing the global information. Without modeling spatial coordinates also leads
to a drop in accuracy. Notably, the greatest deterioration is on the question type of Object. This is
because the Object type seeks information around the object like ?What is next to the stop
sign?". Spatial coordinates help our model reason spatial relationship among objects.
4.4

Comparison with State-of-the-art

We compare performance in Tables 2 and 3 with experimental results on COCO-QA and VQA
respectively. Table 2 shows that our model with only one reasoning layer already outperforms
state-of-the-art 2-layer stacked attention network (SAN) [29]. Two reasoning layers give the best
performance. We also report the per-category accuracy to show the strength and weakness of our
model in Table 2. Our best model outperforms SAN by 2.6% and 2.99% in the question types of
Color and Location respectively, and by 0.56% in Object.
Our analysis is that the SAN model puts its attention on coarser regions obtained from the activation
of last convolutional layer, which may include cluttered and noisy background. In contrast, our model
only deals with selected object proposal regions, which have the good chance to be objects. When
answering questions involving objects, our model gives reasonable results. For the question type
Number, since an object proposal may contain several objects, our counting ability is weakened. In
fact, the counting task is a complete computer vision problem on its own.
6

Methods
BOWIMG [2]
LSTMIMG [2]
iBOWIMG [30]
DPPnet [17]
SAN [29]
WR Sel [20]
FDA [9]
DMN+ [26]
QRU (1)
QRU (2)

Open-Ended (test-dev)
All
Y/N
Num Other
52.64 75.77 33.67 37.37
53.74 78.94 35.24 36.42
55.72 76.55 35.03 42.62
57.22 80.71 37.24 41.71
58.70 79.30 36.60 46.10
59.24 81.14 36.16 45.77
60.37 80.75 37.00 48.25
59.26 80.98 35.93 45.99
60.72 82.29 37.02 47.67

test-std
All
54.06
55.89
57.36
58.90
59.54
60.36
59.44
60.76

Multiple-Choice (test-dev)
All
Y/N Num Other
58.97 75.59 34.35 50.33
57.17 78.95 35.80 43.41
61.68 76.68 37.05 54.44
62.48 80.79 38.94 52.16
62.44 77.62 34.28 55.84
64.01 81.50 39.00 54.72
63.96 81.00 37.08 55.48
65.43 82.24 38.69 57.12

test-std
All
57.57
61.97
62.69
62.43
64.18
64.13
65.43

Table 3: Evaluation results on VQA dataset [2]. ?QRU (1)? and ?QRU (2)? refer to 1 and 2 reasoning
layers incorporated in the system.
Original

Before updating

After updating
with one
reasoning layer

After updating
with two
reasoning layers

What next to two other open laptops?
What next to each other dipicting smartphones?
What next to two boys?
What hooked up to two computers?
What next to each other with visible piping?
What next to two pair of shoes?
What are there laying down with two remotes?
What next to each other depicting smartphones?
What hooked up to two computers?
What next to each other with monitors?
What cubicle with four differnet types of computers?
What plugged with wires?
What next to each other with monitors?
What are open at the table with cell phones?
What is next to the monitor?
What sits on the desk along with 2 monitors?

Figure 3: Retrieved questions before and after update from COCO-QA dataset [19].

Table 3 shows that our model yields prominent improvement on the Other type when compared
with other models [2, 30, 17] that use global representation of the image. Object proposals in our
model are useful since the Other type contains questions such as ?What color ? ? ? ", ?What kind
? ? ? ", ?Where is ? ? ? ", etc. Our model outperforms that of [20] by 3% where the latter also exploits
object proposals. Compared with [20], we use less number of object proposals, demonstrating the
effectiveness of our approach. This table also reveals that our model with two reasoning layers
achieve state-of-the-art results for both open-ended and multiple-choice tasks.
4.5

Qualitative Analysis

To understand the ability of our model in updating question representation, we show an image and
several questions in Figure 3. The retrieved questions from the test set are based on the cosine
similarities to the original question before and after our model updates the representation. It is notable
that before update, 4 out of the top 5 similar questions begin with ?What next". This is because GRU
acts as the language model, making the obtained questions share similar language structure. After we
update question representation, the resulting ones are more related to image content regarding objects
computers and monitors while the originally retrieved questions contain irrelevant words like boys
and shoes. The retrieved questions become even more informative using two reasoning layers.
We visualize a few attention masks generated by our model in Figure 4. Visualization is created
by soft masking the image with a mask created by summing weights of each region. The mask
is normalized with maximum value 1 followed by small Gaussian blur. Our model is capable of
putting attention on important regions closely relevant to the question. To answer the question ?What
is the color of the snowboard?", the proposed model finds the snowboard. For the other
question ?The man holding what on top of a snow covered hill?", it is required to infer
the relation among person, snow covered hill, and snowboard. With these attention masks, it is
possible to predict correct answers since irrelevant image regions are ruled out. More examples are
shown in Figure 5.
7

(a)

(b)

(c)

Q: What is the color of the Q: The man holding what on
snowboard?
top of a snow covered hill?
A: Yellow.
A: Snowboard.

Figure 4: Visualization of attention masks. Our model learns to attend particular image regions that
are relevant to the question.

Q: What is the color Q: What is sitting on Q: What is the man in Q: What are hogging
of the sunflower?
top of table in a
stadium style seats a bed by themselfs?
workshop?
using?
A: Yellow
A: Boat
A: Phone
A: Dogs

Q: What next to a
large building?
A: Clock

Figure 5: Visualization of more attention masks.

5

Conclusion

We have proposed an end-to-end trainable neural network for VQA. Our model learns to answer
questions by updating question representation and inferring over a set of image regions with multilayer
perceptron. Visualization of attention masks demonstrates the ability of our model to focus on image
regions highly related to questions. Experimental results are satisfying on the two challenging VQA
datasets. Future work includes improving object counting ability and word-region relation.

Acknowledgements
This work is supported by a grant from the Research Grants Council of the Hong Kong SAR (project
No. 2150760) and by the National Science Foundation China, under Grant 61133009. We thank
NVIDIA for providing Ruiyu Li a Tesla K40 GPU accelerator for this work.
8

References
[1] J. Andreas, M. Rohrbach, T. Darrell, and D. Klein. Learning to compose neural networks for question
answering. arXiv preprint arXiv:1601.01705, 2016.
[2] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. Lawrence Zitnick, and D. Parikh. Vqa: Visual
question answering. In ICCV, pages 2425?2433, 2015.
[3] K. Chen, J. Wang, L.-C. Chen, H. Gao, W. Xu, and R. Nevatia. Abc-cnn: An attention based convolutional
neural network for visual question answering. arXiv preprint arXiv:1511.05960, 2015.
[4] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image
database. In CVPR, pages 248?255, 2009.
[5] H. Fang, S. Gupta, F. Iandola, R. K. Srivastava, L. Deng, P. Doll?r, J. Gao, X. He, M. Mitchell, J. C. Platt,
et al. From captions to visual concepts and back. In CVPR, pages 1473?1482, 2015.
[6] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection
and semantic segmentation. In CVPR, pages 580?587, 2014.
[7] B. Hu, Z. Lu, H. Li, and Q. Chen. Convolutional neural network architectures for matching natural
language sentences. In NIPS, pages 2042?2050, 2014.
[8] R. Hu, H. Xu, M. Rohrbach, J. Feng, K. Saenko, and T. Darrell. Natural language object retrieval. arXiv
preprint arXiv:1511.04164, 2015.
[9] I. Ilija, Y. Shuicheng, and F. Jiashi. A focused dynamic attention model for visual question answering.
arXiv preprint arXiv:1604.01485, 2016.
[10] A. Karpathy and L. Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In CVPR,
pages 3128?3137, 2015.
[11] R. Kiros, Y. Zhu, R. R. Salakhutdinov, R. Zemel, R. Urtasun, A. Torralba, and S. Fidler. Skip-thought
vectors. In NIPS, pages 3276?3284, 2015.
[12] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural
networks. In NIPS, pages 1097?1105, 2012.
[13] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll?r, and C. L. Zitnick. Microsoft
coco: Common objects in context. In ECCV, pages 740?755, 2014.
[14] L. Ma, Z. Lu, and H. Li. Learning to answer questions from image using convolutional neural network.
arXiv preprint arXiv:1506.00333, 2015.
[15] M. Malinowski and M. Fritz. A multi-world approach to question answering about real-world scenes based
on uncertain input. In NIPS, pages 1682?1690, 2014.
[16] M. Malinowski, M. Rohrbach, and M. Fritz. Ask your neurons: A neural-based approach to answering
questions about images. In ICCV, pages 1?9, 2015.
[17] H. Noh, P. H. Seo, and B. Han. Image question answering using convolutional neural network with
dynamic parameter prediction. arXiv preprint arXiv:1511.05756, 2015.
[18] B. Peng, Z. Lu, H. Li, and K.-F. Wong. Towards neural network-based reasoning. arXiv preprint
arXiv:1508.05508, 2015.
[19] M. Ren, R. Kiros, and R. Zemel. Exploring models and data for image question answering. In NIPS, pages
2935?2943, 2015.
[20] K. J. Shih, S. Singh, and D. Hoiem. Where to look: Focus regions for visual question answering. arXiv
preprint arXiv:1511.07394, 2015.
[21] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition.
arXiv preprint arXiv:1409.1556, 2014.
[22] S. Sukhbaatar, A. Szlam, J. Weston, and R. Fergus. Weakly supervised memory networks. arXiv preprint
arXiv:1503.08895, 2015.
[23] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks. arXiv preprint
arXiv:1409.3215, 2014.
[24] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and tell: A neural image caption generator. In
CVPR, pages 3156?3164, 2015.
[25] Z. Wu and M. Palmer. Verbs semantics and lexical selection. In ACL, pages 133?138, 1994.
[26] C. Xiong, S. Merity, and R. Socher. Dynamic memory networks for visual and textual question answering.
arXiv preprint arXiv:1603.01417, 2016.
[27] H. Xu and K. Saenko. Ask, attend and answer: Exploring question-guided spatial attention for visual
question answering. arXiv preprint arXiv:1511.05234, 2015.
[28] K. Xu, J. Ba, R. Kiros, A. Courville, R. Salakhutdinov, R. Zemel, and Y. Bengio. Show, attend and tell:
Neural image caption generation with visual attention. arXiv preprint arXiv:1502.03044, 2015.
[29] Z. Yang, X. He, J. Gao, L. Deng, and A. Smola. Stacked attention networks for image question answering.
arXiv preprint arXiv:1511.02274, 2015.
[30] B. Zhou, Y. Tian, S. Sukhbaatar, A. Szlam, and R. Fergus. Simple baseline for visual question answering.
arXiv preprint arXiv:1512.02167, 2015.
[31] C. L. Zitnick and P. Doll?r. Edge boxes: Locating object proposals from edges. In ECCV, pages 391?405,
2014.

9


----------------------------------------------------------------

title: 89-neural-network-implementation-approaches-for-the-connection-machine.pdf

127

Neural Network Implementation Approaches
for the
Connection Machine
Nathan H. Brown, Jr.
MRJlPerkin Elmer, 10467 White Granite Dr. (Suite 304), Oakton, Va. 22124

ABSlRACf
The SIMD parallelism of the Connection Machine (eM) allows the construction of
neural network simulations by the use of simple data and control structures. Two
approaches are described which allow parallel computation of a model's nonlinear
functions, parallel modification of a model's weights, and parallel propagation of a
model's activation and error. Each approach also allows a model's interconnect
structure to be physically dynamic. A Hopfield model is implemented with each
approach at six sizes over the same number of CM processors to provide a performance
comparison.
INTRODUCflON
Simulations of neural network models on digital computers perform various
computations by applying linear or nonlinear functions, defined in a program, to
weighted sums of integer or real numbers retrieved and stored by array reference. The
numerical values are model dependent parameters like time averaged spiking frequency
(activation), synaptic efficacy (weight), the error in error back propagation models, and
computational temperature in thermodynamic models. The interconnect structure of a
particular model is implied by indexing relationships between arrays defined in a
program. On the Connection Machine (CM), these relationships are expressed in
hardware processors interconnected by a 16-dimensional hypercube communication
network. Mappings are constructed to defme higher dimensional interconnectivity
between processors on top of the fundamental geometry of the communication
network. Parallel transfers are defined over these mappings. These mappings may be
dynamic. CM parallel operations transform array indexing from a temporal succession
of references to memory to a single temporal reference to spatially distributed
processors.
Two alternative approaches to implementing neural network simulations on the CM
are described. Both approaches use "data parallelism" 1 provided by the *Lisp virtual
machine. Data and control structures associated with each approach and performance
data for a Hopfield model implemented with each approach are presented.
DATA STRUCTURES
The functional components of a neural network model implemented in *Lisp are
stored in a uniform parallel variable (pvar) data structure on the CM. The data structure
may be viewed as columns of pvars. Columns are given to all CM virtual processors.
Each CM physical processor may support 16 virtual processors. In the fust approach
described, CM processors are used to represent the edge set of a models graph
structure. In the second approach described, each processor can represent a unit, an
outgoing link, or an incoming link in a model's structure. Movement of activation (or
error) through a model's interconnect structure is simulated by moving numeric values

? American Institute of Physics 1988

128

over the eM's hypercube. Many such movements can result from the execution of a
single CM macroinstruction. The CM transparently handles message buffering and
collision resolution. However, some care is required on the part of the user to insure
that message traffic is distributed over enough processors so that messages don't stack
up at certain processors, forcing the CM to sequentially handle large numbers of
buffered messages. Each approach requires serial transfers of model parameters and
states over the communication channel between the host and the CM at certain times in a
simulation.
The first approach, "the edge list approach," distributes the edge list of a network
graph to the eM, one edge per CM processor. Interconnect weights for each edge are
stored in the memory of the processors. An array on the host machine stores the
current activation for all units. This approach may be considered to represent abstract
synapses on the eM. The interconnect structure of a model is described by product
sets on an ordered pair of identification (id) numbers, rid and sid. The rid is the id of
units receiving activation and sid the id of units sending activation. Each id is a unique
integer. In a hierarchical network, the ids of input units are never in the set of rids and
the ids of output units are never in the set of sids. Various set relations (e.g. inverse,
reflexive, symmetric, etc.) defined over id ranges can be used as a high level
representation of a network's interconnect structure. These relations can be translated
into pvar columns. The limits to the interconnect complexity of a simulated model are
the virtual processor memory limits of the CM configuration used and the stack space
~uired by functions used to compute the weighted sums of activation. Fig. 1 shows a
R -> R2 -> R4 interconnect structure and its edge list representation on the CM.
6

7

8

:z
eM PROCESSOR

0 1 2 3 4

9

3

5 6 7 8 9 1 0111213

f if

:~ (~?,';)H f HHH ff
SAcr

( 8j ) 1 2 3 1 2 3 4 5 4 5 4 5 4 5

Fig. 1. Edge List Representation of a R3_> R2 -> R4 Interconnect Structure
This representation can use as few as six pvars for a model with Hebbian
adaptation: rid (i), sid (j), interconnect weight (wij), ract (ai), sact (aj), and learn rate
(11)? Error back propagation requires the addition of: error (ei), old interconnect
weight (wij(t-l?, and the momentum term (ex). The receiver and sender unit
identification pvars are described above. The interconnect weight pvar stores the
weight for the interconnect. The activation pvar, sact, stores the current activation, aj'
transfered to the unit specified by rid from the unit specified by sid. The activation
pvar, ract, stores the current weighted activation ajwij- The error pvar stores the error
for the unit specified by the sid. A variety of proclaims (e.g. integer, floating point,
boolean, and field) exist in *Lisp to define the type and size ofpvars. Proclaims
conserve memory and speed up execution. Using a small number of pvars limits the

129

amount of memory used in each CM processor so that maximum virtualization of the
hardware processors can be realized. Any neural model can be specified in this fashion.
Sigma-pi models require multiple input activation pvars be specified. Some edges may
have a different number of input activation pvars than others. To maintain the uniform
data structure of this approach a tag pvar has to be used to determine which input
activation pvars are in use on a particular edge.
The edge list approach allows the structure of a simulated model to "physically"
change because edges may be added (up to the virtual processor limit), or deleted at any
time without affecting the operation of the control structure. Edges may also be placed
in any processor because the subselection (on rid or sid) operation performed before a
particular update operation insures that all processors (edges) with the desired units are
selected for the update.
The second simulation approach, "the composite approach," uses a more
complicated data structure where units, incoming links, and outgoing links are
represented. Update routines for this approach use parallel segmented scans to form
the weighted sum of input activation. Parallel segmented scans allow a MIMD like
computation of the weighted sums for many units at once. Pvar columns have unique
values for unit, incoming link, and outgoing link representations. The data structures
for input units, hidden units, and output units are composed of sets of the three pvar
column types. Fig. 2 shows the representation for the same model as in Fig. 1
implemented with the composite approach.
2

o1

3

5

4

6

7

8

9

2 3 4 5 6 7 8 9 101112 1314151617181920212223242526272829303132333435

rr, f~ ~\'~~Ii~

c - -?.

c

o

c - -?.

+----+
lol

IO~ O.~~
~

~~+-t+*~
t -~. - ~
II I(

II I(

I

Fig. 2. Composite Representation of a R3 -> R2 -> R4 Interconnect Structure
In Fig. 2, CM processors acting as units, outgoing links, and incoming links are
represented respectively by circles, triangles, and squares. CM cube address pointers
used to direct the parallel transfer of activation are shown by arrows below the
structure. These pointers defme the model interconnect mapping. Multiple sets of
these pointers may be stored in seperate pvars. Segmented scans are represented by
operation-arrow icons above the structure. A basic composite approach pvar set for a
model with Hebbian adaptation is: forward B, forward A, forward transfer address,
interconnect weight (Wij), act-l (ai), act-2 (aj), threshold, learn rate (Tl), current unit id
(i), attached unit id U), level, and column type. Back progagation of error requires the
addition of: backward B, backward A, backward transfer address, error (ei), previous
interconnect weight (Wij(t-l?, and the momentum tenn (ex). The forward and
backward boolean pvars control the segmented scanning operations over unit
constructs. Pvar A of each type controls the plus scanning and pvar B of each type
controls the copy scanning. The forward transfer pvar stores cube addresses for

130

forward (ascending cube address) parallel transfer of activation. The backward transfer
pvar stores cube addresses for backward (descending cube address) parallel transfer of
error. The interconnect weight, activation, and error pvars have the same functions as
in the edge list approach. The current unit id stores the current unit's id number. The
attached unit id stores the id number of an attached unit. This is the edge list of the
network's graph. The contents of these pvars only have meaning in link pvar columns.
The level pvar stores the level of a unit in a hierarchical network. The type pvar stores
a unique arbitrary tag for the pvar column type. These last three pvars are used to
subselect processor ranges to reduce the number of processors involved in an
operation.
Again, edges and units can be added or deleted. Processor memories for deleted
units are zeroed out. A new structure can be placed in any unused processors. The
level, column type, current unit id, and attached unit id values must be consistent with
the desired model interconnectivity.
The number of CM virtual processors required to represent a given model on the
CM differs for each approach. Given N units and N(N-1) non-zero interconnects (e.g.
a symmetric model), the edge list approach simply distributes N(N-1) edges to N(N-1)
CM virtual processors. The composite approach requires two virtual processors for
each interconnect and one virtual processor for each unit or N +2 N (N-1) CM virtual
processors total. The difference between the number of processors required by the two
approaches is N2. Table I shows the processor and CM virtualization requirements for
each approach over a range of model sizes.
TABLE I Model Sizes and CM Processors Required
Run No. Grid Size Number of Units Edge List Quart CM Virt. Procs. Virt. LeveL
N(N-1)
1
2
3
4
5
6

82
92
112
13 2
162
192

64
81
121
169
256
361

4032
6480
14520
28392
65280
129960

8192
8192
16384
32768
65536
131072

0
0
0
2
4
8

Run No. Grid Size Number of Units Composite Quart CM Virt. Procs. Virt. LeveL
N+2N(N-1)
7
8
9
10
11
12

82
92
112
132
162
192

64
81
121
169
256
361

8128
13041
29161
56953
130816
260281

8192
16384
32768
65536
131072
262144

0
0
2
4
8
16

131

CONTROL STRUCTURES
The control code for neural network simulations (in *Lisp or C*) is stored and
executed sequentially on a host computer (e.g. Symbolics 36xx and V AX 86xx)
connected to the CM by a high speed communication line. Neural network simulations
executed in *Lisp use a small subset of the total instruction set: processor selection
reset (*all), processor selection (*when), parallel content assignment (*set), global
summation (*sum), parallel multiplication (*!! ), parallel summation (+! I), parallel
exponentiation (exp! I), the parallel global memory references (*pset) and (pref! I), and
the parallel segmented scans (copy!! and +!!). Selecting CM processors puts them in a
"list of active processors" (loap) where their contents may be arithmetically manipulated
in parallel. Copies of the list of active processors may be made and used at any time. A
subset of the processors in the loap may be "subselected" at any time, reducing the loap
contents. The processor selection reset clears the current selected set by setting all
processors as selected. Parallel content assignment allows pvars in the currently
selected processor set to be assinged allowed values in one step. Global summation
executes a tree reduction sum across the CM processors by grid or cube address for
particular pvars. Parallel multiplications and additions multiply and add pvars for all
selected CM processors in one step. The parallel exponential applies the function, eX, to
the contents of a specified pvar, x, over all selected processors. Parallel segmented
scans apply two functions, copy!! and +!!, to subsets ofCM processors by scanning
across grid or cube addresses. Scanning may be forward or backward (Le. by
ascending or descending cube address order, respectively).
Figs. 3 and 4 show the edge list approach kernels required for Hebbian learning for
a R2 -> R2 model. The loop construct in Fig. 3 drives the activation update
(1)

operation. The usual loop to compute each weighted sum for a particular unit has been
replaced by four parallel operations: a selection reset (*all), a subselection of all the
processors for which the particular unit is a receiver of activation (*when (=!! rid (!!
(1+ u??, a parallel multiplication (*!! weight sact), and a tree reduction sum (*sum
... ). Activation is spread for a particular unit, to all others it is connected to, by:
storing the newly computed activation in an array on the host, then subselecting the
processors where the particular unit is a sender of activation (*when (=!! sid (!! (1 +
u??, and broadcasting the array value on the host to those processors.
(dotimes (u 4)
(*all (*when (=!! rid (!! (1+ u?)
(setf (aref activation u)
(some-nonlinearity (*sum (*!! weight sact??
(*set ract (!! (aref activation u?)
(*all (*when (=!! sid (!! (1+ u?)
(*set sact (!! (aref activation u???
Fig. 3. Activation Update Kernel for the Edge Lst Approach.
Fig. 4 shows the Hebbian weight update kernel

132

(2)

(*all
(*set weight
(*!! learn-rate ract sact??
Fig. 4. Hebbian Weight Modification Kernel for the Edge List Approach
The edge list activation update kernel is essentially serial because the steps involved can
only be applied to one unit at a time. The weight modification is parallel. For error
back propagation a seperate loop for computing the errors for the units on each layer of
a model is required. Activation update and error back propagation also require transfers
to and from arrays on the host on every iteration step incurring a concomitant overhead.
Other common computations used for neural networks can be computed in parallel
using the edge list approach. Fig. 5 shows the code kernel for parallel computation of
Lyapunov engergy equations

(3)
where i= 1 to number of units (N).
(+ (* -.5 (*sum (*!! weight ract sact?) (*sum (*!! input sact?)
Fig. 5. Kernel for Computation of the Lyapunov Energy Equation
Although an input pvar, input, is defined for all edges, it is only non-zero for those
edges associated with input units. Fig. 6 shows the pvar structure for parallel
computation of a Hopfield weight prescription, with segmented scanning to produce the
weights in one step,
IJ -l:S
r= I(2ar1?-I)(2arJ?-I)

W? ?

(4)

where wii=O, Wij=Wjh and r=I to the number of patterns, S, to be stored.
seg
t
n
ract
vII V2 1 ...
V I 2 v22' ..
sact
weight

n
t
n
n
VSI vII V2 I ... VSI .. .
VS2 v13 v23 ... VS3 .. .
wI2
w13

Fig. 6. Pvar Structure for Parallel Computation QfHopfield Weight Prescription
Fig. 7 shows the *Lisp kernel used on the pvar structure in Fig. 6.
(set weight
(scan '+!! (*!! (-!! (*!! ract (!! 2? (!! 1? (-!! (*!! sact (!! 2? (!! 1??
:segment-pvar seg :inc1ude-self t)
Fig. 7. Parallel Computation of Hopfield Weight Prescription

133

The inefficiencies of the edge list activation update are solved by the updating
method used in the composite approach. Fig. 8 shows the *Lisp kernel for activation
update using the composite approach. Fig. 9 shows the *Lisp kernel for the Hebbian
learning operation in the composite approach.

(*a1l
(*when (=!! level (!! 1?
(*set act (scan!! act-I 'copy!! :segment-pvar forwardb :include-self t?
(*set act (*!! act-l weight?
(*when (=!! type (!! 2? (*pset :overwrite act-l act-I ftransfer?)
(*when (=!! level (!! 2?
(*set act (scan!! act-l '+!! :segment-pvar forwarda :include-self t?
(*when (=!! type (!! 1? (some-nonlinearity!! act-I??
Fig. 8. Activation Update Kernel for the Composite Approach

(*all
(*set act-l (scan!! act-I 'copy!! :segment-pvar forwardb
:include-self t?
(*when (=!! type (!! 2?
(*set act-2 (pref!! act-I btransfer?)
(*set weight
(+!! weight
(*!! learn-rate act-l act-2??)
Fig. 9. Hebbian Weight Update Kernel for the Composite Approach
It is immediately obvious that no looping is invloved. Any number of interconnects
may be updated by the proper subselection. However, the more subselection is used
the less efficient the computation becomes because less processors are invloved.
COMPLEXITY ANALYSIS
The performance results presented in the next section can be largely anticipated
from an analysis of the space and time requirements of the CM implementation
approaches. For simplicity I use a Rn -> Rn model with Hebbian adaptation. The
oder of magnitude requirements for activation and weight updating are compared for
both CM implementation approaches and a basic serial matrix arithmetic approach.
F~r the given model the space requirements on a conventional serial machine are
2n+n locations or O(n 2). The growth of the space requirement is dominated by the
nxn weight matrix. defining the system interconnect structure. The edge list appro~ch
uses six pvars for each processor and uses nxn processors for the mapping, or 6n
locations or O(n2). The composite approach uses 11 pvars. There are 2n processors
for units and 2n2 proces~ors for interconnects in the given model. The composite
approach uses 11(2n+2n ) locations or O(n2 ). The CM implementations take up
roughly the same space as the serial implementation, but the space for the serial
implementation is composed of passive memory whereas the space for the CM
implementations is composed of interconnected processors with memory .

The time analysis for the approaches compares the time order of magnitudes to
compute the activation update (1) and the Hebbian weight update (2). On a serial

134

machine, the n weighted sums computed for the ac~vation update require n2
multiplicationsffd n(n-l) additions. There are 2n -n operations or time order of
magnitude O(n ~ The time order of magnitude for the weight matrix update is O(n2)
since there are n weight matrix elements.
The edge list approach forms n weighted sums by performing a parallel product of
all of the weights and activations in the model, (*!! weight sact), and then a tree
reduction sum, (*sum ... ), of the products for the n uni~ (see Fig. 4). There are
1+n(nlog2n) operations or time order of magnitude O(n ). This is the same order of
magnitude as obtained on a serial machine. Further, the performance of the activation
update is a function of the number of interconnects to be processed.
The composite approach forms n weighted sums in nine steps (see Fig. 8): five
.selection operations; the segmented copy scan before the parallel multiplication; the
parallel multiplication; the parallel transfer of the products; and the segmented plus
scan, which forms the n sums in one step. This gives the composite activation update a
time order of magnitude O( 1). Performance is independent of the number of
interconnects processed. The next section shows that this is not quite true.
The n2 weights in the model can be updated in three parallel steps using the edge
list approach (see Fig. 4). The n2 weights in the model can be updated in eight parallel
steps using the composite approach (see Fig. 9). In either case, the weight update
operation has a time order of magnitude 0(1).
The time complexity results obtained for the composite approach apply to
computation of the Lyaponov energy equation (3) and the Hopfield weighting
prescription (4), given that pvar structures which can be scanned (see Figs. 1 and 6) are
used. The same operations performed serially are time order of magnitude 0(n2).
The above operations all incur a one time overhead cost for generating the addresses
in the pointer pvars, used for parallel transfers, and arranging the values in segments
for scanning. What the above analysis shows is that time complexity is traded for
space complexity. The goal of CM programming is to use as many processors as
possible at every step.
PERFORMANCE COMPARISON
Simulations of a Hopfield spin-glass model2 were run for six different model sizes
over the same number (16,384) of physical CM processors to provide a performance
comparison between implementation approaches. The Hopfield network was chosen
for the performance comparison because of its simple and well known convergence
dynamics and because it uses a small set of pvars which allows a wide range of
network sizes (degrees of virtualization) to be run. Twelve treaments are run. Six with
the edge list approach and six with the composite approach. Table 3-1 shows the
model sizes run for each treatment. Each treatment was run at the virtualization level
just necessary to accomodate the number of processors required for each simulation.
Two exemplar patterns are stored. Five test patterns are matched against the two
exemplars. Two test patterns have their centers removed, two have a row and column
removed, and one is a random pattern. Each exemplar was hand picked and tested to
insure that it did not produce cross-talk. The number of rows and columns in the
exemplars and patterns increase as the size of the networks for the treatments increases.

135

Since the performance of the CM is at issue, rather than the performance of the network
model used, a simple model and a simple pattern set were chosen to minimize
consideration of the influence of model dynamics on performance.
Performance is presented by plotting execution speed versus model size. Size is
measured by the number of interconnects in a model. The execution speed metric is
interconnects updated per second, N*(N-l )/t, where N is the number of units in a
model and t is the time used to update the activations for all of the units in a model. All
of the units were updated three times for each pattern. Convergence was determined
by the output activation remaining stable over the fmal two updates. The value of t for
a treatment is the average of 15 samples of t. Fig. 10 shows the activation update cycle
time for both approaches. Fig. 11 shows the interconnect update speed plots for both
approaches. The edge list approach is plotted in black. The composite approach is
plotted in white. The performance shown excludes overhead for interpretation of the
*Lisp instructions. The model size categories for each plot correspond to the model
sizes and levels of eM virtualization shown in Table I.
Activation Update Cycle Time vs Model Size

1 .6
1 .4
1.2

?

sees O.B

0.6
0.4

?

?

o
0.2
?
OO___~~~__~O~__~O~__.O__~
1

2

3

4

5

6

Model Size

Fig. 10. Activation Update Cycle Times
Interconnect Update Speed Comparison
Edge Ust Approach vs. Composite Approach

2000000}
0

1500000

0
0

i.p.s. 1000000

0

0

0

500000t

o?1

?
2

?

?

3
4
Model Size

?

5

?
6

Fig. 11. Edge List Interconnect Update Speeds
Fig. 11 shows an order of magnitude performance difference between the
approaches and a roll off in performance for each approach as a function of the number
of virtual processors supported by each physical processor. The performance tum
around is at 4x virtualization for the edge list approach and 2x virtualization for the
composite approach.

136

CONCLUSIONS
Representing the interconnect structure of neural network models with mappings
defined over the set of fine grain processors provided by the CM architecture provides
good performance for a modest programming effort utilizing only a small subset of the
instructions provided by *Lisp. Further, the perfonnance will continue to scale up
linearly as long as not more than 2x virtualization is required. While the complexity
analysis of the composite activation update suggests that its performance should be
independent of the number of interconnects to be processed, the perfonnance results
show that the performance is indirectly dependent on the number of interconnects to be
processed because the level of virtualization required (after the physical processors are
exhausted) is dependent on the number of interconnects to be processed and
virtualization decreases performance linearly. The complexity analysis of the edge list
activation update shows that its perfonnance should be roughly the same as serial
implementations on comparable machines. The results suggest that the composite
approach is to be prefered over the edge list approach but not be used at a virtualization
level higher than 2x.
The mechanism of the composite activation update suggest that hierarchical
networks simulated in this fashion will compare in perfonnance to single layer
networks because the parallel transfers provide a type of pipeline for activation for
synchronously updated hierarchical networks while providing simultaneous activation
transfers for asynchronously updated single layer networks. Researchers at Thinking
Machines Corporation and the M.I.T. AI Laboratory in Cambridge Mass. use a similar
approach for an implementation of NETtalk. Their approach overlaps the weights of
connected units and simultaneously pipelines activation forward and error backward. 3
Perfonnance better than that presented can be gained by translation of the control
code from interpreted *Lisp to PARIS and use of the CM2. In addition to not being
interpreted, PARIS allows explicit control over important registers that aren't
accessable through *Lisp. The CM2 will offer a number of new features which will
enhance perfonnance of neural network simulations: a *Lisp compiler, larger
processor memory (64K), and floating point processors. The complier and floating
point processors will increase execution speeds while the larger processor memories
will provide a larger number of virtual processors at the performance tum around points
allowing higher perfonnance through higher CM utilization.
REFERENCES

1. "Introduction to Data Level Parallelism," Thinking Machines Technical Report
86.14, (April 1986).
2. Hopfield, J. J., "Neural networks and physical systems with emergent collective
computational abilities," Proc. Natl. Acad. Sci., Vol. 79, (April 1982), pp. 2554-2558.
3. Blelloch, G. and Rosenberg, C. Network Learning on the Connection Machine,
M.I.T. Technical Report, 1987.


----------------------------------------------------------------

title: 1259-are-hopfield-networks-faster-than-conventional-computers.pdf

Are Hopfield Networks Faster Than
Conventional Computers?

Ian Parberry* and Hung-Li Tsengt
Department of Computer Sciences
University of North Texas
P.O. Box 13886
Denton, TX 76203-6886

Abstract
It is shown that conventional computers can be exponentiallx faster
than planar Hopfield networks: although there are planar Hopfield
networks that take exponential time to converge, a stable state of an
arbitrary planar Hopfield network can be found by a conventional
computer in polynomial time. The theory of 'P.cS-completeness
gives strong evidence that such a separation is unlikely for nonplanar Hopfield networks, and it is demonstrated that this is also the
case for several restricted classes of nonplanar Hopfield networks,
including those who interconnection graphs are the class of bipartite graphs, graphs of degree 3, the dual of the knight's graph, the
8-neighbor mesh, the hypercube , the butterfly, the cube-connected
cycles, and the shuffle-exchange graph.

1

Introduction

Are Hopfield networks faster than conventional computers? This apparently
straightforward question is complicated by the fact that conventional computers
are universal computational devices, that is, they are capable of simulating any
discrete computational device including Hopfield networks. Thus , a conventional
computer could in a sense cheat by imitating the fastest Hopfield network possible.
* Email: ianGcs. unt .edu. URL: http://hercule .csci. unt. edu/ian.
t Email: ht sengGponder. csci. unt . edu.

I. Parberry and H. Tseng

240

But the question remains, is it faster for a computer to imitate a Hopfield network ,
or to use other computational methods? Although the answer is likely to be different for different benchmark problems, and even for different computer architectures ,
we can make our results meaningful in the long term by measuring scalability, that
is, how the running time of Hopfield networks and conventional computers increases
with the size of any benchmark problem to be solved.
Stated more technically, we are interested in the computational complexity of the
stable state problem for Hopfield networks , which is defined succinctly as follows :
given a Hopfield network, determine a stable configuration. As previously stated,
this stable configuration can be determined by imitation, or by other means. The
following results are known about the scalability of Hopfield network imitation. Any
imitative algorithm for the stable state problem must take exponential time on som e
Hopfield networks, since there exist Hopfield networks that require exponential time
to converge (Haken and Luby [4] , Goles and Martinez [2]) . It is unlikely that even
non-imitative algorithms can solve the stable state problem in polynomial time ,
since the latter is PeS-complete (Papadimitriou , Schaffer, and Yannakakis [9]).
However , the stable state problem is more difficult for some classes of Hopfield
networks than others. Hopfield networks will converge in polynomial time if their
weights are bounded in magnitude by a polynomial of the number of nodes (for
an expository proof see Parberry [11 , Corollary 8.3.4]) . In contrast , the stable
state problem for Hopfield networks whose interconnection graph is bipartite is
peS-complete (this can be proved easily by adapting techniques from Bruck and
Goodman [1]) which is strong evidence that it too requires superpolynomial time
to solve even with a nonimitative algorithm.
We show in this paper that although there exist planar Hopfield networks that t ake
exponential time to converge in the worst case , the stable state problem for planar
Hopfield networks can be solved in polynomial time by a non-imitative algorithm.
This demonstrates that imitating planar Hopfield networks is exponentially slower
than using non-imitative algorithmic techniques. In contrast , we discover that the
stable state problem remains peS-complete for many simple classes of nonplanar
Hopfield network , including bipartite networks , networks of degree 3, and some
networks that are popular in neurocomputing and parallel computing.
The main part of this manuscript is divided into four sections. Section 2 contains
some background definitions and references. Section 3 contains our results about
planar Hopfield networks. Section 4 describes our peS-completeness results , based
on a pivotal lemma about a nonstandard type of graph embedding.

2

Background

This section contains some background which are included for completeness but
may be skipped on a first reading. It is divided into two subsections , the first on
Hopfield networks, and the second on PeS-completeness.

2.1

Hopfield Networks

4- Hopfield network [6] is a discrete neural network model with symmetric connections . Each processor in the network computes a hard binary weighted threshold

Are Hopfield Networks Faster than Conventional Computers?

241

function. Only one processor is permitted to change state at any given time. That
processor becomes active if its excitation level exceeds its threshold, and inactive
otherwise. A Hopfield network is said to be in a stable state if the states of all of
its processors are consistent with their respective excitation levels. It is well-known
that all Hopfield networks converge to a stable state. The proof defines a measure
called energy, and demonstrates that energy is positive but decreases with every
computation step. Essentially then, a Hopfield network finds a local minimum in
some energy landscape.
2.2

P .cS-completeness

While the theory of NP-completeness measures the complexity of global optimization, the theory of p.cS-completeness developed by Johnson, Papadimitriou, and
Yannakakis [7] measures the complexity of local optimization. It is similar to the
theory of NP-completeness in that it identifies a set of difficult problems known
collectively as p.cS-complete problems. These are difficult in the sense that if a
fast algorithm can be developed for any P .cS-complete problem, then it can be
used to give fast algorithms for a substantial number of other local optimization
problems including many important problems for which no fast algorithms are currently known. Recently, Papadimitriou, Schaffer, and Yannakakis [9] proved that
the problem of finding stable states in Hopfield networks is P .cS-complete.

3

Planar Hopfield Networks

A planar Hopfield network is one whose interconnection graph is planar, that is, can
be drawn on the Euclidean plane without crossing edges. Haken and Luby [4] describe a planar Hopfield network that provably takes exponential time to converge,
and hence any imitative algorithm for the stable state problem must take exponential time on some Hopfield network. Yet there exists a nonimitative algorithm for
the stable state problem that runs in polynomial time on all Hopfield networks:
Theorem 3.1 The stable state problem for Hopfield networks with planar interconnection pattern can be solved in polynomial time.
PROOF: (Sketch.) The prooffollows from the fact that the maximal cut in a planar
graph can be found in polynomial time (see , for example, Hadlock [3]), combined
with results of Papadimitriou, Schaffer, and Yannakakis [9]. 0

4

P .cS-completeness Results

Our P .cS-completeness results are a straightforward consequence of a new result
that characterizes the difficulty of the stable state problem of an arbitrary class
of Hopfield networks based on a graph-theoretic property of their interconnection
patterns. Let G = (V, E) and H = (V', E') be graphs. An embedding of G into H
is a function f: V -+ 2Vi such that the following properties hold. (1) For all v E V,
the subgraph of H induced by f(v) is connected. (2) For all (u, v) E E, there exists
a path (which we will denote f(u , v)) in H from a member of f(u) to a member
of f(v). (3) Each vertex w E H is used at most once, either as a member of f(v)

I. Parberry and H. Tseng

242

for some v E V, or as an internal vertex in a path feu, v) for some u, v E V. The
graph G is called the guest graph, and H is called the host graph. Our definition
of embedding is different from the standard notion of embedding (see, for example,
Hong, Mehlhorn, and Rosenberg [5]) in that we allow the image of a single guest
vertex to be a set of host vertices, and we insist in properties (2) and (3) that the
images of guest edges be distinct paths. The latter property is crucial to our results,
and forms the major difficulty in the proofs.
Let 5, T be sets of graphs. 5 is said to be polynomial-time embeddable into T,
written 5 ::;e T, if there exists polynomials Pl(n),P2(n) and a function f with the
following properties: (1) f can be computed in time PI(n), and (2) for every G E 5
with n vertices, there exists H E T with at most p2(n) vertices such that G can
be embedded into H by f. A set 5 of graphs is said to be pliable if the set of all
graphs is polynomial-time embeddable into 5.
Lemma 4.1 If 5 is pliable, then the problem of finding a stable state in Hopfield
networks with interconnection graphs in 5 is 'P ?S-complete.
PROOF: (Sketch.) Let 5 be a set of graphs with the property that the set of all
graphs is polynomial-time embeddable into 5 . By the results of Papadimitriou,
Schaffer, and Yannakakis [9], it is enough to show that the max-cut problem for
graphs in 5 is 'P ?S-complete.

Let G be an arbitrary labeled graph. Suppose G is embedded into H E 5 under the
polynomial-time embedding. For each edge e in G of cost c, select one edge from
the path connecting the vertices in f( e) and assign it cost c. We call this special
edge f' (e). Assign all other edges in the path cost -00. For all v E V, assign the
edges linking the vertices in f(v) a cost of -00. Assign all other edges of H a cost
of zero.
It can be shown that every cut in G induces a cut of the same cost in H, as follows.
Suppose G ~ E is a cut in G, that is, a set of edges that if removed from G,
disconnects it into two components containing vertices VI and V2 respectively. Then,
removing vertices f'(G) and all zero-cost edges from H will disconnect it into two
components containing vertices f(VI ) and f(V2 ) respectively. Furthermore, each
cut of positive cost in H induces a cut of the same cost in G, since a positive cost
cut in H cannot contain any edges of cost -00, and hence must consist only of f'(e)
for some edges e E E. Therefore, every max-cost cut in H induces in polynomial
time a max-cost cut in G. 0

We can now present our 'P ?S-completeness results. A graph has degree 3 if all
vertices are connected to at most 3 other vertices each.
Theorem 4.2 The problem of finding stable states in Hopfield networks of degree
3 is 'P ?S-complete.
PROOF:
(Sketch.) By Lemma 4.1, it suffices to prove that the set of degree-3
graphs is pliable. Suppose G (V, E) is an arbitrary graph. Replace each degree-k
vertex x E V by a path consisting of k vertices, and attach each edge incident with
v by a new edge incident with one of the vertices in the path. Figure 1 shows an
example of this embedding. 0

=

243

Are Hopfield Networks Faster than Conventional Computers?

Figure 1: A guest graph of degree 5 (left), and the corresponding host of degree 3
(right). Shading indicates the high-degree nodes that were embedded into paths.
All other nodes were embedded into single nodes.

Figure 2: An 8-neighbor mesh with 25 vertices (left), and the 8
superimposed on an 8 x 8 board (right).

X

8 knight's graph

The 8-neighbor mesh is the degree-8 graph G = (V, E) defined as follows: V =
{1,2, ... ,m} x {1,2, ... ,n}, and vertex (u,v) is connected to vertices (u,v? 1),
(u ? 1, v), (u ? 1, v ? 1). Figure 2 shows an 8-neighbor mesh with 25 vertices.
Theorem 4.3 The problem of finding stable states in H opfield networks on the
8-neighbor mesh is peS-complete.

PROOF: (Sketch.) By Lemma 4.1, it suffices to prove that the 8-neighbor mesh is
pliable. An arbitrary graph can be embedded on an 8-neighbor mesh by mapping
each node to a set of consecutive nodes in the bottom row of the grid, and mapping
edges to disjoint rectilinear paths which use the diagonal edges of the grid for
crossovers. 0
The knight's graph for an n X n chessboard is the graph G = (V, E) where V =
{(i, j) 11 ~ i, j ~ n}, and E
{((i, j), (k,
I {Ii - kl, Ij - il} {I, 2}}. That is,
there is a vertex for every square of the board and an edge between two vertices
exactly when there is a knight's move from one to the other. For example, Figure 2
shows the knight's graph for the 8 x 8 chessboard. Takefuji and Lee [15] (see also
Parberry [12]) use the dual of the knight's graph for a Hopfield-style network to
solve the knight's tour problem. That is, they have a vertex Ve for each edge e of
the knight's graph, and an edge between two vertices Vd and Ve when d and e share
a common vertex in the knight's graph.

=

i?

=

244

I. Parberry and H. Tseng

Theorem 4.4 The problem of finding stable states in H opfield networks on the dual
of the knight's graph is pes -complete.
PROOF: (Sketch.) By Lemma4.1, it suffices to prove that the dual of the knight 's
graph is pliable. It can be shown that the knight 's graph is pliable using the
technique of Theorem 4.3. It can also he proved that if a set S of graphs is pliable,
then the set consisting of the duals of graphs in S is also pliable. 0

The hypercube is the graph with 2d nodes for some d, labelled with the binary
representations of the d-bit natural numbers , in which two nodes are connected by
an edge iff their labels differ in exactly one bit. The hypercube is an important
graph for parallel computation (see , for example, Leighton [8], and Parberry [lOD .
Theorem 4.5 The problem of finding stable states in Hopfield networks on the
hypercube is peS-complete.
PROOF : (Sketch.) By Lemma 4.1, it suffices to prove that the hypercube is pliable.
Since the " ~e" relation is transitive, it further suffices by Theorem 4.2 to show that
the set of degree-3 graphs is polynomial-time embeddable into the hypercube . To
embed a degree-3 graph G into the hypercube , first break it into a degree-1 graph
G 1 and a degree-2 graph G2 . Since G 2 consists of cycles, paths, and disconnected
vertices, it can easily be embedded into a hypercube (since a hypercube is rich
in cycles). G 1 can be viewed as a permutation of vertices in G and can hence be
realized using a hypercube implementation of Waksman 's permutation network [16] .

o
We conclude by stating PeS-completeness results for three more graphs that are
important in the parallel computing literature the butterfly (see, for example,
Leighton [8]) , the cube- connected cycles (Preparata and Vuillemin [13D , and the
shuffle-exchange (Stone [14]). The proofs use Lemma 4.1 and Theorem 4.5 , and are
omitted for conciseness.
Theorem 4.6 The problem of finding stable states in Hopfield networks on the
butterfly, the cube-connected cycles, and the shuffle-exchange is peS-complete.

Conclusion

Are Hopfield networks faster than conventional computers? The answer seems to be
that it depends on the interconnection graph of the Hopfield network. Conventional
nonimitative algorithms can be exponentially faster than planar Hopfield networks.
The theory of peS-completeness shows us that such an exponential separation
result is unlikely not only for nonplanar graphs, but even for simple nonplanar
graphs such as bipartite graphs , graphs of degree 3, the dual of the knight's graph,
the 8-neighbor mesh , the hypercube , the butterfly, the cube-connected cycles, and
the shuffle-exchange graph.
Acknowledgements

The research described in this paper was supported by the National Science Foundation under grant number CCR- 9302917 , and by the Air Force Office of Scientific

Are Hopfield Networks Faster than Conventional Computers?

245

Research, Air Force Systems Command, USAF, under grant number F49620-93-10100.

References
[1] J. Bruck and J. W. Goodman. A generalized convergence theorem for neural
networks. IEEE Transactions on Information Theory, 34(5):1089-1092, 1988.
[2] E. Goles and S. Martinez. Exponential transient classes of symmetric neural
networks for synchronous and sequential updating. Complex Systems, 3:589597, 1989.
[3] F. Hadlock. Finding a maximum cut of a planar graph in polynomial time.
SIAM Journal on Computing, 4(3) :221-225, 1975.
[4] A. Haken and M. Luby. Steepest descent can take exponential time for symmetric conenction networks. Complex Systems, 2:191-196,1988.
[5] J.-W. Hong, K. Mehlhorn, and A.L. Rosenberg. Cost tradeoffs in graph embeddings. Journal of the ACM, 30 :709-728,1983.
[6] J. J . Hopfield. Neural networks and physical systems with emergent collective
computational abilities. Proc. National Academy of Sciences, 79:2554-2558 ,
April 1982.
[7] D. S. Johnson , C. H. Papadimitriou, and M. Yannakakis. How easy is local
search? In 26th Annual Symposium on Foundations of Computer Science,
pages 39-42. IEEE Computer Society Press , 1985.
[8] F. T. Leighton. Introduction to Parallel Algorithms and Architectures: Arrays
. Trees? Hypercubes. Morgan Kaufmann, 1992.
[9] C. H. Papadimitrioll, A. A. Schaffer, and M. Yannakakis. On the complexity
of local search. In Proceedings of the Twenty Second Annual ACM Symposium
on Theory of Computing, pages 439-445. ACM Press, 1990.
[10] I. Parberry. Parallel Complexity Theory. Research Notes in Theoretical Computer Science. Pitman Publishing, London , 1987.
[11] I. Parberry. Circuit Complexity and Neural Networks. MIT Press , 1994.
[12] I. Patberry. Scalability of a neural network for the knight's tour problem.
Neurocomputing, 12 :19-34, 1996.
[13] F. P. Preparata and J. Vuillemin . The cube-connected cycles: A versatile
network for parallel computation. Communications of the ACM, 24(5):300309 , 1981.
[14] H. S. Stone. Parallel processing with the perfect shuffle. IEEE Transactions
on Computers, C-20(2):153-161, 1971.
[15] Y. Takefuji and K. C. Lee. Neural network computing for knight's tour problems. Neurocomputing, 4(5):249-254 , 1992.
[16] A. Waksman. A permutation network . Journal of the ACM, 15(1):159-163,
January 1968.


----------------------------------------------------------------

title: 804-asynchronous-dynamics-of-continuous-time-neural-networks.pdf

Asynchronous Dynamics of Continuous
Time Neural Networks

Xin Wang
Computer Science Department
University of California at Los Angeles
Los Angeles, CA 90024

Qingnan Li
Department of Mathematics
University of Southern California
Los Angeles, CA 90089-1113

Edward K. Blum
Department of Mathematics
University of Southern California
Los Angeles, CA 90089-1113

ABSTRACT
Motivated by mathematical modeling, analog implementation and
distributed simulation of neural networks, we present a definition of
asynchronous dynamics of general CT dynamical systems defined
by ordinary differential equations, based on notions of local times
and communication times. We provide some preliminary results
on globally asymptotical convergence of asynchronous dynamics
for contractive and monotone CT dynamical systems. When applying the results to neural networks, we obtain some conditions
that ensure additive-type neural networks to be asynchronizable.

1

INTRODUCTION

Neural networks are massively distributed computing systems. A major issue in parallel and distributed computation is synchronization versus asynchronization (Bertsekas and Tsitsiklis, 1989). To fix our idea, we consider a much studied additive-type
model (Cohen and Grossberg, 1983; Hopfield, 1984; Hirsch, 1989) of a continuoustime (CT) neural network of n neurons, whose dynamics is governed by
n

Xi(t)

= -ajXi(t) + L WijO'j (Jlj Xj (t)) + Ii,

i

= 1,2, ... , n,

(1)

j=1

493

494

Wang. Li. and Blum

with neuron states Xi (t) at time t, constant decay rates ai, external inputs h, gains
neuron activation functions Uj and synaptic connection weights Wij. Simulation and implementation of idealized models of neural networks such as (1) on
centralized computers not only limit the size of networks, but more importantly
preclude exploiting the inherent massive parallelism in network computations. A
truly faithful analog implementation or simulation of neural networks defined by
(1) over a distributed network requires that neurons follow a global clock t, communicate timed states Xj(t) to all others instantaneously and synchronize global
dynamics precisely all the time (e.g., the same Xj(t) should be used in evolution of
all Xi(t) at time t). Clearly, hardware and software realities make it very hard and
sometimes impossible to fulfill these requirements; any mechanism used to enforce
such synchronization may have an important effect on performance of the network. Moreover, absolutely insisting on synchronization contradicts the biological
manifestation of inherent asynchrony caused by delays in nerve signal propagation,
variability of neuron parameters such as refractory periods and adaptive neuron
gains. On the other hand, introduction of asynchrony may change network dynamics, for example, from convergent to oscillatory. Therefore, validity of asynchronous
dynamics of neural networks must be assessed in order to ensure desirable dynamics
in a distributed environment.
JJj,

Motivated by the above issues, we study asynchronous dynamics of general CT dynamical systems with neural networks in particular. Asynchronous dynamics has
been thoroughly studied in the context of iterative maps or discrete-time (DT) dynamical systems; see, e.g., (Bertsekas and Tsitsiklis, 1989) and references therein.
Among other results are that P-contractive maps on Rn (Baudet, 1978) and continuous maps on partially ordered sets (Wang and Parker, 1992) are asynchronizable,
i.e., any asynchronous iterations of these maps will converge to the fixed points
under synchronous (or parallel) iterations. The synchronization issue has also been
addressed in the context of neural networks. In fact, the celebrated DT Hopfield
model (Hopfield, 1982) adopts a special kind of asynchronous dynamics: only one
randomly chosen neuron is allowed to update its state at each iterative step. The
issue is also studied in (Barhen and Gulati, 1989) for CT neural networks. The
approach there is, however, to convert the additive model (1) into a DT version
through the Euler discretization and then to apply the existing result for contractive mappings in (Baudet, 1978) to ensure the discretized system to be asynchronizable. Overall, studies for asynchronous dynamics of CT dynamical systems are
still lacking; there are even no reasonable definitions for what it means, at least to
our knowledge.
In this paper, we continue our studies on relationships between CT and DT dynamical systems and neural networks (Wang and Blum, 1992; Wang, Blum and Li,
1993) and concentrate on their asynchronous dynamics. We first extend a concept
of asynchronous dynamics of DT systems to CT systems, by identifying the distinction between synchronous and asynchronous dynamics as (i) presence or absence of
a common global clock used to synchronize the dynamics of the different neurons
and (ii) exclusion or inclusion of delay times in communication between neurons,
and present some preliminary results for asynchronous dynamics of contractive and
monotone CT systems.

Asynchronous Dynamics of Continuous Time Neural Networks

2

MATHEMATICAL FORMULATION

To be general, we consider a CT dynamical system defined by an n-dimensional
system of ordinary differential equations,

(2)
where Ii : Rn --+ R are continuously differentiable and x(t) E Rn for all t in R+ (the
set of all nonnegative real numbers). In contrast to the asynchronous dynamics
given below, dynamics of this system will be called synchronous. An asynchronous
scheme consists of two families of functions Ci : R+ --+ R+ and rj : R+ --+ R+,
i, j
1, ... , n, satisfying the following constraints: for any t > 0,

=

(i) Initiation: Ci(t) ~ 0 and rJ(t) ~ 0;

(ii) Non-starvation:

Ci'S

are differentiable and l\(t)

(iii) Liveness: limt_oo Ci(t) =

00

> 0;

and limt_oo rJ(t) =

00;

(iv) Accessibility: rj(t) ~ Cj(t).
Given an asynchronous scheme ({cd, {rJ}), the associated asynchronous dynamics
of the system (2) is the solution of the following parametrized system:

(3)
We shall call this system an asynchronized system of the original one (2).
The functions Ci(t) should be viewed as respective "local" times (or clocks) of components i, as compared to the "global" time (or clock) t. As each component i
evolves its state according to its local time Ci(t), no shared global time t is needed
explicitly; t only occurs implicitly. The functions rj(t) should be considered as time
instants at which corresponding values Xi of components j are used by component
i; hence the differences (ci(t) - rj(t? ~ 0 can be interprated as delay times in
communication between the components j and i. Constraint (i) reflects the fact
that we are interested in the system dynamics after some global time instance, say
0; constraint (ii) states that the functions Ci are monotone increasing and hence the
local times evolve only forward; constraint (iii) characterizes the live ness property
of the components and communication channels between components; and, finally,
constraint (iv) precludes the possibility that component i accesses states x j ahead
of the local times Cj(t) of components j which have not yet been generated.
Notice that, under the assumption on monotonicity of Ci(t), the inverses C;l(t) exist
and the asynchronized system (3) can be transformed into

(4)
by letting Yi(t) = Xi( Ci(t? and y} (t) = Xj (rJ(t? = Yj (c;l (rJ(t? for i, j = 1,2, ... , n.
The vector form of (4) can be given by

iJ = C F[Y]
f

(5)

495

496

Wang, Li, and Blum

where yet) = [Yl (t), "" Yn(t)]T, C' = diag(dcl (t)/dt, "" dcn(t)/dt) , F
y [Y;] and

=

_
F[Y]

=

/1 cYi(t) , yHt), "" y~(t))
hcYr(t),
y~(t), "" y~(t))
[
,
fn (i/'l (t), y~(t), ""

= [/1, ""

fn]T,

1
'

y~(t))

Notice that the complication in the way F applies to Y ~imply means ,t hat every
component i will use possibly different "global" states [Yi(t) , y2(t) , "" y~(t)] , This
peculiarity makes the equation (5) fit into none ofthe categories of general functional
1, "., n are equal,
differential equations (Hale, 1977), However, if rJ(t) for i
all the components will use a same global state y
[yHt) , y~(t), .. " y~(t)] and
the asynchronized system (5) assumes a form of retarded functional differential
equations,

=

=

(6)
iJ = c' FcY),
We shall call this case uniformly-delayed, which will be a main consideration in the
next section where we discuss asynchronizable systems,
The system (5) includes some special cases. In a no communication delay situation,
rj(t) Cj(t) for all i and the system (5) reduces to iJ C' F(y), This includes the
simplest case where the local times Ci(t) are taken as constant-time scalings cit of
the global time t; specially, when all Ci(t) = t the system goes back to the original
one (2), If, on the other hand, all the local time~ are identi~al to the global time t
and the communication times take the form of rJ(t) t - OJ(t) one obtains a most
general delayed system

=

=

=

(7)
where the state Yj(t) of component j may have different delay times O)(t) for different other components i.
Finally, we should point out that the above definitions of asynchronous schemes and
dynamics are analogues of their counterparts for DT dynamical systems (Bertsekas
and Tsitsiklis, 1989; Blum, 1990), Usually, an asynchronous scheme for a DT
system defined by a map f : X -+ X, where X Xl X X 2 X '" X X n , consists of a
1, ,.. , n} of subset~ of discrete times (N) at which components
family {Ti ~ N I i
i update their states and a family {rJ : N -+ N Ii
1,2"", n} of communication
times, Asynchronous dynamics (or chaotic iteration, relaxation) is then given by

=

X.(t
I

+ 1) = {

=

=

fi(xl(rt(t)), "', xn(r~(t)))
Xi(t)

if t E ~
otherwise.

Notice that the sets Ti can be interpreted as local times of components i . In fact,
one can define local time functions Ci : N -+ N as Ci(O) = 0 and Ci(t + 1) = Ci(t) + 1
if t E 11 and Ci(t) otherwise. The asynchronous dynamics can then be defined by

Xi(t

+ 1) - Xi(t) = (Ci(t + 1) - ci(t))(fi(xl(rf(t)), ... ,Xn(r~(t))) - Xi(t)),

which is analogous to the definition given in (4).

Asynchronous Dynamics of Continuous Time Neural Networks

3

ASYNCHRONIZABLE SYSTEMS

In general, we consider a CT dynamical system as asynchronizable ifits synchronous
dynamics (limit sets and their asymptotic stability) persists for some set of asynchronous schemes. In many cases, asynchronous dynamics of an arbitrary CT system will be different from its synchronous dynamics, especially when delay times
in communication are present. An example can be given for the network (1) with
symmetric matrix W. It is well-known that (synchronous) dynamics of such networks is quasi-convergent, namely, all trajectories approach a set of fixed points
(Hirsch, 1989). But when delay times are taken into consideration, the networks
may have sustained oscillation when the delays exceed some threshold (Marcus and
Westervelt, 1989). A more careful analysis on oscillation induced by delays is given
in (Wu, 1993) for the networks with symmetric circulant weight matrices.
Here, we focus on asynchronizable systems. We consider CT dynamical systems on

Rn of the following general form
Ax(t) = -x(t) + F(x(t?

(8)

where x(t) ERn, A = diag(a1,a2, ... ,an ) with aj > 0 and F = [Ji] E G 1(Rn). It
is easy to see that a point x E Rn is a fixed point of (8) if and only if x is a fixed
point of the map F. Without loss of generality, we assume that 0 is a fixed point
of the map F. According to (5), the asynchronized version of (8) for an arbitrary
asynchronous scheme ({ cd, {rj}) is

Ay
where jj

3.1

= G'( -y + F[Y]),

(9)

= (jjtct), jj~(t), ... , y~(t)].

Contractive Systems

Our first effort attempts to obtain a result similar to the one for P-contractive
maps in (Baudet, 1978). We call the system (8) strongly P-contractive if there is a
symmetric and invertible matrix S such that IS- 1 F(Sx)1 < Ixl for all x E Rn and
IS- 1 F(Sx)1 Ixl only for x 0; here Ixl denotes the vector with components Ixil
and < is component-wise.

=

=

Theorem 1 If the system (8) is strongly P-contractive, then it is asynchronizable
Ci(t) for all
for any asynchronous schemes without self time delays (i. e., rf (t)
i=1,2, ... ,n).

=

Proof. It is not hard to see that synchronous dynamics of a strongly P-contractive
system is globally convergent to the fixed point O. Now, consider the transformation
z = A- 1 y and the system for z

Ai

=G'( -z + S-1 F[SZ]) =G'( -z + G[Z]),

=

where G[Z]
S-1 FS[Z]. This system has the same type of dynamics as (9).
Define a function E : R+ x Rn --+ R+ by E(t) = z T (t)Az(t)j2, whose derivative
with respect to t is

E = z TG' (-z + G(Z? < IIG'II (-z Tz + IzlT IG(Z)!) < IIG'II( -z Tz + IzlT Izl) ::; O.

497

498

Wang, Li, and Blum

Hence E is an energy function and the asynchronous dynamics converges to the
fixed point O.
0
Our second result is for asynchronous dynamics of contractive systems with no
communication delay. The system (8) is called contractive if there is a real constant
o ~ a < 1 such that

IIF(x) - F(y)1I ~
for all x, y E Rn; here

II . II

allz - yll

denotes the usual Euclidean norm on Rn.

Theorem 2 If the system (8) is contractive, then it is asynchronizable for asynchronous schemes with no communication delay.

Proof. The synchronous dynamics of contractive systems is known to be globally
convergent to a unique fixed point (Kelly, 1990). For an asynchronous scheme with
no communication delay, the system (8) is simplified to Ali G' ( -y + F(y?. We
consider again the function E
y T Ay/2, which is an energy function as shown
below.

=

=

E = YT G' (-y + F(y?

~

IIG/II( -lIyll2 + lIyIlIlF(y)ID <

O.

o

Therefore, the asynchronous dynamics converges to the fixed point O.
For the additive-type neural networks (1), we have
Corollary 1 Let the network (1) have neuron activation functions
type with 0 < uHz) ~ SUPzER ui(z) = 1. If it satisfies the condition

Ui

of sigmoidal

(10)

=

where M
diag(J-ll, ... , J-ln), then it is asynchronizable for any asynchronous
schemes with no communication delay.

Proof. The condition (10) ensures the map F(x)
contractive.

= A-I Wu(M x) + A- 1 I

to be
0

Notice that the condition (10) is equivalent to many existing ones on globally asymptotical stability based on various norms of matrix W, especially the contraction condition given in (Kelly, 1990) and some very recent ones in (Matsuoka, 1992). The
condition (10) is also related very closely to the condition in (Barhen and Gulati,
1989) for asynchronous dynamics of a discretized version of (1) and the condition
in (Marcus and Westervelt, 1989) for the networks with delay.
We should emphasize that the results in Theorem 2 and Corollary 1 do not directly
follow from the result in (Kelly, 1990); this is because local times Ci(t) are allowed
to be much more general functions than linear ones Ci t.
3.2

Monotone Systems

A binary relation ~ on Rn is called a partial order if it satisfies that, for all x, y, z E
x ~ x; (ii) x ~ y and y ~ x imply x = y; and (iii) x -< y and y -< z
imply x -< z. For a partial order ~ on Rn , define ~ on Rn by x ~ y iff x < y
and Xi # Yi for all i
1, .. " n. A map F : Rn -I- Rn is monotone if x ~ y implies

Rn, (i)

=

Asynchronous Dynamics of Continuous Time Neural Networks

F(x) -< F(y). A CT dynamical system of the form (2) is monotone if Xl ~ X2 implies
the trajectories Xl(t), X2(t) with Xl(O) = Xl and X2(0) = X2 satisfy Xl(t) ::5 X2(t)
for all t ~ 0 (Hirsch, 1988).

Theorem 3 If the map F in (8) is monotone, then the system (8) is asynchronizable for uniformly-delayed asynchronous schemes, provided that all orbits x(t) have
compact orbit closure and there is a to > 0 with x(to) ~ x(O) or x(to) ~ x(O).

Proof. This is an application of a Henry's theorem (see Hirsch, 1988) that implies that the asynchronized system (9) in the no communication delay situation
is monotone and Hirsch's theorem (Hirsch, 1988) that guarantees the asymptotic
convergence of monotone systems to fixed points.
0

Corollary 2 If the additive-type neural network (1) with sigmoidal activation functions is cooperative (i.e., Wij > 0 for i # j (Hirsch, 1988 and 1989)), then it is
asynchronizable for uniformly-delayed asynchronous schemes, provided that there is
a to > 0 with x(to) ~ x(O) or x(to) ~ x(O).

Proof. According to (Hirsch, 1988), cooperative systems are monotone. As the
network has only bounded dynamics, the result follows from the above theorem. 0

4

CONCLUSION

By incorporating the concepts of local times and communication times, we have
provided a mathematical formulation of asynchronous dynamics of continuous-time
dynamical systems. Asynchronized systems in the most general form haven't been
studied in theories of dynamical systems and functional differential equations. For
contractive and monotone systems, we have shown that for some asynchronous
schemes, the systems are asynchronizable, namely, their asynchronizations preserve
convergent dynamics of the original (synchronous) systems. When applying these
results to the additive-type neural networks, we have obtained some special conditions for the networks to be asynchronizable.
We are currently investigating more general results for asynchronizable dynamical
systems, with a main interest in oscillatory dynamics.

References
G. M. Baudet (1978). Asynchronous iterative methods for multiprocessors. Journal
of the Association for Computing Machinery, 25:226-244.

J. Barhen and S. Gulati (1989). "Chaotic relaxation" in concurrently asynchronous
neurodynamics. In Proceedings of International Conference on Neural Networks,
volume I, pages 619-626, San Diego, California.
Bertsekas and Tsitsiklis (1989). Parallel and Distributed Computation: Numerical
Methods. Englewood Cliffs, NJ: Prentice Hall.
E. K. Blum (1990). Mathematical aspects of outer-product asynchronous contentaddressable memories. Biological Cybernetics, 62:337-348, 1990.

499

500

Wang, Li, and Blum

E. K. Blum and X. Wang (1992). Stability of fixed-points and periodic orbits, and
bifurcations in analog neural networks. Neural Networks, 5:577-587.

J. Hale (1977). Theory of Functional Differential Equations. New York: SpringerVerlag.
M. W. Hirsch (1988). Stability and convergence in strongly monotone dynamical
systems. J. reine angew. Math., 383:1-53.
M. W. Hirsch (1989). Convergent activation dynamics in continuous time networks.
Neural Networks, 2:331-349.

J. Hopfield (1982). Neural networks and physical systems with emergent computational abilities. Proc. Nat. Acad. Sci. USA, 79:2554-2558.
J. Hopfield (1984) . Neurons with graded response have collective computational
properties like those of two-state neurons. Proc. Nat. Acad. Sci. USA, 81:30883092.
D. G. Kelly (1990). Stability in contractive nonlinear neural networks. IEEE Trans.
Biomedi. Eng., 37:231-242.
Q. Li (1993). Mathematical and Numerical Analysis of Biological Neural Networks.
Unpublished Ph.D. Thesis, Mathematics Department, University of Southern California.
C. M. Marcus and R. M. Westervelt (1989). Stability of analog neural networks
with delay. Physical Review A, 39(1):347-359.
K. Matsuoka (1992) . Stability conditions for nonlinear continuous neural networks
with asymmetric connection weights. Neural Networks, 5:495-500 .

J. M. Ortega and W. C. Rheinboldt (1970). Iterative solution of nonlinear equations
in several variables. New York: Academic Press.
X. Wang, E. K. Blum, and Q. Li (1993). Consistency on Local Dynamics and
Bifurcation of Continuous-Time Dynamical Systems and Their Discretizations. To
appear in the AMS proceedings of Symposia in Applied Mathematics, Mathematics
of Computation 1943 - 1993, Vancouver, BC, August, 1993, edited by W. Gautschi.
X. Wang and E. K. Blum (1992). Discrete-time versus continuous-time neural
networks. Computer and System Sciences, 49:1-17 .
X. Wang and D. S. Parker (1992). Computing least fixed points by asynchronous
iterations and random iterations. Technical Report CSD-920025, Computer Science
Department, UCLA.
J .-H. Wu (1993). Delay-Induced Discrete Waves of Large Amplitudes in Neural Networks with Circulant Connection Matrices. Preprint, Department of Mathematics
and Statistics, York University.


----------------------------------------------------------------

title: 313-applications-of-neural-networks-in-video-signal-processing.pdf

Applications of Neural Networks in
Video Signal Processing

John C. Pearson, Clay D. Spence and Ronald Sverdlove
David Sarnoff Research Center
CN5300
Princeton, NJ 08543-5300

Abstract
Although color TV is an established technology, there are a number of
longstanding problems for which neural networks may be suited. Impulse
noise is such a problem, and a modular neural network approach is presented in this paper. The training and analysis was done on conventional
computers, while real-time simulations were performed on a massively parallel computer called the Princeton Engine. The network approach was
compared to a conventional alternative, a median filter. Real-time simulations and quantitative analysis demonstrated the technical superiority of
the neural system. Ongoing work is investigating the complexity and cost
of implementing this system in hardware.

1

THE POTENTIAL FOR NEURAL NETWORKS IN
CONSUMER ELECTRONICS

Neural networks are most often considered for application in emerging new technologies, such as speech recognition, machine vision, and robotics. The fundamental
ideas behind these technologies are still being developed, and it will be some time
before products containing neural networks are manufactured. As a result, research
in these areas will not drive the development of inexpensive neural network hardware which could serve as a catalyst for the field of neural networks in general.
In contrast, neural networks are rarely considered for application in mature technologies, such as consumer electronics. These technologies are based on established
principles of information processing and communication, and they are used in millions of products per year. The embedding of neural networks within such mass-

289

290

Pearson, Spence, and Sverdlove
market products would certainly fuel the development oflow-cost network hardware,
as economics dictates rigorous cost-reduction in every component.

2

IMPULSE NOISE IN TV

The color television signaling standard used in the U.S. was adopted in 1953 (McIlwain and Dean, 1956; Pearson, 1975). The video information is first broadcast as an
amplitude modulated (AM) radio-frequency (RF) signal, and is then demodulated
in the receiver into what is called the composite video signal. The composite signal
is comprised of the high-bandwidth (4.2 MHz) luminance (black and white) signal
and two low-bandwidth color signals whose amplitudes are modulated in quadrature
on a 3.58 MHz subcarrier. This signal is then further decoded into the red, green
and blue signals that drive the display. One image "frame" is formed by interlacing
two successive "fields" of 262.5 horizontal lines.
Electric sparks create broad-band RF emissions which are transformed into oscillatory waveforms in the composite video signal, called AM impulses. See Figure 1.
These impulses appear on a television screen as short, horizontal, multi-colored
streaks which clearly stand out from the picture. Such sparks are commonly created by electric motors. There is little spatial (within a frame) or temporal (between
frames) correlation between impulses.
General considerations suggest a two step approach for the removal of impulses from
the video signal - detect which samples have been corrupted, and replace them with
values derived from their spatio-temporal neighbors. Although impulses are quite
visible, they form a small fraction of the data, so only those samples detected as
corrupted should be altered. An interpolated average of some sort will generally be a
good estimate of impulse-corrupted samples because images are generally smoothly
varying in space and time.
There are a number of difficulties associated with this detection/replacement approach to the problem. There are many impulse-like waveforms present in normal
video, which can cause "false positives" or "false alarms". See Figure 2. The algorithms that decode the composite signal into RGB spread impulses onto neighboring
lines, so it is desirable to remove the impulses in the composite signal. However,
the color encoding within the composite signal complicates matters. The sub carrier
frequency is near the ringing frequency of the impulses and tends to hide the impulses. Furthermore, the replacement function cannot simply average the nearest

Figure 1: Seven Representative AM Impulse Waveforms. They have been digitized
and displayed at the intervals used in digital receivers (8 bits, .07 usec). The largest
amplitude impulses are 20-30 samples wide, approximately 3% of the width of one
line of active video (752 samples).

Applications of Neural Networks in Video Signal Processing

255~~io----_
O~~
~
+l2:~I11-____H,,--_____J,..J\II--!--1-'"-

-128~L____
.. ________________________________________________~~

o

752

Figure 2: Corrupted Video Scan Line. (Top) Scan line of a composite video signal
containing six impulse waveforms. (Bottom) The impulse waveforms, derived by
subtracting the uncorrupted signal from the corrupted signal. Note the presence of
many impulse-like features in the video signal.
samples, because they represent different color components. The impulses also have
a wide variety of waveforms (Figure I), including some variation caused by clipping
in the receiver.

3

MODULAR NEURAL NETWORK SYSTEM

The impulse removal system incorporates three small multi-layer perceptron networks (Rumelhart and McClelland, 1986), and all of the processing is confined to
one field of data. See Figure 3. The replacement function is performed by one
network, termed the i-net ("i" denotes interpolation). Its input is 5 consecutive
samples each from the two lines above and the two lines below the current line.
The network consists of 10 units in the first hidden layer, 5 in the second, and-one
output node trained to estimate the center sample of the current line.
The detection function employs 2 networks in series. (A single network detector
has been tried, but it has never performed as well as this two-stage detector.) The
inputs to the first network are 9 consecutive samples from the current line centered
on the sample of interest. It has 3 nodes in the first layer, and one output node
trained to compute a moving average of the absolute difference between the clean
and noisy signals of the current inputs. It is thus trained to function as a filter for
impulse energy, and is termed the e-net. The output of the e-net is then low-pass
filtered and sub-sampled to remove redundant information.
The inputs to the second network are 3 lines of 5 consecutive samples each, drawn
from the post-processed output of the e-net, centered on the sample of interest.
This network, like the e-net, has 3 nodes in the first layer and one output node. It
is trained to output 1 if the sample of interest is contaminated with impulse noise,
and 0 otherwise. It is thus an impulse detector, and is called the d-net.
The output of the d-net is then fed to a binary switch, which passes through to the
final system output either the output of the i-net or the original signal, depending
on whether the input exceeds an adjustable threshold.

291

292

Pearson, Spence, and Sverdlove

Original Dirty Picture

-

small
Impu_lse_+-~
'pseudo-

Impulse"

--

I

"edge"--f-I---

big Impulse -+--l..,,+

false negative

potential false negalive

potenllaltrue -+-positive

Interpolated Original

-

true positive

-+-_

false positives

-

"Restored" Picture

""""'r+smallimpulse let through
blurred eyes

~-+-big

Impulse removed

Figure 3: The Neural Network AM Impulse Removal System. The cartoon face is
used to illustrate salient image processing characteristics of the system. The e-net
correctly signals the presence of the large impulse (chin), misses the small impulse
(forehead), and incorrectly identifies edges (nose) and points (eyes) as impulses.
The d-net correctly disregards the vertically correlated impulse features (nose) and
detects the large impulse (chin), but incorrectly misses the small impulse (forehead)
and the non-correlated impulse-like features (eyes). The i-net produces a fuzzy
(doubled) version of the original, which is used to replace segments identified as
corrupted by the d-net.
Experience showed that the d-net tended to produce narrow spikes in response to
impulse-like features of the image. To remove this source of false positives, the
output of the d-net is averaged over a 19 sample region centered on the sample of
interest. This reduces the peak amplitude of signals due to impulse-like features
much more than the broad signals produced by true impulses. An impulse is considered to be present if this smoothed signal exceeds a threshold, the level of which
is chosen so as to strike a balance between low false positive rates (high threshold),
and high true positive rates (low threshold).
Experience also showed that the fringes of the impulses were not being detected.
To compensate for this, sub-threshold d-net output samples are set high if they are
within 9 samples of a super-threshold d-net sample. Figure 4 shows the output of
the resulting trained system for one scan line.
The detection networks were trained on one frame of video containing impulses of
5 different amplitudes with the largest twenty times the smallest. Visually, these
ranged from non-objectionable to brightly colored. Standard incremental backpropagation and conjugate gradient (NAG, 1990) were the training proceedures
used. The complexity of the e-net and d-net were reduced in phases. These nets

Applications of Neural Networks in Video Signal Processing

(J'b
255~,,~

o

INPUT

+2~~~___~~~.____
-25~

r--r~--------~.----------~v~----~~
NOISE

~~_ _ _~-~N_E_T______~

SMOOTHED D-NET
THRESHOL~

1\
/\

/\

/\

255

o

o

752

Figure 4: Input and Network Signals.
began as 3 layer nets. After a phase of training, redundant nodes were identified
and removed, and training re-started. This process was repeated until there were
no redundant nodes.

4

REAL-TIME SIMULATION ON THE PRINCETON
ENGINE

The trained system was simulated in real-time on the Princeton Engine (Chin et.
al., 1988), and a video demonstration was presented at the conference. The Princeton Engine (PE) is a 29.3 GIPS image processing system consisting of up to 2048
processing elements in a SIMD configuration. Each processor is responsible for the
output of one column of pixels, and contains a 16-bit arithmetic unit, multiplier, a
64-word triple-port register stack, and 16,000 words of local processor memory. In
addition, an interprocessor communication bus permits exchanges of data between
neighboring processors during one instruction cycle.
While the i-net performs better than conventional interpolation methods, the difference is not significant for this problem because of the small amount of signal
which is replaced. (If the whole image is replaced, the neural net interpolator gave
about 1.5 dB better performance than a conventional method.) Thus it has not
been implemented on the PE. The i-net may be of value in other video tasks, such
as converting from an interlaced to a non-interlaced display.
16-bit fixed point arithmetic was used in these simulations, with 8 bits of fraction,
and 10 bit sigmoid function look-up tables. Comparison with the double-precision
arithmetic used on the conventional computers showed no significant reduction in

293

294

Pearson, Spence, and Sverdlove

10

:zo

----

~

---

100

---

200

.2

O~~====~
o
.02

.04

.08

.08

.1

% FALSE DETtcnoN

O~~====~
o
.02

.04

.08

.08

.1

% FALSE DETtcnON

Figure 5: ROC Analysis of Neural Network and Median Detectors.

performance. Current work is exploring the feasibility of implementing training on
the PE.

5

PERFORMANCE ANALYSIS

The mean squared error (MSE) is well known to be a poor measure of subjective
image quality (Roufs and Bouma, 1980). A better measure of detection performance
is given by the receiver operating characteristic, or ROC (Green and Swets, 1966,
1974). The ROC is a parametric plot of the fraction of corrupted samples correctly
detected versus the fraction of clean samples that were falsely detected. In this case,
the decision threshold for the smoothed output of the d-net was the parameter
varied. Figure 5 (left) shows the neural network detector ROC for five different
impulse amplitudes (tested on a video frame that it was not trained on). This
quantifies the sharp breakdown in performance observed in real-time simulations at
low impulse amplitude. This breakdown is not observed in analysis of the MSE.
Median filters are often suggested for impulse removal tasks, and have been applied
to the removal of impulses from FM TV transmission systems (Perlman, et aI,
1987). In order to assess the relative merits of the neural network detector, a
median detector was designed and analyzed. This detector computes the m~dian of
the current sample and its 4 nearest neighbors with the same color sub-carrier phase.
A detection is registered if the difference between the median and the current sample
is above threshold (the same additional measures were taken to insure that impulse
fringes were detected as were described above for the neural network detector).
Figure 5 (right) shows both the neural network and median detector ROC's for
two different video frames, each of which contained a mixture of all 5 impulse
amplitudes. One frame was used in training the network (TRAIN), and the other
was not (TEST). This verifies that the network was not overtrained, and quantifies
the superior performance of the network detector observed in real-time simulations.

Applications of Neural Networks in Video Signal P1'ocessing

6

CONCLUSIONS

We have presented a system using neural network algorithms that outperforms a
conventional method, median filtering, in removing AM impulses from television
signals. Of course an additional essential criterion is the cost and complexity of
hardware implementations. Median filter chips have been successfully fabricated
(Christopher et al., 1988). We are currently investigating the feasibility of casting
small neural networks into special purpose chips. We are also applying neural nets
to other television signal processing problems.
Acknowledgements

This work was supported by Thomson Consumer Electronics, under Erich Geiger
and Dietrich Westerkamp. This work was part of a larger team effort, and we
acknowledge their help, in particular: Nurit Binenbaum, Jim Gibson, Patrick Hsieh,
and John Ju.
References

Chin, D., J. Passe, F. Bernard, H. Taylor and S. Knight, (1988). The Princeton
Engine: A Real-Time Video System Simulator. IEEE Transactions on Consumer
Electronics 34:2 pp. 285-297.
Christopher, L.A., W.T. Mayweather III, and S. Perlman, (1988). A VLSI Median
Filter for Impulse Noise Elimination in Composite or Component TV Signals. IEEE
Transactions on Consumer Electronics 34:1 p. 262.
Green, D.M., and J .A. Swets, (1966 and 1974). Signal Detection Theory and Psychophysics. New York, Wiley (1966). Reprinted with corrections, Huntington,
N.Y., Krieger (1974).
McIlwain, K. and C.E. Dean (eds.); Hazeltine Corporation Staff, (1956). Principles
of Color Television. New York. John Wiley and Sons.
NAG, (1990). The NAG Fortran Library Manual, Mark 14. Downers Grove, IL
(The Numerical Algorithms Group Inc.).
Pearson, D.E., (1975). Transmission and Display of Pictorial Information. New
York. John Wiley and Sons.
Perlman, S.S, S. Eisenhandler, P.W. Lyons, and M.J. Shumila, (1987). Adaptive
Median Filtering for Impulse Noise Elimination in Real-Time TV Signals. IEEE
Transactions on Communications COM-35:6 p. 646.
Roufs, J .A. and H. Bouma, (1980). Towards Linking Perception Research and
Image Quality. Proceedings of the SID 21:3, pp. 247-270.
Rumelhart, D.E. and J.L. McClelland (eds.), (1986). Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Cambridge, Mass., MIT
Press.

295

Part VII
Visual Processing


----------------------------------------------------------------

