query sentence: machine-learning toolkit 
---------------------------------------------------------------------
title: 5812-matrix-manifold-optimization-for-gaussian-mixtures.pdf

Matrix Manifold Optimization for Gaussian Mixtures

Reshad Hosseini
School of ECE
College of Engineering
University of Tehran, Tehran, Iran
reshad.hosseini@ut.ac.ir

Suvrit Sra
Laboratory for Information and Decision Systems
Massachusetts Institute of Technology
Cambridge, MA.
suvrit@mit.edu

Abstract
We take a new look at parameter estimation for Gaussian Mixture Model (GMMs).
Specifically, we advance Riemannian manifold optimization (on the manifold of
positive definite matrices) as a potential replacement for Expectation Maximization (EM), which has been the de facto standard for decades. An out-of-the-box
invocation of Riemannian optimization, however, fails spectacularly: it obtains
the same solution as EM, but vastly slower. Building on intuition from geometric
convexity, we propose a simple reformulation that has remarkable consequences:
it makes Riemannian optimization not only match EM (a nontrivial result on its
own, given the poor record nonlinear programming has had against EM), but also
outperform it in many settings. To bring our ideas to fruition, we develop a welltuned Riemannian LBFGS method that proves superior to known competing methods (e.g., Riemannian conjugate gradient). We hope that our results encourage a
wider consideration of manifold optimization in machine learning and statistics.

1

Introduction

Gaussian Mixture Models (GMMs) are a mainstay in a variety of areas, including machine learning
and signal processing [4, 10, 16, 19, 21]. A quick literature search reveals that for estimating parameters of a GMM the Expectation Maximization (EM) algorithm [9] is still the de facto choice.
Over the decades, other numerical approaches have also been considered [24], but methods such as
conjugate gradients, quasi-Newton, Newton, have been noted to be usually inferior to EM [34].
The key difficulty of applying standard nonlinear programming methods to GMMs is the positive
definiteness (PD) constraint on covariances. Although an open subset of Euclidean space, this constraint can be difficult to impose, especially in higher-dimensions. When approaching the boundary
of the constraint set, convergence speed of iterative methods can also get adversely affected. A partial remedy is to remove the PD constraint by using Cholesky decompositions, e.g., as exploited in
semidefinite programming [7]. It is believed [30] that in general, the nonconvexity of this decomposition adds more stationary points and possibly spurious local minima.1 Another possibility is
to formulate the PD constraint via a set of smooth convex inequalities [30] and apply interior-point
methods. But such sophisticated methods can be extremely slower (on several statistical problems)
than simpler EM-like iterations, especially for higher dimensions [27].
Since the key difficulty arises from the PD constraint, an appealing idea is to note that PD matrices
form a Riemannian manifold [3, Ch.6] and to invoke Riemannian manifold optimization [1, 6].
Indeed, if we operate on the manifold2 , we implicitly satisfy the PD constraint, and may have a
better chance at focusing on likelihood maximization. While attractive, this line of thinking also
fails: an out-of-the-box invocation of manifold optimization is also vastly inferior to EM. Thus, we
need to think harder before challenging the hegemony of EM. We outline a new approach below.
1

Remarkably, using Cholesky with the reformulation in ?2.2 does not add spurious local minima to GMMs.
Equivalently, on the interior of the constraint set, as is done by interior point methods (their nonconvex
versions); though these turn out to be slow too as they are second order methods.
2

1

Key idea. Intuitively, the mismatch is in the geometry. For GMMs, the M-step of EM is a Euclidean
convex optimization problem, whereas the GMM log-likelihood is not manifold convex3 even for a
single Gaussian. If we could reformulate the likelihood so that the single component maximization
task (which is the analog of the M-step of EM for GMMs) becomes manifold convex, it might have a
substantial empirical impact. This intuition supplies the missing link, and finally makes Riemannian
manifold optimization not only match EM but often also greatly outperform it.
To summarize, the key contributions of our paper are the following:
? Introduction of Riemannian manifold optimization for GMM parameter estimation, for which we
show how a reformulation based on geodesic convexity is crucial to empirical success.
? Development of a Riemannian LBFGS solver; here, our main contribution is the implementation
of a powerful line-search procedure, which ensures convergence and makes LBFGS outperform
both EM and manifold conjugate gradients. This solver may be of independent interest.
We provide substantive experimental evidence on both synthetic and real-data. We compare manifold optimization, EM, and unconstrained Euclidean optimization that reformulates the problem
using Cholesky factorization of inverse covariance matrices. Our results shows that manifold optimization performs well across a wide range of parameter values and problem sizes. It is much less
sensitive to overlapping data than EM, and displays much less variability in running times.
Our results are quite encouraging, and we believe that manifold optimization could open new algorithmic avenues for mixture models, and perhaps other statistical estimation problems.
Note. To aid reproducibility of our results, M ATLAB implementations of our methods are available
as a part of the M IXEST toolbox developed by our group [12]. The manifold CG method that we use
is directly based on the excellent toolkit M ANOPT [6].
Related work. Summarizing published work on EM is clearly impossible. So, let us briefly mention a few lines of related work. Xu and Jordan [34] examine several aspects of EM for GMMs and
counter the claims of Redner and Walker [24], who claimed EM to be inferior to generic secondorder nonlinear programming techniques. However, it is now well-known (e.g., [34]) that EM can
attain good likelihood values rapidly, and scales to much larger problems than amenable to secondorder methods. Local convergence analysis of EM is available in [34], with more refined results
in [18], who show that when data have low overlap, EM can converge locally superlinearly. Our
paper develops Riemannian LBFGS, which can also achieve local superlinear convergence.
For GMMs some innovative gradient-based methods have also been suggested [22, 26], where the
PD constraint is handled via a Cholesky decomposition of covariance matrices. However, these
works report results only for low-dimensional problems and (near) spherical covariances.
The idea of using manifold optimization for GMMs is new, though manifold optimization by itself is
a well-developed subject. A classic reference is [29]; a more recent work is [1]; and even a M ATLAB
toolbox exists [6]. In machine learning, manifold optimization has witnessed increasing interest4 ,
e.g., for low-rank optimization [15, 31], or optimization based on geodesic convexity [27, 33].

2

Background and problem setup

The key object in this paper is the Gaussian Mixture Model (GMM), whose probability density is
XK
p(x) :=
?j pN (x; ?j , ?j ),
x ? Rd ,
j=1

and where pN is a (multivariate) Gaussian with mean ? ? Rd and covariance ?  0. That is,

pN (x; ?, ?) := det(?)?1/2 (2?)?d/2 exp ? 12 (x ? ?)T ??1 (x ? ?) .
? j  0}K and weights ?
? j ? Rd , ?
? ?
Given i.i.d. samples {x1 , . . . , xn }, we wish to estimate {?
j=1
?K , the K-dimensional probability simplex. This leads to the GMM optimization problem
n
XK

X
?j pN (xi ; ?j , ?j ) .
(2.1)
max
log
???K ,{?j ,?j 0}K
j=1

3
4

j=1

i=1

That is, convex along geodesic curves on the PD manifold.
Manifold optimization should not be confused with ?manifold learning? a separate problem altogether.

2

Solving Problem (2.1) can in general require exponential time [20].5 However, our focus is more
pragmatic: similar to EM, we also seek to efficiently compute local solutions. Our methods are set
in the framework of manifold optimization [1, 29]; so let us now recall some material on manifolds.
2.1 Manifolds and geodesic convexity
A smooth manifold is a non-Euclidean space that locally resembles Euclidean space [17]. For optimization, it is more convenient to consider Riemannian manifolds (smooth manifolds equipped with
an inner product on the tangent space at each point). These manifolds possess structure that allows
one to extend the usual nonlinear optimization algorithms [1, 29] to them.
Algorithms on manifolds often rely on geodesics, i.e., curves that join points along shortest paths.
Geodesics help generalize Euclidean convexity to geodesic convexity. In particular, say M is a
Riemmanian manifold, and x, y ? M; also let ? be a geodesic joining x to y, such that
?xy : [0, 1] ? M,

?xy (0) = x, ?xy (1) = y.

Then, a set A ? M is geodesically convex if for all x, y ? A there is a geodesic ?xy contained
within A. Further, a function f : A ? R is geodesically convex if for all x, y ? A, the composition
f ? ?xy : [0, 1] ? R is convex in the usual sense.
The manifold of interest to us is Pd , the manifold of d ? d symmetric positive definite matrices. At
any point ? ? Pd , the tangent space is isomorphic to set of symmetric matrices; and the Riemannian
metric at ? is given by tr(??1 d???1 d?). This metric induces the geodesic [3, Ch. 6]
1/2

?1/2

??1 ,?2 (t) := ?1 (?1

?1/2 t

?2 ?1

1/2

) ?1 ,

0 ? t ? 1.

Thus, a function f : P ? R if geodesically convex on a set A if it satisfies
d

f (??1 ,?2 (t)) ? (1 ? t)f (?1 ) + tf (?2 ),

t ? [0, 1], ?1 , ?2 ? A.

Such functions can be nonconvex in the Euclidean sense, but are globally optimizable due to
geodesic convexity. This property has been important in some matrix theoretic applications [3, 28],
and has gained more extensive coverage in several recent works [25, 27, 33].
We emphasize that even though the mixture cost (2.1) is not geodesically convex, for GMM optimization geodesic convexity seems to play a crucial role, and it has a huge impact on convergence
speed. This behavior is partially expected and analogous to EM, where a convex M-Step makes the
overall method much more practical. Let us now use this intuition to elicit geodesic convexity.
2.2 Problem reformulation
We begin with parameter estimation for a single Gaussian: although this has a closed-form solution
(which ultimately benefits EM), it requires more subtle handling when using manifold optimization.
Consider the following maximum likelihood parameter estimation for a single Gaussian:
Xn
max L(?, ?) :=
log pN (xi ; ?, ?).
(2.2)
i=1

?,?0

Although (2.2) is a Euclidean convex problem, it is not geodesically convex on its domain Rd ? Pd ,
which makes it geometrically handicapped when applying manifold optimization. To overcome this
problem, we invoke a simple reparametrization6 that has far-reaching impact. More precisely, we
augment the sample vectors xi to instead consider yiT = [xTi 1]. Therewith, (2.2) turns into
Xn
b
max L(S)
:=
log qN (yi ; S),
(2.3)
where qN (yi ; S) :=

?

S0

2? exp( 12 )pN (yi ; 0, S).

i=1

Proposition 1 states the key property of (2.3).

b
b
Proposition 1. The map ?(S) ? ?L(S),
where L(S)
is as in (2.3), is geodesically convex.
We omit the proof due to space limits; see [13] for details. Alternatively, see [28] for more general
results on geodesic convexity.
Theorem 2.1 shows that the solution to (2.3) yields the solution to the original problem (2.2) too.
5
6

Though under very strong assumptions, it has polynomial smoothed complexity [11].
This reparametrization in itself is probably folklore; its role in GMM optimization is what is crucial here.

3

23.2

33

23
32

22.6

31

Average log-likelihood

Average log-likelihood

22.8

22.4
22.2
22

30
29
28

21.8

LBFGS, Reformulated MVN
CG, Reformulated MVN
LBFGS, Usual MVN
CG, Usual MVN

21.6
21.4
10?1

100

101
Time (seconds)

LBFGS, Reformulated MVN
CG, Reformulated MVN
LBFGS, Original MVN
CG, Original MVN

27
26 0
10

102

101

(a) Single Gaussian

102
103
Time (seconds)

104

105

(b) Mixtures of seven Gaussians

Figure 1: The effect of reformulation in convergence speed of manifold CG and manifold LBFGS methods
(d = 35); note that the x-axis (time) is on a logarithmic scale.

b ? ) = L(?? , ?? ) for
Theorem 2.1. If ?? , ?? maximize (2.2), and if S ? maximizes (2.3), then L(S
 ?

? + ?? ?? T ??
S? =
.
?? T
1

Proof. We express S by new variables U , t and s by writing S =

U + sttT
stT


st
. The objective
s

b
function L(S)
in terms of the new parameters becomes
b , t, s) =
L(U

n
2

?

d
2

log(2?) ?

n
2

log s ?

n
2

log det(U ) ?

Xn

1
(xi
i=1 2

? t)T U ?1 (xi ? t) ?

n
2s .

Optimizing Lb over s > 0 we see that s? = 1 must hold. Hence, the objective reduces to a ddimensional Gaussian log-likelihood, for which clearly U ? = ?? and t? = ?? .
Theorem 2.1 shows that reformulation (2.3) is ?faithful,? as it leaves the optimum unchanged. Theorem 2.2 proves a local version of this result for GMMs.
Theorem 2.2. A local maximum of the reparameterized GMM log-likelihood
XK

Xn
K
b
L({S
log
?j qN (yi ; Sj )
j }j=1 ) :=
i=1

j=1

is a local maximum of the original log-likelihood
 XK

Xn
L({?j , ?j }K
log
?j pN (xi |?j , ?j ) .
j=1 ) :=
i=1

j=1

The proof can be found in [13].
Theorem 2.2 shows that we can replace problem (2.1) by one whose local maxima agree with those
of (2.1), and whose individual components are geodesically convex. Figure 1 shows the true import
of our reformulation: the dramatic impact on the empirical performance of Riemmanian ConjugateGradient (CG) and Riemannian LBFGS for GMMs is unmistakable.
The final technical piece is to replace the simplex constraint ? ? ?K to make the problem unconstrained. We do this via a commonly used change of variables [14]: ?k = log ??Kk for
k = 1, . . . , K ? 1. Assuming ?K = 0 is a constant, the final GMM optimization problem is:
max

K?1
{Sj 0}K
j=1 ,{?j }j=1

K?1
K
b
L({S
j }j=1 , {?j }j=1 ) :=

n
X
i=1

log

K
X
j=1


exp(?j )
qN (yi ; Sj )
PK
k=1 exp(?k )

(2.4)

We view (2.4) as a manifold optimization problem; specifically, it is an optimization problem on the
QK
d
product manifold
? RK?1 . Let us see how to solve it.
j=1 P
4

3

Manifold Optimization

In unconstrained Euclidean optimization, typically one iteratively (i) finds a descent direction; and
(ii) performs a line-search to obtain sufficient decrease and ensure convergence. On a Riemannian
manifold, the descent direction is computed on the tangent space (this space varies (smoothly) as
one moves along the manifold). At a point X, the tangent space TX is the approximating vector
space (see Fig. 2). Given a descent direction ?X ? TX , line-search is performed along a smooth
curve on the manifold (red curve in Fig. 2). The derivative of this curve at X equals the descent
direction ?X . We refer the reader to [1, 29] for an in depth introduction to manifold optimization.
Successful large-scale Euclidean methods such as
conjugate-gradient and LBFGS combine gradients at the
current point with gradients and descent directions from
previous points to obtain a new descent direction. To adapt
such algorithms to manifolds, in addition to defining gradients on manifolds, we also need to define how to transport vectors in a tangent space at one point to vectors in a
different tangent space at another point.

TX
X

?X

Sd+

On Riemannian manifolds, the gradient is simply a direction on the tangent space, where the inner-product of the
gradient with another direction in the tangent space gives
the directional derivative of the function. Formally, if gX Figure 2: Visualization of line-search on a
manifold: X is a point on the manifold, TX
defines the inner product in the tangent space TX , then
is the tangent space at the point X, ?X is a
Df (X)? = gX (gradf (X), ?), for ? ? TX .
descent direction at X; the red curve is the
Given a descent direction in the tangent space, the curve curve along which line-search is performed.
along which we perform line-search can be a geodesic. A
map that takes the direction and a step length to obtain a corresponding point on the geodesic is
called an exponential map. Riemannian manifolds are also equipped with a natural way of transporting vectors on geodesics, which is called parallel transport. Intuitively, a parallel transport is
a differential map with zero derivative along the geodesics. Using the above ideas, Algorithm 1
sketches a generic manifold optimization algorithm.
Algorithm 1: Sketch of an optimization algorithm (CG, LBFGS) to minimize f (X) on a manifold
Given: Riemannian manifold M with Riemannian metric g; parallel transport T on M; exponential map
R; initial value X0 ; a smooth function f
for k = 0, 1, . . . do
Obtain a descent direction based on stored information and gradf (Xk ) using metric g and transport T
Use line-search to find ? such that it satisfies appropriate (descent) conditions
Calculate the retraction / update Xk+1 = RXk (??k )
Based on the memory and need of algorithm store Xk , gradf (Xk ) and ??k
end for
return estimated minimum Xk

Note that Cartesian products of Riemannian manifolds are again Riemannian, with the exponential
map, gradient and parallel transport defined as the Cartesian product of individual expressions; the
inner product is defined as the sum of inner product of the components in their respective manifolds.
Different variants of Riemannian LBFGS can be obtained depending where to perform the vector
Definition
Tangent space
Metric between two tangent vectors ?, ? at ?
Gradient at ? if Euclidean gradient is ?f (?)
Exponential map at point ? in direction ?
Parallel transport of tangent vector ? from ?1 to ?2

Expression for PSD matrices
Space of symmetric matrices
g? (?, ?) = tr(??1 ???1 ?)
gradf (?) = 12 ?(?f (X) + ?f (X)T )?
R? (?) = ? exp(??1 ?)
1/2
T?1 ,?2 (?) = E?E T , E = (?2 ??1
1 )

Table 1: Summary of key Riemannian objects for the PD matrix manifold.
transport. We found that the version developed in [28] gives the best performance, once we combine
it with a line-search algorithm satisfying Wolfe conditions. We present the crucial details below.
5

3.1

Line-search algorithm satisfying Wolfe conditions

To ensure Riemannian LBFGS always produces a descent direction, it is necessary to ensure that the
line-search algorithm satisfies Wolfe conditions [25]. These conditions are given by:
f (RXk (??k )) ? f (Xk ) + c1 ?Df (Xk )?k ,
Df (Xk+1 )?k+1 ? c2 Df (Xk )?k ,

(3.1)
(3.2)

where 0 < c1 < c2 < 1. Note that ?Df (Xk )?k = gXk (gradf (Xk ), ??k ), i.e., the derivative of
f (Xk ) in the direction ??k is the inner product of descent direction and gradient of the function.
Practical line-search algorithms implement a stronger (Wolfe) version of (3.2) that enforces
|Df (Xk+1 )?k+1 | ? c2 Df (Xk )?k .
Similar to the Euclidean case, our line-search algorithm is also divided into two phases: bracketing
and zooming [23]. During bracketing, we compute an interval such that a point satisfying Wolfe conditions can be found in this interval. In the zooming phase, we obtain such a point in the determined
interval. The one-dimensional function and its gradient used by the line-search are
?0 (?) = ?Df (Xk )?k .

?(?) = f (RXk (??k )),

The algorithm is essentially the same as the line-search in the Euclidean space; the reader can also
see its manifold incarnation in [13]. Theory behind how this algorithm is guaranteed to find a steplength satisfying (strong) Wolfe conditions can be found in [23].
A good choice of initial step-length ?1 can greatly speed up the line-search. We propose the following choice that turns out to be quite effective in our experiments:
?1 = 2

f (Xk ) ? f (Xk?1 )
.
Df (Xk )?k

(3.3)

Equation (3.3) is obtained by finding ?? that minimizes a quadratic approximation of the function
along the geodesic through the previous point (based on f (Xk?1 ), f (Xk ) and Df (Xk?1 )?k?1 ):
?? = 2

f (Xk ) ? f (Xk?1 )
.
Df (Xk?1 )?k?1

(3.4)

Then assuming that first-order change will be the same as in the previous step, we write
?? Df (Xk?1 )?k?1 ? ?1 Df (Xk )?k .

(3.5)

Combining (3.4) and (3.5), we obtain our estimate ?1 expressed in (3.3). Nocedal and Wright [23]
suggest using either ?? of (3.4) for the initial step-length ?1 , or using (3.5) where ?? is set to be
the step-length obtained in the line-search in the previous point. We observed that if one instead
uses (3.3) instead, one obtains substantially better performance than the other two approaches.

4

Experimental Results

We have performed numerous experiments to examine effectiveness of our method. Below we report performance comparisons on both real and simulated data. In all experiments, we initialize the
mixture parameters for all methods using k-means++ [2]. All methods also use the same termination criteria: they stop either when the difference of average log-likelihood (i.e., n1 log-likelihood)
between consecutive iterations falls below 10?6 , or when the number of iterations exceeds 1500.
More extensive empirical results can be found in the longer version of this paper [13].
Simulated Data
EM?s performance is well-known to depend on the degree of separation of the mixture components [18, 34]. To assess the impact of this separation on our methods, we generate data as proposed
in [8, 32]. The distributions are chosen so their means satisfy the following inequality:
?i6=j : kmi ? mj k ? c max{tr(?i ), tr(?j )},
i,j

where c models the degree of separation. Since mixtures with high eccentricity (i.e., the ratio of
the largest eigenvalue of the covariance matrix to its smallest eigenvalue) have smaller overlap, in
6

EM Original
Time (s)
ALL
1.1 ? 0.4
-10.7
30.0 ? 45.5
-12.7

LBFGS Reformulated
Time (s)
ALL
5.6 ? 2.7
-10.7
49.2 ? 35.0
-12.7

CG Reformulated
Time (s)
ALL
3.7 ? 1.5
-10.8
47.8 ? 40.4
-12.7

CG Original
Time (s)
ALL
23.8 ? 23.7
-10.7
206.0 ? 94.2
-12.8

c = 0.2

K=2
K=5

c=1

K=2
K=5

0.5 ? 0.2
104.1 ? 113.8

-10.4
-13.4

3.1 ? 0.8
79.9 ? 62.8

-10.4
-13.3

2.6 ? 0.6
45.8 ? 30.4

-10.4
-13.3

25.6 ? 13.6
144.3 ? 48.1

-10.4
-13.3

c=5

K=2
K=5

0.2 ? 0.2
38.8 ? 65.8

-11.0
-12.8

3.4 ? 1.4
41.0 ? 45.7

-11.0
-12.8

2.8 ? 1.2
29.2 ? 36.3

-11.0
-12.8

43.2 ? 38.8
197.6 ? 118.2

-11.0
-12.8

Table 2: Speed and average log-likelihood (ALL) comparisons for d = 20, e = 10 (each row reports values
averaged over 20 runs over different datasets, so the ALL values are not comparable to each other).
EM Original
Time (s)
ALL
65.7 ? 33.1
17.6
365.6 ? 138.8
17.5

LBFGS Reformulated
Time (s)
ALL
39.4 ? 19.3
17.6
160.9 ? 65.9
17.5

CG Reformulated
Time (s)
ALL
46.4 ? 29.9
17.6
207.6 ? 46.9
17.5

CG Original
Time (s)
ALL
64.0 ? 50.4
17.6
279.8 ? 169.3
17.5

c = 0.2

K=2
K=5

c=1

K=2
K=5

6.0 ? 7.1
40.5 ? 61.1

17.0
16.2

12.9 ? 13.0
51.6 ? 39.5

17.0
16.2

15.7 ? 17.5
63.7 ? 45.8

17.0
16.2

42.5 ? 21.9
203.1 ? 96.3

17.0
16.2

c=5

K=2
K=5

0.2 ? 0.1
17.5 ? 45.6

17.1
16.1

3.0 ? 0.5
20.6 ? 22.5

17.1
16.1

2.8 ? 0.7
20.3 ? 24.1

17.1
16.1

19.6 ? 8.2
93.9 ? 42.4

17.1
16.1

Table 3: Speed and ALL comparisons for d = 20, e = 1.
CG Cholesky Original
e=1
e = 10
Time (s)
ALL
Time (s)
101.5 ? 34.1
17.6
113.9 ? 48.1
627.1 ? 247.3
17.5
521.9 ? 186.9

ALL
-10.7
-12.7

CG Cholesky Reformulated
e=1
e = 10
Time (s)
ALL
Time (s)
36.7 ? 9.8
17.6
23.5 ? 11.9
156.7 ? 81.1
17.5
106.7 ? 39.7

ALL
-10.7
-12.6

c = 0.2

K=2
K=5

c=1

K=2
K=5

135.2 ? 65.4
1016.9 ? 299.8

16.9
16.2

110.9 ? 51.8
358.0 ? 155.5

-10.4
-13.3

38.0 ? 14.5
266.7 ? 140.5

16.9
16.2

49.0 ? 17.8
279.8 ? 111.0

-10.4
-13.4

c=5

K=2
K=5

55.2 ? 27.9
371.7 ? 281.4

17.1
16.1

86.7 ? 47.2
337.7 ? 178.4

-11.0
-12.8

60.2 ? 20.8
270.2 ? 106.5

17.1
16.1

177.6 ? 147.6
562.1 ? 242.7

-11.0
-12.9

Table 4: Speed and ALL for applying CG on Cholesky-factorized problems with d = 20.
addition to high eccentricity e = 10, we also test the (spherical) case where e = 1. We test three
levels of separation c = 0.2 (low), c = 1 (medium), and c = 5 (high). We test two different numbers
of mixture components K = 2 and K = 5; we consider experiments with larger values of K in our
experiments on real data. For e = 10, the results for data with dimensionality d = 20 are given in
Table 2. The results are obtained after running with 20 different random choices of parameters for
each configuration. It is apparent that the performance of EM and Riemannian optimization with our
reformulation is very similar. The variance of computation time shown by Riemmanian optimization
is, however, notably smaller. Manifold optimization on the non-reformulated problem (last column)
performs the worst.
In another set of simulated data experiments, we apply different algorithms to spherical data (e = 1);
the results are shown in Table 3. The interesting instance here is the case of low separation c = 0.2,
where the condition number of the Hessian becomes large. As predicted by theory, the EM converges
very slowly in such a case; Table 3 confirms this claim. It is known that in this case, the performance
of powerful optimization approaches like CG and LBFGS also degrades [23]. But both CG and
LBFGS suffer less than EM, while LBFGS performs noticeably better than CG.
Cholesky decomposition is a commonly suggested idea for dealing with PD constraint. So, we also
compare against unconstrained optimization (using Euclidean CG), where the inverse covariance
matrices are Cholesky factorized. The results for the same data as in Tables 2 and 3 are reported in
Table 4. Although the Cholesky-factorized problem proves to be much inferior to both EM and the
manifold methods, our reformulation seems to also help it in several problem instances.
Real Data
We now present performance evaluation on a natural image dataset, where mixtures of Gaussians
were reported to be a good fit to the data [35]. We extracted 200,000 image patches of size 6?6 from
images and subtracted the DC component, leaving us with 35-dimensional vectors. Performance of
different algorithms are reported in Table 5. Similar to the simulated results, performance of EM and
7

K
K
K
K
K
K
K
K
K

=
=
=
=
=
=
=
=
=

EM Algorithm
Time (s)
ALL
16.61
29.28
90.54
30.95
165.77
31.65
202.36
32.07
228.80
32.36
365.28
32.63
596.01
32.81
900.88
32.94
2159.47
33.05

2
3
4
5
6
7
8
9
10

LBFGS Reformulated
Time (s)
ALL
14.23
29.28
38.29
30.95
106.53
31.65
117.14
32.07
245.74
32.35
192.44
32.63
332.85
32.81
657.24
32.94
658.34
33.06

CG Reformulated
Time (s)
ALL
17.52
29.28
54.37
30.95
153.94
31.65
140.21
32.07
281.32
32.35
318.95
32.63
536.94
32.81
1449.52
32.95
1048.00
33.06

CG Original
Time (s)
ALL
947.35
29.28
3051.89
30.95
6380.01
31.64
5262.27
32.07
10566.76
32.33
10844.52
32.63
14282.80
32.58
15774.88
32.77
17711.87
33.03

CG Cholesky Reformulated
Time (s)
ALL
476.77
29.28
1046.61
30.95
2673.21
31.65
3865.30
32.07
4771.36
32.35
6819.42
32.63
9306.33
32.81
9383.98
32.94
7463.72
33.05

Table 5: Speed and ALL comparisons for natural image data d = 35.
102

102
EM, Usual MVN
LBFGS, Reparameterized MVN
CG, Reparameterized MVN

10

10

100
ALL? - ALL

ALL? - ALL

100

102
EM, Original MVN
LBFGS, Reformulated MVN
CG, Reformulated MVN

1

10?1
10?2
?3

100

10?1
10?2

10?1
10?2

10

10

?3

10?3

10?4

10?4

10?4

10?5

10?5

0

50

100

150

200

250

300

0

# function and gradient evaluations

50

100

150

# function and gradient evaluations

EM, Original MVN
LBFGS, Reformulated MVN
CG, Reformulated MVN

101

ALL? - ALL

1

200

10?5

0

50

100

150

200

250

# function and gradient evaluations

Figure 3: Best ALL minus current ALL values with number of function and gradient evaluations. Left: ?magic
telescope? (K = 5, d = 10). Middle: ?year predict? (K = 6, d = 90). Right: natural images (K = 8, d = 35).

manifold CG on the reformulated parameter space is similar. Manifold LBFGS converges notably
faster (except for K = 6) than both EM and CG. Without our reformulation, performance of the
manifold methods degrades substantially. Note that for K = 8 and K = 9, CG without reformulation stops prematurely because it hits the bound of a maximum 1500 iterations, and therefore its
ALL is smaller than the other two methods. The table also shows results of the Cholesky-factorized
(and reformulated) problem. It is more than 10 times slower than manifold optimization. Optimizing the Cholesky-factorized (non-reformulated) problem is the slowest (not shown) and it always
reaches the maximum number of iterations before finding the local minimum.
Fig. 3 depicts the typical behavior of our manifold optimization methods versus EM. The X-axis
is the number of log-likelihood and gradient evaluations (or the number of E- and M-steps in EM).
Fig. 3(a) and Fig. 3(b) are the results of fitting GMMs to the ?magic telescope? and ?year prediction?
datasets7 . Fig. 3(c) is the result for the natural image data of Table 5. Apparently in the initial few
iterations EM is faster, but manifold optimization methods match EM in a few iterations. This is
remarkable, given that manifold optimization methods need to perform line-search.

5

Conclusions and future work

We introduced Riemannian manifold optimization as an alternative to EM for fitting Gaussian mixture models. We demonstrated that for making manifold optimization succeed, to either match or
outperform EM, it is necessary to represent the parameters in a different space and reformulate
the cost function accordingly. Extensive experimentation with both experimental and real datasets
yielded quite encouraging results, suggesting that manifold optimization could have the potential to
open new algorithmic avenues for mixture modeling.
Several strands of practical importance are immediate (and are a part of our ongoing work): (i)
extension to large-scale GMMs through stochastic optimization [5]; (ii) use of richer classes of
priors with GMMs than the usual inverse Wishart priors (which are typically also used as they make
the M-step convenient), which is actually just one instance of a geodesically convex prior that our
methods can handle; (iii) incorporation of penalties for avoiding tiny clusters, an idea that fits easily
in our framework but not so easily in the EM framework. Finally, beyond GMMs, extension to other
mixture models will be fruitful.
7

Available at UCI machine learning dataset repository via https://archive.ics.uci.edu/ml/datasets

8

References
[1] P.-A. Absil, R. Mahony, and R. Sepulchre. Optimization algorithms on matrix manifolds. Princeton
University Press, 2009.
[2] D. Arthur and S. Vassilvitskii. k-means++: The advantages of careful seeding. In Proceedings of the
eighteenth annual ACM-SIAM symposium on Discrete algorithms (SODA), pages 1027?1035, 2007.
[3] R. Bhatia. Positive Definite Matrices. Princeton University Press, 2007.
[4] C. M. Bishop. Pattern recognition and machine learning. Springer, 2007.
[5] S. Bonnabel. Stochastic gradient descent on riemannian manifolds. Automatic Control, IEEE Transactions
on, 58(9):2217?2229, 2013.
[6] N. Boumal, B. Mishra, P.-A. Absil, and R. Sepulchre. Manopt, a matlab toolbox for optimization on
manifolds. The Journal of Machine Learning Research, 15(1):1455?1459, 2014.
[7] S. Burer, R. D. Monteiro, and Y. Zhang. Solving semidefinite programs via nonlinear programming. part
i: Transformations and derivatives. Technical Report TR99-17, Rice University, Houston TX, 1999.
[8] S. Dasgupta. Learning mixtures of gaussians. In Foundations of Computer Science, 1999. 40th Annual
Symposium on, pages 634?644. IEEE, 1999.
[9] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society, Series B, 39:1?38, 1977.
[10] R. O. Duda, P. E. Hart, and D. G. Stork. Pattern Classification. John Wiley & Sons, 2nd edition, 2000.
[11] R. Ge, Q. Huang, and S. M. Kakade. Learning Mixtures of Gaussians in High Dimensions.
arXiv:1503.00424, 2015.
[12] R. Hosseini and M. Mash?al. Mixest: An estimation toolbox for mixture models. arXiv preprint
arXiv:1507.06065, 2015.
[13] R. Hosseini and S. Sra.
Differential geometric optimization for Gaussian mixture models.
arXiv:1506.07677, 2015.
[14] M. I. Jordan and R. A. Jacobs. Hierarchical mixtures of experts and the em algorithm. Neural computation, 6(2):181?214, 1994.
[15] M. Journ?ee, F. Bach, P.-A. Absil, and R. Sepulchre. Low-rank optimization on the cone of positive
semidefinite matrices. SIAM Journal on Optimization, 20(5):2327?2351, 2010.
[16] R. W. Keener. Theoretical Statistics. Springer Texts in Statistics. Springer, 2010.
[17] J. M. Lee. Introduction to Smooth Manifolds. Number 218 in GTM. Springer, 2012.
[18] J. Ma, L. Xu, and M. I. Jordan. Asymptotic convergence rate of the em algorithm for gaussian mixtures.
Neural Computation, 12(12):2881?2907, 2000.
[19] G. J. McLachlan and D. Peel. Finite mixture models. John Wiley and Sons, New Jersey, 2000.
[20] A. Moitra and G. Valiant. Settling the polynomial learnability of mixtures of gaussians. In Foundations
of Computer Science (FOCS), 2010 51st Annual IEEE Symposium on, pages 93?102. IEEE, 2010.
[21] K. P. Murphy. Machine Learning: A Probabilistic Perspective. MIT Press, 2012.
[22] I. Naim and D. Gildea. Convergence of the EM algorithm for gaussian mixtures with unbalanced mixing
coefficients. In Proceedings of the 29th International Conference on Machine Learning (ICML-12), pages
1655?1662, 2012.
[23] J. Nocedal and S. J. Wright. Numerical Optimization. Springer, 2006.
[24] R. A. Redner and H. F. Walker. Mixture densities, maximum likelihood, and the EM algorithm. Siam
Review, 26:195?239, 1984.
[25] W. Ring and B. Wirth. Optimization methods on riemannian manifolds and their application to shape
space. SIAM Journal on Optimization, 22(2):596?627, 2012.
[26] R. Salakhutdinov, S. T. Roweis, and Z. Ghahramani. Optimization with EM and Expectation-ConjugateGradient. In Proceedings of the 20th International Conference on Machine Learning (ICML-03), pages
672?679, 2003.
[27] S. Sra and R. Hosseini. Geometric optimisation on positive definite matrices for elliptically contoured
distributions. In Advances in Neural Information Processing Systems, pages 2562?2570, 2013.
[28] S. Sra and R. Hosseini. Conic Geometric Optimization on the Manifold of Positive Definite Matrices.
SIAM Journal on Optimization, 25(1):713?739, 2015.
[29] C. Udris?te. Convex functions and optimization methods on Riemannian manifolds. Kluwer, 1994.
[30] R. J. Vanderbei and H. Y. Benson. On formulating semidefinite programming problems as smooth convex
nonlinear optimization problems. Technical report, 2000.
[31] B. Vandereycken. Low-rank matrix completion by riemannian optimization. SIAM Journal on Optimization, 23(2):1214?1236, 2013.
[32] J. J. Verbeek, N. Vlassis, and B. Kr?ose. Efficient greedy learning of gaussian mixture models. Neural
computation, 15(2):469?485, 2003.
[33] A. Wiesel. Geodesic convexity and covariance estimation. IEEE Transactions on Signal Processing, 60
(12):6182?89, 2012.
[34] L. Xu and M. I. Jordan. On convergence properties of the EM algorithm for Gaussian mixtures. Neural
Computation, 8:129?151, 1996.
[35] D. Zoran and Y. Weiss. Natural images, gaussian mixtures and dead leaves. In Advances in Neural
Information Processing Systems, pages 1736?1744, 2012.

9


----------------------------------------------------------------

title: 2276-stochastic-neighbor-embedding.pdf

Stochastic Neighbor Embedding

Geoffrey Hinton and Sam Roweis
Department of Computer Science, University of Toronto
10 King?s College Road, Toronto, M5S 3G5 Canada
hinton,roweis @cs.toronto.edu


Abstract
We describe a probabilistic approach to the task of placing objects, described by high-dimensional vectors or by pairwise dissimilarities, in a
low-dimensional space in a way that preserves neighbor identities. A
Gaussian is centered on each object in the high-dimensional space and
the densities under this Gaussian (or the given dissimilarities) are used
to define a probability distribution over all the potential neighbors of
the object. The aim of the embedding is to approximate this distribution as well as possible when the same operation is performed on the
low-dimensional ?images? of the objects. A natural cost function is a
sum of Kullback-Leibler divergences, one per object, which leads to a
simple gradient for adjusting the positions of the low-dimensional images. Unlike other dimensionality reduction methods, this probabilistic
framework makes it easy to represent each object by a mixture of widely
separated low-dimensional images. This allows ambiguous objects, like
the document count vector for the word ?bank?, to have versions close to
the images of both ?river? and ?finance? without forcing the images of
outdoor concepts to be located close to those of corporate concepts.

1 Introduction
Automatic dimensionality reduction is an important ?toolkit? operation in machine learning, both as a preprocessing step for other algorithms (e.g. to reduce classifier input size)
and as a goal in itself for visualization, interpolation, compression, etc. There are many
ways to ?embed? objects, described by high-dimensional vectors or by pairwise dissimilarities, into a lower-dimensional space. Multidimensional scaling methods[1] preserve
dissimilarities between items, as measured either by Euclidean distance, some nonlinear
squashing of distances, or shortest graph paths as with Isomap[2, 3]. Principal components analysis (PCA) finds a linear projection of the original data which captures as much
variance as possible. Other methods attempt to preserve local geometry (e.g. LLE[4]) or
associate high-dimensional points with a fixed grid of points in the low-dimensional space
(e.g. self-organizing maps[5] or their probabilistic extension GTM[6]). All of these methods, however, require each high-dimensional object to be associated with only a single
location in the low-dimensional space. This makes it difficult to unfold ?many-to-one?
mappings in which a single ambiguous object really belongs in several disparate locations
in the low-dimensional space. In this paper we define a new notion of embedding based on
probable neighbors. Our algorithm, Stochastic Neighbor Embedding (SNE) tries to place
the objects in a low-dimensional space so as to optimally preserve neighborhood identity,
and can be naturally extended to allow multiple different low-d images of each object.

2 The basic SNE algorithm
For each object, , and each potential neighbor,  , we start by computing the asymmetric
probability,  , that would pick  as its neighbor:

	


 	

  
 
 

 

(1)

The dissimilarities,   , may be given as part of the problem definition (and need not be
symmetric), or they may be computed using the scaled squared Euclidean distance (?affinity?) between two high-dimensional points, !"$#$! :

%&%

  
)

'% %
!   !  
() 


(2)

where  is either
) set by hand or (as in some of our experiments) found by a binary search
for the value of  that makes the entropy of the distribution over neighbors equal to *'+,.- .
Here, - is the effective number of local neighbors or ?perplexity? and is chosen by hand.
In the low-dimensional space we also use Gaussian neighborhoods but with a fixed variance
(which we set without loss of generality to be / ) so the induced probability 01 that point
picks point  as its neighbor is a function of the low-dimensional images 23 of all the
objects and is given by the expression:

	4

 %'% 2   2  &% % 
  	4

 '% %    &% % 
2  2

 


0 

(3)

The aim of the embedding is to match these two distributions as well as possible. This is
achieved by minimizing a cost function which is a sum of Kullback-Leibler divergences
between the original (5 ) and induced ( 06 ) distributions over neighbors for each object:

7

98

8




  *'+,

:
@ ?  %&% A  
;8
0
9<>=

(4)

The dimensionality of the 2 space is chosen by hand (much less than the number of objects).
Notice that making 0  large when   is small wastes some of the probability mass in the 0
distribution so there is a cost for modeling a big distance in the high-dimensional space with
a small distance in the low-dimensional space, though it is less than the cost of modeling
a small distance with a big one. In this respect, SNE is an improvement over methods
like LLE [4] or SOM [5] in which widely separated data-points can be ?collapsed? as near
neighbors in the low-dimensional space. The intuition is that while SNE emphasizes local
distances, its cost function cleanly enforces both keeping the images of nearby objects
nearby and keeping the images of widely separated objects relatively far apart.
Differentiating C is tedious because 2
the result is simple: B

B

7

2 



(

8




affects 0  via the normalization term in Eq. 3, but

 2  2   
   0DCEFG  0HG 

(5)

which has the nice interpretation of a sum of forces pulling 2" toward 2 or pushing it away
depending on whether  is observed to be a neighbor more or less often than desired.

7

Given the gradient, there are many possible ways to minimize and we have only just begun the search for the best method. Steepest descent in which all of the points are adjusted
in parallel is inefficient and can get stuck in poor local optima. Adding random jitter that
decreases with time finds much better local optima and is the method we used for the examples in this paper, even though it is still quite slow. We initialize the embedding by putting
all the low-dimensional images in random locations very close to the origin. Several other
minimization methods, including annealing the perplexity, are discussed in sections 5&6.

3 Application of SNE to image and document collections
As a graphic illustration of the ability of SNE to model high-dimensional, near-neighbor
relationships using only two dimensions, we ran the algorithm on a collection of bitmaps of
handwritten digits and on a set of word-author counts taken from the scanned proceedings
of NIPS conference papers. Both of these datasets are likely to have intrinsic structure in
many fewer dimensions than their raw dimensionalities: 256 for the handwritten digits and
13679 for the author-word counts.
To begin, we used a set of  digit bitmaps from the UPS database[7] with  examples
from each
( of the five classes 0,1,2,3,4. The variance of the Gaussian around each point
in the  -dimensional raw pixel image space was set to achieve a perplexity of 15 in the
distribution over high-dimensional neighbors. SNE was initialized by putting all the 2"
in random locations very close to the origin and then was trained using gradient descent
with annealed noise. Although SNE was given no information about class labels, it quite
cleanly separates the digit groups as shown in figure 1. Furthermore, within each region of
the low-dimensional space, SNE has arranged the data so that properties like orientation,
skew and stroke-thickness tend to vary smoothly. For the embedding shown, the SNE
cost function in Eq. 4 has a value of 
	 nats;
( with a uniform
 distribution across lowdimensional neighbors, the cost is  *'+,  			
    	 nats. We also applied
principal component analysis (PCA)[8] to the same data; the projection onto the first two
principal components does not separate classes nearly as cleanly as SNE because PCA is
much more interested in getting the large separations right which causes it to jumble up
some of the boundaries between similar classes. In this experiment, we used digit classes
that do not have very similar pairs like 3 and 5 or 7 and 9. When there are more classes and
only two available dimensions, SNE does not as cleanly separate very similar pairs.
We have also applied SNE to word-document and word-author matrices calculated from
the OCRed text of NIPS volume 0-12 papers[9]. Figure 2 shows a map locating NIPS authors into two dimensions. Each of the 676 authors who published more than one paper
in NIPS vols. 0-12 is shown by a dot at the position 2  found by SNE; larger red dots
and corresponding last names are authors who published six or more papers in that period.
Distances   were computed as the norm of the difference between log aggregate author
word counts, summed across all NIPS papers. Co-authored papers gave fractional counts
evenly to all authors. All words occurring in six or more documents were included, except for stopwords giving a vocabulary size of) 13649. (The bow toolkit[10] was used for
part of
( the pre-processing of the data.) The  were set to achieve a local perplexity of
- 
neighbors. SNE seems to have grouped authors by broad NIPS field: generative
models, support vector machines, neuroscience, reinforcement learning and VLSI all have
distinguishable localized regions.

4 A full mixture version of SNE
The clean probabilistic formulation of SNE makes it easy to modify the cost function so
that instead of a single image, each high-dimensional object can have several different
versions of its low-dimensional image. These alternative versions have mixing proportions
that sum to  . Image-version  of object has location 2  and mixing proportion 5
  . The
low-dimensional neighborhood distribution for is a mixture of the distributions induced
by each of its image-versions across all image-versions of a potential neighbor  :

0 8 : 8

%&%
%'%

  4	 

  $ 2  
2     
 
	4

$ %&% 2    2 '% %  


(6)

In this multiple-image model, the derivatives with respect to the image locations 23 are
straightforward; the derivatives w.r.t the mixing proportions   are most easily expressed

(
Figure 1: The result of running the SNE algorithm on 
 -dimensional grayscale
images of handwritten digits. Pictures of the original data vectors !  (scans of handwritten
digit) are shown at the location corresponding to their low-dimensional images 23 as found
by SNE. The classes are quite well separated even though SNE had no information about
class labels. Furthermore, within each class, properties like orientation, skew and strokethickness tend to vary smoothly across the space. Not all points are shown: to produce this
display, digits are chosen in random order and are only displayed if a   x   region of the
display centered on the 2-D location of the digit in the embedding does not overlap any of
the   x   regions for digits that have already been displayed.
(SNE was initialized by putting all the  in random locations very close to the origin and then was
trained using batch gradient descent (see Eq. 5) with annealed noise. The learning rate was 0.2. For
the first 3500 iterations, each 2-D point was jittered by adding Gaussian noise with a standard deviation of   after each position update. The jitter was then reduced to  for a further 	
 iterations.)

Touretzky

Wiles

Maass
Kailath
Chauvin Munro Shavlik
Sanger
Movellan Baluja Lewicki Schmidhuber
Hertz
Baldi Buhmann Pearlmutter Yang
Tenenbaum
Cottrell
Krogh
Omohundro Abu?Mostafa
Schraudolph
MacKay
Coolen
Lippmann
Robinson Smyth
Cohn
Ahmad Tesauro
Pentland
Goodman
Atkeson
Neuneier
Warmuth
Sollich Moore
Thrun
Pomerleau

Barber

Ruppin
Horn
Meilijson MeadLazzaro
Koch
Obermayer Ruderman
Eeckman HarrisMurray
Bialek Cowan
Baird Andreou
Mel
Cauwenberghs
Brown Li
Jabri
Giles Chen
Spence Principe
Doya Touretzky
Sun
Stork Alspector Mjolsness
Bell
Lee
Maass
Lee
Gold
Pomerleau Kailath Meir
Seung Movellan
Rangarajan
Yang Amari
Tenenbaum
Cottrell Baldi
Abu?Mostafa
MacKay
Nowlan Lippmann
Smyth Cohn Kowalczyk
Waibel
Pouget
Atkeson
Kawato
Viola Bourlard Warmuth
Dayan
Sollich
Morgan Thrun MooreSutton
Barber Barto Singh
Tishby WolpertOpper
Sejnowski
Williamson
Kearns
Singer
Moody
Shawe?Taylor
Saad
Zemel
Saul
Tresp
Bartlett
Platt
Leen
Mozer
Bishop Jaakkola
Solla
Ghahramani
Smola
Williams
Vapnik
Scholkopf
Hinton
Bengio
Jordan
Muller
Graf
LeCun Simard
Denker
Guyon
Bower

Figure 2: Embedding of NIPS authors into two dimensions. Each of the 676 authors
who published more than one paper in NIPS vols. 0-12 is show by a dot at the location 2  found by the SNE algorithm. Larger red dots and corresponding last names
are authors who published six or more papers in that period. The inset in upper left
shows a blowup of the crowded boxed central portion of the space. Dissimilarities between authors were computed based on squared Euclidean distance between vectors of
log aggregate author word counts. Co-authored papers gave fractional counts evenly
to all authors. All words occurring in six or more documents were included, except
for stopwords giving a vocabulary size of 13649. The NIPS text data is available at
http://www.cs.toronto.edu/ roweis/data.html.

in terms of   , the probability that version  of picks version  of  :

@  

&% %
%&%
   4	 

  $ 2    2      
9  
	
    %'% 2  2 '% %  


(7)

The effect on 06 of changing the mixing proportion for version  of object 

B
B 0  	   8

where    

 if 



 $ C 8 

:






  

    8

is given by

@

(8)

and  otherwise. The effect of changing    on the cost, C, is

B 7

B




B
B 0 
  8 8
  
  0   

(9)

Rather than optimizing the mixing proportions directly, it is easier	
to perform unconstrained
	4


   .
optimization on ?softmax weights? defined by     	4


    
As a ?proof-of-concept?, we recently implemented a simplified mixture version in which
every object is represented in the low-dimensional
space by exactly two components that

are constrained to have mixing proportions of  . The two components are pulled together
by a force which increases linearly up to a threshold separation. Beyond this threshold
the force remains constant.1 We ran two experiments with this simplified mixture version
of SNE. We took a dataset containing  pictures of each of the digits 2,3,4 and added

 hybrid digit-pictures that were each constructed by picking new examples of two of
the classes and taking each pixel at random from one of these two ?parents?. After mini  of the hybrids and only 

	  of the non-hybrids had significantly different
mization, 
locations for their two mixture components. Moreover, the mixture components of each
hybrid always lay in the regions of the space devoted to the classes of its two parents and
never in the region devoted to the third class. For this example we used a perplexity of  
in defining the local neighborhoods, a step size of for each position update of 
   times the
gradient, and used a constant jitter of 
   . Our very simple mixture version of SNE also
makes it possible to map a circle onto a line without losing any near neighbor relationships
or introducing any new ones. Points near one ?cut point? on the circle can mapped to a
mixture of two points, one near one end of the line and one near the other end. Obviously,
the location of the cut on the two-dimensional circle gets decided by which pairs of mixture
components split first during the stochastic optimization. For certain optimization parameters that control the ease with which two mixture components can be pulled apart, only
a single cut in the circle is made. For other parameter settings, however, the circle may
fragment into two or more smaller line-segments, each of which is topologically correct
but which may not be linked to each other.
The example with hybrid digits demonstrates that even the most primitive mixture version
of SNE can deal with ambiguous high-dimensional objects that need to be mapped to two
widely separated regions of the low-dimensional space. More work needs to be done before
SNE is efficient enough to cope with large matrices of document-word counts, but it is
the only dimensionality reduction method we know of that promises to treat homonyms
sensibly without going back to the original documents to disambiguate each occurrence of
the homonym.
1
We used a threshold of     . At threshold the force was   
 nats per unit length. The low-d
space has a natural scale because the variance of the Gaussian used to determine    is fixed at 0.5.

5 Practical optimization strategies
Our current method of reducing the SNE cost is to use steepest descent with added jitter
that is slowly reduced. This produces quite good embeddings, which demonstrates that the
SNE cost function is worth minimizing, but it takes several hours to find a good embedding
for just  datapoints so we clearly need a better search algorithm.
The time per iteration could be reduced considerably by ignoring pairs of points for which
all four of   #  G #G0  #G0 G are small. Since the matrix   is fixed during the learning, it is
natural to sparsify it by replacing all entries below a certain threshold with zero and renormalizing. Then pairs H#  for which both 5 and FG are zero can be ignored from gradient
calculations if both 0  and 0 G are small. This can in turn be determined in logarithmic
time in the size of the training set by using sophisticated geometric data structures such
as K-D trees, ball-trees and AD-trees, since the 0 depend only on 42  2  . Computational physics has attacked exactly this same complexity when performing multibody
gravitational or electrostatic simulations using, for example, the fast multipole method.
In the mixture version of SNE there appears to be an interesting way of avoiding local
optima that does not involve annealing the jitter. Consider two components in the mixture
for an object that are far apart in the low-dimensional space. By raising the mixing proportion of one and lowering the mixing proportion of the other, we can move probability mass
from one part of the space to another without it ever appearing at intermediate locations.
This type of ?probability wormhole? seems like a good way to avoid local optima that arise
because a cluster of low-dimensional points must move through a bad region of the space
in order to reach a better one.
Yet another search method, which we have used with some success on toy problems, is
to provide extra dimensions in the low-dimensional space but to penalize non-zero values
on these dimensions. During the search, SNE will use the extra dimensions to go around
lower-dimensional barriers but as the penalty on using these dimensions is increased, they
will cease to be used, effectively constraining the embedding to the original dimensionality.

6 Discussion and Conclusions
Preliminary
experiments show that we can find good optima by first annealing the perplex)
ities   (using high jitter) and only reducing the jitter after the final perplexity
has been
)
reached. This raises the question of what SNE is doing when the variance,   , of the Gaussian centered on each high-dimensional point is very big so that the distribution across
neighbors is almost uniform. It is clear that in the high variance limit, the contribution of
:*&+ ,  : 
 06  to) the SNE cost function is just as important for distant neighbors as for
close ones. When   is very large, it can be shown that SNE is equivalent to minimizing the
mismatch between squared distances in the two spaces, provided all the squared distances
from an object are first normalized by subtracting off their ?antigeometric? mean,    :



@        
           
 
	4

   
 #
   *&+ , 8
   %&% !  ! %&%  
 )  #


  
 
	4

$     

    '% % 2   2  '% %  
 )  #
      *'+, 8 
  
 

	


where  is the number of objects.

;8

(10)
(11)

(12)

This mismatch is very similar to ?stress? functions used in nonmetric versions of MDS,
and enables us to understand the large-variance limit of SNE as a particular variant of such
procedures. We are still investigating the relationship to metric MDS and to PCA.
SNE can also be seen as an interesting special case of Linear Relational Embedding (LRE)
[11]. In LRE the data consists of triples (e.g. Colin has-mother Victoria) and the task
is to predict the third term from the other two. LRE learns an N-dimensional vector for
each object and an NxN-dimensional matrix for each relation. To predict the third term in
a triple, LRE multiplies the vector representing the first term by the matrix representing
the relationship and uses the resulting vector as the mean of a Gaussian. Its predictive
distribution for the third term is then determined by the relative densities of all known
objects under this Gaussian. SNE is just a degenerate version of LRE in which the only
relationship is ?near? and the matrix representing this relationship is the identity.
In summary, we have presented a new criterion, Stochastic Neighbor Embedding, for mapping high-dimensional points into a low-dimensional space based on stochastic selection
of similar neighbors. Unlike self-organizing maps, in which the low-dimensional coordinates are fixed to a grid and the high-dimensional ends are free to move, in SNE the
high-dimensional coordinates are fixed to the data and the low-dimensional points move.
Our method can also be applied to arbitrary pairwise dissimilarities between objects if such
are available instead of (or in addition to) high-dimensional observations. The gradient of
the SNE cost function has an appealing ?push-pull? property in which the forces acting on
2 to bring it closer to points it is under-selecting and further from points it is over-selecting
as its neighbor. We have shown results of applying this algorithm to image and document
collections for which it sensibly placed similar objects nearby in a low-dimensional space
while keeping dissimilar objects well separated.
Most importantly, because of its probabilistic formulation, SNE has the ability to be extended to mixtures in which ambiguous high-dimensional objects (such as the word ?bank?)
can have several widely-separated images in the low-dimensional space.
Acknowledgments We thank the anonymous referees and several visitors to our poster for helpful
suggestions. Yann LeCun provided digit and NIPS text data. This research was funded by NSERC.

References
[1] T. Cox and M. Cox. Multidimensional Scaling. Chapman & Hall, London, 1994.
[2] J. Tenenbaum. Mapping a manifold of perceptual observations. In Advances in Neural Information Processing Systems, volume 10, pages 682?688. MIT Press, 1998.
[3] J. B. Tenenbaum, V. de Silva, and J. C. Langford. A global geometric framework for nonlinear
dimensionality reduction. Science, 290:2319?2323, 2000.
[4] S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding.
Science, 290:2323?2326, 2000.
[5] T. Kohonen. Self-organization and Associative Memory. Springer-Verlag, Berlin, 1988.
[6] C. Bishop, M. Svensen, and C. Williams. GTM: The generative topographic mapping. Neural
Computation, 10:215, 1998.
[7] J. J. Hull. A database for handwritten text recognition research. IEEE Transaction on Pattern
Analysis and Machine Intelligence, 16(5):550?554, May 1994.
[8] I. T. Jolliffe. Principal Component Analysis. Springer-Verlag, New York, 1986.
[9] Yann LeCun. Nips online web site. http://nips.djvuzone.org, 2001.
[10] Andrew Kachites McCallum. Bow: A toolkit for statistical language modeling, text retrieval,
classification and clustering. http://www.cs.cmu.edu/ mccallum/bow, 1996.
[11] A. Paccanaro and G.E. Hinton. Learning distributed representations of concepts from relational
data using linear relational embedding. IEEE Transactions on Knowledge and Data Engineering, 13:232?245, 2000.


----------------------------------------------------------------

title: 5956-scheduled-sampling-for-sequence-prediction-with-recurrent-neural-networks.pdf

Scheduled Sampling for Sequence Prediction with
Recurrent Neural Networks
Samy Bengio, Oriol Vinyals, Navdeep Jaitly, Noam Shazeer
Google Research
Mountain View, CA, USA
{bengio,vinyals,ndjaitly,noam}@google.com

Abstract
Recurrent Neural Networks can be trained to produce sequences of tokens given
some input, as exemplified by recent results in machine translation and image
captioning. The current approach to training them consists of maximizing the
likelihood of each token in the sequence given the current (recurrent) state and the
previous token. At inference, the unknown previous token is then replaced by a
token generated by the model itself. This discrepancy between training and inference can yield errors that can accumulate quickly along the generated sequence.
We propose a curriculum learning strategy to gently change the training process
from a fully guided scheme using the true previous token, towards a less guided
scheme which mostly uses the generated token instead. Experiments on several sequence prediction tasks show that this approach yields significant improvements.
Moreover, it was used succesfully in our winning entry to the MSCOCO image
captioning challenge, 2015.

1

Introduction

Recurrent neural networks can be used to process sequences, either as input, output or both. While
they are known to be hard to train when there are long term dependencies in the data [1], some
versions like the Long Short-Term Memory (LSTM) [2] are better suited for this. In fact, they have
recently shown impressive performance in several sequence prediction problems including machine
translation [3], contextual parsing [4], image captioning [5] and even video description [6].
In this paper, we consider the set of problems that attempt to generate a sequence of tokens of
variable size, such as the problem of machine translation, where the goal is to translate a given
sentence from a source language to a target language. We also consider problems in which the input
is not necessarily a sequence, like the image captioning problem, where the goal is to generate a
textual description of a given image.
In both cases, recurrent neural networks (or their variants like LSTMs) are generally trained to
maximize the likelihood of generating the target sequence of tokens given the input. In practice, this
is done by maximizing the likelihood of each target token given the current state of the model (which
summarizes the input and the past output tokens) and the previous target token, which helps the
model learn a kind of language model over target tokens. However, during inference, true previous
target tokens are unavailable, and are thus replaced by tokens generated by the model itself, yielding
a discrepancy between how the model is used at training and inference. This discrepancy can be
mitigated by the use of a beam search heuristic maintaining several generated target sequences, but
for continuous state space models like recurrent neural networks, there is no dynamic programming
approach, so the effective number of sequences considered remains small, even with beam search.
1

The main problem is that mistakes made early in the sequence generation process are fed as input
to the model and can be quickly amplified because the model might be in a part of the state space it
has never seen at training time.
Here, we propose a curriculum learning approach [7] to gently bridge the gap between training and
inference for sequence prediction tasks using recurrent neural networks. We propose to change the
training process in order to gradually force the model to deal with its own mistakes, as it would
have to during inference. Doing so, the model explores more during training and is thus more robust
to correct its own mistakes at inference as it has learned to do so during training. We will show
experimentally that this approach yields better performance on several sequence prediction tasks.
The paper is organized as follows: in Section 2, we present our proposed approach to better train
sequence prediction tasks with recurrent neural networks; this is followed by Section 3 which draws
links to some related approaches. We then present some experimental results in Section 4 and
conclude in Section 5.

2

Proposed Approach

We are considering supervised tasks where the training set is given in terms of N input/output pairs
i
{X i , Y i }N
i=1 , where X is the input and can be either static (like an image) or dynamic (like a
sequence) while the target output Y i is a sequence y1i , y2i , . . . , yTi i of a variable number of tokens
that belong to a fixed known dictionary.
2.1

Model

Given a single input/output pair (X, Y ), the log probability P (Y |X) can be computed as:
log P (Y |X)

=
=

log P (y1T |X)
T
X

log P (yt |y1t?1 , X)

t=1

(1)
where Y is a sequence of length T represented by tokens y1 , y2 , . . . , yT . The latter term in the above
equation is estimated by a recurrent neural network with parameters ? by introducing a state vector,
ht , that is a function of the previous state, ht?1 , and the previous output token, yt?1 , i.e.
log P (yt |y1t?1 , X; ?) = log P (yt |ht ; ?)
where ht is computed by a recurrent neural network as follows:

f (X; ?)
if t = 1,
ht =
f (ht?1 , yt?1 ; ?) otherwise.

(2)

(3)

P (yt |ht ; ?) is often implemented as a linear projection1 of the state vector ht into a vector of scores,
one for each token of the output dictionary, followed by a softmax transformation to ensure the
scores are properly normalized (positive and sum to 1). f (h, y) is usually a non-linear function that
combines the previous state and the previous output in order to produce the current state.
This means that the model focuses on learning to output the next token given the current state
of the model AND the previous token. Thus, the model represents the probability distribution of
sequences in the most general form - unlike Conditional Random Fields [8] and other models that
assume independence between between outputs at different time steps, given latent variable states.
The capacity of the model is only limited by the representational capacity of the recurrent and
feedforward layers. LSTMs, with their ability to learn long range structure are especially well suited
to this task and make it possible to learn rich distributions over sequences.
In order to learn variable length sequences, a special token, <EOS>, that signifies the end of a
sequence is added to the dictionary and the model. During training, <EOS> is concatenated to the
end of each sequence. During inference, the model generates tokens until it generates <EOS>.
1

Although one could also use a multi-layered non-linear projection.

2

2.2

Training

Training recurrent neural networks to solve such tasks is usually accomplished by using mini-batch
stochastic gradient descent to look for a set of parameters ?? that maximizes the log likelihood of
producing the correct target sequence Y i given the input data X i for all training pairs (X i , Y i ):
X
?? = arg max
log P (Y i |X i ; ?) .
(4)
?

2.3

(X i ,Y i )

Inference

During inference the model can generate the full sequence y1T given X by generating one token at a
time, and advancing time by one step. When an <EOS> token is generated, it signifies the end of
the sequence. For this process, at time t, the model needs as input the output token yt?1 from the
last time step in order to produce yt . Since we do not have access to the true previous token, we can
instead either select the most likely one given our model, or sample according to it.
Searching for the sequence Y with the highest probability given X is too expensive because of the
combinatorial growth in the number of sequences. Instead we use a beam searching procedure to
generate k ?best? sequences. We do this by maintaining a heap of m best candidate sequences. At
each time step new candidates are generated by extending each candidate by one token and adding
them to the heap. At the end of the step, the heap is re-pruned to only keep m candidates. The beam
searching is truncated when no new sequences are added, and k best sequences are returned.
While beam search is often used for discrete state based models like Hidden Markov Models where
dynamic programming can be used, it is harder to use efficiently for continuous state based models
like recurrent neural networks, since there is no way to factor the followed state paths in a continuous
space, and hence the actual number of candidates that can be kept during beam search decoding is
very small.
In all these cases, if a wrong decision is taken at time t ? 1, the model can be in a part of the
state space that is very different from those visited from the training distribution and for which it
doesn?t know what to do. Worse, it can easily lead to cumulative bad decisions - a classic problem in
sequential Gibbs sampling type approaches to sampling, where future samples can have no influence
on the past.
2.4

Bridging the Gap with Scheduled Sampling

The main difference between training and inference for sequence prediction tasks when predicting
token yt is whether we use the true previous token yt?1 or an estimate y?t?1 coming from the model
itself.
We propose here a sampling mechanism that will randomly decide, during training, whether we use
yt?1 or y?t?1 . Assuming we use a mini-batch based stochastic gradient descent approach, for every
token to predict yt ? Y of the ith mini-batch of the training algorithm, we propose to flip a coin
and use the true previous token with probability i , or an estimate coming from the model itself with
probability (1 ? i )2 The estimate of the model can be obtained by sampling a token according to
the probability distribution modeled by P (yt?1 |ht?1 ), or can be taken as the arg maxs P (yt?1 =
s|ht?1 ). This process is illustrated in Figure 1.
When i = 1, the model is trained exactly as before, while when i = 0 the model is trained in
the same setting as inference. We propose here a curriculum learning strategy to go from one to
the other: intuitively, at the beginning of training, sampling from the model would yield a random
token since the model is not well trained, which could lead to very slow convergence, so selecting
more often the true previous token should help; on the other hand, at the end of training, i should
favor sampling from the model more often, as this corresponds to the true inference situation, and
one expects the model to already be good enough to handle it and sample reasonable tokens.
2
Note that in the experiments, we flipped the coin for every token. We also tried to flip the coin once per
sequence, but the results were much worse, most probably because consecutive errors are amplified during the
first rounds of training.

3

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

Figure 1: Illustration of the Scheduled Sampling approach,
where one flips a coin at every time step to decide to use the
true previous token or one sampled from the model itself.

Exponential decay
Inverse sigmoid decay
Linear decay

0

200

Figure 2:
schedules.

400

600

800

1000

Examples of decay

We thus propose to use a schedule to decrease i as a function of i itself, in a similar manner used
to decrease the learning rate in most modern stochastic gradient descent approaches. Examples of
such schedules can be seen in Figure 2 as follows:
? Linear decay: i = max(, k ? ci) where 0 ?  < 1 is the minimum amount of truth to be
given to the model and k and c provide the offset and slope of the decay, which depend on
the expected speed of convergence.
? Exponential decay: i = k i where k < 1 is a constant that depends on the expected speed
of convergence.
? Inverse sigmoid decay: i = k/(k +exp(i/k)) where k ? 1 depends on the expected speed
of convergence.
We call our approach Scheduled Sampling. Note that when we sample the previous token y?t?1 from
the model itself while training, we could back-propagate the gradient of the losses at times t ? T
through that decision. This was not done in the experiments described in this paper and is left for
future work.

3

Related Work

The discrepancy between the training and inference distributions has already been noticed in the
literature, in particular for control and reinforcement learning tasks.
SEARN [9] was proposed to tackle problems where supervised training examples might be different
from actual test examples when each example is made of a sequence of decisions, like acting in a
complex environment where a few mistakes of the model early in the sequential decision process
might compound and yield a very poor global performance. Their proposed approach involves a
meta-algorithm where at each meta-iteration one trains a new model according to the current policy
(essentially the expected decisions for each situation), applies it on a test set and modifies the next
iteration policy in order to account for the previous decisions and errors. The new policy is thus a
combination of the previous one and the actual behavior of the model.
In comparison to SEARN and related ideas [10, 11], our proposed approach is completely online: a
single model is trained and the policy slowly evolves during training, instead of a batch approach,
which makes it much faster to train3 Furthermore, SEARN has been proposed in the context of
reinforcement learning, while we consider the supervised learning setting trained using stochastic
gradient descent on the overall objective.
Other approaches have considered the problem from a ranking perspective, in particular for parsing
tasks [12] where the target output is a tree. In this case, the authors proposed to use a beam search
both during training and inference, so that both phases are aligned. The training beam is used to find
3
In fact, in the experiments we report in this paper, our proposed approach was not meaningfully slower
(nor faster) to train than the baseline.

4

the best current estimate of the model, which is compared to the guided solution (the truth) using a
ranking loss. Unfortunately, this is not feasible when using a model like a recurrent neural network
(which is now the state-of-the-art technique in many sequential tasks), as the state sequence cannot
be factored easily (because it is a multi-dimensional continuous state) and thus beam search is hard
to use efficiently at training time (as well as inference time, in fact).
Finally, [13] proposed an online algorithm for parsing problems that adapts the targets through the
use of a dynamic oracle that takes into account the decisions of the model. The trained model
is a perceptron and is thus not state-based like a recurrent neural network, and the probability of
choosing the truth is fixed during training.

4

Experiments

We describe in this section experiments on three different tasks, in order to show that scheduled
sampling can be helpful in different settings. We report results on image captioning, constituency
parsing and speech recognition.
4.1

Image Captioning

Image captioning has attracted a lot of attention in the past year. The task can be formulated as a
mapping of an image onto a sequence of words describing its content in some natural language, and
most proposed approaches employ some form of recurrent network structure with simple decoding
schemes [5, 6, 14, 15, 16]. A notable exception is the system proposed in [17], which does not
directly optimize the log likelihood of the caption given the image, and instead proposes a pipelined
approach.
Since an image can have many valid captions, the evaluation of this task is still an open problem. Some attempts have been made to design metrics that positively correlate with human evaluation [18], and a common set of tools have been published by the MSCOCO team [19].
We used the MSCOCO dataset from [19] to train our model. We trained on 75k images and report
results on a separate development set of 5k additional images. Each image in the corpus has 5 different captions, so the training procedure picks one at random, creates a mini-batch of examples,
and optimizes the objective function defined in (4). The image is preprocessed by a pretrained convolutional neural network (without the last classification layer) similar to the one described in [20],
and the resulting image embedding is treated as if it was the first word from which the model starts
generating language. The recurrent neural network generating words is an LSTM with one layer
of 512 hidden units, and the input words are represented by embedding vectors of size 512. The
number of words in the dictionary is 8857. We used an inverse sigmoid decay schedule for i for the
scheduled sampling approach.
Table 1 shows the results on various metrics on the development set. Each of these metrics is
a variant of estimating the overlap between the obtained sequence of words and the target one.
Since there were 5 target captions per image, the best result is always chosen. To the best of our
knowledge, the baseline results are consistent (slightly better) with the current state-of-the-art on
that task. While dropout helped in terms of log likelihood (as expected but not shown), it had a
negative impact on the real metrics. On the other hand, scheduled sampling successfully trained a
model more resilient to failures due to training and inference mismatch, which likely yielded higher
quality captions according to all the metrics. Ensembling models also yielded better performance,
both for the baseline and the schedule sampling approach. It is also interesting to note that a model
trained while always sampling from itself (hence in a regime similar to inference), dubbed Always
Sampling in the table, yielded very poor performance, as expected because the model has a hard
time learning the task in that case. We also trained a model with scheduled sampling, but instead
of sampling from the model, we sampled from a uniform distribution, in order to verify that it was
important to build on the current model and that the performance boost was not just a simple form
of regularization. We called this Uniform Scheduled Sampling and the results are better than the
baseline, but not as good as our proposed approach. We also experimented with flipping the coin
once per sequence instead of once per token, but the results were as poor as the Always Sampling
approach.
5

Table 1: Various metrics (the higher the better) on the MSCOCO development set for the image
captioning task.
Approach vs Metric
Baseline
Baseline with Dropout
Always Sampling
Scheduled Sampling
Uniform Scheduled Sampling
Baseline ensemble of 10
Scheduled Sampling ensemble of 5

BLEU-4
28.8
28.1
11.2
30.6
29.2
30.7
32.3

METEOR
24.2
23.9
15.7
24.3
24.2
25.1
25.4

CIDER
89.5
87.0
49.7
92.1
90.9
95.7
98.7

It?s worth noting that we used our scheduled sampling approach to participate in the 2015 MSCOCO
image captioning challenge [21] and ranked first in the final leaderboard.

4.2

Constituency Parsing

Another less obvious connection with the any-to-sequence paradigm is constituency parsing. Recent
work [4] has proposed an interpretation of a parse tree as a sequence of linear ?operations? that build
up the tree. This linearization procedure allowed them to train a model that can map a sentence onto
its parse tree without any modification to the any-to-sequence formulation.
The trained model has one layer of 512 LSTM cells and words are represented by embedding vectors
of size 512. We used an attention mechanism similar to the one described in [22] which helps,
when considering the next output token to produce yt , to focus on part of the input sequence only
by applying a softmax over the LSTM state vectors corresponding to the input sequence. The input
word dictionary contained around 90k words, while the target dictionary contained 128 symbols used
to describe the tree. We used an inverse sigmoid decay schedule for i in the scheduled sampling
approach.
Parsing is quite different from image captioning as the function that one has to learn is almost
deterministic. In contrast to an image having a large number of valid captions, most sentences have
a unique parse tree (although some very difficult cases exist). Thus, the model operates almost
deterministically, which can be seen by observing that the train and test perplexities are extremely
low compared to image captioning (1.1 vs. 7).
This different operating regime makes for an interesting comparison, as one would not expect the
baseline algorithm to make many mistakes. However, and as can be seen in Table 2, scheduled
sampling has a positive effect which is additive to dropout. In this table we report the F1 score on the
WSJ 22 development set [23]. We should also emphasize that there are only 40k training instances,
so overfitting contributes largely to the performance of our system. Whether the effect of sampling
during training helps with regard to overfitting or the training/inference mismatch is unclear, but the
result is positive and additive with dropout. Once again, a model trained by always sampling from
itself instead of using the groundtruth previous token as input yielded very bad results, in fact so bad
that the resulting trees were often not valid trees (hence the ?-? in the corresponding F1 metric).
Table 2: F1 score (the higher the better) on the validation set of the parsing task.
Approach
Baseline LSTM
Baseline LSTM with Dropout
Always Sampling
Scheduled Sampling
Scheduled Sampling with Dropout

6

F1
86.54
87.0
88.08
88.68

4.3

Speech Recognition

For the speech recognition experiments, we used a slightly different setting from the rest of the
paper. Each training example is an input/output pair (X, Y ), where X is a sequence of T input
vectors x1 , x2 , ? ? ? xT and Y is a sequence of T tokens y1 , y2 , ? ? ? yT so each yt is aligned with the
corresponding xt . Here, xt are the acoustic features represented by log Mel filter bank spectra at
frame t, and yt is the corresponding target. The targets used were HMM-state labels generated from
a GMM-HMM recipe, using the Kaldi toolkit [24] but could very well have been phoneme labels.
This setting is different from the other experiments in that the model we used is the following:

log P (Y |X; ?)

=
=

log P (y1T |xT1 ; ?)
T
X

log P (yt |y1t?1 , xt1 ; ?)

t=1

=

T
X

log P (yt |ht ; ?)

(5)

t=1

where ht is computed by a recurrent neural network as follows:

f (oh , S, x1 ; ?)
if t = 1,
ht =
f (ht?1 , yt?1 , xt ; ?) otherwise.

(6)

where oh is a vector of 0?s with same dimensionality as ht ?s and S is an extra token added to the
dictionary to represent the start of each sequence.
We generated data for these experiments using the TIMIT4 corpus and the KALDI toolkit as described in [25]. Standard configurations were used for the experiments - 40 dimensional log Mel
filter banks and their first and second order temporal derivatives were used as inputs to each frame.
180 dimensional targets were generated for each time frame using forced alignment to transcripts
using a trained GMM-HMM system. The training, validation and test sets have 3696, 400 and 192
sequences respectively, and their average length was 304 frames. The validation set was used to
choose the best epoch in training, and the model parameters from that epoch were used to evaluate
the test set.
The trained models had two layers of 250 LSTM cells and a softmax layer, for each of five configurations - a baseline configuration where the ground truth was always fed to the model, a configuration
(Always Sampling) where the model was only fed in its own predictions from the last time step,
and three scheduled sampling configurations (Scheduled Sampling 1-3), where i was ramped linearly from a maximum value to a minimum value over ten epochs and then kept constant at the
final value. For each configuration, we trained 3 models and report average performance over them.
Training of each model was done over frame targets from the GMM. The baseline configurations
typically reached the best validation accuracy after approximately 14 epochs whereas the sampling
models reached the best accuracy after approximately 9 epochs, after which the validation accuracy
decreased. This is probably because the way we trained our models is not exact - it does not account
for the gradient of the sampling probabilities from which we sampled our targets. Future effort at
tackling this problem may further improve results.
Testing was done by finding the best sequence from beam search decoding (using a beam size of
10 beams) and computing the error rate over the sequences. We also report the next step error rate
(where the model was fed in the ground truth to predict the class of the next frame) for each of the
models on the validation set to summarize the performance of the models on the training objective.
Table 3 shows a summary of the results
It can be seen that the baseline performs better next step prediction than the models that sample the
tokens for input. This is to be expected, since the former has access to the groundtruth. However, it
can be seen that the models that were trained with sampling perform better than the baseline during
decoding. It can also be seen that for this problem, the ?Always Sampling? model performs quite
4

https://catalog.ldc.upenn.edu/LDC93S1.

7

well. We hypothesize that this has to do with the nature of the dataset. The HMM-aligned states
have a lot of correlation - the same state appears as the target for several frames, and most of the
states are constrained only to go to a subset of other states. Next step prediction with groundtruth
labels on this task ends up paying disproportionate attention to the structure of the labels (y1t?1 )
and not enough to the acoustics input (xt1 ). Thus it achieves very good next step prediction error
when the groundtruth sequence is fed in with the acoustic information, but is not able to exploit
the acoustic information sufficiently when the groundtruth sequence is not fed in. For this model
the testing conditions are too far from the training condition for it to make good predictions. The
model that is only fed its own prediction (Always Sampling) ends up exploiting all the information
it can find in the acoustic signal, and effectively ignores its own predictions to influence the next
step prediction. Thus at test time, it performs just as well as it does during training. A model such as
the attention model of [26] which predicts phone sequences directly, instead of the highly redundant
HMM state sequences, would not suffer from this problem because it would need to exploit both the
acoustic signal and the language model sufficiently to make predictions. Nevertheless, even in this
setting, adding scheduled sampling still helped to improve the decoding frame error rate.
Note that typically speech recognition experiments use HMMs to decode predictions from neural
networks in a hybrid model. Here we avoid using an HMM altogether and hence we do not have the
advantage of the smoothing that results from the HMM architecture and the language models. Thus
the results are not directly comparable to the typical hybrid model results.
Table 3: Frame Error Rate (FER) on the speech recognition experiments. In next step prediction
(reported on validation set) the ground truth is fed in to predict the next target like it is done during
training. In decoding experiments (reported on test set), beam searching is done to find the best
sequence. We report results on four different linear schedulings of sampling, where i was ramped
down linearly from s to e . For the baseline, the model was only fed in the ground truth. See
Section 4.3 for an analysis of the results.
Approach
Always Sampling
Scheduled Sampling 1
Scheduled Sampling 2
Scheduled Sampling 3
Baseline LSTM

5

s
0
0.25
0.5
0.9
1

e
0
0
0
0.5
1

Next Step FER
34.6
34.3
34.1
19.8
15.0

Decoding FER
35.8
34.5
35.0
42.0
46.0

Conclusion

Using recurrent neural networks to predict sequences of tokens has many useful applications like
machine translation and image description. However, the current approach to training them, predicting one token at a time, conditioned on the state and the previous correct token, is different from
how we actually use them and thus is prone to the accumulation of errors along the decision paths.
In this paper, we proposed a curriculum learning approach to slowly change the training objective
from an easy task, where the previous token is known, to a realistic one, where it is provided by the
model itself. Experiments on several sequence prediction tasks yield performance improvements,
while not incurring longer training times. Future work includes back-propagating the errors through
the sampling decisions, as well as exploring better sampling strategies including conditioning on
some confidence measure from the model itself.

References
[1] Y. Bengio, P. Simard, and P. Frasconi. Learning long term dependencies is hard. IEEE Transactions on
Neural Networks, 5(2):157?166, 1994.
[2] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8), 1997.
[3] I. Sutskever, O. Vinyals, and Q. Le. Sequence to sequence learning with neural networks. In Advances in
Neural Information Processing Systems, NIPS, 2014.
[4] O. Vinyals, L. Kaiser, T. Koo, S. Petrov, I. Sutskever, and G. Hinton. Grammar as a foreign language. In
arXiv:1412.7449, 2014.

8

[5] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and tell: A neural image caption generator. In
IEEE Conference on Computer Vision and Pattern Recognition, CVPR, 2015.
[6] J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, and T. Darrell.
Long-term recurrent convolutional networks for visual recognition and description. In IEEE Conference
on Computer Vision and Pattern Recognition, CVPR, 2015.
[7] Y. Bengio, J. Louradour, R. Collobert, and J. Weston. Curriculum learning. In Proceedings of the International Conference on Machine Learning, ICML, 2009.
[8] J. D. Lafferty, A. McCallum, and F. C. N. Pereira. Conditional random fields: Probabilistic models for
segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on
Machine Learning, ICML, pages 282?289, San Francisco, CA, USA, 2001. Morgan Kaufmann Publishers
Inc.
[9] H. Daum?e III, J. Langford, and D. Marcu. Search-based structured prediction as classification. Machine
Learning Journal, 2009.
[10] S. Ross, G. J. Gordon, and J. A. Bagnell. A reduction of imitation learning and structured prediction
to no-regret online learning. In Proceedings of the Workshop on Artificial Intelligence and Statistics,
AISTATS, 2011.
[11] A. Venkatraman, M. Herbert, and J. A. Bagnell. Improving multi-step prediction of learned time series
models. In Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAI, 2015.
[12] M. Collins and B. Roark. Incremental parsing with the perceptron algorithm. In Proceedings of the
Association for Computational Linguistics, ACL, 2004.
[13] Y. Goldberg and J. Nivre. A dynamic oracle for arc-eager dependency parsing. In Proceedings of COLING, 2012.
[14] J. Mao, W. Xu, Y. Yang, J. Wang, Z. H. Huang, and A. Yuille. Deep captioning with multimodal recurrent
neural networks (m-rnn). In International Conference on Learning Representations, ICLR, 2015.
[15] R. Kiros, R. Salakhutdinov, and R. Zemel. Unifying visual-semantic embeddings with multimodal neural
language models. In TACL, 2015.
[16] A. Karpathy and F.-F. Li. Deep visual-semantic alignments for generating image descriptions. In IEEE
Conference on Computer Vision and Pattern Recognition, CVPR, 2015.
[17] H. Fang, S. Gupta, F. Iandola, R. K. Srivastava, L. Deng, P. Dollar, J. Gao, X. He, M. Mitchell, J. C.
Platt, C. L. Zitnick, and G. Zweig. From captions to visual concepts and back. In IEEE Conference on
Computer Vision and Pattern Recognition, CVPR, 2015.
[18] R. Vedantam, C. L. Zitnick, and D. Parikh. CIDEr: Consensus-based image description evaluation. In
IEEE Conference on Computer Vision and Pattern Recognition, CVPR, 2015.
[19] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll?ar, and C. L. Zitnick. Microsoft
coco: Common objects in context. arXiv:1405.0312, 2014.
[20] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal
covariate shift. In Proceedings of the International Conference on Machine Learning, ICML, 2015.
[21] Y. Cui, M. R. Ronchi, T.-Y. Lin, P. Dollr, and L. Zitnick.
http://mscoco.org/dataset/#captions-challenge2015, 2015.

Microsoft coco captioning challenge.

[22] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate.
In International Conference on Learning Representations, ICLR, 2015.
[23] E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and R. Weischedel. Ontonotes: The 90% solution. In
Proceedings of the Human Language Technology Conference of the NAACL, Short Papers, pages 57?60,
New York City, USA, June 2006. Association for Computational Linguistics.
[24] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlicek,
Y. Qian, P. Schwarz, J. Silovsky, G. Stemmer, and K. Vesely. The kaldi speech recognition toolkit. In
IEEE 2011 Workshop on Automatic Speech Recognition and Understanding. IEEE Signal Processing
Society, December 2011. IEEE Catalog No.: CFP11SRW-USB.
[25] N. Jaitly. Exploring Deep Learning Methods for discovering features in speech signals. PhD thesis,
University of Toronto, 2014.
[26] Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. End-to-end continuous speech
recognition using attention-based recurrent nn: First results. arXiv preprint arXiv:1412.1602, 2014.

9


----------------------------------------------------------------

title: 182-genesis-a-system-for-simulating-neural-networks.pdf

485

GENESIS: A SYSTEM FOR SIMULATING NEURAL
NETWOfl.KS
Matthew A. Wilson, Upinder S. Bhalla, John D. Uhley, James M. Bower.
Division of Biology
California Institute of Technology
Pasadena, CA 91125

ABSTRACT
We have developed a graphically oriented, general purpose
simulation system to facilitate the modeling of neural networks.
The simulator is implemented under UNIX and X-windows and is
designed to support simulations at many levels of detail.
Specifically, it is intended for use in both applied network
modeling and in the simulation of detailed, realistic, biologicallybased models. Examples of current models developed under this
system include mammalian olfactory bulb and cortex, invertebrate
central pattern generators, as well as more abstract connectionist
simulations.

INTRODUCTION
Recently, there has been a dramatic increase in interest in exploring the
computational properties of networks of parallel distributed processing elements
(Rumelhart and McClelland, 1986) often referred to as Itneural networks"
(Anderson, 1988). Much of the current research involves numerical simulations of
these types of networks (Anderson, 1988; Touretzky, 1989). Over the last several
years, there has also been a significant increase in interest in using similar computer
simulation techniques to study the structure and function of biological neural
networks. This effort can be seen as an attempt to reverse-engineer the brain with
the objective of understanding the functional organization of its very complicated
networks (Bower, 1989). Simulations of these systems range from detailed
reconstructions of single neurons, or even components of single neurons, to
simulations of large networks of complex neurons (Koch and Segev, 1989).
Modelers associated with each area of research are likely to benefit from exposure to
a large range of neural network simulations. A simulation package capable of
implementing these varied types of network models would facilitate this interaction.

486

Wilson, Bhalla, Uhley and Bower

DESIGN FEATURES OF THE SIMULATOR
We have built GENESIS (GEneral NEtwork SImulation System) and its graphical
interface XODUS (X-based Output and Display Utility for Simulators) to provide a
standardized and flexible means of constructing neural network simulations while
making minimal assumptions about the actual structure of the neural components.
The system is capable of growing according to the needs of users by incorporating
user-defined code. We will now describe the specific features of this system.
Device independence.
The entire system has been designed to run under UNIX and X-windows (version
11) for maximum portability. The code was developed on Sun workstations and has
been ported to Sun3's, Sun4's, Sun 386i's, and Masscomp computers. It should be
portable to all installations supporting UNIX and X-II. In addition, we will be
developing a parallel implementation of the simulation system (Nelson et al., 1989).
Modular design.
The design of the simulator and interface is based on a "building-block" approach.
Simulations are constructed of modules which receive inputs, perform calculations
on them, and generate outputs (figs. 2,3). This approach is central to the generality
and flexibility of the system as it allows the user to easily add new features
without modification to the base code.
Interactive specification and control.
Network specification and control is done at a high level using graphical tools and a
network specification language (fig. 1). The graphics interface provides the highest
and most user friendly level of interaction. It consists of a number of tools which
the user can configure to suit a particular simulation. Through the graphical
interface the user can display, control and adjust the parameters of simulations. The
network specification language we have developed for network modeling represents a
more basic level of interaction. This language consists of a set of simulator and
interface functions that can be executed interactively from the keyboard or from
text flies storing command sequences (scripts). The language also provides for
arithmetic operations and program control functions such as looping, conditional
statements, and subprograms or macros. Figures 3 and 4 demonstrate how some of
these script functions are used.
Simulator and interrace toolkits.
Extendable toolkits which consist of module libraries, graphical tools and the
simulator base code itself (fig. 2) provide the routines and modules used to
construct specific simulations. The base code provides the common control and
support routines for the entire system.

GENESIS: A System for Simulating Neural Networks

Gra hics Interface

~.. ~

..

Script Files

. DP~~Data

Files

(
Genesis command
window and ke board

Script Language
Interpreter

Genesis 1%

Figure 1. Levels Of Interaction With The Simulator

CONSTRUCTING SIMULATIONS
The first step in using GENESIS involves selecting and linking together those
modules from the toolkits that will be necessary for a particular simulation (fig.
2,3). Additional commands in the scripting language establish the network and the
graphical interface (fig. 4).
Module Classes.
Modules in GENESIS are divided into computational modules, communications
modules and graphical modules. All instances of computational modules are called
elements. These are the central components of simulations, performing all of the
numerical calculations. Elements can communicate in two ways: via links and via
connections. Links allow the passing of data between two elements with no time
delay and with no computation being performed on the data. Thus. links serve to
unify a large number of elements into a single computational unit (e.g. they are
used to link elements together to form the neuron in fig. 3C). Connections. on the
other hand. interconnect computational units via simulated communication channels
which can incorporate time delays and perform transformations on data being
transmitted (e.g. axons in fig. 3C). Graphical modules called widgets are used to
construct the interface. These modules can issue script commands as well as respond
to them, thus allowing interactive access to simulator structures and functions.

487

488

Wilson, Bhalla, Uhley and Bower

Hierarchical organization.
In order to keep track of the structure of a simulation, elements are organized into a
tree hierarchy similar to the directory structure in UNIX (fig. 3B). The tree
structure does not explicitly represent the pattern of links and connections between
elements, it is simply a tool for organizing complex groups of elements in the
simulation.
Simulation example.
As an example of the types of modules available and the process of structuring them
into a network simulation and graphical interface, we will describe the construction
of a simple biological neural simulation (fig. 3). The I11pdel consists of two
neurons. Each neuron contains a passive dendritic compartment, an active cell body,
an axonal output, and a synaptic input onto the dendrite. The axon of one neuron
connects to a synaptic input of the other. Figure 3 shows the basic structure of the
model as implemented under GENESIS. In the model, the synapse, channels,

Simulator and interrace toolkit
-----------------------------------------------------------------~

Graphics Modules

Communications
modules

Computational
Modules
(

A

CLinker

oDCO

Earn

?

Simulation

Simulator

=> __
ffi ~
..
....-----0001 ca
.... ;..........

\.< .

.::<::;:::;";::::,:::-:.<., ..... .

:? j~ : CQdK

Figure 2. Stages In Constructing A Simulation.

.. ...

GENESIS: A System for Simulating Neural Net~orks

network

B

A

~

neuron!

neuron2

~~
cell-body

Na

A

K

dendrite

axon

\

synapse

C

KEY
Element
Connection

dendrite

-Link

D

Figure 3. Implementation of a two neuron model in GENESIS. (A) Schematic diagram of compartmentally modeled neurons. Each cell in this simple model has a passive dendritic compartment, an active cell-body, and an output axon. There is a
synaptic input to the dendrite of one cell and two ionic channels on the cell body.
(B) Hierarchical representation of the components of the simulation as maintained in
GENESIS. The cell-body of neuron 1 is referred to as /network/neuronl/cell-body.
(C) A representation of the functional links between the basic components of one
neuron. (D) Sample interface control and display widgets created using the XODUS
toolkit.

489

490

Wilson, Bhalla, Uhley and Bower

dendritic compartments, cell body and axon are each treated as separate
computational elements (fig. 3C). Links allow elements to share information (e.g.
the Na channel needs to have access to the cell-body membrane voltage). Figure 4
shows a portion of the script used to construct this simulation.

Create different types or elements and
create
create
active compartment
create
passive_compartment
create
synapse

assign them names.
neuronl
cell-body
dendrite
dendrite/synapse

Establish functional "links" between the elements.
link
dendrite
to
cell-body
link
dendrite/synapse
to
dendrite
Set parameters associated with the elements.
set
dendrit~
capacitance
l.Oe-6
Make copies or entire element subtrees.
copy
neuronl
to
neuron2
Establish "connections" between two elements.
connect neuronl/axon
to
neuron2/dendrite/synapse
Set up a graph to monitor an element variable
graph
neuronl/cell-body
potential
Make a control panel with several control "widgets".
xform
control
xdialo g nstep set-nstep -default 200
xdialog dt
set-dt
-default 0.5
Xloggle Euler set-euler
Figure 4. Sample script commands for constructing a simulation (see fig. 3)

SIMULATOR SPECIFICATIONS
Memory requirements or GENESIS.
Currently. GENESIS consists of about 20,000 lines of simulator code and a similar
amount of graphics code, all written in C. The executable binaries take up about 1.5
Megabytes. A rough estimate of the amount of additional memory necessary for a
particular simulation can be calculated from the sizes and number of modules used
in a simulation. Typically, elements use around 100 bytes, connections 16 and
messages 20. Widgets use 5-20 Kbytes each.

GENESIS: A System for Simulating Neural Networks

Performance
The overall efficiency of the GENESIS system is highly simulation specific. To
consider briefly a specific case, the most sophisticated biologically based simulation
currently implemented under GENESIS, is a model of piriform (olfactory) cortex
(Wilson et al., 1986; Wilson and Bower, 1988; Wilson and Bower, 1989). This
simulation consists of neurons of four different types. Each neuron contains from
one to five compartments. Each compartment can contain several channels. On a
SUN 386i with 8 Mbytes of RAM. this simulation with 500 cells runs at I second
per time step.
Other models that have been implemented under GENESIS
The list of projects currently completed under GENESIS includes approximately ten
different simulations. These include models of the olfactory bulb (Bhalla et al.,
1988), the inferior olive (Lee and Bower, 1988). and a motor circuit in the
invertebrate sea slug Tritonia (Ryckebusch et aI., 1989)~ We have also built several
tutorials to allow students to explore compartmental biological models (Hodgkin
and Huxley, 1952), and Hopfield networks (Hopfield. 1982).
Access/use of GENESIS
GENESIS and XODUS will be made available at the cost of distribution to all
interested users. As described above, new user-defined modules can be linked into
the simulator to extend the system. Users are encouraged to support the continuing
development of this system by sending modules they develop to Caltech. These
will be reviewed and compiled into the overall system by GENESIS support staff.
We would also hope that users would send completed published simulations to the
GENESIS data base. This will provide others with an opportunity to observe the
behavior of a simulation first hand. A current listing of modules and full
simulations will be maintained and available through an electronic mail newsgroup.
Babel. Enquiries about the system should be sent to GENESIS@caltech.edu or
GENESIS@caltech.biblet.
Acknowledgments
We would like to thank Mark Nelson for his invaluable assistance in the
development of this system and specifically for his suggestions on the content of
this manuscript. We would also like to recognize Dave Bilitch. Wojtek Furmanski.
Christof Koch, innumerable Caltech students and the students of the 1988 MBL
summer course on Methods in Computational Neuroscience for their contributions
to the creation and evolution of GENESIS (not mutually exclusive). This research
was also supported by the NSF (EET-8700064). the NIH (BNS 22205). the ONR
(Contract NOOOI4-88-K-0513). the Lockheed Corporation. the Caltech Presidents
Fund, the JPL Directors Development Fund. and the Joseph Drown Foundation.

491

492

Wilson, Bhalla, Uhley and Bower

References
D. Anderson. (ed.) Neural information processing systems. American Institute of
Physics, New York (1988).
U.S. Bhalla, M.A. Wilson, & J.M. Bower. Integration of computer simulations
and multi-unit recording in the rat olfactory system. Soc. Neurosci. Abstr. 14:
1188 (1988).
I.M. Bower. Reverse engineering the nervous system: An anatomical, physiological,
and computer based approach.
In: An Introduction to Neural and Electronic
Networks. Zornetzer, Davis, and Lau, editors. Academic Press (1989)(in press).
A.L. Hodgkin and A.F. Huxley. A quantitative description of membrane current and
its application to conduction and excitation in nerve. I.Physiol, (Lond.) 117, 500544 (1952).
1.J. Hopfield. Neural networks and physical systems with emergent collective
computational abilities. Proc. Natl. Acad. Sci. USA. 79,2554-2558 (1982).
C. Koch and I. Segev. (eds.) Methods in Neuronal Modeling: From Synapses to
Networks. MIT Press, Cambridge, MA (in press).
M. Lee and I.M. Bower. A structural simulation of the inferior olivary nucleus.
Soc. Neurosci. Abstr. 14: 184 (1988).
M. Nelson, W. Furmanski and I.M. Bower. Simulating neurons and neuronal
networks on parallel computers. In: Methods in Neuronal Modeling: From Synapses
to Networks. C. Koch and I. Segev, editors. MIT Press, Cambridge, MA (1989)(in
press).
S. Ryckebusch, C. Mead and I.M. Bower. Modeling a central pattern generator in
software and hardware: Tritonia in sea moss (CMOS). (l989)(this volume).
D.E. Rumelhart, 1.L. McClelland and the PDP Research Group. Parallel Distributed
Processing. MIT Press, Cambridge, MA (1986).
D. Touretzky. (ed.) Advances in Neural Network Information Processing Systems.
Morgan Kaufmann Publishers, San Mateo, California (1989).
M.A. Wilson and I.M. Bower. The simulation of large-scale neuronal networks. In:
Methods in Neuronal Modeling: From Synapses to Networks. C. Koch and I. Segev,
editors. MIT Press, Cambridge, MA (1989)(in press).
M.A. Wilson and I.M. Bower. A computer simulation of olfactory cortex with
functional implications for storage and retrieval of olfactory information. In:
Neural information processing systems. pp. 114-126 D. Anderson, editor. Published
by AlP Press, New York, N.Y (1988).
M.A. Wilson, I.M. Bower and L.B. Haberly. A computer simulation of piriform
cortex. Soc. Neurosci. Abstr. 12.1358 (1986).

Part IV
Structured Networks


----------------------------------------------------------------

title: 4022-latent-variable-models-for-predicting-file-dependencies-in-large-scale-software-development.pdf

Latent Variable Models for Predicting File
Dependencies in Large-Scale Software Development

Diane J. Hu1 , Laurens van der Maaten1,2 , Youngmin Cho1 , Lawrence K. Saul1 , Sorin Lerner1
1
Dept. of Computer Science & Engineering, University of California, San Diego
2
Pattern Recognition & Bioinformatics Lab, Delft University of Technology
{dhu,lvdmaaten,yoc002,saul,lerner}@cs.ucsd.edu

Abstract
When software developers modify one or more files in a large code base, they
must also identify and update other related files. Many file dependencies can be
detected by mining the development history of the code base: in essence, groups
of related files are revealed by the logs of previous workflows. From data of this
form, we show how to detect dependent files by solving a problem in binary matrix
completion. We explore different latent variable models (LVMs) for this problem,
including Bernoulli mixture models, exponential family PCA, restricted Boltzmann machines, and fully Bayesian approaches. We evaluate these models on the
development histories of three large, open-source software systems: Mozilla Firefox, Eclipse Subversive, and Gimp. In all of these applications, we find that LVMs
improve the performance of related file prediction over current leading methods.

1

Introduction

As software systems grow in size and complexity, they become more difficult to develop and maintain. Nowadays, it is not uncommon for a code base to contain source files in multiple programming
languages, text documents with meta information, XML documents for web interfaces, and even
platform-dependent versions of the same application. This complexity creates many challenges because no single developer can be an expert in all things.
One such challenge arises whenever a developer wishes to update one or more files in the code
base. Often, seemingly localized changes will require many parts of the code base to be updated.
Unfortunately, these dependencies can be difficult to detect. Let S denote a set of starter files that
the developer wishes to modify, and let R denote the set of relevant files that require updating after
modifying S. In a large system, where the developer cannot possibly be familiar with the entire code
base, automated tools that can recommend files in R given starter files in S are extremely useful.
A number of automated tools now make recommendations of this sort by mining the development
history of the code base [1, 2]. Work in this area has been facilitated by code versioning systems,
such as CVS or Subversion, which record the development histories of large software projects. In
these histories, transactions denote sets of files that have been jointly modified?that is, whose
changes have been submitted to the code base within a short time interval. Statistical analyses of
past transactions can reveal which files depend on each other and need to be modified together.
In this paper, we explore the use of latent variable models (LVMs) for modeling the development
history of large code bases. We consider a number of different models, including Bernoulli mixture
models, exponential family PCA, restricted Boltzmann machines, and fully Bayesian approaches.
In these models, the problem of recommending relevant files can be viewed as a problem in binary
matrix completion. We present experimental results on the development histories of three large
open-source systems: Mozilla Firefox, Eclipse Subversive, and Gimp. In all of these applications,
we find that LVMs outperform the current leading method for mining development histories.
1

2

Related work

Two broad classes of methods are used for identifying file dependencies in large code bases; one
analyzes the semantic content of the code base while the other analyzes its development history.
2.1

Impact analysis

The field of impact analysis [3] draws on tools from software engineering in order to identify the
consequences of code modifications. Most approaches in this tradition attempt to identify program
dependencies by inspecting and/or running the program itself. Such dependence-based techniques
include transitive traversal of the call graph as well as static [4, 5, 6] and dynamic [7, 8] slicing
techniques. These methods can identify many dependencies; however, they have trouble on certain difficult cases such as cross-language dependencies (e.g., between a data configuration file and
the code that uses it) and cross-program dependencies (e.g., between the front and back ends of a
compiler). These difficulties have led researchers to explore the methods we consider next.
2.2

Mining of development histories

Data-driven methods identify file dependencies in large software projects by analyzing their development histories. Two of the most widely recognized works in this area are by Ying et al. [1] and
Zimmerman et al. [2]. Both groups use frequent itemset mining (FIM) [9], a general heuristic for
identifying frequent patterns in large databases. The patterns extracted from development histories
are just those sets of files that have been jointly modified at some point in the past; the frequent
patterns are the patterns that have occurred at least ? times. The parameter ? is called the minimum
support threshold. In practice, it is tuned to yield the best possible balance of precision and recall.
Given a database and a minimum support threshold, the resulting set of frequent patterns is uniquely
specified. Much work has been devoted to making FIM as fast and efficient as possible. Ying et
al. [1] uses a FIM algorithm called FP-growth, which extracts frequent patterns by using a tree-like
data structure that is cleverly designed to prune the number of possible patterns to be searched. FPgrowth is used to find all frequent patterns that contain the set of starter files; the joint sets of these
frequent patterns are then returned as recommendations. As a baseline in our experiments we use a
variant of FP-growth called FP-Max [10] which outputs only maximal sets for added efficiency.
Zimmerman et al. [2] uses the popular Apriori algorithm [11] (which uses FIM to solve a subtask) to
form association rules from the development history. These rules are of the form x1 ? x2 , where
x1 and x2 are disjoint sets; they indicate that ?if x1 is observed, then based on experience, x2 should
also be observed.? After identifying all rules in which starter files appear on the left hand side, their
tool recommends all files that appear on the right hand side. They also work with content on a finer
granularity, recommending not only relevant files, but also relevant code blocks within files.
Both Ying et al. [1] and Zimmerman et al. [2] evaluate the data-driven approach by its f-measure, as
measured against ?ground-truth? recommendations. For Ying et al. [1], these ground-truth recommendations are the files committed for a completed modification task, as recorded in that project?s
Bugzilla. For Zimmerman et al. [2], the ground-truth recommendations are the files checked-in
together at some point in the past, as revealed by the development history.
Other researchers have also used the development history to detect file dependencies, but in
markedly different ways. Shirabad et al. [12] formulate the problem as one of binary classification; they label pairs of source files as relevant or non-relevant based on their joint modification
histories. Robillard [13] analyzes the topology of structural dependencies between files at the codeblock level. Kagdi et al [14] improve on the accuracy of existing file recommendation methods
by considering asymmetric file dependencies; this information is also used to return a partial ordering over recommended files. Finally, Sherriff et al. [15] identify clusters of dependent files by
performing singular value decomposition on the development history.

3

Latent variable modeling of development histories

We examine four latent variable models of file dependence in software systems. All these models
represent the development history as an N ? D large binary matrix, where non-zero elements in
2

the same row indicate files that were checked-in together or jointly modified at some point in time.
To detect dependent files, we infer the values of missing elements in this matrix from the values of
known elements. The inferences are made from the probability distributions defined by each model.
We use the following notation for all models:
1. The file list F = (f1 , . . . , fD ) is an ordered collection of all files referenced in a static
version of the development history.
2. A transaction is a set of files that were modified together, according to the development history. We represent each transaction by a D-dimensional binary vector x = (x1 , . . . , xD ),
where xi = 1 if the fi is a member of the transaction, and xi = 0 otherwise.
3. A development history D is a set of N transaction vectors {x1 , x2 , . . . , xN }. We assume
them to be independently and identically sampled from some underlying joint distribution.
4. A starter set is a set of s starter files S = (fi1 , . . . , fis ) that the developer wishes to modify.
5. A recommendation set is a set of recommended files R = (fj1 , . . . , fjr ) that we label as
relevant to the starter set S.
3.1

Bernoulli mixture model

The simplest model that we explore is a Bernoulli mixture model (BMM). Figure 1(a) shows
the BMM?s graphical model in plate notation. In training, the observed variables are the D binary elements xi ? {0, 1} of each transaction vector. The hidden variable is a multinomial label z ? {1, 2, . . . , k} that can be viewed as assigning each transaction vector to one of k clusters.
The joint distribution of the BMM is given by:
p(x, z|?, ?) = p(z|?)

D
Y

p(xi |z, ?) = ?z

i=1

D
Y

?xizi (1 ? ?iz )1?xi .

(1)

i=1

As implied by the graph in Fig. 1(a), we model the different elements of x as conditionally independent given the label z. Here, the parameter ?z = p(z|?) denotes the prior probability of the
latent variable z, while the parameter ?iz = p(xi = 1|z, ?) denotes the conditional mean of the
observed variable
Q xi . We use the EM algorithm to estimate parameters that maximize the likelihood
p(D|?, ?) = n p(xn |?, ?) of the transactions in the development history.
When a software developer wishes to modify a set of starter files, she can query a trained BMM
to identify a set of relevant files. Let s = {xi1 , . . . , xis } denote the elements of the transaction
vector indicating the files in the starter set S. Let r denote the D ? s remaining elements of the
transaction vector indicating files that may or may not be relevant. In BMMs, we infer which
files are relevant by computing the posterior probability p(r|s = 1, ?, ?). Using Bayes rule and
conditional independence, this posterior probability is given (up to a constant factor) by:
p(r|s = 1, ?, ?) ?

k
X

p(r|z, ?) p(s = 1|z, ?) p(z|?).

(2)

z=1

The most likely set of relevant files, according to the model, is given by the completed transaction r?
that maximizes the right hand side of eq. (2). Unfortunately, while we can efficiently compute the
posterior probability p(r|s = 1) for a particular set of recommended files, it is not straightforward to
maximize eq. (2) over all 2D?s possible ways to complete the transaction. As an approximation, we
sort the possibly relevant files by their individual posterior probabilities p(xi = 1|s = 1) for fi ?
/ S.
Then we recommend all files whose posterior probabilities p(xi = 1|s = 1) exceed some threshold;
we optimize the threshold on a held-out set of training examples.
3.2

Bayesian Bernoulli mixture model

We also explore a Bayesian treatment of the BMM. In a Bayesian Bernoulli mixture (BBM), instead
of learning point estimates of the parameters {?, ?}, we introduce a prior distribution p(?, ?) and
make predictions by averaging over the posterior distribution p(?, ?|D). The generative model for
the BBM is shown graphically in Figure 1(b).
3

?

?
?

?

?,?

z

?

z

c

V

y

u

W

x

x

(a) BMM.

b

K

N

x

x

N

N

N

(b) BBM.

(c) RBM.

(d) Logistic PCA.

Figure 1: Graphical model of the Bernoulli mixture model (BMM), the Bayesian Bernoulli mixture
(BBM), the restricted Boltzmann machine (RBM), and logistic PCA.
In our BBMs, the mixture weight parameters are drawn from a Dirichlet prior1 :
p(?|?) = Dirichlet (? |?/k, . . . , ?/k ) ,

(3)

where k indicates (as before) the number of mixture components and ? is a hyperparameter of the
Dirichlet prior, the so-called concentration parameter2 . Likewise, the parameters of the k Bernoulli
distributions are drawn from Beta priors:
p(?j |?, ?) = Beta(?j |?, ?),

(4)

where ?j is a D-dimensional vector, and ? and ? are hyperparameters of the Beta prior.
As exact inference in BBMs is intractable, we resort to collapsed Gibbs sampling and make predictions by averaging over samples from the posterior. In particular, we integrate out the Bernoulli
parameters ? and the cluster distribution parameters ?, and we sample the cluster assignment variables z. For Gibbs sampling, we must compute the conditional probability p(zn = j|z?n , D) that
the nth transaction is assigned to cluster j, given the training data D and all other cluster assignments z?n . This probability is given by:

D 
N?nj + ?k Y (? + N?nij )xni (? + N?nj ? N?nij )(1?xni )
p(zn = j|z?n , D) =
, (5)
N ? 1 + ? i=1
? + ? + N?nj
where N?nj counts the number of transactions assigned to cluster j (excluding the nth transaction)
and N?nij counts the number of times that the ith file belongs to one of these N?nj transactions.
(t)

After each full Gibbs sweep, we obtain a sample z(t) (and corresponding counts Nj of the number
(t)

of points assigned to cluster j), which can be used to infer the Bernoulli parameters ?j . We use
T of these samples to estimate the probability that a file xi needs to be changed given files in the
starter set S. In particular, averaging predictions over the T Gibbs samples, we estimate:

?
?
(t)
T
k
p
x
=
1|?
X
X
X
i
j
1
1
(t)
?1

 ? , with ?(t)
p(xi = 1|s = 1) ?
Nj
= (t)
xn .
j
(t)
T t=1 N j=1
N
p s = 1|?
(t)
j

j

n:zn =j

(6)
3.3

Restricted Boltzmann Machines

A restricted Boltzmann machine (RBM) is a Markov random field (MRF) whose nodes are (typically) binary random variables [17]. The graphical model of an RBM is a fully connected bipartite
1

In preliminary experiments, we also investigated an infinite mixture of Bernoulli distributions that replaces
the Dirichlet prior by a Dirichlet process [16]. However, we did not find the infinite mixture model to outperform its finite counterpart, so we do not discuss it further.
2
For simplicity, we assume a symmetric Dirichlet prior, i.e. we assume ?j : ?j = ?/k.

4

graph with D observed variables xi in one layer and k latent variables yj in the other; see Fig. 1(c).
Due to the bipartite structure, the latent variables are conditionally independent given the observed
variables (and vice versa). For the RBMs in this paper, we model the joint distribution as:

1
p(x, y) = exp ?x> Wy ? b> x ? c> y ,
(7)
Z
where W stores the weight matrix between layers, b and c store (respectively) the biases on observed and hidden nodes, and Z is a normalization factor that depends on the model?s parameters.
The product form of RBMs can model much sharper distributions over the observed variables than
mixture models [17], making them an interesting alternative to consider for our application.
RBMs are trained by maximum likelihood estimation. Exact inference in RBMs is intractable due to
the exponential sum in the normalization factor Z. However, the conditional distributions required
for Gibbs sampling have a particularly simple form:
X
X 
p(xi = 1|y) = ?
Wij yj +
cj ,
(8)
j
j
X
X 
p(yj = 1|x) = ?
Wij xi +
bi ,
(9)
i

i

where ?(z) = [1 + e?z ]?1 is the sigmoid function. The obtained Gibbs samples can be used to
approximate the gradient of the likelihood function with respect to the model parameters; see [17,
18] for further discussion of sampling strategies3 .
To determine whether a file fi is relevant given starter files in S, we can either (i) clamp the observed variables representing starter files and perform Gibbs sampling on the rest, or (ii) compute
the posterior over the remaining files using a fast, factorized approximation [19]. In preliminary
experiments, we found the latter to work best. Hence, we recommend files by computing
X

k 
Y
p(xi = 1|s = 1) ? exp(bi )
xj Wj` + Wi` + c`
1 + exp
,
(10)
j:fj ?S

`=1

then thresholding these probabilities on some value determined on held-out examples.
3.4

Logistic PCA

Logistic PCA is a method for dimensionality reduction of binary data; see Fig. 1(d) for its graphical model. Logistic PCA belongs to a family of algorithms known as exponential family PCA;
these algorithms generalize PCA to data modeled by non-Gaussian distributions of the exponential
family [20, 21, 22]. To use logistic PCA, we stack the N transaction vectors xn ? {0, 1}D of the
development history into a N ?D binary matrix X. Then, modeling each element of this matrix as
a Bernoulli random variable, we attempt to find a low-rank factorization of the N ?D real-valued
matrix ? whose elements are the log-odds parameters of these random variables.
The low-rank factorization in logistic PCA is computed by maximizing the log-likelihood of the
observed data X. In terms of the log-odds matrix ?, this log-likelihood is given by:

X
LX (?) =
Xnd log ?(?nd ) + (1 ? Xnd ) log ?(??nd ) .
(11)
nd

We obtain a low dimensional representation of the data by factoring the log-odds matrix ? ? <N ?D
as the product of two smaller matrices U ? <N ?L and V ? <L?D . Specifically, we have:
X
?nd =
Un` V`d .
(12)
`

Note that the reduced rank L  D plays a role analogous to the number of clusters k in BMMs.
After obtaining a low-rank factorization of the log-odds matrix ? = UV, we can use it to recommend relevant files from starter files S = {fi1 , fi2 , . . . , fis }. To recommend relevant files, we
compute the vector u that optimizes the regularized log-loss:
s
X
?
(13)
LS (u) =
log ?(u?vij ) + kuk2 ,
2
j=1
3

We use the approach in [17] known as contrastive divergence with m Gibbs sweeps (CD-m).

5

Time Period
Support
10
15
20
25

Mozilla Firefox
March 2007 - Nov 2007

Eclipse Subversive
Dec 2006 - May 2010

Train
9,579
9,015
8,497
8,021

Train
372
316
282
233

Test
2,666
2,266
1,991
1,771

Files
1,264
778
546
411

Test
114
92
79
59

Files
61
38
30
25

Gimp
Nov 2007 - May 2010
Train
5,359
5,084
4,729
4,469

Test
3,608
3,436
3,208
3,012

Files
1,376
899
600
447

Table 1: Datasets statistics, showing the time period from which transactions were extracted, and
the number of transactions and unique files in the training and test sets (for a single starter file).

where in the first term, v` denotes the `th column of the matrix V, and in the second term, ? is a
regularization parameter. The vector u obtained in this way is the low dimensional representation
of the transaction with starter files in S. To determine whether file fi is relevant, we compute the
probability p(xi = 1|u, V) = ?(u ? vi ) and recommend the file if this probability exceeds some
threshold. (We tune the threshold on held-out transactions from the development history).

4

Experiments

We evaluated our models on three datasets4 constructed from check-in records of Mozilla Firefox,
Eclipse Subversive, and Gimp. These open-source projects use software configuration management
(SCM) tools which provide logs that allow us to extract binary vectors indicating which files were
changed during a transaction. Our experimental setup and results are described below.
4.1

Experimental setup

We preprocess the raw data obtained from SCM?s check-in records in two steps. First, following Ying et al [1], we eliminate all transactions consisting of more than 100 files (as these usually do
not correspond to meaningful changes). Second, we simulate the minimum support threshold (see
Section 2.2) by removing all files in the code base that occur very infrequently. This pruning allows
us to make a fair comparison with latent variable models (LVMs).
After pre-processing, the dataset is chronologically ordered; the first two-thirds is used as training
data, and the last one-third as testing data. For each transaction in the test set, we formed a ?query?
and ?label? set by randomly picking a set of changed files as starter files. The remaining files that
were changed in the transaction form the label set, which is the set of files our models must predict.
Following [1], we only include transactions for which the label set is non-empty in the train data.
Table 1 shows the number of transactions for training and test set, as well as the total number of
unique files that appear in these transactions.
We trained the LVMs as follows. The Bernoulli mixture models (BMMs) were trained by 100
or fewer iterations of the EM algorithm. For the Bayesian mixtures (BBMs), we ran 30 separate
Markov chains and made predictions after 30 full Gibbs sweeps5 . The RBMs were trained for 300
iterations of contrastive divergence (CD), starting with CD-1 and gradually increasing the number
of Gibbs sweeps to CD-9 [17]. The parameters U and V of logistic PCA were learned using an
alternating least squares procedure [21] that converges to a local maximum of the log-likelihood.
We initialized the matrices U and V from an SVD of the matrix X.
The parameters of the LVMs (i.e., number of hidden components in the BMM and RBM, as well
as the number of dimensions and the regularization parameter ? in logistic PCA) were selected
based on the performance on a small held-out validation set. The hyperparameters of the Bayesian
Bernoulli mixtures were set based on prior knowledge from the domain: the Beta-prior parameters ?
and ? were set to 0.005 and 0.95, respectively, to reflect our prior knowledge that most files are not
changed in a transaction. The concentration parameter ? was set to 50 to reflect our prior knowledge
that file dependencies typically form a large number of small clusters.
4
5

These binary datasets publicly available at http://cseweb.ucsd.edu/?dhu/research/msr
In preliminary experiments, we found 30 Gibbs sweeps to be sufficient for the Markov chain to mix.

6

Model Support
10
15
FIM
20
25
10
15
BMM
20
25
10
15
BBM
20
25
10
15
RBM
20
25
10
15
LPCA
20
25

Mozilla Firefox
Start = 1
Start = 3
0.106 0.136 0.112 0.195
0.129 0.144 0.127 0.194
0.115 0.137 0.106 0.186
0.124 0.135 0.110 0.195
0.160 0.189 0.106 0.158
0.160 0.202 0.110 0.141
0.172 0.204 0.120 0.147
0.177 0.218 0.130 0.160
0.196 0.325 0.180 0.376
0.192 0.340 0.180 0.376
0.206 0.355 0.191 0.417
0.197 0.360 0.175 0.391
0.157 0.230 0.069 0.307
0.156 0.246 0.063 0.310
0.169 0.260 0.058 0.324
0.172 0.269 0.088 0.340
0.200 0.249 0.169 0.300
0.182 0.254 0.157 0.295
0.182 0.265 0.156 0.308
0.174 0.277 0.162 0.325

Eclipse Subversive
Start = 1
Start = 3
0.133 0.382 0.234 0.516
0.141 0.461 0.319 0.632
0.177 0.550 0.364 0.672
0.227 0.616 0.360 0.637
0.222 0.433 0.206 0.479
0.181 0.486 0.350 0.489
0.196 0.530 0.403 0.514
0.251 0.566 0.382 0.482
0.257 0.547 0.278 0.700
0.202 0.607 0.374 0.769
0.223 0.655 0.413 0.791
0.262 0.694 0.418 0.756
0.170 0.233 0.090 0.405
0.157 0.238 0.138 0.423
0.174 0.307 0.178 0.531
0.200 0.426 0.259 0.524
0.124 0.415 0.230 0.609
0.138 0.452 0.281 0.615
0.212 0.517 0.325 0.667
0.247 0.605 0.344 0.625

Gimp
Start = 1
Start = 3
0.020 0.116 0.016 0.176
0.014 0.091 0.016 0.159
0.007 0.066 0.013 0.129
0.006 0.057 0.010 0.095
0.129 0.177 0.084 0.152
0.134 0.205 0.085 0.143
0.127 0.207 0.085 0.154
0.117 0.212 0.010 0.131
0.114 0.174 0.104 0.177
0.114 0.200 0.107 0.183
0.114 0.205 0.108 0.187
0.110 0.206 0.103 0.179
0.074 0.137 0.028 0.194
0.080 0.148 0.024 0.205
0.074 0.156 0.027 0.242
0.062 0.143 0.025 0.230
0.123 0.187 0.148 0.263
0.124 0.200 0.145 0.288
0.115 0.222 0.135 0.300
0.100 0.205 0.131 0.230

Table 2: Performance of FIM and LVMs on three datasets for queries with 1 or 3 starter files. Each
shaded column presents the f -measure, and each white column presents the correct prediction ratio.

4.2

Results

Our experiments evaluated the performance of each LVM, as well as a highly efficient implementation of FIM called FP-Max [10]. Several experiments were run on different values of starter files
(abbreviated ?Start?) and minimum support thresholds (abbreviated ?Support?). Table 2 shows the
comparison of each model in terms of the f -measure (the harmonic mean of the precision and recall) and the ?correct prediction ratio,? or CPR (the fraction of files we predict correctly, assuming
that the number of files to be predicted is given). The latter measure reflects how well our models
identify relevant files for a particular starter file, without the added complication of thresholding.
Experiments that achieve the highest result for each of the two measures are boldfaced.
From our results, we see that most LVMs outperform the popular FIM approach. In particular, the
BBMs outperform all other approaches on two of the three datasets, with a high of CPR = 79% in
Eclipse Subversive. This means that an average of 79% of all dependent files are detected as relevant
by the BBM. We also observe that f -measure generally decreases with the addition of starter files
? since the average size of transactions is relatively small (around four files for Firefox), adding
starter files must make predictions less obvious in the case that the total number of relevant files is
not given to us. Increasing support, on the other hand, seems to effectively remove noise caused by
infrequent files. Finally, we see that recommendations are most accurate on Eclipse Subversive, the
smallest dataset. We believe this is because a smaller test set does not require a model to predict as
far into the future as a larger one. Thus, our results suggest that an online learning algorithm may
further increase accuracy.

5

Discussion

The use of LVMs has significant advantages over traditional approaches to impact analysis (see
Section 2), namely its ability to find dependent files written in different languages. To show this, we
present the three clusters with the highest weights, as discovered by a BMM in the Firefox data, in
Table 3. The table reveals that the clusters correspond to interpretable structure in the code that span
multiple data formats and languages. The first cluster deals with the JIT compiler for JavaScript,
while the second and third deal with the CSS style sheet manager and web browser properties. The
dependencies in the last two clusters would have been missed by conventional impact analysis.
7

Cluster 1
js/src/jscntxt.h
js/src/jstracer.cpp
js/src/nanojit/Assembler.cpp
js/src/jsregexp.cpp
js/src/jsapi.cpp
js/src/jsarray.cpp
js/src/jsfun.cpp
js/src/jsinterp.cpp
js/src/jsnum.cpp
js/src/jsobj.cpp

Cluster 2
view/src/nsViewManager.cpp
layout/generic/nsHTMLReflowState.cpp
layout/reftests/bugs/reftest.list
layout/style/nsCSSRuleProcessor.cpp
layout/style/nsCSSStyleSheet.cpp
layout/style/nsCSSParser.cpp
layout/base/crashtests/crashtests.list
layout/base/nsBidiPresUtils.cpp
layout/base/nsPresShell.cpp
content/xbl/src/nsBindingManager.cpp

Cluster 3
browser/base/content/browser-context.inc
browser/base/content/browser.js
browser/base/content/pageinfo/pageInfo.xul
browser/locales/en-US/chrome/browser/browser.dtd
toolkit/mozapps/update/src/nsUpdateService.js.in
toolkit/mozapps/update/src/updater/updater.cpp
modules/plugin/base/src/nsNPAPIPluginInstance.h
modules/plugin/base/src/nsPluginHost.cpp
browser/locales/en-US/chrome/browser/browser.properties
view/src/nsViewManager.cpp

Table 3: Three of the clusters from Firefox, identified by the BMM. We show the clusters with
the largest mixing proportion. Within each cluster, the 10 files with highest membership probabilities are shown; note how these files span multiple data formats and program languages, revealing
dependencies that would escape the notice of traditional methods.

LVMs also have important advantages over FIM. Given a set S of starter files, FIM simply looks at
co-occurrence data; it recommends a set of files R for which the number of transactions that contain
both R and S is frequent. By contrast, LVMs can exploit higher-order information by discovering
the underlying structure of the data. Our results suggest that the ability to leverage such structure
leads to better predictions. Admittedly, in terms of computation, LVMs have a larger one-time
training cost than the FIM, as we must first train the model or generate and store the Gibbs samples.
However, for a single query, the time required to compute recommendations is comparable to that
of the FP-Max algorithm we used for FIM.
The results from the previous section also revealed significant differences between the LVMs we
considered. In the majority of our experiments, mixture models (with many mixture components)
appear to outperform RBMs and logistic PCA. This result suggests that our dataset consists of a large
number of transactions with a number of small, highly interrelated files. Modeling such data with a
product of experts such as an RBM is difficult as each individual expert has the ability to ?veto? a
prediction. We tried to resolve this problem by using a sparsity prior on the states of the hidden units
y to make the RBMs behave more like a mixture model [23], but in preliminary experiments, we
did not find this to improve the performance. Another interesting observation is that the Bayesian
treatment of the Bernoulli mixture model generally leads to better predictions than a maximum
likelihood approach, as it is less susceptible to overfitting. This advantage is particularly useful in
file dependency prediction which requires models with a large number of mixture components to
appropriately model data that consists of many small, distinct clusters while having few training
instances (i.e., transactions).

6

Conclusion

In this paper, we have described a new application of binary matrix completion for predicting file
dependencies in software projects. For this application, we investigated the performance of four
different LVMs and compared our results to that of the widely used of FIM. Our results indicate that
LVMs can significantly outperform FIM by exploiting latent, higher-order structure in the data.
Admittedly, our present study is still limited in scope, and it is very likely that our results can be
further improved. For instance, results from the Netflix competition have shown that blending the
predictions from various models often leads to better performance [24]. The raw transactions also
contain additional information that could be harvested to make more accurate predictions. Such
information includes the identity of users who committed transactions to the code base, as well as
the text of actual changes to the source code. It remains a grand challenge to incorporate all the
available information from development histories into a probabilistic model for predicting which
files need to be modified. In future work, we aim to explore discriminative methods for parameter
estimation, as well as online algorithms for tracking non-stationary trends in the code base.
Acknowledgments
LvdM acknowledges support by the Netherlands Organisation for Scientific Research (grant no.
680.50.0908) and by EU-FP7 NoE on Social Signal Processing (SSPNet).
8

References
[1] A.T.T. Ying, G.C. Murphy, R. Ng, and M.C. Chu-Carroll. Predicting source code changes by mining
change history. IEEE Transactions on Software Engineering, 30(9):574?586, 2004.
[2] T. Zimmerman, P. Weibgerber, S. Diehl, and A. Zeller. Mining version histories to guide software changes.
Proceedings of the 26th International Conference on Software Engineering, pages 563?572, 2004.
[3] R. Arnold and S. Bohner. Software Change Impact Analysis. IEEE Computer Society, 1996.
[4] M. Weiser. Program slicing. In Proceedings of the 5th International Conference on Software Engineering,
pages 439?449, 1981.
[5] S. Horwitz, T. Reps, and D. Binkley. Interprocedural slicing using dependence graphs. ACM Transactions
on Programming Languages and Systems, 12(1):26?60, 1990.
[6] F. Tip. A survey of program slicing techniques. Journal of Programming Languages, 3:121?189, 1995.
[7] B. Korel and J. Laski. Dynamic program slicing. Information Processing Letters, 29(3):155?163, 1988.
[8] X. Zhang, R. Gupta, and Y. Zhang. Precise dynamic slicing algorithms. In Proceedings of the 25th
International Conference on Software Engineering, pages 319?329, 2003.
[9] B. Goethals. Frequent set mining. In The Data Mining and Knowledge Discovery Handbook, pages
377?397, 2005.
[10] G. Grahne and J. Zhu. Efficiently using prefix-trees in mining frequent itemsets. Proceedings of the 1st
ICDM Workshop on Frequent Itemset Mining Implementations, 2003.
[11] M.J. Zaki, S. Parthasarathy, M. Ogihara, and W. Li. New algorithms for fast discovery of association
rules. 1997.
[12] J. S. Shirabad, T. C. Lethbridge, and S. Matwin. Mining the maintenance history of a legacy software
system. Proceedings of the 19th International Conference on Software Maintenance, pages 95?104, 2003.
[13] M. Robillard. Automatic generation of suggestions for program investigation. ACM SIGSOFT International Symposium on Foundations of Software Engineering, 30:11?20, 2005.
[14] H. Kagdi, S. Yusaf, and J.I. Maletic. Mining sequences of changed-files from version histories. Proc. of
Int. Workshop on Mining Software Repositories, pages 47?53, 2006.
[15] M. Sherriff, J.M. Lake, and L. Williams. Empirical software change impact analysis using singular value
decomposition. International Conference on Software Testing, Verification, and Validation, 2008.
[16] R.M. Neal. Markov chain sampling methods for Dirichlet process mixture models. Journal of Computational and Graphical Statistics, 9:249?265, 2000.
[17] G.E. Hinton. Training products of experts by minimizing contrastive divergence. Neural Computation,
14(8):1771?1800, 2002.
[18] T. Tieleman. Training Restricted Boltzmann Machines using approximations to the likelihood gradient. In
Proceedings of the International Conference on Machine Learning, volume 25, pages 1064?1071, 2008.
[19] R.R. Salakhutdinov, A. Mnih, and G.E. Hinton. Restricted Boltzmann Machines for collaborative filtering.
In Proceedings of the 24th International Conference on Machine Learning, pages 791?798, 2007.
[20] M. Collins, S. Dasgupta, and R.E. Schapire. A generalization of principal components analysis to the
exponential family. In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural
Information Processing Systems 14, Cambridge, MA, 2002. MIT Press.
[21] A.I. Schein, L.K. Saul, and L.H. Ungar. A generalized linear model for principal component analysis of
binary data. In Proceedings of the 9th International Workshop on Artificial Intelligence and Statistics,
2003.
[22] I. Rish, G. Grabarnik, G. Cecchi, F. Pereira, and G.J. Gordon. Closed-form supervised dimensionality reduction with generalized linear models. In Proceedings of the 25th International Conference on Machine
learning, pages 832?839, 2008.
[23] M.A. Ranzato, Y.L. Boureau, and Y. LeCun. Sparse feature learning for deep belief networks. In Advances
in Neural Information Processing Systems, pages 1185?1192, 2008.
[24] R.M. Bell and Y. Koren. Lessons from the Netflix prize challenge. ACM SIGKDD Explorations Newsletter, 9(2):75?79, 2007.

9


----------------------------------------------------------------

title: 2131-dynamic-time-alignment-kernel-in-support-vector-machine.pdf

Dynamic Time-Alignment Kernel in
Support Vector Machine

Hiroshi Shimodaira
School of Information Science,
Japan Advanced Institute of
Science and Technology
sim@jaist.ac.jp
Mitsuru Nakai
School of Information Science,
Japan Advanced Institute of
Science and Technology
mit@jaist.ac.jp

Ken-ichi Noma
School of Information Science,
Japan Advanced Institute of
Science and Technology
knoma@jaist.ac.jp
Shigeki Sagayama
Graduate School of Information Science
and Technology,
The University of Tokyo
sagayama@hil.t.u-tokyo.ac.jp

Abstract
A new class of Support Vector Machine (SVM) that is applicable to sequential-pattern recognition such as speech recognition is
developed by incorporating an idea of non-linear time alignment
into the kernel function. Since the time-alignment operation of
sequential pattern is embedded in the new kernel function, standard SVM training and classification algorithms can be employed
without further modifications. The proposed SVM (DTAK-SVM)
is evaluated in speaker-dependent speech recognition experiments
of hand-segmented phoneme recognition. Preliminary experimental results show comparable recognition performance with hidden
Markov models (HMMs).

1

Introduction

Support Vector Machine (SVM) [1] is one of the latest and most successful statistical
pattern classifier that utilizes a kernel technique [2, 3]. The basic form of SVM
classifier which classifies an input vector x ? Rn is expressed as
g(x) =

N
X
i=1

?i yi ?(xi ) ? ?(x) + b =

N
X

?i yi K(xi , x) + b,

(1)

i=1

0

where ? is a non-linear mapping function ?(x) : R n 7? Rn , (n  n0 ), ??? denotes
the inner product operator, xi , yi and ?i are the i-th training sample, its class label,
and its Lagrange multiplier, respectively, K is a kernel function, and b is a bias.
Despite the successful applications of SVM in the field of pattern recognition such
as character recognition and text classification, SVM has not been applied to speech

recognition that much. This is because SVM assumes that each sample is a vector
of fixed dimension, and hence it can not deal with the variable length sequences
directly. Because of this, most of the efforts that have been made so far to apply
SVM to speech recognition employ linear time normalization, where input feature
vector sequences with different lengths are aligned to same length [4]. A variant
of this approach is a hybrid of SVM and HMM (hidden Markov model), in which
HMM works as a pre-processor to feed time-aligned fixed-dimensional vectors to
SVM [5]. Another approach is to utilize probabilistic generative models as a SVM
kernel function. This includes the Fisher kernels [6, 7], and conditional symmetric
independence (CSI) kernels [8], both of which employ HMMs as the generative models. Since HMMs can treat sequential patterns, SVM that employs the generative
models based on HMMs can handle sequential patterns as well.
In contrast to those approaches, our approach is a direct extension of the original
SVM to the case of variable length sequence. The idea is to incorporate the operation of dynamic time alignment into the kernel function itself. Because of this,
the proposed new SVM is called ?Dynamic Time-Alignment Kernel SVM (DTAKSVM)?. Unlike the SVM with Fisher kernel that requires two training stages with
different training criteria, one is for training the generative models and the second
is for training the SVM, the DTAK-SVM uses one training criterion as well as the
original SVM.

2

Dynamic Time-Alignment Kernel

We consider a sequence of vectors X = (x1 , x2 , ? ? ? , x L ), where xi ? Rn , L is the
length of the sequence, and the notation |X| is sometimes used to represent the
length of the sequence instead. For simplification, we at first assume the so-called
linear SVM that does not employ non-linear mapping function ?. In such case, the
kernel operation in (1) is identical to the inner product operation.
2.1

Formulation for linear kernel

Assume that we have two vector sequences X and V . If these two patterns are
equal in length, i.e. |X| = |V | = L, then the inner product between X and V can
be obtained easily as a summation of each inner product between xk and v k for
k = 1, ? ? ? , L:
X ?V

=

L
X

xk ? v k ,

(2)

k=1

and therefore an SVM classifier can be defined as given in (1). On the other hand
in case where the two sequences are different in length, the inner product can not
be calculated directly. Even in such case, however, some sort of inner product like
operation can be defined if we align the lengths of the patterns. To that end, let
?(k), ?(k) be the time-warping functions of normalized time frame k for the pattern
X and V , respectively, and let ??? be the new inner product operator instead of
the original inner product ???. Then the new inner product between the two vector
sequences X and V can be given by
X ?V

=

L
1X
x?(k) ? v ?(k) ,
L

(3)

k=1

where L is a normalized length that can be either |X|, |V | or arbitrary positive
integer.

There would be two possible types of time-warping functions. One is a linear timewarping function and the other is a non-linear time-warping function. The linear
time-warping function takes the form as
?(k) = d(|X|/L)ke,

?(k) = d(|V |/L)ke,

where dxe is the ceiling function which gives the smallest integer that is greater than
or equal to x. As it can be seen from the definition given above, the linear warping
function is not suitable for continuous speech recognition, i.e. frame-synchronous
processing, because the sequence lengths, |X| and |V |, should be known beforehand.
On the other hand, non-linear time warping, or dynamic time warping (DTW) [9] in
other word, enables frame-synchronous processing. Furthermore, the past research
on speech recognition has shown that the recognition performance by the non-linear
time normalization outperforms the one by the linear time normalization. Because
of these reasons, we focus on the non-linear time warping based on DTW.
Though the original DTW uses a distance/distortion measure and finds the optimal path that minimizes the accumulated distance/distortion, the DTW that is
employed for SVM uses inner product or kernel function instead and finds the optimal path that maximizes the accumulated similarity:
X ?V
subject to

=

max
?,?

L
1 X
m(k)x?(k) ? v ?(k) ,
M??

(4)

k=1

1 ? ?(k) ? ?(k + 1) ? |X|, ?(k + 1) ? ?(k) ? Q,
1 ? ?(k) ? ?(k + 1) ? |V |, ?(k + 1) ? ?(k) ? Q,

(5)

where m(k) is a nonnegative (path) weighting coefficient, M?? is a (path) normalizing factor, and Q is a constant constraining the local continuity. In the standard
PL
DTW, the normalizing factor M ?? is given as k=1 m(k), and the weighting coefficients m(k) are chosen so that M?? is independent of the warping functions.

The above optimization problem can be solved efficiently by dynamic programming.
The recursive formula in the dynamic programming employed in the present study
is as follows
(
)
G(i ? 1, j) + Inp(i, j),
G(i, j) = max G(i ? 1, j ? 1) + 2 Inp(i, j),
(6)
G(i, j ? 1) + Inp(i, j),
where Inp(i, j) is the standard inner product between the two vectors corresponding
to point i and j. As a result, we have
X ? V = G(|X|, |V |)/(|X| + |V |).
2.2

(7)

Formulation for non-linear kernel

In the last subsection, a linear kernel, i.e. the inner product, for two vector sequences with different lengths has been formulated in the framework of dynamic
time-warping. With a little constraint, similar formulation is possible for the case
where SVM?s non-linear mapping function ? is applied to the vector sequences. To
that end, ? is restricted to the one having the following form:
?(X) = (?(x1 ), ?(x2 ), ? ? ? , ?(x L )),

(8)

where ? is a non-linear mapping function that is applied to each frame vector x i ,
as given in (1). It should be noted that under the above restriction ? preserves the
original length of sequence at the cost of losing long-term correlations such as the

one between x1 and xL . As a result, a new class of kernel can be defined by using
the extended inner product introduced in the previous section;
Ks (X, V )

=

?(X) ? ?(V )

=

max
?,?

=

max
?,?

1
M??

L
X

(9)
m(k)?(x?(k) ) ? ?(v ?(k) )

(10)

k=1

L
1 X
m(k)K(x?(k) , v?(k) ).
M??

(11)

k=1

We call this new kernel ?dynamic time-alignment kernel (DTAK)?.
2.3

Properties of the dynamic time-alignment kernel

It has not been proven that the proposed function Ks (, ) is really an SVM?s admissible kernel which guarantees the existence of a feature space. This is because that
the mapping function to a feature space is not independent but dependent on the
given vector sequences. Although a class of data-dependent asymmetric kernel for
SVM has been developed in [10], our proposed function is more complicated and
difficult to analyze because the input data is a vector sequence with variable length
and non-linear time normalization is embedded in the function. Instead, what have
been known about the proposed function so far are (1) Ks is symmetric, (2) Ks
satisfies the Cauchy-Schwartz like inequality described bellow:
Proposition 1
Ks (X, V )2 ? Ks (X, X)Ks (V, V )

(12)

Proof For simplification, we assume that normalized length L is fixed, and omit
m(k) and M?? in (11). Using the standard Cauchy-Schwartz inequality, the following inequality holds:
Ks (X, V )

=

?

max
?,?

L
X

L
X

?(x?(k) ) ? ?(v ?(k) ) =

k=1

L
X

?(x?? (k) ) ? ?(v ?? (k) )

(13)

k=1

k ?(x?? (k) ) kk ?(v ?? (k) ) k,

(14)

k=1

where ?? (k), ?? (k) represent the optimal warping functions that maximize the RHS
of (13). On the other hand,
Ks (X, X)

=

max
?,?

L
X

?(x?(k) ) ? ?(x?(k) ) =

k=1

L
X

?(x?+ (k) ) ? ?(x?+ (k) ). (15)

k=1

Because here we assume that ?+ (k), ?+ (k) are the optimal warping functions that
maximize (15), for any warping functions including ? ? (k), the following inequality
holds:
L
L
X
X
Ks (X, X) ?
?(x?? (k) ) ? ?(x?? (k) ) =
k ?(x?? (k) ) k2 .
(16)
k=1

k=1

In the same manner, the following holds:
Ks (V, V )

?

L
X

k=1

?(v ?? (k) ) ? ?(v ?? (k) ) =

L
X

k=1

k ?(v ?? (k) ) k2 .

(17)

Therefore,
Ks (X, X)Ks (V, V ) ? Ks (X, V )2
L
X

?

k ?(x?? (k) ) k

2

k=1

=

L X
L
X

!

L
X

k ?(v ?? (k) ) k

k=1

2

!

L
X

?

k ?(x?? (k) ) kk ?(v ?? (k) ) k

k=1

k ?(x?? (i) ) kk ?(v ?? (j) ) k ? k ?(x?? (j) ) kk ?(v ?? (i) ) k

i=1 j=i+1

2

?0

!2

(18)



3

DTAK-SVM

Using the dynamic time-alignment kernel (DTAK) introduced in the previous section, the discriminant function of SVM for a sequential pattern is expressed as
g(X)

=

N
X

?i yi ?(X (i) ) ? ?(X) + b

(19)

?i yi Ks (X (i) , X) + b,

(20)

i=1

=

N
X
i=1

where X (i) represents the i-th training pattern. As it can be seen from these
expressions, the SVM discriminant function for time sequence has the same form
with the original SVM except for the difference in kernels. It is straightforward to
deduce the learning problem which is given as
N

min

W,b,?i

subject to

X
1
W ?W +C
?i ,
2
i=1
(i)

yi (W ? ?(X ) + b) ? 1 ? ?i ,
?i ? 0, i = 1, ? ? ? , N.

(21)
(22)

Again, since the formulation of learning problem defined above is almost the same
with that for the original SVM, same training algorithms for the original SVM can
be used to solve the problem.

4

Experiments

Speech recognition experiments were carried out to evaluated the classification performance of DTAK-SVM. As our objective is to evaluate the basic performance
of the proposed method, very limited task, hand-segmented phoneme recognition
task in which positions of target patterns in the utterance are known, was chosen.
Continuous speech recognition task that does not require phoneme labeling would
be our next step.
4.1

Experimental conditions

The details of the experimental conditions are given in Table 1. The training
and evaluation samples were collected from the ATR speech database: A-set (5240

Table 1: Experimental conditions

Speaker dependency
Phoneme classes
Speakers
Training samples
Evaluation samples
Signal sampling
Feature values
Kernel type

Experiment-1
Experiment-2
dependent
dependent
6 voiced consonants
5 vowels
5 males
5 males and 5 females
200 samples per phoneme
500 samples per phoneme
2,035 samples in all per 2500 samples in all per
speaker
speaker
12kHz, 10ms frame-shift
13-MFCCs and 13-?MFCCs
kx ?x k2
RBF (radial basis function): K(xi , xj ) = exp(? i ? 2 j )

100

C=0.1
C=1.0
C=10

95
90

# SVs / # training samples [%]

Correct classification rate [%]

100

85
80
75
70
65
60
55
50

C=0.1
C=1.0
C=10.0

80
60
40
20
0

0

2

4

6

8

RBF-sigma

(a) Recognition performance

10

1

2

3

4

5

6

7

8

9

10

RBF-sigma

(b) Number of SVs

Figure 1: Experimental results for Experiment-1 (6 voiced-consonants recognition)
showing (a) correct classification rate and (b) the number of SVs as a function of ?
(the parameter of RBF kernel).

Japanese words in vocabulary). In consonant-recognition task (Experiment-1), only
six voiced-consonants /b,d,g,m,n,N/ were used to save time. The classification task
of those 6 phonemes without using contextual information is considered as a relatively difficult task, whereas the classification of 5 vowels /a,i,u,e,o/ (Experiment-2)
is considered as an easier task.
To apply SVM that is basically formulated as a two-class classifier to the multiclass problem, ?one against the others? type of strategy was chosen. The proposed
DTAK-SVM has been implemented with the publicly available toolkit, SVMTorch
[11].
4.2

Experimental results

Fig. 1 depicts the experimental results for Experiment-1, where average values over
5 speakers are shown. It can be seen in Fig. 1 that the best performance of 95.8%
was achieved at ? = 2.0 and C = 10. Similar results were obtained for Experiment-2
as given in Fig. 2.

100

95

# SVs / # training samples [%]

Correct classification rate [%]

100
90
85
80
75
70
65
60
55
50

80
60
40
20
0

0

2

4

6

8

RBF-sigma

10

1

2

3

4

5

6

7

8

9

10

RBF-sigma

(a) Recognition performance

(b) Number of SVs

Figure 2: Experimental results for Experiment-2 (5 vowels recognition) showing
(a) correct classification rate and (b) the number of SVs as a function of ? (the
parameter of RBF kernel).

Table 2: Recognition performance comparison of DTAK-SVM with HMM. Results
of Experiment-1 for 1 male and 1 female speakers are shown. (numbers represent
correct classification rate [%])
Model
HMM (1 mix.)
HMM (4 mix.)
HMM (8 mix.)
HMM (16 mix.)
DTAK-SVM

# training samples/phoneme
male
female
50
100 200
50
100 200
75.0 69.1 77.1 72.2 65.5 76.6
83.3 84.7 90.9 77.3 76.4 86.4
82.8 87.0 92.4 74.6 79.3 88.5
79.9 85.0 93.2 72.9 78.7 89.8
83.8 85.9 92.1 83.5 81.8 87.7

Next, the classification performance of DTAK-SVM was compared with that of the
state-of-the-art HMM. In order to see the effect of generalization performance on
the size of training data set and model complexity, experiments were carried out
by varying the number of training samples (50, 100, 200), and mixtures (1,4,8,16)
for each state of HMM. The HMM used in this experiment was a 3-states, continuous density, Gaussian-distribution mixtures with diagonal covariances, contextindependent model. HTK [12] was employed for this purpose. The parameters of
DTAK-SVM were fixed to C = 10, ? = 2.0. The results for Experiment-1 with
respect to 1 male and 1 female speakers are given in Table 2.
It can be said from the experimental results that DTAK-SVM shows better classification performance when the number of training samples is 50, while comparable
performance when the number of samples is 200. One might argue that the number
of training samples used in this experiment is not enough at all for HMM to achieve
best performance. But such shortage of training samples occurs often in HMMbased real-world speech recognition, especially when context-dependent models are
employed, which prevents HMM from improving the generalization performance.

5

Conclusions

A novel approach to extend the SVM framework for the sequential-pattern classification problem has been proposed by embedding a dynamic time-alignment operation
into the kernel. Though long-term correlations between the feature vectors are omitted at the cost of achieving frame-synchronous processing for speech recognition, the
proposed DTAK-SVMs demonstrated comparable performance in hand-segmented
phoneme recognition with HMMs. The DTAK-SVM is potentially applicable to
continuous speech recognition with some extension of One-pass search algorithm
[9].

References
[1] V. N. Vapnik, Statistical Learning Theory. Wiley, 1998.
[2] B. Sch?
olkopf, C. J. Burges, and A. J. Smola, eds., Advances in Kernel Methods.
The MIT Press, 1998.
[3] ?Kernel machine website,? 2000. http://www.kernel-machines.org/.
[4] P. Clarkson, ?On the Use of Support Vector Machines for Phonetic Classification,? in ICASSP99, pp. 585?588, 1999.
[5] A. Ganapathiraju and J. Picone, ?Hybrid SVM/HMM architectures for speech
recognition,? in ICSLP2000, 2000.
[6] Tommi S. Jaakkola and David Haussler, ?Exploiting generative models in discriminative classifiers,? in Advances in Neural Information Processing Systems
11 (M. S. Kearns and S. A. Solla and D. A. Cohn, ed.), pp. 487?493, The MIT
Press, 1999.
[7] N. Smith and M. Niranjan, ?Data-dependent Kernels in SVM classification of
speech patterns,? in ICSLP-2000, vol. 1, pp. 297?300, 2000.
[8] C. Watkins, ?Dynamic Alignment Kernels,? in Advances in Large Margin Classifiers (A. J. Smola and P. L. Bartlett and B. Sch?
olkopf and D. Schuurmans,
ed.), ch. 3, pp. 39?50, The MIT Press, 2000.
[9] L. Rabiner and B. Juang, Fundamental of Speech Recognition. Prentice Hall,
1993.
[10] K. Tsuda, ?Support Vector Classifier with Asymmetric Kernel Functions,? in
European Symposium on Artificial Neural Networks (ESANN), pp. 183?188,
1999.
[11] R. Collobert,
?SVMTorch:
A Support Vector Machine for
Large-Scale
Regression
and
Classification
Problems,?
2000.
http://www.idiap.ch/learning/SVMTorch.html.
[12] ?The Hidden Markov Model Toolkit (HTK).? http://htk.eng.cam.ac.uk/.


----------------------------------------------------------------

title: 2434-semi-definite-programming-by-perceptron-learning.pdf

Semidefinite Programming
by Perceptron Learning

Ralf Herbrich
Thore Graepel
Microsoft Research Ltd., Cambridge, UK
{thoreg,rherb}@microsoft.com
Andriy Kharechko
John Shawe-Taylor
Royal Holloway, University of London, UK
{ak03r,jst}@ecs.soton.ac.uk

Abstract
We present a modified version of the perceptron learning algorithm
(PLA) which solves semidefinite programs (SDPs) in polynomial
time. The algorithm is based on the following three observations:
(i) Semidefinite programs are linear programs with infinitely many
(linear) constraints; (ii) every linear program can be solved by a
sequence of constraint satisfaction problems with linear constraints;
(iii) in general, the perceptron learning algorithm solves a constraint
satisfaction problem with linear constraints in finitely many updates.
Combining the PLA with a probabilistic rescaling algorithm (which,
on average, increases the size of the feasable region) results in a probabilistic algorithm for solving SDPs that runs in polynomial time.
We present preliminary results which demonstrate that the algorithm works, but is not competitive with state-of-the-art interior
point methods.

1

Introduction

Semidefinite programming (SDP) is one of the most active research areas in optimisation. Its appeal derives from important applications in combinatorial optimisation
and control theory, from the recent development of efficient algorithms for solving
SDP problems and the depth and elegance of the underlying optimisation theory [14],
which covers linear, quadratic, and second-order cone programming as special cases.
Recently, semidefinite programming has been discovered as a useful toolkit in machine
learning with applications ranging from pattern separation via ellipsoids [4] to kernel
matrix optimisation [5] and transformation invariant learning [6].
Methods for solving SDPs have mostly been developed in an analogy to linear programming. Generalised simplex-like algorithms were developed for SDPs [11], but to
the best of our knowledge are currently merely of theoretical interest. The ellipsoid
method works by searching for a feasible point via repeatedly ?halving? an ellipsoid
that encloses the affine space of constraint matrices such that the centre of the ellipsoid is a feasible point [7]. However, this method shows poor performance in practice

as the running time usually attains its worst-case bound. A third set of methods
for solving SDPs are interior point methods [14]. These methods minimise a linear
function on convex sets provided the sets are endowed with self-concordant barrier
functions. Since such a barrier function is known for SDPs, interior point methods
are currently the most efficient method for solving SDPs in practice.
Considering the great generality of semidefinite programming and the complexity of
state-of-the-art solution methods it is quite surprising that the forty year old simple
perceptron learning algorithm [12] can be modified so as to solve SDPs. In this
paper we present a combination of the perceptron learning algorithm (PLA) with a
rescaling algorithm (originally developed for LPs [3]) that is able to solve semidefinite
programs in polynomial time. We start with a short introduction into semidefinite
programming and the perceptron learning algorithm in Section 2. In Section 3 we
present our main algorithm together with some performance guarantees, whose proofs
we only sketch due to space restrictions. While our numerical results presented in
Section 4 are very preliminary, they do give insights into the workings of the algorithm
and demonstrate that machine learning may have something to offer to the field of
convex optimisation.
For the rest of the paper we denote matrices and vectors by bold face upper and
lower case letters, e.g., A and x. We shall use x := x/ kxk to denote the unit length
vector in the direction of x. The notation A ? 0 is used to denote x0 Ax ? 0 for all
x, that is, A is positive semidefinite.

2
2.1

Learning and Convex Optimisation
Semidefinite Programming

In semidefinite programming a linear objective function is minimised over the image
of an affine transformation of the cone of semidefinite matrices, expressed by linear
matrix inequalities (LMI):
minimise
n
x?R

c0 x

subject to

F (x) := F0 +

n
X

xi Fi ? 0 ,

(1)

i=1

where c ? Rn and Fi ? Rm?m for all i ? {0, . . . , n}. The following proposition shows
that semidefinite programs are a direct generalisation of linear programs.
Proposition 1. Every semidefinite program is a linear program with infinitely many
linear constraints.
Proof. Obviously, the objective function in (1) is linear in x. For any u ? Rm , define
the vector au := (u0 F1 u, . . . , u0 Fn u). Then, the constraints in (1) can be written as
?u ? Rm :

u0 F (x) u ? 0

?u ? Rm :

?
m

This is a linear constraint in x for all u ? R

x0 au ? ?u0 F0 u .

(2)

(of which there are infinitely many).

Since the objective function is linear in x, we can solve an SDP by a sequence of
semidefinite constraint satisfaction problems (CSPs) introducing the additional constraint c0 x ? c0 and varying c0 ? R. Moreover, we have the following proposition.
Proposition 2. Any SDP can be solved by a sequence of homogenised semidefinite
CSPs of the following form:
find

x ? Rn+1

subject to

G (x) :=

n
X
i=0

xi Gi ? 0 .

Algorithm 1 Perceptron Learning Algorithm
Require: A (possibly) infinite set A of vectors a ? Rn
Set t ? 0 and xt = 0
while there exists a ? A such that x0t a ? 0 do
xt+1 = xt + a
t?t+1
end while
return xt
Proof. In order to make F0 and c0 dependent on the optimisation variables, we
introduce an auxiliary variable x0 > 0; the solution to the original problem is given
0
by x?1
0 ? x. Moreover, we can repose the two linear constraints c0 x0 ? c x ? 0 and
x0 > 0 as an LMI using the fact that a block-diagonal matrix is positive (semi)definite
if and only if every block is positive (semi)definite. Thus, the following matrices are
sufficient:
?
?
?
!
F0 0 0
Fi
0 0
0 ?ci 0
G0 = ? 00 c0 0 ? ,
Gi =
.
0
0
0
00 0 1
Given an upper and a lower bound on the objective function, repeated bisection can
be used to determine the solution in O(log 1? ) steps to accuracy ?.
In order to simplify notation, we will assume that n ? n+1 and m ? m+2 whenever
we speak about a semidefinite CSP for an SDP in n variables with Fi ? Rm?m .
2.2

Perceptron Learning Algorithm

The perceptron learning algorithm (PLA) [12] is an online procedure which finds a
linear separation of a set of points from the origin (see Algorithm 1). In machine
learning this algorithm is usually applied to two sets A+1 and A?1 of points labelled
+1 and ?1 by multiplying every data vector ai by its class label1 ; the resulting vector
xt (often referred to as the weight vector in perceptron learning) is then read as the
normal of a hyperplane which separates the sets A+1 and A?1 .
A remarkable property of the perceptron learning algorithm is that the total number
t of updates is independent of the cardinality of A but can be upper bounded simply
in terms of the following quantity
? (A) := maxn ? (A, x) := maxn min a0 x .
x?R

x?R

a?A

This quantity is known as the (normalised) margin of A in the machine learning
community or as the radius of the feasible region in the optimisation community.
It quantifies the radius of the largest ball that can be fitted in the convex region
enclosed by all a ? A (the so-called feasible set). Then, the perceptron convergence
theorem [10] states that t ? ??2 (A).
For the purpose of this paper we observe that Algorithm 1 solves a linear CSP where
the linear constraints are given by the vectors a ? A. Moreover, by the last argument
we have the following proposition.
Proposition 3. If the feasible set has a positive radius, then the perceptron learning
algorithm solves a linear CSP in finitely many steps.
It is worth mentioning that in the last few decades a series of modified PLAs A
have been developed (see [2] for a good overview) which mainly aim at guaranteeing
1

Note that sometimes the update equation is given using the unnormalised vector a.

Algorithm 2 Rescaling algorithm
Require: A maximal number T ? N+ of steps and a parameter ? ? R+
Set y uniformly at random in {z : kzk = 1}
for t = 0, . . . , T do
0
?0a
?u := ?Pun G(?y0)u 2 ? ?? (u ? smallest EV of G (?
Find au such that y
y))
(u G u)
j=1

j

if u does not exists then
Set ?i ? {1, . . . , n} : Gi ? Gi + y i G (y); return y
end if
y ? y ? (y0 au ) au ; t ? t + 1
end for
return unsolved
not only feasibility of the solution xt but also a lower bound on ? (A, xt ). These
guarantees usually come at the price of a slightly larger mistake bound which we
shall denote by M (A, ? (A)), that is, t ? M (A, ? (A)).

3

Semidefinite Programming by Perceptron Learning

If we combine Propositions 1, 2 and 3 together with Equation (2) we obtain a perceptron algorithm that sequentially solves SDPs. However, there remain two problems:
1. How do we find a vector a ? A such that x0 a ? 0?
2. How can we make the running time of this algorithm polynomial in the
description length of the data?2
In order to address the first problem we notice that A in Algorithm 1 is not explicitly
given but is defined by virtue of
A (G1 , . . . , Gn ) := {au := (u0 G1 u, . . . , u0 Gn u) | u ? Rm } .
Hence, finding a vector au ? A such that x0 au ? 0 is equivalent to identifying a
vector u ? Rm such that
n
X
xi u0 Gi u = u0 G (x) u ? 0 .
i=1

One possible way of finding such a vector u (and consequently au ) for the current
solution xt in Algorithm 1 is to calculate the eigenvector corresponding to the smallest
eigenvalue of G (xt ); if this eigenvalue is positive, the algorithm stops and outputs
xt . Note, however, that computationally easier procedures can be applied to find a
suitable u ? Rm (see also Section 4).
The second problem requires us to improve the dependency of the runtime from
O(??2 ) to O(? log(?)). To this end we employ a probabilistic rescaling algorithm
(see Algorithm 2) which was originally developed for LPs [3]. The purpose of this algorithm is to enlarge the feasible region (in terms of ? (A (G1 , . . . , Gn ))) by a constant
factor ?, on average, which would imply a decrease in the number of updates of the
perceptron algorithm exponential in the number of calls to this rescaling algorithm.
This is achieved by running Algorithm 2. If the algorithm does not return unsolved
the rescaling procedure on the Gi has the effect that au changes into au + (y0 au ) y
for every u ? Rm . In order to be able to reconstruct the solution xt to the original
problem, whenever we rescale the Gi we need to remember the vector y used for
rescaling. In Figure 1 we have shown the effect of rescaling for three linear con2
Note that polynomial runtime is only guaranteed if ??2 (A (G1 , . . . , Gn )) is bounded by
a polynomial function of the description length of the data.

Figure 1: Illustration of the rescaling procedure. Shown is the feasible region and
one feasible point before (left) and after (left) rescaling with the feasible point.
straints in R3 . The main idea of Algorithm 2 is to find a vector y that is ?-close to
the current feasible region and hence leads to an increase in its radius when used for
rescaling. The following property holds for Algorithm 2.
1
Theorem 1. Assume Algorithm 2 did not return unsolved. Let ? ? 32n
, ? be the
0
radius of the feasible set before rescaling and ? be the radius of the feasible set after
1
rescaling and assume that ? ? 4n
. Then
?
?
1
1. ?0 ? 1 ? 16n
? with probability at most 34 .
?
?
1
2. ?0 ? 1 + 4n
? with probability at least 14 .
The probabilistic nature of the theorem stems from the fact that the rescaling can
only be shown to increase the size of the feasible region if the (random) initial value
y already points sufficiently closely to the feasible region. A consequence of this theorem is that, on average, the radius increases by ? = (1 + 1/64n) > 1. Algorithm 3
combines rescaling and perceptron learning, which results in a probabilistic polynomial runtime algorithm3 which alternates between calls to Algorithm 1 and 2 . This
algorithm may return infeasible in two cases: either Ti many calls to Algorithm 2
have returned unsolved or L many calls of Algorithm 1 together with rescaling have
not returned a solution. Each of these two conditions can either happen because of
an ?unlucky? draw of y in Algorithm 2 or because ? (A (G1 , . . . , Gn )) is too small.
Following the argument in [3] one can show that for L = ?2048n ? ln (?min ) the total
probability of returning infeasible despite ? (A (G1 , . . . , Gn )) > ?min cannot exceed
exp (?n).

4

Experimental Results

The experiments reported in this section fall into two parts. Our initial aim was
to demonstrate that the method works in practice and to assess its efficacy on a
3
Note that we assume that the optimisation problem in line 3 of Algorithm 2 can be
solved in polynomial time with algorithms such as Newton-Raphson.

Algorithm 3 Positive Definite Perceptron Algorithm
Require: G1 , . . . , Gn ? Rm?m and maximal number of iteration L ? N+
Set B = In
for i = 1, . . . , L do
?
?
1
Call Algorithm 1 for at most M A, 4n
many updates
if Algorithm 1 converged then return Bx
ln(?i )
Set ?i = ?23i2 and Ti = ln
( 34 )
for j = 1, . . . , Ti do
1
Call Algorithm 2 with T = 1024n2 ln (n) and ? = 32n
0
if Algorithm 2 returns y then B ? B (In + yy ); goto the outer for-loop
end for
return infeasible
end for
return infeasible
benchmark example from graph bisection [1].
These experiments would also indicate how competitive the baseline method is when
compared to other solvers. The algorithm was implemented in MATLAB and all of
the experiments were run on 1.7GHz machines. The time taken can be compared
with a standard method SDPT3 [13] partially implemented in C but running under
MATLAB.
We considered benchmark problems arising from semidefinite relaxations to the
MAXCUT problems of weighted graphs, which is posed as finding a maximum weight
bisection of a graph. The benchmark MAXCUT problems have the following relaxed
SDP form (see [8]):
1
10 x subject to ? (diag(C1) ? C) + diag (x) ? 0 ,
minimise
(3)
x?Rn
| 4
{z
} |P {z }
F0

i

x i Fi

where C ? Rn?n is the adjacency matrix of the graph with n vertices.
The benchmark used was ?mcp100? provided by SDPLIB 1.2 [1]. For this problem,
n = 100 and it is known that the optimal value of the objective function equals
226.1574. The baseline method used the bisection approach to identify the critical
value of the objective, referred to throughout this section as c0 .
Figure 2 (left) shows a plot of the time per iteration against the value of c0 for the
first four iterations of the bisection method. As can be seen from the plots the time
taken by the algorithm for each iteration is quite long, with the time of the fourth
iteration being around 19,000 seconds. The initial value of 999 for c0 was found
without an objective constraint and converged within 0.012 secs. The bisection then
started with the lower (infeasible) value of 0 and the upper value of 999. Iteration 1
was run with c0 = 499.5, but the feasible solution had an objective value of 492. This
was found in just 617 secs. The second iteration used a value of c0 = 246 slightly
above the optimum of 226. The third iteration was infeasible but since it was quite
far from the optimum, the algorithm was able to deduce this fact quite quickly. The
final iteration was also infeasible, but much closer to the optimal value. The running
time suffered correspondingly taking 5.36 hours. If we were to continue the next
iteration would also be infeasible but closer to the optimum and so would take even
longer.
The first experiment demonstrated several things. First, that the method does indeed work as predicted; secondly, that the running times are very far from being

1000

4

16000

Time (in sec.)

14000
12000
10000

2

3

8000
6000
4000
2000
0
0

1
100

200

Optimal value

Optimal value

Value of objective function (c0)

18000

300

400

Value of objective function (c0)

500

900
800
700
600
500
400
300
200
0

10

20

30

40

50

Iterations

Figure 2: (Left) Four iterations of the bisection method showing time taken per iteration (outer for loop in Algorithm 3) against the value of the objective constraint.
(Right) Decay of the attained objective function value while iterating through Algorithm 3 with a non-zero threshold of ? = 500.
competitive (SDPT3 takes under 12 seconds to solve this problem) and thirdly that
the running times increase as the value of c0 approaches the optimum with those
iterations that must prove infeasibility being more costly than those that find a solution.
The final observation prompted our first adaptation of the base algorithm. Rather
than perform the search using the bisection method we implemented a non-zero
threshold on the objective constraint (see the while-statement in Algorithm 1). The
value of this threshold is denoted ? , following the notation introduced in [9].
Using a value of ? = 500 ensured that when a feasible solution is found, its objective
value is significantly below that of the objective constraint c0 . Figure 2 (right)
shows the values of c0 as a function of the outer for-loops (iterations); the algorithm
eventually approached its estimate of the optimal value at 228.106. This is within
1% of the optimum, though of course iterations could have been continued. Despite
the clear convergence, using this approach the running time to an accurate estimate
of the solution is still prohibitive because overall the algorithm took approximately
60 hours of CPU time to find its solution.
A profile of the execution, however, revealed that up to 93% of the execution time is
spent in the eigenvalue decomposition to identify u. Observe that we do not need a
minimal eigenvector to perform an update, simply a vector u satisfying
u0 G(x)u < 0

(4)

Cholesky decomposition will either return u satisfying (4) or it will converge indicating that G(x) is psd and Algorithm 1 has converged.

5

Conclusions

Semidefinite programming has interesting applications in machine learning. In turn,
we have shown how a simple learning algorithm can be modified to solve higher
order convex optimisation problems such as semidefinite programs. Although the
experimental results given here suggest the approach is far from computationally
competitive, the insights gained may lead to effective algorithms in concrete applications in the same way that for example SMO is a competitive algorithm for solving
quadratic programming problems arising from support vector machines. While the

optimisation setting leads to the somewhat artificial and inefficient bisection method
the positive definite perceptron algorithm excels at solving positive definite CSPs
as found, e.g., in problems of transformation invariant pattern recognition as solved
by Semidefinite Programming Machines [6]. In future work it will be of interest to
consider the combined primal-dual problem at a predefined level ? of granularity so
as to avoid the necessity of bisection search.
Acknowledgments We would like to thank J. Kandola, J. Dunagan, and A. Ambroladze for interesting discussions. This work was supported by EPSRC under grant
number GR/R55948 and by Microsoft Research Cambridge.

References
[1] B. Borchers. SDPLIB 1.2, A library of semidefinite programming test problems.
Optimization Methods and Software, 11(1):683?690, 1999.
[2] R. O. Duda, P. E. Hart, and D. G. Stork. Pattern Classification and Scene
Analysis. John Wiley and Sons, New York, 2001. Second edition.
[3] J. Dunagan and S. Vempala. A polynomial-time rescaling algorithm for solving
linear programs. Technical Report MSR-TR-02-92, Microsoft Research, 2002.
[4] F. Glineur. Pattern separation via ellipsoids and conic programming. M?emoire
de D.E.A., Facult?e Polytechnique de Mons, Mons, Belgium, Sept. 1998.
[5] T. Graepel. Kernel matrix completion by semidefinite programming. In
J. R. Dorronsoro, editor, Proceedings of the International Conference on Neural Networks, ICANN2002, Lecture Notes in Computer Science, pages 694?699.
Springer, 2002.
[6] T. Graepel and R. Herbrich. Invariant pattern recognition by Semidefinite Programming Machines. In S. Thrun, L. Saul, and B. Sch?olkopf, editors, Advances
in Neural Information Processing Systems 16. MIT Press, 2004.
[7] M. Gr?otschel, L. Lov?
asz, and A. Schrijver. Geometric Algorithms and Combinatorial Optimization, volume 2 of Algorithms and Combinatorics. Springer-Verlag,
1988.
[8] C. Helmberg. Semidefinite programming for combinatorial optimization. Technical Report ZR-00-34, Konrad-Zuse-Zentrum f?
ur Informationstechnik Berlin,
Oct. 2000.
[9] Y. Li, H. Zaragoza, R. Herbrich, J. Shawe-Taylor, and J. Kandola. The perceptron algorithm with uneven margins. In Proceedings of the International
Conference of Machine Learning (ICML?2002), pages 379?386, 2002.
[10] A. B. J. Novikoff. On convergence proofs on perceptrons. In Proceedings of the
Symposium on the Mathematical Theory of Automata, volume 12, pages 615?622.
Polytechnic Institute of Brooklyn, 1962.
[11] G. Pataki. Cone-LP?s and semi-definite programs: facial structure, basic solutions, and the symplex method. Technical Report GSIA, Carnegie Mellon
University, 1995.
[12] F. Rosenblatt. The perceptron: A probabilistic model for information storage
and organization in the brain. Psychological Review, 65(6):386?408, 1958.
[13] K. C. Toh, M. Todd, and R. T?
ut?
unc?
u. SDPT3 ? a MATLAB software package
for semidefinite programming. Technical Report TR1177, Cornell University,
1996.
[14] L. Vandenberghe and S. Boyd. Semidefinite programming. SIAM Review,
38(1):49?95, 1996.


----------------------------------------------------------------

title: 5767-high-dimensional-neural-spike-train-analysis-with-generalized-count-linear-dynamical-systems.pdf

High-dimensional neural spike train analysis with
generalized count linear dynamical systems

Lars Buesing
Department of Statistics
Columbia University
New York, NY 10027
lars@stat.columbia.edu

Yuanjun Gao
Department of Statistics
Columbia University
New York, NY 10027
yg2312@columbia.edu
Krishna V. Shenoy
Department of Electrical Engineering
Stanford University
Stanford, CA 94305
shenoy@stanford.edu

John P. Cunningham
Department of Statistics
Columbia University
New York, NY 10027
jpc2181@columbia.edu

Abstract
Latent factor models have been widely used to analyze simultaneous recordings of
spike trains from large, heterogeneous neural populations. These models assume
the signal of interest in the population is a low-dimensional latent intensity that
evolves over time, which is observed in high dimension via noisy point-process
observations. These techniques have been well used to capture neural correlations
across a population and to provide a smooth, denoised, and concise representation of high-dimensional spiking data. One limitation of many current models
is that the observation model is assumed to be Poisson, which lacks the flexibility to capture under- and over-dispersion that is common in recorded neural data,
thereby introducing bias into estimates of covariance. Here we develop the generalized count linear dynamical system, which relaxes the Poisson assumption by
using a more general exponential family for count data. In addition to containing Poisson, Bernoulli, negative binomial, and other common count distributions
as special cases, we show that this model can be tractably learned by extending recent advances in variational inference techniques. We apply our model to
data from primate motor cortex and demonstrate performance improvements over
state-of-the-art methods, both in capturing the variance structure of the data and
in held-out prediction.

1

Introduction

Many studies and theories in neuroscience posit that high-dimensional populations of neural spike
trains are a noisy observation of some underlying, low-dimensional, and time-varying signal of
interest. As such, over the last decade researchers have developed and used a number of methods
for jointly analyzing populations of simultaneously recorded spike trains, and these techniques have
become a critical part of the neural data analysis toolkit [1]. In the supervised setting, generalized
linear models (GLM) have used stimuli and spiking history as covariates driving the spiking of the
neural population [2, 3, 4, 5]. In the unsupervised setting, latent variable models have been used
to extract low-dimensional hidden structure that captures the variability of the recorded data, both
temporally and across the population of neurons [6, 7, 8, 9, 10, 11].
1

In both these settings, however, a limitation is that spike trains are typically assumed to be conditionally Poisson, given the shared signal [8, 10, 11]. The Poisson assumption, while offering algorithmic
conveniences in many cases, implies the property of equal dispersion: the conditional mean and variance are equal. This well-known property is particularly troublesome in the analysis of neural spike
trains, which are commonly observed to be either over- or under-dispersed [12] (variance greater
than or less than the mean). No doubly stochastic process with a Poisson observation can capture
under-dispersion, and while such a model can capture over-dispersion, it must do so at the cost of
erroneously attributing variance to the latent signal, rather than the observation process.
To allow for deviation from the Poisson assumption, some previous work has instead modeled the
data as Gaussian [7] or using more general renewal process models [13, 14, 15]; the former of
which does not match the count nature of the data and has been found inferior [8], and the latter of
which requires costly inference that has not been extended to the population setting. More general
distributions like the negative binomial have been proposed [16, 17, 18], but again these families do
not generalize to cases of under-dispersion. Furthermore, these more general distributions have not
yet been applied to the important setting of latent variable models.
Here we employ a count-valued exponential family distribution that addresses these needs and includes much previous work as special cases. We call this distribution the generalized count (GC)
distribution [19], and we offer here four main contributions: (i) we introduce the GC distribution and
derive a variety of commonly used distributions that are special cases, using the GLM as a motivating example (?2); (ii) we combine this observation likelihood with a latent linear dynamical systems
prior to form a GC linear dynamical system (GCLDS; ?3); (iii) we develop a variational learning algorithm by extending the current state-of-the-art methods [20] to the GCLDS setting (?3.1); and (iv)
we show in data from the primate motor cortex that the GCLDS model provides superior predictive
performance and in particular captures data covariance better than Poisson models (?4).

2

Generalized count distributions

We define the generalized count distribution as the family of count-valued probability distributions:
pGC (k; ?, g(?)) =

exp(?k + g(k))
, k?N
k!M (?, g(?))

(1)

where ? ? R and the function g : N ? R parameterizes the distribution, and M (?, g(?)) =
P? exp(?k+g(k))
is the normalizing constant. The primary virtue of the GC family is that it recovk=0
k!
ers all common count-valued distributions as special cases and naturally parameterizes many common supervised and unsupervised models (as will be shown); for example, the function g(k) = 0
implies a Poisson distribution with rate parameter ? = exp{?}. Generalizations of the Poisson
distribution have been of interest since at least [21], and the paper [19] introduced the GC family
and proved two additional properties: first, that the expectation of any GC distribution is monotonically increasing in ?, for a fixed g(k); and second ? and perhaps most relevant to this study ?
concave (convex) functions g(?) imply under-dispersed (over-dispersed) GC distributions. Furthermore, often desired features like zero truncation or zero inflation can also be naturally incorporated
by modifying the g(0) value [22, 23]. Thus, with ? controlling the (log) rate of the distribution
and g(?) controlling the ?shape? of the distribution, the GC family provides a rich model class for
capturing the spiking statistics of neural data. Other discrete distribution families do exist, such as
the Conway-Maxwell-Poisson distribution [24] and ordered logistic/probit regression [25], but the
GC family offers a rich exponential family, which makes computation somewhat easier and allows
the g(?) functions to be interpreted.
Figure 1 demonstrates the relevance of modeling dispersion in neural data analysis. The left panel
shows a scatterplot where each point is an individual neuron in a recorded population of neurons
from primate motor cortex (experimental details will be described in ?4). Plotted are the mean and
variance of spiking activity of each neuron; activity is considered in 20ms bins. For reference, the
equi-dispersion line implied by a homogeneous Poisson process is plotted in red, and note further
that all doubly stochastic Poisson models would have an implied dispersion above this Poisson line.
These data clearly demonstrate meaningful under-dispersion, underscoring the need for the present
advance. The right panel demonstrates the appropriateness of the GC model class, showing that a
convex/linear/concave function g(k) will produce the expected over/equal/under-dispersion. Given
2

the left panel, we expect under-dispersed GC distributions to be most relevant, but indeed many
neural datasets also demonstrate over and equi-dispersion [12], highlighting the need for a flexible
observation family.
2

3
2.5
Variance

Variance

1.5
neuron 1

1

Convex g
Linear g
Concave g

2
1.5
1

0.5
0.5

neuron 2

0
0

0.5
1
1.5
Mean firing rate per time bin (20ms)

0
0

2

0.5

1
1.5
Expectation

2

2.5

Figure 1: Left panel: mean firing rate and variance of neurons in primate motor cortex during
the peri-movement period of a reaching experiment (see ?4). The data exhibit under-dispersion,
especially for high firing-rate neurons. The two marked neurons will be analyzed in detail in Figure
2. Right panel: the expectation and variance of the GC distribution with different choices of the
function g
To illustrate the generality of the GC family and to lay the foundation for our unsupervised learning
approach, we consider briefly the case of supervised learning of neural spike train data, where generalized linear models (GLM) have been used extensively [4, 26, 17]. We define GCGLM as that which
models a single neuron with count data yi ? N, and associated covariates xi ? Rp (i = 1, ..., n) as
yi ? GC(?(xi ), g(?)), where ?(xi ) = xi ?.
(2)
Here GC(?, g(?)) denotes a random variable distributed according to (1), ? ? Rp are the regression
coefficients. This GCGLM model is highly general. Table 1 shows that many of the commonly
used count-data models are special cases of GCGLM, by restricting the g(?) function to have certain
parametric form. In addition to this convenient generality, one benefit of our parametrization of the
GC model is that the curvature of g(?) directly measures the extent to which the data deviate from
the Poisson assumption, allowing us to meaningfully interrogate the form of g(?). Note that (2) has
no intercept term because it can be absorbed in the g(?) function as a linear term ?k (see Table 1).
Unlike previous GC work [19], our parameterization implies that maximum likelihood parameter
estimation (MLE) is a tractable convex program, which can be seen by considering:
n
n
X
X
? g?(?)) = arg max
log p(yi ) = arg max
[(xi ?)yi + g(yi ) ? log M (xi ?, g(?))] . (3)
(?,
(?,g(?))

i=1

(?,g(?))

i=1

First note that, although we have to optimize over a function g(?) that is defined on all non-negative
integers, we can exploit the empirical support of the distribution to produce a finite optimization
problem. Namely, for any k ? that is not achieved by any data point yi (i.e., the count #{i|yi =
k ? } = 0), the MLE for g(k ? ) must be ??, and thus we only need to optimize g(k) for k that
have empirical support in the data. Thus g(k) is a finite dimensional vector. To avoid the potential
overfitting caused by truncation of gi (?) beyond the empirical support of the data, we can enforce a
large (finite) support and impose a quadratic penalty on the second difference of g(.), to encourage
linearity in g(?) (which corresponds to a Poisson distribution). Second, note that we can fix g(0) = 0
without loss of generality, which ensures model identifiability. With these constraints, the remaining
g(k) values can be fit as free parameters or as convex-constrained (a set of linear inequalities on g(k);
similarly for concave case). Finally, problem convexity is ensured as all terms are either linear or
linear within the log-sum-exp function M (?), leading to fast optimization algorithms [27].

3

Generalized count linear dynamical system model

With the GC distribution in hand, we now turn to the unsupervised setting, namely coupling the GC
observation model with a latent, low-dimensional dynamical system. Our model is a generalization
3

Table 1: Special cases of GCGLM. For all models, the GCGLM parametrization for ? is only associated with the slope ?(x) = ?x, and the intercept ? is absorbed into the g(?) function. In all
cases we have g(k) = ?? outside the stated support of the distribution. Whenever unspecified, the
support of the distribution and the domain of the g(?) function are non-negative integers N.
Model Name
Logistic regression
(e.g. [25])

Typical Parameterization
exp (k(? + x?))
P (y = k) =
1 + exp(? + x?)
?k
P (y = k) =
exp(??);
k!
? = exp(? + x?)

Poisson regression
(e.g., [4, 26] )
Adjacent category regression
(e.g., [25] )

P (y = k + 1)
= exp(?k + x?)
P (y = k)

GCGLM Parametrization
g(k) = ?k; k = 0, 1
g(k) = ?k

g(k) =

k
X

(?i?1 + log i);

i=1

k =0, 1, ..., K
Negative binomial regression
(e.g., [17, 18])
COM-Poisson regression
(e.g., [24])

(k + r ? 1)!
P (y = k) =
(1 ? p)r pk
k!(r ? 1)!
p = exp(? + x?)
+?
?k X ?j
P (y = k) =
/
?
(k!) j=1 (j!)?

g(k) =?k + log (k + r ? 1)!

g(k) = ?k + (1 ? ?) log k!

? = exp(? + x?)

of linear dynamical systems with Poisson likelihoods (PLDS), which have been extensively used
for analysis of populations of neural spike trains [8, 11, 28, 29]. Denoting yrti as the observed
spike-count of neuron i ? {1, ..., N } at time t ? {1, ..., T } on experimental trial r ? {1, ..., R},
the PLDS assumes that the spike activity of neurons is a noisy Poisson observation of an underlying
low-dimensional latent state xrt ? Rp ,(where p  N ), such that:

	
yrti |xrt ? Poisson exp c>
.
(4)
i xrt + di
>

Here C = [c1 ... cN ] ? RN ?p is the factor loading matrix mapping the latent state xrt to a
log rate, with time and trial invariant baseline log rate d ? RN . Thus the vector Cxrt + d denotes
the vector of log rates for trial r and time t. Critically, the latent state xrt can be interpreted as the
underlying signal of interest that acts as the ?common input signal? to all neurons, which is modeled
a priori as a linear Gaussian dynamical system (to capture temporal correlations):
xr1 ? N (?1 , Q1 )
xr(t+1) |xrt ? N (Axrt + bt , Q),

(5)

where ?1 ? Rp and Q1 ? Rp?p parameterize the initial state. The transition matrix A ? Rp?p
and innovations covariance Q ? Rp?p parameterize the dynamical state update. The optional term
bt ? Rp allows the model to capture a time-varying firing rate that is fixed across experimental
trials. The PLDS has been widely used and has been shown to outperform other models in terms of
predictive performance, including in particular the simpler Gaussian linear dynamical system [8].
The PLDS model is naturally extended to what we term the generalized count linear dynamical
system (GCLDS) by modifying equation (4) using a GC likelihood:

yrti |xrt ? GC c>
(6)
i xrt , gi (?) .
Where gi (?) is the g(?) function in (1) that models the dispersion for neuron i. Similar to the GLM,
for identifiability, the baseline rate parameter d is dropped in (6) and we can fix g(0) = 0. As with
the GCGLM, one can recover preexisting models, such as an LDS with a Bernoulli observation, as
special cases of GCLDS (see Table 1).
3.1

Inference and learning in GCLDS

As is common in LDS models, we use expectation-maximization to learn parameters ? =
{A, {bt }t , Q, Q1 , ?1 , {gi (?)}i , C} . Because the required expectations do not admit a closed form
4

as in previous similar work [8, 30], we required an additional approximation step, which we implemented via a variational lower bound. Here we briefly outline this algorithm and our novel
contributions, and we refer the reader to the full details in the supplementary materials.
First, each E-step requires calculating p(xr |yr , ?) for each trial r ? {1, ..., R} (the conditional distribution of the latent trajectories xr = {xrt }1?t?T , given observations yr = {yrti }1?t?T,1?i?N
and parameter ?). For ease of notation below we drop the trial index r. These posterior distributions are intractable, and in the usual way we make a normal approximation p(x|y, ?) ? q(x) =
N (m, V ). We identify the optimal (m, V ) by maximizing a variational Bayesian lower bound (the
so-called evidence lower bound or ?ELBO?) over the variational parameters m, V as:
 

p(x|?)
L(m, V ) =Eq(x) log
+ Eq(x) [log p(y|x, ?)]
(7)
q(x)
 X
1
= log |V | ? tr[??1 V ] ? (m ? ?)T ??1 (m ? ?) +
Eq(xt ) [log p(yti |xt )] + const,
2
t,i
which is the usual form to be maximized in a variational Bayesian EM (VBEM) algorithm [11]. Here
? ? RpT and ? ? RpT ?pT are the expectation and variance of x given by the LDS prior in (5). The
first term of (7) is the negative Kullback-Leibler divergence between the variational distribution and
prior distribution, encouraging the variational distribution to be close to the prior. The second term
involving the GC likelihood encourages the variational distribution to explain the observations well.
The integrations in the second term are intractable (this is in contrast to the PLDS case, where all
integrals can be calculated analytically [11]). Below we use the ideas of [20] to derive a tractable,
further lower bound. Here the term Eq(xt ) [log p(yti |xt )] can be reduced to:
Eq(xt ) [log p(yti |xt )] =Eq(?ti ) [log pGC (y|?ti , gi (?))]
"
=Eq(?ti )

#
K
X
(8)
1
exp(k?ti + gi (k)) ,
yti ?ti + gi (yti ) ? log yti ! ? log
k!
k=0

where ?ti = cTi xt . Denoting P
?tik = k?ti + gi (k) ? log(k!) = kcTi xt + gi (k) ? log k!, (8) is
reduced to Eq(?) [?tiyti ? log( 0?k?K exp(?tik ))]. Since ?tik is a linear transformation of xt ,
under the variational distribution ?tik is also normally distributed ?tik ? N (htik , ?tik ). We have
htik = kcTi mt +gi (k)?log k!, ?tik = k 2 cTi Vt ci , where (mt , Vt ) are the expectation and covariance
matrix of xt under variational distribution. Now we can derive a lower bound for the expectation by
Jensen?s inequality:
"
#
K
X
X
Eq(?ti ) ?tiyti ? log
exp(?tik ) ?htiyti ? log
exp(htik + ?tik /2) =: fti (hti , ?ti ). (9)
k

k=1

Combining (7) and (9), we get a tractable variational lower bound:
 
 X
p(x|?)
?
L(m, V ) ? L (m, V ) = Eq(x) log
+
fti (hti , ?ti ).
q(x)
t,i

(10)

For computational convenience, we complete the E-step by maximizing the new evidence lower
bound L? via its dual [20]. Full details are derived in the supplementary materials.
The M-step then requires maximization of L? over ?. Similar to the PLDS case, the set of parameters involving the latent Gaussian dynamics (A, {bt }t , Q, Q1 , ?1 ) can be optimized analytically [8].
Then, the parameters involving the GC likelihood (C, {gi }i ) can be optimized efficiently via convex
optimization techniques [27] (full details in supplementary material).
In practice we initialize our VBEM algorithm with a Laplace-EM algorithm, and we initialize each
E-step in VBEM with a Laplace approximation, which empirically gives substantial runtime advantages, and always produces a sensible optimum. With the above steps, we have a fully specified
learning and inference algorithm, which we now use to analyze real neural data. Code can be found
at https://bitbucket.org/mackelab/pop_spike_dyn.
5

4

Experimental results

We analyze recordings of populations of neurons in the primate motor cortex during a reaching
experiment (G20040123), details of which have been described previously [7, 8]. In brief, a rhesus
macaque monkey executed 56 cued reaches from a central target to 14 peripheral targets. Before the
subject was cued to move (the go cue), it was given a preparatory period to plan the upcoming reach.
Each trial was thus separated into two temporal epochs, each of which has been suggested to have
their own meaningful dynamical structure [9, 31]. We separately analyze these two periods: the
preparatory period (1200ms period preceding the go cue), and the reaching period (50ms before to
370ms after the movement onset). We analyzed data across all 14 reach targets, and results were
highly similar; in the following for simplicity we show results for a single reaching target (one 56
trial dataset). Spike trains were simultaneously recorded from 96 electrodes (using a Blackrock
multi-electrode array). We bin neural activity at 20ms. To include only units with robust activity, we
remove all units with mean rates less than 1 spike per second on average, resulting in 81 units for the
preparatory period, and 85 units for the reaching period. As we have already shown in Figure 1, the
reaching period data are strongly under-dispersed, even absent conditioning on the latent dynamics
(implying further under-dispersion in the observation noise). Data during the preparatory period are
particularly interesting due to its clear cross-correlation structure.
To fully assess the GCLDS model, we analyze four LDS models ? (i) GCLDS-full: a separate function gi (?) is fitted for each neuron i ? {1, ..., N }; (ii) GCLDS-simple: a single function g(?) is shared
across all neurons (up to a linear term modulating the baseline firing rate); (iii) GCLDS-linear: a
truncated linear function gi (?) is fitted, which corresponds to truncated-Poisson observations; and
(iv) PLDS: the Poisson case is recovered when gi (?) is a linear function on all nonnegative integers.
In all cases we use the learning and inference of ?3.1. We initialize the PLDS using nuclear norm
minimization [10], and initialize the GCLDS models with the fitted PLDS. For all models we vary
the latent dimension p from 2 to 8.
To demonstrate the generality of the GCLDS and verify our algorithmic implementation, we first
considered extensive simulated data with different GCLDS parameters (not shown). In all cases
GCLDS model outperformed PLDS in terms of negative log-likelihood (NLL) on test data, with
high statistical significance. We also compared the algorithms on PLDS data and found very similar performance between GCLDS and PLDS, implying that GCLDS does not significantly overfit,
despite the additional free parameters and computation due to the g(?) functions.
Analysis of the reaching period. Figure 2 compares the fits of the two neural units highlighted
in Figure 1. These two neurons are particularly high-firing (during the reaching period), and thus
should be most indicative of the differences between the PLDS and GCLDS models. The left column
of Figure 2 shows the fitted g(?) functions the for four LDS models being compared. It is apparent in
both the GCLDS-full and GCLDS-simple cases that the fitted g function is concave (though it was
not constrained to be so), agreeing with the under-dispersion observed in Figure 1.
The middle column of Figure 2 shows that all four cases produce models that fit the mean activity of
these two neurons very well. The black trace shows the empirical mean of the observed data, and all
four lines (highly overlapping and thus not entirely visible) follow that empirical mean closely. This
result is confirmatory that the GCLDS matches the mean and the current state-of-the-art PLDS.
More importantly, we have noted the key feature of the GCLDS is matching the dispersion of the
data, and thus we expect it should outperform the PLDS in fitting variance. The right column of
Figure 2 shows this to be the case: the PLDS significantly overestimates the variance of the data.
The GCLDS-full model tracks the empirical variance quite closely in both neurons. The GCLDSlinear result shows that only adding truncation does not materially improve the estimate of variance
and dispersion: the dotted blue trace is quite far from the true data in black, and indeed it is quite
close to the Poisson case. The GCLDS-simple still outperforms the PLDS case, but it does not
model the dispersion as effectively as the GPLDS-full case where each neuron has its own dispersion
parameter (as Figure 1 suggests). The natural next question is whether this outperformance is simply
in these two illustrative neurons, or if it is a population effect. Figure 3 shows that indeed the
population is much better modeled by the GCLDS model than by competing alternatives. The left
and middle panels of Figure 3 show leave-one-neuron-out prediction error of the LDS models. For
each reaching target we use 4-fold cross-validation and the results are averaged across all 14 reaching
6

2.5

2.5

2

Mean

g(k)

0
?2

Variance

3

neuron 1

2
1.5

1.5
1

?4
1
0
5
k (spikes per bin)
neuron 2

0.5

?4
0

Variance

Mean

g(k)

observed data
PLDS
GCLDS?full
GCLDS?simple
GCLDS?linear

1

0
5
k (spikes per bin)

0
100
200
300
Time after movement onset (ms)

1.5

1.5

0
?2

0.5

0
100
200
300
Time after movement onset (ms)

1
0.5
0

0
100
200
300
Time after movement onset (ms)

0
100
200
300
Time after movement onset (ms)

Figure 2: Examples of fitting result for selected high-firing neurons. Each row corresponds to one
neuron as marked in left panel of Figure 1 ? left column: fitted g(?) using GCLDS and PLDS; middle
and right column: fitted mean and variance of PLDS and GCLDS. See text for details.

11.5

PLDS
GCLDS?full
GCLDS?simple
GCLDS?linear

11

10.5

2

4
6
Latent dimension

8

9

2

8

1.5

Fitted variance

% NLL reduction

% MSE reduction

12

7

6

5

1

0.5
PLDS
GCLDS?full

2

4
6
Latent dimension

8

0
0

1
Observed variance

2

Figure 3: Goodness-of-fit for monkey data during the reaching period ? left panel: percentage
reduction of mean-squared-error (MSE) compared to the baseline (homogeneous Poisson process);
middle panel: percentage reduction of predictive negative log likelihood (NLL) compared to the
baseline; right panel: fitted variance of PLDS and GCLDS for all neurons compared to the observed
data. Each point gives the observed and fitted variance of a single neuron, averaged across time.
targets. Critically, these predictions are made for all neurons in the population. To give informative
performance metrics, we defined baseline performance as a straightforward, homogeneous Poisson
process for each neuron, and compare the LDS models with the baseline using percentage reduction
of mean-squared-error and negative log likelihood (thus higher error reduction numbers imply better
performance). The mean-squared-error (MSE; left panel) shows that the GCLDS offers a minor
improvement (reduction in MSE) beyond what is achieved by the PLDS. Though these standard
error bars suggest an insignificant result, a paired t-test is indeed significant (p < 10?8 ). Nonetheless
this minor result agrees with the middle column of Figure 2, since predictive MSE is essentially a
measurement of the mean.
In the middle panel of Figure 3, we see that the GCLDS-full significantly outperforms alternatives
in predictive log likelihood across the population (p < 10?10 , paired t-test). Again this largely
agrees with the implication of Figure 2, as negative log likelihood measures both the accuracy of
mean and variance. The right panel of Figure 3 shows that the GCLDS fits the variance of the data
exceptionally well across the population, unlike the PLDS.
Analysis of the preparatory period. To augment the data analysis, we also considered the
preparatory period of neural activity. When we repeated the analyses of Figure 3 on this dataset,
the same results occurred: the GCLDS model produced concave (or close to concave) g functions
7

and outperformed the PLDS model both in predictive MSE (minority) and negative log likelihood
(significantly). For brevity we do not show this analysis here. Instead, we here compare the temporal
cross-covariance, which is also a common analysis of interest in neural data analysis [8, 16, 32] and,
as noted, is particularly salient in preparatory activity. Figure 4 shows that GCLDS model fits both
the temporal cross-covariance (left panel) and variance (right panel) considerably better than PLDS,
which overestimates both quantities.
?3

x 10

Covariance

8

1
recorded data
GCLDS?full
PLDS

0.8
Fitted variance

10

6
4
2

0.6
0.4
0.2

0
?200

?100

0
100
Time lag (ms)

0
0

200

PLDS
GCLDS?full
0.2
0.4
0.6
Observed variance

0.8

Figure 4: Goodness-of-fit for monkey data during the preparatory period ? Left panel: Temporal
cross-covariance averaged over all 81 units during the preparatory period, compared to the fitted
cross-covariance by PLDS and GCLDS-full. Right panel: fitted variance of PLDS and GCLDS-full
for all neurons compared to the observed data (averaged across time).

5

Discussion

In this paper we showed that the GC family better captures the conditional variability of neural
spiking data, and further improves inference of key features of interest in the data. We note that
it is straightforward to incorporate external stimuli and spike history in the model as covariates, as
has been done previously in the Poisson case [8]. Beyond the GCGLM and GCLDS, the GC family
is also extensible to other models that have been used in this setting, such as exponential family
PCA [10] and subspace clustering [11]. The cost of this performance, compared to the PLDS, is an
extra parameterization (the gi (?) functions) and the corresponding algorithmic complexity. While
we showed that there seems to be no empirical sacrifice to doing so, it is likely that data with few
examples and reasonably Poisson dispersion may cause GCLDS to overfit.
Acknowledgments
JPC received funding from a Sloan Research Fellowship, the Simons Foundation (SCGB#325171
and SCGB#325233), the Grossman Center at Columbia University, and the Gatsby Charitable Trust.
Thanks to Byron Yu, Gopal Santhanam and Stephen Ryu for providing the cortical data.
References
[1] J. P. Cunningham and B. M Yu, ?Dimensionality reduction for large-scale neural recordings,? Nature
neuroscience, vol. 17, no. 71, pp. 1500?1509, 2014.
[2] L. Paninski, ?Maximum likelihood estimation of cascade point-process neural encoding models,? Network: Computation in Neural Systems, vol. 15, no. 4, pp. 243?262, 2004.
[3] W. Truccolo, U. T. Eden, M. R. Fellows, J. P. Donoghue, and E. N. Brown, ?A point process framework
for relating neural spiking activity to spiking history, neural ensemble, and extrinsic covariate effects,?
Journal of neurophysiology, vol. 93, no. 2, pp. 1074?1089, 2005.
[4] J. W. Pillow, J. Shlens, L. Paninski, A. Sher, A. M. Litke, E. Chichilnisky, and E. P. Simoncelli, ?Spatiotemporal correlations and visual signalling in a complete neuronal population,? Nature, vol. 454, no. 7207,
pp. 995?999, 2008.
[5] M. Vidne, Y. Ahmadian, J. Shlens, J. W. Pillow, J. Kulkarni, A. M. Litke, E. Chichilnisky, E. Simoncelli,
and L. Paninski, ?Modeling the impact of common noise inputs on the network activity of retinal ganglion
cells,? Journal of computational neuroscience, vol. 33, no. 1, pp. 97?121, 2012.

8

[6] J. E. Kulkarni and L. Paninski, ?Common-input models for multiple neural spike-train data,? Network:
Computation in Neural Systems, vol. 18, no. 4, pp. 375?407, 2007.
[7] B. M Yu, J. P. Cunningham, G. Santhanam, S. I. Ryu, K. V. Shenoy, and M. Sahani, ?Gaussian-process
factor analysis for low-dimensional single-trial analysis of neural population activity,? in NIPS, pp. 1881?
1888, 2009.
[8] J. H. Macke, L. Buesing, J. P. Cunningham, B. M Yu, K. V. Shenoy, and M. Sahani, ?Empirical models
of spiking in neural populations,? in NIPS, pp. 1350?1358, 2011.
[9] B. Petreska, B. M Yu, J. P. Cunningham, G. Santhanam, S. I. Ryu, K. V. Shenoy, and M. Sahani, ?Dynamical segmentation of single trials from population neural data,? in NIPS, pp. 756?764, 2011.
[10] D. Pfau, E. A. Pnevmatikakis, and L. Paninski, ?Robust learning of low-dimensional dynamics from large
neural ensembles,? in NIPS, pp. 2391?2399, 2013.
[11] L. Buesing, T. A. Machado, J. P. Cunningham, and L. Paninski, ?Clustered factor analysis of multineuronal spike data,? in NIPS, pp. 3500?3508, 2014.
[12] M. M. Churchland, B. M Yu, J. P. Cunningham, L. P. Sugrue, M. R. Cohen, G. S. Corrado, W. T.
Newsome, A. M. Clark, P. Hosseini, B. B. Scott, et al., ?Stimulus onset quenches neural variability:
a widespread cortical phenomenon,? Nature neuroscience, vol. 13, no. 3, pp. 369?378, 2010.
[13] J. P. Cunningham, B. M Yu, K. V. Shenoy, and S. Maneesh, ?Inferring neural firing rates from spike trains
using gaussian processes,? in NIPS, pp. 329?336, 2007.
[14] R. P. Adams, I. Murray, and D. J. MacKay, ?Tractable nonparametric bayesian inference in poisson processes with gaussian process intensities,? in ICML, pp. 9?16, ACM, 2009.
[15] S. Koyama, ?On the spike train variability characterized by variance-to-mean power relationship,? Neural
computation, 2015.
[16] R. L. Goris, J. A. Movshon, and E. P. Simoncelli, ?Partitioning neuronal variability,? Nature neuroscience,
vol. 17, no. 6, pp. 858?865, 2014.
[17] J. Scott and J. W. Pillow, ?Fully bayesian inference for neural models with negative-binomial spiking,? in
NIPS, pp. 1898?1906, 2012.
[18] S. W. Linderman, R. Adams, and J. Pillow, ?Inferring structured connectivity from spike trains under
negative-binomial generalized linear models,? COSYNE, 2015.
[19] J. del Castillo and M. P?erez-Casany, ?Overdispersed and underdispersed poisson generalizations,? Journal
of Statistical Planning and Inference, vol. 134, no. 2, pp. 486?500, 2005.
[20] M. Emtiyaz Khan, A. Aravkin, M. Friedlander, and M. Seeger, ?Fast dual variational inference for nonconjugate latent gaussian models,? in ICML, pp. 951?959, 2013.
[21] C. R. Rao, ?On discrete distributions arising out of methods of ascertainment,? Sankhy?a: The Indian
Journal of Statistics, Series A, pp. 311?324, 1965.
[22] D. Lambert, ?Zero-inflated poisson regression, with an application to defects in manufacturing,? Technometrics, vol. 34, no. 1, pp. 1?14, 1992.
[23] J. Singh, ?A characterization of positive poisson distribution and its statistical application,? SIAM Journal
on Applied Mathematics, vol. 34, no. 3, pp. 545?548, 1978.
[24] K. F. Sellers and G. Shmueli, ?A flexible regression model for count data,? The Annals of Applied Statistics, pp. 943?961, 2010.
[25] C. V. Ananth and D. G. Kleinbaum, ?Regression models for ordinal responses: a review of methods and
applications.,? International journal of epidemiology, vol. 26, no. 6, pp. 1323?1333, 1997.
[26] L. Paninski, J. Pillow, and J. Lewi, ?Statistical models for neural encoding, decoding, and optimal stimulus
design,? Progress in brain research, vol. 165, pp. 493?507, 2007.
[27] S. Boyd and L. Vandenberghe, Convex optimization. Cambridge university press, 2009.
[28] L. Buesing, J. H. Macke, and M. Sahani, ?Learning stable, regularised latent models of neural population
dynamics,? Network: Computation in Neural Systems, vol. 23, no. 1-2, pp. 24?47, 2012.
[29] L. Buesing, J. H. Macke, and M. Sahani, ?Estimating state and parameters in state-space models of spike
trains,? in Advanced State Space Methods for Neural and Clinical Data, Cambridge Univ Press., 2015.
[30] V. Lawhern, W. Wu, N. Hatsopoulos, and L. Paninski, ?Population decoding of motor cortical activity
using a generalized linear model with hidden states,? Journal of neuroscience methods, vol. 189, no. 2,
pp. 267?280, 2010.
[31] M. M. Churchland, J. P. Cunningham, M. T. Kaufman, J. D. Foster, P. Nuyujukian, S. I. Ryu, and K. V.
Shenoy, ?Neural population dynamics during reaching,? Nature, vol. 487, no. 7405, pp. 51?56, 2012.
[32] M. R. Cohen and A. Kohn, ?Measuring and interpreting neuronal correlations,? Nature neuroscience,
vol. 14, no. 7, pp. 811?819, 2011.

9


----------------------------------------------------------------

title: 1992-spectral-relaxation-for-k-means-clustering.pdf

Spectral Relaxation for K-means
Clustering

Hongyuan Zha & Xiaofeng He
Dept. of Compo Sci. & Eng.
The Pennsylvania State University
University Park, PA 16802
{zha,xhe}@cse.psu.edu

Chris Ding & Horst Simon
NERSC Division
Lawrence Berkeley National Lab.
UC Berkeley, Berkeley, CA 94720
{chqding,hdsimon}@lbl.gov

Ming Gu
Dept. of Mathematics
UC Berkeley, Berkeley, CA 95472
mgu@math.berkeley.edu

Abstract
The popular K-means clustering partitions a data set by minimizing a sum-of-squares cost function. A coordinate descend method
is then used to find local minima. In this paper we show that the
minimization can be reformulated as a trace maximization problem
associated with the Gram matrix of the data vectors. Furthermore,
we show that a relaxed version of the trace maximization problem
possesses global optimal solutions which can be obtained by computing a partial eigendecomposition of the Gram matrix, and the
cluster assignment for each data vectors can be found by computing a pivoted QR decomposition of the eigenvector matrix. As a
by-product we also derive a lower bound for the minimum of the
sum-of-squares cost function.

1

Introduction

K-means is a very popular method for general clustering [6]. In K-means clusters
are represented by centers of mass of their members, and it can be shown that the
K-means algorithm of alternating between assigning cluster membership for each
data vector to the nearest cluster center and computing the center of each cluster
as the centroid of its member data vectors is equivalent to finding the minimum of a
sum-of-squares cost function using coordinate descend. Despite the popularity of Kmeans clustering, one of its major drawbacks is that the coordinate descend search
method is prone to local minima. Much research has been done on computing refined
initial points and adding explicit constraints to the sum-of-squares cost function for
K-means clustering so that the search can converge to better local minimum [1 ,2].
In this paper we tackle the problem from a different angle: we find an equivalent
formulation of the sum-of-squares minimization as a trace maximization problem
with special constraints; relaxing the constraints leads to a maximization problem

that possesses optimal global solutions. As a by-product we also have an easily
computable lower bound for the minimum of the sum-of-squares cost function. Our
work is inspired by [9, 3] where connection to Gram matrix and extension of Kmeans method to general Mercer kernels were investigated.
The rest of the paper is organized as follows: in section 2, we derive the equivalent
trace maximization formulation and discuss its spectral relaxation. In section 3, we
discuss how to assign cluster membership using pivoted QR decomposition, taking
into account the special structure of the partial eigenvector matrix. Finally, in
section 4, we illustrate the performance of the clustering algorithms using document
clustering as an example.

Notation. Throughout, II . II denotes the Euclidean norm of a vector. The trace
of a matrix A, i.e., the sum of its diagonal elements, is denoted as trace(A). The
Frobenius norm of a matrix IIAIIF = Jtrace(AT A). In denotes identity matrix of
order n.

2

Spectral Relaxation

Given a set of m-dimensional data vectors ai, i = 1, ... ,n, we form the m-by-n data
matrix A = [a1,"" an]. A partition II of the date vectors can be written in the
following form
(1)
where E is a permutation matrix, and Ai is m-by-si, i.e., the ith cluster contains
the data vectors in A. For a given partition II in (1), the associated sum-of-squares
cost function is defined as
k
Si
Si
ss(II) =
Ila~i) - mi11 2 , m?'l = "a(i)ls?
~
S
2,
s=l
i=l s=l

LL

i.e., mi is the mean vector of the data vectors in cluster i. Let e be a vector
of appropriate dimension with all elements equal to one, it is easy to see that
mi = Aiel Si and
Si
SSi ==
Ila~i) - mil1 2 = IIAi - mieTII} = IIAi(Isi - ee T ISi)II}?
s=l

L

Notice that lSi - ee T I Si is a projection matrix and (Isi - ee T I Si)2 = lSi - ee T lSi,
it follows that
SSi

= trace(Ai(Isi

- ee T I si)Af)

= trace((Isi

- ee T I si)AT Ai).

Therefore,
ss(II) =

t, t,
SSi =

(trace(AT Ai) -

(~) AT Ai (~) )

.

Let the n-by-k orthonormal matrix X be

X =

:~ (e lVsl
Sk

elVSi.

(2)

The sum-of-squares cost function can now be written as
ss(II) = trace(A T A) - trace(XT AT AX),
and its minimization is equivalent to
max{ trace(XT AT AX)

I

X of the form in (2)}.

REMARK. Without loss of generality, let E = I in (1). If we let Xi be the cluster
indicator vector, i.e.,
xT = [0, ... ,0,1, ... ,1,0, .. . ,0].
'---v-----"
Si

Then it is easy to see that
trace(XT AT AX) =

t

t

xT AT AXi =
II Ax il1 2
i=l
XTXi
i=l II x il1 2

Using the partition in (1), the right-hand side of the above can be written as

a weighted sum of the squared Euclidean norms of the mean vector of each clusters.
REMARK. If we consider the elements of the Gram matrix AT A as measuring
similarity between data vectors, then we have shown that Euclidean distance leads
to Euclidean inner-product similarity. This inner-product can be replaced by a
general Mercer kernel as is done in [9, 3].

Ignoring the special structure of X and let it be an arbitrary orthonormal matrix,
we obtain a relaxed maximization problem
max trace(XT AT AX)

(3)

XTX=h

It turns out the above trace maximization problem has a closed-form solution.
Theorem. (Ky Fan) Let H be a symmetric matrix with eigenvalues

Al ::::: A2 ::::: ... ::::: An,
and the corresponding eigenvectors U = [Ul, .. . , Un]. Then

Al

+ ... Ak

=

max trace(XT H X) .

XTX=I k

Moreover, the optimal X* is given by X* = [Ul' ... ' Uk]Q with Q an arbitrary
orthogonal matrix.

It follows from the above theorem that we need to compute the largest k eigenvectors
of the Gram matrix AT A. As a by-product, we have
min{m ,n}

minss(II) ::::: trace(A T A) n

max trace(XT AT AX) =
XT X=h

L

i=k+l

0-; (A),

(4)

where oi(A) is the i largest singular value of A. This gives a lower bound for the
minimum of the sum-of-squares cost function.

It is easy to see from the above derivation that we can replace A with
A - aeT , where a is an arbitrary vector. Then we have the following lower bound

REMARK.

min{m,n}

mJnss(II) ::::: m~

L

u;(A - aeT

).

i=k+l
One might also try the following approach: notice that

REMARK.

IIAi -

",
mi eT2
IIF = 2S1
i ~

'"

~

Ilaj -

aj'11 2 .

aj EAi aj' EAi

Let W = (

Ilai - ajl12 )i,j=l'

and and Xi = [Xij]j=l with
1

Xij = {

o

if aj E Ai
otherwise

Then
k

ss(II) =

T

~ ' " Xi WXi > ~ min
2 ~ XT Xi - 2 ZT Z=h
i=l "

ZTWZ =

~
2

n

'"
~

Ai(W).

i=n-k+l

Unfortunately, some of the smallest eigenvalues of W can be negative.
Let X k be the n-by-k matrix consisting of the k largest eigenvectors of AT A. Each
row of X k corresponds to a data vector , and the above process can be considered as
transforming the original data vectors which live in a m-dimensional space to new
data vectors which now live in a k-dimensional space. One might be attempted to
compute the cluster assignment by applying the ordinary K-means method to those
data vectors in the reduced dimension space. In the next section, we discuss an
alternative that takes into account the structure of the eigenvector matrix X k [5].
REMARK. The similarity of the projection process to principal component analysis
is deceiving: the goal here is not to reconstruct the data matrix using a low-rank
approximation but rather to capture its cluster structure.

3

Cluster Assignment Using Pivoted QR Decomposition

Without loss of generality, let us assume that the best partition of the data vectors in A that minimizes ss(II) is given by A = [AI"'" A k], each submatrix Ai
corresponding to a cluster. Now write the Gram matrix of A as

ATA=[A~A'

~

ArA,

1+ E=:B+E.

o
0
ArAk
If the overlaps among the clusters represented by the submatrices Ai are small, then
the norm of E will be small as compare with the block diagonal matrix B in the
above equation. Let the largest eigenvector of AT Ai be Yi , and
AT AiYi = fJiYi ,
then the columns of the matrix

IIYil1

= 1,

i = 1, ... , k,

span an invariant subspace of B. Let the eigenvalues and eigenvectors of AT A be
A1:::: A2:::: ... :::: An,

AT AXi = AiXi,

i = 1, ... ,n.

Assume that there is a gap between the two eigenvalue sets {fl1,'" flk} and
{Ak+1 , '" An}, i.e. ,

o< J =

min{lfli - Aj II i = 1, ... ,k, j = k + 1, ... ,n}.

Then Davis-Kahan sin(0) theorem states that IlynXk+1,'" ,xn]11
Theorem 3.4]. After some manipulation, it can be shown that
X k == [Xl, ... , Xk]

< IIEII/J [11,

= YkV + O(IIEII) ,

where V is an k-by-k orthogonal matrix. Ignoring the O(IIEII) term, we see that
v

v

cluster 1

cluster k

where we have used y'[ = [Yil , ... ,Yis.], and VT = [V1' ... ,Vk]. A key observation is
that all the Vi are orthogonal to each other: once we have selected a Vi, we can jump
to other clusters by looking at the orthogonal complement of Vi' Also notice that
IIYil1 = 1, so the elements of Yi can not be all small. A robust implementation of
the above idea can be obtained as follows: we pick a column of X k T which has the
lar;est norm, say, it belongs to cluster i , we orthogonalize the rest of the columns of
X k against this column. For the columns belonging to cluster i the residual vector
will have small norm, and for the other columns the residual vectors will tend to
be not small. We then pick another vector with the largest residual norm, and
orthogonalize the other residual vectors against this residual vector. The process
can be carried out k steps, and it turns out to be exactly QR decomposition with
column pivoting applied to X k T [4], i.e., we find a permutation matrix P such that

X'[P = QR = Q[Rl1,Rd,
where Q is a k-by-k orthogonal matrix, and Rl1 is a k-by-k upper triangular matrix.
We then compute the matrix

R=

Rj} [Rl1 ' Rd pT = [Ik' Rj} R12]PT.

Then the cluster membership of each data vector is determined by the row index of
the largest element in absolute value of the corresponding column of k
REMARK. Sometimes it may be advantageous to include more than k eigenvectors
to form Xs T with s > k. We can still use QR decomposition with column pivoting
to select k columns of Xs T to form an s-by-k matrix, say X. Then for each column
z of Xs T we compute the least squares solution of t* = argmintERk li z - Xtll. Then
the cluster membership of z is determined by the row index of the largest element
in absolute value of t* .

4

Experimental Results

In this section we present our experimental results on clustering a dataset of newsgroup articles submitted to 20 newsgroups.1 This dataset contains about 20,000
articles (email messages) evenly divided among the 20 newsgroups. We list the
names of the news groups together with the associated group labels.
lThe newsgroup dataset together with the bow toolkit for processing it can be downloadedfrorn http : //www . cs.cmu.edu/afs/cs/project/theo-ll/www/naive-bayes.html.

0?~.5

0.55

0.6

0.65

0.7

0.75

0.8

0.85

0.9

0.95

0?1 L,

-~--,c-----O~
' _--,c-'-_~-----'
p-Kmeans

p-{)R

Figure 1: Clustering accuracy for five newsgroups NG2/NG9/NG10/NG15/NG18:
p-QR vs. p-Kmeans (left) and p-Kmeans vs. Kmeans (right)

NG1: alt.atheism
NG2: comp.graphics
NG3: comp.os.ms-vindovs.misc
NG4: comp.sys.ibm.pc.hardvare
NG5:comp.sys.mac.hardvare
NG6: comp.vindovs.x
NG7:misc.forsale
NG8: rec.autos
NG9:rec.motorcycles
NG10: rec.sport.baseball
NGll:rec.sport.hockey
NG12: sci. crypt
NG13:sci.electronics
NG14: sci.med
NG15:sci.space
NG16: soc.religion.christian
NG17:talk.politics.guns
NG18: talk.politics.mideast
NG19:talk.politics.misc
NG20: talk.religion.misc

We used the bow toolkit to construct the term-document matrix for this dataset,
specifically we use the tokenization option so that the UseNet headers are stripped,
and we also applied stemming [8]. The following three preprocessing steps are done:
1) we apply the usual tf.idf weighting scheme; 2) we delete words that appear too
few times; 3) we normalized each document vector to have unit Euclidean length.
We tested three clustering algorithms: 1) p-QR, this refers to the algorithm using
the eigenvector matrix followed by pivoted QR decomposition for cluster membership assignment; 2) p-Kmeans, we compute the eigenvector matrix, and then apply
K-means on the rows of the eigenvector matrix; 3) K-means, this is K-means directly
applied to the original data vectors. For both K-means methods, we start with a set
of cluster centers chosen randomly from the (projected) data vectors, and we aslo
make sure that the same random set is used for both for comparison. To assess the
quality of a clustering algorithm, we take advantage of the fact that the news group
data are already labeled and we measure the performance by the accuracy of the
clustering algorithm against the document category labels [10]. In particular, for a
k cluster case, we compute a k-by-k confusion matrix C = [Cij] with Cij the number
of documents in cluster i that belongs to newsgroup category j. It is actually quite
subtle to compute the accuracy using the confusion matrix because we do not know
which cluster matches which newsgroup category. An optimal way is to solve the
following maximization problem
max{ trace(CP)

IP

is a permutation matrix},

and divide the maximum by the total number of documents to get the accuracy.
This is equivalent to finding perfect matching a complete weighted bipartite graph,
one can use Kuhn-Munkres algorithm [7]. In all our experiments, we used a greedy
algorithm to compute a sub-optimal solution.

Table 1: Comparison of p-QR, p-Kmeans, and K-means for two-way clustering
Newsgroups
NG1/NG2
NG2/NG3
NG8/NG9
NG10/NG11
NG1/NG 15
NG18/NG19

89.29 ?
62.37 ?
75.88 ?
73.32 ?
73.32 ?
63.86 ?

p-QR
7.51 %
8.39%
8.88%
9. 08%
9.08%
6.09%

p-Kmeans
89.62 ? 6.90%
63.84 ? 8.74%
77.64 ? 9.00%
74.86 ? 8.89%
74.86 ? 8.89%
64.04 ? 7.23%

K-means
76.25 ? 13.06%
61.62 ? 8.03%
65.65 ? 9.26%
62.04 ? 8.61%
62 .04 ? 8.61%
63.66 ? 8.48%

Table 2: Comparison of p-QR, p-Kmeans, and K-means for multi-way clustering
Newsgroups
NG2/NG3/NG4/NG5/NG6 (50)
NG2/NG3/NG4/NG5/NG6 UOO)
NG2/NG9/NG10/NG15/NG18 l50j
NG2/NG9/NG10/NG15/NG18 (100)
NG1/NG5/NG7/NG8/NG 11/
(50)
NG12/NG13/NG14/NG15/NG17
NG1/NG5/NG7 /NG8/NG 11/
(100)
NG12/NG13/NG14/NG15/NG17

p-QR
5.17%
5.06%
9.26%
9.90%

?
?
?
?
60.21 ? 4.88%
40.36
41.67
77.83
79.91

65.08

? 5.14%

p-Kmeans
41.15 ? 5.73%
42.53 ? 5.02%
70.13 ? 11.67%
75.56 ? 10.63%

K-means
35.77 ? 5.19%
37.20 ? 4.39%
58.10 ? 9.60%
66.37 ? 10.89%

58.18

? 4.41%

40 .18 ? 4.64%

58.99

? 5.22%

48 .33

? 5.64%

1. In this example, we look at binary clustering. We choose 50 random
document vectors each from two newsgroups. We tested 100 runs for each pair
of newsgroups, and list the means and standard deviations in Table 1. The two
clustering algorithms p-QR and p-Kmeans are comparable to each other, and both
are better and sometimes substantially better t han K-means.

EXAMPLE

2. In this example, we consider k-way clustering with k = 5 and k = 10.
Three news group sets are chosen with 50 and 100 random samples from each newsgroup as indicated in the parenthesis. Again 100 runs are used for each tests and the
means and standard deviations are listed in Table 2. Moreover, in Figure 1, we also
plot the accuracy for the 100 runs for the test NG2/NG9/NG10/NG15/NG18 (50).
Both p-QR and p-Kmeans perform better t han Kmeans. For news group sets with
small overlaps, p-QR performs better t han p-Kmeans. This might be explained by
t he fact t hat p-QR explores the special structure of t he eigenvector matrix and is
therefore more efficient. As a less thorough comparison wit h the information bottleneck method used in [10], there for 15 runs of NG2/NG9/NGlO/NG15/NG 18 (100)
mean accuracy 56.67% with maximum accuracy 67.00% is obtained. For 15 runs
of the 10 newsgroup set with 50 samples, mean accuracy 35.00% with maximum
accuracy about 40.00% is obtained.

EXAMPLE

3. We compare the lower bound given in (4). We only list a typical
sample from NG2/NG9/NGlO/NG15/NG18 (50). The column with "NG labels"
indicates clustering using the newsgroup labels and by definition has 100% accuracy.
It is quite clear that the news group categories are not completely captured by
t he sum-of-squares cost function because p-QR and "NG labels" both have higher
accuracy but also larger sum-of-squares values. Interestingly, it seems t hat p-QR
captures some of this information of the newsgroup categories.
EXAMPLE

accuracy
ssm)

p-QR
86.80%
224.1110

p-Kmeans
83.60%
223.8966

K-means
57.60%
228.8416

NG labels
100%
224.4040

lower bound

N/A
219.0266

Acknowledgments

This work was supported in part by NSF grant CCR-9901986 and by Department
of Energy through an LBL LDRD fund.

References
[1] P. S. Bradley and Usama M. Fayyad. (1998). R efining Initial Points for K-Means
Clustering. Proc. 15th International Conf. on Machine Learning, 91- 99.
[2] P. S. Bradley, K. Bennett and A. Demiritz. Constrained K-means Clustering. Microsoft Research, MSR-TR-2000-65, 2000.
[3] M. Girolani. (2001). Mercer Kernel Based Clustering in Feature Space. To appear in
IEEE Transactions on Neural Networks.
[4] G. Golub and C. Van Loan . (1996) . Matrix Computations. Johns Hopkins University
Press, 3rd Edition.
[5] Ming Gu, Hongyuan Zha, Chris Ding, Xiaofeng He and Horst Simon. (2001) . Spectral
Embedding for K- Way Graph Clustering. Technical Report, Department of Computer
Science and Engineering, CSE-OI-007, Pennsylvania State University.
[6] J.A. Hartigan and M.A. Wong. (1979). A K-means Clustering Algorithm. Applied
Statistics, 28:100- 108.
[7] L. Lovasz and M.D. Plummer. (1986) Matching Theory. Amsterdam: North Holland.
[8] A. McCallum. Bow: A toolkit for statistical language modeling, text retrieval, classification and clustering. http : //www . CS. cmu. edu/ mccallum/bow.
[9] B. Schi:ilkopf, A. Smola and K.R. Miiller. (1998). Nonlinear Component Analysis as
a Kernel Eigenvalue Problem. N eural Computation, 10: 1299- 1219.
[10] N . Slonim and N. Tishby. (2000). Document clustering using word clusters via the
information bottleneck method. Proceedings of SIGIR-2000.
[11] G.W. Stewart and J.G. Sun. (1990). Matrix Perturbation Theory. Academic Press,
San Diego , CA.


----------------------------------------------------------------

title: 3963-learning-concept-graphs-from-text-with-stick-breaking-priors.pdf

Learning Concept Graphs from Text with
Stick-Breaking Priors

Padhraic Smyth
Department of Computer Science
University of California, Irvine
Irvine, CA 92607
smyth@ics.uci.edu

America L. Chambers
Department of Computer Science
University of California, Irvine
Irvine, CA 92697
ahollowa@ics.uci.edu

Mark Steyvers
Department of Cognitive Science
University of California, Irvine
Irvine, CA 92697
mark.steyvers@uci.edu

Abstract
We present a generative probabilistic model for learning general graph structures,
which we term concept graphs, from text. Concept graphs provide a visual summary of the thematic content of a collection of documents?a task that is difficult
to accomplish using only keyword search. The proposed model can learn different
types of concept graph structures and is capable of utilizing partial prior knowledge about graph structure as well as labeled documents. We describe a generative
model that is based on a stick-breaking process for graphs, and a Markov Chain
Monte Carlo inference procedure. Experiments on simulated data show that the
model can recover known graph structure when learning in both unsupervised and
semi-supervised modes. We also show that the proposed model is competitive
in terms of empirical log likelihood with existing structure-based topic models
(hPAM and hLDA) on real-world text data sets. Finally, we illustrate the application of the model to the problem of updating Wikipedia category graphs.

1

Introduction

We present a generative probabilistic model for learning concept graphs from text. We define a
concept graph as a rooted, directed graph where the nodes represent thematic units (called concepts)
and the edges represent relationships between concepts. Concept graphs are useful for summarizing
document collections and providing a visualization of the thematic content and structure of large
document sets - a task that is difficult to accomplish using only keyword search. An example of
a concept graph is Wikipedia?s category graph1 . Figure 1 shows a small portion of the Wikipedia
category graph rooted at the category M ACHINE LEARNING2 . From the graph we can quickly infer that the collection of machine learning articles in Wikipedia focuses primarily on evolutionary
algorithms and Markov models with less emphasis on other aspects of machine learning such as
Bayesian networks and kernel methods.
The problem we address in this paper is that of learning a concept graph given a collection of
documents where (optionally) we may have concept labels for the documents and an initial graph
structure. In the latter scenario, the task is to identify additional concepts in the corpus that are
1
2

http://en.wikipedia.org/wiki/Category:Main topic classifications
As of May 5, 2009

1

Applied
Sciences

Software
Engineering
Mathematical
Sciences

Computer
Programming

Applied
Mathematics

Formal
Sciences

Computing

Probability and
Statistics

Philosophy
By field

Thought

Knowledge
Sharing
Algorithms

Society

Cognition

Education
Computational Statistics

Philosophy
Of mind

Artificial
Intelligence

Knowledge

Statistics

Cognitive
Science

Metaphysics

Computer
Science

Learning

Machine learning

Figure 1: A portion of the Wikipedia category supergraph for the node M ACHINE LEARNING

Machine
Learning

Bayesian
Networks

Ensemble
Learning

Classification
Algorithms

Genetic
Algorithms

Evolutionary
Algorithms

Kernel
Methods

Genetic
Programming

Interactive
Evolutionary
Computation

Learning in
Computer
Vision

Markov
Models

Markov
Networks

Statistical
Natural
Language
Processing

Figure 2: A portion of the Wikipedia category subgraph rooted at the node M ACHINE LEARNING

not reflected in the graph or additional relationships between concepts in the corpus (via the cooccurrence of concepts in documents) that are not reflected in the graph. This is particularly suited
for document collections like Wikipedia where the set of articles is changing at such a fast rate
that an automatic method for updating the concept graph may be preferable to manual editing or
re-learning the hierarchy from scratch. The foundation of our approach is latent Dirichlet allocation
(LDA) [1]. LDA is a probabilistic model for automatically identifying topics within a document
collection where a topic is a probability distribution over words. The standard LDA model does
not include any notion of relationships, or dependence, between topics. In contrast, methods such
as the hierarchical topic model (hLDA) [2] learn a set of topics in the form of a tree structure. The
restriction to tree structures however is not well suited for large document collections like Wikipedia.
Figure 1 gives an example of the highly non-tree like nature of the Wikipedia category graph. The
hierarchical Pachinko allocation model (hPAM) [3] is able to learn a set of topics arranged in a fixedsized graph with a nonparametric version introduced in [4]. The model we propose in this paper is
a simpler alternative to hPAM and nonparametric hPAM that can achieve the same flexibility (i.e.
learning arbitrary directed acyclic graphs over a possibly infinite number of nodes) within a simpler
probabilistic framework. In addition, our model provides a formal mechanism for utilizing labeled
data and existing concept graph structures. Other methods for creating concept graphs include the
use of techniques such as hierarchical clustering, pattern mining and formal concept analysis to
construct ontologies from document collections [5, 6, 7]. Our approach differs in that we utilize
a probabilistic framework which enables us (for example) to make inferences about concepts and
documents. Our primary novel contribution is the introduction of a flexible probabilistic framework
for learning general graph structures from text that is capable of utilizing both unlabeled documents
as well as labeled documents and prior knowledge in the form of existing graph structures.
In the next section we introduce the stick-breaking distribution and show how it can be used as
a prior for graph structures. We then introduce our generative model and explain how it can be
adapted for the case where we have an initial graph structure. We derive collapsed Gibbs? sampling
equations for our model and present a series of experiments on simulated and real text data. We
compare our performance against hLDA and hPAM as baselines. We conclude with a discussion of
the merits and limitations of our approach.
2

2

Stick-breaking Distributions

Stick-breaking distributions P(?) are discrete probability distributions of the form:
P(?) =

?
X

?j ?xj (?)

where

j=1

?
X

?j = 1, 0 ? ?j ? 1

j=1

and ?xj (?) is the delta function centered at the atom xj . The xj variables are sampled independently
from a base distribution H (where H is assumed to be continuous). The stick-breaking weights ?j
have the form
j?1
Y
? 1 = v1 ,
? j = vj
(1 ? vk ) for j = 2, 3, . . . , ?
k=1

where the vj are independent Beta(?j , ?j ) random variables. Stick-breaking distributions derive
their name from the analogy of repeatedly breaking the remainder of a unit-length stick at a randomly
chosen breakpoint. See [8] for more details.
Unlike the Chinese restaurant process, the stick-breaking process lacks exchangeability. The probability of sampling a particular cluster from P(?) given the sequences {xj } and {vj } is not equal
to the probability of sampling the same cluster given a permutation of the sequences {x?(j) } and
{v?(j) }. This can be seen in Equation 2 where the probability of sampling xj depends upon the
value of the j ? 1 proceeding Beta random variables {v1 , v2 , . . . , vj?1 }. If we fix xj and permute
every other atom, then the probability of sampling xj changes: it is now determined by the Beta
random variables {v?(1) , v?(2) , . . . , v?(j?1) }.
The stick-breaking distribution can be utilized as a prior distribution on graph structures. We construct a prior on graph structures by specifying a distribution at each node (denoted as Pt ) that
governs the probability of transitioning from node t to another node in the graph. There is some
freedom in choosing Pt ; however we have two constraints. First, making a new transition must have
non-zero probability. In Figure 1 it is clear that from M ACHINE L EARNING we should be able to
transition to any of its children. However we may discover evidence for passing directly to a leaf
node such as S TATISTICAL NATURAL L ANGUAGE P ROCESSING (e.g. if we observe new articles
related to statistical natural language processing that do not use Markov models). Second, making
a transition to a new node must have non-zero probability. For example, we may observe new articles related to the topic of Bioinformatics. In this case, we want to add a new node to the graph
(B IOINFORMATICS) and assign some probability of transitioning to it from other nodes.
With these two requirements we can now provide a formal definition for Pt . We begin with an
initial graph structure G0 with t = 1 . . . T nodes. For each node t we define a feasible set Ft as the
collection of nodes to which t can transition. The feasible set may contain the children of node t or
possible child nodes of node t (as discussed above). In general, Ft is some subset of the nodes in
G0 . We add a special node called the ?exit node? to Ft . If we sample the exit node then we exit
from the graph instead of transitioning forward. We define Pt as a stick-breaking distribution over
the finite set of nodes Ft where the remaining probability mass is assigned to an infinite set of new
nodes (nodes that exist but have not yet been observed). The exact form of Pt is shown below.
Pt (?) =

|Ft |
X

?tj ?ftj (?) +

j=1

?
X

?tj ?xtj (?)

j=|Ft |+1

The first |Ft | atoms of the stick-breaking distribution are the feasible nodes ftj ? Ft . The remaining
atoms are unidentifiable nodes that have yet to be observed (denoted as xtj for simplicity).
This is not yet a working definition unless we explicitly state which nodes are in the set Ft . Our
model does not in general assume any specific form for Ft . Instead, the user is free to define it as
they like. In our experiments, we first assign each node to a unique depth and then define Ft as any
node at the next lower depth. The choice of Ft determines the type of graph structures that can be
learned. For the choice of Ft used in this paper, edges that traverse multiple depths are not allowed
and edges between nodes at the same depth are not allowed. This prevents cycles from forming
and allows inference to be performed in a timely manner. More generally, one could extend the
definition of Ft to include any node at a lower depth.
3

1. For node t ? {1, . . . , ?}
i. Sample stick-break weights {vtj }|?, ? ? Beta(?, ?)
ii. Sample word distribution ?t |? ? Dirichlet(?)
2. For document d ? {1, 2, . . . D}
i. Sample a distribution over levels ?d |a, b ? Beta(a,b)
ii. Sample path pd ? {Pt }?
t=1
iii. For word i ? {1, 2, . . . , Nd }
Sample level ld,i ? TruncatedDiscrete(?d )
Generate word xd,i |{pd , ld,i , ?} ? Multinomial(?pd [ldi ] )
Figure 3: Generative process for GraphLDA

Due to a lack of exchangeability, we must specify the stick-breaking order of the elements in Ft .
Note that despite the order, the elements of Ft always occur before the infinite set of new nodes in
the stick-breaking permutation. We use a Metropolis-Hastings sampler proposed by [10] to learn
the permutation of feasible nodes with the highest likelihood given the data.

3

Generative Process

Figure 3 shows the generative process for our proposed model, which we refer to as GraphLDA.
We observe a collection of documents d = 1 . . . D where document d has Nd words. As discussed
earlier, each node t is associated with a stick-breaking prior Pt . In addition, we associate with each
node a multinomial distribution ?t over words in the fashion of topic models.
A two-stage process is used to generate document d. First, a path through the graph is sampled
from the stick-breaking distributions. We denote this path as pd . The i + 1st node in the path
is sampled from Ppdi (?) which is the stick-breaking distribution at the ith node in the path. This
process continues until an exit node is sampled. Then for each word xi a level in the path, ldi , is
sampled from a truncated discrete distribution. The word xi is generated by the topic at level ldi
of the path pd which we denote as pd [ldi ]. In the case where we observe labeled documents and an
initial graph structure the paths for document d is restricted to end at the concept label of document
d.
One possible option for the length distribution is a multinomial distribution over levels. We take
a different approach and instead use a parametric smooth form. The motivation is to constrain the
length distribution to have the same general functional form across documents (in contrast to the relatively unconstrained multinomial), but to allow the parameters of the distribution to be documentspecific. We considered two simple options: Geometric and Poisson (both truncated to the number
of possible levels). In initial experiments the Geometric performed better than the Poisson, so the
Geometric was used in all experiments reported in this paper. If word xdi has level ldi = 0 then the
word is generated by the topic at the last node on the path and successive levels correspond to earlier
nodes in the path. In the case of labeled documents, this matches our belief that a majority of words
in the document should be assigned to the concept label itself.

4

Inference

We marginalize over the topic distributions ?t and the stick-breaking weights {vtj }. We use a
collapsed Gibbs sampler [9] to infer the path assignment pd for each document, the level distribution
parameter ?d for each document, and the level assignment ldi for each word. Of the five hyperparameters in the model, inference is sensitive to the value of ? and ? so we place an Exponential
prior on both and use a Metropolis-Hastings sampler to learn the best setting.
4.1

Sampling Paths

For each document, we must sample a path pd conditioned on all other paths p?d , the level variables,
and the word tokens. We only consider paths whose length is greater than or equal to the maximum
4

level of the words in the document.
p(pd |x, l, p?d , ? ) ? p(xd |x?d , l, p) ? p(pd |p?d )

(1)

The first term in Equation 1 is the probability of all words in the document given the path pd . We
compute this probability by marginalizing over the topic distributions ?t :
!
P
?d
V
Y
Y
?(V ? + v Np?d
)
?(? + Npd [l],v )
d [l],v
P
p(xd |x?d , l, p) =
?
?d
?(V ? + v Npd [l],v )
?(? + Np [l],v )
v=1
l=1

d

We use ?d to denote the length of path pd . The notation Npd [l],v stands for the number of times
word type v has been assigned to node pd [l]. The superscript ?d means we first decrement the count
Npd [l],v for every word in document d.
The second term is the conditional probability of the path pd given all other paths p?d . We present
the sampling equation under the assumption that there is a maximum number of nodes M allowed
at each level. We first consider the probability of sampling a single edge in the path from a node x
to one of its feasible nodes {y1 , y2 , . . . , yM } where the node y1 has the first position in the stickbreaking permutation, y2 has the second position, y3 the third and so on.
We denote the number of paths that have gone from x to yi as N(x,yi ) . We denote the number of
paths that have gone from x to a node with a strictly higher position in the stick-breaking distribution
PM
than yi as N(x,>yi ) . That is, N(x,>yi ) = k=i+1 N(x,yk ) . Extending this notation we denote the
sum N(x,yi ) + N(x,>yi ) as N(x,?yi ) . The probability of selecting node yi is given by:
p(x ? yi | p?d ) =

i?1
Y ? + N(x,>yr )
? + N(x,yi )
? + ? + N(x,?yi ) r=1 ? + ? + N(x,?yr )

for i = 1 . . . M

If ym is the last node with a nonzero count N(x,ym ) and m << M it is convenient to compute the
probability of transitioning to yi , for i ? m, and the probability of transitioning to any node higher
than ym . The probability of transitioning to a node higher than ym is given by
"
#
M
X
? M ?m
p(x ? yk |p?d ) = ? 1 ?
?+?
k=m+1

?+N(x,>yr )
r=1 ?+?+N(x,?yr ) .

Qm

where ? =
A similar derivation can be used to compute the probability of
sampling a node higher than ym when M is equal to infinity. Now that we have computed the
probability of a single edge, we can compute the probability of an entire path pd :
p(pd |p?d ) =

?d
Y

p(pdj ? pd,j+1 |p?d )

j=1

4.2

Sampling Levels

For the ith word in the dth document we must sample a level ldi conditioned on all other levels l?di ,
the document paths, the level parameters ? , and the word tokens.
!
? + Np?di
(1 ? ?d )ldi ?d
d [ldi ],xdi
p(ldi |x, l?di , p, ? ) =
?
(1 ? (1 ? ?d )?d +1 )
W ? + Np?di
[l ],?
d

di

The first term is the probability of word type xdi given the topic at node pd [ldi ]. The second term is
the probability of the level ldi given the level parameter ?d .
4.3

Sampling ? Variables

Finally, we must sample the level distribution ?d conditioned on the rest of the level parameters ? ?d ,
the level variables, and the word tokens.
!
!
Nd
Y
?da?1 (1 ? ?d )b?1
(1 ? ?d )ldi ?d

(2)
p(?d |x, l, p, ? ?d ) =
?
(1 ? (1 ? ?d )?d +1 )
B a, b
i=1
5

1

1

973

1069

2

1060

973

957

2

4

3

957

9

4

3

3/10
9

486

331

385

524

524

6

5
306

8

7

453

496

278

513

194

5
316

154

2

484

235

384

4/7

274

2

486

283

1

8/4/1

7/10

268

245

1069

973

331

385

6

4

453

524

524

278

513

154

8

7

24

9

10

9/2

957

3

5
306

26

4

8/4

968

512

6/9

275

(b) Learned Graph (0 labeled documents)

3

5/1

4

1

1057

2

5/2

275

20

1

972

682

9

(a) Simulated Graph

2

515

20

7/10

423

10

9

545

6

10

(d) Learned Graph (4000 labeled documents)

(c) Learned Graph (250 labeled documents)

Figure 4: Learning results with simulated data
Due to the normalization constant (1 ? (1 ? ?d )?d +1 ), Equation 2 is not a recognizable probability
distribution and we must use rejection sampling. Since the first term in Equation 2 is always less
than or equal to 1, the sampling distribution is dominated by a Beta(a, b) distribution. According
to the rejection sampling algorithm, we sample a candidate value for ?d from Beta(a, b) and either
QNd (1??d )ldi ?d
accept with probability i=1
or reject and sample again.
(1?(1?? )?d +1 )
d

4.4

Metropolis Hastings for Stick-Breaking Permutations

In addition to the Gibbs sampling, we employ a Metropolis Hastings sampler presented in [10] to
mix over stick-breaking permutations. Consider a node x with feasible nodes {y1 , y2 , . . . , yM }. We
sample two feasible nodes yi and yj from a uniform distribution3 . Assume yi comes before yj in
the stick-breaking distribution. Then the probability of swapping the position of nodes yi and yj is
given by
)
( N(x,y ) ?1
N(x,yj ) ?1
?
i
Y
Y ? + ? + N(x,>yj ) + k
? + ? + N(x,>y
+k
i)
?
min 1,
?
? + ? + N(x,>yj ) + k
? + ? + N(x,>y
+k
i)
k=0

k=0

?
where N(x,>y
= N(x,>yi ) ? N(x,yj ) . See [10] for a full derivation. After every new path assigni)
ment, we propose one swap for each node in the graph.

5

Experiments and Results

In this section, we present experiments performed on both simulated and real text data. We compare
the performance of GraphLDA against hPAM and hLDA.
5.1

Simulated Text Data

In this section, we illustrate how the performance of GraphLDA improves as the fraction of labeled
data increases. Figure 4(a) shows a simulated concept graph with 10 nodes drawn according to the
3
In [10] feasible nodes are sampled from the prior probability distribution. However for small values of ?
and ? this results in extremely slow mixing.

6

stick-breaking generative process with parameter values ? = .025, ? = 10, ? = 10, a = 2 and
b = 5. The vocabulary size is 1, 000 words and we generate 4, 000 documents with 250 words each.
Each edge in the graph is labeled with the number of paths that traverse it.
Figures 4(b)-(d) show the learned graph structures as the fraction of labeled data increases from
0 labeled and 4, 000 unlabeled documents to all 4, 000 documents being labeled. In addition to
labeling the edges, we label each node based upon the similarity of the learned topic at the node to
the topics of the original graph structure. The Gibbs sampler is initialized to a root node when there
is no labeled data. With labeled data, the Gibbs sampler is initialized with the correct placement of
nodes to levels. The sampler does not observe the edge structure of the graph nor the correct number
of nodes at each level (i.e. the sampler may add additional nodes). With no labeled data, the sampler
is unable to recover the relationship between concepts 8 and 10 (due to the relatively small number
of documents that contain words from both concepts). With 250 labeled documents, the sampler is
able to learn the correct placement of both nodes 8 and 10 (although the topics contain some noise).
5.2

Wikipedia Articles

In this section, we compare the performance of GraphLDA to hPAM and hLDA on a set of 518
machine-learning articles taken from Wikipedia. The input to each model is only the article text. All
models are restricted to learning a three-level hierarchical structure. For both GraphLDA and hPAM,
the number of nodes at each level was set to 25. For GraphLDA, the parameters were fixed at ? = 1,
a = 1 and b = 1. The parameters ? and ? were initialized to 1 and .001 respectively and optimized
using a Metropolis Hastings sampler. We used the MALLET toolkit implementation of hPAM4 and
hLDA [11]. For hPAM, we used different settings for the topic hyperparameter ? = (.001, .01, .1).
For hLDA we set ? = .1 and considered ? = (.1, 1, 10) where ? is the smoothing parameter for the
Chinese restaurant process and ? = (.1, 1, 10) where ? is the smoothing over levels in the graph.
All models were run for 9, 000 iterations to ensure burn-in and samples were taken every 100 iterations thereafter, for a total of 10, 000 iterations. The performance of each model was evaluated
on a hold-out set consisting of 20% of the articles using both empirical likelihood and the left-toright evaluation algorithm (see Sections 4.1 and 4.5 of [12]) which are measures of generalization
to unseen data. For both GraphLDA and hLDA we use the distribution over paths that was learned
during training to compute the per-word log likelihood. For hPAM we compute the MLE estimate of
the Dirichlet hyperparameters for both the distribution over super-topics and the distributions over
sub-topics from the training documents. Table 5.2 shows the per-word log-likelihood for each model
averaged over the ten samples. GraphLDA is competitive when computing the empirical log likelihood. We speculate that GraphLDA?s lower performance in terms of left-to-right log-likelihood is
due to our choice of the geometric distribution over levels (and our choice to position the geometric distribution at the last node of the path) and that a more flexible approach could result in better
performance.

Table 1: Per-word log likelihood of test documents
Model
Parameters
Empirical LL Left-to-Right LL
GraphLDA MH opt.
-7.10 ? .003
-7.13 ? .009
? = .1
-7.36 ? .013
-6.11 ? .007
hPAM
? = .01
-7.33 ? .012
-6.47 ? .012
? = .001
-7.38 ? .006
-6.71 ? .013
? = .1, ? = .1
-7.10 ? .004
-6.82 ? .007
? = .1, ? = 1
-7.09 ? .003
-6.86 ? .006
hLDA
? = .1, ? = 10
-7.08 ? .003
-6.90 ? .008
? = 1, ? = .1
-7.08 ? .003
-6.83 ? .007
? = 1, ? = 1
-7.08 ? .002
-6.86 ? .006
? = 1, ? = 10
-7.06 ? .003
-6.88 ? .008
? = 10, ? = .1
-7.07 ? .004
-6.81 ? .006
? = 10, ? = 1
-7.07 ? .003
-6.83? .005
? = 10, ? = 10
-7.06 ? .003
-6.88 ? .010

7

set
data
learning
concept
model

network
neural
neuron
cnn
function

genetic
fitness
mutation
selection
solution

Markov
time
probability
chain
distribution

graph
Markov
network
random
field

evolution
evolutionary
algorithm
individual
search

word
topic
language
model
document

variables
node
network
parent
Bayesian

model
multitask
inference
Bayesian
Dirichlet

learning
data
model
method
kernel

model
noise
algorithm
hidden
training

learning
policy
decision
graph
function

decision
classification
class
classifier
data

clustering
data
principal
component
kmeans

learning
dimensionality
classification
reduction
method

model
selection
rbm
algorithm
feature

kernel
linear
space
vector
point

learning
algorithm
kernel
convex
constraint

algorithm
svm
vector
problem
multiclass

classifier
boosting
ensemble
hypothesis
margin

Figure 5: Wikipedia graph structure with additional machine learning abstracts. The edge widths
correspond to the probability of the edge in the graph

5.3

Wikipedia Articles with a Graph Structure

In our final experiment we illustrate how GraphLDA can be used to update an existing category
graph. We use the aforementioned 518 machine-learning Wikipedia articles, along with their category labels, to learn topic distributions for each node in Figure 1. The sampler is initialized with
the correct placement of nodes and each document is initialized to a random path from the root to
its category label. After 2, 000 iterations, we fix the path assignments for the Wikipedia articles
and introduce a new set of documents. We use a collection of 400 machine learning abstracts from
the International Conference on Machine Learning (ICML). We sample paths for the new collection of documents keeping the paths from the Wikipedia articles fixed. The sampler was allowed
to add new nodes to each level to explain any new concepts that occurred in the ICML text set.
Figure 5 illustrates a portion of the final graph structure. The nodes in bold are the original nodes
from the Wikipedia category graph. The results show that the model is capable of augmenting an
existing concept graph with new concepts (e.g. clustering, support vector machines (SVMs), etc.)
and learning meaningful relationships (e.g. boosting/ensembles are on the same path as the concepts
for SVMs and neural networks).

6

Discussion and Conclusion

Motivated by the increasing availability of large-scale structured collections of documents such as
Wikipedia, we have presented a flexible non-parametric Bayesian framework for learning concept
graphs from text. The proposed approach can combine unlabeled data with prior knowledge in the
form of labeled documents and existing graph structures. Extensions such as allowing the model
to handle multiple paths per document are likely to be worth pursuing. In this paper we did not
discuss scalability to large graphs which is likely to be an important issue in practice. Computing
the probability of every path during sampling, where the number of graphs is a product over the
number of nodes at each level, is a computational bottleneck in the current inference algorithm and
will not scale. Approximate inference methods that can address this issue should be quite useful in
this context.

7

Acknowledgements

This material is based upon work supported in part by the National Science Foundation under Award
Number IIS-0083489, by a Microsoft Scholarship (AC), and by a Google Faculty Research award
(PS). The authors would also like to thank Ian Porteous and Alex Ihler for useful discussions.
4

MALLET implements the ?exit node? version of hPAM

8

References
[1] David Blei, Andrew Ng, and Michael Jordan. Latent dirichlet allocation. Journal of Machine
Learning Research, 3:993?1022, 2003.
[2] David M. Blei, Thomas L. Griffiths, and Michael I. Jordan. The nested chinese restaurant
process and bayesian nonparametric inference of topic hierarchies. Journal of the Acm, 57,
2010.
[3] David Mimno, Wei Li, and Andrew McCallum. mixtures of hierarchical topics with pachinko
allocation. In Proceedings of the 21st Intl. Conf. on Machine Learning, 2007.
[4] Wei Li, David Blei, and Andrew McCallum. Nonparametric bayes pachinko allocation. In
Proceedings of the Twenty-Third Annual Conference on Uncertainty in Artificial Intelligence
(UAI-07), pages 243?250, 2007.
[5] Blaz Fortuna, Marko Grobelnki, and Dunja Mladenic. Ontogen: Semi-automatic ontology
editor. In Proceedings of theHuman Computer Interaction International Conference, volume
4558, pages 309?318, 2007.
[6] S. Bloehdorn, P. Cimiano, and A. Hotho. Learning ontologies to improve text clustering and
classification. In From Data and Inf. Analysis to Know. Eng.: Proc. of the 29th Annual Conf.
the German Classification Society (GfKl ?05), volume 30 of Studies in Classification, Data
Analysis and Know. Org., pages 334?341. Springer, Feb. 2005.
[7] P. Cimiano, A. Hotho, and S. Staab. Learning concept hierarchies from text using formal
concept analysis. J. Artificial Intelligence Research (JAIR), 24:305?339, 2005.
[8] Hemant Ishwaran and Lancelot F. James. Gibbs sampling methods for stick-breaking priors.
Journal of the American Statistical Association, 96(453):161?173, March 2001.
[9] Tom Griffiths and Mark Steyvers. Finding scientific topics. Proceedings of the Natl. Academy
of the Sciences of the U.S.A., 101 Suppl 1:5228?5235, 2004.
[10] Ian Porteous, Alex Ihler, Padhraic Smyth, and Max Welling. Gibbs sampling for coupled
infinite mixture models in the stick-breaking representation. In Proceedings of UAI 2006,
pages 385?392, July 2006.
[11] Andrew Kachites McCallum.
http://mallet.cs.umass.edu, 2002.

Mallet:

A machine learning for language toolkit.

[12] Hanna M. Wallach, Iain Murray, Ruslan Salakhutdinov, and David Mimno. Evaluation methods for topic models. In Proceedings of the 26th Intl. Conf. on Machine Learning (ICML
2009), 2009.

9


----------------------------------------------------------------

