query sentence: machine-learning toolkit 
---------------------------------------------------------------------
title: 5812-matrix-manifold-optimization-for-gaussian-mixtures.pdf

matrix manifold optim gaussian mixtur reshad hosseini school ece colleg engin univers tehran tehran iran reshad.hosseini ut.ac.ir suvrit sra laboratori inform decis system massachusett institut technolog cambridg ma suvrit mit.edu abstract take new look paramet estim gaussian mixtur model gmms specif advanc riemannian manifold optim manifold posit definit matric potenti replac expect maxim de facto standard decad out-of-the-box invoc riemannian optim howev fail spectacular obtain solut em vast slower build intuit geometr convex propos simpl reformul remark consequ make riemannian optim match em nontrivi result given poor record nonlinear program also outperform mani set bring idea fruition develop welltun riemannian lbfgs method prove superior known compet method riemannian conjug gradient hope result encourag wider consider manifold optim machin learn statist introduct gaussian mixtur model gmms mainstay varieti area includ machin learn signal process quick literatur search reveal estim paramet gmm expect maxim algorithm still de facto choic over decad numer approach also consid method conjug gradient quasi-newton newton note usual inferior em key difficulti appli standard nonlinear program method gmms posit definit constraint covari although open subset euclidean space constraint difficult impos especi higher-dimens approach boundari constraint set converg speed iter method also get advers affect partial remedi remov pd constraint use choleski decomposit exploit semidefinit program believ general nonconvex decomposit add stationari point possibl spurious local minima.1 anoth possibl formul pd constraint via set smooth convex inequ appli interior-point method sophist method extrem slower sever statist problem simpler em-lik iter especi higher dimens sinc key difficulti aris pd constraint appeal idea note pd matric form riemannian manifold invok riemannian manifold optim inde oper manifold2 implicit satisfi pd constraint may better chanc focus likelihood maxim while attract line think also fail an out-of-the-box invoc manifold optim also vast inferior em thus need think harder challeng hegemoni em outlin new approach remark use choleski reformul add spurious local minima gmms equival interior constraint set done interior point method nonconvex version though turn slow second order method key idea intuit mismatch geometri gmms m-step em euclidean convex optim problem wherea gmm log-likelihood manifold convex3 even singl gaussian if could reformul likelihood singl compon maxim task analog m-step em gmms becom manifold convex might substanti empir impact this intuit suppli miss link final make riemannian manifold optim match em but often also great outperform summar key contribut paper follow introduct riemannian manifold optim gmm paramet estim we show reformul base geodes convex crucial empir success develop riemannian lbfgs solver main contribut implement power line-search procedur ensur converg make lbfgs outperform em manifold conjug gradient this solver may independ interest we provid substant experiment evid synthet real-data we compar manifold optim em unconstrain euclidean optim reformul problem use choleski factor invers covari matric result show manifold optim perform well across wide rang paramet valu problem size it much less sensit overlap data em display much less variabl run time result quit encourag we believ manifold optim could open new algorithm avenu mixtur model perhap statist estim problem note aid reproduc result atlab implement our method avail part ixest toolbox develop our group manifold cg method we use direct base excel toolkit anopt relat work summar publish work em clear imposs so let us briefli mention line relat work xu jordan examin sever aspect em gmms counter the claim redner walker claim em inferior to generic secondord nonlinear program techniqu howev it well-known em attain good likelihood valu rapid scale to much larger problem amen to secondord method local converg analysi em avail refin result show when data low overlap em converg local superlinear our paper develop riemannian lbfgs also achiev local superlinear converg gmms innov gradient-bas method also suggest the pd constraint handl via a choleski decomposit covari matric howev work report result for low-dimension problem near spheric covari the idea use manifold optim for gmms new though manifold optim a well-develop subject a classic
----------------------------------------------------------------

title: 2276-stochastic-neighbor-embedding.pdf

stochast neighbor embed geoffrey hinton sam rowei depart comput scienc univers toronto king colleg road toronto m5s canada hinton rowei cs.toronto.edu abstract describ probabilist approach task place object describ high-dimension vector pairwis dissimilar low-dimension space way preserv neighbor ident gaussian center object high-dimension space densiti gaussian given dissimilar use defin probabl distribut potenti neighbor object aim embed approxim distribut well possibl oper perform low-dimension imag object natur cost function sum kullback-leibl diverg one per object lead simpl gradient adjust posit low-dimension imag unlik dimension reduct method probabilist framework make easi repres object mixtur wide separ low-dimension imag allow ambigu object like document count vector word bank version close imag river financ without forc imag outdoor concept locat close corpor concept introduct automat dimension reduct import toolkit oper machin learn preprocess step algorithm reduc classifi input size goal visual interpol compress etc mani way emb object describ high-dimension vector pairwis dissimilar lower-dimension space multidimension scale method preserv dissimilar item measur either euclidean distanc nonlinear squash distanc shortest graph path isomap princip compon analysi pca find linear project origin data captur much varianc possibl method attempt preserv local geometri associ high-dimension point fix grid point low-dimension space self-organ map probabilist extens method howev requir high-dimension object associ singl locat low-dimension space make difficult unfold many-to-on map singl ambigu object realli belong sever dispar locat low-dimension space paper defin new notion embed base probabl neighbor algorithm stochast neighbor embed sne tri place object low-dimension space optim preserv neighborhood ident natur extend allow multipl differ low-d imag object basic sne algorithm object potenti neighbor start comput asymmetr probabl would pick neighbor dissimilar may given part problem definit need symmetr may comput use scale squar euclidean distanc affin two high-dimension point either set hand experi found binari search valu make entropi distribut neighbor equal here effect number local neighbor perplex chosen hand low-dimension space also use gaussian neighborhood fix varianc set without loss general induc probabl point pick point neighbor function low-dimension imag object given express aim embed match two distribut well possibl achiev minim cost function sum kullback-leibl diverg origin induc distribut neighbor object dimension space chosen hand much less number object notic make larg small wast probabl mass distribut cost model big distanc high-dimension space small distanc low-dimension space though less cost model small distanc big one respect sne improv method like lle som wide separ data-point collaps near neighbor low-dimension space intuit sne emphas local distanc cost function clean enforc keep imag nearbi object nearbi keep imag wide separ object relat far apart differenti tedious result simpl affect via normal term nice interpret sum forc pull toward push away depend whether observ neighbor less often desir given gradient mani possibl way minim begun search best method steepest descent point adjust parallel ineffici get stuck poor local optima ad random jitter decreas time find much better local optima method use exampl paper even though still quit slow initi embed put low-dimension imag random locat close origin sever minim method includ anneal perplex discuss section applic sne imag document collect graphic illustr abil sne model high-dimension near-neighbor relationship use two dimens ran algorithm collect bitmap handwritten digit set word-author count taken scan proceed nip confer paper dataset like intrins structur mani fewer dimens raw dimension handwritten digit author-word count begin use set digit bitmap up databas exampl five class varianc gaussian around point dimension raw pixel imag space set achiev perplex distribut high-dimension neighbor sne initi put random locat close origin train use gradient descent anneal nois although sne given inform class label quit clean separ digit group shown figur furthermor within region low-dimension space sne arrang data properti like orient skew stroke-thick tend vari smooth embed shown sne cost function valu nat uniform distribut across lowdimension neighbor cost nat also appli princip compon analysi data project onto first two princip compon separ class near clean sne pca much interest get larg separ right caus jumbl boundari similar class experi use digit class similar pair like class two avail dimens sne clean separ similar pair also appli sne word-docu word-author matric calcul ocr text nip volum paper figur show map locat nip author two dimens author publish one paper nip vol shown dot posit found sne larger red dot correspond last name author publish six paper period distanc comput norm differ log aggreg author word count sum across nip paper co-author paper gave fraction count even author word occur six document includ except stopword give vocabulari size bow toolkit use part pre-process data set achiev local perplex neighbor sne seem group author broad nip field generat model support vector machin neurosci reinforc learn vlsi distinguish local region full mixtur version sne clean probabilist formul sne make easi modifi cost function instead singl imag high-dimension object sever differ version low-dimension imag altern version mix proport sum image-vers object locat mix proport low-dimension neighborhood distribut mixtur distribut induc image-vers across image-vers potenti neighbor multiple-imag model deriv respect imag locat straightforward deriv w.r.t mix proport easili express figur result run sne algorithm dimension grayscal imag handwritten digit pictur origin data vector scan handwritten digit shown locat correspond low-dimension imag found sne class quit well separ even though sne inform class label furthermor within class properti like orient skew strokethick tend vari smooth across space point shown produc display digit chosen random order display region display center locat digit embed overlap region digit alreadi display sne initi put random locat close origin train use batch gradient descent anneal nois learn rate first iter point jitter ad gaussian nois standard deviat posit updat jitter reduc iter touretzki wile maass kailath chauvin munro shavlik sanger movellan baluja lewicki schmidhub hertz baldi buhmann pearlmutt yang tenenbaum cottrel krogh omohundro abu mostafa schraudolph mackay coolen lippmann robinson smyth cohn ahmad tesauro pentland goodman atkeson neuneier warmuth sollich moor thrun pomerleau barber ruppin horn meilijson meadlazzaro koch obermay ruderman eeckman harrismurray bialek cowan baird andreou mel cauwenbergh brown li jabri gile chen spenc princip doya touretzki sun stork alspector mjols bell lee maass lee gold pomerleau kailath meir seung movellan rangarajan yang amari tenenbaum cottrel baldi abu mostafa mackay nowlan lippmann smyth cohn kowalczyk waibel pouget atkeson kawato viola bourlard warmuth dayan sollich morgan thrun mooresutton barber barto singh tishbi wolpertopp sejnowski williamson kearn singer moodi shaw taylor saad zemel saul tresp bartlett platt leen mozer bishop jaakkola solla ghahramani smola william vapnik scholkopf hinton bengio jordan muller graf lecun simard denker guyon bower figur embed nip author two dimens author publish one paper nip vol show dot locat found sne algorithm larger red dot correspond last name author publish six paper period inset upper left show blowup crowd box central portion space dissimilar author comput base squar euclidean distanc vector log aggreg author word count co-author paper gave fraction count even author word occur six document includ except stopword give vocabulari size nip text data avail http //www.cs.toronto.edu roweis/data.html term probabl version pick version effect chang mix proport version object given otherwis effect chang cost rather optim mix proport direct easier perform unconstrain optim softmax weight defin proof-of-concept recent implement simplifi mixtur version everi object repres low-dimension space exact two compon constrain mix proport two compon pull togeth forc increas linear threshold separ beyond threshold forc remain constant.1 ran two experi simplifi mixtur version sne took dataset contain pictur digit ad hybrid digit-pictur construct pick new exampl two class take pixel random one two parent mini hybrid non-hybrid signific differ mizat locat two mixtur compon moreov mixtur compon hybrid alway lay region space devot class two parent never region devot third class exampl use perplex defin local neighborhood step size posit updat time gradient use constant jitter our simpl mixtur version sne also make possibl map circl onto line without lose near neighbor relationship introduc new one point near one cut point circl map mixtur two point one near one end line one near end obvious locat cut two-dimension circl get decid pair mixtur compon split first stochast optim certain optim paramet control eas two mixtur compon pull apart singl cut circl made paramet set howev circl may fragment two smaller line-seg topolog correct may link exampl hybrid digit demonstr even primit mixtur version sne deal ambigu high-dimension object need map two wide separ region low-dimension space more work need done sne effici enough cope larg matric document-word count dimension reduct method we know promis treat homonym sensibl without go back origin document disambigu occurr homonym we use threshold threshold forc nat per unit length low-d space natur scale varianc gaussian use determin fix practic optim strategi our current method reduc sne cost use steepest descent ad jitter slowli reduc produc quit good embed demonstr sne cost function worth minim take sever hour find good embed datapoint we clear need better search algorithm time per iter could reduc consider ignor pair point four small sinc matrix fix learn natur sparsifi replac all entri certain threshold zero renorm pair zero ignor gradient calcul both small this turn determin logarithm time size train set use sophist geometr data structur k-d tree ball-tre ad-tre sinc depend comput physic attack exact this complex perform multibodi gravit electrostat simul use exampl fast multipol method mixtur version sne there appear interest way avoid local optima not involv anneal jitter consid two compon mixtur object far apart low-dimension space rais mix proport one lower mix proport we move probabl mass one part space anoth without ever appear at intermedi locat this type probabl wormhol seem like good way avoid local optima aris cluster low-dimension point must move bad region space order reach better one yet anoth search method we use success toy problem provid extra dimens low-dimension space penal non-zero valu dimens dure search sne use extra dimens go around lower-dimension barrier penalti use these dimens increas ceas use effect constrain embed origin dimension discuss conclus preliminari experi show we find good optima first anneal perplex iti use high jitter reduc jitter after final perplex reach this rais question sne when varianc gaussian center high-dimension point big distribut across neighbor almost uniform clear high varianc limit contribut sne cost function import distant neighbor close one when larg shown sne equival minim mismatch squar distanc two space provid all squar distanc object first normal subtract antigeometr mean number object this mismatch similar stress function use nonmetr version mds enabl us understand large-vari limit sne particular variant procedur we still investig relationship metric mds pca sne also seen interest special case linear relat embed lre lre data consist tripl colin has-moth victoria task predict third term other two lre learn n-dimension vector for each object nxn-dimension matrix for each relat predict the third term tripl lre multipli the vector repres the first term the matrix repres the relationship use the result vector the mean gaussian predict distribut for the third term then determin the relat densiti all known object this gaussian sne degener version lre the relationship near the matrix repres this relationship the ident in summari we present a new criterion stochast neighbor embed for map high-dimension point a low-dimension space base stochast select similar neighbor unlik self-organ map in the low-dimension coordin fix a grid the high-dimension end free move in sne the high-dimension coordin fix the data the low-dimension point move our method also appli arbitrari pairwis dissimilar object avail instead in addit high-dimension observ the gradient the sne cost function appeal push-pul properti in the forc act bring closer point under-select point over-select as neighbor we shown result appli this algorithm imag document collect for it sensibl place similar object nearbi in a low-dimension space keep dissimilar object well separ most import it probabilist formul sne the abil extend to mixtur in ambigu high-dimension object as the word bank sever widely-separ imag in the low-dimension space acknowledg we thank the anonym refere sever visitor to our poster for help suggest yann lecun provid digit nip text data this research fund by nserc
----------------------------------------------------------------

title: 5956-scheduled-sampling-for-sequence-prediction-with-recurrent-neural-networks.pdf

schedul sampl sequenc predict recurr neural network sami bengio oriol vinyal navdeep jait noam shazeer googl research mountain view ca usa bengio vinyal ndjait noam google.com abstract recurr neural network train produc sequenc token given input exemplifi recent result machin translat imag caption current approach train consist maxim likelihood token sequenc given current recurr state previous token infer unknown previous token replac token generat model discrep train infer yield error accumul quick along generat sequenc propos curriculum learn strategi gentl chang train process fulli guid scheme use true previous token toward less guid scheme most use generat token instead experi sever sequenc predict task show approach yield signific improv moreov use succes win entri mscoco imag caption challeng introduct recurr neural network use process sequenc either input output known hard train long term depend data version like long short-term memori lstm better suit fact recent shown impress perform sever sequenc predict problem includ machin translat contextu pars imag caption even video descript paper consid set problem attempt generat sequenc token variabl size problem machin translat goal translat given sentenc sourc languag target languag also consid problem input necessarili sequenc like imag caption problem goal generat textual descript given imag case recurr neural network variant like lstms general train maxim likelihood generat target sequenc token given input practic done maxim likelihood target token given current state model summar input past output token previous target token help model learn kind languag model target token howev infer true previous target token unavail thus replac token generat model yield discrep model use train infer discrep mitig use beam search heurist maintain sever generat target sequenc continu state space model like recurr neural network dynam program approach effect number sequenc consid remain small even beam search main problem mistak made earli sequenc generat process fed input model quick amplifi model might part state space never seen train time propos curriculum learn approach gentl bridg gap train infer sequenc predict task use recurr neural network propos chang train process order gradual forc model deal mistak would infer do model explor train thus robust correct mistak infer learn train show experiment approach yield better perform sever sequenc predict task paper organ follow section present propos approach better train sequenc predict task recurr neural network follow section draw link relat approach present experiment result section conclud section propos approach consid supervis task train set given term input/output pair input either static like imag dynam like sequenc target output sequenc y1i y2i yti variabl number token belong fix known dictionari model given singl input/output pair log probabl comput log log log sequenc length repres token y1 y2 yt latter term equat estim recurr neural network paramet introduc state vector ht function previous state previous output token log log ht ht comput recurr neural network follow ht otherwis ht often implement linear projection1 state vector ht vector score one token output dictionari follow softmax transform ensur score proper normal posit sum usual non-linear function combin previous state previous output order produc current state mean model focus learn output next token given current state model previous token thus model repres probabl distribut sequenc general form unlik condit random field model assum independ output differ time step given latent variabl state capac model limit represent capac recurr feedforward layer lstms abil learn long rang structur especi well suit task make possibl learn rich distribut sequenc order learn variabl length sequenc special token signifi end sequenc ad dictionari model train eo concaten end sequenc infer model generat token generat although one could also use multi-lay non-linear project train train recurr neural network solv task usual accomplish use mini-batch stochast gradient descent look set paramet maxim log likelihood produc correct target sequenc given input data train pair arg max log infer infer model generat full sequenc y1t given generat one token time advanc time one step eo token generat signifi end sequenc process time model need input output token last time step order produc sinc access true previous token instead either select like one given model sampl accord search sequenc highest probabl given expens combinatori growth number sequenc instead use beam search procedur generat best sequenc maintain heap best candid sequenc time step new candid generat extend candid one token ad heap end step heap re-prun keep candid beam search truncat new sequenc ad best sequenc return beam search often use discret state base model like hidden markov model dynam program use harder use effici continu state base model like recurr neural network sinc way factor follow state path continu space henc actual number candid kept beam search decod small case wrong decis taken time model part state space differ visit train distribut know wors easili lead cumul bad decis classic problem sequenti gibb sampl type approach sampl futur sampl influenc past bridg gap schedul sampl main differ train infer sequenc predict task predict token whether use true previous token estim come model propos sampl mechan random decid train whether use assum use mini-batch base stochast gradient descent approach everi token predict ith mini-batch train algorithm propos flip coin use true previous token probabl estim come model probabl estim model obtain sampl token accord probabl distribut model taken arg max process illustr figur model train exact model train set infer propos here curriculum learn strategi go one intuit begin train sampl model would yield random token sinc model well train could lead slow converg select often true previous token help hand end train favor sampl model often correspond true infer situat one expect model alreadi good enough handl sampl reason token note experi flip coin everi token also tri flip coin per sequenc result much wors probabl consecut error amplifi first round train figur illustr schedul sampl approach one flip coin everi time step decid use true previous token one sampl model exponenti decay invers sigmoid decay linear decay figur schedul exampl decay thus propos use schedul decreas function similar manner use decreas learn rate modern stochast gradient descent approach exampl schedul seen figur follow linear decay ci minimum amount truth given model provid offset slope decay depend expect speed converg exponenti decay constant depend expect speed converg invers sigmoid decay depend expect speed converg call approach schedul sampl note sampl previous token model train could back-propag gradient loss time decis done experi describ paper left futur work relat work discrep train infer distribut alreadi notic literatur particular control reinforc learn task searn propos tackl problem supervis train exampl might differ actual test exampl exampl made sequenc decis like act complex environ mistak model earli sequenti decis process might compound yield poor global perform propos approach involv meta-algorithm meta-iter one train new model accord current polici essenti expect decis situat appli test set modifi next iter polici order account previous decis error new polici thus combin previous one actual behavior model comparison searn relat idea propos approach complet onlin singl model train polici slowli evolv train instead batch approach make much faster train3 furthermor searn propos context reinforc learn consid supervis learn set train use stochast gradient descent overal object approach consid problem rank perspect particular pars task target output tree case author propos use beam search train infer phase align train beam use find fact experi report paper propos approach meaning slower faster train baselin best current estim model compar guid solut truth use rank loss unfortun feasibl use model like recurr neural network state-of-the-art techniqu mani sequenti task state sequenc factor easili multi-dimension continu state thus beam search hard use effici train time well infer time fact final propos onlin algorithm pars problem adapt target use dynam oracl take account decis model train model perceptron thus state-bas like recurr neural network probabl choos truth fix train experi we describ section experi three differ task order show schedul sampl help differ set we report result imag caption constitu pars speech recognit imag caption imag caption attract lot attent past year task formul map imag onto sequenc word describ content natur languag propos approach employ form recurr network structur simpl decod scheme notabl except system propos direct optim log likelihood caption given imag instead propos pipelin approach sinc imag mani valid caption evalu task still open problem attempt made design metric posit correl human evalu common set tool publish mscoco team we use mscoco dataset train model we train imag report result separ develop set 5k addit imag imag corpus differ caption train procedur pick one random creat mini-batch exampl optim object function defin imag preprocess pretrain convolut neural network without last classif layer similar one describ result imag embed treat first word model start generat languag recurr neural network generat word lstm one layer hidden unit input word repres embed vector size number word dictionari we use invers sigmoid decay schedul schedul sampl approach tabl show result various metric develop set metric variant estim overlap obtain sequenc word target one sinc target caption per imag best result alway chosen best knowledg baselin result consist slight better current state-of-the-art task dropout help term log likelihood expect shown negat impact real metric hand schedul sampl success train model resili failur due train infer mismatch like yield higher qualiti caption accord metric ensembl model also yield better perform baselin schedul sampl approach also interest note model train while alway sampl henc regim similar infer dub alway sampl tabl yield poor perform expect model hard time learn task case we also train model schedul sampl instead sampl model we sampl uniform distribut order verifi import build current model perform boost simpl form regular we call uniform schedul sampl result better baselin good propos approach we also experi flip coin per sequenc instead per token result poor alway sampl approach tabl various metric higher better mscoco develop set imag caption task approach vs metric baselin baselin dropout alway sampl schedul sampl uniform schedul sampl baselin ensembl schedul sampl ensembl bleu-4 meteor cider worth note we use schedul sampl approach particip mscoco imag caption challeng rank first final leaderboard constitu pars anoth less obvious connect any-to-sequ paradigm constitu pars recent work propos interpret pars tree sequenc linear oper build tree linear procedur allow train model map sentenc onto pars tree without modif any-to-sequ formul train model one layer lstm cell word repres embed vector size we use attent mechan similar one describ help consid next output token produc focus part input sequenc appli softmax lstm state vector correspond input sequenc input word dictionari contain around word while target dictionari contain symbol use describ tree we use invers sigmoid decay schedul schedul sampl approach pars quit differ imag caption function one learn almost determinist contrast imag larg number valid caption sentenc uniqu pars tree although difficult case exist thus model oper almost determinist seen observ train test perplex extrem low compar imag caption differ oper regim make interest comparison one would expect baselin algorithm make mani mistak howev seen tabl schedul sampl posit effect addit dropout tabl we report f1 score wsj develop set we also emphas train instanc overfit contribut larg perform system whether effect sampl train help regard overfit training/infer mismatch unclear result posit addit dropout onc model train alway sampl instead use groundtruth previous token input yield bad result fact bad result tree often valid tree henc correspond f1 metric tabl f1 score higher better valid set pars task approach baselin lstm baselin lstm dropout alway sampl schedul sampl schedul sampl dropout f1 speech recognit speech recognit experi we use slight differ set rest paper train exampl input/output pair sequenc input vector xt sequenc token y1 y2 yt align correspond here acoust featur repres log mel filter bank spectra frame correspond target target use hmm-state label generat gmm-hmm recip use kaldi toolkit could well phonem label set differ experi model we use follow log log log xt1 log ht ht comput recurr neural network follow oh ht otherwis oh vector dimension ht extra token ad dictionari repres start sequenc we generat data experi use timit4 corpus kaldi toolkit describ in standard configur use experi dimension log mel filter bank first second order tempor deriv use input frame dimension target generat time frame use forc align transcript use train gmm-hmm system train valid test set sequenc respect their averag length frame valid set use choos best epoch in train model paramet epoch use evalu test set train model two layer lstm cell softmax layer five configur baselin configur ground truth alway fed model configur alway sampl model fed in predict last time step three schedul sampl configur schedul sampl ramp linear maximum valu minimum valu ten epoch kept constant final valu each configur we train model report averag perform train each model done frame target gmm baselin configur typic reach best valid accuraci approxim epoch wherea sampl model reach best accuraci approxim epoch valid accuraci decreas this probabl way we train model exact account gradient sampl probabl we sampl target futur effort at tackl this problem may improv result test done find best sequenc beam search decod use beam size beam comput error rate sequenc we also report next step error rate model fed in ground truth predict class next frame each model valid set summar perform model train object tabl show summari result seen baselin perform better next step predict model sampl token input this expect sinc former access groundtruth howev seen model train sampl perform better baselin dure decod also seen this problem alway sampl model perform quit https //catalog.ldc.upenn.edu/ldc93s1 well we hypothes this natur dataset hmm-align state lot correl state appear target for sever frame state constrain go subset other state next step predict groundtruth label this task end pay disproportion attent structur label enough acoust input thus achiev good next step predict error when groundtruth sequenc fed in acoust inform abl exploit acoust inform suffici when groundtruth sequenc fed in for this model test condit far train condit for make good predict model fed predict alway sampl end exploit inform find in acoust signal effect ignor predict influenc next step predict thus at test time perform well it dure train model attent model predict phone sequenc direct instead high redund hmm state sequenc would suffer this problem it would need exploit acoust signal languag model suffici make predict nevertheless even in this set ad schedul sampl still help improv decod frame error rate note typic speech recognit experi use hmms decod predict neural network in hybrid model here we avoid use hmm altogeth henc we the advantag the smooth result the hmm architectur the languag model thus the result direct compar the typic hybrid model result tabl frame error rate fer the speech recognit experi in next step predict report valid set the ground truth fed in predict the next target like it done dure train in decod experi report test set beam search done find the best sequenc we report result four differ linear schedul sampl ramp linear for the baselin the model fed in the ground truth see section for analysi the result approach alway sampl schedul sampl schedul sampl schedul sampl baselin lstm next step fer decod fer conclus use recurr neural network predict sequenc token mani use applic like machin translat imag descript howev the current approach train predict one token at time condit the state the previous correct token differ we actual use and thus prone the accumul error along the decis path in this paper we propos a curriculum learn approach slowli chang the train object easi task the previous token known to a realist one it provid the model experi sever sequenc predict task yield perform improv while incur longer train time futur work includ back-propag the error the sampl decis well explor better sampl strategi includ condit on some confid measur the model
----------------------------------------------------------------

title: 182-genesis-a-system-for-simulating-neural-networks.pdf

genesi system simul neural netwofl.k matthew a. wilson upind s. bhalla john d. uhley jame m. bower divis biolog california institut technolog pasadena ca abstract develop graphic orient general purpos simul system facilit model neural network simul implement unix x-window design support simul mani level detail specif intend use appli network model simul detail realist biologicallybas model exampl current model develop system includ mammalian olfactori bulb cortex invertebr central pattern generat well abstract connectionist simul introduct recent dramat increas interest explor comput properti network parallel distribut process element rumelhart mcclelland often refer itneur network anderson much current research involv numer simul type network anderson touretzki over last sever year also signific increas interest use similar comput simul techniqu studi structur function biolog neural network effort seen attempt reverse-engin brain object understand function organ complic network bower simul system rang detail reconstruct singl neuron even compon singl neuron simul larg network complex neuron koch segev model associ area research like benefit exposur larg rang neural network simul simul packag capabl implement vari type network model would facilit interact wilson bhalla uhley bower design featur simul built genesi general network simul system graphic interfac xodus x-base output display util simul provid standard flexibl mean construct neural network simul make minim assumpt actual structur neural compon system capabl grow accord need user incorpor user-defin code describ specif featur system devic independ entir system design run unix x-window version maximum portabl code develop sun workstat port sun3 's sun4 's sun masscomp comput portabl instal support unix x-ii addit we develop parallel implement simul system nelson modular design design simul interfac base building-block approach simul construct modul receiv input perform calcul generat output fig approach central general flexibl system allow user easili add new featur without modif base code interact specif control network specif control done high level use graphic tool network specif languag fig graphic interfac provid highest user friend level interact it consist number tool user configur suit particular simul graphic interfac user display control adjust paramet simul network specif languag we develop network model repres basic level interact languag consist set simul interfac function execut interact keyboard text fli store command sequenc script languag also provid arithmet oper program control function loop condit statement subprogram macro figur demonstr script function use simul interrac toolkit extend toolkit consist modul librari graphic tool simul base code fig provid routin modul use construct specif simul base code provid common control support routin entir system genesi system simul neural network gra hic interfac script file dp~~data file genesi command window ke board script languag interpret genesi figur level interact simul construct simul first step use genesi involv select link togeth modul toolkit necessari particular simul fig addit command script languag establish network graphic interfac fig modul class modul genesi divid comput modul communic modul graphic modul instanc comput modul call element these central compon simul perform numer calcul element communic two way via link via connect link allow pass data two element time delay comput perform data thus link serv unifi larg number element singl comput unit use link element togeth form neuron fig connect hand interconnect comput unit via simul communic channel incorpor time delay perform transform data transmit axon fig graphic modul call widget use construct interfac these modul issu script command well respond thus allow interact access simul structur function wilson bhalla uhley bower hierarch organ order keep track structur simul element organ tree hierarchi similar directori structur unix fig tree structur explicit repres pattern link connect element it simpli tool organ complex group element simul simul exampl exampl type modul avail process structur network simul graphic interfac we describ construct simpl biolog neural simul fig i11pdel consist two neuron neuron contain passiv dendrit compart activ cell bodi axon output synapt input onto dendrit axon one neuron connect synapt input figur show basic structur model implement genesi model synaps channel simul interrac toolkit graphic modul communic modul comput modul clinker odco earn simul simul ffi ca cqdk figur stage construct simul genesi system simul neural net~ork network neuron neuron2 cell-bodi na dendrit axon synaps key element connect dendrit link figur implement two neuron model genesi schemat diagram compartment model neuron cell simpl model passiv dendrit compart activ cell-bodi output axon there synapt input dendrit one cell two ionic channel cell bodi hierarch represent compon simul maintain genesi cell-bodi neuron refer network/neuronl/cell-bodi represent function link basic compon one neuron sampl interfac control display widget creat use xodus toolkit wilson bhalla uhley bower dendrit compart cell bodi axon each treat as separ comput element fig link allow element share inform na channel need access cell-bodi membran voltag figur show portion script use construct simul creat differ type element creat creat activ compart creat passive_compart creat synaps assign name neuronl cell-bodi dendrit dendrite/synaps establish function link element link dendrit cell-bodi link dendrite/synaps dendrit set paramet associ element set dendrit capacit make copi entir element subtre copi neuronl neuron2 establish connect two element connect neuronl/axon neuron2/dendrite/synaps set graph monitor element variabl graph neuronl/cell-bodi potenti make control panel sever control widget xform control xdialo nstep set-nstep default xdialog dt set-dt default xloggl euler set-eul figur sampl script command construct a simul fig simul specif memori requir genesi current genesi consist line simul code a similar amount graphic code written c. execut binari take megabyt a rough estim amount addit memori necessari a particular simul calcul size number modul use in a simul typic element use around byte connect messag widget use kbyte each genesi a system simul neural network perform overal effici genesi system high simul specif consid briefli a specif case sophist biolog base simul current implement genesi a model piriform olfactori cortex wilson wilson bower wilson bower simul consist neuron four differ type each neuron contain one five compart each compart contain sever channel a sun mbyte ram this simul cell run i second per time step other model implement genesi list project current complet genesi includ approxim ten differ simul these includ model olfactori bulb bhalla inferior oliv lee bower a motor circuit in invertebr sea slug tritonia ryckebusch we also built sever tutori allow student explor compartment biolog model hodgkin huxley hopfield network hopfield access/us genesi genesi xodus made avail cost distribut all interest user as describ new user-defin modul link the simul extend the system user encourag support the continu develop this system send modul develop caltech these review compil the overal system genesi support staff we would also hope user would send complet publish simul the genesi data base this provid other with opportun observ the behavior a simul first hand a current list modul full simul maintain avail through electron mail newsgroup babel enquiri the system sent genesi caltech.edu genesi caltech.biblet acknowledg we would like thank mark nelson invalu assist in the develop this system specif suggest the content this manuscript we would also like recogn dave bilitch wojtek furmanski christof koch innumer caltech student the student of the mbl summer cours on method in comput neurosci for contribut to the creation evolut of genesi mutual exclus this research also support the nsf the nih bns the onr contract the lockhe corpor the caltech presid fund the jpl director develop fund the joseph drown foundat wilson bhalla uhley bower
----------------------------------------------------------------

title: 4022-latent-variable-models-for-predicting-file-dependencies-in-large-scale-software-development.pdf

latent variabl model predict file depend large-scal softwar develop dian j. hu1 lauren van der maaten1,2 youngmin cho1 lawrenc k. saul1 sorin lerner1 dept comput scienc engin univers california san diego pattern recognit bioinformat lab delft univers technolog dhu lvdmaaten yoc002 saul lerner cs.ucsd.edu abstract when softwar develop modifi one file larg code base must also identifi updat relat file mani file depend detect mine develop histori code base essenc group relat file reveal log previous workflow data form show detect depend file solv problem binari matrix complet explor differ latent variabl model lvms problem includ bernoulli mixtur model exponenti famili pca restrict boltzmann machin fulli bayesian approach evalu model develop histori three larg open-sourc softwar system mozilla firefox eclips subvers gimp applic find lvms improv perform relat file predict current lead method introduct softwar system grow size complex becom difficult develop maintain nowaday uncommon code base contain sourc file multipl program languag text document meta inform xml document web interfac even platform-depend version applic complex creat mani challeng singl develop expert thing one challeng aris whenev develop wish updat one file code base often seem local chang requir mani part code base updat unfortun depend difficult detect let denot set starter file develop wish modifi let denot set relev file requir updat modifi s. larg system develop possibl familiar entir code base autom tool recommend file given starter file extrem use number autom tool make recommend sort mine develop histori code base work area facilit code version system cvs subvers record develop histori larg softwar project histori transact denot set file joint modifi whose chang submit code base within short time interv statist analys past transact reveal file depend need modifi togeth paper explor use latent variabl model lvms model develop histori larg code base consid number differ model includ bernoulli mixtur model exponenti famili pca restrict boltzmann machin fulli bayesian approach model problem recommend relev file view problem binari matrix complet present experiment result develop histori three larg open-sourc system mozilla firefox eclips subvers gimp applic find lvms outperform current lead method mine develop histori relat work two broad class method use identifi file depend larg code base one analyz semant content code base analyz develop histori impact analysi field impact analysi draw tool softwar engin order identifi consequ code modif approach tradit attempt identifi program depend inspect and/or run program dependence-bas techniqu includ transit travers call graph well static dynam slice techniqu method identifi mani depend howev troubl certain difficult case cross-languag depend data configur file code use cross-program depend front back end compil difficulti led research explor method consid next mine develop histori data-driven method identifi file depend larg softwar project analyz develop histori two wide recogn work area ying zimmerman group use frequent itemset mine fim general heurist identifi frequent pattern larg databas pattern extract develop histori set file joint modifi point past frequent pattern pattern occur least time paramet call minimum support threshold practic tune yield best possibl balanc precis recal given databas minimum support threshold result set frequent pattern uniqu specifi much work devot make fim fast effici possibl ying use fim algorithm call fp-growth extract frequent pattern use tree-lik data structur clever design prune number possibl pattern search fpgrowth use find frequent pattern contain set starter file joint set frequent pattern return recommend baselin experi use variant fp-growth call fp-max output maxim set ad effici zimmerman use popular apriori algorithm use fim solv subtask form associ rule develop histori rule form disjoint set indic observ base experi also observ identifi rule starter file appear left hand side tool recommend file appear right hand side also work content finer granular recommend relev file also relev code block within file both ying zimmerman evalu data-driven approach f-measur measur ground-truth recommend ying ground-truth recommend file commit complet modif task record project bugzilla zimmerman ground-truth recommend file checked-in togeth point past reveal develop histori research also use develop histori detect file depend mark differ way shirabad formul problem one binari classif they label pair sourc file relev non-relev base joint modif histori robillard analyz topolog structur depend file codeblock level kagdi al improv accuraci exist file recommend method consid asymmetr file depend inform also use return partial order recommend file final sherriff identifi cluster depend file perform singular valu decomposit develop histori latent variabl model develop histori examin four latent variabl model file depend softwar system model repres develop histori larg binari matrix non-zero element row indic file checked-in togeth joint modifi point time detect depend file infer valu miss element matrix valu known element infer made probabl distribut defin model use follow notat model file list fd order collect file referenc static version develop histori transact set file modifi togeth accord develop histori repres transact d-dimension binari vector xd fi member transact otherwis develop histori set transact vector xn assum independ ident sampl under joint distribut starter set set starter file fis develop wish modifi recommend set set recommend file fjr label relev starter set s. bernoulli mixtur model simplest model explor bernoulli mixtur model figur show bmm graphic model plate notat train observ variabl binari element transact vector hidden variabl multinomi label view assign transact vector one cluster joint distribut bmm given p xi xizi iz impli graph model differ element condit independ given label here paramet denot prior probabl latent variabl paramet iz p xi denot condit mean observ variabl use em algorithm estim paramet maxim likelihood p xn transact develop histori when softwar develop wish modifi set starter file queri train bmm identifi set relev file let xis denot element transact vector indic file starter set s. let denot remain element transact vector indic file may may relev bmms infer file relev comput posterior probabl p r|s use bay rule condit independ posterior probabl given constant factor p r|s p like set relev file accord model given complet transact maxim right hand side eq unfortun effici comput posterior probabl p r|s particular set recommend file straightforward maxim eq possibl way complet transact approxim sort possibl relev file individu posterior probabl p xi fi s. recommend file whose posterior probabl p xi exceed threshold optim threshold held-out set train exampl bayesian bernoulli mixtur model also explor bayesian treatment bmm bayesian bernoulli mixtur instead learn point estim paramet introduc prior distribut make predict averag posterior distribut generat model bbm shown graphic figur bmm bbm rbm logist pca figur graphic model bernoulli mixtur model bayesian bernoulli mixtur restrict boltzmann machin logist pca bbms mixtur weight paramet drawn dirichlet prior1 dirichlet indic number mixtur compon hyperparamet dirichlet prior so-cal concentr parameter2 likewis paramet bernoulli distribut drawn beta prior beta j d-dimension vector hyperparamet beta prior exact infer bbms intract resort collaps gibb sampl make predict averag sampl posterior particular integr bernoulli paramet cluster distribut paramet sampl cluster assign variabl gibb sampl must comput condit probabl p zn j|z n nth transact assign cluster given train data cluster assign z n probabl given n nj n nij xni n nj n nij p zn j|z n n nj n nj count number transact assign cluster exclud nth transact n nij count number time ith file belong one n nj transact after full gibb sweep obtain sampl correspond count nj number point assign cluster use infer bernoulli paramet use sampl estim probabl file need chang given file starter set s. particular averag predict gibb sampl estim p xi nj n zn restrict boltzmann machin restrict boltzmann machin rbm markov random field mrf whose node typic binari random variabl graphic model rbm fulli connect bipartit preliminari experi also investig infinit mixtur bernoulli distribut replac dirichlet prior dirichlet process howev find infinit mixtur model outperform finit counterpart discuss simplic assum symmetr dirichlet prior assum graph observ variabl one layer latent variabl yj see due bipartit structur latent variabl condit independ given observ variabl vice versa rbms paper model joint distribut exp wy store weight matrix layer store respect bias observ hidden node normal factor depend model paramet product form rbms model much sharper distribut observ variabl mixtur model make interest altern consid applic rbms train maximum likelihood estim exact infer rbms intract due exponenti sum normal factor z howev condit distribut requir gibb sampl particular simpl form p xi wij yj cj p yj wij bi e z sigmoid function obtain gibb sampl use approxim gradient likelihood function respect model paramet see discuss sampl strategies3 determin whether file fi relev given starter file either clamp observ variabl repres starter file perform gibb sampl rest comput posterior remain file use fast factor approxim preliminari experi found latter work best henc recommend file comput p xi exp bi wj wi exp j fj then threshold probabl valu determin held-out exampl logist pca logist pca method dimension reduct binari data see graphic model logist pca belong famili algorithm known exponenti famili pca algorithm general pca data model non-gaussian distribut exponenti famili use logist pca stack transact vector develop histori binari matrix then model element matrix bernoulli random variabl attempt find low-rank factor real-valu matrix whose element log-odd paramet random variabl low-rank factor logist pca comput maxim log-likelihood observ data term log-odd matrix log-likelihood given lx xnd log xnd log nd obtain low dimension represent data factor log-odd matrix product two smaller matric specif nd un v`d note reduc rank play role analog number cluster bmms after obtain low-rank factor log-odd matrix uv use recommend relev file starter file fi2 fis recommend relev file comput vector optim regular log-loss ls log u vij kuk2 use approach known contrast diverg gibb sweep time period support mozilla firefox march nov eclips subvers dec may train train test file test file gimp nov may train test file tabl dataset statist show time period transact extract number transact uniqu file train test set singl starter file first term denot th column matrix second term regular paramet vector obtain way low dimension represent transact starter file s. determin whether file fi relev comput probabl p xi vi recommend file probabl exceed threshold tune threshold held-out transact develop histori experi evalu model three datasets4 construct check-in record mozilla firefox eclips subvers gimp these open-sourc project use softwar configur manag scm tool provid log allow us extract binari vector indic file chang transact experiment setup result describ experiment setup preprocess raw data obtain scm check-in record two step first follow ying al elimin transact consist file these usual correspond meaning chang second we simul minimum support threshold section remov file code base occur infrequ prune allow us make fair comparison latent variabl model lvms after pre-process dataset chronolog order first two-third use train data last one-third test data transact test set we form queri label set random pick set chang file starter file remain file chang transact form label set set file model must predict follow we includ transact label set non-empti train data tabl show number transact train test set well total number uniqu file appear these transact we train lvms follow bernoulli mixtur model bmms train fewer iter em algorithm bayesian mixtur bbms we ran separ markov chain made predict after full gibb sweeps5 rbms train iter contrast diverg start gradual increas number gibb sweep paramet logist pca learn use altern least squar procedur converg local maximum log-likelihood we initi matric svd matrix paramet lvms number hidden compon bmm rbm well number dimens regular paramet logist pca select base perform small held-out valid set hyperparamet bayesian bernoulli mixtur set base prior knowledg domain beta-prior paramet set respect reflect prior knowledg file chang in transact concentr paramet set reflect prior knowledg file depend typic form larg number small cluster these binari dataset public avail http //cseweb.ucsd.edu/ dhu/research/msr in preliminari experi we found gibb sweep suffici markov chain mix model support fim bmm bbm rbm lpca mozilla firefox start start eclips subvers start start gimp start start tabl perform fim lvms three dataset queri starter file shade column present measur white column present correct predict ratio result experi evalu perform lvm well high effici implement fim call fp-max sever experi run differ valu starter file abbrevi start minimum support threshold abbrevi support tabl show comparison model in term measur harmon mean precis recal correct predict ratio cpr fraction file we predict correct assum number file predict given latter measur reflect well model identifi relev file particular starter file without ad complic threshold experi achiev highest result two measur boldfac result we see lvms outperform popular fim approach in particular bbms outperform approach two three dataset high cpr in eclips subvers mean averag depend file detect relev bbm we also observ measur general decreas addit starter file sinc averag size transact relat small around four file firefox ad starter file must make predict less obvious in case total number relev file given us increas support other hand seem effect remov nois caus infrequ file final we see recommend most accur eclips subvers smallest dataset we believ smaller test set requir model predict far futur larger one thus result suggest onlin learn algorithm may increas accuraci discuss use lvms signific advantag tradit approach impact analysi section name abil find depend file written in differ languag show we present three cluster highest weight discov bmm in firefox data in tabl tabl reveal cluster correspond interpret structur in code span multipl data format languag first cluster deal jit compil javascript second third deal css style sheet manag web browser properti depend in last two cluster would miss convent impact analysi cluster js/src/jscntxt.h js/src/jstracer.cpp js/src/nanojit/assembler.cpp js/src/jsregexp.cpp js/src/jsapi.cpp js/src/jsarray.cpp js/src/jsfun.cpp js/src/jsinterp.cpp js/src/jsnum.cpp js/src/jsobj.cpp cluster view/src/nsviewmanager.cpp layout/generic/nshtmlreflowstate.cpp layout/reftests/bugs/reftest.list layout/style/nscssruleprocessor.cpp layout/style/nscssstylesheet.cpp layout/style/nscssparser.cpp layout/base/crashtests/crashtests.list layout/base/nsbidipresutils.cpp layout/base/nspresshell.cpp content/xbl/src/nsbindingmanager.cpp cluster browser/base/content/browser-context.inc browser/base/content/browser.j browser/base/content/pageinfo/pageinfo.xul browser/locales/en-us/chrome/browser/browser.dtd toolkit/mozapps/update/src/nsupdateservice.js.in toolkit/mozapps/update/src/updater/updater.cpp modules/plugin/base/src/nsnpapiplugininstance.h modules/plugin/base/src/nspluginhost.cpp browser/locales/en-us/chrome/browser/browser.properti view/src/nsviewmanager.cpp tabl three cluster firefox identifi bmm we show cluster largest mix proport within cluster file highest membership probabl shown note these file span multipl data format program languag reveal depend would escap notic tradit method lvms also import advantag fim given set starter file fim simpli look co-occurr data recommend set file number transact contain both frequent contrast lvms exploit higher-ord inform discov under structur data result suggest abil leverag structur lead better predict admit in term comput lvms larger one-tim train cost the fim we must first train the model generat store the gibb sampl howev for singl queri the time requir comput recommend compar the fp-max algorithm we use for fim the result the previous section also reveal signific differ the lvms we consid in the major experi mixtur model mani mixtur compon appear outperform rbms logist pca this result suggest dataset consist larg number transact number small high interrel file model data product expert such rbm difficult each individu expert the abil veto predict we tri resolv this problem use sparsiti prior the state the hidden unit make the rbms behav like a mixtur model in preliminari experi we find this improv the perform anoth interest observ the bayesian treatment the bernoulli mixtur model general lead better predict a maximum likelihood approach as less suscept overfit this advantag particular use in file depend predict requir model a larg number mixtur compon appropri model data consist mani small distinct cluster train instanc transact conclus in this paper we describ a new applic binari matrix complet for predict file depend in softwar project for this applic we investig the perform four differ lvms compar our result the wide use fim our result indic lvms signific outperform fim exploit latent higher-ord structur in the data admit our present studi still limit in scope like our result improv for instanc result the netflix competit shown blend the predict from various model often lead better perform the raw transact also contain addit inform could harvest to make accur predict such inform includ the ident user commit transact to the code base as well as the text actual chang to the sourc code it remain a grand challeng to incorpor all the avail inform from develop histori a probabilist model for predict file need to modifi in futur work we aim to explor discrimin method for paramet estim as well as onlin algorithm for track non-stationari trend in the code base acknowledg lvdm acknowledg support the netherland organis for scientif research grant by eu-fp7 noe social signal process sspnet
----------------------------------------------------------------

title: 2131-dynamic-time-alignment-kernel-in-support-vector-machine.pdf

dynam time-align kernel support vector machin hiroshi shimodaira school inform scienc japan advanc institut scienc technolog sim jaist.ac.jp mitsuru nakai school inform scienc japan advanc institut scienc technolog mit jaist.ac.jp ken-ichi noma school inform scienc japan advanc institut scienc technolog knoma jaist.ac.jp shigeki sagayama graduat school inform scienc technolog univers tokyo sagayama hil.t.u-tokyo.ac.jp abstract new class support vector machin svm applic sequential-pattern recognit speech recognit develop incorpor idea non-linear time align kernel function sinc time-align oper sequenti pattern embed new kernel function standard svm train classif algorithm employ without modif propos svm dtak-svm evalu speaker-depend speech recognit experi hand-seg phonem recognit preliminari experiment result show compar recognit perform hidden markov model hmms introduct support vector machin svm one latest success statist pattern classifi util kernel techniqu basic form svm classifi classifi input vector rn express k xi non-linear map function rn n0 denot inner product oper i-th train sampl class label lagrang multipli respect kernel function bias despit success applic svm field pattern recognit charact recognit text classif svm appli speech recognit much svm assum sampl vector fix dimens henc deal variabl length sequenc direct effort made far appli svm speech recognit employ linear time normal input featur vector sequenc differ length align length variant approach hybrid svm hmm hidden markov model hmm work pre-processor feed time-align fixed-dimension vector svm anoth approach util probabilist generat model svm kernel function includ fisher kernel condit symmetr independ csi kernel employ hmms generat model sinc hmms treat sequenti pattern svm employ generat model base hmms handl sequenti pattern well contrast approach approach direct extens origin svm case variabl length sequenc idea incorpor oper dynam time align kernel function propos new svm call dynam time-align kernel svm dtaksvm unlik svm fisher kernel requir two train stage differ train criteria one train generat model second train svm dtak-svm use one train criterion well origin svm dynam time-align kernel consid sequenc vector rn length sequenc notat sometim use repres length sequenc instead simplif first assum so-cal linear svm employ non-linear map function case kernel oper ident inner product oper formul linear kernel assum two vector sequenc two pattern equal length inner product obtain easili summat inner product xk xk therefor svm classifi defin given hand case two sequenc differ length inner product calcul direct even case howev sort inner product like oper defin if align length pattern end let time-warp function normal time frame pattern respect let new inner product oper instead origin inner product then new inner product two vector sequenc given 1x normal length either arbitrari posit integ there would two possibl type time-warp function one linear timewarp function non-linear time-warp function linear time-warp function take form dxe ceil function give smallest integ greater equal seen definit given linear warp function suitabl continu speech recognit frame-synchron process sequenc length known beforehand hand non-linear time warp dynam time warp dtw word enabl frame-synchron process furthermor past research speech recognit shown recognit perform non-linear time normal outperform one linear time normal becaus reason focus non-linear time warp base dtw though origin dtw use distance/distort measur find optim path minim accumul distance/distort dtw employ svm use inner product kernel function instead find optim path maxim accumul similar subject max nonneg path weight coeffici path normal factor constant constrain local continu standard pl dtw normal factor given weight coeffici chosen independ warp function optim problem solv effici dynam program recurs formula dynam program employ present studi follow g inp max g inp inp inp standard inner product two vector correspond point result formul non-linear kernel last subsect linear kernel inner product two vector sequenc differ length formul framework dynam time-warp littl constraint similar formul possibl case svm non-linear map function appli vector sequenc end restrict one follow form non-linear map function appli frame vector given note restrict preserv origin length sequenc cost lose long-term correl one xl result new class kernel defin use extend inner product introduc previous section ks max max call new kernel dynam time-align kernel properti dynam time-align kernel it proven propos function ks realli svm admiss kernel guarante exist featur space becaus map function featur space independ depend on given vector sequenc although class data-depend asymmetr kernel svm develop propos function complic difficult analyz becaus input data vector sequenc variabl length non-linear time normal embed function instead known propos function far ks symmetr ks satisfi cauchy-schwartz like inequ describ bellow proposit ks ks x ks proof simplif we assum normal length fix omit use standard cauchy-schwartz inequ follow inequ hold ks max kk repres optim warp function maxim rhs on hand ks max becaus we assum optim warp function maxim warp function includ follow inequ hold ks manner follow hold ks therefor ks x ks ks kk kk kk dtak-svm use dynam time-align kernel dtak introduc previous section discrimin function svm sequenti pattern express ks repres i-th train pattern it seen express svm discrimin function time sequenc form origin svm except differ kernel it straightforward deduc learn problem given min subject n. again sinc formul learn problem defin almost origin svm train algorithm origin svm use solv problem experi speech recognit experi carri evalu classif perform dtak-svm object evalu basic perform propos method limit task hand-seg phonem recognit task posit target pattern utter known chosen continu speech recognit task requir phonem label would next step experiment condit detail experiment condit given tabl train evalu sampl collect atr speech databas a-set tabl experiment condit speaker depend phonem class speaker train sampl evalu sampl signal sampl featur valu kernel type experiment-1 experiment-2 depend depend voic conson vowel male male femal sampl per phonem sampl per phonem sampl per sampl per speaker speaker frame-shift 13-mfccs mfccs kx rbf radial basi function k xi exp svs train sampl correct classif rate rbf-sigma recognit perform rbf-sigma number svs figur experiment result experiment-1 voiced-conson recognit show correct classif rate number svs function paramet rbf kernel japanes word vocabulari consonant-recognit task experiment-1 six voiced-conson use save time classif task phonem without use contextu inform consid as relat difficult task wherea classif vowel experiment-2 consid as easier task appli svm basic formul as two-class classifi multiclass problem one other type strategi chosen propos dtak-svm implement public avail toolkit svmtorch experiment result depict experiment result experiment-1 averag valu speaker shown it seen best perform achiev similar result obtain experiment-2 as given in svs train sampl correct classif rate rbf-sigma rbf-sigma recognit perform number svs figur experiment result experiment-2 vowel recognit show correct classif rate number svs as function paramet rbf kernel tabl recognit perform comparison dtak-svm hmm result experiment-1 male femal speaker shown number repres correct classif rate model hmm mix hmm mix hmm mix hmm mix dtak-svm train samples/phonem male femal next classif perform dtak-svm compar state-of-the-art hmm in order see effect general perform on size train data set model complex experi carri vari number train sampl mixtur state hmm the hmm use in this experi a 3-state continu densiti gaussian-distribut mixtur diagon covari contextindepend model htk employ this purpos the paramet dtak-svm fix the result experiment-1 respect male femal speaker given in tabl it said the experiment result dtak-svm show better classif perform the number train sampl compar perform the number sampl one might argu the number train sampl use in this experi enough hmm to achiev best perform but shortag train sampl occur often in hmmbase real-world speech recognit especi context-depend model employ prevent hmm improv the general perform conclus a novel approach to extend the svm framework for the sequential-pattern classif problem propos embed a dynam time-align oper the kernel though long-term correl the featur vector omit the cost achiev frame-synchron process for speech recognit the propos dtak-svm demonstr compar perform in hand-seg phonem recognit hmms the dtak-svm potenti applic to continu speech recognit with extens one-pass search algorithm
----------------------------------------------------------------

title: 2434-semi-definite-programming-by-perceptron-learning.pdf

semidefinit program perceptron learn ralf herbrich thore graepel microsoft research ltd cambridg uk thoreg rherb microsoft.com andriy kharechko john shawe-taylor royal holloway univers london uk ak03r jst ecs.soton.ac.uk abstract present modifi version perceptron learn algorithm pla solv semidefinit program sdps polynomi time algorithm base follow three observ semidefinit program linear program infinit mani linear constraint everi linear program solv sequenc constraint satisfact problem linear constraint iii general perceptron learn algorithm solv constraint satisfact problem linear constraint finit mani updat combin pla probabilist rescal algorithm averag increas size feasabl region result probabilist algorithm solv sdps run polynomi time present preliminari result demonstr algorithm work competit state-of-the-art interior point method introduct semidefinit program sdp one activ research area optimis appeal deriv import applic combinatori optimis control theori recent develop effici algorithm solv sdp problem depth eleg under optimis theori cover linear quadrat second-ord cone program special case recent semidefinit program discov use toolkit machin learn applic rang pattern separ via ellipsoid kernel matrix optimis transform invari learn method solv sdps most develop analog linear program generalis simplex-lik algorithm develop sdps best knowledg current mere theoret interest ellipsoid method work search feasibl point via repeat halv ellipsoid enclos affin space constraint matric centr ellipsoid feasibl point howev method show poor perform practic run time usual attain worst-cas bound third set method solv sdps interior point method method minimis linear function convex set provid set endow self-concord barrier function sinc barrier function known sdps interior point method current effici method solv sdps practic consid great general semidefinit program complex state-of-the-art solut method quit surpris forti year old simpl perceptron learn algorithm modifi solv sdps paper present combin perceptron learn algorithm pla rescal algorithm origin develop lps abl solv semidefinit program polynomi time start short introduct semidefinit program perceptron learn algorithm section section present main algorithm togeth perform guarante whose proof sketch due space restrict numer result present section preliminari give insight work algorithm demonstr machin learn may someth offer field convex optimis rest paper denot matric vector bold face upper lower case letter shall use kxk denot unit length vector direct notat use denot ax posit semidefinit learn convex optimis semidefinit program semidefinit program linear object function minimis imag affin transform cone semidefinit matric express linear matrix inequ minimis x r c0 subject f0 fi rn fi rm follow proposit show semidefinit program direct generalis linear program proposit everi semidefinit program linear program infinit mani linear constraint proof obvious object function linear rm defin vector au f1 u0 fn constraint written rm u0 rm linear constraint au f0 infinit mani sinc object function linear solv sdp sequenc semidefinit constraint satisfact problem csps introduc addit constraint c0 c0 vari c0 r. moreov follow proposit proposit ani sdp solv sequenc homogenis semidefinit csps follow form find subject gi algorithm perceptron learn algorithm requir possibl infinit set vector rn set exist x0t end return proof order make f0 c0 depend optimis variabl introduc auxiliari variabl solut origin problem given moreov repos two linear constraint c0 lmi use fact block-diagon matrix posit semi definit everi block posit semi definit thus follow matric suffici f0 fi ci g0 c0 gi given upper lower bound object function repeat bisect use determin solut o log step accuraci order simplifi notat assum whenev speak semidefinit csp sdp variabl fi rm perceptron learn algorithm perceptron learn algorithm pla onlin procedur find linear separ set point origin algorithm machin learn algorithm usual appli two set point label multipli everi data vector class label1 result vector often refer weight vector perceptron learn read normal hyperplan separ set remark properti perceptron learn algorithm total number updat independ cardin upper bound simpli term follow quantiti maxn maxn min a0 x r x r quantiti known normalis margin machin learn communiti radius feasibl region optimis communiti quantifi radius largest ball fit convex region enclos so-cal feasibl set perceptron converg theorem state purpos paper observ algorithm solv linear csp linear constraint given vector moreov last argument follow proposit proposit feasibl set posit radius perceptron learn algorithm solv linear csp finit mani step worth mention last decad seri modifi plas develop good overview main aim guarante note sometim updat equat given use unnormalis vector algorithm rescal algorithm requir maxim number step paramet set uniform random kzk pun smallest ev find au exist set gi gi return end au au end return unsolv feasibl solut also lower bound these guarante usual come price slight larger mistak bound shall denot semidefinit program perceptron learn combin proposit togeth equat obtain perceptron algorithm sequenti solv sdps howev remain two problem find vector make run time algorithm polynomi descript length data order address first problem notic algorithm explicit given defin virtu gn au g1 u0 gn rm henc find vector au au equival identifi vector rm u0 gi u0 one possibl way find vector consequ au current solut algorithm calcul eigenvector correspond smallest eigenvalu eigenvalu posit algorithm stop output note howev comput easier procedur appli find suitabl rm also section second problem requir us improv depend runtim end employ probabilist rescal algorithm algorithm origin develop lps purpos algorithm enlarg feasibl region term gn constant factor averag would impli decreas number updat perceptron algorithm exponenti number call rescal algorithm achiev run algorithm algorithm return unsolv rescal procedur gi effect au chang au au everi rm order abl reconstruct solut origin problem whenev rescal gi need rememb vector use rescal figur shown effect rescal three linear con2 note polynomi runtim guarante gn bound a polynomi function descript length data figur illustr rescal procedur shown feasibl region one feasibl point left left rescal feasibl point straint r3 main idea algorithm find a vector close current feasibl region henc lead increas radius use rescal follow properti hold algorithm theorem assum algorithm return unsolv let radius feasibl set rescal radius feasibl set rescal assum 4n probabl 4n probabl least probabilist natur theorem stem fact rescal shown increas size feasibl region if random initi valu alreadi point suffici close feasibl region a consequ theorem averag radius increas algorithm combin rescal perceptron learn result a probabilist polynomi runtim algorithm3 altern call algorithm algorithm may return infeas two case either ti mani call algorithm return unsolv mani call algorithm togeth rescal return a solut these two condit either happen unlucki draw algorithm a gn small follow argument one show min total probabl return infeas despit a gn min exceed exp experiment result experi report section fall two part initi aim demonstr method work practic assess efficaci a note assum optimis problem line algorithm solv polynomi time algorithm newton-raphson algorithm posit definit perceptron algorithm requir g1 gn rm maxim number iter set call algorithm a 4n mani updat if algorithm converg then return bx ln set ti ti call algorithm if algorithm return then yy goto outer for-loop end return infeas end return infeas benchmark exampl graph bisect these experi would also indic how competit baselin method compar solver algorithm implement in matlab experi run machin time taken compar a standard method sdpt3 partial implement in run matlab we consid benchmark problem aris semidefinit relax maxcut problem weight graph pose find a maximum weight bisect a graph benchmark maxcut problem follow relax sdp form subject diag c1 diag minimis x rn f0 fi rn n adjac matrix graph vertic benchmark use provid sdplib problem known optim valu object function equal baselin method use bisect approach identifi critic valu object refer throughout section c0 figur left show a plot time per iter valu c0 first four iter bisect method seen plot time taken algorithm each iter quit long time fourth iter around second initi valu c0 found without object constraint converg within sec bisect then start lower infeas valu upper valu iter run c0 feasibl solut object valu this found in sec second iter use a valu c0 slight optimum third iter infeas sinc quit far optimum algorithm abl deduc this fact quit quick final iter also infeas much closer optim valu run time suffer correspond take hour if we continu next iter would also infeas closer optimum would take even longer first experi demonstr sever thing first method inde work predict second run time far time sec optim valu optim valu valu object function valu object function iter figur left four iter bisect method show time taken per iter outer for loop in algorithm valu object constraint right decay attain object function valu while iter algorithm a non-zero threshold competit sdpt3 take second solv this problem third run time increas valu c0 approach optimum iter must prove infeas cost find a solut final observ prompt our first adapt base algorithm rather perform search use bisect method we implement a non-zero threshold object constraint the while-stat in algorithm the valu this threshold denot follow the notat introduc in use a valu ensur a feasibl solut found object valu signific the object constraint c0 figur right show the valu c0 a function the outer for-loop iter the algorithm eventu approach estim the optim valu this within the optimum though cours iter could continu despit the clear converg use this approach the run time accur estim the solut still prohibit overal the algorithm took approxim hour cpu time find it solut a profil the execut howev reveal the execut time spent in the eigenvalu decomposit identifi observ we need a minim eigenvector perform updat simpli a vector satisfi u0 g x u choleski decomposit either return satisfi it converg indic psd algorithm converg conclus semidefinit program interest applic in machin learn in turn we shown how a simpl learn algorithm modifi solv higher order convex optimis problem semidefinit program although the experiment result given suggest the approach far comput competit the insight gain may lead effect algorithm in concret applic in the way for exampl smo a competit algorithm for solv quadrat program problem aris support vector machin while the optimis set lead the somewhat artifici ineffici bisect method the posit definit perceptron algorithm excel solv posit definit csps found in problem transform invari pattern recognit solv semidefinit program machin in futur work it interest consid the combin primal-du problem a predefin level granular as avoid the necess bisect search acknowledg we would like to thank j. kandola j. dunagan a. ambroladz for interest discuss this work support epsrc grant number microsoft research cambridg
----------------------------------------------------------------

title: 5767-high-dimensional-neural-spike-train-analysis-with-generalized-count-linear-dynamical-systems.pdf

high-dimension neural spike train analysi general count linear dynam system lar bues depart statist columbia univers new york ny lar stat.columbia.edu yuanjun gao depart statist columbia univers new york ny yg2312 columbia.edu krishna v. shenoy depart electr engin stanford univers stanford ca shenoy stanford.edu john p. cunningham depart statist columbia univers new york ny jpc2181 columbia.edu abstract latent factor model wide use analyz simultan record spike train larg heterogen neural popul model assum signal interest popul low-dimension latent intens evolv time observ high dimens via noisi point-process observ techniqu well use captur neural correl across popul provid smooth denois concis represent high-dimension spike data one limit mani current model observ model assum poisson lack flexibl captur over-dispers common record neural data therebi introduc bias estim covari develop general count linear dynam system relax poisson assumpt use general exponenti famili count data addit contain poisson bernoulli negat binomi common count distribut special case show model tractabl learn extend recent advanc variat infer techniqu appli model data primat motor cortex demonstr perform improv state-of-the-art method captur varianc structur data held-out predict introduct mani studi theori neurosci posit high-dimension popul neural spike train noisi observ under low-dimension time-vari signal interest last decad research develop use number method joint analyz popul simultan record spike train techniqu becom critic part neural data analysi toolkit supervis set general linear model glm use stimuli spike histori covari drive spike neural popul unsupervis set latent variabl model use extract low-dimension hidden structur captur variabl record data tempor across popul neuron set howev limit spike train typic assum condit poisson given share signal poisson assumpt offer algorithm conveni mani case impli properti equal dispers condit mean varianc equal well-known properti particular troublesom analysi neural spike train common observ either under-dispers varianc greater less mean no doubli stochast process poisson observ captur under-dispers model captur over-dispers must cost erron attribut varianc latent signal rather observ process allow deviat poisson assumpt previous work instead model data gaussian use general renew process model former match count natur data found inferior latter requir cost infer extend popul set general distribut like negat binomi propos famili general case under-dispers furthermor these more general distribut yet appli import set latent variabl model here employ count-valu exponenti famili distribut address these need includ much previous work special case call distribut general count distribut offer here four main contribut introduc gc distribut deriv varieti common use distribut special case use glm motiv exampl combin observ likelihood latent linear dynam system prior form gc linear dynam system gclds iii we develop variat learn algorithm extend current state-of-the-art method gclds set we show data primat motor cortex gclds model provid superior predict perform particular captur data covari better poisson model general count distribut we defin general count distribut famili count-valu probabl distribut pgc exp k k n k m function parameter distribut normal constant primari virtu gc famili recovk=0 er common count-valu distribut special case natur parameter mani common supervis unsupervis model shown exampl function impli poisson distribut rate paramet general poisson distribut interest sinc least paper introduc gc famili prove two addit properti first expect gc distribut monoton increas fix second perhap relev this studi concav convex function impli under-dispers over-dispers gc distribut furthermor often desir featur like zero truncat zero inflat also natur incorpor modifi valu thus control log rate distribut control shape distribut gc famili provid rich model class captur spike statist neural data other discret distribut famili exist as conway-maxwell-poisson distribut order logistic/probit regress gc famili offer rich exponenti famili make comput somewhat easier allow function to interpret figur demonstr the relev model dispers in neural data analysi the left panel show scatterplot point individu neuron in record popul neuron primat motor cortex experiment detail describ in plot the mean varianc spike activ neuron activ consid in bin for
----------------------------------------------------------------

title: 1992-spectral-relaxation-for-k-means-clustering.pdf

spectral relax k-mean cluster hongyuan zha xiaofeng dept compo sci eng pennsylvania state univers univers park pa zha xhe cse.psu.edu chris ding horst simon nersc divis lawrenc berkeley nation lab uc berkeley berkeley ca chqding hdsimon lbl.gov ming gu dept mathemat uc berkeley berkeley ca mgu math.berkeley.edu abstract popular k-mean cluster partit data set minim sum-of-squar cost function coordin descend method use find local minima paper show minim reformul trace maxim problem associ gram matrix data vector furthermor show relax version trace maxim problem possess global optim solut obtain comput partial eigendecomposit gram matrix cluster assign data vector found comput pivot qr decomposit eigenvector matrix by-product also deriv lower bound minimum sum-of-squar cost function introduct k-mean popular method general cluster k-mean cluster repres center mass member shown k-mean algorithm altern assign cluster membership data vector nearest cluster center comput center cluster centroid member data vector equival find minimum sum-of-squar cost function use coordin descend despit popular kmean cluster one major drawback coordin descend search method prone local minima much research done comput refin initi point ad explicit constraint sum-of-squar cost function k-mean cluster search converg better local minimum paper tackl problem differ angl find equival formul sum-of-squar minim trace maxim problem special constraint relax constraint lead maxim problem possess optim global solut by-product also easili comput lower bound minimum sum-of-squar cost function work inspir connect gram matrix extens kmean method general mercer kernel investig rest paper organ follow section deriv equival trace maxim formul discuss spectral relax section discuss assign cluster membership use pivot qr decomposit take account special structur partial eigenvector matrix final section illustr perform cluster algorithm use document cluster exampl notat throughout denot euclidean norm vector trace matrix sum diagon element denot trace frobenius norm matrix iiaiif jtrace denot ident matrix order spectral relax given set m-dimension data vector form m-by-n data matrix partit date vector written follow form permut matrix ai m-by-si ith cluster contain data vector given partit associ sum-of-squar cost function defin si si ss ii ila~i s=l i=l s=l ll mi mean vector data vector cluster let vector appropri dimens element equal one easi see mi aiel si si ssi ila~i mil1 iiai mietii iiai isi ee isi ii s=l notic lsi ee i si project matrix isi ee i lsi ee lsi follow ssi trace ai isi ee i si af trace isi ee i si ai therefor ss ii ssi trace at ai at ai let n-by-k orthonorm matrix lvsl sk elvsi sum-of-squar cost function written ss ii trace trace xt at minim equival max trace xt at ax i form remark without loss general let i let cluster indic vector xt si easi see trace xt at ax xt at axi ax il1 i=l xtxi i=l il1 use partit right-hand side written weight sum squar euclidean norm mean vector cluster remark if consid element gram matrix at measur similar data vector shown euclidean distanc lead euclidean inner-product similar inner-product replac general mercer kernel done ignor special structur let arbitrari orthonorm matrix obtain relax maxim problem max trace xt at ax xtx=h turn trace maxim problem closed-form solut theorem ky fan let symmetr matrix eigenvalu al a2 correspond eigenvector un al ak max trace xt xtx=i moreov optim given uk q arbitrari orthogon matrix follow theorem need comput largest eigenvector gram matrix at by-product min minss ii trace max trace xt at ax xt x=h i=k+l oi largest singular valu give lower bound minimum sum-of-squar cost function easi see deriv replac aet arbitrari vector follow lower bound remark min n mjnss ii aet i=k+l one might also tri follow approach notic remark iiai mi et2 iif ilaj aj eai aj eai let ilai ajl12 xij j=l xij if aj ai otherwis ss ii wxi min xt zt z=h i=l ztwz i=n-k+l unfortun smallest eigenvalu negat let n-by-k matrix consist largest eigenvector at row correspond data vector process consid transform origin data vector live m-dimension space new data vector live k-dimension space one might attempt comput cluster assign appli ordinari k-mean method data vector reduc dimens space next section discuss altern take account structur eigenvector matrix remark similar project process princip compon analysi deceiv goal reconstruct data matrix use low-rank approxim rather captur cluster structur cluster assign use pivot qr decomposit without loss general let us assum best partit data vector a minim ss ii given a a submatrix ai correspond a cluster now write gram matrix a ata= a~a ara arak if overlap among cluster repres submatric ai small norm small as compar block diagon matrix equat let largest eigenvector at ai yi at aiyi fjiyi then column matrix iiyil1 span invari subspac b let eigenvalu eigenvector at a at axi aixi assum a gap two eigenvalu set flk min lfli aj then davis-kahan theorem state ilynxk+1 theorem after manipul shown xk iieii/j ykv o iieii k-by-k orthogon matrix ignor o iieii term see cluster cluster use yil vt a key observ vi orthogon select a vi jump cluster look at orthogon complement vi also notic iiyil1 element yi small a robust implement idea obtain as follow pick a column lar est norm say belong cluster orthogon rest column column column belong cluster residu vector small norm column residu vector tend small then pick anoth vector largest residu norm orthogon residu vector residu vector process carri step turn exact qr decomposit column pivot appli find a permut matrix qr q rl1 rd a k-by-k orthogon matrix rl1 a k-by-k upper triangular matrix then comput matrix rj rd pt rj then cluster membership data vector determin row index largest element absolut valu correspond column remark sometim it may advantag includ eigenvector form xs still use qr decomposit column pivot select column xs form an s-by-k matrix say then column xs comput least squar solut argminterk li xtll then cluster membership determin row index largest element absolut valu experiment result section present experiment result cluster a dataset newsgroup articl submit newsgroups.1 dataset contain articl email messag even divid among newsgroup list name news group togeth associ group label lthe newsgroup dataset togeth bow toolkit process it downloadedfrorn http www cs.cmu.edu/afs/cs/project/theo-ll/www/naive-bayes.html p-kmean figur cluster accuraci five newsgroup p-qr p-kmean left p-kmean kmean right alt.ath comp.graph comp.os.ms-vindovs.misc comp.sys.ibm.pc.hardvar ng5 comp.sys.mac.hardvar comp.vindovs.x ng7 misc.forsal rec.auto ng9 rec.motorcycl rec.sport.basebal ngll rec.sport.hockey sci crypt ng13 sci.electron sci.m ng15 sci.spac soc.religion.christian ng17 talk.politics.gun talk.politics.mideast ng19 talk.politics.misc talk.religion.misc use bow toolkit construct term-docu matrix dataset specif use token option usenet header strip also appli stem follow three preprocess step done appli usual tf.idf weight scheme delet word appear time normal document vector unit euclidean length test three cluster algorithm p-qr refer algorithm use eigenvector matrix follow pivot qr decomposit cluster membership assign p-kmean comput eigenvector matrix then appli k-mean row eigenvector matrix k-mean k-mean direct appli origin data vector k-mean method start a set cluster center chosen random project data vector aslo make sure random set use comparison assess qualiti a cluster algorithm take advantag fact news group data alreadi label measur perform accuraci cluster algorithm document categori label particular a cluster case we comput a k-by-k confus matrix cij cij number document in cluster belong newsgroup categori it actual quit subtl comput accuraci use confus matrix we know cluster match newsgroup categori an optim way solv follow maxim problem max trace cp ip a permut matrix divid maximum total number document get accuraci this equival find perfect match a complet weight bipartit graph one use kuhn-munkr algorithm in our experi we use a greedi algorithm comput a sub-optim solut tabl comparison p-qr p-kmean k-mean two-way cluster newsgroup ng1/ng2 ng2/ng3 ng8/ng9 ng1/ng p-qr p-kmean k-mean tabl comparison p-qr p-kmean k-mean multi-way cluster newsgroup ng2/ng3/ng4/ng5/ng6 ng2/ng3/ng4/ng5/ng6 uoo ng1/ng5/ng7/ng8/ng ng1/ng5/ng7 ng8/ng p-qr p-kmean k-mean in this exampl we look at binari cluster we choos random document vector two newsgroup we test run pair newsgroup list mean standard deviat in tabl two cluster algorithm p-qr p-kmean compar to better sometim substanti better han k-mean exampl in this exampl we consid k-way cluster three news group set chosen random sampl newsgroup as indic in parenthesi again run use each test mean standard deviat list in tabl moreov in figur we also plot the accuraci for the run for the test p-qr p-kmean perform better han kmean for news group set small overlap p-qr perform better han p-kmean this might explain fact hat p-qr explor the special structur eigenvector matrix therefor effici as a less thorough comparison wit the inform bottleneck method use in for run ng2/ng9/nglo/ng15/ng mean accuraci maximum accuraci obtain for run the newsgroup set sampl mean accuraci maximum accuraci obtain exampl we compar the lower bound given in we list a typic sampl ng2/ng9/nglo/ng15/ng18 the column ng label indic cluster use the newsgroup label definit accuraci it quit clear the news group categori complet captur he sum-of-squar cost function p-qr ng label both higher accuraci also larger sum-of-squar valu interest it seem hat p-qr captur this inform the newsgroup categori exampl accuraci ssm p-qr p-kmean k-mean ng label lower bound n/a acknowledg this work support in part nsf grant depart energi an lbl ldrd fund
----------------------------------------------------------------

title: 3963-learning-concept-graphs-from-text-with-stick-breaking-priors.pdf

learn concept graph text stick-break prior padhraic smyth depart comput scienc univers california irvin irvin ca smyth ics.uci.edu america l. chamber depart comput scienc univers california irvin irvin ca ahollowa ics.uci.edu mark steyver depart cognit scienc univers california irvin irvin ca mark.steyv uci.edu abstract present generat probabilist model learn general graph structur term concept graph text concept graph provid visual summari themat content collect document task difficult accomplish use keyword search propos model learn differ type concept graph structur capabl util partial prior knowledg graph structur well label document describ generat model base stick-break process graph markov chain mont carlo infer procedur experi simul data show model recov known graph structur learn unsupervis semi-supervis mode also show propos model competit term empir log likelihood exist structure-bas topic model hpam hlda real-world text data set final illustr applic model problem updat wikipedia categori graph introduct present generat probabilist model learn concept graph text defin concept graph root direct graph node repres themat unit call concept edg repres relationship concept concept graph use summar document collect provid visual themat content structur larg document set task difficult accomplish use keyword search exampl concept graph wikipedia categori graph1 figur show small portion wikipedia categori graph root categori achin learning2 graph quick infer collect machin learn articl wikipedia focus primarili evolutionari algorithm markov model less emphasi aspect machin learn bayesian network kernel method problem address paper learn concept graph given collect document option may concept label document initi graph structur latter scenario task identifi addit concept corpus http //en.wikipedia.org/wiki/categori main topic classif may appli scienc softwar engin mathemat scienc comput program appli mathemat formal scienc comput probabl statist philosophi field thought knowledg share algorithm societi cognit educ comput statist philosophi mind artifici intellig knowledg statist cognit scienc metaphys comput scienc learn machin learn figur portion wikipedia categori supergraph node achin learn machin learn bayesian network ensembl learn classif algorithm genet algorithm evolutionari algorithm kernel method genet program interact evolutionari comput learn comput vision markov model markov network statist natur languag process figur portion wikipedia categori subgraph root node achin learn reflect graph addit relationship concept corpus via cooccurr concept document reflect graph particular suit document collect like wikipedia set articl chang fast rate automat method updat concept graph may prefer manual edit re-learn hierarchi scratch foundat approach latent dirichlet alloc lda lda probabilist model automat identifi topic within document collect topic probabl distribut word standard lda model includ notion relationship depend topic contrast method hierarch topic model hlda learn set topic form tree structur restrict tree structur howev well suit larg document collect like wikipedia figur give exampl high non-tre like natur wikipedia categori graph hierarch pachinko alloc model hpam abl learn set topic arrang fixeds graph nonparametr version introduc model propos paper simpler altern hpam nonparametr hpam achiev flexibl learn arbitrari direct acycl graph possibl infinit number node within simpler probabilist framework addit model provid formal mechan util label data exist concept graph structur method creat concept graph includ use techniqu hierarch cluster pattern mine formal concept analysi construct ontolog document collect approach differ util probabilist framework enabl us exampl make infer concept document primari novel contribut introduct flexibl probabilist framework learn general graph structur text capabl util unlabel document well label document prior knowledg form exist graph structur next section introduc stick-break distribut show use prior graph structur introduc generat model explain adapt case initi graph structur deriv collaps gibb sampl equat model present seri experi simul real text data compar perform hlda hpam baselin conclud discuss merit limit approach stick-break distribut stick-break distribut discret probabl distribut form delta function center atom variabl sampl independ base distribut assum continu stick-break weight form v1 vj vk vj independ beta j random variabl stick-break distribut deriv name analog repeat break remaind unit-length stick random chosen breakpoint see detail unlik chines restaur process stick-break process lack exchang probabl sampl particular cluster given sequenc vj equal probabl sampl cluster given permut sequenc seen equat probabl sampl depend upon valu proceed beta random variabl v2 fix permut everi atom probabl sampl chang determin beta random variabl stick-break distribut util prior distribut graph structur construct prior graph structur specifi distribut node denot pt govern probabl transit node anoth node graph freedom choos pt howev two constraint first make new transit must non-zero probabl figur clear achin earn abl transit children howev may discov evid pass direct leaf node tatist natur anguag rocess if observ new articl relat statist natur languag process use markov model second make transit new node must non-zero probabl exampl may observ new articl relat topic bioinformat case want add new node graph ioinformat assign probabl transit node two requir provid formal definit pt begin initi graph structur g0 node node defin feasibl set ft collect node transit feasibl set may contain children node possibl child node node discuss general ft subset node g0 add special node call exit node ft if sampl exit node exit graph instead transit forward defin pt stick-break distribut finit set node ft remain probabl mass assign infinit set new node node exist yet observ exact form pt shown pt ft tj ftj tj xtj j=|ft first ft atom stick-break distribut feasibl node ftj ft remain atom unidentifi node yet observ denot xtj simplic yet work definit unless explicit state node set ft model general assum specif form ft instead user free defin like experi first assign node uniqu depth defin ft node next lower depth choic ft determin type graph structur learn choic ft use paper edg travers multipl depth allow edg node depth allow prevent cycl form allow infer perform time manner general one could extend definit ft includ node lower depth node sampl stick-break weight vtj beta ii sampl word distribut dirichlet document sampl distribut level beta b ii sampl path pd pt iii word nd sampl level ld truncateddiscret generat word xd ld multinomi pd ldi figur generat process graphlda due lack exchang must specifi stick-break order element ft note despit order element ft alway occur infinit set new node stick-break permut use metropolis-hast sampler propos learn permut feasibl node highest likelihood given data generat process figur show generat process propos model refer graphlda observ collect document document nd word discuss earlier node associ stick-break prior pt addit associ node multinomi distribut word fashion topic model two-stag process use generat document first path graph sampl stick-break distribut denot path pd 1st node path sampl ppdi stick-break distribut ith node path process continu exit node sampl word level path ldi sampl truncat discret distribut word generat topic level ldi path pd denot pd ldi case observ label document initi graph structur path document restrict end concept label document one possibl option length distribut multinomi distribut level take differ approach instead use parametr smooth form motiv constrain length distribut general function form across document contrast relat unconstrain multinomi allow paramet distribut documentspecif consid two simpl option geometr poisson truncat number possibl level initi experi geometr perform better poisson geometr use experi report paper if word xdi level ldi then word generat topic last node path success level correspond earlier node path case label document match belief major word document assign concept label infer margin topic distribut stick-break weight vtj use collaps gibb sampler infer path assign pd document level distribut paramet document level assign ldi word five hyperparamet model infer sensit valu place exponenti prior use metropolis-hast sampler learn best set sampl path document must sampl path pd condit path p level variabl word token consid path whose length greater equal maximum level word document p pd p p xd p pd first term equat probabl word document given path pd comput probabl margin topic distribut np npd p xd npd np use denot length path pd notat npd stand number time word type assign node pd superscript mean we first decrement count npd everi word document second term condit probabl path pd given path p we present sampl equat assumpt maximum number node allow level we first consid probabl sampl singl edg path node one feasibl node y2 ym node y1 first posit stickbreak permut y2 second posit y3 third we denot number path gone n x yi we denot number path gone node strict higher posit stick-break distribut pm n x yi n x yi n x yk extend notat we denot sum n x yi n x yi n x yi probabl select node given p x p n x yr n x yi n x yi n x yr if ym last node nonzero count n x ym conveni comput probabl transit probabl transit node higher ym probabl transit node higher ym given p x yk qm similar deriv use comput probabl sampl node higher ym equal infin now we comput probabl singl edg we comput probabl entir path pd p pd p pdj sampl level ith word dth document we must sampl level ldi condit other level l di document path level paramet word token np di ldi ldi xdi p ldi l di np di di first term probabl word type xdi given topic node pd ldi second term probabl level ldi given level paramet sampl variabl final we must sampl level distribut condit rest level paramet level variabl word token nd ldi learn graph label document simul graph learn graph label document learn graph label document figur learn result simul data due normal constant equat recogniz probabl distribut we must use reject sampl sinc first term equat alway less equal sampl distribut domin beta distribut accord reject sampl algorithm we sampl candid valu beta either qnd ldi accept probabl reject sampl metropoli hast stick-break permut addit gibb sampl we employ metropoli hast sampler present mix stick-break permut consid node feasibl node y2 ym we sampl two feasibl node yj uniform distribution3 assum come yj stick-break distribut then probabl swap posit node yj given n x n x yj n x yj min n x yj n x yi n x yj see full deriv after everi new path assigni ment we propos one swap node graph experi result section we present experi perform simul real text data we compar perform graphlda hpam hlda simul text data in section we illustr perform graphlda improv fraction label data increas figur show simul concept graph node drawn accord in feasibl node sampl prior probabl distribut howev small valu this result in extrem slow mix stick-break generat process paramet valu vocabulari size word we generat document word edg in graph label number path travers figur show learn graph structur fraction label data increas label unlabel document document label in addit label edg we label node base upon similar learn topic node topic origin graph structur gibb sampler initi root node there label data label data gibb sampler initi correct placement node level sampler observ edg structur graph correct number node level sampler may add addit node label data sampler unabl recov relationship concept due relat small number document contain word concept label document sampler abl learn correct placement node although topic contain nois wikipedia articl in this section we compar perform graphlda hpam hlda set machine-learn articl taken wikipedia input model articl text all model restrict learn three-level hierarch structur graphlda hpam number node level set graphlda paramet fix paramet initi respect optim use metropoli hast sampler we use mallet toolkit implement hpam4 hlda hpam we use differ set topic hyperparamet for hlda we set consid smooth paramet for chines restaur process smooth level in graph all model run for iter ensur burn-in sampl taken everi iter thereaft for total iter perform model evalu hold-out set consist articl use empir likelihood left-toright evalu algorithm section measur general unseen data for graphlda hlda we use distribut path learn train comput per-word log likelihood for hpam we comput mle estim dirichlet hyperparamet for distribut super-top distribut sub-top train document tabl show per-word log-likelihood for model averag ten sampl graphlda competit comput the empir log likelihood we specul graphlda lower perform in term left-to-right log-likelihood due our choic the geometr distribut level our choic posit the geometr distribut the last node the path more flexibl approach could result in better perform tabl per-word log likelihood test document model paramet empir ll left-to-right ll graphlda mh opt hpam hlda set data learn concept model network neural neuron cnn function genet fit mutat select solut markov time probabl chain distribut graph markov network random field evolut evolutionari algorithm individu search word topic languag model document variabl node network parent bayesian model multitask infer bayesian dirichlet learn data model method kernel model nois algorithm hidden train learn polici decis graph function decis classif class classifi data cluster data princip compon kmean learn dimension classif reduct method model select rbm algorithm featur kernel linear space vector point learn algorithm kernel convex constraint algorithm svm vector problem multiclass classifi boost ensembl hypothesi margin figur wikipedia graph structur addit machin learn abstract the edg width correspond the probabl the edg in the graph wikipedia articl graph structur in our final experi we illustr graphlda use updat exist categori graph we use the aforement machine-learn wikipedia articl along with categori label learn topic distribut for node in figur the sampler initi with the correct placement node document initi random path the root categori label after iter we fix the path assign for the wikipedia articl introduc new set document we use collect machin learn abstract the intern confer machin learn icml we sampl path for the new collect document keep the path the wikipedia articl fix the sampler allow add new node level explain new concept occur in the icml text set figur illustr portion the final graph structur the node in bold the origin node the wikipedia categori graph the result show the model capabl augment exist concept graph with new concept cluster support vector machin svms etc learn meaning relationship boosting/ensembl the path the concept for svms neural network discuss conclus motiv the increas avail large-scal structur collect document as wikipedia we present flexibl non-parametr bayesian framework for learn concept graph from text the propos approach combin unlabel data with prior knowledg in the form label document exist graph structur extens as allow the model handl multipl path per document like worth pursu in this paper we discuss scalabl larg graph like an import issu in practic comput the probabl everi path sampl the number graph a product the number of node each level a comput bottleneck in the current infer algorithm scale approxim infer method that address this issu quit use in this context acknowledg this materi base upon work support in part the nation scienc foundat award number a microsoft scholarship by a googl faculti research award the author would also like thank ian porteous alex ihler for use discuss mallet implement the exit node version of hpam
----------------------------------------------------------------

