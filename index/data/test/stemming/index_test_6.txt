query sentence: machine-learning toolkit 
---------------------------------------------------------------------
title: 3963-learning-concept-graphs-from-text-with-stick-breaking-priors.pdf

Learning Concept Graphs from Text with
Stick-Breaking Priors

Padhraic Smyth
Department of Computer Science
University of California, Irvine
Irvine, CA 92607
smyth@ics.uci.edu

America L. Chambers
Department of Computer Science
University of California, Irvine
Irvine, CA 92697
ahollowa@ics.uci.edu

Mark Steyvers
Department of Cognitive Science
University of California, Irvine
Irvine, CA 92697
mark.steyvers@uci.edu

Abstract
We present a generative probabilistic model for learning general graph structures,
which we term concept graphs, from text. Concept graphs provide a visual summary of the thematic content of a collection of documents?a task that is difficult
to accomplish using only keyword search. The proposed model can learn different
types of concept graph structures and is capable of utilizing partial prior knowledge about graph structure as well as labeled documents. We describe a generative
model that is based on a stick-breaking process for graphs, and a Markov Chain
Monte Carlo inference procedure. Experiments on simulated data show that the
model can recover known graph structure when learning in both unsupervised and
semi-supervised modes. We also show that the proposed model is competitive
in terms of empirical log likelihood with existing structure-based topic models
(hPAM and hLDA) on real-world text data sets. Finally, we illustrate the application of the model to the problem of updating Wikipedia category graphs.

1

Introduction

We present a generative probabilistic model for learning concept graphs from text. We define a
concept graph as a rooted, directed graph where the nodes represent thematic units (called concepts)
and the edges represent relationships between concepts. Concept graphs are useful for summarizing
document collections and providing a visualization of the thematic content and structure of large
document sets - a task that is difficult to accomplish using only keyword search. An example of
a concept graph is Wikipedia?s category graph1 . Figure 1 shows a small portion of the Wikipedia
category graph rooted at the category M ACHINE LEARNING2 . From the graph we can quickly infer that the collection of machine learning articles in Wikipedia focuses primarily on evolutionary
algorithms and Markov models with less emphasis on other aspects of machine learning such as
Bayesian networks and kernel methods.
The problem we address in this paper is that of learning a concept graph given a collection of
documents where (optionally) we may have concept labels for the documents and an initial graph
structure. In the latter scenario, the task is to identify additional concepts in the corpus that are
1
2

http://en.wikipedia.org/wiki/Category:Main topic classifications
As of May 5, 2009

1

Applied
Sciences

Software
Engineering
Mathematical
Sciences

Computer
Programming

Applied
Mathematics

Formal
Sciences

Computing

Probability and
Statistics

Philosophy
By field

Thought

Knowledge
Sharing
Algorithms

Society

Cognition

Education
Computational Statistics

Philosophy
Of mind

Artificial
Intelligence

Knowledge

Statistics

Cognitive
Science

Metaphysics

Computer
Science

Learning

Machine learning

Figure 1: A portion of the Wikipedia category supergraph for the node M ACHINE LEARNING

Machine
Learning

Bayesian
Networks

Ensemble
Learning

Classification
Algorithms

Genetic
Algorithms

Evolutionary
Algorithms

Kernel
Methods

Genetic
Programming

Interactive
Evolutionary
Computation

Learning in
Computer
Vision

Markov
Models

Markov
Networks

Statistical
Natural
Language
Processing

Figure 2: A portion of the Wikipedia category subgraph rooted at the node M ACHINE LEARNING

not reflected in the graph or additional relationships between concepts in the corpus (via the cooccurrence of concepts in documents) that are not reflected in the graph. This is particularly suited
for document collections like Wikipedia where the set of articles is changing at such a fast rate
that an automatic method for updating the concept graph may be preferable to manual editing or
re-learning the hierarchy from scratch. The foundation of our approach is latent Dirichlet allocation
(LDA) [1]. LDA is a probabilistic model for automatically identifying topics within a document
collection where a topic is a probability distribution over words. The standard LDA model does
not include any notion of relationships, or dependence, between topics. In contrast, methods such
as the hierarchical topic model (hLDA) [2] learn a set of topics in the form of a tree structure. The
restriction to tree structures however is not well suited for large document collections like Wikipedia.
Figure 1 gives an example of the highly non-tree like nature of the Wikipedia category graph. The
hierarchical Pachinko allocation model (hPAM) [3] is able to learn a set of topics arranged in a fixedsized graph with a nonparametric version introduced in [4]. The model we propose in this paper is
a simpler alternative to hPAM and nonparametric hPAM that can achieve the same flexibility (i.e.
learning arbitrary directed acyclic graphs over a possibly infinite number of nodes) within a simpler
probabilistic framework. In addition, our model provides a formal mechanism for utilizing labeled
data and existing concept graph structures. Other methods for creating concept graphs include the
use of techniques such as hierarchical clustering, pattern mining and formal concept analysis to
construct ontologies from document collections [5, 6, 7]. Our approach differs in that we utilize
a probabilistic framework which enables us (for example) to make inferences about concepts and
documents. Our primary novel contribution is the introduction of a flexible probabilistic framework
for learning general graph structures from text that is capable of utilizing both unlabeled documents
as well as labeled documents and prior knowledge in the form of existing graph structures.
In the next section we introduce the stick-breaking distribution and show how it can be used as
a prior for graph structures. We then introduce our generative model and explain how it can be
adapted for the case where we have an initial graph structure. We derive collapsed Gibbs? sampling
equations for our model and present a series of experiments on simulated and real text data. We
compare our performance against hLDA and hPAM as baselines. We conclude with a discussion of
the merits and limitations of our approach.
2

2

Stick-breaking Distributions

Stick-breaking distributions P(?) are discrete probability distributions of the form:
P(?) =

?
X

?j ?xj (?)

where

j=1

?
X

?j = 1, 0 ? ?j ? 1

j=1

and ?xj (?) is the delta function centered at the atom xj . The xj variables are sampled independently
from a base distribution H (where H is assumed to be continuous). The stick-breaking weights ?j
have the form
j?1
Y
? 1 = v1 ,
? j = vj
(1 ? vk ) for j = 2, 3, . . . , ?
k=1

where the vj are independent Beta(?j , ?j ) random variables. Stick-breaking distributions derive
their name from the analogy of repeatedly breaking the remainder of a unit-length stick at a randomly
chosen breakpoint. See [8] for more details.
Unlike the Chinese restaurant process, the stick-breaking process lacks exchangeability. The probability of sampling a particular cluster from P(?) given the sequences {xj } and {vj } is not equal
to the probability of sampling the same cluster given a permutation of the sequences {x?(j) } and
{v?(j) }. This can be seen in Equation 2 where the probability of sampling xj depends upon the
value of the j ? 1 proceeding Beta random variables {v1 , v2 , . . . , vj?1 }. If we fix xj and permute
every other atom, then the probability of sampling xj changes: it is now determined by the Beta
random variables {v?(1) , v?(2) , . . . , v?(j?1) }.
The stick-breaking distribution can be utilized as a prior distribution on graph structures. We construct a prior on graph structures by specifying a distribution at each node (denoted as Pt ) that
governs the probability of transitioning from node t to another node in the graph. There is some
freedom in choosing Pt ; however we have two constraints. First, making a new transition must have
non-zero probability. In Figure 1 it is clear that from M ACHINE L EARNING we should be able to
transition to any of its children. However we may discover evidence for passing directly to a leaf
node such as S TATISTICAL NATURAL L ANGUAGE P ROCESSING (e.g. if we observe new articles
related to statistical natural language processing that do not use Markov models). Second, making
a transition to a new node must have non-zero probability. For example, we may observe new articles related to the topic of Bioinformatics. In this case, we want to add a new node to the graph
(B IOINFORMATICS) and assign some probability of transitioning to it from other nodes.
With these two requirements we can now provide a formal definition for Pt . We begin with an
initial graph structure G0 with t = 1 . . . T nodes. For each node t we define a feasible set Ft as the
collection of nodes to which t can transition. The feasible set may contain the children of node t or
possible child nodes of node t (as discussed above). In general, Ft is some subset of the nodes in
G0 . We add a special node called the ?exit node? to Ft . If we sample the exit node then we exit
from the graph instead of transitioning forward. We define Pt as a stick-breaking distribution over
the finite set of nodes Ft where the remaining probability mass is assigned to an infinite set of new
nodes (nodes that exist but have not yet been observed). The exact form of Pt is shown below.
Pt (?) =

|Ft |
X

?tj ?ftj (?) +

j=1

?
X

?tj ?xtj (?)

j=|Ft |+1

The first |Ft | atoms of the stick-breaking distribution are the feasible nodes ftj ? Ft . The remaining
atoms are unidentifiable nodes that have yet to be observed (denoted as xtj for simplicity).
This is not yet a working definition unless we explicitly state which nodes are in the set Ft . Our
model does not in general assume any specific form for Ft . Instead, the user is free to define it as
they like. In our experiments, we first assign each node to a unique depth and then define Ft as any
node at the next lower depth. The choice of Ft determines the type of graph structures that can be
learned. For the choice of Ft used in this paper, edges that traverse multiple depths are not allowed
and edges between nodes at the same depth are not allowed. This prevents cycles from forming
and allows inference to be performed in a timely manner. More generally, one could extend the
definition of Ft to include any node at a lower depth.
3

1. For node t ? {1, . . . , ?}
i. Sample stick-break weights {vtj }|?, ? ? Beta(?, ?)
ii. Sample word distribution ?t |? ? Dirichlet(?)
2. For document d ? {1, 2, . . . D}
i. Sample a distribution over levels ?d |a, b ? Beta(a,b)
ii. Sample path pd ? {Pt }?
t=1
iii. For word i ? {1, 2, . . . , Nd }
Sample level ld,i ? TruncatedDiscrete(?d )
Generate word xd,i |{pd , ld,i , ?} ? Multinomial(?pd [ldi ] )
Figure 3: Generative process for GraphLDA

Due to a lack of exchangeability, we must specify the stick-breaking order of the elements in Ft .
Note that despite the order, the elements of Ft always occur before the infinite set of new nodes in
the stick-breaking permutation. We use a Metropolis-Hastings sampler proposed by [10] to learn
the permutation of feasible nodes with the highest likelihood given the data.

3

Generative Process

Figure 3 shows the generative process for our proposed model, which we refer to as GraphLDA.
We observe a collection of documents d = 1 . . . D where document d has Nd words. As discussed
earlier, each node t is associated with a stick-breaking prior Pt . In addition, we associate with each
node a multinomial distribution ?t over words in the fashion of topic models.
A two-stage process is used to generate document d. First, a path through the graph is sampled
from the stick-breaking distributions. We denote this path as pd . The i + 1st node in the path
is sampled from Ppdi (?) which is the stick-breaking distribution at the ith node in the path. This
process continues until an exit node is sampled. Then for each word xi a level in the path, ldi , is
sampled from a truncated discrete distribution. The word xi is generated by the topic at level ldi
of the path pd which we denote as pd [ldi ]. In the case where we observe labeled documents and an
initial graph structure the paths for document d is restricted to end at the concept label of document
d.
One possible option for the length distribution is a multinomial distribution over levels. We take
a different approach and instead use a parametric smooth form. The motivation is to constrain the
length distribution to have the same general functional form across documents (in contrast to the relatively unconstrained multinomial), but to allow the parameters of the distribution to be documentspecific. We considered two simple options: Geometric and Poisson (both truncated to the number
of possible levels). In initial experiments the Geometric performed better than the Poisson, so the
Geometric was used in all experiments reported in this paper. If word xdi has level ldi = 0 then the
word is generated by the topic at the last node on the path and successive levels correspond to earlier
nodes in the path. In the case of labeled documents, this matches our belief that a majority of words
in the document should be assigned to the concept label itself.

4

Inference

We marginalize over the topic distributions ?t and the stick-breaking weights {vtj }. We use a
collapsed Gibbs sampler [9] to infer the path assignment pd for each document, the level distribution
parameter ?d for each document, and the level assignment ldi for each word. Of the five hyperparameters in the model, inference is sensitive to the value of ? and ? so we place an Exponential
prior on both and use a Metropolis-Hastings sampler to learn the best setting.
4.1

Sampling Paths

For each document, we must sample a path pd conditioned on all other paths p?d , the level variables,
and the word tokens. We only consider paths whose length is greater than or equal to the maximum
4

level of the words in the document.
p(pd |x, l, p?d , ? ) ? p(xd |x?d , l, p) ? p(pd |p?d )

(1)

The first term in Equation 1 is the probability of all words in the document given the path pd . We
compute this probability by marginalizing over the topic distributions ?t :
!
P
?d
V
Y
Y
?(V ? + v Np?d
)
?(? + Npd [l],v )
d [l],v
P
p(xd |x?d , l, p) =
?
?d
?(V ? + v Npd [l],v )
?(? + Np [l],v )
v=1
l=1

d

We use ?d to denote the length of path pd . The notation Npd [l],v stands for the number of times
word type v has been assigned to node pd [l]. The superscript ?d means we first decrement the count
Npd [l],v for every word in document d.
The second term is the conditional probability of the path pd given all other paths p?d . We present
the sampling equation under the assumption that there is a maximum number of nodes M allowed
at each level. We first consider the probability of sampling a single edge in the path from a node x
to one of its feasible nodes {y1 , y2 , . . . , yM } where the node y1 has the first position in the stickbreaking permutation, y2 has the second position, y3 the third and so on.
We denote the number of paths that have gone from x to yi as N(x,yi ) . We denote the number of
paths that have gone from x to a node with a strictly higher position in the stick-breaking distribution
PM
than yi as N(x,>yi ) . That is, N(x,>yi ) = k=i+1 N(x,yk ) . Extending this notation we denote the
sum N(x,yi ) + N(x,>yi ) as N(x,?yi ) . The probability of selecting node yi is given by:
p(x ? yi | p?d ) =

i?1
Y ? + N(x,>yr )
? + N(x,yi )
? + ? + N(x,?yi ) r=1 ? + ? + N(x,?yr )

for i = 1 . . . M

If ym is the last node with a nonzero count N(x,ym ) and m << M it is convenient to compute the
probability of transitioning to yi , for i ? m, and the probability of transitioning to any node higher
than ym . The probability of transitioning to a node higher than ym is given by
"
#
M
X
? M ?m
p(x ? yk |p?d ) = ? 1 ?
?+?
k=m+1

?+N(x,>yr )
r=1 ?+?+N(x,?yr ) .

Qm

where ? =
A similar derivation can be used to compute the probability of
sampling a node higher than ym when M is equal to infinity. Now that we have computed the
probability of a single edge, we can compute the probability of an entire path pd :
p(pd |p?d ) =

?d
Y

p(pdj ? pd,j+1 |p?d )

j=1

4.2

Sampling Levels

For the ith word in the dth document we must sample a level ldi conditioned on all other levels l?di ,
the document paths, the level parameters ? , and the word tokens.
!
? + Np?di
(1 ? ?d )ldi ?d
d [ldi ],xdi
p(ldi |x, l?di , p, ? ) =
?
(1 ? (1 ? ?d )?d +1 )
W ? + Np?di
[l ],?
d

di

The first term is the probability of word type xdi given the topic at node pd [ldi ]. The second term is
the probability of the level ldi given the level parameter ?d .
4.3

Sampling ? Variables

Finally, we must sample the level distribution ?d conditioned on the rest of the level parameters ? ?d ,
the level variables, and the word tokens.
!
!
Nd
Y
?da?1 (1 ? ?d )b?1
(1 ? ?d )ldi ?d

(2)
p(?d |x, l, p, ? ?d ) =
?
(1 ? (1 ? ?d )?d +1 )
B a, b
i=1
5

1

1

973

1069

2

1060

973

957

2

4

3

957

9

4

3

3/10
9

486

331

385

524

524

6

5
306

8

7

453

496

278

513

194

5
316

154

2

484

235

384

4/7

274

2

486

283

1

8/4/1

7/10

268

245

1069

973

331

385

6

4

453

524

524

278

513

154

8

7

24

9

10

9/2

957

3

5
306

26

4

8/4

968

512

6/9

275

(b) Learned Graph (0 labeled documents)

3

5/1

4

1

1057

2

5/2

275

20

1

972

682

9

(a) Simulated Graph

2

515

20

7/10

423

10

9

545

6

10

(d) Learned Graph (4000 labeled documents)

(c) Learned Graph (250 labeled documents)

Figure 4: Learning results with simulated data
Due to the normalization constant (1 ? (1 ? ?d )?d +1 ), Equation 2 is not a recognizable probability
distribution and we must use rejection sampling. Since the first term in Equation 2 is always less
than or equal to 1, the sampling distribution is dominated by a Beta(a, b) distribution. According
to the rejection sampling algorithm, we sample a candidate value for ?d from Beta(a, b) and either
QNd (1??d )ldi ?d
accept with probability i=1
or reject and sample again.
(1?(1?? )?d +1 )
d

4.4

Metropolis Hastings for Stick-Breaking Permutations

In addition to the Gibbs sampling, we employ a Metropolis Hastings sampler presented in [10] to
mix over stick-breaking permutations. Consider a node x with feasible nodes {y1 , y2 , . . . , yM }. We
sample two feasible nodes yi and yj from a uniform distribution3 . Assume yi comes before yj in
the stick-breaking distribution. Then the probability of swapping the position of nodes yi and yj is
given by
)
( N(x,y ) ?1
N(x,yj ) ?1
?
i
Y
Y ? + ? + N(x,>yj ) + k
? + ? + N(x,>y
+k
i)
?
min 1,
?
? + ? + N(x,>yj ) + k
? + ? + N(x,>y
+k
i)
k=0

k=0

?
where N(x,>y
= N(x,>yi ) ? N(x,yj ) . See [10] for a full derivation. After every new path assigni)
ment, we propose one swap for each node in the graph.

5

Experiments and Results

In this section, we present experiments performed on both simulated and real text data. We compare
the performance of GraphLDA against hPAM and hLDA.
5.1

Simulated Text Data

In this section, we illustrate how the performance of GraphLDA improves as the fraction of labeled
data increases. Figure 4(a) shows a simulated concept graph with 10 nodes drawn according to the
3
In [10] feasible nodes are sampled from the prior probability distribution. However for small values of ?
and ? this results in extremely slow mixing.

6

stick-breaking generative process with parameter values ? = .025, ? = 10, ? = 10, a = 2 and
b = 5. The vocabulary size is 1, 000 words and we generate 4, 000 documents with 250 words each.
Each edge in the graph is labeled with the number of paths that traverse it.
Figures 4(b)-(d) show the learned graph structures as the fraction of labeled data increases from
0 labeled and 4, 000 unlabeled documents to all 4, 000 documents being labeled. In addition to
labeling the edges, we label each node based upon the similarity of the learned topic at the node to
the topics of the original graph structure. The Gibbs sampler is initialized to a root node when there
is no labeled data. With labeled data, the Gibbs sampler is initialized with the correct placement of
nodes to levels. The sampler does not observe the edge structure of the graph nor the correct number
of nodes at each level (i.e. the sampler may add additional nodes). With no labeled data, the sampler
is unable to recover the relationship between concepts 8 and 10 (due to the relatively small number
of documents that contain words from both concepts). With 250 labeled documents, the sampler is
able to learn the correct placement of both nodes 8 and 10 (although the topics contain some noise).
5.2

Wikipedia Articles

In this section, we compare the performance of GraphLDA to hPAM and hLDA on a set of 518
machine-learning articles taken from Wikipedia. The input to each model is only the article text. All
models are restricted to learning a three-level hierarchical structure. For both GraphLDA and hPAM,
the number of nodes at each level was set to 25. For GraphLDA, the parameters were fixed at ? = 1,
a = 1 and b = 1. The parameters ? and ? were initialized to 1 and .001 respectively and optimized
using a Metropolis Hastings sampler. We used the MALLET toolkit implementation of hPAM4 and
hLDA [11]. For hPAM, we used different settings for the topic hyperparameter ? = (.001, .01, .1).
For hLDA we set ? = .1 and considered ? = (.1, 1, 10) where ? is the smoothing parameter for the
Chinese restaurant process and ? = (.1, 1, 10) where ? is the smoothing over levels in the graph.
All models were run for 9, 000 iterations to ensure burn-in and samples were taken every 100 iterations thereafter, for a total of 10, 000 iterations. The performance of each model was evaluated
on a hold-out set consisting of 20% of the articles using both empirical likelihood and the left-toright evaluation algorithm (see Sections 4.1 and 4.5 of [12]) which are measures of generalization
to unseen data. For both GraphLDA and hLDA we use the distribution over paths that was learned
during training to compute the per-word log likelihood. For hPAM we compute the MLE estimate of
the Dirichlet hyperparameters for both the distribution over super-topics and the distributions over
sub-topics from the training documents. Table 5.2 shows the per-word log-likelihood for each model
averaged over the ten samples. GraphLDA is competitive when computing the empirical log likelihood. We speculate that GraphLDA?s lower performance in terms of left-to-right log-likelihood is
due to our choice of the geometric distribution over levels (and our choice to position the geometric distribution at the last node of the path) and that a more flexible approach could result in better
performance.

Table 1: Per-word log likelihood of test documents
Model
Parameters
Empirical LL Left-to-Right LL
GraphLDA MH opt.
-7.10 ? .003
-7.13 ? .009
? = .1
-7.36 ? .013
-6.11 ? .007
hPAM
? = .01
-7.33 ? .012
-6.47 ? .012
? = .001
-7.38 ? .006
-6.71 ? .013
? = .1, ? = .1
-7.10 ? .004
-6.82 ? .007
? = .1, ? = 1
-7.09 ? .003
-6.86 ? .006
hLDA
? = .1, ? = 10
-7.08 ? .003
-6.90 ? .008
? = 1, ? = .1
-7.08 ? .003
-6.83 ? .007
? = 1, ? = 1
-7.08 ? .002
-6.86 ? .006
? = 1, ? = 10
-7.06 ? .003
-6.88 ? .008
? = 10, ? = .1
-7.07 ? .004
-6.81 ? .006
? = 10, ? = 1
-7.07 ? .003
-6.83? .005
? = 10, ? = 10
-7.06 ? .003
-6.88 ? .010

7

set
data
learning
concept
model

network
neural
neuron
cnn
function

genetic
fitness
mutation
selection
solution

Markov
time
probability
chain
distribution

graph
Markov
network
random
field

evolution
evolutionary
algorithm
individual
search

word
topic
language
model
document

variables
node
network
parent
Bayesian

model
multitask
inference
Bayesian
Dirichlet

learning
data
model
method
kernel

model
noise
algorithm
hidden
training

learning
policy
decision
graph
function

decision
classification
class
classifier
data

clustering
data
principal
component
kmeans

learning
dimensionality
classification
reduction
method

model
selection
rbm
algorithm
feature

kernel
linear
space
vector
point

learning
algorithm
kernel
convex
constraint

algorithm
svm
vector
problem
multiclass

classifier
boosting
ensemble
hypothesis
margin

Figure 5: Wikipedia graph structure with additional machine learning abstracts. The edge widths
correspond to the probability of the edge in the graph

5.3

Wikipedia Articles with a Graph Structure

In our final experiment we illustrate how GraphLDA can be used to update an existing category
graph. We use the aforementioned 518 machine-learning Wikipedia articles, along with their category labels, to learn topic distributions for each node in Figure 1. The sampler is initialized with
the correct placement of nodes and each document is initialized to a random path from the root to
its category label. After 2, 000 iterations, we fix the path assignments for the Wikipedia articles
and introduce a new set of documents. We use a collection of 400 machine learning abstracts from
the International Conference on Machine Learning (ICML). We sample paths for the new collection of documents keeping the paths from the Wikipedia articles fixed. The sampler was allowed
to add new nodes to each level to explain any new concepts that occurred in the ICML text set.
Figure 5 illustrates a portion of the final graph structure. The nodes in bold are the original nodes
from the Wikipedia category graph. The results show that the model is capable of augmenting an
existing concept graph with new concepts (e.g. clustering, support vector machines (SVMs), etc.)
and learning meaningful relationships (e.g. boosting/ensembles are on the same path as the concepts
for SVMs and neural networks).

6

Discussion and Conclusion

Motivated by the increasing availability of large-scale structured collections of documents such as
Wikipedia, we have presented a flexible non-parametric Bayesian framework for learning concept
graphs from text. The proposed approach can combine unlabeled data with prior knowledge in the
form of labeled documents and existing graph structures. Extensions such as allowing the model
to handle multiple paths per document are likely to be worth pursuing. In this paper we did not
discuss scalability to large graphs which is likely to be an important issue in practice. Computing
the probability of every path during sampling, where the number of graphs is a product over the
number of nodes at each level, is a computational bottleneck in the current inference algorithm and
will not scale. Approximate inference methods that can address this issue should be quite useful in
this context.

7

Acknowledgements

This material is based upon work supported in part by the National Science Foundation under Award
Number IIS-0083489, by a Microsoft Scholarship (AC), and by a Google Faculty Research award
(PS). The authors would also like to thank Ian Porteous and Alex Ihler for useful discussions.
4

MALLET implements the ?exit node? version of hPAM

8

References
[1] David Blei, Andrew Ng, and Michael Jordan. Latent dirichlet allocation. Journal of Machine
Learning Research, 3:993?1022, 2003.
[2] David M. Blei, Thomas L. Griffiths, and Michael I. Jordan. The nested chinese restaurant
process and bayesian nonparametric inference of topic hierarchies. Journal of the Acm, 57,
2010.
[3] David Mimno, Wei Li, and Andrew McCallum. mixtures of hierarchical topics with pachinko
allocation. In Proceedings of the 21st Intl. Conf. on Machine Learning, 2007.
[4] Wei Li, David Blei, and Andrew McCallum. Nonparametric bayes pachinko allocation. In
Proceedings of the Twenty-Third Annual Conference on Uncertainty in Artificial Intelligence
(UAI-07), pages 243?250, 2007.
[5] Blaz Fortuna, Marko Grobelnki, and Dunja Mladenic. Ontogen: Semi-automatic ontology
editor. In Proceedings of theHuman Computer Interaction International Conference, volume
4558, pages 309?318, 2007.
[6] S. Bloehdorn, P. Cimiano, and A. Hotho. Learning ontologies to improve text clustering and
classification. In From Data and Inf. Analysis to Know. Eng.: Proc. of the 29th Annual Conf.
the German Classification Society (GfKl ?05), volume 30 of Studies in Classification, Data
Analysis and Know. Org., pages 334?341. Springer, Feb. 2005.
[7] P. Cimiano, A. Hotho, and S. Staab. Learning concept hierarchies from text using formal
concept analysis. J. Artificial Intelligence Research (JAIR), 24:305?339, 2005.
[8] Hemant Ishwaran and Lancelot F. James. Gibbs sampling methods for stick-breaking priors.
Journal of the American Statistical Association, 96(453):161?173, March 2001.
[9] Tom Griffiths and Mark Steyvers. Finding scientific topics. Proceedings of the Natl. Academy
of the Sciences of the U.S.A., 101 Suppl 1:5228?5235, 2004.
[10] Ian Porteous, Alex Ihler, Padhraic Smyth, and Max Welling. Gibbs sampling for coupled
infinite mixture models in the stick-breaking representation. In Proceedings of UAI 2006,
pages 385?392, July 2006.
[11] Andrew Kachites McCallum.
http://mallet.cs.umass.edu, 2002.

Mallet:

A machine learning for language toolkit.

[12] Hanna M. Wallach, Iain Murray, Ruslan Salakhutdinov, and David Mimno. Evaluation methods for topic models. In Proceedings of the 26th Intl. Conf. on Machine Learning (ICML
2009), 2009.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 2236-field-programmable-learning-arrays.pdf

Field-Programmable Learning Arrays

Seth Bridges, Miguel Figueroa, David Hsu, and Chris Diorio
Department of Computer Science and Engineering
University of Washington
114 Sieg Hall, Box 352350
Seattle, WA 98195-2350
seth,miguel,hsud,diorio @cs.washington.edu


Abstract
This paper introduces the Field-Programmable Learning Array, a new
paradigm for rapid prototyping of learning primitives and machinelearning algorithms in silicon. The FPLA is a mixed-signal counterpart
to the all-digital Field-Programmable Gate Array in that it enables rapid
prototyping of algorithms in hardware. Unlike the FPGA, the FPLA is
targeted directly for machine learning by providing local, parallel, online analog learning using floating-gate MOS synapse transistors. We
present a prototype FPLA chip comprising an array of reconfigurable
computational blocks and local interconnect. We demonstrate the viability of this architecture by mapping several learning circuits onto the
prototype chip.

1 Introduction
Implementing machine-learning algorithms in VLSI is a logical step toward enabling realtime or mobile applications of these algorithms [1]. Several machine-learning architectures
such as neural networks and Bayes nets map naturally to VLSI, because each uses many
simple elements in parallel and computes using only local information. Such algorithms,
when implemented in VLSI, can leverage the inherent parallelism offered by the millions of
transistors on a single silicon die. Depending on the design technique, hardware implementations of learning algorithms can realize significant performance increases over standard
computers in terms of speed or power consumption.
Despite the benefits of implementing machine-learning algorithms in VLSI, several issues
have kept hardware implementations from penetrating mainstream machine learning. First,
many previous hardware systems were not scalable due to the size of many primary components such as digital multipliers or digital-to-analog converters[2, 3]. Second, many
systems such as [4] have inflexible circuit topologies, allowing them to be used for only
very specific problems. Third, many hardware learning systems did not comprise a complete solution with on-chip learning [5] and often required external weight updates[3, 6]. In
addition to these problems of scalability and inflexibility, perhaps the biggest impediment
to implementing learning in VLSI is that designing VLSI chips is a time-consuming and
error-prone process. All current VLSI learning implementations required a detailed knowledge of analog and digital circuit design. This prerequisite knowledge impedes hardware
development by a hardware novice; indeed, the design process can challenge even the most

experienced circuit designer. Because we make extensive use of floating-gate synapse transistors [1] in our learning circuits to enable local adaptation, the design process becomes
even more difficult due to slow and inaccurate simulation of these devices.
A reconfigurable learning system would solve these problems by allowing rapid prototyping and flexibility in learning system hardware. Also, reconfigurability allows the system to
adapt to changes in the problem definition. For example, a designer can trade input dimensionality for resolution by reallocating FPLA resources, even after the implementation is
complete. A custom VLSI solution would not allow such tradeoffs after fabrication. When
combined with a simple user interface, a reconfigurable learning system can enable anyone
with a machine-learning background to express his/her ideas in hardware.
In this paper, we propose a mixed analog-digital Field-Programmable Learning Array
(FPLA), a reconfigurable system for rapid prototyping of machine-learning algorithms in
hardware. The FPLA enables the design cycle shown in Figure 1(a) in which the designer
expresses a machine-learning problem as an algorithm, compiles that representation into
an FPLA configuration, and prototypes the algorithm in an FPLA. The FPLA is similar in
concept to all-digital Field-Programmable Gate Arrays (FPGA), in that they both enable
reconfigurable computation and prototyping using arrays of simple elements and reconfigurable wiring. Unlike previous reconfigurable hardware learning solutions [3, 4, 6, 7], the
FPLA is a general-purpose prototyping tool and does not target one specific architecture.
Moreover, our FPLA supports on-chip adaptation and enables rapid prototyping of a large
class of learning algorithms.
We have implemented a prototype core for an FPLA. Our chip comprises a small (2 2)
array of Programmable Learning Blocks (PLBs) as well as a simple interconnect structure
to allow the PLBs to communicate in an all-to-all fashion. Our results show that this prototype system achieves its design goal of enabling rapid prototyping of floating-gate learning
circuits by implementing learning circuits known in the literature as well as new circuits
prototyped for the first time.
The remainder of the paper proceeds as follows. In section 2, we discuss the proposed
FPLA architecture, as well as the subset that is our prototype. Section 3 shows results from
our test chip of the prototype design. Section 4 concludes with a discussion of improvements that we are making to the design and opportunities for future work.

2 FPLA Architecture
2.1 An FPLA Architecture
Our proposed FPLA architecture, shown in Figure 1(b), has three properties that enable
machine learning: 1) a core comprising an array of Programmable Learning Blocks to
compute machine-learning functions, 2) reconfigurable interconnect to enable inter-PLB
communication, 3) the ability to compute with sufficient accuracy, and 4) a simple and
well-defined user interface.
The first two properties are dimensions of the FPLA design space, where tradeoffs between
them results in varying levels of flexibility and functionality at the cost of area and power.
The FPLA core determines the system?s functionality. For example, in a task-oriented
FPLA, the PLBs that compose the core should allow high-level functions such as multiplication and outer-product learning. Likewise, to develop new learning algorithms in silicon,
the PLBs should allow lower-level functions such as current mirrors, differential pairs, and
current sources.
In addition to a multi-functional core, a reconfigurable learning array requires flexible interconnect that provides good local connectivity between neighboring PLBs and global

DAC

Problem

DAC

Hardware compilation

Configured FPLA

Digital In

Algorithmic
Description

Input Filtering

Define and translate algorithm

space

PLB

PLB

PLB

PLB

PLB

PLB

PLB

PLB

PLB

DAC

Training data and learning
Local Interconnect

Output Filtering

Global interconnect

Trained FPLA
Analog
Out
(a)

ADC

ADC

ADC

(b)

Figure 1: (a) FPLA-Based Design Flow. A user programs a machine-learning algorithm and tests it
using standard software tools (e.g. Matlab). The design compiler transforms this code into an FPLA
configuration, which is then downloaded to the chip. At this point, the FPLA runs the algorithm on
a training data set and performs on-chip learning. (b) Proposed FPLA Architecture. The architecture
comprises an array of Programmable Learning Blocks (PLBs), a flexible interconnect, and support
circuitry on the periphery. Local interconnect enables efficient, low-cost communication between
adjacent PLBs. Global interconnect enables distant PLBs to communicate, albeit at a higher cost.
interconnect for long-range connections. The global interconnect must be sparse because
of area constraints in VLSI chips, but flexible enough to allow a wide range of PLB connectivity. Local connectivity is critical to enable the creation of complex learning primitives
from combinations of PLBs and the implementation of large classes of machine-learning
algorithms that exhibit strong local computation.
Analog and mixed signal VLSI systems are typically plagued by offsets and device mismatch. Even though accurate systems are possible[8], the accuracy usually comes at the
cost of increased power consumption and die area. The adaptive properties of floating-gate
transistors can overcome these intrinsic accuracy limitations[9], therefore enabling mixed
analog-digital computation to obtain the best combination of power, area, scalability, and
performance.
A user interface for an FPLA comprises two different components: a design compilation
and configuration tool, and a chip interface that provides both digital and analog I/O. An
FPLA design compiler allows a user to compile an abstract expression of an algorithm (e.g.
Matlab code) to an FPLA configuration. The chip interface provides digital I/O to interface
with standard computers and surrounding digital circuitry, as well as analog I/O to interface
with signals from sensors such as vision chips and implantable devices.
2.2 Prototype Chip
As a first step in designing an FPLA, we built a prototype focusing on the PLB design and
local interconnect. Our design comprises a 2 2 array of PLBs interconnected in an allto-all fashion. The system I/O comprises digital input for programming and bidirectional
analog input/output for system operation. We show the prototype FPLA architecture and
chip micrograph in Figure 2. We fabricated the chip in the TSMC 0.35 m double-poly, four
metal process available from MOSIS. The FPLA included two pFET PLBs and two nFET
PLBs, each containing 8 uncommitted lines, 4 I/O blocks, and the computational primitives
described below. The FPLA occupies 2000 m 700 m including the programming 4-

Configuration Shift Register

I/O

I/O

pFET PLB

pFET PLB

nFET PLB

nFET PLB

I/O

I/O

nFET PLB

(a)

pFET PLB

nFET PLB

Inter?PLB

pFET PLB

Inter?PLB

Inter?PLB Block

Inter?PLB

Decoder

Configuration Decoder

Programming Logic

(b)

Figure 2: (a) Fabricated Chip Architecture. Our prototype FPLA comprises 4 PLBs that contain
simple analog functional primitives. A set of interconnect switches connect the PLBs in an all-toall fashion. (b) Chip Micrograph. The chip photograph shows the four PLBs, inter-PLB blocks,
and programming circuitry. The chip was fabricated in the TSMC 0.35 m double-poly four-metal
process from MOSIS.

to-16 decoder and 108-bit shift register. Through design optimization, we have recently
reduced the size by more than 50%.
Each of the four PLBs comprises computational circuitry and a large switching matrix built
of pass-gates controlled by SRAM. There are two different types of PLBs, the pFET PLB
and the nFET PLB, because nFET and pFET are the two flavors of transistors available
in standard CMOS processes. The computational primitives that compose the PLBs are
two floating-gate transistors, a differential pair, a current mirror, a diode-connected transistor, a bias current source, three transistors with configurable length and width, and two
configurable capacitors. These circuit primitives can be wired into arbitrary configurations
simply by changing the state of the PLB switch matrix. When deciding what functions to
place in the PLBs, our starting point was the decomposition of known primitives [10, 11]
for silicon learning as well as standard analog primitives such as those in Mead?s book
on silicon neural systems [12]. The circuits included in our PLBs are the most common
subcircuits found when decomposing these primitives.
Each of the four PLBs is independent of the others and can be programmed and operated
independently. However, more useful circuits require resources from multiple PLBs. InterPLB blocks provide local connectivity between PLBs where each inter-PLB block is an
array of SRAM pass-gate switches that can connect an uncommitted line in one PLB to
an uncommitted line in another PLB. The six inter-PLB blocks provide a path from one
PLB to any other PLB in the system. To interface with the external world, there are four
I/O connections per PLB, each of which can be configured in one of two ways: as a bare
connection to the pad for voltage inputs or current outputs, or as a voltage output through a
unity-gain buffer. The user configures the FPLA by shifting the configuration bits into the
configuration SRAM, located throughout the PLBs and interconnect.

3 Implementing Machine-Learning Primitives
To show the correct functionality of our chip, we implemented various circuits from the
literature as well as new circuits developed entirely in the FPLA. In the following section,
we show results for three of these circuits.

pFET PLB

2X

2X

80

2X

2X

60

3X

3X

Vb
Vtun

Vtun

W

X

X

X

Weq (nA)

Vb

Custom
FPLA

X
W
2X

Y

40

2X

20

Y
nFET PLB

(a)

2X

0.25

0.5
Pr(X|Y)

(b)

0.75

(c)

Figure 3: (a) Schematic of the correlational-learning circuit described by Shon and Hsu in [11]. (b)
Schematic of the same circuit as implemented in the FPLA. (c) Experimental results comparing the
performance of the custom circuit against the reconfigurable circuit. We scaled the data to compensate
for differences in operating point between the two implementations. The data reported by Shon and
Hsu is smoother because it is averaged over a larger number of experiments.

3.1 Correlational-Learning Primitive
As a first test of our chip, we implemented the correlational-learning circuit described by
Shon and Hsu in [11]. This circuit learns the conditional probability of a binary event
given another binary event . We show the original circuit in Figure 3(a), and the FPLA
implementation Figure 3(b).





We implemented this circuit using primitives from two PLBs. We input the signals and
as voltage pulses. Figure 3(c) compares the results from the custom chip to the results
from the FPLA. Both sets of data can be fit by:

	  	' ( 	 

)


	
 		    ! #"%$&

(1)

where ,
,
, and are fit constants. We conclude from this experiment that
the correlational-learning circuit, when implemented in the FPLA, operates as the original
circuit. SPICE simulations confirm that the interconnect switches have a negligible effect
on circuit performance.
3.2 Regression-Learning Primitive
The regression-learning circuit described in this section is a new hardware learning primitive first implemented in the FPLA. The circuit performs regression learning on a set of 2-D
input data. It comprises two correlational learning circuits like the one shown in Figure 4(a)
to encode a differential weight . Each circuit learns
and
respectively, such that:
(2)
The circuit operates as follows. We apply a zero-mean input signal , encoded as a varying
current plus some DC bias current , to the two inputs of the circuit. The differential
output current
of each circuit represents the product of its stored weight with the input
current.
(3)
(4)
The difference in those output currents represents the total product of the current input and
the weight stored on the floating gate.
(5)

3

*

57698

*,+

* /* +101*.4
57698:+   3#;<4"%*/+
57698=-  3#;<4"%*.-

*.-

2

5768  57698:+1057698=-  3  */+01*.->"?;@4  *A+10*.->"

0.5
Output(nA)

Vb
w

Current
Input
i=x+b

Update
Control

Current
Output
out=w(x+b)

0

?0.5

?1
(a)

?0.5

0
0.5
Input(nA)

1

(b)

Figure 4: (a) Regression Learning Circuit. This circuit is one-half of the regression learning circuit

and learns the positive weight  . The other half of the circuit is identical but used to represent the
negative differential weight  . The difference between the learned weights  and  converges
to the slope of the incoming data. (b) Experimental Data. This data is taken from the FPLA configured
as the circuit on the left. The circuit was shown 388 data points with a slope of 0.5 and zero-mean
Gaussian noise of 5%. The circuit learned a slope of 0.4924.

*/3

where the multiplication is performed by the current mirror formed by the input diode and
, so we remove the scaled input offset
the floating gate. The output prediction we seek is
with a high-pass filter implemented in the test computer.
current

*,4

(6)
5768  
	


  3  *A+0*.- "
Circuit training occurs in a supervised manner. An input 3 is provided to the circuit, and
the circuit predicts an output */3 . The computer running the test compares that predicted
output with the target and feeds an error signal back to the chip. Based on the error signal,
the circuit adapts the weight * . Positive changes in * + increase * , while positive changes
in * - decrease * . We implement a small weight decay on the both synapses. Results from
this circuit are shown in Figure 4(b).
3.3 Clustering Primitive
We tested a new clustering primitive that is based on the adaptive bump circuit introduced
in [10]. The circuit performs two functions: 1) computes the similarity between an input
and a stored value, and 2) adapts the stored value to decrease its distance to the input. This
adaptive bump circuit exhibits improved adaptation over previous versions [10, 13] due
to the inclusion of the autonulling differential pair[14], shown in Figure 5(a) (top). The
autonulling differential pair ensures that the adaptation process increases the similarity between the stored mean and the input. The data in Figure 5(b) shows the clustering primitive
adapting to an input that is initially distant from the stored value. The result of this adaptation is that over time, the circuit learns to produce a maximal output response at the present
input.
This circuit was easily prototyped in the FPLA. Creation of a configuration file took less
than one hour, experimental setup took another hour, and data was produced within two
additional hours. Instead of waiting several months for chip fabrication, we were able to
produce experimental results from a chip in under four hours. Also, the results are a more
accurate model of actual circuit behavior than a SPICE simulation.

   

	


	


	
  


 


	


	
  

 

800
700
600
500
400
300
200
100
0

Iout(nA)


	

adaptation
?2

?1
0
1
V1?V2(V)

2

 

(a)

(b)

Figure 5: (a) Clustering Primitive. This circuit can: 1) compute the similarity between the stored
value and the input, and 2) adapt the stored value to decrease its distance to the input. (b) Experimental Data. This plot shows that circuit adaptation moves the circuit?s peak response toward the
presented input. Adaptation strength decreases as the stored value approaches the input.

4 Future Work
The chip that we developed is effective for prototyping single learning primitives, but is
too small for solving real machine-learning problems. An FPLA whose target is machinelearning algorithms requires PLBs that comprise higher-level functions, such as the primitives presented in the previous section.
To scale up our design for machine-learning applications, we will make the following improvements to our prototype. First, to reduce the size of the PLBs, we will increase the ratio
of computational circuitry to switching circuitry by replacing the low-level functions such
as current mirrors and synapse transistors with higher-level primitives such as those mentioned in the previous section. Second, we will increase the number of PLBs in the design,
which will require an efficient and scalable global interconnect structure. We will base
our revisions on commercial FPGA architectures and other well-known on-chip communication schemes. Third, we will improve the I/O structures to enable multichip systems.
Finally, we have begun work on the design compiler, a software tool that maps machinelearning algorithms to an FPLA configuration.

5 Conclusions
Because of the match between the parallelism offered by hardware and the parallelism
in machine-learning algorithms, mixed analog-digital VLSI is a promising substrate for
machine-learning implementations. However, custom VLSI solutions are costly, inflexible, and difficult to design. To overcome these limitations, we have proposed
Field-Programmable Learning Arrays, a viable reconfigurable architecture for prototyping machine-learning algorithms in hardware. FPLAs combine elements of FPGAs, analog
VLSI, and on-chip learning to provide a scalable and cost-effective solution for learning

in silicon. Our results show that our prototype core and interconnect can effectively implement existing learning primitives and assist in the development of new circuits. An
enhanced version of the FPLA, currently under development, will support complex learning algorithms.
Acknowledgments
This work was supported by ONR grant #N00014-01-1-0566 and an Intel Fellowship.
Chips were fabricated by the MOSIS service.

References
[1] C. Diorio, D. Hsu, and M. Figueroa, ?Adaptive CMOS: From biological inspiration to systemson-a-chip,? Proceedings of the IEEE, vol. 90, no. 3, pp. 245?357, 2002.
[2] J. B. Burr, ?Digital Neural Network Implementations,? in Neural Networks: Concepts, Applications, and Implementations, Volume 2 (P. Antognetti and V. Milutinovic, eds.), pp. 237?285,
Prentice Hall, 1991.
[3] S. Satyanarayana, Y. Tsividis, and H. Graf, ?A reconfigurable VLSI neural network,? IEEE
Journal of Solid-State Circuits, vol. 27, January 1992.
[4] R. Coggins, M. Jabri, B. Flower, and S. Pickard, ?ICEG morphology classification using an
analogue VLSI neural network,? in Advances in Neural Information Processing Systems 7,
pp. 731?738, MIT Press, 1995.
[5] M. Holler, S. Tam, H. Castro, and R. Benson, ?An electrically trainable artificial neural network
with 10240 ?floating gate? synapses,? in Proceedings of the International Joint Conference on
Neural Networks(IJCNN89), vol. 2, (Washington D.C), pp. 191?196, 1989.
[6] E. K. F. Lee and P. G. Gulak, ?A CMOS field programmable analog array,? IEEE Journal of
Solid-State Circuits, vol. 26, December 1991.
[7] A. Montalvo, R. Gyurcsik, and J. Paulos, ?An analog VLSI neural network with on-chip learning,? IEEE Journal of Solid-State Circuits, vol. 32, no. 4, 1997.
[8] R. Genov and G. Cauwenberghs, ?Stochastic mixed-signal VLSI architecture for highdimensional kernel machines,? in Advances in Neural Information Processing Systems 14 (T. G.
Dietterich, S. Becker, and Z. Ghahramani, eds.), (Cambridge, MA), MIT Press, 2002.
[9] J. Hyde, T. Humes, C. Diorio, M. Thomas, and M. Figueroa, ?A floating-gate trimmed, 14bit, 250 ms/s digital-to-analog converter in standard 0.25 m CMOS,? in Symposium on VLSI
Circuits Digest of Technical Papers, pp. 328?331, 2002.
[10] D. Hsu, M. Figueroa, and C. Diorio, ?A silicon primitive for competitive learning,? in Advances
in Neural Information Processing Systems 13 (T. K. Leen, T. G. Dietterich, and V. Tresp, eds.),
pp. 713?719, MIT Press, 2001.
[11] A. P. Shon, D. Hsu, and C. Diorio, ?Learning spike-based correlations and conditional probabilities in silicon,? in Advances in Neural Information Processing Systems 14 (T. G. Dietterich,
S. Becker, and Z. Ghahramani, eds.), (Cambridge, MA), MIT Press, 2002.
[12] C. Mead, Analog VLSI and Neural Systems. Reading, MA: Addison-Wesley, 1989.
[13] P. Hasler, ?Continuous-time feedback in floating-gate MOS circuits,? IEEE Transactions on
Circuits and Systems II, vol. 48, pp. 56?64, January 2001.
[14] D. Hsu, S. Bridges, and C. Diorio, ?Adaptive quantization and density estimation in silicon,?
2002. In submission.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 182-genesis-a-system-for-simulating-neural-networks.pdf

485

GENESIS: A SYSTEM FOR SIMULATING NEURAL
NETWOfl.KS
Matthew A. Wilson, Upinder S. Bhalla, John D. Uhley, James M. Bower.
Division of Biology
California Institute of Technology
Pasadena, CA 91125

ABSTRACT
We have developed a graphically oriented, general purpose
simulation system to facilitate the modeling of neural networks.
The simulator is implemented under UNIX and X-windows and is
designed to support simulations at many levels of detail.
Specifically, it is intended for use in both applied network
modeling and in the simulation of detailed, realistic, biologicallybased models. Examples of current models developed under this
system include mammalian olfactory bulb and cortex, invertebrate
central pattern generators, as well as more abstract connectionist
simulations.

INTRODUCTION
Recently, there has been a dramatic increase in interest in exploring the
computational properties of networks of parallel distributed processing elements
(Rumelhart and McClelland, 1986) often referred to as Itneural networks"
(Anderson, 1988). Much of the current research involves numerical simulations of
these types of networks (Anderson, 1988; Touretzky, 1989). Over the last several
years, there has also been a significant increase in interest in using similar computer
simulation techniques to study the structure and function of biological neural
networks. This effort can be seen as an attempt to reverse-engineer the brain with
the objective of understanding the functional organization of its very complicated
networks (Bower, 1989). Simulations of these systems range from detailed
reconstructions of single neurons, or even components of single neurons, to
simulations of large networks of complex neurons (Koch and Segev, 1989).
Modelers associated with each area of research are likely to benefit from exposure to
a large range of neural network simulations. A simulation package capable of
implementing these varied types of network models would facilitate this interaction.

486

Wilson, Bhalla, Uhley and Bower

DESIGN FEATURES OF THE SIMULATOR
We have built GENESIS (GEneral NEtwork SImulation System) and its graphical
interface XODUS (X-based Output and Display Utility for Simulators) to provide a
standardized and flexible means of constructing neural network simulations while
making minimal assumptions about the actual structure of the neural components.
The system is capable of growing according to the needs of users by incorporating
user-defined code. We will now describe the specific features of this system.
Device independence.
The entire system has been designed to run under UNIX and X-windows (version
11) for maximum portability. The code was developed on Sun workstations and has
been ported to Sun3's, Sun4's, Sun 386i's, and Masscomp computers. It should be
portable to all installations supporting UNIX and X-II. In addition, we will be
developing a parallel implementation of the simulation system (Nelson et al., 1989).
Modular design.
The design of the simulator and interface is based on a "building-block" approach.
Simulations are constructed of modules which receive inputs, perform calculations
on them, and generate outputs (figs. 2,3). This approach is central to the generality
and flexibility of the system as it allows the user to easily add new features
without modification to the base code.
Interactive specification and control.
Network specification and control is done at a high level using graphical tools and a
network specification language (fig. 1). The graphics interface provides the highest
and most user friendly level of interaction. It consists of a number of tools which
the user can configure to suit a particular simulation. Through the graphical
interface the user can display, control and adjust the parameters of simulations. The
network specification language we have developed for network modeling represents a
more basic level of interaction. This language consists of a set of simulator and
interface functions that can be executed interactively from the keyboard or from
text flies storing command sequences (scripts). The language also provides for
arithmetic operations and program control functions such as looping, conditional
statements, and subprograms or macros. Figures 3 and 4 demonstrate how some of
these script functions are used.
Simulator and interrace toolkits.
Extendable toolkits which consist of module libraries, graphical tools and the
simulator base code itself (fig. 2) provide the routines and modules used to
construct specific simulations. The base code provides the common control and
support routines for the entire system.

GENESIS: A System for Simulating Neural Networks

Gra hics Interface

~.. ~

..

Script Files

. DP~~Data

Files

(
Genesis command
window and ke board

Script Language
Interpreter

Genesis 1%

Figure 1. Levels Of Interaction With The Simulator

CONSTRUCTING SIMULATIONS
The first step in using GENESIS involves selecting and linking together those
modules from the toolkits that will be necessary for a particular simulation (fig.
2,3). Additional commands in the scripting language establish the network and the
graphical interface (fig. 4).
Module Classes.
Modules in GENESIS are divided into computational modules, communications
modules and graphical modules. All instances of computational modules are called
elements. These are the central components of simulations, performing all of the
numerical calculations. Elements can communicate in two ways: via links and via
connections. Links allow the passing of data between two elements with no time
delay and with no computation being performed on the data. Thus. links serve to
unify a large number of elements into a single computational unit (e.g. they are
used to link elements together to form the neuron in fig. 3C). Connections. on the
other hand. interconnect computational units via simulated communication channels
which can incorporate time delays and perform transformations on data being
transmitted (e.g. axons in fig. 3C). Graphical modules called widgets are used to
construct the interface. These modules can issue script commands as well as respond
to them, thus allowing interactive access to simulator structures and functions.

487

488

Wilson, Bhalla, Uhley and Bower

Hierarchical organization.
In order to keep track of the structure of a simulation, elements are organized into a
tree hierarchy similar to the directory structure in UNIX (fig. 3B). The tree
structure does not explicitly represent the pattern of links and connections between
elements, it is simply a tool for organizing complex groups of elements in the
simulation.
Simulation example.
As an example of the types of modules available and the process of structuring them
into a network simulation and graphical interface, we will describe the construction
of a simple biological neural simulation (fig. 3). The I11pdel consists of two
neurons. Each neuron contains a passive dendritic compartment, an active cell body,
an axonal output, and a synaptic input onto the dendrite. The axon of one neuron
connects to a synaptic input of the other. Figure 3 shows the basic structure of the
model as implemented under GENESIS. In the model, the synapse, channels,

Simulator and interrace toolkit
-----------------------------------------------------------------~

Graphics Modules

Communications
modules

Computational
Modules
(

A

CLinker

oDCO

Earn

?

Simulation

Simulator

=> __
ffi ~
..
....-----0001 ca
.... ;..........

\.< .

.::<::;:::;";::::,:::-:.<., ..... .

:? j~ : CQdK

Figure 2. Stages In Constructing A Simulation.

.. ...

GENESIS: A System for Simulating Neural Net~orks

network

B

A

~

neuron!

neuron2

~~
cell-body

Na

A

K

dendrite

axon

\

synapse

C

KEY
Element
Connection

dendrite

-Link

D

Figure 3. Implementation of a two neuron model in GENESIS. (A) Schematic diagram of compartmentally modeled neurons. Each cell in this simple model has a passive dendritic compartment, an active cell-body, and an output axon. There is a
synaptic input to the dendrite of one cell and two ionic channels on the cell body.
(B) Hierarchical representation of the components of the simulation as maintained in
GENESIS. The cell-body of neuron 1 is referred to as /network/neuronl/cell-body.
(C) A representation of the functional links between the basic components of one
neuron. (D) Sample interface control and display widgets created using the XODUS
toolkit.

489

490

Wilson, Bhalla, Uhley and Bower

dendritic compartments, cell body and axon are each treated as separate
computational elements (fig. 3C). Links allow elements to share information (e.g.
the Na channel needs to have access to the cell-body membrane voltage). Figure 4
shows a portion of the script used to construct this simulation.

Create different types or elements and
create
create
active compartment
create
passive_compartment
create
synapse

assign them names.
neuronl
cell-body
dendrite
dendrite/synapse

Establish functional "links" between the elements.
link
dendrite
to
cell-body
link
dendrite/synapse
to
dendrite
Set parameters associated with the elements.
set
dendrit~
capacitance
l.Oe-6
Make copies or entire element subtrees.
copy
neuronl
to
neuron2
Establish "connections" between two elements.
connect neuronl/axon
to
neuron2/dendrite/synapse
Set up a graph to monitor an element variable
graph
neuronl/cell-body
potential
Make a control panel with several control "widgets".
xform
control
xdialo g nstep set-nstep -default 200
xdialog dt
set-dt
-default 0.5
Xloggle Euler set-euler
Figure 4. Sample script commands for constructing a simulation (see fig. 3)

SIMULATOR SPECIFICATIONS
Memory requirements or GENESIS.
Currently. GENESIS consists of about 20,000 lines of simulator code and a similar
amount of graphics code, all written in C. The executable binaries take up about 1.5
Megabytes. A rough estimate of the amount of additional memory necessary for a
particular simulation can be calculated from the sizes and number of modules used
in a simulation. Typically, elements use around 100 bytes, connections 16 and
messages 20. Widgets use 5-20 Kbytes each.

GENESIS: A System for Simulating Neural Networks

Performance
The overall efficiency of the GENESIS system is highly simulation specific. To
consider briefly a specific case, the most sophisticated biologically based simulation
currently implemented under GENESIS, is a model of piriform (olfactory) cortex
(Wilson et al., 1986; Wilson and Bower, 1988; Wilson and Bower, 1989). This
simulation consists of neurons of four different types. Each neuron contains from
one to five compartments. Each compartment can contain several channels. On a
SUN 386i with 8 Mbytes of RAM. this simulation with 500 cells runs at I second
per time step.
Other models that have been implemented under GENESIS
The list of projects currently completed under GENESIS includes approximately ten
different simulations. These include models of the olfactory bulb (Bhalla et al.,
1988), the inferior olive (Lee and Bower, 1988). and a motor circuit in the
invertebrate sea slug Tritonia (Ryckebusch et aI., 1989)~ We have also built several
tutorials to allow students to explore compartmental biological models (Hodgkin
and Huxley, 1952), and Hopfield networks (Hopfield. 1982).
Access/use of GENESIS
GENESIS and XODUS will be made available at the cost of distribution to all
interested users. As described above, new user-defined modules can be linked into
the simulator to extend the system. Users are encouraged to support the continuing
development of this system by sending modules they develop to Caltech. These
will be reviewed and compiled into the overall system by GENESIS support staff.
We would also hope that users would send completed published simulations to the
GENESIS data base. This will provide others with an opportunity to observe the
behavior of a simulation first hand. A current listing of modules and full
simulations will be maintained and available through an electronic mail newsgroup.
Babel. Enquiries about the system should be sent to GENESIS@caltech.edu or
GENESIS@caltech.biblet.
Acknowledgments
We would like to thank Mark Nelson for his invaluable assistance in the
development of this system and specifically for his suggestions on the content of
this manuscript. We would also like to recognize Dave Bilitch. Wojtek Furmanski.
Christof Koch, innumerable Caltech students and the students of the 1988 MBL
summer course on Methods in Computational Neuroscience for their contributions
to the creation and evolution of GENESIS (not mutually exclusive). This research
was also supported by the NSF (EET-8700064). the NIH (BNS 22205). the ONR
(Contract NOOOI4-88-K-0513). the Lockheed Corporation. the Caltech Presidents
Fund, the JPL Directors Development Fund. and the Joseph Drown Foundation.

491

492

Wilson, Bhalla, Uhley and Bower

References
D. Anderson. (ed.) Neural information processing systems. American Institute of
Physics, New York (1988).
U.S. Bhalla, M.A. Wilson, & J.M. Bower. Integration of computer simulations
and multi-unit recording in the rat olfactory system. Soc. Neurosci. Abstr. 14:
1188 (1988).
I.M. Bower. Reverse engineering the nervous system: An anatomical, physiological,
and computer based approach.
In: An Introduction to Neural and Electronic
Networks. Zornetzer, Davis, and Lau, editors. Academic Press (1989)(in press).
A.L. Hodgkin and A.F. Huxley. A quantitative description of membrane current and
its application to conduction and excitation in nerve. I.Physiol, (Lond.) 117, 500544 (1952).
1.J. Hopfield. Neural networks and physical systems with emergent collective
computational abilities. Proc. Natl. Acad. Sci. USA. 79,2554-2558 (1982).
C. Koch and I. Segev. (eds.) Methods in Neuronal Modeling: From Synapses to
Networks. MIT Press, Cambridge, MA (in press).
M. Lee and I.M. Bower. A structural simulation of the inferior olivary nucleus.
Soc. Neurosci. Abstr. 14: 184 (1988).
M. Nelson, W. Furmanski and I.M. Bower. Simulating neurons and neuronal
networks on parallel computers. In: Methods in Neuronal Modeling: From Synapses
to Networks. C. Koch and I. Segev, editors. MIT Press, Cambridge, MA (1989)(in
press).
S. Ryckebusch, C. Mead and I.M. Bower. Modeling a central pattern generator in
software and hardware: Tritonia in sea moss (CMOS). (l989)(this volume).
D.E. Rumelhart, 1.L. McClelland and the PDP Research Group. Parallel Distributed
Processing. MIT Press, Cambridge, MA (1986).
D. Touretzky. (ed.) Advances in Neural Network Information Processing Systems.
Morgan Kaufmann Publishers, San Mateo, California (1989).
M.A. Wilson and I.M. Bower. The simulation of large-scale neuronal networks. In:
Methods in Neuronal Modeling: From Synapses to Networks. C. Koch and I. Segev,
editors. MIT Press, Cambridge, MA (1989)(in press).
M.A. Wilson and I.M. Bower. A computer simulation of olfactory cortex with
functional implications for storage and retrieval of olfactory information. In:
Neural information processing systems. pp. 114-126 D. Anderson, editor. Published
by AlP Press, New York, N.Y (1988).
M.A. Wilson, I.M. Bower and L.B. Haberly. A computer simulation of piriform
cortex. Soc. Neurosci. Abstr. 12.1358 (1986).

Part IV
Structured Networks


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 5014-a-stability-based-validation-procedure-for-differentially-private-machine-learning.pdf

A Stability-based Validation Procedure for
Differentially Private Machine Learning

Kamalika Chaudhuri
Department of Computer Science and Engineering
UC San Diego, La Jolla CA 92093
kamalika@cs.ucsd.edu

Staal Vinterbo
Division of Biomedical Informatics
UC San Diego, La Jolla CA 92093
sav@ucsd.edu

Abstract
Differential privacy is a cryptographically motivated definition of privacy which
has gained considerable attention in the algorithms, machine-learning and datamining communities. While there has been an explosion of work on differentially
private machine learning algorithms, a major barrier to achieving end-to-end differential privacy in practical machine learning applications is the lack of an effective procedure for differentially private parameter tuning, or, determining the
parameter value, such as a bin size in a histogram, or a regularization parameter,
that is suitable for a particular application.
In this paper, we introduce a generic validation procedure for differentially private
machine learning algorithms that apply when a certain stability condition holds on
the training algorithm and the validation performance metric. The training data
size and the privacy budget used for training in our procedure is independent of
the number of parameter values searched over. We apply our generic procedure to
two fundamental tasks in statistics and machine-learning ? training a regularized
linear classifier and building a histogram density estimator that result in end-toend differentially private solutions for these problems.

1

Introduction

Privacy-preserving machine learning algorithms are increasingly essential for settings where sensitive and personal data are mined. The emerging standard for privacy-preserving computation for
the past few years is differential privacy [7]. Differential privacy is a cryptographically motivated
definition, which guarantees privacy by ensuring that the log-likelihood of any outcome does not
change by more than ? due to the participation of a single individual; an adversary will thus have
difficulty inferring the private value of a single individual when ? is small. This is achieved by
adding random noise to the data or to the result of a function computed on the data. The value ? is
called the privacy budget, and measures the level of privacy risk allowed. As more noise is needed
to achieve lower ?,the price of higher privacy is reduced utility or accuracy. The past few years
have seen an explosion in the literature on differentially private algorithms, and there currently exist
differentially private algorithms for many statistical and machine-learning tasks such as classification [4, 15, 23, 10], regression [18], PCA [2, 5, 17, 12], clustering [2], density estimation [28, 19],
among others.
Many statistics and machine learning algorithms involve one or more parameters, for example, the
regularization parameter ? in Support Vector Machines and the number of clusters in k-means.
Accurately setting these parameters is critical to performance. However there is no good apriori way
to set these parameters, and common practice is to run the algorithm for a few different plausible
parameter values on a dataset, and then select the output that yields the best performance on held-out
validation data. This process is often called parameter-tuning, and is an essential component of any
practical machine-learning system.
1

A major barrier to achieving end-to-end differential privacy in practical machine-learning applications is the absence of an effective procedure for differentially private parameter-tuning. Most
previous experimental works either assume that a good parameter value is known apriori [15, 5] or
use a heuristic to determine a suitable parameter value [19, 28]. Currently, parameter-tuning with
differential privacy is done in two ways. The first is to run the training algorithm on the same data
multiple times. However re-using the data leads to a degradation in the privacy guarantees, and thus
to maintain the privacy budget ?, for each training, we need to use a privacy budget that shrinks
polynomially with the number of parameter values. The second procedure, used by [4], is to divide
the training data into disjoint sets and train for each parameter value using a different set. Both solutions are highly sub-optimal, particularly, if a large number of parameter values are involved ? the
first due to the lower privacy budget, and the second due to less data. Thus the challenge is to design
a differentially private validation procedure that uses the data and the privacy budget effectively, but
can still do parameter-tuning. This is an important problem, and has been mentioned as an open
question by [28] and [4].
In this paper, we show that it is indeed possible to do effective parameter-tuning with differential
privacy in a fairly general setting, provided the training algorithm and the performance measure
used to evaluate its output on the validation data together obey a certain stability condition. We
characterize this stability condition by introducing a notion of (?1 , ?2 , ?)-stability; loosely speaking,
stability holds if the validation performance measure does not change very much when one person?s
private value in the training set changes, when exactly the same random bits are used in the training
algorithm in both cases or, when one person?s private value in the validation set changes. The second
condition is fairly standard, and our key insight is in characterizing the first condition and showing
that it can help in differentially private parameter tuning.
We next design a generic differentially private training and validation procedure that provides endto-end privacy provided this stability condition holds. The training set size and the privacy budget
used by our training algorithms are independent of k, the number of parameter values, and the
accuracy of our validation procedure degrades only logarithmically with k.
We apply our generic procedure to two fundamental tasks in machine-learning and statistics ? training a linear classifier using regularized convex optimization, and building a histogram density estimator. We prove that existing differentially private algorithms for these problems obey our notion
of stability with respect to standard validation performance measures, and we show how to combine
them to provide end-to-end differentially private solutions for these tasks. In particular, our application to linear classification is based on existing differentially private procedures for regularized
convex optimization due to [4], and our application to histogram density estimation is based on the
algorithm variant due to [19].
Finally we provide an experimental evaluation of our procedure for training a logistic regression
classifier on real data. In our experiments, even for a moderate value of k, our procedure outperformed existing differentially private solutions for parameter tuning, and achieved performance
only slightly worse than knowing the best parameter to use ahead of time. We also observed that
our procedure, in contrast to the other procedures we tested, improved the correspondence between
predicted probabilities and observed outcomes, often referred to as model calibration.
Related Work. Differential privacy, proposed by [7], has gained considerable attention in the algorithms, data-mining and machine-learning communities over the past few years as there has been a
large explosion of theoretical and experimental work on differentially private algorithms for statistical and machine-learning tasks [10, 2, 15, 19, 27, 28, 3] ? see [24] for a recent survey of machine
learning methods with a focus on continuous data. In particular, our case study on linear classification is based on existing differentially private procedures for regularized convex optimization,
which were proposed by [4], and extended by [23, 18, 15]. There has also been a large body of
work on differentially private histogram construction in the statistics, algorithms and database literature [7, 19, 27, 28, 20, 29, 14]. We use the algorithm variant due to [19].
While the problem of differentially private parameter tuning has been mentioned in several works,
to the best of our knowledge, an efficient systematic solution has been elusive. Most previous
experimental works either assume that a good parameter value is known apriori [15, 5] or use a
heuristic to determine a suitable parameter value [19, 28]. [4] use a parameter-tuning procedure
where they divide the training data into disjoint sets, and train for a parameter value on each set. [28]

2

mentions finding a good bin size for a histogram using differentially private validation procedure as
an open problem.
Finally, our analysis uses ideas similar to the analysis of the Multiplicative Weights Method for
answering a set of linear queries [13].

2

Preliminaries

Privacy Definition and Composition Properties. We adopt differential privacy as our notion of
privacy.
Definition 1 A (randomized) algorithm A whose output lies in a domain S is said to be (?, ?)differentially private if for all measurable S ? S, for all datasets D and D0 that differ in the value
of a single individual, it is the case that: Pr(A(D) ? S) ? e? Pr(A(D0 ) ? S) + ?. An algorithm is
said to be ?-differentially private if ? = 0.
Here ? and ? are privacy parameters where lower ? and ? imply higher privacy. Differential privacy
has been shown to have many desirable properties, such as robustness to side information [7] and
resistance to composition attacks [11].
An important property of differential privacy is that the privacy guarantees degrade gracefully if
the same sensitive data is used in multiple private computations. In particular, if we apply an ?differentially private procedure k times on the same data, the
p result is k?-differential private as
well as (?0 , ?)-differentially private for ?0 = k?(e? ? 1) + 2k log(1/?)? [7, 8]. These privacy
composition results are the basis of existing differentially private parameter tuning procedures.
Training Procedure and Validation Score. Typical (non-private) machine learning algorithms
have one or more undetermined parameters, and standard practice is to run the machine learning
algorithm for a number of different parameter values on a training set, and evaluate the outputs on a
separate held-out validation dataset. The final output is the one which performs best on the validation
data. For example, in linear classification, we train logistic regression or SVM classifiers with
several different values of the regularization parameter ?, and then select the classifier which has
the best performance on held-out validation data. Our goal in this paper is to design a differentially
private version of this procedure which uses the privacy budget efficiently.
The full validation process thus has two components ? a training procedure, and a validation score
which evaluates how good the training procedure is.
We assume that training and validation data are drawn from a domain X , and the result of the
differentially private training algorithm lies in a domain C. For example, for linear classification, X
is the set of all labelled examples (x, y) where x ? Rd and y ? {?1, 1}, and C is the set of linear
classifiers in d dimensions. We use n to denote the size of a training set, m to denote the size of a
held-out validation set, and ? to denote a set of parameters.
A differentially private training procedure is a randomized algorithm, which takes as input a (sensitive) training dataset, a parameter (of the training procedure), and a privacy parameter ? and outputs
an element of C; the procedure is expected to be ?-differentially private. For ease of exposition and
proof, we represent a differentially private training procedure T as a tuple T = (G, F ), where G is
a density over sequences of real numbers, and F is a function, which takes as input a training set, a
parameter in the parameter set ?, a privacy parameter ?, and a random sequence drawn from G, and
outputs an element of C. F is thus a deterministic function, and the randomization in the training
procedure is isolated in the draw from G.
Observe that any differentially private algorithm can be represented as such a tuple. For example,
given x1 , . . . , xn ? [0, 1], an ?-differentially private approximation to the sample mean x
? is x
?+
1
?n Z where Z is drawn from the standard Laplace distribution. We can represent this procedure
as a tuple T = (G, F ) as follows: G is the standard Laplace density over reals, and for any ?,
r
F ({x1 , . . . , xn }, ?, ?, r) = x
? + ?n
. In general, more complicated procedures will require more
involved functions F .
A validation score is a function q : C ? X m ? R which takes an object h in C and a validation
dataset V , and outputs a score which reflects the quality of h with respect to V . For example, a
3

common validation score used in linear classification is classification accuracy. In (non-private)
validation, if hi is obtained by running the machine learning algorithm with parameter ?i , then the
goal is to output the i (or equivalently the hi ) which maximizes q(hi , V ); our goal is to output
an i that approximately maximizes q(hi , V ) while still preserving the privacy of V as well as the
sensitive training data used in constructing the hi s.

3

Stability and Generic Validation Procedure

We now introduce and discuss our notion of stability, and provide a generic validation procedure
that uses the privacy budget efficiently when this notion of stability holds.
Definition 2 ((?1 , ?2 , ?)-Stability) A validation score q is said to be (?1 , ?2 , ?)-stable with respect
to a training procedure T = (G, F ), a privacy parameter ?, and a parameter set ? if the following
holds. There exists a set ? such that PrR?G (R ? ?) ? 1 ? ?, and whenever R ? ?, the following
two conditions hold:
1. Training Stability: For all ? ? ?, V , and all training sets T and T 0 that differ in a single
entry, |q(F (T, ?, ?, R), V ) ? q(F (T 0 , ?, ?, R), V )| ? ?n1 .
2. Validation Stability: For all T , ? ? ?, and for all V and V 0 that differ in a single entry,
|q(F (T, ?, ?, R), V ) ? q(F (T, ?, ?, R), V 0 )| ? ?m2 .
Condition (1), the training stability condition, bounds the change in the validation score q, when one
person?s private data in the training set T changes, and the validation set V as well as the value of the
random variable R remains the same. Our validation procedure critically relies on this condition,
and our main contribution in this paper is to identify and exploit it to provide a validation procedure
that uses the privacy budget efficiently.
As F (T, ?, ?, R) is a deterministic function, Condition (2), the validation stability condition, bounds
the change in q when one person?s private data in the validation set V changes, and the output of the
training procedure remains the same. We observe that (some version of) Condition (2) is a standard
requirement in existing differentially private algorithms that preserve the privacy of the validation
dataset while selecting a h ? C that approximately maximizes q(h, V ), even if it is not required to
maintain privacy with respect to the training data.
Several remarks are in order. First, observe that Condition (1) is a property of the differentially
private training algorithm (in addition to q and the non-private quantity being approximated). Even
if all else remains the same, different differentially private approximations to the same non-private
quantity will have different values of ?1 .
Second, Condition (1) does not always hold for small ?1 as an immediate consequence of differential
privacy of the training procedure. Differential privacy ensures that the probability of any outcome is
almost the same when the inputs differ in the value of a single individual; Condition (1) requires that
even when the same randomness is used, the validation score evaluated on the actual output of the
algorithm does not change very much when the inputs differ by a single individual?s private value.
In Section 6.1, we present an example of a problem and two ?-differentially private training algorithms which approximately optimize the same function; the first algorithm is based on exponential
mechanism, and the second on a maximum of Laplace random variables mechanism. We show
that while both provide ?-differential privacy guarantees, the first algorithm does not satisfy training stability for ?1 = o(n) and small enough ? while the second one ensures training stability for
?1 = 1 and ? = 0. In Section 4, we present two case studies of commonly used differentially private
algorithms where Conditions (1) and (2) hold for constant ?1 and ?2 .
When the (?1 , ?2 , ?)-stability condition holds, we can design an end-to-end differentially private
parameter tuning algorithm, which is shown in Algorithm 2. The algorithm first uses a validation
procedure to determine which parameter out of the given set ? is (approximately) optimal based
on the held-out data (see Algorithm 1). In the next step, the training data is re-used along with the
parameter output by Algorithm 1 and fresh randomness to generate the final output. Note that we
use Exp(?) to denote the exponential distribution with expectation ?.
4

Algorithm 1 Validate(?, T , T , V , ?1 , ?2 , ?1 , ?2 )
1: Inputs: Parameter list ? = {?1 , . . . , ?k }, training procedure T = (G, F ), validation score q,

2:
3:
4:
5:
6:
7:

training set T , validation set V , stability parameters ?1 and ?2 , training privacy parameter ?1 ,
validation privacy parameter ?2 .
for i = 1, . . . , k do
Draw Ri ? G. Compute hi = F (T, ?i , ?1 , Ri ).
Let ? = max( ?n1 , ?m2 ).
Let ti = q(hi , V ) + 2?Zi , where Zi ? Exp( ?12 ).
end for
Output i? = argmaxi ti .

Algorithm 1 takes as input a training procedure T , a parameter list ?, a validation score q, training
and validation datasets T and V , and privacy parameters ?1 and ?2 . It runs the training procedure
T on the same training set T with privacy budget ?1 for each parameter in ? to generate outputs
h1 , h2 , . . ., and then uses an ?2 -differentially private procedure to select the index i? such that
the validation score q(hi? , V ) is (approximately) maximum. For simplicity, we use a maximum of
Exponential random variables procedure, inspired by [1], to find the approximate maximum; an
exponential mechanism [21] may also be used instead. Algorithm 2 then re-uses the training data
set T to train with parameter ?i? to get the final output.
Algorithm 2 End-to-end Differentially Private Training and Validation Procedure
1: Inputs: Parameter list ? = {?1 , . . . , ?k }, training procedure T = (G, F ), validation score q,

training set T , validation set V , stability parameters ?1 and ?2 , training privacy parameter ?1 ,
validation privacy parameter ?2 .
2: i? = Validate(?, T , T, V, ?1 , ?2 , ?1 , ?2 ).
3: Draw R ? G. Output h = F (T, ?i? , ?1 , R).

3.1

Performance Guarantees

Theorem 1 shows that Algorithm 1 is (?2 , ?)-differentially private, and Theorem 2 shows privacy
guarantees on Algorithm 2. Detailed proofs of both theorems are provided in the Supplementary
Material. We observe that Conditions (1) and (2) are critical to the proof of Theorem 1.
Theorem 1 (Privacy Guarantees for Validation Procedure) If the validation score q is
(?1 , ?2 , k? )-stable with respect to the training procedure T , the privacy parameter ?1 and the
parameter set ?, then, Algorithm 1 guarantees (?2 , ?)-differential privacy.
Theorem 2 (End-to-end Privacy Guarantees) If the conditions in Theorem 1 hold, and if T is
?1 -differentially private, then Algorithm 2 is (?1 + ?2 , ?)-differentially private.
Theorem 3 shows guarantees on the utility of the validation procedure ? that it selects an index i?
which is not too suboptimal.
Theorem 3 (Utility Guarantees) Let h1 , . . . , hk be the output of the differentially private training procedure in Step (3) of Algorithm 1. Then, with probability ? 1 ? ?0 , q(hi? , V ) ?
0)
max1?i?k q(hi , V ) ? 2? log(k/?
.
?2

4

Case Studies

We next show that Algorithm 2 may be applied to design end-to-end differentially private training
and validation procedures for two fundamental statistical and machine-learning tasks ? training a linear classifier, and building a histogram density estimator. In each case, we use existing differentially
private algorithms and validation scores for these tasks. We show that the validation score satisfies
the (?1 , ?2 , ?)-stability property with respect to the training procedure for small values of ?1 and
5

?2 , and thus we can apply in Algorithm 2 with a small value of ? to obtain end-to-end differential
privacy.
Details of the case study for regularized linear classification is shown in Section 4.1, and those for
histogram density estimation is presented in the Supplementary Material.
4.1

Linear Classification based on Logistic Regression and SVM

Given a set of labelled examples (x1 , y1 ), . . . , (xn , yn ) where xi ? Rd , kxi k ? 1 for all i, and
yi ? {?1, 1}, the goal in linear classification is to train a linear classifier that largely separates
examples from the two classes. A popular solution in machine learning is to find a classifier w? by
solving a regulared convex optimization problem:
n

?
1X
w? = argminw?Rd kwk2 +
`(w, xi , yi )
2
n i=1

(1)

Here ? is a regularization parameter, and ` is a convex loss function. When ` is the logistic loss
>
function `(w, x, y) = log(1 + e?yi w xi ), then we have logistic regression. When ` is the hinge loss
`(w, x, y) = max(0, 1 ? yi w> xi ), then we have Support Vector Machines. The optimal value of ?
is data-dependent, and there is no good pre-defined way to select ? apriori. In practice, the optimal
? is determined by training a small number of classifiers with different ? values, and picking the one
that has the best performance on a held-out validation dataset.
[4] present two algorithms for computing differentially private approximations to these regularized
convex optimization problems for fixed ?: output perturbation and objective perturbation. We restate
output perturbation as Algorithm 4 (in the Supplementary Material) and objective perturbation as
Algorithm 3. It was shown by [4] that provided certain conditions hold on ` and the data, Algorithm 4
is ?-differentially
private; moreover, with some additional conditions on `, Algorithm 3 is ? +

c
2 log 1 + ?n
-differentially private, where c is a constant that depends on the loss function `, and
? is the regularization parameter.
Algorithm 3 Objective Perturbation for Differentially Private Linear Classification
1: Inputs: Regularization parameter ?, training set T = {(xi , yi ), i = 1, . . . , n}, privacy parame-

ter ?.

2: Let G be the following density over Rd : ?G (r) ? e?krk . Draw R ? G.
3: Solve the convex optimization problem:
n

?
1X
2 >
w? = argminw?Rd kwk2 +
`(w, xi , yi ) +
R w
2
n i=1
?n

(2)

4: Output w? .

In the sequel, we use the notation X to denote the set {x ? Rd : kxk ? 1}.
Definition 3 A function g : Rd ? X ? {?1, 1} ? R is said to be L-Lipschitz if for all w, w0 ? Rd ,
for all x ? X , and for all y, |g(w, x, y) ? g(w0 , x, y)| ? L ? kw ? w0 k.
Let V = {(?
xi , y?i ), i = 1, . . . , m} be the validation dataset. For our validation score, we choose a
function of the form:
m
1 X
q(w, V ) = ?
g(w, x
?i , y?i )
(3)
m i=1
where g is an L-Lipschitz loss function. In particular, the logistic loss and the hinge loss are 1Lipschitz, whereas the 0/1 loss is not L-Lipschitz for any L. Other examples of 1-Lipschitz but
non-convex losses include the ramp loss: g(w, x, y) = min(1, max(0, 1 ? yw> x)).
The following theorem shows that any non-negative and L-Lipschitz validation score is stable with
respect to Algorithms 3 and 4 and a set of regularization parameters ?; a detailed proof is provided
in the Supplementary Material. Thus we can use Algorithm 2 along with this training procedure
6

and any L-Lipschitz validation score to get an end-to-end differentially private algorithm for linear
classification.
Theorem 4 (Stability of differentially private linear classifiers) Let ? = {?1 , . . . , ?k } be a set
of regularization parameters, let ?min = minki=1 ?i , and let g ? = max(x,y)?X ,w?Rd g(w, x, y). If
` is convex and 1-Lipschitz, and if g is L-Lipschitz and non-negative, then, the validation score q in
Equation 3 is (?1 , ?2 , k? )-stable with respect to Algorithms 3 and 4, ? and ? for:



d log(dk/?)
2L
L
?
, ?2 = min g ,
1+
?1 =
?min
?min
?n
2
Example. For example, if g is chosen to be the hinge loss, then ?1 = ?min
and ?2 =


d log(dk/?)
1
. This follows from the fact that the hinge loss is 1-Lipschitz, but may be
?min 1 +
?n
unbounded for w of unbounded norm.
2
If g is chosen to be the ramp loss, then ?1 = ?min
, and ?2 = 1 (assuming that ?min ? 1). This
follows from the fact that the ramp loss is 1-Lipschitz, but bounded at 1 for any w and (x, y) ? X .

5

Experiments

In order to evaluate Algorithm 2 empirically, we compare the regularizer parameter values and performance of regularized logistic regression classifiers the algorithm produces with those produced
by four alternative methods. We used datasets from two domains, and used 10 times 10-fold crossvalidation (CV) to reduce variability in the computed performance averages.
The Methods Each method takes input (?, ?, T, V ), where ? denotes the allowed differential
privacy, T is a training set, V is a validation set, and ? = {?1 , . . . , ?k } a list of k regularizer values.
Also, let oplr (?, ?, T ) denote the application of the objective perturbation training procedure given
in Algorithm 3 such that it yields ?-differential privacy.
The first of the five methods we compare is Stability, the application of Algorithm 2 with oplr used
for learning classifiers, ? chosen in an ad-hoc manner to be 0.01, average negative ramp loss used as
validation score q, and with ?1 = ?2 = ?/2.
The four other methods work by performing the following 4 steps: (1) for each ?i ? ?, train a
differentially private classifier fi = oplr (?i , ?i , Ti ), (2) determine the number of errors ei each fi
makes on validation set V , (3) randomly choose i? from {1, 2, . . . , k} with probability P (i? = i|pi ),
and (4) output (?i? , fi? ).
What differentiates the four alternative methods is how ?i , Ti , and pi are determined. For
alphaSplit: ?i = ?/k, Ti = T , pi ? e??ei /2 , dataSplit: ?i = ?, partition T into k equally
sized sets Ti , pi ? e??ei /2 (used in [4]), Random: ?i = ?, Ti = T , pi ? 1, and Control : ?i = ?,
Ti = T , pi ? 1(i = arg maxjp
q(fj , V )). Note that for alphaSplit, ?/k > ?0 where ?0 is the
?0
0
solution of ? = k(e ? 1)? + 2k log(1/?)?0 for all of our experimental settings, except when
? = 0.3, then ?/k > ?0 ? 0.0003. The method Control is not private, and serves to provide an
approximate upper bound on the performance of Stability. The three other alternative methods are
differentially private which we state in the following theorem.
Theorem 5 (Privacy of alternative methods) If T and V are disjoint, both alphaSplit and
dataSplit are ?-differentially private. Random is ? differentially private even if T and V are
not disjoint, in which case alphaSplit and dataSplit are 2?-differentially private.
Procedures and Data We performed 10 10-fold CV as follows. For round i in each of the CV
experiments, fold i was used as a test set W on which the produced classifiers were evaluated, fold
(i mod 10)+1 was used as V , and the remaining 8 folds were used as T . Furthermore k = 10 with
? = {0.001, 0.112, 0.223, 0.334, 0.445, 0.556, 0.667, 0.778, 0.889, 1}. Note that the order of ? is
chosen such that i < j implies ?i < ?j . By Theorems 2 and 5, all methods except Control produce
7

a (?, ?)-differentially private classifier. Classifier performance was evaluated using the area under
the receiver operator curve [25] (AUC) as well as mean squared error (MSE). All computations
were done using the R environment [22], and data sets were scaled such that covariate vectors were
constrained to the unit ball. We used the following data available from the UCI Machine Learning
Repository [9]:
Adult ? 98 predictors (14 original including categorical variables that needed to be recoded). The
data set describes measurements on cases taken from the 1994 Census data base. The classification is
whether or not a person has an annual income exceeding 50000 USD, which has a prevalence of 0.22.
Each experiment involves computing more than 24000 classifiers. In order to reduce computation
time, we selected 52 predictors using the step procedure for a model computed by glm with family
binomial and logit link function.
Magic ? 10 predictors on 19020 cases. The data set describes simulated high energy gamma particles registered by a ground-based atmospheric Cherenkov gamma telescope. The classification is
whether particles are primary gammas (signal) or from hadronic showers initiated by cosmic rays in
the upper atmosphere (background). The prevalence of primary gammas is 0.35.
Adult

?

?

?

Magic

?

Adult

Magic

?

?
0.8

?

?

?

?

?

0.24

?

? ?

?

0.22

?

? ?

0.7

?

0.20

?

?
0.18

?

?

?

2.0

3.0

5.0

0.6
0.3 0.5

1.0

0.3 0.5

1.0

2.0

3.0

5.0

alpha
0.3 0.5

1.0

2.0

3.0

5.0

0.3 0.5

1.0

2.0

3.0

? Stability

5.0

alphaSplit

dataSplit

Random

Control

alpha

(a) Averages of AUC for the two data sets.

(b) Averages of MSE for the two data sets.

Figure 1: A summary of 10 times 10-fold cross-validation experiments for different privacy levels
?. Each point in the figure represents a summary of 100 data points. The error bars indiciate a
boot-strap sample estimate of the 95% confidence interval of the mean. A small amount of jitter was
added to positions on the x-axes to avoid over-plotting.
Results Figure 1 summarizes classifier performances and regularizer choices for the different values of the privacy parameter ?, aggregated over all cross-validation runs. Figure 1a shows average
performance in terms of AUC, and Figure 1b shows average performance in terms of MSE.
Looking at AUC in our experiments, Stability significantly outperformed alphaSplit and dataSplit.
However, Stability only outperformed Random for ? > 1 in the Magic data set, and was in fact outperformed by Random in the Adult data set. In the Adult data set, regularizer choice did not seem
to matter as Random performed equally well to Control . For MSE on the other hand, Stability
outperformed the differentially private alternatives in all experiments. We suggest the following
intuition regarding these results. The calibration of a logistic regression model instance, i.e., the
difference between predicted probabilities and a 0/1 encoding of the corresponding labels, is not
captured well by AUC (or 0/1 error rate) as AUC is insensitive to all strictly monotonically increasing transformations of the probabilities. MSE is often used as a measure of probabilistic model
calibration and can be decomposed into two terms: reliability (a calibration term), and refinement
(a discrimination measure) which is related to the AUC. In the Adult data set, the minor change
in AUC of Control and Random for ? > 0.5, together with the apparent insensitivity of AUC
to regularizer value, suggests that any improvement in Stability performance can only come from
(the observed) improved calibration. Unlike in the Adult data set, there is a AUC performance gap
between Control and Random in the Magic data set. This means that regularizer choice matters for
discrimination, and we observe improvement for Stability in both discrimination and calibration.

Acknowledgements This work was supported by NIH grants R01 LM07273 and U54
HL108460, the Hellman Foundation, and NSF IIS 1253942.
8

References
[1] R Bhaskar, S Laxman, A Smith, and A Thakurta. Discovering frequent patterns in sensitive
data. In KDD, 2010.
[2] A. Blum, C. Dwork, F. McSherry, and K. Nissim. Practical privacy: the SuLQ framework. In
PODS, 2005.
[3] K. Chaudhuri and D. Hsu. Convergence rates for differentially private statistical estimation. In
ICML, 2012.
[4] K. Chaudhuri, C. Monteleoni, and A. D. Sarwate. Differentially private empirical risk minimization. Journal of Machine Learning Research, 12:1069?1109, March 2011.
[5] K. Chaudhuri, A.D. Sarwate, and K. Sinha. Near-optimal algorithms for differentially-private
principal components. Journal of Machine Learning Research, 2013 (to appear).
[6] L. Devroye and G. Lugosi. Combinatorial methods in density estimation. Springer, 2001.
[7] C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private
data analysis. In Theory of Cryptography, Berlin, Heidelberg, 2006.
[8] C. Dwork, G. Rothblum, and S. Vadhan. Boosting and differential privacy. In FOCS, 2010.
[9] A. Frank and A. Asuncion. UCI machine learning repository, 2013.
[10] A. Friedman and A. Schuster. Data mining with differential privacy. In KDD, 2010.
[11] S. R. Ganta, S. P. Kasiviswanathan, and A. Smith. Composition attacks and auxiliary information in data privacy. In KDD, 2008.
[12] M. Hardt and A. Roth. Beyond worst-case analysis in private singular vector computation. In
STOC, 2013.
[13] M. Hardt and G. Rothblum. A multiplicative weights mechanism for privacy-preserving data
analysis. In FOCS, pages 61?70, 2010.
[14] M. Hay, V. Rastogi, G. Miklau, and D. Suciu. Boosting the accuracy of differentially private
histograms through consistency. PVLDB, 3(1):1021?1032, 2010.
[15] P. Jain, P. Kothari, and A. Thakurta. Differentially private online learning. In COLT, 2012.
[16] M C Jones, J S Marron, and S J Sheather. A brief survey of bandwidth selection for density
estimation. JASA, 91(433):401?407, 1996.
[17] M. Kapralov and K. Talwar. On differentially private low rank approximation. In SODA, 2013.
[18] D. Kifer, A. Smith, and A. Thakurta. Private convex optimization for empirical risk minimization with applications to high-dimensional regression. In COLT, 2012.
[19] J. Lei. Differentially private M-estimators. In NIPS 24, 2011.
[20] A. Machanavajjhala, D. Kifer, J. M. Abowd, J. Gehrke, and L. Vilhuber. Privacy: Theory
meets practice on the map. In ICDE, 2008.
[21] F. McSherry and K. Talwar. Mechanism design via differential privacy. In FOCS, 2007.
[22] R Core Team. R: A Language and Environment for Statistical Computing. R Foundation.
[23] B. Rubinstein, P. Bartlett, L. Huang, and N. Taft. Learning in a large function space: Privacypreserving mechanisms for svm learning. Journal of Privacy and Confidentiality, 2012.
[24] A.D. Sarwate and K. Chaudhuri. Signal processing and machine learning with differential
privacy: Algorithms and challenges for continuous data. IEEE Signal Process. Mag., 2013.
[25] J. A. Swets and R. M. Pickett. Evaluation of Diagnostic Systems. Methods from Signal Detection Theory. Academic Press, New York, 1982.
[26] Berwin A Turlach. Bandwidth selection in kernel density estimation: A review. In CORE and
Institut de Statistique. Citeseer, 1993.
[27] S. Vinterbo. Differentially private projected histograms: Construction and use for prediction.
In ECML, 2012.
[28] L. Wasserman and S. Zhou. A statistical framework for differential privacy. JASA,
105(489):375?389, 2010.
[29] J. Xu, Z. Zhang, X. Xiao, Y. Yang, and G. Yu. Differentially private histogram publication. In
ICDE, 2012.

9

6
6.1

Appendix
An Example to Show Training Stability is not a Direct Consequence of Differential
Privacy

We now present an example to illustrate that training stability is a property of the training algorithm
and not a direct consequence of differential privacy. We present a problem and two ?-differentially
private training algorithms which approximately optimize the same function; the first algorithm
is based on exponential mechanism, and the second on a maximum of Laplace random variables
mechanism. We show that while both provide ?-differential privacy guarantees, the first algorithm
does not satisfy training stability while the second one does.
Let i ? {1, . . . , l}, and let f : X n ? R ? [0, 1] be a function such that for all i and all datasets D
and D0 of size n that differ in the value of a single individual, |f (D, i) ? f (D0 , i)| ? n1 .
Consider the following training and validation problem. Given a sensitive dataset D, the private
training procedure A outputs a tuple (i? , t1 , . . . , tl ), where i? is the output of the ?/2-differentially
private exponential mechanism [21] run to approximately maximize f (D, i), and each ti is equal to
2l
f (D, i) plus an independent Laplace random variable with standard deviation ?n
. For any validation
?
?
dataset V , the validation score q((i , t1 , . . . , tl ), V ) = ti .
It follows from standard results that A is ?-differentially private. Moreover, A can be represented
by a tuple TA = (GA , FA ), where GA is the following density over sequences of real numbers of
length l + 1:
1
GA (r0 , r1 , . . . , rl ) = 10?r0 ?1 ? l e?(|r1 |+|r2 |+...+|rl |)
2
Thus GA is the product of the uniform density on [0, 1] and l standard Laplace densities. Consider
the following map E0 . For r ? [0, 1], let
P
P
n?f (D,j)/4
n?f (D,j)/4
j<i e
j?i e
E0 (r) = i, if P n?f (D,j)/4 ? r ? P n?f (D,j)/4
je
je
In other words, E0 (r) is the map that converts a random number r drawn from the uniform distribution on [0, 1] to the ?/2-differentially private exponential mechanism distribution that approximately
maximizes f (D, i). Given a l + 1-tuple R = (R0 , R1 , . . . , Rl ), FA is now the following map:


2lR1
2lR2
2lRl
FA (D, ?, R) = E(R0 ), f (D, 1) +
, f (D, 2) +
, . . . , f (D, l) +
?n
?n
?n
Let l = 2 and D and D0 be two datasets that differ in the value of a single individual. Suppose it
is the case that f (D, 1) = 1, f (D, 2) = 12 and f (D0 , 1) = 1 ? n1 , f (D0 , 2) = 12 + n1 . Observe

en?/4
, and 2 with probability
en?/4 +en?/8
(n?1)?/4
en?/8
e
, where as for D0 , it picks 1 with probability e(n?1)?/4 +e(n+2)?/8 and 2 with probaen?/4 +en?/8
e(n?1)?/4
en?/4
e(n+2)?/8
. Thus, if R0 lies in the interval [ e(n?1)?/4
,
], then,
bility e(n?1)?/4
+e(n+2)?/8
+e(n+2)?/8 en?/4 +en?/8
FA (D, ?, R) = t1 whereas FA (D0 , ?, R) = t2 . When n is large enough, with high probability, |t1 ? t2 | ? 13 ; thus, the training stability condition does not hold for A for ?1 = o(n) and
en?/8 (e?/2 ?1)
? < (en?/8
.
+1)(en?/8 +e?/2 )

that for D, the exponential mechanism picks 1 with probability

Consider a different algorithm A0 which computes t1 , . . . , tl first, and then outputs the index i? that
maximizes ti? . Then A0 can be represented by a tuple TA0 = (GA0 , FA0 ), where GA0 is a density
over sequences of real numbers of length l as follows:
GA (r1 , . . . , rl ) =

1 ?(|r1 |+...+|rl |)
e
2l

and FA0 is the map:


lRi
lR1
lR2
lRl
FA0 (D, ?, R) = argmaxi (f (D, i) +
), f (D, 1) +
, f (D, 2) +
, . . . , f (D, l) +
?n
?n
?n
?n
10

For the same value of R1 , . . . , Rl , if i? = i on input dataset D and if i? = i0 on input dataset D0 ,
then, |f (D, i) ? f (D, i0 )| ? n1 ; this implies that
1
n
with probability 1 over GA0 . Thus the training stability condition holds for ?1 = 1 and ? = 0.
|q(FA0 (D, ?, R), V ) ? q(FA0 (D0 , ?, R), V )| = |ti ? ti0 | = |f (D, i) ? f (D0 , i0 )| ?

6.2

Output Perturbation Algorithm

We present the output perturbation algorithm for regularized linear classification.
Algorithm 4 Output Perturbation for Differentially Private Linear Classification
1: Inputs: Regularization parameter ?, training set T = {(xi , yi ), i = 1, . . . , n}, privacy parame-

ter ?.

2: Let G be the following density over Rd : ?G (r) ? e?krk . Draw R ? G.
3: Solve the convex optimization problem:
n

1
1X
w = argminw?Rd ?kwk2 +
`(w, xi , yi )
2
n i=1
?

4: Output w? +

6.3

(4)

2
??n R.

Case Study: Histogram Density Estimation

Our second case study is developing an end-to-end differentially private solution for histogrambased density estimation. In density estimation, we are given n samples x1 , . . . , xn drawn from
an unknown density f , and our goal is to build an approximation f? to f . In a histogram density
estimator, we divide the range of the data into equal-sized bins of width h; if ni out of n of the input
P1/h ni
? 1(x ? Bin i).
samples lie in bin i, then f? is the density function: f?(x) = i=1 hn
A critical parameter while constructing the histogram density estimator is the bin size h. There is
much theoretical literature on how to choose h ? see [16, 26] for surveys. However, the choice
of h is usually data-dependent, and in practice, the optimal h is often determined by building a
histogram density estimator for a few different values of h, and selecting the one which has the best
performance on held-out validation data.
The most popular measure to evaluate the quality of a density estimator is the L2 -distance or the
Integrated Square Error (ISE) between the density estimate and the true density:
Z
Z
Z
Z
kf? ? f k2 = (f?(x) ? f (x))2 dx =
f 2 (x)dx + f?2 (x)dx ? 2 f (x)f?(x)dx
(5)
x

x

x

x

f is typically unknown, so the ISE cannot be computed exactly. Fortunately it is still possible to
compare multiple density estimates based on this distance. The first term in the right hand side of
Equation 5 depends only on f , and is equal for all f?. The second term is a function of f? only and can
thus be computed. The third term is 2Ex?f [f?(x)], and even though it cannot be computed exactly
without knowledge of f , we can estimate it based on a held out validation dataset. Thus, given a
density estimator f? and a validation dataset V = {z1 , . . . , zm }, we will use the following function
to evaluate the quality of f? on V :
Z
m
2 X?
?
f (zi )
(6)
q(f , V ) = ? f?2 (x)dx +
m i=1
x
A higher value of q indicates a smaller distance kf?? f k2 , and thus a higher quality density estimate.
For other measures, see [6].
In the sequel, we assume that the data lies in the interval [0, 1] and that this interval is known in
advance. For ease of notation, we also assume without loss of generality that h1 is an integer. For
11

ease of exposition, we confine ourselves to one-dimensional data, although the general techniques
can be easily extended to higher dimensions. Given n samples and a bin size h, several works,
including [7, 19, 27, 28, 20, 29, 14] have shown different ways of constructing and sampling from
differentially private histograms. The most basic approach is to construct a non-private histogram
and then add Laplace noise to each cell, followed by some post-processing. Algorithm 5 presents a
variant of a differentially private histogram density estimator due to [19] in our framework.
Algorithm 5 Differentially Private Histogram Density Estimator
1: Inputs: Bin size h (such that 1/h is an integer), data T = {x1 , . . . , xn }, privacy parameter ?.
2: for i = 1, . . . , h1 do
3:
Draw Ri independently from the standard Laplace density: ?G (r) = 21 e?|r| .


h
Pn
i
,
? i = max 0, ni +
Let Ii = i?1
j=1 1(xj ? Ii ), and let n
h
h . Define: ni =
5: end for P
P1/h n? i
6: Let n
? = in
? i . Return the density estimator: f?(x) = i=1 h?
n ? 1(x ? Ii )

4:

2Ri
?



.

The following theorem shows stability guarantees on the differentially private histogram density
estimator described in Algorithm 5.
Theorem 6 (Stability of Private Histogram Density Estimator) Let H = {h1 , . . . , hk } be a set
?
of bin sizes, and let hmin = mini hi . For any fixed ?, if the sample size n ? 1 + 2?ln(4k/?)
, then,
h
min

the validation score q in Equation 6 is (?1 , ?2 , k? )-Stable with respect to Algorithm 5 and H for:
ln(4k/?)
2
6
?
, ?2 = hmin
, where: ? = 2n?
?1 = (1??)h
.
h
min
min

6.4

Proofs of Theorems 1, 2 and 3

We now present the proofs of Theorems 1, 2 and 3. Our proofs involve ideas similar to those in
the analysis of the multiplicative weights update method for answering a set of linear queries in a
differentially private manner [13].
Let A(D) denote the output of Algorithm 1 when the input is a sensitive dataset D = (T, V ), where
T is the training part and V is the validation part. Let D0 = (T 0 , V ) where T and T 0 differ in the
value of a single individual, and let D00 = (T, V 0 ) where V and V 0 differ in the value of a single
individual. The proof of Theorem 1 is a consequence of the following two lemmas.
Lemma 1 Suppose that the conditions in Theorem 1 hold. Then, for all D = (T, V ), all D0 =
(T 0 , V ), such that T and T 0 differ in the value of a single individual, and for any set of outcomes S:
Pr(A(D) ? S) ? e?2 Pr(A(D0 ) ? S) + ?

(7)

Lemma 2 Suppose that the conditions in Theorem 1 hold. Then, for all D = (T, V ), all D00 =
(T, V 0 ) such that V and V 0 differ in the value of a single individual, and for any set of outcomes S,
Pr(A(D) ? S) ? e?2 Pr(A(D00 ) ? S) + ?

(8)

P ROOF : (Of Lemma 1) Let S = (I, C), where I ? [k] is a set of indices and C ? C. Let E be the
event that all of R1 , . . . , Rk lie in the set ?. We will first show that conditioned on E, for all i, it
holds that:
Pr(i? = i|D, E) ? e?2 Pr(i? = i|D0 , E)
(9)
Since Pr(E) ? 1 ? ?, from the conditions in Theorem 1, for any subset I of indices, we can write:
Pr(i? ? I|D) ?
?
?
?

Pr(i? ? I|D, E) Pr(E) + (1 ? Pr(E))
e?2 Pr(i? ? I|D0 , E) Pr(E) + ?
e?2 Pr(i? ? I, E|D0 ) + ?
e?2 Pr(i? ? I|D0 ) + ?
12

(10)

We will now prove Equation 9. For this purpose, we adopt the following notation. We use the
notation Z\i to denote the random variables Z1 , . . . , Zi?1 , Zi+1 , . . . , Zk and z\i to denote the set of
values z1 , . . . , zi?1 , zi+1 , . . . , zk . We also use the notation h(?) to represent the density induced on
the random variables Z1 , . . . , Zk by Algorithm 1. In addition, we use the notation R to denote the
vector (R1 , . . . , Rk ). We first fix a value z\i for Z\i , and a value of R such that R1 , . . . , Rk all lie
in ?, and consider the ratio of probabilities:
Pr(i? = i|Z\i = z\i , D, R)
Pr(i? = i|Z\i = z\i , D0 , R)
Observe that this ratio of probabilities is equal to:
Pr(Zi + q(F (T, ?i , ?1 , Ri ), V ) ? supj6=i zj + q(F (T, ?j , ?1 , Rj ), V ))
Pr(Zi + q(F (T 0 , ?i , ?1 , Ri ), V ) ? supj6=i zj + q(F (T 0 , ?j , ?1 , Rj ), V ))
which is in turn equal to:
Pr(Zi ? supj6=i zj + q(F (T, ?j , ?1 , Rj ), V ) ? q(F (T, ?i , ?1 , Ri ), V ))
Pr(Zi ? supj6=i zj + q(F (T 0 , ?j , ?1 , Rj ), V ) ? q(F (T 0 , ?i , ?1 , Ri ), V ))
Observe that from the stability condition,
|(q(F (T, ?j , ?1 , Rj ), V ) ? q(F (T, ?i , ?1 , Ri ), V )) ? (q(F (T 0 , ?j , ?1 , Rj ), V ) ? q(F (T 0 , ?i , ?1 , Ri ), V ))|
? |q(F (T, ?j , ?1 , Rj ), V ) ? q(F (T 0 , ?j , ?1 , Rj ), V 0 )| + |q(F (T, ?i , ?1 , Ri ), V ) ? q(F (T 0 , ?i , ?1 , Ri ), V )|
2?1
? 2?
?
n
Thus, the ratio of the probabilities is at most the ratio Pr(Zi ? ?)/ Pr(Zi ? ? + 2?) where
? = supj6=i zj +q(F (T, ?j , ?1 , Rj ), V )?q(F (T, ?i , ?1 , Ri ), V ), which is at most e?2 by properties
of the exponential distribution. Thus, we have established that for all z\i , for all R in ?k ,
Pr(i? = i|Z\i = z\i , D, R) ? e?2 ? Pr(i? = i|Z\i = z\i , D0 , R)
Equation 9 follows by integrating over z\i and R. The lemma follows. 
P ROOF :(Of Lemma 2) Let S = (I, C), where I ? [k] is a set of indices and C ? C. Let E be the
event that all of R1 , . . . , Rk lie in ?. We will first show that conditioned on E, for all i, it holds that:
Pr(i? = i|D, E) ? e?2 Pr(i? = i|D00 , E)

(11)

Since Pr(E) ? 1 ? ?, from the conditions in Theorem 1, for any subset I of indices, we can write:
Pr(i? ? I|D) ?
?
?
?

Pr(i? ? I|D, E) Pr(E) + (1 ? Pr(E))
e?2 Pr(i? ? I|D00 , E) Pr(E) + ?
e?2 Pr(i? ? I, E|D00 ) + ?
e?2 Pr(i? ? I|D00 ) + ?

(12)

We will now focus on showing Equation 11. We first consider the case when event E holds, that is,
Rj ? R, for j = 1, . . . , k. In this case, the stability definition and the conditions of the theorem
imply that for all ?j ? ?,
|q(F (T, ?j , ?1 , Rj ), V ) ? q(F (T, ?j , ?1 , Rj ), V 0 )| ?

?2
??
m

(13)

In what follows, we use the notation Z\i to denote the random variables Z1 , . . . , Zi?1 , Zi+1 , . . . , Zk
and z\i to denote the set of values z1 , . . . , zi?1 , zi+1 , . . . , zk . We also use the notation h(?) to
represent the density induced on the random variables Z1 , . . . , Zk by Algorithm 1. In addition, we
use the notation R to denote the vector (R1 , . . . , Rk ). We first fix a value z\i for Z\i , and a value of
R such that E holds, and consider the ratio of probabilities:
Pr(i? = i|Z\i = z\i , D, R)
Pr(i? = i|Z\i = z\i , D00 , R)
13

Observe that this ratio of probabilities is equal to:
Pr(Zi + q(F (T, ?i , ?1 , Ri ), V ) ? supj6=i zj + q(F (T, ?j , ?1 , Rj ), V ))
Pr(Zi + q(F (T, ?i , ?1 , Ri ), V 0 ) ? supj6=i zj + q(F (T, ?j , ?1 , Rj ), V 0 ))
which is in turn equal to:
Pr(Zi ? supj6=i zj + q(F (T, ?j , ?1 , Rj ), V ) ? q(F (T, ?i , ?1 , Ri ), V ))
Pr(Zi ? supj6=i zj + q(F (T, ?j , ?1 , Rj ), V 0 ) ? q(F (T, ?i , ?1 , Ri ), V 0 ))
Observe that from Equation 13,
|(q(F (T, ?j , ?1 , Rj ), V )?q(F (T, ?i , ?1 , Ri ), V ))?(q(F (T, ?j , ?1 , Rj ), V 0 )?q(F (T, ?i , ?1 , Ri ), V 0 ))| ?
Thus, the ratio of the probabilities is at most the ratio Pr(Zi ? ?)/ Pr(Zi ? ? + 2?) for ? =
supj6=i zj + q(F (T, ?j , ?1 , rj ), V ) ? q(F (T, ?i , ?1 , ri ), V ), which is at most e?2 by properties of
the exponential distribution. Thus, we have established that when R ? ?k , for all j,
Pr(i? = i|Z\i = z\i , D, R)
? e?2
Pr(i? = i|Z\i = z\i , D00 , R)
Thus for any such R, we can write:
Pr(i? = i|D, R)
Pr(i? = i|D00 , R)

R
=

z\i

R
z\i

Pr(i? = i|Z\i = z\i , D, R)h(z\i )dz\i
Pr(i? = i|Z\i = z\i , D00 , R)h(z\i )dz\i

? e?2

Equation 11 now follows by integrating R over E. 
P ROOF :(Of Theorem 1) The proof of Theorem 1 follows from a combination of Lemmas 1 and 2.

P ROOF :(Of Theorem 2) The proof of Theorem 2 follows from privacy composition; Theorem 1
ensures that Step (2) of Algorithm 2 is (?2 , ?)-differentially private; moreover the training procedure
T is ?1 -differentially private. The theorem follows by composing these two results. 
P ROOF :(Of Theorem 3) Observe that:




2? log(k/?0 )
log(k/?0 )
? Pr ?j s.t. Zj ?
Pr q(hi? , V ) < max q(hi , V ) ?
1?i?k
?2
?2
By properties of the exponential distribution, for any fixed j, Pr(Zj ?
theorem follows by an Union Bound. 
6.5

log(k/?0 )
)
?2

?

?0
k .

Thus the

Proof of Theorem 4

P ROOF : (Of Theorem 4 for Output Perturbation) Let T and T 0 be two training sets which differ in
a single labelled example ((xn , yn ) vs. (x0n , yn0 )), and let w? (T ) and w? (T 0 ) be the solutions to the
regularized convex optimization problem in Equation 1 when the inputs are T and T 0 respectively.
We observe that for fixed ?, ? and R,
F (T, ?, ?, R) ? F (T 0 , ?, ?, R) = w? (T ) ? w? (T 0 )
When the training sets are T and T 0 , the objective functions in the regularized convex optimization
problems are both ?-strongly convex, and they differ by n1 (`(w, xn , yn )?`(w, x0n , yn0 )). Combining
this fact with Lemma 1 of [4], and using the fact that ` is 1-Lipschitz, we have that for all ? and R,
2
?n
Since g is L-Lipschitz, this implies that for any fixed validation set V , and for all ?, ? and R,
kF (T, ?, ?, R) ? F (T 0 , ?, ?, R)k ?

|q(F (T, ?, ?, R), V ) ? q(F (T 0 , ?, ?, R), V )| ?
14

2L
?n

(14)

2?2
? 2?
m

Now let V and V 0 be two validation sets that differ in the value of a single labelled example
(?
xm , y?m ). Since g ? 0 for all inputs, for any such V and V 0 , and for a fixed ?, ? and R,
|q(F (T, ?, ?, R), V ) ? q(F (T, ?, ?, R), V 0 )| ? gmax
m , where
gmax =

sup g(F (T, ?, ?, R), x, y)
(x,y)?X

By definition, gmax ? g ? . Moreover, as g is L-Lipschitz,
gmax ? L ? kF (T, ?, ?, R)k
Now, let E be the event that kRk ? d log(dk/?). From Lemma 4 of [4], Pr(E) ? 1 ? ?/k. Thus,
provided E holds, we have that:


d log(dk/?)
1
d log(dk/?)
1
d log(dk/?)
?
kF (T, ?, ?, R)k ? kw k +
? +
=
1+
??n
?
??n
?
n?
where the bound on kw? k follows from an application of Lemma 1 of [4] on the functions 12 ?kwk2
Pn
and 21 ?kwk2 + n1 i=1 `(w, xi , yi ). This implies that provided E holds, for all training sets T , and
for all ?,


L
d log(dk/?)
|q(F (T, ?, ?, R), V ) ? q(F (T, ?, ?, R), V 0 )| ?
1+
(15)
?m
n?
The theorem now follows from a combination of Equations 14 and 15, and the definition of g ? . 
P ROOF : (Of Theorem 4 for Objective Perturbation) Let T and T 0 be two training sets which differ in
a single labelled example (xn , yn ). We observe that for a fixed R and ?, the objective of the regularized convex optimization problem in Equation 2 differs in the term n1 (`(w, xn , yn ) ? `(w, x0n , yn0 )).
Combining this with Lemma 1 of [4], and using the fact that ` is 1-Lipschitz, we have that for all ?,
?, R,
2
kF (T, ?, ?, R) ? F (T 0 , ?, ?, R)k ?
?n
Since g is L-Lipschitz, this implies that for any fixed validation set V , and for all ? and r,
|q(F (T, ?, ?, R), V ) ? q(F (T 0 , ?, ?, R), V )| ?

2L
?n

(16)

Now let V and V 0 be two validation sets that differ in the value of a single labelled example
(?
xm , y?m ). Since g ? 0, for any such V and V 0 , |q(F (T, ?, ?, R), V ) ? q(F (T, ?, ?, R), V 0 )| ?
gmax
m , where
gmax = sup g(F (T, ?, ?, R), x, y)
(x,y)?X
?

By definition gmax ? g . Moreover, as g is L-Lipschitz,
gmax ? L ? kF (T, ?, ?, R)k
Let E be the event that kRk ? d log(dk/?). From Lemma 4 of [4], Pr(E) ? 1 ? ?/k. Thus,
provided E holds, we have that:


1 + kRk/(?n)
1
d log(dk/?)
kF (T, ?, ?, R)k ?
?
1+
?
?
n?
This implies that provided E holds, for all training sets T , and for all ?,


d log(dk/?)
L
1+
|q(F (T, ?, ?, R), V ) ? q(F (T, ?, ?, R), V 0 )| ?
?m
n?

(17)

The theorem now follows from a combination of Equations 16 and 17, and the definition of g ? . 

15

6.6

Proof of Theorem 6

Lemma 3 (Concentration of Sum of Laplace Random Variables) Let Z1 , . . . , Zs be s ? 2 iid
standard Laplace random variables, and let Z = Z1 + . . . + Zs . Then, for any ?,

?s
?
?
1
e??/ s ? 4e??/ s
Pr(Z ? ?) ? 1 ?
s
P ROOF : The proof follows from using the method of generating functions. The generating function
1
for the standard Laplace distribution is: ?(X) = E[etX ] = 1?t
2 , for |t| ? 1. As Z1 , . . . , Zs are
tZ
independently distributed, the generating function for Z is E[e ] = (1 ? t2 )?s . Now, we can write:
Pr(Z ? ?)

=
?

Plugging in t =

?1 ,
s

Pr(etZ ? et? )
E[etZ ]
= e?t? ? (1 ? t2 )?s
et?

we get that:

Pr(Z ? ?) ?

1
1?
s

?s

?

e??/

s

The lemma follows by observing that for s ? 2, (1 ? 1s )s ? 14 . 
P ROOF : (Of Theorem 6) Let V = {z1 , . . . , zm } be a validation dataset, and let V 0 be a valida0
). We use the notation R to denote
tion dataset that differs from V in a single sample (zm vs zm
the sequence of values R = (R1 , R2 , . . . , R1/h ). Given an input sample T , a bin size h, a privacy parameter ?, and a sequence R, we use the notation f?T,h,?,R to denote the density estimator
F (T, h, ?, R). For all such T , all h, all ? and all R, we can write:
2 ?
0
|q(F (T, h, ?, R), V ) ? q(F (T, h, ?, R), V 0 )| =
(fT,h,?,R (zm ) ? f?T,h,?,R (zm
))
m
?i
2
2 maxi n
?
?
(18)
?
m
h?
n
mh
For a fixed value of h, we define the following event E:
1/h
X

Ri ? ?

i=1

ln(4k/?)
?
h

Using the symmetry of Laplace random variables and Lemma 3, we get that Pr(E) ? 1 ? ?/k. We
observe that provided the event E holds,
n
? ?n?

1/h
X

Ri ? n ?

i=1

2 ln(4k/?)
?
? n(1 ? ?)
? h

(19)

Let T and T 0 be two input datasets that differ in a single sample (xn vs x0n ). We fix a bin size h, a
value of ?, and a sequence R, and for these fixed values, we use the notation n
? i and n
? 0i to denote
P the
0
value of n
?P
in
Algorithm
5
when
the
inputs
are
T
and
T
respectively.
Similarly,
we
use n
? = in
?i
i
and n
?0 = i n
? 0i .
For any V , we can write:
m

q(F (T, h, ?, R), V ) ? q(F (T 0 , h, ?, R), V )

=

?

2 X ?
(fT,h,?,R (zj ) ? f?T 0 ,h,?,R (zj ))
m j=1
1/h
X
i=1


h?

n
? 2i
n
? 02
i
?
h2 n
?2
h2 n
? 02


(20)

We now look at bounding the right hand side of Equation 20 term by term. Suppose T 0 is obtained
rom T by moving a single sample xn from bin a to bin b in the histogram. Then, depending on the
relative values of n
? a and n
? b , there are four cases:
16

1.
2.
3.
4.

n
? 0a
n
? 0a
n
? 0a
n
? 0a

=n
? a ? 1, n
? 0b = n
? b + 1. Thus n
?0 = n
?.
0
0
=n
? a = 0, n
?b = n
? b + 1. Thus n
? =n
? + 1.
0
0
=n
? a ? 1, n
?b = n
? b = 0. Thus n
? =n
? ? 1.
=n
? a = 0, n
? 0b = n
? b = 0. Thus n
?0 = n
?.

In the fourth case, f?T,h,?,R = f?T 0 ,h,?,R , and thus the right hand side of Equation 20 is 0. Moreover,
the second and the third cases are symmetric. We thus focus on the first two cases.
In the first case, the first term in the right hand side of Equation 20 can be written as:


1/h
1/h
m X
m X
2 X
2 X
n
? 0i 
n
?i
n
?i ? n
? 0i 


=
?
?
1(zj ? Ii ) ?
1(z
?
I
)
?

 ?


j
i
m j=1 i=1
h?
n h?
n0
m j=1 i=1
h?
n
2
1
2
?m?
?
m
h?
n
h?
n
The second term on the right hand side of Equation 20 can be written as:

1/h  2
X
n
? 2a + n
? 2b ? (?
na ? 1)2 ? (?
nb + 1)2
n
?i
n
? 02


i
=
?


h?
n2
h?
n02
h?
n2
i=1
 2?
nb ? 2 
2
 na ? 2?
= 
?
h?
n2
h?
n
where the last step follows from the fact that n
? 0b = n
?b + 1 ? n
? . Thus, for the first case, the right
4
hand side of Equation 20 is at most h?
n.
?

We now consider the second case. The first term on the right hand side of Equation 20 can be written
as:


1/h
m X
2 X
n
?i
n
? 0i 

1(zj ? Ii ) ?
?

 ?
m j=1 i=1
h?
n h?
n0


1/h
m X
 2 X
n
?i
n
? 0i


= 
?
1(zj ? Ii ) ?
?

mh j=1 i=1
n
?
n
?+1
?
?

2
1
?m?
? max(|?
ni (?
n + 1) ? n
?in
? |, |?
ni (?
n + 1) ? n
? (?
ni + 1)|)
hm
n
? (?
n + 1)
1
2
2
?
? max(|?
ni |, |?
n?n
? i |) ?
h n
? (?
n + 1)
h(?
n + 1)

where the last step follows from the fact that max(|?
ni |, |?
n?n
? i |) ? n
? . The second term on the right
hand side of Equation 20 can be written as:

  2
1/h  2
X
X n
n
?i
n
? 02
? 2i
n
? 2i
(?
nb + 1)2 
?


 n
i
?
=
?
+  b2 ?



2
02
2
2
h?
n
h?
n
h?
n
h(?
n + 1)
h?
n
h(?
n + 1)2
i=1
i6=b
 (?
X
2?
n+1
? )(2?
nb n
?+n
?+n
? b ) 
 nb ? n
=
?
n
? 2i + 

2
2
2
2
h?
n (?
n + 1)
h?
n (?
n + 1)
i6=b

?

2?
n+1
n
? ? 2?
n(?
n + 1)
4
+
?
h(?
n + 1)2
h?
n2 (?
n + 1)2
h(?
n + 1)

Thus, in the second case, the right hand side of Equation 20 is at most h(?n6+1) . We observe that the
third case is symmetric to the second case, and thus we can carry out very similar calculations in
6
0
the third case to show that the right hand side is at most h?
n . Thus, we have that for any T and T ,
provided the event E holds,
6
|q(F (T, h, ?, R), V ) ? q(F (T 0 , h, ?, R), V )| ?
(21)
h?
n
The theorem now follows by combining Equation 21 with Equation 19. 

17

6.7

Proof of Theorem 5

Lemma 4 (Parallel construction) Let A = {A1 , A2 , . . . , Ak } be a list of k independently randomized functions, and let Ai be ?i -differentially private. Let {D1 , D2 , . . . , Dk } be k subsets of a set
D such that i 6= j =? Di ? Dj = ?. Algorithm B(D, A) = (A1 (D1 ), A2 (D2 ), . . . , Ak (Dk )) is
max1?i?k ?i -differentially private.
P ROOF : Let D, D0 be two datasets such that their symmetric difference contains one element. We
have that
P (B(D, A) ? S)
P (B(D, A) ? S1 ? ? ? ? ? Sk )
P (A1 (D1 ) ? S1 ) ? ? ? P (Ak (Dk ) ? Sk )
=
=
P (B(D0 , A) ? S)
P (B(D0 , A) ? S1 ? ? ? ? ? Sk )
P (A1 (D10 ) ? S1 ) ? ? ? P (Ak (Dk0 ) ? Sk )
(22)
by independence of randomness in the Ai . Since i 6= j =? Di ? Dj = ?, there exists at most one
index j such that Dj 6= Dj0 . If j does not exist, (22) reduces to e0 ? emax1?i?k ?i . Let j exist, then
P (B(D, A) ? S)
P (Aj (Dj ) ? Sj )
? e?j ? emax1?i?k ?i ,
=
P (B(D0 , A) ? S)
P (Aj (Dj0 ) ? Sj )
which concludes the proof. 
P ROOF : (Theorem 5) We begin by separating task (a) of producing the fi in step 1. from the task
(b) of computing ei in step 2. and selecting i? in step 3.
From the parallel construction Lemma 4 it follows that (a) in dataSplit is ?-differentially private.
From standard composition of privacy it follows that (a) in alphaSplit is ?-differentially private.
Task (b) is for both alphaSplit and dataSplit an application of the exponential mechanism [21],
which for choosing with a probability proportional to (?ei ) yields 2?-differential privacy, where
? is the sensitivity of ei . Since a single change in V can change the number of errors any fixed
classifier can make by at most 1 = ?, we get that task (b) is ?-differentially private for  = ?/2.
If T and V are disjoint, we get by parallel construction that both alphaSplit and dataSplit yield
?-differential privacy. If T and V are not disjoint, by standard composition of privacy we get that
both alphaSplit and dataSplit yield 2?-differential privacy.
In Random, the results of step 2. in task (b) are never used in step 3. Step 3 is done without looking
at the input data and does not incur loss of differential privacy. We can therefore simulate Random
by first choosing i? uniformly at random, and then computing fi at ?-differential privacy, which by
standard privacy composition is ?-differentially private. 
6.8

Experimental selection of regularizer index

18

Adult

Magic

6

?
?
4

?
?

?
?

?

2

0.3 0.5

1.0

?
?

?

?

?

2.0

3.0

5.0

0.3 0.5

1.0

2.0

3.0

5.0

alpha

? Stability

alphaSplit

dataSplit

Random

Control

Figure 2: A summary of 10 times 10-fold cross-validation selection of regularizer index i into ?
for different privacy levels ?. Each point in the figure represents a summary of 100 data points.
The error bars indiciate a boot-strap sample estimate of the 95% confidence interval of the mean. A
small amount of jitter was added to positions on the x-axes to avoid over-plotting.

19


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 4202-on-u-processes-and-clustering-performance.pdf

On U -processes and clustering performance

St?ephan Cl?emenc?on?
LTCI UMR Telecom ParisTech/CNRS No. 5141
Institut Telecom, Paris, 75634 Cedex 13, France
stephan.clemencon@telecom-paristech.fr

Abstract
Many clustering techniques aim at optimizing empirical criteria that are of the
form of a U -statistic of degree two. Given a measure of dissimilarity between
pairs of observations, the goal is to minimize the within cluster point scatter over
a class of partitions of the feature space. It is the purpose of this paper to define
a general statistical framework, relying on the theory of U -processes, for studying the performance of such clustering methods. In this setup, under adequate
assumptions on the complexity of the subsets forming the partition
candidates, the
?
excess of clustering risk is proved to be of the order OP (1/ n). Based on recent
results related to the tail behavior of degenerate U -processes, it is also shown how
to establish tighter rate bounds. Model selection issues, related to the number of
clusters forming the data partition in particular, are also considered.

1

Introduction

In cluster analysis, the objective is to segment a dataset into subgroups, such that data points in the
same subgroup are more similar to each other (in a sense that will be specified) than to those in
other subgroups. Given the wide range of applications of the clustering paradigm, numerous data
segmentation procedures have been introduced in the machine-learning literature (see Chapter 14 in
[HTF09] and Chapter 8 in [CFZ09] for recent overviews of ?off-the-shelf? clustering techniques).
Whereas the design of clustering algorithms is still receiving much attention in machine-learning
(see [WT10] and the references therein for instance), the statistical study of their performance,
with the notable exception of the celebrated K-means approach, see [Har78, Pol81, Pol82, BD04]
and more recently [BDL08] in the functional data analysis setting, may appear to be not sufficiently
well-documented in contrast, as pointed out in [vLBD05, BvL09]. Indeed, in the K-means situation,
the specific form of the criterion (and of its expectation, the clustering risk), as well as that of the
cells defining the clusters and forming a partition of the feature space (Voronoi cells), permits to
use, in a straightforward manner, results of the theory of empirical processes in order to control
the performance of empirical clustering risk minimizers. Unfortunately, this center-based approach
does not carry over into more general situations, where the dissimilarity measure is not a square
hilbertian norm anymore, unless one loses the possibility to interpret the clustering criterion as a
function of pairwise dissimilarities between the observations (cf K-medians).
It is the goal of this paper to establish a general statistical framework for investigating clustering
performance. The present analysis is based on the observation that many statistical criteria for
measuring clustering accuracy are (symmetric) U -statistics (of degree two), functions of a matrix
of dissimilarities between pairs of data points. Such statistics have recently received a good deal of
attention in the machine-learning literature, insofar as empirical performance measures of predictive
rules in problems such as statistical ranking (when viewed as pairwise classification), see [CLV08],
or learning on graphs ([BB06]), are precisely functionals of this type, generalizing sample mean
statistics. By means of uniform deviation results for U -processes, the Empirical Risk Minimization
?

http://www.tsi.enst.fr/?clemenco/.

1

paradigm (ERM) can be extended to situations where natural?estimates of the risk are U -statistics.
In this way, we establish here a rate bound of order OP (1/ n) for the excess of clustering risk
of empirical minimizers under adequate complexity assumptions on the cells forming the partition
candidates (the bias term is neglected in the present analysis). A linearization technique, combined
with sharper tail results in the case of degenerate U -processes is also used in order to show that
tighter rate bounds can be obtained. Finally, it is shown how to use the upper bounds established
in this analysis in order to deal with the problem of automatic model selection, that of selecting the
number of clusters in particular, through complexity penalization.
The paper is structured as follows. In section 2, the notations are set out, a formal description
of cluster analysis, from the ?pairwise dissimilarity? perspective, is given and the main theoretical
concepts involved in the present analysis are briefly recalled. In section 3, an upper bound for the
performance of empirical minimization of the clustering risk is established in the context of general
dissimilarity measures. Section 4 shows how to refine the rate bound previously obtained by means
of a recent inequality for degenerate U -processes, while section 5 deals with automatic selection of
the optimal number of clusters. Technical proofs are deferred to the Appendix section.

2

Theoretical background

In this section, after a brief description of the probabilistic framework of the study, the general
formulation of the clustering objective, based on the notion of dissimilarity between pairs of observations, is recalled and the connection of the problem of investigating clustering performance with
the theory of U -statistics and U -processes is highlighted. Concepts pertaining to this theory and
involved in the subsequent analysis are next recalled.
2.1

Probabilistic setup and first notations

Here and throughout, (X1 , . . . , Xn ) denotes a sample of i.i.d. random vectors, valued in a highdimensional feature space X , typically a subset of the euclidian space Rd with d >> 1, with common probability distribution ?(dx). With no loss of generality, we assume that the feature space X
coincides with the support of the distribution ?(dx). The indicator function of any event E will be
Pd
denoted by I{E}, the usual lp norm on Rd by ||x||p = ( i=1 |xi |p )1/p when 1 ? p < ? and by
||x||? = max1?i?d |xi | in the case p = ?, with x = (x1 , . . . , xd ) ? Rd . When well-defined, the
expectation and the variance of a r.v. Z are denoted by E[Z] and Var(Z) respectively. Finally, we
denote by x+ = max(0, x) the positive part of any real number x.
2.2

Cluster analysis

The goal of clustering techniques is to partition the data (X1 , . . . , Xn ) into a given finite number
of groups, K << n say, so that the observations lying in a same group are more similar to each
other than to those in other groups. When equipped with a (borelian) measure of dissimilarity
D : X 2 ? R?+ , the clustering task can be rigorously cast as the problem of minimizing the criterion
K

cn (P) =
W

X
2
n(n ? 1)

X

D(Xi , Xj ) ? I{(Xi , Xj ) ? Ck2 },

(1)

k=1 1?i<j?n

over all possible partitions P = {Ck : 1 ? k ? K} of the feature space X . The quantity
(1) is generally called the intra-cluster similarity or the within cluster point scatter. The function
D aiming at measuring dissimilarity between pairs of observations, we suppose that it fulfills the
following properties:
? (S YMMETRY ) For all (x, x0 ) ? X 2 , D(x, x0 ) = D(x0 , x)
? (S EPARATION ) For all (x, x0 ) ? X 2 : D(x, x0 ) = 0, ? x = x0
Typical choices for the dissimilarity measure are of the form D(x, x0 ) = ?(||x?x0 ||p ), where p ? 1
and ? : R+ ? R+ is a nondecreasing function such that ?(0) = 0 and ?(t) > 0 for all t > 0. This
includes the so-termed ?standard K-means? setup, where the dissimilarity measure coincides with
2

the square euclidian norm (in this case, p = 2 and ?(t) = t2 for t ? 0). Notice that the expectation
of the r.v. (1) is equal to the following quantity:
W (P) =

K
X



E D(X, X 0 ) ? I{(X, X 0 ) ? Ck2 } ,

(2)

k=1

where (X, X 0 ) denotes a pair of independent r.v.?s drawn from ?(dx). It will be referred to as the
clustering risk of the partition P, while its statistical counterpart (1) will be called the empirical
clustering risk. Optimal partitions of the feature space X are defined as those that minimize W (P).
Remark 1 (M AXIMIZATION FORMULATION ) It is well-known that minimizing the empirical clustering risk (1) P
is equivalent
to maximizing the between-cluster scatter point, which is given by
P
1/(n(n ? 1)) ? k6=l i, j D(Xi , Xj ) ? I{(Xi , Xj ) ? Ck ? Cl }, the sum of these two statistics being
independent from the partition P = {Ck : 1 ? k ? K} considered.
Suppose we are given a (hopefully sufficiently rich) class ? of partitions of the feature space X .
cn over ?, i.e. partitions P
b? in ? such that
Here we consider minimizers of the empirical risk W
n
 
cn (P) .
bn? = min W
cn P
(3)
W
P??

The design of practical algorithms for computing (approximately) empirical clustering risk minimizers is beyond the scope of this paper (refer to [HTF09] for an overview of ?off-the-shelf? clustering
methods). Here, focus is on the performance of such empirically defined rules.
2.3

U -statistics and U -processes

The subsequent analysis crucially relies on the fact that the quantity (1) that one seeks to optimize
is a U -statistic. For clarity?s sake, we recall the definition of this class of statistics, generalizing
sample means.
Definition 1 (U - STATISTIC OF DEGREE TWO .) Let X1 , . . . , Xn be independent copies of a
random vector X drawn from a probability distribution ?(dx) on the space X and K : X 2 ? R be
a symmetric function such that K(X1 , X2 ) is square integrable. By definition, the functional
X
2
Un =
K(Xi , Xj ).
(4)
n(n ? 1)
1?i<j?n

is a (symmetric) U -statistic of degree two, with kernel K. It is said to be degenerate when
def

K(1) (x) = E[K(x, X)] = 0 with probability one for all x ? X , non degenerate otherwise.
RR
The statistic (4) is a natural (unbiased) estimate of the quantity ? =
K(x, x0 )?(dx)?(dx0 ). The
class of U -statistics is very large and include most dispersion measures, including the variance or the
Gini mean difference (with K(x, x0 ) = (x?x0 )2 and K(x, x0 ) = |x?x0 | respectively, (x, x0 ) ? R2 ),
as well as the celebrated Wilcoxon location test statistic (with K(x, x0 ) = I{x + x0 > 0} for
(x, x0 ) ? R2 in this case). Although the dependence structure induced by the summation over all
pairs of observations makes its study more difficult than that of basic sample means, this estimator
has nice properties. It is well-known folklore in mathematical statistics that it is the most efficient
estimator among all unbiased estimators of the parameter ? (i.e. that with minimum variance),
see [vdV98]. Precisely, when non degenerate, it is asymptotically normal with limiting variance
4?Var(K(1) (X)) (refer to Chapter 5 in [Ser80] for an account of asymptotic analysis of U -statistics).
As shall be seen in section 4, the reduced variance property of U -statistics is crucial, when it comes
to establish tight rate bounds.
Going back to the U -statistic of degree two (1) estimating (2), observe that its symmetric kernel is:
?(x, x0 ) ? X 2 , KP (x, x0 ) =

K
X

D(x, x0 ) ? I{(x, x0 ) ? Ck2 }.

(5)

k=1

Assuming that E[D2 (X1 , X2 ) ? I{(X1 , X2 ) ? Ck2 }] < ? for all k ? {1, . . . , K} and placing
ourselves in the situation where K ? 1 is less than X ?s cardinality, the U -statistic (1) is always non
3

degenerate, except in the (sole) case where X is made of K elements exactly and all P?s cells are
singletons. Indeed, for all x ? X , denoting by k(x) the index of {1, . . . , K} such that x ? Ck(x) ,
we have:
Z
def
(1)
KP (x) = E[KP (x, X)] =
D(x, x0 )?(dx0 ).
(6)
x0 ?Ck(x)

As ??s support coincides with X and the separation property is fulfilled by D, the quantity
above is zero iff Ck(x) = {x}. In the non degenerate case, notice finally that the asymptotic
? c
Rvariance of 0 n{Wn0(P) ? W (P)} is equal to 4 ? Var(D(X, Ck(X) ), where we set D(x, C) =
D(x, x )?(dx ) for all x ? X and any measurable set C ? X .
x0 ?X
By definition, a U -process is a collection of U -statistics, one may refer to [dlPG99] for an account
of the theory of U -processes. Echoing the role played by the theory of empirical processes in the
study of the ERM principle in binary classification, the control of the fluctuations of the U -process
n
o
cn (P) ? W (P) : P ? ?
W
indexed by a set ? of partition candidates will naturally lie at the heart of the present analysis. As
shall be seen below, this can be achieved mainly by the means of the Hoeffding representations of
U -statistics, see [Hoe48].

3

A bound for the excess of clustering risk

Here we establish an upper bound for the performance of an empirical minimizer of the clustering
risk over a class ?K of partitions of X with K ? 1 cells, K being fixed here and supposed to be
?
smaller than X ?s cardinality. We denote by WK
the clustering risk minimum over all partitions of
X with K cells. The following global suprema of empirical Rademacher averages, characterizing
the complexity of the cells forming the partition candidates, shall be involved in the subsequent rate
analysis: ?n ? 2,



bn/2c
X


1 
2 
AK,n =
sup
i D(Xi , Xi+bn/2c ) ? I{(Xi , Xi+bn/2c ) ? C } ,
(7)

C?P, P??K bn/2c  i=1

where  = (i )i?1 is a Rademacher chaos, independent from the Xi ?s, see [Kol06].
The following theorem
reveals that the clustering performance of the empirical minimizer (3) is of
?
the order OP (1/ n), when neglecting the bias term (depending on the richness of ?K solely).
Theorem 1 Consider a class ?K of partitions with K ? 1 cells and suppose that:
? there exists B < ? such that for all P in ?K , any C in P, sup(x,x0 )?C 2 D(x, x0 ) ? B,
? the expectation of the Rademacher average AK,n is of the order O(n?1/2 ).
bn? , we have with probability at least 1 ? ?:
Let ? > 0. For any empirical clustering risk minimizer P
r


2 log(1/?)
?
?
?
b
?n ? 2, W (Pn ) ? WK ? 4KE[AK,n ] + 2BK
+
inf W (P) ? WK
P??K
n


K
?
inf W (P) ? WK
,
(8)
? c(B, ?) ? ? +
P??K
n
for some constant c(B, ?) < ?, independent from n and K.
The key for proving (8) is to express the U -statistic Wn (P) in terms of sums of i.i.d. r.v.?s, as that
involved in the Rademacher average (7):
bn/2c
X
1 X
1
Wn (P) =
KP (Xi , Xi+bn/2c ),
n!
bn/2c i=1
??Sn

4

(9)

where the average is taken over Sn , the symmetric group of order n. The main point lies in the fact
that standard techniques in empirical process theory can be then used to control Wn (P) ? W (P)
uniformly over ?K under adequate hypotheses, see the proof in the Appendix for technical details.
We underline that, naturally, the complexity assumption is also a crucial ingredient of the result
stated above, and more generally to clustering consistency results, see Example 1 in [BvL09]. We
also point out that the ERM approach is by no means the sole method to obtain error bounds in the
clustering context. Just like in binary classification (see [KN02]), one may use a notion of stability
of a clustering algorithm to establish such results, see [vL09, ST09] and the references therein. Refer
to [vLBD06, vLBD08] for error bounds proved through the stability approach. Before showing how
the bound for the excess of risk stated above can be improved, a few remarks are in order.
Remark 2 (O N THE COMPLEXITY ASSUMPTION .) We point out that standard entropy metric arguments can be used in order to bound the expected value of the Rademacher average An , see [BBL05]
for instance. In particular, if the set of functions F?K = {(x, x0 ) ? X 2 7? D(x, x0 ) ? I{(x, x0 ) ?
C 2 } : C ? P,
p P ? ?K } is a VC major class with finite VC dimension V (see [Dud99]), then
E[AK,n ] ? c V /n for some universal constant c < ?. This covers a wide variety of situations,
including the case where D(x, x0 ) = ||x ? x0 ||?p and the class of sets {C : C ? P, P ? ?K } is of
finite VC dimension.
Remark 3 (K- MEANS .) In the standard K-means approach, the dissimilarity measure is
D(x, x0 ) = ||x ? x0 ||22 and partition candidates are indexed by a collection c of distinct ?centers?
c1 , . . . , cK in X : Pc = {C1 , . . . , CK } with Ck = {x ? X : ||x ? ck ||2 = min1?l?K ||x ? cl ||2 }
for 1 ? k ? K (with adequate distance-tie breaking). One may easily check that for this specific
collection of partitions ?K and this choice for the dissimilarity measure, the class F?K is a VC
major class with finite VC dimension, see section 19.1 in [DGL96] for instance. Additionally, it
should be noticed than in most practical clustering procedures, center candidates are picked in a
data-driven fashion, being taken as the averages of the observations lying in each cluster/cell. In this
respect, the M -estimation problem formulated here can be considered to a certain extent as closer to
what is actually achieved by K-means clustering techniques in practice, than the usual formulation
of the K-means problem (as an optimization problem over c = (c1 , . . . , cK ) namely).
Remark 4 (W EIGHTED CLUSTERING CRITERIA .) Notice that, in practice, the measure D involved
in (1) may depend on the data. For scaling purpose, one could assign data-dependent weights ? =
b x0 ) = Pd (xi ? x0 )2 /b
(?i )1?i?d in a coordinatewise manner, leading to D(x,
?i2 for instance,
i
i=1
2
where ?
bi denotes the sample variance related to the i-th coordinate. Although the criterion reflecting
the performance is not a U -statistic anymore, the theory we develop here can be straightforwardly
used for investigating clustering accuracy in such a case. Indeed, it is easy to control the difference
Pd
between the latter and the U -statistic (1) with D(x, x0 ) = i=1 (xi ? x0i )2 /?i2 , the ?i2 ?s denoting
the theoretical variances of ??s marginals, under adequate moment assumptions.

4

Tighter bounds for empirical clustering risk minimizers

We now show that one may refine the rate bound established above, by considering another representation of the U -statistic (1), its Hoeffding?s decomposition (see [Ser80]): for all partition P,
Wn (P) ? W (P) = 2Ln (P) + Mn (P),
Ln (P) = (1/n)

Pn

i=1

P

C?P

(10)

(1)

HC (Xi ) being a simple average of i.i.d r.v.?s with, for (x, x0 ) ? X 2 ,
(1)

HC (x, x0 ) = D(x, x0 ) ? I{(x, x0 ) ? C 2 } and HC (x) = D(x, C) ? I{x ? C} ? D(C, C),
R
where D(C, C) = x?C D(x, C)?(dx) and E[HC (x, X)] = D(x, C) ? I{x ? C}, and Mn (P) being a
P
(2)
degenerate U -statistic based on the Xi ?s with kernel given by: C?P HC (x, x0 ), where
(2)

(1)

(1)

HC (x, x0 ) = HC (x, x0 ) ? HC (x) ? HC (x0 ) ? D(C, C),
for all (x,p
x0 ) ? X 2 . The leading term in (10) is the (centered) sample mean 2Ln (P), of the
order OP ( 1/n), while the second term is of the order OP (1/n). Hence, provided this holds true
5

uniformly over P, the main contribution to the rate bound should arise from the quantity
sup |2Ln (P)| ? 2K
P??K

|(1/n)

sup
C?P, P??K

n
X

(1)

HC (Xi ) ? D(C, C)|,

i=1

which thus leads to consider the following suprema of empirical Rademacher averages:
 n


1 X

i D(Xi , C) ? I{Xi ? C} .
RK,n =
sup


C?P, P??K n 

(11)

i=1

This supremum clearly has smaller mean and variance than (7). We also introduce the quantities:



X
X


(2)
(2)
 , U =

sup
sup
i ?j HC (Xi , Xj ),


H
(X
,
X
)
Z =
sup
i
j
i
j
C


P 2
C?P, P??K ?:
C?P, P??K  i,j
?

j j i,j



X


(2)
i HC (Xi , Xj ) .
M =
sup
sup 

C?P, P??K 1?j?n 
i

Theorem 2 Consider a class ?K of partitions with K cells and suppose that:
? there exists B < ? such that sup(x,x0 )?C 2 D(x, x0 ) ? B for all P ? ?K , C ? P.
bn? , with probability at least 1 ? ?: ?n ? 2,
Let ? > 0. For any empirical clustering risk minimizer P
r


log(2/?)
?
?
?
b
W (Pn ) ? WK ? 4KE[RK,n ] + 2BK
+ K?(n, ?) +
inf W (P) ? WK , (12)
P??K
n
where we set for some universal constant C < ?, independent from n, N and K:


p
?(n, ?) = C E[Z ] + log(1/?)E[U ] + (n + E[M ])/ log(1/?) /n2 .

(13)

The result above relies on the moment inequality for degenerate U -processes proved in [CLV08].
Remark 5 (L OCALIZATION .) The same argument can be used to decompose ?n (P) ? ?(P),
cn (P) ? W ? is an estimate of the excess of risk ?(P) = W (P) ? W ? , and, by
where ?n (P) = W
K
K
means of concentration inequalities, to obtain next a sharp upper bound that involves the modulus
of continuity
average indexed by the convex hull of the set of
Pof the variance of the Rademacher
P
functions { C?P D(x, C) ? I{x ? C} ? C ? ?P ? D(x, C ? ) ? {x ? C ? } : P ? ?K }, following in
the footsteps or recent advances in binary classification, see [Kol06] and subsection 5.3 in [BBL05].
Owing to space limitations, this will be dealt with in a forthcoming article.

5

Model selection - choosing the number of clusters

A crucial issue in data segmentation is to determine the number K of cells that exhibits the most the
clustering phenomenon in the data. A variety of automatic procedures for choosing a good value for
K have been proposed in the literature, based on data splitting, resampling or sampling techniques
([PFvN89, TWH01, ST08]). Here we consider a complexity regularization method that avoids to
have recourse to such techniques and uses a data-dependent penalty term based on the analysis
carried out above.
Suppose that we have a sequence ?1 , ?2 , . . . of collections of partitions of the feature space X
such that, for all K ? 1, the elements of ?K are made of K cells and fulfill the assumptions of
Theorem 1. In order to avoid overfitting, consider the (data-driven) complexity penalty given by
27BK log K p
pen(n, K) = 3KE [AK,n ] +
+ (2B log K)/n
(14)
n
b b of the penalized empirical clustering risk, with
and the minimizer P
K,n
n
o
b = arg min W
cn (P
bK,n ) + pen(n, K) and W
cn (P
bK,n ) = min W
cn (P).
K
P??K

K?1

6

The next result shows that the partition thus selected nearly achieves the performance that would be
bK,n ]?W ? ,
obtained with the help of an oracle, revealing the value of the index K that minimizes E[P
with W ? = inf P W (P).
Theorem 3 (A N ORACLE INEQUALITY ) Suppose that, for all K ? 1, the assumptions of Theorem
1 are fulfilled. Then, we have:
!
r
h
i
?2
2
18B
?
?
?
b
E W (PK,n
2B
+
.
(15)
b ) ? W ? min {WK ? W + pen(n, K)} +
K?1
6
n
n
Of course, the penalty could be slightly refined using the results of Section 4. Due to space limitations, such an analysis is not carried out here and is left to the reader.

6

Conclusion

Whereas, until now, the theoretical analysis of clustering performance was mainly limited to the
K-means situation (but not only, cf [BvL09] for instance), this paper establishes bounds for the
success of empirical clustering risk minimization in a general ?pairwise dissimilarity? framework,
relying on the theory of U -processes. The excess of risk of empirical minimizers of the clustering
risk is proved to be of the order OP (n?1/2 ) under mild assumptions on the complexity of the cells
forming the partition candidates. It is also shown how to refine slightly this upper bound through
a linearization technique and the use of recent inequalities for degenerate U -processes. Although
the improvement displayed here can appear as not very significant at first glance, our approach
suggests that much sharper data-dependent bounds could be established this way. To the best of
our knowledge, the present analysis is the first to state results of this nature. As regards complexity
regularization, while focus is here on the choice of the number of clusters, the argument used in this
paper also paves the way for investigating more general model selection issues, including choices
related to the geometry/complexity of the cells of the partition considered.

Appendix - Technical proofs
Proof of Theorem 1
We may classically write:
?
c (P
bn ) ? WK
W



?

P??K

?


?
inf W (P) ? WK
P??K


?
|Un (C) ? u(C)| +
inf W (P) ? WK
,

cn (P) ? W (P)| +
2 sup |W
2K

sup
C?P, P??K

P??K

(16)

where Un (C) denotes the U -statistic with kernel HC (x, x0 ) = D(x, x0 ) ? I{(x, x0 ) ? C 2 } based on
the sample X1 , . . . , Xn and u(C) its expectation. Therefore, mimicking the argument of Corollary
3 in [CLV08], based on the so-termed first Hoeffding?s representation of U -statistics (see Lemma
A.1 in [CLV08]), we may straightforwardly derive the lemma below.
Proposition 1 (U NIFORM DEVIATIONS ) Suppose that Theorem 1?s assumptions are fulfilled. Let
? > 0. With probability at least 1 ? ?, we have: ?n ? 2,
r
2 log(1/?)
.
(17)
sup
|Un (C) ? u(C)| ? 2E[AK,n ] + B
n
C?P, P??K
PROOF. The argument follows in the footsteps of Corollary 3?s proof in [CLV08]. It is based on the
so-termed first Hoeffding?s representation of U -statistics (9), which provides an immediate control
of the moment generating function of the supremum supC |Un (C) ? u(C)| by that of the norm of an
empirical process, namely supC |An (C) ? u(C)|, where, for all C ? P and P ? ?K :
bn/2c
X
1
An (C) =
D(Xi , Xi+bn/2c ) ? I{(Xi , Xi+bn/2c ) ? C 2 }.
bn/2c i=1

7

Lemma 1 (see Lemma A.1 in [CLV08]) Let ? : R ? R be convex and nondecreasing. We have:






E exp ? ? sup |Un (C) ? u(C)|
? E exp ? ? sup |An (C) ? u(C)| .
(18)
C

C

Now, using standard symmetrization and randomization tricks, one obtains that: ?? > 0,



E exp ? ? sup |An (C) ? u(C)|
? E [exp (2? ? AK,n )] .

(19)

C

Observing that the value of AK,n cannot change by more than 2B/n when one of the
(i , Xi , Xi+bn/2c )0 s is changed, while the others are kept fixed, the standard bounded differences
inequality argument applies and yields:


?2 B 2
.
(20)
E [exp (2? ? AK,n )] ? exp 2? ? E[AK,n ] +
2n
Next, Markov?s inequality with ? = (t ? 2E[AK,n ])/B 2 gives: P{supC |An (C) ? u(C)| > t} ?
exp(?n(t ? 2E[AK,n ])2 /(2B 2 )). The desired result is then immediate. 
The rate bound is finally established by combining bounds (16) and (17).
Proof of Theorem 2 (Sketch of)
The theorem can be proved by using the decomposition (10), applying the argument above in order
to control supP |Ln (P)| and the lemma below to handle the degenerate part. The latter is based on a
recent moment inequality for degenerate U -processes, proved in [CLV08]. Due to space limitations,
technical details are left to the reader.
Lemma 2 (see Theorem 11 in [CLV08]) Suppose that Theorem 2?s assumptions are fulfilled. There
exists a universal constant C < ? such that for all ? ? (0, 1), we have with probability at least
1 ? ?: ?n ? 2,
sup |Mn (P)| ? K?(n, ?).
P??K

Proof of Theorem 3
The proof mimics the argument of Theorem 8.1 in [BBL05]. We thus obtain that: ?K ? 1,
h
i
h
i
b b ) ? W ? ? E W (P
bK,n ) ? W ? + pen(K, n)
E W (P
K,n
"
 #
X
c
.
+
E
sup {W (P) ? Wn (P)} ? pen(n, k)
P??k

k?1

+

Reproducing the argument of Theorem 1?s proof, one may easily show that: ?k ? 1,


c
E sup {W (P) ? Wn (P)} ? 2kE[Ak,n ].
P??k

cn (P)} ? pen(n, k) + 2?} is bounded by
Thus, for all k ? 1, the quantity P{supP??k {W (P) ? W




p
c
c
P sup {W (P) ? Wn (P)} ? E sup {W (P) ? Wn (P)} + (2B log k)/n + ?
P??k
P??k


27Bk log k
+ P 3kE [Ak,n ] ? 2kE[Ak,n ] ?
?? .
n
By virtue of the bounded differences inequality (jumps being bounded by 2B/n), the first term is
bounded by exp(?n? 2 /(2B 2 ))/k 2 , while the second term is bounded by, exp(?n?/(9Bk))/k 3 as
shown by Lemma 8.2 in [BBL05] (see the third inequality therein). Integrating over ?, one obtains:
"
 #
p
c
E
sup {W (P) ? Wn (P)} ? pen(n, k)
? (2B 2/n + 18B/n)/k 2 .
P??k

+

Summing next the bounds thus obtained over k leads to the oracle inequality stated in the theorem.
8

References
[BB06]

G. Biau and L. Bleakley. Statistical Inference on Graphs. Statistics & Decisions, 24:209?232, 2006.

[BBL05]

S. Boucheron, O. Bousquet, and G. Lugosi. Theory of Classification: A Survey of Some Recent
Advances. ESAIM: Probability and Statistics, 9:323?375, 2005.

[BD04]

S. Ben-David. A framework for statistical clustering with a constant time approximation algorithms
for k-median clustering. In Proceedings of COLT?04, Lecture Notes in Computer Science, Volume
3120/2004, 415-426, 2004.

[BDL08]

G. Biau, L. Devroye, and G. Lugosi. On the Performance of Clustering in Hilbert Space. IEEE
Trans. Inform. Theory, 54(2):781?790, 2008.

[BvL09]

S. Bubeck and U. von Luxburg. Nearest neighbor clustering: A baseline method for consistent
clustering with arbitrary objective functions. Journal of Machine Learning Research, 10:657?698,
2009.

[CFZ09]

B. Clarke, E. Fokou?e, and H.. Zhang. Principles and Theory for Data-Mining and MachineLearning. Springer, 2009.

[CLV08]

S. Cl?emenc?on, G. Lugosi, and N. Vayatis. Ranking and empirical risk minimization of U-statistics.
The Annals of Statistics, 36(2):844?874, 2008.

[DGL96]

L. Devroye, L. Gy?orfi, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer,
1996.

[dlPG99]

V. de la Pena and E. Gin?e. Decoupling: from Dependence to Independence. Springer, 1999.

[Dud99]

R.M. Dudley. Uniform Central Limit Theorems. Cambridge University Press, 1999.

[Har78]

J.A. Hartigan. Asymptotic distributions for clustering criteria. The Annals of Statistics, 6:117?131,
1978.

[Hoe48]

W. Hoeffding. A class of statistics with asymptotically normal distribution. Ann. Math. Stat.,
19:293?325, 1948.

[HTF09]

T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning (2nd ed.), pages
520?528. Springer, 2009.

[KN02]

S. Kutin and P. Niyogi. Almost-everywhere algorithmic stability and generalization error. In Proceedings of the of the 18th Conference in Uncertainty in Artificial Intelligence, 2002.

[Kol06]

V. Koltchinskii. Local Rademacher complexities and oracle inequalities in risk minimization (with
discussion). The Annals of Statistics, 34:2593?2706, 2006.

[PFvN89] R. Peck, L. Fisher, and J. van Ness. Bootstrap confidence intervals for the number of clusters in
cluster analysis. J. Am. Stat. Assoc., 84:184?191, 1989.
[Pol81]

D. Pollard. Strong consistency of k-means clustering. The Annals of Statistics, 9:135?140, 1981.

[Pol82]

D. Pollard. A central limit theorem for k-means clustering. The Annals of Probability, 10:919?926,
1982.

[Ser80]

R.J. Serfling. Approximation theorems of mathematical statistics. Wiley, 1980.

[ST08]

O. Shamir and N. Tishby. Model selection and stability in k-means clustering. In in Proceedings of
the 21rst Annual Conference on Learning Theory, 2008.

[ST09]

O. Shamir and N. Tishby. On the reliability of clustering stability in the large sample regime. In
Advances in Neural Information Processing Systems 21, 2009.

[TWH01] R. Tibshirani, G. Walther, and T. Hastie. Estimating the number of clusters in a data set via the gap
statistic. J. Royal Stat. Soc., 63(2):411?423, 2001.
[vdV98]

A. van der Vaart. Asymptotic Statistics. Cambridge University Press, 1998.

[vL09]

U. von Luxburg. Clustering stability: An overview. Foundations and Trends in Machine Learning,
2(3):235?274, 2009.

[vLBD05] U. von Luxburg and S. Ben-David. Towards a statistical theory of clustering. In Pascal workshop
on Statistics and Optimization of Clustering, 2005.
[vLBD06] U. von Luxburg and S. Ben-David. A sober look at clustering stability. In Proceedings of the 19th
Conference on Learning Theory, 2006.
[vLBD08] U. von Luxburg and S. Ben-David. Relating clustering stability to properties of cluster boundaries.
In Proceedings of the 21th Conference on Learning Theory, 2008.
[WT10]

D. M. Witten and R. Tibshirani. A framework for feature selection in clustering. J. Amer. Stat.
Assoc., 105(490):713?726, 2010.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 2148-coulomb-classifiers-generalizing-support-vector-machines-via-an-analogy-to-electrostatic-systems.pdf

Coulomb Classifiers: Generalizing
Support Vector Machines via an Analogy
to Electrostatic Systems

Sepp Hochreiter? , Michael C. Mozer? , and Klaus Obermayer?
?
Department of Electrical Engineering and Computer Science
Technische Universit?at Berlin, 10587 Berlin, Germany
?
Department of Computer Science
University of Colorado, Boulder, CO 80309?0430, USA
{hochreit,oby}@cs.tu-berlin.de, mozer@cs.colorado.edu

Abstract
We introduce a family of classifiers based on a physical analogy to
an electrostatic system of charged conductors. The family, called
Coulomb classifiers, includes the two best-known support-vector
machines (SVMs), the ??SVM and the C?SVM. In the electrostatics analogy, a training example corresponds to a charged conductor
at a given location in space, the classification function corresponds
to the electrostatic potential function, and the training objective
function corresponds to the Coulomb energy. The electrostatic
framework provides not only a novel interpretation of existing algorithms and their interrelationships, but it suggests a variety of new
methods for SVMs including kernels that bridge the gap between
polynomial and radial-basis functions, objective functions that do
not require positive-definite kernels, regularization techniques that
allow for the construction of an optimal classifier in Minkowski
space. Based on the framework, we propose novel SVMs and perform simulation studies to show that they are comparable or superior to standard SVMs. The experiments include classification
tasks on data which are represented in terms of their pairwise proximities, where a Coulomb Classifier outperformed standard SVMs.

1

Introduction

Recently, Support Vector Machines (SVMs) [2, 11, 9] have attracted much interest in
the machine-learning community and are considered state of the art for classification
and regression problems. One appealing property of SVMs is that they are based
on a convex optimization problem, which means that a single minimum exists and
can be computed efficiently. In this paper, we present a new derivation of SVMs
by analogy to an electrostatic system of charged conductors. The electrostatic
framework not only provides a physical interpretation of SVMs, but it also gives
insight into some of the seemingly arbitrary aspects of SVMs (e.g., the diagonal of
the quadratic form), and it allows us to derive novel SVM approaches. Although we

are the first to make the analogy between SVMs and electrostatic systems, previous
researchers have used electrostatic nonlinearities in pattern recognition [1] and a
mechanical interpretation of SVMs was introduced in [9].
In this paper, we focus on the classification of an input vector x ? X into one of
two categories, labeled ?+? and ???. We assume a supervised learning paradigm
in which N training examples are available, each example i consisting of an input
xi and a label yi ? {?1, +1}. We will introduce three electrostatic models that
are directly analogous to existing machine-learning (ML) classifiers, each of which
builds on and generalizes the previous. For each model, we describe the physical
system upon which it is based and show its correspondence to an ML classifier.
1.1 Electrostatic model 1: Uncoupled point charges
Consider an electrostatic system of point charges populating a space X 0 homologous
to X . Each point charge corresponds to a particular training example; point charge
i is fixed at location?xi in X 0 , and
? has a charge
? of sign yi . ?We define two sets of
fixed charges: S + = xi | yi = +1 and S ? = xi | yi = ?1 . The charge of point
i is Qi ? yi ?i , where ?i ? 0 is the amount of charge, to be discussed below.
We briefly review some elementary physics. If a unit positive charge is at x in
X 0 , it will be attracted to all charges in S ? and repelled by all charges in S + . To
? the attractive and repelling forces
move the charge from x to some other location x,
must be overcome at every point along the trajectory; the path integral of the force
along the trajectory is called the work and does not depend on the trajectory. The
potential at x is the work that must be done to move a unit positive charge from a
reference point (usually infinity) to x.
? j ?
PN
The potential at x is ? (x) =
j=1 Qj G x , x , where G is a function of the
distance. In electrostatic systems with point charges, G (a, b) = 1/ ka ? bk 2 . From
this definition, one can see that the potential at x is negative (positive) if x is in
a neighborhood of many negative (positive) charges. Thus, the potential indicates
the sign and amount of charge in the local neighborhood.
Turning back to the ML classifier, one might propose a classification rule for some
input x that assigns the label ?+? if ?(x) > 0 or ??? otherwise. Abstracting
from the electrostatic system, if ?i = 1 and G is a function that decreases sufficiently steeply with distance, we obtain a nearest-neighbor classifier. This potential
classifier can be also interpreted as Parzen windows classifier [9].
1.2 Electrostatic model 2: Coupled point charges
Consider now an electrostatic model that extends the previous model in two respects. First, the point charges are replaced by conductors, e.g., metal spheres.
Each conductor i has a self?potential coefficient, denoted Pii , which is a measure
of how much charge it can easily hold; for a metal sphere, Pii is related to sphere?s
diameter. Second, the conductors in S + are coupled, as are the conductors in S ? .
?Coupling? means that charge is free to flow between the conductors. Technically,
S + and S ? can each be viewed as a single conductor.
In this model, we initially place the same charge ?/N on each conductor, and allow
charges within S + and S ? to flow freely (we assume no resistance in the coupling
and no polarization of the conductors). After the charges redistribute, charge will
tend to end up on the periphery of a homogeneous neighborhood of conductors,
because like charges repel. Charge will also tend to end up along the S + ?S ?
boundary because opposite charges attract. Figure 1 depicts the redistribution of
charges, where the shading is proportional to the magnitude ?i . An ML classifier
can be built based on this model, once again using ?(x) > 0 as the decision rule

for classifying an input x. In this model, however, the ?i are not uniform; the
conductors with large ?i will have the greatest influence on the potential function.
Consequently, one can think of ?i as the weight or importance of example i. As we
will show shortly, the examples with ?i > 0 are exactly support vectors of an SVM.

-

+
+

+

+

+

+
+

+
+

+

+
+

-

+ +

+

+

+

+

+

+

+

+

-

-

-

- - - - - - -

Figure 1: Coupled conductor system following charge redistribution. Shading reflects the charge magnitude, and the contour indicates a zero potential.
The redistribution of charges in the electrostatic system is achieved via minimization
of the Coulomb energy. Imagine placing the same total charge magnitude, m, on
S + and S ? by dividing it uniformly among the conductors, i.e., ?i = m/ |S yi |. The
free charge flow in S + and S ? yields a distribution of charges, the ?i , such that
Coulomb energy is minimized.
To introduce Coulomb energy, we begin with some preliminaries. The potential at
conductor i, ?(xi ), which we will denote more compactly as ?i , can be described
PN
in terms of the coefficients of potential Pij [10]: ?i = j=1 Pij Qj , where Pij is the
potential induced on conductor i by charge Qj on conductor j; Pii ? Pij ? 0 and
Pij = Pji . If each conductor i is a metal sphere centered at xi and has radius ri
(radii are enforced to be small enough so that the spheres do not touch? each other),
?
the system can be modeled by a point charge Qi at xi , and Pij = G xi , xj as in
the previous section [10]. The self-potential, Pii , is defined as a function of ri . The
Coulomb energy is defined in terms of the potential on the conductors, ?i :
E =

N
N
1 X
1 T
1X
?i Q i =
Q P Q =
Pij yi yj ?i ?j .
2 i=1
2
2 i,j=1

When the energy minimum is reached, the potential ?i will be the same for all
connected i ? S + (i ? S ? ); we denote this potential ?S + (?S ? ).
Two additional constraints on the system of coupled conductors are necessary in
order to interpret the system in terms of existing machine learning models. First,
the positive and negative potentials must be balanced, i.e., ?S + = ??S ? . This
constraint is achieved by setting the reference point of the potentials ?through
? b,
PN
i
+
?
b = ?0.5 (?S + ?S ), into the potential function: ? (x) = i=1 Qi G x , x + b.
Second, the conductors must be prevented from reversing the sign of their charge,
i.e., ?i ? 0, and from holding more than a quantity C of charge, i.e., ?i ? C. These

requirements can be satisfied in the electrostatic model by disconnecting a conductor
i from the charge flow in S + or S ? when ?i reaches a bound, which will subsequently
freeze its charge. Mathematically, the requirements are satisfied by treating energy
minimization as a constrained optimization problem with 0 ? ?i ? C.
The electrostatic system corresponds to a ??support vector machine (??SVM)
[9]
P
?
with
kernel
G
if
we
set
C
=
1/N
.
The
electrostatic
system
assures
that
+
i =
i?S
P
?
=
0.5
?.
The
identity
holds
because
the
Coulomb
energy
is
exactly
the
?
i
i?S
??SVM quadratic objective function, and the thresholded electrostatic potential
evaluated at a location is exactly the SVM decision rule. The minimization of
potentials differences in the systems S + and S ? corresponds to the minimization
of slack variables in the SVM (slack variables express missing potential due to the
upper bound on ?i ). Mercer?s condition [6], the essence of the nonlinear SVM
theory,Ris equivalent to the fact that continuous electrostatic energy is positive, i.e.,
E =
G (x, z) h (x) h (z) dx dz ? 0. The self-potentials of the electrostatic
system provide an interpretation to the diagonal elements in the quadratic objective
function of the SVM. This interpretation of the diagonal elements allows us to
introduce novel kernels and novel SVM methods, as we discuss later.
1.3 Electrostatic model 3: Coupled point charges with battery
In electrostatic model 2, we control the magnitude of charge applied to S + and S ? .
Although we apply the same charge magnitude to each, we do not have to control
the resulting potentials ?S + and ?S ? , which may be imbalanced. We compensate
for this imbalance via the potential offset b. In electrostatic model 3, we control the
potentials ?S + and ?S + directly by adding a battery to the system. We connect
S + to the positive pole of the battery with potential +1 and S ? to the negative
pole with potential ?1. The battery ensures that ?S + = +1 and ?S ? = ?1 because
charges flow from the battery into or out of the system until the systems take on
the potential of the battery poles. The battery can then be removed. The potential
?i = yi is forced by the battery on conductor i. The total Coulomb energy is the
energy from
P model 2 minus
P the work done by the battery. The work done by the
battery is i?N yi Qi = i?N ?i . The Coulomb energy is
N
N
N
X
X
1 T
1 X
?i =
?i .
Q P Q ?
Pij yi yj ?i ?j ?
2
2 i,j=1
i=1
i=1

This physical system corresponds
to a C?support vector machine (C?SVM) [2, 11].
P
The C?SVM requires that i yi ?i = 0; although this constraint may not be fulfilled
in the system described here, it can be enforced by a slightly different system [4]. A
more straightforward relation to the C?SVM is given in [9] where the authors show
that every ??SVM has the same class boundaries as a C?SVM with appropriate C.

2

Comparison of existing and novel models

2.1 Novel Kernels
The electrostatic perspective makes it easy to understand why SVM algorithms can
break down in high-dimensional spaces: Kernels with rapid fall-off induce small potentials and consequently, almost every conductor retains charge. Because a charged
conductor corresponds to a support vector, the number of support vectors is large,
which leads to two disadvantages: (1) the classification procedure is slow, and (2) the
expected generalization error increases with the number of support vectors [11]. We
therefore should use kernels that do not drop off exponentially. The self?potential

permits the use of kernels that would otherwise be invalid, such as a generalization
?
??l
?
?
?
?
of the electric field: G xi , xj := ?xi ? xj ?2 and G xi , xi := ri?l = Pii , where
ri the radius of the ith sphere. The ri s are increased
to?their maximal values, i.e.
?
until they hit other conductors (ri = 0.5 minj ?xi ? xj ?2 ). These kernels, called
?Coulomb kernels?, are invariant to scaling of the input space in the sense that
scaling does not change the minimum of the objective function. Consequently, such
kernels are appropriate for input data with varying local densities. Figure 2 depicts
a classification task with input regions of varying density. The optimal class boundary is smooth in the low data density regions and has high curvature in regions,
where the data density is high. The classification boundary was constructed using
??
??l/2
?2
?
?
, which is an
a C-SVM with a Plummer kernel G xi , xj := ?xi ? xj ? + ?2
2

approximation to our novel Coulomb kernel but lacks its weak singularities.

Figure 2: Two class data with a dense region and trained with a SVM using the
new kernel. Gray-scales indicate the weights ? support vectors are dark. Boundary
curves are given for the novel kernel (solid), best RBF-kernel SVM which overfits
at high density regions where the resulting boundary goes through a dark circle
(dashed), and optimal boundary (dotted).

2.2 Novel SVM models
Our electrostatic framework can be used to derive novel SVM approaches [4], two
representative examples of which we illustrate here.
2.2.1 ??Support Vector Machine (??SVM):
We can exploit the physical interpretation of Pii as conductor i?s self?potential. The
Pii ?s determine the smoothness of the charge distribution at the energy minimum.
We can introduce a parameter ? to rescale the self potential ? Piinew = ? Piiold .
? controls the complexity of the corresponding SVM. With this modification, and
with C = ?, electrostatic model 3 becomes what we call the ??SVM.
2.2.2 p?Support Vector Machine (p?SVM):
At the Coulomb energy minimum the electrostatic potentials equalize: ? i ? yi =
0, ?i (y is the label vector). This motivates the introduction of potential difference,
2
1
1 T T
1 T
T T
2 kP Q + yk2 = 2 Q P P Q + Q P y + 2 y y as the objective. We obtain
1 T
min
? Y P T P Y ? ? 1T Y P Y ?
?
2
subject to
1T P Y ? = 0 , |?i | ? C,
where 1 is the vector of ones and Y := diag(y). We call this variant of the
optimization problem the potential-SVM (p-SVM). Note that the p-SVM is similar
to the ?empirical kernel map? [9]. However P appears in the objective?s linear term
and the constraints. We classify in a space where P is a dot product matrix. The
constraint 1T P Y ? = 0 ensures that the average potential for each class is equal.
By construction, P T P is positive definite; consequently, this formulation does not
require positive definite kernels. This characteristic is useful for problems in which
the properties of the objects to be classified are described by their pairwise proximities. That is, suppose that instead of representing each input object by an explicit
feature vector, the objects are represented by a matrix which contains a real number indicating the similarity of each object to each other object. We can interpret
the entries of the matrix as being produced by an unknown kernel operating on
unknown feature vectors. In such a matrix, however, positive definiteness cannot
be assured, and the optimal hyperplane must be constructed in Minkowski space.

3

Experiments

UCI Benchmark Repository. For the representative models we have introduced, we perform simulations and make comparisons to standard SVM variants.
All datasets (except ?banana? from [7]) are from the UCI Benchmark Repository
and were preprocessed in [7]. We did 100-fold validation on each data set, restricting
the training set to 200 examples, and using the remainder of examples for testing.
We compared two standard architectures, the C?SVM and the ??SVM, to our novel
architectures: to the ??SVM, to the p?SVM, and to a combination of them, the
??p?SVM. The ??p?SVM is a p?SVM regularized like a ??SVM. We explored the
use of radial basis function (RBF), polynomial (POL), and Plummer (PLU) kernels.
Hyperparameters were determined by 5?fold cross validation on the first 5 training
sets. The search for hyperparameter was not as intensive as in [7].
Table 1 shows the results of our comparisons on the UCI Benchmarks. Our two
novel architectures, the ??SVM and the p?SVM, performed well against the two
existing architectures (note that the differences between the C? and the ??SVM
are due to model selection). As anticipated, the p?SVM requires far fewer support vectors. Additionally, the Plummer kernel appears to be more robust against
hyperparameter and SVM choices than the RBF or polynomial kernels.

C
RBF
POL
PLU

6.4
22.8
6.1

RBF
POL
PLU

33.6
36.0
33.4

RBF
POL
PLU

28.7
33.7
28.8

?

?
p
thyroid
9.4
7.7
5.4
12.6
7.0
13.3
6.2
6.1
5.7
breast?cancer
31.6
33.8 32.4
25.7 29.6 27.1
33.1
33.4 30.6
german
29.3
29.0 27.8
29.6 26.2 31.8
28.5
33.3 27.1

?-p

C

?

8.6
6.9
6.1

21.4
20.4
16.3

19.1
20.4
16.3

33.7
29.1
33.4

13.2
35.3
15.7

36.7
35.0
15.7

?
heart
17.9
19.3
16.3
banana
13.2
11.5
15.7

p

?-p

22.4
23.0
17.4

17.8
19.3
16.3

11.6
22.4
21.9

13.4
11.5
15.7

28.8
26.2
33.3

Table 1: Mean % misclassification on 5 UCI Repository data sets. Each cell in
the table is obtained via 100 replications splitting the data into training and test
sets. The comparison is among five SVMs (the table columns) using three kernel
functions (the table rows). Cells in bold face are the best result for a given data set
and italicized the second and third best.

Pairwise Proximity Data. We applied our p?SVM and the generalized SVM
(G?SVM) [3] to two pairwise-proximity data sets. The first data set, the ?cat cortex? data, is a matrix of connection strengths between 65 cat cortical areas and was
provided by [8], where the available anatomical literature was used to determine
proximity values between cortical areas. These areas belong to four different coarse
brain regions: auditory (A), visual (V), somatosensory (SS), and frontolimbic (FL).
The goal was to classify a given cortical area as belonging to a given region or
not. The second data set, the ?protein? data, is the evolutionary distance of 226 sequences of amino acids of proteins obtained by a structural comparison [5] (provided
by M. Vingron). Most of the proteins are from four classes of globins: hemoglobin-ff
(H-ff), hemoglobin-fi (H-fi), myoglobin (M), and heterogenous globins (GH). The
goal was to classify a protein as belonging to a given globin class or not. As Table 2
shows, our novel architecture, the p?SVM, beats out an existing architecture in the
literature, the G?SVM, on 5 of 8 classification tasks, and ties the G?SVM on 2 of
8; it loses out on only 1 of 8.

Size
G-SVM
G-SVM
G-SVM
p-SVM
p-SVM
p-SVM

Reg.
?
0.05
0.1
0.2
0.6
0.7
0.8

cat
V
18
4.6
4.6
6.1
3.1
3.1
3.1

cortex
A
SS
10
18
3.1 3.1
3.1 6.1
1.5 3.1
1.5 6.1
3.1 4.6
3.1 4.6

FL
19
1.5
1.5
3.1
3.1
1.5
1.5

Reg.
?
0.05
0.1
0.2
300
400
500

protein data
H-? H-?
M
72
72
39
1.3
4.0
0.5
1.8
4.5
0.5
2.2
8.9
0.5
0.4
3.5 0.0
0.4 3.1 0.0
0.4
3.5 0.0

GH
30
0.5
0.9
0.9
0.4
0.9
1.3

Table 2: Mean % misclassifications for the cat-cortex and protein data sets using
the p?SVM and the G?SVM and a range of regularization parameters (indicated in
the column labeled ?Reg.?). The result obtained for the cat-cortex data is via leaveone-out cross validation, and for the protein data is via ten-fold cross validation.
The best result for a given classification problem is printed in bold face.

4

Conclusion

The electrostatic framework and its analogy to SVMs has led to several important
ideas. First, it suggests SVM methods for kernels that are not positive definite.
Second, it suggests novel approaches and kernels that perform as well as standard
methods (will undoubtably perform better on some problems). Third, we demonstrated a new classification technique working in Minkowski space which can be used
for data in form of pairwise proximities. The novel approach treats the proximity
matrix as an SVM Gram matrix which lead to excellent experimental results.
We argued that the electrostatic framework not only characterizes a family of
support-vector machines, but it also characterizes other techniques such as nearest
neighbor classification. Perhaps the most important contribution of the electrostatic framework is that, by interrelating and encompassing a variety of methods,
it lays out a broad space of possible algorithms. At present, the space is sparsely
populated and has barely been explored. But by making the dimensions of this
space explicit, the electrostatic framework allows one to easily explore the space
and discover novel algorithms. In the history of machine learning, such general
frameworks have led to important advances in the field.
Acknowledgments
We thank G. Hinton and J. Schmidhuber for stimulating conversations leading to
this research and an anonymous reviewer who provided helpful advice on the paper.

References
[1] M. A. Aizerman, E. M. Braverman, and L. I. Rozono?er. Theoretical foundations
of the potential function method in pattern recognition learning. Automation
and Remote Control, 25:821?837, 1964.
[2] C. J. C. Burges. A tutorial on support vector machines for pattern recognition.
Data Mining and Knowledge Discovery, 2(2):1?47, 1998.
[3] T. Graepel, R. Herbrich, B. Sch?olkopf, A. J. Smola, P. L. Bartlett, K.-R.
M?
uller, K. Obermayer, and R. C. Williamson. Classification on proximity data
with LP?machines. In Proceedings of the Ninth International Conference on
Artificial Neural Networks, pages 304?309, 1999.
[4] S. Hochreiter and M. C. Mozer. Coulomb classifiers: Reinterpreting SVMs as
electrostatic systems. Technical Report CU-CS-921-01, Department of Computer Science, University of Colorado, Boulder, 2001.
[5] T. Hofmann and J. Buhmann. Pairwise data clustering by deterministic annealing. IEEE Trans. Pattern Anal. and Mach. Intelligence, 19(1):1?14, 1997.
[6] J. Mercer. Functions of positive and negative type and their connection with the
theory of integral equations. Philosophical Transactions of the Royal Society
of London A, 209:415?446, 1909.
[7] G. R?
atsch, T. Onoda, and K.-R. M?
uller. Soft margins for AdaBoost. Technical
Report NC-TR-1998-021, Dep. of Comp. Science, Univ. of London, 1998.
[8] J. W. Scannell, C. Blakemore, and M. P. Young. Analysis of connectivity in
the cat cerebral cortex. The Journal of Neuroscience, 15(2):1463?1483, 1995.
[9] B. Sch?
olkopf and A. J. Smola. Learning with Kernels ? Support Vector Machines, Regularization, Optimization, and Beyond. MIT Press, 2002.
[10] M. Schwartz. Principles of Electrodynamics. Dover Publications, NY, 1987.
Republication of McGraw-Hill Book 1972.
[11] V. Vapnik. The nature of statistical learning theory. Springer, NY, 1995.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 4565-near-optimal-differentially-private-principal-components.pdf

Near-optimal Differentially Private Principal
Components
Kamalika Chaudhuri
UC San Diego
kchaudhuri@ucsd.edu

Anand D. Sarwate
TTI-Chicago
asarwate@ttic.edu

Kaushik Sinha
UC San Diego
ksinha@cs.ucsd.edu

Abstract
Principal components analysis (PCA) is a standard tool for identifying good lowdimensional approximations to data sets in high dimension. Many current data
sets of interest contain private or sensitive information about individuals. Algorithms which operate on such data should be sensitive to the privacy risks in publishing their outputs. Differential privacy is a framework for developing tradeoffs
between privacy and the utility of these outputs. In this paper we investigate the
theory and empirical performance of differentially private approximations to PCA
and propose a new method which explicitly optimizes the utility of the output.
We demonstrate that on real data, there is a large performance gap between the
existing method and our method. We show that the sample complexity for the two
procedures differs in the scaling with the data dimension, and that our method is
nearly optimal in terms of this scaling.

1

Introduction

Dimensionality reduction is a fundamental tool for understanding complex data sets that arise in
contemporary machine learning and data mining applications. Even though a single data point
can be represented by hundreds or even thousands of features, the phenomena of interest are often
intrinsically low-dimensional. By reducing the ?extrinsic? dimension of the data to its ?intrinsic? dimension, analysts can discover important structural relationships between features, more efficiently
use the transformed data for learning tasks such as classification or regression, and greatly reduce
the space required to store the data. One of the oldest and most classical methods for dimensionality
reduction is principal components analysis (PCA), which computes a low-rank approximation to the
second moment matrix of a set of points in Rd . The rank k of the approximation is chosen to be the
intrinsic dimension of the data. We view this procedure as specifying a k-dimensional subspace of
Rd .
Much of today?s machine-learning is performed on the vast amounts of personal information collected by private companies and government agencies about individuals, such as customers, users,
and subjects. These datasets contain sensitive information about individuals and typically involve
a large number of features. It is therefore important to design machine-learning algorithms which
discover important structural relationships in the data while taking into account its sensitive nature.
We study approximations to PCA which guarantee differential privacy, a cryptographically motivated definition of privacy [9] that has gained significant attention over the past few years in the
machine-learning and data-mining communities [19, 21, 20, 10, 23]. Differential privacy measures
privacy risk by a parameter ? that bounds the log-likelihood ratio of output of a (private) algorithm
under two databases differing in a single individual.
There are many general tools for providing differential privacy. The sensitivity method [9] computes
the desired algorithm (PCA) on the data and then adds noise proportional to the maximum change
than can be induced by changing a single point in the data set. The PCA algorithm is very sensitive
1

in this sense because the top eigenvector can change by 90 by changing one point in the data set.
Relaxations such as smoothed sensitivity [24] are difficult to compute in this setting as well. The
SULQ method of Blum et al. [2] adds noise to the second moment matrix and then runs PCA on
the noisy matrix. As our experiments show, the amount of noise required is often quite severe and
SULQ seems impractical for data sets of moderate size.
The general SULQ method does not take into account the quality of approximation to the nonprivate PCA output. We address this by proposing a new method, PPCA, that is an instance of the
exponential mechanism of McSherry and Talwar [22]. For any k < d, this differentially private
method outputs a k-dimensional subspace; the output is biased towards subspaces which are close
to the output of PCA. In our case, the method corresponds to sampling from the matrix Bingham
distribution. We implement this method using a Markov Chain Monte Carlo (MCMC) procedure
due to Hoff [15] and show that it achieves significantly better empirical performance.
In order to understand the performance gap, we prove sample complexity bounds in case of k = 1 for
SULQ and PPCA, as well as a general lower bound on the sample complexity for any differentially
p
private algorithm. We show that (up to log factors) the sample complexity scales as ?(d3/2 d)
for SULQ and as O(d) for PPCA. Furthermore, any differentially private algorithm requires ?(d)
samples, showing that PPCA is nearly optimal in terms of sample complexity as a function of data
dimension. These theoretical results suggest that our experiments exhibit the limit of how well ?differentially private algorithms can perform, and our experiments show that this gap should persist
for general k.
There are several interesting open questions suggested by this work. One set of issues is computational. Differentially privacy is a mathematical definition, but algorithms must be implemented
using finite precision machines. Privacy and computation interact in many places, including pseudorandomness, numerical stability, optimization, and in the MCMC procedure we use to implement
PPCA; investigating the impact of approximate sampling is an avenue for future work. A second set
of issues is theoretical ? while the privacy guarantees of PPCA hold for all k, our theoretical analysis of sample complexity applies only to k = 1 in which the distance and angles between vectors
are related. An interesting direction is to develop theoretical bounds for general k; challenges here
are providing the right notion of approximation of PCA, and extending the theory using packings of
Grassman or Stiefel manifolds.

2

Preliminaries

The data given to our algorithm is a set of n vectors D = {x1 , x2 , . . . , xn } where each xi corresponds to the private value of one individual, xi 2 Rd , and kxi k ? 1 for all i. Let X = [x1 , . . . , xn ]
be the matrix whose columns are the data vectors {xi }. Let A = n1 XX T denote the d ? d second
moment matrix of the data. The matrix A is positive semidefinite, and has Frobenius norm at most
1.
The problem of dimensionality reduction is to find a ?good? low-rank approximation to A. A popular
? F , where k is much
solution is to compute a rank-k matrix A? which minimizes the norm kA Ak
lower than the data dimension d. The Schmidt approximation theorem [25] shows that the minimizer
is given by the singular value decomposition, also known as the PCA algorithm in some areas of
computer science.
Definition 1. Suppose A is a positive semidefinite matrix whose first k eigenvalues are distinct. Let
the eigenvalues of A be 1 (A)
???
0 and let ? be a diagonal matrix with
2 (A)
d (A)
?ii = i (A). The matrix A decomposes as
A = V ?V T ,

(1)

where V is an orthonormal matrix of eigenvectors. The top-k subspace of A is the matrix
Vk (A) = [v1 v2 ? ? ? vk ] ,

(2)

where vi is the i-th column of V in (1).

Given the top-k subspace and the eigenvalue matrix ?, we can form an approximation A(k) =
Vk (A)?k Vk (A)T to A, where ?k contains the k largest eigenvalues in ?. In the special case k = 1
2

we have A(1) = 1 (A)v1 v1T , where v1 is the eigenvector corresponding to 1 (A). We refer to v1 as
the top eigenvector of the data. For a d ? k matrix V? with orthonormal columns, the quality of V? in
approximating A can be measured by
?
?
qF (V? ) = tr V? T AV? .
(3)

The V? which maximizes q(V? ) has columns equal to {vi : i 2 [k]}, corresponding to the top k
eigenvectors of A.

Our theoretical results apply to the special case k = 1. For these results, we measure the inner
product between the output vector v?1 and the true top eigenvector v1 :
(4)

qA (?
v1 ) = |h?
v1 , v1 i| .

This is related to (3). If we write v?1 in the basis spanned by {vi }, then
qF (?
v1 ) =

v1 )
1 qA (?

2

+

d
X

v 1 , vi i
i h?

i=2

2

.

Our proof techniques use the geometric properties of qA (?).
Definition 2. A randomized algorithm A(?) is an (?, ?)-close approximation to the top eigenvector
if for all data sets D of n points,
P (qA (A(D))

?)

1

?,

(5)

where the probability is taken over A(?).
We study approximations to A? that preserve the privacy of the underlying data. The notion of
privacy that we use is differential privacy, which quantifies the privacy guaranteed by a randomized
algorithm P applied to a data set D.
Definition 3. An algorithm A(B) taking values in a set T provides ?-differential privacy if
sup sup

S D,D 0

? (S | B = D)
? e? ,
? (S | B = D0 )

(6)

where the first supremum is over all measurable S ? T , the second is over all data sets D and
D0 differing in a single entry, and ?(?|B) is the conditional distribution (measure) on T induced by
the output A(B) given a data set B. The ratio is interpreted to be 1 whenever the numerator and
denominator are both 0.
Definition 4. An algorithm A(B) taking values in a set T provides (?, )-differential privacy if
P (A(D) 2 S) ? e? P (A(D0 ) 2 S) + ,

(7)

for all all measurable S ? T and all data sets D and D differing in a single entry.
0

Here ? and are privacy parameters, where low ? and ensure more privacy. For more details about
these definitions, see [9, 26, 8]. The second privacy guarantee is weaker; the parameter bounds the
probability of failure, and is typically chosen to be quite small.
In this paper we are interested in proving results on the sample complexity of differentially private algorithms that approximate PCA. That is, for a given ? and ?, how large must the number of
individuals n in the data set be such that it is ?-differentially private and also a (?, ?)-close approximation to PCA? It is well known that as the number of individuals n grows, it is easier to guarantee
the same level of privacy with relatively less noise or perturbation, and therefore the utility of the
approximation also improves. Our results characterize how privacy and utility scale with n and the
tradeoff between them for fixed n.
Related Work Differential privacy was proposed by Dwork et al. [9], and has spawned an extensive literature of general methods and applications [1, 21, 27, 6, 24, 3, 22, 10] Differential privacy
has been shown to have strong semantic guarantees [9, 17] and is resistant to many attacks [12] that
succeed against some other definitions of privacy. There are several standard approaches for designing differentially-private data-mining algorithms, including input perturbation [2], output perturbation [9], the exponential mechanism [22], and objective perturbation [6]. To our knowledge, other
3

than SULQ method [2], which provides a general differentially-private input perturbation algorithm, this is the first work on differentially-private PCA. Independently, [14] consider the problem
of differentially-private low-rank matrix reconstruction for applications to sparse matrices; provided
certain coherence conditions hold, they provide an algorithm for constructing a rank 2k approximation B to a matrix A such that kA BkF is O(kA Ak k) plus some additional terms which
depend on d, k and n; here Ak is the best rank k approximation to A. Because of their additional
assumptions, their bounds are generally incomparable to ours, and our bounds are superior for dense
matrices.
The data-mining community has also considered many different models for privacy-preserving computation ? see Fung et al. for a survey with more references [11]. Many of the models used have
been shown to be susceptible to composition attacks, when the adversary has some amount of prior
knowledge [12]. An alternative line of privacy-preserving data-mining work [28] is in the Secure
Multiparty Computation setting; one work [13] studies privacy-preserving singular value decomposition in this model. Finally, dimension reduction through random projection has been considered
as a technique for sanitizing data prior to publication [18]; our work differs from this line of work
in that we offer differential privacy guarantees, and we only release the PCA subspace, not actual
data. Independently, Kapralov and Talwar [16] have proposed a dynamic programming algorithm
for differentially private low rank matrix approximation which involves sampling from a distribution
induced by the exponential mechanism. The running time of their algorithm is O(d6 ), where d is
the data dimension.

3

Algorithms and results

In this section we describe differentially private techniques for approximating (2). The first is a modified version of the SULQ method [2]. Our new algorithm for differentially-private PCA, PPCA,
is an instantiation of the exponential mechanism due to McSherry and Talwar [22]. Both procedures provide differentially private approximations to the top-k subspace: SULQ provides (?, )differential privacy and PPCA provides ?-differential privacy.
Input perturbation. The only differentially-private approximation to PCA prior to this work is
the SULQ method [2]. The SULQ method perturbs each entry of the empirical second moment matrix A to ensure differential privacy and releases the top k eigenvectors of this perturbed matrix. In
2
2
(d/ )
particular, SULQ recommends adding a matrix N of i.i.d. Gaussian noise of variance 8d log
n2 ? 2
and applies the PCA algorithm to A + N . This guarantees a weaker privacy definition known as
(?, )-differential privacy. One problem with this approach is that with probability 1 the matrix
A + N is not symmetric, so the largest eigenvalue may not be real and the entries of the corresponding eigenvector may be complex. Thus the SULQ algorithm is not a good candidate for practical
privacy-preserving dimensionality reduction.
However, a simple modification to the basic SULQ approach does guarantee (?, ) differential
privacy. Instead of adding a asymmetric Gaussian matrix, the algorithm can add the a symmetric
matrix with i.i.d. Gaussian entries N . That is, for 1 ? i ? j ? d, the variable Nij is an independent
Gaussian random variable with variance 2 . Note that this matrix is symmetric but not necessarily
positive semidefinite, so some eigenvalues may be negative but the eigenvectors are all real. A
derivation for the noise variance is given in Theorem 1.
Algorithm 1: Algorithm MOD-SULQ (input pertubation)
inputs: d ? n data matrix X, privacy parameter ?, parameter
outputs: d ? k matrix V?k = [?
v1 v?2 ? ? ? v?k ] with orthonormal columns
1 Set A = n1 XX T .;
r
?
?
2
d+1
2 Set = n? 2 log d p+d + p1?n . Generate a d ? d symmetric random matrix N whose
2 2?
entries are i.i.d. drawn from N 0, 2 . ;
3 Compute V?k = Vk (A + N ) according to (2). ;

4

Exponential mechanism. Our new method, PPCA, randomly samples a k-dimensional subspace
from a distribution that ensures differential privacy and is biased towards high utility. The distribution from which our released subspace is sampled is known in the statistics literature as the matrix
Bingham distribution [7], which we denote by BMFk (B). The algorithm is in terms of general
k < d but our theoretical results focus on the special case k = 1 where we wish to release a onedimensional approximation to the data covariance matrix. The matrix Bingham distribution takes
values on the set of all k-dimensional subspaces of Rd and has a density equal to
1
f (V ) =
exp(tr(V T BV )),
(8)
1
1
F
k,
d,
B
1 1 2
2
where V is a d ? k matrix whose columns are orthonormal and 1 F1
hypergeometric function [7, p.33].

1
1
2 k, 2 d, B

is a confluent

Algorithm 2: Algorithm PPCA (exponential mechanism)
inputs: d ? n data matrix X, privacy parameter ?, dimension k
outputs: d ? k matrix V?k = [?
v1 v?2 ? ? ? v?k ] with orthonormal columns
1 Set A = n1 XX T ;
2 Sample V?k = BMF n ?
2A ;
By combining results on the exponential mechanism [22] along with properties of PCA algorithm,
we can show that this procedure is differentially private. In many cases, sampling from the distribution specified by the exponential mechanism distribution may be difficult computationally, especially
for continuous-valued outputs. We implement PPCA using a recently-proposed Gibbs sampler due
to Hoff [15]. Gibbs sampling is a popular Markov Chain Monte Carlo (MCMC) technique in which
samples are generated according to a Markov chain whose stationary distribution is the density in
(8). Assessing the ?burn-in time? and other factors for this procedure is an interesting question in its
own right; further details are in Section E.3.
Other approaches. There are other general algorithmic strategies for guaranteeing differential
privacy. The sensitivity method [9] adds noise proportional to the maximum change that can be
induced by changing a single point in the data set. Consider a data set D with m + 1 copies of a unit
vector u and m copies of a unit vector u0 with u ? u0 and let D0 havep
m copies of u and m+1 copies
of u0 . Then v1 (D) = u but v1 (D0 ) = u0 , so kv1 (D) v1 (D0 )k = 2. Thus the global sensitivity
does not scale with the number of data points, so as n increases the variance of the noise required
by the Laplace mechanism [9] will not decrease. An alternative to global sensitivity is smooth
sensitivity [24]; except for special cases, such as the sample median, smooth sensitivity is difficult
to compute for general functions. A third method for computing private, approximate solutions
to high-dimensional optimization problems is objective perturbation [6]; to apply this method, we
require the optimization problems to have certain properties (namely, strong convexity and bounded
norms of gradients), which do not apply to PCA.
Main results. Our theoretical results are sample complexity bounds for PPCA and MOD-SULQ
as well as a general lower bound on the sample complexity for any ?-differentially private algorithm.
These results show that the PPCA is nearly optimal in terms the scaling of the sample complexity
with respect to the data dimension d, privacy parameter ?, and eigengap . We further show that
MOD-SULQ requires more samples as a function of d, despite having a slightly weaker privacy
guarantee. Proofs are deferred to the supplementary material.
Even though both algorithms can output the top-k PCA subspace for general k ? d, we prove results
for the case k = 1. Finding the scaling behavior of the sample complexity with k is an interesting
open problem that we leave for future work; challenges here are finding the right notion of approximation of the PCA, and extending the theory using packings of Grassman or Stiefel manifolds.
Theorem 1. For the in Algorithm 1, the MOD-SULQ algorithm is (?, ) differentially private.
Theorem 2. Algorithm PPCA is ?-differentially private.
The fact that these two algorithms are differentially private follows from some simple calculations.
Our first sample complexity result provides an upper bound on the number of samples required by
5

PPCA to guarantee a certain level of privacy and accuracy. The sample complexity of PPCA n
grows linearly with the dimension d, inversely with ?, and inversely with the correlation gap (1 ?)
and eigenvalue gap 1 (A)
2 (A).
?
?
4 1
Theorem 3 (Sample complexity of PPCA). If n > ?(1 ?)(d 1 2 ) log(1/?)
+
log
,
2
d
(1 ? )( 1
2)
then PPCA is a (?, ?)-close approximation to PCA.

Our second result shows a lower bound on the number of samples required by any ?-differentiallyprivate algorithm to guarantee a certain level of accuracy for a large class of datasets, and uses proof
techniques in [4, 5].
Theorem
4 (Sample complexity
lower bound). Fix d, ?,
? 12 and let 1
=
?
?
ln 8+ln(1+exp(d))
1
exp
2?
. For any ?
1
d 2
16 , no ?-differentially private algorithm A can
approximate PCA with expected utility greater
than
? on all databases with n points in dimension d
?
q
having eigenvalue gap , where n < max d? , 180 ? ?pd1 ? .
Theorem 3 shows that if n scales like ? (1d ?) log 1 1?2 then PPCA produces an approximation v?1
pd
that has correlation ? with v1 , whereas Theorem 4 shows that n must scale like
for any
?

(1 ?)

?-differentially private algorithm. In terms of scaling with d, ? and , the upper and lower bounds
match, and they also match up to square-root factors with respect to the correlation. By contrast, the
following lower bound on the number of samples required by MOD-SULQ to ensure a certain level
of accuracy shows that MOD-SULQ has a less favorable scaling with dimension.
0
Theorem 5 (Sample
p complexity lower bound for MOD-SULQ). There are constants c and c such
d3/2

log(d/ )

that if n < c
(1 c0 (1 ?)), then there is a dataset of size n in dimension d such that
?
the top PCA direction v and the output v? of MOD-SULQ satisfy E[|h?
v1 , v1 i|] ? ?.

Notice that the dependence on n grows as d3/2 in SULQ as opposed to d in PPCA. Dimensionality
reduction via PCA is often used in applications where the data points occupy a low dimensional
space but are presented in high dimensions. These bounds suggest that PPCA is better suited to
such applications than MOD-SULQ. We next turn to validating this intuition on real data.

4

Experiments

We chose four datasets from four different domains ? kddcup99, which includes features
of 494,021 network connections, census, a demographic data set on 199, 523 individuals,
localization, a medical dataset with 164,860 instances of sensor readings on individuals engaged in different activities, and insurance, a dataset on product usage and demographics of
9,822 individuals. After preprocessing, the dimensions of these datasets are 116, 513, 44 and 150
respectively. We chose k to be 4, 8, 10, and 11 such that the top-k PCA subspace had qF (Vk ) at least
80% of kAkF . More details are in Appendix E in the supplementary material.

We ran three algorithms on these data sets : standard (non-private) PCA, MOD-SULQ with ? = 0.1
and = 0.01, and PPCA with ? = 0.1. As a sanity check, we also tried a uniformly generated
random projection ? since this projection is data-independent we would expect it to have low utility.
Standard PCA is non-private; changing a single data point will change the output, and hence violate
differential privacy. We measured the utility qF (U ), where U is the k-dimensional subspace output
by the algorithm; kU k is maximized when U is the top-k PCA subspace, and thus this reflects how
close the output subspace is to the true PCA subspace in terms of representing the data. Although
our theoretical results hold for qA (?), the ?energy? qF (?) is more relevant in practice for larger k.
Figures 1(a), 1(b), 1(c), and 1(d) show qF (U ) as a function of sample size for the k-dimensional
subspace output by PPCA, MOD-SULQ, non-private PCA, and random projections. Each value
in the figure is an average over 5 random permutations of the data, as well as 10 random starting
points of the Gibbs sampler per permutation (for PPCA), and 100 random runs per permutation (for
MOD-SULQ and random projections).
6

0.7
0.6
0.6
0.5
0.5
0.4

0.2

Algorithm

0.4

Nonprivate
PPCA
Random
SULQ

0.3

Utility

Utility

Algorithm

Nonprivate
PPCA
Random
SULQ

0.3
0.2
0.1

0.1

50000

100000

150000

2e+04

4e+04

n

6e+04

8e+04

1e+05

n

(a) census

(b) kddcup
0.5

0.5
0.4

Algorithm

Utility

0.3

Utility

Nonprivate
PPCA
Random
SULQ

0.4

Algorithm

0.3

Nonprivate
PPCA
Random
SULQ

0.2

0.2

0.1

2e+04

4e+04

6e+04

8e+04

1e+05

2000

4000

n

6000

8000

10000

n

(c) localization

(d) insurance

Figure 1: Utility qF (U ) for the four data sets

KDDCUP
LOCALIZATION

Non-private PCA
98.97 ? 0.05
100 ? 0

PPCA
98.95 ? 0.05
100 ? 0

Table 1:

MOD-SULQ
98.18 ? 0.65
97.06 ? 2.17

Random projections
98.23 ? 0.49
96.28 ? 2.34

Classification accuracy in the k-dimensional subspaces for kddcup99(k =
localization(k = 10) in the k-dimensional subspaces reported by the different algorithms.

4), and

The plots show that PPCA always outperforms MOD-SULQ, and approaches the performance of
non-private PCA with increasing sample size. By contrast, for most of the problems and sample
sizes considered by our experiments, MOD-SULQ does not perform much better than random projections. The only exception is localization, which has much lower dimension (44). This
confirms that MOD-SULQ does not scale very well with the data dimension d. The performance of
both MOD-SULQ and PPCA improve as the sample size increases; the improvement is faster for
PPCA than for MOD-SULQ. However, to be fair, MOD-SULQ is simpler and hence runs faster
than PPCA. At the sample sizes in our experiments, the performance of non-private PCA does not
improve much with a further increase in samples. Our theoretical results suggest that the performance of differentially private PCA cannot be significantly improved over these experiments.
Effect of privacy on classification. A common use of a dimension reduction algorithm is as a
precursor to classification or clustering; to evaluate the effectiveness of the different algorithms,
we projected the data onto the subspace output by the algorithms, and measured the classification
accuracy using the projected data. The classification results are summarized in Table 4. We chose
the normal vs. all classification task in kddcup99, and the falling vs. all classification task in
localization. 1 We used a linear SVM for all classification experiments.
For the classification experiments, we used half of the data as a holdout set for computing a projection subspace. We projected the classification data onto the subspace computed based on the holdout
set; 10% of this data was used for training and parameter-tuning, and the rest for testing. We repeated the classification process 5 times for 5 different (random) projections for each algorithm, and
then ran the entire procedure over 5 random permutations of the data. Each value in the figure is
thus an average over 5 ? 5 = 25 rounds of classification.
1
For the other two datasets, census and insurance, the classification accuracy of linear SVM after
(non-private) PCAs is as low as always predicting the majority label.

7

Utility versus privacy parameter
0.7

?? ?
?
?

?

?

?
?

?

Utility q(U)

0.6

Algorithm

?

0.5

Non?Private
SULQ
PPCA 1000

?
0.4
0.3
0.2

??

?
0.5

1.0

1.5

2.0

Privacy parameter alpha

Figure 2: Plot of qF (U ) versus ? for a synthetic data set with n = 5,000, d = 10, and k = 2.
The classification results show that our algorithm performs almost as well as non-private PCA for
classification in the top k PCA subspace, while the performance of MOD-SULQ and random projections are a little worse. The classification accuracy while using MOD-SULQ and random projections
also appears to have higher variance compared to our algorithm and non-private PCA; this can be
explained by the fact that these projections tend to be farther from the PCA subspace, in which the
data has higher classification accuracy.
Effect of the privacy requirement. To check the effect of the privacy requirement,
we generated a synthetic data set of n = 5,000 points drawn from a Gaussian distribution in d = 10 with mean 0 and whose covariance matrix had eigenvalues
{0.5, 0.30, 0.04, 0.03, 0.02, 0.01, 0.004, 0.003, 0.001, 0.001}. In this case the space spanned by the
top two eigenvectors has most of the energy, so we chose k = 2 and plotted the utility qF (?) for nonprivate PCA, MOD-SULQ with = 0.05, and PPCA. We drew 100 samples from each privacypreserving algorithm and the plot of the average utility versus ? is shown in Figure 2. As ? increases,
the privacy requirement is relaxed and both MOD-SULQ and PPCA approach the utility of PCA
without privacy constraints. However, for moderate ? the PPCA still captures most of the utility,
whereas the gap between MOD-SULQ and PPCA becomes quite large.

5

Conclusion

In this paper we investigated the theoretical and empirical performance of differentially private approximations to PCA. Empirically, we showed that MOD-SULQ and PPCA differ markedly in how
well they approximate the top-k subspace of the data.
p The reason for this, theoretically, is that the
sample complexity of MOD-SULQ scales with d3/2 log d whereas PPCA scales with d. Because
PPCA uses the exponential mechanism with qF (?) as the utility function, it is not surprising that
it performs well. However, MOD-SULQ often had a performance comparable to random projections, indicating that the real data sets we used were too small for it to be effective. We furthermore
showed that PPCA is nearly optimal, in that any differentially private approximation to PCA must
use ?(d) samples.
Our investigation brought up many interesting issues to consider for future work. The description of
differentially private algorithms assume an ideal model of computation : real systems require additional security assumptions that have to be verified. The difference between truly random noise and
pseudorandomness and the effects of finite precision can lead to a gap between the theoretical ideal
and practice. Numerical optimization methods used in objective perturbation [6] can only produce
approximate solutions, and have complex termination conditions unaccounted for in the theoretical
analysis. Our MCMC sampling has this flavor : we cannot sample exactly from the Bingham distribution because we must determine the Gibbs sampler?s convergence empirically. Accounting for
these effects is an interesting avenue for future work that can bring theory and practice together.
Finally, more germane to the work on PCA here is to prove sample complexity results for general k
rather than the case k = 1 here. For k = 1 the utility functions qF (?) and qA (?) are related, but for
general k it is not immediately clear what metric best captures the idea of ?approximating? PCA.
Developing a framework for such approximations is of interest more generally in machine learning.

8

References
[1] BARAK , B., C HAUDHURI , K., DWORK , C., K ALE , S., M C S HERRY, F., AND TALWAR , K. Privacy,
accuracy, and consistency too: a holistic solution to contingency table release. In PODS (2007), pp. 273?
282.
[2] B LUM , A., DWORK , C., M C S HERRY, F., AND N ISSIM , K. Practical privacy: the SuLQ framework. In
PODS (2005), pp. 128?138.
[3] B LUM , A., L IGETT, K., AND ROTH , A. A learning theory approach to non-interactive database privacy.
In STOC (2008), R. E. Ladner and C. Dwork, Eds., ACM, pp. 609?618.
[4] C HAUDHURI , K., AND H SU , D. Sample complexity bounds for differentially private learning. In COLT
(2011).
[5] C HAUDHURI , K., AND H SU , D. Convergence rates for differentially private statistical estimation. In
ICML (2012).
[6] C HAUDHURI , K., M ONTELEONI , C., AND S ARWATE , A. D. Differentially private empirical risk minimization. Journal of Machine Learning Research 12 (March 2011), 1069?1109.
[7] C HIKUSE , Y. Statistics on Special Manifolds. No. 174 in Lecture Notes in Statistics. Springer, New York,
2003.
[8] DWORK , C., K ENTHAPADI , K., M C S HERRY, F., M IRONOV, I., AND NAOR , M. Our data, ourselves:
Privacy via distributed noise generation. In EUROCRYPT (2006), vol. 4004, pp. 486?503.
[9] DWORK , C., M C S HERRY, F., N ISSIM , K., AND S MITH , A. Calibrating noise to sensitivity in private
data analysis. In 3rd IACR Theory of Cryptography Conference, (2006), pp. 265?284.
[10] F RIEDMAN , A., AND S CHUSTER , A. Data mining with differential privacy. In KDD (2010), pp. 493?
502.
[11] F UNG , B. C. M., WANG , K., C HEN , R., AND Y U , P. S. Privacy-preserving data publishing: A survey
of recent developments. ACM Comput. Surv. 42, 4 (June 2010), 53 pages.
[12] G ANTA , S. R., K ASIVISWANATHAN , S. P., AND S MITH , A. Composition attacks and auxiliary information in data privacy. In KDD (2008), pp. 265?273.
[13] H AN , S., N G , W. K., AND Y U , P. Privacy-preserving singular value decomposition. In ICDE (29
2009-april 2 2009), pp. 1267 ?1270.
[14] H ARDT, M., AND ROTH , A. Beating randomized response on incoherent matrices. In STOC (2012).
[15] H OFF , P. D. Simulation of the matrix Bingham-von Mises-Fisher distribution, with applications to multivariate and relational data. J. Comp. Graph. Stat. 18, 2 (2009), 438?456.
[16] K APRALOV, M., AND TALWAR , K. On differentially private low rank approximation. In Proc. of SODA
(2013).
[17] K ASIVISWANATHAN , S. P., AND S MITH , A. A note on differential privacy: Defining resistance to
arbitrary side information. CoRR abs/0803.3946 (2008).
[18] L IU , K., K ARGUPTA , H., AND RYAN , J. Random projection-based multiplicative data perturbation for
privacy preserving distributed data mining. IEEE Trans. Knowl. Data Eng. 18, 1 (2006), 92?106.
[19] M ACHANAVAJJHALA , A., K IFER , D., A BOWD , J. M., G EHRKE , J., AND V ILHUBER , L. Privacy:
Theory meets practice on the map. In ICDE (2008), pp. 277?286.
[20] M C S HERRY, F. Privacy integrated queries: an extensible platform for privacy-preserving data analysis.
In SIGMOD Conference (2009), pp. 19?30.
[21] M C S HERRY, F., AND M IRONOV, I. Differentially private recommender systems: Building privacy into
the netflix prize contenders. In KDD (2009), pp. 627?636.
[22] M C S HERRY, F., AND TALWAR , K. Mechanism design via differential privacy. In FOCS (2007), pp. 94?
103.
[23] M OHAMMED , N., C HEN , R., F UNG , B. C. M., AND Y U , P. S. Differentially private data release for
data mining. In KDD (2011), pp. 493?501.
[24] N ISSIM , K., R ASKHODNIKOVA , S., AND S MITH , A. Smooth sensitivity and sampling in private data
analysis. In STOC (2007), D. S. Johnson and U. Feige, Eds., ACM, pp. 75?84.
[25] S TEWART, G. On the early history of the singular value decomposition. SIAM Review 35, 4 (1993),
551?566.
[26] WASSERMAN , L., AND Z HOU , S. A statistical framework for differential privacy. JASA 105, 489 (2010).
[27] W ILLIAMS , O., AND M C S HERRY, F. Probabilistic inference and differential privacy. In NIPS (2010).
[28] Z HAN , J. Z., AND M ATWIN , S. Privacy-preserving support vector machine classification. IJIIDS 1, 3/4
(2007), 356?385.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 3454-predictive-indexing-for-fast-search.pdf

Predictive Indexing for Fast Search

Sharad Goel
Yahoo! Research
New York, NY 10018
goel@yahoo-inc.com

John Langford
Yahoo! Research
New York, NY 10018
jl@yahoo-inc.com

Alex Strehl
Yahoo! Research
New York, NY 10018
strehl@yahoo-inc.com

Abstract
We tackle the computational problem of query-conditioned search. Given a
machine-learned scoring rule and a query distribution, we build a predictive index by precomputing lists of potential results sorted based on an expected score
of the result over future queries. The predictive index datastructure supports an
anytime algorithm for approximate retrieval of the top elements. The general approach is applicable to webpage ranking, internet advertisement, and approximate
nearest neighbor search. It is particularly effective in settings where standard techniques (e.g., inverted indices) are intractable. We experimentally find substantial
improvement over existing methods for internet advertisement and approximate
nearest neighbors.

1

Introduction

The Problem. The objective of web search is to quickly return the set of most relevant web pages
given a particular query string. Accomplishing this task for a fixed query involves both determining
the relevance of potential pages and then searching over the myriad set of all pages for the most
relevant ones. Here we consider only the second problem. More formally, let Q ? Rn be an input
space, W ? Rm a finite output space of size N , and f : Q ? W 7? R a known scoring function.
Given an input (search query) q ? Q, the goal is to find, or closely approximate, the top-k output
objects (web pages) p1 , . . . , pk in W (i.e., the top k objects as ranked by f (q, ?)).
The extreme speed constraint, often 100ms or less, and the large number of web pages (N ? 1010 )
makes web search a computationally-challenging problem. Even with perfect 1000-way parallelization on modern machines, there is far too little time to directly evaluate against every page when a
particular query is submitted. This observation limits the applicability of machine-learning methods
for building ranking functions. The question addressed here is: ?Can we quickly return the highest
scoring pages as ranked by complex scoring rules typical of learning algorithms??
Predictive Indexing. We describe a method for rapidly retrieving the top elements over a large set
as determined by general scoring functions. The standard method for mitigating the computational
difficulties of search is to pre-process the data so that far less computation is necessary at runtime.
Taking the empirical probability distribution of queries into account, we pre-compute collections of
web pages that have a large expected score conditioned on the query falling into particular sets of
related queries {Qi }. For example, we may pre-compute and store the list of web pages that have the
highest average score when the query contains the phrase ?machine learning?. To yield a practical
algorithm, these sets should form meaningful groups of pages with respect to the scoring function
and query distribution. At runtime, we then optimize only over those collections of top-scoring web
pages for sets Qi containing the submitted query.
Our main contribution is optimizing the search index with respect to the query distribution. The empirical evidence presented shows that predictive indexing is an effective technique, making general
machine learning style prediction methods viable for quickly ranking over large numbers of objects.
1

The general methodology applies to other optimization problems as well, including approximate
nearest neighbor search.
In the remainder of Section 1 we describe existing solutions to large-scale search, and their applicability to general scoring functions. Section 2 describes the predictive indexing algorithm and covers
an example and lemma suggesting that predictive indexing has significant advantages over existing
techniques. We present empirical evaluation of the method in Section 3, using both proprietary web
advertising data and public data for nearest neighbor search.
1.1

Feature Representation

One concrete way to map web search into the general predictive index framework is to represent
both queries and pages as sparse binary feature vectors in a high-dimensional Euclidean space.
Specifically, we associate each word with a coordinate: A query (page) has a value of 1 for that
coordinate if it contains the word, and a value of 0 otherwise. We call this the word-based feature
representation, because each query and page can be summarized by a list of its features (i.e., words)
that it contains. The general predictive framework supports many other possible representations,
including those that incorporate the difference between words in the title and words in the body of
the web page, the number of times a word occurs, or the IP address of the user entering the query.
1.2

Related Work

Given the substantial importance of large-scale search, a variety of techniques have been developed
to address the rapid ranking problem. Past work that has referenced the query distribution includes
(Cheng et al., 2006; Chierichetti et al., 2008). Here we describe two commonly applied methods
related to the predictive index approach.
Fagin?s Threshold Algorithm. Fagin?s threshold algorithmP
(Fagin et al., 2003) supports the top-k
n
problem for linear scoring functions of the form f (q, p) = i=1 qi gi (p), where qi ? {0, 1} is the
th
i coordinate of the query q, and gi : W 7? R are partial scores for pages as determined by the ith
feature1 . For each query feature i, construct an ordered list Li containing every web page, sorted
in descending order by their partial scores gi (p). We refer to this as the projective order, since it
is attained by projecting the scoring rule onto individual coordinates. Given a query q, we evaluate
web pages in the lists Li that correspond to features of q. The algorithm maintains two statistics,
upper and lower bounds on the score of the top-k th page, halting when these bounds cross. The
lower bound is the score of the k th best page seen so far; the upper bound is the sum of the partial
scores (i.e., gi (p)) for the next-to-be-scored page in each list. Since the lists are ordered by the
partial scores, the upper threshold does in fact bound the score of any page yet to be seen.
The threshold algorithm is particularly effective when a query contains a small number of features,
facilitating fast convergence of the upper bound. In our experiments, we find that the halting condition is rarely satisfied within the imposed computational restrictions. One can, of course, simply
halt the algorithm when it has expended the computational budget (Fagin, 2002), which we refer to
as the Halted Threshold Algorithm.
Inverted Indices. An inverted index is a datastructure that maps every page feature x to a list of
pages p that contain x. When a new query arrives, a subset of page features relevant to the query is
first determined. For instance, when the query contains ?dog?, the page feature set might be {?dog?,
?canine?, ?collar?, ...}. Note that a distinction is made between query features and page features, and
in particular, the relevant page features may include many more words than the query itself. Once a
set of page features is determined, their respective lists (i.e., inverted indices) are searched, and from
them the final list of output pages is chosen. One method for searching over these lists is to execute
Fagin?s threshold algorithm. Other methods, such as the ?Weighted-And? algorithm (Broder et al.,
2003), use one global order for pages in the lists and walk down the lists synchronously to compute
page scores. See (Zobel & Moffat, 2006) for an overview of inverted indices applied to web search.
Standard approaches based on inverted indices suffer from a shortcoming. The resulting algorithms
are efficient only when it is sufficient to search over a relatively small set of inverted indices for each
1
More general monotone scoring functions (e.g., coordinate-wise product and max) are in fact supported;
for clarity, however, we restrict to the linear case.

2

query. They require, for each query q, that there exists a small set2 Xq of page features such that the
score of any page against q depends only on its intersection with Xq . In other words, the scoring
rule must be extremely sparse, with most words or features in the page having zero contribution to
the score for q. In Section 3.1, we consider a machine-learned scoring rule, derived from internet
advertising data, with the property that almost all page features have substantial influence on the
score for every query, making any straightforward approach based on inverted indices intractable.
Furthermore, algorithms that use inverted indices do not typically optimize the datastructure against
the query distribution and our experiments suggest that doing so may be beneficial.

2

An Algorithm for Rapid Approximate Ranking

Suppose we are provided with a categorization of possible queries into related, potentially overlapping, sets. For example, these sets might be defined as, ?queries containing the word ?France?,?
or ?queries with the phrase ?car rental?.? For each query set, the associated predictive index is an
ordered list of web pages sorted by their expected score for random queries drawn from that set. In
particular, we expect web pages at the top of the ?France? list to be good, on average, for queries
containing the word ?France.? In contrast to an inverted index, the pages in the ?France? list need not
themselves contain the word ?France?. To retrieve results for a particular query (e.g., ?France car
rental?), we optimize only over web pages in the relevant, pre-computed lists. Note that the predictive index is built on top of an already existing categorization of queries, a critical, and potentially
difficult initial step. In the applications we consider, however, we find that predictive indexing works
well even when applied to naively defined query sets. Furthermore, in our application to approximate nearest neighbor search, we found predictive indexing to be robust to cover sets generated via
random projections whose size and shape were varied across experiments.
We represent queries and web pages as points in, respectively, Q ? Rn and W ? Rm . This setting
is general, but for the experimental application we consider n, m ? 106 , with any given page or
query having about 102 non-zero entries (see Section 3.1 for details). Thus, pages and points are
typically sparse vectors in very high dimensional spaces. A coordinate may indicate, for example,
whether a particular word is present in the page/query, or more generally, the number of times that
word appears. Given a scoring function f : Q ? W 7? R, and a query q, we attempt to rapidly find
the top-k pages p1 , . . . , pk . Typically, we find an approximate solution, a set of pages p?1 , . . . , p?k
that are among the top l for l ? k. We assume queries are generated from a probability distribution
D that may be sampled.
2.1

Predictive Indexing for General Scoring Functions

Consider a finite collection Q of sets Qi ? Q that cover the query space (i.e., Q ? ?i Qi ). For each
Qi , define the conditional probability distribution Di over queries in Qi by Di (?) = D(?|Qi ), and
define fi : W 7? R as fi (p) = Eq?Di [f (q, p)]. The function fi (p) is the expected score of the
web page p for the (related) queries in Qi . The hope is that any page p has approximately the same
score for any query q ? Qi . If, for example, Qi is the set of queries that contain the word ?dog?, we
may expect every query in Qi to score high against pages about dogs and to score low against those
pages not about dogs.
For each set of queries Qi we pre-compute a sorted list Li of pages pi1 , pi2 , . . . , piN ordered in
descending order of fi (p). At runtime, given a query q, we identify the query sets Qi containing
q, and compute the scoring function f only on the restricted set of pages at the beginning of their
associated lists Li . We search down these lists for as long as the computational budget allows.
In general, it is difficult to compute exactly the conditional expected scores of pages fi (p). One can,
however, approximate these scores by sampling from the query distribution D. Algorithm 1 outlines
the construction of the sampling-based predictive indexing datastructure. Algorithm 2 shows how
the method operates at run time.
Note that in the special case where we cover Q with a single set, we end up with a global ordering
of web pages, independent of the query, which is optimized for the underlying query distribution.
2

The size of these sets are typically on the order of 100 or smaller.

3

Algorithm 1 Construct-Predictive-Index(Cover Q, Dataset S)
Lj [s] = 0 for all objects s and query sets Qj .
for t random queries q ? D do
for all objects s in the data set do
for all query sets Qj containing q do
Lj [s] ? Lj [s] + f (q, s)
end for
end for
end for
for all lists Lj do
sort Lj
end for
return {L}
Algorithm 2 Find-Top(query q, count k)
i=0
top-k list V = ?
while time remains do
for each query set Qj containing q do
s ? Lj [i]
if f (q, s) > k th best seen so far then
insert s into ordered top-k list V
end if
end for
i?i+1
end while
return V
While this global ordering may not be effective in isolation, it could perhaps be used to order pages
in traditional inverted indices.
2.2

Discussion

We present an elementary example to help develop intuition for why we can sometimes expect
predictive indexing to improve upon projective datastructures such as those used in Fagin?s threshold
algorithm. Suppose we have: two query features t1 and t2 ; three possible queries q1 = {t1 },
q2 = {t2 } and q3 = {t1 , t2 }; and three web pages p1 , p2 and p3 . Further suppose we have a simple
linear scoring function defined by
f (q, p1 ) = It1 ?q ? It2 ?q

f (q, p2 ) = It2 ?q ? It1 ?q

f (q, p3 ) = .5 ? It2 ?q + .5 ? It1 ?q

where I is the indicator function. That is, pi is the best match for query qi , but p3 does not score
highly for either query feature alone. Thus, an ordered, projective datastructure would have
t1 ? {p1 , p3 , p2 }

t2 ? {p2 , p3 , p1 }.

Suppose, however, that we typically only see query q3 . In this case, if we know t1 is in the query,
we infer that t2 is likely to be in the query (and vice versa), and construct the predictive index
t1 ? {p3 , p1 , p2 }

t2 ? {p3 , p2 , p1 }.

On the high probability event, namely query q3 , we see the predictive index outperforms the projective, query independent, index.
We expect predictive indices to generally improve on datastructures that are agnostic to the query
distribution. In the simple case of a single cover set (i.e., a global web page ordering) and when
we wish to optimize the probability of returning the highest-scoring object, Lemma 2.1 shows that
a predictive ordering is the best ordering relative to any particular query distribution.
4

Lemma 2.1. Suppose we have a set of points S, a query distribution D, and a function f that scores
queries against points in S. Further assume that for each query q, there is a unique highest scoring
point Hq . For s ? S, let h(s) = Prq?D (s = Hq ), and let s1 , s2 , . . . , sN be ordered according to
h(s). For any fixed k,
Pr (Hq ? {s1 , ..., sk }) =
max
Pr (Hq ? {s?(1) , ..., s?(k) }).
q?D

permutations ? q?D

Proof. For any ordering of points, s?(1) , . . . , s?(k) , the probability of the highest scoring point apPk
pearing in the top k entries equals j=1 h(s?(j) ). This sum is clearly maximized by ordering the
list according to h(?).

3

Empirical Evaluation

We evaluate predictive indexing for two applications: Internet advertising and approximate nearest
neighbor.
3.1

Internet Advertising

We present results on Internet advertising, a problem closely related to web search. We have obtained proprietary data, both testing and training, from an online advertising company. The data are
comprised of logs of events, where each event represents a visit by a user to a particular web page
p, from a set of web pages Q ? Rn . From a large set of advertisements W ? Rm , the commercial
system chooses a smaller, ordered set of ads to display on the page (generally around 4). The set of
ads seen and clicked by users is logged. Note that the role played by web pages has switched, from
result to query. The total number of ads in the data set is |W | ? 6.5 ? 105 . Each ad contains, on
average, 30 ad features, and a total of m ? 106 ad features are observed. The training data consist
of 5 million events (web page ? ad displays). The total number of distinct web pages is 5 ? 105 .
Each page consists of approximately 50 page features, and a total of n ? 9 ? 105 total page features
are observed.
We used a sparseP
feature representation (see Section 1.1) and trained a linear scoring rule f of the
form f (p, a) = i,j wi,j pi aj , to approximately rank the ads by their probability of click. Here,
wi,j are the learned weights (parameters) of the linear model. The search algorithms we compare
were given the scoring rule f , the training pages, and the ads W for the necessary pre-computations.
They were then evaluated by their serving of k = 10 ads, under a time constraint, for each page
in the test set. There was a clear separation of test and training. We measured computation time
in terms of the number of full evaluations by the algorithm (i.e., the number of ads scored against
a given page). Thus, the true test of an algorithm was to quickly select the most promising T ads
to fully score against the page, where T ? {100, 200, 300, 400, 500} was externally imposed and
varied over the experiments. These numbers were chosen to be in line with real-world computational
constraints.
We tested four methods: halted threshold algorithm (TA), as described in Section 1.2, two variants
of predictive indexing (PI-AVG and PI-DCG), and a fourth method, called best global ordering
(BO), which is a degenerate form of PI discussed in Section 2.1. An inverted index approach is
prohibitively expensive since almost all ad features have substantial influence on the score for every
web page (see Section 1.2).
PI-AVG and PI-DCG require a covering of the web page space. We used the natural covering suggested by the binary features?each page feature i corresponds to a cover set consisting of precisely
those pages p that contain i. The resulting datastructure is therefore similar to that maintained by
the TA algorithm?lists,
for each page feature, containing all the ads. However, while TA orders ads
P
by partial score j wi,j pi aj for each fixed page feature i, the predictive methods order by expected
score. PI-AVG sorts ad lists by expected score of f , Ep?Di [f (p, a)] = Ep?D [f (p, a)|i ? p], conditioned on the page containing feature i. PI-DCG and BO optimize the expected value of a modified
scoring rule, DCGf (p, a) = Ir(p,a)?16 / log2 (r(p, a) + 1), where r is the rank function and I is the
indicator function. Here, r(p, a) = j indicates that ad a has rank j according to f (p, a) over all ads
in W . BO stores a single list of all ads, sorted by expected DCGf (p, a), while PI-DCG stores a list
for each page feature i sorted by Ep?Di [DCGf (p, a)]. We chose this measure because:
5

1. Compared with using the average score of f , we empirically observe that expected DCGf
greatly improves the performance of BO on these data.
2. It is related to ?discounted cumulative gain?, a common measure for evaluating search
results in the information retrieval literature (J?arvelin & Kek?al?ainen, 2002).
3. Expected DCGf is zero for many ads, enabling significant compression of the predictive
index.
4. Lemma 2.1 suggests ordering by the probability an ad is in the top 10. The DCGf score is
a softer version of indicator of top 10.
All three predictive methods were trained by sampling from the training set, as described in 2.1.
Figure 3.1 plots the results of testing the four algorithms on the web advertising data. Each point in
the figure corresponds to one experiment, which consisted of executing each algorithm on 1000 test
pages. Along the x-axis we vary the time constraint imposed on the algorithm. The y-axis plots the
frequency, over the test pages, that the algorithm succeeded in serving the top scoring ad for position
1 (Figure 1(a)) and for position 10 (Figure 1(b)). Thus, vertical slices through each plot show the
difference in performance between the algorithms when they are given the same amount of serving
time per page. The probabilities were computed by off-line scoring of all 6.5 ? 105 ads for each test
page and computing the true top-10 ads. Serving correctly for position 10 is more difficult than for
position 1, because it also requires correctly serving ads for positions 1 through 9. We see that all
three methods of predictive indexing are superior to Fagin?s halted threshold algorithm. In addition,
the use of a richer covering, for PI-DCG and PI-AVG, provides a large boost in performance. These
latter two predictive indexing methods attain relatively high accuracy even when fully evaluating
only 0.05% of the potential results.

0.6

?

0.4

?

0.2

?

?

100

200

300

PI?AVG
PI?DCG
Fixed Ordering
Halted TA

400

500

1.0
0.8
0.6

?

?

?
?

?

?
?
?
?
?
?

100

Number of Full Evaluations

PI?AVG
PI?DCG
Fixed Ordering
Halted TA

?

0.4

?
?

0.2

?

?

0.0

?

Probability of Exact Retrieval?10th Result

Comparison of Serving Algorithms

1.0
0.8

?

?

?

0.0

Probability of Exact Retrieval?1st Result

Comparison of Serving Algorithms

200

300

400

500

Number of Full Evaluations

(a)

(b)

Figure 1: Comparison of the first and tenth results returned from the four serving algorithms on the
web advertisement dataset.
Our implementation of the predictive index, and also the halted threshold algorithm, required about
50ms per display event when 500 ad evaluations are allowed. The RAM use for the predictive index
is also reasonable, requiring about a factor of 2 more RAM than the ads themselves.
3.2

Approximate Nearest Neighbor Search

A special case application of predictive indexing is approximate nearest neighbor search. Given a set
of points W in n-dimensional Euclidean space, and a query point x in that same space, the nearest
neighbor problem is to quickly return the top-k neighbors of x. This problem is of considerable
interest for a variety of applications, including data compression, information retrieval, and pattern
recognition. In the predictive indexing framework, the nearest neighbor problem corresponds to
6

minimizing a scoring function, f (x, y) = ||x ? y||2 , defined by Euclidean distance. We assume
query points are generated from a distribution D that can be sampled.
To start, we define a covering Q of the input space Rn , which we borrow from locality-sensitive
hashing (LSH) (Gionis et al., 1999; Datar et al., 2004), a commonly suggested scheme for the
approximate nearest neighbor problem. Fix positive integer parameters ?, ?. First, we form ?
random partitions of the input space. Geometrically, each partition splits the n-dimensional space
on ? random hyperplanes. Formally, for all 1 ? i ? ? and 1 ? j ? ?, generate a random unitnorm n-vector Y ij = (Y1ij , . . . , Ynij ) ? Rn from the Gaussian (normal) distribution. For fixed
i ? {1, . . . , ?} and subset J ? {1, . . . , ?} define the cover set Qi,J = {x ? Rn : x ? Y ij ?
0 if and only if j ? J}. Note that for fixed i, the set {Qi,J |J ? {1, . . . , k}} partitions the space by
random planes.
S
Given a query point x, consider the union Ux = {Qi,J ?Q | x ? Qi,J } Qi,J of all cover sets containing x. Standard LSH approaches to the nearest neighbor problem work by scoring points in the set
Qx = W ? Ux . That is, LSH considers only those points in W that are covered by at least one of
the same ? sets as x. Predictive indexing, in contrast, maps each cover set Qi,J to an ordered list
of points sorted by their probability of being a top-10 nearest point to points in Qi,J . That is, the
lists are sorted by hQi,J (p) = Prq?D|Qi,J (p is one of the nearest 10 points to q). For the query x,
we then consider those points in W with large probability hQi,J for at least one of the sets Qi,J that
cover x.
We compare LSH and predictive indexing over four data sets: (1) MNIST?60,000 training and
10,000 test points in 784 dimensions; (2) Corel?37,749 points in 32 dimensions, split randomly
into 95% training and 5% test subsets; (3) Pendigits?7494 training and 3498 test points in 17
dimensions; and (4) Optdigits?3823 training and 1797 test points in 65 dimensions. The MNIST
data is available at http://yann.lecun.com/exdb/mnist/ and the remaining three data
sets are available at the UCI Machine Learning Repository (http://archive.ics.uci.edu/
ml/). Random projections were generated for each experiment, inducing a covering of the space that
was provided to both LSH and predictive indexing. The predictive index was generated by sampling
over the training set as discussed in Section 2.1. The number of projections ? per partition was set to
24 for the larger sets (Corel and MNIST) and 63 for the smaller sets (Pendigits and Optdigits), while
the number of partitions ? was varied as an experimental parameter. Larger ? corresponds to more
full evaluations per query, resulting in improved accuracy at the expense of increased computation
time. Both algorithms were restricted to the same average number of full evaluations per query.
Predictive indexing offers substantial improvements over LSH for all four data sets. Figure 2(a)
displays the true rank of the first point returned by LSH and predictive indexing on the MNIST data
set as a function of ?, averaged over all points in the test set and over multiple trials. Predictive
indexing outperforms LSH at each parameter setting, with the difference particularly noticeable
when fewer full evaluation are permitted (i.e., small ?). Figure 2(b) displays the performance of
LSH and predictive indexing for the tenth point returned, over all four data sets, with values of ?
varying from 5 to 70, averaged over the test sets, and replicated by multiple runs. In over 300 trials,
we did not observe a single instance of LSH outperforming predictive indexing.
Recent work has proposed more sophisticated partitionings for LSH (Andoni & Indyk, 2006). Approaches based on metric trees (Liu et al., 2004), which take advantage of the distance metric structure, have also been shown to perform well for approximate nearest neighbor. Presumably, taking
advantage of the query distribution could further improve these algorithms as well, although that is
not studied here.

4

Conclusion

Predictive indexing is the first datastructure capable of supporting scalable, rapid ranking based on
general purpose machine-learned scoring rules. In contrast, existing alternatives such as the Threshold Algorithm (Fagin, 2002) and Inverted Index approaches (Broder et al., 2003) are either substantially slower, inadequately expressive, or both, for common machine-learned scoring rules. In the
special case of approximate nearest neighbors, predictive indexing offers substantial and consistent
improvements over the Locality Sensitive Hashing algorithm.
7

?
?
?

0

?

20

30

40

50

60

70

100
80
60

?
??
?
?

40

30
20

?

10

Rank of 1st Result

LSH
Predictive Indexing

?

?
?
? ?
? ? ?
??
?
?
?
?
?
?
?
???? ? ?
??
?
?
??
?
?
?
?
?
?
??
???
?? ?
? ? ??
??
?
?
?
?
???
???
??
??
??? ? ?
??
?
?
?
??
?
?
?
?? ?
??
??
?
?
?
?
????
?
?
? ? ??
? ?? ?
?
?
?
??
?
?
?
?
?
?
?
?
?
??
?
?
?
?
???
?
?
?
??
?
?
?
?
?
?
?? ?
?? ?
?
?
?
?
??
?
?
???
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
???
?
?
??
?
?
?
?
????
?

20

Number of Partitions ?

??

?
?

20

?
?

LSH vs. Predictive Indexing ? All Data Sets
Predictive Indexing ? Rank of 10th Result

40

LSH vs. Predictive Indexing on MNIST Data

40

60

??

?
?

??

?

?
?

?

?

80

100

LSH ? Rank of 10th Result

(a) The y-axis, ?Rank of 1st Result? measures the (b) Each point represents the outcome of a single extrue rank of the first result returned by each method. periment for one of the four data sets at various paAs the number of partitions ? is increased, improved rameter settings.
accuracy is achieved at the expense of longer computation time.

Figure 2: Comparison of the first and tenth results returned from LSH and predictive indexing.

References
Andoni, A., & Indyk, P. (2006). Near-optimal hashing algorithms for near neighbor problem in high dimensions. Proceedings of the Symposium on Foundations of Computer Science (FOCS?06).
Broder, A. Z., Carmel, D., Herscovici, M., Soffer, A., & Zien, J. (2003). Efficient query evaluation using a
two-level retrieval process. CIKM ?03: Proceedings of the twelfth international conference on Information
and knowledge management (pp. 426?434).
Cheng, C.-S., Chung, C.-P., & Shann, J. J.-J. (2006). Fast query evaluation through document identifier assignment for inverted file-based information retrieval systems. Inf. Process. Manage., 42, 729?750.
Chierichetti, F., Lattanzi, S., Mari, F., & Panconesi, A. (2008). On placing skips optimally in expectation.
WSDM ?08: Proceedings of the international conference on Web search and web data mining (pp. 15?24).
New York, NY, USA: ACM.
Datar, M., Immorlica, N., Indyk, P., & Mirrokni, V. S. (2004). Locality-sensitive hashing scheme based on pstable distributions. SCG ?04: Proceedings of the twentieth annual symposium on Computational geometry
(pp. 253?262). New York, NY, USA: ACM.
Fagin, R. (2002). Combining fuzzy information: an overview. SIGMOD Rec., 31, 109?118.
Fagin, R., Lotem, A., & Naor, M. (2003). Optimal aggregation algorithms for middleware. J. Comput. Syst.
Sci., 66, 614?656.
Gionis, A., Indyk, P., & Motwani, R. (1999). Similarity search in high dimensions via hashing. The VLDB
Journal (pp. 518?529).
J?arvelin, K., & Kek?al?ainen, J. (2002). Cumulated gain-based evaluation of IR techniques. ACM Transactions
on Information Systems, 20, 422?446.
Liu, T., Moore, A., Gray, A., & Yang, K. (2004). An investigation of practical approximate nearest neighbor
algorithms. Neural Information Processing Systems.
Zobel, J., & Moffat, A. (2006). Inverted files for text search engines. ACM Comput. Surv., 38, 6.

8


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 5393-reputation-based-worker-filtering-in-crowdsourcing.pdf

Reputation-based Worker Filtering in Crowdsourcing
Srikanth Jagabathula1 Lakshminarayanan Subramanian2,3 Ashwin Venkataraman2,3
1
Department of IOMS, NYU Stern School of Business
Department of Computer Science, New York University
3
CTED, New York University Abu Dhabi
sjagabat@stern.nyu.edu {lakshmi,ashwin}@cs.nyu.edu
2

Abstract
In this paper, we study the problem of aggregating noisy labels from crowd workers to infer the underlying true labels of binary tasks. Unlike most prior work
which has examined this problem under the random worker paradigm, we consider
a much broader class of adversarial workers with no specific assumptions on their
labeling strategy. Our key contribution is the design of a computationally efficient
reputation algorithm to identify and filter out these adversarial workers in crowdsourcing systems. Our algorithm uses the concept of optimal semi-matchings
in conjunction with worker penalties based on label disagreements, to assign a
reputation score for every worker. We provide strong theoretical guarantees for
deterministic adversarial strategies as well as the extreme case of sophisticated
adversaries where we analyze the worst-case behavior of our algorithm. Finally,
we show that our reputation algorithm can significantly improve the accuracy of
existing label aggregation algorithms in real-world crowdsourcing datasets.

1

Introduction

The growing popularity of online crowdsourcing services (e.g. Amazon Mechanical Turk, CrowdFlower etc.) has made it easy to collect low-cost labels from the crowd to generate training datasets
for machine learning applications. However, these applications remain vulnerable to noisy labels
introduced either unintentionally by unreliable workers or intentionally by spammers and malicious
workers [10, 11]. Recovering the underlying true labels in the face of noisy input in online crowdsourcing environments is challenging due to three key reasons: (a) Workers are often anonymous
and transient and can provide random or even malicious labels (b) The reliabilities or reputations of
the workers are often unknown (c) Each task may receive labels from only a (small) subset of the
workers.
Several existing approaches aim to address the above challenges under the following standard setup.
There is a set T of binary tasks, each with a true label in { 1, 1}. A set of workers W are asked
to label the tasks, and the assignment of the tasks to the workers can be represented by a bipartite
graph with the workers on one side, tasks on the other side, and an edge connecting each worker
to the set of tasks she is assigned. We term this the worker-task assignment graph. Workers are
assumed to generate labels according to a probabilistic model - given a task t, a worker w provides
the true label with probability pw . Note that every worker is assumed to label each task independent
of other tasks. The goal then is to infer the underlying true labels of the tasks by aggregating the
labels provided by the workers. Prior works based on the above model can be broadly classified into
two categories: machine-learning based and linear-algebra based. The machine-learning approaches
are typically based on variants of the EM algorithm [3, 16, 24, 14]. These algorithms perform well
in most scenarios, but they lack any theoretical guarantees. More recently, linear-algebra based
algorithms [9, 6, 2] have been proposed, which provide guarantees on the error in estimating the
true labels of the tasks (under appropriate assumptions), and have also been shown to perform well
on various real-world datasets. While existing work focuses on workers making random errors,
recent work and anecdotal evidence have shown that worker labeling strategies that are common in
practice do not fit the standard random model [19]. Specific examples include vote pollution attacks
1

on Digg [18], malicious behavior in social media [22, 12] and low-precision worker populations in
crowdsourcing experiments [4].
In this paper, we aim to go beyond the standard random model and study the problem of inferring
the true labels of tasks under a much broader class of adversarial worker strategies with no specific
assumptions on their labeling pattern. For instance, deterministic labeling, where the workers always
give the same label, cannot be captured by the standard random model. Also, malicious workers can
employ arbitrary labeling patterns to degrade the accuracy of the inferred labels. Our goal is to
accurately infer the true labels of the tasks without restricting workers? strategies.
Main results. Our main contribution is the design of a reputation algorithm to identify and filter out
adversarial workers in online crowdsourcing systems. Specifically, we propose 2 computationally
efficient algorithms to compute worker reputations using only the labels provided by the workers
(see Algorithms 1 and 2), which are robust to manipulation by adversaries. We compute worker
reputations by assigning penalties to a worker for each task she is assigned. The assigned penalty is
higher for tasks on which there is ?a lot? of disagreement with the other workers. The penalties are
then aggregated in a ?load-balanced? manner using the concept of optimal semi-matchings [7]. The
reputation algorithm is designed to be used in conjunction with any of the existing label aggregation
algorithms that are designed for the standard random worker model: workers with low reputations1
are filtered out and the aggregation algorithm is used on the remaining labels. As a result, our
algorithm can be used to boost the performance of existing label aggregation algorithms.
We demonstrate the effectiveness of our algorithm using a combination of strong theoretical guarantees and empirical results on real-world datasets. Our analysis considers three scenarios. First,
we consider the standard setting in which workers are not adversarial and provide labels according
to the random model. In this setting, we show that when the worker-task assignment graph is (l, r)regular, the reputation scores are proportional to the reliabilities of the workers (see Theorem 1), so
that only unreliable workers are filtered out. As a result, our reputation scores are consistent with
worker reliabilities in the absence of adversarial workers. The analysis becomes significantly complicated for more general graphs (a fact observed in prior works; see [2]); hence, we demonstrate
improvements using simulations and experiments on real world datasets. Second, we evaluate the
performance of our algorithm in the presence of workers who use deterministic labeling strategies
(always label 1 or 1). For these strategies, when the worker-task assignment graph is (l, r)-regular,
we show (see Theorem 2) that the adversarial workers receive lower reputations than their ?honest?
counterparts, provided honest workers have ?high enough? reliabilities ? the exact bound depends
on the prevalence of tasks with true label 1, the fraction of adversarial workers and the average
reliability of the honest workers.
Third, we consider the case of sophisticated adversaries, i.e. worst-case adversarial workers whose
goal is to maximize the number of tasks they affect (i.e. cause to get incorrect labels). Under
this assumption, we provide bounds on the ?damage? they can do: We prove that irrespective of
the label aggregation algorithm (as long as it is agnostic to worker/task identities), there is a nontrivial minimum fraction of tasks whose true label is incorrectly inferred. This bound depends on
the graph structure of the honest worker labeling pattern (see Theorem 3 for details). Our result
is valid across different labeling patterns and a large class of label aggregation algorithms, and
hence provides fundamental limits on the damage achievable by adversaries. Further, we propose a
label aggregation algorithm utilizing the worker reputations computed in Algorithm 2 and prove the
existence of an upper bound on the worst-case accuracy in inferring the true labels (see Theorem 4).
Finally, using several publicly available crowdsourcing datasets (see Section 4), we show that our
reputation algorithm: (a) can help in enhancing the accuracy of state-of-the-art label aggregation
algorithms (b) is able to detect workers in these datasets who exhibit certain ?non-random? strategies.
Additional Related Work: In addition to the references cited above, there have been works which
use gold standard tasks, i.e. tasks whose true label is already known [17, 5, 11] to correct for worker
bias. [8] proposed a way of quantifying worker quality by transforming the observed labels into soft
posterior labels based on the estimated confusion matrix [3]. The authors in [13] propose an empirical Bayesian algorithm to eliminate workers who label randomly without looking at the particular
task (called spammers), and estimate the consensus labels from the remaining workers. Both these
1
As will become evident later, reputations are measures of how adversarial a worker is and are different
from reliabilities of workers.

2

works use the estimated parameters to define ?good workers? whereas we compute the reputation
scores using only the labels provided by the workers. The authors in [20] focus on detecting specific
kinds of spammers and then replace their labels with new workers. We consider all types of adversarial workers, not just spammers and don?t assume access to a pool of workers who can be asked
to label the tasks.

2

Model and reputation algorithms

Notation. Consider a set of tasks T having true labels in {1, 1}. Let yj denotes the true label of a
task tj 2 T and suppose that the tasks are sampled from a population in which the prevalence of the
positive tasks is 2 [0, 1], so that there is a fraction of tasks with true label 1. A set of workers
W provide binary labels to the tasks in T . We let G denote the bipartite worker-task assignment
graph where an edge between worker wi and task tj indicates that wi has labeled tj . Further, let
wi (tj ) denote the label provided by worker wi to task tj , where we set wi (tj ) = 0 if worker wi did
not label task tj . For a task tj , let Wj ? W denote the set of workers who labeled tj and likewise,
for a worker wi , let Ti denote the set of tasks the worker has labeled. Denote by d+
j (resp. dj ) the
|W |?|T |

number of workers labeling task tj as 1 (resp. 1). Finally, let L 2 {1, 0, 1}
denote the
matrix representing the labels assigned by the workers to the tasks, i.e. Lij = wi (tj ). Given L, the
goal is to infer the true labels yj of the tasks.

Worker model. We consider the setting in which workers may be honest or adversarial. That is,
W = H [ A with H \ A = ;. Honest workers are assumed to provide labels according to a
probabilistic model: for task tj with true label yj , worker wi provides label yj with probability pi
and yj with probability 1 pi . Note that the parameter pi doesn?t depend on the particular task that
the worker is labeling, so an honest worker labels each task independently. It is standard to define the
reliability of an honest worker as ?i = 2pi 1, so that we have ?i 2 [ 1, 1]. Further, we assume that
the honest workers are sampled from a population with average reliability ? > 0. Adversaries, on
the other hand, may adopt any arbitrary (deterministic or randomized) labeling strategy that cannot
be described using the above probabilistic process. For instance, the adversary could always label
all tasks as 1, irrespective of the true label. Another example is when the adversary decides her
labels based on existing labels cast by other workers (assuming the adversary has access to such
information). Note however, that adversarial workers need not always provide the incorrect labels.
Essentially, the presence of such workers breaks the assumptions of the model and can adversely
impact the performance of aggregation algorithms. Hence, our focus in this paper is on designing
algorithms to identify and filter out such adversarial workers. Once this is achieved, we can use
existing state-of-the-art label aggregation algorithms on the remaining labels to infer the true labels
of the tasks.
To identify these adversarial workers, we propose an algorithm for computing ?reputation? or ?trust?
scores for each worker. More concretely, we assign penalties (in a suitable way) to every worker and
higher the penalty, worse the reputation of the worker. First note that any task that has all 1 labels
(or 1 labels) does not provide us with any information about the reliabilities of the workers who
labeled the task. Hence, we focus on the tasks that have both 1 and 1 labels and we call this set
the conflict set Tcs . Further, since we have no ?side? information on the identities of workers, any
reputation score computation must be based solely on the labels provided by the workers.
We start with the following basic idea to compute reputation scores: a worker is penalized for every
?conflict? s/he is involved in (a task in the conflict set the worker has labeled on). This idea is
motivated by the fact that in an ideal scenario, where all honest workers have a reliability ?i = 1,
a conflict indicates the presence of an adversary and the reputation score aims to capture a measure
of the number of conflicts each worker is involved in: the higher the number of conflicts, the worse
the reputation score. However, a straightforward aggregation of penalties for each worker may overpenalize (honest) workers who label several tasks.
In order to overcome the issue of over-penalizing (honest) workers, we propose two techniques:
(a) soft and (b) hard assignment of penalties. In the soft assignment of penalties (Algorithm 1),
we assign a penalty of 1/d+
j to all workers who label 1 on task tj and 1/dj to all workers who
label 1 on task tj . Then, for each worker, we aggregate the penalties across all assigned tasks by
taking the average. The above assignment of penalties implicitly rewards agreements by making
the penalty inversely proportional to the number of other workers that agree with a worker. Further,
taking the average normalizes for the number of tasks labeled by the worker. Since we expect the
3

honest workers to agree with the majority more often than not, we expect this technique to assign
lower penalties to honest workers when compared to adversaries. The soft assignment of penalties
can be shown to perform quite well in identifying low reliability and adversarial workers (refer
to Theorems 1 and 2). However, it may still be subject to manipulation by more ?sophisticated?
adversaries who can adapt and modify their labeling strategy to target certain tasks and to inflate
the penalty of specific honest workers. In fact for such worst-case adversaries, we can show that
(Theorem 3) given any honest worker labeling pattern, there exists a lower bound on the number of
tasks whose true label cannot be inferred correctly, by any label aggregation algorithm.
To address the case of these sophisticated adversaries, we propose a hard penalty assignment scheme
(Algorithm 2) where the key idea is not to distribute the penalty evenly across all workers; but to
only choose two workers to penalize per conflict task: one ?representative? worker among those
who labeled 1 and another ?representative? worker among those who labeled 1. While choosing
such workers, the goal is to pick these representative workers in a load-balanced manner to ?spread?
the penalty across all workers, so that it is not concentrated on one/few workers. The final penalty of
each worker is the sum of the accrued penalties across all the (conflict) tasks assigned to the worker.
Intuitively, such hard assignment of penalties will penalize workers with higher degrees and many
conflicts (who are potential ?worst-case? adversaries), limiting their impact.
We use the concept of optimal semi-matchings [7] on bipartite graphs to distribute penalties in a
load balanced manner, which we briefly discuss here. For a bipartite graph B = (U, V, E), a semimatching in B is a set of edges M ? E such that each vertex in V is incident to exactly one edge in
M (note that vertices in U could be incident to multiple edges in M ). A semi-matching generalizes
the notion of matchings on bipartite graphs. To define an optimal semi-matching, we introduce a cost
function for a semi-matching - for each u 2 U , let degM (u) denote the number of edges in M that
PdegM (u)
M (u)+1)
are incident to u and let costM (u) be defined as costM (u)
i = degM (u)(deg
.
2
P= i=1
An optimal semi-matching then, is one which minimizes u2U costM (u). This notion of cost is
motivated by the load balancing problem for scheduling tasks on machines (refer to [7] for further
details). Intuitively, an optimal semi-matching fairly matches the V -vertices across the U -vertices
so that the maximum ?load? on any U -vertex is minimized.
Algorithm 1 SOFT PENALTY

Algorithm 2 HARD PENALTY

1: Input: W , T and L
1: Input: W , T and L
2: For every task tj 2 Tcs , assign penalty sij 2: Create a bipartite graph B cs as follows:

to each worker wi 2 Wj as follows:
sij = d1+ if Lij = 1
j

sij =

if Lij =

1

pen(wi ) =

P

1
dj

3: Output: Penalty of worker wi

3

tj 2Ti \ Tcs

sij

|Ti \ Tcs |

(i) Each worker wi 2 W is represented by
a node on the left (ii) Each task tj 2 Tcs is
represented by two nodes on the right t+
j and
tj (iii) Add the edge (wi , t+
)
if
L
=
1 or
ij
j
edge (wi , tj ) if Lij = 1.
3: Compute an optimal semi-matching OSM on
B cs and let di ( degree of worker wi in OSM
4: Output: Penalty of worker wi pen(wi ) = di

Theoretical Results

Soft penalty. We focus on (l, r)-regular worker-task assignment graphs in which every worker
is assigned l tasks and every object is labeled by r workers. The performance of our reputation
algorithms depend on the reliabilities of the workers as well as the true labels of the tasks. Hence,
we consider the following probabilistic model: for a given (l, r)-regular worker-task assignment
graph G, the reliabilities of the workers and the true labels of tasks are sampled independently (from
distributions described in Section 2). We then analyze the performance of our algorithms as the
task degree r (and hence number of workers |W |) goes to infinity. Specifically, we establish the
following results (the proofs of all theorems are in the supplementary material).
Theorem 1. Suppose there are no adversarial workers, i.e A = ; and that the worker-task assignment graph G is (l, r)-regular. Then, with high probability as r ! 1, for any pair of workers wi
and wi0 , ?i < ?i0 =) pen(wi ) > pen(wi0 ), i.e. higher reliability workers are assigned lower
penalties by Algorithm 1.
4

The probability in the above theorem is according to the model described above. Note that the theorem justifies our definition of the reputation scores by establishing their consistency with worker
reliabilities in the absence of adversarial workers. Next, consider the setting in which adversarial
workers adopt the following uniform strategy: label 1 on all assigned tasks (the 1 case is symmetric).
Theorem 2. Suppose that the worker-task assignment graph G is (l, r)-regular. Let the probability
of an arbitrary worker being honest be q and suppose each adversary adopts the uniform strategy
in which she labels 1 on all the tasks assigned to her. Denote an arbitrary honest worker as hi and
any adversary as a. Then, with high probability as r ! 1, we have
1. If

=

1
2

and ?i = 1, then pen(hi ) < pen(a) if and only if q >

2. If

=

1
2

and q >

1
1+? ,

1
1+?

then pen(hi ) < pen(a) if and only if
?i

(2

q)(1
(2

q q 2 ?2 ) q 2 ?2
q)q + q 2 ?2

The above theorem establishes that when adversaries adopt the uniform strategy, the soft-penalty
algorithm assigns lower penalties to honest workers whose reliability excess a threshold, as long as
the fraction of honest workers is ?large enough?. Although not stated, the result above immediately
extends (with a modified lower bound for ?i ) to the case when > 1/2, which corresponds to
adversaries adopting smart strategies by labeling based on the prevalence of positive tasks.
Sophisticated adversaries. Numerous real-world incidents show evidence of malicious worker behavior in online systems [18, 22, 12]. Moreover, attacks on the training process of ML models
have also been studied [15, 1]. Recent work [21] has also shown the impact of powerful adversarial attacks by administrators of crowdturfing (malicious crowdsourcing) sites. Motivated by these
examples, we consider sophisticated adversaries:
Definition 1. Sophisticated adversaries are computationally unbounded and colluding. Further,
they have knowledge of the labels provided by the honest workers and their goal is to maximize the
number of tasks whose true label is incorrectly identified.
We now raise the following question: In the presence of sophisticated adversaries, does there exist
a fundamental limit on the number of tasks whose true label can be correctly identified, irrespective
of the aggregation algorithm employed to aggregate the worker labels?
In order to answer the above question precisely, we introduce some notation. Let n = |W | and
m
m = |T |. Then, we represent any label aggregation algorithm as a decision rule R : L ! {1, 1} ,
which maps the observed labeling matrix L to a set of output labels for each task. Because of the
absence of any auxiliary information about the workers or the tasks, the class of decision rules, say
C, is invariant to permutations of the identities of workers and/or tasks. More precisely, C denotes
the class of decision rules that satisfy R(P LQ) = R(L)Q, for any n ? n permutation matrix P and
m ? m permutation matrix Q. We say that a task is affected if the decision rule outputs the incorrect
label for the task. We define the quality of a decision rule R(?) as the worst-case number of affected
tasks over all possible true labelings and adversary strategies with a fixed honest worker labeling
pattern. Fixing the honest worker labeling pattern allows isolation of the effect of the adversary
strategy on the accuracy of the decision rule. Considering the worst-case over all possible true
labels makes the metric robust to ground-truth assignments, which are typically application specific.
Next to formally define the quality, let BH denote the honest worker-task assignment graph and
y = (y1 , y2 , . . . , ym ) denote the vector of true labels for the tasks. Note that since the number of
affected tasks also depends on the actual honest worker labels, we further assume that all honest
workers have reliability ?i = 1, i.e they always label correctly. This assumption allows us to
attribute any mis-identification of true labels to the presence of adversaries because, otherwise, in
the absence of any adversaries, the true labels of all the tasks can be trivially identified. Finally, let
Sk denote the strategy space of k adversaries, where each strategy 2 Sk specifies the k ? m label
matrix provided by the adversaries. Since we do not restrict the adversary strategy in any way, it
k?m
follows that Sk = { 1, 0, 1}
. The quality of a decision rule is then defined as
n
o
y,
A?(R, BH , k) =
max
t
2
T
:
R
=
6
y
)
,
j
j
t
j
m
2Sk ,y2{1, 1}

5

where Rty, 2 {1, 1} is the label output by the decision rule R for task t when the true label vector
is y and the adversary strategy is . Note that our notation A?(R, BH , k) makes the dependence of
the quality measure on the honest worker-task assignment graph BH and the number of adversaries k
explicit. We answer the question raised above in the affirmative, i.e. there does exist a fundamental
limit on identification. In the theorem below, PreIm(T 0 ) is the set of honest workers who label
atleast one task in T 0 .
Theorem 3. Suppose that k = |A| and ?h = 1 for all honest workers h 2 H. Then, given
any honest worker-task assignment graph BH , there exists an adversary strategy ? 2 Sk that is
independent of any decision rule R 2 C such that
L ? max m A?(R, ? , y) 8R 2 C, where
y2{ 1,1}

1
max
|T 0 | ,
2 T 0 ?T : |PreIm(T 0 )|?k
and A?(R, ? , y) denotes the number of affected tasks under adversary strategy ? , decision rule
R, and true label vector y (with the assumption that max over an empty set is zero).
L=

We describe the main idea of the proof which proceeds in two steps: (i) we provide an explicit
construction of an adversary strategy ? that depends on BH and y, and (ii) we show the existence of
another true labeling y
? such that R outputs exactly the same labels in both scenarios. The adversary
labeling strategy we construct uses the idea of indistinguishability, which captures the fact that by
carefully choosing their labels, the adversaries can render themselves indistinguishable from honest
workers. Specifically, in the simple case when there is only one honest worker, the adversary simply
labels the opposite of the honest worker on all assigned tasks, so that each task has two labels of
opposite parity. It can be argued that since there is no other information, it is impossible for any
decision rule R 2 C to distinguish the honest worker from the adversary and hence identify the
true label of any task (better than a random guess). We extend this to the general case, where the
adversary ?targets? atmost k honest workers and derives a strategy based on the subgraph of BH
restricted to the targeted workers. The resultant strategy can be shown to result in incorrect labels
for atleast L tasks for some true labeling of the tasks.
Hard penalty. We now analyze the performance of the hard penalty-based reputation algorithm in
the presence of sophisticated adversarial workers. For the purposes of the theorem, we consider a
natural extension of our reputation algorithm to also perform label aggregation (see figure 1).
Theorem 4. Suppose that k = |A| and ?i = 1 for each honest worker, i.e an honest worker always
provides the correct label. Further, let d1
d2
???
d|H| denote the degrees of the honest
workers in the optimal semi-matching on BH . For any true labeling of the tasks and under the
penalty-based label aggregation algorithm (with the convention that di = 0 for i > |H|) :
Pk 1
1. There exists an adversary strategy ? such that the number of tasks affected
i=1 di .
2. No adversary strategy can affect more than U tasks where
Pk
(a) U = i=1 di , when atmost one adversary provides correct labels
P2k
(b) U = i=1 di , in the general case
A few remarks are in order. First, it can be shown [7] that for optimal semi-matchings, the degree
sequence is unique and therefore, the bounds in the theorem above are uniquely defined given BH .
Also, the assumption that ?i = 1 is required for analytical tractability, proving theoretical guarantees in crowd-sourced settings (even without adversaries) for general graph structures is notoriously
hard [2]. Note that the result of Theorem 4 provides both a lower and upper bound for the number
of tasks that can be affected by k adversaries when using the penalty-based aggregation algorithm.
The characterization we obtain is reasonably tight for the case when atmost 1 adversary provides
correct labels; in this case the gap between the upper and lower bounds is dk , which can be ?small?
for k large enough. However, our characterization is loose in the general case when adversaries can
P2k
label arbitrarily; here the gap is i=k di which we attribute to our proof technique and conjecture
Pk
that the upper bound of i=1 di also applies in the more general case.

4

Experiments

In this section, we evaluate the performance of our reputation algorithms on both synthetic and real
datasets. We consider the following popular label aggregation algorithms: (a) simple majority vot6

Random
MV
EM
KOS
KOS +
PRECISION
BEST

Malicious

Uniform

Low

High

Low

High

Low

High

9.9
-1.9
-4.3
-3.9
81.7

7.9
6.3
13.1
7.3
82.1

16.8
-1.6
-8.3
-8.3
92.5

15.6
-49.4
-98.7
-69.6
59.4

26.0
-1.2
-6.5
-6.0
80.8

15.0
-9.1
12.9
10.7
62.4

MV- SOFT MV- HARD MV- SOFT KOS MV- SOFT MV- HARD

PENALTY- BASED AGGREGATION
wt ( worker that task t is mapped
to in OSM in Algorithm 2
Output y(t) = 1 if dwt+ < dwt
y(t) = 1 if dwt+ > dwt and
y(t) = 0 otherwise
(here y refers to the label of the task
and dw refers to the degree of worker
w in OSM)

Figure 1: Left: Percentage decrease in incorrect tasks on synthetic data (negative indicates increase in incorrect
tasks). We implemented both SOFT and HARD and report the best outcome. Also reported is the precision
when removing 15 workers with the highest penalties. The columns specify the three types of adversaries
and High/Low indicates the degree bias of the adversaries. The probability that a worker is honest q was
set to 0.7 and the prevalence of positive tasks was set to 0.5. The numbers reported are an average over
100 experimental runs. The last row lists the combination with the best accuracy in each case. Right: The
penalty-based label aggregation algorithm.

ing MV (b) the EM algorithm [3] (c) the BP-based KOS algorithm [9] and (d) KOS +, a normalized
version of KOS that is amenable for arbitrary graphs (KOS is designed for random regular graphs),
and compare their accuracy in inferring the true labels of the tasks, when implemented in conjunction with our reputation algorithms. We implemented iterative versions of Algorithms 1(SOFT) and
2(HARD), where in each iteration we filtered out the worker with the highest penalty and recomputed
penalties for the remaining workers.
Synthetic Dataset. We analyzed the performance of our soft penalty-based reputation algorithm on
(l, r)-regular graphs in section 3. In many practical scenarios, however, the worker-task assignment
graph forms organically where the workers decide which tasks to label on. To consider this case, we
simulated a setup of 100 workers with a power-law distribution for worker degrees to generate the
bipartite worker-task assignment graph. We assume that an honest worker always labels correctly
(the results are qualitatively similar when honest workers make errors with small probability) and
consider three notions of adversaries: (a) random - who label each task 1 or 1 with prob. 1/2
(b) malicious - who always label incorrectly and (c) uniform - who label 1 on all tasks. Also,
we consider both cases - one where the adversaries are biased to have high degrees and the other
where they have low degrees. Further, we arbitrarily decided to remove 15% of the workers with the
highest penalties and we define precision as the percentage of workers filtered who were adversarial.
Figure 1 shows the performance improvement of the different benchmarks in the presence of our
reputation algorithm.
We make a few observations. First, we are successful in identifying random adversaries as well as
low-degree malicious and uniform adversaries (precision > 80%). This shows that our reputation
algorithms also perform well when worker-task assignment graphs are non-regular, complementing
the theoretical results (Theorems 1 and 2) for regular graphs. Second, our filtering algorithm can
result in significant reduction (upto 26%) in the fraction of incorrect tasks. In fact, in 5 out of 6 cases,
the best performing algorithm incorporates our reputation algorithm. Note that since 15 workers
are filtered out, labels from fewer workers are used to infer the true labels of the tasks. Despite
using fewer labels, we improve performance because the high precision of our algorithms results in
mostly adversaries being filtered out. Third, we note that when the adversaries are malicious and
have high degrees, the removal of 15 workers degrades the performance of the KOS (and KOS +) and
EM algorithms. We attribute this to the fact that while KOS and EM are able to automatically invert
the malicious labels, we discard these labels which hurts performance, since the adversaries have
high degrees. Finally, note that the SOFT (HARD) penalty algorithm tends to perform better when
adversaries are biased towards low (high) degrees, and this insight can be used to aid the choice of
the reputation algorithm to be employed in different scenarios.
Real Datasets. Next, we evaluated our algorithm on some standard datasets: (a) TREC2 : a collection
of topic-document pairs labeled as relevant or non-relevant by workers on AMT. We consider two
versions: stage2 and task2. (b) NLP [17]: consists of annotations by AMT workers for different
NLP tasks (1) rte - which provides binary judgments for textual entailment, i.e. whether one
2

http://sites.google.com/site/treccrowd/home

7

Dataset
rte
temp
bluebird
stage2
task2

MV

EM

KOS +

KOS

Base

Soft

Hard

Base

Soft

Hard

Base

Soft

Hard

Base

Soft

Hard

91.9
93.9
75.9
74.1
64.3

92.1(8)
93.9
75.9
74.1
64.3

92.5(3)
94.3(5)
75.9
81.4(3)
68.4(10)

92.7
94.1
89.8
64.7
66.8

92.7
94.1
89.8
65.3(6)
66.8

93.3(9)
94.1
89.8
78.9(2)
67.3(9)

49.7
56.9
72.2
74.5
57.4

88.8(9)
69.2(4)
75.9(3)
74.5
57.4

91.6(10)
93.7(3)
72.2
75.2(3)
66.7(10)

91.3
93.9
72.2
75.5
59.3

92.7(8)
94.3(7)
75.9(3)
76.6(2)
59.4(4)

92.8(10)
94.3(1)
72.2
77.2(3)
67.9(9)

82.5

81.6 81.7

84.7

62.1 73.2

79.9

78.4 79.8

aggregate 80.0 80.0

80.9

Table 1: Percentage accuracy of benchmark algorithms when combined with our reputation algorithms. For
each benchmark, the best performing combination is shown in bold. The number in the parentheses represents the number of workers filtered by our reputation algorithm (an absence indicates that no performance
improvement was achieved while removing upto 10 workers with the highest penalties).

sentence can be inferred from another (2) temp - which provides binary judgments for temporal
ordering of events. (c) bluebird [23] contains judgments differentiating two kinds of birds in
an image. Table 1 reports the best accuracy achieved when upto 10 workers are filtered using our
iterative reputation algorithms.
The main conclusion we draw is that our reputation algorithms are able to boost the performance
of state-of-the-art aggregation algorithms by a significant amount across the datasets: the average
improvement for MV and KOS + is 2.5%, EM is 3% and for KOS is almost 18%, when using the hard
penalty-based reputation algorithm. Second, we can elevate the performance of simpler algorithms
such as KOS and MV to the levels of the more general (and in some cases, complicated) EM algorithm. The KOS algorithm for instance, is not only easier to implement, but also has tight theoretical
guarantees when the underlying assignment graph is sparse random regular and further is robust to
different initializations [9]. The modified version KOS + can be used in graphs where worker degrees are skewed, but we are still able to enhance its accuracy. Our results provide evidence for the
fact that existing random models are inadequate in capturing the behavior of workers in real-world
datasets, necessitating the need for a more general approach. Third, note that the hard penalty-based
algorithm outperforms the soft version across the datasets. Since the hard penalty algorithm works
well when adversaries have higher degrees (a fact noticed in the simulation results above), this suggests the presence of high-degree adversarial workers in theses datasets. Finally, our algorithm was
successful in identifying the following types of ?adversaries?: (1) uniform workers who always label 1 or 1 (in temp, task2, stage2), (2) malicious workers who mostly label incorrectly (in
bluebird, stage2) and (3) random workers who label each task independent of its true label
(such workers were identified as ?spammers? in [13]). Therefore, our algorithm is able to identify a
broad set of adversary strategies in addition to those detected by existing techniques.

5

Conclusions

This paper analyzes the problem of inferring true labels of tasks in crowdsourcing systems against
a broad class of adversarial labeling strategies. The main contribution is the design of a reputationbased worker filtering algorithm that uses a combination of disagreement-based penalties and optimal semi-matchings to identify adversarial workers. We show that our reputation scores are consistent with the existing notion of worker reliabilities and further can identify adversaries that employ
deterministic labeling strategies. Empirically, we show that our algorithm can be applied in real
crowd-sourced datasets to enhance the accuracy of existing label aggregation algorithms. Further,
we analyze the scenario of worst-case adversaries and establish lower bounds on the minimum
?damage? achievable by the adversaries.
Acknowledgments
We thank the anonymous reviewers for their valuable feedback. Ashwin Venkataraman was supported by the Center for Technology and Economic Development (CTED).
References
[1] B. Biggio, B. Nelson, and P. Laskov. Poisoning attacks against support vector machines. arXiv preprint
arXiv:1206.6389, 2012.

8

[2] N. Dalvi, A. Dasgupta, R. Kumar, and V. Rastogi. Aggregating crowdsourced binary ratings. In Proceedings of the 22nd international conference on World Wide Web, pages 285?294. International World Wide
Web Conferences Steering Committee, 2013.
[3] A. P. Dawid and A. M. Skene. Maximum likelihood estimation of observer error-rates using the em
algorithm. Applied Statistics, pages 20?28, 1979.
[4] G. Demartini, D. E. Difallah, and P. Cudr?e-Mauroux. Zencrowd: leveraging probabilistic reasoning
and crowdsourcing techniques for large-scale entity linking. In Proceedings of the 21st international
conference on World Wide Web, pages 469?478. ACM, 2012.
[5] J. S. Downs, M. B. Holbrook, S. Sheng, and L. F. Cranor. Are your participants gaming the system?:
screening mechanical turk workers. In Proceedings of the 28th international conference on Human factors
in computing systems, pages 2399?2402. ACM, 2010.
[6] A. Ghosh, S. Kale, and P. McAfee. Who moderates the moderators?: crowdsourcing abuse detection
in user-generated content. In Proceedings of the 12th ACM conference on Electronic commerce, pages
167?176. ACM, 2011.
[7] N. J. Harvey, R. E. Ladner, L. Lov?asz, and T. Tamir. Semi-matchings for bipartite graphs and load
balancing. In Algorithms and data structures, pages 294?306. Springer, 2003.
[8] P. G. Ipeirotis, F. Provost, and J. Wang. Quality management on amazon mechanical turk. In Proceedings
of the ACM SIGKDD workshop on human computation, pages 64?67. ACM, 2010.
[9] D. R. Karger, S. Oh, and D. Shah. Iterative learning for reliable crowdsourcing systems. Neural Information Processing Systems, 2011.
[10] A. Kittur, E. H. Chi, and B. Suh. Crowdsourcing user studies with mechanical turk. In Proceedings of the
SIGCHI conference on human factors in computing systems, pages 453?456. ACM, 2008.
[11] J. Le, A. Edmonds, V. Hester, and L. Biewald. Ensuring quality in crowdsourced search relevance evaluation: The effects of training question distribution.
[12] K. Lee, P. Tamilarasan, and J. Caverlee. Crowdturfers, campaigns, and social media: tracking and revealing crowdsourced manipulation of social media. In 7th international AAAI conference on weblogs and
social media (ICWSM), Cambridge, 2013.
[13] V. C. Raykar and S. Yu. Eliminating spammers and ranking annotators for crowdsourced labeling tasks.
The Journal of Machine Learning Research, 13:491?518, 2012.
[14] V. C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez, C. Florin, L. Bogoni, and L. Moy. Learning from crowds.
The Journal of Machine Learning Research, 99:1297?1322, 2010.
[15] B. I. Rubinstein, B. Nelson, L. Huang, A. D. Joseph, S.-h. Lau, S. Rao, N. Taft, and J. Tygar. Antidote:
understanding and defending against poisoning of anomaly detectors. In Proceedings of the 9th ACM
SIGCOMM conference on Internet measurement conference, pages 1?14. ACM, 2009.
[16] P. Smyth, U. Fayyad, M. Burl, P. Perona, and P. Baldi. Inferring ground truth from subjective labelling of
venus images. Advances in neural information processing systems, pages 1085?1092, 1995.
[17] R. Snow, B. O?Connor, D. Jurafsky, and A. Y. Ng. Cheap and fast?but is it good?: evaluating non-expert
annotations for natural language tasks. In Proceedings of the conference on empirical methods in natural
language processing, pages 254?263. Association for Computational Linguistics, 2008.
[18] N. Tran, B. Min, J. Li, and L. Subramanian. Sybil-resilient online content voting. In Proceedings of
the 6th USENIX symposium on Networked systems design and implementation, pages 15?28. USENIX
Association, 2009.
[19] J. Vuurens, A. P. de Vries, and C. Eickhoff. How much spam can you take? an analysis of crowdsourcing
results to increase accuracy. In Proc. ACM SIGIR Workshop on Crowdsourcing for Information Retrieval
(CIR11), pages 21?26, 2011.
[20] J. B. Vuurens and A. P. de Vries. Obtaining high-quality relevance judgments using crowdsourcing.
Internet Computing, IEEE, 16(5):20?27, 2012.
[21] G. Wang, T. Wang, H. Zheng, and B. Y. Zhao. Man vs. machine: Practical adversarial detection of
malicious crowdsourcing workers. In 23rd USENIX Security Symposium, USENIX Association, CA, 2014.
[22] G. Wang, C. Wilson, X. Zhao, Y. Zhu, M. Mohanlal, H. Zheng, and B. Y. Zhao. Serf and turf: crowdturfing
for fun and profit. In Proceedings of the 21st international conference on World Wide Web, pages 679?688.
ACM, 2012.
[23] P. Welinder, S. Branson, S. Belongie, and P. Perona. The multidimensional wisdom of crowds. Advances
in Neural Information Processing Systems, 23:2424?2432, 2010.
[24] J. Whitehill, P. Ruvolo, T. Wu, J. Bergsma, and J. Movellan. Whose vote should count more: Optimal
integration of labels from labelers of unknown expertise. Advances in Neural Information Processing
Systems, 22(2035-2043):7?13, 2009.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 2543-machine-learning-applied-to-perception-decision-images-for-gender-classification.pdf

Machine Learning Applied to Perception:
Decision-Images for Gender Classification

Felix A. Wichmann and Arnulf B. A. Graf
Max Planck Institute for Biological Cybernetics
T?ubingen, Germany
felix.wichmann@tuebingen.mpg.de

Eero P. Simoncelli
Howard Hughes Medical Institute
Center for Neural Science
New York University, USA

?
Heinrich H. Bulthoff
and Bernhard Sch?olkopf
Max Planck Institute for Biological Cybernetics
T?ubingen, Germany

Abstract
We study gender discrimination of human faces using a combination
of psychophysical classification and discrimination experiments together
with methods from machine learning. We reduce the dimensionality of
a set of face images using principal component analysis, and then train a
set of linear classifiers on this reduced representation (linear support vector machines (SVMs), relevance vector machines (RVMs), Fisher linear
discriminant (FLD), and prototype (prot) classifiers) using human classification data. Because we combine a linear preprocessor with linear
classifiers, the entire system acts as a linear classifier, allowing us to visualise the decision-image corresponding to the normal vector of the separating hyperplanes (SH) of each classifier. We predict that the female-tomaleness transition along the normal vector for classifiers closely mimicking human classification (SVM and RVM [1]) should be faster than
the transition along any other direction. A psychophysical discrimination experiment using the decision images as stimuli is consistent with
this prediction.

1

Introduction

One of the central problems in vision science is to identify the features used by human
subjects to classify visual stimuli. We combine machine learning and psychophysical techniques to gain insight into the algorithms used by human subjects during visual classification of faces. Comparing gender classification performance of humans to that of machines
has attracted considerable attention in the past [2, 3, 4, 5]. The main novel aspect of our
study is to analyse the machine algorithms to make inferences about the features used by
human subjects, thus providing an alternative to psychophysical feature extraction techniques such as the ?bubbles? [6] or the noise classification image [7] techniques. In this
?machine-learning-psychophysics research? we first we train machine learning classifiers
on the responses (labels) of human subjects to re-create the human decision boundaries by
learning machines. Then we look for correlations between machine classifiers and sev-

eral characteristics of subjects? responses to the stimuli?proportion correct, reaction times
(RT) and confidence ratings. Ideally this allows us to find preprocessor-classifier pairings
that are closely aligned with the algorithm employed by the human brain for the task at
hand. Thereafter we analyse properties of the machine closest to the human?in our case
support vector machines (SVMs), and to slightly lesser degree, relevance vector machines
(RVMs)?and make predictions about human behaviour based on machine properties.
In the current study we extract a decision-image containing the information relevant for
~ is the image corresponding
classification by the machine classifiers. The decision-image W
to a vector w
~ orthogonal to the SH of the classifier. The decision-image has the same
dimensionality as the (input-) images?in our case 256 ? 256?whereas the normal vector
lives in the (reduced dimensionality) space after preprocessing?in our case in 200 ? 1
after Principal Component Analysis (PCA). Second, we use w
~ of the classifiers to generate
novel stimuli by adding (or subtracting) various ?amounts? (?w)
~ to a genderless face in
w
~
PCA space. The novel stimuli, images, I(?) are generated as I(?) = P CA?1 ? kwk
~ . We
predict that the female-to-maleness transition along the vectors normal to the SHs, w
~ SVM
and w
~ RVM , should be significantly faster than those along the normal vectors of machine
classifiers that do not correlate as well with human subjects. A psychophysical gender
discrimination experiment confirms our predictions: the female-to-maleness axis of the
SVM and, to a smaller extent, RVM, are more closely aligned with the human female-tomaleness axis than those of the prototype (Prot) and a Fisher linear discriminant (FLD)
classifier.

2

Preprocessing and Machine Learning Methods

We preprocessed the faces using PCA. PCA is a good preprocessor in the current context since we have previously shown that in PCA-space strong correlations exist between
man and machine [1]. Second, there is evidence that the PCA representation may be
biologically-plausible [8]. The face stimuli were taken from the gender-balanced Max
Planck Institute (MPI) face database1 composed of 200 greyscale 256 ? 256-pixel frontal
2
views of human faces, yielding a data matrix X ? R200?256 . For the gender discrimination task we adhere to the following convention for the class labels: y = ?1 for females
and y = +1 for males. We consider no dimensionality reduction and keep all 200 components of the PCA. This implies that the reconstruction of the data from the PCA analysis
? T ?X
? = EB where E ? R200?200 is the matis perfect and we can write: E = XB
rix of the encodings (each row is a PCA vector in the space of reduced dimensionality),
2
? the centered data matrix. The comB ? R200?256 is the orthogonal basis matrix and X
bination of the encoding matrix E with the true class labels y of the MPI database yields
the true dataset, whereas its combination with the class labels yest by the subjects yields
the subject dataset.
To model classification in human subjects we use methods from supervised machine learning. In particular, we consider linear classifiers where classification is done using a SH
defined by its normal vector w
~ and offset b. Furthermore the normal vector w
~ of our
classifiers can then be writtenPas a linear combination of the input patterns ~xi with suitable coefficients ?i as w
~ =
xi . We define the distance of a pattern to the SH as
i ?i ~
~ xi+b
?(~x) = hw|~
.
Note
that
in
our
experiments
the ~xi are the PCA coefficients of the imkwk
~
2

ages, that is ~xi ? R200 , whereas the images themselves are in R256 . For the subject dataset
we chose the mean values of w,
~ b and w
~ ? over all subjects.
1

The MPI face database is located at http://faces.kyb.tuebingen.mpg.de

2.1 Machine Classifiers
The Support Vector Machine (SVM, [9, 10]) is a state-of-the-art maximum margin algorithm based on statistical learning theory. SVMs have an intuitive geometrical interpretation: they classify by maximizing the margin separating both classes while minimizing
the classification error.
The Relevance Vector Machine (RVM, [11]) is a probabilistic Bayesian classifier. It optimises the expansion coefficients of a SV-style decision function using a hyperprior which
favours sparse solutions.
Common classifiers in neuroscience, cognitive science and psychology are variants of the
Prototype classifier (Prot, [12]). Their popularity is due to their simplicity: they classify
according to the nearest mean-of-class prototype; in the simplest form all dimensions are
weighted equally but variants exist that weight the dimensions inversely proportional the
class variance along the dimensions. As we cannot estimate class variance along all 200
dimensions from only 200 stimuli, we chose to implement the simplest Prot with equal
weight along all dimensions.
The Fisher linear discriminant classifier (FLD, [13]) finds a direction in the dataset which
allows best linear separation of the two classes. This direction is then used as the normal
vector of the separating hyperplane. In fact, FLD is arguably a more principled whitened
?1
variant of the Prot classifier: Its weight vector can be written as w
~ = SW
(~
?+ ?~
?? ), where
?1
SW is the within class covariance matrix of the two classes, and ?? are the class means.
Consequently, if we disregard the constant offset b, we can write the decision function as
?1/2
?1/2
?1
hw|~
~ xi = hSW
(~
?+ ? ?
~ ? )|~xi = hSW (~
?+ ? ?
~ ? )|SW ~xi, which is a prototype classifier
?1/2
using the prototypes ?
~ ? after whitening the space with SW .
2.2 Decision-Images and Generalised Portraits
? = EB and the linear classifier (SVM, RVM,
We combine the linear preprocessor (PCA) X
Prot, FLD) y(~x) = hw|~
~ xi + b to yield a linear classification system: ~y = w
~ T E T + ~b where
~b = b~1. We define the decision-image as the vector W
~ effectively used for classification as:
~ TX
? T + ~b. We then have w
~ TX
?T ? w
?T = W
~ TX
? T where B ?1
~y = W
~ T ET = W
~ T B ?T X
is the pseudo-inverse of B. For the last condition, we obtain a definition of the decision2
~ = B ?1 w
image W
~ ? R256 . In the case of PCA where B ?1 = B T , we simply have
~ = B T w.
W
~
~ for the four classifiers, SVM, RVM, Prot and FLD.
Figure 1 shows the decision-images W
The decision-images in the first row are those obtained if the classifiers are trained on the
true dataset; those in the second row if trained on the subject dataset, marked on the right
hand side of the figure by ?true data? and ?subj data?, respectively. Decision-images are
represented by a vector pointing to the positive class and can thus be expected to have male
attributes (the negative of it looks female). Both dark and light regions are more important
for classification than the grey regions. Inspection of the decision-images is instructive. For
the prototype learner, the eye and beard regions are most important. SVM, RVM and FLD
have somewhat more ?holistic? decision-images. Equally instructive is the comparison of
the optimal decision-images of the machine classifiers in row one (0 to 1% classification
error for SVM, RVM and FLD) and those trained on the subject labels in row two (the
average subject error is 16 % when classifying the faces; the machines attempt to re-create
the decision boundaries of the subjects and thus show similar mis-classification errors).
The decision-images for the subject dataset are slightly more ?face-like? and less holistic
than those obtained using the true labels; the eye and mouth regions are more strongly
emphasised. This trend is true across all classifiers. This suggest that human subjects base
their gender classification strongly on the eye and mouth regions of the face?clearly a
sub-optimal strategy as revealed by the more holistic true dataset SVM, RVM and FLD

decision-images.
A decision-image thus represents a way to extract the visual cues and features used by human subjects during visual classification without using a priori assumptions or knowledge
about the task at hand.
SVM

RVM

Prot

FLD

trained
on

?
W

true
data

?
W

subj
data

~ for each classifier for both the true and the subject dataset; all
Figure 1: Decision-images W
images are rescaled to [0, 1] and their means set to 128 for illustration purposes (different
scalers for different images).
~ ? . The generalised portraits W
~ ? can be
We can also define generalised portraits2 W
seen as ?summary? faces in each class reflecting the decision rule of the classifier. They
can be viewed as an extension of the concept of a prototype: they are the prototype
of the P
faces the classifier
bases its decision
~ can be written as:
P
P on. We note that w
w
~ =
?
~
x
=
?
~
x
?
|?
|~
x
.
This allows to define
i
i
i
i
i
i
i
i| sign(?i )=+1
i| sign(?i )=?1
~
the generalized portraits as W? which are computed by inverting the PCA transformation
on the patterns w
~? =

P
xi
i| sign(?i )=?1 ?i ~
P
.
i| sign(? )=?1 ?i

The vector w
~ ? is constrained to be in the convex

i

hull of the respective data in order to yield a ?viewable? portrait. The generalised portraits for the SVM, RVM and FLD together with the Prot, where the prototype is the same
as thePgeneralisedPportrait, are shown inP
figure 2. We also note that w
~ can be written as
w
~ = i ?i ~xi = i| sign(?i )=+1 ?i ~xi ? i| sign(?i )=?1 |?i |~xi .
~ + are males whereas
The generalised portraits can be associated with the correct class: W
~ ? are females. The SVM and the FLD use patterns close to the SH for classification
W
and hence their decision-images appear androgynous, whereas Prot and RVM tend to use
patterns distant from the SH resulting in more female and male generalised portraits. Comparison of the optimal, true, generalised portraits to those based on the subject labels shows
that classification has become more difficult: generalised portraits have moved closer to
each other in gender space, narrowing the distance between the classes and thereby diminishing the gender typicality of the generalised portraits for all classifiers.

3

Human Gender Discrimination along the Decision-Image Axes

The decision-images introduced in section 2.2 are based purely on machine learning, albeit
on labels provided by human subjects in the case of the subject dataset. Our previous paper
[1] reported that the subjects? responses to the faces?proportion correct, reaction times
2

This term was introduced by [14] with the idea in mind that when trained on a set of portraits of
members of a family, one would obtain a ?generalized? portrait which captures the essential features
of the family as a superposition of all family members.

SVM

RVM

Prot

FLD

trained
on

?
W

+

true
data

?
W

?

true
data

?
W+

subj
data

?
W?

subj
data

~ ? for each classifier for both the true and the subject
Figure 2: Generalised portraits W
dataset; all images are rescaled to [0, 1] and their means set to 128 for illustration purposes
(different scalers for different images). [Unfortunately the downsampling (low-pass filtering) of the faces necessary to fit them in the figure makes all the faces somewhat more
androgynous than they are viewed at full resolution.]

(RT) and confidence ratings?correlated very well with the distance of the stimuli to their
separating hyperplane (SH) for support and relevance vector machines (SVMs, RVMs) but
not for simple prototype (Prot) classifier. If these correlations really implied that SVM
and RVM capture some crucial aspects of human internal face representation the following
prediction must hold: already for small |?| ISVM (?) and IRVM (?) should look male/female
whereas |?| IProt (?) and IFLD (?) should only be perceptually male/female for larger |?|.
In other words: the female-to-maleness axis of SVM and RVM should be closely aligned
to those of our subjects whereas that is not expected to be the case for FLD and Prot.
3.1 Psychophysical Methods
Four observers?one of the authors (FAW) with extensive psychophysical training and
three na??ve subjects paid for their participation?took part in a standard, spatial (left versus
right) two-alternative forced-choice (2AFC) discrimination experiment. Subjects were
presented with two faces I(??) and I(?) and had to indicate which face looked more
male. Stimuli were presented against the mean luminance (50 cd/m2 ) of a carefully linearised Clinton Monoray CRT driven by a Cambridge Research Systems VSG 2/5 display
controller. Neither male nor female faces changed the mean luminance. Subjects viewed
the screen binocularly with their head stabilised by a headrest. The temporal envelope of
stimulus presentation was a modified Hanning window (a raised cosine function with rise
and fall times of 500 ms and a plateau time of 1000 ms). The probability of the female
face being presented on the left was 0.5 on each trial and observers indicated whether they

0.8

FLD
Prot
RVM
SVM

0.7
0.6
0.5

a. FAW

0.4
0.3
0.2
0.05

0.0 9

0.4
0.8
1.4
length of normalised decision image vector ? W / ||W||
@75% correct

@90% correct

c. FJ

1.4
1

2

1

@75% correct

FLD

RVM

Prot

FLD

RVM

@75% correct

@90% correct

FLD

FLD

RVM

0.6

Prot

0.6

Prot

1

RVM

1

Prot

1.4

FLD

e. KT

1.4

RVM

d. HM

Prot

Prot

@90% correct
1.8

Prot

1.8

FLD

RVM

FLD

Prot

0.6

RVM

threshold elevation re. SVM

@90% correct
1.8

2.5

3

RVM

threshold elevation re. SVM

@75% correct

b. FAW

FLD

proportion correct gender identification

1
0.9

threshold elevation re. SVM

2
@90% correct

@75% correct

f. pooled
1. 5

1

RVM

Prot

FLD

RVM

Prot

FLD

0. 5

Figure 3: a. Shows raw data and fitted psychometric functions for one observer (FAW).
b?e. For each of four observers the threshold elevation for the RVM, Prot and FLD
decision-image relative to that of the SVM; results are shown for both 75 and 90% correct together with 68%-CIs. f. Same as in b?e but pooled across observers.

thought the left or right face was female by touching the corresponding location on a Elo
TouchSystems touch-screen immediately in front of the display; no feedback was provided.
Trials were run in blocks of 256 in which eight repetitions of eight stimulus levels
(??1 . . . ? ?8 ) for each of the four classifiers were randomly intermixed. The na??ve subjects required approximately 2000 trials before their performance stabilised; thereafter they
did another five to six blocks of 256 trials. All results presented below are based on the
trials after training; all training trials were discarded.
3.2 Results and Discussion
Figure 3a shows the raw data and fitted psychometric functions for one of the observers.
Proportion correct gender identification on the y-axis is plotted against ? on the x-axis
on semi-logarithmic coordinates. Psychometric functions were fitted using the psignifit
toolbox for Matlab which implements the constrained maximum-likelihood method described in [15]. 68%-confidence intervals (CIs), indicated by horizontal lines at 75 and
90-% correct in figure 3a, were estimated by the BCa bootstrap method also implemented
in psignifit [16]. The raw data appear noisy because each data point is based on only eight
trials. However, none of fitted psychometric functions failed various Monte Carlo based
goodness-of-fit tests [15].
To summarise the data we extracted the ? required for two performance levels
(?thresholds?), 75 and 90% correct, together with their corresponding 68%-CIs. Figure 3b?
e shows the thresholds for all four observers normalised by ?SVM (the ?threshold elevation?
re. SVM). Thus values larger than 1.0 for RVM, Prot and FLD indicate that more of the
corresponding decision-images had to be added for the human observers to be able to discriminate females from males. In figure 3f we pool the data across observers as the main
trend, poorer performance for Prot and FLD compared to SVM and RVM, is apparent for
all four observers. The difference between SVM and RVM is small; going along the direction of both Prot and FLD, however, results in a much ?slower? transition from female-tomaleness.
The psychophysical data are very clear: all observers require a larger ? for Prot and FLD;
the length ratio ranges from 1.2 to nearly 3.0, and averages to around 1.7 across observers.
In the pooled data all the differences are statistically significant but even at the individual
subject level all differences are significant at the 90% performance level, and five of eight
are significant at the 75% performance level. It thus appears that SVM and RVM capture
more of the psychological face-space of our human observers than Prot and FLD. From
our results we cannot exclude the possibility that some other direction might have yielded
even steeper psychometric functions, i.e. faster female-to-maleness transitions, but we can
conclude that the decision-images of SVM and RVM are closer to the decision-images
used by human subjects than those of Prot and FLD. This is exactly as predicted by the
correlations between proportion correct, RTs and confidence ratings versus distance to the
hyperplane reported in [1]?high correlations for SVM and RVM, low correlations for Prot.

4

Summary and Conclusions

We studied classification and discrimination of human faces both psychophysically as well
as using methods from machine learning. The combination of linear preprocessor (PCA)
and classifier (SVM, RVM, Prot and FLD) allowed us to visualise the decision-images of
a classifier corresponding to the vector normal to the SH of the classifier. Decision-images
can be used to determine the regions of the stimuli most useful for classification simply
by analysing the distribution of light and dark regions in the decision-image. In addition
we defined the generalised portraits to be the prototypes of all faces used by the classifier
to obtain its classification. For the SVM this is the weighted average of all the support

vectors (SVs), for the RVM the weighted average of all the relevance vectors (RVs), and
for the Prot it is the prototype itself. The generalised portraits are, like the decision-images,
another useful visualisation of the categorisation algorithm of the machine classifier.
However, the central result of our paper is the corroboration of the machine-learningpsychophysics research methodology. In the machine-learning-psychophysics research we
substitute a very hard to analyse complex system (the human brain) by a reasonably complex system (learning machine) that is complex enough to capture essentials of our human
subjects? behaviour but is nonetheless amenable to close analysis. From the analysis of
the machines we then derive predictions for human subjects which we subsequently test
psychophysically.
Given the success in predicting the steepness of the female-to-male transition of the w
~ SVM
~
-axis we believe that the decision-image WSVM captures some of the essential characteristics of the human decision algorithm.
Acknowledgements The authors would like to thank Bruce Henning, Frank J?akel, Ulrike
von Luxburg and Christian Wallraven for helpful comments and suggestions. In addition
we thank Frank J?akel for supplying us with the code to run the touch-screen experiment.

References
[1] A.B.A. Graf and F.A. Wichmann. Insights from machine learning applied to human visual
classification. In Advances in Neural Information Processing Systems 16. MIT Press, 2004.
[2] M.S. Gray, D.T. Lawrence, B.A. Golomb, and T.S. Sejnowski. A perceptron reveals the face of
sex. Neural Computation, 7(6):1160?1164, 1995.
[3] P.J.B. Hancock, V. Bruce, and A.M. Burton. A comparison of two computer-based face recognition systems with human perceptions of faces. Vision Research, 38:2277?2288, 1998.
[4] A.J. O?Toole, P.J. Phillips, Y. Cheng, B. Ross, and H.A. Wild. Face recognition algorithms as
models of human face processing. In Proceedings of the 4th IEEE International Conference on
Automatic Face and Gesture Recognition, 2000.
[5] B. Moghaddam and M.-H. Yang. Learning gender with support faces. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 24(5):707?711, 2002.
[6] F. Gosselin and P.G. Schyns. Bubbles: a technique to reveal the use of information in recognition tasks. Vision Research, 41:2261?2271, 2001.
[7] A.J. Ahumada Jr. Classification image weights and internal noise level estimation. Journal of
Vision, 2:121?131, 2002.
[8] M. Turk and A. Pentland. Eigenfaces for recognition. Journal of Cognitive Neuroscience, 3(1),
1991.
[9] V.N. Vapnik. The Nature of Statistical Learning Theory. Springer, second edition, 2000.
[10] B. Sch?olkopf and A.J. Smola. Learning with Kernels. MIT Press, 2002.
[11] M.E. Tipping. Sparse Bayesian learning and the relevance vector machine. Journal of Machine
Learning Research, 1:211?214, 2001.
[12] S.K. Reed. Pattern recognition and categorization. Cognitive Psychology, 3:382?407, 1972.
[13] R. A. Fisher. The use of multiple measurements in taxonomic problems. Annals of Eugenics,
7(2):179?188, 1936.
[14] V. Vapnik and A. Lerner. Pattern recognition using generalized portrait method. Automation
and Remote Control, 24:774?780, 1963.
[15] F.A. Wichmann and N.J. Hill. The psychometric function: I. fitting, sampling and goodness-offit. Perception and Psychophysics, 63(8):1293?1313, 2001.
[16] F.A. Wichmann and N.J. Hill. The psychometric function: II. bootstrap-based confidence intervals and sampling. Perception and Psychophysics, 63(8):1314?1329, 2001.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

