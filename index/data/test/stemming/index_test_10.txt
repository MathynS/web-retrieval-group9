query sentence: Noninfinitesimal algorithm
---------------------------------------------------------------------
title: 1271-online-learning-from-finite-training-sets-an-analytical-case-study.pdf

Online learning from finite training sets:
An analytical case study

Peter Sollich*
Department of Physics
University of Edinburgh
Edinburgh EH9 3JZ, U.K.
P.SollichOed.ac.uk

David Barber t
Neural Computing Research Group
Department of Applied Mathematics
Aston University
Birmingham B4 7ET, U.K.
D.BarberOaston.ac.uk

Abstract
We analyse online learning from finite training sets at noninfinitesimal learning rates TJ. By an extension of statistical mechanics methods, we obtain exact results for the time-dependent
generalization error of a linear network with a large number of
weights N. We find, for example, that for small training sets of
size p ~ N, larger learning rates can be used without compromising asymptotic generalization performance or convergence speed.
Encouragingly, for optimal settings of TJ (and, less importantly,
weight decay ,\) at given final learning time, the generalization performance of online learning is essentially as good as that of offline
learning.

1

INTRODUCTION

The analysis of online (gradient descent) learning, which is one of the most common
approaches to supervised learning found in the neural networks community, has
recently been the focus of much attention [1]. The characteristic feature of online
learning is that the weights of a network ('student') are updated each time a new
training example is presented, such that the error on this example is reduced. In
offline learning, on the other hand, the total error on all examples in the training
set is accumulated before a gradient descent weight update is made. Online and
* Royal Society Dorothy Hodgkin Research Fellow
t Supported by EPSRC grant GR/J75425: Novel Developments in Learning Theory

for Neural Networks

Online Leamingfrom Finite Training Sets: An Analytical Case Study

275

offline learning are equivalent only in the limiting case where the learning rate
T) --* 0 (see, e.g., [2]). The main quantity of interest is normally the evolution of
the generalization error: How well does the student approximate the input-output
mapping ('teacher') underlying the training examples after a given number of weight
updates?
Most analytical treatments of online learning assume either that the size of the
training set is infinite, or that the learning rate T) is vanishingly small. Both of
these restrictions are undesirable: In practice, most training sets are finite, and noninfinitesimal values of T) are needed to ensure that the learning process converges
after a reasonable number of updates. General results have been derived for the
difference between online and offline learning to first order in T), which apply to
training sets of any size (see, e. g., [2]). These results, however, do not directly
address the question of generalization performance. The most explicit analysis of
the time evolution of the generalization error for finite training sets was provided by
Krogh and Hertz [3] for a scenario very similar to the one we consider below. Their
T) --* 0 (i.e., offline) calculation will serve as a baseline for our work. For finite T),
progress has been made in particular for so-called soft committee machine network
architectures [4, 5], but only for the case of infinite training sets.
Our aim in this paper is to analyse a simple model system in order to assess how the
combination of non-infinitesimal learning rates T) and finite training sets (containing
a examples per weight) affects online learning. In particular, we will consider
the dependence of the asymptotic generalization error on T) and a, the effect of
finite a on both the critical learning rate and the learning rate yielding optimal
convergence speed, and optimal values of T) and weight decay A. We also compare
the performance of online and offline learning and discuss the extent to which infinite
training set analyses are -applicable for finite a.

2

MODEL AND OUTLINE OF CALCULATION

We consider online training of a linear student network with input-output relation

Here x is an N-dimensional vector of real-valued inputs, y the single real output
and w the wei~t vector of the network. ,T, denotes the transpose of a vector and
the factor 1/VN is introduced for convenience. Whenever a training example (x, y)
is presented to the network, its weight vector is updated along the gradient of the
squared error on this example, i. e.,

where T) is the learning rate. We are interested in online learning from finite training sets, where for each update an example is randomly chosen from a given set
{(xll,yll),j.l = l. .. p} ofp training examples. (The case of cyclical presentation of
examples [6] is left for future study.) If example J.l is chosen for update n, the weight
vector is changed to

(1)
Here we have also included a weight decay 'Y. We will normally parameterize the
strength of the weight decay in terms of A = 'YO' (where a = p / N is the number

P. Sollich and D. Barber

276

of examples per weight), which plays the same role as the weight decay commonly
used in offline learning [3]. For simplicity, all student weights are assumed to be
initially zero, i.e., Wn=o
o.

=

The main quantity of interest is the evolution of the generalization error of the
student. We assume that the training examples are generated by a linear 'teacher',
i.e., yJJ = W. T x JJ IVN+e, where JJ is zero mean additive noise of variance (72. The
teacher weight vector is taken to be normalized to w. 2 = N for simplicity, and the
input vectors are assumed to be sampled randomly from an isotropic distribution
over the hypersphere x 2 = N. The generalization error, defined as the average of
the squared error between student and teacher outputs for random inputs, is then

e

where

Vn

= Wn -

W?.

In order to make the scenario analytically tractable, we focus on the limit N -+ 00
of a large number of input components and weights, taken at constant number of
examples per weight a = piN and updates per weight ('learning time') t = niN. In
this limit, the generalization error fg(t) becomes self-averaging and can be calculated
by averaging both over the random selection of examples from a given training set
and over all training sets. Our results can be straightforwardly extended to the case
of percept ron teachers with a nonlinear transfer function, as in [7].
The usual statistical mechanical approach to the online learning problem expresses
the generalization error in terms of 'order parameters' like R = ~wJw. whose
(self-averaging) time evolution is determined from appropriately averaged update
equations. This method works because for infinite training sets, the average order parameter updates can again be expressed in terms of the order parameters
alone. For finite training sets, on the other hand, the updates involve new order
parameters such as Rl = ~wJ Aw., where A is the correlation matrix of the
training inputs, A = ~L-P = lx JJ(xJJ)T. Their time evolution is in turn determined
by order parameters involving higher powers of A, yielding an infinite hierarchy
of order parameters. We solve this problem by considering instead order parameter (generating) junctions [8] such as a generalized form of the generalization error
f(t;h) = 2~vJexp(hA)vn . This allows powers of A to be obtained by differentiation with respect to h, reSUlting in a closed system of (partial differential) equations
for f(t; h) and R(t; h) = ~ wJ exp(hA)w ?.
The resulting equations and details of their solution will be given in a future publication. The final solution is most easily expressed in terms of the Laplace transform
of the generalization error
fg(Z) = '!!..
a

fdt

~

fg(t)e-z(f//a)t = fdz)

+ T}f2(Z) + T} 2f 3(Z)
1-

(2)

T}f4(Z)

The functions fi (z) (i = 1 ... 4) can be expressed in closed form in terms of a, (72
and A (and, of course, z). The Laplace transform (2) yields directly the asymptotic
value of the generalization error, foo = fg(t -+ (0) = limz--+o zig{z) , which can be
calculated analytically. For finite learning times t, fg(t) is obtained by numerical
inversion of the Laplace transform.

3

RESULTS AND DISCUSSION

We now discuss the consequences of our main result (2), focusing first on the asymptotic generalization error foo, then the convergence speed for large learning times,

Online Learningfrom Finite Training Sets: An Analytical Case Study
a=O.s

277
<1=2

a=i

Figure 1: Asymptotic generalization error (00 vs 1] and A. a as shown,

(1"2

= 0.1.

and finally the behaviour at small t. For numerical evaluations, we generally take
(1"2
0.1, corresponding to a sizable noise-to-signal ratio of JQ.I ~ 0.32.

=

The asymptotic generalization error (00 is shown in Fig. 1 as a function of 1] and A
for a
0.5, 1, 2. We observe that it is minimal for A (1"2 and 1] 0, as expected
from corresponding resul ts for offline learning [3]1. We also read off that for fixed A,
(00 is an increasing function of 1]: The larger 1], the more the weight updates tend
to overshoot the minimum of the (total, i.e., offline) training error. This causes a
diffusive motion of the weights around their average asymptotic values [2] which
increases (00. In the absence of weight decay (A = 0) and for a < 1, however, (00
is independent of 1]. In this case the training data can be fitted perfectly; every
term in the total sum-of-squares training error is then zero and online learning does
not lead to weight diffusion because all individual updates vanish . In general, the
relative increase (00(1])/(00(1] = 0) - 1 due to nonzero 1] depends significantly on a.
For 1]
1 and a
0.5, for example, this increase is smaller than 6% for all A (at
(1"2 = 0.1), and for a = 1 it is at most 13%. This means that in cases where training
data is limited (p ~ N), 1] can be chosen fairly large in order to optimize learning
speed, without seriously affecting the asymptotic generalization error. In the large
a limit, on the other hand, one finds (00 = ((1"2/2)[1/a + 1]/(2 - 1])]. The relative
increase over the value at 1] = a therefore grows linearly with a; already for a = 2,
increases of around 50% can occur for 1] = 1.

=

=

=

=

=

Fig. 1 also shows that (00 diverges as 1] approaches a critical learning rate 1]e: As
1] -+ 1]e, the 'overshoot' of the weight update steps becomes so large that the weights
eventually diverge. From the Laplace transform (2), one finds that 1]e is determined
by 1]e(4(Z = 0) = 1; it is a function of a and A only. As shown in Fig. 2b-d, 1]e
increases with A. This is reasonable, as the weight decay reduces the length of the
weight vector at each update, counteracting potential weight divergences. In the
small and large a limit, one has 1]e = 2( 1 + A) and 1]e = 2( 1 + A/a), respectively.
For constant A, 1]e therefore decreases 2 with a (Fig. 2b-d) .
We now turn to the large t behaviour of the generalization error (g(t). For small
1], the most slowly decaying contribution (or 'mode') to (g(t) varies as exp( -ct), its
1 The optimal value of the unscaledweight decay decreases with a as 'Y = (1"2 ja, because
for large training sets there is less need to counteract noise in the training data by using
a large weight decay.
2Conversely, for constant 'Y, f"/e increases with a from 2(1 + 'Ya) to 2(1 + 'Y): For large a ,
the weight decay is applied more often between repeat presentations of a training example
that would otherwise cause the weights to diverge.

P. Sollich and D. Barber

278

=

(va -

decay constant c 71['\ +
1)2]/ a scaling linearly with 71, the size of the weight
updates, as expected (Fig. 2a). For small a, the condition ct ? 1 for fg(t) to have
reached its asymptotic value foo is 71(1 + ,\)(t/a) ? 1 and scales with tla, which is
the number of times each training example has been used. For large a, on the other
hand, the condition becomes 71t ? 1: The size of the training set drops out since
convergence occurs before repetitions of training examples become significant.
For larger 71, the picture changes due to a new 'slow mode' (arising from the denominator of (2)). Interestingly, this mode exists only for 71 above a finite threshold
71min = 2/(a 1 / 2 + a- 1 / 2 -1). For finite a, it could therefore not have been predicted
from a small 71 expansion of (g(t). Its decay constant Cslow decreases to zero as
71 -t 71e, and crosses that of the normal mode at 71x(a,'\) (Fig. 2a). For 71 > 71x,
the slow mode therefore determines the convergence speed for large t, and fastest
convergence is obtained for 71 = 71x. However, it may still be advantageous to use
lower values of 71 in order to lower the asymptotic generalization error (see below);
values of 71 > 71x would deteriorate both convergence speed and asymptotic performance. Fig . 2b-d shows the dependence of 71min, 71x and 71e on a and'\. For
,\ not too large, 71x has a maximum at a ~ 1 (where 71x ~ 71e), while decaying as
71x = 1+2a- 1 / 2 ~ ~71e for larger a. This is because for a ~ 1 the (total training) error surface is very anisotropic around its minimum in weight space [9]. The steepest
directions determine 71e and convergence along them would be fastest for 71 = ~71e
(as in the isotropic case). However, the overall convergence speed is determined by
the shallow directions, which require maximal 71 ~ 71e for fastest convergence.
Consider now the small t behaviour of fg(t). Fig. 3 illustrates the dependence of
fg(t) on 71; comparison with simulation results for N = 50 clearly confirms our
calculations and demonstrates that finite N effects are not significant even for such
fairly small N. For a = 0.7 (Fig. 3a), we see that nonzero 71 acts as effective update
noise, eliminating the minimum in fg(t) which corresponds to over-training [3]. foo
is also seen to be essentially independent of 71 as predicted for the small value of
,\ = 10- 4 chosen. For a = 5, Fig. 3b clearly shows the increase of foo with 71. It
also illustrates how convergence first speeds up as 71 is increased from zero and then
slows down again as 71e ~ 2 is approached.
Above, we discussed optimal settings of 71 and ,\ for minimal asymptotic generalization error foo. Fig. 4 shows what happens if we minimize fg(t) instead for
a given final learning time t, corresponding to a fixed amount of computational
effort for training the network. As t increases, the optimal 71 decreases towards
zero as required by the tradeoff between asymptotic performance and convergence
1..=0

1..=0.1

4,-------,

4,---------,

(c)

(b)

(a)

1..=1

11m in

c

o

o

J

2a 3

4

S

o

J

2

a

3

4

S

Figure 2: Definitions of71min, 71x and 71e, and their dependence on a (for'\ as shown).

279

Online Learning /rom Finite Training Sets: An Analytical Case Study
O.511---~--~--~---.,

(a)

a

= 0.7

(b) a = 5

O.20'--~5--1~O-~15--2~O-~25--3~o--'t

Figure 3: fg vs t for different TJ. Simulations for N = 50 are shown by symbols
(standard errors less than symbol sizes). A=1O- 4 , 0- 2 =0.1, a as shown. The learning
rate TJ increases from below (at large t) over the range (a) 0.5 .. . 1.95, (b) 0.5 ... 1. 75.

(a)

0.8

0.06

0.6

/

(b)

0.08

/

(c)

0.25

,

"

0.4

'------

0.2
0.0

0.04

" ........-------

0.02
O. 00

L-L--'---~_'___'______'_~___'___'__'

o

10

20

30

40

50

10

20

30

40

50

'---'--'----'-..t.......'---'-~...J........_'___'

o

10

20

30

40

50

t

Figure 4: Optimal TJ and A vs given final learning time t, and resulting (g.
Solid/dashed lines: a = 1 / a =2; bold/thin lines: online/offline learning. 0- 2 =0.1.
Dotted lines in (a): Fits of form TJ = (a + bIn t)/t to optimal TJ for online learning.
speed. Minimizing (g(t) ::::: (00+ const . exp( -ct) ~ Cl + TJC2 + C3 exp( -C4TJt) leads to
TJopt = (a + bIn t)/t (with some constants a, b, Cl...4). Although derived for small TJ,
this functional form (dotted lines in Fig. 4a) also provides a good description down
to fairly small t , where TJopt becomes large. The optimal weight decay A increases 3
with t towards the limiting value 0- 2 . However, optimizing A is much less important than choosing the right TJ: Minimizing (g(t) for fixed A yields almost the same
generalization error as optimizing both TJ and A (we omit detailed results here 4 ). It
is encouraging to see from Fig. 4c that after as few as t = 10 updates per weight
with optimal TJ, the generalization error is almost indistinguishable from its optimal
value for t --t 00 (this also holds if A is kept fixed). Optimization of the learning
rate should therefore be worthwhile in most practical scenarios.
In Fig. 4c, we also compare the performance of online learning to that of offline
learning (calculated from the appropriate discrete time version of [3]), again with
30 ne might have expected the opposite effect of having larger>. at low t in order to
'contain' potential divergences from the larger optimal learning rates tJ. However, smaller
>. tends to make the asymptotic value foo less sensitive to large values of tJ as we saw
above, and we conclude that this effect dominates.
4Por fixed>. < u 2 , where fg(t) has an over-training minimum (see Pig. 3a), the asymptotic behaviour of tJopt changes to tJopt <X C 1 (without the In t factor), corresponding to a
fixed effective learning time tJt required to reach this minimum.

P. SalJich and D. Barber

280

optimized values of TJ and A for given t. The performance loss from using online
instead of offline learning is seen to be negligible. This may seem surprising given
the effective noise on weight updates implied by online learning, in particular for
small t. However, comparing the respective optimal learning rates (Fig. 4a), we see
that online learning makes up for this deficiency by allowing larger values of TJ to
be used (for large a, for example, TJc(offline) 2/0' ? TJc(online) 2).

=

=

Finally, we compare our finite a results with those for the limiting case a -+ 00.
Good agreement exists for any learning time t if the asymptotic generalization error
(00 (a < 00) is dominated by the contribution from the nonzero learning rate TJ (as is
the case for a -+ 00). In practice, however, one wants TJ to be small enough to make
only a negligible contribution to (00(0' < 00); in this regime, the a -+ 00 results are
essentially useless.

4

CONCLUSIONS

The main theoretical contribution of this paper is the extension of the statistical
mechanics method of order parameter dynamics to the dynamics of order parameter
(generating) functions . The results that we have obtained for a simple linear model
system are also of practical relevance. For example, the calculated dependence on
TJ of the asymptotic generalization error (00 and the convergence speed shows that,
in general, sizable values of TJ can be used for training sets of limited size (a ~ 1),
while for larger a it is important to keep learning rates small. We also found a
simple functional form for the dependence of the optimal TJ on a given final learning
time t. This could be used, for example, to estimate the optimal TJ for large t from
test runs with only a small number of weight updates. Finally, we found that for
optimized TJ online learning performs essentially as well as offline learning, whether
or not the weight decay A is optimized as well. This is encouraging, since online
learning effectively induces noisy weight updates. This allows it to cope better than
offline learning with the problem of local (training error) minima in realistic neural
networks. Online learning has the further advantage that the critical learning rates
are not significantly lowered by input distributions with nonzero mean, whereas for
offline learning they are significantly reduced [10]. In the future, we hope to extend
our approach to dynamic (t-dependent) optimization of TJ (although performance
improvements over optimal fixed TJ may be small [6]), and to more complicated network architectures in which the crucial question of local minima can be addressed.

References
[1]
[2]
[3]
[4]
[5]
[6]
[7]
[8]
[9]
[10]

See for example: The dynamics of online learning. Workshop at NIPS '95.
T. Heskes and B. Kappen. Phys. Ret). A, 44:2718, 1991.
A. Krogh and J. A. Hertz. J. Phys. A, 25:1135, 1992.
D. Saad and S. Solla. Phys. Ret). E, 52:4225, 1995; also in NIPS-8.
M. Biehl and H. Schwarze. J. Phys. A, 28:643-656, 1995.
Z.-Q. Luo. Neur. Comp., 3:226, 1991; T. Heskes and W. Wiegerinck. IEEE
Trans. Neur. Netw., 7:919, 1996.
P. Sollich. J. Phys. A, 28:6125, 1995.
1. L. Bonilla, F. G. Padilla, G. Parisi and F. Ritort. Europhys. Lett., 34:159,
1996; Phys. Ret). B, 54:4170, 1996.
J. A. Hertz, A. Krogh and G. I. Thorbergsson. J. Phys. A, 22:2133, 1989.
T. L. H. Watkin, A. Rau and M. Biehl. Ret). Modern Phys., 65:499, 1993.


----------------------------------------------------------------

title: 1390-on-line-learning-from-finite-training-sets-in-nonlinear-networks.pdf

Online learning from finite training sets
in nonlinear networks
David Barber t

Peter Sollich*

Department of Physics
University of Edinburgh
Edinburgh ERg 3JZ, U.K.

Department of Applied Mathematics
Aston University
Birmingham B4 7ET, U.K.

P.Sollich~ed.ac.uk

D.Barber~aston . ac.uk

Abstract
Online learning is one of the most common forms of neural network training. We present an analysis of online learning from finite
training sets for non-linear networks (namely, soft-committee machines), advancing the theory to more realistic learning scenarios.
Dynamical equations are derived for an appropriate set of order
parameters; these are exact in the limiting case of either linear
networks or infinite training sets. Preliminary comparisons with
simulations suggest that the theory captures some effects of finite
training sets, but may not yet account correctly for the presence of
local minima.

1

INTRODUCTION

The analysis of online gradient descent learning, as one of the most common forms
of supervised learning, has recently stimulated a great deal of interest [1, 5, 7, 3]. In
online learning, the weights of a network ('student') are updated immediately after
presentation of each training example (input-output pair) in order to reduce the
error that the network makes on that example. One of the primary goals of online
learning analysis is to track the resulting evolution of the generalization error - the
error that the student network makes on a novel test example, after a given number
of example presentations. In order to specify the learning problem, the training
outputs are assumed to be generated by a teacher network of known architecture.
Previous studies of online learning have often imposed somewhat restrictive and
? Royal Society Dorothy Hodgkin Research Fellow
tSupported by EPSRC grant GR/J75425: Novel Developments in Learning Theory for
Neural Networks

P. SolIich and D. Barber

358

unrealistic assumptions about the learning framework. These restrictions are, either
that the size of the training set is infinite, or that the learning rate is small[l, 5, 4].
Finite training sets present a significant analytical difficulty as successive weight
updates are correlated, giving rise to highly non-trivial generalization dynamics.
For linear networks, the difficulties encountered with finite training sets and noninfinitesimal learning rates can be overcome by extending the standard set of descriptive ('order') parameters to include the effects of weight update correlations[7].
In the present work, we extend our analysis to nonlinear networks. The particular
model we choose to study is the soft-committee machine which is capable of representing a rich variety of input-output mappings. Its online learning dynamics has
been studied comprehensively for infinite training sets[l, 5]. In order to carry out
our analysis, we adapt tools originally developed in the statistical mechanics literature which have found application, for example, in the study of Hopfield network
dynamics[2].

2

MODEL AND OUTLINE OF CALCULATION

For an N-dimensional input vector x, the output of the soft committee machine is
given by

(I)
where the nonlinear activation function g(hl ) = erf(hz/V2) acts on the activations
hi = wtxl.JFi (the factor 1/.JFi is for convenience only). This is a neural network
with L hidden units, input to hidden weight vectors WI, 1 = I..L, and all hidden to
output weights set to 1.
In online learning the student weights are adapted on a sequence of presented examples to better approximate the teacher mapping. The training examples are drawn,
with replacement, from a finite set, {(X/",yl-') ,j.t I..p}. This set remains fixed
piN.
during training. Its size relative to the input dimension is denoted by a
We take the input vectors xl-' as samples from an N dimensional Gaussian distribution with zero mean and unit variance. The training outputs y'" are assumed to
be generated by a teacher soft committee machine with hidden weight vectors w~,
m = I..M, with additive Gaussian noise corrupting its activations and output.

=

=

The discrepancy between the teacher and student on a particular training example (x, y), drawn from the training set, is given by the squared difference of their
corresponding outputs,

E=

H~9(hl) -yr = H~9(hl) - ~g(km +em) -eor

where the student and teacher activations are, respectively
h,

em,

and
m = I..M and
output respectively.

= {J;wtx

km

= {J;(w:n?x,

(2)

eo are noise variables corrupting the teacher activations and

Given a training example (x, y), the student weights are updated by a gradient
descent step with learning rate "I,

w; - W, = -"I\1wIE = - JNx8h E
l

(3)

359

On-line Learning from Finite Training Sets in Nonlinear Networks

The generalization error is defined to be the average error that the student makes on
a test example selected at random (and uncorrelated with the training set), which
we write as ?g = (E).
Although one could, in principle, model the student weight dynamics directly, this
will typically involve too many parameters, and we seek a more compact representation for the evolution of the generalization error. It is straightforward to show that
the generalization error depends, not on a detailed description of all the network
weights, but only on the overlap parameters Qll' = ~ W WI' and Rim = ~ W w':n
[1, 5, 7]. In the case of infinite 0, it is possible to obtain a closed set of equations
governing the overlap parameters Q, R [5]. For finite training sets, however, this is
no longer possible, due to the correlations between successive weight updates[7].

r

r

In order to overcome this difficulty, we use a technique developed originally to study
statistical physics systems [2] . Initially, consider the dynamics of a general vector of
order parameters, denoted by 0, which are functions of the network weights w. If
the weight updates are described by a transition probability T(w -+ w'), then an
approximate update equation for 0 is

0' - 0 = IfdW' (O(w') - O(w)) T(w -+
\

W'))

(4)
P(w)oc6(O(w)-O)

Intuitively, the integral in the above equation expresses the average change l of 0
caused by a weight update w -+ w', starting from (given) initial weights w. Since
our aim is to develop a closed set of equations for the order parameter dynamics, we
need to remove the dependency on the initial weights w. The only information we
have regarding w is contained in the chosen order parameters 0, and we therefore
average the result over the 'subshell' of all w which correspond to these values of
the order parameters. This is expressed as the 8-function constraint in equation(4).
It is clear that if the integral in (4) depends on w only through O(w), then the
average is unnecessary and the resulting dynamical equations are exact. This is in
fact the case for 0 -+ 00 and 0 = {Q, R}, the standard order parameters mentioned
above[5]. If this cannot be achieved, one should choose a set of order parameters to
obtain approximate equations which are as close as possible to the exact solution.
The motivation for our choice of order parameters is based on the linear perceptron
case where, in addition to the standard parameters Q and R, the overlaps projected
onto eigenspaces of the training input correlation matrix A = ~ E:=l xl' (xl') T are
required 2 . We therefore split the eigenvalues of A into r equal blocks ('Y = 1 ... r)
containing N' = N Ir eigenvalues each, ordering the eigenvalues such that they
increase with 'Y. We then define projectors p'Y onto the corresponding eigenspaces
and take as order parameters:
'Y
R1m

_
-

1 Tp'Y ..
N'w,
wm

UI.'Y

-

~
Nt W,Tp'Yb

II

(5)

where the b B are linear combinations of the noise variables and training inputs,

(6)

1 Here we assume that the system size N is large enough that the mean values of the
parameters alone describe the dynamics sufficiently well (i. e., self-averaging holds).
2The order parameters actually used in our calculation for the linear perceptron[7] are
Laplace transforms of these projected order parameters.

P. Sollich and D. Barber

360

As

r

-+

00,

these order parameters become functionals of a continuous variable3 .

The updates for the order parameters (5) due to the weight updates (3) can be
found by taking the scalar products of (3) with either projected student or teacher
weights, as appropriate. This then introduces the following activation 'components',

k'Y

m

= VNi
ff(w* )Tp'"Yx
m

=

so that the student and teacher activations are h, = ~ E'"Y hi and km ~ E'"Y k~,
respectively. For the linear perceptron, the chosen order parameters form a complete
set - the dynamical equations close, without need for the average in (4).
For the nonlinear case, we now sketch the calculation of the order parameter update
equations (4). Taken together, the integral over Wi (a sum of p discrete terms in
our case, one for each training example) and the subshell average in (4), define
an average over the activations (2), their components (7), and the noise variables
~m, ~o. These variables turn out to be Gaussian distributed with zero mean, and
therefore only their covariances need to be worked out. One finds that these are in
fact given by the naive training set averages. For example,

=
(8)

where we have used p'"Y A = a'"YP'"Y with a'"Y 'the' eigenvalue of A in the ,-th
eigenspace; this is well defined for r -+ 00 (see [6] for details of the eigenvalue
spectrum). The correlations of the activations and noise variables explicitly appearing in the error in (3) are calculated similarly to give,
(h,h,,) =

~

L:; Q~,
'"Y

(h,km) =

~L

:; Rim

(9)

'"Y

(h,~s)

=

~ L ~U,~
'"Y

where the final equation defines the noise variances. The T~m' are projected overlaps between teacher weight vectors, T~m' = ~ (w~)Tp'"Yw:n,. We will assume that
the teacher weights and training inputs are uncorrelated, so that T~m' is independent of ,. The required covariances of the 'component' activations are
a'"YR'"Y

(kinh,)
(c] h,)
(hi h" )

a

'm

-

a'"YU'"Y

-

a'"YQ'"Y

a

a

ls
II'

(k~km')

=

a'"YT'"Y

(c]k m, )

-

0

-

a'"YR'"Y

(hJkm,)

a

a

mm'

'm

(k~~s)

-

0

(C]~8' )

-

a'"Y 2
-(7s588 ,

=

.!.U'"Y

(hJ~s)

a

a

's

(10)
3Note that the limit r -+ 00 is taken after the thermodynamic limit, i.e., r ~ N. This
ensures that the number of order parameters is always negligible compared to N (otherwise
self-averaging would break down).

On-line Learning from Finite Training Sets in Nonlinear Networks
0.03 r f I I : - - - - -........- - - - - - - ,
0.025 I

(a)

0.25

(b)

o
00

OOOOOOOOC

0.2

000000000000000000000000

0000000 00

0.02

L.. ...o~ooo
~

I

0.15
0.01

361

'------~-----~

o

t

50

\

0000

'NNNoaa oa

aaaoaaaaaaaaaaaaaaaac

,,------------

o

100

50

t

100

Figure 1: fg vs t for student and teacher with one hidden unit (L = M = 1);
a = 2, 3, 4 from above, learning rate "I = 1. Noise of equal variance was added to
both activations and output (a) O'~ = 0'5 = 0.01, (b) O'~ = 0'5= 0.1. Simulations
for N = 100 are shown by circles; standard errors are of the order of the symbol
size. The bottom dashed lines show the infinite training set result for comparison.
r = 10 was used for calculating the theoretical predictions; the curved marked "+"
in (b), with r = 20 (and a = 2), shows that this is large enough to be effectively in
the r -+ 00 limit.
Using equation (3) and the definitions (7), we can now write down the dynamical
equations, replacing the number of updates n by the continuous variable t = n/ N
in the limit N -+ 00:
-"I (k-:nOh,E)

OtRim
OtU?s

-"I (c~oh,E)

OtQIz,

-"I (h7 Oh" E) - "I

(h~ Oh, E) + "12 a-y (Oh,Eoh" E)

(11)
a
where the averages are over zero mean Gaussian variables, with covariances (9,10).
Using the explicit form of the error E, we have

oh,E = g'(h,) [L9(hl') - Lg(km
I'

+ em) -

eo]

(12)

m

which, together with the equations (11) completes the description of the dynamics.
The Gaussian averages in (11) can be straightforwardly evaluated in a manner
similar to the infinite training set case[5], and we omit the rather cumbersome
explicit form of the resulting equations.
We note that, in contrast to the infinite training set case, the student activations
hI and the noise variables C and
are now correlated through equation (10).
Intuitively, this is reasonable as the weights become correlated, during training,
with the examples in the training set. In calculating the generalization error, on the
other hand, such correlations are absent, and one has the same result as for infinite
training sets. The dynamical equations (11), together with (9,10) constitute our
main result. They are exact for the limits of either a linear network (R, Q, T -+ 0,
so that g(x) ex: x) or a -+ 00, and can be integrated numerically in a straightforward
way. In principle, the limit r -+ 00 should be taken but, as shown below, relatively
small values of r can be taken in practice.

s

3

es

RESULTS AND DISCUSSION

We now discuss the main consequences of our result (11), comparing the resulting
predictions for the generalization dynamics, fg(t), to the infinite training set theory

P. Sollich and D. Barber

362

k

(a)

0.4 ..----------~--~----,

0.25
02
. 100000000000000000000000
1
______________

0.3

0.15 ,

0.2

0.1

\

0.05

...

,'--

~

--- ---

0.1

O~--~------~----~~~

o

(b)

10

20

30

40

t

50

~ooooooooooooooooooo
o
W
100
1W t 200

OL---~----------~----~

Figure 2: ?g VS t for two hidden units (L = M = 2). Left: a = 0.5, with a = 00
shown by dashed line for comparison; no noise. Right: a = 4, no noise (bottom)
and noise on teacher activations and outputs of variance 0.1 (top). Simulations for
N = 100 are shown by small circles; standard errors are less than the symbol size.
Learning rate fJ = 2 throughout.

and to simulations. Throughout, the teacher overlap matrix is set to
(orthogonal teacher weight vectors of length V'ii).

Tij

=

c5ij

In figure(l), we study the accuracy of our method as a function of the training
set size for a nonlinear network with one hidden unit at two different noise levels.
The learning rate was set to fJ = 1 for both (a) and (b). For small activation
and output noise (0'2 = 0.01), figure(la) , there is good agreement with the simulations for a down to a = 3, below which the theory begins to underestimate
the generalization error, compared to simulations. Our finite a theory, however,
is still considerably more accurate than the infinite a predictions. For larger noise
(0'2 = 0.1, figure(lb?, our theory provides a reasonable quantitative estimate of the
generalization dynamics for a > 3. Below this value there is significant disagreement, although the qualitative behaviour of the dynamics is predicted quite well,
including the overfitting phenomenon beyond t ~ 10. The infinite a theory in this
case is qualitatively incorrect.

In the two hidden unit case, figure(2), our theory captures the initial evolution of
?g(t) very well, but diverges significantly from the simulations at larger t; nevertheless, it provides a considerable improvement on the infinite a theory. One reason for
the discrepancy at large t is that the theory predicts that different student hidden
units will always specialize to individual teacher hidden units for t --+ 00, whatever
the value of a. This leads to a decay of ?g from a plateau value at intermediate times
t. In the simulations, on the other hand, this specialization (or symmetry breaking)
appears to be inhibited or at least delayed until very large t. This can happen even
for zero noise and a 2:: L, where the training data should should contain enough
information to force student and teacher weights to be equal asymptotically. The
reason for this is not clear to us, and deserves further study. Our initial investigations, however, suggest that symmetry breaking may be strongly delayed due to the
presence of saddle points in the training error surface with very 'shallow' unstable
directions.
When our theory fails, which of its assumptions are violated? It is conceivable
that multiple local minima in the training error surface could cause self-averaging
to break down; however, we have found no evidence for this, see figure(3a). On
the other hand, the simulation results in figure(3b) clearly show that the implicit
assumption of Gaussian student activations - as discussed before eq. (8) - can be
violated.

On-line Learning from Finite Training Sets in Nonlinear Networks

(a)

363

(b)

/

Variance over training histories

10"'" ' - - - - - - - - - - - - - - - '
102
N

Figure 3: (a) Variance of fg(t = 20) vs input dimension N for student and teacher
with two hidden units (L = M = 2), a = 0.5, 'fJ = 2, and zero noise. The bottom
curve shows the variance due to different random choices of training examples from
a fixed training set ('training history'); the top curve also includes the variance due
to different training sets. Both are compatible with the liN decay expected if selfaveraging holds (dotted line). (b) Distribution (over training set) of the activation
hI of the first hidden unit of the student. Histogram from simulations for N = 1000,
all other parameter values as in (a).
In summary, the main theoretical contribution of this paper is the extension of online
learning analysis for finite training sets to nonlinear networks. Our approximate
theory does not require the use of replicas and yields ordinary first order differential
equations for the time evolution of a set of order parameters. Its central implicit
assumption (and its Achilles' heel) is that the student activations are Gaussian
distributed. In comparison with simulations, we have found that it is more accurate
than the infinite training set analysis at predicting the generalization dynamics for
finite training sets, both qualitatively and also quantitatively for small learning
times t. Future work will have to show whether the theory can be extended to cope
with non-Gaussian student activations without incurring the technical difficulties
of dynamical replica theory [2], and whether this will help to capture the effects of
local minima and, more generally, 'rough' training error surfaces.
Acknowledgments: We would like to thank Ansgar West for helpful discussions.

References
[1] M. Biehl and H. Schwarze. Journal of Physics A, 28:643-656, 1995.
[2] A. C. C. Coolen, S. N. Laughton, and D. Sherrington. In NIPS 8, pp. 253-259,
MIT Press, 1996; S.N. Laughton, A.C.C. Coolen, and D. Sherrington. Journal
of Physics A, 29:763-786, 1996.
[3] See for example: The dynamics of online learning. Workshop at NIPS'95.
[4] T. Heskes and B. Kappen. Physical Review A, 44:2718-2762, 1994.
[5] D. Saad and S. A. Solla Physical Review E, 52:4225, 1995.
[6] P. Sollich. Journal of Physics A, 27:7771-7784, 1994.
[7] P. Sollich and D. Barber. In NIPS 9, pp.274-280, MIT Press, 1997; Europhysics
Letters, 38:477-482, 1997.


----------------------------------------------------------------

title: 4135-a-theory-of-multiclass-boosting.pdf

A Theory of Multiclass Boosting

Indraneel Mukherjee

Robert E. Schapire

Princeton University, Department of Computer Science, Princeton, NJ 08540
{imukherj,schapire}@cs.princeton.edu

Abstract
Boosting combines weak classifiers to form highly accurate predictors. Although
the case of binary classification is well understood, in the multiclass setting, the
?correct? requirements on the weak classifier, or the notion of the most efficient
boosting algorithms are missing. In this paper, we create a broad and general
framework, within which we make precise and identify the optimal requirements
on the weak-classifier, as well as design the most effective, in a certain sense,
boosting algorithms that assume such requirements.

1

Introduction

Boosting [17] refers to a general technique of combining rules of thumb, or weak classifiers, to form
highly accurate combined classifiers. Minimal demands are placed on the weak classifiers, so that a
variety of learning algorithms, also called weak-learners, can be employed to discover these simple
rules, making the algorithm widely applicable. The theory of boosting is well-developed for the case
of binary classification. In particular, the exact requirements on the weak classifiers in this setting
are known: any algorithm that predicts better than random on any distribution over the training set
is said to satisfy the weak learning assumption. Further, boosting algorithms that minimize loss as
efficiently as possible have been designed. Specifically, it is known that the Boost-by-majority [6]
algorithm is optimal in a certain sense, and that AdaBoost [11] is a practical approximation.
Such an understanding would be desirable in the multiclass setting as well, since many natural classification problems involve more than two labels, e.g. recognizing a digit from its image, natural
language processing tasks such as part-of-speech tagging, and object recognition in vision. However, for such multiclass problems, a complete theoretical understanding of boosting is lacking. In
particular, we do not know the ?correct? way to define the requirements on the weak classifiers, nor
has the notion of optimal boosting been explored in the multiclass setting.
Straightforward extensions of the binary weak-learning condition to multiclass do not work. Requiring less error than random guessing on every distribution, as in the binary case, turns out to be too
weak for boosting to be possible when there are more than two labels. On the other hand, requiring
more than 50% accuracy even when the number of labels is much larger than two is too stringent,
and simple weak classifiers like decision stumps fail to meet this criterion, even though they often
can be combined to produce highly accurate classifiers [9]. The most common approaches so far
have relied on reductions to binary classification [2], but it is hardly clear that the weak-learning
conditions implicitly assumed by such reductions are the most appropriate.
The purpose of a weak-learning condition is to clarify the goal of the weak-learner, thus aiding in
its design, while providing a specific minimal guarantee on performance that can be exploited by a
boosting algorithm. These considerations may significantly impact learning and generalization because knowing the correct weak-learning conditions might allow the use of simpler weak classifiers,
which in turn can help prevent overfitting. Furthermore, boosting algorithms that more efficiently
and effectively minimize training error may prevent underfitting, which can also be important.
In this paper, we create a broad and general framework for studying multiclass boosting that formalizes the interaction between the boosting algorithm and the weak-learner. Unlike much, but not all,
of the previous work on multiclass boosting, we focus specifically on the most natural, and perhaps
1

weakest, case in which the weak classifiers are genuine classifiers in the sense of predicting a single
multiclass label for each instance. Our new framework allows us to express a range of weak-learning
conditions, both new ones and most of the ones that had previously been assumed (often only implicitly). Within this formalism, we can also now finally make precise what is meant by correct
weak-learning conditions that are neither too weak nor too strong.
We focus particularly on a family of novel weak-learning conditions that have an especially appealing form: like the binary conditions, they require performance that is only slightly better than
random guessing, though with respect to performance measures that are more general than ordinary
classification error. We introduce a whole family of such conditions since there are many ways of
randomly guessing on more than two labels, a key difference between the binary and multiclass settings. Although these conditions impose seemingly mild demands on the weak-learner, we show that
each one of them is powerful enough to guarantee boostability, meaning that some combination of
the weak classifiers has high accuracy. And while no individual member of the family is necessary
for boostability, we also show that the entire family taken together is necessary in the sense that for
every boostable learning problem, there exists one member of the family that is satisfied. Thus, we
have identified a family of conditions which, as a whole, is necessary and sufficient for multiclass
boosting. Moreover, we can combine the entire family into a single weak-learning condition that is
necessary and sufficient by taking a kind of union, or logical OR, of all the members. This combined
condition can also be expressed in our framework.
With this understanding, we are able to characterize previously studied weak-learning conditions. In
particular, the condition implicitly used by AdaBoost.MH [19], which is based on a one-against-all
reduction to binary, turns out to be strictly stronger than necessary for boostability. This also applies
to AdaBoost.M1 [9], the most direct generalization of AdaBoost to multiclass, whose conditions
can be shown to be equivalent to those of AdaBoost.MH in our setting. On the other hand, the
condition implicit to Zhu et al.?s SAMME algorithm [21] is too weak in the sense that even when the
condition is satisfied, no boosting algorithm can guarantee to drive down the training error. Finally,
the condition implicit to AdaBoost.MR [19, 9] (also called AdaBoost.M2) turns out to be exactly
necessary and sufficient for boostability.
Employing proper weak-learning conditions is important, but we also need boosting algorithms that
can exploit these conditions to effectively drive down error. For a given weak-learning condition,
the boosting algorithm that drives down training error most efficiently in our framework can be
understood as the optimal strategy for playing a certain two-player game. These games are nontrivial to analyze. However, using the powerful machinery of drifting games [8, 16], we are able to
compute the optimal strategy for the games arising out of each weak-learning condition in the family
described above. These optimal strategies have a natural interpretation in terms of random walks, a
phenomenon that has been observed in other settings [1, 6].
Our focus in this paper is only on minimizing training error, which, for the algorithms we derive,
provably decreases exponentially fast with the number of rounds of boosting. Such results can be
used in turn to derive bounds on the generalization error using standard techniques that have been
applied to other boosting algorithms [18, 11, 13]. (We omit these due to lack of space.)
The game-theoretic strategies are non-adaptive in that they presume prior knowledge about the edge,
that is, how much better than random are the weak classifiers. Algorithms that are adaptive, such as
AdaBoost, are much more practical because they do not require such prior information. We show
therefore how to derive an adaptive boosting algorithm by modifying one of the game-theoretic
strategies.
We present experiments aimed at testing the efficacy of the new methods when working with a very
weak weak-learner to check that the conditions we have identified are indeed weaker than others that
had previously been used. We find that our new adaptive strategy achieves low test error compared
to other multiclass boosting algorithms which usually heavily underfit. This validates the potential
practical benefit of a better theoretical understanding of multiclass boosting.
Previous work. The first boosting algorithms were given by Schapire [15] and Freund [6], followed
by their AdaBoost algorithm [11]. Multiclass boosting techniques include AdaBoost.M1 and AdaBoost.M2 [11], as well as AdaBoost.MH and AdaBoost.MR [19]. Other approaches include [5, 21].
There are also more general approaches that can be applied to boosting including [2, 3, 4, 12]. Two
game-theoretic perspectives have been applied to boosting. The first one [10, 14] views the weak-

2

learning condition as a minimax game, while drifting games [16, 6] were designed to analyze the
most efficient boosting algorithms. These games have been further analyzed in the multiclass and
continuous time setting in [8].

2

Framework

We introduce some notation. Unless otherwise stated, matrices will be denoted by bold capital letters
like M, and vectors by bold small letters like v. Entries of a matrix and vector will be denoted as
M (i, j) or v(i), while M(i) will denote the ith row of a matrix. Inner product of two vectors u, v
is denoted by hu, vi. The Frobenius inner product of two matrices Tr(MM0 ) will be denoted by
M ? M0 . The indicator function is denoted by 1 [?]. The distribution over the set {1, . . . , k} will be
denoted by ? {1, . . . , k}.
In multiclass classification, we want to predict the labels of examples lying in some set X. Each
example x ? X has a unique y label in the set {1, . . . , k}, where k ? 2. We are provided a training
set of labeled examples {(x1 , y1 ), . . . , (xm , ym )}.
Boosting combines several mildly powerful predictors, called weak classifiers, to form a highly
accurate combined classifier, and has been previously applied for multiclass classification. In this
paper, we only allow weak classifier that predict a single class for each example. This is appealing,
since the combined classifier has the same form, although it differs from what has been used in much
previous work.
We adopt a game-theoretic view of boosting. A game is played between two players, Booster and
Weak-Learner, for a fixed number of rounds T . With binary labels, Booster outputs a distribution
in each round, and Weak-Learner returns a weak classifier achieving more than 50% accuracy on
that distribution. The multiclass game is an extension of the binary game. In particular, in each
round t: (1) Booster creates a cost-matrix Ct ? Rm?k , specifying to Weak-Learner that the cost
of classifying example xi as l is C(i, l). The cost-matrix may not be arbitrary, but should conform
to certain restrictions as discussed below. (2) Weak-Learner returns some weakP
classifier ht : X ?
m
{1, . . . , k} from a fixed space ht ? H so that the cost incurred is Ct ? 1ht = i=1 Ct (i, ht (xi )),
is ?small enough?, according to some conditions discussed below. Here by 1h we mean the m ? k
matrix whose (i, j)-th entry is 1 [h(i) = j]. (3) Booster computes a weight ?t for the current weak
classifier based on how much cost was incurred in this round.
At the end, Booster predicts according to the weighted plurality vote of the classifiers returned in
each round:
T
X
M
M
1 [ht (x) = l] ?t .
(1)
H(x) = argmax fT (x, l), where fT (x, l) =
l?{1,...,k}

t=1

By carefully choosing the cost matrices in each round, Booster aims to minimize the training error
of the final classifer H, even when Weak-Learner is adversarial. The restrictions on cost-matrices
created by Booster, and the maximum cost Weak-Learner can suffer in each round, together define
the weak-learning condition being used. For binary labels, the traditional weak-learning condition
states: for any non-negative weights w(1),
P . . . , w(m) on the training set, the error of the weak
classfier returned is at most (1/2 ? ?/2) i wi . Here ? parametrizes the condition. There are many
ways to translate this condition into our language. The one with fewest restrictions on the costmatrices requires labeling correctly should be less costly than labeling incorrectly: ?i : C(i, yi ) ?
C(i, y?i ), while
on the
 returned
 weak classifier h
 requires	 less cost than predicting
P the restriction P
randomly: i C(i, h(xi )) ? i 12 ? ?2 C(i, y?i ) + 21 + ?2 C(i, yi ) . By the correspondence
w(i) = C(i, y?i ) ? C(i, yi ), we may verify the two conditions are the same.
We will rewrite this condition after making some simplifying assumptions. Henceforth, without
loss of generality, we assume that the true label is always 1. Let C bin ? Rm?2 consist of matrices
m?2
C which satisfy C(i, 1) ? C(i, 2). Further, let Ubin
be the matrix whose each row is
? ? R
(1/2 + ?/2, 1/2 ? ?/2). Then, Weak-Learner searching
space H satisfies the binary weak-learning

condition if: ?C ? C bin , ?h ? H : C ? 1h ? Ubin
? 0. There are two main benefits to this refor?
mulation. With linear homogeneous constraints, the mathematics is simplified, as will be apparent
later. More importantly, by varying the restrictions C bin on the cost vectors and the matrix Ubin , we
can generate a vast variety of weak-learning conditions for the multiclass setting k ? 2 as we now
show.
3

Let C ? Rm?k and matrix B ? Rm?k , which we call the baseline; we say a weak classifier space
H satisfies the condition (C, B) if
?C ? C, ?h ? H : C ? (1h ? B) ? 0,

i.e.,

m
X

c(i, h(i)) ?

i=1

m
X

hc(i), B(i)i .

(2)

i=1

In (2), the variable matrix C specifies how costly each misclassification is, while the baseline B
specifies a weight for each misclassification. The condition therefore states that a weak classifier should not exceed the average cost when weighted according to baseline B. This large class
of weak-learning conditions captures many previously used conditions, such as the ones used by
AdaBoost.M1 [9], AdaBoost.MH [19] and AdaBoost.MR [9, 19] (see below), as well as novel conditions introduced in the next section.
By studying this vast class of weak-learning conditions, we hope to find the one that will serve the
main purpose of the boosting game: finding a convex combination of weak classifiers that has zero
training error. For this to be possible, at the minimum the weak classifiers should be sufficiently rich
for such a perfect combination to exist. Formally, a collection H of weak classifiers is eligible for
boosting, or simply boostable,
P if there exists a distribution ? on this space that linearly separates the
data: ?i : argmaxl?{1,...,k} h?H ?(h)1 [h(xi ) = l] = yi . The weak-learning condition plays two
roles. It rejects spaces that are not boostable, and provides an algorithmic means of searching for the
right combination. Ideally, the second factor will not cause the weak-learning condition to impose
additional restrictions on the weak classifiers; in that case, the weak-learning condition is merely a
reformulation of being boostable that is more appropriate for deriving an algorithm. In general, it
could be too strong, i.e. certain boostable spaces will fail to satisfy the conditions. Or it could be too
weak i.e., non-boostable spaces might satisfy such a condition. Booster strategies relying on either
of these conditions will fail to drive down error; the former due to underfitting, and the latter due
to overfitting. In the next section we will describe conditions captured by our framework that avoid
being too weak or too strong.

3

Necessary and sufficient weak-learning conditions

The binary weak-learning condition has an appealing form: for any distribution over the examples,
the weak classifier needs to achieve error not greater than that of a random player who guesses
the correct answer with probability 1/2 + ?. Further, this is the weakest condition under which
boosting is possible as follows from a game-theoretic perspective [10, 14] . Multiclass weak-learning
conditions with similar properties are missing in the literature. In this section we show how our
framework captures such conditions.
In the multiclass setting, we model a random player as a baseline predictor B ? Rm?k whose rows
are distributions over the labels, B(i) ? ? {1, . . . , k}. The prediction on example i is a sample from
B(i). We only consider the space of edge-over-random baselines B?eor ? Rm?k who have a faint
clue about the correct answer. More precisely, any baseline B ? B?eor in this space is ? more likely
to predict the correct label than an incorrect one on every example i: ?l 6= 1, B(i, 1) ? B(i, l) + ?,
with equality holding for some l.
When k = 2, the space B?eor consists of the unique player Ubin
? , and the binary weak-learning
bin
bin
condition is given by (C , U? ). The new conditions generalize this to k > 2. In particular, define
C eor to be the multiclass extension of C bin : any cost-matrix in C eor should
 put the least cost on the
	
correct label, i.e., the rows of the cost-matrices should come from the set c ? Rk : ?l, c(1) ? c(l) .
Then, for every baseline B ? B?eor , we introduce the condition (C eor , B), which we call an edgeover-random weak-learning condition. Since C ? B is the expected cost of the edge-over-random
baseline B on matrix C, the constraints (2) imposed by the new condition essentially require better
than random performance.
We now present the central results of this section. The seemingly mild edge-over-random conditions
guarantee eligibility, meaning weak classifiers that satisfy any one such condition can be combined
to form a highly accurate combined classifier.
Theorem 1 (Sufficiency). If a weak classifier space H satisfies a weak-learning condition (C eor , B),
for some B ? B?eor , then H is boostable.
4

The proof involves the Von-Neumann Minimax theorem, and is in the spirit of the ones in [10]. On
the other hand the family of such conditions, taken as a whole, is necessary for boostability in the
sense that every eligible space of weak classifiers satisfies some edge-over-random condition.
Theorem 2 (Relaxed necessity). For every boostable weak classifier space H, there exists a ? > 0
and B ? B?eor such that H satisfies the weak-learning condition (C eor , B).
The proof shows existence through non-constructive averaging arguments. Theorem 2 states that
any boostable weak classifier space will satisfy some condition in our family,
 but it does not help
us choose the right condition. Experiments in Section 5 suggest C eor , U? is effective with very
simple weak-learners compared to popular boosting algorithms. (Here U? ? B?eor is the edge-overrandom baseline closest to uniform; it has weight (1 ? ?)/k on incorrect labels and (1 ? ?)/k + ?
on the correct label.) However, there are theoretical examples showing each condition in our family
is too strong (supplement).
A perhaps extreme way of weakening the condition is by requiring the performance on a cost matrix
to be competitive not with a fixed baseline B ? B?eor , but with the worst of them:
?C ? C eor , ?h ? H : C ? 1h ? maxeor C ? B.
B?B?

(3)

Condition (3) states that during the course of the same boosting game, Weak-Learner may choose
to beat any edge-over-random baseline B ? B?eor , possibly a different one for every round and every
cost-matrix. This may superficially seem much too weak. On the contrary, this condition turns out
to be equivalent to boostability. In other words, according to our criterion, it is neither too weak nor
too strong as a weak-learning condition. However, unlike the edge-over-random conditions, it also
turns out to be more difficult to work with algorithmically.
Furthermore, this condition can be shown to be equivalent to the one used by AdaBoost.MR [19, 9].
This is perhaps remarkable since the latter is based on the apparently completely unrelated all-pairs
MR
consists of
multiclass to binary reduction: the MR condition is given by (C MR , BMR
? ), where C
cost-matrices that put non-negative costs on incorrect labels and whose rows sum up to zero, while
m?k
BMR
is the matrix that has ? on the first column and ?? on all other columns(supplement).
? ?R
Further, the MR condition, and hence (3), can be shown to be neither too weak nor too strong.
Theorem 3 (MR). A weak classifier space H satisfies AdaBoost.MR?s weak-learning condition
(C MR , BMR
? ) if and only if it satisfies (3). Moreover, this condition is equivalent to being boostable.
Next, we illustrate the strengths of our random-over-edge weak-learning conditions through concrete
comparisons with previous algorithms.
Comparison with SAMME. The SAMME algorithm of [21] requires the weak classifiers to
achieve less error than uniform random guessing for multiple labels; in our language, their weaklearning condition is (C = {(?t, t, t, . . .) : t ? 0} , U? ). As is well-known, this condition is
not sufficient for boosting to be possible. In particular, consider the dataset {(a, 1), (b, 2)} with
k = 3, m = 2, and a weak classifier space consisting of h1 , h2 which always predict 1, 2, respectively. Since neither classifier distinguishes between a, b we cannot achieve perfect accuracy by
combining them in any way. Yet, due to the constraints on the cost-matrix, one of h1 , h2 will always
manage non-positive cost while random always suffers positive cost. On the other hand our weaklearning condition allows the Booster to choose far richer cost matrices. In particular, when the
cost matrix is C = (c(1) = (?1, +1, 0), c(2) = (+1, ?1, 0)) ? C eor , both classifiers in the above
example suffer more loss than the random player U? , and fail to satisfy our condition.
Comparison with AdaBoost.MH. AdaBoost.MH is a popular multiclass boosting algorithm that is
based on the one-against-all reduction[19]. However, we show that its implicit demands on the weak
classifier space is too strong. We construct a classifier space that satisfies the condition (C eor , U? )
in our family, but cannot satisfy AdaBoost.MH?s weak-learning condition.
Consider a space H that has, for every (1/k + ?)m element subset of the examples, a classifier
that predicts correctly on exactly those elements. The expected loss of a randomly chosen classifier
from this space is the same as that of the random player U? . Hence H satisfies this weak-learning
condition. On the other hand, it can be shown (supplement) that AdaBoost.MH?s weak-learning
MH
condition is the pair (C MH , BMH
has non-(positive)negative entries on (in)correct labels,
? ), where C
and where each row of the matrix BMH
is
the
vector (1/2 + ?/2, 1/2 ? ?/2, . . . , 1/2 ? ?/2). A
?
5

quick calculation shows that for any h ? H, and C ? C MH with ?1 in the first column and zeroes
elsewhere, C ? 1h ? BMH
= 1/2 ? 1/k. This is positive when k > 2, so that H fails to satisfy
?
AdaBoost.MH?s condition.

4

Algorithms

In this section we devise algorithms by analyzing the boosting games that employ our edge-overrandom weak-learning conditions. We compute the optimum Booster strategy against a completely
adversarial Weak-Learner, which here is permitted to choose weak classifiers without restriction,
i.e. the entire space Hall of all possible functions mapping examples to labels. By modeling WeakLearner adversarially, we make absolutely no assumptions on the algorithm it might use. Hence,
error guarantees enjoyed in this situation will be universally applicable. Our algorithms are derived
from the very general drifting games framework [16] for solving boosting games, in turn inspired
by Freund?s Boost-by-majority algorithm [6], which we review next.
The OS Algorithm. Fix the number of rounds T and an edge-over-random weak-learning condition
(C, B). For simplicity of presentation we fix the weights ?t = 1 in each round. With fT defined as
in (1), the optimum Booster payoff can be written as
m
X
min
max
. . . min
max
(1/m)
L(fT (xi , 1), fT (xi , 2), . . . , fT (xi , k)).
C1 ?C

h1 ?Hall :
C1 ?(1h1 ?B)?0

CT ?C

hT ?Hall :
CT ?(1hT ?B)?0

i=1

Here the function L : Rk ? R is error, but we can also consider other loss functions such as
exponential loss, hinge loss, etc. that upper-bound error and are proper: i.e. L(x) is increasing in
the weight of the correct label x(1), and decreasing in the weights of the incorrect labels x(l), l 6= 1.
Directly analyzing the optimal payoff is hard. However, Schapire [16] observed that the payoffs
can be very well approximated by certain potential functions. Indeed, for any b ? Rk define the
k
potential function ?b
t : R ? R by the following recurrence:



	
?b
?b
min
max
El?p ?b
0 = L;
t (s) =
t?1 (s + el ) : El?p [c(l)] ? hb, ci , (4)
c?Rk :?l:c(1)?c(l) p??{1,...,k}

where el ? Rk is the unit-vector whose lth coordinate is 1 and the remaining coordinates zero.
These potential functions compute an estimate ?b
t (st ) of whether an example x will be misclassified,
based on its current state st consisting of counts of votes received so far on various classes st (l) =
Pt?1
0
t0 =1 1 [ht (x) = l], and the number of rounds t remaining. Using these functions, Schapire [16]
proposed a Booster strategy, aka the OS strategy, which, in round t, constructs a cost matrix C ? C,
whose each row C(i) achieves the minimum of the right hand side of (4) with b replaced by B(i), t
replaced by T ? t, and s replaced by current state st (i). The following theorem provides a guarantee
for the loss suffered by the OS algorithm, and also shows that it is the game-theoretically optimum
strategy when the number of examples is large.
Theorem 4 (Extension of results in [16]). Suppose the weak-learning condition is given by (C, B), If
Pm B(i)
Booster employs the OS algorithm, then the average potential of the states (1/m) i=1 ?t (s(i))
never increases in any round. In particular, loss suffered after T rounds of play is at most
Pm B(i)
(1/m) i=1 ?T (0). Further, for any  > 0, when the loss function satisfies some mild conditions, and m  T, k, 1/, no Booster strategy can achieve loss  less than the above bound in T
rounds.
Computing the potentials. In order to implement the OS strategy using our weak-learning conditions, we only need to compute the potential ?b
t for distributions b ? ? {1, . . . , k}. Fortunately,
these potentials have a very simple solution in terms of the homogeneous random-walk Rtb (x), the
random position of a particle after t time steps, that starts at location x ? Rk , and in each step moves
in direction el with probability b(l).
Theorem 5. If L is proper, and b ? ? {1, . . . , k} satisfies ?l : b(1) ? b(l), then ?b
t (s) =
E [L (Rtb (s))]. Furthermore, the vector achieving the minimum in the right hand side of (4) is
given by c(l) = ?b
t?1 (s + el ).
Theorem (5) implies the OS strategy chooses the following cost matrix in round t: c(i, l) =
b(i)
?T ?t?1 (st (i) + el ), where st (i) is the state of example i in round t. Therefore everything boils
6

down to computing the potentials, which is made possible by Theorem 5. There is no simple closed
form solution for the non-convex 0-1 loss L(s) = 1[s1 ? (maxi>1 si )]. However, using Theorem 4, we can write the potential ?t (s) explicitly, and then compute it using dynamic programming
in O(t3 k) time. This yields very tight bounds.
To obtain a more efficient procedure, and one that we will soon show can be made adaptive, we next
focus on the exponential loss associated with AdaBoost that does have a closed form solution.
Lemma 1. If L(s) = exp(?2 (s2 ? s1 )) + ? ? ? + exp(?k (sk ? s1 )), where each ?l is positive, then
Pk
t ?l (sl ?s1 )
the solution in Theorem 5 evaluates to ?b
, where al = 1 ? (b1 + bl ) +
t (s) =
l=2 (al ) e
?l
??l
e bl + e b1 .
The proof by induction is straightforward. In particular, when the condition is (C eor , U? ) and
Pk
? = (?, ?, . . .), the relevant potential is ?t (s) = ?(?, ?)t l=2 e?(sl ?s1 ) where ?(?, ?) =
1 + (1??)
(e? + e?? ? 2) ? (1 ? e?? ) ?. The cost-matrix output by the OS algorithm can be
k
simplified by rescaling, or adding the same number to each coordinate of a cost vector, without
affecting the constraints it imposes on a weak classifier, to the following form
(
(e? ? 1) e?(sl ?s1 )
if l > 1,
Pk
c(i, l) =
(5)
(e?? ? 1) j=2 e?(sj ?s1 ) if l = 1,
With such
Pm a choice, Theorem 4 and the form of the potential guarantee that the average loss
(1/m) i=1 L(st (i)) of the states st (i) changes by a factor of at most ? (?, ?) every round. Hence
T
the final loss is at most (k ? 1)? (?, ?) .
Variable edges. So far we have required Weak-Learner to beat random by at least a fixed amount
? > 0 in each round of the boosting game. In reality, the edge over random is larger initially,
and gets smaller as the OS algorithm creates harder cost matrices. Therefore requiring a fixed
edge is either unduly pessimistic or overly optimistic. If the fixed edge is too small, not enough
progress is made in the initial rounds, and if the edge is too large, Weak-Learner fails to meet the
weak-learning condition in latter rounds. We attempt to fix this via two approaches: prescribing a
decaying sequence of edges ?1 , . . . , ?T , or being completely flexible, aka adaptive, with respect to
the edges returned by the weak-learner. In either case, we only use the edge-over-random condition
(C eor , U? ), but with varying values of ?.
Fixed sequence of edges. With a prescribed sequence of edges ?1 , . . . , ?T the weak-learning condition (C eor , U?t ) in each round t is different. We allow the weights ?1 , . . . , ?T to be arbitrary, but they
must be fixed in advance. All the results for uniform ? and weights ?t = 1 hold in this case as well.
Pm Pk
In particular, by the arguments leading to (5), if we want to minimize i=1 l=2 e{ft (i,l)?ft (i,1)} ,
where ft is as defined in (1), then the following strategy is optimal: in round t output the cost matrix
(
if l > 1,
(e?t ? 1) eft?1 (i,j)?ft?1 (i,1)
Pk
C(i, l) =
(6)
(e??t ? 1) j=2 eft?1 (i,j)?ft?1 (i,1) if l = 1.
Pm Pk
This will ensure that the expression i=1 l=2 e{ft (i,l)?ft (i,1)} changes by a factor of at most
QT
?(?t , ?t ) in each round. Hence the final loss will be at most (k ? 1) t=1 ?(?t , ?t ).
Adaptive. In the adaptive setting, we depart from the game-theoretic framework in that WeakLearner is no longer adversarial. Further, we are no longer guaranteed to receive a certain sequence
of edges. Since the choice of cost-matrix in (6) does not depend on the edges, we could fix an
arbitrary set of weights ?t in advance, follow the same algorithm as before and enjoy the same bound
QT
t=1 ?(?t , ?t ). The trouble with this is ?(?t , ?t ) is not less than 1 unless ?t is small compared to
?t . To ensure progress, the weight ?t must be chosen adaptively as a function of ?t . Since we do not
know what edge we will receive, we choose the cost matrix as before but anticipating infinitesimally
small edge, in the spirit of [7], (and with some rescaling)
(
(e? ? 1) eft?1 (i,j)?ft?1 (i,1)
if l > 1,
M 1
Pk
C(i, l) = lim C? (i, l) =
??
f
(i,j)?f
(i,1)
t?1
t?1
??0
? (e ? 1) j=2 e
if l = 1.
(
eft?1 (i,j)?ft?1 (i,1)
if l > 1,
Pk
=
(7)
? j=2 eft?1 (i,j)?ft?1 (i,1) if l = 1.
7

pendigits

100

500

5

20

100

500

0.20

0.20

0.08

0.30

0.14

0.3
0.1
0.0

20

satimage

0.40

0.8
0.4

0.5
0.3
5

poker

0.50

letter
0.5

forest
0.7

0.30 0.35 0.40

connect4

5

20

100

500

5

20 50

200

5

20

100

500

5

20 50

200

(a)

300

500

pendigits

poker

satimage
0.10 0.15 0.20 0.25

0.5

1.0

0 100

300

500

0.50

0.3

0.6
0 100

300

500

0.40

0.1

0.4

0.4

0.6

0.36
0.32
0 100

letter

0.8

1.0

forest

0.8

0.40

connect4

0 100

300

500

0 100

300

500

0 100

300

500

(b)
Figure 1: Figure 1(a) plots the final test-errors of M1(black, dashed), MH(blue, dotted) and New method(red,
solid) against the maximum tree-sizes allowed as weak classifiers. Figure 1(b) plots how fast the test-errors of
these algorithms drop with rounds, when the maximum tree-size allowed is 5.

Since Weak-Learner cooperates, we expect the edge ?t of the returned classifier ht on the supplied
cost-matrix lim??0 C? to be more than just infinitesimal. In that case, by continuity, there are noninfinitesimal choices of the weight ?t such that the edge ?t achieved by ht on the cost-matrix C?t
remains large enough to ensure ?(?t , ?t ) < 1. In fact, with any choice of ?t , we
 get ?(?t , ?t ) ?
1+?t
1
1
1
?t
??t
?t
??t
) ?t + 2 (e + e
? 2) (supplement). Tuning ?t to 2 ln 1??
1 ? 2 (e ? e
results in
t
p
2
loss, o
and hence error, after
? (?t , ?t ) ? 1 ? ?t . This algorithm is adaptive, and ensures
n that the
QT p
PT
2
2
T rounds is at most (k ? 1) t=1 1 ? ?t ? (k ? 1) exp ?(1/2) t=1 ?t .

5

Experiments

We report preliminary experimental results on six, varying multiclass UCI datasets.

0.0

0.1

0.2

0.3

0.4

The first set of experiments were aimed at determining
overall performance of our new algorithm. We compared
MH
a standard implementation M1 of AdaBoost.M1 with C4.5
M1
New Method
as weak learner, and the Boostexter implementation MH
of AdaBoost.MH using stumps [20], with the adaptive
algorithm described in Section 4, which we call New
method, using a naive greedy tree-searching algorithm
Greedy for weak-learner. The size of trees was chosen
to be of the same order as the tree sizes used by M1. Test
errors after 500 rounds of boosting are plotted in Figure 2.
The performance is comparable with M1 and far better
than MH (understandably since stumps are far weaker than
trees), even though our weak-learner is very naive com- Figure 2: This is a plot of the final test-errors
of standard implementations of M1, MH and
pared to C4.5.
connect4

forest

letter

pendigits

poker

satimage

New method after 500 rounds of boosting.

We next investigated how each algorithm performs with
less powerful weak-classifiers, namely, decision trees whose size has been sharply limited to various
pre-specified limits. Figure 1(a) shows test-error plotted as a function of tree size. As predicted by
our theory, our algorithm succeeds in boosting the accuracy even when the tree size is too small
to meet the stronger weak learning assumptions of the other algorithms. The differences in performance are particularly strong when using the smallest tree sizes.
More insight is provided by plots in Figure 1(b) of the rate of convergence of test error with rounds
when the tree size allowed is very small (5). Both M1 and MH drive down the error for a few rounds.
But since boosting keeps creating harder cost-matrices, very soon the small-tree learning algorithms
are no longer able to meet the excessive requirements of M1 and MH. However, our algorithm makes
more reasonable demands that are easily met by the weak learner.
8

References
[1] Jacob Abernethy, Peter L. Bartlett, Alexander Rakhlin, and Ambuj Tewari. Optimal stragies and minimax lower bounds for online convex games. In Proceedings of the Nineteenth Annual Conference on
Computational Learning Theory, pages 415?424, 2008.
[2] Erin L. Allwein, Robert E. Schapire, and Yoram Singer. Reducing multiclass to binary: A unifying
approach for margin classifiers. Journal of Machine Learning Research, 1:113?141, 2000.
[3] Alina Beygelzimer, John Langford, and Pradeep Ravikumar. Error-correcting tournaments. In Algorithmic Learning Theory: 20th International Conference, pages 247?262, 2009.
[4] Thomas G. Dietterich and Ghulum Bakiri. Solving multiclass learning problems via error-correcting
output codes. Journal of Artificial Intelligence Research, 2:263?286, January 1995.
[5] G?unther Eibl and Karl-Peter Pfeiffer. Multiclass boosting for weak classifiers. Journal of Machine Learning Research, 6:189?210, 2005.
[6] Yoav Freund. Boosting a weak learning algorithm by majority.
121(2):256?285, 1995.

Information and Computation,

[7] Yoav Freund. An adaptive version of the boost by majority algorithm. Machine Learning, 43(3):293?318,
June 2001.
[8] Yoav Freund and Manfred Opper. Continuous drifting games. Journal of Computer and System Sciences,
pages 113?132, 2002.
[9] Yoav Freund and Robert E. Schapire. Experiments with a new boosting algorithm. In Machine Learning:
Proceedings of the Thirteenth International Conference, pages 148?156, 1996.
[10] Yoav Freund and Robert E. Schapire. Game theory, on-line prediction and boosting. In Proceedings of
the Ninth Annual Conference on Computational Learning Theory, pages 325?332, 1996.
[11] Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an
application to boosting. Journal of Computer and System Sciences, 55(1):119?139, August 1997.
[12] Trevor Hastie and Robert Tibshirani. Classification by pairwise coupling. Annals of Statistics, 26(2):451?
471, 1998.
[13] V. Koltchinskii and D. Panchenko. Empirical margin distributions and bounding the generalization error
of combined classifiers. Annals of Statistics, 30(1), February 2002.
[14] Gunnar R?atsch and Manfred K. Warmuth. Efficient margin maximizing with boosting. Journal of Machine
Learning Research, 6:2131?2152, 2005.
[15] Robert E. Schapire. The strength of weak learnability. Machine Learning, 5(2):197?227, 1990.
[16] Robert E. Schapire. Drifting games. Machine Learning, 43(3):265?291, June 2001.
[17] Robert E. Schapire. The boosting approach to machine learning: An overview. In MSRI Workshop on
Nonlinear Estimation and Classification, 2002.
[18] Robert E. Schapire, Yoav Freund, Peter Bartlett, and Wee Sun Lee. Boosting the margin: A new explanation for the effectiveness of voting methods. Annals of Statistics, 26(5):1651?1686, October 1998.
[19] Robert E. Schapire and Yoram Singer. Improved boosting algorithms using confidence-rated predictions.
Machine Learning, 37(3):297?336, December 1999.
[20] Robert E. Schapire and Yoram Singer. BoosTexter: A boosting-based system for text categorization.
Machine Learning, 39(2/3):135?168, May/June 2000.
[21] Ji Zhu, Hui Zou, Saharon Rosset, and Trevor Hastie. Multi-class AdaBoost. Statistics and Its Interface,
2:349360, 2009.

9


----------------------------------------------------------------

title: 4745-a-scalable-cur-matrix-decomposition-algorithm-lower-time-complexity-and-tighter-bound.pdf

A Scalable CUR Matrix Decomposition Algorithm:
Lower Time Complexity and Tighter Bound

Shusen Wang and Zhihua Zhang
College of Computer Science & Technology
Zhejiang University
Hangzhou, China 310027
{wss,zhzhang}@zju.edu.cn

Abstract
The CUR matrix decomposition is an important extension of Nystr?om approximation to a general matrix. It approximates any data matrix in terms of a small number of its columns and rows. In this paper we propose a novel randomized CUR
algorithm with an expected relative-error bound. The proposed algorithm has the
advantages over the existing relative-error CUR algorithms that it possesses tighter
theoretical bound and lower time complexity, and that it can avoid maintaining the
whole data matrix in main memory. Finally, experiments on several real-world
datasets demonstrate significant improvement over the existing relative-error algorithms.

1

Introduction

Large-scale matrices emerging from stocks, genomes, web documents, web images and videos everyday bring new challenges in modern data analysis. Most efforts have been focused on manipulating, understanding and interpreting large-scale data matrices. In many cases, matrix factorization
methods are employed to construct compressed and informative representations to facilitate computation and interpretation. A principled approach is the truncated singular value decomposition
(SVD) which finds the best low-rank approximation of a data matrix. Applications of SVD such as
eigenface [20, 21] and latent semantic analysis [4] have been illustrated to be very successful.
However, the basis vectors resulting from SVD have little concrete meaning, which makes it very
difficult for us to understand and interpret the data in question.
An example in [10, 19] has well
?
shown this viewpoint; that is, the vector [(1/2)age ? (1/ 2)height + (1/2)income], the sum of the
significant uncorrelated features from a dataset of people?s features, is not particularly informative.
The authors of [17] have also claimed: ?it would be interesting to try to find basis vectors for all
experiment vectors, using actual experiment vectors and not artificial bases that offer little insight.?
Therefore, it is of great interest to represent a data matrix in terms of a small number of actual
columns and/or actual rows of the matrix.
The CUR matrix decomposition provides such techniques, and it has been shown to be very useful
in high dimensional data analysis [19]. Given a matrix A, the CUR technique selects a subset of
columns of A to construct a matrix C and a subset of rows of A to construct a matrix R, and
? = CUR best approximates A. The typical CUR algorithms [7,
computes a matrix U such that A
8, 10] work in a two-stage manner. Stage 1 is a standard column selection procedure, and Stage 2
does row selection from A and C simultaneously. Thus Stage 2 is more complicated than Stage 1.
The CUR matrix decomposition problem is widely studied in the literature [7, 8, 9, 10, 12, 13, 16,
18, 19, 22]. Perhaps the most widely known work on the CUR problem is [10], in which the authors
devised a randomized CUR algorithm called the subspace sampling algorithm. Particularly, the
algorithm has (1 + ?) relative-error ratio with high probability (w.h.p.).
1

Unfortunately, all the existing CUR algorithms require a large number of columns and rows to be
chosen. For example, for an m ? n matrix A and a target rank k ? min{m, n}, the state-ofthe-art CUR algorithm ? the subspace sampling algorithm in [10] ? requires exactly O(k 4 ??6 )
rows or O(k??4 log2 k) rows in expectation to achieve (1 + ?) relative-error ratio w.h.p. Moreover,
the computational cost of this algorithm is at least the cost of the truncated SVD of A, that is,
O(min{mn2 , nm2 }).1 The algorithms are therefore impractical for large-scale matrices.
In this paper we develop a CUR algorithm which beats the state-of-the-art algorithm in both theory
and experiments. In particular, we show in Theorem 5 a novel randomized CUR algorithm with
lower time complexity and tighter theoretical bound in comparison with the state-of-the-art CUR
algorithm in [10].
The rest of this paper is organized as follows. Section 3 introduces several existing column selection
algorithms and the state-of-the-art CUR algorithm. Section 4 describes and analyzes our novel
CUR algorithm. Section 5 empirically compares our proposed algorithm with the state-of-the-art
algorithm.

2

Notations

For a matrix A = [aij ] ? Rm?n , let a(i) be its i-th row and aj be its j-th column. Let ?A?1 =
?
?
2 1/2
be the Frobenius norm, and ?A?2 be the spectral
i,j |aij | be the ?1 -norm, ?A?F = (
i,j aij )
norm. Moreover, let Im denote an m ? m identity matrix, and 0mn denotes an m ? n zero matrix.
??
T
T
T
T
= UA,k ?A,k VA,k
+ UA,k? ?A,k? VA,k?
be the
Let A = UA ?A VA
= i=0 ?A,i uA,i vA,i
SVD of A, where ? = rank(A), and UA,k , ?A,k , and VA,k correspond to the top k singular values.
T
T
We denote Ak = UA,k ?A,k VA,k
. Furthermore, let A? = UA,? ??1
A,? VA,? be the Moore-Penrose
inverse of A [1].

3 Related Work
Section 3.1 introduces several relative-error column selection algorithms related to this work. Section 3.2 describes the state-of-the-art CUR algorithm in [10]. Section 3.3 discusses the connection
between the column selection problem and the CUR problem.
3.1

Relative-Error Column Selection Algorithms

Given a matrix A ? Rm?n , column selection is a problem of selecting c columns of A to construct
C ? Rm?c to minimize ?A ? CC? A?F . Since there are (nc ) possible choices of constructing C,
so selecting the best subset is a hard problem. In recent years, many polynomial-time approximate
algorithms have been proposed, among which we are particularly interested in the algorithms with
relative-error bounds; that is, with c ? k columns selected from A, there is a constant ? such that
?A ? CC? A?F ? ??A ? Ak ?F .
We call ? the relative-error ratio. We now present some recent results related to this work.
We first introduce a recently developed deterministic algorithm called the dual set sparsification
proposed in [2, 3]. We show their results in Lemma 1. Furthermore, this algorithm is a building
block of some more powerful algorithms (e.g., Lemma 2), and our novel CUR algorithm also relies
on this algorithm. We attach the algorithm in Appendix A.
Lemma 1 (Column Selection via Dual Set Sparsification Algorithm). Given a matrix A ? Rm?n
of rank ? and a target rank k (< ?), there exists a deterministic algorithm to select c (> k) columns
of A and form a matrix C ? Rm?c such that
?




1



? 
?
? Ak 
 .

A ? CC A
 ? 1 +

A
F
F
(1 ? k/c)2
1
Although some partial SVD algorithms, such as Krylov subspace methods, require only O(mnk) time,
they are all numerical unstable. See [15] for more discussions.

2

Moreover, the matrix C can be computed in TVA,k +O(mn+nck 2 ), where TVA,k is the time needed
to compute the top k right singular vectors of A.
There are also a variety of randomized column selection algorithms achieving relative-error bounds
in the literature: [3, 5, 6, 10, 14].
An randomized algorithm in [2] selects only c = 2k
? (1 + o(1)) columns to achieve the expected
relative-error ratio (1 + ?). The algorithm is based on the approximate SVD via random projection [15], the dual set sparsification algorithm [2], and the adaptive sampling algorithm [6]. Here we
present the main results of this algorithm in Lemma 2. Our proposed CUR algorithm is motivated
by and relies on this algorithm.
Lemma 2 (Near-Optimal Column Selection Algorithm). Given a matrix A ? Rm?n of rank ?, a
target rank k (2 ? k < ?), and 0 < ? < 1, there exists a randomized algorithm to select at most
)
2k (
1 + o(1)
c=
?
columns of A to form a matrix C ? Rm?c such that
E2 ?A ? CC? A?F ? E?A ? CC? A?2F ? (1 + ?)?A ? Ak ?2F ,
where the expectations are taken w.r.t. C. Furthermore, the matrix C can be computed in O((mnk+
nk 3 )??2/3 ).
3.2

The Subspace Sampling CUR Algorithm

Drineas et al. [10] proposed a two-stage randomized CUR algorithm which has a relative-error
bound w.h.p. Given a matrix A ? Rm?n and a target rank k, in the first stage the algorithm
chooses exactly c = O(k 2 ??2 log ? ?1 ) columns (or c = O(k??2 log k log ? ?1 ) in expectation) of
A to construct C ? Rm?c ; in the second stage it chooses exactly r = O(c2 ??2 log ? ?1 ) rows (or
r = O(c??2 log c log ? ?1 ) in expectation) of A and C simultaneously to construct R and U. With
probability at least 1 ? ?, the relative-error ratio is 1 + ?. The computational cost is dominated by
the truncated SVD of A and C.
Though the algorithm is ?-optimal with high probability, it requires too many rows get chosen: at
least r = O(k??4 log2 k) rows in expectation. In this paper we seek to devise an algorithm with
mild requirement on column and row numbers.
3.3

Connection between Column Selection and CUR Matrix Decomposition

The CUR problem has a close connection with the column selection problem. As aforementioned,
the first stage of existing CUR algorithms is simply a column selection procedure. However, the
second stage is more complicated. If the second stage is na??vely solved by a column selection
algorithm on AT , then the error ratio will be at least (2 + ?).
For a relative-error CUR algorithm, the first stage seeks to bound a construction error ratio of
?A?CC? A?F
?A?CC? AR? R?F
given C. Actually, the first
?A?Ak ?F , while the section stage seeks to bound
?A?CC? A?F
stage is a special case of the second stage where C = Ak . Given a matrix A, if an algorithm solv?
AR? R?F
ing the second stage results in a bound ?A?CC
? ?, then this algorithm also solves the
?A?CC? A?F
T
column selection problem for A with an ? relative-error ratio. Thus the second stage of CUR is a
generalization of the column selection problem.

4

Main Results

In this section we introduce our proposed CUR algorithm. We call it the fast CUR algorithm because
it has lower time complexity compared with SVD. We describe it in Algorithm 1 and give a theoretical analysis in Theorem 5. Theorem 5 relies on Lemma 2 and Theorem 4, and Theorem 4 relies on
Theorem 3. Theorem 3 is a generalization of [6, Theorem 2.1], and Theorem 4 is a generalization
of [2, Theorem 5].
3

Algorithm 1 The Fast CUR Algorithm.

(
)
1: Input: a real matrix A ? Rm?n , target rank k, ? ? (0, 1], target column number c = 2k
1 + o(1) , target
?
(
)
row number r = 2c
1 + o(1) ;
?
2: // Stage 1: select c columns of A to construct C ? Rm?c
? k?
? kV
? k;
3: Compute approximate truncated SVD via random projection such that Ak ? U
? k?
? kV
? k ); V1 ? columns of V
? kT ;
4: Construct U1 ? columns of (A ? U
5: Compute s1 ? Dual Set Spectral-Frobenius Sparsification Algorithm (U1 , V1 , c ? 2k/?);
6: Construct C1 ? ADiag(s1 ), and then delete the all-zero columns;
7: Residual matrix D ? A ? C1 C?1 A;
8: Compute sampling probabilities: pi = ?di ?22 /?D?2F , i = 1, ? ? ? , n;
9: Sampling c2 = 2k/? columns from A with probability {p1 , ? ? ? , pn } to construct C2 ;
10: // Stage 2: select r rows of A to construct R ? Rr?n
? k?
? kV
? k )T ; V2 ? columns of U
? Tk ;
11: Construct U2 ? columns of (A ? U
12: Compute s2 ? Dual Set Spectral-Frobenius Sparsification Algorithm (U2 , V2 , r ? 2c/?);
13: Construct R1 ? Diag(s2 )A, and then delete the all-zero rows;
14: Residual matrix B ? A ? AR?1 R1 ; Compute qj = ?b(j) ?22 /?B?2F , j = 1, ? ? ? , m;
15: Sampling r2 = 2c/? rows from A with probability {q1 , ? ? ? , qm } to construct R2 ;
16: return C = [C1 , C2 ], R = [RT1 , RT2 ]T , and U = C? AR? .

4.1

Adaptive Sampling

The relative-error adaptive sampling algorithm is established in [6, Theorem 2.1]. The algorithm
is based on the following idea: after selecting a proportion of columns from A to form C1 by
an arbitrary algorithm, the algorithms randomly samples additional c2 columns according to the
residual A ? C1 C?1 A. Boutsidis et al. [2] used the adaptive sampling algorithm to decrease the
residual of the dual set sparsification algorithm and obtained an (1 + ?) relative-error bound. Here
we prove a new bound for the adaptive sampling algorithm. Interestingly, this new bound is a
generalization of the original one in [6, Theorem 2.1]. In other words, Theorem 2.1 of [6] is a direct
corollary of our following theorem in which C = Ak is set.
Theorem 3 (The Adaptive Sampling Algorithm). Given a matrix A ? Rm?n and a matrix C ?
Rm?c such that rank(C) = rank(CC? A) = ?, (? ? c ? n), we let R1 ? Rr1 ?n consist of r1
rows of A, and define the residual B = A ? AR?1 R1 . Additionally, for i = 1, ? ? ? , m, we define
pi = ?b(i) ?22 /?B?2F .
We further sample r2 rows i.i.d. from A, in each trial of which the i-th row is chosen with probability
pi . Let R2 ? Rr2 ?n contains the r2 sampled rows and let R = [RT1 , RT2 ]T ? R(r1 +r2 )?n . Then
the following inequality holds:
?
E?A ? CC? AR? R?2F ? ?A ? CC? A?2F + ?A ? AR?1 R1 ?2F ,
r2
where the expectation is taken w.r.t. R2 .
4.2

The Fast CUR Algorithm

Based on the dual set sparsification algorithm of of Lemma 1 and the adaptive sampling algorithm
of Theorem 3, we develop a randomized algorithm to solve the second stage of CUR problem. We
present the results of the algorithm in Theorem 4. Theorem 5 of [2] is a special case of the following
theorem where C = Ak .
Theorem 4 (The Fast Row Selection Algorithm). Given a matrix A ? Rm?n and a matrix C ?
Rm?c such that rank(C) = rank(CC? A) = ?, (? ? c ? n), and a target rank k (? ?), the
r?n
proposed randomized algorithm selects r = 2?
, such
? (1 + o(1)) rows of A to construct R ? R
that
E?A ? CC? AR? R?2F ? ?A ? CC? A?2F + ??A ? Ak ?2F ,
where the expectation is taken w.r.t. R. Furthermore, the matrix R can be computed in O((mnk +
mk 3 )??2/3 ) time.
Based on Lemma 2 and Theorem 4, here we present the main theorem for the fast CUR algorithm.
4

Table 1: A summary of the datasets.
Dataset
Type
size
Source
Redrocknatural image18000 ? 4000
http://www.agarwala.org/efficient gdc/
Arcene
biology
10000 ? 900 http://archive.ics.uci.edu/ml/datasets/Arcene
Dexter bag of words 20000 ? 2600http://archive.ics.uci.edu/ml/datasets/Dexter

Theorem 5 (The Fast CUR Algorithm). Given a matrix A ? Rm?n and a positive integer k ?
min{m, n}, the fast CUR algorithm (described in Algorithm 1) randomly selects c = 2k
? (1 + o(1))
columns of A to construct C ? Rm?c with the near-optimal column selection algorithm of Lemma 2,
r?n
and then selects r = 2c
with the fast row selection
? (1 + o(1)) rows of A to construct R ? R
algorithm of Theorem 4. Then we have
E?A ? CUR?F = E?A ? C(C? AR? )R?F ? (1 + ?)?A ? Ak ?F .
(
)
Moreover, the algorithm runs in time O mnk??2/3 + (m + n)k 3 ??2/3 + mk 2 ??2 + nk 2 ??4 .
Since k, c, r ? min{m, n} by the assumptions, so the time complexity of the fast CUR algorithm
is lower than that of the SVD of A. This is the main reason why we call it the fast CUR algorithm.
Another advantage of this algorithm is avoiding loading the whole m ? n data matrix A into main
memory. None of three steps ? the randomized SVD, the dual set sparsification algorithm, and the
adaptive sampling algorithm ? requires loading the whole of A into memory. The most memoryexpensive operation throughout the fast CUR Algorithm is computing the Moore-Penrose inverse
of C and R, which requires maintaining an m ? c matrix or an r ? n matrix in memory. In
comparison, the subspace sampling algorithm requires loading the whole matrix into memory to
compute its truncated SVD.

5

Empirical Comparisons

In this section we provide empirical comparisons among the relative-error CUR algorithms on several datasets. We report the relative-error ratio and the running time of each algorithm on each data
set. The relative-error ratio is defined by
?A ? CUR?F
Relative-error ratio =
,
?A ? Ak ?F
where k is a specified target rank.
We conduct experiments on three datasets, including natural image, biology data, and bags of words.
Table 1 briefly summarizes some information of the datasets. Redrock is a large size natural image.
Arcene and Dexter are both from the UCI datasets [11]. Arcene is a biology dataset with 900
instances and 10000 attributes. Dexter is a bag of words dataset with a 20000-vocabulary and 2600
documents. Each dataset is actually represented as a data matrix, upon which we apply the CUR
algorithms.
We implement all the algorithms in MATLAB 7.10.0. We conduct experiments on a workstation
with 12 Intel Xeon 3.47GHz CPUs, 12GB memory, and Ubuntu 10.04 system. According to the
analysis in [10] and this paper, k, c, and r should be integers far less than m and n. For each data
set and each algorithm, we set k = 10, 20, or 50, and c = ?k, r = ?c, where ? ranges in each set of
experiments. We repeat each set of experiments for 20 times and report the average and the standard
deviation of the error ratios. The results are depicted in Figures 1, 2, 3.
The results show that the fast CUR algorithm has much lower relative-error ratio than the subspace
sampling algorithm. The experimental results well match our theoretical analyses in Section 4. As
for the running time, the fast CUR algorithm is more efficient when c and r are small. When c and
r become large, the fast CUR algorithm becomes less efficient. This is because the time complexity
of the fast CUR algorithm is linear in ??4 and large c and r imply small ?. However, the purpose
of CUR is to select a small number of columns and rows from the data matrix, that is, c ? n and
r ? m. So we are not interested in the cases where c and r are large compared with n and m, say
k = 20 and ? = 10.
5

Running Time

Running Time

700

700

600

600

600

500

500

Subspace Sampling (Exactly)
Subspace Sampling (Expected)
Fast CUR

300

500
Subspace Sampling (Exactly)
Subspace Sampling (Expected)
Fast CUR

400

Time (s)

400

Time (s)

Time (s)

Running Time
700

300

400
300

200

200

200

100

100

100

0
2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36

0
2

?

4

Construction Error (Frobenius Norm)

8

0
2

10 12 14 16 18 20 22 24

?

Subspace Sampling (Exactly)
Subspace Sampling (Expected)
Fast CUR

1
0.9
0.8
0.7

1.2

1
0.9
0.8
0.7

0.6

?

?

12

14

16

18

1.1
1
0.9
0.8
0.7
0.6
0.5

0.6
2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36

10

Subspace Sampling (Exactly)
Subspace Sampling (Expected)
Fast CUR

1.2

1.1

0.5
2

8

Construction Error (Frobenius Norm)

Relative Error Ratio

1.1

6

1.3
Subspace Sampling (Exactly)
Subspace Sampling (Expected)
Fast CUR

1.3
Relative Error Ratio

1.2

4

Construction Error (Frobenius Norm)
1.4

1.3

Relative Error Ratio

6

Subspace Sampling (Exactly)
Subspace Sampling (Expected)
Fast CUR

4

6

8

0.4
2

10 12 14 16 18 20 22 24

?

4

6

8

10

?

12

14

16

18

(a) k = 10, c = ?k, and r = ?c. (b) k = 20, c = ?k, and r = ?c. (c) k = 50, c = ?k, and r = ?c.

Figure 1: Empirical results on the Redrock data set.

Running Time

Running Time

14

20

10

12

18
16

6

Time (s)

10

8
Time (s)

Time (s)

Running Time
12

8
6

4
4
Subspace Sampling (Exactly)
Subspace Sampling (Expected)
Fast CUR

2
0
2

4

6

0
2

8 10 12 14 16 18 20 22 24 26 28 30

?

4

Construction Error (Frobenius Norm)

8

10

?

12

14

16

18

10
Subspace Sampling (Exactly)
Subspace Sampling (Expected)
Fast CUR

6
4
2

20

Construction Error (Frobenius Norm)

4

6

8

?

10

12

14

Construction Error (Frobenius Norm)
1.3

Subspace Sampling (Exactly)
Subspace Sampling (Expected)
Fast CUR

1.3
Relative Error Ratio

1.3
1.2
1.1
1
0.9
0.8

Subspace Sampling (Exactly)
Subspace Sampling (Expected)
Fast CUR

1.2
1.1
1
0.9
0.8

0.7

0.7

0.6

0.6

Subspace Sampling (Exactly)
Subspace Sampling (Expected)
Fast CUR

1.2
Relative Error Ratio

1.4

Relative Error Ratio

6

12

8

Subspace Sampling (Exactly)
Subspace Sampling (Expected)
Fast CUR

2

14

1.1
1
0.9
0.8
0.7
0.6
0.5

2

4

6

8 10 12 14 16 18 20 22 24 26 28 30

?

2

4

6

8

10

?

12

14

16

18

20

0.4
2

4

6

8

?

10

12

14

(a) k = 10, c = ?k, and r = ?c. (b) k = 20, c = ?k, and r = ?c. (c) k = 50, c = ?k, and r = ?c.

Figure 2: Empirical results on the Arcene data set.

6

Running Time

Running Time

250

Running Time

300
Subspace Sampling (Exactly)
Subspace Sampling (Expected)
Fast CUR

200

400
Subspace Sampling (Exactly)
Subspace Sampling (Expected)
Fast CUR

250

350

Subspace Sampling (Exactly)
Subspace Sampling (Expected)
Fast CUR

100

Time (s)

150

Time (s)

Time (s)

300
200
150
100

250
200
150
100

50

50

0
2

4

6

0
2

8 10 12 14 16 18 20 22 24 26 28 30

?

50
4

8

0
2

10 12 14 16 18 20 22 24

?

1.1
1.05
1
0.95
0.9
2

4

6

8 10 12 14 16 18 20 22 24 26 28 30

?

1.15
1.1
1.05
1
0.95

?

16

18

1

0.9
0.85
0.8

10 12 14 16 18 20 22 24

14

0.95

0.75
8

12

1.1

0.9

6

?

1.05

0.85
2

4

10

Subspace Sampling (Exactly)
Subspace Sampling (Expected)
Fast CUR

1.15
Relative Error Ratio

Relative Error Ratio

1.15

8

1.2

Subspace Sampling (Exactly)
Subspace Sampling (Expected)
Fast CUR

1.2

6

Construction Error (Frobenius Norm)

1.25
Subspace Sampling (Exactly)
Subspace Sampling (Expected)
Fast CUR

1.2

4

Construction Error (Frobenius Norm)

Construction Error (Frobenius Norm)
1.25

Relative Error Ratio

6

2

4

6

8

10

?

12

14

16

18

(a) k = 10, c = ?k, and r = ?c. (b) k = 20, c = ?k, and r = ?c. (c) k = 50, c = ?k, and r = ?c.

Figure 3: Empirical results on the Dexter data set.

6

Conclusions

In this paper we have proposed a novel randomized algorithm for the CUR matrix decomposition
problem. This algorithm is faster, more scalable, and more accurate than the state-of-the-art algorithm, i.e., the subspace sampling algorithm. Our algorithm requires only c = 2k??1 (1 + o(1))
columns and r = 2c??1 (1 + o(1)) rows to achieve (1+?) relative-error ratio. To achieve the same
relative-error bound, the subspace sampling algorithm requires c = O(k??2 log k) columns and
r = O(c??2 log c) rows selected from the original matrix. Our algorithm also beats the subspace
sampling algorithm in time-complexity. Our algorithm costs O(mnk??2/3 + (m + n)k 3 ??2/3 +
mk 2 ??2 + nk 2 ??4 ) time, which is lower than O(min{mn2 , m2 n}) of the subspace sampling algorithm when k is small. Moreover, our algorithm enjoys another advantage of avoiding loading the
whole data matrix into main memory, which also makes our algorithm more scalable. Finally, the
empirical comparisons have also demonstrated the effectiveness and efficiency of our algorithm.

A The Dual Set Sparsification Algorithm
For the sake of completeness, we attach the dual set sparsification algorithm here and describe
some implementation details. The dual set sparsification algorithms are deterministic algorithms
established in [2]. The fast CUR algorithm calls the dual set spectral-Frobenius sparsification algorithm [2, Lemma 13] in both stages. We show this algorithm in Algorithm 2 and its bounds in
Lemma 6.
Lemma 6 (Dual Set Spectral-Frobenius Sparsification). Let U = {x1 , ? ? ? , xn } ? Rl , (l < n),
l?n
k
contains the columns of an arbitrary matrix
?n X ? RT . Let V = {v1 , ? ? ? , vn } ? R , (k < n),
be a decompositions of the identity, i.e.
v
v
=
I
.
Given
an
integer
r
with
k
< r < n,
k
i=1 i i
Algorithm 2 deterministically computes a set of weights si ? 0 (i = 1, ? ? ? , n) at most r of which
are non-zero, such that
? )
n
n
(?
(?
)
) (
k 2
T
and
tr
?k
si xi xTi ? ?X?2F .
si vi vi ? 1 ?
r
i=1
i=1
7

Algorithm 2 Deterministic Dual Set Spectral-Frobenius Sparsification Algorithm.

?n
l
n
k
T
1: Input: U = {xi }n
i=1 ? R , (l < n); V = {vi }i=1 ? R , with
i=1 vi vi = Ik (k < n); k < r < n;
2: Initialize: s0 = 0m?1 , A0 = 0k?k ;
?n
?x ?2
3: Compute ?xi ?22 for i = 1, ? ? ? , n, and then compute ?U = i=1? i 2 ;
1?

k/r

4: for ? = 0 to r ? 1 do
5:
Compute the eigenvalue decomposition of A? ;
6:
Find an index j in {1, ? ? ? , n} and compute a weight t > 0 such that
(
)?2
(
)?1
vjT A? ? (L? + 1)Ik
vj
?1
2
?1
?U ?xj ?2 ? t
?
vj ;
? vjT A? ? (L? + 1)Ik
?(L? + 1, A? ) ? ?(L? , A? )
where
?(L, A) =

k (
)?1
?
?i (A) ? L
,

L? = ? ?

?

rk;

i=1

7:
Update the j-th component of s? and A? :
8: end for
?
1? k/r
9: return s =
sr .
r

s? +1 [j] = s? [j] + t,

A? +1 = A? + tvj vjT ;

The weights si can be computed deterministically in O(rnk 2 + nl) time.
Here we would like to mention the implementation of Algorithm 2, which is not described in detailed
by [2]. In each iteration the algorithm performs once eigenvalue decomposition: A? = W?WT .
(A? is guaranteed to be positive semi-definite in each iteration). Since
(
)q
(
)
A? ? ?Ik = WDiag (?1 ? ?)q , ? ? ? , (?k ? ?)q WT ,
we can efficiently compute (A? ? (L? + 1)Ik )q based on the eigenvalue decomposition of A? . With
the eigenvalues at hand, ?(L, A? ) can also be computed directly.

Acknowledgments
This work has been supported in part by the Natural Science Foundations of China (No. 61070239),
the Google visiting faculty program, and the Scholarship Award for Excellent Doctoral Student
granted by Ministry of Education.

References
[1] Adi Ben-Israel and Thomas N.E. Greville. Generalized Inverses: Theory and Applications.
Second Edition. Springer, 2003.
[2] Christos Boutsidis, Petros Drineas, and Malik Magdon-Ismail. Near-optimal column-based
matrix reconstruction. CoRR, abs/1103.0995, 2011.
[3] Christos Boutsidis, Petros Drineas, and Malik Magdon-Ismail. Near optimal column-based
matrix reconstruction. In Proceedings of the 2011 IEEE 52nd Annual Symposium on Foundations of Computer Science, FOCS ?11, pages 305?314, 2011.
[4] Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard
Harshman. Indexing by latent semantic analysis. Journal of The American Society for Information Science, 41(6):391?407, 1990.
[5] Amit Deshpande and Luis Rademacher. Efficient volume sampling for row/column subset selection. In Proceedings of the 2010 IEEE 51st Annual Symposium on Foundations of Computer
Science, FOCS ?10, pages 329?338, 2010.
[6] Amit Deshpande, Luis Rademacher, Santosh Vempala, and Grant Wang. Matrix approximation
and projective clustering via volume sampling. Theory of Computing, 2(2006):225?247, 2006.
[7] Petros Drineas. Pass-efficient algorithms for approximating large matrices. In In Proceeding
of the 14th Annual ACM-SIAM Symposium on Dicrete Algorithms, pages 223?232, 2003.
8

[8] Petros Drineas, Ravi Kannan, and Michael W. Mahoney. Fast monte carlo algorithms for
matrices iii: Computing a compressed approximate matrix decomposition. SIAM Journal on
Computing, 36(1):184?206, 2006.
[9] Petros Drineas and Michael W. Mahoney. On the Nystr?om method for approximating a gram
matrix for improved kernel-based learning. Journal of Machine Learning Research, 6:2153?
2175, 2005.
[10] Petros Drineas, Michael W. Mahoney, and S. Muthukrishnan. Relative-error CUR matrix decompositions. SIAM Journal on Matrix Analysis and Applications, 30(2):844?881, September
2008.
[11] A. Frank and A. Asuncion. UCI machine learning repository, 2010.
[12] S. A. Goreinov, E. E. Tyrtyshnikov, and N. L. Zamarashkin. A theory of pseudoskeleton
approximations. Linear Algebra and Its Applications, 261:1?21, 1997.
[13] S. A. Goreinov, N. L. Zamarashkin, and E. E. Tyrtyshnikov. Pseudo-skeleton approximations
by matrices of maximal volume. Mathematical Notes, 62(4):619?623, 1997.
[14] Venkatesan Guruswami and Ali Kemal Sinop. Optimal column-based low-rank matrix reconstruction. In Proceedings of the Twenty-Third Annual ACM-SIAM Symposium on Discrete
Algorithms, SODA ?12, pages 1207?1214. SIAM, 2012.
[15] Nathan Halko, Per-Gunnar Martinsson, and Joel A. Tropp. Finding structure with randomness:
Probabilistic algorithms for constructing approximate matrix decompositions. SIAM Review,
53(2):217?288, 2011.
[16] John Hopcroft and Ravi Kannan. Computer Science Theory for the Information Age. 2012.
[17] Finny G. Kuruvilla, Peter J. Park, and Stuart L. Schreiber. Vector algebra in the analysis of
genome-wide expression data. Genome Biology, 3:research0011?research0011.1, 2002.
[18] Lester Mackey, Ameet Talwalkar, and Michael I. Jordan. Divide-and-conquer matrix factorization. In Advances in Neural Information Processing Systems 24. 2011.
[19] Michael W. Mahoney and Petros Drineas. CUR matrix decompositions for improved data
analysis. Proceedings of the National Academy of Sciences, 106(3):697?702, 2009.
[20] L. Sirovich and M. Kirby. Low-dimensional procedure for the characterization of human faces.
Journal of the Optical Society of America A, 4(3):519?524, Mar 1987.
[21] Matthew Turk and Alex Pentland. Eigenfaces for recognition. Journal of Cognitive Neuroscience, 3(1):71?86, 1991.
[22] Eugene E. Tyrtyshnikov. Incomplete cross approximation in the mosaic-skeleton method.
Computing, 64:367?380, 2000.

9


----------------------------------------------------------------

title: 5074-low-rank-matrix-reconstruction-and-clustering-via-approximate-message-passing.pdf

Low-rank matrix reconstruction and clustering via
approximate message passing

Ryosuke Matsushita
NTT DATA Mathematical Systems Inc.
1F Shinanomachi Rengakan, 35,
Shinanomachi, Shinjuku-ku, Tokyo,
160-0016, Japan
matsur8@gmail.com

Toshiyuki Tanaka
Department of Systems Science,
Graduate School of Informatics, Kyoto University
Yoshida Hon-machi, Sakyo-ku, Kyoto-shi,
606-8501 Japan
tt@i.kyoto-u.ac.jp

Abstract
We study the problem of reconstructing low-rank matrices from their noisy observations. We formulate the problem in the Bayesian framework, which allows
us to exploit structural properties of matrices in addition to low-rankedness, such
as sparsity. We propose an efficient approximate message passing algorithm, derived from the belief propagation algorithm, to perform the Bayesian inference for
matrix reconstruction. We have also successfully applied the proposed algorithm
to a clustering problem, by reformulating it as a low-rank matrix reconstruction
problem with an additional structural property. Numerical experiments show that
the proposed algorithm outperforms Lloyd?s K-means algorithm.

1

Introduction

Low-rankedness of matrices has frequently been exploited when one reconstructs a matrix from its
noisy observations. In such problems, there are often demands to incorporate additional structural
properties of matrices in addition to the low-rankedness. In this paper, we consider the case where
a matrix A0 ? Rm?N to be reconstructed is factored as A0 = U0 V0? , U0 ? Rm?r , V0 ? RN ?r
(r ? m, N ), and where one knows structural properties of the factors U0 and V0 a priori. Sparseness
and non-negativity of the factors are popular examples of such structural properties [1, 2].
Since the properties of the factors to be exploited vary according to the problem, it is desirable
that a reconstruction method has enough flexibility to incorporate a wide variety of properties. The
Bayesian approach achieves such flexibility by allowing us to select prior distributions of U0 and V0
reflecting a priori knowledge on the structural properties. The Bayesian approach, however, often
involves computationally expensive processes such as high-dimensional integrations, thereby requiring approximate inference methods in practical implementations. Monte Carlo sampling methods
and variational Bayes methods have been proposed for low-rank matrix reconstruction to meet this
requirement [3?5].
We present in this paper an approximate message passing (AMP) based algorithm for Bayesian lowrank matrix reconstruction. Developed in the context of compressed sensing, the AMP algorithm reconstructs sparse vectors from their linear measurements with low computational cost, and achieves
a certain theoretical limit [6]. AMP algorithms can also be used for approximating Bayesian inference with a large class of prior distributions of signal vectors and noise distributions [7]. These
successes of AMP algorithms motivate the use of the same idea for low-rank matrix reconstruction.
The IterFac algorithm for the rank-one case [8] has been derived as an AMP algorithm. An AMP
algorithm for the general-rank case is proposed in [9], which, however, can only treat estimation of
posterior means. We extend their algorithm so that one can deal with other estimations such as the
maximum a posteriori (MAP) estimation. It is the first contribution of this paper.
1

As the second contribution, we apply the derived AMP algorithm to K-means type clustering to
obtain a novel efficient clustering algorithm. It is based on the observation that our formulation
of the low-rank matrix reconstruction problem includes the clustering problem as a special case.
Although the idea of applying low-rank matrix reconstruction to clustering is not new [10, 11], our
proposed algorithm is, to our knowledge, the first that directly deals with the constraint that each
datum should be assigned to exactly one cluster in the framework of low-rank matrix reconstruction.
We present results of numerical experiments, which show that the proposed algorithm outperforms
Lloyd?s K-means algorithm [12] when data are high-dimensional.
Recently, AMP algorithms for dictionary learning and blind calibration [13] and for matrix reconstruction with a generalized observation model [14] were proposed. Although our work has some
similarities to these studies, it differs in that we fix the rank r rather than the ratio r/m when taking
the limit m, N ? ? in the derivation of the algorithm. Another difference is that our formulation,
explained in the next section, does not assume statistical independence among the components of
each row of U0 and V0 . A detailed comparison among these algorithms remains to be made.

2
2.1

Problem setting
Low-rank matrix reconstruction

We consider the following problem setting. A matrix A0 ? Rm?N to be estimated is defined
by two matrices U0 := (u0,1 , . . . , u0,m )? ? Rm?r and V0 := (v0,1 , . . . , v0,N )? ? RN ?r as
A0 := U0 V0? , where u0,i , v0,j ? Rr . We consider the case where r ? m, N . Observations of A0
are corrupted by additive noise W ? Rm?N , whose components Wi,j are i.i.d. Gaussian random
variables following N (0, m? ). Here ? > 0 is a noise variance parameter and N (a, ? 2 ) denotes the
Gaussian distribution with mean a and variance ? 2 . The factor m in the noise variance is introduced
to allow a proper scaling in the limit where m and N go to infinity in the same order, which is
employed in deriving the algorithm. An observed matrix A ? Rm?N is given by A := A0 + W .
Reconstructing A0 and (U0 , V0 ) from A is the problem considered in this paper.
We take the Bayesian approach to address this problem, in which one requires prior distributions
of variables to be estimated, as well as conditional distributions relating observations with variables
to be estimated. These distributions need not be the true ones because in some cases they are not
available so that one has to assume them arbitrarily, and in some other cases one expects advantages
by assuming them in some specific manner in view of computational efficiencies. In this paper, we
suppose that one uses the true conditional distribution
(
)
1
1
p(A|U0 , V0 ) =
?A ? U0 V0? ?2F ,
(1)
mN exp ?
2m?
(2?m? ) 2
where ? ? ?F denotes the Frobenius norm. Meanwhile, we suppose that the assumed prior distributions of U0 and V0 , denoted by p?U and p?V , respectively, may be different from the true distributions
?
pU and pV , respectively.
We restrict p?U and p?V to distributions of the form p?U (U0 ) = i p?u (u0,i )
?
and p?V (V0 ) = j p?v (v0,j ), respectively, which allows us to construct computationally efficient
algorithms. When U ? p?U (U ) and V ? p?V (V ), the posterior distribution of (U, V ) given A is
(
)
1
?A ? U V ? ?2F p?U (U )?
pV (V ).
(2)
p?(U, V |A) ? exp ?
2m?
Prior probability density functions (p.d.f.s) p?u and p?v can be improper, that is, they can integrate to
infinity, as long as the posterior p.d.f. (2) is proper. We also consider cases where the assumed rank
r? may be different from the true rank r. We thus suppose that estimates U and V are of size m ? r?
and N ? r?, respectively.
We consider two problems appearing in the Bayesian approach. The first problem, which we call
the marginalization problem, is to calculate the marginal posterior distributions given A,
?
?
?
p?i,j (ui , vj |A) := p?(U, V |A)
duk
dvl .
(3)
k?=i
?

l?=j

These are used to calculate
the posterior mean E[U V |A] and the
?
? marginal MAP estimates
MMAP
uMMAP
:=
arg
max
p
?
(u,
v|A)dv
and
v
:=
arg
max
p?i,j (u, v|A)du. Because
u
i,j
v
i
j
2

calculation of p?i,j (ui , vj |A) typically involves high-dimensional integrations requiring high computational cost, approximation methods are needed.
The second problem, which we call the MAP problem, is to calculate the MAP estimate
arg maxU,V p?(U, V |A). It is formulated as the following optimization problem:
min C MAP (U, V ),
U,V

(4)

where C MAP (U, V ) is the negative logarithm of (2):
C MAP (U, V ) :=

m
N
?
?
1
?A ? U V ? ?2F ?
log p?u (ui ) ?
log p?v (vj ).
2m?
i=1
j=1

(5)

Because ?A ? U V ? ?2F is a non-convex function of (U, V ), it is generally hard to find the global
optimal solutions of (4) and therefore approximation methods are needed in this problem as well.
2.2

Clustering as low-rank matrix reconstruction

A clustering problem can be formulated as a problem of low-rank matrix reconstruction [11]. Suppose that v0,j ? {e1 , . . . , er }, j = 1, . . . , N , where el ? {0, 1}r is the vector whose lth component
is 1 and the others are 0. When V0 and U0 are fixed, aj follows one of the r Gaussian distributions
? 0,l , m? I), l = 1, . . . , r, where u
? 0,l is the lth column of U0 . We regard that each Gaussian
N (u
? 0,l being the center of cluster l and v0,j representing the cluster
distribution defines a cluster, u
assignment of the datum aj . One can then perform clustering on the dataset {a1 , . . . , aN } by reconstructing U0 and V0 from A = (a1 , . . . , aN ) under the structural constraint that every row of V0
should belong to {e1 , . . . , er?}, where r? is an assumed number of clusters.
Let us consider maximum likelihood estimation arg maxU,V p(A|U, V ), or equivalently, MAP esti?r?
mation with the (improper) uniform prior distributions p?u (u) = 1 and p?v (v) = r??1 l=1 ?(v?el ).
The corresponding MAP problem is
min

r ,V ?{0,1}N ??
r
U ?Rm??

?A ? U V ? ?2F

subject to vj ? {e1 , . . . , er?}.

(6)

?N ?r?
When V satisfies the constraints, the objective function ?A ? U V ? ?2F =
j=1
l=1 ?aj ?
? l ?22 I(vj = el ) is the sum of squared distances, each of which is between a datum and the center of
u
the cluster that the datum is assigned to. The optimization problem (6), its objective function, and
clustering based on it are called in this paper the K-means problem, the K-means loss function, and
the K-means clustering, respectively.
One can also use the marginal MAP estimation for clustering. If U0 and V0 follow p?U and p?V , respectively, the marginal MAP estimation is optimal in the sense that it maximizes the expectation of
accuracy with respect to p?(V0 |A). Here, accuracy is defined as the fraction of correctly assigned data
among all data. We call the clustering using approximate marginal MAP estimation the maximum
accuracy clustering, even when incorrect prior distributions are used.

3

Previous work

Existing methods for approximately solving the marginalization problem and the MAP problem
are divided into stochastic methods such as Markov-Chain Monte-Carlo methods and deterministic
ones. A popular deterministic method is to use the variational Bayesian formalism. The variational
Bayes matrix factorization [4, 5] approximates the posterior distribution p(U, V |A) as the product
VB
of two functions pVB
U (U ) and pV (V ), which are determined so that the Kullback-Leibler (KL)
VB
VB
divergence from pU (U )pV (V ) to p(U, V |A) is minimized. Global minimization of the KL divergence is difficult except for some special cases [15], so that an iterative method to obtain a local
minimum is usually adopted. Applying the variational Bayes matrix factorization to the MAP problem, one obtains the iterated conditional modes (ICM) algorithm, which alternates minimization of
C MAP (U, V ) over U for fixed V and minimization over V for fixed U .
The representative algorithm to solve the K-means problem approximately is Lloyd?s K-means algorithm [12]. Lloyd?s K-means algorithm is regarded as the ICM algorithm: It alternates minimization
of the K-means loss function over U for fixed V and minimization over V for fixed U iteratively.
3

Algorithm 1 (Lloyd?s K-means algorithm).
ntl =

N
?

I(vjt = el ),

? tl =
u

j=1

ljt+1 = arg

min

l?{1,...,?
r}

? tl ?22 ,
?aj ? u

N
1 ?
aj I(vjt = el ),
ntl j=1

(7a)

vjt+1 = elt+1 .

(7b)

j

Throughout this paper, we represent an algorithm by a set of equations as in the above. This representation means that the algorithm begins with a set of initial values and repeats the update of the
variables using the equations presented until it satisfies some stopping criteria. Lloyd?s K-means
algorithm begins with a set of initial assignments V 0 ? {e1 , . . . , er?}N . This algorithm easily gets
stuck in local minima and its performance heavily depends on the initial values of the algorithm.
Some methods for initialization to obtain a better local minimum are proposed [16].
Maximum accuracy clustering can be solved approximately by using the variational Bayes matrix
factorization, since it gives an approximation to the marginal posterior distribution of vj given A.

4
4.1

Proposed algorithm
Approximate message passing algorithm for low-rank matrix reconstruction

We first discuss the general idea of the AMP algorithm and advantages of the AMP algorithm compared with the variational Bayes matrix factorization. The AMP algorithm is derived by approximating the belief propagation message passing algorithm in a way thought to be asymptotically exact for
large-scale problems with appropriate randomness. Fixed points of the belief propagation message
passing algorithm correspond to local minima of the KL divergence between a kind of trial function
and the posterior distribution [17]. Therefore, the belief propagation message passing algorithm can
be regarded as an iterative algorithm based on an approximation of the posterior distribution, which
is called the Bethe approximation. The Bethe approximation can reflect dependence of random variables (dependence between U and V in p?(U, V |A) in our problem) to some extent. Therefore, one
can intuitively expect that performance of the AMP algorithm is better than that of the variational
Bayes matrix factorization, which treats U and V as if they were independent in p?(U, V |A).
An important property of the AMP algorithm, aside from its efficiency and effectiveness, is that
one can predict performance of the algorithm accurately for large-scale problems by using a set of
equations, called the state evolution [6]. Analysis with the state evolution also shows that required
iteration numbers are O(1) even when the problem size is large. Although we can present the state
evolution for the algorithm proposed in this paper and give a proof of its validity like [8, 18], we do
not discuss the state evolution here due to the limited space available.
We introduce a one-parameter extension of the posterior distribution p?(U, V |A) to treat the marginalization problem and the MAP problem in a unified manner. It is defined as follows:
(
)(
)?
?
p?(U, V |A; ?) ? exp ?
?A ? U V ? ?2F p?U (U )?
pV (V ) ,
2m?

(8)

which is proportional to p?(U, V |A)? , where ? > 0 is the parameter. When ? = 1, p?(U, V |A; ?)
is reduced to p?(U, V |A). In the limit ? ? ?, the distribution p?(U, V |A; ?) concentrates on the
maxima of p?(U, V |A). An algorithm for the marginalization problem on p?(U, V |A; ?) is particularized to the algorithms for the marginalization problem and for the MAP problem for the original
posterior distribution p?(U, V |A) by letting ? = 1 and ? ? ?, respectively. The AMP algorithm
for the marginalization problem on p?(U, V |A; ?) is derived in a way similar to that described in [9],
as detailed in the Supplementary Material.
In the derived algorithm, the values of variables But = (btu,1 , . . . , btu,m )? ? Rm??r , Bvt =
(btv,1 , . . . , btv,N )? ? RN ??r , ?tu ? Rr???r , ?tv ? Rr???r , U t = (ut1 , . . . , utm )? ? Rm??r ,
t ?
t
V t = (v1t , . . . , vN
) ? RN ??r , S1t , . . . , Sm
? Rr???r , and T1t , . . . , TNt ? Rr???r are calculated iteratively, where the superscript t ? N ? {0} represents iteration numbers. Variables with a negative
iteration number are defined as 0. The algorithm is as follows:
4

Algorithm 2.
But =

N
1
1 t?1 ? t
AV t ?
U
Tj ,
m?
m?
j=1

?tu =

N
N
1 ? t
1
1 ? t
(V t )? V t +
Tj ?
T , (9a)
m?
?m? j=1
m? j=1 j

uti = f (btu,i , ?tu ; p?u ),

Sit = G(btu,i , ?tu ; p?u ),
m
m
m
1
1 ? t
1 t? t
1 ? t
1 ? t
Si , ?tv =
A U ?
V
(U t )? U t +
Si ?
S ,
Bvt =
m?
m?
m?
?m? i=1
m? i=1 i
i=1
vjt+1 = f (btv,j , ?tv ; p?v ),

Tjt+1 = G(btv,j , ?tv ; p?v ).

(9b)
(9c)
(9d)

Algorithm 2 is almost symmetric in U and V . Equations (9a)?(9b) and (9c)?(9d) update quantities
related to the estimates of U0 and V0 , respectively. The algorithm requires an initial value V 0 and
begins with Tj0 = O. The functions f (?, ?; p?) : Rr??Rr???r ? Rr? and G(?, ?; p?) : Rr??Rr???r ? Rr???r ,
which have a p.d.f. p? : Rr? ? R as a parameter, are defined by
?
?f (b, ?; p?)
f (b, ?; p?) := u?
q (u; b, ?, p?)du,
G(b, ?; p?) :=
,
(10)
?b
where q?(u; b, ?, p?) is the normalized p.d.f. of u defined by
( (1
))
q?(u; b, ?, p?) ? exp ?? u? ?u ? b? u ? log p?(u) .
2

(11)

One can see that f (b, ?; p?) is the mean of the distribution q?(u; b, ?, p?) and that G(b, ?; p?) is its
covariance matrix scaled by ?. The function f (b, ?; p?) need not be differentiable everywhere;
Algorithm 2 works if f (b, ?; p?) is differentiable at b for which one needs to calculate G(b, ?; p?) in
running the algorithm.
We assume in the rest of this section the convergence of Algorithm 2, although the convergence is
?
?
?
?
?
not guaranteed in general. Let Bu? , Bv? , ??
be the converged values
u , ?v , Si , Tj , U , and V
of the respective variables. First, consider running Algorithm 2 with ? = 1. The marginal posterior
distribution is then approximated as
?
?
?v ).
?u )?
q (vj ; b?
p?i,j (ui , vj |A) ? q?(ui ; b?
v,j , ?v , p
u,i , ?u , p

(12)

?
?
?
Since u?
of q?(u; b?
?u ) and q?(v; b?
?v ), respectively, the
i and vj are the means
u,i , ?u , p
v,j , ?v , p
?
?
?
posterior mean E[U V |A] = U V p?(U, V |A)dU dV is approximated as

E[U V ? |A] ? U ? (V ? )? .

(13)

and vjMMAP are approximated as
The marginal MAP estimates uMMAP
i
?
?v ).
vjMMAP ? arg max q?(v; b?
v,j , ?v , p

?
?u ),
uMMAP
? arg max q?(u; b?
u,i , ?u , p
i

v

u

(14)

Taking the limit ? ? ? in Algorithm 2 yields an algorithm for the MAP problem (4). In this case,
the functions f and G are replaced with
[1
]
?f? (b, ?; p?)
f? (b, ?; p?) := arg min u? ?u ? b? u ? log p?(u) , G? (b, ?; p?) :=
. (15)
u
2
?b
One may calculate G? (b, ?; p?) from the Hessian of log p?(u) at u = f? (b, ?; p?), denoted by H,
(
)?1
via the identity G? (b, ?; p?) = ??H
. This identity follows from the implicit function theorem
under some additional assumptions and helps in the case where the explicit form of f? (b, ?; p?) is
not available. The MAP estimate is approximated by (U ? , V ? ).
4.2

Properties of the algorithm

Algorithm 2 has several plausible properties. First, it has a low computational cost. The computational cost per iteration is O(mN ), which is linear in the number of components of the matrix
A. Calculation of f (?, ?; p?) and G(?, ?; p?) is performed O(N + m) times per iteration. The constant
5

factor depends on p? and ?. Calculation of f for ? < ? generally involves an r?-dimensional numerical integration, although they are not needed in cases where an analytic expression of the integral
is available and cases where the variables take only discrete values. Calculation of f? involves
minimization over an r?-dimensional vector. When ? log p? is a convex function and ? is positive
semidefinite, this minimization problem is convex and can be solved at relatively low cost.
Second, Algorithm 2 has a form similar to that of an algorithm based on the variational Bayesian
matrix factorization. In fact, if the last terms on the right-hand sides of the four equations in (9a)
and (9c) are removed, the resulting algorithm is the same as an algorithm based on the variational
Bayesian matrix factorization proposed in [4] and, in particular, the same as the ICM algorithm when
? ? ?. (Note, however, that [4] only treats the case where the priors p?u and p?v are multivariate
Gaussian distributions.) Note that additional computational cost for these extra terms is O(m + N ),
which is insignificant compared with the cost of the whole algorithm, which is O(mN ).
Third, when one deals with the MAP problem, the value of C MAP (U, V ) may increase in iterations of Algorithm 2. The following proposition, however, guarantees optimality of the output of
Algorithm 2 in a certain sense, if it has converged.
?
Proposition 1. Let (U ? , V ? , S1? , . . . , Sm
, T ? , . . . , TN? ) be a fixed point of the AMP algorithm
?m 1 ?
?N
for the MAP problem and suppose that i=1 Si and j=1 Tj? are positive semidefinite. Then
U ? is a global minimum of C MAP (U, V ? ) and V ? is a global minimum of C MAP (U ? , V ).
The proof is in the Supplementary Material. The key to the proof is the following reformulation:
N
)]
[
(
)
( 1 ?
MAP
t
t?1
Tjt (U ? U t?1 )?
U = arg min C
(U, V ) ? tr (U ? U )
U
2m? j=1
t

(16)

?N
If j=1 Tjt is positive semidefinite, the second term of the minimand is the negative squared pseudometric between U and U t?1 , which is interpreted as a penalty on nearness to the temporal estimate.
?m
?N
Positive semidefiniteness of i=1 Sit and j=1 Tjt holds in almost all cases. In fact, we only have
to assume lim??? G(b, ?; p?) = G? (b, ?; p?), since G(b, ?; p?) is a scaled covariance matrix of
q?(u; b, ?, p?), which is positive semidefinite. It follows from Proposition 1 that any fixed point of the
AMP algorithm is also a fixed point of the ICM algorithm. It has two implications: (i) Execution
of the ICM algorithm initialized with the converged values of the AMP algorithm does not improve
C MAP (U t , V t ). (ii) The AMP algorithm has not more fixed points than the ICM algorithm. The
second implication may help the AMP algorithm avoid getting stuck in bad local minima.
4.3

Clustering via AMP algorithm

One can use the AMP algorithm for the MAP problem to perform the K-means clustering by letting
?r?
p?u (u) = 1 and p?v (v) = r??1 l=1 ?(v ? el ). Noting that f? (b, ?; p?v ) is piecewise constant with
respect to b and hence G? (b, ?; p?v ) is O almost everywhere, we obtain the following algorithm:
Algorithm 3 (AMP algorithm for the K-means clustering).
1
1
AV t , ?tu =
(V t )? V t , U t = But (?tu )?1 ,
m?
m?
1 ? t 1 t t
1
1
Bvt =
A U ? V S , ?tv =
(U t )? U t ? S t ,
m?
?
m?
?
[1
]
v ? ?tv v ? v ? btv,j .
vjt+1 = arg
min
v?{e1 ,...,er? } 2
But =

S t = (?tu )?1 ,

(17a)
(17b)
(17c)

It is initialized with an assignment V 0 ? {e1 , . . . , er?}N . Algorithm 3 is rewritten as follows:
ntl =

N
?
j=1

ljt+1 = arg

I(vjt = el ),

? tl =
u

N
1 ?
aj I(vjt = el ),
ntl j=1

[ 1
2m
m]
? tl ?22 + t I(vjt = el ) ? t ,
?aj ? u
nl
nl
l?{1,...,?
r } m?
min

6

(18a)
vjt+1 = elt+1 .
j

(18b)

The parameter ? appearing in
algorithm does not exist in the?
K-means clustering problem. In
?the
m
m
fact, ? appears because m?2 i=1 A2ij Sit was estimated by ? m?1 i=1 Sit in deriving Algorithm 2,
which can be justified for large-sized problems. In practice, we propose using m?2 N ?1 ?A ?
U t (V t )? ?2F as a temporary estimate of ? at tth iteration. While the AMP algorithm for the Kmeans clustering updates the value of U in the same way as Lloyd?s K-means algorithm, it performs
assignments of data to clusters in a different way. In the AMP algorithm, in addition to distances
from data to centers of clusters, the assignment at present is taken into consideration in two ways:
(i) A datum is less likely to be assigned to the cluster that it is assigned to at present. (ii) Data are
more likely to be assigned to a cluster whose size at present is smaller. The former can intuitively be
understood by observing that if vjt = el , one should take account of the fact that the cluster center
? tl is biased toward aj . The term 2m(ntl )?1 I(vjt = el ) in (18b) corrects this bias, which, as it
u
should be, is inversely proportional to the cluster size.
The AMP algorithm for maximum accuracy clustering is obtained by letting ? = 1 and p?v (v) be
a discrete distribution on {e1 , . . . , er?}. After the algorithm converges, arg maxv q?(v; vj? , ??
?v )
v ,p
gives the final cluster assignment of the jth datum and U ? gives the estimate of the cluster centers.

5

Numerical experiments

We conducted numerical experiments on both artificial and real data sets to evaluate performance
of the proposed algorithms for clustering. In the experiment on artificial data sets, we set m = 800
? 0,l , l = 1, . . . , r, were generated according to the
and N = 1600 and let r? = r. Cluster centers u
multivariate Gaussian distribution N (0, I). Cluster assignments v0,j , j = 1, . . . , N, were generated
according to the uniform distribution on {e1 , . . . , er }. For fixed ? = 0.1 and r, we generated 500
problem instances and solved them with five algorithms: Lloyd?s K-means algorithm (K-means),
the AMP algorithm for the K-means clustering (AMP-KM), the variational Bayes matrix factorization [4] for maximum accuracy clustering (VBMF-MA), the AMP algorithm for maximum accuracy
clustering (AMP-MA), and the K-means++ [16]. The K-means++ updates the variables in the same
way as Lloyd?s K-means algorithm with an initial value chosen in a sophisticated manner. For the
other algorithms, initial values vj0 , j = 1, . . . , N, were randomly generated from the same distribution as v0,j . We used the true prior distributions of U and V for maximum accuracy clustering.
We ran Lloyd?s K-means algorithm and the K-means++ until no change was observed. We ran the
AMP algorithm for the K-means clustering until either V t = V t?1 or V t = V t?2 is satisfied.
This is because we observed oscillations of assignments of a small number of data. For the other
two algorithms, we terminated the iteration when ?U t ? U t?1 ?2F < 10?15 ?U t?1 ?2F and ?V t ?
V t?1 ?2F < 10?15 ?V t?1 ?2F were met or the number of iterations exceeded 3000. We then evaluated
the following performance measures for the obtained solution (U ? , V ? ):
?N
?
? := N1 N
? ?22 ), where a
? Normalized K-means loss ?A?U ? (V ? )? ?2F /( j=1 ?aj ? a
j=1 aj .
?
N
? Accuracy maxP N ?1 j=1 I(P vj? = v0,j ), where the maximization is taken over all
r-by-r permutation matrices. We used the Hungarian algorithm [19] to solve this maximization problem efficiently.
? Number of iterations needed to converge.
We calculated the averages and the standard deviations of these performance measures over 500
instances. We conducted the above experiments for various values of r.
Figure 1 shows the results. The AMP algorithm for the K-means clustering achieves the smallest Kmeans loss among the five algorithms, while the Lloyd?s K-means algorithm and K-means++ show
large K-means losses for r ? 5. We emphasize that all the three algorithms are aimed to minimize
the same K-means loss and the differences lie in the algorithms for minimization. The AMP algorithm for maximum accuracy clustering achieves the highest accuracy among the five algorithms. It
also shows fast convergence. In particular, the convergence speed of the AMP algorithm for maximum accuracy clustering is comparable to that of the AMP algorithm for the K-means clustering
when the two algorithms show similar accuracy (r < 9). This is in contrast to the common observation that the variational Bayes method often shows slower convergence than the ICM algorithm.
7

1

1
K-means
AMP-KM
VBMF-MA
AMP-MA
K-means++

K-means
AMP-KM
VBMF-MA
AMP-MA
K-means++

0.8

0.99

Accuracy

Normalized K-means loss

0.995

0.985
0.98

0.6

0.4

0.2

0.975
0.97

2

4

6

8

10

r

12

14

16

0

18

2

4

6

8

(a)

r

12

14

16

18

(b)

2500

1
K-means
AMP-KM
VBMF-MA
AMP-MA
K-means++

2000

Number of iterations

10

0.8

Accuracy

1500

1000

500

0.6

0.4
AMP-KM
VBMF-MA
AMP-MA

0.2

0

2

4

6

8

10

r

12

14

16

0

18

0

10

20

30

Iteration number

(c)

40

50

(d)

Figure 1: (a)?(c) Performance for different r: (a) Normalized K-means loss. (b) Accuracy. (c)
Number of iterations needed to converge. (d) Dynamics for r = 5. Average accuracy at each
iteration is shown. Error bars represent standard deviations.

0.45

0.75

K-means++
AMP-KM

K-means++
AMP-KM
0.7

0.44

Accuracy

Normalized K-means loss

0.46

0.43
0.42

0.65

0.6

0.41
0.55
0.4
0.39

0

10

20

30

Number of trials

40

0.5

50

(a)

0

10

20

30

Number of trials

40

50

(b)

Figure 2: Performance measures in real-data experiments. (a) Normalized K-means loss. (b) Accuracy. The results for the 50 trials are shown in the descending order of performance for AMP-KM.
The worst two results for AMP-KM are out of the range.
In the experiment on real data, we used the ORL Database of Faces [20], which contains 400 images
of human faces, ten different images of each of 40 distinct subjects. Each image consists of 112 ?
92 = 10304 pixels whose value ranges from 0 to 255. We divided N = 400 images into r? = 40
clusters with the K-means++ and the AMP algorithm for the K-means clustering. We adopted the
initialization method of the K-means++ also for the AMP algorithm, because random initialization
often yielded empty clusters and almost all data were assigned to only one cluster. The parameter ?
was estimated in the way proposed in Subsection 4.3. We ran 50 trials with different initial values,
and Figure 2 summarizes the results.
The AMP algorithm for the K-means clustering outperformed the standard K-means++ algorithm
in 48 out of the 50 trials in terms of the K-means loss and in 47 trials in terms of the accuracy.
The AMP algorithm yielded just one cluster with all data assigned to it in two trials. The attained
minimum value of K-means loss is 0.412 with the K-means++ and 0.400 with the AMP algorithm.
The accuracies at these trials are 0.635 with the K-means++ and 0.690 with the AMP algorithm. The
average number of iterations was 6.6 with the K-means++ and 8.8 with the AMP algorithm. These
results demonstrate efficiency of the proposed algorithm on real data.
8

References
[1] P. Paatero, ?Least squares formulation of robust non-negative factor analysis,? Chemometrics and Intelligent Laboratory Systems, vol. 37, no. 1, pp. 23?35, May 1997.
[2] P. O. Hoyer, ?Non-negative matrix factorization with sparseness constraints,? The Journal of Machine
Learning Research, vol. 5, pp. 1457?1469, Dec. 2004.
[3] R. Salakhutdinov and A. Mnih, ?Bayesian probabilistic matrix factorization using Markov chain Monte
Carlo,? in Proceedings of the 25th International Conference on Machine Learning, New York, NY, Jul. 5?
Aug. 9, 2008, pp. 880?887.
[4] Y. J. Lim and Y. W. Teh, ?Variational Bayesian approach to movie rating prediction,? in Proceedings of
KDD Cup and Workshop, San Jose, CA, Aug. 12, 2007.
[5] T. Raiko, A. Ilin, and J. Karhunen, ?Principal component analysis for large scale problems with lots
of missing values,? in Machine Learning: ECML 2007, ser. Lecture Notes in Computer Science, J. N.
Kok, J. Koronacki, R. L. de Mantaras, S. Matwin, D. Mladeni?c, and A. Skowron, Eds. Springer Berlin
Heidelberg, 2007, vol. 4701, pp. 691?698.
[6] D. L. Donoho, A. Maleki, and A. Montanari, ?Message-passing algorithms for compressed sensing,?
Proceedings of the National Academy of Sciences USA, vol. 106, no. 45, pp. 18 914?18 919, Nov. 2009.
[7] S. Rangan, ?Generalized approximate message passing for estimation with random linear mixing,? in Proceedings of 2011 IEEE International Symposium on Information Theory, St. Petersburg, Russia, Jul. 31?
Aug. 5, 2011, pp. 2168?2172.
[8] S. Rangan and A. K. Fletcher, ?Iterative estimation of constrained rank-one matrices in noise,? in Proceedings of 2012 IEEE International Symposium on Information Theory, Cambridge, MA, Jul. 1?6, 2012,
pp. 1246?1250.
[9] R. Matsushita and T. Tanaka, ?Approximate message passing algorithm for low-rank matrix reconstruction,? in Proceedings of the 35th Symposium on Information Theory and its Applications, Oita, Japan,
Dec. 11?14, 2012, pp. 314?319.
[10] W. Xu, X. Liu, and Y. Gong, ?Document clustering based on non-negative matrix factorization,? in Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, Toronto, Canada, Jul. 28?Aug. 1, 2003, pp. 267?273.
[11] C. Ding, T. Li, and M. Jordan, ?Convex and semi-nonnegative matrix factorizations,? IEEE Transactions
on Pattern Analysis and Machine Intelligence, vol. 32, no. 1, pp. 45?55, Jan. 2010.
[12] S. P. Lloyd, ?Least squares quantization in PCM,? IEEE Transactions on Information Theory, vol. IT-28,
no. 2, pp. 129?137, Mar. 1982.
[13] F. Krzakala, M. M?ezard, and L. Zdeborov?a, ?Phase diagram and approximate message passing for blind
calibration and dictionary learning,? preprint, Jan. 2013, arXiv:1301.5898v1 [cs.IT].
[14] J. T. Parker, P. Schniter, and V. Cevher, ?Bilinear generalized approximate message passing,? preprint,
Oct. 2013, arXiv:1310.2632v1 [cs.IT].
[15] S. Nakajima and M. Sugiyama, ?Theoretical analysis of Bayesian matrix factorization,? Journal of Machine Learning Research, vol. 12, pp. 2583?2648, Sep. 2011.
[16] D. Arthur and S. Vassilvitskii, ?k-means++: the advantages of careful seeding,? in SODA ?07 Proceedings
of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms, New Orleans, Louisiana, Jan. 7?9,
2007, pp. 1027?1035.
[17] J. S. Yedidia, W. T. Freeman, and Y. Weiss, ?Constructing free-energy approximations and generalized
belief propagation algorithms,? IEEE Transactions on Information Theory, vol. 51, no. 7, pp. 2282?2312,
Jul. 2005.
[18] M. Bayati and A. Montanari, ?The dynamics of message passing on dense graphs, with applications to
compressed sensing,? IEEE Transactions on Information Theory, vol. 57, no. 2, pp. 764?785, Feb. 2011.
[19] H. W. Kuhn, ?The Hungarian method for the assignment problem,? Naval Research Logistics Quarterly,
vol. 2, no. 1?2, pp. 83?97, Mar. 1955.
[20] F. S. Samaria and A. C. Harter, ?Parameterisation of a stochastic model for human face identification,? in
Proceedings of 2nd IEEE Workshop on Applications of Computer Vision, Sarasota FL, Dec. 1994, pp.
138?142. [Online]. Available: http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html

9


----------------------------------------------------------------

title: 3812-streaming-k-means-approximation.pdf

Streaming k-means approximation
Nir Ailon
Google Research
nailon@google.com

Ragesh Jaiswal?
Columbia University
rjaiswal@gmail.com

Claire Monteleoni?
Columbia University
cmontel@ccls.columbia.edu

Abstract
We provide a clustering algorithm that approximately optimizes the k-means objective, in the one-pass streaming setting. We make no assumptions about the
data, and our algorithm is very light-weight in terms of memory, and computation. This setting is applicable to unsupervised learning on massive data sets, or
resource-constrained devices. The two main ingredients of our theoretical work
are: a derivation of an extremely simple pseudo-approximation batch algorithm
for k-means (based on the recent k-means++), in which the algorithm is allowed
to output more than k centers, and a streaming clustering algorithm in which batch
clustering algorithms are performed on small inputs (fitting in memory) and combined in a hierarchical manner. Empirical evaluations on real and simulated data
reveal the practical utility of our method.

1

Introduction

As commercial, social, and scientific data sources continue to grow at an unprecedented rate, it is
increasingly important that algorithms to process and analyze this data operate in online, or one-pass
streaming settings. The goal is to design light-weight algorithms that make only one pass over the
data. Clustering techniques are widely used in machine learning applications, as a way to summarize
large quantities of high-dimensional data, by partitioning them into ?clusters? that are useful for
the specific application. The problem with many heuristics designed to implement some notion of
clustering is that their outputs can be hard to evaluate. Approximation guarantees, with respect to
some reasonable objective, are therefore useful. The k-means objective is a simple, intuitive, and
widely-cited clustering objective for data in Euclidean space. However, although many clustering
algorithms have been designed with the k-means objective in mind, very few have approximation
guarantees with respect to this objective.
In this work, we give a one-pass streaming algorithm for the k-means problem. We are not aware
of previous approximation guarantees with respect to the k-means objective that have been shown
for simple clustering algorithms that operate in either online or streaming settings. We extend work
of Arthur and Vassilvitskii [AV07] to provide a bi-criterion approximation algorithm for k-means,
in the batch setting. They define a seeding procedure which chooses a subset of k points from a
batch of points, and they show that this subset gives an expected O(log (k))-approximation to the kmeans objective. This seeding procedure is followed by Lloyd?s algorithm1 which works very well
in practice with the seeding. The combined algorithm is called k-means++, and is an O(log (k))approximation algorithm, in expectation.2 We modify k-means++ to obtain a new algorithm, kmeans#, which chooses a subset of O(k log (k)) points, and we show that the chosen subset of
?

Department of Computer Science. Research supported by DARPA award HR0011-08-1-0069.
Center for Computational Learning Systems
1
Lloyd?s algorithm is popularly known as the k-means algorithm
2
Since the approximation guarantee is proven based on the seeding procedure alone, for the purposes of this
exposition we denote the seeding procedure as k-means++.
?

1

points gives a constant approximation to the k-means objective. Apart from giving us a bi-criterion
approximation algorithm, our modified seeding procedure is very simple to analyze.
[GMMM+03] defines a divide-and-conquer strategy to combine multiple bi-criterion approximation
algorithms for the k-medoid problem to yield a one-pass streaming approximation algorithm for
k-median. We extend their analysis to the k-means problem and then use k-means++ and k-means#
in the divide-and-conquer strategy, yielding an extremely efficient single pass streaming algorithm
with an O(c? log (k))-approximation guarantee, where ? ? log n/ log M , n is the number of input
points in the stream and M is the amount of work memory available to the algorithm. Empirical
evaluations, on simulated and real data, demonstrate the practical utility of our techniques.
1.1

Related work

There is much literature on both clustering algorithms [Gon85, Ind99, VW02, GMMM+03,
KMNP+04, ORSS06, AV07, CR08, BBG09, AL09], and streaming algorithms [Ind99, GMMM+03,
M05, McG07].3 There has also been work on combining these settings: designing clustering algorithms that operate in the streaming setting [Ind99, GMMM+03, CCP03]. Our work is inspired by
that of Arthur and Vassilvitskii [AV07], and Guha et al. [GMMM+03], which we mentioned above
and will discuss in further detail. k-means++, the seeding procedure in [AV07], had previously been
analyzed by [ORSS06], under special assumptions on the input data.
In order to be useful in machine learning applications, we are concerned with designing algorithms
that are extremely light-weight and practical. k-means++ is efficient, very simple, and performs
well in practice. There do exist constant approximations to the k-means objective, in the nonstreaming setting, such as a local search technique due to [KMNP+04].4 A number of works
[LV92, CG99, Ind99, CMTS02, AGKM+04] give constant approximation algorithms for the related k-median problem in which the objective is to minimize the sum of distances of the points to
their nearest centers (rather than the square of the distances as in k-means), and the centers must be
a subset of the input points. It is popularly believed that most of these algorithms can be extended to
work for the k-means problem without too much degredation of the approximation, however there
is no formal evidence for this yet. Moreover, the running times of most of these algorithms depend
worse than linearly on the parameters (n, k, and d) which makes these algorithms less useful in practice. As future work, we propose analyzing variants of these algorithms in our streaming clustering
algorithm, with the goal of yielding a streaming clustering algorithm with a constant approximation
to the k-means objective.
Finally, it is important to make a distinction from some lines of clustering research which involve
assumptions on the data to be clustered. Common assumptions include i.i.d. data, e.g. [BL08], and
data that admits a clustering with well separated means e.g. in [VW02, ORSS06, CR08]. Recent
work [BBG09] assumes a ?target? clustering for the specific application and data set, that is close
to any constant approximation of the clustering objective. In contrast, we prove approximation
guarantees with respect to the optimal k-means clustering, with no assumptions on the input data.5
As in [AV07], our probabilistic guarantees are only with respect to randomness in the algorithm.
1.1.1

Preliminaries

The k-means clustering problem is defined as follows: Given n points X ? Rd and a weight
d
function w : X ? R
!, the goal is to find a2subset C ? R , |C| = k such that the following quantity is
6
minimized: ?C = x?X w(x)?D(x, C) , where D(x, C) denotes the #2 distance of x to the nearest
point in C. When the subset C is clear from the context, we denote this distance by D(x). Also,
for two points x, y, D(x, y) denotes the #2 distance between x and y. The subset C is alternatively
called a clustering of X and ?C is called the potential function corresponding to the clustering. We
will use the term ?center? to refer to any c ? C.
3

For a comprehensive survey of streaming results and literature, refer to [M05].
In recent, independent work, Aggarwal, Deshpande, and Kannan [ADK09] extend the seeding procedure of
k-means++ to obtain a constant factor approximation algorithm which outputs O(k) centers. They use similar
techniques to ours, but reduce the number of centers by using a stronger concentration property.
5
It may be interesting future work to analyze our algorithm in special cases, such as well-separated clusters.
6
For the unweighted case, we can assume that w(x) = 1 for all x.
4

2

Definition 1.1 (Competitive ratio, b-approximation). Given an algorithm B for the k-means problems, let ?C be the potential of the clustering C returned by B (on some input set which is implicit)
and let ?COP T denote the potential of the optimal clustering COP T . Then the competitive ratio is
defined to be the worst case ratio ?C?C . The algorithm B is said to be b-approximation algorithm

if

?C
?COP T

OP T

? b.

The previous definition might be too strong for an approximation algorithm for some purposes. For
example, the clustering algorithm performs poorly when it is constrained to output k centers but it
might become competitive when it is allowed to output more centers.
Definition 1.2 ((a, b)-approximation). We call an algorithm B, (a, b)-approximation for the kmeans problem if it outputs a clustering C with ak centers with potential ?C such that ?C?C ? b in
OP T
the worst case. Where a > 1, b > 1.
Note that for simplicity, we measure the memory in terms of the words which essentially means that
we assume a point in Rd can be stored in O(1) space.

2

k-means#: The advantages of careful and liberal seeding

The k-means++ algorithm is an expected ?(log k)-approximation algorithm. In this section, we
extend the ideas in [AV07] to get an (O(log k), O(1))-approximation algorithm. Here is the kmeans++ algorithm:
1. Choose an initial center c1 uniformly at random from X .
2. Repeat (k ? 1) times:
! 2
)
3.
Choose the next center ci , selecting ci = x# ? X with probability P D(xD(x)
2.
x?X
(here D(.) denotes the distances w.r.t. to the subset of points chosen in the previous rounds)
Algorithm 1: k-means++
In the original definition of k-means++ in [AV07], the above algorithm is followed by Lloyd?s
algorithm. The above algorithm is used as a seeding step for Lloyd?s algorithm which is known
to give the best results in practice. On the other hand, the theoretical guarantee of the k-means++
comes from analyzing this seeding step and not Lloyd?s algorithm. So, for our analysis we focus on
this seeding step. The running time of the algorithm is O(nkd).
In the above algorithm X denotes the set of given points and for any point x, D(x) denotes the
distance of this point from the nearest center among the centers chosen in the previous rounds. To
get an (O(log k), O(1))-approximation algorithm, we make a simple change to the above algorithm.
We first set up the tools for analysis. These are the basic lemmas from [AV07]. We will need the
following definition first:
Definition 2.1 (Potential w.r.t. a set). Given a clustering
C, its potential with respect to some set A
!
is denoted by ?C (A) and is defined as ?C (A) = x?A D(x)2 , where D(x) is the distance of the
point x from the nearest point in C.
Lemma 2.2 ([AV07], Lemma 3.1). Let A be an arbitrary cluster in COP T , and let C be the clustering
with just one center, chosen uniformly at random from A. Then Exp[?C (A)] = 2 ? ?COP T (A).

Corollary 2.3. Let A be an arbitrary cluster in COP T , and let C be the clustering with just one
center, which is chosen uniformly at random from A. Then, Pr[?C (A) < 8?COP T (A)] ? 3/4
Proof. The proof follows from Markov?s inequality.

Lemma 2.4 ([AV07], Lemma 3.2). Let A be an arbitrary cluster in COP T , and let C be an arbitrary
clustering. If we add a random center to C from A, chosen with D2 weighting to get C # , then
Exp[?C ! (A)] ? 8 ? ?COP T (A).

Corollary 2.5. Let A be an arbitrary cluster in COP T , and let C be an arbitrary clustering. If
we add a random center to C from A, chosen with D2 weighting to get C # , then Pr[?C ! (A) <
32 ? ?COP T (A)] ? 3/4.
3

We will use k-means++ and the above two lemmas to obtain a (O(log k), O(1))-approximation
algorithm for the k-means problem. Consider the following algorithm:
1. Choose 3 ? log k centers independently and uniformly at random from X .
2. Repeat (k ? 1) times.
! 2
)
3.
Choose 3 ? log k centers independently and with probability P D(xD(x)
2.
x?X
(here D(.) denotes the distances w.r.t. to the subset of points chosen in the previous rounds)
Algorithm 2: k-means#
Note that the algorithm is almost the same as the k-means++ algorithm except that in each round
of choosing centers, we pick O(log k) centers rather than a single center. The running time of the
above algorithm is clearly O(ndk log k).
Let A = {A1 , ..., Ak } denote the set of clusters in the optimal clustering COP T . Let C i denote the
clustering after ith round of choosing centers. Let Aic denote the subset of clusters ? A such that
?A ? Aic , ?C i (A) ? 32 ? ?COP T (A).

We call this subset of clusters, the ?covered? clusters. Let Aiu = A\Aic be the subset of ?uncovered?
clusters. The following simple lemma shows that with constant probability step (1) of k-means#
picks a center such that at least one of the clusters gets covered, or in other words, |A1c | ? 1. Let us
call this event E.
Lemma 2.6. Pr[E] ? (1 ? 1/k).
Proof. The proof easily follows from Corollary 2.3.
Let Xci = ?A?Aic A and let Xui = X \ Xci . Now after the ith round, either ?C i (Xci ) ? ?C i (Xui )
or otherwise. In the former case, using Corollary 2.5, we show that the probability of covering an
uncovered cluster in the (i + 1)th round is large. In the latter case, we will show that the current set
of centers is already competitive with constant approximation ratio. Let us start with the latter case.
Lemma 2.7. If event E occurs ( |A1c | ? 1) and for any i > 1, ?C i (Xci ) > ?C i (Xui ), then ?C i ?
64?COP T .
Proof. We get the main result using the following sequence of inequalities: ?C i = ?C i (Xci ) +
?C i (Xui ) ? ?C i (Xci ) + ?C i (Xci ) ? 2 ? 32 ? ?COP T (Xci ) ? 64 ?COP T (using the definition of Xci ).
i
Lemma 2.8. If for any i ? 1, ?C i (Xci ) ? ?C i (Xui ), then Pr[|Ai+1
c | ? |Ac | + 1] ? (1 ? 1/k).

Proof. Note that in the (i + 1)th round, the probability that a center is chosen from a cluster ?
/ Aic is
?

(X i )

i
u
at least ? i (X Ci )+?
? 1/2. Conditioned on this event, with probability at least 3/4 any of the
i
u
C
C i (Xc )
centers x chosen in round (i + 1) satisfies ?C i ?x (A) ? 32 ? ?COP T (A) for some uncovered cluster
A ? Aiu . This means that with probability at least 3/8 any of the chosen centers x in round (i + 1)
satisfies ?C i ?x (A) ? 32 ? ?COP T (A) for some uncovered cluster A ? Aiu . This further implies that
with probability at least (1 ? 1/k) at least one of the chosen centers x in round (i + 1) satisfies
?C i ?x (A) ? 32 ? ?COP T (A) for some uncovered cluster A ? Aiu .

We use the above two lemmas to prove our main theorem.
Theorem 2.9. k-means# is a (O(log k), O(1))-approximation algorithm.
Proof. From Lemma 2.6 we know that event E (i.e., |Aic | ? 1) occurs. Given this, suppose for
any i > 1, after the ith round ?C i (Xc ) > ?C i (Xu ). Then from Lemma 2.7 we have ?C ? ?C i ?
64?COP T . If no such i exist, then from Lemma 2.8 we get that the probability that there exists a
cluster A ? A such that A is not covered even after k rounds(i.e., end of the algorithm) is at most:
1 ? (1 ? 1/k)k ? 3/4. So with probability at least 1/4, the algorithm covers all the clusters in A.
In this case from Lemma 2.8, we have ?C = ?C k ? 32 ? ?COP T .
We have shown that k-means# is a randomized algorithm for clustering which with probability at
least 1/4 gives a clustering with competitive ratio 64.
4

3

A single pass streaming algorithm for k-means

In this section, we will provide a single pass streaming algorithm. The basic ingredients for the algorithm is a divide and conquer strategy defined by [GMMM+03] which uses bi-criterion approximation algorithms in the batch setting. We will use k-means++ which is a (1, O(log k))-approximation
algorithm and k-means# which is a (O(log k), O(1))-approximation algorithm, to construct a single
pass streaming O(log k)-approximation algorithm for k-means problem. In the next subsection, we
develop some of the tools needed for the above.
3.1 A streaming (a,b)-approximation for k-means
We will show that a simple streaming divide-and-conquer scheme, analyzed by [GMMM+03] with
respect to the k-medoid objective, can be used to approximate the k-means objective. First we
present the scheme due to [GMMM+03], where in this case we use k-means-approximating algorithms as input.
Inputs: (a) Point set S ? Rd . Let n = |S|.
(b) Number of desired clusters, k ? N .
(c) A, an (a, b)-approximation algorithm to the k-means objective.
(d) A# , an (a# , b# )-approximation algorithm to the k-means objective.
1.
2.
3.
4.
5.
6.
7.

Divide S into groups S1 , S2 , . . . , S#
For each i ? {1, 2, . . . , #}
Run A on Si to get ? ak centers Ti = {ti1 , ti2 , . . .}
Denote the induced clusters of Si as Si1 ? Si2 ? ? ? ?
Sw ? T1 ? T2 ? ? ? ? ? T# , with weights w(tij ) ? |Sij |
Run A# on Sw to get ? a# k centers T
Return T
Algorithm 3: [GMMM+03] Streaming divide-and-conquer clustering

?
?
First note that when every batch Si has size nk, this algorithm takes one pass, and O(a nk)
memory. Now we will give an approximation guarantee.
Theorem 3.1. The algorithm above outputs a clustering that is an (a# , 2b + 4b# (b + 1))approximation to the k-means objective.
The a# approximation of the desired number of centers follows directly from the approximation
property of A# , with respect to the number of centers, since A# is the last algorithm to be run. It
remains to show the approximation of the k-means objective. The proof, which appears in the
Appendix, involves extending the analysis of [GMMM+03], to the case of the k-means objective.
Using the exposition in Dasgupta?s lecture notes [Das08], of the proof due to [GMMM+03], our
extension is straightforward, and differs in the following ways from the k-medoid analysis.
1. The k-means objective involves squared distance (as opposed to k-medoid in which the
distance is not squared), so the triangle inequality cannot be invoked directly. We replace it
with an application of the triangle inequality, followed by (a+b)2 ? 2a2 +2b2 , everywhere
it occurs, introducing several factors of 2.
2. Cluster centers are chosen from Rd , for the k-means problem, so in various parts of the
proof we save an approximation a factor of 2 from the k-medoid problem, in which cluster
centers must be chosen from the input data.
3.2 Using k-means++ and k-means# in the divide-and-conquer strategy
In the previous subsection, we saw how a (a, b)-approximation algorithm A and an (a# , b# )approximation algorithm A# can be used to get a single pass (a# , 2b + 4b# (b + 1))-approximation
streaming algorithm. We now have two randomized algorithms, k-means# which with probability
at least 1/4 is a (3 log k, 64)-approximation algorithm and k-means++ which is a (1, O(log k))approximation algorithm (the approximation factor being in expectation). We can now use these
two algorithms in the divide-and-conquer strategy to obtain a single pass streaming algorithm.
We use the following as algorithms as A and A# in the divide-and-conquer strategy (3):
5

A: ?Run k-means# on the data 3 log n times independently, and pick the clustering
with the smallest cost.?
A?: ?Run k-means++?
Weighted versus non-weighted. Note that k-means and k-means# are approximation algorithms
for the non-weighted case (i.e. w(x) = 1 for all points x). On the other hand, in the divide-andconquer strategy we need the algorithm A# , to work for the weighted case where the weights are
integers. Note that both k-means and k-means# can be easily generalized for the weighted case
when the weights are integers. Both algorithms compute probabilities based on the cost with respect
to the current clustering. This cost can be computed by taking into account the weights. For the
analysis, we can assume points with multiplicities equal to the integer weight of the point. The
memory required remains logarithmic in the input size, including the storing the weights.
"
#
"
#
Analysis. With probability at least 1 ? (3/4)3 log n ? 1 ? n1 , algorithm A is a (3 log k, 64)approximation algorithm. Moreover, the space requirement remains logarithmic ?
in the input size. In
step (3) of $
Algorithm 3 we run A on batches of data. Since each batch is of size nk the number of
batches is n/k, the probability
that A is a (3 log k, 64)-approximation algorithm for all of these
"
#?n/k
1
batches is at least 1 ? n
? 1/2. Conditioned on this event, the divide-and-conquer strategy
?
gives a O(log k)-approximation algorithm. The memory required is O(log(k) ? nk) times the
logarithm of the input size. Moreover, the algorithm has running time O(dnk log n log k).
3.3 Improved memory-approximation tradeoffs
We saw in the last section how to obtain a single-pass (a# , cbb# )-approximation for k-means using
first an (a, b)-approximation on input blocks and then an (a# , b# )-approximation on the union of the
output center
? sets, where c is some global constant. The optimal memory required for this scheme
was O(a nk). This immediately implies a tradeoff between the memory requirements (growing
like a), the number of centers outputted (which is a# k) and the approximation to the potential (which
is cbb# ) with respect to the optimal solution using k centers. A more subtle tradeoff is possible by a
recursive application of the technique in multiple levels. Indeed, the (a, b)-approximation could be
broken up in turn into two levels, and so on. This idea was used in [GMMM+03]. Here we make a
more precise account of the tradeoff between the different parameters.
Assume we have subroutines for performing (ai , bi )-approximation for k-means in batch mode, for
i = 1, . . . r (we will choose a1 , . . . , ar , b1 , . . . , br later). We will hold r buffers B1 , . . . , Br as
work areas, where the size of buffer Bi is Mi . In the topmost level, we will divide the input into
equal blocks of size M1 , and run our (a1 , b1 )-approximation algorithm on each block. Buffer B1
will be repeatedly reused for this task, and after each application of the approximation algorithm,
the outputted set of (at most) ka1 centers will be added to B2 . When B2 is filled, we will run
the (a2 , b2 )-approximation algorithm on the data and add the ka2 outputted centers to B3 . This
will continue until buffer Br fills, and the (ar , br )-approximation algorithm outputs the final ar k
centers. Let ti denote the number of times the i?th level algorithm is executed. Clearly we have
ti kai = Mi+1 ti+1 for i = 1, . . . , r ? 1. For the last stage we have tr = 1, which means that tr?1 =
Mr /kar?1 , tr?2 = Mr?1 Mr /k 2 ar?2 ar?1 and generally ti = Mi+1 ? ? ? Mr /k r?i ai ? ? ? ar?1 .7 But
M1 ???Mr
we must also have t1 = n/M1 , implying n = kr?1
a1 ???ar?1 . In order to minimize the total memory
!
Mi under the last constraint, using standard arguments in multivariate analysis we must have
"
#1/r
M1 = ? ? ? = Mr , or in other words Mi = nk r?1 a1 ? ? ? ar?1
? n1/r k(a1 ? ? ? ar?1 )1/r for all i.
The resulting one-pass algorithm will have an approximation guarantee of (ar , cr?1 b1 ? ? ? br ) (using
a straightforward extension of the result in the previous section) and memory requirement of at most
rn1/r k(a1 ? ? ? ar?1 )1/r .
Assume now that we are in the realistic setting in which the available memory is of fixed size
M ? k. We will choose r (below), and for each i = 1..r ? 1 we choose to either run k-means++
or the repeated k-means# (algorithm A in the previous subsection), i.e., (ai , bi ) = (1, O(log k))
or (3 log k, O(1)) for each i. For i = r, we choose k-means++, i.e., (ar , br ) = (1, O(log k)) (we
are interested in outputting exactly k centers as the final solution). Let q denote the number of

7
We assume all quotients are integers for simplicity of the proof, but note that fractional blocks would arise
in practice.

6

25

8

40
35

4
3

Cost in units of 107

6

5

Batch Lloyds
Divide and Conquer with km# and km++
Divide and Conquer with km++

45

20

Cost in units of 10

Cost in units of 109

6

50

Batch Lloyds
Divide and Conquer with km# and km++
Divide and Conquer with km++

Batch Lloyds
Online Lloyds
Divide and Conquer with km# and km++
Divide and Conquer with km++

7

15

10

30
25
20
15

2
5

10

1
0

5

5

10

15
k

20

25

0

5

10

15
k

20

25

0

5

10

15
k

20

25

Figure 1: Cost vs. k: (a) Mixtures of gaussians simulation, (b) Cloud data, (c) Spam data,.
indexes i ? [r ? 1] such that (ai , bi ) = (3 log k, O(1)). By the above discussion, the memory is
used optimally if M = rn1/r k(3 log k)q/r , in which case the final approximation guarantee will be
c?r?1 (log k)r?q , for some global c? > 0. We concentrate on the case M growing polynomially in n,
say M = n? for some ? < 1. In this case, the memory optimality constraint implies r = 1/? for
n large enough (regardless of the choice of q). This implies that the final approximation guarantee
is best if q = r ? 1, in other words, we choose the repeated k-means# for levels 1..r ? 1, and
k-means++ for level r. Summarizing, we get:
Theorem 3.2. If there is access to memory of size M = n? for some fixed ? > 0, then for sufficiently
large n the best application of the multi-level scheme described above is obtained by running r =
-?. = -log n/ log M . levels, and choosing the repeated k-means# for all but the last level, in which
k-means++ is chosen. The resulting algorithm is a randomized one-pass streaming approximation
to k-means, with an approximation ratio of O(?
cr?1 (log k)), for some global c? > 0. The running
time of the algorithm is O(dnk 2 log n log k).
We should compare the above multi-level streaming algorithm with the state-of-art (in terms of
memory vs. approximation tradeoff) streaming algorithm for the k-median problem. Charikar,
Callaghan, and Panigrahy [CCP03] give a one-pass streaming algorithm for the k-median problem
which gives a constant factor approximation and uses O(k?poly log(n)) memory. The main problem
with this algorithm from a practical point of view is that the average processing time per item is
large. It is proportional to the amount of memory used, which is poly-logarithmic in n. This might
be undesirable in practical scenarios where we need to process a data item quickly when it arrives. In
contrast, the average per item processing time using the divide-and-conquer-strategy is constant and
furthermore the algorithm can be pipelined (i.e. data items can be temporarily stored in a memory
buffer and quickly processed before the the next memory buffer is filled). So, even if [CCP03] can
be extended to the k-means setting, streaming algorithms based on the divide-and-conquer-strategy
would be more interesting from a practical point of view.

4

Experiments

Datasets. In our discussion, n denotes the number of points in the data, d denotes the dimension,
and k denotes the number of clusters. Our first evaluation, detailed in Tables 1a)-c) and Figure 1,
compares our algorithms on the following data: (1) norm25 is synthetic data generated in the following manner: we choose 25 random vertices from a 15 dimensional hypercube of side length 500.
We then add 400 gaussian random points (with variance 1) around each of these points.8 So, for this
data n = 10, 000 and d = 15. The optimum cost for k = 25 is 1.5026 ? 105 . (2) The UCI Cloud
dataset consists of cloud cover data [AN07]. Here n = 1024 and d = 10. (3) The UCI Spambase
dataset is data for an e-mail spam detection task [AN07]. Here n = 4601 and d = 58.
To compare against a baseline method known to be used in practice, we used Lloyd?s algorithm,
commonly referred to as the k-means algorithm. Standard Lloyd?s algorithm operates in the batch
setting, which is an easier problem than the one-pass streaming setting, so we ran experiments with
this algorithm to form a baseline. We also compare to an online version of Lloyd?s algorithm,
however the performance is worse than the batch version, and our methods, for all problems, so we
8

Testing clustering algorithms on this simulation distribution was inspired by [AV07].

7

k
5
10
15
20
25
k
5
10
15
20
25
k
5
10
15
20
25

BL
5.1154 ? 109
3.3080 ? 109
2.0123 ? 109
1.4225 ? 109
0.8602 ? 109

OL
6.5967 ? 109
6.0146 ? 109
4.3743 ? 109
3.7794 ? 109
2.8859 ? 109

DC-1
7.9398 ? 109
4.5954 ? 109
2.5468 ? 109
1.0718 ? 109
2.7842 ? 105

DC-2
7.8474 ? 109
4.6829 ? 109
2.5898 ? 109
1.1403 ? 109
2.7298 ? 105

BL
1.25
2.05
3.88
8.62
13.13

OL
1.32
2.45
3.49
4.69
6.04

DC-1
14.37
45.39
95.22
190.73
283.19

BL
1.12
1.20
2.18
2.59
2.43

OL
0.13
0.25
0.35
0.47
0.52

DC-1
1.73
5.64
10.98
25.72
36.17

BL
4.9139 ? 108
1.6952 ? 108
1.5670 ? 108
1.5196 ? 108
1.5168 ? 108

OL
1.7001 ? 109
1.6930 ? 109
1.4762 ? 109
1.4766 ? 109
1.4754 ? 109

DC-1
3.4021 ? 108
1.0206 ? 108
5.5095 ? 107
3.3400 ? 107
2.3151 ? 107

DC-2
3.3963 ? 108
1.0463 ? 108
5.3557 ? 107
3.2994 ? 107
2.3391 ? 107

BL
9.68
34.78
67.54
100.44
109.41

OL
0.70
1.31
1.88
2.57
3.04

DC-1
11.65
40.14
77.75
194.01
274.42

BL
1.7707 ? 107
0.7683 ? 107
0.5012 ? 107
0.4388 ? 107
0.3839 ? 107

OL
1.2401 ? 108
8.5684 ? 107
8.4633 ? 107
6.5110 ? 107
6.3758 ? 107

DC-1
2.2924 ? 107
8.3363 ? 106
4.9667 ? 106
3.7479 ? 106
2.8895 ? 106

DC-2
2.2617 ? 107
8.7788 ? 106
4.8806 ? 106
3.7536 ? 106
2.9014 ? 106

DC-2
9.93
21.09
30.34
41.49
53.07
DC-2
0.92
1.87
2.67
4.19
4.82
DC-2
5.14
9.75
14.41
22.76
27.10

Table 1: Columns 2-5 have the clustering cost and columns 6-9 have time in sec. a) norm25 dataset,
b) Cloud dataset, c) Spambase dataset.
Memory/
#levels
1024/0
480/1
360/2

Cost

Time
6

8.74 ? 10
8.59 ? 106
8.61 ? 106

5.5
3.6
3.8

Memory/
#levels
2048/0
1250/1
1125/2

Cost

Time
4

5.78 ? 10
5.36 ? 104
5.15 ? 104

30
25
26

Memory/
#levels
4601/0
880/1
600/2

Cost

Time
8

1.06 ? 10
0.99 ? 108
1.03 ? 108

34
20
19.5

Table 2: Multi-level hierarchy evaluation: a) Cloud dataset, k = 10, b) A subset of norm25 dataset,
n = 2048, k = 25, c) Spambase dataset, k = 10. The memory size decreases as the number of
levels of the hierarchy increases. (0 levels means running batch k-means++ on the data.)
do not include it in our plots for the real data sets.9 Tables 1a)-c) shows average k-means cost (over
10 random restarts for the randomized algorithms: all but Online Lloyd?s) for these algorithms:
(1) BL: Batch Lloyd?s, initialized with random centers in the input data, and run to convergence.10
(2) OL: Online Lloyd?s.
(3) DC-1: The simple 1-stage divide and conquer algorithm of Section 3.2.
(4) DC-2: The simple 1-stage divide and conquer algorithm 3 of Section 3.1. The sub-algorithms
used are A = ?run k-means++ 3 ? log n times and pick best clustering,? and A? is k-means++. In our
context, k-means++ and k-means# are only the seeding step, not followed by Lloyd?s algorithm.
In all problems, our streaming methods achieve much lower cost than Online Lloyd?s, for all settings
of k, and lower cost than Batch Lloyd?s for most settings of k (including the correct k = 25, in
norm25). The gains with respect to batch are noteworthy, since the batch problem is less constrained
than the one-pass streaming problem. The performance of DC-1 and DC-2 is comparable.
Table 2 shows an evaluation of the one-pass multi-level hierarchical algorithm of Section 3.3, on the
different datasets, simulating different memory restrictions. Although our worst-case theoretical results imply an exponential clustering cost as a function of the number of levels, our results show a far
more optimistic outcome in which adding levels (and limiting memory) actually improves the outcome. We conjecture that our data contains enough information for clustering even on chunks that fit
in small buffers, and therefore the results may reflect the benefit of the hierarchical implementation.
Acknowledgements. We thank Sanjoy Dasgupta for suggesting the study of approximation algorithms for k-means in the streaming setting, for excellent lecture notes, and for helpful discussions.
9
10

Despite the poor performance we observed, this algorithm is apparently used in practice, see [Das08].
We measured convergence by change in cost less than 1.

8

References
[ADK09] Ankit Aggarwal, Amit Deshpande and Ravi Kannan: Adaptive Sampling for k-means Clustering.
APPROX, 2009.
[AL09] Nir Ailon and Edo Liberty: Correlation Clustering Revisited: The ?True? Cost of Error Minimization
Problems. To appear in ICALP 2009.
[AMS96] Noga Alon, Yossi Matias, and Mario Szegedy.: The space complexity of approximating the frequency moments. In Proceedings of the Twenty-Eighth Annual ACM Symposium on Theory of Computing, pages 20?29, 1996.
[AV06] David Arthur and Sergei Vassilvitskii: Worst-case and smoothed analyses of the icp algorithm, with
an application to the k-means method. FOCS, 2006
[AV07] David Arthur and Sergei Vassilvitskii: k-means++: the advantages of careful seeding. SODA, 2007.
[AGKM+04] V. Arya, N. Garg, R. Khandekar, A. Meyerson, K. Munagala, and V. Pandit: Local search heuristics for k-median and facility location problems. Siam Journal of Computing, 33(3):544?562, 2004.
[AN07] A.
Asuncion
and
D.J.
Newman:
UCI
Machine
Learning
Repository.
http://www.ics.uci.edu/?mlearn/MLRepository.html, University of California, Irvine, School of
Information and Computer Sciences, 2007.
[BBG09] Maria-Florina Balcan, Avrim Blum, and Anupam Gupta: Approximate Clustering without the Approximation. SODA, 2009.
[BL08] S. Ben-David and U. von Luxburg: Relating clustering stability to properties of cluster boundaries.
COLT, 2008
[CCP03] Moses Charikar and Liadan O?Callaghan and Rina Panigrahy: Better streaming algorithms for clustering problems. STOC, 2003.
[CG99] Moses Charikar and Sudipto Guha: Improved combinatorial algorithms for the facility location and
k-medians problem. FOCS, 1999.
[CMTS02] M. Charikar, S. Guha , E Tardos, and D. Shmoys: A Constant Factor Approximation Algorithm
for the k-Median Problem. Journal of Computer and System Sciences, 2002.
[CR08] Kamalika Chaudhuri and Satish Rao: Learning Mixtures of Product Distributions using Correlations
and Independence. COLT, 2008.
[Das08] Sanjoy Dasgupta.: Course notes, CSE 291: Topics in unsupervised learning. http://wwwcse.ucsd.edu/ dasgupta/291/index.html, University of California, San Diego, Spring 2008.
[Gon85] T. F. Gonzalez: Clustering to minimize the maximum intercluster distance. Theoretical Computer
Science, 38, pages 293?306, 1985.
[GMMM+03] Sudipto Guha, Adam Meyerson, Nina Mishra, Rajeev Motwani, and Liadan O?Callaghan: Clustering Data Streams: Theory and Practice. IEEE Transactions on Knowledge and Data Engineering,
15(3): 515?528, 2003.
[Ind99] Piotr Indyk: Sublinear Time Algorithms for Metric Space Problems. STOC, 1999.
[JV01]

K. Jain and Vijay Vazirani: Approximation Algorithms for Metric Facility Location and k-Median
Problems Using the Primal-Dual Schema and Lagrangian Relaxation. Journal of the ACM. 2001.

[KMNP+04] T. Kanungo, D. M. Mount, N. Netanyahu, C. Piatko, R. Silverman, and A. Y. Wu: A Local
Search Approximation Algorithm for k-Means Clustering, Computational Geometry: Theory and
Applications, 28, 89-112, 2004.
[LV92] J. Lin and J. S. Vitter: Approximation Algorithms for Geometric Median Problems. Information
Processing Letters, 1992.
[McG07] Andrew McGregor: Processing Data Streams. Ph.D. Thesis, Computer and Information Science,
University of Pennsylvania, 2007.
[M05]

S. Muthukrishnan: Data Streams: Algorithms and Applications, NOW Publishers, Inc., Hanover MA

[ORSS06] Rafail Ostrovsky, Yuval Rabani, Leonard J. Schulman, Chaitanya Swamy: The effectiveness of
Lloyd-type methods for the k-means problem. FOCS, 2006.
[VW02] V. Vempala and G. Wang: A spectral algorithm of learning mixtures of distributions. pages 113?123,
FOCS, 2002.

9


----------------------------------------------------------------

title: 1076-learning-sparse-perceptrons.pdf

Learning Sparse Perceptrons

Jeffrey C. Jackson
Mathematics & Computer Science Dept.
Duquesne University
600 Forbes Ave
Pittsburgh, PA 15282
jackson@mathcs.duq.edu

Mark W. Craven
Computer Sciences Dept.
University of Wisconsin-Madison
1210 West Dayton St.
Madison, WI 53706
craven@cs.wisc.edu

Abstract
We introduce a new algorithm designed to learn sparse perceptrons over input representations which include high-order features.
Our algorithm, which is based on a hypothesis-boosting method,
is able to PAC-learn a relatively natural class of target concepts.
Moreover, the algorithm appears to work well in practice: on a set
of three problem domains, the algorithm produces classifiers that
utilize small numbers of features yet exhibit good generalization
performance. Perhaps most importantly, our algorithm generates
concept descriptions that are easy for humans to understand.

1

Introd uction

Multi-layer perceptron (MLP) learning is a powerful method for tasks such as concept classification. However, in many applications, such as those that may involve
scientific discovery, it is crucial to be able to explain predictions. Multi-layer perceptrons are limited in this regard, since their representations are notoriously difficult
for humans to understand. We present an approach to learning understandable,
yet accurate, classifiers. Specifically, our algorithm constructs sparse perceptrons,
i.e., single-layer perceptrons that have relatively few non-zero weights. Our algorithm for learning sparse perceptrons is based on a new hypothesis boosting algorithm (Freund & Schapire, 1995). Although our algorithm was initially developed
from a learning-theoretic point of view and retains certain theoretical guarantees (it
PAC-learns the class of sparse perceptrons), it also works well in practice. Our experiments in a number of real-world domains indicate that our algorithm produces
perceptrons that are relatively comprehensible, and that exhibit generalization performance comparable to that of backprop-trained MLP's (Rumelhart et al., 1986)
and better than decision trees learned using C4.5 (Quinlan, 1993).

Learning Sparse Perceptrons

655

We contend that sparse perceptrons, unlike MLP's, are comprehensible because they
have relatively few parameters, and each parameter describes a simple (Le. linear)
relationship. As evidence that sparse perceptrons are comprehensible, consider that
such linear functions are commonly used to express domain knowledge in fields such
as medicine (Spackman, 1988) and molecular biology (Stormo, 1987).

2

Sparse Perceptrons

A perceptron is a weighted threshold over the set of input features and over higherorder features consisting of functions operating on only a limited number of the
input features. Informally, a sparse perceptron is any perceptron that has relatively
few non-zero weights. For our later theoretical results we will need a more precise
definition of sparseness which we develop now. Consider a Boolean function I :
{O, 1}n -t { -1, + 1}. Let Ck be the set of all conjunctions of at most k of the inputs
to I. C k includes the "conjunction" of 0 inputs, which we take as the identically
1 function. All of the functions in C k map to {-1,+1}, and every conjunction in
C k occurs in both a positive sense (+1 represents true) and a negated sense (-1
represents true). Then the function I is a k-perceptron if there is some integer s
such that I(x) = sign(L::=1 hi(x)), where for all i, hi E Ck, and sign(y) is undefined
if y = 0 and is y/lyl otherwise. Note that while we have not explicitly shown any
weights in our definition of a k-perceptron I, integer weights are implicitly present
in that we allow a particular hi E Ck to appear more than once in the sum defining
I. In fact, it is often convenient to think of a k-perceptron as a simple linear
discriminant function with integer weights defined over a feature space with O(nk)
features, one feature for each element of Ck ?
We call a given collection of s conjunctions hi E C k a k-perceptron representation of
the corresponding function I, and we call s the size of the representation. We define
the size of a given k-perceptron function I as the minimal size of any k-perceptron
representation of I. An s-sparse k-perceptron is a k-perceptron I such that the size
of I is at most s. We denote by PI: the set of Boolean functions over {O, 1}n which
can be represented as k-perceptrons, and we define Pk = Un Pi:. The subclass of
s-sparse k-perceptrons is denoted by Pk,/l" We are also interested in the class P~
of k-perceptrons with real-valued weights, at most r of which are non-zero.

3

The Learning Algorithm

In this section we develop our learning algorithm and prove certain performance
guarantees. Our algorithm is based on a recent "hypothesis boosting" algorithm
that we describe after reviewing some basic learning-theory terminology.
3.1

PAC Learning and Hypothesis Boosting

Following Valiant (1984), we say that a function class :F (such as Pk for fixed k)
is (strongly) PAC-learnable if there is an algorithm A and a polynomial function
PI such that for any positive f and 8, any I E :F (the target junction), and any
probability distribution D over the domain of I, with probability at least 1 8, algorithm A(EX(f, D), f, 8) produces a function h (the hypothesis) such that
Pr[PrD[/(x) I- hex)] > f] < 8. The outermost probability is over the random choices
made by the EX oracle and any random choices made by A. Here EX(f, D) denotes
an oracle that, when queried, chooses a vector of input values x with probability
D and returns the pair (x,/(x)) to A. The learning algorithm A must run in time
PI (n, s, c 1 , 8- 1 ), where n is the length of the input vector to I and s is the size of

J. C. JACKSON, M. W. CRAVEN

656

AdaBoost
Input: training set S of m examples of function
is (~ - 'Y)-approximate, l'
Algorithm:

f, weak learning algorithm WL that

1. T +-- ~ In(m)
2. for all xES, w(x) +-- l/m
3. for i = 1 to T do
4.
for all XES, Di(X) +-- w(x)/
w(x).
5.
invoke WL on S and distribution Di, producing weak hypothesis hi

L:l=l

6.

+-- L:z .h;(z);oI:/(z) Di(X)
(3i +-- ?i/ (1 - ?i)
?i

7.
8.
for all XES, if h(x) = f(x) then w(x) +-- w(x) . (3i
9. enddo
Output: h(x) == sign

(L::=l -In((3i) . hi{x))
Figure 1: The AdaBoost algorithm.

f; the algorithm is charged one unit of time for each call to EX. We sometimes
call the function h output by A an ?-approximator (or strong approximator) to f
with respect to D. If F is PAC-learnable by an algorithm A that outputs only
hypotheses in class 1? then we say that F is PAC-learnable by 1?. If F is PAClearnable for ? = 1/2 - 1/'P2(n, s), where'P2 is a polynomial function, then :F is
weakly PA C-learnable, and the output hypothesis h in this case is called a weak
approximator.

Our algorithm for finding sparse perceptrons is, as indicated earlier, based on the
notion of hypothesis boosting. The specific boosting algorithm we use (Figure 1)
is a version of the recent AdaBoost algorithm (Freund & Schapire, 1995). In the
next section we apply AdaBoost to "boost" a weak learning algorithm for Pk,8 into
a strong learner for Pk,8' AdaBoost is given a set S of m examples of a function
f : {O,1}n ---+ {-1, +1} and a weak learning algorithm WL which takes ? = ! - l'
for a given l' b must be bounded by an inverse polynomial in nand s). Adaf300st
runs for T = In(m)/(2'Y2) stages. At each stage it creates a probability distribution
Di over the training set and invokes WL to find a weak hypothesis hi with respect
to Di (note that an example oracle EX(j, Di) can be simulated given Di and S).
At the end of the T stages a final hypothesis h is output; this is just a weighted
threshold over the weak hypotheses {hi I 1 ~ i ~ T}. If the weak learner succeeds
in producing a (~-'Y)-approximator at each stage then AdaBoost's final hypothesis
is guaranteed to be consistent with the training set (Freund & Schapire, 1995).
3.2

PAC-Learning Sparse k-Perceptrons

We now show that sparse k-perceptrons are PAC learnable by real-weighted kperceptrons having relatively few nonzero weights. Specifically, ignoring log factors,
Pk,8 is learnable by P~O(82) for any constant k. We first show that, given a training
set for any f E Pk,8' we can efficiently find a consistent h E p~( 8 2 )' This consistency algorithm is the basis of the algorithm we later apply to empirical learning
problems. We then show how to turn the consistency algorithm into a PAC learning
algorithm. Our proof is implicit in somewhat more general work by Freund (1993),
although he did not actually present a learning algorithm for this class or analyze

Learning Sparse Perceptrons

657

the sample size needed to ensure f-approximation, as we do. Following Freund, we
begin our development with the following lemma (Goldmann et al., 1992):
Lemma 1 (Goldmann Hastad Razhorov) For I: {0,1}n -+ {-1,+1} and H,

any set 01 functions with the same domain and range, il I can be represented as
I(x) = sign(L::=l hi(X?, where hi E H, then lor any probability distribution D
over {O, 1}n there is some hi such that PrD[f(x) ?- hi(x)] ~ ~ - 218 '

If we specialize this lemma by taking H = Ck (recall that Ck is the set of conjunctions of at most k input features of f) then this implies that for any I E Pk,8 and
any probability distribution D over the input features of I there is some hi E Ck
that weakly approximates I with respect to D. Therefore, given a training set S
and distribution D that has nonzero weight only on instances in S, the following
simple algorithm is a weak learning algorithm for Pk: exhaustively test each of the
O(nk) possible conjunctions of at most k features until we find a conjunction that
218 )-approximates I with respect to D (we can efficiently compute the approximation of a conjunction hi by summing the values of D over those inputs where hi

a-

and I agree). Any such conjunction can be returned as the weak hypothesis. The
above lemma proves that if I is a k-perceptron then this exhaustive search must
succeed at finding such a hypothesis. Therefore, given a training set of m examples
of any s-sparse k-perceptron I, AdaBoost run with the above weak learner will, after 2s2In(m) stages, produce a hypothesis consistent with the training set. Because
each stage adds one weak hypothesis to the output hypothesis, the final hypothesis
will be a real-weighted k-perceptron with at most 2s2In(m) nonzero weights.
We can convert this consistency algorithm to a PAC learning algorithm as follows.
First, given a finite set of functions F, it is straightforward to show the following
(see, e.g., Haussler, 1988):
Lemma 2 Let F be a finite set ollunctions over a domain X. For any function
lover X, any probability distribution D over X, and any positive f and ~, given a
set S ofm examples drawn consecutively from EX(f, D), where m ~ f-1(ln~-1 +
In IFI), then Pr[3h E F I "Ix E S f(x) = h(x) & Prv[/(x) ?- h(x)] > f] < ~, where
the outer probability is over the random choices made by EX(f,D).
The consistency algorithm above finds a consistent hypothesis in P~, where r

=

2s2 In(m). Also, based on a result of Bruck (1990), it can be shown that In IP~I =
o (r2 + kr log n). Therefore, ignoring log factors, a randomly-generated training set
of size O(kS4 If) is sufficient to guarantee that, with high probability, our algorithm
will produce an f-approximator for any s-sparse k-perceptron target. In other words,
the following is a PAC algorithm for Pk,8: compute sufficiently large (but polynomial
in the PAC parameters) m, draw m examples from EX(f, D) to create a training
set, and run the consistency algorithm on this training set.
So far we have shown that sparse k-perceptrons are learnable by sparse perceptron
hypotheses (with potentially polynomially-many more weights). In practice, of
course, we expect that many real-world classification tasks cannot be performed
exactly by sparse perceptrons. In fact, it can be shown that for certain (reasonable)
definitions of "noisy" sparse perceptrons (loosely, functions that are approximated
reasonably well by sparse perceptrons), the class of noisy sparse k-perceptrons is
still PAC-learnable. This claim is based on results of Aslam and Decatur (1993),
who present a noise-tolerant boosting algorithm. In fact, several different boosting
algorithms could be used to learn Pk,s (e.g., Freund, 1993). We have chosen to use
AdaBoost because it seems to offer significant practical advantages, particularly in
terms of efficiency. Also, our empirical results to date indicate that our algorithm

J. C. JACKSON, M. W. CRAVEN

658

works very well on difficult (presumably "noisy") real-world problems. However,
one potential advantage of basing the algorithm on one of these earlier boosters
instead of AdaBoost is that the algorithm would then produce a perceptron with
integer weights while still maintaining the sparseness guarantee of the AdaBoostbased algorithm.
3.3

Practical Considerations

We turn now to the practical details of our algorithm, which is based on the consistency algorithm above. First, it should be noted that the theory developed above
works over discrete input domains (Boolean or nominal-valued features). Thus, in
this paper, we consider only tasks with discrete input features. Also, because the
algorithm uses exhaustive search over all conjunctions of size k, learning time depends exponentially on the choice of k. In this study we to use k = 2 throughout,
since this choice results in reasonable learning times.
Another implementation concern involves deciding when the learning algorithm
should terminate. The consistency algorithm uses the size of the target function
in calculating the number of boosting stages. Of course, such size information is
not available in real-world applications, and in fact, the target function may not be
exactly representable as a sparse perceptron. In practice, we use cross validation
to determine an appropriate termination point. To facilitate comprehensibility, we
also limit the number of boosting stages to at most the number of weights that
would occur in an ordinary perceptron for the task. For similar reasons, we also
modify the criteria used to select the weak hypothesis at each stage so that simple
features are preferred over conjunctive features. In particular, given distribution
D at some stage j, for each hi E Ck we compute a correlation Ev[/ . hi]. We
then mUltiply each high-order feature's correlation by i. The hi with the largest
resulting correlation serves as the weak hypothesis for stage j.

4

Empirical Evaluation

In our experiments, we are interested in assessing both the generalization ability
and the complexity of the hypotheses produced by our algorithm. We compare our
algorithm to ordinary perceptrons trained using backpropagation (Rumelhart et al.,
1986), multi-layer perceptrons trained using backpropagation, and decision trees
induced using the C4.5 system (Quinlan, 1993). We use C4.5 in our experiments as
a representative of "symbolic" learning algorithms. Symbolic algorithms are widely
believed to learn hypotheses that are more comprehensible than neural networks.
Additionally, to test the hypothesis that the performance of our algorithm can be
explained solely by its use of second-order features, we train ordinary perceptrons
using feature sets that include all pairwise conjunctions, as well as the ordinary
features. To test the hypothesis that the performance of our algorithm can be
explained by its use of relatively few weights, we consider ordinary perceptrons
which have been pruned using a variant of the Optimal Brain Damage (OBD)
algorithm (Le Cun et al., 1989). In our version of OBD, we train a perceptron until
the stopping criteria are met, prune the weight with the smallest salience, and then
iterate the process. We use a validation set to decide when to stop pruning weights.
For each training set, we use cross-validation to select the number of hidden units
(5, 10, 20, 40 or 80) for the MLP's, and the pruning confidence level for the C4.5
trees. We use a validation set to decide when to stop training for the MLP's.
We evaluate our algorithm using three real-world domains: the voting data set from
the UC-Irvine database; a promoter data set which is a more complex superset of

Learning Sparse Perceptrons

domain
voting
promoter
coding

boosting
91.5%
92.7
72.9

659

Ta ble 1: 11est -set accuracy.
perceptrons
C4.5
multi-layer ordinary 2nd-order
90.8%
89.2%
89.2% * 92.2%
*
90.6
90.0
88.7
84.4
*
*
*
71.6
69.8
70.7
62.6

*

*

*

Table 2: Hypothesis complexity (# weights).
perceptrons
domain
boosting multi-layer ordinary 2nd-order
voting
450
12
651
30
promoters
41
2267
228
25764
protein coding
52
4270
60
1740

*

pruned
87.6% *
88.2
*
70.3

*

pruned
12
59
37

UC-Irvine one; and a data set in which the task is to recognize protein-coding
regions in DNA (Craven & Shavlik, 1993). We remove the physician-fee-freeze
feature from the voting data set to make the problem more difficult. We conduct
our experiments using a lO-fold cross validation methodology, except for in the
protein-coding domain. Because of certain domain-specific characteristics of this
data set, we use 4-fold cross-validation for our experiments with it.
Table 1 reports test-set accuracy for each method on all three domains. We measure the statistical significance of accuracy differences using a paired, two-tailed
t-test. The symbol '*' marks results in cases where another algorithm is less accurate than our boosting algorithm at the p ::; 0.05 level of significance. No other
algorithm is significantly better than our boosting method in any of the domains.
From these results we conclude that (1) our algorithm exhibits good generalization
performance on number of interesting real-world problems, and (2) the generalization performance of our algorithm is not explained solely by its use of second-order
features, nor is it solely explained by the sparseness of the perceptrons it produces.
An interesting open question is whether perceptrons trained with both pruning and
second-order features are able to match the accuracy of our algorithm; we plan to
investigate this question in future work.
Table 2 reports the average number of weights for all of the perceptrons. For all
three problems, our algorithm produces perceptrons with fewer weights than the
MLP's, the ordinary perceptrons, and the perceptrons with second-order features.
The sizes of the OBD-pruned perceptrons and those produced by our algorithm
are comparable for all three domains. Recall, however, that for all three tasks,
the perceptrons learned by our algorithm had significantly better generalization
performance than their similar-sized OBD-pruned counterparts. We contend that
the sizes of the perceptrons produced by our algorithm are within the bounds of
what humans can readily understand. In the biological literature, for example, linear
discriminant functions are frequently used to communicate domain knowledge about
sequences of interest. These functions frequently involve more weights than the
perceptrons produced by our algorithm. We conclude, therefore, that our algorithm
produces hypotheses that are not only accurate, but also comprehensible.
We believe that the results on the protein-coding domain are especially interesting.
The input representation for this problem consists of 15 nominal features representing 15 consecutive bases in a DNA sequence. In the regions of DNA that encode
proteins (the positive examples in our task), non-overlapping triplets of consecu-

660

J. C. JACKSON, M. W. eRA VEN

tive bases represent meaningful "words" called codons. In previous work (Craven
& Shavlik, 1993), it has been found that a feature set that explicitly represents
codons results in better generalization than a representation of just bases. However, we used the bases representation in our experiments in order to investigate the
ability of our algorithm to select the "right" second-order features. Interestingly,
nearly all of the second-order features included in our sparse perceptrons represent
conjunctions of bases that are in the same codon. This result suggests that our
algorithm is especially good at selecting relevant features from large feature sets.

5

Future Work

Our present algorithm has a number of limitations which we plan to address. Two
areas of current research are generalizing the algorithm for application to problems
with real-valued features and developing methods for automatically suggesting highorder features to be included in our algorithm's feature set.
Acknowledgements

Mark Craven was partially supported by ONR grant N00014-93-1-0998. Jeff Jackson
was partially supported by NSF grant CCR-9119319.

References
Aslam, J. A. & Decatur, S. E. (1993). General bounds on statistical query learning and
PAC learning with noise via hypothesis boosting. In Proc. of the 34th Annual Annual
Symposium on Foundations of Computer Science, (pp. 282-291).
Bruck, J . (1990). Harmonic analysis of polynomial threshold functions. SIAM Journal
of Discrete Mathematics, 3(2):168-177.
Craven, M . W. & Shavlik, J. W. (1993) . Learning to represent codons: A challenge
problem for constructive induction. In Proc. of the 13th International Joint Conf. on
Artificial Intelligence, (pp. 1319-1324), Chambery, France.
Freund, Y. (1993). Data Filtering and Distribution Modeling Algorithms for Machine
Learning. PhD thesis, University of California at Santa Cruz.
Freund, Y. & Schapire, R. E. (1995). A decision-theoretic generalization of on-line learning and an application to boosting. In Proc. of the ~nd Annual European Conf. on
Computational Learning Theory.
Goldmann, M., Hastad, J., & Razborov, A. (1992). Majority gates vs. general weighted
threshold gates. In Proc. of the 7th IEEE Conf. on Structure in Complexity Theory.
Haussler, D. (1988). Quantifying inductive bias: AI learning algorithms and Valiant's
learning framework. Artificial Intelligence, (pp. 177-221).
Le Cun, Y., Denker, J. S., & Solla, S. A. (1989). Optimal brain damage. In Touretzky,
D., editor, Advances in Neural Information Processing Systems (volume ~).
Quinlan, J. R. (1993). C4.5: Programs for Machine Learning. Morgan Kaufmann.
Rumelhart, D., Hinton, G., & Williams, R. (1986). Learning internal representations
by error propagation. In Rumelhart, D. & McClelland, J., editors, Parallel Distributed
Processing: Explorations in the microstructure of cognition. Volume 1. MIT Press.
Spackman, K. A. (1988). Learning categorical decision criteria. In Proc. of the 5th
International Conf. on Machine Learning, (pp. 36-46), Ann Arbor, MI.
Stormo, G. (1987). Identifying coding sequences. In Bishop, M. J. & Rawlings, C. J.,
editors, Nucleic Acid and Protein Sequence Analysis: A Practical Approach. IRL Press.
Valiant,1. G. (1984). A theory of the learnable. Comm. of the ACM, 27(11):1134-1142.


----------------------------------------------------------------

title: 1194-an-apobayesian-relative-of-winnow.pdf

An Apobayesian Relative of Winnow

Nick Littlestone
NEC Research Institute
4 Independence Way
Princeton, NJ 08540

Chris Mesterharm
NEC Research Institute
4 Independence Way
Princeton, NJ 08540

Abstract
We study a mistake-driven variant of an on-line Bayesian learning algorithm (similar to one studied by Cesa-Bianchi, Helmbold,
and Panizza [CHP96]). This variant only updates its state (learns)
on trials in which it makes a mistake. The algorithm makes binary
classifications using a linear-threshold classifier and runs in time linear in the number of attributes seen by the learner. We have been
able to show, theoretically and in simulations, that this algorithm
performs well under assumptions quite different from those embodied in the prior of the original Bayesian algorithm. It can handle
situations that we do not know how to handle in linear time with
Bayesian algorithms. We expect our techniques to be useful in
deriving and analyzing other apobayesian algorithms.

1

Introduction

We consider two styles of on-line learning. In both cases, learning proceeds in a
sequence of trials. In each trial, a learner observes an instance to be classified,
makes a prediction of its classification, and then observes a label that gives the
correct classification. One style of on-line learning that we consider is Bayesian.
The learner uses probabilistic assumptions about the world (embodied in a prior
over some model class) and data observed in past trials to construct a probabilistic
model (embodied in a posterior distribution over the model class). The learner uses
this model to make a prediction in the current trial. When the learner is told the
correct classification of the instance, the learner uses this information to update the
model, generating a new posterior to be used in the next trial.

In the other style of learning that we consider, the attention is on the correctness
of the predictions rather than on the model of the world. The internal state of the

An Apobayesian Relative o!Winnow

205

learner is only changed when the learner makes a mistake (when the prediction fails
to match the label). We call such an algorithm mistake-driven. (Such algorithms are
often called conservative in the computational learning theory literature.) There is a
simple way to derive a mistake-driven algorithm from anyon-line learning algorithm
(we restrict our attention in this paper to deterministic algorithms). The derived
algorithm is just like the original algorithm, except that before every trial, it makes
a record of its entire state, and after every trial in which its prediction is correct,
it resets its state to match the recorded state, entirely forgetting the intervening
trial. (Typically this is actually implemented not by making such a record, but by
merely omitting the step that updates the state.) For example, if some algorithm
keeps track of the number of trials it has seen, then the mistake-driven version of
this algorithm will end up keeping track of the number of mistakes it has made.
Whether the original or mistake-driven algorithm will do better depends on the task
and on how the algorithms are evaluated.
We will start with a Bayesian learning algorithm that we call SBSB and use this
procedure to derive a mistake-driven variant, SASB. Note that the variant cannot
be expected to be a Bayesian learning algorithm (at least in the ordinary sense)
since a Bayesian algorithm would make a prediction that minimizes the Bayes risk
based on all the available data, and the mistake-driven variant has forgotten quite
a bit. We call such algorithms apobayesian learning algorithms. This name is
intended to suggest that they are derived from Bayesian learning algorithms, but
are not themselves Bayesian. Our algorithm SASB is very close to an algorithm
of [CHP96). We study its application to different tasks than they do, analyzing its
performance when it is applied to linearly separable data as described below.
In this paper instances will be chosen from the instance space X = {a, l}n for some
n. Thus instances are composed of n boolean attributes. We consider only two
category classifications tasks, with predictions and labels chosen from Y = {a, I} .
We obtain a' bound on the number of mistakes SASB makes that is comparable to
bounds for various Winnow family algorithms given in [Lit88,Lit89). As for those
algorithms, the bound holds under the assumption that the points labeled 1 are
linearly separable from the points labeled 0, and the bound depends on the size 8 of
the gap between the two classes. (See Section 3 for a definition of 8.) The mistake
bound for SASB is 0 ( /or log ~ ). While this bound has an extra factor of log ~ not
present in the bounds for the Winnow algorithms, SASB has the advantage of not
needing any parameters. The Winnow family algorithms have parameters, and the
algorithms' mistake bounds depend on setting the parameters to values that depend
on 8. (Often, the value of 8 will not be known by the learner.) We expect the
techniques used to obtain this bound to be useful in analyzing other apobayesian
learning algorithms.
A number of authors have done related research regarding worst-case on-line
loss bounds including [Fre96,KW95,Vov90). Simulation experiments involving a
Bayesian algorithm and a mistake-driven variant are described in [Lit95). That
paper provides useful background for this paper. Note that our present analysis
techniques do not apply to the apobayesian algorithm studied there. The closest of
the original Winnow family algorithms to SASB appears to be the Weighted MaJority algorithm [LW94], which was analyzed for a case similar to that considered
in this paper in [Lit89). One should get a roughly correct impression of SASB if

N. Littlestone and C. Mesterharm

206

one thinks of it as a version of the Weighted Majority algorithm that learns its
parameters.
In the next section we describe the Bayesian algorithm that we start with. In
Section 3 we discuss its mistake-driven apobayesian variant. Section 4 mentions
some simulation experiments using these algorithms, and Section 5 is the conclusion.

2

A Bayesian Learning Algorithm

To describe the Bayesian learning algorithm we must specify a family of distributions over X x Y and a prior over this family of distributions. We parameterize
the distributions with parameters ((h, ... , 8n + l ) chosen from e = [0, 1]n+l. The
parameter 8n +1 gives the probability that the label is 1, and the parameter 8i gives
the probability that the ith attribute matches the label. Note that the probability
that the ith attribute is 1 given that the label is 1 equals the probability that the
ith attribute is 0 given that the label is O. We speak of this linkage between the
probabilities for the two classes as a symmetry condition. With this linkage, the
observation of a point from either class will affect the posterior distribution for both
classes. It is perhaps more typical to choose priors that allow the two classes to be
treated separately, so that the posterior for each class (giving the probability of elements of X conditioned on the label) depends only on the prior and on observations
from that class. The symmetry condition that we impose appears to be important
to the success of our analysis of the apobayesian variant of this algorithm. (Though
we impose this condition to derive the algorithm, it turns out that the apobayesian
variant can actually handle tasks where this condition is not satisfied.)
We choose a prior on e that gives probability 1 to the set of all elements
() = (81, ... , 8n +l ) E e for which at most one of 81 , ... ,8n does not equal
The prior is uniform on this set. Note that for any () in this set only a single attribute has a probability other than ~ of matching the label, and thus only a single
attribute is relevant. Concentrating on this set turns out to lead to an apobayesian
algorithm that can, in fact, handle more than one relevant attribute and that performs particularly well when only a small fraction of the attributes are relevant.

!.

This prior is related to to the familiar Naive Bayes model, which also assumes
that the attributes are conditionally independent given the labels. However, in the
typical Naive Bayes model there is no restriction to a single relevant attribute and
the symmetry condition linking the two classes is not imposed.

Our prior leads to the following algorithm. (The name SBSB stands for "Symmetric
Bayesian Algorithm with Singly-variant prior for Bernoulli distribution.")

Algorithm SBSB Algorithm SBSB maintains counts Si of the number of times
each attribute matches the label, a count M of the number of times the label is 1,
and a count t of the number of trials.
Initialization
Prediction

(M

+ 1)

f=

Si

M t-O

tt-O

Predict 1 given instance (Xl, ... ,xn ) if and only if
XiCSi+l)+Clixi)(t-Si+1)

i=l

Update

t- 0 for i = 1, ... ,n

M t- M

(S,)

> (t - M + 1)

f= (1-Xi)(Si+1~+XiCt-si+l)
(S.)

i=l

+ y, t t- t + 1, and for each i, if Xi

= Y then

Si

t-

Si

+1

An Apobayesian Relative of Winnow

3

207

An Apobayesian Algorithm

We construct an apobayesian algorithm by converting algorithm SBSB into a
mistake-driven algorithm using the standard conversion given in the introduction.
We call the resulting learning algorithm SASBj we have replaced "Bayesian" with
"Apobayesian" in the acronym.
In the previous section we made assumptions made about the generation of the
instances and labels that led to SBSB and thence to SASB. These assumptions
have served their purpose and we now abandon them. In analyzing the apobayesian
algorithm we do not assume that the instances and labels are generated by some
stochastic process. Instead we assume that the instance-label pairs in all of the
trials are linearly-separable, that is, that there exist some WI, ., . ,Wn , and c such
that for every instance-label pair (x, y) we have E~=I WiXi ;::: c when y = 1 and
2:~=1 WiXi ::; c when y = O. We actually make a somewhat stronger assumption,
given in the following theorem, which gives our bound for the apobayesian algorithm.
Theorem 1 Suppose that 'Yi ;::: 0 and "Ii ;::: 0 for i = 1, ... , n, and that 2:~=1 'Yi +
"I i = 1. Suppose that 0 ::; bo < bi ::; 1 and let 8 = bi - bo . Suppose that algorithm
SASB is run on a sequence of trials such that the instance x and label y in each
trial satisfy 2:~=1 'YiXi + "Ii (1 - Xi) ::; bo if y = 0 and 2:~=1 'YiXi + "Ii (1 - Xi) ;::: bi if
y = 1. Then the number of mistakes made by SASB will be bounded by
log

*

8; .

We have space to say only a little about how the derivation of this bound proceeds.
Details are given in [Lit96].
In analyzing SASB we work with an abstract description of the associated algorithm
SBSB. This algorithm starts with a prior on e as described above. We represent
this with a density Po. Then after each trial it calculates a new posterior density
Pt(O) = t-d8kP(X'YI~k, where Pt is the density after trial t and P(x, ylO) is the
pt-d )P(x,y

)

conditional probability ofthe instance x and label y observed in trial t given O. Thus
we can think of the algorithm as maintaining a current distribution on e that is
initially the prior. SASB is similar, but it leaves the current distribution unchanged
when a mistake is not made. For there to exist a finite mistake bound there must
exist some possible choice for the current distribution for which SASB would make
perfect predictions, should it ever arrive at that distribution. We call any such
distribution leading to perfect predictions a possible target distribution. It turns out
that the separability condition given in Theorem 1 guarantees that a suitable target
distribution exists. The analysis proceeds by showing that for an appropriate choice
of a target density p the relative entropy of the current distribution with respect to
the target distribution, p( 0) log(p( 0) / Pt (0)), decreases by at least some amount
R > 0 whenever a mistake is made. Since the relative entropy is never negative, the
number of mistakes is bounded by the initial relative entropy divided by R. This
form of analysis is very similar to the analysis of the various members of the Winnow
family in [Lit89,Lit91].

J

The same technique can be applied to other apobayesian algorithms. The abstract
update of Pt given above is quite general. The success of the analysis depends on
conditions on Po and P(x, ylO) that we do not have space here to discuss.

N. LittLestone and C. Mesterharm

208

p

=0.01 k =1 n =20

250r---~--~----~--~--~

250

p

=0.1 k =5 n =20

.-----.-----,-----,---~--~

,
' Optimal'
'SBSB'
'SASB'
'SASB + voting'

200

f/)

~

... .
-_.-

200

'Optimal'
'SBSB'
.
'SASB'
: 'SASB + voting"

/
/
/

/

.

/

150
/

/

S

.~
:e

/

/

/

,

'"

/

/

/

100

/'

.,,/
/'

/

50

o~--~--~----~--~--~

4000

..

./
r

/

:'/.",,~

./::: .. . .

2000

,/

/

/'" -.---::..' -.. '
/
/ . . . <- ...

o

/

/

/
,/
/'

50

/

/'

,/

/

,/

100

.. ..
//
-.,-. - / /

6000

8000

10000

Trials

/'

,/

/

O""'---...L-------I.-----'----~--~

o

2000

4000

6000

8000

10000

Trials

Figure 1: Comparison of SASB with SBSB

4

Simulation Experiments

The bound of the previous section was for perfectly linearly-separable data. We
have also done some simulation experiments exploring the performance of SASB on
non-separable data and comparing it with SBSB and with various other mistakedriven algorithms. A sample comparison of SASB with SBSB is shown in Figure
1. In each experimental run we generated 10000 trials with the instances and labels
chosen randomly according to a distribution specified by (h = '" = Ok = 1 - p,
Ok+l = ... = 0n+l = .5 where 01 , ??. ,On+l are interpreted as specified in Section
2, n is the number of attributes, and n, p, and k are as specified at the top of each
plot. The line labeled "optimal" shows the performance obtained by an optimal
predictor that knows the distribution used to generate the data ahead of time, and
thus does not need to do any learning. The lines labeled "SBSB" and "SASB" show
the performance of the corresponding learning algorithms. The lines labeled "SASB
+ voting" show the performance of SASB with the addition of a voting procedure
described in [Lit95]. This procedure improves the asymptotic mistake rate of the
algorithms. Each line on the graph is the average of 30 runs. Each line plots the
cumulative number of mistakes made by the algorithm from the beginning of the run
as a function of the number of trials.
In the left hand plot, there is only 1 relevant attribute. This is exactly the case that
SBSB is intended for, and it does better than SASB. In right hand plot, there are 5
relevant attributes; SBSB appears unable to take advantage of the extra information
present in the extra relevant attributes, but SASB successfully does.
Comparison of SASB and previous Winnow family algorithms is still in progress,
and we defer presenting details until a clearer picture has been obtained. SASB and
the Weighted Majority algorithm often perform similarly in simulations. Typically,
as one would expect, the Weighted Majority algorithm does somewhat better than

An Apobayesian Relative of Winnow

209

SASB when its parameters are chosen optimally for the particular learning task, and
worse for bad choices of parameters.

5

Conclusion

Our mistake bounds and simulations suggest that SASB may be a useful alternative
to the existing algorithms in the Winnow family. Based on the analysis style and the
bounds, SASB should perhaps itself be considered a Winnow family algorithm. Further experiments are in progress comparing SASB with Winnow family algorithms
run with a variety of parameter settings.
Perhaps of even greater interest is the potential application of our analytic techniques
to a variety of other apobayesian algorithms (though as we have observed earlier,
the techniques do not appear to apply to all such algorithms) . We have already
obtained some preliminary results regarding an interpretation of the Perceptron
algorithm as an apobayesian algorithm. We are interested in looking for entirely
new algorithms that can be derived in this way and also in better understanding
the scope of applicability of our techniques. All of the analyses that we have looked
at depend on symmetry conditions relating the probabilities for the two classes. It
would be of interest to see what can be said when such symmetry conditions do not
hold. In simulation experiments [Lit95], a mistake-driven variant of the standard
Naive Bayes algorithm often does very well, despite the absence of such symmetry
in the prior that it is based on.
Our simulation experiments and also the analysis of the related algorithm Winnow
[Lit91] suggest that SASB can be expected to handle some instance-label pairs inside
of the separating gap or on the wrong side, especially if they are not too far on the
wrong side. In particular it appears to be able to handle data generated according
to the distributions on which SBSB is based, which do not in general yield perfectly
separable data.
It is of interest to compare the capabilities of the original Bayesian algorithm with
the derived apobayesian algorithm. When the data is stochastically generated in a
manner consistent with the assumptions behind the original algorithm, the original
Bayesian algorithm can be expected to do better (see, for example, Figure 1). On
the other hand, the apobayesian algorithm can handle data beyond the capabilities of the original Bayesian algorithm. For example, in the case we consider, the
apobayesian algorithm can take advantage of the presence of more than one relevant
attribute, even though the prior behind the original Bayesian algorithm assumes a
single relevant attribute. Furthermore, as for all of the Winnow family algorithms,
the mistake bound for the apobayesian algorithm does not depend on details of the
behavior of the irrelevant attributes (including redundant attributes).
Instead of using the apobayesian variant, one might try to construct a Bayesian
learning algorithm for a prior that reflects the actual dependencies among the attributes and the labels. However, it may not be clear what the appropriate prior is.
It may be particularly unclear how to model the behavior of the irrelevant attributes. Furthermore, such a Bayesian algorithm may end up being computationally
expensive. For example, attempting to keep track of correlations among all pairs
of attributes may lead to an algorithm that needs time and space quadratic in the
number of attributes. On the other hand, if we start with a Bayesian algorithm that

210

N. Littlestone and C. Mesterharm

uses time and space linear in the number of attributes we can obtain an apobayesian
algorithm that still uses linear time and space but that can handle situations beyond
the capabilities of the original Bayesian algorithm.
Acknowledgments

This paper has benefited from discussions with Adam Grove.

References
[CHP96] Nicolo Cesa-Bianchi, David P. Helmbold, and Sandra Panizza. On bayes
methods for on-line boolean prediction. In Proceedings of the Ninth Annual
Conference on Computational Learning Theory, pages 314-324, 1996.
[Fre96] Yoav Freund. Predicting a binary sequence almost as well as the optimal
biased coin. In Proceedings of the Ninth Annual Conference on Computational Learning Theory, pages 89-98, 1996.
[KW95] J. Kivinen and M. K. Warmuth. Additive versus exponentiated gradient
updates for linear prediction. In Proc. 27th ACM Symp. on Theory of
Computing, pages 209-218, 1995.
[Lit88] N. Littlestone. Learning quickly when irrelevant attributes abound: A new
linear-threshold algorithm. Machine Learning, 2:285-318, 1988.
[Lit89] N. Littlestone. Mistake Bounds and Logarithmic Linear-threshold Learning
Algorithms. PhD thesis, Tech. Rept. UCSC-CRL-89-11, Univ. of Calif.,
Santa Cruz, 1989.
[Lit91] N. Littlestone. Redundant noisy attributes, attribute errors, and linearthreshold learning using Winnow. In Proc. 4th Annu. Workshop on Comput. Learning Theory, pages 147- 156. Morgan Kaufmann, San Mateo, CA,
1991.
[Lit95] N. Littlestone. Comparing several linear-threshold learning algorithms on
tasks involving superfluous attributes. In Proceedings of the XII International conference on Machine Learning, pages 353- 361, 1995.
[Lit96] N. Littlestone. Mistake-driven bayes sports: Bounds for symmetric
apobayesian learning algorithms. Technical report, NEC Research Institute, Princeton, NJ, 1996.
[LW94] N. Littlestone and M. K. Warmuth. The weighted majority algorithm.
Information and Computation, 108:212-261, 1994.
[Vov90] Volodimir G. Vovk. Aggregating strategies. In Proceedings of the 1990
Workshop on Computational Learning Theory, pages 371-383, 1990.


----------------------------------------------------------------

title: 2434-semi-definite-programming-by-perceptron-learning.pdf

Semidefinite Programming
by Perceptron Learning

Ralf Herbrich
Thore Graepel
Microsoft Research Ltd., Cambridge, UK
{thoreg,rherb}@microsoft.com
Andriy Kharechko
John Shawe-Taylor
Royal Holloway, University of London, UK
{ak03r,jst}@ecs.soton.ac.uk

Abstract
We present a modified version of the perceptron learning algorithm
(PLA) which solves semidefinite programs (SDPs) in polynomial
time. The algorithm is based on the following three observations:
(i) Semidefinite programs are linear programs with infinitely many
(linear) constraints; (ii) every linear program can be solved by a
sequence of constraint satisfaction problems with linear constraints;
(iii) in general, the perceptron learning algorithm solves a constraint
satisfaction problem with linear constraints in finitely many updates.
Combining the PLA with a probabilistic rescaling algorithm (which,
on average, increases the size of the feasable region) results in a probabilistic algorithm for solving SDPs that runs in polynomial time.
We present preliminary results which demonstrate that the algorithm works, but is not competitive with state-of-the-art interior
point methods.

1

Introduction

Semidefinite programming (SDP) is one of the most active research areas in optimisation. Its appeal derives from important applications in combinatorial optimisation
and control theory, from the recent development of efficient algorithms for solving
SDP problems and the depth and elegance of the underlying optimisation theory [14],
which covers linear, quadratic, and second-order cone programming as special cases.
Recently, semidefinite programming has been discovered as a useful toolkit in machine
learning with applications ranging from pattern separation via ellipsoids [4] to kernel
matrix optimisation [5] and transformation invariant learning [6].
Methods for solving SDPs have mostly been developed in an analogy to linear programming. Generalised simplex-like algorithms were developed for SDPs [11], but to
the best of our knowledge are currently merely of theoretical interest. The ellipsoid
method works by searching for a feasible point via repeatedly ?halving? an ellipsoid
that encloses the affine space of constraint matrices such that the centre of the ellipsoid is a feasible point [7]. However, this method shows poor performance in practice

as the running time usually attains its worst-case bound. A third set of methods
for solving SDPs are interior point methods [14]. These methods minimise a linear
function on convex sets provided the sets are endowed with self-concordant barrier
functions. Since such a barrier function is known for SDPs, interior point methods
are currently the most efficient method for solving SDPs in practice.
Considering the great generality of semidefinite programming and the complexity of
state-of-the-art solution methods it is quite surprising that the forty year old simple
perceptron learning algorithm [12] can be modified so as to solve SDPs. In this
paper we present a combination of the perceptron learning algorithm (PLA) with a
rescaling algorithm (originally developed for LPs [3]) that is able to solve semidefinite
programs in polynomial time. We start with a short introduction into semidefinite
programming and the perceptron learning algorithm in Section 2. In Section 3 we
present our main algorithm together with some performance guarantees, whose proofs
we only sketch due to space restrictions. While our numerical results presented in
Section 4 are very preliminary, they do give insights into the workings of the algorithm
and demonstrate that machine learning may have something to offer to the field of
convex optimisation.
For the rest of the paper we denote matrices and vectors by bold face upper and
lower case letters, e.g., A and x. We shall use x := x/ kxk to denote the unit length
vector in the direction of x. The notation A ? 0 is used to denote x0 Ax ? 0 for all
x, that is, A is positive semidefinite.

2
2.1

Learning and Convex Optimisation
Semidefinite Programming

In semidefinite programming a linear objective function is minimised over the image
of an affine transformation of the cone of semidefinite matrices, expressed by linear
matrix inequalities (LMI):
minimise
n
x?R

c0 x

subject to

F (x) := F0 +

n
X

xi Fi ? 0 ,

(1)

i=1

where c ? Rn and Fi ? Rm?m for all i ? {0, . . . , n}. The following proposition shows
that semidefinite programs are a direct generalisation of linear programs.
Proposition 1. Every semidefinite program is a linear program with infinitely many
linear constraints.
Proof. Obviously, the objective function in (1) is linear in x. For any u ? Rm , define
the vector au := (u0 F1 u, . . . , u0 Fn u). Then, the constraints in (1) can be written as
?u ? Rm :

u0 F (x) u ? 0

?u ? Rm :

?
m

This is a linear constraint in x for all u ? R

x0 au ? ?u0 F0 u .

(2)

(of which there are infinitely many).

Since the objective function is linear in x, we can solve an SDP by a sequence of
semidefinite constraint satisfaction problems (CSPs) introducing the additional constraint c0 x ? c0 and varying c0 ? R. Moreover, we have the following proposition.
Proposition 2. Any SDP can be solved by a sequence of homogenised semidefinite
CSPs of the following form:
find

x ? Rn+1

subject to

G (x) :=

n
X
i=0

xi Gi ? 0 .

Algorithm 1 Perceptron Learning Algorithm
Require: A (possibly) infinite set A of vectors a ? Rn
Set t ? 0 and xt = 0
while there exists a ? A such that x0t a ? 0 do
xt+1 = xt + a
t?t+1
end while
return xt
Proof. In order to make F0 and c0 dependent on the optimisation variables, we
introduce an auxiliary variable x0 > 0; the solution to the original problem is given
0
by x?1
0 ? x. Moreover, we can repose the two linear constraints c0 x0 ? c x ? 0 and
x0 > 0 as an LMI using the fact that a block-diagonal matrix is positive (semi)definite
if and only if every block is positive (semi)definite. Thus, the following matrices are
sufficient:
?
?
?
!
F0 0 0
Fi
0 0
0 ?ci 0
G0 = ? 00 c0 0 ? ,
Gi =
.
0
0
0
00 0 1
Given an upper and a lower bound on the objective function, repeated bisection can
be used to determine the solution in O(log 1? ) steps to accuracy ?.
In order to simplify notation, we will assume that n ? n+1 and m ? m+2 whenever
we speak about a semidefinite CSP for an SDP in n variables with Fi ? Rm?m .
2.2

Perceptron Learning Algorithm

The perceptron learning algorithm (PLA) [12] is an online procedure which finds a
linear separation of a set of points from the origin (see Algorithm 1). In machine
learning this algorithm is usually applied to two sets A+1 and A?1 of points labelled
+1 and ?1 by multiplying every data vector ai by its class label1 ; the resulting vector
xt (often referred to as the weight vector in perceptron learning) is then read as the
normal of a hyperplane which separates the sets A+1 and A?1 .
A remarkable property of the perceptron learning algorithm is that the total number
t of updates is independent of the cardinality of A but can be upper bounded simply
in terms of the following quantity
? (A) := maxn ? (A, x) := maxn min a0 x .
x?R

x?R

a?A

This quantity is known as the (normalised) margin of A in the machine learning
community or as the radius of the feasible region in the optimisation community.
It quantifies the radius of the largest ball that can be fitted in the convex region
enclosed by all a ? A (the so-called feasible set). Then, the perceptron convergence
theorem [10] states that t ? ??2 (A).
For the purpose of this paper we observe that Algorithm 1 solves a linear CSP where
the linear constraints are given by the vectors a ? A. Moreover, by the last argument
we have the following proposition.
Proposition 3. If the feasible set has a positive radius, then the perceptron learning
algorithm solves a linear CSP in finitely many steps.
It is worth mentioning that in the last few decades a series of modified PLAs A
have been developed (see [2] for a good overview) which mainly aim at guaranteeing
1

Note that sometimes the update equation is given using the unnormalised vector a.

Algorithm 2 Rescaling algorithm
Require: A maximal number T ? N+ of steps and a parameter ? ? R+
Set y uniformly at random in {z : kzk = 1}
for t = 0, . . . , T do
0
?0a
?u := ?Pun G(?y0)u 2 ? ?? (u ? smallest EV of G (?
Find au such that y
y))
(u G u)
j=1

j

if u does not exists then
Set ?i ? {1, . . . , n} : Gi ? Gi + y i G (y); return y
end if
y ? y ? (y0 au ) au ; t ? t + 1
end for
return unsolved
not only feasibility of the solution xt but also a lower bound on ? (A, xt ). These
guarantees usually come at the price of a slightly larger mistake bound which we
shall denote by M (A, ? (A)), that is, t ? M (A, ? (A)).

3

Semidefinite Programming by Perceptron Learning

If we combine Propositions 1, 2 and 3 together with Equation (2) we obtain a perceptron algorithm that sequentially solves SDPs. However, there remain two problems:
1. How do we find a vector a ? A such that x0 a ? 0?
2. How can we make the running time of this algorithm polynomial in the
description length of the data?2
In order to address the first problem we notice that A in Algorithm 1 is not explicitly
given but is defined by virtue of
A (G1 , . . . , Gn ) := {au := (u0 G1 u, . . . , u0 Gn u) | u ? Rm } .
Hence, finding a vector au ? A such that x0 au ? 0 is equivalent to identifying a
vector u ? Rm such that
n
X
xi u0 Gi u = u0 G (x) u ? 0 .
i=1

One possible way of finding such a vector u (and consequently au ) for the current
solution xt in Algorithm 1 is to calculate the eigenvector corresponding to the smallest
eigenvalue of G (xt ); if this eigenvalue is positive, the algorithm stops and outputs
xt . Note, however, that computationally easier procedures can be applied to find a
suitable u ? Rm (see also Section 4).
The second problem requires us to improve the dependency of the runtime from
O(??2 ) to O(? log(?)). To this end we employ a probabilistic rescaling algorithm
(see Algorithm 2) which was originally developed for LPs [3]. The purpose of this algorithm is to enlarge the feasible region (in terms of ? (A (G1 , . . . , Gn ))) by a constant
factor ?, on average, which would imply a decrease in the number of updates of the
perceptron algorithm exponential in the number of calls to this rescaling algorithm.
This is achieved by running Algorithm 2. If the algorithm does not return unsolved
the rescaling procedure on the Gi has the effect that au changes into au + (y0 au ) y
for every u ? Rm . In order to be able to reconstruct the solution xt to the original
problem, whenever we rescale the Gi we need to remember the vector y used for
rescaling. In Figure 1 we have shown the effect of rescaling for three linear con2
Note that polynomial runtime is only guaranteed if ??2 (A (G1 , . . . , Gn )) is bounded by
a polynomial function of the description length of the data.

Figure 1: Illustration of the rescaling procedure. Shown is the feasible region and
one feasible point before (left) and after (left) rescaling with the feasible point.
straints in R3 . The main idea of Algorithm 2 is to find a vector y that is ?-close to
the current feasible region and hence leads to an increase in its radius when used for
rescaling. The following property holds for Algorithm 2.
1
Theorem 1. Assume Algorithm 2 did not return unsolved. Let ? ? 32n
, ? be the
0
radius of the feasible set before rescaling and ? be the radius of the feasible set after
1
rescaling and assume that ? ? 4n
. Then
?
?
1
1. ?0 ? 1 ? 16n
? with probability at most 34 .
?
?
1
2. ?0 ? 1 + 4n
? with probability at least 14 .
The probabilistic nature of the theorem stems from the fact that the rescaling can
only be shown to increase the size of the feasible region if the (random) initial value
y already points sufficiently closely to the feasible region. A consequence of this theorem is that, on average, the radius increases by ? = (1 + 1/64n) > 1. Algorithm 3
combines rescaling and perceptron learning, which results in a probabilistic polynomial runtime algorithm3 which alternates between calls to Algorithm 1 and 2 . This
algorithm may return infeasible in two cases: either Ti many calls to Algorithm 2
have returned unsolved or L many calls of Algorithm 1 together with rescaling have
not returned a solution. Each of these two conditions can either happen because of
an ?unlucky? draw of y in Algorithm 2 or because ? (A (G1 , . . . , Gn )) is too small.
Following the argument in [3] one can show that for L = ?2048n ? ln (?min ) the total
probability of returning infeasible despite ? (A (G1 , . . . , Gn )) > ?min cannot exceed
exp (?n).

4

Experimental Results

The experiments reported in this section fall into two parts. Our initial aim was
to demonstrate that the method works in practice and to assess its efficacy on a
3
Note that we assume that the optimisation problem in line 3 of Algorithm 2 can be
solved in polynomial time with algorithms such as Newton-Raphson.

Algorithm 3 Positive Definite Perceptron Algorithm
Require: G1 , . . . , Gn ? Rm?m and maximal number of iteration L ? N+
Set B = In
for i = 1, . . . , L do
?
?
1
Call Algorithm 1 for at most M A, 4n
many updates
if Algorithm 1 converged then return Bx
ln(?i )
Set ?i = ?23i2 and Ti = ln
( 34 )
for j = 1, . . . , Ti do
1
Call Algorithm 2 with T = 1024n2 ln (n) and ? = 32n
0
if Algorithm 2 returns y then B ? B (In + yy ); goto the outer for-loop
end for
return infeasible
end for
return infeasible
benchmark example from graph bisection [1].
These experiments would also indicate how competitive the baseline method is when
compared to other solvers. The algorithm was implemented in MATLAB and all of
the experiments were run on 1.7GHz machines. The time taken can be compared
with a standard method SDPT3 [13] partially implemented in C but running under
MATLAB.
We considered benchmark problems arising from semidefinite relaxations to the
MAXCUT problems of weighted graphs, which is posed as finding a maximum weight
bisection of a graph. The benchmark MAXCUT problems have the following relaxed
SDP form (see [8]):
1
10 x subject to ? (diag(C1) ? C) + diag (x) ? 0 ,
minimise
(3)
x?Rn
| 4
{z
} |P {z }
F0

i

x i Fi

where C ? Rn?n is the adjacency matrix of the graph with n vertices.
The benchmark used was ?mcp100? provided by SDPLIB 1.2 [1]. For this problem,
n = 100 and it is known that the optimal value of the objective function equals
226.1574. The baseline method used the bisection approach to identify the critical
value of the objective, referred to throughout this section as c0 .
Figure 2 (left) shows a plot of the time per iteration against the value of c0 for the
first four iterations of the bisection method. As can be seen from the plots the time
taken by the algorithm for each iteration is quite long, with the time of the fourth
iteration being around 19,000 seconds. The initial value of 999 for c0 was found
without an objective constraint and converged within 0.012 secs. The bisection then
started with the lower (infeasible) value of 0 and the upper value of 999. Iteration 1
was run with c0 = 499.5, but the feasible solution had an objective value of 492. This
was found in just 617 secs. The second iteration used a value of c0 = 246 slightly
above the optimum of 226. The third iteration was infeasible but since it was quite
far from the optimum, the algorithm was able to deduce this fact quite quickly. The
final iteration was also infeasible, but much closer to the optimal value. The running
time suffered correspondingly taking 5.36 hours. If we were to continue the next
iteration would also be infeasible but closer to the optimum and so would take even
longer.
The first experiment demonstrated several things. First, that the method does indeed work as predicted; secondly, that the running times are very far from being

1000

4

16000

Time (in sec.)

14000
12000
10000

2

3

8000
6000
4000
2000
0
0

1
100

200

Optimal value

Optimal value

Value of objective function (c0)

18000

300

400

Value of objective function (c0)

500

900
800
700
600
500
400
300
200
0

10

20

30

40

50

Iterations

Figure 2: (Left) Four iterations of the bisection method showing time taken per iteration (outer for loop in Algorithm 3) against the value of the objective constraint.
(Right) Decay of the attained objective function value while iterating through Algorithm 3 with a non-zero threshold of ? = 500.
competitive (SDPT3 takes under 12 seconds to solve this problem) and thirdly that
the running times increase as the value of c0 approaches the optimum with those
iterations that must prove infeasibility being more costly than those that find a solution.
The final observation prompted our first adaptation of the base algorithm. Rather
than perform the search using the bisection method we implemented a non-zero
threshold on the objective constraint (see the while-statement in Algorithm 1). The
value of this threshold is denoted ? , following the notation introduced in [9].
Using a value of ? = 500 ensured that when a feasible solution is found, its objective
value is significantly below that of the objective constraint c0 . Figure 2 (right)
shows the values of c0 as a function of the outer for-loops (iterations); the algorithm
eventually approached its estimate of the optimal value at 228.106. This is within
1% of the optimum, though of course iterations could have been continued. Despite
the clear convergence, using this approach the running time to an accurate estimate
of the solution is still prohibitive because overall the algorithm took approximately
60 hours of CPU time to find its solution.
A profile of the execution, however, revealed that up to 93% of the execution time is
spent in the eigenvalue decomposition to identify u. Observe that we do not need a
minimal eigenvector to perform an update, simply a vector u satisfying
u0 G(x)u < 0

(4)

Cholesky decomposition will either return u satisfying (4) or it will converge indicating that G(x) is psd and Algorithm 1 has converged.

5

Conclusions

Semidefinite programming has interesting applications in machine learning. In turn,
we have shown how a simple learning algorithm can be modified to solve higher
order convex optimisation problems such as semidefinite programs. Although the
experimental results given here suggest the approach is far from computationally
competitive, the insights gained may lead to effective algorithms in concrete applications in the same way that for example SMO is a competitive algorithm for solving
quadratic programming problems arising from support vector machines. While the

optimisation setting leads to the somewhat artificial and inefficient bisection method
the positive definite perceptron algorithm excels at solving positive definite CSPs
as found, e.g., in problems of transformation invariant pattern recognition as solved
by Semidefinite Programming Machines [6]. In future work it will be of interest to
consider the combined primal-dual problem at a predefined level ? of granularity so
as to avoid the necessity of bisection search.
Acknowledgments We would like to thank J. Kandola, J. Dunagan, and A. Ambroladze for interesting discussions. This work was supported by EPSRC under grant
number GR/R55948 and by Microsoft Research Cambridge.

References
[1] B. Borchers. SDPLIB 1.2, A library of semidefinite programming test problems.
Optimization Methods and Software, 11(1):683?690, 1999.
[2] R. O. Duda, P. E. Hart, and D. G. Stork. Pattern Classification and Scene
Analysis. John Wiley and Sons, New York, 2001. Second edition.
[3] J. Dunagan and S. Vempala. A polynomial-time rescaling algorithm for solving
linear programs. Technical Report MSR-TR-02-92, Microsoft Research, 2002.
[4] F. Glineur. Pattern separation via ellipsoids and conic programming. M?emoire
de D.E.A., Facult?e Polytechnique de Mons, Mons, Belgium, Sept. 1998.
[5] T. Graepel. Kernel matrix completion by semidefinite programming. In
J. R. Dorronsoro, editor, Proceedings of the International Conference on Neural Networks, ICANN2002, Lecture Notes in Computer Science, pages 694?699.
Springer, 2002.
[6] T. Graepel and R. Herbrich. Invariant pattern recognition by Semidefinite Programming Machines. In S. Thrun, L. Saul, and B. Sch?olkopf, editors, Advances
in Neural Information Processing Systems 16. MIT Press, 2004.
[7] M. Gr?otschel, L. Lov?
asz, and A. Schrijver. Geometric Algorithms and Combinatorial Optimization, volume 2 of Algorithms and Combinatorics. Springer-Verlag,
1988.
[8] C. Helmberg. Semidefinite programming for combinatorial optimization. Technical Report ZR-00-34, Konrad-Zuse-Zentrum f?
ur Informationstechnik Berlin,
Oct. 2000.
[9] Y. Li, H. Zaragoza, R. Herbrich, J. Shawe-Taylor, and J. Kandola. The perceptron algorithm with uneven margins. In Proceedings of the International
Conference of Machine Learning (ICML?2002), pages 379?386, 2002.
[10] A. B. J. Novikoff. On convergence proofs on perceptrons. In Proceedings of the
Symposium on the Mathematical Theory of Automata, volume 12, pages 615?622.
Polytechnic Institute of Brooklyn, 1962.
[11] G. Pataki. Cone-LP?s and semi-definite programs: facial structure, basic solutions, and the symplex method. Technical Report GSIA, Carnegie Mellon
University, 1995.
[12] F. Rosenblatt. The perceptron: A probabilistic model for information storage
and organization in the brain. Psychological Review, 65(6):386?408, 1958.
[13] K. C. Toh, M. Todd, and R. T?
ut?
unc?
u. SDPT3 ? a MATLAB software package
for semidefinite programming. Technical Report TR1177, Cornell University,
1996.
[14] L. Vandenberghe and S. Boyd. Semidefinite programming. SIAM Review,
38(1):49?95, 1996.


----------------------------------------------------------------

title: 5438-optimal-regret-minimization-in-posted-price-auctions-with-strategic-buyers.pdf

Optimal Regret Minimization in Posted-Price
Auctions with Strategic Buyers

Mehryar Mohri
Courant Institute and Google Research
251 Mercer Street
New York, NY 10012

? Medina
Andres Munoz
Courant Institute
251 Mercer Street
New York, NY 10012

mohri@cims.nyu.edu

munoz@cims.nyu.edu

Abstract
We study revenue optimization learning algorithms for posted-price auctions with
strategic buyers. We analyze a very broad family of monotone regret minimization
algorithms for this problem, which includes the previously best known algorithm,
and show
? that no algorithm in that family admits a strategic regret more favorable
than ?( T ). We then introduce a new algorithm that achieves a strategic regret
differing from the lower bound only by a factor in O(log T ), an exponential improvement upon the previous best algorithm. Our new algorithm admits a natural
analysis and simpler proofs, and the ideas behind its design are general. We also
report the results of empirical evaluations comparing our algorithm with the previous state of the art and show a consistent exponential improvement in several
different scenarios.

1

Introduction

Auctions have long been an active area of research in Economics and Game Theory [Vickrey, 2012,
Milgrom and Weber, 1982, Ostrovsky and Schwarz, 2011]. In the past decade, however, the advent
of online advertisement has prompted a more algorithmic study of auctions, including the design of
learning algorithms for revenue maximization for generalized second-price auctions or second-price
auctions with reserve [Cesa-Bianchi et al., 2013, Mohri and Mu?noz Medina, 2014, He et al., 2013].
These studies have been largely motivated by the widespread use of AdExchanges and the vast
amount of historical data thereby collected ? AdExchanges are advertisement selling platforms using second-price auctions with reserve price to allocate advertisement space. Thus far, the learning
algorithms proposed for revenue maximization in these auctions critically rely on the assumption
that the bids, that is, the outcomes of auctions, are drawn i.i.d. according to some unknown distribution. However, this assumption may not hold in practice. In particular, with the knowledge that a
revenue optimization algorithm is being used, an advertiser could seek to mislead the publisher by
under-bidding. In fact, consistent empirical evidence of strategic behavior by advertisers has been
found by Edelman and Ostrovsky [2007]. This motivates the analysis presented in this paper of the
interactions between sellers and strategic buyers, that is, buyers that may act non-truthfully with the
goal of maximizing their surplus.
The scenario we consider is that of posted-price auctions, which, albeit simpler than other mechanisms, in fact matches a common situation in AdExchanges where many auctions admit a single
bidder. In this setting, second-price auctions with reserve are equivalent to posted-price auctions: a
seller sets a reserve price for a good and the buyer decides whether or not to accept it (that is to bid
higher than the reserve price). In order to capture the buyer?s strategic behavior, we will analyze an
online scenario: at each time t, a price pt is offered by the seller and the buyer must decide to either
accept it or leave it. This scenario can be modeled as a two-player repeated non-zero sum game with
1

incomplete information, where the seller?s objective is to maximize his revenue, while the advertiser
seeks to maximize her surplus as described in more detail in Section 2.
The literature on non-zero sum games is very rich [Nachbar, 1997, 2001, Morris, 1994], but much of
the work in that area has focused on characterizing different types of equilibria, which is not directly
relevant to the algorithmic questions arising here. Furthermore, the problem we consider admits a
particular structure that can be exploited to design efficient revenue optimization algorithms.
From the seller?s perspective, this game can also be viewed as a bandit problem [Kuleshov and Precup, 2010, Robbins, 1985] since only the revenue (or reward) for the prices offered is accessible to
the seller. Kleinberg and Leighton [2003] precisely studied this continuous bandit setting under the
assumption of an oblivious buyer, that is, one that does not exploit the seller?s behavior (more precisely, the authors assume that at each round the seller interacts with a different buyer). The authors
presented a tight regret bound of ?(log log T ) for the scenario of a buyer holding a fixed valuation
2
and a regret bound of O(T 3 ) when facing an adversarial buyer by using an elegant reduction to a
discrete bandit problem. However, as argued by Amin et al. [2013], when dealing with a strategic
buyer, the usual definition of regret is no longer meaningful. Indeed, consider the following example: let the valuation of the buyer be given by v ? [0, 1] and assume that an algorithm with sublinear
regret such as Exp3 [Auer et al., 2002b] or UCB [Auer et al., 2002a] is used for T rounds by the
seller. A possible strategy for the buyer, knowing the seller?s algorithm, would be to accept prices
only if they are smaller than some small value , certain that the seller would eventually learn to offer
only prices less than . If   v, the buyer would considerably boost her surplus while, in theory,
the seller would have not incurred a large regret since in hindsight, the best fixed strategy would
have been to offer price  for all rounds. This, however is clearly not optimal for the seller. The
stronger notion of policy regret introduced by Arora et al. [2012] has been shown to be the appropriate one for the analysis of bandit problems with adaptive adversaries. However, for the example
just described, a sublinear policy regret can be similarly achieved. Thus, this notion of regret is also
not the pertinent one for the study of our scenario.
We will adopt instead the definition of strategic-regret, which was introduced by Amin et al. [2013]
precisely for the study of this problem. This notion of regret also matches the concept of learning
loss introduced by [Agrawal, 1995] when facing an oblivious adversary. Using this definition, Amin
et al. [2013] presented both upper and lower bounds for the regret of a seller facing a strategic
buyer and showed that the buyer?s surplus must be discounted over time in order to be able to
achieve sublinear regret?(see Section 2). However, the gap between the upper and lower bounds
they presented is in O( T ). In the following, we analyze a very broad family of monotone regret
minimization algorithms for this problem (Section 3), which includes the algorithm of Amin et al.
[2013],
? and show that no algorithm in that family admits a strategic regret more favorable than
?( T ). Next, we introduce a nearly-optimal algorithm that achieves a strategic regret differing
from the lower bound at most by a factor in O(log T ) (Section 4). This represents an exponential
improvement upon the existing best algorithm for this setting. Our new algorithm admits a natural
analysis and simpler proofs. A key idea behind its design is a method deterring the buyer from lying,
that is rejecting prices below her valuation.

2

Setup

We consider the following game played by a buyer and a seller. A good, such as an advertisement
space, is repeatedly offered for sale by the seller to the buyer over T rounds. The buyer holds a
private valuation v ? [0, 1] for that good. At each round t = 1, . . . , T , a price pt is offered by the
seller and a decision at ? {0, 1} is made by the buyer. at takes value 1 when the buyer accepts
to buy at that price, 0 otherwise. We will say that a buyer lies whenever at = 0 while pt < v.
At the beginning of the game, the algorithm A used by the seller to set prices is announced to the
buyer. Thus, the buyer plays strategically against this algorithm. The knowledge of A is a standard
assumption in mechanism design and also matches the practice in AdExchanges.
For any ? ? (0, 1), define the discounted surplus of the buyer as follows:
Sur(A, v) =

T
X

? t?1 at (v ? pt ).

t=1

2

(1)

The value of the discount factor ? indicates the strength of the preference of the buyer for current
surpluses versus future ones. The performance of a seller?s algorithm is measured by the notion of
strategic-regret [Amin et al., 2013] defined as follows:
Reg(A, v) = T v ?

T
X

at pt .

(2)

t=1

The buyer?s objective is to maximize his discounted surplus, while the seller seeks to minimize his
regret. Note that, in view of the discounting factor ?, the buyer is not fully adversarial. The problem
consists of designing algorithms achieving sublinear strategic regret (that is a regret in o(T )).
The motivation behind the definition of strategic-regret is straightforward: a seller, with access to
the buyer?s valuation, can set a fixed price for the good  close to this value. The buyer, having no
control on the prices offered, has no option but to accept this price in order to optimize his utility.
The revenue per round of the seller is therefore v?. Since there is no scenario where higher revenue
can be achieved, this is a natural setting to compare the performance of our algorithm.
To gain more intuition about the problem, let us examine some of the complications arising when
dealing with a strategic buyer. Suppose the seller attempts to learn the buyer?s valuation v by performing a binary search. This would be a natural algorithm when facing a truthful buyer. However,
in view of the buyer?s knowledge of the algorithm, for ?  0, it is in her best interest to lie on the
initial rounds, thereby quickly, in fact exponentially, decreasing the price offered by the seller. The
seller would then incur an ?(T ) regret. A binary search approach is therefore ?too aggressive?. Indeed, an untruthful buyer can manipulate the seller into offering prices less than v/2 by lying about
her value even just once! This discussion suggests following a more conservative approach. In the
next section, we discuss a natural family of conservative algorithms for this problem.

3

Monotone algorithms

The following conservative pricing strategy was introduced by Amin et al. [2013]. Let p1 = 1
and ? < 1. If price pt is rejected at round t, the lower price pt+1 = ?pt is offered at the next
round. If at any time price pt is accepted, then this price is offered for all the remaining rounds. We
will denote this algorithm by monotone. The motivation behind its design is clear: for a suitable
choice of ?, the seller can slowly decrease the prices offered, thereby pressing the buyer to reject
many prices
? (which is not convenient for her) before obtaining a favorable price. The authors present
an O(T? T ) regret bound for this algorithm, with T? =
?1/(1 ? ?). A more careful analysis shows
p
that this bound can be further tightened to O( T? T + T ) when the discount factor ? is known to
the seller.
Despite its sublinear regret, the monotone algorithm remains sub-optimal for certain choices of
?. Indeed, consider a scenario with ?  1. For this setting, the buyer would no longer have an
incentive to lie, thus, an algorithm such as binary search would achieve logarithmic
regret, while the
?
regret achieved by the monotone algorithm is only guaranteed to be in O( T ).
One may argue that the monotone algorithm is too specific since it admits a single parameter
? and that perhaps a more complex algorithm with the same monotonic idea could achieve a more
favorable regret. Let us therefore analyze a generic monotone algorithm Am defined by Algorithm 1.
Definition 1. For any buyer?s valuation v ? [0, 1], define the acceptance time ?? = ?? (v) as the
first time a price offered by the seller using algorithm Am is accepted.
Proposition 1. For any decreasing sequence of prices (pt )Tt=1 , there exists a truthful buyer with
valuation v0 such that algorithm Am suffers regret of at least
q
?
1
Reg(Am , v0 ) ?
T ? T.
4
?
Proof. By definition of the regret,
? ?? )(v ? p?? ). We can
? we have Reg(Am , v) = v?? + (T ?
?
consider two cases: ? (v0 ) > T for some v0 ??[1/2, 1] ?
and ? (v) ? T for every v ? [1/2, 1].
In the former case, we have Reg(Am , v0 ) ? v0 T ? 12 T , which implies the statement of the
proposition. Thus, we can assume the latter condition.

3

Algorithm 1 Family of monotone algorithms.

Algorithm 2 Definition of Ar .
n = the root of T (T )
while Offered prices less than T do
Offer price pn
if Accepted then
n = r(n)
else
Offer price pn for r rounds
n = l(n)
end if
end while

Let p1 = 1 and pt ? pt?1 for t = 2, . . . T .
t?1
p ? pt
Offer price p
while (Buyer rejects p) and (t < T ) do
t?t+1
p ? pt
Offer price p
end while
while (t < T ) do
t?t+1
Offer price p
end while

Let v be uniformly distributed over [ 12 , 1]. In view of Lemma 4 (see Appendix 8.1), we have
?
?
1
1
T? T
?
?
?
?
E[v? ] + E[(T ? ? )(v ? p?? )] ? E[? ] + (T ? T )E[(v ? p?? )] ? E[? ] +
.
2
2
32E[?? ]
? ?
T? T
The right-hand side is minimized for E[?? ] =
. Plugging in this value yields
4
? ?
? ?
T? T
T? T
E[Reg(Am , v)] ?
,
which
implies
the
existence
of
v
with
Reg(A
,
v
)
?
.
0
m 0
4
4
?
We have thus shown that any monotone algorithm Am suffers a regret of at least ?( T ), even when
facing a truthful buyer. A tighter lower bound can be given under a mild condition on the prices
offered.
Definition 2. A sequence (pt )Tt=1 is said to be convex if it verifies pt ? pt+1 ? pt+1 ? pt+2 for
t = 1, . . . , T ? 2.
An instance of a convex sequence is given by the prices offered by the monotone algorithm. A
seller offering prices forming a decreasing convex sequence seeks to control the number of lies of
the buyer by slowly reducing prices. The following proposition gives a lower bound on the regret of
any algorithm in this family.
Proposition 2. Let (pt )Tt=1 be a decreasing convex sequence of prices. There exists a valuation
v0
p
for the buyer such that the regret of the monotone algorithm defined by these prices is ?( T C? +
?
?
T ), where C? = 2(1??)
.
The full proof of this proposition is given in Appendix 8.1. The proposition shows that when the
discount factor ? is known, the monotone algorithm is in fact asymptotically optimal in its class.
The results just presented suggest that the dependency on T cannot be improved by any monotone
algorithm. In some sense, this family of algorithms is ?too conservative?. Thus, to achieve a more
favorable regret guarantee, an entirely different algorithmic idea must be introduced. In the next
section, we describe a new algorithm that achieves a substantially more advantageous strategic regret
by combining the fast convergence properties of a binary search-type algorithm (in a truthful setting)
with a method penalizing untruthful behaviors of the buyer.

4

A nearly optimal algorithm

Let A be an algorithm for revenue optimization used against a truthful buyer. Denote by T (T ) the
tree associated to A after T rounds. That is, T (T ) is a full tree of height T with nodes n ? T (T )
labeled with the prices pn offered by A. The right and left children of n are denoted by r(n) and
l(n) respectively. The price offered when pn is accepted by the buyer is the label of r(n) while the
price offered by A if pn is rejected is the label of l(n). Finally, we will denote the left and right
subtrees rooted at node n by L (n) and R(n) respectively. Figure 1 depicts the tree generated by an
algorithm proposed by Kleinberg and Leighton [2003], which we will describe later.
4

1/2

1/16

1/2

1/4

3/4

5/16

9/16

1/4

3/4

13/16

(a)

13/16

(b)

Figure 1: (a) Tree T (3) associated to the algorithm proposed in [Kleinberg and Leighton, 2003]. (b) Modified
tree T 0 (3) with r = 2.
Since the buyer holds a fixed valuation, we will consider algorithms that increase prices only after a
price is accepted and decrease it only after a rejection. This is formalized in the following definition.
Definition 3. An algorithm A is said to be consistent if maxn0 ?L (n) pn0 ? pn ? minn0 ?R(n) pn0
for any node n ? T (T ).
For any consistent algorithm A, we define a modified algorithm Ar , parametrized by an integer
r ? 1, designed to face strategic buyers. Algorithm Ar offers the same prices as A, but it is defined
with the following modification: when a price is rejected by the buyer, the seller offers the same
price for r rounds. The pseudocode of Ar is given in Algorithm 2. The motivation behind the
modified algorithm is given by the following simple observation: a strategic buyer will lie only if
she is certain that rejecting a price will boost her surplus in the future. By forcing the buyer to reject
a price for several rounds, the seller ensures that the future discounted surplus will be negligible,
thereby coercing the buyer to be truthful.
We proceed to formally analyze algorithm Ar . In particular, we will quantify the effect of the
parameter r on the choice of the buyer?s strategy. To do so, a measure of the spread of the prices
offered by Ar is needed.
Definition 4. For any node n ? T (T ) define the right increment of n as ?nr := pr(n) ? pn . Similarly,
define its left increment to be ?nl := maxn0 ?L (n) pn ? pn0 .
The prices offered by Ar define a path in T (T ). For each node in this path, we can define time
t(n) to be the number of rounds needed for this node to be reached by Ar . Note that, since r may
be greater than 1, the path chosen by Ar might not necessarily reach the leaves of T (T ). Finally,
let S : n 7? S(n) be the function representing the surplus obtained by the buyer when playing an
optimal strategy against Ar after node n is reached.
Lemma 1. The function S satisfies the following recursive relation:
S(n) = max(? t(n)?1 (v ? pn ) + S(r(n)), S(l(n))).

(3)

Proof. Define a weighted tree T 0 (T ) ? T (T ) of nodes reachable by algorithm Ar . We assign
weights to the edges in the following way: if an edge on T 0 (T ) is of the form (n, r(n)), its weight
is set to be ? t(n)?1 (v ? pn ), otherwise, it is set to 0. It is easy to see that the function S evaluates
the weight of the longest path from node n to the leafs of T 0 (T ). It thus follows from elementary
graph algorithms that equation (3) holds.
The previous lemma immediately gives us necessary conditions for a buyer to reject a price.
Proposition 3. For any reachable node n, if price pn is rejected by the buyer, then the following
inequality holds:
?r
v ? pn <
(? l + ??nr ).
(1 ? ?)(1 ? ? r ) n
Proof. A direct implication of Lemma 1 is that price pn will be rejected by the buyer if and only if
? t(n)?1 (v ? pn ) + S(r(n)) < S(l(n)).
5

(4)

However, by definition, the buyer?s surplus obtained by following any path in R(n) is bounded
above by S(r(n)). In particular, this is true for the path which rejects pr(n) and accepts every price
PT
afterwards. The surplus of this path is given by t=t(n)+r+1 ? t?1 (v ? pbt ) where (b
pt )Tt=t(n)+r+1
are the prices the seller would offer if price pr(n) were rejected. Furthermore, since algorithm Ar is
consistent, we must have pbt ? pr(n) = pn + ?nr . Therefore, S(r(n)) can be bounded as follows:
S(r(n)) ?

T
X

? t?1 (v ? pn ? ?nr ) =

t=t(n)+r+1

? t(n)+r ? ? T
(v ? pn ? ?nr ).
1??

(5)

We proceed to upper bound S(l(n)). Since pn ? p0n ? ?nl for all n0 ? L (n), v ? pn0 ? v ? pn + ?nl
and
T
X
? t(n)+r?1 ? ? T
(v ? pn + ?nl ).
(6)
S(l(n)) ?
? t?1 (v ? pn + ?nl ) =
1
?
?
t=t +r
n

Combining inequalities (4), (5) and (6) we conclude that
? t(n)?1 (v ? pn ) +
?

? t(n)+r ? ? T
? t(n)+r?1 ? ? T
(v ? pn ? ?nr ) ?
(v ? pn + ?nl )
1??
1??


? r ?nl + ? r+1 ?nr ? ? T ?t(n)+1 (?nr + ?nl )
? r+1 ? ? r
?
(v ? pn ) 1 +
1??
1??

? r (?nl + ??nr )
.
1??
Rearranging the terms in the above inequality yields the desired result.
?

(v ? pn )(1 ? ? r ) ?

Let us consider the following instantiation of algorithm A introduced in [Kleinberg and Leighton,
2003]. The algorithm keeps track of a feasible interval [a, b] initialized to [0, 1] and an increment
parameter  initialized to 1/2. The algorithm works in phases. Within each phase, it offers prices
a + , a + 2, . . . until a price is rejected. If price a + k is rejected, then a new phase starts with
the feasible interval set to [a + (k ? 1), a + k] and the increment parameter set to 2 . This process
continues until b ? a < 1/T at which point the last phase starts and price a is offered for the
remaining rounds. It is not hard to see that the number of phases needed by the algorithm is less
than dlog2 log2 T e+1. A more surprising fact is that this algorithm has been shown to achieve regret
O(log log T ) when the seller faces a truthful buyer. We will show that the modification Ar of this
algorithm admits a particularly favorable regret bound. We will call this algorithm PFSr (penalized
fast search algorithm).
Proposition 4. For any value of v ? [0, 1] and any ? ? (0, 1), the regret of algorithm PFSr admits
the following upper bound:
(1 + ?)? r T
.
(7)
Reg(PFSr , v) ? (vr + 1)(dlog2 log2 T e + 1) +
2(1 ? ?)(1 ? ? r )
Note that for r = 1 and ? ? 0 the upper bound coincides with that of [Kleinberg and Leighton,
2003].
Proof. Algorithm PFSr can accumulate regret in two ways: the price offered pn is rejected, in which
case the regret is v, or the price is accepted and its regret is v ? pn .
Let K = dlog2 log2 T e + 1 be the number of phases run by algorithm PFSr . Since at most K
different prices are rejected by the buyer (one rejection per phase) and each price must be rejected
for r rounds, the cumulative regret of all rejections is upper bounded by vKr.
The second type of regret can also be bounded straightforwardly. For any phase i, let i and [ai , bi ]
denote the corresponding search parameter and feasible interval respectively. If v ? [ai , bi ],?the
regret accrued in the case where the buyer accepts a price in this interval is bounded
? by bi ?ai = i .
If, on the other hand v ? bi , then it readily follows that v ? pn < v ? bi + i for all prices pn
offered in phase i. Therefore, the regret obtained in acceptance rounds is bounded by
K
K

X
?  X
Ni (v ? bi )1v>bi + i ?
(v ? bi )1v>bi Ni + K,
i=1

i=1

6

where Ni ?

?1
i

denotes the number of prices offered during the i-th round.

Finally, notice that, in view of the algorithm?s definition, every bi corresponds to a rejected price.
Thus, by Proposition 3, there exist nodes ni (not necessarily distinct) such that pni = bi and
v ? bi = v ? pni ?

?r
(? l + ??nr i ).
(1 ? ?)(1 ? ? r ) ni

It is immediate that ?nr ? 1/2 and ?nl ? 1/2 for any node n, thus, we can write
K
X

K

(v ? bi )1v>bi Ni ?

i=1

X
? r (1 + ?)
? r (1 + ?)
N
?
T.
i
2(1 ? ?)(1 ? ? r ) i=1
2(1 ? ?)(1 ? ? r )

The last inequality holds since at most T prices are offered by our algorithm. Combining the bounds
for both regret types yields the result.
When an upper bound on the discount factor ? is known to the seller, he can leverage this information
and optimize upper bound (7) with respect to the parameter r.
l
m
?0r T
Theorem 1. Let 1/2 < ? < ?0 < 1 and r? = argminr?1 r + (1??0 )(1??
r ) . For any v ? [0, 1],
0
if T > 4, the regret of PFSr? satisfies
Reg(PFSr? , v) ? (2v?0 T?0 log cT + 1 + v)(log2 log2 T + 1) + 4T?0 ,
where c = 4 log 2.
The proof of this theorem is fairly technical and is deferred to the Appendix. The theorem helps
us define conditions under which logarithmic regret can be achieved. Indeed, if ?0 = e?1/ log T =
O(1 ? log1 T ), using the inequality e?x ? 1 ? x + x2 /2 valid for all x > 0 we obtain
log2 T
1
?
? log T.
1 ? ?0
2 log T ? 1
It then follows from Theorem 1 that
Reg(PFSr? , v) ? (2v log T log cT + 1 + v)(log2 log2 T + 1) + 4 log T.
Let us compare the regret bound given by Theorem 1 with the one given by Amin et al. [2013]. The
above discussion shows that for certain values of ?, an exponentially better regret can be achieved
by our algorithm. It can be argued that the knowledge of an upper bound on ?
? is required, whereas
this is not needed for the monotone algorithm. However, if ? > 1 ? 1/ T , the regret bound
on monotone is super-linear, and therefore uninformative.
Thus, in order to properly compare
?
both algorithms, we may
assume
that
?
<
1
?
1/
T
in
which
case, by Theorem 1, the regret
?
of our algorithm is O( T log T ) whereas only linear regret
can
be
guaranteed by the monotone
?
p
algorithm. Even under the more favorable bound of O( T? T + T ), for any ? < 1 and ? <
?+1
1 ? 1/T ? , the monotone algorithm will achieve regret O(T 2 ) while a strictly better regret
O(T ? log T log log T ) is attained by ours.

5

Lower bound

The following lower bounds have been derived in previous work.
Theorem 2 ([Amin et al., 2013]). Let ? > 0 be fixed. For any algorithm A, there exists a valuation
1
v for the buyer such that Reg(A, v) ? 12
T? .
This theorem is in fact given for the stochastic setting where the buyer?s valuation is a random
variable taken from some fixed distribution D. However, the proof of the theorem selects D to be a
point mass, therefore reducing the scenario to a fixed priced setting.
Theorem 3 ( [Kleinberg and Leighton, 2003]). Given any algorithm A to be played against a
truthful buyer, there exists a value v ? [0, 1] such that Reg(A, v) ? C log log T for some universal
constant C.
7

? = .95, v = .75
2500

PFS
1000 mon

2000
Regret

Regret

800
600
400

? = .75, v = .25

PFS
mon

120

80

80

PFS
100 mon

1500
1000

40
20

20

0

0

0

2.5

3

3.5

4

4.5

2

2.5

3

3.5

4

4.5

Number of rounds (log-scale)

60

40

500

2

PFS
100 mon

60

200

Number of rounds (log-scale)

? = .80, v = .25

120

Regret

1200

Regret

? = .85, v = .75

2

2.5

3

3.5

4

4.5

Number of rounds (log-scale)

0

2

2.5

3

3.5

4

4.5

Number of rounds (log-scale)

Figure 2: Comparison of the monotone algorithm and PFSr for different choices of ? and v. The regret of
each algorithm is plotted as a function of the number rounds when ? is not known to the algorithms (first two
figures) and when its value is made accessible to the algorithms (last two figures).

Combining these results leads immediately to the following.
Corollary 1. Given
 any algorithm A, there exists a buyer?s valuation v ? [0, 1] such that
Reg(A, v) ? max

1
12 T? , C

log log T , for a universal constant C.

We now compare the upper bounds given in the previous section with the bound of Corollary 1. For
? > 1/2, we have Reg(PFSr , v) = O(T? log T log log T ). On the other hand, for ? ? 1/2, we may
choose r = 1, in which case, by Proposition 4, Reg(PFSr , v) = O(log log T ). Thus, the upper and
lower bounds match up to an O(log T ) factor.

6

Empirical results

In this section, we present the result of simulations comparing the monotone algorithm and our
algorithm PFSr . The experiments were carried out as follows: given a buyer?s valuation v, a discrete
set of false valuations vb were selected out of the set {.03, .06, . . . , v}. Both algorithms were run
against a buyer making the seller believe her valuation is vb instead of v. The value of vb achieving
the best utility for the buyer was chosen and the regret for both algorithms is reported in Figure 2.
We considered two sets of experiments. First, the value of parameter ? was left unknown to both
algorithms and the value of r was set to log(T ). This choice is motivated by the discussion following
Theorem 1 since, for large values of T , we can expect to achieve logarithmic regret. The first two
plots (from left to right) in Figure 2 depict these results. The apparent stationarity in the regret of
PFSr is just a consequence of the scale of the plots as the regret is in fact growing as log(T ). For
the second set of experiments, we allowed access to the parameter ? to both algorithms. The value
of r was chosen optimally
based on the resultsp
of Theorem
? 1 and the parameter ? of monotone
p
was set to 1 ? 1/ T T? to ensure regret in O( T T? + T ). It is worth noting that even though
our algorithm was designed under the assumption of some knowledge about the value of ?, the
experimental results show that an exponentially better performance over the monotone algorithm
is still attainable and in fact the performances of the optimized and unoptimized versions of our
algorithm are comparable. A more comprehensive series of experiments is presented in Appendix 9.

7

Conclusion

We presented a detailed analysis of revenue optimization algorithms against strategic buyers. In
doing so, we reduced the gap between upper and lower bounds on strategic regret to a logarithmic
factor. Furthermore, the algorithm we presented is simple to analyze and reduces to the truthful
scenario in the limit of ? ? 0, an important property that previous algorithms did not admit. We
believe that our analysis helps gain a deeper understanding of this problem and that it can serve as a
tool for studying more complex scenarios such as that of strategic behavior in repeated second-price
auctions, VCG auctions and general market strategies.

Acknowledgments
We thank Kareem Amin, Afshin Rostamizadeh and Umar Syed for several discussions about the
topic of this paper. This work was partly funded by the NSF award IIS-1117591.
8

References
R. Agrawal. The continuum-armed bandit problem. SIAM journal on control and optimization, 33
(6):1926?1951, 1995.
K. Amin, A. Rostamizadeh, and U. Syed. Learning prices for repeated auctions with strategic buyers.
In Proceedings of NIPS, pages 1169?1177, 2013.
R. Arora, O. Dekel, and A. Tewari. Online bandit learning against an adaptive adversary: from
regret to policy regret. In Proceedings of ICML, 2012.
P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem.
Machine Learning, 47(2-3):235?256, 2002a.
P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. The nonstochastic multiarmed bandit
problem. SIAM J. Comput., 32(1):48?77, 2002b.
N. Cesa-Bianchi, C. Gentile, and Y. Mansour. Regret minimization for reserve prices in second-price
auctions. In Proceedings of SODA, pages 1190?1204, 2013.
B. Edelman and M. Ostrovsky. Strategic bidder behavior in sponsored search auctions. Decision
Support Systems, 43(1), 2007.
D. He, W. Chen, L. Wang, and T. Liu. A game-theoretic machine learning approach for revenue
maximization in sponsored search. In Proceedings of IJCAI, pages 206?213, 2013.
R. D. Kleinberg and F. T. Leighton. The value of knowing a demand curve: Bounds on regret for
online posted-price auctions. In Proceedings of FOCS, pages 594?605, 2003.
V. Kuleshov and D. Precup. Algorithms for the multi-armed bandit problem. Journal of Machine
Learning, 2010.
P. Milgrom and R. Weber. A theory of auctions and competitive bidding. Econometrica: Journal of
the Econometric Society, pages 1089?1122, 1982.
M. Mohri and A. Mu?noz Medina. Learning theory and algorithms for revenue optimization in
second-price auctions with reserve. In Proceedings of ICML, 2014.
P. Morris. Non-zero-sum games. In Introduction to Game Theory, pages 115?147. Springer, 1994.
J. Nachbar. Bayesian learning in repeated games of incomplete information. Social Choice and
Welfare, 18(2):303?326, 2001.
J. H. Nachbar. Prediction, optimization, and learning in repeated games. Econometrica: Journal of
the Econometric Society, pages 275?309, 1997.
M. Ostrovsky and M. Schwarz. Reserve prices in internet advertising auctions: A field experiment.
In Proceedings of EC, pages 59?60. ACM, 2011.
H. Robbins. Some aspects of the sequential design of experiments. In Herbert Robbins Selected
Papers, pages 169?177. Springer, 1985.
W. Vickrey. Counterspeculation, auctions, and competitive sealed tenders. The Journal of finance,
16(1):8?37, 2012.

9


----------------------------------------------------------------

