query sentence: Noninfinitesimal algorithm
---------------------------------------------------------------------
title: 1271-online-learning-from-finite-training-sets-an-analytical-case-study.pdf

Online learning from finite training sets:
An analytical case study

Peter Sollich*
Department of Physics
University of Edinburgh
Edinburgh EH9 3JZ, U.K.
P.SollichOed.ac.uk

David Barber t
Neural Computing Research Group
Department of Applied Mathematics
Aston University
Birmingham B4 7ET, U.K.
D.BarberOaston.ac.uk

Abstract
We analyse online learning from finite training sets at noninfinitesimal learning rates TJ. By an extension of statistical mechanics methods, we obtain exact results for the time-dependent
generalization error of a linear network with a large number of
weights N. We find, for example, that for small training sets of
size p ~ N, larger learning rates can be used without compromising asymptotic generalization performance or convergence speed.
Encouragingly, for optimal settings of TJ (and, less importantly,
weight decay ,\) at given final learning time, the generalization performance of online learning is essentially as good as that of offline
learning.

1

INTRODUCTION

The analysis of online (gradient descent) learning, which is one of the most common
approaches to supervised learning found in the neural networks community, has
recently been the focus of much attention [1]. The characteristic feature of online
learning is that the weights of a network ('student') are updated each time a new
training example is presented, such that the error on this example is reduced. In
offline learning, on the other hand, the total error on all examples in the training
set is accumulated before a gradient descent weight update is made. Online and
* Royal Society Dorothy Hodgkin Research Fellow
t Supported by EPSRC grant GR/J75425: Novel Developments in Learning Theory

for Neural Networks

Online Leamingfrom Finite Training Sets: An Analytical Case Study

275

offline learning are equivalent only in the limiting case where the learning rate
T) --* 0 (see, e.g., [2]). The main quantity of interest is normally the evolution of
the generalization error: How well does the student approximate the input-output
mapping ('teacher') underlying the training examples after a given number of weight
updates?
Most analytical treatments of online learning assume either that the size of the
training set is infinite, or that the learning rate T) is vanishingly small. Both of
these restrictions are undesirable: In practice, most training sets are finite, and noninfinitesimal values of T) are needed to ensure that the learning process converges
after a reasonable number of updates. General results have been derived for the
difference between online and offline learning to first order in T), which apply to
training sets of any size (see, e. g., [2]). These results, however, do not directly
address the question of generalization performance. The most explicit analysis of
the time evolution of the generalization error for finite training sets was provided by
Krogh and Hertz [3] for a scenario very similar to the one we consider below. Their
T) --* 0 (i.e., offline) calculation will serve as a baseline for our work. For finite T),
progress has been made in particular for so-called soft committee machine network
architectures [4, 5], but only for the case of infinite training sets.
Our aim in this paper is to analyse a simple model system in order to assess how the
combination of non-infinitesimal learning rates T) and finite training sets (containing
a examples per weight) affects online learning. In particular, we will consider
the dependence of the asymptotic generalization error on T) and a, the effect of
finite a on both the critical learning rate and the learning rate yielding optimal
convergence speed, and optimal values of T) and weight decay A. We also compare
the performance of online and offline learning and discuss the extent to which infinite
training set analyses are -applicable for finite a.

2

MODEL AND OUTLINE OF CALCULATION

We consider online training of a linear student network with input-output relation

Here x is an N-dimensional vector of real-valued inputs, y the single real output
and w the wei~t vector of the network. ,T, denotes the transpose of a vector and
the factor 1/VN is introduced for convenience. Whenever a training example (x, y)
is presented to the network, its weight vector is updated along the gradient of the
squared error on this example, i. e.,

where T) is the learning rate. We are interested in online learning from finite training sets, where for each update an example is randomly chosen from a given set
{(xll,yll),j.l = l. .. p} ofp training examples. (The case of cyclical presentation of
examples [6] is left for future study.) If example J.l is chosen for update n, the weight
vector is changed to

(1)
Here we have also included a weight decay 'Y. We will normally parameterize the
strength of the weight decay in terms of A = 'YO' (where a = p / N is the number

P. Sollich and D. Barber

276

of examples per weight), which plays the same role as the weight decay commonly
used in offline learning [3]. For simplicity, all student weights are assumed to be
initially zero, i.e., Wn=o
o.

=

The main quantity of interest is the evolution of the generalization error of the
student. We assume that the training examples are generated by a linear 'teacher',
i.e., yJJ = W. T x JJ IVN+e, where JJ is zero mean additive noise of variance (72. The
teacher weight vector is taken to be normalized to w. 2 = N for simplicity, and the
input vectors are assumed to be sampled randomly from an isotropic distribution
over the hypersphere x 2 = N. The generalization error, defined as the average of
the squared error between student and teacher outputs for random inputs, is then

e

where

Vn

= Wn -

W?.

In order to make the scenario analytically tractable, we focus on the limit N -+ 00
of a large number of input components and weights, taken at constant number of
examples per weight a = piN and updates per weight ('learning time') t = niN. In
this limit, the generalization error fg(t) becomes self-averaging and can be calculated
by averaging both over the random selection of examples from a given training set
and over all training sets. Our results can be straightforwardly extended to the case
of percept ron teachers with a nonlinear transfer function, as in [7].
The usual statistical mechanical approach to the online learning problem expresses
the generalization error in terms of 'order parameters' like R = ~wJw. whose
(self-averaging) time evolution is determined from appropriately averaged update
equations. This method works because for infinite training sets, the average order parameter updates can again be expressed in terms of the order parameters
alone. For finite training sets, on the other hand, the updates involve new order
parameters such as Rl = ~wJ Aw., where A is the correlation matrix of the
training inputs, A = ~L-P = lx JJ(xJJ)T. Their time evolution is in turn determined
by order parameters involving higher powers of A, yielding an infinite hierarchy
of order parameters. We solve this problem by considering instead order parameter (generating) junctions [8] such as a generalized form of the generalization error
f(t;h) = 2~vJexp(hA)vn . This allows powers of A to be obtained by differentiation with respect to h, reSUlting in a closed system of (partial differential) equations
for f(t; h) and R(t; h) = ~ wJ exp(hA)w ?.
The resulting equations and details of their solution will be given in a future publication. The final solution is most easily expressed in terms of the Laplace transform
of the generalization error
fg(Z) = '!!..
a

fdt

~

fg(t)e-z(f//a)t = fdz)

+ T}f2(Z) + T} 2f 3(Z)
1-

(2)

T}f4(Z)

The functions fi (z) (i = 1 ... 4) can be expressed in closed form in terms of a, (72
and A (and, of course, z). The Laplace transform (2) yields directly the asymptotic
value of the generalization error, foo = fg(t -+ (0) = limz--+o zig{z) , which can be
calculated analytically. For finite learning times t, fg(t) is obtained by numerical
inversion of the Laplace transform.

3

RESULTS AND DISCUSSION

We now discuss the consequences of our main result (2), focusing first on the asymptotic generalization error foo, then the convergence speed for large learning times,

Online Learningfrom Finite Training Sets: An Analytical Case Study
a=O.s

277
<1=2

a=i

Figure 1: Asymptotic generalization error (00 vs 1] and A. a as shown,

(1"2

= 0.1.

and finally the behaviour at small t. For numerical evaluations, we generally take
(1"2
0.1, corresponding to a sizable noise-to-signal ratio of JQ.I ~ 0.32.

=

The asymptotic generalization error (00 is shown in Fig. 1 as a function of 1] and A
for a
0.5, 1, 2. We observe that it is minimal for A (1"2 and 1] 0, as expected
from corresponding resul ts for offline learning [3]1. We also read off that for fixed A,
(00 is an increasing function of 1]: The larger 1], the more the weight updates tend
to overshoot the minimum of the (total, i.e., offline) training error. This causes a
diffusive motion of the weights around their average asymptotic values [2] which
increases (00. In the absence of weight decay (A = 0) and for a < 1, however, (00
is independent of 1]. In this case the training data can be fitted perfectly; every
term in the total sum-of-squares training error is then zero and online learning does
not lead to weight diffusion because all individual updates vanish . In general, the
relative increase (00(1])/(00(1] = 0) - 1 due to nonzero 1] depends significantly on a.
For 1]
1 and a
0.5, for example, this increase is smaller than 6% for all A (at
(1"2 = 0.1), and for a = 1 it is at most 13%. This means that in cases where training
data is limited (p ~ N), 1] can be chosen fairly large in order to optimize learning
speed, without seriously affecting the asymptotic generalization error. In the large
a limit, on the other hand, one finds (00 = ((1"2/2)[1/a + 1]/(2 - 1])]. The relative
increase over the value at 1] = a therefore grows linearly with a; already for a = 2,
increases of around 50% can occur for 1] = 1.

=

=

=

=

=

Fig. 1 also shows that (00 diverges as 1] approaches a critical learning rate 1]e: As
1] -+ 1]e, the 'overshoot' of the weight update steps becomes so large that the weights
eventually diverge. From the Laplace transform (2), one finds that 1]e is determined
by 1]e(4(Z = 0) = 1; it is a function of a and A only. As shown in Fig. 2b-d, 1]e
increases with A. This is reasonable, as the weight decay reduces the length of the
weight vector at each update, counteracting potential weight divergences. In the
small and large a limit, one has 1]e = 2( 1 + A) and 1]e = 2( 1 + A/a), respectively.
For constant A, 1]e therefore decreases 2 with a (Fig. 2b-d) .
We now turn to the large t behaviour of the generalization error (g(t). For small
1], the most slowly decaying contribution (or 'mode') to (g(t) varies as exp( -ct), its
1 The optimal value of the unscaledweight decay decreases with a as 'Y = (1"2 ja, because
for large training sets there is less need to counteract noise in the training data by using
a large weight decay.
2Conversely, for constant 'Y, f"/e increases with a from 2(1 + 'Ya) to 2(1 + 'Y): For large a ,
the weight decay is applied more often between repeat presentations of a training example
that would otherwise cause the weights to diverge.

P. Sollich and D. Barber

278

=

(va -

decay constant c 71['\ +
1)2]/ a scaling linearly with 71, the size of the weight
updates, as expected (Fig. 2a). For small a, the condition ct ? 1 for fg(t) to have
reached its asymptotic value foo is 71(1 + ,\)(t/a) ? 1 and scales with tla, which is
the number of times each training example has been used. For large a, on the other
hand, the condition becomes 71t ? 1: The size of the training set drops out since
convergence occurs before repetitions of training examples become significant.
For larger 71, the picture changes due to a new 'slow mode' (arising from the denominator of (2)). Interestingly, this mode exists only for 71 above a finite threshold
71min = 2/(a 1 / 2 + a- 1 / 2 -1). For finite a, it could therefore not have been predicted
from a small 71 expansion of (g(t). Its decay constant Cslow decreases to zero as
71 -t 71e, and crosses that of the normal mode at 71x(a,'\) (Fig. 2a). For 71 > 71x,
the slow mode therefore determines the convergence speed for large t, and fastest
convergence is obtained for 71 = 71x. However, it may still be advantageous to use
lower values of 71 in order to lower the asymptotic generalization error (see below);
values of 71 > 71x would deteriorate both convergence speed and asymptotic performance. Fig . 2b-d shows the dependence of 71min, 71x and 71e on a and'\. For
,\ not too large, 71x has a maximum at a ~ 1 (where 71x ~ 71e), while decaying as
71x = 1+2a- 1 / 2 ~ ~71e for larger a. This is because for a ~ 1 the (total training) error surface is very anisotropic around its minimum in weight space [9]. The steepest
directions determine 71e and convergence along them would be fastest for 71 = ~71e
(as in the isotropic case). However, the overall convergence speed is determined by
the shallow directions, which require maximal 71 ~ 71e for fastest convergence.
Consider now the small t behaviour of fg(t). Fig. 3 illustrates the dependence of
fg(t) on 71; comparison with simulation results for N = 50 clearly confirms our
calculations and demonstrates that finite N effects are not significant even for such
fairly small N. For a = 0.7 (Fig. 3a), we see that nonzero 71 acts as effective update
noise, eliminating the minimum in fg(t) which corresponds to over-training [3]. foo
is also seen to be essentially independent of 71 as predicted for the small value of
,\ = 10- 4 chosen. For a = 5, Fig. 3b clearly shows the increase of foo with 71. It
also illustrates how convergence first speeds up as 71 is increased from zero and then
slows down again as 71e ~ 2 is approached.
Above, we discussed optimal settings of 71 and ,\ for minimal asymptotic generalization error foo. Fig. 4 shows what happens if we minimize fg(t) instead for
a given final learning time t, corresponding to a fixed amount of computational
effort for training the network. As t increases, the optimal 71 decreases towards
zero as required by the tradeoff between asymptotic performance and convergence
1..=0

1..=0.1

4,-------,

4,---------,

(c)

(b)

(a)

1..=1

11m in

c

o

o

J

2a 3

4

S

o

J

2

a

3

4

S

Figure 2: Definitions of71min, 71x and 71e, and their dependence on a (for'\ as shown).

279

Online Learning /rom Finite Training Sets: An Analytical Case Study
O.511---~--~--~---.,

(a)

a

= 0.7

(b) a = 5

O.20'--~5--1~O-~15--2~O-~25--3~o--'t

Figure 3: fg vs t for different TJ. Simulations for N = 50 are shown by symbols
(standard errors less than symbol sizes). A=1O- 4 , 0- 2 =0.1, a as shown. The learning
rate TJ increases from below (at large t) over the range (a) 0.5 .. . 1.95, (b) 0.5 ... 1. 75.

(a)

0.8

0.06

0.6

/

(b)

0.08

/

(c)

0.25

,

"

0.4

'------

0.2
0.0

0.04

" ........-------

0.02
O. 00

L-L--'---~_'___'______'_~___'___'__'

o

10

20

30

40

50

10

20

30

40

50

'---'--'----'-..t.......'---'-~...J........_'___'

o

10

20

30

40

50

t

Figure 4: Optimal TJ and A vs given final learning time t, and resulting (g.
Solid/dashed lines: a = 1 / a =2; bold/thin lines: online/offline learning. 0- 2 =0.1.
Dotted lines in (a): Fits of form TJ = (a + bIn t)/t to optimal TJ for online learning.
speed. Minimizing (g(t) ::::: (00+ const . exp( -ct) ~ Cl + TJC2 + C3 exp( -C4TJt) leads to
TJopt = (a + bIn t)/t (with some constants a, b, Cl...4). Although derived for small TJ,
this functional form (dotted lines in Fig. 4a) also provides a good description down
to fairly small t , where TJopt becomes large. The optimal weight decay A increases 3
with t towards the limiting value 0- 2 . However, optimizing A is much less important than choosing the right TJ: Minimizing (g(t) for fixed A yields almost the same
generalization error as optimizing both TJ and A (we omit detailed results here 4 ). It
is encouraging to see from Fig. 4c that after as few as t = 10 updates per weight
with optimal TJ, the generalization error is almost indistinguishable from its optimal
value for t --t 00 (this also holds if A is kept fixed). Optimization of the learning
rate should therefore be worthwhile in most practical scenarios.
In Fig. 4c, we also compare the performance of online learning to that of offline
learning (calculated from the appropriate discrete time version of [3]), again with
30 ne might have expected the opposite effect of having larger>. at low t in order to
'contain' potential divergences from the larger optimal learning rates tJ. However, smaller
>. tends to make the asymptotic value foo less sensitive to large values of tJ as we saw
above, and we conclude that this effect dominates.
4Por fixed>. < u 2 , where fg(t) has an over-training minimum (see Pig. 3a), the asymptotic behaviour of tJopt changes to tJopt <X C 1 (without the In t factor), corresponding to a
fixed effective learning time tJt required to reach this minimum.

P. SalJich and D. Barber

280

optimized values of TJ and A for given t. The performance loss from using online
instead of offline learning is seen to be negligible. This may seem surprising given
the effective noise on weight updates implied by online learning, in particular for
small t. However, comparing the respective optimal learning rates (Fig. 4a), we see
that online learning makes up for this deficiency by allowing larger values of TJ to
be used (for large a, for example, TJc(offline) 2/0' ? TJc(online) 2).

=

=

Finally, we compare our finite a results with those for the limiting case a -+ 00.
Good agreement exists for any learning time t if the asymptotic generalization error
(00 (a < 00) is dominated by the contribution from the nonzero learning rate TJ (as is
the case for a -+ 00). In practice, however, one wants TJ to be small enough to make
only a negligible contribution to (00(0' < 00); in this regime, the a -+ 00 results are
essentially useless.

4

CONCLUSIONS

The main theoretical contribution of this paper is the extension of the statistical
mechanics method of order parameter dynamics to the dynamics of order parameter
(generating) functions . The results that we have obtained for a simple linear model
system are also of practical relevance. For example, the calculated dependence on
TJ of the asymptotic generalization error (00 and the convergence speed shows that,
in general, sizable values of TJ can be used for training sets of limited size (a ~ 1),
while for larger a it is important to keep learning rates small. We also found a
simple functional form for the dependence of the optimal TJ on a given final learning
time t. This could be used, for example, to estimate the optimal TJ for large t from
test runs with only a small number of weight updates. Finally, we found that for
optimized TJ online learning performs essentially as well as offline learning, whether
or not the weight decay A is optimized as well. This is encouraging, since online
learning effectively induces noisy weight updates. This allows it to cope better than
offline learning with the problem of local (training error) minima in realistic neural
networks. Online learning has the further advantage that the critical learning rates
are not significantly lowered by input distributions with nonzero mean, whereas for
offline learning they are significantly reduced [10]. In the future, we hope to extend
our approach to dynamic (t-dependent) optimization of TJ (although performance
improvements over optimal fixed TJ may be small [6]), and to more complicated network architectures in which the crucial question of local minima can be addressed.

References
[1]
[2]
[3]
[4]
[5]
[6]
[7]
[8]
[9]
[10]

See for example: The dynamics of online learning. Workshop at NIPS '95.
T. Heskes and B. Kappen. Phys. Ret). A, 44:2718, 1991.
A. Krogh and J. A. Hertz. J. Phys. A, 25:1135, 1992.
D. Saad and S. Solla. Phys. Ret). E, 52:4225, 1995; also in NIPS-8.
M. Biehl and H. Schwarze. J. Phys. A, 28:643-656, 1995.
Z.-Q. Luo. Neur. Comp., 3:226, 1991; T. Heskes and W. Wiegerinck. IEEE
Trans. Neur. Netw., 7:919, 1996.
P. Sollich. J. Phys. A, 28:6125, 1995.
1. L. Bonilla, F. G. Padilla, G. Parisi and F. Ritort. Europhys. Lett., 34:159,
1996; Phys. Ret). B, 54:4170, 1996.
J. A. Hertz, A. Krogh and G. I. Thorbergsson. J. Phys. A, 22:2133, 1989.
T. L. H. Watkin, A. Rau and M. Biehl. Ret). Modern Phys., 65:499, 1993.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 1390-on-line-learning-from-finite-training-sets-in-nonlinear-networks.pdf

Online learning from finite training sets
in nonlinear networks
David Barber t

Peter Sollich*

Department of Physics
University of Edinburgh
Edinburgh ERg 3JZ, U.K.

Department of Applied Mathematics
Aston University
Birmingham B4 7ET, U.K.

P.Sollich~ed.ac.uk

D.Barber~aston . ac.uk

Abstract
Online learning is one of the most common forms of neural network training. We present an analysis of online learning from finite
training sets for non-linear networks (namely, soft-committee machines), advancing the theory to more realistic learning scenarios.
Dynamical equations are derived for an appropriate set of order
parameters; these are exact in the limiting case of either linear
networks or infinite training sets. Preliminary comparisons with
simulations suggest that the theory captures some effects of finite
training sets, but may not yet account correctly for the presence of
local minima.

1

INTRODUCTION

The analysis of online gradient descent learning, as one of the most common forms
of supervised learning, has recently stimulated a great deal of interest [1, 5, 7, 3]. In
online learning, the weights of a network ('student') are updated immediately after
presentation of each training example (input-output pair) in order to reduce the
error that the network makes on that example. One of the primary goals of online
learning analysis is to track the resulting evolution of the generalization error - the
error that the student network makes on a novel test example, after a given number
of example presentations. In order to specify the learning problem, the training
outputs are assumed to be generated by a teacher network of known architecture.
Previous studies of online learning have often imposed somewhat restrictive and
? Royal Society Dorothy Hodgkin Research Fellow
tSupported by EPSRC grant GR/J75425: Novel Developments in Learning Theory for
Neural Networks

P. SolIich and D. Barber

358

unrealistic assumptions about the learning framework. These restrictions are, either
that the size of the training set is infinite, or that the learning rate is small[l, 5, 4].
Finite training sets present a significant analytical difficulty as successive weight
updates are correlated, giving rise to highly non-trivial generalization dynamics.
For linear networks, the difficulties encountered with finite training sets and noninfinitesimal learning rates can be overcome by extending the standard set of descriptive ('order') parameters to include the effects of weight update correlations[7].
In the present work, we extend our analysis to nonlinear networks. The particular
model we choose to study is the soft-committee machine which is capable of representing a rich variety of input-output mappings. Its online learning dynamics has
been studied comprehensively for infinite training sets[l, 5]. In order to carry out
our analysis, we adapt tools originally developed in the statistical mechanics literature which have found application, for example, in the study of Hopfield network
dynamics[2].

2

MODEL AND OUTLINE OF CALCULATION

For an N-dimensional input vector x, the output of the soft committee machine is
given by

(I)
where the nonlinear activation function g(hl ) = erf(hz/V2) acts on the activations
hi = wtxl.JFi (the factor 1/.JFi is for convenience only). This is a neural network
with L hidden units, input to hidden weight vectors WI, 1 = I..L, and all hidden to
output weights set to 1.
In online learning the student weights are adapted on a sequence of presented examples to better approximate the teacher mapping. The training examples are drawn,
with replacement, from a finite set, {(X/",yl-') ,j.t I..p}. This set remains fixed
piN.
during training. Its size relative to the input dimension is denoted by a
We take the input vectors xl-' as samples from an N dimensional Gaussian distribution with zero mean and unit variance. The training outputs y'" are assumed to
be generated by a teacher soft committee machine with hidden weight vectors w~,
m = I..M, with additive Gaussian noise corrupting its activations and output.

=

=

The discrepancy between the teacher and student on a particular training example (x, y), drawn from the training set, is given by the squared difference of their
corresponding outputs,

E=

H~9(hl) -yr = H~9(hl) - ~g(km +em) -eor

where the student and teacher activations are, respectively
h,

em,

and
m = I..M and
output respectively.

= {J;wtx

km

= {J;(w:n?x,

(2)

eo are noise variables corrupting the teacher activations and

Given a training example (x, y), the student weights are updated by a gradient
descent step with learning rate "I,

w; - W, = -"I\1wIE = - JNx8h E
l

(3)

359

On-line Learning from Finite Training Sets in Nonlinear Networks

The generalization error is defined to be the average error that the student makes on
a test example selected at random (and uncorrelated with the training set), which
we write as ?g = (E).
Although one could, in principle, model the student weight dynamics directly, this
will typically involve too many parameters, and we seek a more compact representation for the evolution of the generalization error. It is straightforward to show that
the generalization error depends, not on a detailed description of all the network
weights, but only on the overlap parameters Qll' = ~ W WI' and Rim = ~ W w':n
[1, 5, 7]. In the case of infinite 0, it is possible to obtain a closed set of equations
governing the overlap parameters Q, R [5]. For finite training sets, however, this is
no longer possible, due to the correlations between successive weight updates[7].

r

r

In order to overcome this difficulty, we use a technique developed originally to study
statistical physics systems [2] . Initially, consider the dynamics of a general vector of
order parameters, denoted by 0, which are functions of the network weights w. If
the weight updates are described by a transition probability T(w -+ w'), then an
approximate update equation for 0 is

0' - 0 = IfdW' (O(w') - O(w)) T(w -+
\

W'))

(4)
P(w)oc6(O(w)-O)

Intuitively, the integral in the above equation expresses the average change l of 0
caused by a weight update w -+ w', starting from (given) initial weights w. Since
our aim is to develop a closed set of equations for the order parameter dynamics, we
need to remove the dependency on the initial weights w. The only information we
have regarding w is contained in the chosen order parameters 0, and we therefore
average the result over the 'subshell' of all w which correspond to these values of
the order parameters. This is expressed as the 8-function constraint in equation(4).
It is clear that if the integral in (4) depends on w only through O(w), then the
average is unnecessary and the resulting dynamical equations are exact. This is in
fact the case for 0 -+ 00 and 0 = {Q, R}, the standard order parameters mentioned
above[5]. If this cannot be achieved, one should choose a set of order parameters to
obtain approximate equations which are as close as possible to the exact solution.
The motivation for our choice of order parameters is based on the linear perceptron
case where, in addition to the standard parameters Q and R, the overlaps projected
onto eigenspaces of the training input correlation matrix A = ~ E:=l xl' (xl') T are
required 2 . We therefore split the eigenvalues of A into r equal blocks ('Y = 1 ... r)
containing N' = N Ir eigenvalues each, ordering the eigenvalues such that they
increase with 'Y. We then define projectors p'Y onto the corresponding eigenspaces
and take as order parameters:
'Y
R1m

_
-

1 Tp'Y ..
N'w,
wm

UI.'Y

-

~
Nt W,Tp'Yb

II

(5)

where the b B are linear combinations of the noise variables and training inputs,

(6)

1 Here we assume that the system size N is large enough that the mean values of the
parameters alone describe the dynamics sufficiently well (i. e., self-averaging holds).
2The order parameters actually used in our calculation for the linear perceptron[7] are
Laplace transforms of these projected order parameters.

P. Sollich and D. Barber

360

As

r

-+

00,

these order parameters become functionals of a continuous variable3 .

The updates for the order parameters (5) due to the weight updates (3) can be
found by taking the scalar products of (3) with either projected student or teacher
weights, as appropriate. This then introduces the following activation 'components',

k'Y

m

= VNi
ff(w* )Tp'"Yx
m

=

so that the student and teacher activations are h, = ~ E'"Y hi and km ~ E'"Y k~,
respectively. For the linear perceptron, the chosen order parameters form a complete
set - the dynamical equations close, without need for the average in (4).
For the nonlinear case, we now sketch the calculation of the order parameter update
equations (4). Taken together, the integral over Wi (a sum of p discrete terms in
our case, one for each training example) and the subshell average in (4), define
an average over the activations (2), their components (7), and the noise variables
~m, ~o. These variables turn out to be Gaussian distributed with zero mean, and
therefore only their covariances need to be worked out. One finds that these are in
fact given by the naive training set averages. For example,

=
(8)

where we have used p'"Y A = a'"YP'"Y with a'"Y 'the' eigenvalue of A in the ,-th
eigenspace; this is well defined for r -+ 00 (see [6] for details of the eigenvalue
spectrum). The correlations of the activations and noise variables explicitly appearing in the error in (3) are calculated similarly to give,
(h,h,,) =

~

L:; Q~,
'"Y

(h,km) =

~L

:; Rim

(9)

'"Y

(h,~s)

=

~ L ~U,~
'"Y

where the final equation defines the noise variances. The T~m' are projected overlaps between teacher weight vectors, T~m' = ~ (w~)Tp'"Yw:n,. We will assume that
the teacher weights and training inputs are uncorrelated, so that T~m' is independent of ,. The required covariances of the 'component' activations are
a'"YR'"Y

(kinh,)
(c] h,)
(hi h" )

a

'm

-

a'"YU'"Y

-

a'"YQ'"Y

a

a

ls
II'

(k~km')

=

a'"YT'"Y

(c]k m, )

-

0

-

a'"YR'"Y

(hJkm,)

a

a

mm'

'm

(k~~s)

-

0

(C]~8' )

-

a'"Y 2
-(7s588 ,

=

.!.U'"Y

(hJ~s)

a

a

's

(10)
3Note that the limit r -+ 00 is taken after the thermodynamic limit, i.e., r ~ N. This
ensures that the number of order parameters is always negligible compared to N (otherwise
self-averaging would break down).

On-line Learning from Finite Training Sets in Nonlinear Networks
0.03 r f I I : - - - - -........- - - - - - - ,
0.025 I

(a)

0.25

(b)

o
00

OOOOOOOOC

0.2

000000000000000000000000

0000000 00

0.02

L.. ...o~ooo
~

I

0.15
0.01

361

'------~-----~

o

t

50

\

0000

'NNNoaa oa

aaaoaaaaaaaaaaaaaaaac

,,------------

o

100

50

t

100

Figure 1: fg vs t for student and teacher with one hidden unit (L = M = 1);
a = 2, 3, 4 from above, learning rate "I = 1. Noise of equal variance was added to
both activations and output (a) O'~ = 0'5 = 0.01, (b) O'~ = 0'5= 0.1. Simulations
for N = 100 are shown by circles; standard errors are of the order of the symbol
size. The bottom dashed lines show the infinite training set result for comparison.
r = 10 was used for calculating the theoretical predictions; the curved marked "+"
in (b), with r = 20 (and a = 2), shows that this is large enough to be effectively in
the r -+ 00 limit.
Using equation (3) and the definitions (7), we can now write down the dynamical
equations, replacing the number of updates n by the continuous variable t = n/ N
in the limit N -+ 00:
-"I (k-:nOh,E)

OtRim
OtU?s

-"I (c~oh,E)

OtQIz,

-"I (h7 Oh" E) - "I

(h~ Oh, E) + "12 a-y (Oh,Eoh" E)

(11)
a
where the averages are over zero mean Gaussian variables, with covariances (9,10).
Using the explicit form of the error E, we have

oh,E = g'(h,) [L9(hl') - Lg(km
I'

+ em) -

eo]

(12)

m

which, together with the equations (11) completes the description of the dynamics.
The Gaussian averages in (11) can be straightforwardly evaluated in a manner
similar to the infinite training set case[5], and we omit the rather cumbersome
explicit form of the resulting equations.
We note that, in contrast to the infinite training set case, the student activations
hI and the noise variables C and
are now correlated through equation (10).
Intuitively, this is reasonable as the weights become correlated, during training,
with the examples in the training set. In calculating the generalization error, on the
other hand, such correlations are absent, and one has the same result as for infinite
training sets. The dynamical equations (11), together with (9,10) constitute our
main result. They are exact for the limits of either a linear network (R, Q, T -+ 0,
so that g(x) ex: x) or a -+ 00, and can be integrated numerically in a straightforward
way. In principle, the limit r -+ 00 should be taken but, as shown below, relatively
small values of r can be taken in practice.

s

3

es

RESULTS AND DISCUSSION

We now discuss the main consequences of our result (11), comparing the resulting
predictions for the generalization dynamics, fg(t), to the infinite training set theory

P. Sollich and D. Barber

362

k

(a)

0.4 ..----------~--~----,

0.25
02
. 100000000000000000000000
1
______________

0.3

0.15 ,

0.2

0.1

\

0.05

...

,'--

~

--- ---

0.1

O~--~------~----~~~

o

(b)

10

20

30

40

t

50

~ooooooooooooooooooo
o
W
100
1W t 200

OL---~----------~----~

Figure 2: ?g VS t for two hidden units (L = M = 2). Left: a = 0.5, with a = 00
shown by dashed line for comparison; no noise. Right: a = 4, no noise (bottom)
and noise on teacher activations and outputs of variance 0.1 (top). Simulations for
N = 100 are shown by small circles; standard errors are less than the symbol size.
Learning rate fJ = 2 throughout.

and to simulations. Throughout, the teacher overlap matrix is set to
(orthogonal teacher weight vectors of length V'ii).

Tij

=

c5ij

In figure(l), we study the accuracy of our method as a function of the training
set size for a nonlinear network with one hidden unit at two different noise levels.
The learning rate was set to fJ = 1 for both (a) and (b). For small activation
and output noise (0'2 = 0.01), figure(la) , there is good agreement with the simulations for a down to a = 3, below which the theory begins to underestimate
the generalization error, compared to simulations. Our finite a theory, however,
is still considerably more accurate than the infinite a predictions. For larger noise
(0'2 = 0.1, figure(lb?, our theory provides a reasonable quantitative estimate of the
generalization dynamics for a > 3. Below this value there is significant disagreement, although the qualitative behaviour of the dynamics is predicted quite well,
including the overfitting phenomenon beyond t ~ 10. The infinite a theory in this
case is qualitatively incorrect.

In the two hidden unit case, figure(2), our theory captures the initial evolution of
?g(t) very well, but diverges significantly from the simulations at larger t; nevertheless, it provides a considerable improvement on the infinite a theory. One reason for
the discrepancy at large t is that the theory predicts that different student hidden
units will always specialize to individual teacher hidden units for t --+ 00, whatever
the value of a. This leads to a decay of ?g from a plateau value at intermediate times
t. In the simulations, on the other hand, this specialization (or symmetry breaking)
appears to be inhibited or at least delayed until very large t. This can happen even
for zero noise and a 2:: L, where the training data should should contain enough
information to force student and teacher weights to be equal asymptotically. The
reason for this is not clear to us, and deserves further study. Our initial investigations, however, suggest that symmetry breaking may be strongly delayed due to the
presence of saddle points in the training error surface with very 'shallow' unstable
directions.
When our theory fails, which of its assumptions are violated? It is conceivable
that multiple local minima in the training error surface could cause self-averaging
to break down; however, we have found no evidence for this, see figure(3a). On
the other hand, the simulation results in figure(3b) clearly show that the implicit
assumption of Gaussian student activations - as discussed before eq. (8) - can be
violated.

On-line Learning from Finite Training Sets in Nonlinear Networks

(a)

363

(b)

/

Variance over training histories

10"'" ' - - - - - - - - - - - - - - - '
102
N

Figure 3: (a) Variance of fg(t = 20) vs input dimension N for student and teacher
with two hidden units (L = M = 2), a = 0.5, 'fJ = 2, and zero noise. The bottom
curve shows the variance due to different random choices of training examples from
a fixed training set ('training history'); the top curve also includes the variance due
to different training sets. Both are compatible with the liN decay expected if selfaveraging holds (dotted line). (b) Distribution (over training set) of the activation
hI of the first hidden unit of the student. Histogram from simulations for N = 1000,
all other parameter values as in (a).
In summary, the main theoretical contribution of this paper is the extension of online
learning analysis for finite training sets to nonlinear networks. Our approximate
theory does not require the use of replicas and yields ordinary first order differential
equations for the time evolution of a set of order parameters. Its central implicit
assumption (and its Achilles' heel) is that the student activations are Gaussian
distributed. In comparison with simulations, we have found that it is more accurate
than the infinite training set analysis at predicting the generalization dynamics for
finite training sets, both qualitatively and also quantitatively for small learning
times t. Future work will have to show whether the theory can be extended to cope
with non-Gaussian student activations without incurring the technical difficulties
of dynamical replica theory [2], and whether this will help to capture the effects of
local minima and, more generally, 'rough' training error surfaces.
Acknowledgments: We would like to thank Ansgar West for helpful discussions.

References
[1] M. Biehl and H. Schwarze. Journal of Physics A, 28:643-656, 1995.
[2] A. C. C. Coolen, S. N. Laughton, and D. Sherrington. In NIPS 8, pp. 253-259,
MIT Press, 1996; S.N. Laughton, A.C.C. Coolen, and D. Sherrington. Journal
of Physics A, 29:763-786, 1996.
[3] See for example: The dynamics of online learning. Workshop at NIPS'95.
[4] T. Heskes and B. Kappen. Physical Review A, 44:2718-2762, 1994.
[5] D. Saad and S. A. Solla Physical Review E, 52:4225, 1995.
[6] P. Sollich. Journal of Physics A, 27:7771-7784, 1994.
[7] P. Sollich and D. Barber. In NIPS 9, pp.274-280, MIT Press, 1997; Europhysics
Letters, 38:477-482, 1997.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 4135-a-theory-of-multiclass-boosting.pdf

A Theory of Multiclass Boosting

Indraneel Mukherjee

Robert E. Schapire

Princeton University, Department of Computer Science, Princeton, NJ 08540
{imukherj,schapire}@cs.princeton.edu

Abstract
Boosting combines weak classifiers to form highly accurate predictors. Although
the case of binary classification is well understood, in the multiclass setting, the
?correct? requirements on the weak classifier, or the notion of the most efficient
boosting algorithms are missing. In this paper, we create a broad and general
framework, within which we make precise and identify the optimal requirements
on the weak-classifier, as well as design the most effective, in a certain sense,
boosting algorithms that assume such requirements.

1

Introduction

Boosting [17] refers to a general technique of combining rules of thumb, or weak classifiers, to form
highly accurate combined classifiers. Minimal demands are placed on the weak classifiers, so that a
variety of learning algorithms, also called weak-learners, can be employed to discover these simple
rules, making the algorithm widely applicable. The theory of boosting is well-developed for the case
of binary classification. In particular, the exact requirements on the weak classifiers in this setting
are known: any algorithm that predicts better than random on any distribution over the training set
is said to satisfy the weak learning assumption. Further, boosting algorithms that minimize loss as
efficiently as possible have been designed. Specifically, it is known that the Boost-by-majority [6]
algorithm is optimal in a certain sense, and that AdaBoost [11] is a practical approximation.
Such an understanding would be desirable in the multiclass setting as well, since many natural classification problems involve more than two labels, e.g. recognizing a digit from its image, natural
language processing tasks such as part-of-speech tagging, and object recognition in vision. However, for such multiclass problems, a complete theoretical understanding of boosting is lacking. In
particular, we do not know the ?correct? way to define the requirements on the weak classifiers, nor
has the notion of optimal boosting been explored in the multiclass setting.
Straightforward extensions of the binary weak-learning condition to multiclass do not work. Requiring less error than random guessing on every distribution, as in the binary case, turns out to be too
weak for boosting to be possible when there are more than two labels. On the other hand, requiring
more than 50% accuracy even when the number of labels is much larger than two is too stringent,
and simple weak classifiers like decision stumps fail to meet this criterion, even though they often
can be combined to produce highly accurate classifiers [9]. The most common approaches so far
have relied on reductions to binary classification [2], but it is hardly clear that the weak-learning
conditions implicitly assumed by such reductions are the most appropriate.
The purpose of a weak-learning condition is to clarify the goal of the weak-learner, thus aiding in
its design, while providing a specific minimal guarantee on performance that can be exploited by a
boosting algorithm. These considerations may significantly impact learning and generalization because knowing the correct weak-learning conditions might allow the use of simpler weak classifiers,
which in turn can help prevent overfitting. Furthermore, boosting algorithms that more efficiently
and effectively minimize training error may prevent underfitting, which can also be important.
In this paper, we create a broad and general framework for studying multiclass boosting that formalizes the interaction between the boosting algorithm and the weak-learner. Unlike much, but not all,
of the previous work on multiclass boosting, we focus specifically on the most natural, and perhaps
1

weakest, case in which the weak classifiers are genuine classifiers in the sense of predicting a single
multiclass label for each instance. Our new framework allows us to express a range of weak-learning
conditions, both new ones and most of the ones that had previously been assumed (often only implicitly). Within this formalism, we can also now finally make precise what is meant by correct
weak-learning conditions that are neither too weak nor too strong.
We focus particularly on a family of novel weak-learning conditions that have an especially appealing form: like the binary conditions, they require performance that is only slightly better than
random guessing, though with respect to performance measures that are more general than ordinary
classification error. We introduce a whole family of such conditions since there are many ways of
randomly guessing on more than two labels, a key difference between the binary and multiclass settings. Although these conditions impose seemingly mild demands on the weak-learner, we show that
each one of them is powerful enough to guarantee boostability, meaning that some combination of
the weak classifiers has high accuracy. And while no individual member of the family is necessary
for boostability, we also show that the entire family taken together is necessary in the sense that for
every boostable learning problem, there exists one member of the family that is satisfied. Thus, we
have identified a family of conditions which, as a whole, is necessary and sufficient for multiclass
boosting. Moreover, we can combine the entire family into a single weak-learning condition that is
necessary and sufficient by taking a kind of union, or logical OR, of all the members. This combined
condition can also be expressed in our framework.
With this understanding, we are able to characterize previously studied weak-learning conditions. In
particular, the condition implicitly used by AdaBoost.MH [19], which is based on a one-against-all
reduction to binary, turns out to be strictly stronger than necessary for boostability. This also applies
to AdaBoost.M1 [9], the most direct generalization of AdaBoost to multiclass, whose conditions
can be shown to be equivalent to those of AdaBoost.MH in our setting. On the other hand, the
condition implicit to Zhu et al.?s SAMME algorithm [21] is too weak in the sense that even when the
condition is satisfied, no boosting algorithm can guarantee to drive down the training error. Finally,
the condition implicit to AdaBoost.MR [19, 9] (also called AdaBoost.M2) turns out to be exactly
necessary and sufficient for boostability.
Employing proper weak-learning conditions is important, but we also need boosting algorithms that
can exploit these conditions to effectively drive down error. For a given weak-learning condition,
the boosting algorithm that drives down training error most efficiently in our framework can be
understood as the optimal strategy for playing a certain two-player game. These games are nontrivial to analyze. However, using the powerful machinery of drifting games [8, 16], we are able to
compute the optimal strategy for the games arising out of each weak-learning condition in the family
described above. These optimal strategies have a natural interpretation in terms of random walks, a
phenomenon that has been observed in other settings [1, 6].
Our focus in this paper is only on minimizing training error, which, for the algorithms we derive,
provably decreases exponentially fast with the number of rounds of boosting. Such results can be
used in turn to derive bounds on the generalization error using standard techniques that have been
applied to other boosting algorithms [18, 11, 13]. (We omit these due to lack of space.)
The game-theoretic strategies are non-adaptive in that they presume prior knowledge about the edge,
that is, how much better than random are the weak classifiers. Algorithms that are adaptive, such as
AdaBoost, are much more practical because they do not require such prior information. We show
therefore how to derive an adaptive boosting algorithm by modifying one of the game-theoretic
strategies.
We present experiments aimed at testing the efficacy of the new methods when working with a very
weak weak-learner to check that the conditions we have identified are indeed weaker than others that
had previously been used. We find that our new adaptive strategy achieves low test error compared
to other multiclass boosting algorithms which usually heavily underfit. This validates the potential
practical benefit of a better theoretical understanding of multiclass boosting.
Previous work. The first boosting algorithms were given by Schapire [15] and Freund [6], followed
by their AdaBoost algorithm [11]. Multiclass boosting techniques include AdaBoost.M1 and AdaBoost.M2 [11], as well as AdaBoost.MH and AdaBoost.MR [19]. Other approaches include [5, 21].
There are also more general approaches that can be applied to boosting including [2, 3, 4, 12]. Two
game-theoretic perspectives have been applied to boosting. The first one [10, 14] views the weak-

2

learning condition as a minimax game, while drifting games [16, 6] were designed to analyze the
most efficient boosting algorithms. These games have been further analyzed in the multiclass and
continuous time setting in [8].

2

Framework

We introduce some notation. Unless otherwise stated, matrices will be denoted by bold capital letters
like M, and vectors by bold small letters like v. Entries of a matrix and vector will be denoted as
M (i, j) or v(i), while M(i) will denote the ith row of a matrix. Inner product of two vectors u, v
is denoted by hu, vi. The Frobenius inner product of two matrices Tr(MM0 ) will be denoted by
M ? M0 . The indicator function is denoted by 1 [?]. The distribution over the set {1, . . . , k} will be
denoted by ? {1, . . . , k}.
In multiclass classification, we want to predict the labels of examples lying in some set X. Each
example x ? X has a unique y label in the set {1, . . . , k}, where k ? 2. We are provided a training
set of labeled examples {(x1 , y1 ), . . . , (xm , ym )}.
Boosting combines several mildly powerful predictors, called weak classifiers, to form a highly
accurate combined classifier, and has been previously applied for multiclass classification. In this
paper, we only allow weak classifier that predict a single class for each example. This is appealing,
since the combined classifier has the same form, although it differs from what has been used in much
previous work.
We adopt a game-theoretic view of boosting. A game is played between two players, Booster and
Weak-Learner, for a fixed number of rounds T . With binary labels, Booster outputs a distribution
in each round, and Weak-Learner returns a weak classifier achieving more than 50% accuracy on
that distribution. The multiclass game is an extension of the binary game. In particular, in each
round t: (1) Booster creates a cost-matrix Ct ? Rm?k , specifying to Weak-Learner that the cost
of classifying example xi as l is C(i, l). The cost-matrix may not be arbitrary, but should conform
to certain restrictions as discussed below. (2) Weak-Learner returns some weakP
classifier ht : X ?
m
{1, . . . , k} from a fixed space ht ? H so that the cost incurred is Ct ? 1ht = i=1 Ct (i, ht (xi )),
is ?small enough?, according to some conditions discussed below. Here by 1h we mean the m ? k
matrix whose (i, j)-th entry is 1 [h(i) = j]. (3) Booster computes a weight ?t for the current weak
classifier based on how much cost was incurred in this round.
At the end, Booster predicts according to the weighted plurality vote of the classifiers returned in
each round:
T
X
M
M
1 [ht (x) = l] ?t .
(1)
H(x) = argmax fT (x, l), where fT (x, l) =
l?{1,...,k}

t=1

By carefully choosing the cost matrices in each round, Booster aims to minimize the training error
of the final classifer H, even when Weak-Learner is adversarial. The restrictions on cost-matrices
created by Booster, and the maximum cost Weak-Learner can suffer in each round, together define
the weak-learning condition being used. For binary labels, the traditional weak-learning condition
states: for any non-negative weights w(1),
P . . . , w(m) on the training set, the error of the weak
classfier returned is at most (1/2 ? ?/2) i wi . Here ? parametrizes the condition. There are many
ways to translate this condition into our language. The one with fewest restrictions on the costmatrices requires labeling correctly should be less costly than labeling incorrectly: ?i : C(i, yi ) ?
C(i, y?i ), while
on the
 returned
 weak classifier h
 requires	 less cost than predicting
P the restriction P
randomly: i C(i, h(xi )) ? i 12 ? ?2 C(i, y?i ) + 21 + ?2 C(i, yi ) . By the correspondence
w(i) = C(i, y?i ) ? C(i, yi ), we may verify the two conditions are the same.
We will rewrite this condition after making some simplifying assumptions. Henceforth, without
loss of generality, we assume that the true label is always 1. Let C bin ? Rm?2 consist of matrices
m?2
C which satisfy C(i, 1) ? C(i, 2). Further, let Ubin
be the matrix whose each row is
? ? R
(1/2 + ?/2, 1/2 ? ?/2). Then, Weak-Learner searching
space H satisfies the binary weak-learning

condition if: ?C ? C bin , ?h ? H : C ? 1h ? Ubin
? 0. There are two main benefits to this refor?
mulation. With linear homogeneous constraints, the mathematics is simplified, as will be apparent
later. More importantly, by varying the restrictions C bin on the cost vectors and the matrix Ubin , we
can generate a vast variety of weak-learning conditions for the multiclass setting k ? 2 as we now
show.
3

Let C ? Rm?k and matrix B ? Rm?k , which we call the baseline; we say a weak classifier space
H satisfies the condition (C, B) if
?C ? C, ?h ? H : C ? (1h ? B) ? 0,

i.e.,

m
X

c(i, h(i)) ?

i=1

m
X

hc(i), B(i)i .

(2)

i=1

In (2), the variable matrix C specifies how costly each misclassification is, while the baseline B
specifies a weight for each misclassification. The condition therefore states that a weak classifier should not exceed the average cost when weighted according to baseline B. This large class
of weak-learning conditions captures many previously used conditions, such as the ones used by
AdaBoost.M1 [9], AdaBoost.MH [19] and AdaBoost.MR [9, 19] (see below), as well as novel conditions introduced in the next section.
By studying this vast class of weak-learning conditions, we hope to find the one that will serve the
main purpose of the boosting game: finding a convex combination of weak classifiers that has zero
training error. For this to be possible, at the minimum the weak classifiers should be sufficiently rich
for such a perfect combination to exist. Formally, a collection H of weak classifiers is eligible for
boosting, or simply boostable,
P if there exists a distribution ? on this space that linearly separates the
data: ?i : argmaxl?{1,...,k} h?H ?(h)1 [h(xi ) = l] = yi . The weak-learning condition plays two
roles. It rejects spaces that are not boostable, and provides an algorithmic means of searching for the
right combination. Ideally, the second factor will not cause the weak-learning condition to impose
additional restrictions on the weak classifiers; in that case, the weak-learning condition is merely a
reformulation of being boostable that is more appropriate for deriving an algorithm. In general, it
could be too strong, i.e. certain boostable spaces will fail to satisfy the conditions. Or it could be too
weak i.e., non-boostable spaces might satisfy such a condition. Booster strategies relying on either
of these conditions will fail to drive down error; the former due to underfitting, and the latter due
to overfitting. In the next section we will describe conditions captured by our framework that avoid
being too weak or too strong.

3

Necessary and sufficient weak-learning conditions

The binary weak-learning condition has an appealing form: for any distribution over the examples,
the weak classifier needs to achieve error not greater than that of a random player who guesses
the correct answer with probability 1/2 + ?. Further, this is the weakest condition under which
boosting is possible as follows from a game-theoretic perspective [10, 14] . Multiclass weak-learning
conditions with similar properties are missing in the literature. In this section we show how our
framework captures such conditions.
In the multiclass setting, we model a random player as a baseline predictor B ? Rm?k whose rows
are distributions over the labels, B(i) ? ? {1, . . . , k}. The prediction on example i is a sample from
B(i). We only consider the space of edge-over-random baselines B?eor ? Rm?k who have a faint
clue about the correct answer. More precisely, any baseline B ? B?eor in this space is ? more likely
to predict the correct label than an incorrect one on every example i: ?l 6= 1, B(i, 1) ? B(i, l) + ?,
with equality holding for some l.
When k = 2, the space B?eor consists of the unique player Ubin
? , and the binary weak-learning
bin
bin
condition is given by (C , U? ). The new conditions generalize this to k > 2. In particular, define
C eor to be the multiclass extension of C bin : any cost-matrix in C eor should
 put the least cost on the
	
correct label, i.e., the rows of the cost-matrices should come from the set c ? Rk : ?l, c(1) ? c(l) .
Then, for every baseline B ? B?eor , we introduce the condition (C eor , B), which we call an edgeover-random weak-learning condition. Since C ? B is the expected cost of the edge-over-random
baseline B on matrix C, the constraints (2) imposed by the new condition essentially require better
than random performance.
We now present the central results of this section. The seemingly mild edge-over-random conditions
guarantee eligibility, meaning weak classifiers that satisfy any one such condition can be combined
to form a highly accurate combined classifier.
Theorem 1 (Sufficiency). If a weak classifier space H satisfies a weak-learning condition (C eor , B),
for some B ? B?eor , then H is boostable.
4

The proof involves the Von-Neumann Minimax theorem, and is in the spirit of the ones in [10]. On
the other hand the family of such conditions, taken as a whole, is necessary for boostability in the
sense that every eligible space of weak classifiers satisfies some edge-over-random condition.
Theorem 2 (Relaxed necessity). For every boostable weak classifier space H, there exists a ? > 0
and B ? B?eor such that H satisfies the weak-learning condition (C eor , B).
The proof shows existence through non-constructive averaging arguments. Theorem 2 states that
any boostable weak classifier space will satisfy some condition in our family,
 but it does not help
us choose the right condition. Experiments in Section 5 suggest C eor , U? is effective with very
simple weak-learners compared to popular boosting algorithms. (Here U? ? B?eor is the edge-overrandom baseline closest to uniform; it has weight (1 ? ?)/k on incorrect labels and (1 ? ?)/k + ?
on the correct label.) However, there are theoretical examples showing each condition in our family
is too strong (supplement).
A perhaps extreme way of weakening the condition is by requiring the performance on a cost matrix
to be competitive not with a fixed baseline B ? B?eor , but with the worst of them:
?C ? C eor , ?h ? H : C ? 1h ? maxeor C ? B.
B?B?

(3)

Condition (3) states that during the course of the same boosting game, Weak-Learner may choose
to beat any edge-over-random baseline B ? B?eor , possibly a different one for every round and every
cost-matrix. This may superficially seem much too weak. On the contrary, this condition turns out
to be equivalent to boostability. In other words, according to our criterion, it is neither too weak nor
too strong as a weak-learning condition. However, unlike the edge-over-random conditions, it also
turns out to be more difficult to work with algorithmically.
Furthermore, this condition can be shown to be equivalent to the one used by AdaBoost.MR [19, 9].
This is perhaps remarkable since the latter is based on the apparently completely unrelated all-pairs
MR
consists of
multiclass to binary reduction: the MR condition is given by (C MR , BMR
? ), where C
cost-matrices that put non-negative costs on incorrect labels and whose rows sum up to zero, while
m?k
BMR
is the matrix that has ? on the first column and ?? on all other columns(supplement).
? ?R
Further, the MR condition, and hence (3), can be shown to be neither too weak nor too strong.
Theorem 3 (MR). A weak classifier space H satisfies AdaBoost.MR?s weak-learning condition
(C MR , BMR
? ) if and only if it satisfies (3). Moreover, this condition is equivalent to being boostable.
Next, we illustrate the strengths of our random-over-edge weak-learning conditions through concrete
comparisons with previous algorithms.
Comparison with SAMME. The SAMME algorithm of [21] requires the weak classifiers to
achieve less error than uniform random guessing for multiple labels; in our language, their weaklearning condition is (C = {(?t, t, t, . . .) : t ? 0} , U? ). As is well-known, this condition is
not sufficient for boosting to be possible. In particular, consider the dataset {(a, 1), (b, 2)} with
k = 3, m = 2, and a weak classifier space consisting of h1 , h2 which always predict 1, 2, respectively. Since neither classifier distinguishes between a, b we cannot achieve perfect accuracy by
combining them in any way. Yet, due to the constraints on the cost-matrix, one of h1 , h2 will always
manage non-positive cost while random always suffers positive cost. On the other hand our weaklearning condition allows the Booster to choose far richer cost matrices. In particular, when the
cost matrix is C = (c(1) = (?1, +1, 0), c(2) = (+1, ?1, 0)) ? C eor , both classifiers in the above
example suffer more loss than the random player U? , and fail to satisfy our condition.
Comparison with AdaBoost.MH. AdaBoost.MH is a popular multiclass boosting algorithm that is
based on the one-against-all reduction[19]. However, we show that its implicit demands on the weak
classifier space is too strong. We construct a classifier space that satisfies the condition (C eor , U? )
in our family, but cannot satisfy AdaBoost.MH?s weak-learning condition.
Consider a space H that has, for every (1/k + ?)m element subset of the examples, a classifier
that predicts correctly on exactly those elements. The expected loss of a randomly chosen classifier
from this space is the same as that of the random player U? . Hence H satisfies this weak-learning
condition. On the other hand, it can be shown (supplement) that AdaBoost.MH?s weak-learning
MH
condition is the pair (C MH , BMH
has non-(positive)negative entries on (in)correct labels,
? ), where C
and where each row of the matrix BMH
is
the
vector (1/2 + ?/2, 1/2 ? ?/2, . . . , 1/2 ? ?/2). A
?
5

quick calculation shows that for any h ? H, and C ? C MH with ?1 in the first column and zeroes
elsewhere, C ? 1h ? BMH
= 1/2 ? 1/k. This is positive when k > 2, so that H fails to satisfy
?
AdaBoost.MH?s condition.

4

Algorithms

In this section we devise algorithms by analyzing the boosting games that employ our edge-overrandom weak-learning conditions. We compute the optimum Booster strategy against a completely
adversarial Weak-Learner, which here is permitted to choose weak classifiers without restriction,
i.e. the entire space Hall of all possible functions mapping examples to labels. By modeling WeakLearner adversarially, we make absolutely no assumptions on the algorithm it might use. Hence,
error guarantees enjoyed in this situation will be universally applicable. Our algorithms are derived
from the very general drifting games framework [16] for solving boosting games, in turn inspired
by Freund?s Boost-by-majority algorithm [6], which we review next.
The OS Algorithm. Fix the number of rounds T and an edge-over-random weak-learning condition
(C, B). For simplicity of presentation we fix the weights ?t = 1 in each round. With fT defined as
in (1), the optimum Booster payoff can be written as
m
X
min
max
. . . min
max
(1/m)
L(fT (xi , 1), fT (xi , 2), . . . , fT (xi , k)).
C1 ?C

h1 ?Hall :
C1 ?(1h1 ?B)?0

CT ?C

hT ?Hall :
CT ?(1hT ?B)?0

i=1

Here the function L : Rk ? R is error, but we can also consider other loss functions such as
exponential loss, hinge loss, etc. that upper-bound error and are proper: i.e. L(x) is increasing in
the weight of the correct label x(1), and decreasing in the weights of the incorrect labels x(l), l 6= 1.
Directly analyzing the optimal payoff is hard. However, Schapire [16] observed that the payoffs
can be very well approximated by certain potential functions. Indeed, for any b ? Rk define the
k
potential function ?b
t : R ? R by the following recurrence:



	
?b
?b
min
max
El?p ?b
0 = L;
t (s) =
t?1 (s + el ) : El?p [c(l)] ? hb, ci , (4)
c?Rk :?l:c(1)?c(l) p??{1,...,k}

where el ? Rk is the unit-vector whose lth coordinate is 1 and the remaining coordinates zero.
These potential functions compute an estimate ?b
t (st ) of whether an example x will be misclassified,
based on its current state st consisting of counts of votes received so far on various classes st (l) =
Pt?1
0
t0 =1 1 [ht (x) = l], and the number of rounds t remaining. Using these functions, Schapire [16]
proposed a Booster strategy, aka the OS strategy, which, in round t, constructs a cost matrix C ? C,
whose each row C(i) achieves the minimum of the right hand side of (4) with b replaced by B(i), t
replaced by T ? t, and s replaced by current state st (i). The following theorem provides a guarantee
for the loss suffered by the OS algorithm, and also shows that it is the game-theoretically optimum
strategy when the number of examples is large.
Theorem 4 (Extension of results in [16]). Suppose the weak-learning condition is given by (C, B), If
Pm B(i)
Booster employs the OS algorithm, then the average potential of the states (1/m) i=1 ?t (s(i))
never increases in any round. In particular, loss suffered after T rounds of play is at most
Pm B(i)
(1/m) i=1 ?T (0). Further, for any  > 0, when the loss function satisfies some mild conditions, and m  T, k, 1/, no Booster strategy can achieve loss  less than the above bound in T
rounds.
Computing the potentials. In order to implement the OS strategy using our weak-learning conditions, we only need to compute the potential ?b
t for distributions b ? ? {1, . . . , k}. Fortunately,
these potentials have a very simple solution in terms of the homogeneous random-walk Rtb (x), the
random position of a particle after t time steps, that starts at location x ? Rk , and in each step moves
in direction el with probability b(l).
Theorem 5. If L is proper, and b ? ? {1, . . . , k} satisfies ?l : b(1) ? b(l), then ?b
t (s) =
E [L (Rtb (s))]. Furthermore, the vector achieving the minimum in the right hand side of (4) is
given by c(l) = ?b
t?1 (s + el ).
Theorem (5) implies the OS strategy chooses the following cost matrix in round t: c(i, l) =
b(i)
?T ?t?1 (st (i) + el ), where st (i) is the state of example i in round t. Therefore everything boils
6

down to computing the potentials, which is made possible by Theorem 5. There is no simple closed
form solution for the non-convex 0-1 loss L(s) = 1[s1 ? (maxi>1 si )]. However, using Theorem 4, we can write the potential ?t (s) explicitly, and then compute it using dynamic programming
in O(t3 k) time. This yields very tight bounds.
To obtain a more efficient procedure, and one that we will soon show can be made adaptive, we next
focus on the exponential loss associated with AdaBoost that does have a closed form solution.
Lemma 1. If L(s) = exp(?2 (s2 ? s1 )) + ? ? ? + exp(?k (sk ? s1 )), where each ?l is positive, then
Pk
t ?l (sl ?s1 )
the solution in Theorem 5 evaluates to ?b
, where al = 1 ? (b1 + bl ) +
t (s) =
l=2 (al ) e
?l
??l
e bl + e b1 .
The proof by induction is straightforward. In particular, when the condition is (C eor , U? ) and
Pk
? = (?, ?, . . .), the relevant potential is ?t (s) = ?(?, ?)t l=2 e?(sl ?s1 ) where ?(?, ?) =
1 + (1??)
(e? + e?? ? 2) ? (1 ? e?? ) ?. The cost-matrix output by the OS algorithm can be
k
simplified by rescaling, or adding the same number to each coordinate of a cost vector, without
affecting the constraints it imposes on a weak classifier, to the following form
(
(e? ? 1) e?(sl ?s1 )
if l > 1,
Pk
c(i, l) =
(5)
(e?? ? 1) j=2 e?(sj ?s1 ) if l = 1,
With such
Pm a choice, Theorem 4 and the form of the potential guarantee that the average loss
(1/m) i=1 L(st (i)) of the states st (i) changes by a factor of at most ? (?, ?) every round. Hence
T
the final loss is at most (k ? 1)? (?, ?) .
Variable edges. So far we have required Weak-Learner to beat random by at least a fixed amount
? > 0 in each round of the boosting game. In reality, the edge over random is larger initially,
and gets smaller as the OS algorithm creates harder cost matrices. Therefore requiring a fixed
edge is either unduly pessimistic or overly optimistic. If the fixed edge is too small, not enough
progress is made in the initial rounds, and if the edge is too large, Weak-Learner fails to meet the
weak-learning condition in latter rounds. We attempt to fix this via two approaches: prescribing a
decaying sequence of edges ?1 , . . . , ?T , or being completely flexible, aka adaptive, with respect to
the edges returned by the weak-learner. In either case, we only use the edge-over-random condition
(C eor , U? ), but with varying values of ?.
Fixed sequence of edges. With a prescribed sequence of edges ?1 , . . . , ?T the weak-learning condition (C eor , U?t ) in each round t is different. We allow the weights ?1 , . . . , ?T to be arbitrary, but they
must be fixed in advance. All the results for uniform ? and weights ?t = 1 hold in this case as well.
Pm Pk
In particular, by the arguments leading to (5), if we want to minimize i=1 l=2 e{ft (i,l)?ft (i,1)} ,
where ft is as defined in (1), then the following strategy is optimal: in round t output the cost matrix
(
if l > 1,
(e?t ? 1) eft?1 (i,j)?ft?1 (i,1)
Pk
C(i, l) =
(6)
(e??t ? 1) j=2 eft?1 (i,j)?ft?1 (i,1) if l = 1.
Pm Pk
This will ensure that the expression i=1 l=2 e{ft (i,l)?ft (i,1)} changes by a factor of at most
QT
?(?t , ?t ) in each round. Hence the final loss will be at most (k ? 1) t=1 ?(?t , ?t ).
Adaptive. In the adaptive setting, we depart from the game-theoretic framework in that WeakLearner is no longer adversarial. Further, we are no longer guaranteed to receive a certain sequence
of edges. Since the choice of cost-matrix in (6) does not depend on the edges, we could fix an
arbitrary set of weights ?t in advance, follow the same algorithm as before and enjoy the same bound
QT
t=1 ?(?t , ?t ). The trouble with this is ?(?t , ?t ) is not less than 1 unless ?t is small compared to
?t . To ensure progress, the weight ?t must be chosen adaptively as a function of ?t . Since we do not
know what edge we will receive, we choose the cost matrix as before but anticipating infinitesimally
small edge, in the spirit of [7], (and with some rescaling)
(
(e? ? 1) eft?1 (i,j)?ft?1 (i,1)
if l > 1,
M 1
Pk
C(i, l) = lim C? (i, l) =
??
f
(i,j)?f
(i,1)
t?1
t?1
??0
? (e ? 1) j=2 e
if l = 1.
(
eft?1 (i,j)?ft?1 (i,1)
if l > 1,
Pk
=
(7)
? j=2 eft?1 (i,j)?ft?1 (i,1) if l = 1.
7

pendigits

100

500

5

20

100

500

0.20

0.20

0.08

0.30

0.14

0.3
0.1
0.0

20

satimage

0.40

0.8
0.4

0.5
0.3
5

poker

0.50

letter
0.5

forest
0.7

0.30 0.35 0.40

connect4

5

20

100

500

5

20 50

200

5

20

100

500

5

20 50

200

(a)

300

500

pendigits

poker

satimage
0.10 0.15 0.20 0.25

0.5

1.0

0 100

300

500

0.50

0.3

0.6
0 100

300

500

0.40

0.1

0.4

0.4

0.6

0.36
0.32
0 100

letter

0.8

1.0

forest

0.8

0.40

connect4

0 100

300

500

0 100

300

500

0 100

300

500

(b)
Figure 1: Figure 1(a) plots the final test-errors of M1(black, dashed), MH(blue, dotted) and New method(red,
solid) against the maximum tree-sizes allowed as weak classifiers. Figure 1(b) plots how fast the test-errors of
these algorithms drop with rounds, when the maximum tree-size allowed is 5.

Since Weak-Learner cooperates, we expect the edge ?t of the returned classifier ht on the supplied
cost-matrix lim??0 C? to be more than just infinitesimal. In that case, by continuity, there are noninfinitesimal choices of the weight ?t such that the edge ?t achieved by ht on the cost-matrix C?t
remains large enough to ensure ?(?t , ?t ) < 1. In fact, with any choice of ?t , we
 get ?(?t , ?t ) ?
1+?t
1
1
1
?t
??t
?t
??t
) ?t + 2 (e + e
? 2) (supplement). Tuning ?t to 2 ln 1??
1 ? 2 (e ? e
results in
t
p
2
loss, o
and hence error, after
? (?t , ?t ) ? 1 ? ?t . This algorithm is adaptive, and ensures
n that the
QT p
PT
2
2
T rounds is at most (k ? 1) t=1 1 ? ?t ? (k ? 1) exp ?(1/2) t=1 ?t .

5

Experiments

We report preliminary experimental results on six, varying multiclass UCI datasets.

0.0

0.1

0.2

0.3

0.4

The first set of experiments were aimed at determining
overall performance of our new algorithm. We compared
MH
a standard implementation M1 of AdaBoost.M1 with C4.5
M1
New Method
as weak learner, and the Boostexter implementation MH
of AdaBoost.MH using stumps [20], with the adaptive
algorithm described in Section 4, which we call New
method, using a naive greedy tree-searching algorithm
Greedy for weak-learner. The size of trees was chosen
to be of the same order as the tree sizes used by M1. Test
errors after 500 rounds of boosting are plotted in Figure 2.
The performance is comparable with M1 and far better
than MH (understandably since stumps are far weaker than
trees), even though our weak-learner is very naive com- Figure 2: This is a plot of the final test-errors
of standard implementations of M1, MH and
pared to C4.5.
connect4

forest

letter

pendigits

poker

satimage

New method after 500 rounds of boosting.

We next investigated how each algorithm performs with
less powerful weak-classifiers, namely, decision trees whose size has been sharply limited to various
pre-specified limits. Figure 1(a) shows test-error plotted as a function of tree size. As predicted by
our theory, our algorithm succeeds in boosting the accuracy even when the tree size is too small
to meet the stronger weak learning assumptions of the other algorithms. The differences in performance are particularly strong when using the smallest tree sizes.
More insight is provided by plots in Figure 1(b) of the rate of convergence of test error with rounds
when the tree size allowed is very small (5). Both M1 and MH drive down the error for a few rounds.
But since boosting keeps creating harder cost-matrices, very soon the small-tree learning algorithms
are no longer able to meet the excessive requirements of M1 and MH. However, our algorithm makes
more reasonable demands that are easily met by the weak learner.
8

References
[1] Jacob Abernethy, Peter L. Bartlett, Alexander Rakhlin, and Ambuj Tewari. Optimal stragies and minimax lower bounds for online convex games. In Proceedings of the Nineteenth Annual Conference on
Computational Learning Theory, pages 415?424, 2008.
[2] Erin L. Allwein, Robert E. Schapire, and Yoram Singer. Reducing multiclass to binary: A unifying
approach for margin classifiers. Journal of Machine Learning Research, 1:113?141, 2000.
[3] Alina Beygelzimer, John Langford, and Pradeep Ravikumar. Error-correcting tournaments. In Algorithmic Learning Theory: 20th International Conference, pages 247?262, 2009.
[4] Thomas G. Dietterich and Ghulum Bakiri. Solving multiclass learning problems via error-correcting
output codes. Journal of Artificial Intelligence Research, 2:263?286, January 1995.
[5] G?unther Eibl and Karl-Peter Pfeiffer. Multiclass boosting for weak classifiers. Journal of Machine Learning Research, 6:189?210, 2005.
[6] Yoav Freund. Boosting a weak learning algorithm by majority.
121(2):256?285, 1995.

Information and Computation,

[7] Yoav Freund. An adaptive version of the boost by majority algorithm. Machine Learning, 43(3):293?318,
June 2001.
[8] Yoav Freund and Manfred Opper. Continuous drifting games. Journal of Computer and System Sciences,
pages 113?132, 2002.
[9] Yoav Freund and Robert E. Schapire. Experiments with a new boosting algorithm. In Machine Learning:
Proceedings of the Thirteenth International Conference, pages 148?156, 1996.
[10] Yoav Freund and Robert E. Schapire. Game theory, on-line prediction and boosting. In Proceedings of
the Ninth Annual Conference on Computational Learning Theory, pages 325?332, 1996.
[11] Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an
application to boosting. Journal of Computer and System Sciences, 55(1):119?139, August 1997.
[12] Trevor Hastie and Robert Tibshirani. Classification by pairwise coupling. Annals of Statistics, 26(2):451?
471, 1998.
[13] V. Koltchinskii and D. Panchenko. Empirical margin distributions and bounding the generalization error
of combined classifiers. Annals of Statistics, 30(1), February 2002.
[14] Gunnar R?atsch and Manfred K. Warmuth. Efficient margin maximizing with boosting. Journal of Machine
Learning Research, 6:2131?2152, 2005.
[15] Robert E. Schapire. The strength of weak learnability. Machine Learning, 5(2):197?227, 1990.
[16] Robert E. Schapire. Drifting games. Machine Learning, 43(3):265?291, June 2001.
[17] Robert E. Schapire. The boosting approach to machine learning: An overview. In MSRI Workshop on
Nonlinear Estimation and Classification, 2002.
[18] Robert E. Schapire, Yoav Freund, Peter Bartlett, and Wee Sun Lee. Boosting the margin: A new explanation for the effectiveness of voting methods. Annals of Statistics, 26(5):1651?1686, October 1998.
[19] Robert E. Schapire and Yoram Singer. Improved boosting algorithms using confidence-rated predictions.
Machine Learning, 37(3):297?336, December 1999.
[20] Robert E. Schapire and Yoram Singer. BoosTexter: A boosting-based system for text categorization.
Machine Learning, 39(2/3):135?168, May/June 2000.
[21] Ji Zhu, Hui Zou, Saharon Rosset, and Trevor Hastie. Multi-class AdaBoost. Statistics and Its Interface,
2:349360, 2009.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 4745-a-scalable-cur-matrix-decomposition-algorithm-lower-time-complexity-and-tighter-bound.pdf

A Scalable CUR Matrix Decomposition Algorithm:
Lower Time Complexity and Tighter Bound

Shusen Wang and Zhihua Zhang
College of Computer Science & Technology
Zhejiang University
Hangzhou, China 310027
{wss,zhzhang}@zju.edu.cn

Abstract
The CUR matrix decomposition is an important extension of Nystr?om approximation to a general matrix. It approximates any data matrix in terms of a small number of its columns and rows. In this paper we propose a novel randomized CUR
algorithm with an expected relative-error bound. The proposed algorithm has the
advantages over the existing relative-error CUR algorithms that it possesses tighter
theoretical bound and lower time complexity, and that it can avoid maintaining the
whole data matrix in main memory. Finally, experiments on several real-world
datasets demonstrate significant improvement over the existing relative-error algorithms.

1

Introduction

Large-scale matrices emerging from stocks, genomes, web documents, web images and videos everyday bring new challenges in modern data analysis. Most efforts have been focused on manipulating, understanding and interpreting large-scale data matrices. In many cases, matrix factorization
methods are employed to construct compressed and informative representations to facilitate computation and interpretation. A principled approach is the truncated singular value decomposition
(SVD) which finds the best low-rank approximation of a data matrix. Applications of SVD such as
eigenface [20, 21] and latent semantic analysis [4] have been illustrated to be very successful.
However, the basis vectors resulting from SVD have little concrete meaning, which makes it very
difficult for us to understand and interpret the data in question.
An example in [10, 19] has well
?
shown this viewpoint; that is, the vector [(1/2)age ? (1/ 2)height + (1/2)income], the sum of the
significant uncorrelated features from a dataset of people?s features, is not particularly informative.
The authors of [17] have also claimed: ?it would be interesting to try to find basis vectors for all
experiment vectors, using actual experiment vectors and not artificial bases that offer little insight.?
Therefore, it is of great interest to represent a data matrix in terms of a small number of actual
columns and/or actual rows of the matrix.
The CUR matrix decomposition provides such techniques, and it has been shown to be very useful
in high dimensional data analysis [19]. Given a matrix A, the CUR technique selects a subset of
columns of A to construct a matrix C and a subset of rows of A to construct a matrix R, and
? = CUR best approximates A. The typical CUR algorithms [7,
computes a matrix U such that A
8, 10] work in a two-stage manner. Stage 1 is a standard column selection procedure, and Stage 2
does row selection from A and C simultaneously. Thus Stage 2 is more complicated than Stage 1.
The CUR matrix decomposition problem is widely studied in the literature [7, 8, 9, 10, 12, 13, 16,
18, 19, 22]. Perhaps the most widely known work on the CUR problem is [10], in which the authors
devised a randomized CUR algorithm called the subspace sampling algorithm. Particularly, the
algorithm has (1 + ?) relative-error ratio with high probability (w.h.p.).
1

Unfortunately, all the existing CUR algorithms require a large number of columns and rows to be
chosen. For example, for an m ? n matrix A and a target rank k ? min{m, n}, the state-ofthe-art CUR algorithm ? the subspace sampling algorithm in [10] ? requires exactly O(k 4 ??6 )
rows or O(k??4 log2 k) rows in expectation to achieve (1 + ?) relative-error ratio w.h.p. Moreover,
the computational cost of this algorithm is at least the cost of the truncated SVD of A, that is,
O(min{mn2 , nm2 }).1 The algorithms are therefore impractical for large-scale matrices.
In this paper we develop a CUR algorithm which beats the state-of-the-art algorithm in both theory
and experiments. In particular, we show in Theorem 5 a novel randomized CUR algorithm with
lower time complexity and tighter theoretical bound in comparison with the state-of-the-art CUR
algorithm in [10].
The rest of this paper is organized as follows. Section 3 introduces several existing column selection
algorithms and the state-of-the-art CUR algorithm. Section 4 describes and analyzes our novel
CUR algorithm. Section 5 empirically compares our proposed algorithm with the state-of-the-art
algorithm.

2

Notations

For a matrix A = [aij ] ? Rm?n , let a(i) be its i-th row and aj be its j-th column. Let ?A?1 =
?
?
2 1/2
be the Frobenius norm, and ?A?2 be the spectral
i,j |aij | be the ?1 -norm, ?A?F = (
i,j aij )
norm. Moreover, let Im denote an m ? m identity matrix, and 0mn denotes an m ? n zero matrix.
??
T
T
T
T
= UA,k ?A,k VA,k
+ UA,k? ?A,k? VA,k?
be the
Let A = UA ?A VA
= i=0 ?A,i uA,i vA,i
SVD of A, where ? = rank(A), and UA,k , ?A,k , and VA,k correspond to the top k singular values.
T
T
We denote Ak = UA,k ?A,k VA,k
. Furthermore, let A? = UA,? ??1
A,? VA,? be the Moore-Penrose
inverse of A [1].

3 Related Work
Section 3.1 introduces several relative-error column selection algorithms related to this work. Section 3.2 describes the state-of-the-art CUR algorithm in [10]. Section 3.3 discusses the connection
between the column selection problem and the CUR problem.
3.1

Relative-Error Column Selection Algorithms

Given a matrix A ? Rm?n , column selection is a problem of selecting c columns of A to construct
C ? Rm?c to minimize ?A ? CC? A?F . Since there are (nc ) possible choices of constructing C,
so selecting the best subset is a hard problem. In recent years, many polynomial-time approximate
algorithms have been proposed, among which we are particularly interested in the algorithms with
relative-error bounds; that is, with c ? k columns selected from A, there is a constant ? such that
?A ? CC? A?F ? ??A ? Ak ?F .
We call ? the relative-error ratio. We now present some recent results related to this work.
We first introduce a recently developed deterministic algorithm called the dual set sparsification
proposed in [2, 3]. We show their results in Lemma 1. Furthermore, this algorithm is a building
block of some more powerful algorithms (e.g., Lemma 2), and our novel CUR algorithm also relies
on this algorithm. We attach the algorithm in Appendix A.
Lemma 1 (Column Selection via Dual Set Sparsification Algorithm). Given a matrix A ? Rm?n
of rank ? and a target rank k (< ?), there exists a deterministic algorithm to select c (> k) columns
of A and form a matrix C ? Rm?c such that
?




1



? 
?
? Ak 
 .

A ? CC A
 ? 1 +

A
F
F
(1 ? k/c)2
1
Although some partial SVD algorithms, such as Krylov subspace methods, require only O(mnk) time,
they are all numerical unstable. See [15] for more discussions.

2

Moreover, the matrix C can be computed in TVA,k +O(mn+nck 2 ), where TVA,k is the time needed
to compute the top k right singular vectors of A.
There are also a variety of randomized column selection algorithms achieving relative-error bounds
in the literature: [3, 5, 6, 10, 14].
An randomized algorithm in [2] selects only c = 2k
? (1 + o(1)) columns to achieve the expected
relative-error ratio (1 + ?). The algorithm is based on the approximate SVD via random projection [15], the dual set sparsification algorithm [2], and the adaptive sampling algorithm [6]. Here we
present the main results of this algorithm in Lemma 2. Our proposed CUR algorithm is motivated
by and relies on this algorithm.
Lemma 2 (Near-Optimal Column Selection Algorithm). Given a matrix A ? Rm?n of rank ?, a
target rank k (2 ? k < ?), and 0 < ? < 1, there exists a randomized algorithm to select at most
)
2k (
1 + o(1)
c=
?
columns of A to form a matrix C ? Rm?c such that
E2 ?A ? CC? A?F ? E?A ? CC? A?2F ? (1 + ?)?A ? Ak ?2F ,
where the expectations are taken w.r.t. C. Furthermore, the matrix C can be computed in O((mnk+
nk 3 )??2/3 ).
3.2

The Subspace Sampling CUR Algorithm

Drineas et al. [10] proposed a two-stage randomized CUR algorithm which has a relative-error
bound w.h.p. Given a matrix A ? Rm?n and a target rank k, in the first stage the algorithm
chooses exactly c = O(k 2 ??2 log ? ?1 ) columns (or c = O(k??2 log k log ? ?1 ) in expectation) of
A to construct C ? Rm?c ; in the second stage it chooses exactly r = O(c2 ??2 log ? ?1 ) rows (or
r = O(c??2 log c log ? ?1 ) in expectation) of A and C simultaneously to construct R and U. With
probability at least 1 ? ?, the relative-error ratio is 1 + ?. The computational cost is dominated by
the truncated SVD of A and C.
Though the algorithm is ?-optimal with high probability, it requires too many rows get chosen: at
least r = O(k??4 log2 k) rows in expectation. In this paper we seek to devise an algorithm with
mild requirement on column and row numbers.
3.3

Connection between Column Selection and CUR Matrix Decomposition

The CUR problem has a close connection with the column selection problem. As aforementioned,
the first stage of existing CUR algorithms is simply a column selection procedure. However, the
second stage is more complicated. If the second stage is na??vely solved by a column selection
algorithm on AT , then the error ratio will be at least (2 + ?).
For a relative-error CUR algorithm, the first stage seeks to bound a construction error ratio of
?A?CC? A?F
?A?CC? AR? R?F
given C. Actually, the first
?A?Ak ?F , while the section stage seeks to bound
?A?CC? A?F
stage is a special case of the second stage where C = Ak . Given a matrix A, if an algorithm solv?
AR? R?F
ing the second stage results in a bound ?A?CC
? ?, then this algorithm also solves the
?A?CC? A?F
T
column selection problem for A with an ? relative-error ratio. Thus the second stage of CUR is a
generalization of the column selection problem.

4

Main Results

In this section we introduce our proposed CUR algorithm. We call it the fast CUR algorithm because
it has lower time complexity compared with SVD. We describe it in Algorithm 1 and give a theoretical analysis in Theorem 5. Theorem 5 relies on Lemma 2 and Theorem 4, and Theorem 4 relies on
Theorem 3. Theorem 3 is a generalization of [6, Theorem 2.1], and Theorem 4 is a generalization
of [2, Theorem 5].
3

Algorithm 1 The Fast CUR Algorithm.

(
)
1: Input: a real matrix A ? Rm?n , target rank k, ? ? (0, 1], target column number c = 2k
1 + o(1) , target
?
(
)
row number r = 2c
1 + o(1) ;
?
2: // Stage 1: select c columns of A to construct C ? Rm?c
? k?
? kV
? k;
3: Compute approximate truncated SVD via random projection such that Ak ? U
? k?
? kV
? k ); V1 ? columns of V
? kT ;
4: Construct U1 ? columns of (A ? U
5: Compute s1 ? Dual Set Spectral-Frobenius Sparsification Algorithm (U1 , V1 , c ? 2k/?);
6: Construct C1 ? ADiag(s1 ), and then delete the all-zero columns;
7: Residual matrix D ? A ? C1 C?1 A;
8: Compute sampling probabilities: pi = ?di ?22 /?D?2F , i = 1, ? ? ? , n;
9: Sampling c2 = 2k/? columns from A with probability {p1 , ? ? ? , pn } to construct C2 ;
10: // Stage 2: select r rows of A to construct R ? Rr?n
? k?
? kV
? k )T ; V2 ? columns of U
? Tk ;
11: Construct U2 ? columns of (A ? U
12: Compute s2 ? Dual Set Spectral-Frobenius Sparsification Algorithm (U2 , V2 , r ? 2c/?);
13: Construct R1 ? Diag(s2 )A, and then delete the all-zero rows;
14: Residual matrix B ? A ? AR?1 R1 ; Compute qj = ?b(j) ?22 /?B?2F , j = 1, ? ? ? , m;
15: Sampling r2 = 2c/? rows from A with probability {q1 , ? ? ? , qm } to construct R2 ;
16: return C = [C1 , C2 ], R = [RT1 , RT2 ]T , and U = C? AR? .

4.1

Adaptive Sampling

The relative-error adaptive sampling algorithm is established in [6, Theorem 2.1]. The algorithm
is based on the following idea: after selecting a proportion of columns from A to form C1 by
an arbitrary algorithm, the algorithms randomly samples additional c2 columns according to the
residual A ? C1 C?1 A. Boutsidis et al. [2] used the adaptive sampling algorithm to decrease the
residual of the dual set sparsification algorithm and obtained an (1 + ?) relative-error bound. Here
we prove a new bound for the adaptive sampling algorithm. Interestingly, this new bound is a
generalization of the original one in [6, Theorem 2.1]. In other words, Theorem 2.1 of [6] is a direct
corollary of our following theorem in which C = Ak is set.
Theorem 3 (The Adaptive Sampling Algorithm). Given a matrix A ? Rm?n and a matrix C ?
Rm?c such that rank(C) = rank(CC? A) = ?, (? ? c ? n), we let R1 ? Rr1 ?n consist of r1
rows of A, and define the residual B = A ? AR?1 R1 . Additionally, for i = 1, ? ? ? , m, we define
pi = ?b(i) ?22 /?B?2F .
We further sample r2 rows i.i.d. from A, in each trial of which the i-th row is chosen with probability
pi . Let R2 ? Rr2 ?n contains the r2 sampled rows and let R = [RT1 , RT2 ]T ? R(r1 +r2 )?n . Then
the following inequality holds:
?
E?A ? CC? AR? R?2F ? ?A ? CC? A?2F + ?A ? AR?1 R1 ?2F ,
r2
where the expectation is taken w.r.t. R2 .
4.2

The Fast CUR Algorithm

Based on the dual set sparsification algorithm of of Lemma 1 and the adaptive sampling algorithm
of Theorem 3, we develop a randomized algorithm to solve the second stage of CUR problem. We
present the results of the algorithm in Theorem 4. Theorem 5 of [2] is a special case of the following
theorem where C = Ak .
Theorem 4 (The Fast Row Selection Algorithm). Given a matrix A ? Rm?n and a matrix C ?
Rm?c such that rank(C) = rank(CC? A) = ?, (? ? c ? n), and a target rank k (? ?), the
r?n
proposed randomized algorithm selects r = 2?
, such
? (1 + o(1)) rows of A to construct R ? R
that
E?A ? CC? AR? R?2F ? ?A ? CC? A?2F + ??A ? Ak ?2F ,
where the expectation is taken w.r.t. R. Furthermore, the matrix R can be computed in O((mnk +
mk 3 )??2/3 ) time.
Based on Lemma 2 and Theorem 4, here we present the main theorem for the fast CUR algorithm.
4

Table 1: A summary of the datasets.
Dataset
Type
size
Source
Redrocknatural image18000 ? 4000
http://www.agarwala.org/efficient gdc/
Arcene
biology
10000 ? 900 http://archive.ics.uci.edu/ml/datasets/Arcene
Dexter bag of words 20000 ? 2600http://archive.ics.uci.edu/ml/datasets/Dexter

Theorem 5 (The Fast CUR Algorithm). Given a matrix A ? Rm?n and a positive integer k ?
min{m, n}, the fast CUR algorithm (described in Algorithm 1) randomly selects c = 2k
? (1 + o(1))
columns of A to construct C ? Rm?c with the near-optimal column selection algorithm of Lemma 2,
r?n
and then selects r = 2c
with the fast row selection
? (1 + o(1)) rows of A to construct R ? R
algorithm of Theorem 4. Then we have
E?A ? CUR?F = E?A ? C(C? AR? )R?F ? (1 + ?)?A ? Ak ?F .
(
)
Moreover, the algorithm runs in time O mnk??2/3 + (m + n)k 3 ??2/3 + mk 2 ??2 + nk 2 ??4 .
Since k, c, r ? min{m, n} by the assumptions, so the time complexity of the fast CUR algorithm
is lower than that of the SVD of A. This is the main reason why we call it the fast CUR algorithm.
Another advantage of this algorithm is avoiding loading the whole m ? n data matrix A into main
memory. None of three steps ? the randomized SVD, the dual set sparsification algorithm, and the
adaptive sampling algorithm ? requires loading the whole of A into memory. The most memoryexpensive operation throughout the fast CUR Algorithm is computing the Moore-Penrose inverse
of C and R, which requires maintaining an m ? c matrix or an r ? n matrix in memory. In
comparison, the subspace sampling algorithm requires loading the whole matrix into memory to
compute its truncated SVD.

5

Empirical Comparisons

In this section we provide empirical comparisons among the relative-error CUR algorithms on several datasets. We report the relative-error ratio and the running time of each algorithm on each data
set. The relative-error ratio is defined by
?A ? CUR?F
Relative-error ratio =
,
?A ? Ak ?F
where k is a specified target rank.
We conduct experiments on three datasets, including natural image, biology data, and bags of words.
Table 1 briefly summarizes some information of the datasets. Redrock is a large size natural image.
Arcene and Dexter are both from the UCI datasets [11]. Arcene is a biology dataset with 900
instances and 10000 attributes. Dexter is a bag of words dataset with a 20000-vocabulary and 2600
documents. Each dataset is actually represented as a data matrix, upon which we apply the CUR
algorithms.
We implement all the algorithms in MATLAB 7.10.0. We conduct experiments on a workstation
with 12 Intel Xeon 3.47GHz CPUs, 12GB memory, and Ubuntu 10.04 system. According to the
analysis in [10] and this paper, k, c, and r should be integers far less than m and n. For each data
set and each algorithm, we set k = 10, 20, or 50, and c = ?k, r = ?c, where ? ranges in each set of
experiments. We repeat each set of experiments for 20 times and report the average and the standard
deviation of the error ratios. The results are depicted in Figures 1, 2, 3.
The results show that the fast CUR algorithm has much lower relative-error ratio than the subspace
sampling algorithm. The experimental results well match our theoretical analyses in Section 4. As
for the running time, the fast CUR algorithm is more efficient when c and r are small. When c and
r become large, the fast CUR algorithm becomes less efficient. This is because the time complexity
of the fast CUR algorithm is linear in ??4 and large c and r imply small ?. However, the purpose
of CUR is to select a small number of columns and rows from the data matrix, that is, c ? n and
r ? m. So we are not interested in the cases where c and r are large compared with n and m, say
k = 20 and ? = 10.
5

Running Time

Running Time

700

700

600

600

600

500

500

Subspace Sampling (Exactly)
Subspace Sampling (Expected)
Fast CUR

300

500
Subspace Sampling (Exactly)
Subspace Sampling (Expected)
Fast CUR

400

Time (s)

400

Time (s)

Time (s)

Running Time
700

300

400
300

200

200

200

100

100

100

0
2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36

0
2

?

4

Construction Error (Frobenius Norm)

8

0
2

10 12 14 16 18 20 22 24

?

Subspace Sampling (Exactly)
Subspace Sampling (Expected)
Fast CUR

1
0.9
0.8
0.7

1.2

1
0.9
0.8
0.7

0.6

?

?

12

14

16

18

1.1
1
0.9
0.8
0.7
0.6
0.5

0.6
2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36

10

Subspace Sampling (Exactly)
Subspace Sampling (Expected)
Fast CUR

1.2

1.1

0.5
2

8

Construction Error (Frobenius Norm)

Relative Error Ratio

1.1

6

1.3
Subspace Sampling (Exactly)
Subspace Sampling (Expected)
Fast CUR

1.3
Relative Error Ratio

1.2

4

Construction Error (Frobenius Norm)
1.4

1.3

Relative Error Ratio

6

Subspace Sampling (Exactly)
Subspace Sampling (Expected)
Fast CUR

4

6

8

0.4
2

10 12 14 16 18 20 22 24

?

4

6

8

10

?

12

14

16

18

(a) k = 10, c = ?k, and r = ?c. (b) k = 20, c = ?k, and r = ?c. (c) k = 50, c = ?k, and r = ?c.

Figure 1: Empirical results on the Redrock data set.

Running Time

Running Time

14

20

10

12

18
16

6

Time (s)

10

8
Time (s)

Time (s)

Running Time
12

8
6

4
4
Subspace Sampling (Exactly)
Subspace Sampling (Expected)
Fast CUR

2
0
2

4

6

0
2

8 10 12 14 16 18 20 22 24 26 28 30

?

4

Construction Error (Frobenius Norm)

8

10

?

12

14

16

18

10
Subspace Sampling (Exactly)
Subspace Sampling (Expected)
Fast CUR

6
4
2

20

Construction Error (Frobenius Norm)

4

6

8

?

10

12

14

Construction Error (Frobenius Norm)
1.3

Subspace Sampling (Exactly)
Subspace Sampling (Expected)
Fast CUR

1.3
Relative Error Ratio

1.3
1.2
1.1
1
0.9
0.8

Subspace Sampling (Exactly)
Subspace Sampling (Expected)
Fast CUR

1.2
1.1
1
0.9
0.8

0.7

0.7

0.6

0.6

Subspace Sampling (Exactly)
Subspace Sampling (Expected)
Fast CUR

1.2
Relative Error Ratio

1.4

Relative Error Ratio

6

12

8

Subspace Sampling (Exactly)
Subspace Sampling (Expected)
Fast CUR

2

14

1.1
1
0.9
0.8
0.7
0.6
0.5

2

4

6

8 10 12 14 16 18 20 22 24 26 28 30

?

2

4

6

8

10

?

12

14

16

18

20

0.4
2

4

6

8

?

10

12

14

(a) k = 10, c = ?k, and r = ?c. (b) k = 20, c = ?k, and r = ?c. (c) k = 50, c = ?k, and r = ?c.

Figure 2: Empirical results on the Arcene data set.

6

Running Time

Running Time

250

Running Time

300
Subspace Sampling (Exactly)
Subspace Sampling (Expected)
Fast CUR

200

400
Subspace Sampling (Exactly)
Subspace Sampling (Expected)
Fast CUR

250

350

Subspace Sampling (Exactly)
Subspace Sampling (Expected)
Fast CUR

100

Time (s)

150

Time (s)

Time (s)

300
200
150
100

250
200
150
100

50

50

0
2

4

6

0
2

8 10 12 14 16 18 20 22 24 26 28 30

?

50
4

8

0
2

10 12 14 16 18 20 22 24

?

1.1
1.05
1
0.95
0.9
2

4

6

8 10 12 14 16 18 20 22 24 26 28 30

?

1.15
1.1
1.05
1
0.95

?

16

18

1

0.9
0.85
0.8

10 12 14 16 18 20 22 24

14

0.95

0.75
8

12

1.1

0.9

6

?

1.05

0.85
2

4

10

Subspace Sampling (Exactly)
Subspace Sampling (Expected)
Fast CUR

1.15
Relative Error Ratio

Relative Error Ratio

1.15

8

1.2

Subspace Sampling (Exactly)
Subspace Sampling (Expected)
Fast CUR

1.2

6

Construction Error (Frobenius Norm)

1.25
Subspace Sampling (Exactly)
Subspace Sampling (Expected)
Fast CUR

1.2

4

Construction Error (Frobenius Norm)

Construction Error (Frobenius Norm)
1.25

Relative Error Ratio

6

2

4

6

8

10

?

12

14

16

18

(a) k = 10, c = ?k, and r = ?c. (b) k = 20, c = ?k, and r = ?c. (c) k = 50, c = ?k, and r = ?c.

Figure 3: Empirical results on the Dexter data set.

6

Conclusions

In this paper we have proposed a novel randomized algorithm for the CUR matrix decomposition
problem. This algorithm is faster, more scalable, and more accurate than the state-of-the-art algorithm, i.e., the subspace sampling algorithm. Our algorithm requires only c = 2k??1 (1 + o(1))
columns and r = 2c??1 (1 + o(1)) rows to achieve (1+?) relative-error ratio. To achieve the same
relative-error bound, the subspace sampling algorithm requires c = O(k??2 log k) columns and
r = O(c??2 log c) rows selected from the original matrix. Our algorithm also beats the subspace
sampling algorithm in time-complexity. Our algorithm costs O(mnk??2/3 + (m + n)k 3 ??2/3 +
mk 2 ??2 + nk 2 ??4 ) time, which is lower than O(min{mn2 , m2 n}) of the subspace sampling algorithm when k is small. Moreover, our algorithm enjoys another advantage of avoiding loading the
whole data matrix into main memory, which also makes our algorithm more scalable. Finally, the
empirical comparisons have also demonstrated the effectiveness and efficiency of our algorithm.

A The Dual Set Sparsification Algorithm
For the sake of completeness, we attach the dual set sparsification algorithm here and describe
some implementation details. The dual set sparsification algorithms are deterministic algorithms
established in [2]. The fast CUR algorithm calls the dual set spectral-Frobenius sparsification algorithm [2, Lemma 13] in both stages. We show this algorithm in Algorithm 2 and its bounds in
Lemma 6.
Lemma 6 (Dual Set Spectral-Frobenius Sparsification). Let U = {x1 , ? ? ? , xn } ? Rl , (l < n),
l?n
k
contains the columns of an arbitrary matrix
?n X ? RT . Let V = {v1 , ? ? ? , vn } ? R , (k < n),
be a decompositions of the identity, i.e.
v
v
=
I
.
Given
an
integer
r
with
k
< r < n,
k
i=1 i i
Algorithm 2 deterministically computes a set of weights si ? 0 (i = 1, ? ? ? , n) at most r of which
are non-zero, such that
? )
n
n
(?
(?
)
) (
k 2
T
and
tr
?k
si xi xTi ? ?X?2F .
si vi vi ? 1 ?
r
i=1
i=1
7

Algorithm 2 Deterministic Dual Set Spectral-Frobenius Sparsification Algorithm.

?n
l
n
k
T
1: Input: U = {xi }n
i=1 ? R , (l < n); V = {vi }i=1 ? R , with
i=1 vi vi = Ik (k < n); k < r < n;
2: Initialize: s0 = 0m?1 , A0 = 0k?k ;
?n
?x ?2
3: Compute ?xi ?22 for i = 1, ? ? ? , n, and then compute ?U = i=1? i 2 ;
1?

k/r

4: for ? = 0 to r ? 1 do
5:
Compute the eigenvalue decomposition of A? ;
6:
Find an index j in {1, ? ? ? , n} and compute a weight t > 0 such that
(
)?2
(
)?1
vjT A? ? (L? + 1)Ik
vj
?1
2
?1
?U ?xj ?2 ? t
?
vj ;
? vjT A? ? (L? + 1)Ik
?(L? + 1, A? ) ? ?(L? , A? )
where
?(L, A) =

k (
)?1
?
?i (A) ? L
,

L? = ? ?

?

rk;

i=1

7:
Update the j-th component of s? and A? :
8: end for
?
1? k/r
9: return s =
sr .
r

s? +1 [j] = s? [j] + t,

A? +1 = A? + tvj vjT ;

The weights si can be computed deterministically in O(rnk 2 + nl) time.
Here we would like to mention the implementation of Algorithm 2, which is not described in detailed
by [2]. In each iteration the algorithm performs once eigenvalue decomposition: A? = W?WT .
(A? is guaranteed to be positive semi-definite in each iteration). Since
(
)q
(
)
A? ? ?Ik = WDiag (?1 ? ?)q , ? ? ? , (?k ? ?)q WT ,
we can efficiently compute (A? ? (L? + 1)Ik )q based on the eigenvalue decomposition of A? . With
the eigenvalues at hand, ?(L, A? ) can also be computed directly.

Acknowledgments
This work has been supported in part by the Natural Science Foundations of China (No. 61070239),
the Google visiting faculty program, and the Scholarship Award for Excellent Doctoral Student
granted by Ministry of Education.

References
[1] Adi Ben-Israel and Thomas N.E. Greville. Generalized Inverses: Theory and Applications.
Second Edition. Springer, 2003.
[2] Christos Boutsidis, Petros Drineas, and Malik Magdon-Ismail. Near-optimal column-based
matrix reconstruction. CoRR, abs/1103.0995, 2011.
[3] Christos Boutsidis, Petros Drineas, and Malik Magdon-Ismail. Near optimal column-based
matrix reconstruction. In Proceedings of the 2011 IEEE 52nd Annual Symposium on Foundations of Computer Science, FOCS ?11, pages 305?314, 2011.
[4] Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard
Harshman. Indexing by latent semantic analysis. Journal of The American Society for Information Science, 41(6):391?407, 1990.
[5] Amit Deshpande and Luis Rademacher. Efficient volume sampling for row/column subset selection. In Proceedings of the 2010 IEEE 51st Annual Symposium on Foundations of Computer
Science, FOCS ?10, pages 329?338, 2010.
[6] Amit Deshpande, Luis Rademacher, Santosh Vempala, and Grant Wang. Matrix approximation
and projective clustering via volume sampling. Theory of Computing, 2(2006):225?247, 2006.
[7] Petros Drineas. Pass-efficient algorithms for approximating large matrices. In In Proceeding
of the 14th Annual ACM-SIAM Symposium on Dicrete Algorithms, pages 223?232, 2003.
8

[8] Petros Drineas, Ravi Kannan, and Michael W. Mahoney. Fast monte carlo algorithms for
matrices iii: Computing a compressed approximate matrix decomposition. SIAM Journal on
Computing, 36(1):184?206, 2006.
[9] Petros Drineas and Michael W. Mahoney. On the Nystr?om method for approximating a gram
matrix for improved kernel-based learning. Journal of Machine Learning Research, 6:2153?
2175, 2005.
[10] Petros Drineas, Michael W. Mahoney, and S. Muthukrishnan. Relative-error CUR matrix decompositions. SIAM Journal on Matrix Analysis and Applications, 30(2):844?881, September
2008.
[11] A. Frank and A. Asuncion. UCI machine learning repository, 2010.
[12] S. A. Goreinov, E. E. Tyrtyshnikov, and N. L. Zamarashkin. A theory of pseudoskeleton
approximations. Linear Algebra and Its Applications, 261:1?21, 1997.
[13] S. A. Goreinov, N. L. Zamarashkin, and E. E. Tyrtyshnikov. Pseudo-skeleton approximations
by matrices of maximal volume. Mathematical Notes, 62(4):619?623, 1997.
[14] Venkatesan Guruswami and Ali Kemal Sinop. Optimal column-based low-rank matrix reconstruction. In Proceedings of the Twenty-Third Annual ACM-SIAM Symposium on Discrete
Algorithms, SODA ?12, pages 1207?1214. SIAM, 2012.
[15] Nathan Halko, Per-Gunnar Martinsson, and Joel A. Tropp. Finding structure with randomness:
Probabilistic algorithms for constructing approximate matrix decompositions. SIAM Review,
53(2):217?288, 2011.
[16] John Hopcroft and Ravi Kannan. Computer Science Theory for the Information Age. 2012.
[17] Finny G. Kuruvilla, Peter J. Park, and Stuart L. Schreiber. Vector algebra in the analysis of
genome-wide expression data. Genome Biology, 3:research0011?research0011.1, 2002.
[18] Lester Mackey, Ameet Talwalkar, and Michael I. Jordan. Divide-and-conquer matrix factorization. In Advances in Neural Information Processing Systems 24. 2011.
[19] Michael W. Mahoney and Petros Drineas. CUR matrix decompositions for improved data
analysis. Proceedings of the National Academy of Sciences, 106(3):697?702, 2009.
[20] L. Sirovich and M. Kirby. Low-dimensional procedure for the characterization of human faces.
Journal of the Optical Society of America A, 4(3):519?524, Mar 1987.
[21] Matthew Turk and Alex Pentland. Eigenfaces for recognition. Journal of Cognitive Neuroscience, 3(1):71?86, 1991.
[22] Eugene E. Tyrtyshnikov. Incomplete cross approximation in the mosaic-skeleton method.
Computing, 64:367?380, 2000.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 1194-an-apobayesian-relative-of-winnow.pdf

An Apobayesian Relative of Winnow

Nick Littlestone
NEC Research Institute
4 Independence Way
Princeton, NJ 08540

Chris Mesterharm
NEC Research Institute
4 Independence Way
Princeton, NJ 08540

Abstract
We study a mistake-driven variant of an on-line Bayesian learning algorithm (similar to one studied by Cesa-Bianchi, Helmbold,
and Panizza [CHP96]). This variant only updates its state (learns)
on trials in which it makes a mistake. The algorithm makes binary
classifications using a linear-threshold classifier and runs in time linear in the number of attributes seen by the learner. We have been
able to show, theoretically and in simulations, that this algorithm
performs well under assumptions quite different from those embodied in the prior of the original Bayesian algorithm. It can handle
situations that we do not know how to handle in linear time with
Bayesian algorithms. We expect our techniques to be useful in
deriving and analyzing other apobayesian algorithms.

1

Introduction

We consider two styles of on-line learning. In both cases, learning proceeds in a
sequence of trials. In each trial, a learner observes an instance to be classified,
makes a prediction of its classification, and then observes a label that gives the
correct classification. One style of on-line learning that we consider is Bayesian.
The learner uses probabilistic assumptions about the world (embodied in a prior
over some model class) and data observed in past trials to construct a probabilistic
model (embodied in a posterior distribution over the model class). The learner uses
this model to make a prediction in the current trial. When the learner is told the
correct classification of the instance, the learner uses this information to update the
model, generating a new posterior to be used in the next trial.

In the other style of learning that we consider, the attention is on the correctness
of the predictions rather than on the model of the world. The internal state of the

An Apobayesian Relative o!Winnow

205

learner is only changed when the learner makes a mistake (when the prediction fails
to match the label). We call such an algorithm mistake-driven. (Such algorithms are
often called conservative in the computational learning theory literature.) There is a
simple way to derive a mistake-driven algorithm from anyon-line learning algorithm
(we restrict our attention in this paper to deterministic algorithms). The derived
algorithm is just like the original algorithm, except that before every trial, it makes
a record of its entire state, and after every trial in which its prediction is correct,
it resets its state to match the recorded state, entirely forgetting the intervening
trial. (Typically this is actually implemented not by making such a record, but by
merely omitting the step that updates the state.) For example, if some algorithm
keeps track of the number of trials it has seen, then the mistake-driven version of
this algorithm will end up keeping track of the number of mistakes it has made.
Whether the original or mistake-driven algorithm will do better depends on the task
and on how the algorithms are evaluated.
We will start with a Bayesian learning algorithm that we call SBSB and use this
procedure to derive a mistake-driven variant, SASB. Note that the variant cannot
be expected to be a Bayesian learning algorithm (at least in the ordinary sense)
since a Bayesian algorithm would make a prediction that minimizes the Bayes risk
based on all the available data, and the mistake-driven variant has forgotten quite
a bit. We call such algorithms apobayesian learning algorithms. This name is
intended to suggest that they are derived from Bayesian learning algorithms, but
are not themselves Bayesian. Our algorithm SASB is very close to an algorithm
of [CHP96). We study its application to different tasks than they do, analyzing its
performance when it is applied to linearly separable data as described below.
In this paper instances will be chosen from the instance space X = {a, l}n for some
n. Thus instances are composed of n boolean attributes. We consider only two
category classifications tasks, with predictions and labels chosen from Y = {a, I} .
We obtain a' bound on the number of mistakes SASB makes that is comparable to
bounds for various Winnow family algorithms given in [Lit88,Lit89). As for those
algorithms, the bound holds under the assumption that the points labeled 1 are
linearly separable from the points labeled 0, and the bound depends on the size 8 of
the gap between the two classes. (See Section 3 for a definition of 8.) The mistake
bound for SASB is 0 ( /or log ~ ). While this bound has an extra factor of log ~ not
present in the bounds for the Winnow algorithms, SASB has the advantage of not
needing any parameters. The Winnow family algorithms have parameters, and the
algorithms' mistake bounds depend on setting the parameters to values that depend
on 8. (Often, the value of 8 will not be known by the learner.) We expect the
techniques used to obtain this bound to be useful in analyzing other apobayesian
learning algorithms.
A number of authors have done related research regarding worst-case on-line
loss bounds including [Fre96,KW95,Vov90). Simulation experiments involving a
Bayesian algorithm and a mistake-driven variant are described in [Lit95). That
paper provides useful background for this paper. Note that our present analysis
techniques do not apply to the apobayesian algorithm studied there. The closest of
the original Winnow family algorithms to SASB appears to be the Weighted MaJority algorithm [LW94], which was analyzed for a case similar to that considered
in this paper in [Lit89). One should get a roughly correct impression of SASB if

N. Littlestone and C. Mesterharm

206

one thinks of it as a version of the Weighted Majority algorithm that learns its
parameters.
In the next section we describe the Bayesian algorithm that we start with. In
Section 3 we discuss its mistake-driven apobayesian variant. Section 4 mentions
some simulation experiments using these algorithms, and Section 5 is the conclusion.

2

A Bayesian Learning Algorithm

To describe the Bayesian learning algorithm we must specify a family of distributions over X x Y and a prior over this family of distributions. We parameterize
the distributions with parameters ((h, ... , 8n + l ) chosen from e = [0, 1]n+l. The
parameter 8n +1 gives the probability that the label is 1, and the parameter 8i gives
the probability that the ith attribute matches the label. Note that the probability
that the ith attribute is 1 given that the label is 1 equals the probability that the
ith attribute is 0 given that the label is O. We speak of this linkage between the
probabilities for the two classes as a symmetry condition. With this linkage, the
observation of a point from either class will affect the posterior distribution for both
classes. It is perhaps more typical to choose priors that allow the two classes to be
treated separately, so that the posterior for each class (giving the probability of elements of X conditioned on the label) depends only on the prior and on observations
from that class. The symmetry condition that we impose appears to be important
to the success of our analysis of the apobayesian variant of this algorithm. (Though
we impose this condition to derive the algorithm, it turns out that the apobayesian
variant can actually handle tasks where this condition is not satisfied.)
We choose a prior on e that gives probability 1 to the set of all elements
() = (81, ... , 8n +l ) E e for which at most one of 81 , ... ,8n does not equal
The prior is uniform on this set. Note that for any () in this set only a single attribute has a probability other than ~ of matching the label, and thus only a single
attribute is relevant. Concentrating on this set turns out to lead to an apobayesian
algorithm that can, in fact, handle more than one relevant attribute and that performs particularly well when only a small fraction of the attributes are relevant.

!.

This prior is related to to the familiar Naive Bayes model, which also assumes
that the attributes are conditionally independent given the labels. However, in the
typical Naive Bayes model there is no restriction to a single relevant attribute and
the symmetry condition linking the two classes is not imposed.

Our prior leads to the following algorithm. (The name SBSB stands for "Symmetric
Bayesian Algorithm with Singly-variant prior for Bernoulli distribution.")

Algorithm SBSB Algorithm SBSB maintains counts Si of the number of times
each attribute matches the label, a count M of the number of times the label is 1,
and a count t of the number of trials.
Initialization
Prediction

(M

+ 1)

f=

Si

M t-O

tt-O

Predict 1 given instance (Xl, ... ,xn ) if and only if
XiCSi+l)+Clixi)(t-Si+1)

i=l

Update

t- 0 for i = 1, ... ,n

M t- M

(S,)

> (t - M + 1)

f= (1-Xi)(Si+1~+XiCt-si+l)
(S.)

i=l

+ y, t t- t + 1, and for each i, if Xi

= Y then

Si

t-

Si

+1

An Apobayesian Relative of Winnow

3

207

An Apobayesian Algorithm

We construct an apobayesian algorithm by converting algorithm SBSB into a
mistake-driven algorithm using the standard conversion given in the introduction.
We call the resulting learning algorithm SASBj we have replaced "Bayesian" with
"Apobayesian" in the acronym.
In the previous section we made assumptions made about the generation of the
instances and labels that led to SBSB and thence to SASB. These assumptions
have served their purpose and we now abandon them. In analyzing the apobayesian
algorithm we do not assume that the instances and labels are generated by some
stochastic process. Instead we assume that the instance-label pairs in all of the
trials are linearly-separable, that is, that there exist some WI, ., . ,Wn , and c such
that for every instance-label pair (x, y) we have E~=I WiXi ;::: c when y = 1 and
2:~=1 WiXi ::; c when y = O. We actually make a somewhat stronger assumption,
given in the following theorem, which gives our bound for the apobayesian algorithm.
Theorem 1 Suppose that 'Yi ;::: 0 and "Ii ;::: 0 for i = 1, ... , n, and that 2:~=1 'Yi +
"I i = 1. Suppose that 0 ::; bo < bi ::; 1 and let 8 = bi - bo . Suppose that algorithm
SASB is run on a sequence of trials such that the instance x and label y in each
trial satisfy 2:~=1 'YiXi + "Ii (1 - Xi) ::; bo if y = 0 and 2:~=1 'YiXi + "Ii (1 - Xi) ;::: bi if
y = 1. Then the number of mistakes made by SASB will be bounded by
log

*

8; .

We have space to say only a little about how the derivation of this bound proceeds.
Details are given in [Lit96].
In analyzing SASB we work with an abstract description of the associated algorithm
SBSB. This algorithm starts with a prior on e as described above. We represent
this with a density Po. Then after each trial it calculates a new posterior density
Pt(O) = t-d8kP(X'YI~k, where Pt is the density after trial t and P(x, ylO) is the
pt-d )P(x,y

)

conditional probability ofthe instance x and label y observed in trial t given O. Thus
we can think of the algorithm as maintaining a current distribution on e that is
initially the prior. SASB is similar, but it leaves the current distribution unchanged
when a mistake is not made. For there to exist a finite mistake bound there must
exist some possible choice for the current distribution for which SASB would make
perfect predictions, should it ever arrive at that distribution. We call any such
distribution leading to perfect predictions a possible target distribution. It turns out
that the separability condition given in Theorem 1 guarantees that a suitable target
distribution exists. The analysis proceeds by showing that for an appropriate choice
of a target density p the relative entropy of the current distribution with respect to
the target distribution, p( 0) log(p( 0) / Pt (0)), decreases by at least some amount
R > 0 whenever a mistake is made. Since the relative entropy is never negative, the
number of mistakes is bounded by the initial relative entropy divided by R. This
form of analysis is very similar to the analysis of the various members of the Winnow
family in [Lit89,Lit91].

J

The same technique can be applied to other apobayesian algorithms. The abstract
update of Pt given above is quite general. The success of the analysis depends on
conditions on Po and P(x, ylO) that we do not have space here to discuss.

N. LittLestone and C. Mesterharm

208

p

=0.01 k =1 n =20

250r---~--~----~--~--~

250

p

=0.1 k =5 n =20

.-----.-----,-----,---~--~

,
' Optimal'
'SBSB'
'SASB'
'SASB + voting'

200

f/)

~

... .
-_.-

200

'Optimal'
'SBSB'
.
'SASB'
: 'SASB + voting"

/
/
/

/

.

/

150
/

/

S

.~
:e

/

/

/

,

'"

/

/

/

100

/'

.,,/
/'

/

50

o~--~--~----~--~--~

4000

..

./
r

/

:'/.",,~

./::: .. . .

2000

,/

/

/'" -.---::..' -.. '
/
/ . . . <- ...

o

/

/

/
,/
/'

50

/

/'

,/

/

,/

100

.. ..
//
-.,-. - / /

6000

8000

10000

Trials

/'

,/

/

O""'---...L-------I.-----'----~--~

o

2000

4000

6000

8000

10000

Trials

Figure 1: Comparison of SASB with SBSB

4

Simulation Experiments

The bound of the previous section was for perfectly linearly-separable data. We
have also done some simulation experiments exploring the performance of SASB on
non-separable data and comparing it with SBSB and with various other mistakedriven algorithms. A sample comparison of SASB with SBSB is shown in Figure
1. In each experimental run we generated 10000 trials with the instances and labels
chosen randomly according to a distribution specified by (h = '" = Ok = 1 - p,
Ok+l = ... = 0n+l = .5 where 01 , ??. ,On+l are interpreted as specified in Section
2, n is the number of attributes, and n, p, and k are as specified at the top of each
plot. The line labeled "optimal" shows the performance obtained by an optimal
predictor that knows the distribution used to generate the data ahead of time, and
thus does not need to do any learning. The lines labeled "SBSB" and "SASB" show
the performance of the corresponding learning algorithms. The lines labeled "SASB
+ voting" show the performance of SASB with the addition of a voting procedure
described in [Lit95]. This procedure improves the asymptotic mistake rate of the
algorithms. Each line on the graph is the average of 30 runs. Each line plots the
cumulative number of mistakes made by the algorithm from the beginning of the run
as a function of the number of trials.
In the left hand plot, there is only 1 relevant attribute. This is exactly the case that
SBSB is intended for, and it does better than SASB. In right hand plot, there are 5
relevant attributes; SBSB appears unable to take advantage of the extra information
present in the extra relevant attributes, but SASB successfully does.
Comparison of SASB and previous Winnow family algorithms is still in progress,
and we defer presenting details until a clearer picture has been obtained. SASB and
the Weighted Majority algorithm often perform similarly in simulations. Typically,
as one would expect, the Weighted Majority algorithm does somewhat better than

An Apobayesian Relative of Winnow

209

SASB when its parameters are chosen optimally for the particular learning task, and
worse for bad choices of parameters.

5

Conclusion

Our mistake bounds and simulations suggest that SASB may be a useful alternative
to the existing algorithms in the Winnow family. Based on the analysis style and the
bounds, SASB should perhaps itself be considered a Winnow family algorithm. Further experiments are in progress comparing SASB with Winnow family algorithms
run with a variety of parameter settings.
Perhaps of even greater interest is the potential application of our analytic techniques
to a variety of other apobayesian algorithms (though as we have observed earlier,
the techniques do not appear to apply to all such algorithms) . We have already
obtained some preliminary results regarding an interpretation of the Perceptron
algorithm as an apobayesian algorithm. We are interested in looking for entirely
new algorithms that can be derived in this way and also in better understanding
the scope of applicability of our techniques. All of the analyses that we have looked
at depend on symmetry conditions relating the probabilities for the two classes. It
would be of interest to see what can be said when such symmetry conditions do not
hold. In simulation experiments [Lit95], a mistake-driven variant of the standard
Naive Bayes algorithm often does very well, despite the absence of such symmetry
in the prior that it is based on.
Our simulation experiments and also the analysis of the related algorithm Winnow
[Lit91] suggest that SASB can be expected to handle some instance-label pairs inside
of the separating gap or on the wrong side, especially if they are not too far on the
wrong side. In particular it appears to be able to handle data generated according
to the distributions on which SBSB is based, which do not in general yield perfectly
separable data.
It is of interest to compare the capabilities of the original Bayesian algorithm with
the derived apobayesian algorithm. When the data is stochastically generated in a
manner consistent with the assumptions behind the original algorithm, the original
Bayesian algorithm can be expected to do better (see, for example, Figure 1). On
the other hand, the apobayesian algorithm can handle data beyond the capabilities of the original Bayesian algorithm. For example, in the case we consider, the
apobayesian algorithm can take advantage of the presence of more than one relevant
attribute, even though the prior behind the original Bayesian algorithm assumes a
single relevant attribute. Furthermore, as for all of the Winnow family algorithms,
the mistake bound for the apobayesian algorithm does not depend on details of the
behavior of the irrelevant attributes (including redundant attributes).
Instead of using the apobayesian variant, one might try to construct a Bayesian
learning algorithm for a prior that reflects the actual dependencies among the attributes and the labels. However, it may not be clear what the appropriate prior is.
It may be particularly unclear how to model the behavior of the irrelevant attributes. Furthermore, such a Bayesian algorithm may end up being computationally
expensive. For example, attempting to keep track of correlations among all pairs
of attributes may lead to an algorithm that needs time and space quadratic in the
number of attributes. On the other hand, if we start with a Bayesian algorithm that

210

N. Littlestone and C. Mesterharm

uses time and space linear in the number of attributes we can obtain an apobayesian
algorithm that still uses linear time and space but that can handle situations beyond
the capabilities of the original Bayesian algorithm.
Acknowledgments

This paper has benefited from discussions with Adam Grove.

References
[CHP96] Nicolo Cesa-Bianchi, David P. Helmbold, and Sandra Panizza. On bayes
methods for on-line boolean prediction. In Proceedings of the Ninth Annual
Conference on Computational Learning Theory, pages 314-324, 1996.
[Fre96] Yoav Freund. Predicting a binary sequence almost as well as the optimal
biased coin. In Proceedings of the Ninth Annual Conference on Computational Learning Theory, pages 89-98, 1996.
[KW95] J. Kivinen and M. K. Warmuth. Additive versus exponentiated gradient
updates for linear prediction. In Proc. 27th ACM Symp. on Theory of
Computing, pages 209-218, 1995.
[Lit88] N. Littlestone. Learning quickly when irrelevant attributes abound: A new
linear-threshold algorithm. Machine Learning, 2:285-318, 1988.
[Lit89] N. Littlestone. Mistake Bounds and Logarithmic Linear-threshold Learning
Algorithms. PhD thesis, Tech. Rept. UCSC-CRL-89-11, Univ. of Calif.,
Santa Cruz, 1989.
[Lit91] N. Littlestone. Redundant noisy attributes, attribute errors, and linearthreshold learning using Winnow. In Proc. 4th Annu. Workshop on Comput. Learning Theory, pages 147- 156. Morgan Kaufmann, San Mateo, CA,
1991.
[Lit95] N. Littlestone. Comparing several linear-threshold learning algorithms on
tasks involving superfluous attributes. In Proceedings of the XII International conference on Machine Learning, pages 353- 361, 1995.
[Lit96] N. Littlestone. Mistake-driven bayes sports: Bounds for symmetric
apobayesian learning algorithms. Technical report, NEC Research Institute, Princeton, NJ, 1996.
[LW94] N. Littlestone and M. K. Warmuth. The weighted majority algorithm.
Information and Computation, 108:212-261, 1994.
[Vov90] Volodimir G. Vovk. Aggregating strategies. In Proceedings of the 1990
Workshop on Computational Learning Theory, pages 371-383, 1990.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 5074-low-rank-matrix-reconstruction-and-clustering-via-approximate-message-passing.pdf

Low-rank matrix reconstruction and clustering via
approximate message passing

Ryosuke Matsushita
NTT DATA Mathematical Systems Inc.
1F Shinanomachi Rengakan, 35,
Shinanomachi, Shinjuku-ku, Tokyo,
160-0016, Japan
matsur8@gmail.com

Toshiyuki Tanaka
Department of Systems Science,
Graduate School of Informatics, Kyoto University
Yoshida Hon-machi, Sakyo-ku, Kyoto-shi,
606-8501 Japan
tt@i.kyoto-u.ac.jp

Abstract
We study the problem of reconstructing low-rank matrices from their noisy observations. We formulate the problem in the Bayesian framework, which allows
us to exploit structural properties of matrices in addition to low-rankedness, such
as sparsity. We propose an efficient approximate message passing algorithm, derived from the belief propagation algorithm, to perform the Bayesian inference for
matrix reconstruction. We have also successfully applied the proposed algorithm
to a clustering problem, by reformulating it as a low-rank matrix reconstruction
problem with an additional structural property. Numerical experiments show that
the proposed algorithm outperforms Lloyd?s K-means algorithm.

1

Introduction

Low-rankedness of matrices has frequently been exploited when one reconstructs a matrix from its
noisy observations. In such problems, there are often demands to incorporate additional structural
properties of matrices in addition to the low-rankedness. In this paper, we consider the case where
a matrix A0 ? Rm?N to be reconstructed is factored as A0 = U0 V0? , U0 ? Rm?r , V0 ? RN ?r
(r ? m, N ), and where one knows structural properties of the factors U0 and V0 a priori. Sparseness
and non-negativity of the factors are popular examples of such structural properties [1, 2].
Since the properties of the factors to be exploited vary according to the problem, it is desirable
that a reconstruction method has enough flexibility to incorporate a wide variety of properties. The
Bayesian approach achieves such flexibility by allowing us to select prior distributions of U0 and V0
reflecting a priori knowledge on the structural properties. The Bayesian approach, however, often
involves computationally expensive processes such as high-dimensional integrations, thereby requiring approximate inference methods in practical implementations. Monte Carlo sampling methods
and variational Bayes methods have been proposed for low-rank matrix reconstruction to meet this
requirement [3?5].
We present in this paper an approximate message passing (AMP) based algorithm for Bayesian lowrank matrix reconstruction. Developed in the context of compressed sensing, the AMP algorithm reconstructs sparse vectors from their linear measurements with low computational cost, and achieves
a certain theoretical limit [6]. AMP algorithms can also be used for approximating Bayesian inference with a large class of prior distributions of signal vectors and noise distributions [7]. These
successes of AMP algorithms motivate the use of the same idea for low-rank matrix reconstruction.
The IterFac algorithm for the rank-one case [8] has been derived as an AMP algorithm. An AMP
algorithm for the general-rank case is proposed in [9], which, however, can only treat estimation of
posterior means. We extend their algorithm so that one can deal with other estimations such as the
maximum a posteriori (MAP) estimation. It is the first contribution of this paper.
1

As the second contribution, we apply the derived AMP algorithm to K-means type clustering to
obtain a novel efficient clustering algorithm. It is based on the observation that our formulation
of the low-rank matrix reconstruction problem includes the clustering problem as a special case.
Although the idea of applying low-rank matrix reconstruction to clustering is not new [10, 11], our
proposed algorithm is, to our knowledge, the first that directly deals with the constraint that each
datum should be assigned to exactly one cluster in the framework of low-rank matrix reconstruction.
We present results of numerical experiments, which show that the proposed algorithm outperforms
Lloyd?s K-means algorithm [12] when data are high-dimensional.
Recently, AMP algorithms for dictionary learning and blind calibration [13] and for matrix reconstruction with a generalized observation model [14] were proposed. Although our work has some
similarities to these studies, it differs in that we fix the rank r rather than the ratio r/m when taking
the limit m, N ? ? in the derivation of the algorithm. Another difference is that our formulation,
explained in the next section, does not assume statistical independence among the components of
each row of U0 and V0 . A detailed comparison among these algorithms remains to be made.

2
2.1

Problem setting
Low-rank matrix reconstruction

We consider the following problem setting. A matrix A0 ? Rm?N to be estimated is defined
by two matrices U0 := (u0,1 , . . . , u0,m )? ? Rm?r and V0 := (v0,1 , . . . , v0,N )? ? RN ?r as
A0 := U0 V0? , where u0,i , v0,j ? Rr . We consider the case where r ? m, N . Observations of A0
are corrupted by additive noise W ? Rm?N , whose components Wi,j are i.i.d. Gaussian random
variables following N (0, m? ). Here ? > 0 is a noise variance parameter and N (a, ? 2 ) denotes the
Gaussian distribution with mean a and variance ? 2 . The factor m in the noise variance is introduced
to allow a proper scaling in the limit where m and N go to infinity in the same order, which is
employed in deriving the algorithm. An observed matrix A ? Rm?N is given by A := A0 + W .
Reconstructing A0 and (U0 , V0 ) from A is the problem considered in this paper.
We take the Bayesian approach to address this problem, in which one requires prior distributions
of variables to be estimated, as well as conditional distributions relating observations with variables
to be estimated. These distributions need not be the true ones because in some cases they are not
available so that one has to assume them arbitrarily, and in some other cases one expects advantages
by assuming them in some specific manner in view of computational efficiencies. In this paper, we
suppose that one uses the true conditional distribution
(
)
1
1
p(A|U0 , V0 ) =
?A ? U0 V0? ?2F ,
(1)
mN exp ?
2m?
(2?m? ) 2
where ? ? ?F denotes the Frobenius norm. Meanwhile, we suppose that the assumed prior distributions of U0 and V0 , denoted by p?U and p?V , respectively, may be different from the true distributions
?
pU and pV , respectively.
We restrict p?U and p?V to distributions of the form p?U (U0 ) = i p?u (u0,i )
?
and p?V (V0 ) = j p?v (v0,j ), respectively, which allows us to construct computationally efficient
algorithms. When U ? p?U (U ) and V ? p?V (V ), the posterior distribution of (U, V ) given A is
(
)
1
?A ? U V ? ?2F p?U (U )?
pV (V ).
(2)
p?(U, V |A) ? exp ?
2m?
Prior probability density functions (p.d.f.s) p?u and p?v can be improper, that is, they can integrate to
infinity, as long as the posterior p.d.f. (2) is proper. We also consider cases where the assumed rank
r? may be different from the true rank r. We thus suppose that estimates U and V are of size m ? r?
and N ? r?, respectively.
We consider two problems appearing in the Bayesian approach. The first problem, which we call
the marginalization problem, is to calculate the marginal posterior distributions given A,
?
?
?
p?i,j (ui , vj |A) := p?(U, V |A)
duk
dvl .
(3)
k?=i
?

l?=j

These are used to calculate
the posterior mean E[U V |A] and the
?
? marginal MAP estimates
MMAP
uMMAP
:=
arg
max
p
?
(u,
v|A)dv
and
v
:=
arg
max
p?i,j (u, v|A)du. Because
u
i,j
v
i
j
2

calculation of p?i,j (ui , vj |A) typically involves high-dimensional integrations requiring high computational cost, approximation methods are needed.
The second problem, which we call the MAP problem, is to calculate the MAP estimate
arg maxU,V p?(U, V |A). It is formulated as the following optimization problem:
min C MAP (U, V ),
U,V

(4)

where C MAP (U, V ) is the negative logarithm of (2):
C MAP (U, V ) :=

m
N
?
?
1
?A ? U V ? ?2F ?
log p?u (ui ) ?
log p?v (vj ).
2m?
i=1
j=1

(5)

Because ?A ? U V ? ?2F is a non-convex function of (U, V ), it is generally hard to find the global
optimal solutions of (4) and therefore approximation methods are needed in this problem as well.
2.2

Clustering as low-rank matrix reconstruction

A clustering problem can be formulated as a problem of low-rank matrix reconstruction [11]. Suppose that v0,j ? {e1 , . . . , er }, j = 1, . . . , N , where el ? {0, 1}r is the vector whose lth component
is 1 and the others are 0. When V0 and U0 are fixed, aj follows one of the r Gaussian distributions
? 0,l , m? I), l = 1, . . . , r, where u
? 0,l is the lth column of U0 . We regard that each Gaussian
N (u
? 0,l being the center of cluster l and v0,j representing the cluster
distribution defines a cluster, u
assignment of the datum aj . One can then perform clustering on the dataset {a1 , . . . , aN } by reconstructing U0 and V0 from A = (a1 , . . . , aN ) under the structural constraint that every row of V0
should belong to {e1 , . . . , er?}, where r? is an assumed number of clusters.
Let us consider maximum likelihood estimation arg maxU,V p(A|U, V ), or equivalently, MAP esti?r?
mation with the (improper) uniform prior distributions p?u (u) = 1 and p?v (v) = r??1 l=1 ?(v?el ).
The corresponding MAP problem is
min

r ,V ?{0,1}N ??
r
U ?Rm??

?A ? U V ? ?2F

subject to vj ? {e1 , . . . , er?}.

(6)

?N ?r?
When V satisfies the constraints, the objective function ?A ? U V ? ?2F =
j=1
l=1 ?aj ?
? l ?22 I(vj = el ) is the sum of squared distances, each of which is between a datum and the center of
u
the cluster that the datum is assigned to. The optimization problem (6), its objective function, and
clustering based on it are called in this paper the K-means problem, the K-means loss function, and
the K-means clustering, respectively.
One can also use the marginal MAP estimation for clustering. If U0 and V0 follow p?U and p?V , respectively, the marginal MAP estimation is optimal in the sense that it maximizes the expectation of
accuracy with respect to p?(V0 |A). Here, accuracy is defined as the fraction of correctly assigned data
among all data. We call the clustering using approximate marginal MAP estimation the maximum
accuracy clustering, even when incorrect prior distributions are used.

3

Previous work

Existing methods for approximately solving the marginalization problem and the MAP problem
are divided into stochastic methods such as Markov-Chain Monte-Carlo methods and deterministic
ones. A popular deterministic method is to use the variational Bayesian formalism. The variational
Bayes matrix factorization [4, 5] approximates the posterior distribution p(U, V |A) as the product
VB
of two functions pVB
U (U ) and pV (V ), which are determined so that the Kullback-Leibler (KL)
VB
VB
divergence from pU (U )pV (V ) to p(U, V |A) is minimized. Global minimization of the KL divergence is difficult except for some special cases [15], so that an iterative method to obtain a local
minimum is usually adopted. Applying the variational Bayes matrix factorization to the MAP problem, one obtains the iterated conditional modes (ICM) algorithm, which alternates minimization of
C MAP (U, V ) over U for fixed V and minimization over V for fixed U .
The representative algorithm to solve the K-means problem approximately is Lloyd?s K-means algorithm [12]. Lloyd?s K-means algorithm is regarded as the ICM algorithm: It alternates minimization
of the K-means loss function over U for fixed V and minimization over V for fixed U iteratively.
3

Algorithm 1 (Lloyd?s K-means algorithm).
ntl =

N
?

I(vjt = el ),

? tl =
u

j=1

ljt+1 = arg

min

l?{1,...,?
r}

? tl ?22 ,
?aj ? u

N
1 ?
aj I(vjt = el ),
ntl j=1

(7a)

vjt+1 = elt+1 .

(7b)

j

Throughout this paper, we represent an algorithm by a set of equations as in the above. This representation means that the algorithm begins with a set of initial values and repeats the update of the
variables using the equations presented until it satisfies some stopping criteria. Lloyd?s K-means
algorithm begins with a set of initial assignments V 0 ? {e1 , . . . , er?}N . This algorithm easily gets
stuck in local minima and its performance heavily depends on the initial values of the algorithm.
Some methods for initialization to obtain a better local minimum are proposed [16].
Maximum accuracy clustering can be solved approximately by using the variational Bayes matrix
factorization, since it gives an approximation to the marginal posterior distribution of vj given A.

4
4.1

Proposed algorithm
Approximate message passing algorithm for low-rank matrix reconstruction

We first discuss the general idea of the AMP algorithm and advantages of the AMP algorithm compared with the variational Bayes matrix factorization. The AMP algorithm is derived by approximating the belief propagation message passing algorithm in a way thought to be asymptotically exact for
large-scale problems with appropriate randomness. Fixed points of the belief propagation message
passing algorithm correspond to local minima of the KL divergence between a kind of trial function
and the posterior distribution [17]. Therefore, the belief propagation message passing algorithm can
be regarded as an iterative algorithm based on an approximation of the posterior distribution, which
is called the Bethe approximation. The Bethe approximation can reflect dependence of random variables (dependence between U and V in p?(U, V |A) in our problem) to some extent. Therefore, one
can intuitively expect that performance of the AMP algorithm is better than that of the variational
Bayes matrix factorization, which treats U and V as if they were independent in p?(U, V |A).
An important property of the AMP algorithm, aside from its efficiency and effectiveness, is that
one can predict performance of the algorithm accurately for large-scale problems by using a set of
equations, called the state evolution [6]. Analysis with the state evolution also shows that required
iteration numbers are O(1) even when the problem size is large. Although we can present the state
evolution for the algorithm proposed in this paper and give a proof of its validity like [8, 18], we do
not discuss the state evolution here due to the limited space available.
We introduce a one-parameter extension of the posterior distribution p?(U, V |A) to treat the marginalization problem and the MAP problem in a unified manner. It is defined as follows:
(
)(
)?
?
p?(U, V |A; ?) ? exp ?
?A ? U V ? ?2F p?U (U )?
pV (V ) ,
2m?

(8)

which is proportional to p?(U, V |A)? , where ? > 0 is the parameter. When ? = 1, p?(U, V |A; ?)
is reduced to p?(U, V |A). In the limit ? ? ?, the distribution p?(U, V |A; ?) concentrates on the
maxima of p?(U, V |A). An algorithm for the marginalization problem on p?(U, V |A; ?) is particularized to the algorithms for the marginalization problem and for the MAP problem for the original
posterior distribution p?(U, V |A) by letting ? = 1 and ? ? ?, respectively. The AMP algorithm
for the marginalization problem on p?(U, V |A; ?) is derived in a way similar to that described in [9],
as detailed in the Supplementary Material.
In the derived algorithm, the values of variables But = (btu,1 , . . . , btu,m )? ? Rm??r , Bvt =
(btv,1 , . . . , btv,N )? ? RN ??r , ?tu ? Rr???r , ?tv ? Rr???r , U t = (ut1 , . . . , utm )? ? Rm??r ,
t ?
t
V t = (v1t , . . . , vN
) ? RN ??r , S1t , . . . , Sm
? Rr???r , and T1t , . . . , TNt ? Rr???r are calculated iteratively, where the superscript t ? N ? {0} represents iteration numbers. Variables with a negative
iteration number are defined as 0. The algorithm is as follows:
4

Algorithm 2.
But =

N
1
1 t?1 ? t
AV t ?
U
Tj ,
m?
m?
j=1

?tu =

N
N
1 ? t
1
1 ? t
(V t )? V t +
Tj ?
T , (9a)
m?
?m? j=1
m? j=1 j

uti = f (btu,i , ?tu ; p?u ),

Sit = G(btu,i , ?tu ; p?u ),
m
m
m
1
1 ? t
1 t? t
1 ? t
1 ? t
Si , ?tv =
A U ?
V
(U t )? U t +
Si ?
S ,
Bvt =
m?
m?
m?
?m? i=1
m? i=1 i
i=1
vjt+1 = f (btv,j , ?tv ; p?v ),

Tjt+1 = G(btv,j , ?tv ; p?v ).

(9b)
(9c)
(9d)

Algorithm 2 is almost symmetric in U and V . Equations (9a)?(9b) and (9c)?(9d) update quantities
related to the estimates of U0 and V0 , respectively. The algorithm requires an initial value V 0 and
begins with Tj0 = O. The functions f (?, ?; p?) : Rr??Rr???r ? Rr? and G(?, ?; p?) : Rr??Rr???r ? Rr???r ,
which have a p.d.f. p? : Rr? ? R as a parameter, are defined by
?
?f (b, ?; p?)
f (b, ?; p?) := u?
q (u; b, ?, p?)du,
G(b, ?; p?) :=
,
(10)
?b
where q?(u; b, ?, p?) is the normalized p.d.f. of u defined by
( (1
))
q?(u; b, ?, p?) ? exp ?? u? ?u ? b? u ? log p?(u) .
2

(11)

One can see that f (b, ?; p?) is the mean of the distribution q?(u; b, ?, p?) and that G(b, ?; p?) is its
covariance matrix scaled by ?. The function f (b, ?; p?) need not be differentiable everywhere;
Algorithm 2 works if f (b, ?; p?) is differentiable at b for which one needs to calculate G(b, ?; p?) in
running the algorithm.
We assume in the rest of this section the convergence of Algorithm 2, although the convergence is
?
?
?
?
?
not guaranteed in general. Let Bu? , Bv? , ??
be the converged values
u , ?v , Si , Tj , U , and V
of the respective variables. First, consider running Algorithm 2 with ? = 1. The marginal posterior
distribution is then approximated as
?
?
?v ).
?u )?
q (vj ; b?
p?i,j (ui , vj |A) ? q?(ui ; b?
v,j , ?v , p
u,i , ?u , p

(12)

?
?
?
Since u?
of q?(u; b?
?u ) and q?(v; b?
?v ), respectively, the
i and vj are the means
u,i , ?u , p
v,j , ?v , p
?
?
?
posterior mean E[U V |A] = U V p?(U, V |A)dU dV is approximated as

E[U V ? |A] ? U ? (V ? )? .

(13)

and vjMMAP are approximated as
The marginal MAP estimates uMMAP
i
?
?v ).
vjMMAP ? arg max q?(v; b?
v,j , ?v , p

?
?u ),
uMMAP
? arg max q?(u; b?
u,i , ?u , p
i

v

u

(14)

Taking the limit ? ? ? in Algorithm 2 yields an algorithm for the MAP problem (4). In this case,
the functions f and G are replaced with
[1
]
?f? (b, ?; p?)
f? (b, ?; p?) := arg min u? ?u ? b? u ? log p?(u) , G? (b, ?; p?) :=
. (15)
u
2
?b
One may calculate G? (b, ?; p?) from the Hessian of log p?(u) at u = f? (b, ?; p?), denoted by H,
(
)?1
via the identity G? (b, ?; p?) = ??H
. This identity follows from the implicit function theorem
under some additional assumptions and helps in the case where the explicit form of f? (b, ?; p?) is
not available. The MAP estimate is approximated by (U ? , V ? ).
4.2

Properties of the algorithm

Algorithm 2 has several plausible properties. First, it has a low computational cost. The computational cost per iteration is O(mN ), which is linear in the number of components of the matrix
A. Calculation of f (?, ?; p?) and G(?, ?; p?) is performed O(N + m) times per iteration. The constant
5

factor depends on p? and ?. Calculation of f for ? < ? generally involves an r?-dimensional numerical integration, although they are not needed in cases where an analytic expression of the integral
is available and cases where the variables take only discrete values. Calculation of f? involves
minimization over an r?-dimensional vector. When ? log p? is a convex function and ? is positive
semidefinite, this minimization problem is convex and can be solved at relatively low cost.
Second, Algorithm 2 has a form similar to that of an algorithm based on the variational Bayesian
matrix factorization. In fact, if the last terms on the right-hand sides of the four equations in (9a)
and (9c) are removed, the resulting algorithm is the same as an algorithm based on the variational
Bayesian matrix factorization proposed in [4] and, in particular, the same as the ICM algorithm when
? ? ?. (Note, however, that [4] only treats the case where the priors p?u and p?v are multivariate
Gaussian distributions.) Note that additional computational cost for these extra terms is O(m + N ),
which is insignificant compared with the cost of the whole algorithm, which is O(mN ).
Third, when one deals with the MAP problem, the value of C MAP (U, V ) may increase in iterations of Algorithm 2. The following proposition, however, guarantees optimality of the output of
Algorithm 2 in a certain sense, if it has converged.
?
Proposition 1. Let (U ? , V ? , S1? , . . . , Sm
, T ? , . . . , TN? ) be a fixed point of the AMP algorithm
?m 1 ?
?N
for the MAP problem and suppose that i=1 Si and j=1 Tj? are positive semidefinite. Then
U ? is a global minimum of C MAP (U, V ? ) and V ? is a global minimum of C MAP (U ? , V ).
The proof is in the Supplementary Material. The key to the proof is the following reformulation:
N
)]
[
(
)
( 1 ?
MAP
t
t?1
Tjt (U ? U t?1 )?
U = arg min C
(U, V ) ? tr (U ? U )
U
2m? j=1
t

(16)

?N
If j=1 Tjt is positive semidefinite, the second term of the minimand is the negative squared pseudometric between U and U t?1 , which is interpreted as a penalty on nearness to the temporal estimate.
?m
?N
Positive semidefiniteness of i=1 Sit and j=1 Tjt holds in almost all cases. In fact, we only have
to assume lim??? G(b, ?; p?) = G? (b, ?; p?), since G(b, ?; p?) is a scaled covariance matrix of
q?(u; b, ?, p?), which is positive semidefinite. It follows from Proposition 1 that any fixed point of the
AMP algorithm is also a fixed point of the ICM algorithm. It has two implications: (i) Execution
of the ICM algorithm initialized with the converged values of the AMP algorithm does not improve
C MAP (U t , V t ). (ii) The AMP algorithm has not more fixed points than the ICM algorithm. The
second implication may help the AMP algorithm avoid getting stuck in bad local minima.
4.3

Clustering via AMP algorithm

One can use the AMP algorithm for the MAP problem to perform the K-means clustering by letting
?r?
p?u (u) = 1 and p?v (v) = r??1 l=1 ?(v ? el ). Noting that f? (b, ?; p?v ) is piecewise constant with
respect to b and hence G? (b, ?; p?v ) is O almost everywhere, we obtain the following algorithm:
Algorithm 3 (AMP algorithm for the K-means clustering).
1
1
AV t , ?tu =
(V t )? V t , U t = But (?tu )?1 ,
m?
m?
1 ? t 1 t t
1
1
Bvt =
A U ? V S , ?tv =
(U t )? U t ? S t ,
m?
?
m?
?
[1
]
v ? ?tv v ? v ? btv,j .
vjt+1 = arg
min
v?{e1 ,...,er? } 2
But =

S t = (?tu )?1 ,

(17a)
(17b)
(17c)

It is initialized with an assignment V 0 ? {e1 , . . . , er?}N . Algorithm 3 is rewritten as follows:
ntl =

N
?
j=1

ljt+1 = arg

I(vjt = el ),

? tl =
u

N
1 ?
aj I(vjt = el ),
ntl j=1

[ 1
2m
m]
? tl ?22 + t I(vjt = el ) ? t ,
?aj ? u
nl
nl
l?{1,...,?
r } m?
min

6

(18a)
vjt+1 = elt+1 .
j

(18b)

The parameter ? appearing in
algorithm does not exist in the?
K-means clustering problem. In
?the
m
m
fact, ? appears because m?2 i=1 A2ij Sit was estimated by ? m?1 i=1 Sit in deriving Algorithm 2,
which can be justified for large-sized problems. In practice, we propose using m?2 N ?1 ?A ?
U t (V t )? ?2F as a temporary estimate of ? at tth iteration. While the AMP algorithm for the Kmeans clustering updates the value of U in the same way as Lloyd?s K-means algorithm, it performs
assignments of data to clusters in a different way. In the AMP algorithm, in addition to distances
from data to centers of clusters, the assignment at present is taken into consideration in two ways:
(i) A datum is less likely to be assigned to the cluster that it is assigned to at present. (ii) Data are
more likely to be assigned to a cluster whose size at present is smaller. The former can intuitively be
understood by observing that if vjt = el , one should take account of the fact that the cluster center
? tl is biased toward aj . The term 2m(ntl )?1 I(vjt = el ) in (18b) corrects this bias, which, as it
u
should be, is inversely proportional to the cluster size.
The AMP algorithm for maximum accuracy clustering is obtained by letting ? = 1 and p?v (v) be
a discrete distribution on {e1 , . . . , er?}. After the algorithm converges, arg maxv q?(v; vj? , ??
?v )
v ,p
gives the final cluster assignment of the jth datum and U ? gives the estimate of the cluster centers.

5

Numerical experiments

We conducted numerical experiments on both artificial and real data sets to evaluate performance
of the proposed algorithms for clustering. In the experiment on artificial data sets, we set m = 800
? 0,l , l = 1, . . . , r, were generated according to the
and N = 1600 and let r? = r. Cluster centers u
multivariate Gaussian distribution N (0, I). Cluster assignments v0,j , j = 1, . . . , N, were generated
according to the uniform distribution on {e1 , . . . , er }. For fixed ? = 0.1 and r, we generated 500
problem instances and solved them with five algorithms: Lloyd?s K-means algorithm (K-means),
the AMP algorithm for the K-means clustering (AMP-KM), the variational Bayes matrix factorization [4] for maximum accuracy clustering (VBMF-MA), the AMP algorithm for maximum accuracy
clustering (AMP-MA), and the K-means++ [16]. The K-means++ updates the variables in the same
way as Lloyd?s K-means algorithm with an initial value chosen in a sophisticated manner. For the
other algorithms, initial values vj0 , j = 1, . . . , N, were randomly generated from the same distribution as v0,j . We used the true prior distributions of U and V for maximum accuracy clustering.
We ran Lloyd?s K-means algorithm and the K-means++ until no change was observed. We ran the
AMP algorithm for the K-means clustering until either V t = V t?1 or V t = V t?2 is satisfied.
This is because we observed oscillations of assignments of a small number of data. For the other
two algorithms, we terminated the iteration when ?U t ? U t?1 ?2F < 10?15 ?U t?1 ?2F and ?V t ?
V t?1 ?2F < 10?15 ?V t?1 ?2F were met or the number of iterations exceeded 3000. We then evaluated
the following performance measures for the obtained solution (U ? , V ? ):
?N
?
? := N1 N
? ?22 ), where a
? Normalized K-means loss ?A?U ? (V ? )? ?2F /( j=1 ?aj ? a
j=1 aj .
?
N
? Accuracy maxP N ?1 j=1 I(P vj? = v0,j ), where the maximization is taken over all
r-by-r permutation matrices. We used the Hungarian algorithm [19] to solve this maximization problem efficiently.
? Number of iterations needed to converge.
We calculated the averages and the standard deviations of these performance measures over 500
instances. We conducted the above experiments for various values of r.
Figure 1 shows the results. The AMP algorithm for the K-means clustering achieves the smallest Kmeans loss among the five algorithms, while the Lloyd?s K-means algorithm and K-means++ show
large K-means losses for r ? 5. We emphasize that all the three algorithms are aimed to minimize
the same K-means loss and the differences lie in the algorithms for minimization. The AMP algorithm for maximum accuracy clustering achieves the highest accuracy among the five algorithms. It
also shows fast convergence. In particular, the convergence speed of the AMP algorithm for maximum accuracy clustering is comparable to that of the AMP algorithm for the K-means clustering
when the two algorithms show similar accuracy (r < 9). This is in contrast to the common observation that the variational Bayes method often shows slower convergence than the ICM algorithm.
7

1

1
K-means
AMP-KM
VBMF-MA
AMP-MA
K-means++

K-means
AMP-KM
VBMF-MA
AMP-MA
K-means++

0.8

0.99

Accuracy

Normalized K-means loss

0.995

0.985
0.98

0.6

0.4

0.2

0.975
0.97

2

4

6

8

10

r

12

14

16

0

18

2

4

6

8

(a)

r

12

14

16

18

(b)

2500

1
K-means
AMP-KM
VBMF-MA
AMP-MA
K-means++

2000

Number of iterations

10

0.8

Accuracy

1500

1000

500

0.6

0.4
AMP-KM
VBMF-MA
AMP-MA

0.2

0

2

4

6

8

10

r

12

14

16

0

18

0

10

20

30

Iteration number

(c)

40

50

(d)

Figure 1: (a)?(c) Performance for different r: (a) Normalized K-means loss. (b) Accuracy. (c)
Number of iterations needed to converge. (d) Dynamics for r = 5. Average accuracy at each
iteration is shown. Error bars represent standard deviations.

0.45

0.75

K-means++
AMP-KM

K-means++
AMP-KM
0.7

0.44

Accuracy

Normalized K-means loss

0.46

0.43
0.42

0.65

0.6

0.41
0.55
0.4
0.39

0

10

20

30

Number of trials

40

0.5

50

(a)

0

10

20

30

Number of trials

40

50

(b)

Figure 2: Performance measures in real-data experiments. (a) Normalized K-means loss. (b) Accuracy. The results for the 50 trials are shown in the descending order of performance for AMP-KM.
The worst two results for AMP-KM are out of the range.
In the experiment on real data, we used the ORL Database of Faces [20], which contains 400 images
of human faces, ten different images of each of 40 distinct subjects. Each image consists of 112 ?
92 = 10304 pixels whose value ranges from 0 to 255. We divided N = 400 images into r? = 40
clusters with the K-means++ and the AMP algorithm for the K-means clustering. We adopted the
initialization method of the K-means++ also for the AMP algorithm, because random initialization
often yielded empty clusters and almost all data were assigned to only one cluster. The parameter ?
was estimated in the way proposed in Subsection 4.3. We ran 50 trials with different initial values,
and Figure 2 summarizes the results.
The AMP algorithm for the K-means clustering outperformed the standard K-means++ algorithm
in 48 out of the 50 trials in terms of the K-means loss and in 47 trials in terms of the accuracy.
The AMP algorithm yielded just one cluster with all data assigned to it in two trials. The attained
minimum value of K-means loss is 0.412 with the K-means++ and 0.400 with the AMP algorithm.
The accuracies at these trials are 0.635 with the K-means++ and 0.690 with the AMP algorithm. The
average number of iterations was 6.6 with the K-means++ and 8.8 with the AMP algorithm. These
results demonstrate efficiency of the proposed algorithm on real data.
8

References
[1] P. Paatero, ?Least squares formulation of robust non-negative factor analysis,? Chemometrics and Intelligent Laboratory Systems, vol. 37, no. 1, pp. 23?35, May 1997.
[2] P. O. Hoyer, ?Non-negative matrix factorization with sparseness constraints,? The Journal of Machine
Learning Research, vol. 5, pp. 1457?1469, Dec. 2004.
[3] R. Salakhutdinov and A. Mnih, ?Bayesian probabilistic matrix factorization using Markov chain Monte
Carlo,? in Proceedings of the 25th International Conference on Machine Learning, New York, NY, Jul. 5?
Aug. 9, 2008, pp. 880?887.
[4] Y. J. Lim and Y. W. Teh, ?Variational Bayesian approach to movie rating prediction,? in Proceedings of
KDD Cup and Workshop, San Jose, CA, Aug. 12, 2007.
[5] T. Raiko, A. Ilin, and J. Karhunen, ?Principal component analysis for large scale problems with lots
of missing values,? in Machine Learning: ECML 2007, ser. Lecture Notes in Computer Science, J. N.
Kok, J. Koronacki, R. L. de Mantaras, S. Matwin, D. Mladeni?c, and A. Skowron, Eds. Springer Berlin
Heidelberg, 2007, vol. 4701, pp. 691?698.
[6] D. L. Donoho, A. Maleki, and A. Montanari, ?Message-passing algorithms for compressed sensing,?
Proceedings of the National Academy of Sciences USA, vol. 106, no. 45, pp. 18 914?18 919, Nov. 2009.
[7] S. Rangan, ?Generalized approximate message passing for estimation with random linear mixing,? in Proceedings of 2011 IEEE International Symposium on Information Theory, St. Petersburg, Russia, Jul. 31?
Aug. 5, 2011, pp. 2168?2172.
[8] S. Rangan and A. K. Fletcher, ?Iterative estimation of constrained rank-one matrices in noise,? in Proceedings of 2012 IEEE International Symposium on Information Theory, Cambridge, MA, Jul. 1?6, 2012,
pp. 1246?1250.
[9] R. Matsushita and T. Tanaka, ?Approximate message passing algorithm for low-rank matrix reconstruction,? in Proceedings of the 35th Symposium on Information Theory and its Applications, Oita, Japan,
Dec. 11?14, 2012, pp. 314?319.
[10] W. Xu, X. Liu, and Y. Gong, ?Document clustering based on non-negative matrix factorization,? in Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, Toronto, Canada, Jul. 28?Aug. 1, 2003, pp. 267?273.
[11] C. Ding, T. Li, and M. Jordan, ?Convex and semi-nonnegative matrix factorizations,? IEEE Transactions
on Pattern Analysis and Machine Intelligence, vol. 32, no. 1, pp. 45?55, Jan. 2010.
[12] S. P. Lloyd, ?Least squares quantization in PCM,? IEEE Transactions on Information Theory, vol. IT-28,
no. 2, pp. 129?137, Mar. 1982.
[13] F. Krzakala, M. M?ezard, and L. Zdeborov?a, ?Phase diagram and approximate message passing for blind
calibration and dictionary learning,? preprint, Jan. 2013, arXiv:1301.5898v1 [cs.IT].
[14] J. T. Parker, P. Schniter, and V. Cevher, ?Bilinear generalized approximate message passing,? preprint,
Oct. 2013, arXiv:1310.2632v1 [cs.IT].
[15] S. Nakajima and M. Sugiyama, ?Theoretical analysis of Bayesian matrix factorization,? Journal of Machine Learning Research, vol. 12, pp. 2583?2648, Sep. 2011.
[16] D. Arthur and S. Vassilvitskii, ?k-means++: the advantages of careful seeding,? in SODA ?07 Proceedings
of the 18th Annual ACM-SIAM Symposium on Discrete Algorithms, New Orleans, Louisiana, Jan. 7?9,
2007, pp. 1027?1035.
[17] J. S. Yedidia, W. T. Freeman, and Y. Weiss, ?Constructing free-energy approximations and generalized
belief propagation algorithms,? IEEE Transactions on Information Theory, vol. 51, no. 7, pp. 2282?2312,
Jul. 2005.
[18] M. Bayati and A. Montanari, ?The dynamics of message passing on dense graphs, with applications to
compressed sensing,? IEEE Transactions on Information Theory, vol. 57, no. 2, pp. 764?785, Feb. 2011.
[19] H. W. Kuhn, ?The Hungarian method for the assignment problem,? Naval Research Logistics Quarterly,
vol. 2, no. 1?2, pp. 83?97, Mar. 1955.
[20] F. S. Samaria and A. C. Harter, ?Parameterisation of a stochastic model for human face identification,? in
Proceedings of 2nd IEEE Workshop on Applications of Computer Vision, Sarasota FL, Dec. 1994, pp.
138?142. [Online]. Available: http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 3812-streaming-k-means-approximation.pdf

Streaming k-means approximation
Nir Ailon
Google Research
nailon@google.com

Ragesh Jaiswal?
Columbia University
rjaiswal@gmail.com

Claire Monteleoni?
Columbia University
cmontel@ccls.columbia.edu

Abstract
We provide a clustering algorithm that approximately optimizes the k-means objective, in the one-pass streaming setting. We make no assumptions about the
data, and our algorithm is very light-weight in terms of memory, and computation. This setting is applicable to unsupervised learning on massive data sets, or
resource-constrained devices. The two main ingredients of our theoretical work
are: a derivation of an extremely simple pseudo-approximation batch algorithm
for k-means (based on the recent k-means++), in which the algorithm is allowed
to output more than k centers, and a streaming clustering algorithm in which batch
clustering algorithms are performed on small inputs (fitting in memory) and combined in a hierarchical manner. Empirical evaluations on real and simulated data
reveal the practical utility of our method.

1

Introduction

As commercial, social, and scientific data sources continue to grow at an unprecedented rate, it is
increasingly important that algorithms to process and analyze this data operate in online, or one-pass
streaming settings. The goal is to design light-weight algorithms that make only one pass over the
data. Clustering techniques are widely used in machine learning applications, as a way to summarize
large quantities of high-dimensional data, by partitioning them into ?clusters? that are useful for
the specific application. The problem with many heuristics designed to implement some notion of
clustering is that their outputs can be hard to evaluate. Approximation guarantees, with respect to
some reasonable objective, are therefore useful. The k-means objective is a simple, intuitive, and
widely-cited clustering objective for data in Euclidean space. However, although many clustering
algorithms have been designed with the k-means objective in mind, very few have approximation
guarantees with respect to this objective.
In this work, we give a one-pass streaming algorithm for the k-means problem. We are not aware
of previous approximation guarantees with respect to the k-means objective that have been shown
for simple clustering algorithms that operate in either online or streaming settings. We extend work
of Arthur and Vassilvitskii [AV07] to provide a bi-criterion approximation algorithm for k-means,
in the batch setting. They define a seeding procedure which chooses a subset of k points from a
batch of points, and they show that this subset gives an expected O(log (k))-approximation to the kmeans objective. This seeding procedure is followed by Lloyd?s algorithm1 which works very well
in practice with the seeding. The combined algorithm is called k-means++, and is an O(log (k))approximation algorithm, in expectation.2 We modify k-means++ to obtain a new algorithm, kmeans#, which chooses a subset of O(k log (k)) points, and we show that the chosen subset of
?

Department of Computer Science. Research supported by DARPA award HR0011-08-1-0069.
Center for Computational Learning Systems
1
Lloyd?s algorithm is popularly known as the k-means algorithm
2
Since the approximation guarantee is proven based on the seeding procedure alone, for the purposes of this
exposition we denote the seeding procedure as k-means++.
?

1

points gives a constant approximation to the k-means objective. Apart from giving us a bi-criterion
approximation algorithm, our modified seeding procedure is very simple to analyze.
[GMMM+03] defines a divide-and-conquer strategy to combine multiple bi-criterion approximation
algorithms for the k-medoid problem to yield a one-pass streaming approximation algorithm for
k-median. We extend their analysis to the k-means problem and then use k-means++ and k-means#
in the divide-and-conquer strategy, yielding an extremely efficient single pass streaming algorithm
with an O(c? log (k))-approximation guarantee, where ? ? log n/ log M , n is the number of input
points in the stream and M is the amount of work memory available to the algorithm. Empirical
evaluations, on simulated and real data, demonstrate the practical utility of our techniques.
1.1

Related work

There is much literature on both clustering algorithms [Gon85, Ind99, VW02, GMMM+03,
KMNP+04, ORSS06, AV07, CR08, BBG09, AL09], and streaming algorithms [Ind99, GMMM+03,
M05, McG07].3 There has also been work on combining these settings: designing clustering algorithms that operate in the streaming setting [Ind99, GMMM+03, CCP03]. Our work is inspired by
that of Arthur and Vassilvitskii [AV07], and Guha et al. [GMMM+03], which we mentioned above
and will discuss in further detail. k-means++, the seeding procedure in [AV07], had previously been
analyzed by [ORSS06], under special assumptions on the input data.
In order to be useful in machine learning applications, we are concerned with designing algorithms
that are extremely light-weight and practical. k-means++ is efficient, very simple, and performs
well in practice. There do exist constant approximations to the k-means objective, in the nonstreaming setting, such as a local search technique due to [KMNP+04].4 A number of works
[LV92, CG99, Ind99, CMTS02, AGKM+04] give constant approximation algorithms for the related k-median problem in which the objective is to minimize the sum of distances of the points to
their nearest centers (rather than the square of the distances as in k-means), and the centers must be
a subset of the input points. It is popularly believed that most of these algorithms can be extended to
work for the k-means problem without too much degredation of the approximation, however there
is no formal evidence for this yet. Moreover, the running times of most of these algorithms depend
worse than linearly on the parameters (n, k, and d) which makes these algorithms less useful in practice. As future work, we propose analyzing variants of these algorithms in our streaming clustering
algorithm, with the goal of yielding a streaming clustering algorithm with a constant approximation
to the k-means objective.
Finally, it is important to make a distinction from some lines of clustering research which involve
assumptions on the data to be clustered. Common assumptions include i.i.d. data, e.g. [BL08], and
data that admits a clustering with well separated means e.g. in [VW02, ORSS06, CR08]. Recent
work [BBG09] assumes a ?target? clustering for the specific application and data set, that is close
to any constant approximation of the clustering objective. In contrast, we prove approximation
guarantees with respect to the optimal k-means clustering, with no assumptions on the input data.5
As in [AV07], our probabilistic guarantees are only with respect to randomness in the algorithm.
1.1.1

Preliminaries

The k-means clustering problem is defined as follows: Given n points X ? Rd and a weight
d
function w : X ? R
!, the goal is to find a2subset C ? R , |C| = k such that the following quantity is
6
minimized: ?C = x?X w(x)?D(x, C) , where D(x, C) denotes the #2 distance of x to the nearest
point in C. When the subset C is clear from the context, we denote this distance by D(x). Also,
for two points x, y, D(x, y) denotes the #2 distance between x and y. The subset C is alternatively
called a clustering of X and ?C is called the potential function corresponding to the clustering. We
will use the term ?center? to refer to any c ? C.
3

For a comprehensive survey of streaming results and literature, refer to [M05].
In recent, independent work, Aggarwal, Deshpande, and Kannan [ADK09] extend the seeding procedure of
k-means++ to obtain a constant factor approximation algorithm which outputs O(k) centers. They use similar
techniques to ours, but reduce the number of centers by using a stronger concentration property.
5
It may be interesting future work to analyze our algorithm in special cases, such as well-separated clusters.
6
For the unweighted case, we can assume that w(x) = 1 for all x.
4

2

Definition 1.1 (Competitive ratio, b-approximation). Given an algorithm B for the k-means problems, let ?C be the potential of the clustering C returned by B (on some input set which is implicit)
and let ?COP T denote the potential of the optimal clustering COP T . Then the competitive ratio is
defined to be the worst case ratio ?C?C . The algorithm B is said to be b-approximation algorithm

if

?C
?COP T

OP T

? b.

The previous definition might be too strong for an approximation algorithm for some purposes. For
example, the clustering algorithm performs poorly when it is constrained to output k centers but it
might become competitive when it is allowed to output more centers.
Definition 1.2 ((a, b)-approximation). We call an algorithm B, (a, b)-approximation for the kmeans problem if it outputs a clustering C with ak centers with potential ?C such that ?C?C ? b in
OP T
the worst case. Where a > 1, b > 1.
Note that for simplicity, we measure the memory in terms of the words which essentially means that
we assume a point in Rd can be stored in O(1) space.

2

k-means#: The advantages of careful and liberal seeding

The k-means++ algorithm is an expected ?(log k)-approximation algorithm. In this section, we
extend the ideas in [AV07] to get an (O(log k), O(1))-approximation algorithm. Here is the kmeans++ algorithm:
1. Choose an initial center c1 uniformly at random from X .
2. Repeat (k ? 1) times:
! 2
)
3.
Choose the next center ci , selecting ci = x# ? X with probability P D(xD(x)
2.
x?X
(here D(.) denotes the distances w.r.t. to the subset of points chosen in the previous rounds)
Algorithm 1: k-means++
In the original definition of k-means++ in [AV07], the above algorithm is followed by Lloyd?s
algorithm. The above algorithm is used as a seeding step for Lloyd?s algorithm which is known
to give the best results in practice. On the other hand, the theoretical guarantee of the k-means++
comes from analyzing this seeding step and not Lloyd?s algorithm. So, for our analysis we focus on
this seeding step. The running time of the algorithm is O(nkd).
In the above algorithm X denotes the set of given points and for any point x, D(x) denotes the
distance of this point from the nearest center among the centers chosen in the previous rounds. To
get an (O(log k), O(1))-approximation algorithm, we make a simple change to the above algorithm.
We first set up the tools for analysis. These are the basic lemmas from [AV07]. We will need the
following definition first:
Definition 2.1 (Potential w.r.t. a set). Given a clustering
C, its potential with respect to some set A
!
is denoted by ?C (A) and is defined as ?C (A) = x?A D(x)2 , where D(x) is the distance of the
point x from the nearest point in C.
Lemma 2.2 ([AV07], Lemma 3.1). Let A be an arbitrary cluster in COP T , and let C be the clustering
with just one center, chosen uniformly at random from A. Then Exp[?C (A)] = 2 ? ?COP T (A).

Corollary 2.3. Let A be an arbitrary cluster in COP T , and let C be the clustering with just one
center, which is chosen uniformly at random from A. Then, Pr[?C (A) < 8?COP T (A)] ? 3/4
Proof. The proof follows from Markov?s inequality.

Lemma 2.4 ([AV07], Lemma 3.2). Let A be an arbitrary cluster in COP T , and let C be an arbitrary
clustering. If we add a random center to C from A, chosen with D2 weighting to get C # , then
Exp[?C ! (A)] ? 8 ? ?COP T (A).

Corollary 2.5. Let A be an arbitrary cluster in COP T , and let C be an arbitrary clustering. If
we add a random center to C from A, chosen with D2 weighting to get C # , then Pr[?C ! (A) <
32 ? ?COP T (A)] ? 3/4.
3

We will use k-means++ and the above two lemmas to obtain a (O(log k), O(1))-approximation
algorithm for the k-means problem. Consider the following algorithm:
1. Choose 3 ? log k centers independently and uniformly at random from X .
2. Repeat (k ? 1) times.
! 2
)
3.
Choose 3 ? log k centers independently and with probability P D(xD(x)
2.
x?X
(here D(.) denotes the distances w.r.t. to the subset of points chosen in the previous rounds)
Algorithm 2: k-means#
Note that the algorithm is almost the same as the k-means++ algorithm except that in each round
of choosing centers, we pick O(log k) centers rather than a single center. The running time of the
above algorithm is clearly O(ndk log k).
Let A = {A1 , ..., Ak } denote the set of clusters in the optimal clustering COP T . Let C i denote the
clustering after ith round of choosing centers. Let Aic denote the subset of clusters ? A such that
?A ? Aic , ?C i (A) ? 32 ? ?COP T (A).

We call this subset of clusters, the ?covered? clusters. Let Aiu = A\Aic be the subset of ?uncovered?
clusters. The following simple lemma shows that with constant probability step (1) of k-means#
picks a center such that at least one of the clusters gets covered, or in other words, |A1c | ? 1. Let us
call this event E.
Lemma 2.6. Pr[E] ? (1 ? 1/k).
Proof. The proof easily follows from Corollary 2.3.
Let Xci = ?A?Aic A and let Xui = X \ Xci . Now after the ith round, either ?C i (Xci ) ? ?C i (Xui )
or otherwise. In the former case, using Corollary 2.5, we show that the probability of covering an
uncovered cluster in the (i + 1)th round is large. In the latter case, we will show that the current set
of centers is already competitive with constant approximation ratio. Let us start with the latter case.
Lemma 2.7. If event E occurs ( |A1c | ? 1) and for any i > 1, ?C i (Xci ) > ?C i (Xui ), then ?C i ?
64?COP T .
Proof. We get the main result using the following sequence of inequalities: ?C i = ?C i (Xci ) +
?C i (Xui ) ? ?C i (Xci ) + ?C i (Xci ) ? 2 ? 32 ? ?COP T (Xci ) ? 64 ?COP T (using the definition of Xci ).
i
Lemma 2.8. If for any i ? 1, ?C i (Xci ) ? ?C i (Xui ), then Pr[|Ai+1
c | ? |Ac | + 1] ? (1 ? 1/k).

Proof. Note that in the (i + 1)th round, the probability that a center is chosen from a cluster ?
/ Aic is
?

(X i )

i
u
at least ? i (X Ci )+?
? 1/2. Conditioned on this event, with probability at least 3/4 any of the
i
u
C
C i (Xc )
centers x chosen in round (i + 1) satisfies ?C i ?x (A) ? 32 ? ?COP T (A) for some uncovered cluster
A ? Aiu . This means that with probability at least 3/8 any of the chosen centers x in round (i + 1)
satisfies ?C i ?x (A) ? 32 ? ?COP T (A) for some uncovered cluster A ? Aiu . This further implies that
with probability at least (1 ? 1/k) at least one of the chosen centers x in round (i + 1) satisfies
?C i ?x (A) ? 32 ? ?COP T (A) for some uncovered cluster A ? Aiu .

We use the above two lemmas to prove our main theorem.
Theorem 2.9. k-means# is a (O(log k), O(1))-approximation algorithm.
Proof. From Lemma 2.6 we know that event E (i.e., |Aic | ? 1) occurs. Given this, suppose for
any i > 1, after the ith round ?C i (Xc ) > ?C i (Xu ). Then from Lemma 2.7 we have ?C ? ?C i ?
64?COP T . If no such i exist, then from Lemma 2.8 we get that the probability that there exists a
cluster A ? A such that A is not covered even after k rounds(i.e., end of the algorithm) is at most:
1 ? (1 ? 1/k)k ? 3/4. So with probability at least 1/4, the algorithm covers all the clusters in A.
In this case from Lemma 2.8, we have ?C = ?C k ? 32 ? ?COP T .
We have shown that k-means# is a randomized algorithm for clustering which with probability at
least 1/4 gives a clustering with competitive ratio 64.
4

3

A single pass streaming algorithm for k-means

In this section, we will provide a single pass streaming algorithm. The basic ingredients for the algorithm is a divide and conquer strategy defined by [GMMM+03] which uses bi-criterion approximation algorithms in the batch setting. We will use k-means++ which is a (1, O(log k))-approximation
algorithm and k-means# which is a (O(log k), O(1))-approximation algorithm, to construct a single
pass streaming O(log k)-approximation algorithm for k-means problem. In the next subsection, we
develop some of the tools needed for the above.
3.1 A streaming (a,b)-approximation for k-means
We will show that a simple streaming divide-and-conquer scheme, analyzed by [GMMM+03] with
respect to the k-medoid objective, can be used to approximate the k-means objective. First we
present the scheme due to [GMMM+03], where in this case we use k-means-approximating algorithms as input.
Inputs: (a) Point set S ? Rd . Let n = |S|.
(b) Number of desired clusters, k ? N .
(c) A, an (a, b)-approximation algorithm to the k-means objective.
(d) A# , an (a# , b# )-approximation algorithm to the k-means objective.
1.
2.
3.
4.
5.
6.
7.

Divide S into groups S1 , S2 , . . . , S#
For each i ? {1, 2, . . . , #}
Run A on Si to get ? ak centers Ti = {ti1 , ti2 , . . .}
Denote the induced clusters of Si as Si1 ? Si2 ? ? ? ?
Sw ? T1 ? T2 ? ? ? ? ? T# , with weights w(tij ) ? |Sij |
Run A# on Sw to get ? a# k centers T
Return T
Algorithm 3: [GMMM+03] Streaming divide-and-conquer clustering

?
?
First note that when every batch Si has size nk, this algorithm takes one pass, and O(a nk)
memory. Now we will give an approximation guarantee.
Theorem 3.1. The algorithm above outputs a clustering that is an (a# , 2b + 4b# (b + 1))approximation to the k-means objective.
The a# approximation of the desired number of centers follows directly from the approximation
property of A# , with respect to the number of centers, since A# is the last algorithm to be run. It
remains to show the approximation of the k-means objective. The proof, which appears in the
Appendix, involves extending the analysis of [GMMM+03], to the case of the k-means objective.
Using the exposition in Dasgupta?s lecture notes [Das08], of the proof due to [GMMM+03], our
extension is straightforward, and differs in the following ways from the k-medoid analysis.
1. The k-means objective involves squared distance (as opposed to k-medoid in which the
distance is not squared), so the triangle inequality cannot be invoked directly. We replace it
with an application of the triangle inequality, followed by (a+b)2 ? 2a2 +2b2 , everywhere
it occurs, introducing several factors of 2.
2. Cluster centers are chosen from Rd , for the k-means problem, so in various parts of the
proof we save an approximation a factor of 2 from the k-medoid problem, in which cluster
centers must be chosen from the input data.
3.2 Using k-means++ and k-means# in the divide-and-conquer strategy
In the previous subsection, we saw how a (a, b)-approximation algorithm A and an (a# , b# )approximation algorithm A# can be used to get a single pass (a# , 2b + 4b# (b + 1))-approximation
streaming algorithm. We now have two randomized algorithms, k-means# which with probability
at least 1/4 is a (3 log k, 64)-approximation algorithm and k-means++ which is a (1, O(log k))approximation algorithm (the approximation factor being in expectation). We can now use these
two algorithms in the divide-and-conquer strategy to obtain a single pass streaming algorithm.
We use the following as algorithms as A and A# in the divide-and-conquer strategy (3):
5

A: ?Run k-means# on the data 3 log n times independently, and pick the clustering
with the smallest cost.?
A?: ?Run k-means++?
Weighted versus non-weighted. Note that k-means and k-means# are approximation algorithms
for the non-weighted case (i.e. w(x) = 1 for all points x). On the other hand, in the divide-andconquer strategy we need the algorithm A# , to work for the weighted case where the weights are
integers. Note that both k-means and k-means# can be easily generalized for the weighted case
when the weights are integers. Both algorithms compute probabilities based on the cost with respect
to the current clustering. This cost can be computed by taking into account the weights. For the
analysis, we can assume points with multiplicities equal to the integer weight of the point. The
memory required remains logarithmic in the input size, including the storing the weights.
"
#
"
#
Analysis. With probability at least 1 ? (3/4)3 log n ? 1 ? n1 , algorithm A is a (3 log k, 64)approximation algorithm. Moreover, the space requirement remains logarithmic ?
in the input size. In
step (3) of $
Algorithm 3 we run A on batches of data. Since each batch is of size nk the number of
batches is n/k, the probability
that A is a (3 log k, 64)-approximation algorithm for all of these
"
#?n/k
1
batches is at least 1 ? n
? 1/2. Conditioned on this event, the divide-and-conquer strategy
?
gives a O(log k)-approximation algorithm. The memory required is O(log(k) ? nk) times the
logarithm of the input size. Moreover, the algorithm has running time O(dnk log n log k).
3.3 Improved memory-approximation tradeoffs
We saw in the last section how to obtain a single-pass (a# , cbb# )-approximation for k-means using
first an (a, b)-approximation on input blocks and then an (a# , b# )-approximation on the union of the
output center
? sets, where c is some global constant. The optimal memory required for this scheme
was O(a nk). This immediately implies a tradeoff between the memory requirements (growing
like a), the number of centers outputted (which is a# k) and the approximation to the potential (which
is cbb# ) with respect to the optimal solution using k centers. A more subtle tradeoff is possible by a
recursive application of the technique in multiple levels. Indeed, the (a, b)-approximation could be
broken up in turn into two levels, and so on. This idea was used in [GMMM+03]. Here we make a
more precise account of the tradeoff between the different parameters.
Assume we have subroutines for performing (ai , bi )-approximation for k-means in batch mode, for
i = 1, . . . r (we will choose a1 , . . . , ar , b1 , . . . , br later). We will hold r buffers B1 , . . . , Br as
work areas, where the size of buffer Bi is Mi . In the topmost level, we will divide the input into
equal blocks of size M1 , and run our (a1 , b1 )-approximation algorithm on each block. Buffer B1
will be repeatedly reused for this task, and after each application of the approximation algorithm,
the outputted set of (at most) ka1 centers will be added to B2 . When B2 is filled, we will run
the (a2 , b2 )-approximation algorithm on the data and add the ka2 outputted centers to B3 . This
will continue until buffer Br fills, and the (ar , br )-approximation algorithm outputs the final ar k
centers. Let ti denote the number of times the i?th level algorithm is executed. Clearly we have
ti kai = Mi+1 ti+1 for i = 1, . . . , r ? 1. For the last stage we have tr = 1, which means that tr?1 =
Mr /kar?1 , tr?2 = Mr?1 Mr /k 2 ar?2 ar?1 and generally ti = Mi+1 ? ? ? Mr /k r?i ai ? ? ? ar?1 .7 But
M1 ???Mr
we must also have t1 = n/M1 , implying n = kr?1
a1 ???ar?1 . In order to minimize the total memory
!
Mi under the last constraint, using standard arguments in multivariate analysis we must have
"
#1/r
M1 = ? ? ? = Mr , or in other words Mi = nk r?1 a1 ? ? ? ar?1
? n1/r k(a1 ? ? ? ar?1 )1/r for all i.
The resulting one-pass algorithm will have an approximation guarantee of (ar , cr?1 b1 ? ? ? br ) (using
a straightforward extension of the result in the previous section) and memory requirement of at most
rn1/r k(a1 ? ? ? ar?1 )1/r .
Assume now that we are in the realistic setting in which the available memory is of fixed size
M ? k. We will choose r (below), and for each i = 1..r ? 1 we choose to either run k-means++
or the repeated k-means# (algorithm A in the previous subsection), i.e., (ai , bi ) = (1, O(log k))
or (3 log k, O(1)) for each i. For i = r, we choose k-means++, i.e., (ar , br ) = (1, O(log k)) (we
are interested in outputting exactly k centers as the final solution). Let q denote the number of

7
We assume all quotients are integers for simplicity of the proof, but note that fractional blocks would arise
in practice.

6

25

8

40
35

4
3

Cost in units of 107

6

5

Batch Lloyds
Divide and Conquer with km# and km++
Divide and Conquer with km++

45

20

Cost in units of 10

Cost in units of 109

6

50

Batch Lloyds
Divide and Conquer with km# and km++
Divide and Conquer with km++

Batch Lloyds
Online Lloyds
Divide and Conquer with km# and km++
Divide and Conquer with km++

7

15

10

30
25
20
15

2
5

10

1
0

5

5

10

15
k

20

25

0

5

10

15
k

20

25

0

5

10

15
k

20

25

Figure 1: Cost vs. k: (a) Mixtures of gaussians simulation, (b) Cloud data, (c) Spam data,.
indexes i ? [r ? 1] such that (ai , bi ) = (3 log k, O(1)). By the above discussion, the memory is
used optimally if M = rn1/r k(3 log k)q/r , in which case the final approximation guarantee will be
c?r?1 (log k)r?q , for some global c? > 0. We concentrate on the case M growing polynomially in n,
say M = n? for some ? < 1. In this case, the memory optimality constraint implies r = 1/? for
n large enough (regardless of the choice of q). This implies that the final approximation guarantee
is best if q = r ? 1, in other words, we choose the repeated k-means# for levels 1..r ? 1, and
k-means++ for level r. Summarizing, we get:
Theorem 3.2. If there is access to memory of size M = n? for some fixed ? > 0, then for sufficiently
large n the best application of the multi-level scheme described above is obtained by running r =
-?. = -log n/ log M . levels, and choosing the repeated k-means# for all but the last level, in which
k-means++ is chosen. The resulting algorithm is a randomized one-pass streaming approximation
to k-means, with an approximation ratio of O(?
cr?1 (log k)), for some global c? > 0. The running
time of the algorithm is O(dnk 2 log n log k).
We should compare the above multi-level streaming algorithm with the state-of-art (in terms of
memory vs. approximation tradeoff) streaming algorithm for the k-median problem. Charikar,
Callaghan, and Panigrahy [CCP03] give a one-pass streaming algorithm for the k-median problem
which gives a constant factor approximation and uses O(k?poly log(n)) memory. The main problem
with this algorithm from a practical point of view is that the average processing time per item is
large. It is proportional to the amount of memory used, which is poly-logarithmic in n. This might
be undesirable in practical scenarios where we need to process a data item quickly when it arrives. In
contrast, the average per item processing time using the divide-and-conquer-strategy is constant and
furthermore the algorithm can be pipelined (i.e. data items can be temporarily stored in a memory
buffer and quickly processed before the the next memory buffer is filled). So, even if [CCP03] can
be extended to the k-means setting, streaming algorithms based on the divide-and-conquer-strategy
would be more interesting from a practical point of view.

4

Experiments

Datasets. In our discussion, n denotes the number of points in the data, d denotes the dimension,
and k denotes the number of clusters. Our first evaluation, detailed in Tables 1a)-c) and Figure 1,
compares our algorithms on the following data: (1) norm25 is synthetic data generated in the following manner: we choose 25 random vertices from a 15 dimensional hypercube of side length 500.
We then add 400 gaussian random points (with variance 1) around each of these points.8 So, for this
data n = 10, 000 and d = 15. The optimum cost for k = 25 is 1.5026 ? 105 . (2) The UCI Cloud
dataset consists of cloud cover data [AN07]. Here n = 1024 and d = 10. (3) The UCI Spambase
dataset is data for an e-mail spam detection task [AN07]. Here n = 4601 and d = 58.
To compare against a baseline method known to be used in practice, we used Lloyd?s algorithm,
commonly referred to as the k-means algorithm. Standard Lloyd?s algorithm operates in the batch
setting, which is an easier problem than the one-pass streaming setting, so we ran experiments with
this algorithm to form a baseline. We also compare to an online version of Lloyd?s algorithm,
however the performance is worse than the batch version, and our methods, for all problems, so we
8

Testing clustering algorithms on this simulation distribution was inspired by [AV07].

7

k
5
10
15
20
25
k
5
10
15
20
25
k
5
10
15
20
25

BL
5.1154 ? 109
3.3080 ? 109
2.0123 ? 109
1.4225 ? 109
0.8602 ? 109

OL
6.5967 ? 109
6.0146 ? 109
4.3743 ? 109
3.7794 ? 109
2.8859 ? 109

DC-1
7.9398 ? 109
4.5954 ? 109
2.5468 ? 109
1.0718 ? 109
2.7842 ? 105

DC-2
7.8474 ? 109
4.6829 ? 109
2.5898 ? 109
1.1403 ? 109
2.7298 ? 105

BL
1.25
2.05
3.88
8.62
13.13

OL
1.32
2.45
3.49
4.69
6.04

DC-1
14.37
45.39
95.22
190.73
283.19

BL
1.12
1.20
2.18
2.59
2.43

OL
0.13
0.25
0.35
0.47
0.52

DC-1
1.73
5.64
10.98
25.72
36.17

BL
4.9139 ? 108
1.6952 ? 108
1.5670 ? 108
1.5196 ? 108
1.5168 ? 108

OL
1.7001 ? 109
1.6930 ? 109
1.4762 ? 109
1.4766 ? 109
1.4754 ? 109

DC-1
3.4021 ? 108
1.0206 ? 108
5.5095 ? 107
3.3400 ? 107
2.3151 ? 107

DC-2
3.3963 ? 108
1.0463 ? 108
5.3557 ? 107
3.2994 ? 107
2.3391 ? 107

BL
9.68
34.78
67.54
100.44
109.41

OL
0.70
1.31
1.88
2.57
3.04

DC-1
11.65
40.14
77.75
194.01
274.42

BL
1.7707 ? 107
0.7683 ? 107
0.5012 ? 107
0.4388 ? 107
0.3839 ? 107

OL
1.2401 ? 108
8.5684 ? 107
8.4633 ? 107
6.5110 ? 107
6.3758 ? 107

DC-1
2.2924 ? 107
8.3363 ? 106
4.9667 ? 106
3.7479 ? 106
2.8895 ? 106

DC-2
2.2617 ? 107
8.7788 ? 106
4.8806 ? 106
3.7536 ? 106
2.9014 ? 106

DC-2
9.93
21.09
30.34
41.49
53.07
DC-2
0.92
1.87
2.67
4.19
4.82
DC-2
5.14
9.75
14.41
22.76
27.10

Table 1: Columns 2-5 have the clustering cost and columns 6-9 have time in sec. a) norm25 dataset,
b) Cloud dataset, c) Spambase dataset.
Memory/
#levels
1024/0
480/1
360/2

Cost

Time
6

8.74 ? 10
8.59 ? 106
8.61 ? 106

5.5
3.6
3.8

Memory/
#levels
2048/0
1250/1
1125/2

Cost

Time
4

5.78 ? 10
5.36 ? 104
5.15 ? 104

30
25
26

Memory/
#levels
4601/0
880/1
600/2

Cost

Time
8

1.06 ? 10
0.99 ? 108
1.03 ? 108

34
20
19.5

Table 2: Multi-level hierarchy evaluation: a) Cloud dataset, k = 10, b) A subset of norm25 dataset,
n = 2048, k = 25, c) Spambase dataset, k = 10. The memory size decreases as the number of
levels of the hierarchy increases. (0 levels means running batch k-means++ on the data.)
do not include it in our plots for the real data sets.9 Tables 1a)-c) shows average k-means cost (over
10 random restarts for the randomized algorithms: all but Online Lloyd?s) for these algorithms:
(1) BL: Batch Lloyd?s, initialized with random centers in the input data, and run to convergence.10
(2) OL: Online Lloyd?s.
(3) DC-1: The simple 1-stage divide and conquer algorithm of Section 3.2.
(4) DC-2: The simple 1-stage divide and conquer algorithm 3 of Section 3.1. The sub-algorithms
used are A = ?run k-means++ 3 ? log n times and pick best clustering,? and A? is k-means++. In our
context, k-means++ and k-means# are only the seeding step, not followed by Lloyd?s algorithm.
In all problems, our streaming methods achieve much lower cost than Online Lloyd?s, for all settings
of k, and lower cost than Batch Lloyd?s for most settings of k (including the correct k = 25, in
norm25). The gains with respect to batch are noteworthy, since the batch problem is less constrained
than the one-pass streaming problem. The performance of DC-1 and DC-2 is comparable.
Table 2 shows an evaluation of the one-pass multi-level hierarchical algorithm of Section 3.3, on the
different datasets, simulating different memory restrictions. Although our worst-case theoretical results imply an exponential clustering cost as a function of the number of levels, our results show a far
more optimistic outcome in which adding levels (and limiting memory) actually improves the outcome. We conjecture that our data contains enough information for clustering even on chunks that fit
in small buffers, and therefore the results may reflect the benefit of the hierarchical implementation.
Acknowledgements. We thank Sanjoy Dasgupta for suggesting the study of approximation algorithms for k-means in the streaming setting, for excellent lecture notes, and for helpful discussions.
9
10

Despite the poor performance we observed, this algorithm is apparently used in practice, see [Das08].
We measured convergence by change in cost less than 1.

8

References
[ADK09] Ankit Aggarwal, Amit Deshpande and Ravi Kannan: Adaptive Sampling for k-means Clustering.
APPROX, 2009.
[AL09] Nir Ailon and Edo Liberty: Correlation Clustering Revisited: The ?True? Cost of Error Minimization
Problems. To appear in ICALP 2009.
[AMS96] Noga Alon, Yossi Matias, and Mario Szegedy.: The space complexity of approximating the frequency moments. In Proceedings of the Twenty-Eighth Annual ACM Symposium on Theory of Computing, pages 20?29, 1996.
[AV06] David Arthur and Sergei Vassilvitskii: Worst-case and smoothed analyses of the icp algorithm, with
an application to the k-means method. FOCS, 2006
[AV07] David Arthur and Sergei Vassilvitskii: k-means++: the advantages of careful seeding. SODA, 2007.
[AGKM+04] V. Arya, N. Garg, R. Khandekar, A. Meyerson, K. Munagala, and V. Pandit: Local search heuristics for k-median and facility location problems. Siam Journal of Computing, 33(3):544?562, 2004.
[AN07] A.
Asuncion
and
D.J.
Newman:
UCI
Machine
Learning
Repository.
http://www.ics.uci.edu/?mlearn/MLRepository.html, University of California, Irvine, School of
Information and Computer Sciences, 2007.
[BBG09] Maria-Florina Balcan, Avrim Blum, and Anupam Gupta: Approximate Clustering without the Approximation. SODA, 2009.
[BL08] S. Ben-David and U. von Luxburg: Relating clustering stability to properties of cluster boundaries.
COLT, 2008
[CCP03] Moses Charikar and Liadan O?Callaghan and Rina Panigrahy: Better streaming algorithms for clustering problems. STOC, 2003.
[CG99] Moses Charikar and Sudipto Guha: Improved combinatorial algorithms for the facility location and
k-medians problem. FOCS, 1999.
[CMTS02] M. Charikar, S. Guha , E Tardos, and D. Shmoys: A Constant Factor Approximation Algorithm
for the k-Median Problem. Journal of Computer and System Sciences, 2002.
[CR08] Kamalika Chaudhuri and Satish Rao: Learning Mixtures of Product Distributions using Correlations
and Independence. COLT, 2008.
[Das08] Sanjoy Dasgupta.: Course notes, CSE 291: Topics in unsupervised learning. http://wwwcse.ucsd.edu/ dasgupta/291/index.html, University of California, San Diego, Spring 2008.
[Gon85] T. F. Gonzalez: Clustering to minimize the maximum intercluster distance. Theoretical Computer
Science, 38, pages 293?306, 1985.
[GMMM+03] Sudipto Guha, Adam Meyerson, Nina Mishra, Rajeev Motwani, and Liadan O?Callaghan: Clustering Data Streams: Theory and Practice. IEEE Transactions on Knowledge and Data Engineering,
15(3): 515?528, 2003.
[Ind99] Piotr Indyk: Sublinear Time Algorithms for Metric Space Problems. STOC, 1999.
[JV01]

K. Jain and Vijay Vazirani: Approximation Algorithms for Metric Facility Location and k-Median
Problems Using the Primal-Dual Schema and Lagrangian Relaxation. Journal of the ACM. 2001.

[KMNP+04] T. Kanungo, D. M. Mount, N. Netanyahu, C. Piatko, R. Silverman, and A. Y. Wu: A Local
Search Approximation Algorithm for k-Means Clustering, Computational Geometry: Theory and
Applications, 28, 89-112, 2004.
[LV92] J. Lin and J. S. Vitter: Approximation Algorithms for Geometric Median Problems. Information
Processing Letters, 1992.
[McG07] Andrew McGregor: Processing Data Streams. Ph.D. Thesis, Computer and Information Science,
University of Pennsylvania, 2007.
[M05]

S. Muthukrishnan: Data Streams: Algorithms and Applications, NOW Publishers, Inc., Hanover MA

[ORSS06] Rafail Ostrovsky, Yuval Rabani, Leonard J. Schulman, Chaitanya Swamy: The effectiveness of
Lloyd-type methods for the k-means problem. FOCS, 2006.
[VW02] V. Vempala and G. Wang: A spectral algorithm of learning mixtures of distributions. pages 113?123,
FOCS, 2002.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 2385-online-classification-on-a-budget.pdf

Online Classification on a Budget

Koby Crammer
Computer Sci. & Eng.
Hebrew University
Jerusalem 91904, Israel

Jaz Kandola
Royal Holloway,
University of London
Egham, UK

Yoram Singer
Computer Sci. & Eng.
Hebrew University
Jerusalem 91904, Israel

kobics@cs.huji.ac.il

jaz@cs.rhul.ac.uk

singer@cs.huji.ac.il

Abstract
Online algorithms for classification often require vast amounts of memory and computation time when employed in conjunction with kernel
functions. In this paper we describe and analyze a simple approach for an
on-the-fly reduction of the number of past examples used for prediction.
Experiments performed with real datasets show that using the proposed
algorithmic approach with a single epoch is competitive with the support vector machine (SVM) although the latter, being a batch algorithm,
accesses each training example multiple times.

1

Introduction and Motivation

Kernel-based methods are widely being used for data modeling and prediction because of
their conceptual simplicity and outstanding performance on many real-world tasks. The
support vector machine (SVM) is a well known algorithm for finding kernel-based linear
classifiers with maximal margin [7]. The kernel trick can be used to provide an effective
method to deal with very high dimensional feature spaces as well as to model complex input phenomena via embedding into inner product spaces. However, despite generalization
error being upper bounded by a function of the margin of a linear classifier, it is notoriously
difficult to implement such classifiers efficiently. Empirically this often translates into very
long training times. A number of alternative algorithms exist for finding a maximal margin
hyperplane many of which have been inspired by Rosenblatt?s Perceptron algorithm [6]
which is an on-line learning algorithm for linear classifiers. The work on SVMs has inspired a number of modifications and enhancements to the original Perceptron algorithm.
These incorporate the notion of margin to the learning and prediction processes whilst exhibiting good empirical performance in practice. Examples of such algorithms include the
Relaxed Online Maximum Margin Algorithm (ROMMA) [4], the Approximate Maximal
Margin Classification Algorithm (ALMA) [2], and the Margin Infused Relaxed Algorithm
(MIRA) [1] which can be used in conjunction with kernel functions.
A notable limitation of kernel based methods is their computational complexity since the
amount of computer memory that they require to store the so called support patterns grows
linearly with the number prediction errors. A number of attempts have been made to speed
up the training and testing of SVM?s by enforcing a sparsity condition. In this paper we
devise an online algorithm that is not only sparse but also generalizes well. To achieve
this goal our algorithm employs an insertion and deletion process. Informally, it can be
thought of as revising the weight vector after each example on which a prediction mistake

has been made. Once such an event occurs the algorithm adds the new erroneous example
(the insertion phase), and then immediately searches for past examples that appear to be
redundant given the recent addition (the deletion phase). As we describe later, making this
adjustment to the algorithm allows us to modify the standard online proof techniques so as
to provide a bound on the total number of examples the algorithm keeps.
This paper is organized as follows. In Sec. 2 we formalize the problem setting and provide
a brief outline of our method for obtaining a sparse set of support patterns in an online
setting. In Sec. 3 we present both theoretical and algorithmic details of our approach and
provide a bound on the number of support patterns that constitute the cache. Sec. 4 provides
experimental details, evaluated on three real world datasets, to illustrate the performance
and merits of our sparse online algorithm. We end the paper with conclusions and ideas for
future work.

2

Problem Setting and Algorithms

This work focuses on online additive algorithms for classification tasks. In such problems
we are typically given a stream of instance-label pairs (x1 , y1 ), . . . , (xt , yt ), . . .. we assume
that each instance is a vector xt ? Rn and each label belongs to a finite set Y. In this
and the next section we assume that Y = {?1, +1} but relax this assumption in Sec. 4
where we describe experiments with datasets consisting of more than two labels. When
dealing with the task of predicting new labels, thresholded linear classifiers of the form
h(x) = sign(w ? x) are commonly employed. The vector w P
is typically represented as
a weighted linear combination of the examples, namely w = t ?t yt xt where ?t ? 0.
The instances for which ?t > 0 are referred to as support patterns. Under this assumption,
the output of the classifier solely depends on inner-products of the form x ? xt the use of
kernel functions can easily be employed simply by replacing the standard scalar product
with a function K(?, ?) which satisfies Mercer conditions
[7]. The resulting classification
P
rule takes the form h(x) = sign(w ? x) = sign( t ?t yt K(x, xt )).
The majority of additive online algorithms for classification, for example the well known
Perceptron [6], share a common algorithmic structure. These online algorithms typically
work in rounds. On the tth
P round, an online algorithm receives an instance xt , computes
the inner-products st = i<t ?i yi K(xi , xt ) and sets the predicted label to be sign(st ).
The algorithm then receives the correct label yt and evaluates whether yt st ? ?t . The exact
value of parameter ?t depends on the specific algorithm being used for classification. If the
result of this test is negative, the algorithms do not modify wt and thus ?t is implicitly
set to 0. Otherwise, the algorithms modifies its classification using a predetermined update
rule. Informally we can consider this update to be decomposed into three stages. Firstly, the
algorithms choose a non-negative value for ?t (again the exact choice of the parameter ?t is
algorithm dependent). Secondly, the prediction vector is replaced with a linear combination
of the current vector wt and the example, wt+1 = wt + ?t yt xt . In a third, optional stage
(see for example [4]), the norm of the newly updated weight vector is scaled, w t+1 ?
ct wt+1 for some ct > 0. The various online algorithms differ in the way the values of the
parameters ?t , ?t and ct are set. A notable example of an online algorithm is the Perceptron
algorithm [6] for which we set ?t = 0, ?t = 1 and ct = 1. More recent algorithms
such as the Relaxed Online Maximum Margin Algorithm (ROMMA) [4] the Approximate
Maximal Margin Classification Algorithm (ALMA) [2] and the Margin Infused Relaxed
Algorithm (MIRA) [1] can also be described in this framework although the constants
?t , ?t and ct are not as simple as the ones employed by the Perceptron algorithm.
An important computational consideration needs to be made when employing kernel functions for machine learning tasks. This is because the amount of memory required to
store the so called support patterns grows linearly with the number prediction errors. In

Input: Tolerance ?.
Initialize: Set ?t ?t = 0 , w0 = 0 , C0 = ?.
Loop: For t = 1, 2, . . . , T
? Get a new instance xt ? Rn .
? Predict y?t = sign (yt (xt ? wt?1 )).
? Get a new label yt .
? if yt (xt ? wt?1 ) ? ? update:
1. Insert Ct ? Ct?1 ? {t}.
2. Set ?t = 1.
3. Compute wt ? wt?1 + yt ?t xt .
4. DistillCache(Ct , wt , (?1 , . . . , ?t )).
Output : H(x) = sign(wT ? x).
Figure 1: The aggressive Perceptron algorithm with a variable-size cache.

this paper we shift the focus to the problem of devising online algorithms which are
budget-conscious as they attempt to keep the number of support patterns small. The
approach is attractive for at least two reasons. Firstly, both the training time and classification time can be reduced significantly if we store only a fraction of the potential
support patterns. Secondly, a classier with a small number of support patterns is intuitively ?simpler?, and hence are likely to exhibit good generalization properties rather
than complex classifiers with large numbers of support patterns. (See for instance [7]
for formal results connecting the number of support patterns to the generalization error.)
In Sec. 3 we present a formal analysis and
Input: C, w, (?1 , . . . , ?t ).
the algorithmic details of our approach.
Loop:
Let us now provide a general overview
? Choose i ? C such that
of how to restrict the number of support
? ? yi (w ? ?i yi xi ).
patterns in an online setting. Denote by
Ct the indices of patterns which consti? if no such i exists then return.
tute the classification vector wt . That is,
? Remove the example i :
i ? Ct if and only if ?i > 0 on round
1. ?i = 0.
t when xt is received. The online classification algorithms discussed above keep
2. w ? w ? ?i yi xi .
enlarging Ct ? once an example is added
3. C ? C/{i}
to Ct it will never be deleted. However,
Return : C, w, (?1 , . . . , ?t ).
as the online algorithm receives more examples, the performance of the classifier
Figure 2: DistillCache
improves, and some of the past examples
may have become redundant and hence
can be removed. Put another way, old examples may have been inserted into the cache simply due the lack of support patterns in early rounds. As more examples are observed, the
old examples maybe replaced with new examples whose location is closer to the decision
boundary induced by the online classifier. We thus add a new stage to the online algorithm
in which we discard a few old examples from the cache CP
of
t . We suggest a modification

the online algorithm structure as follows. Whenever yt
i<t ?i yi K(x, xi ) ? ?t , then
after adding xt to w and inserting the tth into Ct , we scan the cache Ct for seemingly
redundant examples by examining the margin conditions of old examples in C t . If such
an example is found, we discard it from the both the classifier and the cache by updating
wt ? wt ? ?i yi xi and setting Ct ? Ct /{i}. The pseudocode for this ?budget-conscious?
version of the aggressive Perceptron algorithm [3] is given in Fig. 1. We say that the algo-

rithm employs a variable-size cache since we do no limit explicitly the number of support
patterns though we do attempt to discard as many patterns as possible from the cache. A
similar modification, to that described for aggressive Perceptron, can be made to all of the
online classification algorithms outlined above. In particular, we use a modification of the
MIRA [1] algorithm in our experiments.

3

Analysis

In this section we provide our main formal result for the algorithm described in the previous
section. Informally, the theorem below states that the actual size of the cache that the algorithm builds is inversely proportional to the square of the best margin that can be achieved
on the data. This form of bound is common to numerous online learning algorithms for
classification. However, here the bound is on the size of the cache whereas in common settings the corresponding bounds are on the number of prediction mistakes. The bound also
depends on ?, the margin used by the algorithm to check whether a new example should be
added to the cache and to discard old examples attaining a large margin. Clearly, the larger
the value of ? the more often we add examples to the cache.
Theorem 1 Let (x1 , y1 ), . . . , (xT , yT ) be an input sequence for the algorithm given in
Fig. 1, where xt ? Rn and yt ? {?1, +1}. Denote by R = maxt kxt k. Assume that there
exists a vector u of unit norm (kuk = 1) which classifies the entire sequence correctly with
a margin ? = mint yt (u ? xt ) > 0. Then the number of support patterns constituting the
cache is at most S ? (R2 + 2?)/? 2 .
Proof: The proof of the theorem is based on the mistake bound of the Perceptron algorithm [5]. To prove the theorem we bound kwT k22 from above and below and compare the
bounds. Denote by ?it the weight of the ith example at the end of round t (after stage 4 of
the algorithm). Similarly, we denote by ?
? it to be the weight of the ith example on round
t after stage 3, before calling the DistillCache (Fig. 2) procedure. We analogously
? t the corresponding instantaneous classifiers. First, we derive a lower
denote by wt and w
bound on kwT k2 by bounding the term wT ? u from below in a recursive manner.
X
wT ? u =
?tT yt (xt ? u)
t?CT

? ?

X

?tT = ? S .

(1)

t?CT

We now turn to upper bound kwT k2 . Recall that each example may be added to the cache
and removed from the cache a single time. Let us write kwT k2 as a telescopic sum,
? T k2 ) + (kw
? T k2 ? kwT ?1 k2 ) + . . . + (kw
? 1 k2 ? kw0 k2 ) . (2)
kwT k2 = (kwT k2 ? kw
We now consider three different scenarios that may occur for each new example. The
first case is when we did not insert the tth example into the cache at all. In this case,
? t k2 ? kwt?1 k2 ) = 0. The second scenario is when an example is inserted into the
(kw
cache and is never discarded in future rounds, thus,
? t k2 = kwt?1 + yt xt k2 = kwt?1 k2 + 2yt (wt?1 ? xt ) + kxt k2 .
kw
Since we inserted (xt , yt ), the condition yt (wt?1 ? xt ) ? ? must hold. Combining this
? t k2 ?
with the assumption that the examples are enclosed in a ball of radius R we get, (k w
kwt?1 k2 ) ? 2? + R2 . The last scenario occurs when an example is inserted into the cache
on some round t, and is then later on removed from the cache on round t + p for p > 0. As
in the previous case we can bound the value of summands in Equ. (2),
? t k2 ? kwt?1 k2 ) + (kwt+p k2 ? kw
? t+p k2 )
(kw

Input: Tolerance ?, Cache Limit n.
Initialize: Set ?t ?t = 0 , w0 = 0 , C0 = ?.
Loop: For t = 1, 2, . . . , T
? Get a new instance xt ? Rn .
? Predict y?t = sign (yt (xt ? wt?1 )).
? Get a new label yt .
? if yt (xt ? wt?1 ) ? ? update:
1. If |Ct | = n remove one example:
(a) Find i = arg maxj?Ct {yj (wt?1 ? ?j yj xj )}.
(b) Update wt?1 ? wt?1 ? ?i yi xi .
(c) Remove Ct?1 ? Ct?1 /{i}
2. Insert Ct ? Ct?1 ? {t}.
3. Set ?t = 1.
4. Compute wt ? wt?1 + yt ?t xt .
Output : H(x) = sign(wT ? x).
Figure 3: The aggressive Perceptron algorithm with as fixed-size cache.
? t+p ? xt ) + kxt k2
= 2yt (wt?1 ? xt ) + kxt k2 ? 2yt (w
? t+p ? yt xt ) ? xt )]
= 2 [yt (wt?1 ? xt ) ? yt ((w
? t+p ? yt xt ) ? xt )] .
? 2 [? ? yt ((w
? t+p ? yt xt ) ? xt ) ? ?, and
Based on the form of the cache update we know that yt ((w
thus,
? t k2 ? kwt?1 k2 ) + (kwt+p k2 ? kw
? t+p k2 ) ? 0 .
(kw
Summarizing all three cases we see that only the examples which persist in the cache
contribute a factor of R2 + 2? each to the bound of the telescopic sum of Equ. (2) and
the rest of the examples do contribute anything to the bound. Hence, we can bound the
norm of wT as follows,

kwT k2 ? S R2 + 2? .
(3)
We finish up the proof by applying the Cauchy-Swartz inequality and the assumption
kuk = 1. Combining Equ. (1) and Equ. (3) we get,
? 2 S 2 ? (wT ? u)2 ? kwT k2 kuk2 ? S(2? + R2 ) ,
which gives the desired bound.

4

Experiments

In this section we describe the experimental methods that were used to compare the performance of standard online algorithms with the new algorithm described above. We also
describe shortly another variant that sets a hard limit on the number of support patterns.
The experiments were designed with the aim of trying to answer the following questions.
First, what is effect of the number of support patterns on the generalization error (measured in terms of classification accuracy on unseen data), and second, would the algorithm
described in Fig. 2 be able to find an optimal cache size that is able to achieve the best
generalization performance. To examine each question separately we used a modified version of the algorithm described by Fig. 2 in which we restricted ourselves to have a fixed
bounded cache. This modified algorithm (which we refer to as the fixed budget Perceptron)

Name
mnist
letter
usps

No. of
Training Examples
60000
16000
7291

No. of
Test Examples
10000
4000
2007

No. of
Classes
10
26
10

No. of
Attributes
784
16
256

Table 1: Description of the datasets used in experiments.

simulates the original Perceptron algorithm with one notable difference. When the number of support patterns exceeds a pre-determined limit, it chooses a support pattern from
the cache and discards it. With this modification the number of support patterns can never
exceed the pre-determined limit. This modified algorithm is described in Fig. 3. The algorithm deletes the example which seemingly attains the highest margin after the removal of
the example itself (line 1(a) in Fig. 3).
Despite the simplicity of the original Perceptron algorithm [6] its good generalization performance on many datasets is remarkable. During the last few year a number of other additive online algorithms have been developed [4, 2, 1] that have shown better performance on
a number of tasks. In this paper, we have preferred to embed these ideas into another online
algorithm and start with a higher baseline performance. We have chosen to use the Margin
Infused Relaxed Algorithm (MIRA) as our baseline algorithm since it has exhibited good
generalization performance in previous experiments [1] and has the additional advantage
that it is designed to solve multiclass classification problem directly without any recourse
to performing reductions.
The algorithms were evaluated on three natural datasets: mnist1 , usps2 and letter3 .
The characteristics of these datasets has been summarized in Table 1. A comprehensive
overview of the performance of various algorithms on these datasets can be found in a
recent paper [2]. Since all of the algorithms that we have evaluated are online, it is not
implausible for the specific ordering of examples to affect the generalization performance.
We thus report results averaged over 11 random permutations for usps and letter and
over 5 random permutations for mnist. No free parameter optimization was carried out
and instead we simply used the values reported in [1]. More specifically, the margin parameter was set to ? = 0.01 for all algorithms and for all datasets. A homogeneous polynomial
kernel of degree 9 was used when training on the mnist and usps data sets, and a RBF
kernel for letter data set. (The variance of the RBF kernel was identical to the one used
in [1].)
We evaluated the performance of four algorithms in total. The first algorithm was the
standard MIRA online algorithm, which does not incorporate any budget constraints. The
second algorithm is the version of MIRA described in Fig. 3 which uses a fixed limited
budget. Here we enumerated the cache size limit in each experiment we performed. The
different sizes that we tested are dataset dependent but for each dataset we evaluated at
least 10 different sizes. We would like to note that such an enumeration cannot be done in
an online fashion and the goal of employing the the algorithm with a fixed-size cache is to
underscore the merit of the truly adaptive algorithm. The third algorithm is the version of
MIRA described in Fig. 2 that adapts the cache size during the running of the algorithms.
We also report additional results for a multiclass version of the SVM [1]. Whilst this
algorithm is not online and during the training process it considers all the examples at once,
this algorithm serves as our gold-standard algorithm against which we want to compare
1

Available from http://www.research.att.com/?yann
Available from ftp.kyb.tuebingen.mpg.de
3
Available from http://www.ics.uci.edu/?mlearn/MLRepository.html
2

usps

mnist
Fixed
Adaptive
SVM
MIRA

1.8

4.8
4.7

letter

Fixed
Adaptive
SVM
MIRA

5.5

1.7

4.6

5

1.5

1.4

Test Error

4.5
Test Error

Test Error

1.6

Fixed
Adaptive
SVM
MIRA

6

4.4
4.3

4.5
4
3.5

4.2
4.1

3

4

2.5

1.3

1.2

3.9

0.2

0.4

0.6

0.8

1
1.2
1.4
# Support Patterns

1.6

1.8

2

2.2

500

4

2
1000

1500

x 10

mnist

2000
2500
# Support Patterns

3000

3500

1000

2000

3000

usps
Fixed
Adaptive
MIRA

1550

7000

8000

9000

letter
Fixed
Adaptive
MIRA

270

4000
5000
6000
# Support Patterns

Fixed
Adaptive
MIRA

1500

265

1500

1400

260

Training Online Errors

Training Online Errors

Training Online Errors

1450

1450

255
250
245

1400

1350

1300

1350

240
1250

235

1300
0.2

0.4

0.6

0.8

1
1.2
1.4
# Support Patterns

1.6

1.8

2

2.2

500

4

1000

1500

x 10

mnist

4

x 10

2000
2500
# Support Patterns

3000

3500

1000

usps
6500

Fixed
Adaptive
MIRA

5.5

2000

3000

4000
5000
6000
# Support Patterns

7000

Fixed
Adaptive
MIRA

1.5

6000

9000

letter

4

x 10
1.6

Fixed
Adaptive
MIRA

8000

4

3.5

3

1.4

5500

Training Margin Errors

Training Margin Errors

Training Margin Errors

5

4.5

5000

4500

1.3

1.2

1.1

4000
1

2.5

3500
0.9

2
0.2

0.4

0.6

0.8

1
1.2
1.4
# Support Patterns

1.6

1.8

2

2.2
4

x 10

500

1000

1500

2000
2500
# Support Patterns

3000

3500

1000

2000

3000

4000
5000
6000
# Support Patterns

7000

8000

9000

Figure 4: Results on a three data sets - mnist (left), usps (center) and letter (right). Each
point in a plot designates the test error (y-axis) vs. the number of support patterns used
(x-axis). Four algorithms are compared - SVM, MIRA, MIRA with a fixed cache size and
MIRA with a variable cache size.
performance. Note that for the multiclass SVM we report the results using the best set of
parameters, which does not coincide with the set of parameters used for the online training.
The results are summarized in Fig 4. This figure is composed of three different plots organized in columns. Each of these plots corresponds to a different dataset - mnist (left),
usps (center) and letter (right). In each of the three plots the x-axis designates number of
support patterns the algorithm uses. The results for the fixed-size cache are connected with
a line to emphasize the performance dependency on the size of the cache.
The top row of the three columns shows the generalization error. Thus the y-axis designates
the test error of an algorithm on unseen data at the end of the training. Looking at the error
of the algorithm with a fixed-size cache reveals that there is a broad range of cache size
where the algorithm exhibits good performance. In fact for MNIST and USPS there are
sizes for which the test error of the algorithm is better than SVM?s test error. Naturally, we
cannot fix the correct size in hindsight so the question is whether the algorithm with variable
cache size is a viable automatic size-selection method. Analyzing each of the datasets in
turn reveals that this is indeed the case ? the algorithm obtains a very similar number
of support patterns and test error when compared to the SVM method. The results are
somewhat less impressive for the letter dataset which contains less examples per class. One
possible explanation is that the algorithm had fewer chances to modify and distill the cache.
Nonetheless, overall the results are remarkable given that all the online algorithms make a
single pass through the data and the variable-size method finds a very good cache size while

making it also comparable to the SVM in terms of performance. The MIRA algorithm,
which does not incorporate any form of example insertion or deletion in its algorithmic
structure, obtains the poorest level of performance not only in terms of generalization error
but also in terms of number of support patterns.
The plot of online training error against the number of support patterns, in row 2 of Fig 4,
can be considered to be a good on-the-fly validation of generalization performance. As the
plots indicate, for the fixed and adaptive versions of the algorithm, on all the datasets, a
low online training error translates into good generalization performance. Comparing the
test error plots with the online error plots we see a nice similarity between the qualitative
behavior of the two errors. Hence, one can use the online error, which is easy to evaluate,
to choose a good cache size for the fixed-size algorithm.
The third row gives the online training margin errors that translates directly to the number
of insertions into the cache. Here we see that the good test error and compactness of the
algorithm with a variable cache size come with a price. Namely, the algorithm makes
significantly more insertions into the cache than the fixed size version of the algorithm.
However, as the upper two sets of plots indicate, the surplus in insertions is later taken care
of by excess deletions and the end result is very good overall performance. In summary, the
online algorithm with a variable cache and SVM obtains similar levels of generalization and
also number of support patterns. While the SVM is still somewhat better in both aspects
for the letter dataset, the online algorithm is much simpler to implement and performs a
single sweep through the training data.

5

Summary

We have described and analyzed a new sparse online algorithm that attempts to deal with
the computational problems implicit in classification algorithms such as the SVM. The
proposed method was empirically tested and its performance in both the size of the resulting
classifier and its error rate are comparable to SVM. There are a few possible extensions and
enhancements. We are currently looking at alternative criteria for the deletions of examples
from the cache. For instance, the weight of examples might relay information on their
importance for accurate classification. Incorporating prior knowledge to the insertion and
deletion scheme might also prove important. We hope that such enhancements would make
the proposed approach a viable alternative to SVM and other batch algorithms.
Acknowledgements: The authors would like to thank John Shawe-Taylor for many helpful
comments and discussions. This research was partially funded by the EU project KerMIT
No. IST-2000-25341.

References
[1] K. Crammer and Y. Singer. Ultraconservative online algorithms for multiclass problems. Jornal
of Machine Learning Research, 3:951?991, 2003.
[2] C. Gentile. A new approximate maximal margin classification algorithm. Journal of Machine
Learning Research, 2:213?242, 2001.
[3] M?ezard M. Krauth W. Learning algorithms with optimal stability in neural networks. Journal of
Physics A., 20:745, 1987.
[4] Y. Li and P. M. Long. The relaxed online maximum margin algorithm. Machine Learning,
46(1?3):361?387, 2002.
[5] A. B. J. Novikoff. On convergence proofs on perceptrons. In Proceedings of the Symposium on
the Mathematical Theory of Automata, volume XII, pages 615?622, 1962.
[6] F. Rosenblatt. The perceptron: A probabilistic model for information storage and organization in
the brain. Psychological Review, 65:386?407, 1958. (Reprinted in Neurocomputing (MIT Press,
1988).).
[7] V. N. Vapnik. Statistical Learning Theory. Wiley, 1998.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 5101-statistical-active-learning-algorithms.pdf

Statistical Active Learning Algorithms
Vitaly Feldman
IBM Research - Almaden
vitaly@post.harvard.edu

Maria Florina Balcan
Georgia Institute of Technology
ninamf@cc.gatech.edu

Abstract
We describe a framework for designing efficient active learning algorithms that are
tolerant to random classification noise and differentially-private. The framework
is based on active learning algorithms that are statistical in the sense that they rely
on estimates of expectations of functions of filtered random examples. It builds
on the powerful statistical query framework of Kearns [30].
We show that any efficient active statistical learning algorithm can be automatically converted to an efficient active learning algorithm which is tolerant to random classification noise as well as other forms of ?uncorrelated? noise. We show
that commonly studied concept classes including thresholds, rectangles, and linear separators can be efficiently actively learned in our framework. These results
combined with our generic conversion lead to the first computationally-efficient
algorithms for actively learning some of these concept classes in the presence of
random classification noise that provide exponential improvement in the dependence on the error  over their passive counterparts. In addition, we show that our
algorithms can be automatically converted to efficient active differentially-private
algorithms. This leads to the first differentially-private active learning algorithms
with exponential label savings over the passive case.

1

Introduction

Most classic machine learning methods depend on the assumption that humans can annotate all the
data available for training. However, many modern machine learning applications have massive
amounts of unannotated or unlabeled data. As a consequence, there has been tremendous interest
both in machine learning and its application areas in designing algorithms that most efficiently utilize the available data, while minimizing the need for human intervention. An extensively used and
studied technique is active learning, where the algorithm is presented with a large pool of unlabeled
examples and can interactively ask for the labels of examples of its own choosing from the pool,
with the goal to drastically reduce labeling effort. This has been a major area of machine learning
research in the past decade [19], with several exciting developments on understanding its underlying
statistical principles [27, 18, 4, 3, 29, 21, 15, 7, 31, 10, 34, 6]. In particular, several general characterizations have been developed for describing when active learning can in principle have an advantage
over the classic passive supervised learning paradigm, and by how much. However, these efforts
were primarily focused on sample size bounds rather than computation, and as a result many of the
proposed algorithms are not computationally efficient. The situation is even worse in the presence
of noise where active learning appears to be particularly hard. In particular, prior to this work, there
were no known efficient active algorithms for concept classes of super-constant VC-dimension that
are provably robust to random and independent noise while giving improvements over the passive
case.
Our Results: We propose a framework for designing efficient (polynomial time) active learning
algorithms which is based on restricting the way in which examples (both labeled and unlabeled) are
accessed by the algorithm. These restricted algorithms can be easily simulated using active sampling
and, in addition, possess a number of other useful properties. The main property we will consider is
1

tolerance to random classification noise of rate ? (each label is flipped randomly and independently
with probability ? [1]). Further, as we will show, the algorithms are tolerant to other forms of noise
and can be simulated in a differentially-private way.
In our restriction, instead of access to random examples from some distribution P over X ? Y the
learning algorithm only gets ?active? estimates of the statistical properties of P in the following
sense. The algorithm can choose any filter function ?(x) : X ? [0, 1] and a query function ? :
X ? Y ? [?1, 1] for any ? and ?. For simplicity we can think of ? as an indicator function of
some set ?S ? X of ?informative? points and of ? as some useful property of the target function.
For this pair of functions the learning algorithm can get an estimate of E(x,y)?P [?(x, y) | x ? ?S ].
For ? and ?0 chosen by the algorithm the estimate is provided to within tolerance ? as long as
E(x,y)?P [x ? ?S ] ? ?0 (nothing is guaranteed otherwise). Here the inverse of ? corresponds to
the label complexity of the algorithm and the inverse of ?0 corresponds to its unlabeled sample
complexity. Such a query is referred to as active statistical query (SQ) and algorithms using active
SQs are referred to as active statistical algorithms.
Our framework builds on the classic statistical query (SQ) learning framework of Kearns [30] defined in the context of PAC learning model [35]. The SQ model is based on estimates of expectations
of functions of examples (but without the additional filter function) and was defined in order to design efficient noise tolerant algorithms in the PAC model. Despite the restrictive form, most of the
learning algorithms in the PAC model and other standard techniques in machine learning and statistics used for problems over distributions have SQ analogues [30, 12, 11, ?]1 . Further, statistical
algorithms enjoy additional properties: they can be simulated in a differentially-private way [11],
automatically parallelized on multi-core architectures [17] and have known information-theoretic
characterizations of query complexity [13, 26]. As we show, our framework inherits the strengths of
the SQ model while, as we will argue, capturing the power of active learning.
At a first glance being active and statistical appear to be incompatible requirements on the algorithm.
Active algorithms typically make label query decisions on the basis of examining individual samples
(for example as in binary search for learning a threshold or the algorithms in [27, 21, 22]). At the
same time statistical algorithms can only examine properties of the underlying distribution. But
there also exist a number of active learning algorithms that can be seen as applying passive learning
techniques to batches of examples that are obtained from querying labels of samples that satisfy the
same filter. These include the general A2 algorithm [4] and, for example, algorithms in [3, 20, 9, 8].
As we show, we can build on these techniques to provide algorithms that fit our framework.
We start by presenting a general reduction showing that any efficient active statistical learning algorithm can be automatically converted to an efficient active learning algorithm which is tolerant to
random classification noise as well as other forms of ?uncorrelated? noise. We then demonstrate the
generality of our framework by showing that the most commonly studied concept classes including thresholds, balanced rectangles, and homogenous linear separators can be efficiently actively
learned via active statistical algorithms. For these concept classes, we design efficient active learning algorithms that are statistical and provide the same exponential improvements in the dependence
on the error  over passive learning as their non-statistical counterparts.
The primary problem we consider is active learning of homogeneous halfspaces, a problem that has
attracted a lot of interest in the theory of active learning [27, 18, 3, 9, 22, 16, 23, 8, 28]. We describe two algorithms for the problem. First, building on insights from margin based analysis [3, 8],
we give an active statistical learning algorithm for homogeneous halfspaces over all isotropic logconcave distributions, a wide class of distributions that includes many well-studied density functions
and has played an important role in several areas including sampling, optimization, and learning
[32]. Our algorithm for this setting proceeds in rounds; in round t we build a better approximation
wt to the target function by using a passive SQ learning algorithm (e.g., the one of [24]) over a
distribution Dt that is a mixture of distributions in which each component is the original distribution
conditioned on being within a certain distance from the hyperplane defined by previous approximations wi . To perform passive statistical queries relative to Dt we use active SQs with a corresponding
real valued filter. This algorithm is computationally efficient and uses only poly(d, log(1/)) active
statistical queries of tolerance inverse-polynomial in the dimension d and log(1/).

1

The sample complexity of the SQ analogues might increase sometimes though.

2

For the special case of the uniform distribution over the unit ball we give a new, simpler and substantially more efficient active statistical learning algorithm. Our algorithm is based on measuring the
error of a halfspace conditioned on being within some margin of that halfspace. We show that such
measurements performed on the perturbations of the current hypothesis along the d basis vectors
can be combined to derive a better hypothesis. This approach differs substantially from the previous
algorithms for this problem [3, 22].
? The algorithm is computationally efficient and uses d log(1/)
active SQs with tolerance of ?(1/ d) and filter tolerance of ?(1/).
These results, combined with our generic simulation of active statistical algorithms in the presence
of random classification noise (RCN) lead to the first known computationally efficient algorithms
for actively learning halfspaces which are RCN tolerant and give provable label savings over the
passive case. For the uniform distribution case this leads to an algorithm with sample complexity of
O((1 ? 2?)?2 ? d2 log(1/) log(d log(1/))) and for the general isotropic log-concave case we get
sample complexity of poly(d, log(1/), 1/(1 ? 2?)). This is worse than the sample complexity in
the noiseless case which is just O((d + log log(1/)) log(1/)) [8]. However, compared to passive
learning in the presence of RCN, our algorithms have exponentially better dependence on  and essentially the same dependence on d and 1/(1 ? 2?). One issue with the generic simulation is that
it requires knowledge of ? (or an almost precise estimate). Standard approach to dealing with this
issue does not always work in the active setting and for our log-concave and the uniform distribution algorithms we give a specialized argument that preserves the exponential improvement in the
dependence on .
Differentially-private active learning: In many application of machine learning such as medical
and financial record analysis, data is both sensitive and expensive to label. However, to the best
of our knowledge, there are no formal results addressing both of these constraints. We address
the problem by defining a natural model of differentially-private active learning. In our model we
assume that a learner has full access to unlabeled portion of some database of n examples S ?
X ? Y which correspond to records of individual participants in the database. In addition, for
every element of the database S the learner can request the label of that element. As usual, the
goal is to minimize the number of label requests (such setup is referred to as pool-based active
learning [33]). In addition, we would like to preserve the differential privacy of the participants in
the database, a now-standard notion of privacy introduced in [25]. Informally speaking, an algorithm
is differentially private if adding any record to S (or removing a record from S) does not affect the
probability that any specific hypothesis will be output by the algorithm significantly.
As first shown by [11], SQ algorithms can be automatically translated into differentially-private
algorithms. Using a similar approach, we show that active SQ learning algorithms can be automatically transformed into differentially-private active learning algorithms. Using our active statistical
algorithms for halfspaces we obtain the first algorithms that are both differentially-private and give
exponential improvements in the dependence of label complexity on the accuracy parameter .
Additional related work: As we have mentioned, most prior theoretical work on active learning
focuses on either sample complexity bounds (without regard for efficiency) or the noiseless case.
For random classification noise in particular, [6] provides a sample complexity analysis based on
the splitting index that is optimal up to polylog factors and works for general concept classes and
distributions, but it is not computationally efficient. In addition, several works give active learning
algorithms with empirical evidence of robustness to certain types of noise [9, 28];
In [16, 23] online learning algorithms in the selective sampling framework are presented, where
labels must be actively queried before they are revealed. Under the assumption that the label conditional distribution is a linear function determined by a fixed target vector, they provide bounds
on the regret of the algorithm and on the number of labels it queries when faced with an adaptive
adversarial strategy of generating the instances. As pointed out in [23], these results can also be
converted to a distributional PAC setting where instances xt are drawn i.i.d. In this setting they
obtain exponential improvement in label complexity over passive learning. These interesting results
and techniques are not directly comparable to ours. Our framework is not restricted to halfspaces.
Another important difference is that (as pointed out in [28]) the exponential improvement they give
is not possible in the noiseless version of their setting. In other words, the addition of linear noise
defined by the target makes the problem easier for active sampling. By contrast RCN can only make
the classification task harder than in the realizable case.
3

Due to space constraint details of most proofs and further discussion appear in the full version of
this paper [5].

2

Active Statistical Algorithms

Let X be a domain and P be a distribution over labeled examples on X. We represent such a
distribution by a pair (D, ?) where D is the marginal distribution of P on X and ? : X ? [?1, 1]
is a function defined as ?(z) = E(x,`)?P [` | x = z]. We will be primarily considering learning in
the PAC model (realizable case) where ? is a boolean function, possibly corrupted by random noise.
When learning with respect to a distribution P = (D, ?), an active statistical learner has access to
active statistical queries. A query of this type is a pair of functions (?, ?), where ? : X ? [0, 1]
is the filter function which for a point x, specifies the probability with which the label of x should
be queried. The function ? : X ? {?1, 1} ? [?1, 1] is the query function and depends on both
point and the label. The filter function ? defines the distribution D conditioned on ? as follows:
for each x the density function D|? (x) is defined as D|? (x) = D(x)?(x)/ED [?(x)]. Note that if
? is an indicator function of some set S then D|? is exactly D conditioned on x being in S. Let
P|? denote the conditioned distribution (D|? , ?). In addition, a query has two tolerance parameters:
filter tolerance ?0 and query tolerance ? . In response to such a query the algorithm obtains a value
? such that if ED [?(x)] ? ?0 then


? ? EP [?(x, `)] ? ?
|?

(and nothing is guaranteed when ED [?(x)] < ?0 ).
An active statistical learning algorithm can also ask target-independent queries with tolerance ?
which are just queries over unlabeled samples. That is for a query ? : X ? [?1, 1] the algorithm
obtains a value ?, such that |? ? ED [?(x)]| ? ? . Such queries are not necessary when D is
known to the learner. Also for the purposes of obtaining noise tolerant algorithms one can relax the
requirements of model and give the learning algorithm access to unlabelled samples.
Our definition generalizes the statistical query framework of Kearns [30] which does not include
filtering function, in other words a query is just a function ? : X ? {?1, 1} ? [?1, 1] and it has a
single tolerance parameter ? . By definition, an active SQ (?, ?) with tolerance ? relative to P is the
same as a passive statistical query ? with tolerance ? relative to the distribution P|? . In particular, a
(passive) SQ is equivalent to an active SQ with filter ? ? 1 and filter tolerance 1.
We note that from the definition of active SQ we can see that
EP|? [?(x, `)] = EP [?(x, `) ? ?(x)]/EP [?(x)].
This implies that an active statistical query can be estimated using two passive statistical queries.
However to estimate EP|? [?(x, `)] with tolerance ? one needs to estimate EP [?(x, `) ? ?(x)] with
tolerance ? ? EP [?(x)] which can be much lower than ? . Tolerance of a SQ directly corresponds to
the number of examples needed to evaluate it and therefore simulating active SQs passively might
require many more labeled examples.
2.1

Simulating Active Statistical Queries

We first note that a valid response to a target-independent query with tolerance ? can be obtained,
with probability at least 1 ? ?, using O(? ?2 log (1/?)) unlabeled samples.
A natural way of simulating an active SQ is by filtering points drawn randomly from D: draw a
random point x, let B be drawn from Bernoulli distribution with probability of success ?(x); ask
for the label of x when B = 1. The points for which we ask for a label are distributed according to
D|? . This implies that the empirical average of ?(x, `) on O(? ?2 log (1/?)) labeled examples will
then give ?. Formally we get the following theorem.
Theorem 2.1. Let P = (D, ?) be a distribution over X ? {?1, 1}. There exists an active sampling
algorithm that given functions ? : X ? [0, 1], ? : X ? {?1, 1} ? [?1, 1], values ?0 > 0,
? > 0, ? > 0, and access to samples from P , with probability at least 1 ? ?, outputs a valid
response to active statistical query (?, ?) with tolerance parameters (?0 , ? ). The algorithm uses
O(? ?2 log (1/?)) labeled examples from P and O(?0?1 ? ?2 log (1/?)) unlabeled samples from D.
4

A direct way to simulate all the queries of an active SQ algorithm is to estimate the response to each
query using fresh samples and use the union bound to ensure that, with probability at least 1 ? ?, all
queries are answered correctly. Such direct simulation of an algorithm that uses at most q queries can
be done using O(q? ?2 log(q/?)) labeled examples and O(q?0?1 ? ?2 log (q/?)) unlabeled samples.
However in many cases a more careful analysis can be used to reduce the sample complexity of
simulation.
Labeled examples can be shared to simulate queries that use the same filter ? and do not depend on
each other. This implies that the sample size sufficient for simulating q non-adaptive queries with the
same filter scales logarithmically with q. More generally, given a set of q query functions (possibly
chosen adaptively) which belong to some set Q of low complexity (such as VC dimension) one can
reduce the sample complexity of estimating the answers to all q queries (with the same filter) by
invoking the standard bounds based on uniform convergence (e.g. [14]).
2.2

Noise tolerance

An important property of the simulation described in Theorem 2.1 is that it can be easily adapted
to the case when the labels are corrupted by random classification noise [1]. For a distribution
P = (D, ?) let P ? denote the distribution P with the label flipped with probability ? randomly and
independently of an example. It is easy to see that P ? = (D, (1 ? 2?)?). We now show that, as in
the SQ model [30], active statistical queries can be simulated given examples from P ? .
Theorem 2.2. Let P = (D, ?) be a distribution over examples and let ? ? [0, 1/2) be a noise
rate. There exists an active sampling algorithm that given functions ? : X ? [0, 1], ? : X ?
{?1, 1} ? [?1, 1], values ?, ?0 > 0, ? > 0, ? > 0, and access to samples from P ? , with
probability at least 1 ? ?, outputs a valid response to active statistical query (?, ?) with tolerance
parameters (?0 , ? ). The algorithm uses O(? ?2 (1 ? 2?)?2 log (1/?)) labeled examples from P ? and
O(?0?1 ? ?2 (1 ? 2?)?2 log (1/?)) unlabeled samples from D.
Note that the sample complexity of the resulting active sampling algorithm has informationtheoretically optimal quadratic dependence on 1/(1 ? 2?), where ? is the noise rate.
Remark 2.3. This simulation assumes that ? is given to the algorithm exactly. It is easy to see from
1?2?
the proof, that any value ? 0 such that 1?2?
0 ? [1 ? ? /4, 1 + ? /4] can be used in place of ? (with
1
? [ (?(x, 1) ? ?(x, ?1)) ? `] set to (1 ? 2?)? /4). In some learning
the tolerance of estimating EP|?
2
scenarios even an approximate value of ? is not known but it is known that ? ? ?0 < 1/2. To address
this issue one can construct a sequence ?1 , . . . , ?k of guesses of ?, run the learning algorithm with
each of those guesses in place of the true ? and let h1 , . . . , hk be the resulting hypotheses [30]. One
can then return the hypothesis hi among those that has the best agreement with a suitably large
sample. It is not hard to see that k = O(? ?1 ? log(1/(1 ? 2?0 ))) guesses will suffice for this strategy
to work [2].
Passive hypothesis testing requires ?(1/) labeled examples and might be too expensive to be used
with active learning algorithms. It is unclear if there exists a general approach for dealing with
unknown ? in the active learning setting that does not increase substantially the labelled example
complexity. However, as we will demonstrate, in the context of specific active learning algorithms
variants of this approach can be used to solve the problem.
We now show that more general types of noise can be tolerated as long as they are ?uncorrelated?
with the queries and the target function. Namely, we represent label noise using a function ? : X ?
[0, 1], where ?(x) gives the probability that the label of x is flipped. The rate of ? when learning
with respect to marginal distribution D over X is ED [?(x)]. For a distribution P = (D, ?) over
examples, we denote by P ? the distribution P corrupted by label noise ?. It is easy to see that
P ? = (D, ? ? (1 ? 2?)). Intuitively, ? is ?uncorrelated? with a query if the way that ? deviates
from its rate is almost orthogonal to the query on the target distribution.
Definition 2.4. Let P = (D, ?) be a distribution over examples and ? 0 > 0. For functions ? :
X ? [0, 1], ? : X ? {?1, 1} ? [?1, 1], we say that a noise function ? : X ? [0, 1] is (?, ? 0 )uncorrelated with ? and ? over P if,





?(x, 1) ? ?(x, ?1)
ED
?(x) ? (1 ? 2(?(x) ? ?))  ? ? 0 .
 |?
2
5

In this definition (1 ? 2(?(x) ? ?)) is the expectation of {?1, 1} coin that is flipped with probability
?(x) ? ?, whereas (?(x, 1) ? ?(x, ?1))?(x) is the part of the query which measures the correlation
with the label. We now give an analogue of Theorem 2.2 for this more general setting.
Theorem 2.5. Let P = (D, ?) be a distribution over examples, ? : X ? [0, 1], ? : X ? {?1, 1} ?
[?1, 1] be a query and a filter functions, ? ? [0, 1/2), ? > 0 and ? be a noise function that is
(?, (1 ? 2?)? /4)-uncorrelated with ? and ? over P . There exists an active sampling algorithm that
given functions ? and ?, values ?, ?0 > 0, ? > 0, ? > 0, and access to samples from P ? , with
probability at least 1 ? ?, outputs a valid response to active statistical query (?, ?) with tolerance
parameters (?0 , ? ). The algorithm uses O(? ?2 (1 ? 2?)?2 log (1/?)) labeled examples from P ? and
O(?0?1 ? ?2 (1 ? 2?)?2 log (1/?)) unlabeled samples from D.
An immediate implication of Theorem 2.5 is that one can simulate an active SQ algorithm A using
examples corrupted by noise ? as long as ? is (?, (1 ? 2?)? /4)-uncorrelated with all A?s queries of
tolerance ? for some fixed ?.
2.3

Simple examples

Thresholds: We show that a classic example of active learning a threshold function on an interval
can be easily expressed using active SQs. For simplicity and without loss of generality we can
assume that the interval is [0, 1] and the distribution is uniform over it (as usual, we can bring the
distribution to be close enough to this form using unlabeled samples or target-independent queries).
Assume that we know that the threshold ? belongs to the interval [a, b] ? [0, 1]. We ask a query
?(x, `) = (`+1)/2 with filter ?(x) which is the indicator function of the interval [a, b] with tolerance
1/4 and filter tolerance b ? a. Let v be the response to the query. By definition, E[?(x)] = b ? a
and therefore we have that |v ? E[?(x, `) | x ? [a, b]]| ? 1/4. Note that,
E[?(x, `) | x ? [a, b]] = (b ? ?)/(b ? a) .
We can therefore conclude that (b ? ?)/(b ? a) ? [v ? 1/4, v + 1/4] which means that ? ?
[b ? (v + 1/4)(b ? a), b ? (v ? 1/4)(b ? a)] ? [a, b]. Note that the length of this interval is at most
(b ? a)/2. This means that after at most log2 (1/) + 1 iterations we will reach an interval [a, b]
of length at most . In each iteration only constant 1/4 tolerance is necessary and filter tolerance is
never below . A direct simulation of this algorithm can be done using log(1/) ? log(log(1/)/?)
?
labeled examples and O(1/)
? log(1/?) unlabeled samples.
Learning of thresholds can also be easily used to obtain a simple algorithm for learning axis-aligned
rectangles whose weight under the target distribution is not too small.
A2 : We now note that the general and well-studied A2 algorithm of [4] falls naturally into our
framework. At a high level, the A2 algorithm is an iterative, disagreement-based active learning
algorithm. It maintains a set of surviving classifiers Ci ? C, and in each round the algorithm asks
for the labels of a few random points that fall in the current region of disagreement of the surviving
classifiers. Formally, the region of disagreement DIS(Ci ) of a set of classifiers Ci is the of set of
instances x such that for each x ? DIS(Ci ) there exist two classifiers f, g ? Ci that disagree about
the label of x. Based on the queried labels, the algorithm then eliminates hypotheses that were
still under consideration, but only if it is statistically confident (given the labels queried in the last
round) that they are suboptimal. In essence, in each round A2 only needs to estimate the error rates
(of hypotheses still under consideration) under the conditional distribution of being in the region of
disagreement. This can be easily done via active statistical queries. Note that while the number of
active statistical queries needed to do this could be large, the number of labeled examples needed
to simulate these queries is essentially the same as the number of labeled examples needed by the
known A2 analyses [29]. While in general the required computation of the disagreement region
and manipulations of the hypothesis space cannot be done efficiently, efficient implementation is
possible in a number of simple cases such as when the VC dimension of the concept class is a
constant. It is not hard to see that in these cases the implementation can also be done using a
statistical algorithm.
6

3

Learning of halfspaces

In this section we outline our reduction from active learning to passive learning of homogeneous
linear separators based on the analysis of Balcan and Long [8]. Combining it with the SQ learning
algorithm for halfspaces by Dunagan and Vempala [24], we obtain the first efficient noise-tolerant
active learning of homogeneous halfspaces for any isotropic log-concave distribution. One of the
key point of this result is that it is relatively easy to harness the involved results developed for SQ
framework to obtain new active statistical algorithms.
Let Hd denote the concept class of all homogeneous halfspaces. Recall that a distribution over Rd
is log-concave if log f (?) is concave, where f is its associated density function. It is isotropic if its
mean is the origin and its covariance matrix is the identity. Log-concave distributions form a broad
class of distributions: for example, the Gaussian, Logistic, Exponential, and uniform distribution
over any convex set are log-concave distributions. Using results in [24] and properties of log-concave
distributions, we can show:
Theorem 3.1. There exists a SQ algorithm LearnHS that learns Hd to accuracy 1 ?  over any
distribution D|? , where D is an isotropic log-concave distribution and ? : Rd ? [0, 1] is a filter
function. Further LearnHS outputs a homogeneous halfspace, runs in time polynomial in d,1/
and log(1/?) and uses SQs of tolerance ? 1/poly(d, 1/, log(1/?)), where ? = ED [?(x)].
We now state the properties of our new algorithm formally.
Theorem 3.2. There exists an active SQ algorithm ActiveLearnHS-LogC (Algorithm 1) that
for any isotropic log-concave distribution D on Rd , learns Hd over D to accuracy 1 ?  in time
poly(d, log(1/)) and using active SQs of tolerance ? 1/poly(d, log(1/)) and filter tolerance ?().
Algorithm 1 ActiveLearnHS-LogC: Active SQ learning of homogeneous halfspaces over
isotropic log-concave densities
1: %% Constants c, C1 , C2 and C3 are determined by the analysis.
2: Run LearnHS with error C2 to obtain w0 .
3: for k = 1 to s = dlog2 (1/(c))e do
4:
Let bk?1 = C1 /2k?1
5:
Let ?k equal
Pthe indicator function of being within margin bk?1 of wk?1
6:
Let ?k = ( i?k ?i )/k
7:
Run LearnHS over Dk = D|?k with error C2 /k by using active queries with filter ?k and
filter tolerance C3  to obtain wk
8: end for
9: return ws
We remark that, as usual, we can first bring the distribution to an isotropic position by using target
independent queries to estimate the mean and the covariance matrix of the distribution. Therefore
our algorithm can be used to learn halfspaces over general log-concave densities as long as the target
halfspace passes through the mean of the density.
We can now apply Theorem 2.2 (or more generally Theorem 2.5) to obtain an efficient active learning algorithm for homogeneous halfspaces over log-concave densities in the presence of random
classification noise of known rate. Further since our algorithm relies on LearnHS which can also
be simulated when the noise rate is unknown (see Remark 2.3) we obtain an active algorithm which
does not require the knowledge of the noise rate.
Corollary 3.3. There exists a polynomial-time active learning algorithm that for any ? ? [0, 1/2),
learns Hd over any log-concave distributions with random classification noise of rate ? to error 
using poly(d, log(1/), 1/(1 ? 2?)) labeled examples and a polynomial number of unlabeled samples.
For the special case of the uniform distribution on the unit sphere (or, equivalently for our purposes,
unit ball) we give a substantially simpler and more efficient algorithm in terms of both sample and
computational complexity. This setting was previously studied in [3, 22]. The detailed presentation
of the technical ideas appears in the full version of the paper [5].
7

Theorem 3.4. There exists an active SQ algorithm ActiveLearnHS-U that learns Hd over
the uniform distribution on the (d ? 1)-dimensional
unit sphere to accuracy 1 ? , uses (d +
?
1) log(1/) active SQs with tolerance of ?(1/ d) and filter tolerance of ?(1/) and runs in time
d ? poly(log (d/)).

4

Differentially-private active learning

In this section we show that active SQ learning algorithms can also be used to obtain differentially
private active learning algorithms. Formally, for some domain X ? Y , we will call S ? X ? Y a
database. Databases S, S 0 ? X ?Y are adjacent if one can be obtained from the other by modifying
a single element. Here we will always have Y = {?1, 1}. In the following, A is an algorithm that
takes as input a database D and outputs an element of some finite set R.
Definition 4.1 (Differential privacy [25]). A (randomized) algorithm A : 2X?Y ? R is ?differentially private if for all r ? R and every pair of adjacent databases S, S 0 , we have
Pr[A(S) = r] ? e Pr[A(S 0 ) = r].
Here we consider algorithms that operate on S in an active way. That is the learning algorithm
receives the unlabeled part of each point in S as an input and can only obtain the label of a point
upon request. The total number of requests is the label complexity of the algorithm.
Theorem 4.2. Let A be an algorithm that learns a class of functions H to accuracy 1 ?  over distribution D using M1 active SQs of tolerance ? and filter tolerance ?0 and M2 target-independent
queries of tolerance ?u . There exists a learning algorithm A0 that given ? > 0, ? > 0 and ac1
tive access to database S ? X ? {?1, 1} is ?-differentially private and uses at most O([ M
?? +
M1
M1
M1
M2
M2
? 2 ] log(M1 /?)) labels. Further, for some n = O([ ??0 ? + ?0 ? 2 + ??u + ?u2 ] log((M1 + M2 )/?)),
if S consists of at least n examples drawn randomly from D then with, probability at least 1 ? ?, A0
outputs a hypothesis with accuracy ? 1 ?  (relative to distribution D). The running time of A0 is
the same as the running time of A plus O(n).
An immediate consequence of Theorem 4.2 is that for learning of homogeneous halfspaces over
uniform or log-concave distributions we can obtain differential privacy while essentially preserving
the label complexity. For example, by combining Theorems 4.2 and 3.4, we can efficiently and
differentially-privately learn homogeneous halfspaces?under the uniform distribution with privacy
parameter ? and error parameter  by using only O(d d log(1/))/? + d2 log(1/)) labels. However, it is known that any 
passive learning algorithm, even ignoring
? privacy considerations and noise
requires ? d + 1 log 1? labeled examples. So for ? ? 1/ d and small enough  we get better
label complexity.

5

Discussion

Our work suggests that, as in passive learning, active statistical algorithms might be essentially
as powerful as example-based efficient active learning algorithms. It would be interesting to find
more general evidence supporting this claim or, alternatively, a counterexample. A nice aspect of
(passive) statistical learning algorithms is that it is possible to prove unconditional lower bounds on
such algorithms using SQ dimension [13] and its extensions. It would be interesting to develop an
active analogue of these techniques and give meaningful lower bounds based on them.
Acknowledgments We thank Avrim Blum and Santosh Vempala for useful discussions. This work
was supported in part by NSF grants CCF-0953192 and CCF-1101215, AFOSR grant FA9550-091-0538, ONR grant N00014-09-1-0751, and a Microsoft Research Faculty Fellowship.

References
[1] D. Angluin and P. Laird. Learning from noisy examples. Machine Learning, 2:343?370, 1988.
[2] J. Aslam and S. Decatur. Specification and simulation of statistical query algorithms for efficiency and
noise tolerance. JCSS, 56:191?208, 1998.
[3] M. Balcan, A. Broder, and T. Zhang. Margin based active learning. In COLT, pages 35?50, 2007.

8

[4] M. F. Balcan, A. Beygelzimer, and J. Langford. Agnostic active learning. In ICML, 2006.
[5] M. F. Balcan and V. Feldman. Statistical active learning algorithms, 2013. ArXiv:1307.3102.
[6] M.-F. Balcan and S. Hanneke. Robust interactive learning. In COLT, 2012.
[7] M.-F. Balcan, S. Hanneke, and J. Wortman. The true sample complexity of active learning. In COLT,
2008.
[8] M.-F. Balcan and P. M. Long. Active and passive learning of linear separators under log-concave distributions. JMLR - COLT proceedings (to appear), 2013.
[9] A. Beygelzimer, S. Dasgupta, and J. Langford. Importance weighted active learning. In ICML, pages
49?56, 2009.
[10] A. Beygelzimer, D. Hsu, J. Langford, and T. Zhang. Agnostic active learning without constraints. In
NIPS, 2010.
[11] A. Blum, C. Dwork, F. McSherry, and K. Nissim. Practical privacy: the SuLQ framework. In Proceedings
of PODS, pages 128?138, 2005.
[12] A. Blum, A. Frieze, R. Kannan, and S. Vempala. A polynomial time algorithm for learning noisy linear
threshold functions. Algorithmica, 22(1/2):35?52, 1997.
[13] A. Blum, M. Furst, J. Jackson, M. Kearns, Y. Mansour, and S. Rudich. Weakly learning DNF and characterizing statistical query learning using Fourier analysis. In STOC, pages 253?262, 1994.
[14] A. Blumer, A. Ehrenfeucht, D. Haussler, and M. Warmuth. Learnability and the Vapnik-Chervonenkis
dimension. Journal of the ACM, 36(4):929?965, 1989.
[15] R. Castro and R. Nowak. Minimax bounds for active learning. In COLT, 2007.
[16] N. Cesa-Bianchi, C. Gentile, and L. Zaniboni. Learning noisy linear classifiers via adaptive and selective
sampling. Machine Learning, 2010.
[17] C. Chu, S. Kim, Y. Lin, Y. Yu, G. Bradski, A. Ng, and K. Olukotun. Map-reduce for machine learning on
multicore. In Proceedings of NIPS, pages 281?288, 2006.
[18] S. Dasgupta. Coarse sample complexity bounds for active learning. In NIPS, volume 18, 2005.
[19] S. Dasgupta. Active learning. Encyclopedia of Machine Learning, 2011.
[20] S. Dasgupta and D. Hsu. Hierarchical sampling for active learning. In ICML, pages 208?215, 2008.
[21] S. Dasgupta, D.J. Hsu, and C. Monteleoni. A general agnostic active learning algorithm. NIPS, 20, 2007.
[22] S. Dasgupta, A. Tauman Kalai, and C. Monteleoni. Analysis of perceptron-based active learning. Journal
of Machine Learning Research, 10:281?299, 2009.
[23] O. Dekel, C. Gentile, and K. Sridharan. Selective sampling and active learning from single and multiple
teachers. JMLR, 2012.
[24] J. Dunagan and S. Vempala. A simple polynomial-time rescaling algorithm for solving linear programs.
In STOC, pages 315?320, 2004.
[25] C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data analysis.
In TCC, pages 265?284, 2006.
[26] V. Feldman. A complete characterization of statistical query learning with applications to evolvability.
Journal of Computer System Sciences, 78(5):1444?1459, 2012.
[27] Y. Freund, H.S. Seung, E. Shamir, and N. Tishby. Selective sampling using the query by committee
algorithm. Machine Learning, 28(2-3):133?168, 1997.
[28] A. Gonen, S. Sabato, and S. Shalev-Shwartz. Efficient pool-based active learning of halfspaces. In ICML,
2013.
[29] S. Hanneke. A bound on the label complexity of agnostic active learning. In ICML, 2007.
[30] M. Kearns. Efficient noise-tolerant learning from statistical queries. JACM, 45(6):983?1006, 1998.
[31] V. Koltchinskii. Rademacher complexities and bounding the excess risk in active learning. JMLR,
11:2457?2485, 2010.
[32] L. Lov?asz and S. Vempala. The geometry of logconcave functions and sampling algorithms. Random
Struct. Algorithms, 30(3):307?358, 2007.
[33] A. McCallum and K. Nigam. Employing EM in pool-based active learning for text classification. In
ICML, pages 350?358, 1998.
[34] M. Raginsky and A. Rakhlin. Lower bounds for passive and active learning. In NIPS, pages 1026?1034,
2011.
[35] L. G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134?1142, 1984.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 5438-optimal-regret-minimization-in-posted-price-auctions-with-strategic-buyers.pdf

Optimal Regret Minimization in Posted-Price
Auctions with Strategic Buyers

Mehryar Mohri
Courant Institute and Google Research
251 Mercer Street
New York, NY 10012

? Medina
Andres Munoz
Courant Institute
251 Mercer Street
New York, NY 10012

mohri@cims.nyu.edu

munoz@cims.nyu.edu

Abstract
We study revenue optimization learning algorithms for posted-price auctions with
strategic buyers. We analyze a very broad family of monotone regret minimization
algorithms for this problem, which includes the previously best known algorithm,
and show
? that no algorithm in that family admits a strategic regret more favorable
than ?( T ). We then introduce a new algorithm that achieves a strategic regret
differing from the lower bound only by a factor in O(log T ), an exponential improvement upon the previous best algorithm. Our new algorithm admits a natural
analysis and simpler proofs, and the ideas behind its design are general. We also
report the results of empirical evaluations comparing our algorithm with the previous state of the art and show a consistent exponential improvement in several
different scenarios.

1

Introduction

Auctions have long been an active area of research in Economics and Game Theory [Vickrey, 2012,
Milgrom and Weber, 1982, Ostrovsky and Schwarz, 2011]. In the past decade, however, the advent
of online advertisement has prompted a more algorithmic study of auctions, including the design of
learning algorithms for revenue maximization for generalized second-price auctions or second-price
auctions with reserve [Cesa-Bianchi et al., 2013, Mohri and Mu?noz Medina, 2014, He et al., 2013].
These studies have been largely motivated by the widespread use of AdExchanges and the vast
amount of historical data thereby collected ? AdExchanges are advertisement selling platforms using second-price auctions with reserve price to allocate advertisement space. Thus far, the learning
algorithms proposed for revenue maximization in these auctions critically rely on the assumption
that the bids, that is, the outcomes of auctions, are drawn i.i.d. according to some unknown distribution. However, this assumption may not hold in practice. In particular, with the knowledge that a
revenue optimization algorithm is being used, an advertiser could seek to mislead the publisher by
under-bidding. In fact, consistent empirical evidence of strategic behavior by advertisers has been
found by Edelman and Ostrovsky [2007]. This motivates the analysis presented in this paper of the
interactions between sellers and strategic buyers, that is, buyers that may act non-truthfully with the
goal of maximizing their surplus.
The scenario we consider is that of posted-price auctions, which, albeit simpler than other mechanisms, in fact matches a common situation in AdExchanges where many auctions admit a single
bidder. In this setting, second-price auctions with reserve are equivalent to posted-price auctions: a
seller sets a reserve price for a good and the buyer decides whether or not to accept it (that is to bid
higher than the reserve price). In order to capture the buyer?s strategic behavior, we will analyze an
online scenario: at each time t, a price pt is offered by the seller and the buyer must decide to either
accept it or leave it. This scenario can be modeled as a two-player repeated non-zero sum game with
1

incomplete information, where the seller?s objective is to maximize his revenue, while the advertiser
seeks to maximize her surplus as described in more detail in Section 2.
The literature on non-zero sum games is very rich [Nachbar, 1997, 2001, Morris, 1994], but much of
the work in that area has focused on characterizing different types of equilibria, which is not directly
relevant to the algorithmic questions arising here. Furthermore, the problem we consider admits a
particular structure that can be exploited to design efficient revenue optimization algorithms.
From the seller?s perspective, this game can also be viewed as a bandit problem [Kuleshov and Precup, 2010, Robbins, 1985] since only the revenue (or reward) for the prices offered is accessible to
the seller. Kleinberg and Leighton [2003] precisely studied this continuous bandit setting under the
assumption of an oblivious buyer, that is, one that does not exploit the seller?s behavior (more precisely, the authors assume that at each round the seller interacts with a different buyer). The authors
presented a tight regret bound of ?(log log T ) for the scenario of a buyer holding a fixed valuation
2
and a regret bound of O(T 3 ) when facing an adversarial buyer by using an elegant reduction to a
discrete bandit problem. However, as argued by Amin et al. [2013], when dealing with a strategic
buyer, the usual definition of regret is no longer meaningful. Indeed, consider the following example: let the valuation of the buyer be given by v ? [0, 1] and assume that an algorithm with sublinear
regret such as Exp3 [Auer et al., 2002b] or UCB [Auer et al., 2002a] is used for T rounds by the
seller. A possible strategy for the buyer, knowing the seller?s algorithm, would be to accept prices
only if they are smaller than some small value , certain that the seller would eventually learn to offer
only prices less than . If   v, the buyer would considerably boost her surplus while, in theory,
the seller would have not incurred a large regret since in hindsight, the best fixed strategy would
have been to offer price  for all rounds. This, however is clearly not optimal for the seller. The
stronger notion of policy regret introduced by Arora et al. [2012] has been shown to be the appropriate one for the analysis of bandit problems with adaptive adversaries. However, for the example
just described, a sublinear policy regret can be similarly achieved. Thus, this notion of regret is also
not the pertinent one for the study of our scenario.
We will adopt instead the definition of strategic-regret, which was introduced by Amin et al. [2013]
precisely for the study of this problem. This notion of regret also matches the concept of learning
loss introduced by [Agrawal, 1995] when facing an oblivious adversary. Using this definition, Amin
et al. [2013] presented both upper and lower bounds for the regret of a seller facing a strategic
buyer and showed that the buyer?s surplus must be discounted over time in order to be able to
achieve sublinear regret?(see Section 2). However, the gap between the upper and lower bounds
they presented is in O( T ). In the following, we analyze a very broad family of monotone regret
minimization algorithms for this problem (Section 3), which includes the algorithm of Amin et al.
[2013],
? and show that no algorithm in that family admits a strategic regret more favorable than
?( T ). Next, we introduce a nearly-optimal algorithm that achieves a strategic regret differing
from the lower bound at most by a factor in O(log T ) (Section 4). This represents an exponential
improvement upon the existing best algorithm for this setting. Our new algorithm admits a natural
analysis and simpler proofs. A key idea behind its design is a method deterring the buyer from lying,
that is rejecting prices below her valuation.

2

Setup

We consider the following game played by a buyer and a seller. A good, such as an advertisement
space, is repeatedly offered for sale by the seller to the buyer over T rounds. The buyer holds a
private valuation v ? [0, 1] for that good. At each round t = 1, . . . , T , a price pt is offered by the
seller and a decision at ? {0, 1} is made by the buyer. at takes value 1 when the buyer accepts
to buy at that price, 0 otherwise. We will say that a buyer lies whenever at = 0 while pt < v.
At the beginning of the game, the algorithm A used by the seller to set prices is announced to the
buyer. Thus, the buyer plays strategically against this algorithm. The knowledge of A is a standard
assumption in mechanism design and also matches the practice in AdExchanges.
For any ? ? (0, 1), define the discounted surplus of the buyer as follows:
Sur(A, v) =

T
X

? t?1 at (v ? pt ).

t=1

2

(1)

The value of the discount factor ? indicates the strength of the preference of the buyer for current
surpluses versus future ones. The performance of a seller?s algorithm is measured by the notion of
strategic-regret [Amin et al., 2013] defined as follows:
Reg(A, v) = T v ?

T
X

at pt .

(2)

t=1

The buyer?s objective is to maximize his discounted surplus, while the seller seeks to minimize his
regret. Note that, in view of the discounting factor ?, the buyer is not fully adversarial. The problem
consists of designing algorithms achieving sublinear strategic regret (that is a regret in o(T )).
The motivation behind the definition of strategic-regret is straightforward: a seller, with access to
the buyer?s valuation, can set a fixed price for the good  close to this value. The buyer, having no
control on the prices offered, has no option but to accept this price in order to optimize his utility.
The revenue per round of the seller is therefore v?. Since there is no scenario where higher revenue
can be achieved, this is a natural setting to compare the performance of our algorithm.
To gain more intuition about the problem, let us examine some of the complications arising when
dealing with a strategic buyer. Suppose the seller attempts to learn the buyer?s valuation v by performing a binary search. This would be a natural algorithm when facing a truthful buyer. However,
in view of the buyer?s knowledge of the algorithm, for ?  0, it is in her best interest to lie on the
initial rounds, thereby quickly, in fact exponentially, decreasing the price offered by the seller. The
seller would then incur an ?(T ) regret. A binary search approach is therefore ?too aggressive?. Indeed, an untruthful buyer can manipulate the seller into offering prices less than v/2 by lying about
her value even just once! This discussion suggests following a more conservative approach. In the
next section, we discuss a natural family of conservative algorithms for this problem.

3

Monotone algorithms

The following conservative pricing strategy was introduced by Amin et al. [2013]. Let p1 = 1
and ? < 1. If price pt is rejected at round t, the lower price pt+1 = ?pt is offered at the next
round. If at any time price pt is accepted, then this price is offered for all the remaining rounds. We
will denote this algorithm by monotone. The motivation behind its design is clear: for a suitable
choice of ?, the seller can slowly decrease the prices offered, thereby pressing the buyer to reject
many prices
? (which is not convenient for her) before obtaining a favorable price. The authors present
an O(T? T ) regret bound for this algorithm, with T? =
?1/(1 ? ?). A more careful analysis shows
p
that this bound can be further tightened to O( T? T + T ) when the discount factor ? is known to
the seller.
Despite its sublinear regret, the monotone algorithm remains sub-optimal for certain choices of
?. Indeed, consider a scenario with ?  1. For this setting, the buyer would no longer have an
incentive to lie, thus, an algorithm such as binary search would achieve logarithmic
regret, while the
?
regret achieved by the monotone algorithm is only guaranteed to be in O( T ).
One may argue that the monotone algorithm is too specific since it admits a single parameter
? and that perhaps a more complex algorithm with the same monotonic idea could achieve a more
favorable regret. Let us therefore analyze a generic monotone algorithm Am defined by Algorithm 1.
Definition 1. For any buyer?s valuation v ? [0, 1], define the acceptance time ?? = ?? (v) as the
first time a price offered by the seller using algorithm Am is accepted.
Proposition 1. For any decreasing sequence of prices (pt )Tt=1 , there exists a truthful buyer with
valuation v0 such that algorithm Am suffers regret of at least
q
?
1
Reg(Am , v0 ) ?
T ? T.
4
?
Proof. By definition of the regret,
? ?? )(v ? p?? ). We can
? we have Reg(Am , v) = v?? + (T ?
?
consider two cases: ? (v0 ) > T for some v0 ??[1/2, 1] ?
and ? (v) ? T for every v ? [1/2, 1].
In the former case, we have Reg(Am , v0 ) ? v0 T ? 12 T , which implies the statement of the
proposition. Thus, we can assume the latter condition.

3

Algorithm 1 Family of monotone algorithms.

Algorithm 2 Definition of Ar .
n = the root of T (T )
while Offered prices less than T do
Offer price pn
if Accepted then
n = r(n)
else
Offer price pn for r rounds
n = l(n)
end if
end while

Let p1 = 1 and pt ? pt?1 for t = 2, . . . T .
t?1
p ? pt
Offer price p
while (Buyer rejects p) and (t < T ) do
t?t+1
p ? pt
Offer price p
end while
while (t < T ) do
t?t+1
Offer price p
end while

Let v be uniformly distributed over [ 12 , 1]. In view of Lemma 4 (see Appendix 8.1), we have
?
?
1
1
T? T
?
?
?
?
E[v? ] + E[(T ? ? )(v ? p?? )] ? E[? ] + (T ? T )E[(v ? p?? )] ? E[? ] +
.
2
2
32E[?? ]
? ?
T? T
The right-hand side is minimized for E[?? ] =
. Plugging in this value yields
4
? ?
? ?
T? T
T? T
E[Reg(Am , v)] ?
,
which
implies
the
existence
of
v
with
Reg(A
,
v
)
?
.
0
m 0
4
4
?
We have thus shown that any monotone algorithm Am suffers a regret of at least ?( T ), even when
facing a truthful buyer. A tighter lower bound can be given under a mild condition on the prices
offered.
Definition 2. A sequence (pt )Tt=1 is said to be convex if it verifies pt ? pt+1 ? pt+1 ? pt+2 for
t = 1, . . . , T ? 2.
An instance of a convex sequence is given by the prices offered by the monotone algorithm. A
seller offering prices forming a decreasing convex sequence seeks to control the number of lies of
the buyer by slowly reducing prices. The following proposition gives a lower bound on the regret of
any algorithm in this family.
Proposition 2. Let (pt )Tt=1 be a decreasing convex sequence of prices. There exists a valuation
v0
p
for the buyer such that the regret of the monotone algorithm defined by these prices is ?( T C? +
?
?
T ), where C? = 2(1??)
.
The full proof of this proposition is given in Appendix 8.1. The proposition shows that when the
discount factor ? is known, the monotone algorithm is in fact asymptotically optimal in its class.
The results just presented suggest that the dependency on T cannot be improved by any monotone
algorithm. In some sense, this family of algorithms is ?too conservative?. Thus, to achieve a more
favorable regret guarantee, an entirely different algorithmic idea must be introduced. In the next
section, we describe a new algorithm that achieves a substantially more advantageous strategic regret
by combining the fast convergence properties of a binary search-type algorithm (in a truthful setting)
with a method penalizing untruthful behaviors of the buyer.

4

A nearly optimal algorithm

Let A be an algorithm for revenue optimization used against a truthful buyer. Denote by T (T ) the
tree associated to A after T rounds. That is, T (T ) is a full tree of height T with nodes n ? T (T )
labeled with the prices pn offered by A. The right and left children of n are denoted by r(n) and
l(n) respectively. The price offered when pn is accepted by the buyer is the label of r(n) while the
price offered by A if pn is rejected is the label of l(n). Finally, we will denote the left and right
subtrees rooted at node n by L (n) and R(n) respectively. Figure 1 depicts the tree generated by an
algorithm proposed by Kleinberg and Leighton [2003], which we will describe later.
4

1/2

1/16

1/2

1/4

3/4

5/16

9/16

1/4

3/4

13/16

(a)

13/16

(b)

Figure 1: (a) Tree T (3) associated to the algorithm proposed in [Kleinberg and Leighton, 2003]. (b) Modified
tree T 0 (3) with r = 2.
Since the buyer holds a fixed valuation, we will consider algorithms that increase prices only after a
price is accepted and decrease it only after a rejection. This is formalized in the following definition.
Definition 3. An algorithm A is said to be consistent if maxn0 ?L (n) pn0 ? pn ? minn0 ?R(n) pn0
for any node n ? T (T ).
For any consistent algorithm A, we define a modified algorithm Ar , parametrized by an integer
r ? 1, designed to face strategic buyers. Algorithm Ar offers the same prices as A, but it is defined
with the following modification: when a price is rejected by the buyer, the seller offers the same
price for r rounds. The pseudocode of Ar is given in Algorithm 2. The motivation behind the
modified algorithm is given by the following simple observation: a strategic buyer will lie only if
she is certain that rejecting a price will boost her surplus in the future. By forcing the buyer to reject
a price for several rounds, the seller ensures that the future discounted surplus will be negligible,
thereby coercing the buyer to be truthful.
We proceed to formally analyze algorithm Ar . In particular, we will quantify the effect of the
parameter r on the choice of the buyer?s strategy. To do so, a measure of the spread of the prices
offered by Ar is needed.
Definition 4. For any node n ? T (T ) define the right increment of n as ?nr := pr(n) ? pn . Similarly,
define its left increment to be ?nl := maxn0 ?L (n) pn ? pn0 .
The prices offered by Ar define a path in T (T ). For each node in this path, we can define time
t(n) to be the number of rounds needed for this node to be reached by Ar . Note that, since r may
be greater than 1, the path chosen by Ar might not necessarily reach the leaves of T (T ). Finally,
let S : n 7? S(n) be the function representing the surplus obtained by the buyer when playing an
optimal strategy against Ar after node n is reached.
Lemma 1. The function S satisfies the following recursive relation:
S(n) = max(? t(n)?1 (v ? pn ) + S(r(n)), S(l(n))).

(3)

Proof. Define a weighted tree T 0 (T ) ? T (T ) of nodes reachable by algorithm Ar . We assign
weights to the edges in the following way: if an edge on T 0 (T ) is of the form (n, r(n)), its weight
is set to be ? t(n)?1 (v ? pn ), otherwise, it is set to 0. It is easy to see that the function S evaluates
the weight of the longest path from node n to the leafs of T 0 (T ). It thus follows from elementary
graph algorithms that equation (3) holds.
The previous lemma immediately gives us necessary conditions for a buyer to reject a price.
Proposition 3. For any reachable node n, if price pn is rejected by the buyer, then the following
inequality holds:
?r
v ? pn <
(? l + ??nr ).
(1 ? ?)(1 ? ? r ) n
Proof. A direct implication of Lemma 1 is that price pn will be rejected by the buyer if and only if
? t(n)?1 (v ? pn ) + S(r(n)) < S(l(n)).
5

(4)

However, by definition, the buyer?s surplus obtained by following any path in R(n) is bounded
above by S(r(n)). In particular, this is true for the path which rejects pr(n) and accepts every price
PT
afterwards. The surplus of this path is given by t=t(n)+r+1 ? t?1 (v ? pbt ) where (b
pt )Tt=t(n)+r+1
are the prices the seller would offer if price pr(n) were rejected. Furthermore, since algorithm Ar is
consistent, we must have pbt ? pr(n) = pn + ?nr . Therefore, S(r(n)) can be bounded as follows:
S(r(n)) ?

T
X

? t?1 (v ? pn ? ?nr ) =

t=t(n)+r+1

? t(n)+r ? ? T
(v ? pn ? ?nr ).
1??

(5)

We proceed to upper bound S(l(n)). Since pn ? p0n ? ?nl for all n0 ? L (n), v ? pn0 ? v ? pn + ?nl
and
T
X
? t(n)+r?1 ? ? T
(v ? pn + ?nl ).
(6)
S(l(n)) ?
? t?1 (v ? pn + ?nl ) =
1
?
?
t=t +r
n

Combining inequalities (4), (5) and (6) we conclude that
? t(n)?1 (v ? pn ) +
?

? t(n)+r ? ? T
? t(n)+r?1 ? ? T
(v ? pn ? ?nr ) ?
(v ? pn + ?nl )
1??
1??


? r ?nl + ? r+1 ?nr ? ? T ?t(n)+1 (?nr + ?nl )
? r+1 ? ? r
?
(v ? pn ) 1 +
1??
1??

? r (?nl + ??nr )
.
1??
Rearranging the terms in the above inequality yields the desired result.
?

(v ? pn )(1 ? ? r ) ?

Let us consider the following instantiation of algorithm A introduced in [Kleinberg and Leighton,
2003]. The algorithm keeps track of a feasible interval [a, b] initialized to [0, 1] and an increment
parameter  initialized to 1/2. The algorithm works in phases. Within each phase, it offers prices
a + , a + 2, . . . until a price is rejected. If price a + k is rejected, then a new phase starts with
the feasible interval set to [a + (k ? 1), a + k] and the increment parameter set to 2 . This process
continues until b ? a < 1/T at which point the last phase starts and price a is offered for the
remaining rounds. It is not hard to see that the number of phases needed by the algorithm is less
than dlog2 log2 T e+1. A more surprising fact is that this algorithm has been shown to achieve regret
O(log log T ) when the seller faces a truthful buyer. We will show that the modification Ar of this
algorithm admits a particularly favorable regret bound. We will call this algorithm PFSr (penalized
fast search algorithm).
Proposition 4. For any value of v ? [0, 1] and any ? ? (0, 1), the regret of algorithm PFSr admits
the following upper bound:
(1 + ?)? r T
.
(7)
Reg(PFSr , v) ? (vr + 1)(dlog2 log2 T e + 1) +
2(1 ? ?)(1 ? ? r )
Note that for r = 1 and ? ? 0 the upper bound coincides with that of [Kleinberg and Leighton,
2003].
Proof. Algorithm PFSr can accumulate regret in two ways: the price offered pn is rejected, in which
case the regret is v, or the price is accepted and its regret is v ? pn .
Let K = dlog2 log2 T e + 1 be the number of phases run by algorithm PFSr . Since at most K
different prices are rejected by the buyer (one rejection per phase) and each price must be rejected
for r rounds, the cumulative regret of all rejections is upper bounded by vKr.
The second type of regret can also be bounded straightforwardly. For any phase i, let i and [ai , bi ]
denote the corresponding search parameter and feasible interval respectively. If v ? [ai , bi ],?the
regret accrued in the case where the buyer accepts a price in this interval is bounded
? by bi ?ai = i .
If, on the other hand v ? bi , then it readily follows that v ? pn < v ? bi + i for all prices pn
offered in phase i. Therefore, the regret obtained in acceptance rounds is bounded by
K
K

X
?  X
Ni (v ? bi )1v>bi + i ?
(v ? bi )1v>bi Ni + K,
i=1

i=1

6

where Ni ?

?1
i

denotes the number of prices offered during the i-th round.

Finally, notice that, in view of the algorithm?s definition, every bi corresponds to a rejected price.
Thus, by Proposition 3, there exist nodes ni (not necessarily distinct) such that pni = bi and
v ? bi = v ? pni ?

?r
(? l + ??nr i ).
(1 ? ?)(1 ? ? r ) ni

It is immediate that ?nr ? 1/2 and ?nl ? 1/2 for any node n, thus, we can write
K
X

K

(v ? bi )1v>bi Ni ?

i=1

X
? r (1 + ?)
? r (1 + ?)
N
?
T.
i
2(1 ? ?)(1 ? ? r ) i=1
2(1 ? ?)(1 ? ? r )

The last inequality holds since at most T prices are offered by our algorithm. Combining the bounds
for both regret types yields the result.
When an upper bound on the discount factor ? is known to the seller, he can leverage this information
and optimize upper bound (7) with respect to the parameter r.
l
m
?0r T
Theorem 1. Let 1/2 < ? < ?0 < 1 and r? = argminr?1 r + (1??0 )(1??
r ) . For any v ? [0, 1],
0
if T > 4, the regret of PFSr? satisfies
Reg(PFSr? , v) ? (2v?0 T?0 log cT + 1 + v)(log2 log2 T + 1) + 4T?0 ,
where c = 4 log 2.
The proof of this theorem is fairly technical and is deferred to the Appendix. The theorem helps
us define conditions under which logarithmic regret can be achieved. Indeed, if ?0 = e?1/ log T =
O(1 ? log1 T ), using the inequality e?x ? 1 ? x + x2 /2 valid for all x > 0 we obtain
log2 T
1
?
? log T.
1 ? ?0
2 log T ? 1
It then follows from Theorem 1 that
Reg(PFSr? , v) ? (2v log T log cT + 1 + v)(log2 log2 T + 1) + 4 log T.
Let us compare the regret bound given by Theorem 1 with the one given by Amin et al. [2013]. The
above discussion shows that for certain values of ?, an exponentially better regret can be achieved
by our algorithm. It can be argued that the knowledge of an upper bound on ?
? is required, whereas
this is not needed for the monotone algorithm. However, if ? > 1 ? 1/ T , the regret bound
on monotone is super-linear, and therefore uninformative.
Thus, in order to properly compare
?
both algorithms, we may
assume
that
?
<
1
?
1/
T
in
which
case, by Theorem 1, the regret
?
of our algorithm is O( T log T ) whereas only linear regret
can
be
guaranteed by the monotone
?
p
algorithm. Even under the more favorable bound of O( T? T + T ), for any ? < 1 and ? <
?+1
1 ? 1/T ? , the monotone algorithm will achieve regret O(T 2 ) while a strictly better regret
O(T ? log T log log T ) is attained by ours.

5

Lower bound

The following lower bounds have been derived in previous work.
Theorem 2 ([Amin et al., 2013]). Let ? > 0 be fixed. For any algorithm A, there exists a valuation
1
v for the buyer such that Reg(A, v) ? 12
T? .
This theorem is in fact given for the stochastic setting where the buyer?s valuation is a random
variable taken from some fixed distribution D. However, the proof of the theorem selects D to be a
point mass, therefore reducing the scenario to a fixed priced setting.
Theorem 3 ( [Kleinberg and Leighton, 2003]). Given any algorithm A to be played against a
truthful buyer, there exists a value v ? [0, 1] such that Reg(A, v) ? C log log T for some universal
constant C.
7

? = .95, v = .75
2500

PFS
1000 mon

2000
Regret

Regret

800
600
400

? = .75, v = .25

PFS
mon

120

80

80

PFS
100 mon

1500
1000

40
20

20

0

0

0

2.5

3

3.5

4

4.5

2

2.5

3

3.5

4

4.5

Number of rounds (log-scale)

60

40

500

2

PFS
100 mon

60

200

Number of rounds (log-scale)

? = .80, v = .25

120

Regret

1200

Regret

? = .85, v = .75

2

2.5

3

3.5

4

4.5

Number of rounds (log-scale)

0

2

2.5

3

3.5

4

4.5

Number of rounds (log-scale)

Figure 2: Comparison of the monotone algorithm and PFSr for different choices of ? and v. The regret of
each algorithm is plotted as a function of the number rounds when ? is not known to the algorithms (first two
figures) and when its value is made accessible to the algorithms (last two figures).

Combining these results leads immediately to the following.
Corollary 1. Given
 any algorithm A, there exists a buyer?s valuation v ? [0, 1] such that
Reg(A, v) ? max

1
12 T? , C

log log T , for a universal constant C.

We now compare the upper bounds given in the previous section with the bound of Corollary 1. For
? > 1/2, we have Reg(PFSr , v) = O(T? log T log log T ). On the other hand, for ? ? 1/2, we may
choose r = 1, in which case, by Proposition 4, Reg(PFSr , v) = O(log log T ). Thus, the upper and
lower bounds match up to an O(log T ) factor.

6

Empirical results

In this section, we present the result of simulations comparing the monotone algorithm and our
algorithm PFSr . The experiments were carried out as follows: given a buyer?s valuation v, a discrete
set of false valuations vb were selected out of the set {.03, .06, . . . , v}. Both algorithms were run
against a buyer making the seller believe her valuation is vb instead of v. The value of vb achieving
the best utility for the buyer was chosen and the regret for both algorithms is reported in Figure 2.
We considered two sets of experiments. First, the value of parameter ? was left unknown to both
algorithms and the value of r was set to log(T ). This choice is motivated by the discussion following
Theorem 1 since, for large values of T , we can expect to achieve logarithmic regret. The first two
plots (from left to right) in Figure 2 depict these results. The apparent stationarity in the regret of
PFSr is just a consequence of the scale of the plots as the regret is in fact growing as log(T ). For
the second set of experiments, we allowed access to the parameter ? to both algorithms. The value
of r was chosen optimally
based on the resultsp
of Theorem
? 1 and the parameter ? of monotone
p
was set to 1 ? 1/ T T? to ensure regret in O( T T? + T ). It is worth noting that even though
our algorithm was designed under the assumption of some knowledge about the value of ?, the
experimental results show that an exponentially better performance over the monotone algorithm
is still attainable and in fact the performances of the optimized and unoptimized versions of our
algorithm are comparable. A more comprehensive series of experiments is presented in Appendix 9.

7

Conclusion

We presented a detailed analysis of revenue optimization algorithms against strategic buyers. In
doing so, we reduced the gap between upper and lower bounds on strategic regret to a logarithmic
factor. Furthermore, the algorithm we presented is simple to analyze and reduces to the truthful
scenario in the limit of ? ? 0, an important property that previous algorithms did not admit. We
believe that our analysis helps gain a deeper understanding of this problem and that it can serve as a
tool for studying more complex scenarios such as that of strategic behavior in repeated second-price
auctions, VCG auctions and general market strategies.

Acknowledgments
We thank Kareem Amin, Afshin Rostamizadeh and Umar Syed for several discussions about the
topic of this paper. This work was partly funded by the NSF award IIS-1117591.
8

References
R. Agrawal. The continuum-armed bandit problem. SIAM journal on control and optimization, 33
(6):1926?1951, 1995.
K. Amin, A. Rostamizadeh, and U. Syed. Learning prices for repeated auctions with strategic buyers.
In Proceedings of NIPS, pages 1169?1177, 2013.
R. Arora, O. Dekel, and A. Tewari. Online bandit learning against an adaptive adversary: from
regret to policy regret. In Proceedings of ICML, 2012.
P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem.
Machine Learning, 47(2-3):235?256, 2002a.
P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. The nonstochastic multiarmed bandit
problem. SIAM J. Comput., 32(1):48?77, 2002b.
N. Cesa-Bianchi, C. Gentile, and Y. Mansour. Regret minimization for reserve prices in second-price
auctions. In Proceedings of SODA, pages 1190?1204, 2013.
B. Edelman and M. Ostrovsky. Strategic bidder behavior in sponsored search auctions. Decision
Support Systems, 43(1), 2007.
D. He, W. Chen, L. Wang, and T. Liu. A game-theoretic machine learning approach for revenue
maximization in sponsored search. In Proceedings of IJCAI, pages 206?213, 2013.
R. D. Kleinberg and F. T. Leighton. The value of knowing a demand curve: Bounds on regret for
online posted-price auctions. In Proceedings of FOCS, pages 594?605, 2003.
V. Kuleshov and D. Precup. Algorithms for the multi-armed bandit problem. Journal of Machine
Learning, 2010.
P. Milgrom and R. Weber. A theory of auctions and competitive bidding. Econometrica: Journal of
the Econometric Society, pages 1089?1122, 1982.
M. Mohri and A. Mu?noz Medina. Learning theory and algorithms for revenue optimization in
second-price auctions with reserve. In Proceedings of ICML, 2014.
P. Morris. Non-zero-sum games. In Introduction to Game Theory, pages 115?147. Springer, 1994.
J. Nachbar. Bayesian learning in repeated games of incomplete information. Social Choice and
Welfare, 18(2):303?326, 2001.
J. H. Nachbar. Prediction, optimization, and learning in repeated games. Econometrica: Journal of
the Econometric Society, pages 275?309, 1997.
M. Ostrovsky and M. Schwarz. Reserve prices in internet advertising auctions: A field experiment.
In Proceedings of EC, pages 59?60. ACM, 2011.
H. Robbins. Some aspects of the sequential design of experiments. In Herbert Robbins Selected
Papers, pages 169?177. Springer, 1985.
W. Vickrey. Counterspeculation, auctions, and competitive sealed tenders. The Journal of finance,
16(1):8?37, 2012.

9


<<----------------------------------------------------------------------------------------------------------------------------------------->>

