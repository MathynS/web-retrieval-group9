query sentence: Noninfinitesimal algorithm
---------------------------------------------------------------------
title: 1271-online-learning-from-finite-training-sets-an-analytical-case-study.pdf

onlin learn finit train set analyt case studi peter sollich depart physic univers edinburgh edinburgh eh9 u.k. p.sollichoed.ac.uk david barber neural comput research group depart appli mathemat aston univers birmingham b4 u.k. d.barberoaston.ac.uk abstract analys onlin learn finit train set noninfinitesim learn rate tj extens statist mechan method obtain exact result time-depend general error linear network larg number weight n. find exampl small train set size larger learn rate use without compromis asymptot general perform converg speed encourag optim set tj less import weight decay given final learn time general perform onlin learn essenti good offlin learn introduct analysi onlin gradient descent learn one common approach supervis learn found neural network communiti recent focus much attent characterist featur onlin learn weight network student updat time new train exampl present error exampl reduc offlin learn hand total error exampl train set accumul gradient descent weight updat made onlin royal societi dorothi hodgkin research fellow support epsrc grant novel develop learn theori neural network onlin leamingfrom finit train set analyt case studi offlin learn equival limit case learn rate see main quantiti interest normal evolut general error well student approxim input-output map teacher under train exampl given number weight updat analyt treatment onlin learn assum either size train set infinit learn rate vanish small restrict undesir practic train set finit noninfinitesim valu need ensur learn process converg reason number updat general result deriv differ onlin offlin learn first order appli train set size see these result howev direct address question general perform explicit analysi time evolut general error finit train set provid krogh hertz scenario similar one consid offlin calcul serv baselin work finit progress made particular so-cal soft committe machin network architectur case infinit train set aim paper analys simpl model system order assess combin non-infinitesim learn rate finit train set contain exampl per weight affect onlin learn particular consid depend asymptot general error effect finit critic learn rate learn rate yield optim converg speed optim valu weight decay also compar perform onlin offlin learn discuss extent infinit train set analys applic finit model outlin calcul consid onlin train linear student network input-output relat n-dimension vector real-valu input singl real output wei~t vector network denot transpos vector factor introduc conveni whenev train exampl present network weight vector updat along gradient squar error exampl learn rate interest onlin learn finit train set updat exampl random chosen given set xll yll j.l ofp train exampl case cyclic present exampl left futur studi exampl j.l chosen updat weight vector chang here also includ weight decay normal parameter strength weight decay term number p. sollich d. barber exampl per weight play role weight decay common use offlin learn simplic student weight assum initi zero wn=o main quantiti interest evolut general error student assum train exampl generat linear teacher yjj w. jj ivn+ jj zero mean addit nois varianc teacher weight vector taken normal simplic input vector assum sampl random isotrop distribut hyperspher n. general error defin averag squar error student teacher output random input vn wn w order make scenario analyt tractabl focus limit larg number input compon weight taken constant number exampl per weight pin updat per weight learn time nin limit general error fg becom self-averag calcul averag random select exampl given train set train set result straightforward extend case percept ron teacher nonlinear transfer function usual statist mechan approach onlin learn problem express general error term order paramet like wjw whose self-averag time evolut determin appropri averag updat equat method work infinit train set averag order paramet updat express term order paramet alon finit train set hand updat involv new order paramet rl wj correl matrix train input lx jj xjj time evolut turn determin order paramet involv higher power yield infinit hierarchi order paramet solv problem consid instead order paramet generat junction general form general error 2~vjexp ha vn allow power obtain differenti respect result close system partial differenti equat wj exp ha w result equat detail solut given futur public final solut easili express term laplac transform general error fg z fdt fg e-z f//a fdz 2f function fi express close form term cours laplac transform yield direct asymptot valu general error foo fg limz +o zig z calcul analyt finit learn time fg obtain numer invers laplac transform result discuss discuss consequ main result focus first asymptot general error foo converg speed larg learn time onlin learningfrom finit train set an analyt case studi a=o. a=i figur asymptot general error vs a. shown final behaviour small numer evalu general take correspond sizabl noise-to-sign ratio jq.i asymptot general error shown function observ minim expect correspond resul ts offlin learn also read fix an increas function larger weight updat tend overshoot minimum total offlin train error caus diffus motion weight around their averag asymptot valu increas absenc weight decay howev independ case train data fit perfect everi term total sum-of-squar train error zero onlin learn lead weight diffus individu updat vanish general relat increas due nonzero depend signific exampl increas smaller mean case train data limit chosen fair larg order optim learn speed without serious affect asymptot general error larg limit hand one find relat increas valu therefor grow linear alreadi increas around occur also show diverg approach critic learn rate overshoot weight updat step becom larg weight eventu diverg laplac transform one find determin function shown increas reason weight decay reduc length weight vector updat counteract potenti weight diverg small larg limit one respect constant therefor decreas turn larg behaviour general error small slowli decay contribut mode vari exp optim valu unscaledweight decay decreas ja larg train set less need counteract nois train data use larg weight decay 2convers constant increas larg weight decay appli often repeat present train exampl would otherwis caus weight diverg p. sollich d. barber va decay constant scale linear size weight updat expect small condit ct fg reach asymptot valu foo scale tla number time train exampl use larg hand condit becom size train set drop sinc converg occur repetit train exampl becom signific larger pictur chang due new slow mode aris denomin interest mode exist finit threshold 71min finit could therefor predict small expans decay constant cslow decreas zero cross normal mode slow mode therefor determin converg speed larg fastest converg obtain howev may still advantag use lower valu order lower asymptot general error valu would deterior converg speed asymptot perform fig show depend larg maximum decay larger total train error surfac anisotrop around minimum weight space steepest direct determin converg along would fastest isotrop case howev overal converg speed determin shallow direct requir maxim fastest converg consid small behaviour illustr depend fg comparison simul result clear confirm calcul demonstr finit effect signific even fair small n. we see nonzero act effect updat nois elimin minimum fg correspond over-train foo also seen essenti independ predict small valu chosen 3b clear show increas foo also illustr how converg first speed increas zero slow approach we discuss optim set minim asymptot general error foo show happen we minim fg instead given final learn time correspond fix amount comput effort train network increas optim decreas toward zero requir tradeoff asymptot perform converg in 2a figur definit of71min their depend shown onlin learn rom finit train set an analyt case studi figur fg vs differ tj simul shown symbol standard error less symbol size shown learn rate tj increas larg rang figur optim tj a vs given final learn time result solid/dash line a a bold/thin line online/offlin learn dot line in fit form tj bin optim tj onlin learn speed minim const exp cl tjc2 c3 exp c4tjt lead tjopt bin constant a although deriv small tj function form dot line in also provid a good descript fair small tjopt becom larg optim weight decay a increas toward limit valu howev optim a much less import choos right tj minim fix a yield almost general error optim both tj a we omit detail result here encourag see 4c updat per weight optim tj general error almost indistinguish it optim valu for this also hold a kept fix optim learn rate therefor worthwhil in most practic scenario in we also compar perform onlin learn offlin learn calcul appropri discret time version ne might expect opposit effect larger low in order contain potenti diverg larger optim learn rate tj howev smaller tend make asymptot valu foo less sensit larg valu tj we saw abov we conclud this effect domin 4por fix fg t an over-train minimum pig asymptot behaviour tjopt chang tjopt without in factor correspond a fix effect learn time tjt requir reach this minimum p. saljich d. barber optim valu tj a for given perform loss use onlin instead offlin learn seen neglig this may seem surpris given effect nois weight updat impli onlin learn in particular for small howev compar the respect optim learn rate we see onlin learn make for this defici allow larger valu tj use for larg a for exampl tjc offlin tjc onlin final we compar our finit a result for the limit case a good agreement exist for learn time if the asymptot general error domin the contribut the nonzero learn rate tj the case for a in practic howev one want tj small enough make a neglig contribut in this regim the a result essenti useless conclus the main theoret contribut this paper the extens the statist mechan method order paramet dynam the dynam order paramet generat function the result we obtain for a simpl linear model system also practic relev for exampl the calcul depend tj the asymptot general error and the converg speed show in general sizabl valu tj use for train set limit size for larger a it import keep learn rate small we also found a simpl function form for the depend the optim tj a given final learn time this could use for exampl estim the optim tj for larg from test run a small number weight updat final we found for optim tj onlin learn perform essenti as well as offlin learn whether the weight decay a optim as well this encourag sinc onlin learn effect induc noisi weight updat this allow it cope better offlin learn the problem local train error minima in realist neural network onlin learn the advantag the critic learn rate signific lower by input distribut nonzero mean wherea for offlin learn signific reduc in the futur we hope extend our approach dynam t-depend optim tj although perform improv optim fix tj may small and complic network architectur in the crucial question of local minima address
----------------------------------------------------------------

title: 1390-on-line-learning-from-finite-training-sets-in-nonlinear-networks.pdf

onlin learn finit train set nonlinear network david barber peter sollich depart physic univers edinburgh edinburgh erg u.k. depart appli mathemat aston univers birmingham b4 u.k. p.sollich~ed.ac.uk d.barber~aston ac.uk abstract onlin learn one common form neural network train present analysi onlin learn finit train set non-linear network name soft-committe machin advanc theori realist learn scenario dynam equat deriv appropri set order paramet exact limit case either linear network infinit train set preliminari comparison simul suggest theori captur effect finit train set may yet account correct presenc local minima introduct analysi onlin gradient descent learn one common form supervis learn recent stimul great deal interest onlin learn weight network student updat immedi present train exampl input-output pair order reduc error network make exampl one primari goal onlin learn analysi track result evolut general error error student network make novel test exampl given number exampl present order specifi learn problem train output assum generat teacher network known architectur previous studi onlin learn often impos somewhat restrict royal societi dorothi hodgkin research fellow tsupport epsrc grant novel develop learn theori neural network p. soliich d. barber unrealist assumpt learn framework restrict either size train set infinit learn rate small l finit train set present signific analyt difficulti success weight updat correl give rise high non-trivi general dynam linear network difficulti encount finit train set noninfinitesim learn rate overcom extend standard set descript order paramet includ effect weight updat correl present work extend analysi nonlinear network particular model choos studi soft-committe machin capabl repres rich varieti input-output map onlin learn dynam studi comprehens infinit train set l order carri analysi adapt tool origin develop statist mechan literatur found applic exampl studi hopfield network dynam model outlin calcul n-dimension input vector output soft committe machin given nonlinear activ function g hl erf hz/v2 act activ hi wtxl.jfi factor conveni neural network hidden unit input hidden weight vector wi hidden output weight set onlin learn student weight adapt sequenc present exampl better approxim teacher map train exampl drawn replac finit set set remain fix pin train size relat input dimens denot take input vector sampl dimension gaussian distribut zero mean unit varianc train output assum generat teacher soft committe machin hidden weight vector addit gaussian nois corrupt activ output discrep teacher student particular train exampl drawn train set given squar differ correspond output yr g km eor student teacher activ respect em output respect j wtx km eo nois variabl corrupt teacher activ given train exampl student weight updat gradient descent step learn rate jnx8h on-lin learn finit train set nonlinear network general error defin averag error student make test exampl select random uncorrel train set write although one could principl model student weight dynam direct typic involv mani paramet seek compact represent evolut general error straightforward show general error depend detail descript network weight overlap paramet qll wi rim case infinit possibl obtain close set equat govern overlap paramet finit train set howev longer possibl due correl success weight updat order overcom difficulti use techniqu develop origin studi statist physic system initi consid dynam general vector order paramet denot function network weight weight updat describ transit probabl w approxim updat equat ifdw t w intuit integr equat express averag chang caus weight updat start given initi weight sinc aim develop close set equat order paramet dynam need remov depend initi weight inform regard contain chosen order paramet therefor averag result subshel correspond valu order paramet express 8-function constraint equat clear integr depend averag unnecessari result dynam equat exact fact case standard order paramet mention if achiev one choos set order paramet obtain approxim equat close possibl exact solut motiv choic order paramet base linear perceptron case addit standard paramet overlap project onto eigenspac train input correl matrix xl requir therefor split eigenvalu equal block contain ir eigenvalu order eigenvalu increas defin projector p'i onto correspond eigenspac take order paramet r1m tp'i wm ui 'y nt w tp'yb linear combin nois variabl train input here assum system size larg enough mean valu paramet alon describ dynam suffici well self-averag hold 2the order paramet actual use calcul linear perceptron laplac transform project order paramet p. sollich d. barber order paramet becom function continu variable3 updat order paramet due weight updat found take scalar product either project student teacher weight appropri introduc follow activ compon k'i vni ff w tp yx student teacher activ hi km respect linear perceptron chosen order paramet form complet set dynam equat close without need averag nonlinear case sketch calcul order paramet updat equat taken togeth integr wi sum discret term case one train exampl subshel averag defin averag activ compon nois variabl these variabl turn gaussian distribut zero mean therefor covari need work one find these fact given naiv train set averag exampl use eigenvalu eigenspac well defin detail eigenvalu spectrum correl activ nois variabl explicit appear error calcul similar give rim final equat defin nois varianc project overlap teacher weight vector assum teacher weight train input uncorrel independ requir covari compon activ kinh hi ls hjkm mm 3note limit taken thermodynam limit n. ensur number order paramet alway neglig compar otherwis self-averag would break on-lin learn finit train set nonlinear network i i i ooooooooc l.. i nnnoaa oa aaaoaaaaaaaaaaaaaaaac figur fg vs student teacher one hidden unit learn rate i nois equal varianc ad activ output simul shown circl standard error order symbol size bottom dash line show infinit train set result comparison use calcul theoret predict curv mark show larg enough effect limit use equat definit write dynam equat replac number updat continu variabl limit otrim otu otqiz oh i oh a-i oh eoh averag zero mean gaussian variabl covari use explicit form error we oh e lg km i em eo togeth equat complet descript dynam gaussian averag straightforward evalu manner similar infinit train set we omit rather cumbersom explicit form result equat we note contrast infinit train set case student activ hi nois variabl correl equat intuit reason weight becom correl train exampl train set calcul general error hand correl absent one result infinit train set dynam equat togeth constitut main result they exact limit either linear network ex integr numer in straightforward way in principl limit taken shown relat small valu taken in practic es result discuss we discuss main consequ result compar result predict general dynam infinit train set theori p. sollich d. barber ooooooooooooooooooo 1w figur vs two hidden unit left shown dash line comparison nois right nois bottom nois teacher activ output varianc simul shown small circl standard error less symbol size learn rate fj throughout simul throughout teacher overlap matrix set orthogon teacher weight vector length tij c5ij in figur l we studi accuraci method function train set size nonlinear network one hidden unit two differ nois level learn rate set fj small activ output nois figur la good agreement simul theori begin underestim general error compar simul finit theori howev still consider accur infinit predict larger nois figur lb theori provid reason quantit estim general dynam below valu signific disagr although qualit behaviour dynam predict quit well includ overfit phenomenon beyond infinit theori in this case qualit incorrect in two hidden unit case figur theori captur initi evolut well diverg signific simul larger nevertheless it provid consider improv infinit theori one reason discrep larg theori predict differ student hidden unit alway special individu teacher hidden unit whatev valu this lead decay plateau valu intermedi time in simul hand this special symmetri break appear inhibit least delay larg this happen even zero nois a train data contain enough inform forc student teacher weight equal asymptot reason this clear us deserv studi our initi investig howev suggest symmetri break may strong delay due presenc saddl point in train error surfac shallow unstabl direct when our theori fail it assumpt violat it conceiv multipl local minima in train error surfac could caus self-averag break howev we found evid for this see figur 3a on hand simul result in figur 3b clear show the implicit assumpt gaussian student activ discuss eq violat on-lin learn finit train set in nonlinear network varianc train histori figur varianc fg t vs input dimens for student teacher two hidden unit a fj zero nois the bottom curv show the varianc due differ random choic train exampl a fix train set train histori the top curv also includ the varianc due differ train set compat the lin decay expect if selfaverag hold dot line distribut train set the activ hi the first hidden unit the student histogram simul for paramet valu as in in summari the main theoret contribut this paper the extens onlin learn analysi for finit train set nonlinear network our approxim theori requir the use replica yield ordinari first order differenti equat for the time evolut a set order paramet it central implicit assumpt it achill heel the student activ gaussian distribut in comparison simul we found it accur the infinit train set analysi predict the general dynam for finit train set both qualit also quantit for small learn time futur work show whether the theori extend cope non-gaussian student activ without incur the technic difficulti dynam replica theori and whether this help captur the effect of local minima and general rough train error surfac acknowledg we would like thank ansgar west for help discuss
----------------------------------------------------------------

title: 4135-a-theory-of-multiclass-boosting.pdf

theori multiclass boost indraneel mukherje robert e. schapir princeton univers depart comput scienc princeton nj imukherj schapir cs.princeton.edu abstract boost combin weak classifi form high accur predictor although case binari classif well understood multiclass set correct requir weak classifi notion effici boost algorithm miss paper creat broad general framework within make precis identifi optim requir weak-classifi well design effect certain sens boost algorithm assum requir introduct boost refer general techniqu combin rule thumb weak classifi form high accur combin classifi minim demand place weak classifi varieti learn algorithm also call weak-learn employ discov simpl rule make algorithm wide applic theori boost well-develop case binari classif particular exact requir weak classifi set known algorithm predict better random distribut train set said satisfi weak learn assumpt boost algorithm minim loss effici possibl design specif known boost-by-major algorithm optim certain sens adaboost practic approxim understand would desir multiclass set well sinc mani natur classif problem involv two label recogn digit imag natur languag process task part-of-speech tag object recognit vision howev multiclass problem complet theoret understand boost lack particular know correct way defin requir weak classifi notion optim boost explor multiclass set straightforward extens binari weak-learn condit multiclass work requir less error random guess everi distribut binari case turn weak boost possibl two label hand requir accuraci even number label much larger two stringent simpl weak classifi like decis stump fail meet criterion even though often combin produc high accur classifi common approach far reli reduct binari classif hard clear weak-learn condit implicit assum reduct appropri purpos weak-learn condit clarifi goal weak-learn thus aid design provid specif minim guarante perform exploit boost algorithm consider may signific impact learn general know correct weak-learn condit might allow use simpler weak classifi turn help prevent overfit furthermor boost algorithm effici effect minim train error may prevent underfit also import paper creat broad general framework studi multiclass boost formal interact boost algorithm weak-learn unlik much previous work multiclass boost focus specif natur perhap weakest case weak classifi genuin classifi sens predict singl multiclass label instanc new framework allow us express rang weak-learn condit new one one previous assum often implicit within formal also final make precis meant correct weak-learn condit neither weak strong focus particular famili novel weak-learn condit especi appeal form like binari condit requir perform slight better random guess though respect perform measur general ordinari classif error introduc whole famili condit sinc mani way random guess two label key differ binari multiclass set although condit impos seem mild demand weak-learn show one power enough guarante boostabl mean combin weak classifi high accuraci individu member famili necessari boostabl also show entir famili taken togeth necessari sens everi boostabl learn problem exist one member famili satisfi thus identifi famili condit whole necessari suffici multiclass boost moreov combin entir famili singl weak-learn condit necessari suffici take kind union logic member combin condit also express framework understand abl character previous studi weak-learn condit particular condit implicit use adaboost.mh base one-against-al reduct binari turn strict stronger necessari boostabl also appli adaboost.m1 direct general adaboost multiclass whose condit shown equival adaboost.mh set hand condit implicit zhu al samm algorithm weak sens even condit satisfi boost algorithm guarante drive train error final condit implicit adaboost.mr also call adaboost.m2 turn exact necessari suffici boostabl employ proper weak-learn condit import also need boost algorithm exploit condit effect drive error given weak-learn condit boost algorithm drive train error effici framework understood optim strategi play certain two-play game game nontrivi analyz howev use power machineri drift game abl comput optim strategi game aris weak-learn condit famili describ optim strategi natur interpret term random walk phenomenon observ set focus paper minim train error algorithm deriv provabl decreas exponenti fast number round boost result use turn deriv bound general error use standard techniqu appli boost algorithm omit due lack space game-theoret strategi non-adapt presum prior knowledg edg much better random weak classifi algorithm adapt adaboost much practic requir prior inform show therefor deriv adapt boost algorithm modifi one game-theoret strategi present experi aim test efficaci new method work weak weak-learn check condit identifi inde weaker other previous use find new adapt strategi achiev low test error compar multiclass boost algorithm usual heavili underfit valid potenti practic benefit better theoret understand multiclass boost previous work first boost algorithm given schapir freund follow adaboost algorithm multiclass boost techniqu includ adaboost.m1 adaboost.m2 well adaboost.mh adaboost.mr approach includ also general approach appli boost includ two game-theoret perspect appli boost first one view weak learn condit minimax game drift game design analyz effici boost algorithm game further analyz multiclass continu time set framework introduc notat unless otherwis state matric denot bold capit letter like vector bold small letter like entri matrix vector denot denot ith row matrix inner product two vector denot hu vi frobenius inner product two matric tr mm0 denot m0 indic function denot distribut set denot multiclass classif want predict label exampl lie set exampl uniqu label set provid train set label exampl y1 xm ym boost combin sever mild power predictor call weak classifi form high accur combin classifi previous appli multiclass classif paper allow weak classifi predict singl class exampl appeal sinc combin classifi form although differ use much previous work adopt game-theoret view boost game play two player booster weak-learn fix number round binari label booster output distribut round weak-learn return weak classifi achiev accuraci distribut multiclass game extens binari game particular round booster creat cost-matrix ct rm k specifi weak-learn cost classifi exampl cost-matrix may arbitrari conform certain restrict discuss weak-learn return weakp classifi ht fix space ht cost incur ct 1ht ct ht small enough accord condit discuss 1h mean matrix whose j -th entri booster comput weight current weak classifi base much cost incur round end booster predict accord weight plural vote classifi return round ht argmax ft ft care choos cost matric round booster aim minim train error final classif even weak-learn adversari restrict cost-matric creat booster maximum cost weak-learn suffer round togeth defin weak-learn condit use binari label tradit weak-learn condit state non-neg weight train set error weak classfier return here parametr condit mani way translat condit languag one fewest restrict costmatric requir label correct less cost label incorrect return weak classifi requir less cost predict restrict random h xi correspond may verifi two condit rewrit condit make simplifi assumpt henceforth without loss general assum true label alway let bin consist matric satisfi further let ubin matrix whose row weak-learn search space satisfi binari weak-learn condit bin 1h ubin two main benefit refor mulat linear homogen constraint mathemat simplifi appar later import vari restrict bin cost vector matrix ubin generat vast varieti weak-learn condit multiclass set show let rm k matrix rm k call baselin say weak classifi space satisfi condit b variabl matrix specifi cost misclassif baselin specifi weight misclassif condit therefor state weak classifi exceed averag cost weight accord baselin b larg class weak-learn condit captur mani previous use condit one use adaboost.m1 adaboost.mh adaboost.mr well novel condit introduc next section studi vast class weak-learn condit hope find one serv main purpos boost game find convex combin weak classifi zero train error possibl minimum weak classifi suffici rich perfect combin exist formal collect weak classifi elig boost simpli boostabl exist distribut space linear separ data h h h xi weak-learn condit play two role reject space boostabl provid algorithm mean search right combin ideal second factor caus weak-learn condit impos addit restrict weak classifi case weak-learn condit mere reformul boostabl appropri deriv algorithm general could strong certain boostabl space fail satisfi condit could weak non-boost space might satisfi condit booster strategi reli either these condit fail drive error former due underfit latter due overfit next section describ condit captur framework avoid weak strong necessari suffici weak-learn condit binari weak-learn condit appeal form distribut exampl weak classifi need achiev error greater random player guess correct answer probabl further weakest condit boost possibl follow game-theoret perspect multiclass weak-learn condit similar properti miss literatur section show framework captur condit multiclass set model random player baselin predictor rm k whose row distribut label predict exampl sampl consid space edge-over-random baselin b eor rm k faint clue correct answer precis baselin b eor space like predict correct label incorrect one everi exampl equal hold space b eor consist uniqu player ubin binari weak-learn bin bin condit given new condit general particular defin eor multiclass extens bin cost-matrix eor put least cost correct label row cost-matric come set rk everi baselin b eor introduc condit eor call edgeover-random weak-learn condit sinc expect cost edge-over-random baselin matrix constraint impos new condit essenti requir better random perform present central result section seem mild edge-over-random condit guarante elig mean weak classifi satisfi one condit combin form high accur combin classifi theorem suffici weak classifi space satisfi weak-learn condit eor b eor boostabl proof involv von-neumann minimax theorem spirit one hand famili condit taken whole necessari boostabl sens everi elig space weak classifi satisfi edge-over-random condit theorem relax necess everi boostabl weak classifi space there exist b eor satisfi weak-learn condit eor b proof show exist non-construct averag argument theorem state boostabl weak classifi space satisfi condit famili help us choos right condit experi section suggest eor effect simpl weak-learn compar popular boost algorithm here b eor edge-overrandom baselin closest uniform weight incorrect label correct label howev there theoret exampl show condit famili strong supplement perhap extrem way weaken condit requir perform cost matrix competit fix baselin b eor worst eor 1h maxeor b condit state cours boost game weak-learn may choos beat edge-over-random baselin b eor possibl differ one everi round everi cost-matrix may superfici seem much weak contrari condit turn equival boostabl word accord criterion neither weak strong weak-learn condit howev unlik edge-over-random condit also turn difficult work algorithm furthermor condit shown equival one use adaboost.mr perhap remark sinc latter base appar complet unrel all-pair mr consist multiclass binari reduct mr condit given mr bmr cost-matric put non-neg cost incorrect label whose row sum zero k bmr matrix first column column supplement further mr condit henc shown neither weak strong theorem weak classifi space satisfi adaboost.mr weak-learn condit mr bmr satisfi moreov condit equival boostabl next illustr strength random-over-edg weak-learn condit concret comparison previous algorithm comparison samm samm algorithm requir weak classifi achiev less error uniform random guess multipl label languag weaklearn condit well-known condit suffici boost possibl particular consid dataset weak classifi space consist h1 h2 alway predict respect sinc neither classifi distinguish achiev perfect accuraci combin way yet due constraint cost-matrix one h1 h2 alway manag non-posit cost random alway suffer posit cost hand weaklearn condit allow booster choos far richer cost matric particular cost matrix eor classifi exampl suffer loss random player fail satisfi condit comparison adaboost.mh adaboost.mh popular multiclass boost algorithm base one-against-al reduct howev show implicit demand weak classifi space strong construct classifi space satisfi condit eor famili satisfi adaboost.mh weak-learn condit consid space everi element subset exampl classifi predict correct exact element expect loss random chosen classifi this space random player henc satisfi this weak-learn condit hand shown supplement adaboost.mh weak-learn mh condit pair mh bmh non- posit negat entri correct label row matrix bmh vector quick calcul show mh first column zero elsewher 1h bmh this posit fail satisfi adaboost.mh condit algorithm this section devis algorithm analyz boost game employ edge-overrandom weak-learn condit comput optimum booster strategi complet adversari weak-learn here permit choos weak classifi without restrict entir space hall possibl function map exampl label model weaklearn adversari make absolut assumpt algorithm might use henc error guarante enjoy this situat univers applic algorithm deriv general drift game framework solv boost game turn inspir freund boost-by-major algorithm review next os algorithm fix number round edge-over-random weak-learn condit b simplic present fix weight round ft defin optimum booster payoff written min max min max l ft ft ft c1 h1 hall c1 ct ht hall ct here function rk error also consid loss function exponenti loss hing loss etc upper-bound error proper increas weight correct label decreas weight incorrect label direct analyz optim payoff hard howev schapir observ payoff well approxim certain potenti function inde rk defin potenti function follow recurr min max el p el el p hb ci c rk el rk unit-vector whose lth coordin remain coordin zero these potenti function comput estim st whether exampl misclassifi base current state st consist count vote receiv far various class st t0 ht number round remain use these function schapir propos booster strategi aka os strategi round construct cost matrix whose row achiev minimum right hand side replac replac replac current state st follow theorem provid guarante loss suffer os algorithm also show game-theoret optimum strategi number exampl larg theorem extens result suppos weak-learn condit given pm booster employ os algorithm averag potenti state never increas round particular loss suffer round play pm further loss function satisfi mild condit booster strategi achiev loss less bound round comput potenti order implement os strategi use weak-learn condit need comput potenti for distribut fortun these potenti simpl solut term homogen random-walk rtb random posit particl time step start locat rk step move direct el probabl theorem proper satisfi rtb furthermor vector achiev minimum right hand side given el theorem impli os strategi choos follow cost matrix round st el st state exampl round therefor everyth boil comput potenti made possibl theorem there simpl close form solut for non-convex loss maxi si howev use theorem write potenti explicit comput use dynam program time this yield tight bound obtain effici procedur one we soon show made adapt we next focus on exponenti loss associ adaboost close form solut lemma s1 exp k sk s1 posit then pk sl solut in theorem evalu al bl al bl b1 proof induct straightforward in particular condit eor pk relev potenti e sl cost-matrix output os algorithm simplifi rescal ad number coordin cost vector without affect constraint it impos on weak classifi follow form e sl pk e sj such pm choic theorem form potenti guarante averag loss l st state st chang factor everi round henc final loss variabl edg so far we requir weak-learn beat random least fix amount in round boost game in realiti edg random larger initi get smaller os algorithm creat harder cost matric therefor requir fix edg either unduli pessimist or over optimist fix edg small enough progress made in initi round edg larg weak-learn fail meet weak-learn condit in latter round we attempt fix this via two approach prescrib decay sequenc edg or complet flexibl aka adapt respect edg return weak-learn in either case we use edge-over-random condit eor vari valu fix sequenc edg prescrib sequenc edg weak-learn condit eor u in round differ we allow weight arbitrari must fix in advanc all result for uniform weight hold in this case well pm pk in particular argument lead we want minim e ft ft defin in then follow strategi optim in round output cost matrix eft pk eft if pm pk this ensur express e ft chang factor qt in round henc final loss adapt in adapt set we depart game-theoret framework in weaklearn longer adversari further we longer guarante receiv certain sequenc edg sinc choic cost-matrix in depend on edg we could fix arbitrari set weight in advanc follow algorithm enjoy bound qt troubl this less unless small compar ensur progress weight must chosen adapt function sinc we know edg we receiv we choos cost matrix anticip infinitesim small edg in the spirit rescal eft if pk lim if eft if pk eft if pendigit satimag poker letter forest connect4 pendigit poker satimag letter forest connect4 figur figur plot the final test-error m1 black dash mh blue dot new method red solid the maximum tree-siz allow weak classifi figur plot fast the test-error these algorithm drop round the maximum tree-siz allow sinc weak-learn cooper we expect the edg the return classifi ht on the suppli cost-matrix more infinitesim in case continu there noninfinitesim choic the weight such the edg achiev ht on the cost-matrix c remain larg enough ensur in fact with choic we get supplement tune result in loss henc error this algorithm adapt ensur the qt pt round exp experi we report preliminari experiment result on six vari multiclass uci dataset the first set experi aim at determin overal perform new algorithm we compar mh a standard implement m1 adaboost.m1 with m1 new method weak learner the boostext implement mh adaboost.mh use stump with the adapt algorithm describ in section we call new method use a naiv greedi tree-search algorithm greedi for weak-learn the size tree chosen the order the tree size use by m1 test error round boost plot in figur the perform compar with m1 far better mh understand sinc stump far weaker tree even though weak-learn naiv com figur this a plot the final test-error standard implement mh pare connect4 forest letter pendigit poker satimag new method round boost we next investig each algorithm perform with less power weak-classifi name decis tree whose size sharpli limit various pre-specifi limit figur show test-error plot as a function tree size as predict by our theori our algorithm succeed in boost the accuraci even the tree size small to meet the stronger weak learn assumpt the other algorithm the differ in perform particular strong use the smallest tree size more insight provid by plot in figur the rate converg test error with round when the tree size allow small both m1 mh drive the error for a round but sinc boost keep creat harder cost-matric soon the small-tre learn algorithm longer abl to meet the excess requir m1 and mh howev our algorithm make more reason demand easili met by the weak learner
----------------------------------------------------------------

title: 4745-a-scalable-cur-matrix-decomposition-algorithm-lower-time-complexity-and-tighter-bound.pdf

scalabl cur matrix decomposit algorithm lower time complex tighter bound shusen wang zhihua zhang colleg comput scienc technolog zhejiang univers hangzhou china wss zhzhang zju.edu.cn abstract cur matrix decomposit import extens nystr om approxim general matrix approxim data matrix term small number column row paper propos novel random cur algorithm expect relative-error bound propos algorithm advantag exist relative-error cur algorithm possess tighter theoret bound lower time complex avoid maintain whole data matrix main memori final experi sever real-world dataset demonstr signific improv exist relative-error algorithm introduct large-scal matric emerg stock genom web document web imag video everyday bring new challeng modern data analysi effort focus manipul understand interpret large-scal data matric mani case matrix factor method employ construct compress inform represent facilit comput interpret principl approach truncat singular valu decomposit svd find best low-rank approxim data matrix applic svd eigenfac latent semant analysi illustr success howev basi vector result svd littl concret mean make difficult us understand interpret data question exampl well shown viewpoint vector height sum signific uncorrel featur dataset peopl featur particular inform author also claim would interest tri find basi vector experi vector use actual experi vector artifici base offer littl insight therefor great interest repres data matrix term small number actual column and/or actual row matrix cur matrix decomposit provid techniqu shown use high dimension data analysi given matrix cur techniqu select subset column construct matrix subset row construct matrix cur best approxim typic cur algorithm comput matrix work two-stag manner stage standard column select procedur stage row select simultan thus stage complic stage cur matrix decomposit problem wide studi literatur perhap wide known work cur problem author devis random cur algorithm call subspac sampl algorithm particular algorithm relative-error ratio high probabl unfortun exist cur algorithm requir larg number column row chosen exampl matrix target rank min state-ofthe-art cur algorithm subspac sampl algorithm requir exact o k row log2 row expect achiev relative-error ratio moreov comput cost algorithm least cost truncat svd o min mn2 nm2 algorithm therefor impract large-scal matric paper develop cur algorithm beat state-of-the-art algorithm theori experi particular show theorem novel random cur algorithm lower time complex tighter theoret bound comparison state-of-the-art cur algorithm rest paper organ follow section introduc sever exist column select algorithm state-of-the-art cur algorithm section describ analyz novel cur algorithm section empir compar propos algorithm state-of-the-art algorithm notat matrix aij rm n let i-th row aj j-th column let frobenius norm spectral j aij norm j aij norm moreov let im denot ident matrix 0mn denot zero matrix ua k va k ua k va k let ua va ua va svd rank ua k va k correspond top singular valu denot ak ua k va k furthermor let moore-penros invers relat work section introduc sever relative-error column select algorithm relat work section describ state-of-the-art cur algorithm section discuss connect column select problem cur problem relative-error column select algorithm given matrix rm n column select problem select column construct rm c minim cc f sinc nc possibl choic construct select best subset hard problem recent year mani polynomial-tim approxim algorithm propos among particular interest algorithm relative-error bound column select constant cc f ak call relative-error ratio present recent result relat work first introduc recent develop determinist algorithm call dual set sparsif propos show result lemma furthermor algorithm build block power algorithm lemma novel cur algorithm also reli algorithm attach algorithm appendix a. lemma column select via dual set sparsif algorithm given matrix rm n rank target rank exist determinist algorithm select column form matrix rm c ak cc although partial svd algorithm krylov subspac method requir o mnk time numer unstabl see discuss moreov matrix comput tva k o mn+nck tva k time need comput top right singular vector also varieti random column select algorithm achiev relative-error bound literatur random algorithm select 2k column achiev expect relative-error ratio algorithm base approxim svd via random project dual set sparsif algorithm adapt sampl algorithm present main result algorithm lemma propos cur algorithm motiv reli algorithm lemma near-optim column select algorithm given matrix rm n rank target rank there exist random algorithm select 2k column form matrix rm c e2 cc f e cc ak expect taken c. furthermor matrix comput o mnk nk subspac sampl cur algorithm drinea propos two-stag random cur algorithm relative-error bound given matrix rm n target rank first stage algorithm choos exact o k log column log log expect construct rm c second stage choos exact log row log log expect simultan construct u probabl least relative-error ratio comput cost domin truncat svd a c. though algorithm optim high probabl requir mani row get chosen least log2 row expect paper seek devis algorithm mild requir column row number connect column select cur matrix decomposit cur problem a close connect column select problem aforement first stage exist cur algorithm simpli a column select procedur howev second stage complic second stage na veli solv a column select algorithm error ratio least a relative-error cur algorithm first stage seek bound a construct error ratio a f ar r f given c. actual first a ak section stage seek bound a f stage a special case second stage ak given a matrix a if algorithm solv ar r f ing second stage result a bound a cc algorithm also solv a f column select problem a relative-error ratio thus second stage cur a general column select problem main result section introduc propos cur algorithm call fast cur algorithm lower time complex compar svd describ algorithm give a theoret analysi theorem theorem reli lemma theorem theorem reli theorem theorem a general theorem theorem a general theorem algorithm fast cur algorithm input a real matrix a rm n target rank target column number 2k target row number 2c stage select column a construct rm c kv comput approxim truncat svd via random project ak kv v1 column kt construct u1 column a comput s1 dual set spectral-frobenius sparsif algorithm v1 construct c1 adiag s1 delet all-zero column residu matrix a c1 a comput sampl probabl pi di sampl c2 column a probabl pn construct c2 stage select row a construct rr n kv v2 column tk construct u2 column a comput s2 dual set spectral-frobenius sparsif algorithm v2 construct r1 diag s2 delet all-zero row residu matrix a r1 comput qj sampl r2 row a probabl qm construct r2 return c2 rt2 ar adapt sampl relative-error adapt sampl algorithm establish theorem algorithm base follow idea select a proport column a form c1 arbitrari algorithm algorithm random sampl addit c2 column accord residu a c1 a. boutsidi use adapt sampl algorithm decreas residu dual set sparsif algorithm obtain relative-error bound prove a new bound adapt sampl algorithm interest new bound a general origin one theorem word theorem a direct corollari follow theorem ak set theorem adapt sampl algorithm given a matrix a rm n a matrix rm c rank c rank cc a let r1 rr1 consist r1 row a defin residu a r1 addit defin pi sampl r2 row a trial i-th row chosen probabl pi let r2 rr2 contain r2 sampl row let rt2 follow inequ hold e a cc ar a cc a r1 r2 expect taken r2 fast cur algorithm base dual set sparsif algorithm lemma adapt sampl algorithm theorem develop a random algorithm solv second stage cur problem present result algorithm theorem theorem a special case follow theorem ak theorem fast row select algorithm given a matrix a rm n a matrix rm c rank c rank cc a a target rank r n propos random algorithm select row a construct e a cc ar a cc ak expect taken r. furthermor matrix comput o mnk mk time base lemma theorem here present main theorem fast cur algorithm tabl a summari dataset dataset type size sourc redrocknatur http //www.agarwala.org/effici gdc arcen biolog http //archive.ics.uci.edu/ml/datasets/arcen dexter bag word 2600http //archive.ics.uci.edu/ml/datasets/dext theorem fast cur algorithm given a matrix a rm n a posit integ min fast cur algorithm describ algorithm random select 2k column a construct rm c near-optim column select algorithm lemma r n select 2c fast row select row a construct algorithm theorem then we e a cur f e a ar ak moreov algorithm run time n k mk nk sinc min assumpt time complex fast cur algorithm lower svd a main reason we call it fast cur algorithm anoth advantag algorithm avoid load whole data matrix a main memori none three step random svd dual set sparsif algorithm adapt sampl algorithm requir load whole a memori memoryexpens oper throughout fast cur algorithm comput moore-penros invers requir maintain matrix matrix memori comparison subspac sampl algorithm requir load whole matrix memori comput truncat svd empir comparison section we provid empir comparison among relative-error cur algorithm sever dataset we report relative-error ratio run time algorithm data set relative-error ratio defin a cur f relative-error ratio a ak a specifi target rank we conduct experi three dataset includ natur imag biolog data bag word tabl briefli summar inform dataset redrock a larg size natur imag arcen dexter uci dataset arcen a biolog dataset instanc attribut dexter a bag word dataset a 20000-vocabulari document dataset actual repres as a data matrix upon we appli cur algorithm we implement algorithm matlab we conduct experi a workstat intel xeon cpus memori ubuntu system accord analysi paper integ far less data set algorithm we set rang set experi we repeat set experi time report averag standard deviat error ratio result depict figur result show fast cur algorithm much lower relative-error ratio subspac sampl algorithm experiment result well match theoret analys section as run time fast cur algorithm effici small becom larg fast cur algorithm becom less effici time complex fast cur algorithm linear larg impli small howev purpos cur select a small number column row data matrix so we interest case larg compar say run time run time subspac sampl exact subspac sampl expect fast cur subspac sampl exact subspac sampl expect fast cur time time time run time construct error frobenius norm subspac sampl exact subspac sampl expect fast cur subspac sampl exact subspac sampl expect fast cur construct error frobenius norm relat error ratio subspac sampl exact subspac sampl expect fast cur relat error ratio construct error frobenius norm relat error ratio subspac sampl exact subspac sampl expect fast cur figur empir result redrock data set run time run time time time time run time subspac sampl exact subspac sampl expect fast cur construct error frobenius norm subspac sampl exact subspac sampl expect fast cur construct error frobenius norm construct error frobenius norm subspac sampl exact subspac sampl expect fast cur relat error ratio subspac sampl exact subspac sampl expect fast cur subspac sampl exact subspac sampl expect fast cur relat error ratio relat error ratio subspac sampl exact subspac sampl expect fast cur figur empir result arcen data set run time run time run time subspac sampl exact subspac sampl expect fast cur subspac sampl exact subspac sampl expect fast cur subspac sampl exact subspac sampl expect fast cur time time time subspac sampl exact subspac sampl expect fast cur relat error ratio relat error ratio subspac sampl exact subspac sampl expect fast cur construct error frobenius norm subspac sampl exact subspac sampl expect fast cur construct error frobenius norm construct error frobenius norm relat error ratio figur empir result dexter data set conclus in this paper we propos a novel random algorithm cur matrix decomposit problem this algorithm faster scalabl accur the state-of-the-art algorithm the subspac sampl algorithm algorithm requir column row achiev relative-error ratio achiev the relative-error bound the subspac sampl algorithm requir log column log row select the origin matrix algorithm also beat the subspac sampl algorithm in time-complex our algorithm cost n k mk nk time lower o min mn2 m2 the subspac sampl algorithm when small moreov our algorithm enjoy anoth advantag avoid load the whole data matrix main memori also make our algorithm scalabl final the empir comparison also demonstr the effect effici our algorithm a the dual set sparsif algorithm for the sake complet we attach the dual set sparsif algorithm here describ implement detail the dual set sparsif algorithm determinist algorithm establish in the fast cur algorithm call the dual set spectral-frobenius sparsif algorithm lemma in stage we show this algorithm in algorithm bound in lemma lemma dual set spectral-frobenius sparsif let rl l n contain the column arbitrari matrix rt let vn a decomposit the ident i given an integ algorithm determinist comput a set weight si most non-zero tr si xti si vi vi algorithm determinist dual set spectral-frobenius sparsif algorithm input vi with vi vi ik initi s0 a0 comput for then comput k/r for comput the eigenvalu decomposit a find an index in comput a weight vjt a vj vj vjt a a a a rk updat the j-th compon a end for k/r return sr a a tvj vjt the weight si comput determinist in o rnk nl time here we would like mention the implement algorithm describ in detail in iter the algorithm perform eigenvalu decomposit a w wt guarante to posit semi-definit in each iter sinc a ik wdiag wt we effici comput base the eigenvalu decomposit a with the eigenvalu at hand a also comput direct acknowledg this work support in part the natur scienc foundat china the googl visit faculti program the scholarship award for excel doctor student grant ministri educ
----------------------------------------------------------------

title: 5074-low-rank-matrix-reconstruction-and-clustering-via-approximate-message-passing.pdf

low-rank matrix reconstruct cluster via approxim messag pass ryosuk matsushita ntt data mathemat system inc. 1f shinanomachi rengakan shinanomachi shinjuku-ku tokyo japan matsur8 gmail.com toshiyuki tanaka depart system scienc graduat school informat kyoto univers yoshida hon-machi sakyo-ku kyoto-shi japan tt i.kyoto-u.ac.jp abstract studi problem reconstruct low-rank matric noisi observ formul problem bayesian framework allow us exploit structur properti matric addit low-ranked sparsiti propos effici approxim messag pass algorithm deriv belief propag algorithm perform bayesian infer matrix reconstruct also success appli propos algorithm cluster problem reformul low-rank matrix reconstruct problem addit structur properti numer experi show propos algorithm outperform lloyd k-mean algorithm introduct low-ranked matric frequent exploit one reconstruct matrix noisi observ problem often demand incorpor addit structur properti matric addit low-ranked paper consid case matrix a0 rm n reconstruct factor a0 u0 u0 rm r v0 rn one know structur properti factor u0 v0 priori spars non-neg factor popular exampl structur properti sinc properti factor exploit vari accord problem desir reconstruct method enough flexibl incorpor wide varieti properti bayesian approach achiev flexibl allow us select prior distribut u0 v0 reflect priori knowledg structur properti bayesian approach howev often involv comput expens process high-dimension integr therebi requir approxim infer method practic implement mont carlo sampl method variat bay method propos low-rank matrix reconstruct meet requir present paper approxim messag pass amp base algorithm bayesian lowrank matrix reconstruct develop context compress sens amp algorithm reconstruct spars vector linear measur low comput cost achiev certain theoret limit amp algorithm also use approxim bayesian infer larg class prior distribut signal vector nois distribut success amp algorithm motiv use idea low-rank matrix reconstruct iterfac algorithm rank-on case deriv amp algorithm amp algorithm general-rank case propos howev treat estim posterior mean extend algorithm one deal estim maximum posteriori map estim first contribut paper second contribut appli deriv amp algorithm k-mean type cluster obtain novel effici cluster algorithm base observ formul low-rank matrix reconstruct problem includ cluster problem special case although idea appli low-rank matrix reconstruct cluster new propos algorithm knowledg first direct deal constraint datum assign exact one cluster framework low-rank matrix reconstruct present result numer experi show propos algorithm outperform lloyd k-mean algorithm data high-dimension recent amp algorithm dictionari learn blind calibr matrix reconstruct general observ model propos although work similar studi differ fix rank rather ratio r/m take limit deriv algorithm anoth differ formul explain next section assum statist independ among compon row u0 v0 detail comparison among algorithm remain made problem set low-rank matrix reconstruct consid follow problem set matrix a0 rm n estim defin two matric u0 rm r v0 rn a0 u0 rr consid case observ a0 corrupt addit nois rm n whose compon wi j gaussian random variabl follow nois varianc paramet denot gaussian distribut mean varianc factor nois varianc introduc allow proper scale limit go infin order employ deriv algorithm observ matrix rm n given a0 reconstruct a0 v0 problem consid paper take bayesian approach address problem one requir prior distribut variabl estim well condit distribut relat observ variabl estim distribut need true one case avail one assum arbitrarili case one expect advantag assum specif manner view comput effici paper suppos one use true condit distribut v0 u0 mn exp denot frobenius norm meanwhil suppos assum prior distribut u0 v0 denot p u p v respect may differ true distribut pu pv respect restrict p u p v distribut form p u p u p v p v respect allow us construct comput effici algorithm p u p v posterior distribut given p u pv exp prior probabl densiti function p u p v improp integr infin long posterior proper also consid case assum rank may differ true rank thus suppos estim size respect we consid two problem appear bayesian approach first problem we call margin problem calcul margin posterior distribut given p j ui vj duk dvl use calcul posterior mean e u margin map estim mmap ummap arg max v|a dv arg max p j v|a du j calcul p j ui vj typic involv high-dimension integr requir high comput cost approxim method need second problem we call map problem calcul map estim arg maxu v formul follow optim problem min map u v map negat logarithm map log p u ui log p v vj non-convex function general hard find global optim solut therefor approxim method need problem well cluster low-rank matrix reconstruct cluster problem formul problem low-rank matrix reconstruct suppos er el vector whose lth compon other v0 u0 fix aj follow one gaussian distribut lth column u0 we regard gaussian center cluster repres cluster distribut defin cluster assign datum aj one perform cluster dataset reconstruct u0 v0 structur constraint everi row v0 belong assum number cluster let us consid maximum likelihood estim arg maxu v equival map esti r mation improp uniform prior distribut p u p v correspond map problem min subject vj satisfi constraint object function aj vj el sum squar distanc datum center cluster datum assign optim problem object function cluster base call paper k-mean problem k-mean loss function k-mean cluster respect one also use margin map estim cluster u0 v0 follow p u p v respect margin map estim optim sens maxim expect accuraci respect here accuraci defin fraction correct assign data among data we call cluster use approxim margin map estim maximum accuraci cluster even incorrect prior distribut use previous work exist method approxim solv margin problem map problem divid stochast method markov-chain monte-carlo method determinist one popular determinist method use variat bayesian formal variat bay matrix factor approxim posterior distribut product vb two function pvb pv determin kullback-leibl vb vb diverg pu pv minim global minim kl diverg difficult except special case iter method obtain local minimum usual adopt appli variat bay matrix factor map problem one obtain iter condit mode icm algorithm altern minim map fix minim fix repres algorithm solv k-mean problem approxim lloyd k-mean algorithm lloyd k-mean algorithm regard icm algorithm altern minim k-mean loss function fix minim fix iter algorithm lloyd k-mean algorithm ntl vjt el tl ljt+1 arg min tl aj aj vjt el ntl vjt+1 elt+1 throughout paper we repres algorithm set equat represent mean algorithm begin set initi valu repeat updat variabl use equat present satisfi stop criteria lloyd k-mean algorithm begin set initi assign er n algorithm easili get stuck local minima perform heavili depend initi valu algorithm method initi obtain better local minimum propos maximum accuraci cluster solv approxim use variat bay matrix factor sinc give approxim margin posterior distribut vj given propos algorithm approxim messag pass algorithm low-rank matrix reconstruct we first discuss general idea amp algorithm advantag amp algorithm compar variat bay matrix factor amp algorithm deriv approxim belief propag messag pass algorithm way thought asymptot exact large-scal problem appropri random fix point belief propag messag pass algorithm correspond local minima kl diverg kind trial function posterior distribut therefor belief propag messag pass algorithm regard iter algorithm base approxim posterior distribut call beth approxim beth approxim reflect depend random variabl depend problem extent therefor one intuit expect perform amp algorithm better variat bay matrix factor treat independ import properti amp algorithm asid effici effect one predict perform algorithm accur large-scal problem use set equat call state evolut analysi state evolut also show requir iter number even problem size larg although we present state evolut algorithm propos paper give proof valid like we discuss state evolut here due limit space avail we introduc one-paramet extens posterior distribut treat margin problem map problem unifi manner defin follow exp p u pv proport paramet reduc limit distribut concentr maxima algorithm margin problem particular algorithm margin problem map problem origin posterior distribut let respect amp algorithm margin problem deriv way similar describ detail supplementari materi deriv algorithm valu variabl but btu rm r bvt btv n rn tu tv utm rm r vn rn s1t sm t1t tnt calcul iter superscript repres iter number variabl negat iter number defin algorithm follow algorithm but av tj tu tj uti btu tu p u sit g btu tu p u si tv si bvt vjt+1 btv j tv p v tjt+1 g btv j tv p v algorithm almost symmetr equat updat quantiti relat estim u0 v0 respect algorithm requir initi valu begin tj0 function rr rr a paramet defin normal defin exp log one see mean distribut covari matrix scale function need differenti everywher algorithm work differenti one need calcul run algorithm we assum rest section converg algorithm although converg guarante general let bu bv converg valu si tj respect variabl first consid run algorithm margin posterior distribut approxim vj p j ui vj q ui v j u i sinc respect vj mean u i v j posterior mean e u a du dv approxim e u vjmmap approxim margin map estim ummap vjmmap arg max v j ummap arg max u i take limit algorithm yield algorithm map problem case function replac arg min log one may calcul hessian log denot via ident ident follow implicit function theorem some addit assumpt help case explicit form avail map estim approxim properti algorithm algorithm sever plausibl properti first a low comput cost comput cost per iter o mn linear number compon matrix a calcul perform o n time per iter constant factor depend calcul general involv r -dimension numer integr although need case an analyt express integr avail case variabl take discret valu calcul involv minim an r -dimension vector when log a convex function posit semidefinit this minim problem convex solv relat low cost second algorithm a form similar an algorithm base variat bayesian matrix factor fact last term right-hand side four equat remov result algorithm an algorithm base variat bayesian matrix factor propos particular icm algorithm when note howev treat case prior p u p v multivari gaussian distribut note addit comput cost these extra term o insignific compar cost whole algorithm o mn third when one deal map problem valu map may increas iter algorithm follow proposit howev guarante optim output algorithm a certain sens it converg proposit let sm tn a fix point amp algorithm map problem suppos si tj posit semidefinit a global minimum map a global minimum map proof supplementari materi key proof follow reformul map tjt arg min tr if tjt posit semidefinit second term minimand negat squar pseudometr interpret a penalti near tempor estim posit semidefinit sit tjt hold almost case fact we assum sinc a scale covari matrix posit semidefinit it follow proposit fix point amp algorithm also a fix point icm algorithm it two implic execut icm algorithm initi converg valu amp algorithm improv map amp algorithm fix point icm algorithm second implic may help amp algorithm avoid get stuck bad local minima cluster via amp algorithm one use amp algorithm map problem perform k-mean cluster let p u p v el note p v piecewis constant respect henc p v almost everywher we obtain follow algorithm algorithm amp algorithm k-mean cluster av tu but bvt a tv tv btv j vjt+1 arg min but it initi an assign er n algorithm rewritten follow ntl ljt+1 arg i vjt el tl aj i vjt el ntl 2m tl i vjt el aj nl nl min vjt+1 elt+1 paramet appear algorithm exist k-mean cluster problem fact appear a2ij sit estim sit deriv algorithm justifi large-s problem practic we propos use a a temporari estim tth iter amp algorithm kmean cluster updat valu way lloyd k-mean algorithm it perform assign data cluster a differ way amp algorithm addit distanc data center cluster assign present taken consider two way a datum less like assign cluster it assign present data like assign a cluster whose size present smaller former intuit understood observ if vjt el one take account fact cluster center tl bias toward aj term 2m ntl i vjt el correct this bias it invers proport cluster size amp algorithm maximum accuraci cluster obtain let p v a discret distribut after algorithm converg arg maxv vj give final cluster assign jth datum give estim cluster center numer experi we conduct numer experi artifici real data set evalu perform propos algorithm cluster experi artifici data set we set generat accord let cluster center multivari gaussian distribut i cluster assign generat accord uniform distribut er fix we generat problem instanc solv five algorithm lloyd k-mean algorithm k-mean amp algorithm k-mean cluster amp-km variat bay matrix factor maximum accuraci cluster vbmf-ma amp algorithm maximum accuraci cluster amp-ma k-mean k-mean updat variabl way lloyd k-mean algorithm an initi valu chosen in a sophist manner algorithm initi valu vj0 random generat distribut as we use true prior distribut maximum accuraci cluster we ran lloyd k-mean algorithm k-mean chang observ we ran amp algorithm k-mean cluster either satisfi this becaus we observ oscil assign a small number data two algorithm we termin iter when met number iter exceed we then evalu follow perform measur obtain solut n1 a normal k-mean loss aj a aj accuraci maxp i p vj maxim taken r-by-r permut matric we use hungarian algorithm solv this maxim problem effici number iter need converg we calcul averag standard deviat these perform measur instanc we conduct experi various valu figur show result amp algorithm k-mean cluster achiev smallest kmean loss among five algorithm while lloyd k-mean algorithm k-mean show larg k-mean loss we emphas three algorithm aim minim k-mean loss differ lie in algorithm minim amp algorithm maximum accuraci cluster achiev highest accuraci among the five algorithm it also show fast converg in particular the converg speed the amp algorithm maximum accuraci cluster compar the amp algorithm the k-mean cluster when the two algorithm show similar accuraci this in contrast the common observ the variat bay method often show slower converg the icm algorithm k-mean amp-km vbmf-ma amp-ma k-mean k-mean amp-km vbmf-ma amp-ma k-mean accuraci normal k-mean loss k-mean amp-km vbmf-ma amp-ma k-mean number iter accuraci amp-km vbmf-ma amp-ma iter number figur perform differ normal k-mean loss accuraci number iter need converg dynam averag accuraci iter shown error bar repres standard deviat k-mean amp-km k-mean amp-km accuraci normal k-mean loss number trial number trial figur perform measur in real-data experi normal k-mean loss accuraci the result the trial shown in the descend order perform amp-km the worst two result amp-km the rang in the experi real data we use the orl databas face contain imag human face ten differ imag distinct subject each imag consist pixel whose valu rang we divid imag cluster the k-mean the amp algorithm for the k-mean cluster we adopt the initi method the k-mean also for the amp algorithm becaus random initi often yield empti cluster almost data assign one cluster the paramet estim in the way propos in subsect we ran trial differ initi valu figur summar the result the amp algorithm for the k-mean cluster outperform the standard k-mean algorithm in the trial in term the k-mean loss in trial in term the accuraci the amp algorithm yield one cluster data assign it in two trial the attain minimum valu k-mean loss the k-mean the amp algorithm the accuraci these trial the k-mean the amp algorithm the averag number iter the k-mean the amp algorithm these result demonstr effici the propos algorithm real data
----------------------------------------------------------------

title: 3812-streaming-k-means-approximation.pdf

stream k-mean approxim nir ailon googl research nailon google.com ragesh jaiswal columbia univers rjaiswal gmail.com clair monteleoni columbia univers cmontel ccls.columbia.edu abstract provid cluster algorithm approxim optim k-mean object one-pass stream set make assumpt data algorithm light-weight term memori comput set applic unsupervis learn massiv data set resource-constrain devic two main ingredi theoret work deriv extrem simpl pseudo-approxim batch algorithm k-mean base recent k-mean algorithm allow output center stream cluster algorithm batch cluster algorithm perform small input fit memori combin hierarch manner empir evalu real simul data reveal practic util method introduct commerci social scientif data sourc continu grow unpreced rate increas import algorithm process analyz data oper onlin one-pass stream set goal design light-weight algorithm make one pass data cluster techniqu wide use machin learn applic way summar larg quantiti high-dimension data partit cluster use specif applic problem mani heurist design implement notion cluster output hard evalu approxim guarante respect reason object therefor use k-mean object simpl intuit widely-cit cluster object data euclidean space howev although mani cluster algorithm design k-mean object mind approxim guarante respect object work give one-pass stream algorithm k-mean problem awar previous approxim guarante respect k-mean object shown simpl cluster algorithm oper either onlin stream set extend work arthur vassilvitskii provid bi-criterion approxim algorithm k-mean batch set defin seed procedur choos subset point batch point they show subset give expect o log k -approxim kmean object seed procedur follow lloyd algorithm1 work well practic seed combin algorithm call k-mean o log k approxim algorithm expectation.2 modifi k-mean obtain new algorithm kmean choos subset o k log point show chosen subset depart comput scienc research support darpa award center comput learn system lloyd algorithm popular known k-mean algorithm sinc approxim guarante proven base seed procedur alon purpos exposit denot seed procedur k-mean point give constant approxim k-mean object apart give us bi-criterion approxim algorithm modifi seed procedur simpl analyz defin divide-and-conqu strategi combin multipl bi-criterion approxim algorithm k-medoid problem yield one-pass stream approxim algorithm k-median extend analysi k-mean problem use k-mean k-mean divide-and-conqu strategi yield extrem effici singl pass stream algorithm log k -approxim guarante log log number input point stream amount work memori avail algorithm empir evalu simul real data demonstr practic util techniqu relat work much literatur cluster algorithm orss06 stream algorithm also work combin set design cluster algorithm oper stream set work inspir arthur vassilvitskii guha mention discuss detail k-mean seed procedur previous analyz special assumpt input data order use machin learn applic concern design algorithm extrem light-weight practic k-mean effici simpl perform well practic exist constant approxim k-mean object nonstream set local search techniqu due number work cmts02 give constant approxim algorithm relat k-median problem object minim sum distanc point nearest center rather squar distanc k-mean center must subset input point popular believ algorithm extend work k-mean problem without much degred approxim howev there formal evid yet moreov run time algorithm depend wors linear paramet make algorithm less use practic futur work propos analyz variant algorithm stream cluster algorithm goal yield stream cluster algorithm constant approxim k-mean object final import make distinct line cluster research involv assumpt data cluster common assumpt includ data data admit cluster well separ mean orss06 recent work assum target cluster specif applic data set close constant approxim cluster object contrast prove approxim guarante respect optim k-mean cluster assumpt input data.5 probabilist guarante respect random algorithm preliminari k-mean cluster problem defin follow given point rd weight function goal find a2subset follow quantiti minim x x denot distanc nearest point c. subset clear context denot distanc also two point denot distanc subset altern call cluster call potenti function correspond cluster use term center refer c. comprehens survey stream result literatur refer recent independ work aggarw deshpand kannan extend seed procedur k-mean obtain constant factor approxim algorithm output center they use similar techniqu reduc number center use stronger concentr properti may interest futur work analyz algorithm special case well-separ cluster unweight case assum definit competit ratio b-approxim given algorithm k-mean problem let potenti cluster return input set implicit let cop denot potenti optim cluster cop competit ratio defin worst case ratio algorithm said b-approxim algorithm cop op previous definit might strong approxim algorithm purpos exampl cluster algorithm perform poor constrain output center might becom competit allow output center definit b -approxim call algorithm b -approxim kmean problem output cluster ak center potenti op worst case note simplic measur memori term word essenti mean assum point rd store space k-mean advantag care liber seed k-mean algorithm expect log k -approxim algorithm section extend idea get o log o -approxim algorithm kmean algorithm choos initi center c1 uniform random repeat time choos next center ci select ci probabl d xd x x x denot distanc subset point chosen previous round algorithm k-mean origin definit k-mean algorithm follow lloyd algorithm algorithm use seed step lloyd algorithm known give best result practic hand theoret guarante k-mean come analyz seed step lloyd algorithm analysi focus seed step run time algorithm o nkd algorithm denot set given point point denot distanc point nearest center among center chosen previous round get o log o -approxim algorithm make simpl chang algorithm first set tool analysi basic lemma need follow definit first definit potenti set given cluster potenti respect set denot defin x distanc point nearest point c. lemma lemma let arbitrari cluster cop let cluster one center chosen uniform random exp c cop corollari let arbitrari cluster cop let cluster one center chosen uniform random pr c cop proof proof follow markov inequ lemma lemma let arbitrari cluster cop let arbitrari cluster add random center chosen d2 weight get exp c cop corollari let arbitrari cluster cop let arbitrari cluster add random center chosen d2 weight get pr c cop use k-mean two lemma obtain o log o -approxim algorithm k-mean problem consid follow algorithm choos log center independ uniform random repeat time choos log center independ probabl d xd x x x here denot distanc subset point chosen previous round algorithm k-mean note algorithm almost k-mean algorithm except round choos center pick o log center rather singl center run time algorithm clear o ndk log let ak denot set cluster optim cluster cop let denot cluster ith round choos center let aic denot subset cluster aic cop call subset cluster cover cluster let aiu a\aic subset uncov cluster follow simpl lemma show constant probabl step k-mean pick center least one cluster get cover word let us call event e. lemma pr e proof proof easili follow corollari let xci aic let xui xci ith round either xci xui otherwis former case use corollari show probabl cover uncov cluster round larg latter case show current set center alreadi competit constant approxim ratio let us start latter case lemma event occur xci xui proof get main result use follow sequenc inequ xci xui xci xci cop xci cop use definit xci lemma if xci xui ac proof note round probabl center chosen cluster aic least ci condit event probabl least xc center chosen round satisfi cop uncov cluster aiu mean probabl least chosen center round satisfi cop uncov cluster aiu impli probabl least least one chosen center round satisfi cop uncov cluster aiu use two lemma prove main theorem theorem k-mean o log o -approxim algorithm proof lemma know event aic occur given suppos ith round xc xu lemma if exist then lemma get probabl there exist cluster cover even round i.e end algorithm probabl least algorithm cover cluster case lemma cop shown k-mean random algorithm cluster probabl least give cluster competit ratio singl pass stream algorithm k-mean section provid singl pass stream algorithm basic ingredi algorithm divid conquer strategi defin use bi-criterion approxim algorithm batch set use k-mean o log k -approxim algorithm k-mean o log o -approxim algorithm construct singl pass stream o log k -approxim algorithm k-mean problem next subsect develop tool need stream b -approxim k-mean show simpl stream divide-and-conqu scheme analyz respect k-medoid object use approxim k-mean object first present scheme due case use k-means-approxim algorithm input input point set rd let number desir cluster b -approxim algorithm k-mean object approxim algorithm k-mean object divid group s1 s2 run si get ak center ti ti2 denot induc cluster si si1 si2 sw t1 t2 weight w tij sij run sw get center return algorithm stream divide-and-conqu cluster first note everi batch si size nk algorithm take one pass o nk memori give approxim guarante theorem algorithm output cluster 2b approxim k-mean object a approxim desir number center follow direct approxim properti a respect number center sinc a last algorithm run remain show approxim k-mean object proof appear appendix involv extend analysi case k-mean object use exposit dasgupta lectur note proof due extens straightforward differ follow way k-medoid analysi k-mean object involv squar distanc oppos k-medoid distanc squar triangl inequ invok direct replac applic triangl inequ follow everywher it occur introduc sever factor cluster center chosen rd k-mean problem various part proof save approxim a factor k-medoid problem cluster center must chosen input data use k-mean k-mean divide-and-conqu strategi previous subsect saw a b -approxim algorithm a approxim algorithm a use get a singl pass 2b -approxim stream algorithm two random algorithm k-mean probabl least a log -approxim algorithm k-mean a o log k approxim algorithm approxim factor expect now use two algorithm divide-and-conqu strategi obtain a singl pass stream algorithm use follow algorithm a a divide-and-conqu strategi a run k-mean data log time independ pick cluster smallest cost run k-mean weight versus non-weight note k-mean k-mean approxim algorithm non-weight case point hand divide-andconqu strategi need algorithm a work weight case weight integ note k-mean k-mean easili general weight case weight integ both algorithm comput probabl base cost respect current cluster cost comput take account weight analysi assum point multipl equal integ weight point memori requir remain logarithm input size includ store weight analysi probabl least log n1 algorithm a a log approxim algorithm moreov space requir remain logarithm input size step algorithm run a batch data sinc batch size nk number batch probabl a a log -approxim algorithm batch least condit event divide-and-conqu strategi give a o log k -approxim algorithm memori requir o log k nk time logarithm input size moreov algorithm run time o dnk log log improv memory-approxim tradeoff we saw last section obtain a single-pass cbb approxim k-mean use first b -approxim input block then approxim union output center set global constant optim memori requir scheme o a immedi impli a tradeoff memori requir grow like number center output a approxim potenti cbb respect optim solut use center a subtl tradeoff possibl a recurs applic techniqu multipl level inde b -approxim could broken turn two level idea use here we make a precis account tradeoff differ paramet assum we subroutin perform bi approxim k-mean batch mode we choos a1 ar b1 br later we hold buffer b1 br work area size buffer bi mi topmost level we divid input equal block size m1 run b1 approxim algorithm block buffer b1 repeat reus this task applic approxim algorithm output set ka1 center ad b2 when b2 fill we run b2 approxim algorithm data add ka2 output center b3 this continu buffer br fill ar br approxim algorithm output final ar center let ti denot number time th level algorithm execut clear we ti kai last stage we tr mean mr mr general ti mr r m1 we must also t1 impli a1 order minim total memori mi last constraint use standard argument multivari analysi we must m1 mr word mi nk a1 result one-pass algorithm approxim guarante ar b1 br use a straightforward extens result previous section memori requir rn1/r assum now we realist set avail memori fix size we choos we choos either run k-mean repeat k-mean algorithm a previous subsect bi o log log we choos k-mean ar br o log we interest output exact center final solut let denot number we assum quotient integ simplic proof note fraction block would aris practic cost unit batch lloyd divid conquer km divid conquer cost unit cost unit batch lloyd divid conquer km divid conquer batch lloyd onlin lloyd divid conquer km divid conquer figur cost mixtur gaussian simul cloud data spam data index bi log discuss memori use optim if rn1/r log k q/r case final approxim guarante log k r q global we concentr case grow polynomi say this case memori optim constraint impli larg enough regardless choic this impli final approxim guarante best if word we choos repeat k-mean level k-mean level summar we get theorem if there access memori size fix then suffici larg best applic multi-level scheme describ obtain run log log level choos repeat k-mean last level k-mean chosen result algorithm a random one-pass stream approxim k-mean approxim ratio log global run time algorithm o dnk log log we compar multi-level stream algorithm state-of-art term memori approxim tradeoff stream algorithm k-median problem charikar callaghan panigrahi give a one-pass stream algorithm k-median problem give a constant factor approxim use o k poli log n memori main problem this algorithm a practic point view averag process time per item larg it proport amount memori use poly-logarithm in this might undesir in practic scenario where we need process a data item quick when it arriv in contrast averag per item process time use divide-and-conquer-strategi constant furthermor algorithm pipelin data item temporarili store in a memori buffer quick process next memori buffer fill so even if extend k-mean set stream algorithm base divide-and-conquer-strategi would interest a practic point view experi dataset in discuss denot number point in data denot dimens denot the number cluster first evalu detail in tabl figur compar algorithm the follow data norm25 synthet data generat in the follow manner we choos random vertic from a dimension hypercub side length we then add gaussian random point varianc around points.8 so this data the optimum cost the uci cloud dataset consist cloud cover data here the uci spambas dataset data e-mail spam detect task here compar a baselin method known use in practic we use lloyd algorithm common refer as the k-mean algorithm standard lloyd algorithm oper in the batch set easier problem the one-pass stream set so we ran experi this algorithm form a baselin we also compar onlin version lloyd algorithm howev the perform wors the batch version method problem so we test cluster algorithm this simul distribut inspir bl ol bl ol bl ol bl ol bl ol bl ol tabl column the cluster cost column time in sec norm25 dataset cloud dataset spambas dataset memori level cost time memori level cost time memori level cost time tabl multi-level hierarchi evalu cloud dataset a subset norm25 dataset spambas dataset the memori size decreas as the number level the hierarchi increas level mean run batch k-mean the data includ it in plot the real data sets.9 tabl show averag k-mean cost random restart the random algorithm but onlin lloyd these algorithm bl batch lloyd initi random center in the input data run to convergence.10 ol onlin lloyd the simpl 1-stage divid conquer algorithm section the simpl 1-stage divid conquer algorithm section the sub-algorithm use a run k-mean log time pick best cluster a k-mean in context k-mean k-mean the seed step follow lloyd algorithm in problem stream method achiev much lower cost onlin lloyd for set lower cost batch lloyd for set includ the correct in the gain with respect to batch noteworthi sinc the batch problem less constrain the one-pass stream problem the perform compar tabl show evalu the one-pass multi-level hierarch algorithm section on the differ dataset simul differ memori restrict although worst-cas theoret result impli exponenti cluster cost as a function the number level our result show a far optimist outcom in ad level limit memori actual improv the outcom we conjectur our data contain enough inform for cluster even on chunk fit in small buffer therefor the result may reflect the benefit the hierarch implement acknowledg we thank sanjoy dasgupta for suggest the studi approxim algorithm for k-mean in the stream set for excel lectur note for help discuss despit the poor perform we observ this algorithm appar use in practic see we measur converg by chang in cost less
----------------------------------------------------------------

title: 1076-learning-sparse-perceptrons.pdf

learn spars perceptron jeffrey c. jackson mathemat comput scienc dept duquesn univers forb ave pittsburgh pa jackson mathcs.duq.edu mark w. craven comput scienc dept univers wisconsin-madison west dayton st. madison wi craven cs.wisc.edu abstract introduc new algorithm design learn spars perceptron input represent includ high-ord featur algorithm base hypothesis-boost method abl pac-learn relat natur class target concept moreov algorithm appear work well practic set three problem domain algorithm produc classifi util small number featur yet exhibit good general perform perhap import algorithm generat concept descript easi human understand introd uction multi-lay perceptron mlp learn power method task concept classif howev mani applic may involv scientif discoveri crucial abl explain predict multi-lay perceptron limit regard sinc represent notori difficult human understand present approach learn understand yet accur classifi specif algorithm construct spars perceptron single-lay perceptron relat non-zero weight algorithm learn spars perceptron base new hypothesi boost algorithm freund schapir although algorithm initi develop learning-theoret point view retain certain theoret guarante pac-learn class spars perceptron also work well practic experi number real-world domain indic algorithm produc perceptron relat comprehens exhibit general perform compar backprop-train mlp 's rumelhart better decis tree learn use quinlan learn spars perceptron contend spars perceptron unlik mlp 's comprehens relat paramet paramet describ simpl linear relationship evid spars perceptron comprehens consid linear function common use express domain knowledg field medicin spackman molecular biolog stormo spars perceptron perceptron weight threshold set input featur higherord featur consist function oper limit number input featur inform spars perceptron perceptron relat non-zero weight later theoret result need precis definit spars develop consid boolean function i let ck set conjunct input i. includ conjunct input take ident function function map everi conjunct occur posit sens repres true negat sens repres true function i k-perceptron integ hi ck sign undefin y/lyl otherwis note explicit shown weight definit k-perceptron i integ weight implicit present allow particular hi ck appear sum defin i fact often conveni think k-perceptron simpl linear discrimin function integ weight defin featur space o nk featur one featur element ck call given collect conjunct hi k-perceptron represent correspond function i call size represent defin size given k-perceptron function i minim size k-perceptron represent i s-spars k-perceptron k-perceptron i size i denot pi set boolean function repres k-perceptron defin pk un pi subclass s-spars k-perceptron denot also interest class k-perceptron real-valu weight non-zero learn algorithm section develop learn algorithm prove certain perform guarante algorithm base recent hypothesi boost algorithm describ review basic learning-theori terminolog pac learn hypothesi boost follow valiant say function class pk fix strong pac-learn algorithm polynomi function pi posit i target junction probabl distribut domain i probabl least algorithm ex f produc function hypothesi pr prd x i hex outermost probabl random choic made ex oracl random choic made here ex f denot oracl queri choos vector input valu probabl return pair learn algorithm must run time pi length input vector i size j. c. jackson m. w. craven adaboost input train set exampl function y -approxim algorithm weak learn algorithm wl xes l/m xes di x invok wl distribut di produc weak hypothesi hi l l=l l z di x xes enddo output sign figur adaboost algorithm algorithm charg one unit time call ex sometim call function output approxim strong approxim respect d. pac-learn algorithm output hypothes class say pac-learn if paclearn where'p2 polynomi function weak pa c-learnabl output hypothesi case call weak approxim algorithm find spars perceptron indic earlier base notion hypothesi boost specif boost algorithm use figur version recent adaboost algorithm freund schapir next section appli adaboost boost weak learn algorithm strong learner adaboost given set exampl function weak learn algorithm wl take given must bound invers polynomi nand adaf300st run stage stage creat probabl distribut di train set invok wl find weak hypothesi hi respect di note exampl oracl ex j di simul given di end stage final hypothesi output weight threshold weak hypothes hi i t if weak learner succeed produc y -approxim stage adaboost 's final hypothesi guarante consist train set freund schapir pac-learn spars k-perceptron show spars k-perceptron pac learnabl real-weight kperceptron relat nonzero weight specif ignor log factor learnabl constant first show given train set effici find consist consist algorithm basi algorithm later appli empir learn problem show turn consist algorithm pac learn algorithm proof implicit somewhat general work freund although actual present learn algorithm class analyz learn spars perceptron sampl size need ensur f-approxim follow freund begin develop follow lemma goldmann lemma goldmann hastad razhorov i set function domain rang il i repres sign l :=l hi lor probabl distribut hi prd f x if special lemma take ck recal ck set conjunct input featur impli i probabl distribut input featur i hi ck weak approxim i respect d. therefor given train set distribut nonzero weight instanc follow simpl algorithm weak learn algorithm pk exhaust test o nk possibl conjunct featur find conjunct approxim i respect effici comput approxim conjunct hi sum valu input hi i agre conjunct return weak hypothesi lemma prove if i k-perceptron exhaust search must succeed find hypothesi therefor given train set exampl s-spars k-perceptron i adaboost run weak learner stage produc hypothesi consist train set stage add one weak hypothesi output hypothesi final hypothesi real-weight k-perceptron nonzero weight convert consist algorithm pac learn algorithm follow first given finit set function straightforward show follow see haussler lemma let finit set ollunct domain function lover probabl distribut posit given set ofm exampl drawn consecut ex f ifi pr 3h i ix outer probabl random choic made consist algorithm find consist hypothesi also base result bruck shown ip~i kr log therefor ignor log factor randomly-gener train set size o ks4 if suffici guarante high probabl algorithm produc f-approxim s-spars k-perceptron target word follow pac algorithm comput suffici larg polynomi pac paramet draw exampl ex f creat train set run consist algorithm train set far shown spars k-perceptron learnabl spars perceptron hypothes potenti polynomially-mani weight practic cours expect mani real-world classif task perform exact spars perceptron fact shown certain reason definit noisi spars perceptron loos function approxim reason well spars perceptron class noisi spars k-perceptron still pac-learn claim base result aslam decatur present noise-toler boost algorithm fact sever differ boost algorithm could use learn pk s freund chosen use adaboost seem offer signific practic advantag particular term effici also empir result date indic algorithm j. c. jackson m. w. craven work well difficult presum noisi real-world problem howev one potenti advantag base algorithm one earlier booster instead adaboost algorithm would produc perceptron integ weight still maintain spars guarante adaboostbas algorithm practic consider turn practic detail algorithm base consist algorithm first note theori develop work discret input domain boolean nominal-valu featur thus paper consid task discret input featur also becaus algorithm use exhaust search conjunct size learn time depend exponenti choic studi use throughout sinc choic result reason learn time anoth implement concern involv decid learn algorithm termin consist algorithm use size target function calcul number boost stage cours size inform avail real-world applic fact target function may exact represent spars perceptron practic we use cross valid determin appropri termin point facilit comprehens we also limit number boost stage number weight would occur an ordinari perceptron task similar reason we also modifi criteria use select weak hypothesi stage so simpl featur prefer conjunct featur particular given distribut stage hi ck we comput correl we multipli high-ord featur 's correl hi largest result correl serv weak hypothesi stage empir evalu experi we interest assess general abil complex hypothes produc algorithm we compar algorithm ordinari perceptron train use backpropag rumelhart multi-lay perceptron train use backpropag decis tree induc use system quinlan we use experi repres symbol learn algorithm symbol algorithm wide believ learn hypothes comprehens neural network addit test hypothesi perform algorithm explain sole use second-ord featur we train ordinari perceptron use featur set includ pairwis conjunct well as ordinari featur test hypothesi perform algorithm explain use relat weight we consid ordinari perceptron prune use variant optim brain damag obd algorithm le cun in version obd we train perceptron stop criteria met prune weight smallest salienc then iter process we use valid set decid stop prune weight train set we use cross-valid select number hidden unit mlp 's prune confid level tree we use valid set decid stop train mlp we evalu algorithm use three real-world domain vote data set uc-irvin databas promot data set a complex superset learn spars perceptron domain vote promot code boost ta ble 11est set accuraci perceptron multi-lay ordinari 2nd-order tabl hypothesi complex weight perceptron domain boost multi-lay ordinari 2nd-order vote promot protein code prune prune uc-irvin one a data set in task recogn protein-cod region in dna craven shavlik we remov physician-fee-freez featur vote data set make problem difficult we conduct experi use a lo-fold cross valid methodolog except in protein-cod domain becaus certain domain-specif characterist data set we use 4-fold cross-valid experi tabl report test-set accuraci method three domain we measur statist signific accuraci differ use a pair two-tail t-test symbol mark result in case anoth algorithm less accur boost algorithm at level signific no algorithm signific better boost method in ani domain result we conclud algorithm exhibit good general perform number interest real-world problem general perform algorithm explain sole use second-ord featur sole explain spars perceptron produc an interest open question whether perceptron train prune second-ord featur abl match accuraci algorithm we plan investig this question in futur work tabl report averag number weight perceptron three problem algorithm produc perceptron fewer weight mlp 's ordinari perceptron perceptron second-ord featur size obd-prun perceptron produc algorithm compar for three domain recal howev for three task perceptron learn algorithm signific better general perform similar-s obd-prun counterpart we contend size perceptron produc algorithm within bound human readili understand in the biolog literatur for exampl linear discrimin function frequent use communic domain knowledg sequenc interest these function frequent involv weight the perceptron produc algorithm we conclud therefor algorithm produc hypothes accur also comprehens we believ the result the protein-cod domain especi interest the input represent for this problem consist nomin featur repres consecut base in a dna sequenc in the region dna encod protein the posit exampl in our task non-overlap triplet consecu j. c. jackson m. w. era ven tive base repres meaning word call codon in previous work craven shavlik found a featur set explicit repres codon result in better general a represent base howev we use the base represent in our experi in order investig the abil our algorithm select the right second-ord featur interest near all the second-ord featur includ in our spars perceptron repres conjunct base in the codon this result suggest our algorithm especi good at select relev featur from larg featur set futur work our present algorithm a number limit we plan address two area of current research general the algorithm for applic to problem real-valu featur develop method for automat suggest highord featur to includ in our algorithm 's featur set acknowledg mark craven partial support onr grant jeff jackson partial support nsf grant
----------------------------------------------------------------

title: 1194-an-apobayesian-relative-of-winnow.pdf

apobayesian relat winnow nick littleston nec research institut independ way princeton nj chris mesterharm nec research institut independ way princeton nj abstract studi mistake-driven variant on-lin bayesian learn algorithm similar one studi cesa-bianchi helmbold panizza variant updat state learn trial make mistak algorithm make binari classif use linear-threshold classifi run time linear number attribut seen learner abl show theoret simul algorithm perform well assumpt quit differ embodi prior origin bayesian algorithm handl situat know handl linear time bayesian algorithm expect techniqu use deriv analyz apobayesian algorithm introduct consid two style on-lin learn case learn proceed sequenc trial trial learner observ instanc classifi make predict classif observ label give correct classif one style on-lin learn consid bayesian learner use probabilist assumpt world embodi prior model class data observ past trial construct probabilist model embodi posterior distribut model class learner use model make predict current trial learner told correct classif instanc learner use inform updat model generat new posterior use next trial style learn consid attent correct predict rather model world intern state apobayesian relat winnow learner chang learner make mistak predict fail match label call algorithm mistake-driven algorithm often call conserv comput learn theori literatur simpl way deriv mistake-driven algorithm anyon-lin learn algorithm restrict attent paper determinist algorithm deriv algorithm like origin algorithm except everi trial make record entir state everi trial predict correct reset state match record state entir forget interven trial typic actual implement make record mere omit step updat state exampl algorithm keep track number trial seen mistake-driven version algorithm end keep track number mistak made whether origin mistake-driven algorithm better depend task algorithm evalu start bayesian learn algorithm call sbsb use procedur deriv mistake-driven variant sasb note variant expect bayesian learn algorithm least ordinari sens sinc bayesian algorithm would make predict minim bay risk base avail data mistake-driven variant forgotten quit bit call algorithm apobayesian learn algorithm name intend suggest deriv bayesian learn algorithm bayesian algorithm sasb close algorithm studi applic differ task analyz perform appli linear separ data describ paper instanc chosen instanc space l n thus instanc compos boolean attribut consid two categori classif task predict label chosen i obtain bound number mistak sasb make compar bound various winnow famili algorithm given algorithm bound hold assumpt point label linear separ point label bound depend size gap two class see section definit mistak bound sasb log while bound extra factor log present bound winnow algorithm sasb advantag need paramet winnow famili algorithm paramet algorithm mistak bound depend set paramet valu depend often valu known learner expect techniqu use obtain bound use analyz apobayesian learn algorithm number author done relat research regard worst-cas on-lin loss bound includ simul experi involv bayesian algorithm mistake-driven variant describ paper provid use background paper note present analysi techniqu appli apobayesian algorithm studi closest origin winnow famili algorithm sasb appear weight major algorithm analyz case similar consid paper one get rough correct impress sasb n. littleston c. mesterharm one think version weight major algorithm learn paramet next section describ bayesian algorithm start section discuss mistake-driven apobayesian variant section mention simul experi use algorithm section conclus bayesian learn algorithm describ bayesian learn algorithm must specifi famili distribut prior famili distribut parameter distribut paramet 8n chosen paramet 8n give probabl label paramet 8i give probabl ith attribut match label note probabl ith attribut given label equal probabl ith attribut given label speak linkag probabl two class symmetri condit linkag observ point either class affect posterior distribut class perhap typic choos prior allow two class treat separ posterior class give probabl element condit label depend prior observ class symmetri condit impos appear import success analysi apobayesian variant algorithm though impos condit deriv algorithm turn apobayesian variant actual handl task condit satisfi choos prior give probabl set element 8n one equal prior uniform set note set singl attribut probabl match label thus singl attribut relev concentr set turn lead apobayesian algorithm fact handl one relev attribut perform particular well small fraction attribut relev prior relat familiar naiv bay model also assum attribut condit independ given label howev typic naiv bay model restrict singl relev attribut symmetri condit link two class impos prior lead follow algorithm name sbsb stand symmetr bayesian algorithm singly-vari prior bernoulli distribut algorithm sbsb algorithm sbsb maintain count si number time attribut match label count number time label count number trial initi predict si t-o tt-o predict given instanc xicsi+l +clixi t-si+1 i=l updat i=l si si apobayesian relat winnow apobayesian algorithm construct apobayesian algorithm convert algorithm sbsb mistake-driven algorithm use standard convers given introduct call result learn algorithm sasbj we replac bayesian apobayesian acronym previous section we made assumpt made generat instanc label led sbsb thenc sasb these assumpt serv purpos we abandon analyz apobayesian algorithm we assum instanc label generat stochast process instead we assum instance-label pair trial linearly-separ exist wi wn everi instance-label pair we wixi wixi we actual make somewhat stronger assumpt given follow theorem give bound apobayesian algorithm theorem suppos yi ii yi i suppos bo bi let bi bo suppos algorithm sasb run sequenc trial instanc label trial satisfi yixi ii bo yixi ii bi then number mistak made sasb bound log we space say littl deriv this bound proceed detail given analyz sasb we work abstract descript associ algorithm sbsb this algorithm start prior describ we repres this densiti po then trial calcul new posterior densiti pt o t-d8kp x'yi~k pt densiti trial ylo pt-d condit probabl ofth instanc label observ trial given thus we think algorithm maintain current distribut initi prior sasb similar leav current distribut unchang mistak made exist finit mistak bound must exist possibl choic current distribut sasb would make perfect predict ever arriv distribut we call distribut lead perfect predict possibl target distribut turn separ condit given theorem guarante suitabl target distribut exist analysi proceed show appropri choic target densiti relat entropi current distribut respect target distribut log p pt decreas least amount whenev mistak made sinc relat entropi never negat number mistak bound initi relat entropi divid r. this form analysi similar analysi various member winnow famili techniqu appli apobayesian algorithm abstract updat pt given quit general success analysi depend condit po ylo we space discuss n. littleston c. mesterharm optim sbsb sasb sasb vote optim sbsb sasb sasb vote trial trial figur comparison sasb sbsb simul experi bound previous section perfect linearly-separ data we also done simul experi explor perform sasb non-separ data compar sbsb various mistakedriven algorithm sampl comparison sasb sbsb shown figur experiment run we generat trial instanc label chosen random accord distribut specifi ok ok+l on+l interpret specifi section number attribut specifi top plot line label optim show perform obtain optim predictor know distribut use generat data ahead time thus need learn line label sbsb sasb show perform correspond learn algorithm line label sasb vote show perform sasb addit vote procedur describ this procedur improv asymptot mistak rate algorithm each line graph averag run each line plot cumul number mistak made algorithm begin run function number trial left hand plot relev attribut this exact case sbsb intend better sasb right hand plot there relev attribut sbsb appear unabl take advantag extra inform present extra relev attribut sasb success comparison sasb previous winnow famili algorithm still progress we defer present detail clearer pictur obtain sasb weight major algorithm often perform similar in simul typic one would expect weight major algorithm somewhat better an apobayesian relat winnow sasb paramet chosen optim particular learn task wors bad choic paramet conclus our mistak bound simul suggest sasb may use altern exist algorithm in winnow famili base analysi style bound sasb perhap consid winnow famili algorithm further experi in progress compar sasb winnow famili algorithm run varieti paramet set perhap even greater interest potenti applic our analyt techniqu varieti apobayesian algorithm though we observ earlier techniqu appear appli algorithm we alreadi obtain preliminari result regard an interpret perceptron algorithm an apobayesian algorithm we interest in look entir new algorithm deriv in this way also in better understand scope applic our techniqu analys we look depend symmetri condit relat probabl two class it would interest see said when symmetri condit hold in simul experi mistake-driven variant standard naiv bay algorithm often well despit absenc symmetri in prior it base our simul experi also analysi relat algorithm winnow suggest sasb expect handl instance-label pair insid separ gap wrong side especi far wrong side in particular it appear abl handl data generat accord distribut sbsb base in general yield perfect separ data it interest compar capabl origin bayesian algorithm deriv apobayesian algorithm when data stochast generat in manner consist assumpt behind origin algorithm origin bayesian algorithm expect better see exampl figur hand apobayesian algorithm handl data beyond capabl origin bayesian algorithm exampl in case we consid apobayesian algorithm take advantag the presenc one relev attribut even though the prior behind the origin bayesian algorithm assum singl relev attribut furthermor as for the winnow famili algorithm the mistak bound for the apobayesian algorithm depend on detail the behavior the irrelev attribut includ redund attribut instead use the apobayesian variant one might tri construct bayesian learn algorithm for a prior reflect the actual depend among the attribut the label howev it may clear the appropri prior it may particular unclear model the behavior the irrelev attribut furthermor such a bayesian algorithm may end comput expens for exampl attempt keep track correl among all pair attribut may lead to an algorithm need time space quadrat in the number attribut on the hand we start a bayesian algorithm n. littleston c. mesterharm use time space linear in the number attribut we obtain an apobayesian algorithm still use linear time space that handl situat beyond the capabl the origin bayesian algorithm acknowledg this paper benefit discuss with adam grove
----------------------------------------------------------------

title: 2434-semi-definite-programming-by-perceptron-learning.pdf

semidefinit program perceptron learn ralf herbrich thore graepel microsoft research ltd cambridg uk thoreg rherb microsoft.com andriy kharechko john shawe-taylor royal holloway univers london uk ak03r jst ecs.soton.ac.uk abstract present modifi version perceptron learn algorithm pla solv semidefinit program sdps polynomi time algorithm base follow three observ semidefinit program linear program infinit mani linear constraint everi linear program solv sequenc constraint satisfact problem linear constraint iii general perceptron learn algorithm solv constraint satisfact problem linear constraint finit mani updat combin pla probabilist rescal algorithm averag increas size feasabl region result probabilist algorithm solv sdps run polynomi time present preliminari result demonstr algorithm work competit state-of-the-art interior point method introduct semidefinit program sdp one activ research area optimis appeal deriv import applic combinatori optimis control theori recent develop effici algorithm solv sdp problem depth eleg under optimis theori cover linear quadrat second-ord cone program special case recent semidefinit program discov use toolkit machin learn applic rang pattern separ via ellipsoid kernel matrix optimis transform invari learn method solv sdps most develop analog linear program generalis simplex-lik algorithm develop sdps best knowledg current mere theoret interest ellipsoid method work search feasibl point via repeat halv ellipsoid enclos affin space constraint matric centr ellipsoid feasibl point howev method show poor perform practic run time usual attain worst-cas bound third set method solv sdps interior point method method minimis linear function convex set provid set endow self-concord barrier function sinc barrier function known sdps interior point method current effici method solv sdps practic consid great general semidefinit program complex state-of-the-art solut method quit surpris forti year old simpl perceptron learn algorithm modifi solv sdps paper present combin perceptron learn algorithm pla rescal algorithm origin develop lps abl solv semidefinit program polynomi time start short introduct semidefinit program perceptron learn algorithm section section present main algorithm togeth perform guarante whose proof sketch due space restrict numer result present section preliminari give insight work algorithm demonstr machin learn may someth offer field convex optimis rest paper denot matric vector bold face upper lower case letter shall use kxk denot unit length vector direct notat use denot ax posit semidefinit learn convex optimis semidefinit program semidefinit program linear object function minimis imag affin transform cone semidefinit matric express linear matrix inequ minimis x r c0 subject f0 fi rn fi rm follow proposit show semidefinit program direct generalis linear program proposit everi semidefinit program linear program infinit mani linear constraint proof obvious object function linear rm defin vector au f1 u0 fn constraint written rm u0 rm linear constraint au f0 infinit mani sinc object function linear solv sdp sequenc semidefinit constraint satisfact problem csps introduc addit constraint c0 c0 vari c0 r. moreov follow proposit proposit ani sdp solv sequenc homogenis semidefinit csps follow form find subject gi algorithm perceptron learn algorithm requir possibl infinit set vector rn set exist x0t end return proof order make f0 c0 depend optimis variabl introduc auxiliari variabl solut origin problem given moreov repos two linear constraint c0 lmi use fact block-diagon matrix posit semi definit everi block posit semi definit thus follow matric suffici f0 fi ci g0 c0 gi given upper lower bound object function repeat bisect use determin solut o log step accuraci order simplifi notat assum whenev speak semidefinit csp sdp variabl fi rm perceptron learn algorithm perceptron learn algorithm pla onlin procedur find linear separ set point origin algorithm machin learn algorithm usual appli two set point label multipli everi data vector class label1 result vector often refer weight vector perceptron learn read normal hyperplan separ set remark properti perceptron learn algorithm total number updat independ cardin upper bound simpli term follow quantiti maxn maxn min a0 x r x r quantiti known normalis margin machin learn communiti radius feasibl region optimis communiti quantifi radius largest ball fit convex region enclos so-cal feasibl set perceptron converg theorem state purpos paper observ algorithm solv linear csp linear constraint given vector moreov last argument follow proposit proposit feasibl set posit radius perceptron learn algorithm solv linear csp finit mani step worth mention last decad seri modifi plas develop good overview main aim guarante note sometim updat equat given use unnormalis vector algorithm rescal algorithm requir maxim number step paramet set uniform random kzk pun smallest ev find au exist set gi gi return end au au end return unsolv feasibl solut also lower bound these guarante usual come price slight larger mistak bound shall denot semidefinit program perceptron learn combin proposit togeth equat obtain perceptron algorithm sequenti solv sdps howev remain two problem find vector make run time algorithm polynomi descript length data order address first problem notic algorithm explicit given defin virtu gn au g1 u0 gn rm henc find vector au au equival identifi vector rm u0 gi u0 one possibl way find vector consequ au current solut algorithm calcul eigenvector correspond smallest eigenvalu eigenvalu posit algorithm stop output note howev comput easier procedur appli find suitabl rm also section second problem requir us improv depend runtim end employ probabilist rescal algorithm algorithm origin develop lps purpos algorithm enlarg feasibl region term gn constant factor averag would impli decreas number updat perceptron algorithm exponenti number call rescal algorithm achiev run algorithm algorithm return unsolv rescal procedur gi effect au chang au au everi rm order abl reconstruct solut origin problem whenev rescal gi need rememb vector use rescal figur shown effect rescal three linear con2 note polynomi runtim guarante gn bound a polynomi function descript length data figur illustr rescal procedur shown feasibl region one feasibl point left left rescal feasibl point straint r3 main idea algorithm find a vector close current feasibl region henc lead increas radius use rescal follow properti hold algorithm theorem assum algorithm return unsolv let radius feasibl set rescal radius feasibl set rescal assum 4n probabl 4n probabl least probabilist natur theorem stem fact rescal shown increas size feasibl region if random initi valu alreadi point suffici close feasibl region a consequ theorem averag radius increas algorithm combin rescal perceptron learn result a probabilist polynomi runtim algorithm3 altern call algorithm algorithm may return infeas two case either ti mani call algorithm return unsolv mani call algorithm togeth rescal return a solut these two condit either happen unlucki draw algorithm a gn small follow argument one show min total probabl return infeas despit a gn min exceed exp experiment result experi report section fall two part initi aim demonstr method work practic assess efficaci a note assum optimis problem line algorithm solv polynomi time algorithm newton-raphson algorithm posit definit perceptron algorithm requir g1 gn rm maxim number iter set call algorithm a 4n mani updat if algorithm converg then return bx ln set ti ti call algorithm if algorithm return then yy goto outer for-loop end return infeas end return infeas benchmark exampl graph bisect these experi would also indic how competit baselin method compar solver algorithm implement in matlab experi run machin time taken compar a standard method sdpt3 partial implement in run matlab we consid benchmark problem aris semidefinit relax maxcut problem weight graph pose find a maximum weight bisect a graph benchmark maxcut problem follow relax sdp form subject diag c1 diag minimis x rn f0 fi rn n adjac matrix graph vertic benchmark use provid sdplib problem known optim valu object function equal baselin method use bisect approach identifi critic valu object refer throughout section c0 figur left show a plot time per iter valu c0 first four iter bisect method seen plot time taken algorithm each iter quit long time fourth iter around second initi valu c0 found without object constraint converg within sec bisect then start lower infeas valu upper valu iter run c0 feasibl solut object valu this found in sec second iter use a valu c0 slight optimum third iter infeas sinc quit far optimum algorithm abl deduc this fact quit quick final iter also infeas much closer optim valu run time suffer correspond take hour if we continu next iter would also infeas closer optimum would take even longer first experi demonstr sever thing first method inde work predict second run time far time sec optim valu optim valu valu object function valu object function iter figur left four iter bisect method show time taken per iter outer for loop in algorithm valu object constraint right decay attain object function valu while iter algorithm a non-zero threshold competit sdpt3 take second solv this problem third run time increas valu c0 approach optimum iter must prove infeas cost find a solut final observ prompt our first adapt base algorithm rather perform search use bisect method we implement a non-zero threshold object constraint the while-stat in algorithm the valu this threshold denot follow the notat introduc in use a valu ensur a feasibl solut found object valu signific the object constraint c0 figur right show the valu c0 a function the outer for-loop iter the algorithm eventu approach estim the optim valu this within the optimum though cours iter could continu despit the clear converg use this approach the run time accur estim the solut still prohibit overal the algorithm took approxim hour cpu time find it solut a profil the execut howev reveal the execut time spent in the eigenvalu decomposit identifi observ we need a minim eigenvector perform updat simpli a vector satisfi u0 g x u choleski decomposit either return satisfi it converg indic psd algorithm converg conclus semidefinit program interest applic in machin learn in turn we shown how a simpl learn algorithm modifi solv higher order convex optimis problem semidefinit program although the experiment result given suggest the approach far comput competit the insight gain may lead effect algorithm in concret applic in the way for exampl smo a competit algorithm for solv quadrat program problem aris support vector machin while the optimis set lead the somewhat artifici ineffici bisect method the posit definit perceptron algorithm excel solv posit definit csps found in problem transform invari pattern recognit solv semidefinit program machin in futur work it interest consid the combin primal-du problem a predefin level granular as avoid the necess bisect search acknowledg we would like to thank j. kandola j. dunagan a. ambroladz for interest discuss this work support epsrc grant number microsoft research cambridg
----------------------------------------------------------------

title: 5438-optimal-regret-minimization-in-posted-price-auctions-with-strategic-buyers.pdf

optim regret minim posted-pric auction strateg buyer mehryar mohri courant institut googl research mercer street new york ny medina andr munoz courant institut mercer street new york ny mohri cims.nyu.edu munoz cims.nyu.edu abstract studi revenu optim learn algorithm posted-pric auction strateg buyer analyz broad famili monoton regret minim algorithm problem includ previous best known algorithm show algorithm famili admit strateg regret favor introduc new algorithm achiev strateg regret differ lower bound factor log exponenti improv upon previous best algorithm new algorithm admit natur analysi simpler proof idea behind design general also report result empir evalu compar algorithm previous state art show consist exponenti improv sever differ scenario introduct auction long activ area research econom game theori vickrey milgrom weber ostrovski schwarz past decad howev advent onlin advertis prompt algorithm studi auction includ design learn algorithm revenu maxim general second-pric auction second-pric auction reserv cesa-bianchi mohri mu noz medina studi larg motiv widespread use adexchang vast amount histor data therebi collect adexchang advertis sell platform use second-pric auction reserv price alloc advertis space thus far learn algorithm propos revenu maxim auction critic reli assumpt bid outcom auction drawn accord unknown distribut howev assumpt may hold practic particular knowledg revenu optim algorithm use advertis could seek mislead publish under-bid fact consist empir evid strateg behavior advertis found edelman ostrovski motiv analysi present paper interact seller strateg buyer buyer may act non-truth goal maxim surplus scenario consid posted-pric auction albeit simpler mechan fact match common situat adexchang mani auction admit singl bidder set second-pric auction reserv equival posted-pric auction seller set reserv price good buyer decid whether accept bid higher reserv price order captur buyer strateg behavior analyz onlin scenario time price pt offer seller buyer must decid either accept leav scenario model two-play repeat non-zero sum game incomplet inform seller object maxim revenu advertis seek maxim surplus describ detail section literatur non-zero sum game rich nachbar morri much work area focus character differ type equilibria direct relev algorithm question aris furthermor problem consid admit particular structur exploit design effici revenu optim algorithm seller perspect game also view bandit problem kuleshov precup robbin sinc revenu reward price offer access seller kleinberg leighton precis studi continu bandit set assumpt oblivi buyer one exploit seller behavior precis author assum round seller interact differ buyer author present tight regret bound log log scenario buyer hold fix valuat regret bound o t face adversari buyer use eleg reduct discret bandit problem howev argu amin deal strateg buyer usual definit regret longer meaning inde consid follow exampl let valuat buyer given assum algorithm sublinear regret exp3 auer ucb auer use round seller possibl strategi buyer know seller algorithm would accept price smaller small valu certain seller would eventu learn offer price less buyer would consider boost surplus theori seller would incur larg regret sinc hindsight best fix strategi would offer price round howev clear optim seller stronger notion polici regret introduc arora shown appropri one analysi bandit problem adapt adversari howev exampl describ sublinear polici regret similar achiev thus notion regret also pertin one studi scenario adopt instead definit strategic-regret introduc amin precis studi problem notion regret also match concept learn loss introduc agraw face oblivi adversari use definit amin present upper lower bound regret seller face strateg buyer show buyer surplus must discount time order abl achiev sublinear regret see section howev gap upper lower bound present follow analyz broad famili monoton regret minim algorithm problem section includ algorithm amin show algorithm famili admit strateg regret favor next introduc nearly-optim algorithm achiev strateg regret differ lower bound factor o log section repres exponenti improv upon exist best algorithm set new algorithm admit natur analysi simpler proof key idea behind design method deter buyer lie reject price valuat setup consid follow game play buyer seller good advertis space repeat offer sale seller buyer round buyer hold privat valuat good round price pt offer seller decis made buyer take valu buyer accept buy price otherwis say buyer lie whenev pt begin game algorithm use seller set price announc buyer thus buyer play strateg algorithm knowledg standard assumpt mechan design also match practic adexchang defin discount surplus buyer follow sur pt valu discount factor indic strength prefer buyer current surplus versus futur one perform seller algorithm measur notion strategic-regret amin defin follow reg pt buyer object maxim discount surplus seller seek minim regret note view discount factor buyer fulli adversari problem consist design algorithm achiev sublinear strateg regret regret o t motiv behind definit strategic-regret straightforward seller access buyer valuat set fix price good close valu buyer control price offer option accept price order optim util revenu per round seller therefor sinc scenario higher revenu achiev natur set compar perform algorithm gain intuit problem let us examin complic aris deal strateg buyer suppos seller attempt learn buyer valuat perform binari search would natur algorithm face truth buyer howev view buyer knowledg algorithm best interest lie initi round therebi quick fact exponenti decreas price offer seller seller would incur regret binari search approach therefor aggress inde untruth buyer manipul seller offer price less lie valu even discuss suggest follow conserv approach next section discuss natur famili conserv algorithm problem monoton algorithm follow conserv price strategi introduc amin let p1 price pt reject round lower price pt offer next round time price pt accept price offer remain round denot algorithm monoton motiv behind design clear suitabl choic seller slowli decreas price offer therebi press buyer reject mani price conveni obtain favor price author present regret bound algorithm care analysi show bound tighten discount factor known seller despit sublinear regret monoton algorithm remain sub-optim certain choic inde consid scenario set buyer would longer incent lie thus algorithm binari search would achiev logarithm regret regret achiev monoton algorithm guarante one may argu monoton algorithm specif sinc admit singl paramet perhap complex algorithm monoton idea could achiev favor regret let us therefor analyz generic monoton algorithm am defin algorithm definit buyer valuat defin accept time first time price offer seller use algorithm am accept proposit decreas sequenc price pt exist truth buyer valuat v0 algorithm am suffer regret least reg am v0 t. proof definit regret reg am consid two case v0 everi former case reg am v0 v0 impli statement proposit thus assum latter condit algorithm famili monoton algorithm algorithm definit ar root offer price less offer price pn accept els offer price pn round end end let p1 pt pt offer price buyer reject pt offer price end offer price end let uniform distribut view lemma appendix right-hand side minim plug valu yield e reg am impli exist reg thus shown monoton algorithm am suffer regret least even face truth buyer tighter lower bound given mild condit price offer definit sequenc pt said convex verifi pt instanc convex sequenc given price offer monoton algorithm seller offer price form decreas convex sequenc seek control number lie buyer slowli reduc price follow proposit give lower bound regret algorithm famili proposit let pt decreas convex sequenc price exist valuat v0 buyer regret monoton algorithm defin price full proof proposit given appendix proposit show discount factor known monoton algorithm fact asymptot optim class result present suggest depend improv monoton algorithm sens famili algorithm conserv thus achiev favor regret guarante entir differ algorithm idea must introduc next section describ new algorithm achiev substanti advantag strateg regret combin fast converg properti binari search-typ algorithm truth set method penal untruth behavior buyer near optim algorithm let algorithm revenu optim use truth buyer denot tree associ round full tree height node label price pn offer right left children denot respect price offer pn accept buyer label price offer pn reject label final denot left right subtre root node respect figur depict tree generat algorithm propos kleinberg leighton describ later figur tree associ algorithm propos kleinberg leighton modifi tree sinc buyer hold fix valuat consid algorithm increas price price accept decreas reject formal follow definit definit algorithm said consist maxn0 pn0 pn minn0 pn0 node consist algorithm defin modifi algorithm ar parametr integ design face strateg buyer algorithm ar offer price defin follow modif price reject buyer seller offer price round pseudocod ar given algorithm motiv behind modifi algorithm given follow simpl observ strateg buyer lie certain reject price boost surplus futur forc buyer reject price sever round seller ensur futur discount surplus neglig therebi coerc buyer truth proceed formal analyz algorithm ar particular quantifi effect paramet choic buyer strategi measur spread price offer ar need definit node defin right increment nr pr n pn similar defin left increment nl maxn0 pn pn0 price offer ar defin path node path defin time number round need node reach ar note sinc may greater path chosen ar might necessarili reach leav final let function repres surplus obtain buyer play optim strategi ar node reach lemma function satisfi follow recurs relat max pn proof defin weight tree node reachabl algorithm ar assign weight edg follow way edg form weight set pn otherwis set easi see function evalu weight longest path node leaf thus follow elementari graph algorithm equat hold previous lemma immedi give us necessari condit buyer reject price proposit reachabl node price pn reject buyer follow inequ hold pn proof direct implic lemma price pn reject buyer pn howev definit buyer surplus obtain follow path bound particular true path reject pr n accept everi price pt afterward surplus path given pbt pt price seller would offer price pr n reject furthermor sinc algorithm ar consist we must pbt pr n pn nr therefor bound follow pn nr pn nr we proceed upper bound sinc pn p0n nl n0 pn0 pn nl pn nl pn nl t=t combin inequ we conclud pn pn nr pn nl nl nr nl pn rearrang term inequ yield desir result pn let us consid follow instanti algorithm introduc kleinberg leighton algorithm keep track feasibl interv initi increment paramet initi algorithm work phase within phase offer price price reject if price reject new phase start feasibl interv set increment paramet set process continu point last phase start price a offer remain round it hard see number phase need algorithm less dlog2 log2 a surpris fact algorithm shown achiev regret o log log seller face a truth buyer we show modif ar this algorithm admit a particular favor regret bound we call this algorithm pfsr penal fast search algorithm proposit valu regret algorithm pfsr admit follow upper bound reg pfsr vr log2 note upper bound coincid kleinberg leighton proof algorithm pfsr accumul regret two way price offer pn reject case regret price accept regret pn let dlog2 log2 number phase run algorithm pfsr sinc at differ price reject buyer one reject per phase price must reject round cumul regret reject upper bound vkr second type regret also bound straightforward phase let bi denot correspond search paramet feasibl interv respect if bi regret accru case buyer accept a price this interv bound bi if hand bi it readili follow pn bi price pn offer phase therefor regret obtain accept round bound ni bi bi ni ni denot number price offer i-th round final notic view algorithm definit everi bi correspond a reject price thus proposit exist node ni necessarili distinct pni bi bi pni ni it immedi nr nl node thus we write bi ni t. last inequ hold sinc at price offer algorithm combin bound regret type yield result upper bound discount factor known seller he leverag this inform optim upper bound respect paramet theorem let argminr if regret pfsr satisfi reg pfsr log ct v log2 log2 log proof this theorem fair technic defer appendix theorem help us defin condit logarithm regret achiev inde if log log1 use inequ e x valid we obtain log2 log t. log it follow theorem reg pfsr log log ct v log2 log2 log t. let us compar regret bound given theorem one given amin discuss show certain valu exponenti better regret achiev algorithm it argu knowledg upper bound requir wherea this need monoton algorithm howev if regret bound monoton super-linear therefor uninform thus order proper compar algorithm we may assum case theorem regret algorithm log wherea linear regret guarante monoton algorithm even favor bound monoton algorithm achiev regret o t a strict better regret o t log log log attain lower bound follow lower bound deriv in previous work theorem amin let fix for algorithm a exist a valuat for buyer reg a this theorem in fact given for stochast set buyer valuat a random variabl taken fix distribut d. howev proof theorem select a point mass therefor reduc scenario a fix price set theorem kleinberg leighton given algorithm a play a truth buyer exist a valu reg a log log for univers constant c. pfs mon regret regret pfs mon pfs mon number round log-scal pfs mon number round log-scal regret regret number round log-scal number round log-scal figur comparison monoton algorithm pfsr for differ choic regret algorithm plot a function the number round known the algorithm first two figur when valu made access the algorithm last two figur combin result lead immedi the follow corollari given algorithm there exist a buyer valuat reg a max log log for a univers constant c. we compar the upper bound given in the previous section the bound corollari for we reg pfsr log log log the hand for we may choos in case proposit reg pfsr o log log thus the upper lower bound match o log factor empir result in this section we present the result simul compar the monoton algorithm algorithm pfsr the experi carri follow given a buyer valuat a discret set fals valuat vb select the set algorithm run a buyer make the seller believ valuat vb instead the valu vb achiev the best util for the buyer chosen the regret for algorithm report in figur we consid two set experi first the valu paramet left unknown algorithm the valu set log t this choic motiv by the discuss follow theorem sinc for larg valu we expect achiev logarithm regret the first two plot from left right in figur depict these result the appar stationar in the regret pfsr a consequ the scale the plot the regret in fact grow log t for the second set experi we allow access the paramet both algorithm the valu chosen optim base the resultsp theorem the paramet monoton set ensur regret in it worth note even though algorithm design the assumpt knowledg the valu the experiment result show an exponenti better perform the monoton algorithm still attain in fact the perform the optim unoptim version our algorithm compar a comprehens seri experi present in appendix conclus we present a detail analysi revenu optim algorithm strateg buyer in we reduc the gap upper lower bound on strateg regret a logarithm factor furthermor the algorithm we present simpl to analyz reduc to the truth scenario in the limit an import properti previous algorithm admit we believ our analysi help gain a deeper understand this problem it serv a tool for studi complex scenario that strateg behavior in repeat second-pric auction vcg auction general market strategi acknowledg we thank kareem amin afshin rostamizadeh umar sy for sever discuss the topic this paper this work part fund by the nsf award
----------------------------------------------------------------

