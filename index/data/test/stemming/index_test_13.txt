query sentence: Self-organization of associative database and its applications
---------------------------------------------------------------------
title: 1-self-organization-of-associative-database-and-its-applications.pdf

767

SELF-ORGANIZATION OF ASSOCIATIVE DATABASE
AND ITS APPLICATIONS
Hisashi Suzuki and Suguru Arimoto
Osaka University, Toyonaka, Osaka 560, Japan
ABSTRACT
An efficient method of self-organizing associative databases is proposed together with
applications to robot eyesight systems. The proposed databases can associate any input
with some output. In the first half part of discussion, an algorithm of self-organization is
proposed. From an aspect of hardware, it produces a new style of neural network. In the
latter half part, an applicability to handwritten letter recognition and that to an autonomous
mobile robot system are demonstrated.

INTRODUCTION
Let a mapping f : X -+ Y be given. Here, X is a finite or infinite set, and Y is another
finite or infinite set. A learning machine observes any set of pairs (x, y) sampled randomly
from X x Y. (X x Y means the Cartesian product of X and Y.) And, it computes some
estimate j : X -+ Y of f to make small, the estimation error in some measure.
Usually we say that: the faster the decrease of estimation error with increase of the number of samples, the better the learning machine. However, such expression on performance
is incomplete. Since, it lacks consideration on the candidates of J of j assumed preliminarily. Then, how should we find out good learning machines? To clarify this conception,
let us discuss for a while on some types of learning machines. And, let us advance the
understanding of the self-organization of associative database .
. Parameter Type
An ordinary type of learning machine assumes an equation relating x's and y's with
parameters being indefinite, namely, a structure of f. It is equivalent to define implicitly a
set F of candidates of
(F is some subset of mappings from X to Y.) And, it computes
values of the parameters based on the observed samples. We call such type a parameter
type.
For a learning machine defined well, if F 3 f, j approaches f as the number of samples
increases. In the alternative case, however, some estimation error remains eternally. Thus,
a problem of designing a learning machine returns to find out a proper structure of f in this
sense.
On the other hand, the assumed structure of f is demanded to be as compact as possible
to achieve a fast learning. In other words, the number of parameters should be small. Since,
if the parameters are few, some j can be uniquely determined even though the observed
samples are few. However, this demand of being proper contradicts to that of being compact.
Consequently, in the parameter type, the better the compactness of the assumed structure
that is proper, the better the learning machine. This is the most elementary conception
when we design learning machines .

1.

. Universality and Ordinary Neural Networks
Now suppose that a sufficient knowledge on f is given though J itself is unknown. In
this case, it is comparatively easy to find out proper and compact structures of J. In the
alternative case, however, it is sometimes difficult. A possible solution is to give up the
compactness and assume an almighty structure that can cover various 1's. A combination
of some orthogonal bases of the infinite dimension is such a structure. Neural networks 1 ,2
are its approximations obtained by truncating finitely the dimension for implementation.

? American Institute of Physics 1988

768
A main topic in designing neural networks is to establish such desirable structures of 1.
This work includes developing practical procedures that compute values of coefficients from
the observed samples. Such discussions are :flourishing since 1980 while many efficient methods have been proposed. Recently, even hardware units computing coefficients in parallel
for speed-up are sold, e.g., ANZA, Mark III, Odyssey and E-1.
Nevertheless, in neural networks, there always exists a danger of some error remaining
eternally in estimating /. Precisely speaking, suppose that a combination of the bases of a
finite number can define a structure of 1 essentially. In other words, suppose that F 3 /, or
1 is located near F. In such case, the estimation error is none or negligible. However, if 1
is distant from F, the estimation error never becomes negligible. Indeed, many researches
report that the following situation appears when 1 is too complex. Once the estimation
error converges to some value (> 0) as the number of samples increases, it decreases hardly
even though the dimension is heighten. This property sometimes is a considerable defect of
neural networks .
. Recursi ve Type
The recursive type is founded on another methodology of learning that should be as
follows. At the initial stage of no sample, the set Fa (instead of notation F) of candidates
of I equals to the set of all mappings from X to Y. After observing the first sample
(Xl, Yl) E X x Y, Fa is reduced to Fi so that I(xt) = Yl for any I E F. After observing
the second sample (X2' Y2) E X x Y, Fl is further reduced to F2 so that i(xt) = Yl and
I(X2) = Y2 for any I E F. Thus, the candidate set F becomes gradually small as observation
of samples proceeds. The after observing i-samples, which we write
is one of the most
likelihood estimation of 1 selected in fi;. Hence, contrarily to the parameter type, the
recursive type guarantees surely that j approaches to 1 as the number of samples increases.
The recursive type, if observes a sample (x" yd, rewrites values 1,-l(X),S to I,(x)'s for
some x's correlated to the sample. Hence, this type has an architecture composed of a rule
for rewriting and a free memory space. Such architecture forms naturally a kind of database
that builds up management systems of data in a self-organizing way. However, this database
differs from ordinary ones in the following sense. It does not only record the samples already
observed, but computes some estimation of l(x) for any x E X. We call such database an
associative database.
The first subject in constructing associative databases is how we establish the rule for
rewri ting. For this purpose, we adap t a measure called the dissimilari ty. Here, a dissimilari ty
means a mapping d : X x X -+ {reals > O} such that for any (x, x) E X x X, d(x, x) > 0
whenever l(x) t /(x). However, it is not necessarily defined with a single formula. It is
definable with, for example, a collection of rules written in forms of "if? .. then?? .. "
The dissimilarity d defines a structure of 1 locally in X x Y. Hence, even though
the knowledge on f is imperfect, we can re:flect it on d in some heuristic way. Hence,
contrarily to neural networks, it is possible to accelerate the speed of learning by establishing
d well. Especially, we can easily find out simple d's for those l's which process analogically
information like a human. (See the applications in this paper.) And, for such /'s, the
recursive type shows strongly its effectiveness.
We denote a sequence of observed samples by (Xl, Yd, (X2' Y2),???. One of the simplest
constructions of associative databases after observing i-samples (i = 1,2,.,,) is as follows.

i

i"

I,

Algorithm 1. At the initial stage, let So be the empty set. For every i =
1,2" .. , let i,-l(x) for any x E X equal some y* such that (x*,y*) E S,-l and

d(x, x*) =

min
(%,y)ES.-t

d(x, x) .

Furthermore, add (x" y,) to S;-l to produce Sa, i.e., S, = S,_l U {(x"

(1)

y,n.

769

Another version improved to economize the memory is as follows.

Algorithm 2, At the initial stage, let So be composed of an arbitrary element
in X x Y. For every i = 1,2"", let ii-lex) for any x E X equal some y. such
that (x?, y.) E Si-l and
d(x, x?) =

min

d(x, x) .

(i,i)ES.-l

Furthermore, if ii-l(Xi) # Yi then let Si = Si-l, or add (Xi, Yi) to Si-l to
produce Si, i.e., Si = Si-l U {(Xi, Yi)}'
In either construction, ii approaches to f as i increases. However, the computation time
grows proportionally to the size of Si. The second subject in constructing associative
databases is what addressing rule we should employ to economize the computation time. In
the subsequent chapters, a construction of associative database for this purpose is proposed.
It manages data in a form of binary tree.

SELF-ORGANIZATION OF ASSOCIATIVE DATABASE
Given a sample sequence (Xl, Yl), (X2' Y2), .. " the algorithm for constructing associative
database is as follows.

Algorithm 3,'

Step I(Initialization): Let (x[root], y[root]) = (Xl, Yd. Here, x[.] and y[.] are
variables assigned for respective nodes to memorize data.. Furthermore, let t = 1.
Step 2: Increase t by 1, and put x, in. After reset a pointer n to the root, repeat
the following until n arrives at some terminal node, i.e., leaf.
Notations nand
d(xt, x[n)), let n

n mean the descendant nodes of n.
=n. Otherwise, let n =n.

If d(x" r[n)) ~

Step 3: Display yIn] as the related information. Next, put y, in. If yIn] = y" back
to step 2. Otherwise, first establish new descendant nodes n and n. Secondly,
let

(x[n], yIn))
(x[n], yIn))

(x[n], yIn)),
(Xt, y,).

(2)
(3)

Finally, back to step 2. Here, the loop of step 2-3 can be stopped at any time
and also can be continued.
Now, suppose that gate elements, namely, artificial "synapses" that play the role of branching by d are prepared. Then, we obtain a new style of neural network with gate elements
being randomly connected by this algorithm.

LETTER RECOGNITION
Recen tly, the vertical slitting method for recognizing typographic English letters3 , the
elastic matching method for recognizing hand written discrete English letters4 , the global
training and fuzzy logic search method for recognizing Chinese characters written in square
styleS, etc. are published. The self-organization of associative database realizes the recognition of handwritten continuous English letters.

770

9 /wn"

NOV

~ ~ ~ -xk :La.t

~~ ~ ~~~

dw1lo'

~~~~~of~~

~~~ 4,-?~~4Fig. 1. Source document.
2~~---------------'

lOO~---------------'

H

o

o
Fig. 2. Windowing.

1000

2000

3000

4000

Number of samples

o

1000

2000

3000

4000

NUAlber of sampl es

Fig. 3. An experiment result.

An image scanner takes a document image (Fig. 1). The letter recognizer uses a parallelogram window that at least can cover the maximal letter (Fig. 2), and processes the
sequence of letters while shifting the window. That is, the recognizer scans a word in a
slant direction. And, it places the window so that its left vicinity may be on the first black
point detected. Then, the window catches a letter and some part of the succeeding letter.
If recognition of the head letter is performed, its end position, namely, the boundary line
between two letters becomes known. Hence, by starting the scanning from this boundary
and repeating the above operations, the recognizer accomplishes recursively the task. Thus
the major problem comes to identifying the head letter in the window.
Considering it, we define the following.
? Regard window images as x's, and define X accordingly.
? For a (x, x) E X x X, denote by B a black point in the left area from the boundary on
window image X. Project each B onto window image x. Then, measure the Euclidean
distance 6 between fj and a black point B on x being the closest to B. Let d(x, x) be
the summation of 6's for all black points B's on x divided by the number of B's.
? Regard couples of the "reading" and the position of boundary as y's, and define Y
accordingly.
An operator teaches the recognizer in interaction the relation between window image and
reading& boundary with algorithm 3. Precisely, if the recalled reading is incorrect, the
operator teaches a correct reading via the console. Moreover, if the boundary position is
incorrect, he teaches a correct position via the mouse.
Fig. 1 shows partially a document image used in this experiment. Fig. 3 shows the
change of the number of nodes and that of the recognition rate defined as the relative
frequency of correct answers in the past 1000 trials. Speciiications of the window are height
= 20dot, width = 10dot, and slant angular = 68deg. In this example, the levels of tree
were distributed in 6-19 at time 4000 and the recognition rate converged to about 74%.
Experimentally, the recognition rate converges to about 60-85% in most cases, and to 95% at
a rare case. However, it does not attain 100% since, e.g., "c" and "e" are not distinguishable
because of excessive lluctuation in writing. If the consistency of the x, y-relation is not
assured like this, the number of nodes increases endlessly (d. Fig. 3). Hence, it is clever to
stop the learning when the recognition rate attains some upper limit. To improve further
the recognition rate, we must consider the spelling of words. It is one of future subjects.

771

OBSTACLE AVOIDING MOVEMENT
Various systems of camera type autonomous mobile robot are reported flourishingly6-1O.
The system made up by the authors (Fig. 4) also belongs to this category. Now, in mathematical methodologies, we solve usually the problem of obstacle avoiding movement as
a cost minimization problem under some cost criterion established artificially. Contrarily,
the self-organization of associative database reproduces faithfully the cost criterion of an
operator. Therefore, motion of the robot after learning becomes very natural.
Now, the length, width and height of the robot are all about O.7m, and the weight is
about 30kg. The visual angle of camera is about 55deg. The robot has the following three
factors of motion. It turns less than ?30deg, advances less than 1m, and controls speed less
than 3km/h. The experiment was done on the passageway of wid th 2.5m inside a building
which the authors' laboratories exist in (Fig. 5). Because of an experimental intention, we
arrange boxes, smoking stands, gas cylinders, stools, handcarts, etc. on the passage way at
random. We let the robot take an image through the camera, recall a similar image, and
trace the route preliminarily recorded on it. For this purpose, we define the following.
? Let the camera face 28deg downward to take an image, and process it through a low
pass filter. Scanning vertically the filtered image from the bottom to the top, search
the first point C where the luminance changes excessively. Then, su bstitu te all points
from the bottom to C for white, and all points from C to the top for black (Fig. 6).
(If no obstacle exists just in front of the robot, the white area shows the ''free'' area
where the robot can move around.) Regard binary 32 x 32dot images processed thus
as x's, and define X accordingly.
? For every (x, x) E X x X, let d(x, x) be the number of black points on the exclusive-or
image between x and X.
? Regard as y's the images obtained by drawing routes on images x's, and define Y
accordingly.
The robot superimposes, on the current camera image x, the route recalled for x, and
inquires the operator instructions. The operator judges subjectively whether the suggested
route is appropriate or not. In the negative answer, he draws a desirable route on x with the
mouse to teach a new y to the robot. This opera.tion defines implicitly a sample sequence
of (x, y) reflecting the cost criterion of the operator.

.::l" !
-

IibUBe

_. -

22

11

Roan

12

{-

13

Stationary uni t

Fig. 4. Configuration of
autonomous mobile robot system.

~

I

,

23

24

North
14

rmbi Ie unit (robot)

-

Roan

y

t

Fig. 5. Experimental
environment.

772

Wall

Camera image

Preprocessing

A

::: !fa

?

Preprocessing

0

O

Course
suggest ion

??

..

Search

A

Fig. 6. Processing for
obstacle avoiding movement.

x

Fig. 1. Processing for
position identification.
We define the satisfaction rate by the relative frequency of acceptable suggestions of
route in the past 100 trials. In a typical experiment, the change of satisfaction rate showed
a similar tendency to Fig. 3, and it attains about 95% around time 800. Here, notice that
the rest 5% does not mean directly the percentage of collision. (In practice, we prevent the
collision by adopting some supplementary measure.) At time 800, the number of nodes was
145, and the levels of tree were distributed in 6-17.
The proposed method reflects delicately various characters of operator. For example, a
robot trained by an operator 0 moves slowly with enough space against obstacles while one
trained by another operator 0' brushes quickly against obstacles. This fact gives us a hint
on a method of printing "characters" into machines.
POSITION IDENTIFICATION
The robot can identify its position by recalling a similar landscape with the position data
to a camera image. For this purpose, in principle, it suffices to regard camera images and
position data as x's and y's, respectively. However, the memory capacity is finite in actual
compu ters. Hence, we cannot but compress the camera images at a slight loss of information.
Such compression is admittable as long as the precision of position identification is in an
acceptable area. Thus, the major problem comes to find out some suitable compression
method.
In the experimental environment (Fig. 5), juts are on the passageway at intervals of
3.6m, and each section between adjacent juts has at most one door. The robot identifies
roughly from a surrounding landscape which section itself places in. And, it uses temporarily
a triangular surveying technique if an exact measure is necessary. To realize the former task,
we define the following .
? Turn the camera to take a panorama image of 360deg. Scanning horizontally the
center line, substitute the points where the luminance excessively changes for black
and the other points for white (Fig. 1). Regard binary 360dot line images processed
thus as x's, and define X accordingly.
? For every (x, x) E X x X, project each black point A on x onto x. And, measure the
Euclidean distance 6 between A and a black point A on x being the closest to A. Let
the summation of 6 be S. Similarly, calculate S by exchanging the roles of x and X.
Denoting the numbers of A's and A's respectively by nand n, define

773

d(x, x) =

~(~
+ ~).
2 n
n

(4)

? Regard positive integers labeled on sections as y's (cf. Fig. 5), and define Y accordingly.
In the learning mode, the robot checks exactly its position with a counter that is reset periodically by the operator. The robot runs arbitrarily on the passageways within 18m area
and learns the relation between landscapes and position data. (Position identification beyond 18m area is achieved by crossing plural databases one another.) This task is automatic
excepting the periodic reset of counter, namely, it is a kind of learning without teacher.
We define the identification rate by the relative frequency of correct recalls of position
data in the past 100 trials. In a typical example, it converged to about 83% around time
400. At time 400, the number of levels was 202, and the levels oftree were distributed in 522. Since the identification failures of 17% can be rejected by considering the trajectory, no
pro blem arises in practical use. In order to improve the identification rate, the compression
ratio of camera images must be loosened. Such possibility depends on improvement of the
hardware in the future.
Fig. 8 shows an example of actual motion of the robot based on the database for obstacle
avoiding movement and that for position identification. This example corresponds to a case
of moving from 14 to 23 in Fig. 5. Here, the time interval per frame is about 40sec.

,~. .~ (
;~"i..
~

"

"

.

..I

I

?
?

"

I'
.
'.1
t

;

i

-:
, . . , 'II

Fig. 8. Actual motion of the robot.

774

CONCLUSION
A method of self-organizing associative databases was proposed with the application to
robot eyesight systems. The machine decomposes a global structure unknown into a set of
local structures known and learns universally any input-output response. This framework
of problem implies a wide application area other than the examples shown in this paper.
A defect of the algorithm 3 of self-organization is that the tree is balanced well only
for a subclass of structures of f. A subject imposed us is to widen the class. A probable
solution is to abolish the addressing rule depending directly on values of d and, instead, to
establish another rule depending on the distribution function of values of d. It is now under
investigation.

REFERENCES
1. Hopfield, J. J. and D. W. Tank, "Computing with Neural Circuit: A Model/'

Science 233 (1986), pp. 625-633.
2. Rumelhart, D. E. et al., "Learning Representations by Back-Propagating Errors," Nature 323 (1986), pp. 533-536.

3. Hull, J. J., "Hypothesis Generation in a Computational Model for Visual Word
Recognition," IEEE Expert, Fall (1986), pp. 63-70.
4. Kurtzberg, J. M., "Feature Analysis for Symbol Recognition by Elastic Matching," IBM J. Res. Develop. 31-1 (1987), pp. 91-95.

5. Wang, Q. R. and C. Y. Suen, "Large Tree Classifier with Heuristic Search and
Global Training," IEEE Trans. Pattern. Anal. & Mach. Intell. PAMI 9-1
(1987) pp. 91-102.
6. Brooks, R. A. et al, "Self Calibration of Motion and Stereo Vision for Mobile
Robots," 4th Int. Symp. of Robotics Research (1987), pp. 267-276.
7. Goto, Y. and A. Stentz, "The CMU System for Mobile Robot Navigation," 1987
IEEE Int. Conf. on Robotics & Automation (1987), pp. 99-105.
8. Madarasz, R. et al., "The Design of an Autonomous Vehicle for the Disabled,"
IEEE Jour. of Robotics & Automation RA 2-3 (1986), pp. 117-125.
9. Triendl, E. and D. J. Kriegman, "Stereo Vision and Navigation within Buildings," 1987 IEEE Int. Conf. on Robotics & Automation (1987), pp. 1725-1730.
10. Turk, M. A. et al., "Video Road-Following for the Autonomous Land Vehicle,"
1987 IEEE Int. Conf. on Robotics & Automation (1987), pp. 273-279.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 1105-cholinergic-suppression-of-transmission-may-allow-combined-associative-memory-function-and-self-organization-in-the-neocortex.pdf

Cholinergic suppression of transmission may
allow combined associative memory function and
self-organization in the neocortex.
Michael E. Hasselmo and Milos Cekic
Department of Psychology and Program in Neurosciences,
Harvard University, 33 Kirkland St., Cambridge, MA 02138
hasselmo@katIa.harvard.edu

Abstract
Selective suppression of transmission at feedback synapses during
learning is proposed as a mechanism for combining associative feedback with self-organization of feed forward synapses. Experimental
data demonstrates cholinergic suppression of synaptic transmission in
layer I (feedback synapses), and a lack of suppression in layer IV (feedforward synapses). A network with this feature uses local rules to learn
mappings which are not linearly separable. During learning, sensory
stimuli and desired response are simultaneously presented as input.
Feedforward connections form self-organized representations of input,
while suppressed feedback connections learn the transpose of feedforward connectivity. During recall, suppression is removed, sensory input
activates the self-organized representation, and activity generates the
learned response.

1

INTRODUCTION

The synaptic connections in most models of the cortex can be defined as either associative
or self-organizing on the basis of a single feature: the relative infl uence of modifiable synapses on post-synaptic activity during learning (figure 1). In associative memories, postsynaptic activity during learning is determined by nonmodifiable afferent input connections, with no change in the storage due to synaptic transmission at modifiable synapses
(Anderson, 1983; McNaughton and Morris, 1987). In self-organization, post-synaptic
activity is predominantly influenced by the modifiable synapses, such that modification of
synapses influences subsequent learning (Von der Malsburg, 1973; Miller et al., 1990).
Models of cortical function must combine the capacity to form new representations and
store associations between these representations. Networks combining self-organization
and associative memory function can learn complex mapping functions with more biologically plausible learning rules (Hecht-Nielsen, 1987; Carpenter et al., 1991; Dayan et at.,

132

M. E. HASSELMO, M. CEKIC

1995), but must control the influence of feedback associative connections on self-organization. Some networks use special activation dynamics which prevent feedback from
influencing activity unless it coincides with feedforward activity (Carpenter et al., 1991).
A new network alternately shuts off feedforward and feedback synaptic transmission
(Dayan et al., 1995).

A.

Self-organizing

c.

Afferent
Self-organizing
feedforward

Associative
feedback
Figure 1 - Defining characteristics of self-organization and associative memory. A. At
self-organizing synapses, post-synaptic activity during learning depends predominantly
upon transmission at the modifiable synapses. B. At synapses mediating associative memory function, post-synaptic activity during learning does not depend primarily on the modifiable synapses, but is predominantly influenced by separate afferent input. C. Selforganization and associative memory function can be combined if associative feedback
synapses are selectively suppressed during learning but not recall.
Here we present a model using selective suppression of feedback synaptic transmission
during learning to allow simultaneous self-organization and association between two
regions. Previous experiments show that the neuromodulator acetylcholine selectively
suppresses synaptic transmission within the olfactory cortex (Hasselmo and Bower, 1992;
1993) and hippocampus (Hasselmo and Schnell, 1994). If the model is valid for neocortical structures, cholinergic suppression should be stronger for feedback but not feedforward synapses. Here we review experimental data (Hasselmo and Cekic, 1996) comparing
cholinergic suppression of synaptic transmission in layers with predominantly feedforward or feedback synapses.

2. BRAIN SLICE PHYSIOLOGY
As shown in Figure 2, we utilized brain slice preparations of the rat somatosensory neocortex to investigate whether cholinergic suppression of synaptic transmission is selective
for feedback but not feedforward synaptic connections. This was possible because feedforward and feedback connections show different patterns of termination in neocortex. As
shown in Figure 2, Layer I contains primarily feedback synapses from other cortical
regions (Cauller and Connors, 1994), whereas layer IV contains primarily afferent synapses from the thalamus and feedforward synapses from more primary neocortical structures (Van Essen and Maunsell, 1983). Using previously developed techniques (Cauller
and Connors, 1994; Li and Cauller, 1995) for testing of the predominantly feedback connections in layer I, we stimulated layer I and recorded in layer I (a cut prevented spread of

133

Cholinergic Suppression of Transmission in the Neocortex

activity from layers II and III). For testing the predominantly feedforward connections
terminating in layer IV, we elicited synaptic potentials by stimulating the white matter
deep to layer VI and recorded in layer IV. We tested suppression by measuring the change
in height of synaptic potentials during perfusion of the cholinergic agonist carbachol at
lOOJ,1M. Figure 3 shows that perfusion of carbachol caused much stronger suppression of
synaptic transmission in layer I as compared to layer IV (Hasselmo and Cekic, 1996), suggesting that cholinergic suppression of transmission is selective for feedback synapses and
not for feedforward synapses.

I

ll-ill
I

IV

~~

V-VI
Layer IV
recording

Region 1

.1

Foedback

I

ll-ill
IV
V-VI

Region 2

White matter /
stimulation

1/

Figure 2. A. Brain slice preparation of somatosensory cortex showing location of stimulation and recording electrodes for testing suppression of synaptic transmission in layer I
and in layer IV. Experiment based on procedures developed by Cauller (Cauller and Connors, 1994; Li and Cauller, 1995). B. Anatomical pattern of feedforward and feedback
connectivity within cortical structures (based on Van Essen and Maunsell, 1983).

Feedforward -layer IV

Control

Carbachol (1 OOJlM)

Wash

I~

-0

5ms

Feedback - layer I
'!oi'

Control

Carbachol (1 OOJlM)

Wash

Figure 3 - Suppression of transmission in somatosensory neocortex. Top: Synaptic potentials recorded in layer IV (where feedforward and afferent synapses predominate) show
little effect of l00J.tM carbachol. Bottom: Synaptic potentials recorded in layer I (where
feedback synapses predominate) show suppression in the presence of lOOJ,1M carbachol.

M. E. HASSELMO, M. CEKIC

134

3. COMPUTATIONAL MODELING
These experimental results supported the use of selective suppression in a computational
model (Hasselmo and Cekic, 1996) with self-organization in its feedforward synaptic connections and associative memory function in its feedback synaptic connections (Figs 1 and
4). The proposed network uses local, Hebb-type learning rules supported by evidence on
the physiology of long-tenn potentiation in the hippocampus (Gustafsson and Wigstrom,
1986). The learning rule for each set of connections in the network takes the fonn:
tlWS:'Y)

= 11 (a?) -

9(Y? g (ar?

Where W(x. Y) designates the connections from region x to region y, 9 is the threshold of
synaptic modification in region y, 11 is the rate of modification, and the output function is
g(a;.(x~ = [tanh(~(x) - J.1(x~]+ where []+ represents the constraint to positive values only.
Feedforward connections (Wi/x<y? have self-organizing properties, while feedback connections (Wir>=Y~ have associative memory properties. This difference depends entirely
upon the selective suppression of feedback synapses during learning, which is implemented in the activation rule in the form (I-c). For the entire network, the activation rule
takes the fonn:
M II(X)

a?)

= A?) + 2, 2,

N II(X)

Wi~<Y) g (a~x? +

x=lk=l

2, 2,

II(Y)

(1- c) Wi~~Y) g (a~x? -

x=lk=l

2, Hi~) (g (af?)
k=l

where a;.(y) represents the activity of each of the n(y) neurons in region y, ~ (x) is the activity of each of the n(x) neurons in other regions x, M is the total number of regions providing feedforward input, N is the total number of regions providing feedback input, Aj(y) is
the input pattern to region y, H(Y) represents the inhibition between neurons in region y,
and (1 - c) represents the suppression of synaptic transmission. During learning, c takes a
value between 0 and 1. During recall, suppression is removed, c = O. In this network, synapses (W) between regions only take positive values, reflecting the fact that long-range
connections between cortical regions consist of excitatory synapses arising from pyramidal cells. Thus, inhibition mediated by the local inhibitory interneurons within a region is
represented by a separate inhibitory connectivity matrix H.
After each step of learning, the total weight of synaptic connections is nonnalized pre-synaptically for each neuron j in each region:
~--------------

W ij (t+l)

=

[Wij(t) + l1W;j(t)]I(

.i

1=

[Wij(t) +l1Wij (t)]

2)

1

Synaptic weights are then normalized post-synaptically for each neuron i in each region
(replacing i with j in the sum in the denominator in equation 3). This nonnalization of
synaptic strength represents slower cellular mechanisms which redistribute pre and postsynaptic resources for maintaining synapses depending upon local influences.
In these simulations, both the sensory input stimuli and the desired output response to be
learned are presented as afferent input to the neurons in region 1. Most networks using
error-based learning rules consist of feedforward architectures with separate layers of
input and output units. One can imagine this network as an auto-encoder network folded
back on itself, with both input and output units in region 1, and hidden units in region 2.

135

Cholinergic Suppression of Transmission in the Neocortex

As an example of its functional properties, the network presented here was trained on the
XOR problem. The XOR problem has previously been used as an example of the capability of error based training schemes for solving problems which are not linearly separable.
The specific characteristics of the network and patterns used for this simulation are shown
in figure 4. The two logical states of each component of the XOR problem are represented
by two separate units (designated on or off in figures 4 and 5), ensuring that activation of
the network is equal for each input condition. The problem has the appearance of two
XOR problems with inverse logical states being solved simultaneously.
As shown in figure 4, the input and desired output of the network are presented simultaneously during learning to region 1. The six neurons in region 1 project along feedforward connections to four neurons in region 2, the hidden units of the network. These four
neurons project along feedback connections to the six neurons in region 1. All connections take random initial weights. During learning, the feedforward connections undergo
self-organization which ultimately causes the hidden units to become feature detectors
responding to each of the four patterns of input to region 1. Thus, the rows of the feedforward synaptic connectivity matrix gradually take the form of the individual input patterns.
on

1.
2.

3.

4.

STIMULUS
off on off

RESPONSE
yes no

oeeo eo
oeoe oe
eooe eo

9 9 9

Afferent
input

"'-Region 1

Figure 4 - Network for learning the XOR problem, with 6 units in region 1 and 4 units in
region 2. Four different patterns of afferent input are presented successively to region 1.
The input stimuli of the XOR problem are represented by the four units on the left, and the
desired output designation of XOR or not-XOR is represented by the two units on the
right. The XOR problem has four basic states: on-off and off-on on the input is categorized by yes on the output, while on-on and off-off on the input is categorized by no on the
output.
Modulation is applied during learning in the form of selective suppression of synaptic
transmission along feedback connections (this suppression need not be complete), giving
these connections associative memory function. Hebbian synaptic modification causes
these connections to link each of the feature detecting hidden units in region 2 with the
cells in region 1 activated by the pattern to which the hidden unit responds. Gradually, the
feedback synaptic connectivity matrix becomes the transpose of the feedforward connectivity matrix. (parameters used in simulation: Aj(l) 0 or I, h 2.0, q(l) 0.5, q(2)
0.6, (1) 0 .2, (2) 0.5, c 1.0 and Hik(2) 0.6). Function was similar and convergence
was obtained more rapidly with c = 0.5. Feedback synaptic transmission prevented con-

=

=

=

=

=

=

=

=

M. E. HASSELMO. M. CEKIC

136

vergence during learning when c = 0.367).
During recall, modulation of synaptic transmission is removed, and the various input stimuli of the XOR problem are presented to region 1 without the corresponding output pattern. Activity spreads along the self-organized feedforward connections to activate the
specific hidden layer unit responding to that pattern. Activity then spreads back along
feedback connections from that particular unit to activate the desired output units. The
activity in the two regions settles into a final pattern of recall. Figure 5 shows the settled
recall of the network at different stages of learning. It can be seen that the network initially may show little recall activity, or erroneous recall activity, but after several cycles of
learning, the network settles into the proper response to each of the XOR problem states.
Convergence during learning and recall have been obtained with other problems, including recognition of whether on units were on the left or right, symmetry of on units, and
number of on units. In addition, larger scale problems involving multiple feedforward and
feedback layers have been shown to converge.

~
?L

3
R
1
0>

--- ------------..:11-==
_
--.-?
--.
?
---------- --- --- --?
-.
?
.=-.:
-.
???
..--------.------..:
------ --- --- -?
-:=
.:
-.-- --- ---- -?- -? ---- ----- -?- .:--=:=:
-?
?
?
=:
??
:=.
.:
?= -? -- --- ?- ---?? --- -- ---?? -- -- ?=:..-- -- --- -? .:=:
-:==
-- -- -- -- - -- - -on on no

---

????
-

:-=.::
::=::

<

-<

.

??

??

--

??
?-? ???

?--

-:==~:

:11

I.

==

26

off on yes

??

. . . =-=: -

~.

3.

2.
off off no

1.

en
~.

3

:;0

0

~

Region 1

I

t

4.

-.. ? ?
. ---- --- - - ? ?
on off yes

-- 11------?-- ?
-------- ---- ------- ---- ?--- ---.-- -? ?- -? ------ -?
?
---- --? -?= ------ ?- ?=- --11---

11:11::
.~.:

?

?

Region 2

Figure 5 - Output neuronal activity in the network shown at different learning steps. The
four input patterns are shown at top. Below these are degraded patterns presented during
recall, missing the response components of the input pattern. The output of the 6 region 1
units and the 4 region 2 units are shown at each stage of learning. As learning progresses,
gradually one region 2 unit starts to respond selectively to each input pattern, and the correct output unit becomes active in response to the degraded input. Note that as learning
progresses the response to pattern 4 changes gradually from incorrect (yes) to correct (no).

Cholinergic Suppression of Transmission in the Neocortex

137

References
Anderson, 1.A. (1983) Cognitive and psychological computation with neural models.
IEEE Trans. Systems, Man, Cybem. SMC-13,799-815.
Carpenter, G.A., Grossberg, S. and Reynolds, 1.H. (1991) ARTMAP: Supervised realtime learning and classification of nonstationary data by a self-organizing neural network.
Neural Networks 4: 565-588.
Cauller, LJ. and Connors, B.W. (1994) Synaptic physiology of horizontal afferents to
layer I in slices of rat SI neocortex. 1. Neurosci. 14: 751-762.
Dayan, P., Hinton, G.E., Neal, RM. and Zemel, RS. (1995) The Helmholtz machine.
Neural Computation.
Gustafsson, B. and Wigstrom, H. (1988) Physiological mechanisms underlying long-term
potentiation. Trends Neurosci. 11: 156-162.
Hasselmo, M.E. (1993) Acetylcholine and learning in a cortical associative memory.
Neural Computation. 5(1}: 32-44.
Hasselmo M.E. and Bower 1.M. (1992) Cholinergic suppression specific to intrinsic not
afferent fiber synapses in rat piriform (olfactory) cortex. 1. Neurophysiol. 67: 1222-1229.
Hasselmo, M.E. and Bower, 1.M. (1993) Acetylcholine and Memory. Trends Neurosci.
26: 218-222.
Hasselmo, M.E. and Cekic, M. (1996) Suppression of synaptic transmission may allow
combination of associative feedback and self-organizing feed forward connections in the
neocortex. Behav. Brain Res. in press.
Hasselmo M.E., Anderson B.P. and Bower 1.M. (1992) Cholinergic modulation of cortical
associative memory function. 1. Neurophysiol. 67: 1230-1246.
Hasselmo M.E. and Schnell, E. (1994) Laminar selectivity of the cholinergic suppression
of synaptic transmission in rat hippocampal region CAl: Computational modeling and
brain slice physiology. 1. Neurosci. 15: 3898-3914.
Hecht-Nielsen, R (1987) Counterpropagation networks. Applied Optics 26: 4979-4984.
Li, H. and Cauller, L.l. (1995) Acetylcholine modulation of excitatory synaptic inputs
from layer I to the superficial layers of rat somatosensory neocortex in vitro. Soc. Neurosci. Abstr. 21: 68.
Linsker, R (1988) Self-organization in a perceptual network. Computer 21: 105-117.
McNaughton B.L. and Morris RG.M. (1987) Hippocampal synaptic enhancement and
information storage within a distributed memory system. Trends in Neurosci. 10:408-415.
Miller, K.D., Keller, 1.B. and Stryker, M.P. (1989) Ocular dominance column development Analysis and simulation. Science 245: 605-615.
van Essen, D.C. and Maunsell, 1.H.R. (1983) Heirarchical organization and functional
streams in the visual cortex. Trends Neurosci. 6: 370-375.
von der Malsburg, C. (1973) Self-organization of orientation sensitive cells in the striate
cortex. Kybemetik 14: 85-100.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 251-a-self-organizing-associative-memory-system-for-control-applications.pdf

332

Hormel

A Sell-organizing Associative
Memory System lor Control
Applications

Michael Bormel
Department of Control Theory and Robotics
Technical University of Darmstadt
Schlossgraben 1
6100 Darmstadt/W.-Ger.any

ABSTRACT
The CHAC storage scheme has been used as a basis
for a software implementation of an associative
.emory system AHS, which itself is a major part
of the learning control loop LERNAS. A major
disadvantage of this CHAC-concept is that the
degree of local generalization (area of interpolation) is fixed. This paper deals with an algorithm for self-organizing variable generalization for the AKS, based on ideas of T. Kohonen.

1 INTRODUCTION
For several years research at the Department of Control Theory and Robotics at the Technical University of Darmstadt
has been concerned with the design of a learning real-time
control loop with neuron-like associative memories (LERNAS)

A Self-organizing Associative Memory System for Control Applications

for the control of unknown, nonlinear processes (Ersue,
Tolle, 1988). This control concept uses an associative memory system AHS, based on the cerebellar cortex model CHAC by
Albus (Albus, 1972), for the storage of a predictive nonlinear process model and an appropriate nonlinear control strategy (Fig. 1).
e&;ected process response

I>.

o
o
planne~

~
.

control inputs

I"-

-

-

~~
-

red setpoint

-...
-?
-.-....
co

predictive
process .ode!"
IF

C

c

?
>

opti.iud
control input

-

::

eValuation/
opti.ization

actual/past
process infor.ation

f

1----

~~

control strate y

- -

actual control input
1-

-

r:;.:>

u

- -

-

-

I-

unknown process

..?

I>.

c:-

--..
..
o

I ~

o

t}

{J

o

c
o

u

short ter.
.e.ory

process infor.ation

..?
E

-?.
I

?

I

laSSOCialive lIe.ory syste.

'\~IS

Figure 1: The learning control loop LERNAS
One problem for adjusting the control loop to a process is,
however, to find a suitable set of parameters for the associative memory. The parameters in question determine the
degree of generalization within the memory and therefore
have a direct influence on the number of training steps required to learn the process behaviour. For a good performance of the control loop it ? is desirable to have a very
small generalization around a given setpoint but to have a
large generalization elsewhere. Actually, the amount of collected data is small during the transition phase between two

333

334

Hormel

setpoints but is large during setpoint control. Therefore a
self-organizing variable generalization, adapting itself to
the amount of available data would be very advantageous.
Up to now, when working with fixed generalization, finding
the right parameters has meant to find the best compromise
between performance and learning time required to generate a
process model. This paper will show a possibility to introduce a self-organizing variable generalization capability
into the existing AMS/CMAC algorithm.

2 THE AMS-CONCEPT
The associative memory syste. AMS is based on the "Cerebellar Model Articulation Controller CMAC" as presented by J.S.
Albus. The information processing structure of AMS can be
divided into three stages.
1.) Each component of a n-dimensional input vector (stimulus) activates a fixed number p of sensory cells, the
receptive fields of which are overlapping. So n?p sensory cells become active.
2.) The active sensory cells are grouped to form p n-dimensional vectors. These vectors are mapped to p association cells. The merged receptive fields of the sensory
cells described by one vector can be seen as a hypercube
in the n-dimensional input space and therefore as the
receptive field of the association cell. In normal applications the total number of available association
cells is about 100?p.
3.) The association cells are connected to the output cells
by modifiable synaptic weights. The output cell computes
the mean value of all weights that are connected to active association cells (active weights).
Figure 2 shows the basic principle of the associative memory
system AMS.

A Self-organizing Associative Memory System for Control Applications

output value
input space
adjustable weights

Figure 2: The basic aechanism of AMS
During training the generated output is compared with a desired output, the error is computed and equally distributed
over all active weights. For the mapping of sensory cells to
association cells a hash-coding mechanism is used.

3 THE SELF-ORGANIZING FEATURE MAP
An approach for explaining the self-organizing capabilities
of the nervous system has been presented by T. Kohonen (Kohonen, 1988).
In his "self-organizing feature mapft a network of laterally
interconnected neurons can adapt itself according to the
density of trained points in the input space. Presenting a
n-diaensional input vector to the network causes every neuron to produce an output signal which is correlated with the
similarity between the input vector and a "template vector"
which may be stored in the synaptic weights of the neuron.
Due to the "mexican-hat" coupling function between the neurons, the one with the maximum output activity will excite
its nearest neighbours but will inhibit neurons farther away, therefore generating a localized response in the network. The active cells can now adapt their input weights in
order to increase their similarity to the input vector. If
we define the receptive field of a neuron by the number of
input vectors for which the neurons activity is greater than

335

336

Hormel

that of any other neuron in the net, this yields the effect
that in areas with a high density of trained points the receptive fields become small whereas in areas with a low density of trained points the size of the receptive fields is
large. Is mentioned above this is a desired effect when
workin; with a learning control loop.

4 SELF-ORGANIZING VARIABLE GENERALIZATION
Both of the approaches above have several advantages and
disadvantages when using them for real-time control applications.
In the AKS algorithm one does not have to care for predefining a network and the coupling functions or coupling matrices among the elements of the network. Association and
weight cells are generated when they are needed during
training and can be adressed very quietly to produce a memory response. One of the disadvantages is the fixed generalization once the parameters of a .eaory unit have been
chosen.
Unlike AHS, the feature map allows the adaption of the network according to the input data. This advantage has to be
payed for by extensive search for the best matching neuron
in the network and therefore the response time of the network aay be too large for real-tiae control when working
with big networks.
These problems can be overcome when allowing that the mapping of sensory cells to association cells in AKS is no
longer fixed but can be changed during training.
To accomplish this a template vector t is introduced for
every association cell. This vector i serves as an indicator
for the stimuli by which the association cell has been accessed previously. During an associative recall for a stimulus !o a preliminary set of p association cells is activated
by the hash coding mechanism. Due to the self-organizing
process during training the template vectors do not need to
correspond to the input vector !o. For the search for the

A Self-organizing Associative Memory System for Control Applications

best aatching cell the template vector 10 of the accessed
association cell is compared to the stiaulus and a difference vector is calculated.
6.

,

n

=."
t.

i = O, ??? ,n

- L.v

(1)

s

number of searching steps

s

This vector can now be used to compute a virtual stimulus
which compensates the mapping errors of the hash-coding
mechanism.

~+1 = ~ -

-4

i=O, ??? ,n

(2)

s

The best matching cell is found for
j =

i

ain
II '1.
6. "
.

= O, ??? ,ns

(3)

1

and can be adressed by the virtual stimulus ~j when using
the hash coding mechanism. This search mechanism ensures
that the best matching cell is found even if self organization is in effect.
During training the template
cells are updated by
t(t+l)
d

= a(k,d) ?(!(k)

-let?~

vectors of

the

association

+ t(k)

(4)

lateral distance of neurons in the network

where t(k) denotes the value of the teaplate vector at time
k and ~(k) denotes the stimulus. a(t,d) is a monotonic decreasing function of time and the lateral distance between
neurons in the network.

6 SIMULATION RESULTS
Figure 3 and 4 show some simulation results of the presented
algorithm for the dase of a two dimensional stimulus vector.

337

338

Hormel

Figure J shows the expected positions in input space of the
untrained template vectors ( x denotes untrained association
cells).
? ? ? ?
? ? ? ?
? ? ? ?
? ? ? ?
? ? ? ?

? ? ?
? ? ?
? ? ?
? ? ?
? ? ?

? ? ? ? ?
? ? ? ? ?
? ? ? ? ?
? ? ? ? ?
? ? ? ? ?

? ?
? ?
? ?
? ?

?

?
?
?
? ? ?

?? .? .? .? .? .? .? .? .? .? . . .? .? .?
?? .? .? .? .? .? .? .? .? .? . . .. .. ..
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?

?? .? .? .? .? .? .? .? .? .? .? .? .? .? .?
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?

Figure J: Untrained .etwork
Figure 4 shows the network after 2000 training steps with
stimuli of gaussian distribution in input space. The position of the template vectors of trained cells has shifted
into the direction of the better trained areas, so that more
association cells are used to represent this area than before. Therefore the stored information will be more exact in
this area.
? ? ? ?
? ? ? ?
? ? ? ?
? ? ? ?
? ? ?
? ? ? ?
? ?
? ?
? ?
? ? ?
?

??
?

?

? ? ? ? ? ? ? ? ?
? ? ? ? ? ? ? ? ?
? ?
?
? ? ? ?
?
?

? ? ?

? ?

?

? ?

?

?
?

?
?
?
???
? ? ? ? ? ? ?
? ?

? ?
?
? ?
? ? ? ? ? ? ?
? ? ? ? ? ? ? ? ?

?

? ?
? ?
? ?
? ?

?? .. .. .. ..

Figure 4: Network after 2000 training steps

A Self-organizing Associative Memory System for Control Applications

6 CONCLUSION
The ney algorithm presented above introduces the capability
to adapt the storage mechanisms of a CMAC-type associative
memory according to the arriving stimuli. This will result
in various degrees of generalization depending on the number
of trained points in a given area. It therefore will make it
unnecessary to choose a generalization factor as a compromise between several constraints when representing nonlinear
functions by storing them in this type of associative memory. Some results on tests will be presented together with a
comparison on respective results for the original AMS.
Acknowledgements
This work was sponsored by the German !inistry for
and Technology (BMFT) under grant no. ITR 8800 B/5

Research

References

E.

Ersue, H. Tolle. (1988) Learning Control Structures with
Neuron-Like Associative memories. In: v. Seelen, Shaw, Leinhos (Eds.) Organization of Neural Networks, VCH Verlagsgesellschaft, Weinheim, FRG, 1988
J.S. llbu~ (1972) Theoretical and experimental aspects of a
cerebellar model, PhD thesis, University of Maryland, USA

E. Ersue, X. Mao (1983) Control of pH by Use of a Self-organizing Concept with Associative Memories. ACI'83, Kopenhagen, Denmark
E. Ersue, J. Militzel (1984) Real-tiae Implementation of an
Associative Memory-based Learning Control Scheme for Nonlin-ear Jfultivariable Systems. SymposiuDl on "Applications of
Multivariable System Techniques", Plymouth, UK
T. Kohonen. (1988) Self-Organization and Associative Memory,
2nd Ed., Springer Verlag

339


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 972-a-model-of-the-hippocampus-combining-self-organization-and-associative-memory-function.pdf

A model of the hippocampus combining selforganization and associative memory function.
Michael E. Hasselmo, Eric Schnell
Joshua Berke and Edi Barkai

Dept. of Psychology, Harvard University
33 Kirkland St., Cambridge, MA 02138
hasselmo@katla.harvard.edu

Abstract
A model of the hippocampus is presented which forms rapid self-organized representations of input arriving via the perforant path, performs
recall of previous associations in region CA3, and performs comparison
of this recall with afferent input in region CA 1. This comparison drives
feedback regulation of cholinergic modulation to set appropriate
dynamics for learning of new representations in region CA3 and CA 1.
The network responds to novel patterns with increased cholinergic modulation, allowing storage of new self-organized representations, but
responds to familiar patterns with a decrease in acetylcholine, allowing
recall based on previous representations. This requires selectivity of the
cholinergic suppression of synaptic transmission in stratum radiatum of
regions CA3 and CAl, which has been demonstrated experimentally.

1

INTRODUCTION

A number of models of hippocampal function have been developed (Burgess et aI., 1994;
Myers and Gluck, 1994; Touretzky et al., 1994), but remarkably few simulations have
addressed hippocampal function within the constraints provided by physiological and anatomical data. Theories of the function of specific subregions of the hippocampal formation often do not address physiological mechanisms for changing dynamics between
learning of novel stimuli and recall of familiar stimuli. For example, the afferent input to
the hippocampus has been proposed to form orthogonal representations of entorhinal
activity (Marr, 1971; McNaughton and Morris, 1987; Eichenbaum and Buckingham,
1990), but simulations have not addressed the problem of when these representations

78

Michael E. Hasselmo. Eric Schnell. Joshua Berke. Edi Barkai

should remain stable, and when they should be altered. In addition, models of autoassociative memory function in region CA3 (Marr, 1971; McNaughton and Morris, 1987;
Levy, 1989; Eichenbaum and Buckingham, 1990) and heteroassociative memory function
at the Schaffer collaterals projecting from region CA3 to CAl (Levy, 1989; McNaughton,
1991) require very different activation dynamics during learning versus recall.
Acetylcholine may set appropriate dynamics for storing new information in the cortex
(Hasselmo et aI., 1992, 1993; Hasselmo, 1993, 1994; Hasselmo and Bower, 1993). Acetylcholine has been shown to selectively suppress synaptic transmission at intrinsic but
not afferent fiber synapses (Hasselmo and Bower, 1992), to suppress the neuronal adaptation of cortical pyramidal cells (Hasselmo et aI., 1994; Barkai and Hasselmo, 1994), and
to enhance long-term potentiation of synaptic potentials (Hasselmo, 1994b). Models
show that suppression of synaptic transmission during learning prevents recall of previously stored information from interfering with the storage of new information (Hasselmo
et al., 1992, 1993; Hasselmo, 1993, 1994a), while cholinergic enhancement of synaptic
modification enhances the rate of learning (Hasselmo, 1994b).
Feedback regulation of cholinergic modulation may set the appropriate level of cholinergic modulation dependent upon the novelty or familiarity of a particular input pattern.
We have explored possible mechanisms for the feedback regulation of cholinergic modulation in simulations of region CAl (Hasselmo and Schnell, 1994) and region CA3. Here
we show that self-regulated learning and recall of self-organized representations can be
obtained in a network simulation of the hippocampal formation. This model utilizes selective cholinergic suppression of synaptic transmission in stratum radiatum of region CA3,
which has been demonstrated in brain slice preparations of the hippocampus.

2

METHODS

2.1. SIMPLIFIED REPRESENTA nON OF HIPPOCAMPAL NEURONS.

In place of the sigmoid input-output functions used in many models, this model uses a
simple representation in which the output of a neuron is not explicitly constrained, but the
total network activity is regulated by feedback from inhibitory interneurons and adaptation due to intracellular calcium concentration. Separate variables represent pyramidal
cell membrane potential a, intracellular calcium concentration c, and the membrane potential of inhibitory interneurons h:

l1a i = Ai -l1 ai - J..l.C +

L Wijg(aj - e) - Hikg(h k - e h)
j

I1c?I = 'Vg(a
.i?
I
I1hk

e )- Qc
C

= IWkjg(aj-eo)-l1hk- IHk/g(h/-e)
j

where A = afferent input, "

/

=passive decay of membrane potential, Il =strength of cal-

A Model of Hippocampus

79

cium-dependent potassium current (proportional to intracellular calcium), Wij = excitatory
recurrent synapses (longitudinal association path tenninating in stratum radiatum), gO is a
threshold linear function proportional to the amount by which membrane potential
exceeds an output threshold 00 or threshold for calcium current Oc' 'Y = strength of voltagedependent calcium current, n = diffusion constant of calcium, Wki = excitatory synapses
inhibitory interneurons, Hilc = inhibitory synapses from interneurons to pyramidal cells,
Hk}= inhibitory synapses between interneurons. This representation gives neurons adaptation characteristics similar to those observed with intracellular recording (Barkai and Hasselmo, 1994), including a prominent afterhyperpolarization potential (see Figure 1).

An

B

.... ..
~

....J

'N~JJL

C

lO

\..--14

Figure 1. Comparison of pyramidal cell model with experimental data.
In Figure I, A shows the membrane potential of a modeled pyramidal cell in response to
simulated current injection. Output of this model is a continuous variable proportional to
how much membrane potential exceeds threshold. This is analogous to the reciprocal of
interspike interval in real neuronal recordings. Note that the model displays adaptation
during current injection and afterhyperpolarization afterwards, due to the calcium-dependent potassium current. B shows the intracellularly recorded membrane potential in a pirifonn cortex pyramidal cell, demonstrating adaptation of firing frequency due to
activation of calcium-dependent potassium current. The firing rate falls off in a manner
similar to the smooth decrease in firing rate in the simplified representation. C shows an
intracellular recording illustrating long-tenn afterhyperpolarization caused by calcium
influx induced by spiking of the neuron during current injection.
2.2. NETWORK CONNECTIVITY

A schematic representation of the network simulation of the hippocampal fonnation is
shown in Figure 2. The anatomy of the hippocampal fonnation is summarized on the left
in A, and the function of these different subregions in the model is shown on the right in
B. Each of the subregions in the model contained a population of excitatory neurons with
a single inhibitory interneuron mediating feedback inhibition and keeping excitatory
activity bounded. Thus, the local activation dynamics in each region follow the equations
presented above. The connectivity of the network is further summarized in Figure 3 in the
Results section. A learning rule of the Hebbian type was utilized at all synaptic connections, with the exception of the mossy fibers from the dentate gyrus to region CA3, and the
connections to and from the medial septum. Self-organization of perforant path synapses
was obtained through decay of synapses with only pre or post-synaptic activity, and
growth of synapses with combined activity. Associative memory function at synapses

80

Michael E. Hasse/mo, Eric Schnell, Joshua Berke, Edi Barkai

arising from region CA3 was obtained through synaptic modification during cholinergic
suppression of synaptic transmission.

B

Entorhinal cortex

""""""""""

??

?
???

: Self-organized
: representation
: r-----..L.\

?

~

Comparison

.L-______~~.-----~~~--~~

Feedback regulation of
cholinergic modulation

Regulation of
learning dynamics

Figure 2. Schematic representation of hippocampal circuitry
and the corresponding function of connections in the model.

2.3. CHOLINERGIC MODULAnON
The total output from region CAl determined the level of cholinergic modulation within
both region CA3 and CAl, with increased output causing decreased modulation. This is
consistent with experimental evidence suggesting that activity in region CAl and region
CA3 can inhibit activity in the medial septum, and thereby downregulate cholinergic modulation. This effect was obtained in the model by excitatory connections from region CAl
to an inhibitory interneuron in the medial septum, which suppressed the activity of a cholinergic neuron providing modulation to the full network. When levels of cholinergic
modulation were high, there was strong suppression of synaptic transmission at the excitatory recurrent synapses in CA3 and the Schaffer collaterals projecting from region CA3 to
CAL This prevented the spread of activity due to previous learning from interfering with
self-organization. When levels of cholinergic modulation were decreased, the strength of
synaptic transmission was increased, allowing associative recall to dominate. Cholinergic
modulation also increased the rate of synaptic modification and depolarized neurons.

2.4. TESTS OF SELF-REGULATED LEARNING AND RECALL
Simulations of the full hippocampal network evaluated the response to the sequential presentation of a series of highly overlapping activity patterns in the entorhinal cortex. Recall
was tested with interspersed presentation of degraded versions of previously presented
activity patterns. For effective recall, the pattern of activity in entorhinal cortex layer IV
evoked by degraded patterns matched the pattern evoked by the full learned version of
these patterns. The function of the full network is illustrated in Figure 3. In simulations

A Model of Hippocampus

81

focused on region CA3, activity patterns were induced sequentially in region CA3, representing afferent input from the entorhinal cortex. Different levels of external activation of
the cholinergic neuron resulted in different levels of learning of new overlapping patterns.
These results are illustrated in Figure 4.

2.5. BRAIN SLICE EXPERIMENTS
The effects in the simulations of region CA3 depended upon the cholinergic suppression
of synaptic transmission in stratum radiatum of this region The cholinergic suppression of
glutamatergic synaptic transmission in region CA3 was tested in brain slice preparations
by analysis of the influence of the cholinergic agonist carbachol on the size of field potentials elicited by stimulation of stratum radiatum. These experiments used techniques similar to previously published work in region CAl (Hasselmo and Schnell, 1994).

3 RESULTS
In the full hippocampal simulation, input of an unfamiliar pattern to entorhinal cortex
layer II resulted in high levels of acetylcholine. This allowed rapid self-organization of
the perforant path input to the dentate gyrus and region CAl. Cholinergic suppression of
synaptic transmission in region CAl prevented recall from interfering with self-organization. Instead, recurrent collaterals in region CA3 stored an autoassociative representation
of the input from the dentate gyrus to region CA3, and connections from CA3 to CA 1
stored associations between the pattern of activity in CA3 and the associated self-organized representation in region CAl.
identity
" self-org "matrix ~
,auto-"
,,~>
M assoc
u .....
?
?
'at)
u
u
c >.
--I.~ ?
c
?
c
?
:.a.?:i
Self-org ~
iden~ity
.9
hetero.9 hetero8
matrix
~
assoc
~ assoc
C!

~

~

?

~

111111 I "T I' , If
,r
2 I I I II
n
I I
ld II
r I" I 'I"'"'I II , , ,
Q)2dl' r II' I I ' l l I
~ 311111 I' I I I n
"

j

4

3d
4d
ld

2d

n,n

II
II
II

'f
I I II I n
I
II I
I
,
,
II
,
I
1I
'I

I

'I

.. ,
II f'l't?
U i
I I Itt

~

I.Ll

r"

U
I

"
H
,
I
I
U
I I 1 II r

~'

,,

1

,

'(
( U.

"

I I

I

1'1 I I
I I I n
I"'" , 'I II I II

1 I ' ,I

I

l 11 I J1
't III I'

?"

'I . .

I

I

, I I II
I II ? .1
l " I II
I III I I

W.

Jl 1

lU

Neuron #
Figure 3. Activity in each subregion of the full network simulation of the hippocampal
formation during presentation of a sequence of activity patterns in entorhinal cortex.

82

Michael E. Hasselmo, Eric Schnell, Joshua Berke, Edi Barkai

In Figure 3. width of the lines represents the activity of each neuron at a particular time
step. As seen here. the network forms a self-organized representation of each new pattern
consisting of active neurons in the dentate gyrus and region CAL At the same time. an
association is formed between the self-organized representation in region CAl and the
same afferent input pattern presented to entorhinal cortex layer IV. Four overlapping patterns (1-4) are presented sequentially. each of which results in learning of a separate selforganized representation in the dentate gyrus and region CAl. with an association formed
between this representation and the full input pattern in entorhinal cortex.
The recall characteristics of the network are apparent when degraded versions of the afferent input patterns are presented in the sequence (ld-4d). This degraded afferent input
weakly activates the same representations previously formed in the dentate gyrus. Recurrent excitation in region CA3 enhances this activity. giving robust recall of the full version
of this pattern. This activity then reaches CA 1. where it causes strong activation if it
matches the pattern of afferent input from the entorhinal cortex. Strong activation in
region CAl decreases cholinergic modulation. preventing formation of a new representation and allowing recall to dominate. Strong activation of the representation stored in
region CAl then activates the full representation of the pattern in entorhinal cortex layer
IV. Thus. the network can accurately recall each of many highly overlapping patterns.
The effect of cholinergic modulation on the level of learning or recall can be seen more
clearly in a simulation of auto-associative memory function in region CA3 as shown in
Figure 4. Each box shows the response of the network to sequential presentation of full
and degraded versions of two highly overlapping input patterns. The width of the black
traces represents the activity of each of 10 CA3 pyramidal cells during each simulation
step. In the top row. level of cholinergic modulation (ACh) is plotted. In A. external activation of the cholinergic neuron is absent. so there is no cholinergic suppression of synaptic transmission. In this case. the first pattern is learned and recalled properly. but
subsequent presentation of a second overlapping pattern results only in recall of the previously learned pattern. In B. with greater cholinergic suppression. recall is suppressed sufficiently to allow learning of a combination of the two input patterns. Finally. in C. strong
cholinergic suppression prevents recall. allowing learning of the new overlapping pattern
to dominate over the previously stored pattern.

A

Stored
patterns

???

??
??

?
?

-.gN.g
N

-

ACh
Inhib
Q\

.- .--.

ACh input = 0.0

..... ......

,11",,11.

.....

B

ACh input

=0.15

C

__
ACh input

=0.3

...11'_..,.......,.. ......

111 ... 111...... 11 . . . ."'... 11 ? ?111 ??,

'

I ? ? " '? ? _ _

'111"

.';::
~

~

?'. _"11.. ? .'.? .
.,~

. ",.

.u. . . . .... ,

0

. . . . . . . . . . 111. . . .... .

N

Figure 4. Increased cholinergic suppression of synaptic transmission in region CA3
causes greater learning of new aspects of afferent input patterns.

A Model of Hippocampus

83

Extracellular recording in brain slice preparations of hippocampal region CA3 have demonstrated that perfusion of the cholinergic agonist carbachol strongly suppresses synaptic
potentials recorded in stratum radiatum, as shown in Figure 5. In contrast, suppression of
synaptic transmission at the afferent fiber synapses arising from entorhinal cortex is much
weaker. At a concentration of 20J..tM, carbachol suppressed synaptic potentials in stratum
radiatum on average by 54.4% (n=5). Synaptic potentials elicited in stratum lacunosum
were more weakly suppressed, with an average suppression of28%.

Control

Carbachol
(20JlM)

Wash

Figure 5. Cholinergic suppression of synaptic transmission in stratum radiatum of CA3.

4

DISCUSSION

In this model of the hippocampus, self-organization at perforant path synapses forms compressed representations of specific patterns of cortical activity associated with events in
the environment. Feedback regulation of cholinergic modulation sets appropriate dynamics for learning in response to novel stimuli, allowing predominance of self-organization,
and appropriate dynamics for recall in response to familiar stimuli, allowing predominance of associative memory function. This combination of self-organization and associative memory function may also occur in neocortical structures. The selective cholinergic
suppression of feedback and intrinsic synapses has been proposed to allow self-organization of feedforward synapses while feedback synapses mediate storage of associations
between higher level representations and activity in primary cortical areas (Hasselmo,
1994b). This previous proposal could provide a physiological justification for a similar
mechanism utilized in recent models (Dayan et al., 1995). Detailed modeling of cholinergic effects in the hippocampus provides a theoretical framework for linking the considerable behavioral evidence for a role of acetylcholine in memory function (Hagan and
Morris, 1989) to the neurophysiological evidence for the effects of acetylcholine within
cortical structures (Hasselmo and Bower, 1992; 1993; Hasselmo, 1994a, 1994b).

Acknowledgements
This work supported by a pilot grant from the Massachusetts Alzheimer's Disease
Research Center and by an NIMH FIRST award MH52732-01.

References
Barkai E, Hasselmo ME (1994) Modulation of the input/output function of rat piriform
cortex pyramidal cells. J. Neurophysiol. 72: 644-658.

84

Michael E. Hasselmo, Eric Schnell, Joshua Berke, Edi Barkai

Barkai E, Bergman RE, Horwitz G, Hasselmo ME (1994) Modulation of associative memory function in a biophysical simulation of rat pirifonn cortex. J. Neurophysiol. 72:659677.
Burgess N, Reece M, O'Keefe J (1994) A model of hippocampal function. Neural Networks 7: 1065-1081.
Dayan P, Hinton GE, Neal RM and Zemel RS (1995) The Helmholtz machine. Neural
computation in press.
Eichenbaum, H. and Buckingham, J. (1990) Studies on hippocampal processing: experiment, theory and model. In: Learning and computational neuroscience: foundations of
adaptive networks, M. Gabriel and J. Moore, eds., Cambridge, MA: MIT Press.
Hagan, JJ and Morris, RGM (1989) The cholinergic hypothesis of memory: A review of
animal experiments. In Psychopharmacology of the Aging Nervous System, L.L. Iversen,
S.D. Iversen and S.H. Snyder, eds. New York: Plenum Press, p. 237-324.
Hasselmo, M.E. (1993) Acetylcholine and learning in a cortical associative memory. Neural Compo 5: 22-34.
Hasselmo ME (1994a) Runaway synaptic modification in models of cortex: Implications
for Alzheimer's disease. Neural Networks 7: 13-40.
Hasselmo ME (1994b) Neuromodulation and cortical function. Behav. Brain Res. in press
Hasselmo ME, Anderson, BP and Bower, JM (1992) Cholinergic modulation of cortical
associative memory function. J. Neurophysiol. 67(5): 1230-1246.
Hasselmo ME, Bower JM (1992) Cholinergic suppression specific to intrinsic not afferent
fiber synapses in rat pirifonn (olfactory) cortex. J. Neurophysiol. 67(5): 1222-1229.
Hasselmo ME, Bower JM (1993) Acetylcholine and memory. Trends Neurosci 16:218222.
Hasselmo ME, Barkai E, Horwitz G, Bergman RE (1993) Modulation of neuronal adaptation and cortical associative memory function. In: Computation and Neural Systems II
(Eeckman F, Bower JM, ed). Norwell, MA: Kluwer Academic Publishers.
Hasselmo ME, Schnell E (1994) Laminar selectivity of the cholinergic suppression of synaptic transmission in rat hippocampal region CAl: Computational modeling and brain
slice physiology. J. Neurosci. 14: 3898-3914.
Levy WB (1989) A computational approach to hippocampal function. In: Computational
models of learning in simple neural systems (Hawkins RD, Bower GH, ed), pp. 243-305.
Orlando, FL: Academic Press.
Myers CE and Gluck M (1994) Context, conditioning and hippocampal rerepresentation
in animal learning. Behav. Neurosci. 108: 835-847.
Marr 0 (1971) Simple memory: A theory for archicortex. Phil. Trans. Roy. Soc. B
B262:23-81
McNaughton BL (1991) Associative pattern completion in hippocampal circuits: New
evidence and new questions. Brain Res. Rev. 16:193-220.
McNaughton BL, Morris RGM (1987) Hippocampal synaptic enhancement and infonnation storage within a distributed memory system. Trends Neurosci. 10:408-415.
Touretzky OS, Wan HS and Redish AD (1994) Neural representation of space in rats and
robots. In Zurada JM and Marks RJ (eds) Computational Intelligence: Imitating life.
IEEE Press.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 1210-self-organizing-and-adaptive-algorithms-for-generalized-eigen-decomposition.pdf

Self-Organizing and Adaptive Algorithms for
Generalized Eigen-Decomposition
Chanchal Chatterjee

Vwani P. Roychowdhury

Newport Corporation
1791 Deere Avenue, Irvine, CA 92606

Electrical Engineering Department
UCLA, Los Angeles, CA 90095

ABSTRACT
The paper is developed in two parts where we discuss a new approach
to self-organization in a single-layer linear feed-forward network. First,
two novel algorithms for self-organization are derived from a two-layer
linear hetero-associative network performing a one-of-m classification,
and trained with the constrained least-mean-squared classification error
criterion. Second, two adaptive algorithms are derived from these selforganizing procedures to compute the principal generalized
eigenvectors of two correlation matrices from two sequences of
random vectors. These novel adaptive algorithms can be implemented
in a single-layer linear feed-forward network. We give a rigorous
convergence analysis of the adaptive algorithms by using stochastic
approximation theory. As an example, we consider a problem of online
signal detection in digital mobile communications.

1. INTRODUCTION
We study the problems of hetero-associative trammg, linear discriminant analysis,
generalized eigen-decomposition and their theoretical connections. The paper is divided
into two parts. In the first part, we study the relations between hetero-associative training
with a linear feed-forward network, and feature extraction by the linear discriminant
analysis (LOA) criterion. Here we derive two novel algorithms that unify the two
problems. In the second part, we generalize the self-organizing algorithm for LOA to
obtain adaptive algorithms for generalized eigen-decomposition, for which we provide a
rigorous proof of convergence by using stochastic approximation theory.

1.1 HETERO-ASSOCIATION AND LINEAR DISCRIMINANT ANALYSIS
In this discussion, we consider a special case of hetero-association that deals with the
classification problems. Here the inputs belong to a finite m-set of pattern classes, and the

Self-Organizing and Adaptive Generalized Eigen-Decomposition

397

outputs indicate the classes to which the inputs belong. Usually, the ith standard basis
vector ei is chosen to indicate that a particular input vector x belongs to class i.
The LDA problem, on the other hand, aims at projecting a multi-class data in a lower
dimensional subspace such that it is grouped into well-separated clusters for the m
classes. The method is based upon a set of scatter matrices commonly known as the
mixture scatter Sm and between class scatter Sb (Fukunaga, 1990). These matrices are
used to formulate criteria such as tr(Sm-ISb) and det(Sb)1 det(Sm) which yield a linear
transform <1> that satisfy the generalized eigenvector problem Sb<1>=Sm<1>A, where A is the
generalized eigenvalue matrix. If Sm is positive definite, we obtain a <1> such that <1>TSm<1>
=1 and <1>TSb<1>=A. Furthermore, the significance of each eigenvector (for class
separability) is determined by the corresponding generalized eigenvalue.
A relation between hetero-association and LDA was demonstrated by Gallinari et al.
(1991). Their work made explicit that for a linear multi-layer perceptron performing a
one-from-m classification that minimized the total mean square error (MSE) at the
network output, also maximized a criterion det(Sb)/det(Sm) for LDA at the final hidden
layer. This study was generalized by Webb and Lowe (1990) by using a nonlinear
transform from the input data to the final hidden units, and a linear transform in the final
layer. This has been further generalized by Chatterjee and Roychowdhury (1996) by
including the Bayes cost for misclassification into the criteria tr(Sm-ISb).
Although the above studies offer useful insights into the relations between heteroassociation and LDA, they do not suggest an algorithm to extract the optimal LDA
transform <1>. Since the criteria for class separability are insensitive to multiplication by
nonsingular matrices, the above studies suggest that any training procedure that
minimizes the MSE at the network output will yield a nonsingular transformation of <1>;
i.e., we obtain Q<1> where Q is a nonsingular matrix. Since Q<1> does not satisfy the
generalized eigenvector problem Sb<1>=Sm<1>A for any arbitrary nonsingular matrix Q, we
need to determine an algorithm that will yield Q=I.
In order to obtain the optimum linear transform <1>, we constrain the training of a twolayer linear feed-forward network, such that at convergence, the weights for the first
layer simultaneously diagonalizes Sm and Sb. Thus, the hetero-associative network is
trained by minimizing a constrained MSE at the network output. This training procedure
yields two novel algorithms for LDA.
1.2 LDA AND GENERALIZED EIGEN-DECOMPOSITION

Since the LDA problem is a generalized eigen-decomposition problem for the
symmetric-definite case, the self-organizing algorithms derived from the heteroassociative networks lead us to construct adaptive algorithms for generalized eigendecomposition. Such adaptive algorithms are required in several applications of image
and signal processing. As an example, we consider the problem of online interference
cancellation in digital mobile communications.
Similar to the LDA problem Sb<1>=Sm<1>A, the generalized eigen-decomposition problem
A<1>=B<1>A involves the matrix pencil (A ,B), where A and B are assumed to be real,
symmetric and positive definite. Although a solution to the problem can be obtained by a
conventional method, there are several applications in image and signal processing where
an online solution of generalized eigen-decomposition is desired. In these real-time
situations, the matrices A and B are themselves unknown. Instead, there are available two

398

C. Chatterjee and V. P Roychowdhury

sequences of random vectors {xk} and {Yk} with limk~ooE[x~/J =A and limk~oo
E[Yky/'I=B, where xk and Yk represent the online observations of the application. For
every sample (x/C>Yk), we need to obtain the current estimates <1>k and Ak of <1> and A
respectively, such that <1>k and Ak converge strongly to their true values.
The conventional approach for evaluating <1> and A requires the computation of (A,B)
after collecting all of the samples, and then the application of a numerical procedure; i.e.,
the approach works in a batch fashion. There are two problems with this approach.
Firstly, the dimension of the samples may be large so that even if all of the samples are
available, performing the generalized eigen-decomposition may take prohibitively large
amount of computational time. Secondly, the conventional schemes can not adapt to slow
or small changes in the data. So the approach is not suitable for real-time applications
where the samples come in an online fashion.
Although the adaptive generalized eigen-decomposition algorithms are natural
generalizations of the self-organizing algorithms for LDA, their derivations do not
constitute a proof of convergence. We, therefore, give a rigorous proof of convergence
by stochastic approximation theory, that shows that the estimates obtained from our
adaptive algorithms converge with probability one to the generalized eigenvectors.
In summary, the study offers the following contributions: (1) we present two novel
algorithms that unify the problems of hetero-associative training and LDA feature
extraction; and (2) we discuss two single-stage adaptive algorithms for generalized eigendecomposition from two sequences of random vectors.
In our experiments, we consider an example of online interference cancellation in digital
mobile communications. In this problem, the signal from a desired user at a far distance
from the receiver is corrupted by another user very near to the base. The optimum linear
transform w for weighting the signal is the first principal generalized eigenvector of the
signal correlation matrix with respect to the interference correlation matrix. Experiments
with our algorithm suggest a rapid convergence within four bits of transmitted signal, and
provides a significant advantage over many current methods.

2. HETERO-ASSOCIATIVE TRAINING AND LDA
We consider a two-layer linear network performing a one-from-m classification. Let XE
9t n be an input to the network to be classified into one out of m classes ro l'''''ro m. If x E ro j
then the desired output d=e j (ith std. basis vector). Without loss of generality, we assume
the inputs to be a zero-mean stationary process with a nonsingular covariance matrix.

2.1 EXTRACTING THE PRINCIPAL LDA COMPONENTS
In the two-layer linear hetero-associative network, let there be p neurons in the hidden
layer, and m output units. The aim is to develop an algorithm so that indi",idual weight
vectors for the first layer converge to the first p~m generalized eigenvectors
corresponding to the p significant generalized eigenvalues arranged in decreasing order.
Let WjE9t n (i=I, ... ,n) be the weight vectors for the input layer, and VjE9t m (i=I, ... ,m) be
the weight vectors for the output layer.
The neurons are trained sequentially; i.e., the training of the jlh neuron is started only
after the weight vector of the (j_I)fh neuron has converged. Assume that all the j-I
previous neurons have already been trained and their weights have converged to the

399

Self-Organizing and Adaptive Generalized Eigen-Decomposition

optimal weight vectors wi for i E (1 J-l]. To extract the J'h generalized eigenvector in the
output of the /h neuron, the updating model for this neuron should be constructed by
subtracting the results from all previously computed j-I generalized eigenvectors from
the desired output dj as below
-

dj

= dj

j-I
-

T

L vi W i x.

(1)

i=1

This process is equivalent to the deflation of the desired output.
The scatter matrices Sm and Sb can be obtained from x and d as Sm=E[xx T] and Sb=
MMT, where M=E[xd1). We need to extract the j1h LOA transform Wj that satisfies the
generalized eigenvector equation SbWj=AlmWj such that Aj is the J'h largest generalized
eigenvalue. The constrained MSE criterion at the network output is
Jh,Vj

)=,lld j <~:v;wTx-vjWJxr]+

p{wJSmw j -I).

(2)

Using (2), we obtain the update equation for Wj as

w(J)

hI

= w(J)
+ {Mv(J) k
k

S w(J)(w(J)T Mv(J?)- S j~1 w(J)v(i)T v(J?)
m k
k
k
m L.. k k
k .
;=1

(3)

Differentiating (2) with respect to vi' and equating it to zero, we obtain the optimum
value ofvj as MTWj . Substituting this Vj in (3) we obtain

w(J) = w(J) +
k+1
k

{s

w(J) - S w(J)(w(J)TS w(J?) - S j~1 wU)w(i)T S w(J?)
b k
m k
k
b k
m L.. k k b k .

(4)

i=1

Let Wk be the matrix whose ith column is w~). Then (4) can be written in matrix form as
Wk+1

= Wk + r{SbWk -SmWkU~W[SbWk

p,

(5)

where UT[?] sets all elements below the diagonal of its matrix argument to zero, thereby
making it upper triangular.

2.2 ANOTHER SELF-ORGANIZING ALGORITHM FOR LDA
In the previous analysis for a two-layer linear hetero-associative network, we observed
that the optimum value for V=WTM, where the jlh column of Wand row of V are formed
by Wi and Vi respectively. It is, therefore, worthwhile to explore the gradient descent
procedure on the error function below instead of (2)
J(W)

=

E[lld- MTWWTxI12}

(6)

By differentiating this error function with respect to W, and including the deflation
process, we obtain the following update procedure for W instead of (5)
Wk+1

= Wk + ~2SbWk

- Sm Wk UT [W[ SbWk ] - SbWkUT[ W[ SmWk]).

(7)

3. LDA AND GENERALIZED EIGEN-DECOMPOSITION
Since LOA consists of solving the generalized eigenvector problem Sb<P=Sm<PA, we can
naturally generalize algorithms (5) and (7) to obtain adaptive algorithms for the
generalized eigen-decomposition problem A<P=B<PA, where A and B are assumed to be
symmetric and positive definite. Here, we do not have the matrices A and B. Instead,

400

C. Chatterjee and V. P. Roychowdhury

there are available two sequences of random vectors {xk} and {Yk} with limk~ooE[xp/]
=A and limk~~[Yky/]=B, where xk and Yk represent the online observations.
From (5), we obtain the following adaptive algorithm for generalized eigendecomposition
(8)

Here {17k} is a sequence of scalar gains, whose properties are described in Section 4. The
sequences {Ak} and {B k} are instantaneous values of the matrices A and B respectively.
Although the Ak and Bk values can be obtained from xk and Yk as xp/ and YkY/
respectively, our algorithm requires that at least one of the {Ak} or {B k} sequences have a
dominated convergence property. Thus, the {Ak} and {Bk } sequences may be obtained
from xp/ and YkY/ from the following algorithms

Ak

= Ak_1 +Yk(XkXk -A k- I )

and Bk

= Bk- I

+Yk(YkYk -Bk-d,

(9)

where Ao and Bo are symmetric, and {Yk} is a scalar gain sequence.
As done before, we can generalize (7) to obtain the following adaptive algorithm for
generalized eigen-decomposition from a sequence of samples {Ak} and {Bk}

Wk+1

= Wk + l7k(2A k Wk -

BkWkUT[ W[ AkWk ] - AkWkUT[ W[ BkWk ]).

(10)

Although algorithms (8) and (10) were derived from the network MSE by the gradient
descent approach, this derivation does not guarantee their convergence. In order to prove
their convergence, we use stochastic approximation theory. We give the convergence
results only for algorithm (l0).

4. STOCHASTIC APPROX. CONVG. PROOF FOR ALG. (10)
In order to prove the con vergence of (10), we use stochastic approximation theory due to
Ljung (1977). In stochastic approximation theory, we study the asymptotic properties of
(10) in terms of the ordinary differential equation (ODE)

~ W(t)= 1!!! E[2AkW -

BkWUT[ W T AkW]- AkWUT[ W T BkW]],

where W(t) is the continuous time counterpart of Wk with t denoting continuous time. The
method of proof requires the following steps: (1) establishing a set of conditions to be
imposed on A, B, A", B", and 17", (2) finding the stable stationary points of the ODE; and
(3) demonstrating that Wk visits a compact subset of the domain of attraction of a stable
stationary point infinitely often.
We use Theorem 1 of Ljung (1977) for the convergence proof. The following is a general
set of assumptions for the convergence proof of (10):
Assumption (AI). Each xk and Yk is bounded with probability one, and limk~ooE[xp/]
= A and limk~ooE[y kY k1) = B, where A and B are positive definite.
Assumption (A2). {l7kE9t+} satisfies l7kJ..O, Lk=Ol7k =OO,Lk=Ol7k <00 for some r>1 and

limk~oo sup(l7i l -l7i~l) <00.
Assumption (A3). The p largest generalized eigenvalues of A with respect to B are each
of unit mUltiplicity.
Lemma 1. Let Al and A2 hold. Let w* be a locally asymptotically stable (in the sense of
Liapunov) solution to the ordinary differential equation (ODE):

Self-Organizing and Adaptive Generalized Eigen-Decomposition

~ W(t) = 2AW(t) -

BW(t)U4W(t/ AW(t)] - AW(t)U4W(t/ BW(t)],

401

(11)

with domain of attraction D(W). Then if there is a compact subset S of D(W) such that
?
Wk E S infinitely often, then we have Wk ~ W with probability one as k ~ 00.

We denote A\ > ~ > ... > Ap ~ ... ~ An > 0 as the generalized eigenvalues of A with
respect to B, and 4>; as the generalized eigenvector corresponding to A; such that 4>\, ... ,4>n
are orthonormal with respect to B. Let <l>=[4>\ ... 4>n l and A=diag(A\, ... ,An ) denote the
matrix of generalized eigenvectors and eigenvalues of A with respect to B. Note that if 4>;
is a generalized eigenvector, then d;4>; (ld;l= 1) is also a generalized eigenvector.
In the next two lemmas, we first prove that all the possible equilibrium points ofthe ODE
(11) are up to an arbitrary permutation of the p generalized eigenvectors of A with
respect to B corresponding to the p largest generalized eigenvalues. We next prove that
all these equilibrium points of the ODE (11) are unstable equilibrium points, except for
[d\4>\ ... dn4> nl, where Id;I=1 for i=I, ... ,p.
Lemma 2. For the ordinary differential equation (11), let Al and A3 hold Then W=<l>DP
are equilibrium points of (11), where D=[D\IOV is a nXp matrix with DI being a pXp
diagonal matrix with diagonal elements d; such that Id;l= 1 or d;=O, and P is a nXn
arbitrary permutation matrix.
?
Lemma 3. Let Al and A3 hold Then W=<l>D (where D=[D\101~ D\ =diag(d\, ...,dp )'
Id;I=I) are stable equilibrium points of the ODE (11). In addition, W=<l>DP (d;=O for i~p
or P~J) are unstable equilibrium points of the ODE (11) .
?
Lemma 4. For the ordinary differential equation (11) , let Al and A3 hold Then the
points W=<l>D (where D=[D\101~ D\ =diag(d\, ... ,dp )' Id;I=1 for i=I, ... ,p) are
?
asymptotically stable.
Lemma 5. Let AI-A3 hold Then there exists a uniform upper boundfor 17k such that Wk
is uniformly bounded w.p . I .

?

The convergence of alg. (10) can now be established by referring to Theorem 1 of Ljung.
Theorem 1. Let A I-A3 hold Assume that with probability one the process {Wk } visits
infinitely often a compact subset of the domain of attraction of one of the asymptotically
stable points <l>D. Then with probability one
lim Wk = <l>D.
k~OCl

Proof. By Lemma 2, <l>D (ld;I=I) are asymptotically stable points of the ODE (11). Since
we assume that {Wk } visits a compact subset of the domain of attraction of <l>D infmitely
often, Lemma 1 then implies the theorem.
?

5. EXPERIMENT AL RESULTS
We describe the performance of algorithms (8) and (10) with an example of online
interference cancellation in a high-dimensional signal, in a digital mobile communication
problem. The problem occurs when the desired user transmits a signal from a far distance
to the receiver, while another user simultaneously transmits very near to the base. For
common receivers, the quality of the received signal from the desired user is dominated
by interference from the user close to the base. Due to the high rate and large dimension
of the data, the system demands an accurate detection method for just a few data samples.

C. Chatterjee and V. P. Roychowdhury

402

If we use conventional (numerical analysis) methods, signal detection will require a
significant part of the time slot allotted to a receiver, accordingly reducing the effective
communication rate. Adaptive generalized eigen-decomposition algorithms, on the other
hand, allow the tracking of slow changes, and directly performs signal detection.
The details of the data model can be found in Zoltowski et al. (1996). In this application,
the duration for each transmitted code is 127 IlS, within which we have lOllS of signal
and 1171ls of interference. We take 10 frequency samples equi-spaced between -O.4MHz
to +O.4MHz. Using 6 antennas, the signal and interference correlation matrices are of
dimension 60X60 in the complex domain.
We use both algorithms (8) and (10) for the cancellation of the interference. Figure 1
shows the convergence of the principal generalized eigenvector and eigenvalue. The
closed form solution is obtained after collecting all of the signal and interference
samples. In order to measure the accuracy of the algorithms, we compute the direction
cosine of the estimated principal generalized eigenvector and the generalized eigenvector
computed by the conventional method. The optimum value is one. We also show the
estimated principal generalized eigenvalue in Figure 1b. The results show that both
algorithms converge after the 4th bit of signal.
-

Algonthm (1 0)

-

-

Algonlhm (8)

Algonthm (10)
- Algonlhm (8)
35 ...----.---r--r----r-....-,-..,--.......,--...----.---,

1.1r--.----T---~--__r~r__-,--.......,

. ._09.rf

CLOSl!D FORM SOUlTlON
1.0 ?? ? ? ???.... .? ???.........????.?..??.?..
? ?......

~
....

lll

08

~

:; 07

/--

I

.-- - - - -

25

I

I

Iii

~20

~~: /'

~

....

;::: 04

~

03

Iii

Q

OJ

~

13

~

I

15

I

I::

10 III

Iii

!ii ~

0.1
O.OD?'------=5DD:-----lI~m----:-:I5DD'::-'

NUMBER OF SAMPLES

(a)

?D?'------5DD~---~I~----IJ5DD---~DOO

NUMBER OF SAMPLES

(b)

Figure 1. (a) Direction Cosine of Estimated First Principal Generalized Eigenvector, and
(b) Estimated First Principal Generalized Eigenvalue.
References
C.Chatterjee and V.Roychowdhury (1996), "Statistical Risk Analysis for Classification and
Feature Extraction by Multilayer Perceptrons", Proceedings IEEE Int 'l Conference on Neural
Networks, Washington D.C.
K.Fukunaga (1990), Introduction to Statistical Pattern Recognition, 2nd Edition, New York:
Academic Press.
P.Gallinari, S.Thiria, F.Badran, F.Fogelman-Soulie (1991), "On the Relations Between
Discriminant Analysis and Multilayer Perceptrons", Neural Networks, Vol. 4, pp. 349-360.
L.Ljung (1977), "Analysis of Recursive Stochastic Algorithms", IEEE Transactions on Automatic
Control, Vol. AC-22, No. 4, pp. 551-575.
A.R.Webb and D.Lowe (1990), "The Optimised Internal Representation of Multilayer Classifier
Networks Perfonns Nonlinear Discriminant Analysis", Neural Networks, Vol. 3, pp. 367-375.
M.D.Zoltowski, C.Chatterjee, V.Roychowdhury and J.Ramos (1996), "Blind Adaptive 2D RAKE
Receiver for CDMA Based on Space-Time MVDR Processing", submitted to IEEE Transactions
on Signal Processing.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 1774-effective-learning-requires-neuronal-remodeling-of-hebbian-synapses.pdf

Effective Learning Requires Neuronal
Remodeling of Hebbian Synapses

Gal Chechik Isaac Meilijson Eytan Ruppin
School of Mathematical Sciences
Tel-Aviv University Tel Aviv, Israel
ggal@math.tau.ac.il isaco@math.tau.ac.il ruppin@math.tau.ac.il

Abstract
This paper revisits the classical neuroscience paradigm of Hebbian
learning. We find that a necessary requirement for effective associative memory learning is that the efficacies of the incoming
synapses should be uncorrelated. This requirement is difficult to
achieve in a robust manner by Hebbian synaptic learning, since it
depends on network level information. Effective learning can yet be
obtained by a neuronal process that maintains a zero sum of the incoming synaptic efficacies. This normalization drastically improves
the memory capacity of associative networks, from an essentially
bounded capacity to one that linearly scales with the network's size.
It also enables the effective storage of patterns with heterogeneous
coding levels in a single network. Such neuronal normalization can
be successfully carried out by activity-dependent homeostasis of the
neuron's synaptic efficacies, which was recently observed in cortical
tissue. Thus, our findings strongly suggest that effective associative learning with Hebbian synapses alone is biologically implausible and that Hebbian synapses must be continuously remodeled by
neuronally-driven regulatory processes in the brain.

1

Introduction

Synapse-specific changes in synaptic efficacies, carried out by long-term potentiation
(LTP) and depression (LTD) are thought to underlie cortical self-organization and
learning in the brain. In accordance with the Hebbian paradigm, LTP and LTD
modify synaptic efficacies as a function of the firing of pre and post synaptic neurons.
This paper revisits the Hebbian paradigm showing that synaptic learning alone
cannot provide effective associative learning in a biologically plausible
manner, and must be complemented with neuronally-driven synaptic
remodeling.
The importance of neuronally driven normalization processes has already been
demonstrated in the context of self-organization of cortical maps [1, 2] and in continuous unsupervised learning as in principal-component-analysis networks [3]. In
these scenarios normalization is necessary to prevent the excessive growth of synap-


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 1014-associative-decorrelation-dynamics-a-theory-of-self-organization-and-optimization-in-feedback-networks.pdf

Associative Decorrelation Dynamics:
A Theory of Self-Organization and
Optimization in Feedback Networks

Dawei W. Dong*
Lawrence Berkeley Laboratory
University of California
Berkeley, CA 94720

Abstract
This paper outlines a dynamic theory of development and adaptation in neural networks with feedback connections. Given input ensemble, the connections change in strength according to an
associative learning rule and approach a stable state where the
neuronal outputs are decorrelated . We apply this theory to primary visual cortex and examine the implications of the dynamical
decorrelation of the activities of orientation selective cells by the
intracortical connections. The theory gives a unified and quantitative explanation of the psychophysical experiments on orientation
contrast and orientation adaptation. Using only one parameter , we
achieve good agreements between the theoretical predictions and
the experimental data.

1

Introduction

The mammalian visual system is very effective in detecting the orientations of lines
and most neurons in primary visual cortex selectively respond to oriented lines and
form orientation columns [1) . Why is the visual system organized as such? We
*Present address: Rockefeller University, B272, 1230 York Avenue, NY, NY 10021-6399.

926

Dawei W Dong

believe that the visual system is self-organized, in both long term development and
short term adaptation, to ensure the optimal information processing.
Linsker applied Hebbian learning to model the development of orientation selectivity and later proposed a principle of maximum information preservation in early
visual pathways [2]. The focus of his work has been on the feedforward connections
and in his model the feedback connections are isotropic and unchanged during the
development of orientation columns; but the actual circuitry of visual cortex involves extensive, columnar specified feedback connections which exist even before
functional columns appear in cat striate cortex [3].
Our earlier research emphasized the important role of the feedback connections in
the development of the columnar structure in visual cortex. We developed a theoretical framework to help understand the dynamics of Hebbian learning in feedback networks and showed how the columnar structure originates from symmetry
breaking in the development of the feedback connections (intracortical, or lateral
connections within visual cortex) [4].
Figure 1 illustrates our theoretical predictions. The intracortical connections break
symmetry and develop strip-like patterns with a characteristic wave length which
is comparable to the developed intracortical inhibitory range and the LGN-cortex
afferent range (left). The feedforward (LGN-cortex) connections develop under the
influence of the symmetry breaking development of the intracortical connections.
The developed feedforward connections for each cell form a receptive field which
is orientation selective and nearby cells have similar orientation preference (right) .
Their orientations change in about the same period as the strip-like pattern of the
intracortical connections.

Figure 1: The results of the development of visual cortex with feedback connections. The
simulated cortex consists of 48 X 48 neurons, each of which connects to 5 X 5 other cortical
neurons (left) and receives inputs from 7 X 7 LGN neurons (right). In this figure, white
inclicates positive connections and black inclicates negative connections. One can see that
the change of receptive field's orientation (right) is highly correlated with the strip-like
pattern of intracortical connections (left).

Many aspects of our theoretical predictions agree qualitatively with neurobiological observations in primary visual cortex. Another way to test the idea of optimal

Associative Correlation Dynamics

927

information processing or any self-organization theory is through quantitative psychophysical studies. The idea is to look for changes in perception following changes
in input environments. The psychophysical experiments on orientation illusions
offer some opportunities to test our theory on orientation selectivity.
Orientation illusions are the effects that the perceived orientations of lines are affected by the neighboring (in time or space) oriented stimuli, which have been
observed in many psychophysical experiments and were attributed to the inhibitory
interactions between channels tuned to different orientations [5]. But there is no unified and quantitative explanation. Neurophysiological evidences support our earlier
computational model in which intracortical inhibition plays the role of gain-control
in orientation selectivity [6]. But in order for the gain-control mechanism to be
effective to signals of different statistics, the system has to develop and adapt in
different environments.
In this paper we examine the implication of the hypothesis that the intracortical
connections dynamically decorrelate the activities of orientation selective cells, i.e.,
the intracortical connections are actively adapted to the visual environment, such
that the output activities of orientation selective cells are decorrelated. The dynamics which ensures such decorrelation through associative learning is outlined in the
next section as the theoretical framework for the development and the adaptation
of intracortical connections. We only emphasize the feedback connections in the
following sections and assume that the feedforward connections developed orientation selectivities based on our earlier works. The quantitative comparisons of the
theory and the experiments are presented in section 3.

2

Associative Decorrelation Dynamics

There are two different kinds of variables in neural networks. One class of variables
represents the activity of the nerve cells, or neurons. The other class of variables
describes the synapses, or connections, between the nerve cells. A complete model
of an adaptive neural system requires two sets of dynamical equations, one for each
class of variables, to specify the evolution and behavior of the neural system.
The set of equations describing the change of the state of activity of the neurons is
dVi

adt
-I

= -ViI

+ ~T.
L..J .. v.. + 1I}}

I

(1)

j

in which a is a time constant, Tij is the strength of the synaptic connection from
neuron j to neuron i, and Ii is the additional feedforward input to the neuron besides
those described by the feedback connection matrix nj . A second set of equations
describes the way the synapses change with time due to neuronal activity. The
learning rule proposed here is
B dnj = (V,. - V.')!,
dt

in which B is a time constant and
in the following.

Vi'

I

I}

(2)

is the feedback learning signal as described

The feedback learning signal Vi' is generated by a Hopfield type associative memory
network: Vi' = Lj T/j Vi , in which T/j is the strength of the associative connection

928

Dawei W Dong

from neuron j to neuron i, which is the recent correlation between the neuronal
activities Vi and Vj determined by Hebbian learning with a decay term [4]

B

,dTfj

,

dt = -Iij + ViVj

(3)

in which B' is a time constant. The Vi' and T[j are only involved in learning and
do not directly affect the network outputs.
It is straight forward to show that when the time constants B
dynamics reduces to

dT

B dt

= (1- < VVT ?

> > B' > > a, the

< VIT >

(4)

where bold-faced quantities are matrices and vectors and <> denotes ensemble
average. It is not difficult to show that this equation has a Lyapunov or "energy"
function
L = Tr(1- < VV T ?(1- < VVT
(5)
which is lower bounded and satisfies

>f

dL

<0

dt -

and

dL =0

dt

-+-

dTij =
dt

0 I'lor at,)
11"

(6)

Thus the dynamics is stable. When it is stable, the output activities are decorrelated ,
<VVT >= 1
(7)
The above equation shows that this dynamics always leads to a stable state where
the neuronal activities are decorrelated and their correlation matrix is orthonormal.
Yet the connections change in an associative fashion - equation (2) and (3) are
almost Hebbian . That is why we call it associative decorrelation dynamics. From information processing point of view, a network, self-organized to satisfy equation (7),
is optimized for Gaussian input ensembles and white output noises [7].

Linear First Order Analysis
In applying our theory of associative decorrelation dynamics to visual cortex to
compare with the psychophysical experiments on orientation illusions, the linear
first-order approximation is used, which is

T = TO + 6T,
V = Va +6V,

TO = 0, 6T ex - < I IT >
Va = I, 6V = TI

(8)

where it is assumed that the input correlations are small. It is interesting to notice
that the linear first-order approximation leads to anti-Hebbian feedback connections: Iij ex - < /i/j > which is guarantteed to be stable around T = 0 [8].

3

Quantitative Predictions of Orientation Illusions

The basic phenomena of orientation illusions are demonstrated in figure 2 (left).
On the top, is the effect of orientation contrast (also called tilt illusion): within the
two surrounding circles there are tilted lines; the orientation of a center rectangle

Associative Correlation Dynamics

929

appears rotated to the opposite side of its surrounding tilt. Both the two rectangles and the one without surround (at the left-center of this figure) are, in fact,
exactly same. On the bottom, is the effect of orientation adaptation (also called
tilt aftereffect): if one fixates at the small circle in one of the two big circles with
tilted lines for 20 seconds or so and then look at the rectangle without surround,
the orientation of the lines of the rectangle appears tilted to the opposite side.
These two effects of orientation illusions are both in the direction of repulsion: the
apparent orientation of a line is changed to increase its difference from the inducing
line. Careful experimental measurements also revealed that the angle with the
inducing line is
100 for maximum orientation adaptation effect [9] but 20 0 for
orientation contrast [10].
<"V

<"V

1

Ol..---~-~-""';:"'''''''''---'

-90

-45

o

Stimulus orientation

45
(J

90

(degree)

Figure 2: The effects of orientation contrast (upper-left) and orientation adaptation (lowerleft) are attributed to feedback connections between cells tuned to different orientations
(upper-right, network; lower-right, tuning curve).

Orientation illusions are attributed to the feedback connections between orientation selective cells. This is illustrated in figure 2 (right). On the top is the network
of orientation selective cells with feedback connections. Only four cells are shown.
From the left, they receive orientation selective feedforward inputs optimal at -45 0 ,
00 ,45 0 , and 90 0 , respectively. The dotted lines represent the feedback connections
(only the connections from the second cell are drawn). On the bottom is the orientation tuning curve of the feedforward input for the second cell, optimally tuned to
stimulus of 00 (vertical), which is assumed to be Gaussian of width (T = 20 0 ? Because of the feedback connections, the output of the second cell will have different
tuning curves from its feedforward input, depending on the activities of other cells.
For primary visual cortex, we suppose that there are orientation selective neurons
tuned to all orientations. It is more convenient to use the continuous variable e
instead of the index i to represent neuron which is optimally tuned to the orientation
of angle e. The neuronal activity is represented by V(e) and the feedforward input
to each neuron is represented by I(e). The feedforward input itself is orientation

930

Dawei W. Dong

selective: given a visual stimulus of orientation

J(e) =

eo, the input is

e-(9-9 o )2/ q 2

(9)
This kind of the orientation tuning has been measured by experiments (for references, see [6]). Various experiments give a reasonable tuning width around 20?
?(7" = 20? is used for all the predictions).
Predicted Orientation Adaptation
For the orientation adaptation to stimulus of angle eo, substituting equation (9)
into equation (8), it is not difficult to derive that the network response to stimulus
of angle 0 (vertical) is changed to

V(e)

= e_ 92 / q2 _

ae-(9-9 o )2/ q 2 e-9~/2q2

(10)

in which (7" is the feedforward tuning width chosen to be 20? and a is the parameter
of the strength of decorrelation feedback.
The theoretical curve of perceived orientation ?(eo) is derived by assuming the
maximum likelihood of the the neural population, i.e., the perceived angle ? is the
angle at which Vee) is maximized. It is shown in figure 3 (right). The solid line is
the theoretical curve and the experimental data come from [9] (they did not give
the errors, the error bars are of our estimation,...., 0.2?). The parameter obtained
through X2 fit is the strength of decorrelation feedback: a = 0.42.
2.0 ;--'"T""---,--...,.....-----.------,

-

~ 1.5

-

~ 3.0

~

~
II)

II)

CD 1.0

}

."
~

."

~

0.5

~

2.0

II)
> 1.0
'il

'il
Q.,

4.0

0.0 f - - - - - - - - - - - - - J

o

10
20
30
40
Surround angle 80 (degree)

50

i:!
II)

Q.,

0.0
0

10
20
30
40
50
Adaptation angle 80 (degree)

Figure 3: Quantitative comparison of the theoretical predictions with the experimental
data of orientation contrast (left) and orientation adaptation (right).

It is very interesting that we can derive a relationship which is independent of the
parameter of the strength of decorrelation feedback a,

(eo - ?m)(3e o - 2?m) = (7"2
(11)
in which eo is the adaptation angle at which the tilt aftereffect is most significant
and ?m is the perceived angle.
Predicted Orientation Contrast
For orientation contrast, there is no specific adaptation angle, i.e., the network has
developed in an environment of all possible angles. In this case, when the surround
is of angle eo, the network response to a stimulus of angle e1 is
Vee) = e-(9-9 1)2/ q 2 _ ae-(9-9 o )2/ 3q 2
(12)

Associative Correlation Dynamics

931

in which fr and a has the same meaning as for orientation adaptation. Again assuming the maximum likelihood, ?(eo), the stimulus angle e1 at which it is perceived
as angle 0, is derived and shown in figure 3 (left). The solid line is the theoretical
curve and the experimental data come from [10] and their estimated error is "" 0.20.
The parameter obtained through X 2 fit is the strength of decorrelation feedback:
a = 0.32.
We can derive the peak position eo, i.e., the surrounding angle
orientation contrast is most significant,

~e~ =
3

fr2

eo

at which the

(13)

For fr = 20 0 , one immediately gets eo = 24 0 ? This is in good agreement with
experiments, most people experience the maximum effect of orientation contrast
around this angle.
Our theory predicts that the peak position of the surround angle for orientation
contrast should be constant since the orientation tuning width fr is roughly the
same for different human observers and is not going to change much for different
experimental setups. But the peak value of the perceived angle is not constant since
the decorrelation feedback parameter a is not necessarily same, indeed, it could be
quite different for different human observers and different experimental setups.

4

Discussion

First, we want to emphasis that in all the comparisons, the same tuning width fr is
used and the strength of decorrelation feedback a is the only fit parameter. It does
not take much imagination to see that the quantitative agreements between the
theory and the experiments are good. Further more, we derived the relationships
for the maximum effects, which are independent of the parameter a and have been
partially confirmed by the experiments.
Recent neurophysiological experiments revealed that the surrounding lines did influence the orientation selectivity of cells in primary visual cortex of the cat [11].
Those single cell experiments land further support to our theory. But one should
be cautioned that the cells in our theory should be considered as the average over
a large population of cells in cortex.
The theory not only explains the first order effects which are dominant in angle
range of 00 to 50 0 , as shown here, but also accounts for the second order effects
which can be seen in 500 to 90 0 range, where the sign of the effects is reversed.
The theory also makes some predictions for which not much experiment has been
done yet, for example, the prediction about how orientation contrast depends on
the distance of surrounding stimuli from the test stimulus [7].
Finally, this is not merely a theory for the development and the adaptation of
orientation selective cells, it can account for effect such as human vision adaptation
to colors as well [7]. We can derive the same equation as Atick etal [12] which agrees
with the experiment on the appearance of color hue after adaptation. We believe
that future psychophysical experiments could give us more quantitative results to
further test our theory and help our understanding of neural systems in general.

932

Dawei W. Dong

Acknowledgements
This work was supported in part by the Director, Office of Energy Research, Division of Nuclear Physics of the Office of High Energy and Nuclear Physics of the
U.S. Department of Energy under Contract No. DE-AC03-76SF00098.

References
[1] Hubel DH, Wiesel TN, 1962 Receptive fields, binocular interactions, and functional
architecture in the cat's visual cortex J Physiol (London) 160, 106- 54. - 1963
Shape and arrangement of columns in cat's striate cortex J Physiol (London) 165,
559-68.
[2] Linsker R, 1986 From basic network principles to neural architecture ... Proc Natl
Acad Sci USA 83, 7508 8390 8779. - , 1989 An application of the principle of maximum information preservation to linear systems Advances in Neural Information
Processing Systems 1, Touretzky DS, ed, Morgan Kaufman, San Mateo, CA 186-94.
[3] Gilbert C, Wiesel T, 1989 Columnar Specificity of intrinsic horizontal and corticocortical connections in cat visual cortex J Neurosci 9(7), 2432-42. Luhmann HJ,
Martinez L, Singer W, 1986 Development of horizontal intrinsic connections in cat
striate cortex Exp Brain Res 63, 443-8.
[4] Dong DW, 1991 Dynamic properties of neural network with adapting synapses Proc
International Joint Conference on Neural Networks, Seattle, 2, 255- 260. - , 1991
Dynamic Properties of Neural Networks Ph D thesis, University Microfilms International, Ann Arbor, ML Dong DW, Hopfield JJ, 1992 Dynamic properties of neural
networks with adapting synapses Network: Computation in Neural Systems, 3(3),
267- 83.
[5] Gibson J J, Radner M, 1937 Adaptation, after-effect and contrast in the perception
of tilted lines J of Exp Psy 20, 453-67. Carpenter RHS, Blakemore C, 1973 Interactions between orientations in human vision Exp Brain Res 18, 287-303. Tolhurst
DJ, Thompson PG, 1975 Orientation illusions and after-effects: Inhibition between
channels Vis Res 15,967-72. Barlow HB, Foldiak P, 1989 Adaptation and decorrelation in the cortex The Computing Neuron, Durbin R, Miall C, Mitchison G, eds,
Addison- Wesley, New York, NY.
[6] Wehmeier U, Dong DW, Koch C, Van Essen DC, 1989 Modeling the mammalian
visual system Methods in Neuronal Modeling: From Synapses to Networks, Koch C,
Segev I, eds, MIT Press, Cambridge, MA 335-60.
[7] Dong DW, 1993 Associative Decorrelation Dynamics in Visual Cortex Lawrence
Berkeley Laboratory Technical Report LBL-34491.
[8] Dong DW, 1993 Anti-Hebbian dynamics and total recall of associative memory Proc
World Congress on Neural Networks, Portland, 2, 275-9.
[9] Campbell FW, Maffei L, 1971 The tilt after-effect: a fresh look Vis Res 11, 833-40.
[10] Westheimer G, 1990 Simultaneous orientation contrast for lines in the human fovea
Vis Res 30, 1913-21.

[11] Gilbert CD, Wiesel TN, 1990 The influence of contextual stimuli on the orientation
selectivity of cells in primary visual cortex of the cat Vis Res 30,1689-701.
[12] Atick JJ, Li Z, Redlich AN, 1993 What does post-adaptation color appearance reveal
about cortical color representation Vis Res 33, 123-9.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 161-neural-approach-for-tv-image-compression-using-a-hopfield-type-network.pdf

264

NEURAL APPROACH FOR TV IMAGE COMPRESSION
USING A HOPFIELD TYPE NETWORK

Martine NAILLON
Jean-Bernard THEETEN
Laboratoire d'Electronique et de Physique Appliquee *
3 Avenue DESCARTES, BP 15
94451 LIMEIL BREVANNES Cedex FRANCE.
ABSTRACT

A self-organizing Hopfield network has been
developed in the context of Vector Ouantiza-tion, aiming at compression of television
images. The metastable states of the spin
glass-like network are used as an extra
storage resource using the Minimal Overlap
learning rule (Krauth and Mezard 1987) to
optimize the organization of the attractors.
The sel f-organi zi ng scheme that we have
devised results in the generation of an
adaptive codebook for any qiven TV image.

I NTRODOCTI ON

The ability of an Hopfield network (Little,1974;
Hopfield, 1982,1986; Amit. and al., 1987; Personnaz and
al. 1985; Hertz, 1988) to behave as an associative memory
usua 11 y aSSlJ11es a pri ori knowl edge of the patterns to be
stored. As in many applications they are unknown, the aim
of this work is to develop a network capable to learn how
to select its attractors. TV image compression using
Vector Quantization (V.Q.)(Gray, 1984), a key issue for
HOTV transmission, is a typical case, since the non
neural algorithms which generate the list of codes (the
codebookl are suboptimal. As an alternative to the
prani si ng neural canpressi on techni ques (Jackel et al.,
1987; Kohonen, 1988; Grossberg, 1987; Cottrel et al.,
19B7) our idea is to use the metastability in a spin
glass-like net as an additional storage resource and to
cl usteri nq a1gori thm a
derive after a "cl assi cal
sel f-organi zi ng sheme for generatf ng adaptively the
codebook. We present the illustrative case of 2D-vectors.
II

* LEP : A member of the Philips Research Organization.

Neural Approach for TV Image Compression

NON NEURAL APPROACH
In V.O., the image is divided into blocks, named vectors,
of N pixels (typically 4 x 4 pixels). Given the codebook,
each vector is coded by associating it with the nearest
element of the list (Nearest Neighbour Classifier)
( fi g ure 1).
EMCaD?"
INPUT
YEtTa"

COP1PARE

INDEX

ftECDNINDEX ~ CODEBOOK ~ STRUCTED
VECTOR

CODE BOOK

Figure 1 : Basic scheme of a vector quantizer.
For designing an optimal codebook, a clustering algorithm
is app1 ied to a training set of vectors (figure 2), the
criterium of optimality being a distorsion measure
between the training set and the codebook. The algorithm
is actua 11 y subopt ima1, especi a11 y for non connex
training set, as it is based on an iterative computation
of centers of grav i ty whi ch tends to overcode the dense
regions of poi nts whereas the 1ight ones are undercoded
(figure 2).
--

---- - - - - - - - - - - - - -

a~r_---------~~-~---~

1::-

..
..
a+--____~-------~--------~
110.0
230.0
PIXEl.. 1 .
.
.
.
-----.
--------.- - - - - - -

Figure 2 : Training set of two pixels vectors and the
associated codebook canputed by a non neural c1 ustering
algorithm: overcoding of the dense regions (pixel 1 148)
and subcoding of the light ones.

265

266

Naillon and Theeten

NEURAL APPROACH
In a Hopfield neural network, the code vectors are the
attractors of the net and the neural dynamics (resolution
phase)
is
substituted to the nearest neighbourg
classification.
~en patterns - referred to as II prototypes" and named
here "explicit memory"
are prescribed in a spin
glass-like net, other attractors
referred to as
"me tastable states" - are induced in the net (Sherrington
and Kirkpatrick, 1975; Toulouse, 1977; Hopfield, 1982;
Mezard and al., 1984). We consider those induced
attractors as additional memory named here "impl icit
memory" whi ch can be used by the network to code the
previously mentioned light regions of points. This
provides a higher flexibility to the net during the
self-organization process, as it can choose in a large
basis of explicit and implicit attractors the ones which
will optimize the coding task.
NEURAL NOTATION
A vector of 2 pixels with 8 bits per pel is a vector of
2 dimensions in an Eucl idean space where each dimension
corresponds to 256 grey levels. To preserve the Euclidean
di stance, we use the well-known themometri c notati on :
256 neurons for 256 level s per dimens i on, the number of
neurons set to one, wi th a reg ul ar orderi ng, g iv i ng the
pixel luminance, e.g. 2 = 1 1-1-1-1-1 ??? For vectors of
dimension 2, 512 neurons will be used, e.g. v=(2,3)=
(1 1-1-1 ?????? -1,1 1 1-1-1 ??? ,-1)
INDUCTION PROCESS
The induced impl icit memory depends on the prescription
rule. We have compared the Projection rule (Personnaz and
al., 1985) and the Minimal Overlap rule (Krauth and
Mezard, 1987).
The metastable states are detected by relaxing any point
of the training set of the figure 2, to its
corresponding prescribe or induced attractor marked in
figure 3 with a small diamond.
For the two rules, the induction process is rather
detenni ni stic, generati ng an orthogonal mesh : if two
prototypes (P11,P12) and (P21,P22) are prescribed, a
metastable state is induced at the cross-points, namely
(P11,P22) and (P21,P12) (figure 3).

Neural Approach for TV Image Compression

a+-__
ar---~----~--~
... ~-.____~.. __~.....
.. ... ...
PDIB. 1

PDCEI. 1

Figure 3 : Comparaison of the induction process for 2
prescription rules. The prescribed states are the full
squares, the induced states the open diamonds.
What differs between the two rul es ; s the number of
induced attractors. For 50 prototypes and a training set
of 2000 2d-vectors, the projection rule induces about
1000 metastable states (ratio 1000/50 = 20) whereas Min
Over induces only 234 (ratio 4.6). This is due to the
different stabil ity of the prescribed and the induced
states in the case of Min Over (Naillon and Theeten, to
be published).
GENERALIZED ATTRACTORS
Some attractors are induced out of the image space
(Figure 4) as the 512 neurons space has 2512
configurations to be compared with the (2 8 )2= 216 image
configurati ons.
We extend the image space by defi n1 ng a "genera 1i ze d
attractor" as the class of patterns having the same
number of neurons set to one for each pixel, whatever
thei r orderi ng. Such a notati on corresponds to a random
thermometri c neural representati on. The simul ati on has
shown that the generalized attractors correspond to
acceptable states (Figure 4) i.e. they are located at the
place when one would like to obtain a normal attractor.

267

268

NsiIlon and Theeten

i

NO GENERAUZATION

i

WITI-I GENERAUZATJON

i?

~;m""ING~
r' ~ / ,
wrTHOVT AT

0-\ -,'~ \l'J!'ft
~ ~!~~ . '~~6
....

--< ?. ~

i

~ CII!JeMIJZm

AJ'TAACT'OR

1'-.(f;-6

,~

.A

'1ft 6..

i

..

~~
"~ ~~~r.~,J::.,..,.lf~ .
\---J. ...
-~
~
~.
-.,... -(?1.f~ll1"
'fl ??
f~?'.
. 1
~6

\

i

I'
l!Jl!.

...

",b.

.... ...
..... ,,'

~~/ ~.. ;;~ (J.,". -

..._

-

j.J ~.

t,)t ~~'~ ~5\ :?.... ... ...
I

PIXEL!

I't hjlt

~

~
6+6

?

~

--

....

t

f.
& -.

;

[ '&
6"'6.- 6

'- l!.4--6

~ ~J -'-t~~i &~ ~

~,

+6A-+

4-

.. ....
..

to

PIXEL !

Figure 4 : The induced bassins of attractions are
represented with arrows. In the left plot, some training
vectors have no attractor in the image space. After
generalization (randon thermometric notation), the right
~ot shows their corresponding attractors.

ADAPTIVE NEURAL CODEBOOK LEARNING
An iterative sel f- organi zi ng process has been developed
to optimi ze the codebook. For a given TV image, the
codebook is defined, at each step of the process, as the
set of prescribed and induced attractors, selected by the
training set of vectors. The self-organizing scheme is
controlled by a cost function, the distorsion measure
between the training set and the codebook. Having a
target of 50 code vectors, we have to prescri be at each
step, as discussed above, typically 50/4.6 = 11
prototypes. As seen in figure Sa, we choose 11 initial
prototypes uniformly distributed along the bisecting
line. Using the training set of vectors of the figure 2,
the induced metastable states are detected with their
corresponding bassins of attraction. The 11 most
frequent, prescribed or induced, attractors are selected
and the
11 centers of gravi ty of thei r bassi ns of
attracti on are taken as new prototypes (figure 5b ).
After 3 iterations, the distorsion measure stabilizes
(Table 1).

Neural Approach for TV Image Compression

INmALIZATION
n:

?
? ?
i
? "?
"
? ~
~
?
?s?
s
?
?
?
i

.....- --- .... ...
PlXB.1

--

PIXEl. 1

....

Initialization of the self-organizing scheme.

Fi gure 5a

ITERATION ?1
FAST ORGANIZATION

PROTOTYPES

i

?
?
??
??
??

i
"

~

?

,! 8

N

~

?

I, ?? ?
,

i

Figure 5b
scheme.

??

First iteration of the self-organizinq
?Iobal
codebook
dislofsion size
1001

1

itrrllioM

2

3
4
5

1571
1031
97
97
98 .

53
57
79
84
68

?eneralized
aUraclors

!
i

I
i

0
4
20
20
15

Table 1 : Evolution of the distorsion measure versus the
iterations of the self-organizing scheme. It stabilizes
in 3 iterations.

269

270

NOOllon and Theeten

Fourty 1i nes of a TV image (the port of Ba 1timore) of 8
bits per pel, has been coded with an adaptive neural
codebook of 50 20-vectors. The coherence of the coding is
visible from the apparent continuity of the image
(Figure 6).
The coded image has 2.5 bits per pel.

I
-

j
? 1

Figure 6 : Neural coded image with 2.5 bits per pel.

CONCLUSION
Using
a
"classical"
clusterinq
algorithm,
a
self-organizing scheme has been developed in a Hopfield
network f.or the adaptive design of a codebook of small
d imensi on vectors ina Vector Quanti zati on techni Que. It
has been shown that using the Minimal Overlap
prescription rule, the metastable states induced in a
spin gl ass-like network can be used as extra-codes. The
optimal organization of the prescribed and induced
attractors, has been defined as the limit organization
obtained from the iterative learning process. It is an
example of "learning by selection" as already proposed by
physicists and biologists (Toulouse and ale 1986).
Hard~re
impl ementation on the neural VLSI ci rcuit
curren~y
designed at LEP should allow for on-line
codebook computations.
We woul d like to thank J.J. Hopfield who has inspired
this study as well H. Bosma and W. Kreuwel s from Phil ips
Research Laboratories, Eindhoven, who have allow to
initialize this research.

Neural Approach for TV Image Compression

REFERENCES
1

- J.J. Hopfield, Proc. Nat. Acad. Sci. USA, 79, 2554 - 2558
(1982); J.J. Hopfield and D.W. Tank, SC1ence 233 , 625
(1986) ; W.A. Little, Math. Biosi.,..!2., 101-120 :-T1974).

2

- D.J. ftrnit, H. Gutfreund, and H. Sanpolinslc.y, Phys.Rev. 32,
Ann. Phys. 173, 30 (1987).
-

3

- L. Personnaz, I. Guyon and G. Dreyfus, J. Phys. Lett. 46,
L359 (1985).

4

- J.A. Hertz, 2nd

5

- M.A. Virasoro, Disorder Systems and Biological Organization,

6

- R.M. Gray, IEEE ASSP Magazi ne 5 (Apr. 1984).

7

- L.D. Jackel, R.E. Howard, J.S. Denker, W. Hubbard and
S.A. ~ol1a, ADpl ied Ootics, Vol. 26, Q, (1987).

8

- i. Kononen, Finland, Helsinky University of Technology,
Tech. ~eo. No. iKK..;:"?A601; T. Kahanen, ~Jeural Networks, 1,
~jumoer :, (1988).
-

9

- S. Grossoerg, Cognitive ScL,.!.!., 23-63 (1987).

International Conference on "Vector and
pa ra 11 e 1 canputi ng, Transo, Norway, June (1988).

ed. E. Bienenstoclc., Springer, Berlin (1985); H. Gutfreund
(Racah Institute of Physics, Jerusalem) (1986); C. Cortes,
A. Kro<;lh and J .A. Hertz, J. of Phys. A., (1986).

10 - G.W. Cottrell, P. Murro and D.Z. Zioser, Institute of
cognitive Science, Report 8702 (1987).

11 - D. Sherrington and S. Kirkpatrick, Phys. Rev. Lett. 35 t
1792 (1975); G. Toulouse, Commun. Phys. 2, 115-119 (lID);
M. Mezard , G. Parisi, N. Sourlas , G. Toulouse and
M. Virasoro, Phys. Dey. Lett., g, 1156-1159 (1984).
12 - W. Krauth and M. Mezard
L 745-L 752 (1987)
13 - M.

~Jaillon

t

J. Phys.A : Math. Gen. 20,

and J.B. Theeten, to be published.

14 - G. Toulouse, S. Dehaene and J.P. Changeux, Pro. Natl.Acad.
Sci. USA,~, 1695, (1986).

271


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 1554-neuronal-regulation-implements-efficient-synaptic-pruning.pdf

Neuronal Regulation Implements
Efficient Synaptic Pruning

Gal Chechik and Isaac Meilijson
School of Mathematical Sciences
Tel Aviv University, Tel Aviv 69978, Israel
ggal@math.tau.ac.il isaco@math.tau.ac.il
Eytan Ruppin
Schools of Medicine and Mathematical Sciences
Tel Aviv University, Tel Aviv 69978, Israel
ruppin@math.tau.ac.il

Abstract
Human and animal studies show that mammalian brain undergoes
massive synaptic pruning during childhood , removing about half of
the synapses until puberty. We have previously shown that maintaining network memory performance while synapses are deleted,
requires that synapses are properly modified and pruned, removing the weaker synapses. We now show that neuronal regulation , a
mechanism recently observed to maintain the average neuronal input field , results in weight-dependent synaptic modification . Under
the correct range of the degradation dimension and synaptic upper bound, neuronal regulation removes the weaker synapses and
judiciously modifies the remaining synapses . It implements near
optimal synaptic modification, and maintains the memory performance of a network undergoing massive synaptic pruning. Thus ,
this paper shows that in addition to the known effects of Hebbian
changes, neuronal regulation may play an important role in the
self-organization of brain networks during development .

1

Introduction

This paper studies one of the fundamental puzzles in brain development: the massive synaptic pruning observed in mammals during childhood , removing more than
half of the synapses until puberty (see [1] for review) . This phenomenon is observed in various areas of the brain both in animal studies and human studies. How
can the brain function after such massive synaptic elimination? what could be the
computational advantage of such a seemingly wasteful developmental strategy? In

G. Chechik. I. Meilijson and E. Ruppin

98

previous work [2], we have shown that synaptic overgrowth followed by judicial
pruning along development improves the performance of an associative memory
network with limited synaptic resources, thus suggesting a new computational explanation for synaptic pruning in childhood. The optimal pruning strategy was
found to require that synapses are deleted according to their efficacy, removing the
weaker synapses first.
But is there a mechanism that can implement these theoretically-derived synaptic
pruning strategies in a biologically plausible manner? To answer this question , we
focus here on studying the role of neuronal regulation (NR) , a mechanism operating
to maintain the homeostasis of the neuron 's membrane potential. NR has been recently identified experimentally by [3], who showed that neurons both up-regulate
and down-regulate the efficacy of their incoming excitatory synapses in a multiplicative manner, maintaining their membrane potential around a baseline level.
Independently, [4] have studied NR theoretically, showing that it can efficiently
maintain the memory performance of networks undergoing synaptic degradation .
Both [3] and [4] have hypothesized that NR may lead to synaptic pruning during
development.
In this paper we show that this hypothesis is both computationally feasible and
biologically plausible by studying the modification of synaptic values resulting from
the operation of NR. Our work thus gives a possible account for the way brain
networks maintain their performance while undergoing massive synaptic pruning.

2

The Model

NR-driven synaptic modification (NRSM) results from two concomitant processes:
synaptic degradation (which is the inevitable consequence of synaptic turnover
[5]) , and neuronal regulation (NR) operating to compensate for the degradation.
We therefore model NRSM by a sequence of degradation-strengthening steps. At
each time step, synaptic degradation stochastically reduces the synaptic strength
W t (W t > 0) to W't+l by
W't+l = W t - (wtt'1]t; 1] "" N(J..{/ , (1"1/)
(1)

where 1] is noise term with positive mean and the power a defines the degradation
dimension parameter chosen in the range [0,1] . Neuronal regulation is modeled by
letting the post-synaptic neuron multiplicatively strengthen all its synapses by a
common factor to restore its original input field
W t +1 = W'tH li~
(2)

Ii

where If is the input field of neuron i at time t. The excitatory synaptic efficacies are
assumed to have a viability lower bound B- below which a synapse degenerates and
vanishes, and a soft upper bound B+ beyond which a synapse is strongly degraded
reflecting their maximal efficacy. To study of the above process in a network, a
model incorporating a segregation between inhibitory and excitatory neurons (i.e.
obeying Dale's law) is required . To generate this essential segregation, we modify
the standard low-activity associative memory model proposed by [6] by adding a
small positive term to the synaptic learning rule. In this model, M memories
are stored in an excitatory N -neuron network forming attractors of the network
dynamics. The synaptic efficacy Wij between the jth (pre-synaptic) neuron and
the ith (post-synaptic) neuron is
M

Wij

=

I: [(er - p)(ej - p) + a] , 1 ~ i i= j

~

N

(3)

99

Neuronal Regulation Implements Efficient Synaptic Prnning

where {e'}~=l are {O, I} memory patterns with coding level p (fraction of firing
neurons), and a is some positive constant 1. The updating rule for the state Xf of
the ith neuron at time t is
N

xI+1

= (J(Jf),

If

N

= ~ L9(Wij)Xj - ~ L
j=l

xj -

T,

(J(J)

= 1 + sign(J)

(4)

2

j=l

where T is the neuronal threshold, and I is the inhibition strength. 9 is a general
modification function over the excitatory synapses, which is either derived explicitly
(See Section 4), or determined implicitly by the operation of NRSM. If 9 is linear
and I = Mathe model reduces to the original model described by [6]. The overlap
mil (or similarity) between the network 's activity pattern X and the memory ~II
serves to measure memory performance (retrieval acuity), and is defined as mil

~
3

Ef=l (~j -

=

p)Xj.

N euronally Regulated Synaptic Modification

NRSM was studied by simulating the degradation-strengthening sequence in a network in which memory patterns were stored according to Eq.3. Figure la plots a
typical distribution of synaptic values as traced along a sequence of degradationstrengthening steps (Eq. 1,2) . As evident, the synaptic values diverge: some of the
weights are strengthened and lie close to the upper synaptic bounds, while the other
synapses degenerate and vanish. Using probabilistic considerations, it can be shown
that the synaptic distribution converge to a meta-stable state where it remains for
long waiting times. Figure Ib describes the metastable synaptic distribution as
calculated for different 0 values .
Evolving distribution of synaptic efficacies
a. Simulation results
b. Numerical results
10000
CJ)

Q)
CJ)

c..

1.0

/1, 5000
r
r
r
I

I

I

r

\1000

I

0.8

ctl

c::
>CJ)

r

1400

-....

I

_._. Alpha=O.O
- - - Alpha=O.5
Alpha=O.9

~.6
'(j)

c::
~0.4

0

I

i
i

Q)

.0

E

0.2

:::l

c::

0.0

/

/

..

//

0

Figure 1: Distribution of synaptic strengths following a degradation-strengthening
process. a) Synaptic distribution after 0,200 , 400, 1000 and 5000 degradationstrengthening steps of a 400 neurons network with 1000 stored memory patterns.
0=0.8, p = 0.1, B- = 10- 5 , B+ = 18 and T/ '" N(0.05, 0.05). Qualitatively similar
results were obtained for a wide range of simulation parameters. b) The synaptic
distribution of the remaining synapses at the meta-stable state was calculated as
the main eigen vector of the transition probability matrix.
the weights are normally distributed with expectation M a > 0 and standard deviation O(VM) , the probability of a negative synapse vanishes as M goes to infinity (and
is negligible already for several dozens of memories in the parameters' range used here).
1 As

G. Chechik, I Meilijson and E. Ruppin

100

a. NRSM functions at the
Metastable state
, 20

b. NRSM and
random deletion

i ----r--- --j::=::::==

1.0

r-:-~-~---_r__-_-~---,

~-?-v~?.M,~

r~

,

\
\
\

\
\

~

\
\

c

'"
E

.g
Q)

a...

0 .5

\

\
\
\
\

\
\

NR modification
- - - Random deletion

\
\
\

,,

' ......

0 .0 '---''-------'''--.......''"'''-...?...-~--------,'
0.0
4 .0
8.0
,2.0

Original synaptic strength

0 .0 '---- ' - -- - ',--~----"~.,.~
--~
, 0.9
0 .8
0.7
0.6
0.5

- ''--~
04
0 .3

Network's Connectivity

Figure 2: a) NRSM functions at the metastable state for different a values. Results were obtained in a 400-neurons network after performing 5000 degradationstrengthening steps. Parameter values are as in Figure 1, except B+
12. b)
Performance of NR modification and random deletion. The retrieval acuity of 200
memories stored in a network of 800 neurons is portrayed as a function of network
connectivity, as the network undergoes continuous pruning until NR reaches the
metastable state. a = 0, B+ = 7.5, p = 0.1, rna = 0.80, a = 0.01, T = 0.35,
B- = 10- 5 and TJ"'" N(O.OI, 0.01).

=

To further investigate which synapses are strengthened and which are pruned, we
study the resulting synaptic modification function. Figure 2a plots the value of
synaptic efficacy at the metastable state as a function of the initial synaptic efficacy, for different values of the degradation dimension a. As observed, a sigmoidal
dependency is obtained, where the slope of the sigmoid s.trongly depends on the
degradatiori dimension. In the two limit cases, additive degradation (a = 0) results
in a step function at the metastable state, while multiplicative degradation (a = 1)
results in random diffusion of the synaptic weights toward a memory less mean value.
Different values of a and B+ result in different levels of synaptic pruning: When
the synaptic upper bound B+ is high, the surviving synapses assume high values,
leading to massive pruning to maintain the neuronal input field, which in turn reduces network 's performance. Low B+ values lead to high connectivity, but limit
synapses to a small set of possible values, again reducing memory performance. Our
simulations show that optimal memory retrieval is obtained for B+ values that lead
to deletion levels of 40% - 60%, in which NR indeed maintains the network performance. Figure 2b traces the average retrieval acuity of a network throughout the
operation of NR, versus a network subject to random deletion at the same pruning
levels. While the retrieval of a randomly pruned network collapses already at low
deletion levels of about 20%, a network undergoing NR performs well even in high
deletion levels.

4

Optimal Modification In Excitatory-Inhibitory Networks

To obtain a a comparative yardstick to evaluate the efficiency of NR as a selective
pruning mechanism, we derive optimal modification functions maximizing memory
performance in our excitatory-inhibitory model and compare them to the NRSM
functions.

101

Neuronal Regulation Implements Efficient Synaptic Pruning

We study general synaptic modification functions, which prune some of the synapses
and possibly modify the rest, while satisfying global constraints on synapses such
as the number or total strength of the synapses. These constraints reflect the
observation that synaptic activity is strongly correlated with energy consumption
in the brain [7], and synaptic resources may hence be inherently limited in the adult
brain.
We evaluate the impact of these functions on the network's retrieval performance,
by deriving their effect on the signal to noise ratio (SIN) of the neuron's input field
(Eqs. 3,4)' known to be the primary determinant of retrieval capacity ([8]). This
analysis, conducted in a similar manner to [2] yields

where z'" N(O, 1) and 9 is the modification function of Eq. 4 but is now explicitly
applied to the synapses. To derive optimal synaptic modification functions with
limited synaptic resources, we consider 9 functions that zero all synapses except
those in some set A, and keep the integral

i

l(z)?(z)dz

k

= 0, 1, ...

;

= OVz ~ A

g(z)

(6)

limited. We then maximize the SIN under this constraint using the Lagrange
method. Our results show that without any synaptic constraints the optimal function is the identity function, that is, the original Hebbian rule is optimal. When
the number of synapses is restricted (k = 0), the optimal modification function is a
linear function for all the remaining synapses
a

g(W)

= aW -J.ta+b

-

L z2?(z)dz

where { b
(Ta

J z?(z )dz

L ?(z)dz)

A

(1-

E(W)
V(W)

(7)

for any deletion set A. To find the synapses that should be deleted, we have numerically searched for a deletion set maximizing SIN while limiting g(W) to positive
values (as required by the segregation between excitatory and inhibitory neurons).
The results show, that weak synapses pruning, a modification strategy that removes the weakest synapses and modifies the rest according to Eq. 7, is optimal
at deletion levels above 50% . For lower deletion levels, the above 9 function fails
to satisfy the positivity constraint for any set A. When the positivity constraint is
ignored, SIN is maximized if the weights closest to the mean are deleted and the
remaining synapses are modified according to Eq 7. We name this strategy mean
synapses pruning. Figure 3 plots the memory capacity under weak-synapses
pruning (compared with random deletion and mean-synaptic pruning) showing that
pruning the weak synapses performs at least near optimally for lower deletion levels
as well. Even more interesting, under the correct parameter values weak-synapses
pruning results in a modification function that has a similar form to the NR-driven
modification function studied in the previous Section: both strategies remove the
weakest synapses and linearly modify the remaining synapses in a similar manner.
In the case of limited overall synaptic strength (k > 0 in Eq. 6), the optimal 9
satisfies
z - 2"Y1 [g(z) - E(g(z))] - "Y2kg(z)k-1 = 0
(8)
and thus for k = 1 and k = 2 the optimal modification function is again linear. For
k > 2 a sublinear modification function is optimal, where 9 is a function of zl/(k-1),

G. Chechik, I. Meilijson and E. Ruppin

102

Capacity of different synaptic modification functions g(w)
a. Analysis results
b. Simulations results
1oo0r---~----~--~----~---'

=---==-.......,::...:.... _.-.-........
800

800

.~600

~6oo

?I

?I

?I 400

?1400

(.)

c..
(.)

200

(.)

c..
(.)

200

'..... .........

.... ....

.

..... .... ..

'" '" '"

'".
'".
'"'" ,.,
'"'" .,

'",,.,.

"

'\.

,.

1\

\

Figure 3: Comparison between performance of different modification strategies as a
function of the deletion level (percentage of synapses pruned). Capacity is measured
as the number of patterns that can be stored in the network (N = 2000) and be
recalled almost correctly (rn > 0.95) from a degraded pattern (rna = 0.80).
and is thus unbounded for all k. Therefore, in our model, bounds on the synaptic
efficacies are not dictated by the optimization process. Their computational advantage arises from their effect on preserving memory capacity in face of ongoing
synaptic pruning.

5

Discussion

By studying NR-driven synaptic modification in the framework of associative memory networks, we show that NR prunes the weaker synapses and modifies the remaining synapses in a sigmoidal manner. The critical variables that govern the
pruning process are the degradation dimension and the upper synaptic bound. Our
results show that in the correct range of these parameters, NR implements
a near optimal strategy, maximizing memory capacity in the sparse connectivity levels observed in the brain.
A fundamental requirement of central nervous system development is that the system should continuously function, while undergoing major structural and functional developmental changes. It has been proposed that a major functional role
of neuronal down-regulation during early infancy is to maintain neuronal activity
at its baseline levels while facing continuous increase in the number and efficacy
of synapses [3]. Focusing on up-regulation, our work shows that NR has another
important interesting effect: that of modifying and pruning synapses in a continuously optimal manner. Neuronally regulated synaptic modifications may play the
same role also in the peripheral nervous system: It was recently shown that in the
neuro-muscular junction the muscle regulates its incoming synapses in a way similar to NR [9]. Our analysis suggests this process may be the underlying cause for
the finding that synapses in the neuro-muscular junction are either strengthened or
pruned according to their initial efficacy [10].
The significance of our work goes beyond understanding synaptic organization and
remodeling in the associative memory models studied in this paper. Our analysis
bears relevance to two other fundamental paradigms: Hetero Associative memory
and self organizing maps, sharing the same basic synaptic structure of storing as-

Neuronal Regulation Implements Efficient Synaptic Pruning

103

sociations between sets of patterns via a Hebbian learning rule.
Combining the investigation of a biologically identified mechanism with the analytic study of performance optimization in neural network models, this paper shows
the biologically plausible and beneficial role of weight dependent synaptic pruning.
Thus, in addition to the known effects of Hebbian learning, neuronal regulation may
play an important role in the self-organization of brain networks during development .

References
[1] G.M. Innocenti. Exuberant development of connections and its possible permissive role in cortical evolution. Trends Neurosci, 18:397-402, 1995.
[2] G. Chechik, I. Meilijson, and E. Ruppin. Synaptic pruning during development:
A computational account. Neural Computation. In press., 1998.
[3] G.G. Turrigano, K. Leslie, N. Desai, and S.B. Nelson . Activity dependent scaling of quantal amplitude in neocoritcal pyramidal neurons. Nature,
391(6670):892-896,1998.
[4] D. Horn, N. Levy, and E. Ruppin. Synaptic maintenance via neuronal regulation. Neural Computation, 10(1):1- 18,1998.
[5] J .R. Wolff, R. Laskawi, W.B. Spatz, and M. Missler. Structural dynamics of
synapses and synaptic components. Behavioral Brain Research, 66(1-2):13- 20,
1995.
[6] M.V . Tsodyks and M. Feigel'man. Enhanced storage capacity in neural networks with low activity level. Europhys. Lett., 6:101- 105,1988.
[7] Per E. Roland. Brain Activation. Willey-Liss, 1993 .
[8] I. Meilijson and E. Ruppin. Optimal firing in sparsely-connected low-activity
attractor networks. Biological cybernetics, 74:479-485, 1996.
[9] G .W . Davis and C .S. Goodman. Synapse-specific control of synaptic efficacy
at the terminals of a single neuron. Nature, 392(6671):82- 86, 1998.
[10] H. Colman, J . Nabekura, and J. W. Lichtman. Alterations in synaptic strength
preceding axon withdrawal. Science, 275(5298):356-361, 1997.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 353-self-organization-of-hebbian-synapses-in-hippocampal-neurons.pdf

Self-organization of Hebbian Synapses
in Hippocampal Neurons

Thomas H. Brown,t Zachary F. Mainen,t Anthony M. Zador,t and Brenda J. Claiborne?

t Department of Psychology

? Division of Life Sciences

Yale University

University of Texas

New Haven, cr 06511

San Antonio, TX 78285

ABSTRACT
We are exploring the significance of biological complexity for neuronal
computation. Here we demonstrate that Hebbian synapses in realistically-modeled hippocampal pyramidal cells may give rise to two novel
forms of self-organization in response to structured synaptic input. First,
on the basis of the electrotonic relationships between synaptic contacts,
a cell may become tuned to a small subset of its input space. Second, the
same mechanisms may produce clusters of potentiated synapses across
the space of the dendrites. The latter type of self-organization may be
functionally significant in the presence of nonlinear dendritic conductances.

1 INTRODUCTION
Long-term potentiation (LTP) is an experimentally observed form of synaptic plasticity
that has been interpreted as an instance of a Hebbian modification (Kelso et al, 1986;
Brown et al, 1990). The induction ofLTP requires synchronous presynaptic activity and
postsynaptic depolarization (Kelso et al, 1986). We have previously developed a detailed
biophysical model of the LTP observed at synapses onto hippocampal region CAl pyrami-

39

40

Brown, Mainen, Zador, and Claiborne

Figure 1: Two-dimensional projection of a reconstructed hippocampal CAl pyramidal cell.

dal neurons (Zador et al, 1990). The synapses at which this form of LTP occurs are distributed across an extensive dendritic arbor (Fig. 1). During synaptic stimulation, the
membrane voltage at each synapse is different. In this way, a biological neuron differs

from the processing elements typically used in neural network models, where the postsynaptic activity can be represented by a single state variable. We have developed an electrotonic model based on an anatomically reconstructed neuron. We have used this model to
explore how the spatial distribution of inputs and the temporal relationships of their activation affect synaptic potentiation.

2 THE NEURONAL MODEL
Standard compartmental modeling techniques were used to represent the electrical structure of hippocampal CAl pyramidal cells.
2.1 MORPHOLOGY AND ELECTRICAL PARAMETERS

Morphometric data were obtained from three-dimensional reconstructions (Brown et al.,
1991) of hippocampal neurons (Fig. 1). A correction factor was applied to the membrane
area based on an estimate for spine density of 2/llm. The original measurements divided
a single neuron into 3000-4000 cylinders with an average length of 5.5 J.1m. For simulation
purposes, this structure was collapsed into 300-400 compartments, preserving the connectivity pattern and changes in process diameter. Electrical constants were Rm = 70 ID-cm 2,
em= 1 JlF'lcrrll, Ri = 200 n-cm (Spruston & Johnston 1990). The membrane was electrically passive. Synaptic currents were modeled as the sum of fast AMPA and slow NMDA
conductances on the head of a two-compartment spine (Zador et al., 1990). The AMPA
conductance was represented by an alpha function (Jack et al., 1975) with time constant of
1.5 msec (Brown and Johnston, 1983). The NMDA conductance was represented by a
more complicated function with two time constants and a voltage dependence due to voltage-sensitive channel blocking by Mg2+ ions (see Zador et aI., 1990; Brown et al. 1991).
The initial peak conductances, gAMPA and gNMDA' were set to 0.5 and 0.1 nS respectively.

Self-organization of Hebbian Synapses in Hippocampal Neurons
2.2 SIMULATION AND SYNAPTIC MODIFICATION

Simulations were run on a Sun 4/330 workstation using a customized version of NEURON.
a simulator developed by Michael Hines (Hines. 1989). Prior to a simulation. 5 patterns of
40 synapses were selected at random from a pool of synapses distributed unifonnly over
the apical and basal dendrites. Simulations were divided into trials of 100 msec. At the
beginning of each trial a particular pattern of synapses was activated synchronously (3
stimuli at intervals of 3 msec). The sequential presentation of all 5 selected patterns
constituted an epoch. An entire simulation consisted of 20 presentation epochs. Over the
course of each trial. membrane potential was computed at each location in the dendritic
tree. and these voltages were used to compute weight changes .!\Wij according to the
Hebbian algorithm described below. After each trial. the actual peak AMPA conductances
(gAMPA. hereafter denoted g$1J were scaled by the sigmoidal function
gmax

(1)

where 0' detennines the steepness of the sigmoid. and gfM% was set to 1.0 nS.
The rule for synaptic modification was based on a biophysical interpretation (Kairiss et aI .?
1991; Brown et aI .? 1991) of a generalized bilinear fonn of Hebbian algorithm (Brown et
aI.? 1990):

(2)
where a. ~. and 'Y are functionals.l) is a constant. a.(t) represents postsynaptic activity and
a .(t) represents presynaptic activity. This equation specifies an interactive fonn of synaptic
ehhancement combined with three noninteractive forms of synaptic depression, all of
which have possible neurobiological analogs (Brown et aI. 1990). The interactive tenn was
derived from a biophysical model of LTP induction in a spine (Zador et aI'21990). A simplified version of this model was used to compute the concentration of Ca +-bound calmodulin. [CaM-C84]. It has been suggested that CaM-C84 may trigger protein kinases
responsible for LTP induction. In general [CaM-C8.4] was a nonlinear function of subsynaptic voltage (Zador et al .? 1990).
The biophysical mechanisms underlying synaptic depression are less well understood. The
constant l) represents a passive decay process and was generally set to zero. The functional
~ represents heterosynaptic depression based on postsynaptic activity. In these simulations, ~ was proportional the amount of depolarization of the subsynaptic membrane from
resting potential (V$111 - V ). The functional 'Y represents homosynaptic depression based
on presynaptic activity. Were. 'Y was proportional to the AMPA conductance. which can
be considered a measure of exclusively presynaptic activity because it is insensitive to
postsynaptic voltage. The three activity-dependent tenns were integrated over the period
of the trial in order to obtain a measure of weight change. Reinterpreting a. ~. and 'Yas constants. the equation is thus:
.!\Wij

=

f
"ial

[a [CamCa 4] -

~ (V.rYII- V,..,,)

- 'YgAMPA -l)] dt.

(3)

41

42

Brown, Mainen, Zador, and Claiborne
-40

-40

-40

-60

-60

-80 IL.-_ _ __

o

............ .

-====

-801.:..':_'

o

tOO

50

: ....... .

:...

m.sec

50

tOO

m.sec

................................................

....

. ...... .... .. -............ ......

-80~~~~~~~~~~~~~~~~~~

o

5

10

15

20

epochs
Figure 2: Interactions among Hebbian synapses produce differing global effects ("winning" and
"losing" patterns) on the basis of the spatial distribution of synapses. The PSP (always measured
at the soma) due to two different patterns of 40 synapses are plotted as a function of the presentation
epoch. Initially, pattern 1 (solid line) evoked a slightly greater PSP than pattern 2 (dotted line; inset, top right). Mter 20 epochs these responses were reversed: thePSP due to pattern 1 was depressed while the PSP due to pattern 2 was potentiated (inset, top left).

3 RESULTS
Analysis of the simulations revealed self-organization in the form of differential modification of synaptic strengths (Mainen et al. 1990). Two aspects of the self-organization phenomena were distinguished. In some simulations, a form of pattern selection was observed
in which clear "winners" and "losers" emerged. In other simulations, the average synaptic
efficacy remained about the same, but spatial heterogeneities~lustering~f synaptic
strength developed. Different measures were used to assess these phenomena.
3.1 PATTERNSELECTION

The change in the peak postsynaptic potential recorded at the soma (P SP) provided one useful measure of pattern selection. In many simulations, pattern selection resulted in a
marked potentiation of the PSP due to some patterns and a depression of the PSP due to
others. The PSP can be regarded as an indirect measure of the functional consequence of
self-organization. In the simulation illustrated in Fig. 2, patterns of 40 synapses produced
an average PSP of 15 mV before learning. After learning, responses ranged from 10% to
150% of this amount Underlying.pattern selection was a ch8!!ge in the average peak synaptic conductance for the patterng8YIIO).1 The initial value of g8YII was .!he same for all patterns, and its final value was bounded by eq. 1. In many simulations, g8YII approached the
upper bound for some patterns and the lower bound for other patterns (Fig. 3). In this way,
the neuron became selectively tuned to a subset of its original set of inputs. The specificity

Self-organization of Hebbian Synapses in Hippocampal Neurons
1.0

!

....... ..... .....

............................

0.5/?????????????????????????

~

~

o

5

10

15

20

epochs
Figure 3. The mean synaptic conductance gSy"of two patterns is plotted as a function of the presentation epoch. Both patterns began with iaenucal total synaptic strength (40 synapses with gs,r& =
0.5 nS). Synaptic conductances were constrained to the range [0.0, 1.0] nS. Mter twenty epochs,
gSY" of pattern 1 (solid line) approached the minimum ofO.OnS while gsy" of pattern 2 (dotted line)
approached the maximum of 1.0 nS.

of this tuning was dependent on the parameter values of the neuronal model, learning rule,
and stimulus set.
3.2 CLUSTER FORMAnON

Heterogeneity in the spatial distribution of strengthened and weakened synapses was often
observed. After learning, spatial clusters of synapses with similar conductances formed.
These spatial heterogeneities can be illustrated in several ways. In one convenient method
(see Brown et al., 1991), synapses are represented as colored points superimposed on a rendition of the neuronal morphology as illustrated in Fig. 1. By COlor-coding gsyn for each
synapse in a pattern, correlations in synaptic strength across dendritic space are immediately apparent. In a second method, better suited to the monochrome graphics available in the
present text, the evolution of the variance of gsyn is plotted as a function of time (Fig. 4).
In the simulation illustrated here, the increase in variance was due to the formation of a single, relatively large cluster of strengthened synapses. Within other parameter regimes, multiple clusters of smaller size were formed.

4

DISCUSSION

The important differences between synaptic modifications in the biophysically-modeled
neuron and those in simple processing elements arise from voltage gradients present in the
realistic model (Brown et aI., 1991; Kairiss et al., 1990). In standard processing elements,

g

1 Although SJ" and the somatic PSP were generally correlated, the relationship between the two is
not linear, as was often evident in simulations (compare initial trials in Figs. 2 and 3).

43

44

Brown, Mainen, Zador, and Claiborne
1.0

--_.-._--.-.----_._._.--_.

til

b

o. 0

"'""""'-----'--'--"---'~.............-'--~--'---''__'_...........__'

L-..-...........

o

5

10

15

20

epochs
Figure 4: Synaptic heterogeneity is indicated by increases in the variance (02) of the set of synaptic
conductances for each pattern. The variances of the peak synaptic conductances. (g'l,J of 4 patterns
are plotted as ajy)lction of the epoch. The variance of all 4 patterns approached the theoretical
maximum of JO.5. In this parameter regime, the variance was due to the potentiation of a single
large cluster of synapes combined with the depression of other synapses.

a single state variable represents postsynaptic activity. In contrast, the critical subsynaptic
voltages which represent postsynaptic activity in the neuron are correlated but are not strictly equal. The structure and electrical properties of the cell interact with its synaptic input
to detennine the precise spatiotemporal pattern of membrane voltage. Thus, the voltage at
any synapse depends strongly on its electrotonic relationships to other active synapses. The
way in which this local depolarization affects the nature of self-organization depends on the
specific mechanisms of the synaptic modification rule. We have modeled a pair of opposing voltage-dependent mechanisms. An interactive potentiation mechanism (the functional
ex) promotes cooperativity between spatially proximal synapses with temporally correlated
activity. A heterosynaptic depression mechanism (the functional P), which is independent
of presynaptic activity, promotes competition among spatially proximal synapses.
Through mechanisms such as these, the specific electrotonic structure of a neuron predetennines a complex set of interactions between any given spatial distribution of synaptic
inputs. We have shown that these higher-order interactions can give rise to self-organization with at least two interesting effects.
4.1 SPARSE REPRESENTATION

The phenomenon of pattern selection demonstrates how Hebbian self-organization may
naturally tune neurons to respond to a subset of their input space. This tuning mechanism
might allow a large field of neurons to develop a sparse coding of the activity in a set of
input fibers, since each neuron would respond to a particular small portion of the input
space. Sparse coding may be advantageous to associative learning and other types of neural
computation (Kanerva, 1988).

Self-organization of Hebbian Synapses in Hippocampal Neurons

4.2 CLUSTERING AND NONLINEAR COMPUTATION
The fonnation of clusters of strengthened synapses illustrates a property of Hebbian selforganization whose functional significance might only be appreciated in the presence of
nonlinear (voltage-dependent) dendritic conductances. We have examined the self-organization process in an electrically passive neuron. Under these conditions, the presence of
clustering within patterns has little effect on the observed output. In fact, it is known that
hippocampal cells of the type modeled possess a variety of spatially heterogeneous nonlinear dendritic conductances (Jones et al., 1989). The computational role of such nonlinearities is just beginning to be explored. It is possible that interactions between synaptic
clustering and nonlinear membrane patches may significantly affect both the perfonnance
of dendritic computations and the process of self-organization itself.
Acknowledgments
This research was supported by grants from the Office of Naval Research, the Defense Advanced Research Projects Agency, and the Air Force Office of Scientific Research.
References
Brown, T .H. and Johnston, D. (1983) Voltage-clamp analysis of mossy fiber synaptic input
to hippocampal neurons. J. Neurophysiol. SO: 487-507.
Brown, T.H., Kairiss, E.W. and Keenan, C.L. (1990) Hebbian synapses: biophysical mechanisms and algorithms. Annu. Rev. Neurosci. 13: 475-512.
Brown, T.H., Zador, A.M., Mainen, Z.F. and Claiborne, BJ. (1991) Hebbian modifications
in hippocampal neurons. In J. Davis and M. Baudry (eds.), LTP: A Debate of Current Issues (Cambridge, MA: MIT Press).
Hines, M. (1989) A program for simulation of nerve equations with branching geometries.
Int. J. Bio-Med Comp 24: 55-68.
Jack, J., Noble, A. and Tsien, R.W. (1975) Electrical Current Flow in Excitable Membranes (London: Oxford Univ. Press).
Jones, O.T., Kunze, D.L and Angelides, KJ. (1989) Localization and mobility ofw-conotoxin-sensitive Ca2+ channels in hippocampal CAl neurons. Science 244: 1189-1193.
Kairiss, E.W., Mainen, Z.F., Claiborne, BJ. and Brown, T.H. (1991) Dendritic control of
hebbian compuations. In F. Eeckman (ed.), Analysis and Modeling of Neural Systems
(Boston, MA: Kluwer Academic Publishers).
Kanerva, P. (1988) Sparse distributed memory. (Cambridge, MA: MIT Press).
Kelso, S.R., Ganong, Brown, T.H. (1986) Hebbian synapses in hippocampus. Proc. Natl.
Acad. Sci. USA 83: 5326-5330.
Mainen, Z.M., Zador, A.M., Claiborne, B. and Brown, T.H. (1990) Hebbian synapses induce feature mosaics in hippocampal dendrites. Soc. Neurosci. Abstr. 16: 492.
Spruston, N. and Johnston, D. (1990) Whole-cell patch clamp analysis of the passive membrane properties of hippocampal neurons. Soc. N eurosci. Abstr. 16: 1297.
Zador, A., Koch, C. and Brown, T.H. (1990) Biophysical model of a hebbian synapse.
Proc. Natl. Acad. Sci. USA 87: 6718-6722.

45


<<----------------------------------------------------------------------------------------------------------------------------------------->>

