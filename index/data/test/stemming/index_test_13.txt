query sentence: Self-organization of associative database and its applications
---------------------------------------------------------------------
title: 1-self-organization-of-associative-database-and-its-applications.pdf

767

SELF-ORGANIZATION OF ASSOCIATIVE DATABASE
AND ITS APPLICATIONS
Hisashi Suzuki and Suguru Arimoto
Osaka University, Toyonaka, Osaka 560, Japan
ABSTRACT
An efficient method of self-organizing associative databases is proposed together with
applications to robot eyesight systems. The proposed databases can associate any input
with some output. In the first half part of discussion, an algorithm of self-organization is
proposed. From an aspect of hardware, it produces a new style of neural network. In the
latter half part, an applicability to handwritten letter recognition and that to an autonomous
mobile robot system are demonstrated.

INTRODUCTION
Let a mapping f : X -+ Y be given. Here, X is a finite or infinite set, and Y is another
finite or infinite set. A learning machine observes any set of pairs (x, y) sampled randomly
from X x Y. (X x Y means the Cartesian product of X and Y.) And, it computes some
estimate j : X -+ Y of f to make small, the estimation error in some measure.
Usually we say that: the faster the decrease of estimation error with increase of the number of samples, the better the learning machine. However, such expression on performance
is incomplete. Since, it lacks consideration on the candidates of J of j assumed preliminarily. Then, how should we find out good learning machines? To clarify this conception,
let us discuss for a while on some types of learning machines. And, let us advance the
understanding of the self-organization of associative database .
. Parameter Type
An ordinary type of learning machine assumes an equation relating x's and y's with
parameters being indefinite, namely, a structure of f. It is equivalent to define implicitly a
set F of candidates of
(F is some subset of mappings from X to Y.) And, it computes
values of the parameters based on the observed samples. We call such type a parameter
type.
For a learning machine defined well, if F 3 f, j approaches f as the number of samples
increases. In the alternative case, however, some estimation error remains eternally. Thus,
a problem of designing a learning machine returns to find out a proper structure of f in this
sense.
On the other hand, the assumed structure of f is demanded to be as compact as possible
to achieve a fast learning. In other words, the number of parameters should be small. Since,
if the parameters are few, some j can be uniquely determined even though the observed
samples are few. However, this demand of being proper contradicts to that of being compact.
Consequently, in the parameter type, the better the compactness of the assumed structure
that is proper, the better the learning machine. This is the most elementary conception
when we design learning machines .

1.

. Universality and Ordinary Neural Networks
Now suppose that a sufficient knowledge on f is given though J itself is unknown. In
this case, it is comparatively easy to find out proper and compact structures of J. In the
alternative case, however, it is sometimes difficult. A possible solution is to give up the
compactness and assume an almighty structure that can cover various 1's. A combination
of some orthogonal bases of the infinite dimension is such a structure. Neural networks 1 ,2
are its approximations obtained by truncating finitely the dimension for implementation.

? American Institute of Physics 1988

768
A main topic in designing neural networks is to establish such desirable structures of 1.
This work includes developing practical procedures that compute values of coefficients from
the observed samples. Such discussions are :flourishing since 1980 while many efficient methods have been proposed. Recently, even hardware units computing coefficients in parallel
for speed-up are sold, e.g., ANZA, Mark III, Odyssey and E-1.
Nevertheless, in neural networks, there always exists a danger of some error remaining
eternally in estimating /. Precisely speaking, suppose that a combination of the bases of a
finite number can define a structure of 1 essentially. In other words, suppose that F 3 /, or
1 is located near F. In such case, the estimation error is none or negligible. However, if 1
is distant from F, the estimation error never becomes negligible. Indeed, many researches
report that the following situation appears when 1 is too complex. Once the estimation
error converges to some value (> 0) as the number of samples increases, it decreases hardly
even though the dimension is heighten. This property sometimes is a considerable defect of
neural networks .
. Recursi ve Type
The recursive type is founded on another methodology of learning that should be as
follows. At the initial stage of no sample, the set Fa (instead of notation F) of candidates
of I equals to the set of all mappings from X to Y. After observing the first sample
(Xl, Yl) E X x Y, Fa is reduced to Fi so that I(xt) = Yl for any I E F. After observing
the second sample (X2' Y2) E X x Y, Fl is further reduced to F2 so that i(xt) = Yl and
I(X2) = Y2 for any I E F. Thus, the candidate set F becomes gradually small as observation
of samples proceeds. The after observing i-samples, which we write
is one of the most
likelihood estimation of 1 selected in fi;. Hence, contrarily to the parameter type, the
recursive type guarantees surely that j approaches to 1 as the number of samples increases.
The recursive type, if observes a sample (x" yd, rewrites values 1,-l(X),S to I,(x)'s for
some x's correlated to the sample. Hence, this type has an architecture composed of a rule
for rewriting and a free memory space. Such architecture forms naturally a kind of database
that builds up management systems of data in a self-organizing way. However, this database
differs from ordinary ones in the following sense. It does not only record the samples already
observed, but computes some estimation of l(x) for any x E X. We call such database an
associative database.
The first subject in constructing associative databases is how we establish the rule for
rewri ting. For this purpose, we adap t a measure called the dissimilari ty. Here, a dissimilari ty
means a mapping d : X x X -+ {reals > O} such that for any (x, x) E X x X, d(x, x) > 0
whenever l(x) t /(x). However, it is not necessarily defined with a single formula. It is
definable with, for example, a collection of rules written in forms of "if? .. then?? .. "
The dissimilarity d defines a structure of 1 locally in X x Y. Hence, even though
the knowledge on f is imperfect, we can re:flect it on d in some heuristic way. Hence,
contrarily to neural networks, it is possible to accelerate the speed of learning by establishing
d well. Especially, we can easily find out simple d's for those l's which process analogically
information like a human. (See the applications in this paper.) And, for such /'s, the
recursive type shows strongly its effectiveness.
We denote a sequence of observed samples by (Xl, Yd, (X2' Y2),???. One of the simplest
constructions of associative databases after observing i-samples (i = 1,2,.,,) is as follows.

i

i"

I,

Algorithm 1. At the initial stage, let So be the empty set. For every i =
1,2" .. , let i,-l(x) for any x E X equal some y* such that (x*,y*) E S,-l and

d(x, x*) =

min
(%,y)ES.-t

d(x, x) .

Furthermore, add (x" y,) to S;-l to produce Sa, i.e., S, = S,_l U {(x"

(1)

y,n.

769

Another version improved to economize the memory is as follows.

Algorithm 2, At the initial stage, let So be composed of an arbitrary element
in X x Y. For every i = 1,2"", let ii-lex) for any x E X equal some y. such
that (x?, y.) E Si-l and
d(x, x?) =

min

d(x, x) .

(i,i)ES.-l

Furthermore, if ii-l(Xi) # Yi then let Si = Si-l, or add (Xi, Yi) to Si-l to
produce Si, i.e., Si = Si-l U {(Xi, Yi)}'
In either construction, ii approaches to f as i increases. However, the computation time
grows proportionally to the size of Si. The second subject in constructing associative
databases is what addressing rule we should employ to economize the computation time. In
the subsequent chapters, a construction of associative database for this purpose is proposed.
It manages data in a form of binary tree.

SELF-ORGANIZATION OF ASSOCIATIVE DATABASE
Given a sample sequence (Xl, Yl), (X2' Y2), .. " the algorithm for constructing associative
database is as follows.

Algorithm 3,'

Step I(Initialization): Let (x[root], y[root]) = (Xl, Yd. Here, x[.] and y[.] are
variables assigned for respective nodes to memorize data.. Furthermore, let t = 1.
Step 2: Increase t by 1, and put x, in. After reset a pointer n to the root, repeat
the following until n arrives at some terminal node, i.e., leaf.
Notations nand
d(xt, x[n)), let n

n mean the descendant nodes of n.
=n. Otherwise, let n =n.

If d(x" r[n)) ~

Step 3: Display yIn] as the related information. Next, put y, in. If yIn] = y" back
to step 2. Otherwise, first establish new descendant nodes n and n. Secondly,
let

(x[n], yIn))
(x[n], yIn))

(x[n], yIn)),
(Xt, y,).

(2)
(3)

Finally, back to step 2. Here, the loop of step 2-3 can be stopped at any time
and also can be continued.
Now, suppose that gate elements, namely, artificial "synapses" that play the role of branching by d are prepared. Then, we obtain a new style of neural network with gate elements
being randomly connected by this algorithm.

LETTER RECOGNITION
Recen tly, the vertical slitting method for recognizing typographic English letters3 , the
elastic matching method for recognizing hand written discrete English letters4 , the global
training and fuzzy logic search method for recognizing Chinese characters written in square
styleS, etc. are published. The self-organization of associative database realizes the recognition of handwritten continuous English letters.

770

9 /wn"

NOV

~ ~ ~ -xk :La.t

~~ ~ ~~~

dw1lo'

~~~~~of~~

~~~ 4,-?~~4Fig. 1. Source document.
2~~---------------'

lOO~---------------'

H

o

o
Fig. 2. Windowing.

1000

2000

3000

4000

Number of samples

o

1000

2000

3000

4000

NUAlber of sampl es

Fig. 3. An experiment result.

An image scanner takes a document image (Fig. 1). The letter recognizer uses a parallelogram window that at least can cover the maximal letter (Fig. 2), and processes the
sequence of letters while shifting the window. That is, the recognizer scans a word in a
slant direction. And, it places the window so that its left vicinity may be on the first black
point detected. Then, the window catches a letter and some part of the succeeding letter.
If recognition of the head letter is performed, its end position, namely, the boundary line
between two letters becomes known. Hence, by starting the scanning from this boundary
and repeating the above operations, the recognizer accomplishes recursively the task. Thus
the major problem comes to identifying the head letter in the window.
Considering it, we define the following.
? Regard window images as x's, and define X accordingly.
? For a (x, x) E X x X, denote by B a black point in the left area from the boundary on
window image X. Project each B onto window image x. Then, measure the Euclidean
distance 6 between fj and a black point B on x being the closest to B. Let d(x, x) be
the summation of 6's for all black points B's on x divided by the number of B's.
? Regard couples of the "reading" and the position of boundary as y's, and define Y
accordingly.
An operator teaches the recognizer in interaction the relation between window image and
reading& boundary with algorithm 3. Precisely, if the recalled reading is incorrect, the
operator teaches a correct reading via the console. Moreover, if the boundary position is
incorrect, he teaches a correct position via the mouse.
Fig. 1 shows partially a document image used in this experiment. Fig. 3 shows the
change of the number of nodes and that of the recognition rate defined as the relative
frequency of correct answers in the past 1000 trials. Speciiications of the window are height
= 20dot, width = 10dot, and slant angular = 68deg. In this example, the levels of tree
were distributed in 6-19 at time 4000 and the recognition rate converged to about 74%.
Experimentally, the recognition rate converges to about 60-85% in most cases, and to 95% at
a rare case. However, it does not attain 100% since, e.g., "c" and "e" are not distinguishable
because of excessive lluctuation in writing. If the consistency of the x, y-relation is not
assured like this, the number of nodes increases endlessly (d. Fig. 3). Hence, it is clever to
stop the learning when the recognition rate attains some upper limit. To improve further
the recognition rate, we must consider the spelling of words. It is one of future subjects.

771

OBSTACLE AVOIDING MOVEMENT
Various systems of camera type autonomous mobile robot are reported flourishingly6-1O.
The system made up by the authors (Fig. 4) also belongs to this category. Now, in mathematical methodologies, we solve usually the problem of obstacle avoiding movement as
a cost minimization problem under some cost criterion established artificially. Contrarily,
the self-organization of associative database reproduces faithfully the cost criterion of an
operator. Therefore, motion of the robot after learning becomes very natural.
Now, the length, width and height of the robot are all about O.7m, and the weight is
about 30kg. The visual angle of camera is about 55deg. The robot has the following three
factors of motion. It turns less than ?30deg, advances less than 1m, and controls speed less
than 3km/h. The experiment was done on the passageway of wid th 2.5m inside a building
which the authors' laboratories exist in (Fig. 5). Because of an experimental intention, we
arrange boxes, smoking stands, gas cylinders, stools, handcarts, etc. on the passage way at
random. We let the robot take an image through the camera, recall a similar image, and
trace the route preliminarily recorded on it. For this purpose, we define the following.
? Let the camera face 28deg downward to take an image, and process it through a low
pass filter. Scanning vertically the filtered image from the bottom to the top, search
the first point C where the luminance changes excessively. Then, su bstitu te all points
from the bottom to C for white, and all points from C to the top for black (Fig. 6).
(If no obstacle exists just in front of the robot, the white area shows the ''free'' area
where the robot can move around.) Regard binary 32 x 32dot images processed thus
as x's, and define X accordingly.
? For every (x, x) E X x X, let d(x, x) be the number of black points on the exclusive-or
image between x and X.
? Regard as y's the images obtained by drawing routes on images x's, and define Y
accordingly.
The robot superimposes, on the current camera image x, the route recalled for x, and
inquires the operator instructions. The operator judges subjectively whether the suggested
route is appropriate or not. In the negative answer, he draws a desirable route on x with the
mouse to teach a new y to the robot. This opera.tion defines implicitly a sample sequence
of (x, y) reflecting the cost criterion of the operator.

.::l" !
-

IibUBe

_. -

22

11

Roan

12

{-

13

Stationary uni t

Fig. 4. Configuration of
autonomous mobile robot system.

~

I

,

23

24

North
14

rmbi Ie unit (robot)

-

Roan

y

t

Fig. 5. Experimental
environment.

772

Wall

Camera image

Preprocessing

A

::: !fa

?

Preprocessing

0

O

Course
suggest ion

??

..

Search

A

Fig. 6. Processing for
obstacle avoiding movement.

x

Fig. 1. Processing for
position identification.
We define the satisfaction rate by the relative frequency of acceptable suggestions of
route in the past 100 trials. In a typical experiment, the change of satisfaction rate showed
a similar tendency to Fig. 3, and it attains about 95% around time 800. Here, notice that
the rest 5% does not mean directly the percentage of collision. (In practice, we prevent the
collision by adopting some supplementary measure.) At time 800, the number of nodes was
145, and the levels of tree were distributed in 6-17.
The proposed method reflects delicately various characters of operator. For example, a
robot trained by an operator 0 moves slowly with enough space against obstacles while one
trained by another operator 0' brushes quickly against obstacles. This fact gives us a hint
on a method of printing "characters" into machines.
POSITION IDENTIFICATION
The robot can identify its position by recalling a similar landscape with the position data
to a camera image. For this purpose, in principle, it suffices to regard camera images and
position data as x's and y's, respectively. However, the memory capacity is finite in actual
compu ters. Hence, we cannot but compress the camera images at a slight loss of information.
Such compression is admittable as long as the precision of position identification is in an
acceptable area. Thus, the major problem comes to find out some suitable compression
method.
In the experimental environment (Fig. 5), juts are on the passageway at intervals of
3.6m, and each section between adjacent juts has at most one door. The robot identifies
roughly from a surrounding landscape which section itself places in. And, it uses temporarily
a triangular surveying technique if an exact measure is necessary. To realize the former task,
we define the following .
? Turn the camera to take a panorama image of 360deg. Scanning horizontally the
center line, substitute the points where the luminance excessively changes for black
and the other points for white (Fig. 1). Regard binary 360dot line images processed
thus as x's, and define X accordingly.
? For every (x, x) E X x X, project each black point A on x onto x. And, measure the
Euclidean distance 6 between A and a black point A on x being the closest to A. Let
the summation of 6 be S. Similarly, calculate S by exchanging the roles of x and X.
Denoting the numbers of A's and A's respectively by nand n, define

773

d(x, x) =

~(~
+ ~).
2 n
n

(4)

? Regard positive integers labeled on sections as y's (cf. Fig. 5), and define Y accordingly.
In the learning mode, the robot checks exactly its position with a counter that is reset periodically by the operator. The robot runs arbitrarily on the passageways within 18m area
and learns the relation between landscapes and position data. (Position identification beyond 18m area is achieved by crossing plural databases one another.) This task is automatic
excepting the periodic reset of counter, namely, it is a kind of learning without teacher.
We define the identification rate by the relative frequency of correct recalls of position
data in the past 100 trials. In a typical example, it converged to about 83% around time
400. At time 400, the number of levels was 202, and the levels oftree were distributed in 522. Since the identification failures of 17% can be rejected by considering the trajectory, no
pro blem arises in practical use. In order to improve the identification rate, the compression
ratio of camera images must be loosened. Such possibility depends on improvement of the
hardware in the future.
Fig. 8 shows an example of actual motion of the robot based on the database for obstacle
avoiding movement and that for position identification. This example corresponds to a case
of moving from 14 to 23 in Fig. 5. Here, the time interval per frame is about 40sec.

,~. .~ (
;~"i..
~

"

"

.

..I

I

?
?

"

I'
.
'.1
t

;

i

-:
, . . , 'II

Fig. 8. Actual motion of the robot.

774

CONCLUSION
A method of self-organizing associative databases was proposed with the application to
robot eyesight systems. The machine decomposes a global structure unknown into a set of
local structures known and learns universally any input-output response. This framework
of problem implies a wide application area other than the examples shown in this paper.
A defect of the algorithm 3 of self-organization is that the tree is balanced well only
for a subclass of structures of f. A subject imposed us is to widen the class. A probable
solution is to abolish the addressing rule depending directly on values of d and, instead, to
establish another rule depending on the distribution function of values of d. It is now under
investigation.

REFERENCES
1. Hopfield, J. J. and D. W. Tank, "Computing with Neural Circuit: A Model/'

Science 233 (1986), pp. 625-633.
2. Rumelhart, D. E. et al., "Learning Representations by Back-Propagating Errors," Nature 323 (1986), pp. 533-536.

3. Hull, J. J., "Hypothesis Generation in a Computational Model for Visual Word
Recognition," IEEE Expert, Fall (1986), pp. 63-70.
4. Kurtzberg, J. M., "Feature Analysis for Symbol Recognition by Elastic Matching," IBM J. Res. Develop. 31-1 (1987), pp. 91-95.

5. Wang, Q. R. and C. Y. Suen, "Large Tree Classifier with Heuristic Search and
Global Training," IEEE Trans. Pattern. Anal. & Mach. Intell. PAMI 9-1
(1987) pp. 91-102.
6. Brooks, R. A. et al, "Self Calibration of Motion and Stereo Vision for Mobile
Robots," 4th Int. Symp. of Robotics Research (1987), pp. 267-276.
7. Goto, Y. and A. Stentz, "The CMU System for Mobile Robot Navigation," 1987
IEEE Int. Conf. on Robotics & Automation (1987), pp. 99-105.
8. Madarasz, R. et al., "The Design of an Autonomous Vehicle for the Disabled,"
IEEE Jour. of Robotics & Automation RA 2-3 (1986), pp. 117-125.
9. Triendl, E. and D. J. Kriegman, "Stereo Vision and Navigation within Buildings," 1987 IEEE Int. Conf. on Robotics & Automation (1987), pp. 1725-1730.
10. Turk, M. A. et al., "Video Road-Following for the Autonomous Land Vehicle,"
1987 IEEE Int. Conf. on Robotics & Automation (1987), pp. 273-279.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 1149-laterally-interconnected-self-organizing-maps-in-hand-written-digit-recognition.pdf

Laterally Interconnected Self-Organizing
Maps in Hand-Written Digit Recognition

Yoonsuck Choe, Joseph Sirosh, and Risto Miikkulainen
Department of Computer Sciences
The University of Texas at Austin
Austin, TX 78712
yschoe,sirosh,risto@cs. u texas .ed u

Abstract
An application of laterally interconnected self-organizing maps
(LISSOM) to handwritten digit recognition is presented. The lateral connections learn the correlations of activity between units on
the map. The resulting excitatory connections focus the activity
into local patches and the inhibitory connections decorrelate redundant activity on the map. The map thus forms internal representations that are easy to recognize with e.g. a perceptron network. The
recognition rate on a subset of NIST database 3 is 4.0% higher with
LISSOM than with a regular Self-Organizing Map (SOM) as the
front end, and 15.8% higher than recognition of raw input bitmaps
directly. These results form a promising starting point for building
pattern recognition systems with a LISSOM map as a front end.

1

Introduction

Hand-written digit recognition has become one of the touchstone problems in neural
networks recently. Large databases of training examples such as the NIST (National
Institute of Standards and Technology) Special Database 3 have become available,
and real-world applications with clear practical value, such as recognizing zip codes
in letters, have emerged. Diverse architectures with varying learning rules have
been proposed, including feed-forward networks (Denker et al. 1989; Ie Cun et al.
1990; Martin and Pittman 1990), self-organizing maps (Allinson et al. 1994), and
dedicated approaches such as the neocognitron (Fukushima and Wake 1990) .
The problem is difficult because handwriting varies a lot, some digits are easily
confusable, and recognition must be based on small but crucial differences. For example, the digits 3 and 8, 4 and 9, and 1 and 7 have several overlapping segments,
and the differences are often lost in the noise. Thus, hand-written digit recognition can be seen as a process of identifying the distinct features and producing an
internal representation where the significant differences are magnified, making the
recognition easier.

Laterally Interconnected Self-organizing Maps in Handwritten Digit Recognition

737

In this paper, the Laterally Interconnected Synergetically Self-Organizing Map architecture (LISSOM; Sirosh and Miikkulainen 1994, 1995, 1996) was employed to
form such a separable representation. The lateral inhibitory connections of the LISSOM map decorrelate features in the input, retaining only those differences that are
the most significant . Using LISSOM as a front end, the actual recognition can be
performed by any standard neural network architecture, such as the perceptron.
The experiments showed that while direct recognition of the digit bitmaps with a
simple percept ron network is successful 72.3% of the time , and recognizing them
using a standard self-organizing map (SOM) as the front end 84.1% of the time,
the recognition rate is 88.1 % based on the LISSOM network . These results suggest
that LISSOM can serve as an effective front end for real-world handwritten character
recognition systems.

2

The Recognition System

2.1 Overall architecture
The system consists of two networks: a 20 x 20 LISSOM map performs the feature
analysis and decorrelation of the input, and a single layer of 10 perceptrons the final
recognition (Figure 1 (a)) . The input digit is represented as a bitmap on the 32 x 32
input layer. Each LISSOM unit is fully connected to the input layer through the afferent connections, and to the other units in the map through lateral excitatory and
inhibitory connections (Figure 1 (b)). The excitatory connections are short range,
connecting only to the closest neighbors of the unit, but the inhibitory connections
cover the whole map. The percept ron layer consists of 10 units, corresponding to
digits 0 to 9. The perceptrons are fully connected to the LISSOM map, receiving the full activation pattern on the map as their input . The perceptron weights
are learned through the delta rule, and the LISSOM afferent and lateral weights
through Hebbian learning.
2.2 LISSOM Activity Generation and Weight Adaptation
The afferent and lateral weights in LISSOM are learned through Hebbian adaptation. A bitmap image is presented to the input layer , and the initial activity of
the map is calculated as the weighted sum of the input . For unit (i, j), the initial
response TJij IS
TJij =

(7

('2:

(1)

eabllij,ab) ,

a,b

where eab is the activation of input unit (a, b), Ilij ,ab is the afferent weight connecting
input unit (a, b) to map unit (i, j), and (7 is a piecewise linear approximation of
the sigmoid activation function . The activity is then settled through the lateral
connections. Each new activity TJij (t) at step t depends on the afferent activation
and the lateral excitation and inhibition:
TJiAt) =

(7

('2:
a,b

eabllij,ab

+ Ie

'2:
k,l

Eij ,kITJkl(t - 1) - Ii

'2:
k ,l

Iij,kITJkl(t -

1)),

(2)

where Eij ,kl and Iij,kl are the excitatory and inhibitory connection weights from
map unit (k, l) to (i, j) and TJkl(t - 1) is the activation of unit (k , I) during the
previous time step. The constants I e and Ii control the relative strength of the
lateral excitation and inhibition.
After the activity has settled, the afferent and lateral weights are modified according
to the Hebb rule. Afferent weights are normalized so that the length of the weight

Y. CHOE, J. SIROSH, R. MIIKKULAINEN

738
Output Layer (10)

.Lq?'Li7.L:17.La7'LV.87..,.Li7.LWLp'
:

...... LISSOM Map

LaY~/~~~X20)

L::7.L7.L7""'-.L::7LI7

~
.......,.......c:7\

.
..c:7L:7.&l?7'..,......
L7
; .L7.L7.?7.L7LSJ7L7
'L7.AlFL7.L7..c:7L7
.L7.A11P".AIIP"L7.L7.o
"

\.

:mput L~yer (32x32)

L7L7~~~~~~~L7L7

".

L7.L7L:7.L7.L7..c:7L7 L7L7~~~~~~~L7L7 .
.L7.L7............,..L7..c:7 L7L7L7L7L7L7L7L7L7L7L7. '
.L7..,..L7L::7.L7.L7..c:7 L7L7L7L7L7L7L7L7L7L7L70 '

..c:7..,..L7.....~..c:7..c:7

0

20

.
L7..,...,..L7.L?..,.L7
. . ..c:7..,..L7L7.L7..,..L7
. . L7.L:7..,...,...,.L/.L:7
:L:7.L7..c:7.L7.L7..c:7L7

OJ)

?

Unit

tII'd

Units with excitatory lateral connections to (iJ)

?

Units with inhibitory lateral connections to (iJ)

(a)

(b)

Figure 1: The system architecture. (a) The input layer is activated according to the
bitmap image of digit 6. The activation propagates through the afferent connections to
the LISSOM map, and settles through its lateral connections into a stable pattern. This
pattern is the internal representation of the input that is then recognized by the perceptron
layer. Through ,t he connections from LISSOM to the perceptrons, the unit representing 6
is strongly activated, with weak activations on other units such as 3 and 8. (b) The lateral
connections to unit (i, j), indicated by the dark square, are shown. The neighborhood
of excitatory connections (lightly shaded) is elevated from the map for a clearer view.
The units in the excitatory region also have inhibitory lateral connections (indicated by
medium shading) to the center unit. The excitatory radius is 1 and the inhibitory radius
3 in this case.
vector remains the same; lateral weights are normalized to keep the sum of weights
constant (Sirosh and Miikkulainen 1994):
..

(t

+ 1) -

IllJ,mn

-

..
(t
W1J,kl

+ crinp1]ij~mn
+ crinp1]ij~mnF'
Wij,kl(t) + cr1]ij1]kl
"'" [
( )
] ,
wkl Wij ,kl t + cr1]ij1]kl
Ilij,mn(t)

(3)

VLmn[llij,mn(t)

+ 1) _
-

(4)

where Ilij,mn is the afferent weight from input unit (m, n) to map unit (i, j), and
crinp is the input learning rate; Wij ,kl is the lateral weight (either excitatory Eij ,kl
or inhibitory Iij ,kl) from map unit (k, I) to (i, j), and cr is the lateral learning rate
(either crexc or crinh).

2.3 Percept ron Output Generation and Weight Adaptation
The perceptrons at the output of the system receive the activation pattern on the
LISSOM map as their input. The perceptrons are trained after the LISSOM map
has been organized. The activation for the perceptron unit Om is

Om

= CL1]ij Vij,m,

(5)

i,j

where C is a scaling constant, 1]ij is the LISSOM map unit (i,j), and Vij,m is the
connection weight between LISSOM map unit (i,j) and output layer unit m. The
delta rule is used to train the perceptrons: the weight adaptation is proportional to
the map activity and the difference between the output and the target:
Vij,m(t

+ 1) = Vij,m(t) + crout1]ij((m -

Om),

(6)

where crout is the learning rate of the percept ron weights, 1]ij is the LISSOM map
unit activity, (m is the target activation for unit m. ((m = 1 if the correct digit =
m, 0 otherwise).

Laterally Interconnected Self-organizing Maps in Handwritten Digit Recognition

I Representation I
LISSOM
SOM
Raw Input

Training
93.0/ 0.76
84.5/ 0.68
99.2/ 0.06

739

Test
88.1/ 3.10
84.1/ 1.71
72.3/ 5.06

Table 1: Final Recognition Results. The average recognition percentage and its
variance over the 10 different splits are shown for the training and test sets. The
differences in each set are statistically significant with p > .9999.

3

Experiments

A subset of 2992 patterns from the NIST Database 3 was used as training and
testing data. 1 The patterns were normalized to make sure taht each example had
an equal effect on the LISSOM map (Sirosh and Miikkulainen 1994). LISSOM
was trained with 2000 patterns. Of these, 1700 were used to train the perceptron
layer, and the remaining 300 were used as the validation set to determine when
to stop training the perceptrons. The final recognition performance of the whole
system was measured on the remaining 992 patterns, which neither LISSOM nor
the perceptrons had seen during training . The experiment was repeated 10 times
with different random splits of the 2992 input patterns into training, validation ,
and testing sets.
The LISSOM map can be organized starting from initially random weights. However, if the input dimensionality is large, as it is in case of the 32 X 32 bitmaps,
each unit on the map is activated roughly to the same degree, and it is difficult to
bootstrap the self-organizing process (Sirosh and Miikkulainen 1994, 1996). The
standard Self-Organizing Map algorithm can be used to preorganize the map in
this case. The SOM performs preliminary feature analysis of the input, and forms
a coarse topological map of the input space. This map can then be used as the
starting point for the LISSOM algorithm, which modifies the topological organization and learns lateral connections that decorrelate and represent a more clear
categorization of the input patterns.
The initial self-organizing map was formed in 8 epochs over the training set, gradually reducing the neighborhood radius from 20 to 8. The lateral connections were
then added to the system, and over another 30 epochs, the afferent and lateral
weights of the map were adapted according to equations 3 and 4. In the beginning,
the excitation radius was set to 8 and the inhibition radius to 20. The excitation
radius was gradually decreased to 1 making the activity patterns more concentrated
and causing the units to become more selective to particular types of input patterns. For comparison, the initial self-organized map was also trained for another 30
epochs, gradually decreasing the neighborhood size to 1 as well. The final afferent
weights for the SOM and LISSOM maps are shown in figures 2 and 3.
After the SOM and LISSOM maps were organized, a complete set of activation
patterns on the two maps were collected. These patterns then formed the training
input for the perceptron layer. Two separate versions were each trained for 500
epochs, one with SOM and the other with LISSOM patterns. A third perceptron
layer was trained directly with the input bitmaps as well.
Recognition performance was measured by counting how often the most highly active perceptron unit was the correct one. The results were averaged over the 10
different splits. On average, the final LISSOM+perceptron system correctly recognized 88.1% of the 992 pattern test sets. This is significantly better than the 84.1%
1

Downloadable at ftp:j jsequoyah.ncsl.nist.gov jpubjdatabasesj.

Y. CHOE, J. SIROSH, R. MIIKKULAINEN

740

Iliiji
'?~1,;i;:!il ,
'8 .. . . ?? Slll .. ". "1111

"Q"
""'

.'11
.1 11/1
<?1,1111

Figure 2: Final Afferent Weights of the SOM map. The digit-like patterns represent
the afferent weights of each map unit projected on the input layer. For example, the lower
left corner represents the afferent weights of unit (0,0). High weight values are shown in
black and low in white. The pattern of weights shows the input pattern to which this unit
is most sensitive (6 in this case). There are local clusters sensitive to each digit category.
of the SOM+perceptron system, and the 72.3% achieved by the perceptron layer
alone (Table 1). These results suggest that the internal representations generated
by the LISSOM map are more distinct and easier to recognize than the raw input
patterns and the representations generated by the SOM map.

4

Discussion

The architecture was motivated by the hypothesis that the lateral inhibitory connections of the LISSOM map would decorrelate and force the map activity patterns
to become more distinct . The recognition could then be performed by even the
simplest classification architectures, such as the perceptron. Indeed, the LISSOM
representations were easier to recognize than the SOM patterns, which lends evidential support to the hypothesis. In additional experiments , the percept ron output
layer was replaced by a two-weight-Iayer backpropagation network and a Hebbian
associator net, and trained with the same patterns as the perceptrons. The recognition results were practically the same for the perceptron, backpropagation, and
Hebbian output networks, indicating that the internal representations formed by
the LISSOM map are the crucially important part of the recognition system.
A comparison of the learning curves reveals two interesting effects (figure 4). First,
even though the perceptron net trained with the raw input patterns initially performs well on the test set, its generalization decreases dramatically during training.
This is because the net only learns to memorize the training examples, which does
not help much with new noisy patterns. Good internal representations are therefore crucial for generalization. Second, even though initially the settling process
of the LISSOM map forms patterns that are significantly easier to recognize than

Laterally Interconnected Self-organizing Maps in Handwritten Digit Recognition

741

Figure 3: Final Afferent Weights of the LISSOM map. The squares identify the
above-average inhibitory lateral connections to unit (10,4) (indicated by the thick square).
Note that inhibition comes mostly from areas of similar functionality (i.e. areas sensitive to
similar input), thereby decorrelating the map activity and forming a sparser representation
of the input .
the initial, unsettled patterns (formed through the afferent connections only), this
difference becomes insignificant later during training. The afferent connections are
modified according to the final, settled patterns, and gradually learn to anticipate
the decorrelated internal representations that the lateral connections form.

5

Conclusion

The experiments reported in this paper show that LISSOM forms internal representations of the input patterns that are easier to categorize than the raw inputs and
the patterns on the SOM map, and suggest that LISSOM can form a useful front
end for character recognition systems, and perhaps for other pattern recognition
systems as well (such as speech) . The main direction of future work is to apply
the approach to larger data sets, including the full NIST 3 database, to use a more
powerful recognition network instead of the perceptron, and to increase the map
size to obtain a richer representation of the input space.

Acknowledgements
This research was supported in part by National Science Foundation under grant
#IRI-9309273. Computer time for the simulations was provided by the Pittsburgh
Supercomputing Center under grants IRI930005P and IRI940004P, and by a High
Performance Computer Time Grant from the University of Texas at Austin .

References
Allinson, N. M., Johnson , M. J., and Moon, K. J. (1994). Digital realisation of selforganising maps. In Touretzky, D. S., editor, Advances in Neural Information
Processing Systems 6. San Mateo, CA: Morgan Kaufmann.

742

Y. CHOE. J. SIROSH. R. MIIKKULAINEN
Comparison:Test

100
'SettIEi<CLlSSOU'
'Unsettled LISSOM'
. - 'SOM'
:.Rawj~~~t'

95

----.
.... .
... .

90
~--

"0

-... -- --- -------_.- ----~------ -- -

~

0

()

.

85

... j . .

.... -.---_ .....

-oe.

.. . . . . ... ,..

80

.

--.- ........ . .

",

..~

75

o

.................... --.....

____L -_ _-L____L -_ _-L____L -_ _-L____L -_ _
50

100

150

200

250

300

350

400

__

450

~

..

~

__

~

~

0

7

. '" ..... - ~. ...

500

Epochs

Figure 4: Comparison of the learning curves, A perceptron network was trained to
recognize four different kinds of internal representations: the settled LISSOM patterns,
the LISSOM patterns before settling, the patterns on the final SOM network, and raw
input bitmaps. The recognition accuracy on the test set was then measured and averaged
over 10 simulations. The generalization of the raw input + perceptron system decreases
rapidly as the net learns to memorize the training patterns. The difference of using settled
and unsettled LISSOM patterns diminishes as the afferent weights of LISSOM learn to
take into account the decorrelation performed by the lateral weights.
Denker, J. S., Gardner, W. R., Graf, H. P., Henderson, D., Howard, R. E., Hubbard,
W., Jackel, L. D., Baird, H. S., and Guyon, I. (1989). Neural network recognizer
for hand-written zip code digits. In Touretzky, D . S., editor, Advances in Neural
Information Processing Systems 1. San Mateo, CA: Morgan Kaufmann .
Fukushima, K., and Wake, N. (1990). Alphanumeric character recognition by
neocognitron. In Advanced Neural Computers, 263- 270. Elsevier Science Publishers B.V . (North-Holland).
Ie Cun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard,
W., and Jackel, 1. D. (1990) . Handwritten digit recognition with a backpropagation network. In Touretzky, D. S., editor, Advances in Neural Information Processing Systems 2. San Mateo, CA: Morgan Kaufmann.
Martin, G. L., and Pittman, J. A. (1990). Recognizing hand-printed letters and
digits. In Touretzky, D. S., editor, Advances in Neural Information Processing
Systems 2. San Mateo, CA: Morgan Kaufmann.
Sirosh, J. , and Miikkulainen, R. (1994). Cooperative self-organization of afferent
and lateral connections in cortical maps . Biological Cybernetics, 71:66- 78.
Sirosh, J., and Miikkulainen, R. (1995). Ocular dominance and patterned lateral
connections in a self-organizing model of the primary visual cortex. In Tesauro,
G ., Touretzky, D. S., and Leen, T . K., editors, Advances in Neural Information
Processing Systems 7. Cambridge, MA: MIT Press.
Sirosh, J., and Miikkulainen, R. (1996). Topographic receptive fields and patterned
lateral interaction in a self-organizing model of the primary visual cortex. Neural Computation (in press).


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 570-a-self-organizing-integrated-segmentation-and-recognition-neural-net.pdf

A Self-Organizing Integrated Segmentation And
Recognition Neural Net

Jim Keeler *
MCC
3500 West Balcones Center Drive
Austin, TX 78729

David E. Rumelhart
Psychology Department
Stanford University
Stanford, CA 94305

Abstract
We present a neural network algorithm that simultaneously performs segmentation and recognition of input patterns that self-organizes to detect
input pattern locations and pattern boundaries. We demonstrate this neural network architecture on character recognition using the NIST database
and report on results herein. The resulting system simultaneously segments and recognizes touching or overlapping characters, broken characters, and noisy images with high accuracy.

1

INTRODUCTION

Standard pattern recognition systems usually involve a segmentation step prior to
the recognition step. For example, it is very common in character recognition to
segment characters in a pre-processing step then normalize the individual characters
and pass them to a recognition engine such as a neural network, as in the work of
LeCun et al. 1988, Martin and Pittman (1988).
This separation between segmentation and recognition becomes unreliable if the
characters are touching each other, touching bounding boxes, broken, or noisy.
Other applications such as scene analysis or continuous speech recognition pose
similar and more severe segmentation problems. The difficulties encountered in
these applications present an apparent dilemma: one cannot recognize the patterns
*keeler@mcc.com Reprint requests: coila@mcc.com or at the above address.

496

A Self-Organizing Integrated Segmentation and Recognition Neural Net

Sz Outputs: pz = - I + Sz

I
I
I
ItfIM.............
I
I
I

5

Summing Units:

Sz =

LXxyz
xy

I~~~

I
I
I
I
I

,

2.AFI

1ABY/

?AW

Grey-scale
Input image

I(.X,y)

Figure 1: The ISR network architecture. The input image may contain several
characters and is presented to the network in a two-dimensional grey-scale image.
The units in the first block, hij", have linked-local receptive field connections to the
input image. Block 2, Hr'JI'z" has a three-dimensional linked-local receptive field
to block 1, and the exponential unit block, block 3, has three-dimensional linkedlocal receptive field connections to block 2. These linked fields insure translational
invariance (except for edge-effects at the boundary). The exponential unit block
has one layer for each output category. These units are the output units in the test
mode, but hidden units during training: the exponential unit activity is summed
over (sz) to project out the positional information, then converted to a probability
Pz. Once trained, the exponential unit layers serve as "smart histograms" giving
sharp peaks of activity directly above the corresponding characters in the input
image, as shown to the left.

497

498

Keeler and Rumelhart

until they are segmented, yet in many cases one cannot segment the patterns until
they are recognized.
A solution to this apparent dilemm is to simultaneously segment and recognize
the patterns. Integration of the segmentation and recognition steps is essential for
further progress in these difficult pattern recognition tasks, and much effort has been
devoted to this topic in speech recognition. For example, Hidden Markov models
integrate the task of segmentation and recognition as a part of the word-recognition
module. Nevertheless, little neural network research in pattern recognition has
focused on the integrated segmentation and recognition (ISR) problem.
There are several ways to achieve ISR in a neural network. The first use of backpropagation ISR neural networks for character recognition was reported by Keeler,
Rumelhart and Leow (1991a). The ISR neural network architecture is similar to
the time-delayed neural network architecture for speech recognition used by Lang,
Hinton, and Waibel (1990).
The following section outlines the neural network algorithm and architecture. Details and rationale for the exact structure and assumptions of the network can be
found in Keeler et al. (1991a,b).

2

NETWORK ARCHITECTURE AND ALGORITHM

The basic organization of the network is illustrated in Figure 2. The input consists
of a twcrdimensional grey-scale image representing the pattern to be processed. We
designate this input pattern by the twcrdimensional field lex, y). In general, we
assume that any pattern can be presented at any location and that the characters
may touch, overlap or be broken or noisy. The input then projects to a linked-Iocalreceptive-field block of sigmoidal hidden units (to enforce translational invariance).
We designate the activation of the sigmoidal units in this block by h ij It.
The second block of hidden units, H 1:'J/' z', is a linked-local receptive field block of
sigmoidal units that receives input from a three-dimensional receptive field in the
hiilt block. In a standard neural network architecture we would normally connect
block H to the output units. However we connect block H to a block of exponential
units X1:J/z, The X block serves as the outputs after the network has been trained;
there is a sheet of exponential units for each output category. These units are
e"''''"?,
connected to block H via a linked-local receptive field structure. X1:J/z
where the net input to the unit is

=

TJ1:J/Z =

L W:,~~z,H1:'J/'z' + /3z,

(1)

1:'J/'

and W:,~~z' is the weight from hidden unit H1:'J/'z' to the exponential unit X1:J/z,
Since we use linked weights in each block, the entire structure is translationally
invariant. We make use of this property in our training algorithm and project out
the positional information by summing over the entire layer, Sz = L1:Y X1:J/z, This
allows us to give non-specific target information in the form of "the input contains
a 5 and a 3, but I will not say where." We do this by converting the summed
information in"to an output probability, pz = 1!5?.

A Self-Organizing Integrated Segmentation and Recognition Neural Net

2.1

The learning Rule

There are two objective functions that we have used to train ISR networks: cross
entropy and total-sum-square-error. I Ez tzlnpz + (1 - t z )ln(l - Pz), where t z
equals 1 if pattern z is presented and 0 otherwise. Computing the gradient with
respect to the net input to a particular exponential unit yields the following term
in our learning rule:

=

~ -- (t z _ pz )
8TJ~yz

X~yz

(2)

E~y X~yz

It should be noted that this is a kind of competitive rule in which the learning is

proportional to the relative strength of the activation at the unit at a particular
location in the X layer to the strength of activation in the entire layer. For example,
suppose that X2,3,5
1000 and X5,3,5= 100. Given the above rules, X2,3,5 would
receive about 10 times more of the output error than the unit X5,3,5. Thus the units
compete with each other for the credit or blame of the output, and the "rich get
richer" until the proper target is achieved. This favors self-organization of highly
localized spikes of activity in the exponential layers directly above the particular
character that the exponential layer detects ("smart histograms" as shown in Figure 1). Note that we never give positional information in the network but that the
network self-organizes the exponential unit activity to discern the positional information. The second function is the total-sum-square error, E = Ez(tz - pz)2. For
the total-sum-square error measure, the gradient term becomes

=

8E
uTJ~yz

-~-

=

(

tz -

pz

)

X~yz

~

(1 + L.~y X~yz

)2 .

(3)

Again this has a competitive term, but the competition is only important for X~yz
large, otherwise the denominator is dominated by 1 for small E~y X~yz. We used
the quadratic error function for the networks reported in the next section.

3
3.1

NIST DATABASE RECOGNITION
Data

We tested this neural network algorithm on the problem of segmenting and recognizing handwritten numerals from the NIST database. This database contains
approximately 273,000 samples of handwritten numerals collected from the Bureau
of Census field staff. There were 50 different forms used in the study, each with
33 fields, 28 of which contain handwritten numerals ranging in length from 2 to 10
digits per field. We only used fields of length 2 to 6 (field numbers 6 to 30). We
used two test sets: a small test set, Test Set A of approximately 4,000 digits, 1,000
fields, from forms labeled f1800 to f1840 and a larger test set, Test Set B, containing
20,000 numerals 5,000 fields and 200 forms from f1800 to f1899 and f2000 to f2199.
We used two different training sets: a hand-segmented training set containing approximately 33,000 digits from forms mooo to m636 (the Segmented Training Set)
and another training set that was never hand-segmented from forms mooo to f1800
(the Unsegmented Training Set. We pre-processed the fields with a simple boxremoval and size-normalization program before they were input to the ISR net.

499

500

Keeler and Rumelhart

The hand segmentation was conventional in the sense that boxes were drawn around
each of the characters, but we the boxes included any other portions of characters
that may be nearby or touching in the natural context. Note that precise labeling of
the characters is not essential at all. We have trained systems where only the center
information the characters was used and found no degradation in performance. This
is due to the fact that the system self-organizes the positional information, so it is
only required that we know whether a character is in a field, not precisely where.

3.2

TRAINING

We trained several nets on the NIST database. The best training procedure was
as follows: Step 1): train the network to an intermediate level of accuracy (96%
or so on single characters, about 12 epochs of training set 1). Note that when we
train on single characters, we do not need isolated characters - there are often
portions of other nearby characters within the input field. Indeed, it helps the ISR
performance to use this natural context. There are two reasons for this step: the
first is speed - training goes much faster with single characters because we can use a
small network. We also found a slight generalization accuracy benefit by including
this training step. Step 2): copy the weights of this small network into a larger
network and start training on 2 and 3 digit fields from the database without hand
segmentation. These are fields numbered 6,7,11,15,19,20,23,24,27, and 28. The
reason that we use these fields is that we do not have to hand-segment them - we
present the fields to the net with the answer that the person was supposed to write
in the field. (There were several cases where the person wrote the wrong numbers
or didn't write anything. These cases were NOT screened from the training set.)
Taking these fields from forms mooo to f1800 gives us another 45,000 characters to
train on without ever segmenting them.
There were several reasons that we use fields of length 2 and 3 and not fields of
4,5,or 6 for training (even though we used these in testing). First, 3 characters
covers the most general case: a character either has no characters on either side,
one to the left, one to the right or one on both sides (3 characters total). If we train
on 3 characters and duplicate the weights, we have covered the most general case for
any number of characters, and it is clearly faster to train on shorter fields. Second,
training with more characters confuses the net. As pointed out in our previous
work (keeler 1991a), the learning algorithm that we use is only valid for one or no
characters of a given type presented in the input field. Thus, the field '39541' is ok
to train on, but the field '288' violates one of the assumptions of the training rule.
In this case the two 8 's would be competing with each other for the answer and
the rule favors only one winner. Even though this problem occurs 1/lth of the
time for two digit fields, it is not serious enough to prevent the net from learning.
(Clearly it would not learn fields of length 10 where all of the target units are
turned on and there would be no chance for discrimination.) This problem could
be avoided by incorporating order information into training and we have proposed
several mechanisms for incorporating order information in training, but do not use
them in the present system. Note that this biases the training toward the a-priori
distribution of characters in the 2 and 3 digit fields, which is a different distribution
from that of the testing set.
The two networks that we used had the following architectures: Net1: Input: 28x24

A Self-Organizing Integrated Segmentation and Recognition Neural Net

receptive fields 6x6 shift 2x2. hidden 1: 12xllx12 receptive fields 4x4x12 shift
2x2x12. hidden 2: 5x4x18 receptive fields 3x3x18 shift lxlxl8. exponentials (block
3): 3x2xlO 10 summing, 10 outputs.
Net2: Input: 28x26 receptive fields 6x6 shift 2x4. hidden 1: 12x6x12 receptive
fields 5x4x12 shift lx2xl2. hidden 2: 8x2x18 receptive fields 5x2x18 shift lxlxl8.
exponentials (block 3): 4xlxlO 10 summing, 10 outputs.
100

0/0

c

99

0

98

r
r

97

e
c
t

A

n1&2

B

96

99. 5 t--+-+--t---:lhr-t:~rI-',

n2

95
94
93
92

91 . . . .______. ._ .....

o

5

10

15

20

25

3C

98 ----'--I-

,6

0/0 Rejected

97.5 .........._ ....~............_ ..............
5

10 15 20 25 30 35 40 45 50 55 60

Figure 2: Average combined network performance on the NIST database. Figure
2A shows the generalization performance of two neural networks on the NIST Test
Set A. The individual nets Netl and Net2 (nl, n2 respectively) and the combined
performance of nets 1 and 2 are shown where fields are rejected when the nets differ.
The curves show results for fields ranging length 2 to 6 averaged over all fields for
1,000 total fields, 4,000 characters. Note that Net2 is not nearly as accurate as Netl
on fields, but that the combination of the two is significantly better than either.
For this test set the rejection rate is 17% (83% acceptance) with an accuracy rate of
99.3% (error rate 0.7%) overall on fields of average length 4. Figure 2B shows the
per-field performance for test-set B (5,000 fields, 20,000 digits) Again both nets are
used for the rejection criterion. For comparison, 99% accuracy on fields of length 4
is achieved at 23% rejection.
Figure 2 shows the generalization performance on the NIST database for Netl, Net2
and their combination. For the combination, we accepted the answer only when the
networks agreed and rejected further based on a simple confidence measure (the
difference of the two highest activations) of each individual net.

501

502

Keeler and Rumelhart

../f.

.!

~./.~;,
I .I

1"'"

I

Figure 3: Examples of correctly recognized fields in the NIST database. This figure
shows examples of fields that were correctly recognized by the ISR network. Note
the cases of touching characters, multiple touching characters, characters touching
in multiple places, fields with extrinsic noise, broken characters and touching, broken
characters with noise. Because of the integrated nature of the segmentation and
recognition, the same system is able to handle all of these cases.

4

DISCUSSION AND CONCLUSIONS

This investigation has demonstrated that the ISR algorithm can be used for integrated segmentation and recognition and achieve high-accuracy results on a large
database of hand-printed numerals. The overall accuracy rates of 83% acceptance
with 99.3% accuracy on fields of average length 4 is competitive with accuracy reported in commercial products. One should be careful making such comparisons.
We found a variance of 7% or more in rejection performance on different test sets
with more than 1,000 fields (a good statistical sample). Perhaps more important
than the high accuracy, we have demonstrated that the ISR system is able to deal
with touching, broken and noisy characters. In other investigations we have demonstrated the ISR system on alphabetic characters with good results, and on speech
recognition (Keeler, Rumelhart, Zand-Biglari, 1991) where the results are slightly
better than Hidden Markov Model results.
There are several attractive aspects about the ISR algorithm: 1) Labeling can be
"sloppy" in the sense that the borders of the characters do not have to be defined.
This reduces the labor burden of getting a system running. 2) The final weights can
be duplicated so that the system can all run in parallel. Even with both networks
running, the number of weights and activations needed to be stored in memory is
quite small - about 30,000 floating point numbers, and the system is quite fast
in the feed-forward mode: peak performance is about 2.5 characters/sec on a Dec
5000 (including everything: both networks running, input pre-processing, parsing
the answers, printing results, etc.). This structure is ideal for VLSI implementation
since it contains a very small number of weights (about 5,000). This is one possible
way around the computational bottleneck facing encountered in processing complex
scenes - the ISR net can do very-fast first-cut scene analysis with good discrimi-

A Self-Organizing Integrated Segmentation and Recognition Neural Net

nation of similar objects - an extremely difficult task. 3) The ISR algorithm and
architecture presents a new and powerful approach of using forward models to convert position-independent training information into position-specific error signals.
4) There is no restriction to one-dimension; The same ISR structure has been used
for two-dimensional parsing.
Nevertheless, there are several aspects of the ISR net that require improvement for
future progress. First, the algorithmic assumption of having one pattern of a given
type in the input field is too restrictive and can cause confusion in some training
examples. Second, we are throwing some information away when we project out
all of the positional information order information could be incorporated into the
training information. This extra information should improve training performance
due to the more-specific error signals. Finally, normalization is still a problem.
We do a crude normalization, and the networks are able to segment and recognize
characters as long as the difference in size is not too large. A factor of two in
size difference is easily handled with the ISR system, but a factor of four decreases
recognition accuracy by about 3-5% on the character recognition rates. This requires a tighter coupling between the segmentation/recognition and normalization.
Just as one must segment and recognize simultaneously, in many cases one can't
properly normalize until segmentation/recognition has occurred. Fortunately, in
most document processing applications, crude normalization to within a factor of
two is simple to achieve, allowing high accuracy networks.
Acknowledgements

We thank Wee-Kheng Leow, Steve O'Hara, John Canfield, for useful discussions
and coding.
References

(1] J.D. Keeler, D.E. Rumelhart, and W.K. Leow (1991a) "Integrated Segmentation and Recognition of Hand-printed Numerals". In: Lippmann, Moody and
Touretzky, Editors, Neural Information Processing Systems 3, 557-563.
[2] J.D. Keeler, D.E. Rumelhart, and S. Zand-Biglari (1991b) "A Neural Network
For Integrated Segmentation and Recognition of Continuous Speech". MCC
Technical Report ACT-NN-359-91.
[3] K. Lang, A. Waibel, G. Hinton. (1990) A time delay Neural Network Architecture for Isolated Word Recognition. Neural Networks, 3 23-44.
[4] Y. Le Cun, B. Boser, J .S. Denker, S. Solla, R. Howard, and L. Jackel.
(1990) "Back-Propagation applied to Handwritten Zipcode Recognition." Neural Computation 1(4):541-551.
[5] G. Martin, J. Pittman (1990) "Recognizing hand-printed letters and digits."
In D. Touretzky (Ed.). Neural Information Processing Systems 2, 405-414,
Morgan Kauffman Publishers, San Mateo, CA.
[6] The NIST database can be obtained by writing to: Standard Reference Data
National Institute of Standards and Technology 221/ A323 Gaithersburg, MD
20899 USA and asking for NIST special database 1 (HWDB).

503


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 2029-hyperbolic-self-organizing-maps-for-semantic-navigation.pdf

Hyperbolic Self-Organizing Maps for Semantic
Navigation

J?org Ontrup
Neuroinformatics Group
Faculty of Technology
Bielefeld University
D-33501 Bielefeld, Germany
jontrup@techfak.uni-bielefeld.de

Helge Ritter
Neuroinformatics Group
Faculty of Technology
Bielefeld University
D-33501 Bielefeld, Germany
helge@techfak.uni-bielefeld.de

Abstract
We introduce a new type of Self-Organizing Map (SOM) to navigate
in the Semantic Space of large text collections. We propose a ?hyperbolic SOM? (HSOM) based on a regular tesselation of the hyperbolic
plane, which is a non-euclidean space characterized by constant negative
gaussian curvature. The exponentially increasing size of a neighborhood
around a point in hyperbolic space provides more freedom to map the
complex information space arising from language into spatial relations.
We describe experiments, showing that the HSOM can successfully be
applied to text categorization tasks and yields results comparable to other
state-of-the-art methods.

1 Introduction
For many tasks of exploraty data analysis the Self-Organizing Maps (SOM), as introduced
by Kohonen more than a decade ago, have become a widely used tool [1, 2]. So far, the
overwhelming majority of SOM approaches have taken it for granted to use a flat space
as their data model and, motivated by its convenience for visualization, have favored the
(suitably discretized) euclidean plane as their chief ?canvas? for the generated mappings.
However, even if our thinking is deeply entrenched with euclidean space, an obvious limiting factor is the rather restricted neighborhood that ?fits? around a point on a euclidean 2D
surface. Hyperbolic spaces in contrast offer an interesting loophole. They are characterized
by uniform negative curvature, resulting in a geometry such that the size of a neighborhood
around a point increases exponentially with its radius . This exponential scaling behavior
allows to create novel displays of large hierarchical structures that are particular accessible
to visual inspection [3, 4].
Consequently, we suggest to use hyperbolic spaces also in conjunction with the SOM. The
lattice structure of the resulting hyperbolic SOMs (HSOMs) is based on a tesselation of
the hyperbolic space (in 2D or 3D) and the lattice neighborhood reflects the hyperbolic
distance metric that is responsible for the non-intuitive properties of hyperbolic spaces.

After a brief introduction to the construction of hyperbolic spaces we describe several computer experiments that indicate that the HSOM offers new interesting perspectives in the
field of text-mining.

2 Hyperbolic Spaces
Hyperbolic and spherical spaces are the only non-euclidean geometries that are homogeneous and have isotropic distance metrics [5, 6]. The geometry of H2 is a standard topic in
Riemannian geometry (see, e.g. [7]), and the relationships for the area and the circumference of a circle of radius are given by



	

     	
 

(1)

These formulae exhibit the highly remarkable property that both quantities grow exponentially with the radius . It is this property that was observed in [3, 4] to make hyperbolic
spaces extremely useful for accommodating hierarchical structures.



 ! 

we must find suitable
To use this potential for the SOM, we must solve two problems:
discretization lattices on H2 to which we can ?attach? the SOM prototype vectors.
after having constructed the SOM, we must somehow project the (hyperbolic!) lattice into
?flat space? in order to be able to inspect the generated maps.
2.1 Projections of Hyperbolic Spaces

"

To construct an isometric (i.e., distance preserving) embedding of the hyperbolic plane into
a ?flat? space, we may use a Minkowski space [8]. In such a space, the squared distance
between two points
and
is given by

$# %&'&  #&( % ( ' ( 
"  )$#+*,# (  .-  % * % (   /*  ' * ' (  

(2)

i.e., it ceases to be positive definite. Still, this is a space with zero curvature and its somewhat peculiar distance measure allows to construct an isometric embedding of the hyperbolic plane H2, given by

#0

1 2 435 687 9% 
:$2  
:87 9'  365 ; 2 
(3)
$

2
7
where   are polar coordinates on the H2. Under this embedding,
plane
 )= - #  - the%  hyperbolic
about the ' -axis.
appears as the surface < swept out by rotating the curve '

From this embedding, we can construct two further ones, the so-called Klein model and the
M
Poincar?e model [5, 9] (we will use the latter to
u
visualize HSOMs below). Both achieve a proA
jection of the infinite H2 into the unit disk, how1
B
ever, at the price of distorting distances. The
N
Klein model is obtained by projecting the points
C
onto the plane
along rays passing
of
through
the
origin
(see
Fig.
1). Obviously,
D
O
1
this projects all points of
into the ?flat? unit
disk
of
. (e.g.,
).
S
The Poincar?e Model results if we add two further steps: first a perpendicular projection of
Figure 1: Construction steps underlying
the Klein Model onto the (?northern?) surface
Klein and Poincar?e-models of the space H2
of the unit sphere centered at the origin (e.g.,
), and then a stereographic projection of the ?northern? hemisphere onto the unit
circle about the origin in the ground plane
(point ). It turns out that the resulting projection of H2 has a number of pleasant properties, among them the preservation of

<

GHDF 

' >=
?
#  - % A@ = B C < 

' JI

K

EDF G

angles and the mapping of shortest paths onto circular arcs belonging to circles that intersect the unit disk at right angles. Distances in the original H2 are strongly distorted in its
Poincar?e (and also in the Klein) image (cf. Eq. (5)), however, in a rather useful way: the
mapping exhibits a strong ?fish-eye?-effect. The neighborhood of the H2 origin is mapped
almost faithfully (up to a linear shrinkage factor of 2), while more distant regions become
increasingly ?squeezed?. Since asymptotically the radial distances and the circumference
grow both according to the same exponential law, the squeezing is ?conformal?, i.e., (sufficiently small) shapes painted onto H2 are not deformed, only their size shrinks with increasing distance from the origin. By translating the original H2, the fish-eye-fovea can be
moved to any other part of H2, allowing to selectively zoom-in on interesting portions of a
map painted on H2 while still keeping a coarser view of its surrounding context.
2.2 Tesselations of the Hyperbolic Plane
To complete the set-up for a hyperbolic SOM we still need an equivalent of a regular grid in
the hyperbolic plane. For the hyperbolic plane there exist an infinite number of tesselations
with congruent polygons such that each grid point is surrounded by the same number of
neighbors [9, 10]. Fig. 2 shows two example tesselations (for the minimal value of
and for
), using the Poincar?e model for their visualization. While these tesselations
appear non-uniform, this is only due to the fish-eye effect of the Poincar?e projection. In the
original H2, each tesselation triangle has the same size.



)= I

One way to generate these tesselations algorithmically is by repeated application of a suitable set of generators of their symmetry group to a (suitably sized, cf. below) ?starting
triangle?, for more details cf. [11].


	 

Figure 2: Regular triangle tesselations of the hyperbolic plane, projected into the unit disk using
the Poincar?e mapping. The left tesselation shows the case where the minimal number (
)
of equilateral triangles meet at each vertex, the right figure was constructed with
. In the
Poincar?e projection, only sides passing through the origin appear straight, all other sides appear as
circular arcs, although in the original space all triangles are congruent.

3 Hyperbolic SOM Algorithm
We have now all ingredients required for a ?hyperbolic SOM?. We organize the nodes of
a lattice as described above in ?rings? around an origin node. The numbers of nodes of
such a lattice grows very rapidly (asymptotically exponentially) with the chosen lattice
radius (its number of rings). For instance, a lattice with
contains 1625
nodes. Each lattice node carries a prototype vector
from some -dimensional
feature space (if we wish to make any non-standard assumptions about the metric structure
of this space, we would build this into the distance metric that is used for determining the
best-match node). The SOM is then formed in the usual way, e.g., in on-line mode by

C

  C 

 

  B C
K

    in a radial
  
    
	   #+ *    
(4)
	







:



*














with
. However, since
work on a hyperbolic lattice,
"
   weandnow
  we
have to determine both the neighborhood 
the (squared) node distance "
repeatedly determining the winner node and adjusting all nodes
lattice neighborhood
around according to the familiar rule



according to the natural metric that is inherited by the hyperbolic lattice.

The simplest way to do this is to keep with each node a complex number to identify its
position in the Poincar?e model. The node distance is then given (using the Poincar?e model,
see e.g. [7]) as

   

"  arctanh  =  * *    
  



  

(5)



The neighborhood
can be defined as the subset of nodes within a certain graph
distance (which is chosen as a small multiple of the neighborhood radius ) around .

4 Experiments
Some introductory experiments where several examples illustrate the favorable properties
of the HSOM as compared to the ?standard? euclidean SOM can be found in [11, 12]. A
major example of the use of the SOM for text mining is the WEBSOM project [2].
4.1 Text Categorization
In order to apply the HSOM to natural text categorization, i.e. the assignment of natural language documents to a number of predefined categories, we follow the widely used
vector-space-model of Information Retrieval (IR). For each document we construct a fea, where the components are determined by the frequency of which term
ture vector
occurs in that document. Following standard practice [13] we choose a term frequency
inverse document frequency weighting scheme:

 " 




"



       ! #"%$'&        
(6)

"




where the term frequency   !  denotes the  number
of times term  occurs in ") ( ,  the




number of documents in the training set and "
the document frequency of  , i.e. the

number of documents occurs in.

 

The HSOM can be utilized for text categorization in the following manner. In a first step,
the training set is used to adapt the weight vectors
according to (4). During the second
step, the training set is mapped onto the HSOM lattice. To this end, for each training
example its best match node is determined such that

"(

     *   
 " (  +*

     *    
(7)
 " (  -,


where  ")(  denotes the feature vector of document "( , as described above. After all
examples have been presented to the net, each node is labelled with the union .  of all
categories that belonged to the documents that were mapped to this node. A new, unknown
text is then classified into the union .  of categories which are associated with its winner
node selected in the HSOM.

Text Collection. We used the Reuters-215781 data set since it provides a well known
baseline which is also used by other authors to evaluate their approaches, c.f. [14, 15]. We

/

1
As compiled by David Lewis from the AT&T Research Lab in 1987. The data can be found at
http://www.research.att.com/ lewis/

have used the ?ModApte? split, leading to 9603 training and 3299 test documents. After
preprocessing, our training set contained 5561 distinct terms.

 

 C  

 C 

Performance Evaluation. The classification effectiveness is commonly measured
in terms

of precision
and recall
[16], which can be estimated as
 	  


are the numbers of documents correctly classified, and

 	
 where  and 
are the
correctly not classified to category  , respectively. Analogous,  and 
corresponding numbers of falsely classified documents.









 



 

For each node and each category  a confidence value
is determined. It describes the
number of training documents belonging to class  which were mapped to node . When
retrieving documents from a given category  , we compare 
for each node its associated
against a threshold  . Documents from nodes with
 become then included
into the retrieval set. For nodes which contain a set of documents
, the order of the
, where
.
retrieval set is ranked by 

 



 
   " (   K  

$   " (     

K  

In this way the number of retrieved documents can be controlled and we obtain the
precision-recall-diagrams as shown in Fig. 3.
In
 order to compare the
 HSOM?s performance for text categorization, we also evaluated a
-nearest neighbor ( -NN) classifier with our training set. Apart from boosting methods
[16] only  support vector machines [14] have shown better performances. The confidence
level of a -NN classifier to assign document to class  is

   " (  

)(   $  " (  " ( 
(8)
 
! #"%$'&
 (
(
documents " for which
  $ " (  "  is maximum. The assign-

-NN

  " ( 

 I"





"(



where
)( is the set of
(

ment factor
is 1,+if*
belongs to category  and 0 otherwise. According to [14, 17] we
&
nearest neighbors.
have chosen the

C )



Text Categorization Results. The results of three experiments are shown
-, in Table 1. We
have compared a HSOM with
rings and a tesselation with
neighbors (summing up to 1306 nodes) to a  spherical standard euclidean SOM as described in [11] with
approx. 1300 nodes, and
 the -NN classifier. Our results indicate that the HSOM does not
perform better than a -NN classifier, but to a certain extent also does not play significantly
worse either. It is noticable that for less dominant categories the HSOM yields superior
results to those of the standard SOM. This is due to the fact, that the nodes in H2 cover
a much broader space and therefore offer more freedom to map smaller portions of the
original dataspace with less distortions as compared to euclidean space.



As the -NN results suggest, other state-of-the-art techniques like support vector machines
will probably lead to better numerical categorization results than the HSOM. However,
since the main purpose of the HSOM is the visualization of relationships between texts
and text categories, we believe that the observed categorization performance of the HSOM
compares sufficiently well with the more specialized (non-visualization) techniques to warrant its efficient use for creating insightful maps of large bodies of document data.
Table 1: Precision-recall breakeven points for the ten most prominent categories.

SOM
HSOM
.
-NN

earn
90.0
90.2
93.8

acq
81.2
81.6
83.7

mny-fx
61.7
68.7
69.3

crude
70.3
78.8
84.7

grain
69.4
76.2
81.9

trade
48.8
56.8
61.9

interest
57.1
66.4
71.0

wheat
61.9
69.3
69.0

ship
54.8
61.8
77.5

corn
50.3
53.6
67.9

1

1

0.9

0.9

0.8

0.8
0.7

0.7

earn
acq
money?fx

0.6

0.6

0.4
0

0.2

0.4

0.6

0.8

0.4

1

0

0.2

.

(a) -NN

0.4

0.6

0.8

1

(b) HSOM
2: 0.69

Figure 3: Precision-recall curves for the three most frequent categories earn, acq and money-fx.

4.2 Text Mining & Semantic Navigation
A major advantage of the HSOM is its remarkable capability to map high-dimensional
similarity relationships to a low-dimensional space which can be more easily handled and
interpreted by the human observer. This feature and the particular ?fish-eye? capability motivates our approach to visualize whole text collections with the HSOM. It can be regarded
as an interface capturing the semantic structure of a text database and provides a way to
guide the users attention. In preliminary experiments we have labelled the nodes with
glyphs corresponding to the categories of the documents mapped to that node. In Fig. 4
two HSOM views of the Reuters data set are shown. Note, that the major amount of data
gets mapped to the outermost region, where the nodes of the HSOM make use of the large
space offered by the hyperbolic geometry. During the unsupervised training process, the
document?s categories were not presented to the HSOM. Nevertheless, several document
clusters can be clearly identified. The two most prominent are the earn and acquisition
region of the map, reflecting the large proportion of these categories in the Reuters-21578
collection. Note, that categories which are semantically similar are located beside each
other, as can be seen in the corn, wheat, grain the interest, money-fx or the crude, ship area
of the map. Additional to the category (glyph type) and the number of training documents
per node (glyph size), the number of test documents mapped to each node is shown as the
height of the symbol above the ground plane. In this way the HSOM can be used as a
novelty detector in chronological document streams. For the Reuters-21578 dataset, a particular node strikes out. It corresponds to the small glyph tagged with the ?ship? label in
Fig. 4. Only a few documents from the training collection are mapped to that node as shown
by it?s relatively small glyph size. The large -value on the other hand indicates that it contains a large number of test documents, and is therefore probably semantically connected
to a significant, novel event only contained in the test collection. The right image of Fig. 4
shows the same map, but the focal view now moved into the direction of the conspicious
?ship? node, resulting in a magnification of the corresponding area. A closer inspection reveals, that the vast majority (35 of 40) of the test documents describe an incident where an
Iranian oil rig was attacked in the gulf. Although no document of the training set describes
this incident (because the text collection is ordered by time and the attack took place ?after?
the split into train and test set), the HSOM generalizes well and maps the semantic content
of these documents to the proper area of the map, located between the regions for crude
and ship.



The next example illustrates that the HSOM can provide more information about an unknown text than just it?s category. For this experiment we have taken movie reviews from
the rec.art.movies.reviews newsgroup. Since all the reviews describe a certain movie, we
retrieved their associated genres from the Internet Movie Database (http://www.imdb.com)
to build a set of category labels for each document. The training set contained 8923 ran-

money?fx

ship

trade

corn wheat

grain

interest
acq

crude

earn

 



Figure 4: The left figure shows a central view of the Reuters data. We used a HSOM with
rings and a tesselation with
neighbors. Ten different glyphs were used to visualize the ten most
frequent categories. They were manually tagged to indicate the correspondence between category
and symbol type. The glyph sizes and the -values (height above ground plane) reflect the number of
training and test documents mapped to the corresponding node, respectively.



domly selected reviews (without their genre information) from films released before 2000.
We then presented the system with five reviews from the film ?Atlantis?, a Disney cartoon
released in 2001. The HSOM correctly classified all of the five texts as reviews for an animation movie. In Fig. 5 the projection of the five new documents onto the map with the
previously acquired text collection is shown. It can be seen that there exist several clusters
related to the animation genre. By moving the fovea of the HSOM we can now ?zoom?
into that region which contains the five new texts. In the right of Fig. 5 it can be seen
that all of the ?Atlantis? reviews where mapped to a node in immediate vicinity of documents describing other Disney animation movies. This example motivates the approach of
?semantic navigation? to rapidly visualize the linkage between unknown documents and
previously acquired semantic concepts.
Mulan
Beauty and
the beast Anastasia
Pocahontas
Hercules
Aladin
Atlantis

Tarzan
Chicken Run
Dinosaur

South Park
Tarzan
Mulan
The Iron Giant

Antz
A Bug?s Life
The Prince
of Egypt



   

Figure 5: A HSOM with
and a tesselation with
neighbors was used to map movie
rewies from newsgroup channels. In both figures, glyph size and -value indicate the number of
texts related to the animation genre mapped to the corresponding node. Nodes exceeding a certain
threshold were labelled with the title corresponding to the most frequently occuring movie mapped
to that node. The underlined label in the right figure indicates the position of the node to which five
new documents were mapped to.

5 Conclusion
Efficient navigation in ?Sematic Space? requires to address two challenges: (i) how to create a low dimensional display of semantic relationship of documents, and (ii) how to obtain
these relationships by automated text categorization. Our results show that the HSOM can
provide a good solution to both demands simultaneously and within a single framework.

The HSOM is able to exploit the peculiar geometric properties of hyperbolic space to successfully compress complex semantic relationships between text documents. Additionally,
the use of hyperbolic lattice topology for the arrangement of the HSOM nodes offers new
and attractive features for interactive ?semantic navigation?. Large document databases
can be inspected at a glance while the HSOM provides additional information which was
captured during a previous training step, allowing e.g. to rapidly visualize relationships
between new documents and previously acquired collections.
Future work will address more sophisticated visualization strategies based on the new approach, as well as the exploration of other text representations which might take advantage
of hyperbolic space properties.

References
[1] T. Kohonen. Self-Organizing Maps. Springer Series in Information Sciences. 3rd edition, 2001.
[2] Teuvo Kohonen, Samuel Kaski, Krista Lagus, Jarkko Saloj?arvi, Vesa Paatero, and Antti Saarela.
Organization of a massive document collection. IEEE Transactions on Neural Networks, Special Issue on Neural Networks for Data Mining and Knowledge Discovery, 11(3):574?585, May
2000.
[3] John Lamping and Ramana Rao. Laying out and visualizing large trees using a hyperbolic
space. In Proceedings of UIST?94, pages 13?14, 1994.
[4] T. Munzer. Exploring large graphs in 3D hyperbolic space. IEEE Computer Graphics and
Applications, 18(4):18?23, July/August 1998.
[5] H. S. M. Coxeter. Non Euclidean Geometry. Univ. of Toronto Press, Toronto, 1957.
[6] J.A. Thorpe. Elementary Topics in Differential Geometry. Springer-Verlag, New York, 1979.
[7] Frank Morgan. Riemannian Geometry: A Beginner?s Guide. Jones and Bartlett Publishers,
Boston, London, 1993.
[8] Charles W. Misner, J. A. Wheeler, and Kip S. Thorne. Gravitation. Freeman, 1973.
[9] R. Fricke and F. Klein. Vorlesungen u? ber die Theorie der automorphen Funktionen, volume 1.
Teubner, Leipzig, 1897. Reprinted by Johnson Reprint, New York, 1965.
[10] W. Magnus. Noneuclidean Tesselations and Their Groups. Academic Press, 1974.
[11] Helge Ritter. Self-organizing maps in non-euclidian spaces. In E. Oja and S. Kaski, editors,
Kohonen Maps, pages 97?108. Amer Elsevier, 1999.
[12] J. Ontrup and H. Ritter. Text categorization and semantic browsing with self-organizing maps
on non-euclidean spaces. In Proc. of the PKDD-01, 2001.
[13] G. Salton and C. Buckley. Term-weighting approaches in automatic text retrieval. Information
Processing and Management, 24(5):513?523, 1988.
[14] T. Joachims. Text categorization with support vector machines: learning with many relevant
features. In Proc. of ECML-98, number 1398, pages 137?142, Chemnitz, DE, 1998.
[15] Huma Lodhi, John Shawe-Taylor, Nello Cristianini, and Chris Watkins. Text classification using
string kernels. In Todd K. Leen, Thomas G. Dietterich, and Volker Tresp, editors, Advances in
Neural Information Processing Systems 13, pages 563?569. MIT Press, 2001.
[16] F. Sebastiani, A. Sperduti, and N. Valdambrini. An improved boosting algorithm and its application to automated text categorization. In Proc. of CIKM-00, pages 78?85, 2000.
[17] Y. Yang. An evaluation of statistical approaches to text categorization. Information Retrieval,
1-2(1):69?90, 1999.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 248-the-effects-of-circuit-integration-on-a-feature-map-vector-quantizer.pdf

226

Mann

The Effects of Circuit Integration on a Feature
Map Vector Quantizer

Jim lVIann
MIT Lincoln Laboratory
244 Wood St.
Lexington, ~IA 02173
email: mann@vlsi.ll.mit.edu

ABSTRACT
The effects of parameter modifications imposed by hardware constraints on a self-organizing feature map algorithm were examined.
Performance was measured by the error rate of a speech recognition system which included this algorithm as part of the front-end
processing. System parameters which were varied included weight
(connection strength) quantization, adap tation quantization, distance measures and circuit approximations which include device
characteristics and process variability. Experiments using the TI
isolated word database for 16 speakers demonstrated degradation in
performance when weight quantization fell below 8 bits. The competitive nature of the algorithm rela..xes constraints on uniformity
and linearity which makes it an excellent candidate for a fully analog circuit implementation. Prototype circuits have been fabricated
and characterized following the constraints established through the
simulation efforts.

1

Introduction

The self-organizing feature map algorithm developed by Kohonen [Kohonen, 1988]
readily lends itself to the task of vector quantization for use in such areas as speech
recognition. However, in considering practical imp lementations, it is necessary to

The Effects of Circuit Integration on a Feature Map Vector Quantizer
100

,,

90
80 -

,,

EUCLIDEAN

,,

,0

o?

W

70

<
ex:

60

~

ex:
0
ex:
ex:
w
0

,

? - - - - - Dor PRODUCT

,
\
\

\
\
\
\

50

\

\
\

40

ex:

0

~

30
20

,,
,,
,,
,,
,

,,
..

10
0

0

1

2 3

4

5

6

7

. -8

9 10 11 12 13

NUMBER OF WEIGHT BITS

Figure 1: Recognition performance of the Euclidean and dot product activity
calculators plotted as a function of weight precision .
understand the limitations imposed by circuitry on algorithm performance. In order
to test the effects of these constraints on overall performance a simulation was
written which permits ready variation of critical system parameters.
The feature map algorithm was placed in the frontend of a discrete hidden Ylarkov
model (H111'I) speech recognition program as the vector quantizer (VQ) in order to
track the effects of feature map algorithm modifications by monitoring overall word
recognition accuracy. The system was tested on TI's 20 isolated word database
consisting of 16 speakers . Each speaker had 1 training session consisting of 10 repetitions of each word in the vocabulary and 8 test sessions consisting of 2 repetitions
of each word.
The key parameters tested include; quantization of both the weight coefficients and
learning rule, and several different activation computations, the dot product and
the mean squared error (i.e. squared Euclidean distance), as well as the circuit
approximations to these calculators.

2

Results

A unique dependency between weight quantization and distance measure emerged
from the simulations and is illustrated in the graph presented in Figure 1. The
network equipped with the mean squared error activity calculator shows a "knee"
in the word error rate at 6 bits of precision in the weight representation. The overall
performance dropped only slightly between the essentially ideal floating point case,
at 1.45% error rate, and the 6 bit case, at 2.99% error rate. At 4 bits, the error rate
climbs to 7.62%. This still corresponds to a recognition accuracy of better than
92% but does show a marked degradation in performance.

227

228

Mann

-./

?

-./

-L
Wi/

I--

Xn

Wi-I.;

-./-V

?
...L

I--

Xn_1

wo.;

I--

Xo

Figure 2: .-\ circuit approximation to the dot product calculator.

The dot product does not degrade as gracefully with reduced precision in the weight
representation as the mean squared error activity calculation. This is due to the
normalization required on the input, and subsequently the weight vectors, which
compresses the space onto the unit hypersphere. This step is necessary because of
the inherent sensitivity of this metric to vector magnitude in making decisions of
relative distance. Here the" knee" in the error curve occurs at 8 bits. Below 8 bits,
performance drops off dramatically, reaching 40.6% error rate at 6 bits. The double
precision floating point case starts off at 1.68% and is 3.44% at 8 bits .
Circuit approximations to these activity calculators were also included in the simulations. An approximation to the dot product operation can be implemented with
single transistors operating in the ohmic region at each connection as illustrated in
Figure 2.
These area. related considerations can often overshadow the performance penalties
associated with their implementation. The simulation results from this circuit approximation match the performance of the digital calculation of the dot product
almost exactly as seen in Figure 3. This indicates that the performance of the
system depends more on the monotonicity of the product operation performed at
each connection then its linearity.
Effects of process variations on transistor thresholds were also examined. There
appears to be a gradual decrease in system performance with increasing variability
in transistor thresholds as seen in Figure 4. The cause of this phenomena remains
to be investigated .
A weight adjustment rule which simplifies circuitry consists of quantizing the learning rate gain term. An integer step is added to or subtracted from the weight
depending on the magnitude of the difference between it and the input. In the

The Effects of Circuit Integration on a Feature Map Vector Quantizer
100 ~_.~
_________ ~

.~.

90

-

80

W

70

,0

,

"""

~

o'

,

~

",
\

\

~

I

~

<

a::
a::
0
a::
a::

KOHONEN LEARNING RULE

\

?I

60
50

.,,

0

40

':,

0

30

w

a::
~

INCi
DEC LEARNING RULE
.........................................

t

t

DOT
PRODUCT

KO.~f'!~ .LE.4:.R~l~G ~UL.e. ~TRANSISTER
INC:DEC LEARNING RULE

CIRCUIT

20
10
0~--~~~~~~~~======3

o

I

2

3 4 5 6 7
8 9 10 11 12 13
NUMBER OF WEIGHT BITS

Figure 3: Similarity between the transistor circuit simulation and the digital calculation of the dot product

100
90 -

-

80

w
~
<

70

,0

~

a::
a::
0
a::
a::
w

60
50

a

40

0

30

a::
~

20
10
0

0

10

20

30

40

50

60

70

80

90 100

STD. DEV. (mV)

Figure 4: The effects of transistor threshold variation on recognition performance.
(8 bit weight; Gaussian distributed, mean(Vth) = 0.75 volts).

229

230

Mann
2.0

I

1.8 I-

-

1.6 I-

-

1.4

r-

0

1.2

r-

w

1.0 l-

, 0

o?

a::
a::
a::

e

a:

0

~

I

0.8

r-

--------

-

-

0.6 r-

-

0.4 I-

-

r-

-

0.2

I

2

4

I

I

6

8

MAX. WEIGHT ADJUST (+I-)

DBl
PRECISION

Figure 5: 'Nord recognition error rate as a function of learning rate gain quantization.
simplest case . a fixed increment or decrement operation is performed based only
upon the sign of the difference between the two terms. Even in this simplest case
no degradation in performance was noted while using an 8 bit weight representation as demonstrated in the graph shown in Figure 5. In fact, performance was
often improved over the original learning rule. The error rates using an increment/decrement learning rule with 8 weight bits was O.9i% and 2.0% for the mean
squared error and the dot product, respectively.
An additional learning rule is being tested, targeted at a floating gate implementation which uses a "flash" EPROM memory structure at each synapse. Weight
changes are restricted to positive adjustments locally while all negative adjustments
are made globally to all weights. This corresponds to a forgetting term, or constant
weight decay, in the learning rule. This rule was chosen to be compatible with one
technique in non-volatile charge storage which allows selective write but only block
erase.

3

Hardware

A prototype synaptic array and weight adaptation circuit have been designed and
fabricated [Mann, 1989]. A single transistor synapse computes its contribution to
the dot product activity calculation. The weight is stored dynamically as charge on
the gate of the synapse transistor. The input is represented as a voltage on the drain
of the transistor. The current through the transistor is proportional to the product
of the gate voltage (i.e. the weight) and the drain voltage (i.e. the input strength)
with the source connected to a virtual ground (see Figure 2). The sources of several
of these synapse connected together form the accumulation needed to realize the dot
product. Circuitry for accessing stored weight information has also been included.

The Effects of Circuit Integration on a Feature Map Vector Quantizer

The synapse array works as expected except for circuitry used to read the weight
contents. This circuit requires very high on-chip voltages causing other circuits to
latch-up when the clocks are turned on .
The weight adaptation circuit performs the simple increment/decrement operation
based on the comparison between the input and weight magnitudes. Both quantities are first converted to a digital representation by a flash A/D converter before
comparison. This circuit also performs the required refresh operation on weight.
contents, much like that required for dynamic RAM's but requiring analog charge
storage. This insures that weight drift is constrained to lie within boundaries defined
by the precision of the weight representation determined by the A/D con version process. This circuit was functional in the refresh and increment modes, but would not
decrement correctly.
Further tests are being conducted to establish the causes of the circuit problems
detected thus far. Additional work is proceeding on a non-volatile charge storage
version of this device. Some test structures have been fabricated and are currently
being characterized for compatibility with this task.
This work was supported by the Department of the Air Force.
References
T. Kohonen. (1988) Self-Organization and Associative Memory, Berlin: SpringerVerlag.

J. Mann & S. Gilbert. (1989) An Analog Self-Organizing Neural Network Chip .
In D. S. Touretzky (ed.), Advances in Neural Information Processing Systems 1,
739-747. San Mateo, CA: Morgan Kaufmann.

231


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 251-a-self-organizing-associative-memory-system-for-control-applications.pdf

332

Hormel

A Sell-organizing Associative
Memory System lor Control
Applications

Michael Bormel
Department of Control Theory and Robotics
Technical University of Darmstadt
Schlossgraben 1
6100 Darmstadt/W.-Ger.any

ABSTRACT
The CHAC storage scheme has been used as a basis
for a software implementation of an associative
.emory system AHS, which itself is a major part
of the learning control loop LERNAS. A major
disadvantage of this CHAC-concept is that the
degree of local generalization (area of interpolation) is fixed. This paper deals with an algorithm for self-organizing variable generalization for the AKS, based on ideas of T. Kohonen.

1 INTRODUCTION
For several years research at the Department of Control Theory and Robotics at the Technical University of Darmstadt
has been concerned with the design of a learning real-time
control loop with neuron-like associative memories (LERNAS)

A Self-organizing Associative Memory System for Control Applications

for the control of unknown, nonlinear processes (Ersue,
Tolle, 1988). This control concept uses an associative memory system AHS, based on the cerebellar cortex model CHAC by
Albus (Albus, 1972), for the storage of a predictive nonlinear process model and an appropriate nonlinear control strategy (Fig. 1).
e&;ected process response

I>.

o
o
planne~

~
.

control inputs

I"-

-

-

~~
-

red setpoint

-...
-?
-.-....
co

predictive
process .ode!"
IF

C

c

?
>

opti.iud
control input

-

::

eValuation/
opti.ization

actual/past
process infor.ation

f

1----

~~

control strate y

- -

actual control input
1-

-

r:;.:>

u

- -

-

-

I-

unknown process

..?

I>.

c:-

--..
..
o

I ~

o

t}

{J

o

c
o

u

short ter.
.e.ory

process infor.ation

..?
E

-?.
I

?

I

laSSOCialive lIe.ory syste.

'\~IS

Figure 1: The learning control loop LERNAS
One problem for adjusting the control loop to a process is,
however, to find a suitable set of parameters for the associative memory. The parameters in question determine the
degree of generalization within the memory and therefore
have a direct influence on the number of training steps required to learn the process behaviour. For a good performance of the control loop it ? is desirable to have a very
small generalization around a given setpoint but to have a
large generalization elsewhere. Actually, the amount of collected data is small during the transition phase between two

333

334

Hormel

setpoints but is large during setpoint control. Therefore a
self-organizing variable generalization, adapting itself to
the amount of available data would be very advantageous.
Up to now, when working with fixed generalization, finding
the right parameters has meant to find the best compromise
between performance and learning time required to generate a
process model. This paper will show a possibility to introduce a self-organizing variable generalization capability
into the existing AMS/CMAC algorithm.

2 THE AMS-CONCEPT
The associative memory syste. AMS is based on the "Cerebellar Model Articulation Controller CMAC" as presented by J.S.
Albus. The information processing structure of AMS can be
divided into three stages.
1.) Each component of a n-dimensional input vector (stimulus) activates a fixed number p of sensory cells, the
receptive fields of which are overlapping. So n?p sensory cells become active.
2.) The active sensory cells are grouped to form p n-dimensional vectors. These vectors are mapped to p association cells. The merged receptive fields of the sensory
cells described by one vector can be seen as a hypercube
in the n-dimensional input space and therefore as the
receptive field of the association cell. In normal applications the total number of available association
cells is about 100?p.
3.) The association cells are connected to the output cells
by modifiable synaptic weights. The output cell computes
the mean value of all weights that are connected to active association cells (active weights).
Figure 2 shows the basic principle of the associative memory
system AMS.

A Self-organizing Associative Memory System for Control Applications

output value
input space
adjustable weights

Figure 2: The basic aechanism of AMS
During training the generated output is compared with a desired output, the error is computed and equally distributed
over all active weights. For the mapping of sensory cells to
association cells a hash-coding mechanism is used.

3 THE SELF-ORGANIZING FEATURE MAP
An approach for explaining the self-organizing capabilities
of the nervous system has been presented by T. Kohonen (Kohonen, 1988).
In his "self-organizing feature mapft a network of laterally
interconnected neurons can adapt itself according to the
density of trained points in the input space. Presenting a
n-diaensional input vector to the network causes every neuron to produce an output signal which is correlated with the
similarity between the input vector and a "template vector"
which may be stored in the synaptic weights of the neuron.
Due to the "mexican-hat" coupling function between the neurons, the one with the maximum output activity will excite
its nearest neighbours but will inhibit neurons farther away, therefore generating a localized response in the network. The active cells can now adapt their input weights in
order to increase their similarity to the input vector. If
we define the receptive field of a neuron by the number of
input vectors for which the neurons activity is greater than

335

336

Hormel

that of any other neuron in the net, this yields the effect
that in areas with a high density of trained points the receptive fields become small whereas in areas with a low density of trained points the size of the receptive fields is
large. Is mentioned above this is a desired effect when
workin; with a learning control loop.

4 SELF-ORGANIZING VARIABLE GENERALIZATION
Both of the approaches above have several advantages and
disadvantages when using them for real-time control applications.
In the AKS algorithm one does not have to care for predefining a network and the coupling functions or coupling matrices among the elements of the network. Association and
weight cells are generated when they are needed during
training and can be adressed very quietly to produce a memory response. One of the disadvantages is the fixed generalization once the parameters of a .eaory unit have been
chosen.
Unlike AHS, the feature map allows the adaption of the network according to the input data. This advantage has to be
payed for by extensive search for the best matching neuron
in the network and therefore the response time of the network aay be too large for real-tiae control when working
with big networks.
These problems can be overcome when allowing that the mapping of sensory cells to association cells in AKS is no
longer fixed but can be changed during training.
To accomplish this a template vector t is introduced for
every association cell. This vector i serves as an indicator
for the stimuli by which the association cell has been accessed previously. During an associative recall for a stimulus !o a preliminary set of p association cells is activated
by the hash coding mechanism. Due to the self-organizing
process during training the template vectors do not need to
correspond to the input vector !o. For the search for the

A Self-organizing Associative Memory System for Control Applications

best aatching cell the template vector 10 of the accessed
association cell is compared to the stiaulus and a difference vector is calculated.
6.

,

n

=."
t.

i = O, ??? ,n

- L.v

(1)

s

number of searching steps

s

This vector can now be used to compute a virtual stimulus
which compensates the mapping errors of the hash-coding
mechanism.

~+1 = ~ -

-4

i=O, ??? ,n

(2)

s

The best matching cell is found for
j =

i

ain
II '1.
6. "
.

= O, ??? ,ns

(3)

1

and can be adressed by the virtual stimulus ~j when using
the hash coding mechanism. This search mechanism ensures
that the best matching cell is found even if self organization is in effect.
During training the template
cells are updated by
t(t+l)
d

= a(k,d) ?(!(k)

-let?~

vectors of

the

association

+ t(k)

(4)

lateral distance of neurons in the network

where t(k) denotes the value of the teaplate vector at time
k and ~(k) denotes the stimulus. a(t,d) is a monotonic decreasing function of time and the lateral distance between
neurons in the network.

6 SIMULATION RESULTS
Figure 3 and 4 show some simulation results of the presented
algorithm for the dase of a two dimensional stimulus vector.

337

338

Hormel

Figure J shows the expected positions in input space of the
untrained template vectors ( x denotes untrained association
cells).
? ? ? ?
? ? ? ?
? ? ? ?
? ? ? ?
? ? ? ?

? ? ?
? ? ?
? ? ?
? ? ?
? ? ?

? ? ? ? ?
? ? ? ? ?
? ? ? ? ?
? ? ? ? ?
? ? ? ? ?

? ?
? ?
? ?
? ?

?

?
?
?
? ? ?

?? .? .? .? .? .? .? .? .? .? . . .? .? .?
?? .? .? .? .? .? .? .? .? .? . . .. .. ..
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?

?? .? .? .? .? .? .? .? .? .? .? .? .? .? .?
? ? ? ? ? ? ? ? ? ? ? ? ? ? ?

Figure J: Untrained .etwork
Figure 4 shows the network after 2000 training steps with
stimuli of gaussian distribution in input space. The position of the template vectors of trained cells has shifted
into the direction of the better trained areas, so that more
association cells are used to represent this area than before. Therefore the stored information will be more exact in
this area.
? ? ? ?
? ? ? ?
? ? ? ?
? ? ? ?
? ? ?
? ? ? ?
? ?
? ?
? ?
? ? ?
?

??
?

?

? ? ? ? ? ? ? ? ?
? ? ? ? ? ? ? ? ?
? ?
?
? ? ? ?
?
?

? ? ?

? ?

?

? ?

?

?
?

?
?
?
???
? ? ? ? ? ? ?
? ?

? ?
?
? ?
? ? ? ? ? ? ?
? ? ? ? ? ? ? ? ?

?

? ?
? ?
? ?
? ?

?? .. .. .. ..

Figure 4: Network after 2000 training steps

A Self-organizing Associative Memory System for Control Applications

6 CONCLUSION
The ney algorithm presented above introduces the capability
to adapt the storage mechanisms of a CMAC-type associative
memory according to the arriving stimuli. This will result
in various degrees of generalization depending on the number
of trained points in a given area. It therefore will make it
unnecessary to choose a generalization factor as a compromise between several constraints when representing nonlinear
functions by storing them in this type of associative memory. Some results on tests will be presented together with a
comparison on respective results for the original AMS.
Acknowledgements
This work was sponsored by the German !inistry for
and Technology (BMFT) under grant no. ITR 8800 B/5

Research

References

E.

Ersue, H. Tolle. (1988) Learning Control Structures with
Neuron-Like Associative memories. In: v. Seelen, Shaw, Leinhos (Eds.) Organization of Neural Networks, VCH Verlagsgesellschaft, Weinheim, FRG, 1988
J.S. llbu~ (1972) Theoretical and experimental aspects of a
cerebellar model, PhD thesis, University of Maryland, USA

E. Ersue, X. Mao (1983) Control of pH by Use of a Self-organizing Concept with Associative Memories. ACI'83, Kopenhagen, Denmark
E. Ersue, J. Militzel (1984) Real-tiae Implementation of an
Associative Memory-based Learning Control Scheme for Nonlin-ear Jfultivariable Systems. SymposiuDl on "Applications of
Multivariable System Techniques", Plymouth, UK
T. Kohonen. (1988) Self-Organization and Associative Memory,
2nd Ed., Springer Verlag

339


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 936-sardnet-a-self-organizing-feature-map-for-sequences.pdf

SARDNET: A Self-Organizing Feature
Map for Sequences

Daniel L. James and Risto Miikkulainen
Department of Computer Sciences
The University of Texas at Austin
Austin, TX 78712
dljames,risto~cs.utexas.edu

Abstract
A self-organizing neural network for sequence classification called
SARDNET is described and analyzed experimentally. SARDNET
extends the Kohonen Feature Map architecture with activation retention and decay in order to create unique distributed response
patterns for different sequences. SARDNET yields extremely dense
yet descriptive representations of sequential input in very few training iterations. The network has proven successful on mapping arbitrary sequences of binary and real numbers, as well as phonemic
representations of English words. Potential applications include
isolated spoken word recognition and cognitive science models of
sequence processing.

1

INTRODUCTION

While neural networks have proved a good tool for processing static patterns, classifying sequential information has remained a challenging task. The problem involves
recognizing patterns in a time series of vectors, which requires forming a good internal representation for the sequences. Several researchers have proposed extending
the self-organizing feature map (Kohonen 1989, 1990), a highly successful static
pattern classification method, to sequential information (Kangas 1991; Samarabandu and Jakubowicz 1990; Scholtes 1991). Below, three of the most recent of
these networks are briefly described. The remainder of the paper focuses on a new
architecture designed to overcome the shortcomings of these approaches.

578

Daniel L. James, Risto Miikkulainen

Recently, Chappel and Taylor (1993) proposed the Temporal Kohonen Map (TKM)
architecture for classifying sequences. The TKM keeps track of the activation history of each node by updating a value called leaky integrator potential, inspired by
the membrane potential in biological neural systems. The activity of a node depends
both on the current input vector and the previous input vectors, represented by the
node's potential. A given sequence is processed by mapping one vector at a time,
and the last winning node serves to represent the entire sequence. This way, there
needs to be a separate node for every possible sequence, which is a disadvantage
when the number of sequences to be classified is large. The TKM also suffers from
loss of context. Which node wins depends almost entirely upon the most recent
input vectors. For example, the string baaaa would most likely map to the same
node as aaaaa, making the approach applicable only to short sequences.
The SOFM-S network proposed by van Harmelen (1993) extends TKM such that
the activity of each map node depends on the current input vector and the past
activation of all map nodes. The SOFM-S is an improvement of TKM in that contextual information is not lost as quickly, but it still uses a single node to represent
a sequence.
The TRACE feature map (Zandhuis 1992) has two feature map layers. The first
layer is a topological map of the individual input vectors , and is used to generate
a trace (i.e. path) of the input sequence on the map. The second layer then maps
the trace pattern to a single node . In TRACE, the sequences are represented by
distributed patterns on the first layer, potentially allowing for larger capacity, but
it is difficult to encode sequences where the same vectors repeat, such as baaaa. All
a-vectors would be mapped on the same unit in the first layer, and any number of
a-vectors would be indistinguishable.
The architecture described in this paper, SARDNET (Sequential Activation Retention and Decay NETwork), also uses a subset of map nodes to represent the
sequence of vectors. Such a distributed approach allows a large number of representations be "packed" into a small map-like sardines. In the following sections,
we will examine how SARDNET differs from conventional self-organizing maps and
how it can be used to represent and classify a large number of complex sequences.

2

THE SARDNET ARCHITECTURE

Input to SARDNET consists of a sequence of n-dimensional vectors S =
V I, V 2 , V 3 , ... , VI (figure 1) . The components of each vector are real values in
the interval [0,1]. For example, each vector might represent a sample of a speech
signal in n different frequencies, and the entire sequence might constitute a spoken
word. The SARDNET input layer consists of n nodes, one for each component in
the input vector, and their values are denoted as A = (aI, a2, a3, ... , an). The map
consists of m x m nodes with activation Ojk , 1 ~ j, k ~ m. Each node has an
n-dimensional input weight vector Wjk, which determines the node's response to
the input activation.

In a conventional feature map network as well as in SARDNET, each input vector
is mapped on a particular unit on the map, called the winner or the maximally
responding unit . In SARDNET, however, once a node wins an input, it is made

SARDNET: A Self-Organizing Feature Map for Sequences

579

Sequence of Input vectors S
V1 V2 V3 V4
V,

---II

Previous winners

Input weight vector wJk.l
Winning unit jlc

Figure 1: The SARDNET architecture. A sequence of input vectors activates
units on the map one at a time. The past winners are excluded from further
competition, and their activation is decayed gradually to indicate position in the
sequence.
INITIALIZATION: Clear all map nodes to zero.
MAIN LOOP: While not end of seihence
1. Find unactivated weight vector t at best matches the input.
2. Assign 1.0 activation to that unit.
3. Adjust weight vectors of the nodes in the neighborhood.
4. Exclude the winning unit from subse~ent competition.
S. Decrement activation values for all ot er active nodes.
RESULT: Sequence representation = activated nodes ordered by activation values

Table 1: The SARDNET training algorithm.
uneligible to respond to the subsequent inputs in the sequence. This way a different
map node is allocated for every vector in the sequence. As more vectors come in,
the activation of the previous winners decays. In other words, each sequence of
length 1 is represented by 1 active nodes on the map, with their activity indicating
the order in which they were activated. The algorithm is summarized in table 1.
Assume the maximum length ofthe sequences we wish to classify is I, and each input
vector component can take on p possible values. Since there are pn possible input
vectors, Ipn map nodes are needed to represent all possible vectors in all possible
positions in the sequence, and a distributed pattern over the Ipn nodes can be used
to represent all pnl different sequences. This approach offers a significant advantage
over methods in which pnl nodes would be required for pnl sequences.
The specific computations of the SARDNET algorithm are as follows: The winning
node (j, k) in each iteration is determined by the Euclidean distance Djk of the

580

Daniel L. James, Risto Miikkulainen

input vector A and the node 's weight vector

W j k:

n
Djk

=

1)Wjk ,i -

a;)2.

(1)

i=O

The unit with the smallest distance is selected as the winner and activated with 1.0.
The weights of this node and all nodes in its neighborhood are changed according
to the standard feature map adaptation rule:

(2)
where a denotes the learning rate. As usual, the neighborhood starts out large
and is gradually decreased as the map becomes more ordered. As the last step in
processing an input vector , the activation 7]jk of all active units in the map are
decayed proportional to the decay parameter d:
O<d<1.

(3)

As in the standard feature map, as the weight vectors adapt, input vectors gradually
become encoded in the weight vectors of the winning units. Because weights are
changed in local neighborhoods , neighboring weight vectors are forced to become
as similar as possible , and eventually the network forms a topological layout of the
input vector space. In SARDNET, however , if an input vector occurs multiple times
in the same input sequence, it will be represented multiple times on the map as well.
In other words, the map representation expands those areas of the input space that
are visited most often during an input sequence.

3

EXPERIMENTS

SARDNET has proven successful in learning and recognizing arbitrary sequences
of binary and real numbers , as well as sequences of phonemic representations for
English words. This section presents experiments on mapping three-syllable words.
This dat a was selected because it shows how SARDNET can be applied to complex
input derived from a real-world task .
3.1

INPUT DATA

The phonemic word representations were obtained from the CELEX database of the
Max Planck Institute for Psycholinguistics and converted into International Phonetic Alphabet (IPA)-compliant representation, which better describes similarities
among the phonemes. The words vary from five to twelve phonemes in length. Each
phoneme is represented by five values: place, manner, sound, chromacity and sonority. For example, the consonant p is represented by a single vector (bilabial, stop,
unvoiced, nil , nil) , or in terms of real numbers, (.125, .167, .750,0 , 0). The diphthong sound ai as in "buy" , is represented by the two vectors (nil , vowel, voiced,
front, low) and (nil, vowel , voiced , front-center, hi-mid), or in real numbers ,
(0, 1, .25, .2, 1) and (0, 1, .25, .4, .286).
There are a total of 43 phonemes in this data set, including 23 consonants and 20
vowels. To represent all phonemic sequences of length 12, TKM and SOFM-S would

SARDNET: A Self-Organizing Feature Map for Sequences

581

o.ee
0.118
0.87

0.118
0.85 .......- -

Figure 2: Accuracy of SARDNET for different map and data set sizes.
The accuracy is measured as a percentage of unique representations out of all word
sequences.

need to have 45 12 ~ 6.919 map nodes, whereas SARDNET would need only 45 x 12
= 540 nodes . Of course, only a very small subset of the possible sequences actually
occur in the data. Three data sets consisting of 713 , 988, and 1628 words were used
in the experiments. If the maximum number of occurrences of phoneme i in any
single sequence is Cj I then the number of nodes SARDNET needs is C = L:~o Cj I
where N is the number of phonemes. This number of nodes will allow SARDNET
to map each phoneme in each sequence to a unit with an exact representation of
that phoneme in its weights . Calculated this way, SARDNET should scale up very
well with the number of words: it would need 81 nodes for representing the 713
word set , 84 for the 988 set and 88 for the 1628 set.

3.2

DENSENESS AND ACCURACY

A series of experiments with the above three data sets and maps of 16 to 81
nodes were run to see how accurately SARDNET can represent the sequences.
Self-organization was quite fast: each simulation took only about 10 epochs, with
a = 0.45 and the neighborhood radius decreasing gradually from 5-1 to zero. Figure 2 shows the percentage of unique representations for each data set and map
SIze.
SARDNET shows remarkable representational power: accuracy for all sets is better
than 97.7%, and SARDNET manages to pack 1592 unique representations even
on the smallest 16-node map. Even when there are not enough units to represent
each phoneme in each sequence exactly, the map is sometimes able to "reuse" units
to represent multiple similar phonemes. For example, assume units with exact
representations for the phonemes a and b exist somewhere on the map, and the
input data does not contain pairs of sequences such as aba-abb, in which it is
crucial to distinguished the second a from the second b. In this case, the second
occurrence of both phonemes could be represented by the same unit with a weight
vector that is the average of a and b. This is exactly what the map is doing: it is
finding the most descriptive representation of the data, given the available resources.

582

Daniel L. James, Risto Miikkulainen

Note that it would be possible to determine the needed C = L:f:o Cj phoneme
representation vectors directly from the input data set, and without any learning or
a map structure at all, establish distributed representations on these vectors with
the SARDNET algorithm. However, feature map learning is necessary ifthe number
of available representation vectors is less than C. The topological organization of
the map allows finding a good set of reusable vectors that can stand for different
phonemes in different sequences, making the representation more efficient.

3.3

REPRESENTING SIMILARITY

Not only are the representations densely packed on the map, they are also descriptive
in the sense that similar sequences have similar representations. Figure 3 shows the
final activation patterns on the 36-unit, 713-word map for six example words. The
first two words, "misplacement" and "displacement," sound very similar, and are
represented by very similar patterns on the map. Because there is only one m in
"displacement" , it is mapped on the same unit as the initial m of "misplacement."
Note that the two IDS are mapped next to each other, indicating that the map is
indeed topological, and small changes in the input cause only small changes in
the map representation. Note also how the units in this small map are reused to
represent several different phonemes in different contexts.
The other examples in figure 3 display different types of similarities with "misplacement". The third word, "miscarried", also begins with "mis", and shares that
subpart of the representation exactly. Similarly, "repayment" shares a similar tail
and "pessimist" the subsequence "mis" in a different part or the word. Because they
appear in a different context, these subsequences are mapped on slightly different
units, but still very close to their positions with "misplacement." The last word,
"burundi" sounds very different, as its representation on the map indicates.
Such descriptive representations are important when the map has to represent information that is incomplete or corrupted with noise. Small changes in the input
sequence cause small changes in the pattern, and the sequence can still be recognized. This property should turn out extremely important in real-world applications
of SARDNET, as well as in cognitive science models where confusing similar patterns with each other is often plausible behavior.

4

DISCUSSION AND FUTURE RESEARCH

Because the sequence representations on the map are distributed , the number of
possible sequences that can be represented in m units is exponential in m, instead
of linear as in most previous sequential feature map architectures. This denseness
together with the tendency to map similar sequences to similar representations
should turn out useful in real-world applications, which often require scale-up to
large and noisy data sets. For example, SARDNET could form the core of an
isolated word recognition system. The word input would be encoded in durationnormalized sequences of sound samples such as a string of phonemes, or perhaps
representations of salient transitions in the speech signal. It might also be possible
to modify SARDNET to form a more continuous trajectory on the map so that
SARDNET itself would take care of variability in word duration. For example, a

SARDNEf: A Self-Organizing Feature Map for Sequences

(a)
~~t

583

(b)

(c)

(e)

(f)

? ript.ItDt

(d)

Figure 3: Example map representations.

sequence of redundant inputs could be reduced to a single node if all these inputs
fall within the same neighborhood.
Even though the sequence representations are dense, they are also descriptive. Category memberships are measured not by labels of the maximally responding units,
but by the differences in the response patterns themselves. This sort of distributed
representation should be useful in cognitive systems where sequential input must
be mapped to an internal static representation for later retrieval and manipulation. Similarity-based reasoning on sequences should be easy to implement, and
the sequence can be easily recreated from the activity pattern on the map.
Given part of a sequence, SARDNET may also be modified to predict the rest of
the sequence. This can be done by adding lateral connections between the nodes
in the map layer. The lateral connections between successive winners would be
strengthened during training. Thus, given part of a sequence, one could follow the
strongest lateral connections to complete the sequence.

584

5

Daniel L. James, Risto Miikkulainen

CONCLUSION

SARDNET is a novel feature map architecture for classifying sequences of input
vectors. Each sequence is mapped on a distributed representation on the map,
making it possible to pack a remarkable large number of category representations
on a small feature map . The representations are not only dense, they also represent
the similarities of the sequences, which should turn out useful in cognitive science
as well as real-world applications of the architecture.
Acknowledgments
Thanks to Jon Hilbert for converting CELEX data into the International Phonetic
Alphabet format used in the experiments. This research was supported in part by
the National Science Foundation under grant #IRI-9309273.

References
Chappel, G. J., and Taylor, J. G. (1993). The temporal Kohonen map. Neural
Networks, 6:441-445.
Kangas, J. (1991). Time-dependent self-organizing maps for speech recognition.
In Proceedings of the International Conference on Artificial Neural Networks
(Espoo, Finland), 1591-1594. Amsterdam; New York: North-Holland.
Kohonen, T. (1989). Self-Organization and Associative Memory. Berlin; Heidelberg;
New York: Springer. Third edition.
Kohonen, T . (1990) . The self-organizing map. Proceedings of the IEEE, 78:14641480.
Samarabandu, J. K., and Jakubowicz, O. G. (1990). Principles of sequential feature maps in multi-level problems. In Proceedings of the International Joint
Conference on Neural Networks (Washington, DC), vol. II, 683-686. Hillsdale,
NJ: Erlbaum.
Scholtes, J. C. (1991). Recurrent Kohonen self-organization in natural language
processing. In Proceedings of the International Conference on Artificial Neural Networks (Espoo, Finland), 1751-1754. Amsterdam; New York: NorthHolland.
van Harmelen, H. (1993). Time dependent self-organizing feature map for speech
recognition. Master's thesis, University of Twente, Enschede, the Netherlands.
Zandhuis, J. A. (1992). Storing sequential data in self-organizing feature maps.
Internal Report MPI-NL-TG-4/92, Max-Planck-Institute fur Psycholinguistik,
Nijmegen, the Netherlands.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 2612-discrete-profile-alignment-via-constrained-information-bottleneck.pdf

Discrete profile alignment via constrained
information bottleneck
Sean O?Rourke?
seano@cs.ucsd.edu

Gal Chechik?
gal@stanford.edu

Robin Friedman?
rcfriedm@ucsd.edu

Eleazar Eskin?
eeskin@cs.ucsd.edu

Abstract
Amino acid profiles, which capture position-specific mutation probabilities, are a richer encoding of biological sequences than the individual sequences themselves. However, profile comparisons are
much more computationally expensive than discrete symbol comparisons, making profiles impractical for many large datasets. Furthermore, because they are such a rich representation, profiles can
be difficult to visualize. To overcome these problems, we propose a
discretization for profiles using an expanded alphabet representing
not just individual amino acids, but common profiles. By using an
extension of information bottleneck (IB) incorporating constraints
and priors on the class distributions, we find an informationally
optimal alphabet. This discretization yields a concise, informative
textual representation for profile sequences. Also alignments between these sequences, while nearly as accurate as the full profileprofile alignments, can be computed almost as quickly as those
between individual or consensus sequences. A full pairwise alignment of SwissProt would take years using profiles, but less than
3 days using a discrete IB encoding, illustrating how discrete encoding can expand the range of sequence problems to which profile
information can be applied.

1

Introduction

One of the most powerful techniques in protein analysis is the comparison of a
target amino acid sequence with phylogenetically related or homologous proteins.
Such comparisons give insight into which portions of the protein are important by
revealing the parts that were conserved through natural selection. While mutations
in non-functional regions may be harmless, mutations in functional regions are often
lethal. For this reason, functional regions of a protein tend to be conserved between
organisms while non-functional regions diverge.
?
?

Department of Computer Science and Engineering, University of California San Diego
Department of Computer Science, Stanford University

Many of the state-of-the-art protein analysis techniques incorporate homologous
sequences by representing a set of homologous sequences as a probabilistic profile,
a sequence of the marginal distributions of amino acids at each position in the
sequence. For example, Yona et al.[10] uses profiles to align distant homologues from
the SCOP database[3]; the resulting alignments are similar to results from structural
alignments, and tend to reflect both secondary and tertiary protein structure. The
PHD algorithm[5] uses profiles purely for structure prediction. PSI?BLAST[6] uses
them to refine database searches.
Although profiles provide a lot of information about the sequence, the use of profiles comes at a steep price. While extremely efficient string algorithms exist for
aligning protein sequences (Smith-Waterman[8]) and performing database queries
(BLAST[6]), these algorithms operate on strings and are not immediately applicable to profile alignment or profile database queries. While profile-based methods
can be substantially more accurate than sequence-based ones, they can require at
least an order of magnitude more computation time, since substitution penalties
must be calculated by computing distances between probability distributions. This
makes profiles impractical for use with large bioinformatics databases like SwissProt,
which recently passed 150,000 sequences. Another drawback of profile as compared
to string representations is that it is much more difficult to visually interpret a
sequence of 20 dimensional vectors than a sequence of letters.
Discretizing the profiles addresses both of these problems. First, once a profile is represented using a discrete alphabet, alignment and database search can be performed
using the efficient string algorithms developed for sequences. For example, when
aligning sequences of 1000 elements, runtime decreases from 20 seconds for profiles
to 2 for discrete sequences. Second, by representing each class as a letter, discretized
profiles can be presented in plain text like the original or consensus sequences, while
conveying more information about the underlying profiles. This makes them more
accurate than consensus sequences, and more dense than sequence logos (see figure
1). To make this representation intuitive, we want the discretization not only to
minimize information loss, but also to reflect biologically meaningful categories by
forming a superset of the standard 20-character amino acid alphabet. For example,
we use ?A? and ?a? for strongly- and weakly-conserved Alanine. This formulation
demands two types of constraints: similarities of the centroids to predefined values,
and specific structural similarities between strongly- and weakly-conserved variants.
We show below how these constraints can be added to the original IB formalism.
In this paper, we present a new discrete representation of proteins that takes into
account information from homologues. The main idea behind our approach is to
compress the space of probabilistic profiles in a data-dependent manner by clustering
the actual profiles and representing them by a small alphabet of distributions. Since
this discretization removes some of the information carried by the full profiles,
we cluster the distribution in a way that is directly targeted at minimizing the
information loss. This is achieved using a variant of Information Bottleneck (IB)[9],
a distributional clustering approach for informationally optimal discretization.
We apply our algorithm to a subset of MEROPS[4], a database of peptidases organized structurally by family and clan, and analyze the results in terms of both
information loss and alignment quality. We show that multivariate IB in particular
preserves much of the information in the original profiles using a small number of
classes. Furthermore, optimal alignments for profile sequences encoded with these
classes are much closer to the original profile-profile alignments than are alignments
between the seed proteins. IB discretization is therefore an attractive way to gain
some of the additional sensitivity of profiles with less computational cost.

0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.5
0.0
0.0
0.0
0.5
0.0
0.0
0.0
0.0

0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
1.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0

0.0
0.0
1.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0

0.09
0.04
0.01
0.38
0.06
0.00
0.02
0.00
0.04
0.01
0.00
0.05
0.02
0.04
0.04
0.16
0.02
0.00
0.00
0.01

0.34
0.01
0.05
0.04
0.00
0.06
0.00
0.00
0.01
0.01
0.00
0.05
0.00
0.05
0.01
0.10
0.10
0.14
0.00
0.00

0.23
0.01
0.14
0.00
0.08
0.01
0.04
0.03
0.01
0.00
0.03
0.01
0.23
0.00
0.00
0.06
0.05
0.03
0.00
0.04

0.12
0.03
0.09
0.04
0.04
0.03
0.00
0.00
0.00
0.09
0.00
0.01
0.00
0.00
0.00
0.29
0.20
0.04
0.00
0.04

0.0
0.0
0.0
0.0
0.0
1.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0

0.0
0.0
1.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0

0.0
0.0
0.0
0.0
1.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0

ND

N
S

GDF

AS
EAP
T
V
S
S

A

D
T
D
F

A

F

G
L
S

N

D

K

Q

E

T

N

R

A
A

A

H

F

Y

Q
E

V

Y

A
A
A

(b)
P00790 Seq.:
---EAPT--Consensus Seq.: NNDEAASGDF
IB Seq.:
NNDeaptGDF
(c)

(a)
Figure 1: (a) Profile, (b) sequence logo[2], and (c) textual representations for part
of an alignment of Pepsin A precursor P00790, showing IB?s concision compared to
profiles and logos, and its precision compared to single sequences.

2

Information Bottleneck

Information Bottleneck [9] is an information theoretic approach for distributional
clustering. Given a joint distribution p(X, Y ) of two random variables X and Y , the
goal is to obtain a compressed representation C of X, while preserving the information about Y . The two goals of compression and information preservation are quanP
p(x,y)
tified by the same measure of mutual information I(X; Y ) = x,y p(x, y) log p(x)p(y)
and the problem is therefore defined as the constrained optimization problem
minp(c|x):I(C;Y )>K I(C; X) where K is a constraint on the level of information
preserved about Y , and thePproblem should also obey the constraints p(y|c) =
P
x p(y|x)p(x|c) and p(y) =
x p(y|x)p(x). This constrained optimization can be
reformulated using Lagrange multipliers, and turned into a tradeoff optimization
function with Lagrange multiplier ?:
def

min L = I(C; X) ? ?I(C; Y )

(1)

p(c|x)

As an unsupervised learning technique, IB aims to characterize the set of solutions
for the complete spectrum of constraint values K. This set of solutions is identical to
the set of solutions of the tradeoff optimization problem obtained for the spectrum
of ? values.
When X is discrete, its natural compression is fuzzy clustering. In this case, the
problem is not convex and cannot be guaranteed to contain a single global minimum.
Fortunately, its solutions can be characterized analytically by a set of self consistent
equations. These self consistent equations can then be used in an iterative algorithm
that is guaranteed to converge to a local minimum. While the optimal solutions of
the IB functional are in general soft clusters, in practice, hard cluster solutions are
sometimes more easily interpreted. A series of algorithms was developed for hard
IB, including an algorithm that can be viewed as a one-step look-ahead sequential
version of K-Means [7].
To apply IB to the problem of profiles discretization discussed here, X is a given
set of probabilistic profiles obtained from a set of aligned sequences and Y is the
set of 20 amino acids.

2.1

Constraints on centroids? semantics

The application studied in this paper differs from standard IB applications in that
we are interested in obtaining a representation that is both efficient and biologically meaningful. This requires that we add two kinds of constraints on clusters?
distributions, discussed below.
First, some clusters? meanings are naturally determined by limiting them to correspond to the common 20-letter alphabet used to describe amino acids. From the
point of view of distributions over amino acids, each of these symbols is used today
as the delta function distribution which is fully concentrated on a single amino acid.
For the goal of finding an efficient representation, we require the centroids to be
close to these delta distributions. More generally, we require the centroids to be
close to some predefined values c?i , thus adding constraints to the IB target function
of the form DKL[p(y|?
ci )||p(y|ci )] < Ki for each constrained centroid. While solving
the constrained optimization problem is difficult, the corresponding tradeoff optimization problem can be made very similar to standard IB. With the additional
constraints, the IB trade-off optimization problem becomes
X
min L0 ? I(C; X) ? ?I(C; Y ) + ?
?(ci )DKL[p(y|?
ci )||p(y|ci )] .
(2)
p(c|x)

ci ?C

We now use the following identity
X
p(x, c)DKL[p(y|x)||p(y|c)]
x,c

=

X

p(x)

X

x

p(y|x) log p(y|x) ?

y

X
c

p(c)

X

log p(y|c)

y

X

p(y|x)p(x|c)

x

= ?H(Y |X) + H(Y |C) = I(X; Y ) ? I(Y ; C)
to rewrite the IB functional of Eq. (1) as
XX
L = I(C; X) + ?
p(x, c)DKL[p(y|x)||p(y|c)] ? ?I(X; Y )
c?C x?X

When

P

?(ci ) ? 1 we can similarly rewrite Eq. (2) as
X
X
L0 = I(C; X) + ?
p(x)
p(ci |x)DKL[p(y|x)||p(y|ci )]
x?X

+?

X

(3)

ci ?C

?(ci )DKL[p(y|?
ci )||p(y|ci )] ? ?I(X; Y )

ci ?C

= I(C; X) + ?

X
x0 ?X 0

p(x0 )

X

p(ci |x0 )DKL[p(y|x0 )||p(y|ci )] ? ?I(X; Y )

ci ?C

The optimization problem therefore becomes equivalent to the original IB problem,
but with a modified set of samples x ? X 0 , containing X plus additional ?pseudocounts? or biases. This is similar to the inclusion of priors in Bayesian estimation.
Formulated this way, the biases can be easily incorporated in standard IB algorithms
by adding additional pseudo-counts x0 with prior probability p(x0 ) = ?i (c).
2.2

Constraints on relations between centroids

We want our discretization to capture correlations between strongly- and weaklyconserved variants of the same symbol. This can be done with standard IB using

separate classes for the alternatives. However, since the distributions of other amino
acids in these two variants are likely to be related, it is preferable to define a single
shared prior for both variants, and to learn a model capturing their correlation.
Friedman et al.[1] describe multivariate information bottleneck (mIB), an extension
of information bottleneck to joint distributions over several correlated input and
cluster variables. For profile discretization, we define two compression variables
connected as in Friedman?s ?parallel IB?: an amino acid class C ? {A, C, . . .} with
an associated prior, and a strength S ? {0, 1}. Since this model correlates strong
and weak variants of each category, it requires fewer priors than simple IB. It also
has fewer parameters: a multivariate model with ns strengths and nc classes has as
many categories as a univariate one with nc0 = ns nc classes, but has only ns +nc ?2
free parameters for each x, instead of ns nc ? 1.

3

Results

To test our method, we apply it to data from MEROPS[4]. Proteins within the same
family typically contain high-confidence alignments, those from different families
in the same clan less so. For each protein, we generate a profile from alignments
obtained from PSI?BLAST with standard parameters, and compute IB classes from
a large subset of these profiles using the priors described below. Finally, we encode
and align pairs of profiles using the learned classes, comparing the results to those
obtained both with the full profiles and with just the original sequences.
For univariate IB, we have used four types of priors reflecting biases on stability,
physical properties, and observed substitution frequencies: (1) Strongly conserved
classes, in which a single symbol is seen with S% probability. These are the only
priors used for multivariate IB. (2) Weakly conserved classes, in which a single
symbol occurs with W % probability; (S ?W )% of the remaining probability mass is
distributed among symbols with non-negative log-odds of substitution. (3) Physical
trait classes, in which all symbols with the same hydrophobicity, charge, polarity,
or aromaticity occur uniformly S% of the time. (4) A uniform class, in which all
symbols occur with their background probabilities.
The choice of S and W depends upon both the data and one?s prior notions of
?strong? and ?weak? conservation. Unbiased IB on a large subset of MEROPS
with several different numbers of unbiased categories yielded a mean frequency
approaching 0.7 for the most common symbol in the 20 most sharply-distributed
classes (0.59 ? 0.13 for |C| = 52; 0.66 ? 0.12 for |C| = 80; 0.70 ? 0.09 for |C| = 100).
Similarly, the next 20 classes have a mean most-likely-symbol frequency around
0.4. These numbers can be seen as lower bounds on S and W . We therefore chose
S = 0.8 and W = 0.5, reflecting a bias toward stronger definitions of conservation
than those inferred from the data.
3.1

Iterative vs. Sequential IB

Slonim[7] compares several IB algorithms, concluding that best hard clustering results are obtained with a sequential method (sIB), in which elements are first assigned to a fixed number of clusters and then individually moved from cluster to
cluster while calculating a 1-step lookahead score, until the score converges. While
sIB is more efficient than exhaustive bottom-up clustering, it neglects information
about the best potential candidates to be assigned to a cluster, yielding slow convergence. Furthermore updates are expensive, since each requires recomputing the
class centroids. Therefore instead of sIB, we use iterative IB (iIB) with hard clustering, which only recomputes the centroids after performing all updates. This reduces

ACDEFGH I KLMNPQRSTVWY
ACDEFGH I KLMNPQRSTVWY

DE G

A

KLMNPQRSTV

H
C
YLPSLSLSLSLSLS
A I
SESVKLSGGGVGWL
S SF
G LLL
S GKL TR ELSVKLEG
LGSD
SKKE
VL SSLG
V
STL I NSGLDGGYVTRVK
ST T AL
SGR I ST TEQ
G
VL
G LAVTP I VAV I
KLK
G
GTVP
TQ
VA

S
A

Q

T

Y
L
K
S
S

N

DA

A

E
V
S

R
P
L
G
R
S
E

L

NT

VG

I

G

V
KR

T

G

E

L
V I A TGG I KNA
LLD
V TK T
I T I
I V
AL
GD
I GEV
TDAP
TNFG TDVDA
VAK I
GKDVDAA
AT
QA
TLEQA
DE
RASLDV TDDNFF I Q I R I DDPA
I KNP
S
VRT TFF AE
ETEY A TFDD
NPK
SFF I D
KNGTSA T APS
I GAPWN
PD
TA
Q I
MV TK
I N
P I F I T Y VSNP I PQG
Q
QNERN
E I
REEFNQ
A
VK AE
I
E
I
M
C
EQVRN
N
P
F
VLDT T VP
RYQYN I QDNSYRF
P
F
FK
NPQ

L

G

V
F
T

S

A

L
Y

KT

RQ

I

E

E

I

G

K

P

Q

F

L

R

A

Q

I

P

F

E

S

H

I

F

F

R

D

E

K

L

R

D

P

G

F

G

R

L

P

A

I

T

I

R

N

P

H

H

D
Q

K

M

I

L

V

I

M

R

R
P

F

M

D

M

C

N

P

T

P

P

E

G

Y

V

R

S

V

P

W

C

E

M
M

H
M

K

W

H

W

H

M

H

VT

G

A
S
LGTQVR
MA

N

A

E

S

F

I

D
S

S

E
N

S
A

G
Q

K

Y

W

I

L

F
L

T

V

T

P

K

E

G
Q

G

W

M

P

F

C

T

C

S

M

Y

I

T

C

N

W

M

W

D

E

S

V

F

K

D

Y

Q

Q
W

W

H

L

M

H

C

R

H

L

I

Y

F

I NQV TKVV

KG

G

H
Y
P

N

H

H

R

G

H

Y

H

H
P

V

A

A

P

R

W

N

T

C

G

L

G

M

N

P

P

S

W
R

P

H

N

G

R

H

Y

A

D

A

A

K

K

P

K

A

P

K

V

G
P

P

P

M

M

I

D
I

F

DYN

M
S
P
T
E
K
PT
E
KV
P
E
PA
WQ
A
K
E
QP
N
FC
A
QY VPK
W
K
D
E
R
EQD
QC
P
QG
N
CV
DDK S
G
RN
YKY
N
FN
D
QDQG
N
RR
R
DR
M
P
L
RY V I RN
R
R
R
KF
YQQ
Y
FE
F
V
L
W
E
H
E
M
N
E
NEK
PD
F
H
TH
H
MY
F
N
P
F
I
EYF
Q
K
I
M
I
EQ
M
YM
YQ
H
K
R
N
H
P
KR
M
M
K
TV
P
E
C
D
K
P
F
I
R
M
R
YM
M
V
Y
Y
M
H
H

H

R

L

M

D

Y

Q

D

G

Y

R

W

H

H

I

I

Y

P

M

N

A

F

Q
M

L

G

I

K

M

M

Q

D

KR
KM
H

H

M
Y

M

W

E

G

R

D

E

E

W

G

Y

Q

V

Q

T

H

C

M

M

NL
P
A
AA T
VQT T V
TSAA T T Y
ELGGSKA
NA I SSFNS
VNT V T A I D
DNGLTR
G
PS
RA
QA AK TPNKDLDVD I
TFEF
I TD
QVF
QSV
DG
AKD
DVLRN
AEFNQRA W
EAAFF
T TS
SL
YA
N
GQE I NE
I TQEVPVM
I G
PY A
L TRG
Q
ARA A
LS
NQ
E
I PQ
QYP

S

M
D

L

R

D
K

R
V

D

N

D

L

M

M

M

M

M

M

R

H
R

C

C

F

H

E

G

K

L

G
V

G
N

T

V

N

Y

K

L
F

H

P

Q

R

Y

H
C
H
H

M

A
SG
PSEGLSGL
I
YK
Q
G L
EF
SELS
K
SH
G
TLSRG
K GT
SLR
CL
VSVL L NLSSLRMK
SVK VKD
I W VV
I GG I K TETS I ASADDPFYD
V
I
GPL
V
E
I
G
T
L
G
P
L
T
I
D
Q
T
V
T
A

NL
G
FS
S

I
ER

Q

R
EY
E
I
Q
N

K

H

T
S
L

S

F

G

F

V

D

N

G

K

L

H

H

C

A

P

G
KY

PQ

V
L

L

A

F

L
F

RSA

R

K

T

I

ET

T

D

I

Q
V
N
T
D

P

W

I L
GN

F

S
N

A
D

E

K

D

TH

Q
R
I
I
I
A
H
K
I
P
C
S
A
MQ
V
N
N
V
R
G
YQ
K
DA
E
R
I
A
Y
M
E
T
H

V

P

K

R
P
D
E

I
TA
R

V

Q

R

I

V
L
L
T
TT
KESS

SY

N

L

F

L
ND
GP
I D

SA

F

C

F

M
C
H
R
M
Y
Q

Y

M

H

C

DEFG KLNP STV

AC

LK
S

Q
R
E
V
E
N

H
F

P

E
M

C

W

H

P

H

H
F
D
WQ
C
R
M
M
H
H
F
M
M

Y

H

M

H

L

M

F

M
M

T

M

A

Q

A

G

M

Q

H

R

Y

A

M

C

E
Q

T

K

N

D

V

G

H

S
N
D

D

C

V

G

F

W

V

G

L

L
M
T
K

D

V

E

F

L
Y

F

Q

K

D

G

Y

H
F

P

N
V

R

V

R

C

D

K

G

R

H

C

Q
L
NW

Q

Y

F

R

S

D

Y

E

W

T

M
L

S

V

Q

M

A

K
F

K

L

NV

T

F

P

L
V

A

D

T

A

Y

A
VF QG
A
F QNKN
RNQKE
R
R
Q
EQD WPK
PDSKGQRVD
SQ
QDAFPQ
P I G
EQDR
KRG
YE
R
DWN
QN
W
E I RN
RAN
P
P
K
N
Q
LN
FPWCPE
E
M
I
DYK V
F YD
EYQ
F
F
QP
NY
CQ
I
I KD
F YMYK
M
EY Y
H
RY Y
K
FP
E
FN
N
W
Y
YW
P
Y
H
FH
RY
P
H
RY
YM
M
R
M
P
E
K
W
NC
C

N
R
AV

E
KS
Q
Q
M
A
Q
NA
G
I
T
CV
L
I
T
S
K
D
P
Q
S
I
P
NT
ET
Q
A
I
N
E
Q
D
Y
P
S
D
AR
G
N
N
AS
FT
KS
G
D
I
I
E
Y
K
P
Q
E
E
D

TA

G
N

E

WQ

M

C
W

SQ

E

P

C

F

F

F
H

F

C

M

E

H

Y

H

M

D

F
R

M

C
M

M

P

I

H

Q

H

R

M

M

Y

W

W

H

H

W

R

C

F

H
C
Y

H
H

M

W

R

H

Figure 2: Stretched sequence logos for categories found by iIB (top) and sIB (bottom), ordered by primary symbol and decreasing information.

the convergence time from several hours to around ten minutes.
Since Slonim argues that sIB outperforms soft iIB in part because sIB?s discrete
steps allow it to escape local optima, we expect hard iIB to have similar behavior.
To test this, we applied three complete sIB iterations initialized with categories
from multivariate iIB. sIB decreased the loss L by only about 3 percent (from 0.380
to 0.368), with most of this gain occurring in the first iteration. Also, the resulting
categories were mostly conserved up to exchanging labels, suggesting that hard iIB
finds categories similar sIB ones (see figure 2).
3.2

Information Loss and Alignments

One measure of the quality of the resulting clusters is the amount of information
about Y lost through discretization, I(Y ; X) ? I(Y ; C). Figure (3b) shows the effect on information loss of varying the prior weight w with three sets of priors: 20
strongly conserved symbols and one background; these plus 20 weakly conserved
symbols; and these plus 10 categories for physical characteristics. As expected,
both decreasing the number of categories and increasing the number or weight of
priors increases information loss. However, with a fixed number of free categories,
information loss is nearly independent of prior strength, suggesting that our priors correspond to actual regularities in the data. Finally, note that despite having
fewer free parameters than the univariate models, mIB?s achieves comparable performance, suggesting that our decomposition into conserved class and degree of
conservation is reasonable.
Since we are ultimately using these classes in alignments, the true cost of discretization is best measured by the amount of change between profile and IB alignments,
and the significance of this change. The latter is important because the best path
can be very sensitive to small changes in the sequences or scoring matrix; if two radically different alignments have similar scores, neither is clearly ?correct?. We can
represent an alignment as a pair of index-insertion sequences, one for each profile
sequence to be aligned (e.g. ?1,2, , ,3,...? versus ?1, ,2, ,3,...?). The edit distance
between these sequences for two alignments then measures how much they differ.
However, even when this distance is large, the difference between two alignments
may not be significant if both choices? scores are nearly the same. That is, if the
optimal profile alignment?s score is only slightly lower than the optimal IB class
alignment?s score as computed with the original profiles, either might be correct.
Figure 4 shows at left both the edit distance and score change per length between
profile alignments and those using IB classes, mIB classes, and the original sequences with the BLOSUM62 scoring matrix. To compare the profile and sequence
alignments, profiles corresponding to gaps in the original sequences are replaced

64
Profile-profile
IB-profile
2e-5 * L^2 + 0.1
3e-3 * L - 0.1

I(Y;X)!-!I(Y;C)

0.46

Time!(s)

16

4

multivariate
21/52 priors
41/52 priors
51/52 priors

0.42

0.38

1
400

800

Length
(a)

1600

0.2

0.4

w
(b)

0.6

0.8

Figure 3: (a) Running times for profile-profile versus IB-profile alignment, showing
speedups of 3.5-12.5x for pairwise global alignment. (b)I(Y ; X) ? I(Y ; C) as a
function of w for different groups of priors. The information loss for 52 categories
without priors is 0.359, for 10, 0.474.

mIB
IB
BLOSUM
mIB
IB
BLOSUM

Edit distance Score change
Same Superfamily
0.154 ? 0.182 0.086 ? 0.166
0.170 ? 0.189 0.107 ? 0.198
0.390 ? 0.065
Same Clan
0.124 ? 0.209 0.019 ? 0.029
0.147 ? 0.232 0.022 ? 0.037
0.360 ? 0.062

Figure 4: Left: alignment differences for IB models and sequence alignment, within
and between superfamilies. Right: ROC curve for same/different superfamily classification by alignment score.
by gaps, and resulting pairs of aligned gaps in the profile-profile alignment are removed. We consider both sequences from the same family and those from other
families in the same clan, the former being more similar than the latter, and therefore having better alignments. Assuming the profile-profile alignment is closest to
the ?true? alignment, iIB alignment significantly outperforms sequence alignment
in both cases, with mIB showing a slight additional improvement. At right is the
ROC curve for detecting superfamily relationships between profiles from different
families based on alignment scores, showing that while IB fares worse than profiles,
simple sequences perform essentially at chance.
Finally, figure 3a compares the performance of profile and IB alignment for different
sequence lengths. To use a profile alphabet for novel alignments, we must map
each input profile to the closest IB class. To be consistent with Yona[10], we use
the Jensen-Shannon (JS) distance with mixing coefficient 0.5 rather than the KL
distance optimized in creating the categories. Aligning two sequences of lengths n
and m requires computing the |C|(n+m) JS-distances between each profile and each
category, a significant improvement over the mn distance computations required for
profile-profile alignment when |C|  min(m,n)
. Our results show that JS distance
2
computations dominate running time, since IB alignment time scales linearly with
the input size, while profile alignment scales quadratically, yielding an order of
magnitude improvement for typical 500- to 1000-base-pair sequences.

4

Discussion

We have described a discrete approximation to amino acid profiles, based on minimizing information loss, that allows profile information to be used for alignment
and search without additional computational cost compared to simple sequence
alignment. Alignments of sequences encoded with a modest number of classes correspond to the original profile alignments significantly better than alignments of the
original sequences. In addition to minimizing information loss, the classes can be
constrained to correspond to the standard amino acid representation, yielding an
intuitive, compact textual form for profile information.
Our model is useful in three ways: (1) it makes it possible to apply existing fast
discrete algorithms to arbitrary continuous sequences; (2) it models rich conditional
distribution structures; and (3) its models can incorporate a variety of class constraints. We can extend our approach in each of these directions. For example,
adjacent positions are highly correlated: the average entropy of a single profile is
0.99, versus 1.23 for an adjacent pair. Therefore pairs can be represented more compactly than the cross-product of a single-position alphabet. More generally, we can
encode arbitrary conserved regions and still treat them symbolically for alignment
and search. Other extensions include incorporating structural information in the
input representation; assigning structural significance to the resulting categories;
and learning multivariate IB?s underlying model?s structure.

References
[1] Nir Friedman, Ori Mosenzon, Noam Slonim, and Naftali Tishby. Multivariate
information bottleneck. In Uncertainty in Artificial Intelligence: Proceedings
of the Seventeenth Conference (UAI-2001), pages 152?161, San Francisco, CA,
2001. Morgan Kaufmann Publishers.
[2] Crooks GE, Hon G, Chandonia JM, and Brenner SE. WebLogo: a sequence
logo generator. Genome Research, in press, 2004.
[3] A. G. Murzin, S. E. Brenner, T. Hubbard, and C. Chothia. SCOP: a structural classification of proteins database for the investigation of sequences and
structures. J. Mol. Biol., 247:536?40, 1995.
[4] N.D. Rawlings, D.P. Tolle, and A.J. Barrett. MEROPS: the peptidase
database. Nucleic Acids Res, 32 Database issue:D160?4, 2004.
[5] B. Rost and C. Sander. Prediction of protein secondary structure at better
than 70% accuracy. J. Mol. Bio., 232:584?99, 1993.
[6] Altschul SF, Gish W, Miller W, Myers EW, and Lipman DJ. Basic local
alignment search tool. J Mol Biol, 215(3):403?10, October 1990.
[7] Noam Slonim. The Information Bottleneck: Theory and Applications. PhD
thesis, Hebrew University, Jerusalem, Israel, 2002.
[8] T. F. Smith and M. S. Waterman. Identification of common molecular subsequences. Journal of Molecular Biology, 147:195?197, 1981.
[9] Naftali Tishby, Fernando C. Pereira, and William Bialek. The information
bottleneck method. In Proc. of the 37-th Annual Allerton Conference on Communication, Control and Computing, pages 368?77, 1999.
[10] Golan Yona and Michael Levitt. Within the twilight zone: A sensitive profileprofile comparison tool based on information theory. Journal of Molecular
Biology, 315:1257?75, 2002.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 876-illumination-invariant-face-recognition-with-a-contrast-sensitive-silicon-retina.pdf

Illumination-Invariant Face Recognition with a
Contrast Sensitive Silicon Retina

Joachim M. Buhmann
Rheinische Friedrich-Wilhelms-U niversitiit
Institut fUr Informatik II, RomerstraBe 164
0-53117 Bonn, Germany
Martin Lades
Ruhr-Universitiit Bochum
Institut fiir Neuroinformatik
0-44780 Bochum, Germany

Frank Eeckman
Lawrence Livermore National Laboratory
ISCR, P.D.Box 808, L-426
Livermore, CA 94551

Abstract
Changes in lighting conditions strongly effect the performance and reliability of computer vision systems. We report face recognition results
under drastically changing lighting conditions for a computer vision system which concurrently uses a contrast sensitive silicon retina and a conventional, gain controlled CCO camera. For both input devices the face
recognition system employs an elastic matching algorithm with wavelet
based features to classify unknown faces. To assess the effect of analog
on -chip preprocessing by the silicon retina the CCO images have been
"digitally preprocessed" with a bandpass filter to adjust the power spectrum. The silicon retina with its ability to adjust sensitivity increases
the recognition rate up to 50 percent. These comparative experiments
demonstrate that preprocessing with an analog VLSI silicon retina generates image data enriched with object-constant features.

1 Introdnction
Neural computation as an information processing paradigm promises to enhance artificial
pattern recognition systems with the learning capabilities of the cerebral cortex and with the

769

770

Buhmann, Lades, and Eeckman

adaptivity of biological sensors. Rebuilding sensory organs in silicon seems to be particularly promising since their neurophysiology and neuroanatomy, including the connections
to cortex, are known in great detail. This knowledge might serve as a blueprint for the design
of artificial sensors which mimic biological perception. Analog VLSI retinas and cochleas,
as designed by Carver Mead (Mead, 1989; Mahowald, Mead, 1991) and his collaborators
in a seminal research program, will ultimately be integrated in vision and communication
systems for autonomous robots and other intelligent information processing systems.
The study reported here explores the influence of analog retinal preprocessing on the
recognition performance of a face recognition system. Face recognition is a challenging
classification task where object inherent distortions, like facial expressions and perspective
changes, have to be separated from other image variations like changing lighting conditions.
Preprocessing with a silicon retina is expected to yield an increased recognition rate since the
first layers of the retina adjust their local contrast sensitivity and thereby achieve invariance
to variations in lighting conditions.
Our face recognizer is equipped with a silicon retina as an adaptive camera. For comparison
purposes all images are registered simultaneously by a conventional CCD camera with
automatic gain control. Galleries with images of 109 different test persons each are taken
under three different lighting conditions and two different viewing directions (see Fig. 1).
These different galleries provide separate statistics to measure the sensitivity of the system
to variations in light levels or contrast and image changes due to perspective distortions.
Naturally, the performance of an object recognition system depends critically on the classification strategy pursued to identify unknown objects in an image with the models stored in a
database. The matching algorithm selected to measure the performance enhancing effect of
retinal preprocessing deforms prototype faces in an elastic fashion (Buhmann et aI., 1989;
Buhmann et al., 1990; Lades et al., 1993). Elastic matching has been shown to perform
well on the face classification task recognizing up to 80 different faces reliably (Lades et al.,
1993) and in a translation, size and rotation invariant fashion (Buhmann et aI., 1990). The
face recognition algorithm was initially suggested as a simplified version of the Dynamic
Link A rchitecture (von der Malsburg, 1981), an innovative neural classification strategy with
fast changes in the neural connectivity during recognition stage. Our recognition results
and conclusions are expected to be qualitatively typical for a whole range of face/object
recognition systems (Turk, Pentland, 1991; Yuille, 1991; Brunelli, Poggio, 1993), since any
image preprocessing with emphasis on object constant features facilitates the search for the
correct prototype.

2

The Silicon Retina

The silicon retina used in the recognition experiments models the interactions between
receptors and horizontal cells taking place in the outer plexiform layer of the vertebrate
retina. All cells and their interconnections are explicitly represented in the chip so that
the following description simultaneously refers to both biological wetware and silicon
hardware. Receptors and horizontal cells are electrically coupled to their neighbors. The
weak electrical coupling between the receptors smoothes the image and reduces the influence of voltage offsets between adjacent receptors. The horizontal cells have a strong
lateral electrical coupling and compute a local background average. There are reciprocal
excitatory-inhibitory synapses between the receptors and the horizontal cells. The horizontal cells use shunting inhibition to adjust the membrane conductance of the receptors and

Illumination-Invariant Face Recognition with a Contrast Sensitive Silicon Retina

thereby adjust their sensitivity locally. This feedback interaction produces an antagonistic
center/surround organization of receptive fields at the output The center is represented
by the weakly coupled excitatory receptors and the surround by the more strongly coupled
inhibitory horizontal cells. The center/surround organization removes the average intensity
and expands the dynamic range without response compression. Furthennore, it enhances
edges.
In contrast to this architecture, a conventional CCD camera can be viewed as a very primitive
retina with only one layer of non-interacting detectors. There is no DC background removal,
causing potential over- and underexposure in parts of the image which reduces the useful
dynamic range. A mechanical iris has to be provided to adjust the mean luminance level
to the appropriate setting. Since cameras are designed for faithful image registration rather
than vision, on-chip pixel processing, if provided at all, is used to improve the camera
resolution and signal-to-noise ratio.
Three adjustable parameters allow us to fine tune the retina chip for an object recognition
experiment: (i) the diffusivity of the cones (ii) the diffusivity ofthe horizontal cells (iii) the
leak in the horizontal cell membrane. Changes in the diffusivities affect the shape of the
receptive fields, e.g., a large diffusivity between cones smoothes out edges and produces a
blurred image. The other extreme of large diffusivity between horizontal cells pronounces
edges and enhances the contrast gain . The retina chip has a resolution of 90 x 92 pixels,
it was designed by (Boahen, Andreou, 1992) and fabricated in 2flm n-well technology by
MOSIS.

3

Elastic Matching Algorithm for Face Recognition

Elastic matching is a pattern classification strategy which explicitly accounts for local
distortions. A prototype template is elastically deformed to measure local deviations from a
new, unknown pattern. The amount of deformation and the similarity oflocal image features
provide us with a decision criterion for pattern classification. The rubbersheet-like behavior
of the prototype transformation makes elastic matching a particularly attractive method for
face recognition where ubiquitous local distortions are caused for example by perspective
changes and different facial expressions. Originally, the technique was developed for
handwritten character recognition (Burr, 1981). The version of elastic matching employed
for our face recognition experiments is based on attributed graph matching. A detailed
description with a plausible interpretation in neural networks terms is published in (Lades
et al., (993). Each prototype face is encoded as a planar graph with feature vectors attached
to the vertices of the graph and metric information attached to the edges. The feature vectors
extract local image information at pixel Xi in a multiscale fashion, i.e., they are functions
of wavelet coefficients. Each feature vector establishes a correspondence between a vertex
i of a prototype graph and a pixel Xi in the image. The components of a feature vector are
defined as the magnitudes of the convolution of an image with a set of two-dimensional,
DC free Gaussian kernels centered at pixel Xi. The kernels with the form

1/!'k (X)

fl exp
= (72

(flx2
- ) - exp (-(72/2) 1
- 2(72 ) [exp (ikX

(I)

are parameterized by the wave vector k defining their orientations and their sizes. To
construct a self-similar set of filter functions we select eight different orientations and five

771

772

Buhmann, Lades, and Eeckman

different scales according to
k(v, tt) =

~ Tv/2

(cos( itt), sin( itt))

(2)

with v E {O, ... ,4};tt E {O, ... , 7}. The multi-resolution data format represents local
distortions in a robust way, i.e., only feature vectors in the vicinity x of an image distortion
are altered by the changes. The edge labels encode metric information, in particular we
choose the difference vectors AXij == Xi - Xj as edge labels.
To generate a new prototype graph for the database, the center of a new face is determined
by matching a generic face template to it. A 7 x 10 rectangular grid with 10 pixel spacing
between vertices and edges between adjacent vertices is then centered at that point. The
saliency of image points is taken into account by deforming that generic grid so that each
vertex is moved to the nearest pixel with a local maximum in feature vector length.
The classification of an unknown face as one of the models in the database or its rejection
as an unclassified object is achieved by computing matching costs and distortion costs.
The matching costs are designed to maximize the similarity between feature vector J;M of
vertex i in the model graph (M) and feature vector Jl (Xi) associated with pixel Xi in the
new image (I). The cosine of the angle between both feature vectors
-[....,

S(JI(x) jM) =

'"

-M

J (Xi) . Ji

(3)
M

Ilf (Xi)IIIIJ; II
1

is suited as a similarity function for elastic matching since global contrast changes in images
only scale feature vectors but do not rotate them. Besides maximizing the similarity between
feature vectors the elastic matching algorithm penalizes large distortions. The distortion
cost term is weighted by a factor ,\ which can be interpreted as a prior for expected
distortions. The combined matching cost function which is used in the face recognition
system compromises between feature similarity and distortion, i.e, it minimizes the cost
function

(4)
for the model M in the face database with respectto the correspondence points {xf}. (i, j)
in Eq. (4) denotes that index j runs over the neighborhood of vertex i and index i runs
in the new image
over all vertices. By minimizing Eq. (4) the algorithm assigns pixel
I to vertex i in the prototype graph M. Numerous classification experiments revealed that
a steepest descent algorithm is sufficient to minimize cost function (4) although it is nonconvex and local minima may cause non-optimal correspondences with reduced recognition
rates.

x;

During a recognition experiment all prototype graphs in the database are matched to the
new image. A new face is classified as prototype A if H A is minimal and if the significance
criterion
(5)

is fulfilled. The average costs (Ji) and their standard deviation LH are calculated excluding
match A. This heuristic is based on the assumption that a new face image strongly

Illumination-Invariant Face Recognition with a Contrast Sensitive Silicon Retina

l>

gr.tl rr.m.1

l>gr.rr~

l>~"~

l>~.tr. .llo'"
~1Ib.ka-:2to,..

>

Workstation

Datacube

~~
. ..

Figure I: Laboratory setup of the face recognition experiments.
correlates with the correct prototype but the matching costs to all the other prototype faces
is approximately Gaussian distributed with mean (1l) and standard deviation I.H. The
threshold parameter 0 is used to limit the rate of false positive matches, i.e., to exclude
significant matches to wrong prototypes.

4

Face Recognition Results

To measure the recognition rate of the face recognition system using a silicon retina or a
CCD camera as input devices, pictures of 109 different persons are taken under 3 different
lighting conditions and 2 different viewing directions. This setup allows us to quantify the
influence of changes in lighting conditions on the recognition performance separate from
the influence of perspective distortions. Figure 2 shows face images of one person taken
under two different lighting setups. The images in Figs. 2a,c with both lights on are used
as the prototype images for the respective input devices. To test the influence of changing
lighting conditions the left light is switched off. The faces are now strongly illuminated
from the right side. The CCD camera images (Figs. 2a,b) document the drastic changes of
the light settings. The corresponding responses of the silicon retina shown in Figs. 2c,d
clearly demonstrate that the local adaptivity of the silicon retina enables the recognition
system to extract object structure from the bright and the dark side of the face. For control
purposes all recognition experiments have been repeated with filtered CCD camera images.
The filter was adjusted such that the power spectra of the retina chip images and the filtered
CCD images are identical. The images (e,f) are filtered versions of the images (a,b). It
is evident that information in the dark part of image (b) has been erased due to saturation
effects of the CCD camera and cannot be recovered by any local filtering procedure.
We first measure the performance of the silicon retina under uniform lighting conditions,

n3

b

~
....

..
~

~

It

,.

...'.

-

'\.

..
...

C
....

.-

?...
~

?.

.

~

.~

.....

?

1?

Figure 2: (a) Conventional CCD camera images (a,b) and silicon retina image (c,d) under
different lighting conditions. The images (e,O are filtered CCD camera images with a
power spectrum adjusted to the images in (c,d). The images (a,c) are used to generate the

Illumination-Invariant Face Recognition with a Contrast Sensitive Silicon Retina

Table 1: (a) Face recognition results in a well illuminated environment and (b) in an
environment with drastic changes in lighting conditions.

a

b

f. p. rate
100%
100/0
50/0
10/0
1000/0
10%
50/0
10/0

silicon retina
83.5
81.7
76.2
71.6
96.3
96.3
96.3
93.6

cony. CCD
86.2
83.5
82.6
79.8
80.7
76.2
72.5
64.2

filt. CCD
85.3
84.4
80.7
75.2
78.0
75.2
72.5
62.4

i.e., both lamps are on and the person looks 20-30 degrees to the right. The recognition
system has to deal with perspective distortions only. A gallery of 109 faces is matched
to a face database of the same 109 persons. Table la shows that the recognition rate
reaches values between 80 and 90 percent if we accept the best match without checking
its significance. Such a decision criterion is unpractical for many applications since it
corresponds to a false positive rate (f. p. rate) of 100 percent. If we increase the threshold E>
to limit false positive matches to less than 1 percent the face recognizer is able to identify
three out of four unknown faces. Filtering the CCD imagery does not hurt the recognition
performance as the third column in Table 1a demonstrates. All necessary information for
recognition is preserved in the filtered CCD images.
The situation changes dramatically when we switch off the lamp on the left side of the
test person. We compare a test gallery of persons looking straight ahead, but illuminated
only from the right side, to our model gallery. Table 1b summarizes the recognition results
for different false positive rates. The advantage of using a silicon retina are 20 to 45
percent higher recognition rates than for a system with a CCD camera. For a false positive
rate below one percent a silicon retina based recognition system identifies two third more
persons than a conventional system. Filtering does not improve the recognition rate of a
system that uses a CCD camera as can be seen in the third column.
Our comparative face recognition experiment clearly demonstrates that a face recognizer
with a retina chip is performing substantially better than conventional CCD camera based
systems in environments with uncontrolled, substantially changing lighting conditions.
Retina-like preprocessing yields increased recognition rates and increased significance
levels. We expect even larger discrepancies in recognition rates if object without a bilateral
symmetry have to be classified. In this sense the face recognition task does not optimally
explore the potential of adaptive preprocessing by a silicon retina. Imagine an object
recognition task where the most significant features for discrimination are hardly visible
or highly ambiguous due to poor illumination. High error rates and very low significance
levels are an inevitable consequence of such lighting conditions.
The limited resolution and poor signal-to-noise ratio of silicon retina chips are expected to
be improved by a new generation of chips fabricated in 0.7 /lm CMOS technology with a

775

776

Buhmann, Lades, and Eeckman

potential resolution of256 x 256 pixels. Lighting conditions as simulated in ourrecognition
experiment are ubiquitous in natural environments. Autonomous robots and vehicles or
surveillance systems are expected to benefit from the silicon retina technology by gaining
robustness and reliability. Silicon retinas and more elaborate analog VLSI chips for low
level vision are expected to be an important component of an Adaptive Vision System.
Acknowledgement: It is a pleasure to thank K. A. Boahen for providing us with the
retina chips. We acknowledge stimulating discussions with C. von der Malsburg and C.
Mead. This work was supported by the German Ministry of Science and Technology
(lTR-8800-H 1) and by the Lawrence Livermore National Laboratory (W-7405-Eng-48).

References
Boahen, K., Andreou, A. 1992. A Contrast Sensitive Silicon Retina with Reciprocal
Synapses. Pages 764-772 of: NIPS91 Proceedings. IEEE.
Brunelli, R., Poggio, T. (1993). Face Recognition: Features versus Templates. IEEE Trans.
on Pattern Analysis Machine Intelligence, 15, 1042-1052.
Buhmann, J., Lange, J., von der Malsburg, C. 1989. Distortion Invariant Object Recognition
by Matching Hierarchically Labeled Graphs. Pages I 155-159 of' Proc. llCNN,
Washington. IEEE.
Buhmann, J., Lades, M., von der Malsburg, C. 1990. Size and Distortion Invariant Object
Recognition by Hierarchical Graph Matching. Pages II 411-416 of' Proc. llCNN,
SanDiego. IEEE.
Burr, D. J. (1981). Elastic Matching of Line Drawings. IEEE Trans. on Pat. An. Mach.
Intel., 3, 708-713.
Lades, M., Vorbriiggen, J.C., Buhmann, J., Lange, J., von der Malsburg, C., Wurtz, R.P.,
Konen, W. (1993). Distortion Invariant Object Recognition in the Dynamic Link
Architecture. IEEE Transactions on Computers, 42, 300-311.
Mahowald, M., Mead, C. (1991). The Silicon Retina. Scientific American, 264(5), 76.
Mead, C. (1989). Analog VLSI and Neural Systems. New York: Addison Wesley.
Turk, M., Pentland, A. (1991). Eigenfaces for Recognition. J. Cog. Sci., 3, 71-86.
von der Malsburg, Christoph. 1981. The Correlation Theory of Brain Function. Internal
Report. Max-Planck-Institut, Biophys. Chern., Gottingen, Germany.
Yuille, A. (1991). Deformable Templates for Face Recognition. J. Cog. Sci., 3, 60-70.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

title: 2582-chemosensory-processing-in-a-spiking-model-of-the-olfactory-bulb-chemotopic-convergence-and-center-surround-inhibition.pdf

Chemosensory processing in a spiking
model of the olfactory bulb: chemotopic
convergence and center surround
inhibition

Baranidharan Raman and Ricardo Gutierrez-Osuna
Department of Computer Science
Texas A&M University
College Station, TX 77840
{barani,rgutier}@cs.tamu.edu

Abstract
This paper presents a neuromorphic model of two olfactory signalprocessing primitives: chemotopic convergence of olfactory
receptor neurons, and center on-off surround lateral inhibition in
the olfactory bulb. A self-organizing model of receptor
convergence onto glomeruli is used to generate a spatially
organized map, an olfactory image. This map serves as input to a
lattice of spiking neurons with lateral connections. The dynamics
of this recurrent network transforms the initial olfactory image into
a spatio-temporal pattern that evolves and stabilizes into odor- and
intensity-coding attractors. The model is validated using
experimental data from an array of temperature-modulated gas
sensors. Our results are consistent with recent neurobiological
findings on the antennal lobe of the honeybee and the locust.

1

In trod u ction

An artificial olfactory system comprises of an array of cross-selective chemical
sensors followed by a pattern recognition engine. An elegant alternative for the
processing of sensor-array signals, normally performed with statistical pattern
recognition techniques [1], involves adopting solutions from the biological olfactory
system. The use of neuromorphic approaches provides an opportunity for
formulating new computational problems in machine olfaction, including mixture
segmentation, background suppression, olfactory habituation, and odor-memory
associations.
A biologically inspired approach to machine olfaction involves (1) identifying key
signal processing primitives in the olfactory pathway, (2) adapting these primitives
to account for the unique properties of chemical sensor signals, and (3) applying the
models to solving specific computational problems.

The biological olfactory pathway can be divided into three general stages: (i)
olfactory epithelium, where primary reception takes place, (ii) olfactory bulb (OB),
where the bulk of signal processing is performed and, (iii) olfactory cortex, where
odor associations are stored. A review of literature on olfactory signal processing
reveals six key primitives in the olfactory pathway that can be adapted for use in
machine olfaction. These primitives are: (a) chemical transduction into a
combinatorial code by a large population of olfactory receptor neurons (ORN), (b)
chemotopic convergence of ORN axons onto glomeruli (GL), (c) logarithmic
compression through lateral inhibition at the GL level by periglomerular
interneurons, (d) contrast enhancement through lateral inhibition of mitral (M)
projection neurons by granule interneurons, (e) storage and association of odor
memories in the piriform cortex, and (f) bulbar modulation through cortical
feedback [2, 3].
This article presents a model that captures the first three abovementioned
primitives: population coding, chemotopic convergence and contrast enhancement.
The model operates as follows. First, a large population of cross-selective pseudosensors is generated from an array of metal-oxide (MOS) gas sensors by means of
temperature modulation. Next, a self-organizing model of convergence is used to
cluster these pseudo-sensors according to their relative selectivity. This clustering
generates an initial spatial odor map at the GL layer. Finally, a lattice of spiking
neurons with center on-off surround lateral connections is used to transform the GL
map into identity- and intensity-specific attractors.
The model is validated using a database of temperature-modulated sensor patterns
from three analytes at three concentration levels. The model is shown to address the
first problem in biologically-inspired machine olfaction: intensity and identity
coding of a chemical stimulus in a manner consistent with neurobiology [4, 5].

2

M o d e l i n g c h e m o t opi c c o n v e r g e n c e

The projection of sensory signals onto the olfactory bulb is organized such that
ORNs expressing the same receptor gene converge onto one or a few GLs [3]. This
convergence transforms the initial combinatorial code into an organized spatial
pattern (i.e., an olfactory image). In addition, massive convergence improves the
signal to noise ratio by integrating signals from multiple receptor neurons [6].
When incorporating this principle into machine olfaction, a fundamental difference
between the artificial and biological counterparts must be overcome: the input
dimensionality at the receptor/sensor level. The biological olfactory system employs
a large population of ORNs (over 100 million in humans, replicated from 1,000
primary receptor types), whereas its artificial analogue uses a few chemical sensors
(commonly one replica of up to 32 different sensor types).
To bridge this gap, we employ a sensor excitation technique known as temperature
modulation [7]. MOS sensors are conventionally driven in an isothermal fashion by
maintaining a constant temperature. However, the selectivity of these devices is a
function of the operating temperature. Thus, capturing the sensor response at
multiple temperatures generates a wealth of additional information as compared to
the isothermal mode of operation. If the temperature is modulated slow enough
(e.g., mHz), the behavior of the sensor at each point in the temperature cycle can
then be treated as a pseudo-sensor, and thus used to simulate a large population of
cross-selective ORNs (refer to Figure 1(a)).
To model chemotopic convergence, these temperature-modulated pseudo-sensors
(referred to as ORNs in what follows) must be clustered according to their

selectivity [8]. As a first approximation, each ORN can be modeled by an affinity
vector [9] consisting of the responses across a set of C analytes:
r
K i = K i1 , K i2 ,..., K iC
(1)

[

]

where K ia is the response of the ith ORN to analyte a. The selectivity of this ORN
r
is then defined by the orientation of the affinity vector ? i .
A close look at the OB also shows that neighboring GLs respond to similar odors
[10]. Therefore, we model the ORN-GL projection with a Kohonen self-organizing
map (SOM) [11]. In our model, the SOM is trained to model the distribution of
r
ORNs in chemical sensitivity space, defined by the affinity vector ? i . Once the
training of the SOM is completed, each ORN is assigned to the closest SOM node (a
simulated GL) in affinity space, thereby forming a convergence map. The response
of each GL can then be computed as
G aj = ?

(?

N
i =1

Wij ? ORN ia

)

(2)

where ORN ia is the response of pseudo-sensor i to analyte a, Wij=1 if pseudo-sensor i
converges to GL j and zero otherwise, and ? (?) is a squashing sigmoidal function
that models saturation.
This convergence model works well under the assumption that the different sensory
inputs are reasonably uncorrelated. Unfortunately, most gas sensors are extremely
collinear. As a result, this convergence model degenerates into a few dominant GLs
that capture most of the sensory activity, and a large number of dormant GLs that do
not receive any projections. To address this issue, we employ a form of competition
known as conscience learning [12], which incorporates a habituation mechanism to
prevent certain SOM nodes from dominating the competition. In this scheme, the
fraction of times that a particular SOM node wins the competition is used as a bias
to favor non-winning nodes. This results in a spreading of the ORN projections to
neighboring units and, therefore, significantly reduces the number of dormant units.
We measure the performance of the convergence mapping with the entropy across
the lattice, H = ?? Pi log Pi , where Pi is the fraction of ORNs that project to SOM
node i [13]. To compare Kohonen and conscience learning, we built convergence
mappings with 3,000 pseudo-sensors and 400 GL units (refer to section 4 for
details). The theoretical maximum of the entropy for this network, which
corresponds to a uniform distribution, is 8.6439. When trained with Kohonen?s
algorithm, the entropy of the SOM is 7.3555. With conscience learning, the entropy
increases to 8.2280. Thus, conscience is an effective mechanism to improve the
spreading of ORN projections across the GL lattice.

3

M o d e l i n g t h e o l f a c t o r y b u l b n e t wo r k

Mitral cells, which synapse ORNs at the GL level, transform the initial olfactory
image into a spatio-temporal code by means of lateral inhibition. Two roles have
been suggested for this lateral inhibition: (a) sharpening of the molecular tuning
range of individual M cells with respect to that of their corresponding ORNs [10],
and (b) global redistribution of activity, such that the bulb-wide representation of an
odorant, rather than the individual tuning ranges, becomes specific and concise over
time [3]. More recently, center on-off surround inhibitory connections have been
found in the OB [14]. These circuits have been suggested to perform pattern
normalization, noise reduction and contrast enhancement of the spatial patterns.

We model each M cell using a leaky integrate-and-fire spiking neuron [15]. The
input current I(t) and change in membrane potential u(t) of a neuron are given by:
I (t ) =

du
u (t )
+C
dt
R

(3)
du
?
= ?u (t ) + R ? I (t ) [? = RC ]
dt
Each M cell receives current Iinput from ORNs and current Ilateral from lateral
connections with other M cells:
I input ( j ) = ?Wij ? ORNi
i

(4)

I lateral ( j , t ) = ? Lkj ? ? (k , t ? 1)
k

where Wij indicates the presence/absence of a synapse between ORNi and Mj, as
determined by the chemotopic mapping, Lkj is the efficacy of the lateral connection
between Mk and Mj, and ?(k,t-1) is the post-synaptic current generated by a spike at
Mk:

? (k , t ? 1) = ? g (k , t ? 1) ? [u ( j, t ? 1) + ? Esyn ]

(5)

g(k,t-1) is the conductance of the synapse between Mk and Mj at time t-1, u(j,t-1) is
the membrane potential of Mj at time t-1 and the + subscript indicates this value
becomes zero if negative, and Esyn is the reverse synaptic potential. The change in
conductance of post-synaptic membrane is:
g& (k , t ) =

dg (k , t ) ? g (k , t )
=
+ z (k , t )
dt
? syn

z& ( k , t ) =

dz (k , t ) ? z ( k , t )
=
+ g norm ? spk ( k , t )
dt
? syn

(6)

where z(.) and g(.) are low pass filters of the form exp(-t/?syn) and t ? exp(?t / ? syn ) ,
respectively, ?syn is the synaptic time constant, gnorm is a normalization constant, and
spk(j,t) marks the occurrence of a spike in neuron i at time t:
?1 u ( j , t ) = Vspike ?
spk ( j , t ) = ?
?
?0 u ( j , t ) ? Vspike ?

(7)

Combining equations (3) and (4), the membrane potential can be expressed as:
du ( j , t ) ? u ( j, t ) I lateral ( j, t ) I input ( j )
=
+
+
dt
RC
C
C
?u ( j , t ? 1) + u& ( j , t ? 1) ? dt u ( j, t ) < Vthreshold ?
u ( j, t ) = ?
?
Vspike
u ( j, t ) ? Vthreshold ?
?

u& ( j, t ) =

(8)

When the membrane potential reaches Vthreshold, a spike is generated, and the
membrane potential is reset to Vrest. Any further inputs to the neuron are ignored
during the subsequent refractory period.
Following [14], lateral interactions are modeled with a center on-off surround
matrix Lij. Each M cell makes excitatory synapses to nearby M cells (d<de), where
d is the Manhattan distance measured in the lattice, and inhibitory synapses with

distant M cells (de<d<di) through granule cells (implicit in our model). Excitatory
synapses are assigned uniform random weights between [0, 0.1]. Inhibitory
synapses are assigned negative weights in the same interval. Model parameters are
summarized in Table 1.
Table 1. Parameters of the OB spiking neuron lattice
Parameter
Peak synaptic conductance (Gpeak)
Capacitance (C)
Resistance (R)
Spike voltage (V spike )
Threshold voltage (Vthreshold )
Synapse Reverse potential (E syn)

Value
0.01
1 nF
10 MOhm
70 mV
5 mV
70 mV

Excitatory distance (d e )

d <

4

1

Parameter
Synaptic time constants (? syn)
Total simulation time (t tot )
Integration time step (dt)
Refractory period (t ref)
Number of mitral cells (N)
Normalization constant (g norm)

1

Inhibitory distance (d i )

N

6

Value
10 ms
500 ms
1 ms
3 ms
400
0.0027
2

N <d <

6

6

N

Results

The proposed model is validated on an experimental dataset containing gas sensor
signals for three analytes: acetone (A), isopropyl alcohol (B) and ammonia (C), at
three different concentration levels per analyte. Two Figaro MOS sensors (TGS
2600, TGS 2620) were temperature modulated using a sinusoidal heater voltage (0-7
V; 2.5min period; 10Hz sampling frequency). The response of the two sensors to the
three analytes at the three concentration levels is shown in Figure 1(a). This
response was used to generate a population of 3,000 ORNs, which were then
mapped onto a GL layer with 400 units arranged as a 20?20 lattice.

Sensor Conductance
(Iso-propyl alcohol

Sensor conductance
(Acetone)

Sensor 1

Sensor 2

0.9

5

5

0.8

5

0.7

A3

0.6

10

10

15

15

10

0.5

A2

0.4
0.3

15

0.2

A1

0.1
500

1000

1500

A1

20
2000

2500

3000

5

15

5

20

10

15

A3

20

20

2

4

6

8

10

12

14

16

2

4

6

8

10

12

14

16

2

4

6

8

10

12

14

16

18

20

0.9
0.8

5

5

5

0.7

B3

0.6

10

10

0.5

10

0.4

B2

0.3
0.2

15

15

15

B1

0.1

B1

20
500

1000

1500

2000

2500

B2

20

10

15

5

20

10

15

B3

20

3000

5

Sensor Conductance
(Ammonia)

10

A2

20

20

18

20

0.9

5

0.8

5
5

C3

0.7
0.6

10

10

15

15

C2

0.5

10

0.4
0.3
0.2

15

C1

0.1

C1

20
500

1000

1500

2000

2500

3000

5

10

15

20

C2

20

5

10

15

Pseudo-Sensors

Concentration

(a)

(b)

20

C3

20

18

20

Figure 1. (a) Temperature modulated response to the three analytes (A,B,C) at three
concentrations (A3: highest concentration of A), and (b) initial GL maps.
The sensor response to the highest concentration of each analyte was used to
generate the SOM convergence map. Figure 1(b) shows the initial odor map of the
three analytes following conscience learning of the SOM. These olfactory images
show that the identity of the stimulus is encoded by the spatial pattern across the
lattice, whereas the intensity is encoded by the overall amplitude of this pattern.

Analytes A and B, which induce similar responses on the MOS sensors, also lead to
very similar GL maps.
The GL maps are input to the lattice of spiking neurons for further processing. As a
result of the dynamics induced by the recurrent connections, these initial maps are
transformed into a spatio-temporal pattern. Figure 2 shows the projection of
membrane potential of the 400 M cells along their first three principal components.
Three trajectories are shown per analyte, which correspond to the sensor response to
the highest analyte concentration on three separate days of data collection. These
results show that the spatio-temporal pattern is robust to the inherent drift of
chemical sensors. The trajectories originate close to each other, but slowly migrate
and converge into unique odor-specific attractors. It is important to note that these
trajectories do not diverge indefinitely, but in fact settle into an attractor, as
illustrated by the insets in Figure 2.

Odor B
20
15
10
5

Odor C

0
-5
-10
-15
-200
-150
-100
-50

100
50

0
0
-50

50
-100
100

-150
-200
150

-250

Odor A

Figure 2. Odor-specific attractors from experimental sensor data. Three trajectories
are shown per analyte, corresponding to the sensor response on three separate days.
These results show that the attractors are repeatable and robust to sensor drift.
To illustrate the coding of identity and intensity performed by the model, Figure 3
shows the trajectories of the three analytes at three concentrations. The OB network
activity evolves to settle into an attractor, where the identity of the stimulus is
encoded by the direction of the trajectory relative to the initial position, and the
intensity is encoded by the length along the trajectory. This emerging code is also
consistent with recent findings in neurobiology, as discussed next.

5

D i s c u s s i on

A recent study of spatio-temporal activity in projection neurons (PN) of the
honeybee antennal lobe (analogous to M cells in mammalian OB) reveals evolution
and convergence of the network activity into odor-specific attractors [4]. Figure
4(a) shows the projection of the spatio-temporal response of the PNs along their
first three principal components. These trajectories begin close to each other, and
evolve over time to converge into odor specific regions. These experimental results
are consistent with the attractor patterns emerging from our model. Furthermore, an
experimental study of odor identity and intensity coding in the locust show

hierarchical groupings of spatio-temporal PN activity according to odor identity,
followed by odor intensity [5]. Figure 4(b) illustrates this grouping in the activity
of 14 PNs when exposed to three odors at five concentrations. Again, these results
closely resemble the grouping of attractors in our model, shown in Figure 3.

B3
B2
B1
200

A3

150

A2

PC3

A1
100

50

C1
C2
C3

0

50

-50

0

350
-50

300
250

-100
200

PC2

-150

150
100

-200
50

PC1

-250

0
-50

-300

Figure 3. Identity and intensity coding using dynamic attractors.
Previous studies by Pearce et al. [6] using a large population of optical micro-bead
chemical sensors have shown that massive convergence of sensory inputs can be
used to provide sensory hyperacuity by averaging out uncorrelated noise. In
contrast, the focus of our work is on the coding properties induced by chemotopic
convergence. Our model produces an initial spatial pattern or olfactory image,
whereby odor identity is coded by the spatial activity across the GL lattice, and odor
intensity is encoded by the amplitude of this pattern. Hence, the bulk of the
identity/intensity coding is performed by this initial convergence primitive.
Subsequent processing by a lattice of spiking neurons introduces time as an
additional coding dimension. The initial spatial maps are transformed into a spatiotemporal pattern by means of center on-off surround lateral connections. Excitatory
lateral connections allow the model to spread M cell activity, and are responsible for
moving the attractors away from their initial coordinates. In contrast, inhibitory
connections ensure that these trajectories eventually converge onto an attractor,
rather than diverge indefinitely. It is the interplay between excitatory and inhibitory
connections that allows the model to enhance the initial coding produced by the
chemotopic convergence mapping.
(b)

(a)

octanol

hexanol

nonanol

isoamylacetate

Figure 4. (a) Odor trajectories formed by spatio-temporal activity in the honeybee
AL (adapted from [4]). (b) Identity and intensity clustering of spatio-temporal
activity in the locust AL (adapted from [5]; arrows indicate the direction of
increasing concentration).

At present, our model employs a center on-off surround kernel that is constant
throughout the lattice. Further improvements can be achieved through adaptation of
these lateral connections by means of Hebbian and anti-Hebbian learning. These
extensions will allow us to investigate additional computational functions (e.g.,
pattern completion, orthogonalization, coding of mixtures) in the processing of
information from chemosensor arrays.
Acknowledgments
This material is based upon work supported by the National Science Foundation
under CAREER award 9984426/0229598. Takao Yamanaka, Alexandre PereraLluna and Agustin Gutierrez-Galvez are gratefully acknowledged for valuable
suggestions during the preparation of this manuscript.
References
[1]
[2]
[3]
[4]
[5]
[6]

[7]
[8]
[9]
[10]
[11]
[12]
[13]
[14]
[15]

Gutierrez-Osuna, R. (2002) Pattern Analysis for Machine Olfaction: A Review. IEEE
Sensors Journal 2(3): 189-202.
Pearce, T. C. (1999) Computational parallels between the biological olfactory pathway and
its analogue ?The Electronic Nose?: Part I. Biologiacal olfaction. BioSystems 41: 43-67.
Laurent, G. (1999) A Systems Perspective on Early Olfactory Coding. Science 286(22):
723-728.
Gal?n, R. F.,Sachse, S., Galizia, C.G., & Herz, A.V. (2003) Odor-driven attractor dynamics
in the antennal lobe allow for simple and rapid olfactory pattern classification. Neural
Computation 16(5): 999-1012.
Stopfer, M., Jayaraman, V., & Laurent, G. (2003) Intensity versus Identity Coding in an
Olfactory System. Neuron 39: 991-1004.
Pearce, T.C., Verschure, P.F.M.J., White, J., & Kauer, J. S. (2001) Robust Stimulus
Encoding in Olfactory Processing: Hyperacuity and Efficient Signal Transmission. In S.
Wermter, J. Austin and D. Willshaw (Eds.), Emergent Neural Computation Architectures
Based on Neuroscience. pp. 461-479. Springer-Verlag.
Lee. A. P., & Reedy, B. J. (1999) Temperature modulation in semiconductor gas sensing.
Sensors and Actuators B 60: 35-42.
Vassar, R., Chao, S.K., Sitcheran, R., Nunez, J. M., Vosshall, L.B., & Axel, A. (1994)
Topographic Organization of Sensory Projections to the Olfactory Bulb. Cell 79(6): 981991.
Gutierrez-Osuna, R. (2002) A Self-organizing Model of Chemotopic Convergence for
Olfactory Coding. In Proceedings of the 2nd EMBS-BMES Conference, pp. 23-26. Texas.
Mori, K., Nagao, H., & Yoshihara, Y. (1999) The Olfactory Bulb: Coding and Processing
of Odor molecule information. Science 286: 711-715.
Kohonen, T. (1982) Self-organized formation of topologically correct feature maps.
Biological Cybernetics 43: 59-69.
DeSieno, D. (1988) Adding conscience to competitive learning. In Proceedings of
International Conference on Neural Networks (ICNN), pp. 117-124. Piscataway, NJ.
Laaksonen, J., Koskela, M., & Oja, E. (2003) Probability interpretation of distributions on
SOM surfaces. In Proceedings of Workshop on Self-Organizing Maps. Hibikino, Japan.
Aungst et al. (2003) Center-surround inhibition among olfactory bulb glomeruli. Nature 26:
623- 629.
Gerstner, W., & Kistler, W. (2002) Spiking Neuron Models: Single Neurons, Populations,
Plasticity. Cambridge, University Press.


<<----------------------------------------------------------------------------------------------------------------------------------------->>

