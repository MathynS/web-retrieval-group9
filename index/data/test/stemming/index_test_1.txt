query sentence: weakest link in subnetworks
---------------------------------------------------------------------
title: 3491-measures-of-clustering-quality-a-working-set-of-axioms-for-clustering.pdf

Measures of Clustering Quality: A Working Set of
Axioms for Clustering

Margareta Ackerman and Shai Ben-David
School of Computer Science
University of Waterloo, Canada

Abstract
Aiming towards the development of a general clustering theory, we discuss abstract axiomatization for clustering. In this respect, we follow up on the work of
Kleinberg, ([1]) that showed an impossibility result for such axiomatization. We
argue that an impossibility result is not an inherent feature of clustering, but rather,
to a large extent, it is an artifact of the specific formalism used in [1].
As opposed to previous work focusing on clustering functions, we propose to
address clustering quality measures as the object to be axiomatized. We show that
principles like those formulated in Kleinberg?s axioms can be readily expressed in
the latter framework without leading to inconsistency.
A clustering-quality measure (CQM) is a function that, given a data set and its partition into clusters, returns a non-negative real number representing how strong or
conclusive the clustering is. We analyze what clustering-quality measures should
look like and introduce a set of requirements (axioms) for such measures. Our
axioms capture the principles expressed by Kleinberg?s axioms while retaining
consistency.
We propose several natural clustering quality measures, all satisfying the proposed
axioms. In addition, we analyze the computational complexity of evaluating the
quality of a given clustering and show that, for the proposed CQMs, it can be
computed in polynomial time.

1

Introduction

In his highly influential paper, [1], Kleinberg advocates the development of a theory of clustering that
will be ?independent of any particular algorithm, objective function, or generative data model.? As a
step in that direction, Kleinberg sets up a set of ?axioms? aimed to define what a clustering function
is. Kleinberg suggests three axioms, each sounding plausible, and shows that these seemingly natural
axioms lead to a contradiction - there exists no function that satisfies all three requirements.
Kleinberg?s result is often interpreted as stating the impossibility of defining what clustering is, or
even of developing a general theory of clustering. We disagree with this view. In this paper we show
that the impossibility result is, to a large extent, due to the specific formalism used by Kleinberg
rather than being an inherent feature of clustering.
Rather than attempting to define what a clustering function is, and demonstrating a failed attempt,
as [1] does, we turn our attention to the closely related issue of evaluating the quality of a given
data clustering. In this paper we develop a formalism and a consistent axiomatization of that latter
notion.
As it turns out, the clustering-quality framework is richer and more flexible than that of clustering
functions. In particular, it allows the postulation of axioms that capture the features that Kleinberg?s
axioms aim to express, without leading to a contradiction.
1

A clustering-quality measure is a function that maps pairs of the form (dataset, clustering) to
some ordered set (say, the set of non-negative real numbers), so that these values reflect how ?good?
or ?cogent? that clustering is.
Measures for the quality of a clusterings are of interest not only as a vehicle for axiomatizing clustering. The need to measure the quality of a given data clustering arises naturally in many clustering
issues. The aim of clustering is to uncover meaningful groups in data. However, not any arbitrary
partitioning of a given data set reflects such a structure. Upon obtaining a clustering, usually via
some algorithm, a user needs to determine whether this clustering is sufficiently meaningful to rely
upon for further data mining analysis or practical applications. Clustering-quality measures (CQMs)
aim to answer that need by quantifying how good is any specific clustering.
Clustering-quality measures may also be used to help in clustering model-selection by comparing
different clusterings over the same data set (e.g., comparing the results of a given clustering paradigm
over different choices of clustering parameters, such as the number of clusters).
When posed with the problem of finding a clustering-quality measure, a first attempt may be to
invoke the loss (or objective) function used by a clustering algorithm, such as k-means or k-median,
as a clustering-quality measure. However, such measures have some shortcomings for the purpose
at hand. Namely, these measures are usually not scale-invariant, and they cannot be used to compare
the quality of clusterings obtained by different algorithms aiming to minimize different clustering
costs (e.g., k-means with different values of k). See Section 6 for more details.
Clustering quality has been previously discussed in the applied statistics literature, where a variety
of techniques for evaluating ?cluster validity? were proposed. Many of these methods, such as the
external criteria discussed in [2], are based on assuming some predetermined data generative model,
and as such do not answer our quest for a general theory of clustering. In this work, we are concerned
with quality measures regardless of any specific generative model, for examples, see the internal
criteria surveyed in [2].
We formulate a theoretical basis for clustering-quality evaluations. We propose a set of requirements (?axioms?) of clustering-quality measures. We demonstrate the relevance and consistency of
these axioms by showing that several natural notions satisfy these requirements. In particular, we
introduce quality-measures that reflect the underlying intuition of center-based and linkage-based
clustering. These notions all satisfy our axioms, and, given a data clustering, their value on that
clustering can be computed in polynomial time.
Paper outline: we begin by presenting Kleinberg?s axioms for clustering functions and discuss their
failure. In Section 4.3 we show how these axioms can be translated into axioms pertaining clustering
quality measures, and prove that the resulting set of axioms is consistent. In Section 4, we discuss
desired properties of an axiomatization and propose an accordingly revised set of axioms. Next, in
Section 5 we present several clustering-quality measures, and claim that they all satisfy our axioms.
Finally, in Section 5.3, we show that the quality of a clustering can be computed in polynomial time
with respect to our proposed clustering-quality measures.

2

Definitions and Notation

Let X be some domain set (usually finite). A function d : X ? X ? R is a distance function over
X if d(xi , xi ) ? 0 for all xi ? X, for any xi , xj ? X, d(xi , xj ) > 0 if and only if xi 6= xj , and
d(xi , xj ) = d(xj , xi ) otherwise. Note that we do not require the triangle inequality.
A k-clustering of X is a k-partition, C = {C1 , C2 , . . . , Ck }. That is, Ci ? Cj = ? for i 6= j and
?ki=1 Ci = X. A clustering of X is a k-clustering of X for some k ? 1. A clustering is trivial if
each of its clusters contains just one point, or if it consists of just one cluster.
For x, y ? X and clustering C of X, we write x ?C y whenever x and y are in the same cluster of
clustering C and x 6?C y, otherwise.
A clustering function for some domain set X is a function that takes a distance function d over X,
and outputs a clustering of X.
2

A clustering-quality measure (CQM) is a function that is given a clustering C over (X, d) (where
d is a distance function over X) and returns a non-negative real number, as well as satisfies some
additional requirements. In this work we explore the question of what these requirements should be.

3

Kleinberg?s Axioms

Kleinberg, [1], proposes the following three axioms for clustering functions. These axioms are
intended to capture the meaning of clustering by determining which functions (from a domain set
endowed with a distance function) are worthy of being considered clustering functions and which
are not. Kleinberg shows that the set is inconsistent - there exist no functions that satisfies all three
axioms.
The first two axioms require invariance of the clustering that f defines under some changes of the
input distance function.
Function Scale Invariance: Scale invariance requires that the output of a clustering function be
invariant to uniform scaling of the input.
A function f is scale-invariant if for every distance function d and positive ?, f (d) = f (?d) (where
?d is defined by setting, for every pair of domain points x, y, ?d(x, y) = ? ? d(x, y)).
Function Consistency: Consistency requires that if within-cluster distances are decreased, and
between-cluster distances are increased, then the output of a clustering function does not change.
Formally,
? Given a clustering C over (X, d), a distance function d0 is a C-consistent variant of d, if
d0 (x, y) ? d(x, y) for all x ?C y, and d0 (x, y) ? d(x, y) for all x 6?C y.
? A function f is consistent if f (d) = f (d0 ) whenever d0 is an f (d)-consistent variant of d.
Function Richness: Richness requires that by modifying the distance function, any partition of the
underlying data set can be obtained.
A function f is rich if for each partitioning, C, of X, there exists a distance function d over X so
that f (d) = C.
Theorem 1 (Kleinberg, [1]) There exists no clustering function that simultaneously satisfies scale
invariance, consistency and richness.
Discussion: The intuition behind these axioms is rather clear. Let us consider, for example, the
Consistency requirement. It seems reasonable that by pulling closer points that are in the same
cluster and pushing further apart points in different clusters, our confidence in the given clustering
will only rise. However, while this intuition can be readily formulated in terms of clustering quality
(namely, ?changes as these should not decrease the quality of a clustering?), the formulation through
clustering functions says more. It actually requires that such changes to the underlying distance
function should not create any new contenders for the best-clustering of the data.
For example, consider Figure 1, where we illustrate a good 6-clustering. On the right hand-side, we
show a consistent change of this 6-clustering. Notice that the resulting data has a 3-clustering that is
reasonably better than the original 6-clustering. While one may argue that the quality of the original
6-clustering has not decreased as a result of the distance changes, the quality of the 3-clustering has
improved beyond that of the 6-clustering. This illustrates a significant weakness of the consistency
axiom for clustering functions.
The implicit requirement that the original clustering remains the best clustering following a consistent change is at the heart of Kleinberg?s impossibility result. As we shall see below, once we relax
that extra requirement the axioms are no longer unsatisfiable.

4

Axioms of Clustering-Quality Measures

In this section we change the primitive that is being defined by the axioms from clustering functions
to clustering-quality measures (CQM). We reformulate the above three axioms in terms of CQMs
3

Figure 1: A consistent change of a 6-clustering.
and show that this revised formulation is not only consistent, but is also satisfied by a number of
natural clustering quality measures. In addition, we extend the set of axioms by adding another
axiom (of clustering-quality measures) that is required to rule out some measures that should not be
counted as CQMs.
4.1

Clustering-Quality Measure Analogues to Kleinberg?s Axioms

The translation of the Scale Invariance axiom to the CQM terminology is straightforward:
Definition 1 (Scale Invariance) A quality measure m satisfies scale invariance if for every clustering C of (X, d), and every positive ?, m(C, X, d) = m(C, X, ?d).
The translation of the Consistency axiom is the place where the resulting CQM formulation is indeed
weaker than the original axiom for functions. While it clearly captures the intuition that consistent
changes to d should not hurt the quality of a given partition, it allows the possibility that, as a result
of such a change, some partitions will improve more than others1 .
Definition 2 (Consistency) A quality measure m satisfies consistency if for every clustering C over
(X, d), whenever d0 is a C consistent variant of d, then m(C, X, d0 ) ? m(C, X, d).
Definition 3 (Richness) A quality measure m satisfies richness if for each non-trivial clustering C
of X, there exists a distance function d over X such that C = Argmax{m(C, X, d)}.
Theorem 2 Consistency, scale invariance, and richness for clustering-quality measures form a consistent set of requirements.
Proof: To show that scale-invariance, consistency, and richness form a consistent set of axioms, we
present a clustering quality measure that satisfies the three axioms. This measure captures a quality
that is intuitive for center-based clusterings. In Section 5, we introduce more quality measures that
capture the goal of other types of clusterings. All of these CQM?s satisfy the above three axioms.
For each point in the data set, consider the ratio of the distance from the point to its closest center to
the distance from the point to its second closest center. Intuitively, the smaller this ratio is, the better
the clustering (points are ?more confident? about their cluster membership). We use the average of
this ratio as a quality measure.
Definition 4 (Relative Point Margin) The K-Relative Point Margin of x ? X is K-RMX,d (x) =
d(x,cx )
0
d(x,cx0 ) , where cx ? K is the closest center to x, cx ? K is a second closest center to x, and
K ? X.
1
The following formalization assumes that larger values of m indicate better clustering quality. For some
quality measures, smaller values indicate better clustering quality - in which case we reverse the direction of
inequalities for consistency and use Argmin instead of Argmax for richness.

4

A set K is a representative set of a clustering C if it consists of exactly one point from each cluster
of C.
Definition 5 (Representative Set) A set K is a representative set of clustering C
{C1 , C2 , . . . , Ck } if |K| = k and for all i, K ? Ci 6= ?.

=

Definition 6 (Relative Margin) The Relative Margin of a clustering C over (X, d) is
RMX,d (C) =

min

K is a representative set of C

avgx?X\K K-RMX,d (x).

Smaller values of Relative Margin indicate better clustering quality.
Lemma 1 Relative Margin is scale-invariant.
proof: Let C be a clustering of (X, d). Let d0 be a distance function so that d0 (x, y) = ?d(x, y)
d0 (x,y)
for all x, y ? X and some ? ? R+ . Then for any points x, y, z ? X, d(x,y)
d(x,z) = d0 (x,z) . Note also
that scaling does not change the centers selected by Relative Margin. Therefore, RMX,d0 (C) =
RMX,d (C).
Lemma 2 Relative Margin is consistent.
proof: Let C be a clustering of distance function (X, d). Let d0 be a C consistent variant of d. Then
for x ?C y, d0 (x, y) ? d(x, y) and for x 6?C y, d0 (x, y) ? d(x, y). Therefore, RMX,d0 (C) ?
RMX,d (C).
Lemma 3 Relative Margin is rich.
proof: Given a non-trivial clustering C over a data set X, consider the distance function d where
d(x, y) = 1 for all x ?C y, and d(x, y) = 10 for all x 6?C y. Then C = Argmin{m(C, X, d)}.
It follows that scale-invariance, consistency, and richness are consistent axioms.
4.2

Soundness and Completeness of Axioms

What should a set of ?axioms for clustering? satisfy? Usually, when a set of axioms is proposed
for some semantic notion (or a class of objects, say clustering functions), the aim is to have both
soundness and completeness. Soundness means that every element of the described class satisfies
all axioms (so, in particular, soundness implies consistency of the axioms), and completeness means
that every property shared by all objects of the class is implied by the axioms. Intuitively, ignoring
logic subtleties, a set of axioms is complete for a class of objects if any element outside that class
fails at least one of these axioms.
In our context, there is a major difficulty - there exist no semantic definition of what clustering is.
We wish to use the axioms as a definition of clustering functions, but then what is the meaning of
soundness and completeness? We have to settle for less. While we do not have a clear definition of
what is clustering and what is not, we do have some examples of functions that should be considered
clustering functions, and we can come up with some examples of partitionings that are clearly not
worthy of being called ?clustering?. We replace soundness by the requirement that all of our axioms
are satisfied by all these examples of common clustering functions (relaxed soundness), and we want
that partitioning functions that are clearly not clusterings fail at least one of our axioms (relaxed
completeness).
In this respect, the axioms of [1] badly fail (the relaxed version of) soundness. For each of these
axioms there are natural clustering functions that fail to satisfy it (this is implied by Kleinberg?s
demonstration that any pair of axioms is satisfied by a natural clustering, while the three together
never hold). We argue that our scale invariance, consistency, and richness, are sound for the class
of CQMs. However, they do not make a complete set of axioms, even in our relaxed sense. There
are functions that should not be considered ?reasonable clustering quality measures? and yet they
satisfy these three axioms. One type of ?non-clustering-functions? are functions that make cluster
membership decisions based on the identity of domain points. For example, the function that returns
5

the Relative Margin of a data set whenever some specific pair of data points belong to the same
cluster, and twice the Relative Margin of the data set otherwise. We overcome this problem by
introducing a new axiom.
4.3

Isomorphism Invariance

This axiom resembles the permutation invariance objective function axiom by Puzicha et al. [3],
modeling the requirement that clustering should be indifferent to the individual identity of clustered elements. This axiom of clustering-quality measures does not have a corresponding Kleinberg
axiom.
Definition 7 (Clustering Isomorphism) Two clusterings C and C 0 over the same domain, (X, d),
are isomorphic, denoted C ?d C 0 , if there exists a distance-preserving isomorphism ? : X ? X,
such that for all x, y ? X, x ?C y if and only if ?(x) ?C 0 ?(y).
Definition 8 (Isomorphism Invariance) A quality measure m is isomorphism -invariant if for all
clusterings C, C 0 over (X, d) where C ?d C 0 , m(C, X, d) = m(C 0 , X, d).
Theorem 3 The set of axioms consisting of Isomorphism Invariance, Scale Invariance, Consistency,
and Richness, (all in their CQM formulation) is a consistent set of axioms.
Proof: Just note that the Relative Margin quality measure satisfies all four axioms.
As mentioned in the above discussion, to have a satisfactory axiom system, for any notion, one needs
to require more than just consistency. To be worthy of being labeled ?axioms?, the requirements we
propose should be satisfied by any reasonable notion of CQM. Of course, since we cannot define
what CQMs are ?reasonable?, we cannot turn this into a formal statement. What we can do, however,
is demonstrate that a variety of natural CQMs do satisfy all our axioms. This is done in the next
section.

5

Examples of Clustering Quality Measures

In a survey of validity measures, Milligan [2] discusses examples of quality measures that satisfy
our axioms (namely, scale-invariance, consistency, richness, and perturbation invariance). We have
verified that the best performing internal criteria examined in [2], satisfy all our axioms.
In this section, we introduce two novel QCMs; a measure that reflects the underlying intuition of
linkage-based clustering, and a measure for center-based clustering. In addition to satisfying the
axioms, given a clustering, these measures can computed in polynomial time.
5.1

Weakest Link

In linkage-based clustering, whenever a pair of points share the same cluster they are connected via
a tight chain of points in that cluster. The weakest link quality measure focuses on the longest link
in such a chain.
Definition 9 (Weakest Link Between Points)
C-W LX,d (x, y) =

min

x1 ,x2 ,...,x` ?Ci

(max(d(x, x1 ), d(x1 , x2 ), . . . , d(x` , y))),

where C is a clustering over (X, d) and Ci is a cluster in C.
The weakest link of C is the maximal value of C-W LX,d (x, y) over all pairs of points belonging to
the same cluster, divided by the shortest between-cluster distance.
Definition 10 (Weakest Link of C) The Weakest Link of a clustering C over (X, d) is
W L(C) =

maxx?C y C-W LX,d (x, y)
.
minx6?C y d(x, y)

The range of values of weakest link is (0, ?).
6

5.2

Additive Margin

In Section 4.3, we introduced Relative Margin, a quality measure for center-based clustering. We
now introduce another quality measure for center-based clustering. Instead of looking at ratios,
Additive Margin evaluates differences.
Definition 11 (Additive Point Margin) The K-Additive Point Margin of x is K-AMX,d (x) =
d(x, cx0 ) ? d(x, cx ), where cx ? K is the closest center to x, cx0 ? K is a second closest center to x, and K ? X.
The Additive Margin of a clustering is the average Additive Point Margin, divided by the average
within-cluster distance. The normalization is necessary for scale invariance.
Definition 12 (Additive Margin) The Additive Margin of a center-based clustering C over (X, d)
is
P
1
x?X K-AMX,d (x)
|X|
P
AMX,d (C) =
min
.
1
K is a representative set of C
x?C y d(x, y)
|{{x,y}?X|x?C y}|
Unlike Relative Margin, Additive Margin gives higher values to better clusterings.
5.3

Computational complexity

For a clustering-quality measure to be useful, it is important to be able to quickly compute the quality
of a clustering using that measure. The quality of a clustering using the measures presented in this
paper can be computed in polynomial time in terms of n (the number of points in the data set).
Using relative or Additive Margin, it takes O(nk+1 ) operations to compute the clustering quality
of a data set, which is exponential in k. If a set of centers is given, the Relative Margin can be
computed in O(nk) operations and the Additive Margin can be computed in O(n2 ) operations. The
weakest link of a clustering can be computed in O(n3 ) operations.
5.4

Variants of quality measures

Given a clustering-quality measure, we can construct new quality measures with different characteristics by applying the quality measure on a subset of clusters. It suffices to consider a quality
measure m that is defined for clusterings consisting of 2 clusters. Given such measure, we can
create new quality measures. For example,
mmin (C, X, d) =

min

m(S, X, d),

S?C,|S|=2

measures the worst quality of a pair of clusters in C.
Alternately, we can define, mmax (C, X, d) and mavg (C, X, d), which evaluate the best or average
quality of a pair of clusters in C. A nice feature of these variations is that if m satisfies the four
axioms of clustering-quality measures then so do mmin , mmax , and mavg .
More generally, if m is defined for clusterings on an arbitrary number of clusters, we can define a
quality measure as a function of m over larger clusterings. For example, mmax subset (C, X, d) =
maxS?C,|S|?2 m(S, X, d). Many such variations, which apply existing clustering-quality measures
on subsets of clusters, satisfy the axioms of clustering-quality measures whenever the original quality measure satisfies the axioms.

6

Dependence on Number of Clusters

The clustering-quality measures discussed in this paper up to now are independent of the number
of clusters, which enables the comparison of clusterings with a different number of clusters. In this
section we discuss an alternative type of clustering quality evaluation, that depends on the number of
clusters. Such quality measures arise naturally from common loss functions (or, objective functions)
that drive clustering algorithm, such as k-means or k-median.
7

These common loss functions fail to satisfy two of our axioms, scale-invariance and richness. One
can easily overcome the dependence on scaling by normalization. As we will show, the resulting
normalized loss functions make a different type of clustering-quality measures from the measures
we previously discussed, due to their dependence on the number of clusters.
A natural remedy to the failure of scale invariance is to normalize a loss function by dividing it by
the variance of the data, or alternatively, by the loss of the 1-clustering of the data.
Definition 13 (L-normalization) The L-normalization of a clustering C over (X, d) is
L-normalize(C, X, d) =

L(Call , X, d)
.
L(C, X, d)

where Call denotes the 1-clustering of X.
Common loss functions, even after normalization, usually have a bias towards either more refined
or towards more coarse clusterings ? they assign lower cost (that is, higher quality) to more refined
(respectively, coarse) clusterings. This prevents using them as a meaningful tool for comparing
the quality of clusterings with different number of clusters. We formalize this feature of common
clustering loss functions through the notion of refinement preference:
Definition 14 (Refinement and coarsening) For a pair of clusterings C, C 0 of the same domain,
we say C 0 is a refinement of C (or, equivalently, that C is a coarsening of C 0 ) if for every cluster Ci
of C, Ci is a union of clusters of C 0 .
Definition 15 (Refinement/Coarsening Preference) A measure m is refinement-preferring if for
every clustering C of (X, d) if it has a non-trivial refinement, then there exists such a refinement C 0 of
C for which m(C 0 , X, d) > m(C, X, d). Coarsening-preferring measures are defined analogously.
Note that both refinement preferring and coarsening preferring measures fail to satisfy the Richness
axiom.
It seems that there is a divide between two types of evaluations for clusterings; those that satisfy
richness, and those that satisfy either refinement or coarsening preference. To evaluate the quality of
a clustering using a refinement (and coarsening) preferring measure, it is essential to fix the number
of clusters. Since the correct number of clusters is often unknown, measures that are independent of
the number of clusters apply in a more general setting.

7

Conclusions

We have investigated the possibility of providing a general axiomatic basis for clustering. Our
starting point was the impossibility result of Kleinberg. We argue that a natural way to overcome
these negative conclusions is by changing the primitive used to formulate the axioms from clustering
functions to clustering quality measures (CQMs). We demonstrate the merits of the latter framework
by providing a set of axioms for CQMs that captures the essence of all of Kleinberg?s axioms while
maintaining consistency. We propose several CQMs that satisfy our proposed set of axioms. We
hope that this work, and our demonstration of a way to overcome the ?impossibility result? will
stimulate further research towards a general theory of clustering.

References
[1] Jon Kleinberg. ?An Impossibility Theorem for Clustering.? Advances in Neural Information Processing
Systems (NIPS) 15, 2002.
[2] Glen W. Milligan. ?A Monte-Carlo study of 30 internal criterion measures for cluster-analysis.? Psychometrica 46, 187-195, 1981.
[3] J. Puzicha, T. Hofmann, and J. Buhmann. ?Theory of Proximity Based Clustering: Structure Detection by
Optimization,? Pattern Recognition, 33(2000).

8


----------------------------------------------------------------

title: 4659-topic-partitioned-multinetwork-embeddings.pdf

Topic-Partitioned Multinetwork Embeddings

Peter Krafft?
CSAIL
MIT
pkrafft@mit.edu

?

Juston Moore? , Bruce Desmarais? , Hanna Wallach?
Department of Computer Science, ? Department of Political Science
University of Massachusetts Amherst
?
{jmoore, wallach}@cs.umass.edu
?
desmarais@polsci.umass.edu

Abstract
We introduce a new Bayesian admixture model intended for exploratory analysis of communication networks?specifically, the discovery and visualization of
topic-specific subnetworks in email data sets. Our model produces principled visualizations of email networks, i.e., visualizations that have precise mathematical
interpretations in terms of our model and its relationship to the observed data.
We validate our modeling assumptions by demonstrating that our model achieves
better link prediction performance than three state-of-the-art network models and
exhibits topic coherence comparable to that of latent Dirichlet allocation. We
showcase our model?s ability to discover and visualize topic-specific communication patterns using a new email data set: the New Hanover County email network.
We provide an extensive analysis of these communication patterns, leading us to
recommend our model for any exploratory analysis of email networks or other
similarly-structured communication data. Finally, we advocate for principled visualization as a primary objective in the development of new network models.

1

Introduction

The structures of organizational communication networks are critical to collaborative problem solving [1]. Although it is seldom possible for researchers to directly observe complete organizational
communication networks, email data sets provide one means by which they can at least partially observe and reason about them. As a result?and especially in light of their rich textual detail, existing
infrastructure, and widespread usage?email data sets hold the potential to answer many important scientific and practical questions within the organizational and social sciences. While some
questions may be answered by studying the structure of an email network as a whole, other, more
nuanced, questions can only be answered at finer levels of granularity?specifically, by studying
topic-specific subnetworks. For example, breaks in communication (or duplicated communication)
about particular topics may indicate a need for some form of organizational restructuring. In order to
facilitate the study of these kinds of questions, we present a new Bayesian admixture model intended
for discovering and summarizing topic-specific communication subnetworks in email data sets.
There are a number of probabilistic models that incorporate both network and text data. Although
some of these models are specifically for email networks (e.g., McCallum et al.?s author?recipient?
topic model [2]), most are intended for networks of documents, such as web pages and the links
between them [3] or academic papers and their citations [4]. In contrast, an email network is more
naturally viewed as a network of actors exchanging documents, i.e., actors are associated with nodes
while documents are associated with edges. In other words, an email network defines a multinetwork
in which there may be multiple edges (one per email) between any pair of actors. Perhaps more
importantly, much of the recent work on modeling networks and text has focused on tasks such as
?

Work done at the University of Massachusetts Amherst

1

=

+

+

Figure 1: Our model partitions an observed email network (left) into topic-specific subnetworks
(right) by associating each author?recipient edge in the observed network with a single topic.
predicting links or detecting communities. Instead, we take a complementary approach and focus on
exploratory analysis. Specifically, our goal is to discover and visualize topic-specific subnetworks.
Rather than taking a two-stage approach in which subnetworks are discovered using one model and
visualized using another, we present a single probabilistic model that partitions an observed email
network into topic-specific subnetworks while simultaneously producing a visual representation of
each subnetwork. If network modeling and visualization are undertaken separately, the resultant visualizations may not directly reflect the model and its relationship to the observed data. Rather, these
visualizations provide a view of the model and the data seen through the lens of the visualization
algorithm and its associated assumptions, so any conclusions drawn from such visualizations can be
biased by artifacts of the visualization algorithm. Producing principled visualizations of networks,
i.e., visualizations that have precise interpretations in terms of an associated network model and its
relationship to the observed data, remains an open challenge in statistical network modeling [5].
Addressing this open challenge was a primary objective in the development of our new model.
In order to discover and visualize topic-specific subnetworks, our model must associate each author?
recipient edge in the observed email network with a topic, as shown in Figure 1. Our model draws
upon ideas from latent Dirichlet allocation (LDA) [6] to identify a set of corpus-wide topics of
communication, as well as the subset of topics that best describe each observed email. We model
network structure using an approach similar to that of Hoff et al.?s latent space model (LSM) [7] so
as to facilitate visualization. Given an observed network, LSM associates each actor in the network
with a point in K-dimensional Euclidean space. For any pair of actors, the smaller the distance
between their points, the more likely they are to interact. If K = 2 or K = 3, these interaction
probabilities, collectively known as a ?communication pattern?, can be directly visualized in 2- or
3-dimensional space via the locations of the actor-specific points. Our model extends this idea by
associating a K-dimensional Euclidean space with each topic. Observed author?recipient edges are
explicitly associated with topics via the K-dimensional topic-specific communication patterns.
In the next section, we present the mathematical details of our new model and outline a corresponding inference algorithm. We then introduce a new email data set: the New Hanover County (NHC)
email network. Although our model is intended for exploratory analysis, we test our modeling assumptions via three validation tasks. In Section 4.1, we show that our model achieves better link
prediction performance than three state-of-the-art network models. We also demonstrate that our
model is capable of inferring topics that are as coherent as those inferred using LDA. Together,
these experiments indicate that our model is an appropriate model of network structure and that
modeling this structure does not compromise topic quality. As a final validation experiment, we
show that synthetic data generated using our model possesses similar network statistics to those of
the NHC email network. In Section 4.4, we showcase our model?s ability to discover and visualize
topic-specific communication patterns using the NHC network. We give an extensive analysis of
these communication patterns and demonstrate that they provide accessible visualizations of emailbased collaboration while possessing precise, meaningful interpretations within the mathematical
framework of our model. These findings lead us to recommend our model for any exploratory analysis of email networks or other similarly-structured communication data. Finally, we advocate for
principled visualization as a primary objective in the development of new network models.

2

Topic-Partitioned Multinetwork Embeddings

In this section, we present our new probabilistic generative model (and associated inference algorithm) for communication networks. For concreteness, we frame our discussion of this model in
2

terms of email data, although it is generally applicable to any similarly-structured communication
data. The generative process and graphical model are provided in the supplementary materials.
(d)

(d)

A single email, indexed by d, is represented by a set of tokens w(d) = {wn }N
n=1 that comprise the
text of that email, an integer a(d) ? {1, ..., A} indicating the identity of that email?s author, and a
(d)
set of binary variables y (d) = {yr }A
r=1 indicating whether each of the A actors in the network is
a recipient of that email. For simplicity, we assume that authors do not send emails to themselves
(d)
(i.e., yr = 0 if r = a(d) ). Given a real-world email data set D = {{w(d) , a(d) , y (d) }}D
d=1 , our
model permits inference of the topics expressed in the text of the emails, a set of topic-specific
K-dimensional embeddings (i.e., points in K-dimensional Euclidean space) of the A actors in the
network, and a partition of the full communication network into a set of topic-specific subnetworks.
As in LDA [6], a ?topic? t is characterized by a discrete distribution over V word types with probability vector ?(t) . A symmetric Dirichlet prior with concentration parameter ? is placed over
? = {?(1) , ..., ?(T ) }. To capture the relationship between the topics expressed in an email and
that email?s recipients, each topic t is also associated with a ?communication pattern?: an A ? A
(t)
matrix of probabilities P (t) . Given an email about topic t, authored by actor a, element par is the
probability of actor a including actor r as a recipient of that email. Inspired by LSM [7], each communication pattern P (t) is represented implicitly via a set of A points in K-dimensional Euclidean
(t)
(t)
(t)
(t)
(t)
(t)
such that par = pra = ?(b(t) ? ksa ? sr k)
space S (t) = {sa }A
a=1 and a scalar bias term b
(t)
with sa ? N (0, ?12 I) and b(t) ? N (?, ?22 ).1 If K = 2 or K = 3, this representation enables
each topic-specific communication pattern to be visualized in 2- or 3-dimensional space via the locations of the points associated with the A actors. It is worth noting that the dimensions of each
(t)
K-dimensional space have no inherent meaning. In isolation, each point sa conveys no information; however, the distance between any two points has a precise and meaningful interpretation in the
generative process. Specifically, the recipients of any email associated with topic t are more likely
to be those actors near to the email?s author in the Euclidean space corresponding to that topic.
Each email, indexed by d, has a discrete distribution over topics ? (d) . A symmetric Dirichlet prior
(d)
with concentration parameter ? is placed over ? = {? (1) , ..., ? (D) }. Each token wn is associated
(d)
(d)
(d)
(d)
with a topic assignment zn , such that zn ? ? (d) and wn ? ?(t) for zn = t. Our model does
not include a distribution over authors; the generative process is conditioned upon their identities.
(d)
The email-specific binary variables y (d) = {yr }A
r=1 indicate the recipients of email d and thus the
presence (or absence) of email-specific edges from author a(d) to each of the A ? 1 other actors.
Consequently, there may be multiple edges (one per email) between any pair of actors, and D defines
a multinetwork over the entire set of actors. We assume that the complete multinetwork comprises T
(d)
topic-specific subnetworks. In other words, each yr is associated with some topic t and therefore
(t)
(d)
with topic-specific communication pattern P (t) such that yr ? Bern(par ) for a(d) = a. A natural
(d)
way to associate each yr with a topic would be to draw a topic assignment from ? (d) in a manner
(d)
analogous to the generation of zn ; however, as outlined by Blei and Jordan [8], this approach can
result in the undesirable scenario in which one subset of topics is associated with tokens, while another (disjoint) subset is associated with edges. Additionally, models of annotated data that possess
this exchangeable structure tend to exhibit poor generalization [3, 8]. A better approach, advocated
(d)
by Blei and Jordan, is to draw a topic assignment for each yr from the empirical distribution over
(d)
topics defined by z . By definition, the set of topics associated with edges will therefore be a subset of the topics associated with tokens. One way of simulating this generative process is to associate
(d)
(d)
each yr with a position n = 1, . . . , max (1, N (d) ) and therefore with the topic assignment zn at
(d)
(d)
that position2 by drawing a position assignment xr ? U(1, . . . , max (1, N (d) )) for each yr . This
(d)
(t)
(d)
(d)
indirect procedure ensures that yr ? Bern(par ) for a(d) = a, xr = n, and zn = t, as desired.
The function ?(?) is the logistic function, while the function k ? k is the l2 -norm.
Emails that do not contain any text (i.e., N (d) = 0) convey information about the frequencies of communication between their authors and recipients. As a result, we do not omit such emails from D; instead, we
(d)
(d)
augment each one with a single, ?dummy? topic assignment z1 for which there is no associated token w1 .
1

2

3

2.1

Inference

(d) D
For real-world data D = {w(d) , a(d) , y (d) }D
}d=1 , authors A =
d=1 , the tokens W = {w
(d) D
(d) D
{a }d=1 , and recipients Y = {y }d=1 are observed, while ?, ?, S = {S (t) }Tt=1 , B = {b(t) }Tt=1 ,
(d) D
Z = {z (d) }D
}d=1 are unobserved. Dirichlet?multinomial conjugacy allows ?
d=1 , and X = {x
and ? to be marginalized out [9], while typical values for the remaining unobserved variables can
be sampled from their joint posterior distribution using Markov chain Monte Carlo methods. In
this section, we outline a Metropolis-within-Gibbs sampling algorithm that operates by sequentially
(t)
(d)
(d)
resampling the value of each latent variable (i.e., sa , bt , zn , or xr ) from its conditional posterior.
(d)

Since zn is a discrete random variable, new values may be sampled directly using
P (zn(d) = t | wn(d) = v, W\d,n , A, Y, S, B, Z\d,n , X , ?, ?)
?
(v|t)
yr(d)
1?yr(d)
+ V? Q
(t)
(t)
?
?(N (t|d) + ? ) N\d,n
(p
)
(1
?
p
)
(d)
(t)
\d,n
T
a(d) r
a(d) r
r:xr =n
N\d,n +?
?
(d)
(d)
?
y
1?y
Q
r
r
?
(t)
(t)
(1 ? pa(d) r )
r:r6=a(d) (pa(d) r )

for N (d) > 0
otherwise,

where subscript ?\d, n? denotes a quantity excluding data from position n in email d. Count N (t) is
the total number of tokens in W assigned to topic t by Z, of which N (v|t) are of type v and N (t|d)
(d)
belong to email d. New values for discrete random variable xr may be sampled directly using
(t)

(d)
P (x(d)
r = n | A, Y, S, B, zn = t, Z\d,n ) ? (pa(d) r )

yr(d)

(t)

(1 ? pa(d) r )

1?yr(d)

.

(t)

New values for continuous random variables sa and b(t) cannot be sampled directly from their
conditional posteriors, but may instead be obtained using the Metropolis?Hastings algorithm. With
(t)
(t)
(t)
a non-informative prior over sa (i.e., sa ? N (0, ?)), the conditional posterior over sa is
Y
N (1|a,r,t) +N (1|r,a,t)
N (0|a,r,t) +N (0|r,a,t)
(t) (t)
P (s(t)
(p(t)
(1 ? p(t)
,
a | A, Y, S \a , b , Z, X ) ?
ar )
ar )
r:r6=a

where count N (1|a,r,t) =

(d)

PD

(d)
= a) 1(yr = 1)
d=1 1(a

N (d)
n=1

P


(d)
(d)
1(xr = n) 1(zn = t) .3 Counts

N (1|r,a,t) , N (0|a,r,t) , and N (0|r,a,t) are defined similarly. Likewise, with an improper, noninformative prior over b(t) (i.e., b(t) ? N (0, ?)), the conditional posterior over b(t) is
P (b(t) | A, Y, S (t) , Z, X ) ?

A Y
Y

(p(t)
ar )

N (1|a,r,t) +N (1|r,a,t)

N (0|a,r,t) +N (0|r,a,t)

(1 ? p(t)
ar )

.

a=1 r:r<a

3

Data

Due to a variety of factors involving personal privacy concerns and the ownership of content by
email service providers, academic researchers rarely have access to organizational email data. For
example, the Enron data set [10]?arguably the most widely studied email data set?was only released because of a court order. The public record is an alternative source of organizational email
data. Public record data sets are widely available and can be continually updated, yet remain relatively untapped by the academic community. We therefore introduce and analyze a new public
record email data set relevant to researchers in the organizational and social sciences as well as machine learning researchers. This data set consists of emails between the managers of the departments
that constitute the executive arm of government at the county level for New Hanover County, North
Carolina. In this semi-autonomous local government, county managers act as executives, and the individual departments are synonymous with the individual departments and agencies in, for instance,
the U.S. federal government. Therefore, not only does this email data set offer a view into the communication patterns of the managers of New Hanover County, but analyses of it also serve as case
studies in modeling inter-agency communications in the U.S. federal government administration.
3

The function 1(?) evaluates to one if its argument evaluates to true and evaluates to zero otherwise.

4

?
?

?
?

??

?

our model
Erosheva
baseline 2
0

?

MMSB
baseline 1
LSM

50
100
150
Number of Topics

(a)

200

?
?

?

?

?

?

?
?

?
?

?
?
?

0.1

?
?
?
?
?

?
?

?

?

?
?

0

our model
Erosheva
baseline 2

MMSB
baseline 1
LSM

50
100
150
Number of Topics

our model
Erosheva
baseline 2
LDA

?

?
?
?
?

?
?
?

?

?
?
?

?
?
?
?

?
?
?

?
?
?

?
?
?

?
?
?

200

0

(b)

Average Topic Coherence
?90 ?80 ?70 ?60 ?50

?
?

?

our model
Erosheva
baseline 2
LDA

??
??
?
??
?
?

?
?
?
?

?110

?

?

?

?
?

Average Topic Coherence
?90 ?80 ?70 ?60 ?50

?

?

?

Average F1 Score
0.2
0.3

?

?

?

0.0

Average F1 Score
0.2
0.3

?

?

?

0.0

0.1

?
?

?

?110

0.4

0.4

?

?

50
100
150
Number of Topics

(c)

200

?
?

?
?
?

?
?
?

0

?
?
?

50
100
150
Number of Topics

?
?
?

200

(d)

Figure 2: Average link prediction performance for (a) the NHC email network and (b) the Enron data
set. For MMSB and LSM, we only report results obtained using the best-performing hyperparameter
values. Average topic coherence scores for (c) the NHC email network and (d) the Enron data set.

The New Hanover County (NHC) email network comprises the complete inboxes and outboxes of
30 department managers from the month of February, 2011. In total, there are 30,909 emails, of
which 8,097 were authored by managers. Of these 8,097 emails, 1,739 were sent to other managers
(via the ?To? or ?Cc? fields), excluding any emails sent from a manager to him- or herself only.
For our experiments, we used these 1,739 emails between 30 actors. To verify that our model is
applicable beyond the NHC email network, we also performed two validation experiments using
the Enron email data set [10]. For this data set, we treated each unique @enron email address
as an actor and used only those emails between the 50 most active actors (determined by the total
numbers of emails sent and received). Emails that were not sent to at least one other active actor (via
the ?To? or ?Cc? fields) were discarded. To avoid duplicate emails, we retained only those emails
from ? sent mail?, ?sent?, or ?sent items? folders. These steps resulted in a total of 8,322 emails
involving 50 actors. Both data sets were preprocessed to concatenate the text of subject lines and
message bodies and to remove any stop words, URLs, quoted text, and (where possible) signatures.

4

Experiments

Our model is primarily intended as an exploratory analysis tool for organizational communication
networks. In this section, we use the NHC email network to showcase our model?s ability to discover
and visualize topic-specific communication subnetworks. First, however, we test our underlying
modeling assumptions via three quantitative validation tasks, as recommended by Schrodt [11].
4.1

Link Prediction

In order to gauge our model?s predictive performance, we evaluated its ability to predict the recipients of ?test? emails, from either the NHC email network or the Enron data set, conditioned on the
text of those emails and the identities of their authors. For each test email d, the binary variables
(d)
indicating the recipients of that email, i.e., {yr }A
r=1 , were treated as unobserved. Typical values
for these variables were sampled from their joint posterior distribution and compared to the true
values to yield an F1 score. We formed a test split of each data set by randomly selecting emails
with probability 0.1. For each data set, we averaged the F1 scores over five random test splits.
We compared our model?s performance with that of two baselines and three existing network models, thereby situating it within the existing literature. Given a test email authored by actor a, our
simplest baseline na??vely predicts that actor a will include actor r as a recipient of that email with
probability equal to the number of non-test emails sent from actor a to actor r divided by the total number of non-test emails sent by actor a. Our second baseline is a variant of our model in
which each topic-specific communication pattern P (t) is represented explicitly via A(A + 1) / 2
probabilities drawn from a symmetric Beta prior with concentration parameter ?. Comparing our
model to this variant enables us to validate our assumption that topic-specific communication patterns can indeed be accurately represented by a set of A points (one per actor) in K-dimensional
Euclidean space. We also compared our model?s performance to that of three existing network models: a variant of Erosheva et al.?s model for analyzing scientific publications [4], LSM [7], and the
5

mixed-membership stochastic blockmodel (MMSB) [12]. Erosheva et al.?s model can be viewed as
(d)
a variant of our model in which the topic assignment for each yr is drawn from ? (d) instead of
(d)
the empirical distribution over topics defined by z . Like our second baseline, each topic-specific
communication pattern is represented explicitly via probabilities drawn from a symmetric Beta prior
with concentration parameter ?; however, unlike this baseline, each one is represented using A prob(t)
(t)
abilities such that par = pr . LSM can be viewed as a network-only variant of our model in which
text is not modeled. As a result, there are no topics and a single communication pattern P . This pattern is represented implicitly via a set of A actor-specific points in K-dimensional Euclidean space.
Finally, MMSB is a widely-used model for mixed-membership community discovery in networks.
(d)

For our model and all its variants, typical values for {yr }A
r=1 can be sampled from their joint
posterior distribution using an appropriately-modified version of the Metropolis-within-Gibbs algorithm in Section 2.1. In all our experiments, we ran this algorithm for 40,000?50,000 iterations. On
iteration i, we defined each proposal distribution to be a Gaussian distribution centered on the value
from iteration i ? 1 with covariance matrix max (1, 100 / i) I, thereby resulting in larger covariances
for earlier iterations. Beta?binomial conjugacy allows the elements of P (t) to be marginalized out
in both our second baseline and Erosheva et al.?s model. For MMSB, typical values can be sampled using a modified version of Chang?s Gibbs sampling algorithm [13]. We ran this algorithm
for 5,000 iterations. For all models involving topics, we set concentration parameter ? to 1 for the
NHC network and 2 for the Enron data set. For both data sets, we set concentration parameter ? to
0.01V .4 We varied the number of topics from 1 to 200. In order to facilitate visualization, we used
2-dimensional Euclidean spaces for our model. For LSM, however, we varied the dimensionality of
the Euclidean space from 1 to 200. We report only those results obtained using the best-performing
dimensionality. For our second baseline and Erosheva et al.?s model, we set concentration parameter
? to 0.02. For MMSB, we performed a grid search over all hyperparameter values and the number
of blocks and, as with LSM, report only those results obtained using the best-performing values.5
F1 scores, averaged over five random test splits of each data set, are shown in Figure 2. Although our
model is intended for exploratory analysis, it achieves better link prediction performance than the
other models. Furthermore, the fact that our model outperforms our second baseline and Erosheva
et al.?s model validates our assumption that topic-specific communication patterns can indeed be
accurately represented by a set of A actor-specific points in 2-dimensional Euclidean space.
4.2

Topic Coherence

When evaluating unsupervised topic models, topic coherence metrics [14, 15] are often used as a
proxy for subjective evaluation of semantic coherence. In order to demonstrate that incorporating
network data does not impair our model?s ability to model text, we compared the coherence of topics
inferred using our model with the coherence of topics inferred using LDA, our second baseline, and
Erosheva et al.?s model. For each model, we varied the number of topics from 1 to 200 and drew five
samples from the joint posterior distribution over the unobserved random variables in that model. We
evaluated the topics resulting from each sample using Mimno et al.?s coherence metric [14]. Topic
coherence, averaged over the five samples, is shown in Figure 2. Our model achieves coherence
comparable to that of LDA. This result, when combined with the results in Section 4.1, demonstrates
that our model can achieve state-of-the-art predictive performance while producing coherent topics.
4.3

Posterior Predictive Checks

We used posterior predictive checking to assess the extent to which our model is a ?good fit? for the
NHC email network [16, 17]. Specifically, we defined four network statistics (i.e., four discrepancy
functions) that summarize meaningful aspects of the NHC network: generalized graph transitivity,
the dyad intensity distribution, the vertex degree distribution, and the geodesic distance distribution.6
We then generated 1,000 synthetic networks from the posterior predictive distribution implied by our
4
These values were obtained by slice sampling typical values for the concentration parameters in LDA.
They are consistent with the concentration parameter values used in previous work [9].
5
These values correspond to a Dir(0.1, . . . , 0.1) prior over block memberships, a Beta(0.1, 0.1) prior over
diagonal entries of the blockmodel, a Beta(0.01, 0.01) prior over off-diagonal entries, and 30 blocks.
6
These statistics are defined in the supplementary materials.

6

50

60

?
?

40

0.655

0.665

Transitivity

(a)

400
200

0

0.645

600

?

?
?
?
?
?
?

20

0

Simulated Quantile

100

1.2

800
Degree

Simulated Quantile

Frequency

150

1.4

?
?

80

200

??
??

??
???
?

?
?
??
?

0
0

20

40

?
?
??
?????
???

??
????

60

1.0
0.8
0.6
0.4
0.2
0.0
0.0 0.2 0.4 0.6 0.8 1.0 1.2

Observed Quantile

Actor (Sorted by Observed Degree)

(b)

(c)

Observed Quantile

(d)

Figure 3: Four posterior predictive checks of our model using the NHC email network and 100
topics: (a) a histogram of the graph transitivity of the synthetic networks, with the graph transitivity
of the NHC email network indicated by a vertical line; (b) a quantile?quantile plot comparing the
distribution of dyadic intensities in the synthetic networks to that of the observed network; (c) a box
plot indicating the sampled degree of each manager in the synthetic networks, with managers sorted
from highest to lowest observed degree and their observed degrees indicated by a line; and (d) a
quantile?quantile plot comparing the observed and synthetic geodesic distance distributions.

model and the NHC network. We applied each discrepancy function to each synthetic network to
yield four distributions over the values of the four network statistics. If our model is a ?good fit?
for the NHC network, these distributions should be centered around the values of the corresponding
discrepancy functions when computed using the observed NHC network. As shown in Figure 3,
our model generates synthetic networks with dyad intensity, vertex degree, and geodesic distance
distributions that are very similar to those of the NHC network. The distribution over synthetic graph
transitivity values is not centered around the observed graph transitivity, but the observed transitivity
is not sufficiently far into the tail of the distribution to warrant reparameterization of our model.
4.4

Exploratory Analysis

In order to demonstrate our model?s novel ability to discover and visualize topic-specific communication patterns, we performed an exploratory analysis of four such patterns inferred from the NHC
email network using our model. These patterns are visualized in Figure 4. Each pattern is represented implicitly via a single set of A points in 2-dimensional Euclidean space drawn from their joint
posterior distribution. The recipients of any email associated with topic t are more likely to be those
actors near to the email?s author in the Euclidean space corresponding to that topic. We selected the
patterns in Figure 4 so as to highlight the types of insights that can be obtained using our model.
Although many structural properties may be of interest, we focus on modularity and assortativity.
For each topic-specific communication pattern, we examined whether there are active, disconnected
components in that topic?s Euclidean space (i.e., high modularity). The presence of such components indicates that there are groups of actors who engage in within- but not between-group communication about that topic. We also used a combination of node proximity and node coloration
to determine whether there is more communication between departments that belong to the same
?division? in the New Hanover County government organizational chart than between departments
within different divisions (i.e., assortativity). In Figure 4, we show one topic that exhibits strong
modularity and little assortativity (the ?Public Signage? topic), one topic that exhibits strong assortativity and little modularity (the ?Broadcast Messages? topic), and one topic that exhibits both
strong assortativity and strong modularity (the ?Meeting Scheduling? topic). The ?Public Relations?
topic, which includes communication with news agencies, is mostly dominated by a cluster involving many departments. Finally, the ?Meeting Scheduling? topic displays hierarchical structure, with
two assistant county managers located at the centers of groups that correspond to their divisions.
Exploratory analysis of communication patterns is a powerful tool for understanding organizational
communication networks. For example, examining assortativity can reveal whether actual communication patterns resemble official organizational structures. Similarly, if a communication pattern
exhibits modularity, each disconnected component may benefit from organizational efforts to facilitate inter-component communication. Finally, structural properties other than assortativity and
modularity may also yield scientific or practical insights, depending on organizational needs.
7

Public Signage

Broadcast Messages

15
change signs sign process
ordinance

17
fw fyi bulletin summary
week legislative
?
PM

CC
?

400

300

HL
?

?
AM
SF
?

?
PS
?
DS

200

EG
?

?
EG

LB
?

AM
?

?
DS
?
FN

PS
?

100

200

?
PI

TX
?

?
BG

LB
?

FS
?

0

?
HR

MS
?

?
CM

RD
?

AM
AM
EM PG
?

?
IT

CE
?

EM
?

RD
?

PG
?

CA
?

CC
?

SF
?

?

TX
?

FS
?

?300

?
SS

?

?

?200

EV
?

?
IT

MS
?

?100

?
PM

?200

?
RM

0
?
BG
EL
?

?400

?
EV

?
CM

CA
?

?
FN

?
YS
?
PI
?
SS

RM
?

YS
?

?
VS

?
HL

HR
?

VS
?

EL
?

CE
?

?400

?200

0

200

400

?300

?200

?100

0

100

200

300

Public Relations

Meeting Scheduling

31
city breakdown information
give

63
meeting march board
agenda week

EL
?

FS
?

400

400

SF
?
CA
?

PS
?
RM
?

200

200

RD
?
EG
?

RM
?

?
HR

?
MS
PI
?

?
DS

0

FS
?CE
?
EV
?

?
CM

? EL
PG
?

RD
?

?
TX

CC
?

?
LB

?
CE

?
EM

?
AM

?
HR
BG
?

?
FN

?
HL

0

?
PM
?
? SS
LB
?
HL

?
EM
?
AM
?

?200

IT
?
PG
?
BG

CA
?

MS
?

?200

?
PS

?
CM

?
EG

?
SS

IT
?

TX
?

0

?

?600

?400

CC
?

?400

AM

YS
?

?
AM

VS
?

DS

?
PI

?200

?
FN

YS
??

?400

?
PM

VS
?

SF
?

Assistant County Manager
Budget
Cooperative Extension
County Attorney
County Commissioners
County Manager
Development Services
Elections
Emergency Management
Engineering
Environmental Management
Finance
Fire Services
Health
Human Resources
Information Technology
Library
Museum
Parks and Gardens
Planning and Inspections
Pretrial Release Screening
Property Management
Register of Deeds
Risk Management
Sheriff
Social Services
Tax
?
Veteran Services
Youth Empowerment Services

AM
BG
CE
CA
CC
CM
DS
EL
EM
EG
EV
FN
FS
HL
HR
IT
LB
MS
PG
PI
PS
PM
RD
RM
SF
SS
TX
VS
YS

EV
?

200

400

600

?200

0

200

400

Figure 4: Four topic-specific communication patterns inferred from the NHC email network. Each
pattern is labeled with a human-selected name for the corresponding topic, along with that topic?s
most probable words in order of decreasing
probability. The size of each manager?s acronym in
q
(t)

(t)

(t)

topic t?s pattern (given by 0.45 + 1.25 da / maxa da , where da is the degree of actor a in that
subnetwork) indicates how often that manager communicates about that topic. Managers? acronyms
are colored according to their respective division in the New Hanover County organizational chart.
The acronym ?AM? appears twice in all plots because there are two assistant county managers.

5

Conclusions

We introduced a new Bayesian admixture model for the discovery and visualization of topic-specific
communication subnetworks. Although our model is intended for exploratory analysis, the validation experiments described in Sections 4.1 and 4.2 demonstrate that our model can achieve stateof-the-art predictive performance while exhibiting topic coherence comparable to that of LDA. To
showcase our model?s ability to discover and visualize topic-specific communication patterns, we
introduced a new data set (the NHC email network) and analyzed four such patterns inferred from
this data set using our model. Via this analysis, were are able to examine the extent to which actual
communication patterns resemble official organizational structures and identify groups of managers
who engage in within- but not between-group communication about certain topics. Together, these
predictive and exploratory analyses lead us to recommend our model for any exploratory analysis of
email networks or other similarly-structured communication data. Finally, our model is capable of
producing principled visualizations of email networks, i.e., visualizations that have precise mathematical interpretations in terms of this model and its relationship to the observed data. We advocate
for principled visualization as a primary objective in the development of new network models.

Acknowledgments
This work was supported in part by the Center for Intelligent Information Retrieval and in part by
the NSF GRFP under grant #1122374. Any opinions, findings, and conclusions or recommendations
expressed in this material are those of the authors and do not necessarily reflect those of the sponsors.
8

References
[1] W. Mason and D.J. Watts. Collaborative learning in networks. Proceedings of the National
Academy of Sciences, 109(3):764?769, 2012.
[2] A. McCallum, A. Corrada-Emmanuel, and X. Wang. Topic and role discovery in social networks. In Proceedings of the International Joint Conference on Artificial Intelligence, 2005.
[3] J. Chang and D.M. Blei. Relational topic models for document networks. In Proceedings of
the Twelfth International Conference on Artificial Intelligence and Statistics, 2009.
[4] E. Erosheva, S. Fienberg, and J. Lafferty. Mixed-membership models of scientific publications.
Proceedings of the National Academy of Sciences, 101(Suppl. 1), 2004.
[5] S.E Fienberg. A brief history of statistical models for network analysis and open challenges.
Journal of Computational and Graphical Statistics, 22, 2012.
[6] D.M. Blei, A.Y. Ng, and M.I. Jordan. Latent Dirichlet allocation. Journal of Machine Learning
Research, 3:993?1022, 2003.
[7] P.D. Hoff, A.E. Raftery, and M.S. Handcock. Latent space approaches to social network analysis. Journal of the American Statistical Association, 97(460):1090?1098, 2002.
[8] D.M. Blei and M.I. Jordan. Modeling annotated data. In Proceedings of the Twenty-Sixth
Annual International ACM SIGIR Conference on Research and Development in Information
Retrieval, pages 127?134, 2003.
[9] T.L. Griffiths and M. Steyvers. Finding scientific topics. Proceedings of the National Academy
of Sciences, 101(Suppl. 1), 2004.
[10] B. Klimt and Y. Yang. Introducing the Enron corpus. In Proceedings of the First Conference
on Email and Anti-Spam, 2004.
[11] P.A Schrodt. Seven deadly sins of contemporary quantitative political analysis. In Proceedings
of the Annual American Political Science Association Meeting and Exhibition, 2010.
[12] E.M. Airoldi, D.M. Blei, S.E. Fienberg, and E.P. Xing. Mixed membership stochastic blockmodels. Journal of Machine Learning Research, 9:1981?2014, 2008.
[13] J. Chang. Uncovering, Understanding, and Predicting Links. PhD thesis, Princeton Unversity,
2011.
[14] D. Mimno, H.M. Wallach, E.T.M. Leenders, and A. McCallum. Optimizing semantic coherence in topic models. In Proceedings of the Conference on Empirical Methods in Natural
Language Processing, 2011.
[15] D. Newman, J.H. Lau, K. Grieser, and T. Baldwin. Automatic evaluation of topic coherence. In
Proceedings of Human Language Technologies: The Annual Conference of the North American
Chapter of the Association for Computational Linguistics, pages 100?108, 2010.
[16] D.R. Hunter, M.S. Handcock, C.T. Butts, S.M. Goodreau, and M. Morris. ergm: A package
to fit, simulate and diagnose exponential-family models for networks. Journal of Statistical
Software, 24(3):1?29, 2008.
[17] D. Mimno and D.M. Blei. Bayesian checking for topic models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 227?237, 2011.

9


----------------------------------------------------------------

title: 964-an-input-output-hmm-architecture.pdf

An Input Output HMM Architecture
Yoshua Bengio*
Dept. Informatique et Recherche
Operationnelle
Universite de Montreal, Qc H3C-3J7
bengioyOIRO.UMontreal.CA

Paolo Frasconi
Dipartimento di Sistemi e Informatica
Universita di Firenze (Italy)
paoloOmcculloch.ing.unifi.it

Abstract
We introduce a recurrent architecture having a modular structure
and we formulate a training procedure based on the EM algorithm.
The resulting model has similarities to hidden Markov models, but
supports recurrent networks processing style and allows to exploit
the supervised learning paradigm while using maximum likelihood
estimation.

1

INTRODUCTION

Learning problems involving sequentially structured data cannot be effectively dealt
with static models such as feedforward networks. Recurrent networks allow to model
complex dynamical systems and can store and retrieve contextual information in
a flexible way. Up until the present time, research efforts of supervised learning
for recurrent networks have almost exclusively focused on error minimization by
gradient descent methods. Although effective for learning short term memories,
practical difficulties have been reported in training recurrent neural networks to
perform tasks in which the temporal contingencies present in the input/output
sequences span long intervals (Bengio et al., 1994; Mozer, 1992).
Previous work on alternative training algorithms (Bengio et al., 1994) could suggest
that the root of the problem lies in the essentially discrete nature of the process
of storing information for an indefinite amount of time. Thus, a potential solution
is to propagate, backward in time, targets in a discrete state space rather than
differential error information. Extending previous work (Bengio & Frasconi, 1994a),
in this paper we propose a statistical approach to target propagation, based on the
EM algorithm. We consider a parametric dynamical system with discrete states and
we introduce a modular architecture, with subnetworks associated to discrete states.
The architecture can be interpreted as a statistical model and can be trained by the
EM or generalized EM (GEM) algorithms (Dempster et al., 1977), considering the
internal state trajectories as missing data. In this way learning is decoupled into
? also, AT&T Bell Labs, Holmdel, N J 07733

428

Yoshua Bengio, Paolo Frasconi

a temporal credit assignment subproblem and a static learning subproblem that
consists of fitting parameters to the next-state and output mappings defined by the
estimated trajectories . In order to iteratively tune parameters with the EM or GEM
algorithms, the system propagates forward and backward a discrete distribution over
the n states, resulting in a procedure similar to the Baum-Welch algorithm used
to train standard hidden Markov models (HMMs) (Levinson et al., 1983) . HMMs
however adjust their parameters using unsupervised learning, whereas we use EM
in a supervised fashion . Furthermore, the model presented here could be called
Input/Output HMM, or IOHMM , because it can be used to learn to map input
sequences to output sequences (unlike standard HMMs, which learn the output
sequence distribution) . This model can also be seen as a recurrent version of the
Mixture of Experts architecture (Jacobs et al. , 1991) , related to the model already
proposed in (Cacciatore and Nowlan, 1994). Experiments on artificial tasks (Bengio
& Frasconi , 1994a) have shown that EM recurrent learning can deal with long
term dependencies more effectively than backpropa~ation through time and other
alternative algorithms. However, the model used in (Bengio & Frasconi, 1994a) has
very limited representational capabilities and can only map an input sequence to a
final discrete state. In the present paper we describe an extended architecture that
allows to fully exploit both input and output portions of the data, as required by
the supervised learning paradigm. In this way, general sequence processing tasks,
such as production, classification, or prediction, can be dealt with.

2

THE PROPOSED ARCHITECTURE

We consider a discrete state dynamical system based on the following state space
x - f(x
U )
description:
t t-l, t
(1)
Yt = 9(Xt, Ut)
where Ut E R m is the input vector at time t, Yt E R r is the output vector, and
Xt E {I , 2, .. . , n} is a discrete state. These equations define a generalized Mealy
finite state machine, in which inputs and outputs may take on continuous values . In
this paper , we consider a probabilistic version of these dynamics, where the current
inputs and the current state distribution are used to estimate the state distribution
and the output distribution for the next time step. Admissible state transitions will
be specified by a directed graph 9 whose vertices correspond to the model 's states
and the set of successors for state j is Sj .
The system defined by equations (1) can be modeled by the recurrent architecture
depicted in Figure l(a) . The architecture is composed by a set of state n etworks
N j, j = 1 .. . n and a set of output networks OJ, j = 1 . .. n. Each one of the state
and output networks is uniquely associated to one of the states,and all networks
share the same input Ut . Each state network M has the task of predicting the next
state distribution , based on the current input and given that Xt-l = j . Similarly,
each output network OJ predicts the output of the system, given the current state
and input. All the subnetworks are assumed to be static and they are defined by
means of smooth mappings Nj (Ut ;9j ) and OJ (Ut; 1Jj), where 9j and 1Jj are vectors
of adjustable parameters (e .g., connection weights). The ranges of the functions
N j 0 may be constrained in order to account for the underlying transition graph
9 . Each output 'Pij ,t of the state subnetwork Nj (at time t) is associated to one
of the successors i of state j . Thus the last layer of M has as many units as the
cardinality of Sj. For convenience of notation, we suppose that 'Pij,t are defined for
each i, j = 1, ... , n and we impose the condition 'Pij ,t = 0 for each i not belonging
to S j . The softmax function is used in the last layer: 'Pij ,t = e a,j ,t ILlEsj ea l j,t, j =
1, ... , n , i E Sj where aij, t are intermediate variables that can be thought of as the

An Input Output HMM Architecture

cu ... nt Input

EIYt

...

Xt-l

current ??pectod output,
given PIlat Input Mquenc.

11 t

429

current atilt. dlatrtbutton

Ct= Pl' t I Ul )

lull

...

...

Xt

Xt+l

1 1 1

Yt-l

Yt

Yt+l

Xt-l

X(

Xt+l

'-'t

\.Yt-l

\Yt

I

I

Ut-l

HMM

\.Yt+l

I

Ut

.. ....

IOHMM

Ut+l

(b)

(a)

Figure 1: (a): The proposed IOHMM architecture. (b): Bottom: Bayesian network
expressing conditional dependencies for an IOHMM; top: Bayesian network for a
standard HMM

't

activations of the output units of subnetwork N j . In this way L:7=1 'Pij ,t = 1 Tij,t.
The vector
E R n represents the internal state of the model and it is computed as
a linear combination of the outputs of the state networks, gated by the previously
n
computed internal state:
't

= L ( j ,t-IIPj,t

(2)

j=1

where IPj,t = ['PIj,t, ... , 'Pnj,t]'. Output networks compete to predict the global
output of the system 1Jt E R r :
n
1Jt

= L (jt1Jjt

(3)

j=1

where 1Jjt E R r is the output of subnetwork OJ. At this level , we do not need
to further specify the internal architecture of the state and output subnetworks.
Depending on the task, the designer may decide whether to include hidden layers
and what activation rule to use for the hidden units.
This connectionist architecture can be also interpreted as a probability model. Let
us assume a multinomial distribution for the state variable Xt and let us consider
the main variable of the temporal recurrence (2). If we initialize the vector
to positive numbers summing to 1, it can be interpreted as a vector of initial state
probabilities. In general, we obtain relation (it = P(Xt = i I
having denoted
with ui the subsequence of inputs from time 1 to t, inclusively. Equation (2) then
has the following probabilistic interpretation:

't,

un,

'0

n

P(Xt

= i

lui) =

L

P(Xt

= i I Xt-I = j, ut}P(Xt-1 = j

lui-I)

(4)

j=l

i.e., the subnetworks N j compute transition probabilities conditioned on the input
sequence Ut:
P( Xt = ~. I Xt-l = ),.Ut
)
(5)
'Pij,t =
As in neural networks trained to minimize the output squared error, the output
1Jt of this architecture can be interpreted as an expected "position parameter"
for the probability distribution of the output Yt. However, in addition to being
conditional on an input Ut, this expectation is also conditional on the state Xt, i.e.

Yoshua Bengio, Paolo Frasconi

430

= E[Yt I Xt,Ut] . The actual form of the output density, denoted !Y(Yt ; 7]t), will
be chosen according to the task . For example a multinomial distribution is suitable
for sequence classification, or for symbolic mutually exclusive outputs. Instead, a
Gaussian distribution is adequate for producing continuous outputs. In the first
case we use a softmax function at the output of subnetworks OJ; in the second case
we use linear output units for the subnetworks OJ.
7]t

In order to reduce the amount of computation, we introduce an independency model
among the variables involved in the probabilistic interpretation of the architecture.
We shall use a Bayesian network to characterize the probabilistic dependencies
among these variables. Specifically, we suppose that the directed acyclic graph
9 depicted at the bottom of Figure 1b is a Bayesian network for the dependency
One of the most evident consequences
model associated to the variables u
of this independency model is that only the previous state and the current input are
relevant to determine the next-state. This one-step memory property is analogue
to the Markov assumption in hidden Markov models (HMM). In fact, the Bayesian
network for HMMs can be obtained by simply removing the Ut nodes and arcs from
them (see top of Figure Ib) .

I ,xI, YI.

3

A SUPERVISED LEARNING ALGORITHM

The learning algorithm for the proposed architecture is derived from the maximum
likelihood principle. The training data are a set of P pairs of input/ output sequences
(of length Tp): 1) = {(uip(p),Yip(p));p = 1 .. . P}. Let ?J denote the vector of
parameters obtained by collecting all the parameters (Jj and iJi of the architecture.
The likelihood function is then given by
p

L(?J; 1))

= II p(Yip(p) I uip(p); ?J).

(6)

p=l

The output values (used here as targets) may also be specified intermittently. For
example, in sequence classification tasks , one may only be interested in the output YT at the end of each sequence. The modification of the likelihood to account
for intermittent targets is straightforward. According to the maximum likelihood
principle, the optimal parameters are obtained by maximizing (6). In order to
apply EM to our case we begin by noting that the state variables Xt are not observed. Knowledge of the model's state trajectories would allow one to decompose
the temporal learning problem into 2n static learning subproblems. Indeed , if Xt
were known, the probabilities (it would be either 0 or 1 and it would be possible
to train each subnetwork separately, without taking into account any temporal dependency. This observation allows to link EM learning to the target propagation
approach discussed in the introduction . Note that if we used a Viterbi-like approximation (i .e., considering only the most likely path) , we would indeed have 2n static
learning problems at each epoch. In order to we derive the learning equations, let
us define the complete data as 1)c = {(uiP(p),yiP(p),xiP(p));p = 1 ... P}. The
corresponding complete data l%-likelihood is
T
T
T
Ic(?J;1)c) = """
L...IOgP(YIP(P),ZlP(P)
I u1P(p);
?J).

(7)

p=l

Since lc( ?J; 1)c) depends on the hidden state variables it cannot be maximized directly. The MLE optimization is then solved by introducing the auxiliary function
Q(?J; 0) and iterating the following two,steps for k = 1, 2 ... :,
Estimation:
Compute Q(?J ; ?J) = E[lc(?J; 1)c) r 1), ?J]
(8)
Maximization: Update the parameters as 0 t- arg max?J Q( ?J; 0)

An Input Output HMM Architecture

431

The expectation of (7) can be expressed as
p

Q(0 ; 0)

Tp

N

N

= L: L: L: (it!og P(Yt I Xt = i , Ut i 0) + L: hij,tlog<Pij ,t
p=1 t=1 i=1

(9)

j=1

I

where hij ,t = EIZitzj,t-l uf, yf; 0J, denoting Zit for an indicator variable = 1 if
Xt = i and 0 otherwise. The hat in ( it and hij ,t means that these variables are
computed using the "old" parameters 0 . In order to compute hij ,t we introduce
the forward probabilities Qit = P(YL Xt = i ; uD and the backward probabilities
f3it = p(yf I Xt = i ,
that are updated as follows:

un,

f3it = fY(Yt;l1it) Lt <Pti(Ut+df3l,t+l

(10)

Qit = fY(Yt; l1it) Lt <pa(ut} Qt ,t-l .
(11)
- f3it Qj ,t-l<Pij (ut)
"
wi QiT
Each iteration of the EM algorithm requires to maximize Q(0 ; 0). We first
consider a simplified case, in which the inputs are quantized (i .e., belonging
to a finite alphabet {0"1, "" O"K}) and the subnetworks behave like lookup tables addressed by the input symbols O"t, i.e. we interpret each parameter as
Wi' k = P(Xt = i I Xt-l = j , O"t = k). For simplicity, we restrict the analysis to classification tasks and we suppose that targets are specified as desired final states for
each sequence. Furthermore, no output subnetworks are used in this particular
application of the algorithm. In this case we obtain the reestimation formulae:
h

Wijk =

ij ,t -

"

"p"

(12)

. .

{j,t(j,t-l
w i ESj wp=1 wt :Ut=k , + T

?

"'T '

In general, however, if the subnetworks have hidden sigmoidal units, or use a softmax function to constrain their outputs to sum to one, the maximum of Q cannot
be found analytically. In these cases we can resort to a GEM algorithm, that simply produces an increase in Q, for example by gradient ascent. In this case, the
derivatives of Q with respect to the parameters can be easily computed as follows.
Let Ojlt be a generic weight in the state subnetwork N j . From equation (9):
8Q(0;0)
80jk

= L:L:L:hij,t_l_8<pij ,t
p

t

i

<Pij,t 80jk

(13)

where the partial derivatives &:e~;t can be computed using backpropagation . Similarly, denoting with t'Jik a generic weight of the output subnetwork Oi, we have:
8Q( 0; 0)
'" '" '" .
8
87]a t
8t'J ?
= L..JL..JL..J(i , t~logfY(Yy;l1it) 8t'J .'
p
t
t
7],t,t
,k
,k

(14)

where ~;;:~t are also computed using backpropagation. Intuitively, the parameters
are updated as if the estimation step of EM had provided targets for the outputs of
the 2n subnetworks , for each time t . Although GEM algorithms are also guaranteed
to find a local maximum of the likelihood, their convergence may be significantly
slower compared to EM. In several experiments we noticed that convergence can be
accelerated with stochastic gradient ascent .

432

4

Yoshua Bengio, Paolo Frasconi

COMPARISONS

It appears natural to find similarities between the recurrent architecture described
so far and standard HMMs (Levinson et al., 1983). The architecture proposed in this
paper differs from standard HMMs in two respects: computing style and learning.
With IOHMMs, sequences are processed similarly to recurrent networks, e.g., an
input sequence can be synchronously transformed into an output sequence. This
computing style is real-time and predictions of the outputs are available as the input
sequence is being processed. This architecture thus allows one to implement all three
fundamental sequence processing tasks: production, prediction, and classification.
Finally, transition probabilities in standard HMMs are fixed, i.e. states form a
homogeneous Markov chain. In IOHMMs, transition probabilities are conditional
on the input and thus depend on time, resulting in an inhomogeneous Markov chain.
Consequently, the dynamics of the system (specified by the transition probabilities)
are not fixed but are adapted in time depending on the input sequence.
The other fundamental difference is in the learning procedure. While interesting
for their capabilities of modeling sequential phenomena, a major weakness of standard HMMs is their poor discrimination power due to unsupervised learning. An
approach that has been found useful to improve discrimination in HMMs is based
on maximum mutual information (MMI) training . It has been pointed out that
supervised learning and discriminant learning criteria like MMI are actually strictly
related (Bridle, 1989). Although the parameter adjusting procedure we have defined
is based on MLE,
is used as desired output in response to the input
resulting
in discriminant supervised learning. Finally, it is worth mentioning that a number
of hybrid approaches have been proposed to integrate connectionist approaches into
the HMM frame\'Vork. For example in (Bengio et al. , 1992) the observations used
by the HMM are generated by a feedforward neural network. In (Bourlard and
Wellekens, 1990) a feedforward network is used to estimate state probabilities, conditional to the acoustic sequence. A common feature of these algorithms and the
one proposed in this paper is that neural networks are used to extract temporally
local information whereas a Markovian system integrates long-term constraints.

yf

uf ,

We can also establish a link between IOHMMs and adaptive mixtures of experts
(ME) (Jacobs et al., 1991). Recently, Cacciatore & Nowlan (1994) have proposed a
recurrent extension to the ME architecture, called mixture of controllers (MC), in
which the gating network has feedback connections, thus allowing to take temporal
context into account . Our IOHMM architecture can be interpreted as a special case
of the MC architecture, in which the set of state subnetworks play the role of a
gating network having a modular structure and second order connections.

5

REGULAR GRAMMAR INFERENCE

In this section we describe an application of our architecture to the problem of
grammatical inference. In this task the learner is presented a set of labeled strings
and is requested to infer a set of rules that define a formal language. It can be
considered as a prototype for more complex language processing problems. However,
even in the "simplest" case, i.e. regular grammars, the task can be proved to
be NP-complete (Angluin and Smith, 1983) . We report experimental results on
a set of regular grammars introduced by Tomita (1982) and afterwards used by
other researchers to measure the accuracy of inference methods based on recurrent
networks (Giles et al. , 1992; Pollack, 1991; Watrous and Kuhn , 1992) .
We used a scalar output with supervision on the final output YT that was modeled
as a Bernoulli variable fy (YT ; 7]T) = 7]~T (1 - 7] ) l-YT, with YT = 0 if the string
is rejected and YT = 1 if it is accepted . In tbis application we did not apply

An Input Output HMM Architecture

433

Table 1: Summary of experimental results on the seven Tomita's grammars.
Grammar

n*
1
2
3
4
5
6
7

2
8
7
4
4
3
3

Sizes
FSA min
2
3
5
4
4
3
5

Convergence
.600
.800
.150
.100
.100
.350
.450

Average
1.000
.965
.867
1.000
1.000
1.000
.856

Accuracies
Worst
Best
1.000 1.000
.834 1.000
.775 1.000
1.000 1.000
1.000 1.000
1.000 1.000
.815 1.000

W&K Best
1.000
1.000
.783
.609
.668
.462
.557

external inputs to the output networks . This corresponds to modeling a Moore
finite state machine. Given the absence of prior knowledge about plausible state
paths, we used an ergodic transition graph (i.e., fully connected).In the experiments
we measured convergence and generalization performance using different sizes for
the recurrent architecture. For each setting we ran 20 trials with different seeds
for the initial weights. We considered a trial successful if the trained network was
able to correctly label all the training strings. The model size was chosen using a
cross-validation criterion based on performance on 20 randomly generated strings
of length T ::; 12. For comparison, in Table 1 we also report for each grammar
the number of states of the minimal recognizing FSA (Tomita, 1982). We tested
the trained networks on a corpus of 213 - 1 binary strings of length T ::; 12. The
final results are summarized in Table 1. The column "Convergence" reports the
fraction of trials that succeeded to separate the training set. The next three columns
report averages and order statistics (worst and best trial) of the fraction of correctly
classified strings, measured on the successful trials. For each grammar these results
refer to the model size n* selected by cross-validation. Generalization was always
perfect on grammars 1,4,5 and 6. For each grammar, the best trial also attained
perfect generalization. These results compare very favorably to those obtained with
second-order networks trained by gradient descent, when using the learning sets
proposed by Tomita. For comparison , in the last column of Table 1 we reproduce
the results reported by Watrous & Kuhn (1992) in the best of five trials. In most
of the successful trials the model learned an actual FSA behavior with transition
probabilities asymptotically converging either to 0 or to 1. This renders trivial the
extraction of the corresponding FSA . Indeed, for grammars 1,4,5, and 6, we found
that the trained networks behave exactly like the minimal recognizing FSA .
A potential training problem is the presence of local maxima in the likelihood function. For example, the number of converged trials for grammars 3, 4, and 5 is quite
small and the difficulty of discovering the optimal solution might become a serious
restriction for tasks involving a large number of states. In other experiments (Bengio & Frasconi , 1994a) , we noticed that restricting the connectivity of the transition
graph can significantly help to remove problems of convergence. Of course, this approach can be effectively exploited only if some prior knowledge about the state
space is available. For example, applications of HMMs to speech recognition always
rely on structured topologies.

6

CONCLUSIONS

There are still a number of open questions . In particular, the effectiveness of the
model on tasks involving large or very large state spaces needs to be carefully evaluated. In (Bengio & Frasconi 1994b) we show that learning long term dependencies
in these models becomes more difficult as we increase the connectivity of the state

434

Yoshua Bengio, Paolo Frasconi

transition graph. However, because transition probabilities of IOHMMs change at
each t, they deal better with this problem of long-term dependencies than standard
HMMs. Another interesting aspect to be investigated is the capability of the model
to successfully perform tasks of sequence production or prediction. For example,
interesting tasks that could also be approached are those related to time series
modeling and motor control learning.

References
Angluin, D. and Smith, C. (1983). Inductive inference: Theory and methods. Computing Surveys, 15(3):237-269.
Bengio, Y. and Frasconi, P. (1994a) . Credit assignment through time: Alternatives
to backpropagation. In Cowan, J., Tesauro, G., and Alspector, J., editors,
Advances in Neural Information Processing Systems 6. Morgan Kaufmann.
Bengio, Y. and Frasconi, P. (1994b). An EM Approach to Learning Sequential
Behavior. Tech. Rep. RT-DSI/11-94, University of Florence.
Bengio, Y., De Mori, R., Flammia, G., and Kompe, R. (1992). Global optimization
of a neural network-hidden markov model hybrid. IEEE Transactions on Neural
Networks, 3(2):252-259.
Bengio, Y., Simard, P., and Frasconi, P. (1994) . Learning long-term dependencies
with gradient descent is difficult. IEEE Trans. Neural Networks, 5(2).
Bourlard, H. and Wellekens, C. (1990). Links between hidden markov models and
multilayer perceptrons. IEEE Trans. Pattern An. Mach. Intell., 12:1167-1178.
Bridle, J. S. (1989). Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters. In
D.S.Touretzky, ed., NIPS2, pages 211-217. Morgan Kaufmann.
Cacciatore, T. W. and Nowlan, S. J. (1994). Mixtures of controllers for jump
linear and non-linear plants. In Cowan, J. et. al., editors, Advances in Neural
Information Processing Systems 6, San Mateo, CA. Morgan Kaufmann.
Dempster, A. P., Laird, N. M., and Rubin, D. B. (1977). Maximum-likelihood from
incomplete data via the EM algorithm. J. Royal Stat. Soc. B,39:1-38.
Giles, C. L., Miller, C. B ., Chen, D., Sun, G. Z., Chen, H. H., and Lee, Y. C. (1992).
Learning and extracting finite state automata with second-order recurrent neural networks. Neural Computation, 4(3):393-405.
Jacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E. (1991). Adaptive
mixture of local experts. Neural Computation, 3:79-87.
Levinson, S. E., Rabiner, L. R., and Sondhi, M. M. (1983). An introduction to
the application of the theory of probabilistic functIons of a markov process to
automatic speech recognition. Bell System Technical Journal, 64(4):1035-1074.
Mozer, M. C. (1992). The induction of multiscale temporal structure. In Moody,
J. et. al., eds, NIPS 4 pages 275-282. Morgan Kaufmann.
Pollack, J. B. (1991) . The induction of dynamical recognizers. Machine Learning,
7(2):196-227.
Tomita, M. (1982). Dynamic construction of finite-state automata from examples
using hill-climbing. Proc. 4th Cog. Science Con!, pp. 105-108, Ann Arbor MI.
Watrous, R. 1. and Kuhn, G. M. (1992). Induction of finite-state languages using
second-order recurrent networks. Neural Computation, 4(3):406-414 .


----------------------------------------------------------------

title: 2428-clustering-with-the-connectivity-kernel.pdf

Clustering with the Connectivity Kernel

Bernd Fischer, Volker Roth and Joachim M. Buhmann
Institute of Computational Science
Swiss Federal Institute of Technology Zurich
CH-8092 Zurich, Switzerland
{bernd.fischer, volker.roth,jbuhmann}@inf.ethz.ch

Abstract
Clustering aims at extracting hidden structure in dataset. While the problem of finding compact clusters has been widely studied in the literature, extracting arbitrarily formed elongated structures is considered a
much harder problem. In this paper we present a novel clustering algorithm which tackles the problem by a two step procedure: first the data
are transformed in such a way that elongated structures become compact
ones. In a second step, these new objects are clustered by optimizing a
compactness-based criterion. The advantages of the method over related
approaches are threefold: (i) robustness properties of compactness-based
criteria naturally transfer to the problem of extracting elongated structures, leading to a model which is highly robust against outlier objects;
(ii) the transformed distances induce a Mercer kernel which allows us
to formulate a polynomial approximation scheme to the generally N Phard clustering problem; (iii) the new method does not contain free kernel
parameters in contrast to methods like spectral clustering or mean-shift
clustering.

1

Introduction

Clustering or grouping data is an important topic in machine learning and pattern recognition research. Among various possible grouping principles, those methods which try to
find compact clusters have gained particular importance. Presumingly the most prominent
method of this kind is the K-means clustering for vectorial data [6]. Despite the powerful
modeling capabilities of compactness-based clustering methods, they mostly fail in finding
elongated structures. The fast single linkage algorithm [9] is the most often used algorithm
to search for elongated structures, but it is known to be very sensitive to outliers in the
dataset. Mean shift clustering [3], another method of this class, is capable of extracting
elongated clusters only if all modes of the underlying probability distribution have one single maximum. Furthermore, a suitable kernel bandwidth parameter has to be preselected
[2]. Spectral clustering [10] shows good performance in many cases, but the algorithm is
only analyzed for special input instances while a complete analysis of the algorithm is still
missing. Concerning the preselection of a suitable kernel width, spectral clustering suffers
from similar problems as mean shift clustering.
In this paper we present an alternative method for clustering elongated structures. Apart
from the number of clusters, it is a completely parameter-free grouping principle. We build
up on the work on path-based clustering [7]. For a slight modification of the original prob-

lem we show that the defined path distance induces a kernel matrix fulfilling Mercers condition. After the computation of the path-based distance, the compactness-based pairwise
clustering principle is used to partition the data. While for the general N P-hard pairwise
clustering problem no approximation algorithms are known, we present a polynomial time
approximation scheme (PTAS) for our special case with path-based distances. The Mercer
property of these distances allows us to embed the data in a (n ? 1) dimensional vector
space even for non-metric input graphs. In this vector space, pairwise clustering reduces to
minimizing the K-means cost function in (n ? 1) dimensions [13]. For the latter problem,
however, there exists a PTAS [11].
In addition to this theoretical result, we also present an efficient practical algorithm resorting to a 2-approximation algorithm which is based on kernel PCA. Our experiments suggest that kernel PCA effectively reduces the noise in the data while preserving the coarse
cluster structure. Our method is compared to spectral clustering and mean shift clustering
on selected artificial datasets. In addition, the performance is demonstrated on the USPS
handwritten digits dataset.

2

Clustering by Connectivity

The main idea of our clustering criterion is to transform elongated structures into compact
ones in a preprocessing step. Given the transformed data, we then infer a clustering solution
by optimizing a compactness based criterion. The advantage of circumventing the problem
of directly finding connected (elongated) regions in the data as e.g. in the spanning tree approach is the following: while spanning tree algorithms are extremely sensitive to outliers,
the two-step procedure may benefit from the statistical robustness of certain compactness
based methods. Concerning the general case of datasets which are not given in a vector
space, but only characterized by pairwise dissimilarities, the pairwise clustering model has
been shown to be robust against outliers in the dataset [12]. It may, thus, be a natural
choice to formulate the second step as searching for the partition vector c ? {1, . . . , K} n
that minimizes the pairwise clustering cost function
PK
P
P
H PC (c; D) = ?=1 n1? i:ci =? j:cj =? dij ,
(1)
where K denotes the number of clusters, n? = |{i : ci = ?}| denotes the number of
objects in cluster ?, and dij is the pairwise ?effective? dissimilarity between objects i and
j as computed by a preprocessing step.
The idea of this preprocessing step is to define distances between objects by considering
certain paths through the total object set. The natural formalization of such path problems
is to represent the objects as a graph: consider a connected graph G = (V, E, d0 ) with n
vertices (the objects) and symmetric nonnegative edge weights d0ij on the edge (i, j) (the
original dissimilarities). Let us denote by Pij all paths from vertex i to vertex j. In order
to make those objects more similar which are connected by ?bridges? of other objects,
we define for each path p ? Pij the effective dissimilarity dpij between i and j connected
by p as the maximum weight on this path, i.e. the ?weakest link? on this path. The total
dissimilarity between vertices i and j is then defined as the minimum of all path-specific
effective dissimilarities dpij :
dij := min {

max

p?Pij 1?h?|p|?1

d0p[h]p[h+1] }.

(2)

Figure 1 illustrates the definition of the effective dissimilarity. If the objects are in the same
cluster their pairwise effective dissimilarities will be small (fig. 1(a)). If the two objects
belong to two different clusters, however, all paths contain at least one large dissimilarity
and the resulting effective dissimilarity will be large (fig. 1(b)). Note that single outliers
as in (fig. 1(a,b)) do not affect the basic structure in the path-based distances. A problem

dij
dij

(a)

(b)

dij

(c)

Figure 1: Effective dissimilarities. (a) If objects belong to the same high-density region, d ij is small.
(b) If they are in different regions, dij is larger. (c) To regions connected by a ?bridge?.

can only occur, if the point density along a ?bridge? between the two clusters is as high as
the density on the backbone of the clusters, see 1(c). In such a case, however, the points
belonging to the ?bridge? can hardly be considered as ?outliers?. The reader should notice
that the single linkage algorithm does not posses the robustness properties, since it will
separate the three most distant outlier objects in example 1(a) from the remaining data, but
it will not detect the dominant structure.
Summarizing the above model, we formalize the path-based clustering problem as:
INPUT: A symmetric (n ? n) matrix D 0 = (d0ij )1?i,j?n of nonnegative pairwise dissimilarities between n objects, with zero diagonal elements.
QUESTION: Find clusters by minimizing H PC (c; D), where the matrix D represents the
effective dissimilarities derived from D 0 by eq. (2).

3

The Connectivity Kernel

In this section we show that the effective dissimilarities induce a Mercer kernel on the
weighted graph G. The Mercer property will then allow us to derive several approximation
results for the N P-hard pairwise clustering problem in section 4.
Definition 1. A metric D is called ultra-metric if it satisfies the condition dij ?
max(dik , dkj ) for all distinct i, j, k.
Theorem 1. The dissimilarities defined by (2) induce an ultra-metric on G.
Proof. We have to check the axioms of a metric distance measure plus the restricted triangle inequality dij ? max(dik , dkj ): (i) dij ? 0, since the weights are nonnegative; (ii)
dij = dji , since we consider symmetric weights; (iii) dii = 0 follows immediately from
definition (2); (iv) The proof of the restricted triangle inequality follows by contradiction:
suppose, there exists a triple i, j, k for which dij > max(dik , dkj ). This situation, however,
contradicts the above definition (2) of dij : in this case there exists a path from i to j over
k, the weakest link of which is shorter than dij . Equation (2) then implies that dij must be
smaller or equal to max(dik , dkj ).
Definition 2. A metric D is `2 embeddable, if there exists a set of vectors {xi }ni=1 , xi ?
Rp , p ? n ? 1 such that for all pairs i, j kxi ? xj k2 = dij .
A proof for the following lemma has been given in [4]:
?
Lemma 1. For every ultra-metric D, D is `2 embeddable.
Now we are considered with a realization of such an embedding. We introduce the notion
of a centralized matrix. Let P be an (n ? n) matrix and let Q = In ? n1 en e>
n , where
>
en = (1, 1, . . . 1) is a n-vector of ones and In the n ? n identity matrix. We define the
centralized P as P c = QP Q.
The following lemma (for a proof see e.g. [15]) characterizes `2 embeddings:

?
Lemma 2. Given a metric D, D is `2 embeddable iff D c = QDQ is negative
(semi)definite.
The combination of both lemmata yields the following theorem.
Theorem 2. For the distance matrix D defined in the setting of theorem 1, the matrix
S c = ? 12 Dc with D c = QDQ is a Gram matrix or Mercer kernel. It contains dot products
between a set of vectors {xi }ni=1 with squared Euclidean distances kxi ? xj k22 = dij .
?
Proof. (i) Since D is ultra-metric, D is `2 embeddable by lemma 1, and D c is negative
(semi)definite by lemma (2). Thus, S c = ? 12 Dc is positive (semi)definite. As any positive
(semi)definite matrix, S c defines a Gram matrix or Mercer kernel. (ii) Since scij is a dotproduct between two vectors xi and xj , the squaredEuclidean distance
 between xi and xj
is defined by kxi ? xj k22 = scii + scjj ? 2scij = ? 21 dcii + dcjj ? 2dcij . With the definition
of the centralized distances,
 it can be seen easily that all but one term, namely the original
distance, cancel out: ? 12 dcii + dcjj ? 2dcij = dij .

4

Approximation Results

Pairwise clustering is known to be N P-hard [1]. To our knowledge there is no polynomial
time approximation algorithm known for the general case of pairwise clustering. For our
special case in which the data are transformed into effective dissimilarities, however, we
now present a polynomial time approximation scheme.
A Polynomial Time Approximation Scheme. Let us first consider the computation of the
effective dissimilarities D. Despite the fact that the path-based distance is a minimum over
all paths from i to j, the whole distance matrix can be computed in polynomial time.
Lemma 3. The path-based dissimilarity matrix D defined by equation 2 can be computed
in running time O(n2 log n).
Proof. The computation of the connectivity kernel matrix is an extention of Kruskal?s minimum spanning tree algorithm. We start with n clusters each containing one single object. In each iteration step the two clusters Ci and Cj are merged with minimal costs
dij = minp?Ci ,q?Cj d0pq where d0pq is the edge weight on the input graph. The link dij
gives the effective dissimilarity of all objects in Ci to all objects in Cj . To proof this, one
can consider the case, where dij is not the effective dissimilarity between Ci and Cj . Then
there exists a path over some other cluster Ck , where all objects on this path have a smaller
weight, implying the existence of another pair of clusters with smaller merging costs. The
running time is O(n2 log n) for the spanning tree algorithm on the the complete input graph
and additional O(n2 ) for filling all elements in the matrix D.

Let us now discuss the clustering step. Recall first the problem of K-means clustering:
given n vectors X = {x1 , . . . , xn ? Rp }, the task is to partition the vectors in such a way
that the squared Euclidean distance to the cluster centroids is minimized. The objective
function for K-means is given by
PK P
P
H KM (c; X ) = ?=1 i:ci =? (xi ? y? )2
where
y? = n1? j:cj =? xj (3)
Minimizing the K-means objective function for squared Euclidean distances is N P-hard
if the dimension of the vectors is growing with n.
Lemma 4. There exists a polynomial time approximation scheme (PTAS) for H KM in arbitrary dimensions and for fixed K.
Proof. In [11] Ostrovsky and Rabani presented a PTAS for K-means.
Using this approximation lemma we are able to proof the existence of a PTAS for pairwise
data clustering using the distance defined by (2).

Theorem 3. for distances defined by (2), there exists a PTAS for H PC .
Proof. By lemma 3 the dissimilarity matrix D can be computed in polynomial time. By
theorem 2 we can find vectors x1 , . . . xn ? Rp (p ? n ? 1) with dij = ||xi ? xj ||22 . For
squared Euclidean distances, however, there is an algebraic identity between H PC (c; D)
and H KM (c; X ) [13]. By lemma 4 there exists a PTAS for H KM and thus for H PC .
A 2-approximation by Kernel PCA. While the existence of a PTAS is an interesting
theoretical approximation result, it does not automatically follow that a PTAS can be used
in a constructive way to derive practical algorithms. Taking such a practical viewpoint,
we now consider another (weaker) approximation result from which, however, an efficient
algorithm can be designed easily. From the fact that we can define a connectivity kernel
matrix we can use kernel PCA [14] to reduce the data dimension. The vectors are projected
on the first principle components. Diagonalization of the centered kernel matrix S c leads to
S c = V t ?V , with an orthogonal matrix V = (v1 , . . . , vn ) containing the eigenvectors of
S c , and a diagonal matrix ? = diag(?1 , . . . , ?n ) containing the corresponding eigenvalues
on its diagonal. Assuming now that the eigenvalues are in descending
p (? 1 ? ?2 ?
Pp order
? ? ? ? ?n ), the data are projected on the first p eigenvectors: x0i = j=1 ?j vji .
Theorem 4. Embedding the path-based distances into RK by kernel PCA and enumerating
2
over all possible Voronoi partitions yields an O(nK +1 ) algorithm which approximates
path-based clustering within a constant factor of 2.

Proof. The solution of the K-means cost function induces a Voronoi partition on the
dataset. If the dimension p of the data is kept fix, the number of different Voronoi partitions is at most O(nKp ), and they can be enumerated in O(nKp+1 ) time [8]. Further, if
the embedding dimension is chosen as p = K, K-means in RK is a 2-approximation algorithm for K-means in Rn?1 [5]. Combining both results, we arrive at a 2-approximation
2
algorithm with running time O(nK +1 ).
Heuristics without approximation guarantees. The running time of the 2-approximation
algorithm may still be too large for many applications, therefore we will refer to two heuristic optimization methods without approximation guarantees. Instead of enumerating all
possible Voronoi partitions, one can simply partition the data with the fast classical Kmeans algorithm. In one sweep it assigns each object to the nearest centroid, while keeping
all other object assignments fixed. Then the centroids are relocated according to the new
assignments. Since the running time grows linear with the data dimension, it is useful to
first embed the data in K dimensions which leads us to a functional which optimal solution
is even in the worst case within a factor of two of the desired solution, as we know from
the above approximation results. In this reduced space, the K-means heuristics is applied
with the hope that there exist only few local minima in the low-dimensional subspace.
As a second heuristic one can apply Ward?s method which is an agglomerative optimization
of the K-means objective function.1 It starts with n clusters, each containing one object,
and in each step the two clusters that minimize the K-means objective function are merged.
Ward?s method produces a cluster hierarchy. For applications of this method see figure 3.

5

Experiments

We first compare our method with the classical single linkage algorithm on artificial data
consisting of three noisy spirals, see figure 2. Our main concern in these experiments is
the robustness against noise in the data. Figure 3(a) shows the dendrogram produced by
single linkage. The leaves of the tree are the objects of figure 2. For better visualization
of the tree structure, the bar diagrams below the tree show the labels of the three cluster
1
It has been shown in [12] that Ward?s method is an optimization heuristics for H P C . Due to the
equivalence of H P C and H KM in our special case, this property carries over to K-means.

(a)

(b)

(c)

Figure 2: Comparison to other clustering methods. (a) Mean shift clustering, (b) Spectral Clustering,
(c) Connectivity kernel clustering. (Color images at http://www.inf.ethz.ch/?befische/nips03)

(a)

(b)

(c)

Figure 3: Hierarchical Clustering Solutions for example 2(c). (a) Single Linkage, (b) Ward?s method
with connectivity kernel, applied to embedded objects in n ? 1 dimensions. (c) Ward?s method after
kernel PCA embedding in 3 dimensions.

solution as drawn in fig. 2(c). The height of the inner nodes depicts the merging costs for
two subtrees. Each level of the hierarchy is one cluster solution. It is obvious that the main
parts of the spiral arms are found, but the objects drawn on the right side are separated
from the rest of the cluster. The respective objects are the outliers that are separated in the
highest hierarchical levels of the algorithm. We conclude that for small K, single linkage
has the tendency to separates single outlier objects from the data.
By way of the connectivity kernel we can transform the original dyadic data to n ? 1
dimensional vectorial data. To show comparable results for the connectivity kernel, we
apply Ward?s method to the embedded vectors. Figure 3(b) shows the cluster hierarchy
for Ward?s method in the full space of n ? 1 dimensions. Opposed to the single linkage
results, the main structure of the spiral arms has been successfully found in the hierarchy
corresponding to the three cluster solution. Below the three cluster lever, the tree appears
to be very noisy. It should also be noticed that the costs of the three cluster solution are
not much larger as the costs of the four cluster solution, indicating that the three cluster
solution does not form a distinctly separated hierarchical level.
Figure 3(c) demonstrates that more distinctly separated levels can be found after applying
kernel PCA and embedding the objects into a low-dimensional space (here 3 dimensions).
Ward?s method is then applied to the embedded objects. One can see that the coarse struc-

ture of the tree has been preserved, while the costs of cluster solutions for K > 3 have been
shrunken towards zero. We conclude that PCA has the effect of de-noising the hierarchical
tree, leading to a more robust agglomerative algorithm.
Now we compare our results to other recently published clustering techniques, that have
been designed to extract elongated structures. Mean shift clustering [3] computes a trajectory of vectors towards the gradient of the underlying probability density. The probability
distribution is estimated with a density estimation kernel, e.g. a Gaussian kernel. The trajectories starting at each point in the feature space converge at the local maxima of the
probability distribution. Mean shift clustering is only applicable to finite dimensional vector spaces, because it implicitly involves density estimation. A potential shortcoming of
mean-shift clustering is the following: if the modes of the distribution have multiple local
maxima (as e.g. in the spiral arm example), there does not exist any kernel bandwidth to
successfully separate the data according to the underlying structure. In figure 2(a) the best
result for mean shift clustering is drawn. For smaller values of ? the spiral arms are further
subdivided into additional clusters, and for a larger bandwidth values, the result becomes
more and more similar to compactness-based criteria like K-means.
Spectral methods [10] have become quite popular in the last years. Usually the Laplacian
matrix based on a Gaussian kernel is computed. By way of PCA, the data are embedded
in a low dimensional space. The K-means algorithm on the embedded data then gives
the resulting partition. It has also been proposed to project the data on the unit sphere
before applying K-means. Spectral clustering with a Gaussian kernel is known to be able
to separate nested circles, but we observed that it has severe problems to extract the noisy
spiral arms, see 2(b). In spectral clustering, the kernel width ? is a free parameter which has
to be selected ?correctly?. If ? is too large, spectral clustering becomes similar to standard
K-means and fails to extract elongated structures. If, on the other hand, ? is too small, the
algorithm becomes increasingly sensitive to outliers, in the sense that it has the tendency to
separate single outlier objects.
Our approach to clustering with the connectivity kernel, however, could successfully extract
the three spiral arms as can be seen in figure 2(c). The reader should notice, that this method
does not require the user to preselect any kernel parameter.

(a)

(b)

(c)

Figure 4: Example from the USPS dataset. Training example of digits 2 and 9 embedded in two
dimensions. (a) Ground truth labels. (b) K-means labels and (c) clustering with connectivity kernel.

In a last experiment, we show the advantages of our method compared to a parameter-free
compactness criterion (K-means) on the problem of clustering digits ?2? and ?9? from the
USPS digits dataset. Figure 4 shows the clustering result of our method using the connectivity kernel. The 16x16 digit gray-value images of the USPS dataset are interpreted as
vectors and projected on the two leading principle components. In figure 4(a) the ground
truth solution is drawn. Figure 4(b) shows the partition by directly applying K-means clustering, and figure 4(c) shows the result produced by our method. Compared to the ground

truth solution, path-based clustering succeeded in extracting the elongated structures, resulting in a very small error of only 1.5% mislabeled digits. The compactness-based Kmeans method, on the other hand, produces clearly suboptimal clusters with an error rate
of 30.6%.

6

Conclusion

In this paper we presented a clustering approach, that is based on path-based distances in the
input graph. In a first step, elongated structures are transformed into compact ones, which
in the second step are partitioned by the compactness-based pairwise clustering method.
We showed that the transformed distances induce a Mercer kernel, which in turn allowed
us to derive a polynomial time approximation scheme for the generally N P-hard pairwise
clustering problem. Moreover, Mercers property renders it possible to embed the data
into low-dimensional subspaces by Kernel PCA. These embeddings form the basis for an
efficient 2-approximation algorithm, and also for de-noising the data to ?robustify? fast
agglomerative optimization heuristics. Compared to related methods like single linkage,
mean shift clustering and spectral clustering, our method has been shown to successfully
overcome the problem of sensitivity to outlier objects, while being capable of extracting
nested elongated structures. Our method does not involve any free kernel parameters, which
we consider to be a particular advantage over both mean shift? and spectral clustering.

References
[1] P. Brucker. On the complexity of clustering problems. Optimization and Operations Research,
pages 45?54, 1977.
[2] D. Comaniciu. An algorithm for data-driven bandwidth selection. IEEE T-PAMI, 25(2):281?
288, 2003.
[3] D. Comaniciu and P. Meer. Mean shift: A robust approach toward feature space analysis. IEEE
T-PAMI, 24(5):603?619, 2002.
[4] M. Deza and M. Laurent. Applications of cut polyhedra. J. Comp. Appl. Math., 55:191?247,
1994.
[5] P. Drineas, A. Frieze, R. Kannan, S. Vempala, and V. Vinay. Clustering on large graphs and
matrices. In Proc. of the ACM-SIAM Symp. on Discrete Algorithm., pages 291?299, 1999.
[6] R. Duda, P. Hart, and D. Stork. Pattern Classification. Wiley & Sons, 2001.
[7] B. Fischer and J.M. Buhmann. Path-based clustering for grouping of smooth curves and texture
segmentation. IEEE T-PAMI, 25(4):513?518, 2003.
[8] M. Inaba, N. Katoh, and H. Imai. Applications of weighted voronoi diagrams and randomization
to variance-based k-clustering. In 10th ACM Sympos. Computat. Geom., pages 332?339, 1994.
[9] A. Jain and R. Dubes. Algorithms for Clustering Data. Prentice Hall, 1988.
[10] A.Y. Ng, M.I. Jordan, and Y. Weiss. On spectral clustering: Analysis and an algorithm. In
NIPS, volume 14, pages 849?856, 2002.
[11] R. Ostrovsky and Y. Rabani. Polynomial time approximation schemes for geometric min-sum
median clustering. Journal of the ACM, 49(2):139?156, 2002.
[12] J. Puzicha, T. Hofmann, and J.M. Buhmann. A theory of proximity based clustering: Structure
detection by optimization. Pattern Recognition, 2000.
[13] V. Roth, J. Laub, J.M. Buhmann, and K.-R. Mu? ller. Going metric: Denoising pairwise data. In
NIPS, volume 15, 2003. to appear.
[14] B. Sch?olkopf, A. Smola, and K.-R. M?uller. Nonlinear component analysis as a kernel eigenvalue
problem. Neural Computation, 10:1299?1319, 1998.
[15] G. Young and A. S. Householder. Discussion of a set of points in terms of their mutual distances.
Psychometrica, 3:19?22, 1938.


----------------------------------------------------------------

title: 3957-penalized-principal-component-regression-on-graphs-for-analysis-of-subnetworks.pdf

Penalized Principal Component Regression on
Graphs for Analysis of Subnetworks

George Michailidis
Department of Statistics and EECS
University of Michigan
Ann Arbor, MI 48109
gmichail@umich.edu

Ali Shojaie
Department of Statistics
University of Michigan
Ann Arbor, MI 48109
shojaie@umich.edu

Abstract
Network models are widely used to capture interactions among component of
complex systems, such as social and biological. To understand their behavior, it
is often necessary to analyze functionally related components of the system, corresponding to subsystems. Therefore, the analysis of subnetworks may provide
additional insight into the behavior of the system, not evident from individual
components. We propose a novel approach for incorporating available network
information into the analysis of arbitrary subnetworks. The proposed method offers an efficient dimension reduction strategy using Laplacian eigenmaps with
Neumann boundary conditions, and provides a flexible inference framework for
analysis of subnetworks, based on a group-penalized principal component regression model on graphs. Asymptotic properties of the proposed inference method,
as well as the choice of the tuning parameter for control of the false positive rate
are discussed in high dimensional settings. The performance of the proposed
methodology is illustrated using simulated and real data examples from biology.

1

Introduction

Simultaneous analysis of groups of system components with similar functions, or subsystems, has
recently received considerable attention. This problem is of particular interest in high dimensional
biological applications, where changes in individual components may not reveal the underlying
biological phenomenon, whereas the combined effect of functionally related components could improve the efficiency and interpretability of results. This idea has motivated the method of gene set
enrichment analysis (GSEA), along with a number of related methods [1, 2]. The main premise
of this method is that by assessing the significance of sets rather than individual components (i.e.
genes), interactions among them can be preserved, and more efficient inference methods can be
developed. A different class of models (see e.g. [3, 4] and references therein) has focused on directly incorporating the network information in order to achieve better efficiency in assessing the
significance of individual components.
These ideas have been combined in [5, 6], by introducing a model for incorporating the regulatory
gene network, and developing an inference framework for analysis of subnetworks defined by biological pathways. In this frameworks, called NetGSA, a global model is introduced with parameters
1

for individual genes/proteins, and the parameters are then combined appropriately in order to assess
the significance of biological pathways. However, the main challenge in applying NetGSA in realworld biological applications is the extensive computational time. In addition, the total number of
parameters allowed in the model are limited by the available sample size n (see Section 5).
In this paper, we propose a dimension reduction technique for networks, based on Laplacian eigenmaps, with the goal of providing an optimal low-dimensional projection for the space of random
variables in each subnetwork. We then propose a general inference framework for analysis of subnetworks by reformulating the inference problem as a penalized principal regression problem on the
graph. In Section 2, we review the Laplacian eigenmaps and establish their connection to principal
component analysis (PCA) for random variables on a graph. Inference for significance of subnetworks is discussed in Section 3, where we introduce Laplacian eigenmaps with Neumann boundary
conditions and present the group-penalized principal component regression framework for analysis of arbitrary subnetworks. Results of applying the new methodology to simulated and real data
examples are presented in Section 4, and the results are summarized in Section 5.

2

Laplacian Eigenmaps

Consider p random variables Xi , i = 1, . . . , p (e.g. expression values of genes) defined on nodes of
an undirected (weighted) graph G = (V, E). Here V is the set of nodes of G and E ? V ?V its edge
set. Throughout this paper, we represent the edge set and the strength of associations among nodes
through the adjacency matrix of the graph A. Specifically, Ai j ? 0 and i and j are adjacent if the Ai j
(and hence A ji ) is non-zero. In this case we write i ? j. Finally, we denote the observed values of
the random variables by the n ? p data matrix X.
The subnetworks of interest are defined based on additional knowledge about their attributes and
functions. In biological applications, these subnetworks are defined by common biological function,
co-regulation or chromosomal location. The objective of the current paper is to develop dimension
reduction methods on networks, in order to assess the significance of a priori defined subnetworks
(e.g. biological pathways) with minimal information loss.
2.1

Graph Laplacian and Eigenmaps

Laplacian eigenmaps are defined using the eigenfunctions of the graph Laplacian, which is commonly used in spectral graph theory, computer science and image processing. Applications based
on Laplacian eigenmaps include image segmentation and the normalized cut algorithm of [7], spectral clustering [8, 9] and collaborative filtering [10].
The Laplacian matrix and its eigenvectors have also been used in biological applications. For example, in [11], the Laplacian matrix has been used to define a network-penalty for variable selection
on graphs, and the interpretation of Laplacian eigenmaps as a Fourier basis was exploited in [12] to
propose supervised and unsupervised classification methods.
Different definitions and representations have been proposed for the spectrum of a graph, and the
results may vary depending on the definition of the Laplacian matrix (see [13] for a review). Here,
we follow the notation in [13], and consider the normalized Laplacian matrix of the graph. To that
end, let D denote the diagonal degree matrix for A, i.e. Dii = ? j Ai j ? di , and define the Laplacian
matrix of the graph by L = D?1/2 (D ? A)D?1/2 , or alternatively
?
Ajj
?
j = i, d j 6= 0
? 1 ? dj
?
Ai j
Li j =
??
j?i
?
di d j
?
?
0
o.w.
2

It can be shown that [13] L is positive semidefinite with eigenvalues 0 = ?0 ? ?1 ? . . . ? ? p?1 ? 2.
Its eigenfunctions are known as the spectrum of G , and optimize the Rayleigh quotient
hg, L gi ?i? j ( f (i) ? f ( j))2
,
=
hg, gi
? j f ( j)2 d j

(1)

It can be seen from (1), that the 0-eigenvalue of L is g = D1/2 1, corresponding to the average
over the graph G . The first non-zero eigenvalue ?1 is the harmonic eigenfunction of L , which
corresponds to the Laplace-Beltrami operator on Reimannian manifolds, and is given by
? j?i ( f (i) ? f ( j))2
f ?D1
? j f ( j)2 d j

?1 = inf

More generally, denoting by Ck?1 the projection to the subspace of the first k ? 1 eigenfunctions,
?k =
2.2

? j?i ( f (i) ? f ( j))2
.
f ?DCk?1
? j f ( j)2 d j
inf

Principal Component Analysis on Graphs

Previous applications of the graph Laplacian and its spectrum often focus on the properties of the
graph; however, the connection to the probability distribution of the random variables on nodes of
the graph has not been strongly emphasized. In graphical models, the undirected graph G among
random variables corresponds naturally to a Markov random field [14]. The following result establishes the relationship between the Laplacian eigenmaps and the principal components of the
random variables defined on the nodes of the graph, in case of Gaussian observations.
Lemma 1. Let X = (X1 , . . . , X p ) be random variables defined on the nodes of graph G = (V, E)
and denote by L and L + the Laplacian matrix of G and its Moore-Penrose generalized inverse.
If X ? N(0, ?), then L and L + correspond to ? and ?, respectively (? ? ??1 ). In addition, let
?0 , . . . , ? p?1 denote the eigenfunctions corresponding to eigenvalues of L . Then ?0 , . . . , ? p?1 are
the principal components of X, with ?0 corresponding to the leading principal component.
Proof. For Gaussian random variables, the inverse covariance (or precision) matrix has the same
non-zero pattern as the adjacency matrix of the graph, i.e. for i 6= j, ?i j = 0 iff Ai j = 0. Moreover,
?ii = ?i?2 , where ?i2 is the partial variance of Xi (see e.g. [15]). However, using the conditional
autoregression (CAR) representation of Gaussian Markov random fields [16], we can write
E(Xi |X?i ) = ? ci j X j

(2)

j?i

where ?i ? {1 . . . p}\i and C = [ci j ] has the same non-zero pattern as the adjacency matrix of
the graph A, and amounts to a proper probability
 distribution for X. In 	particular, by Brook?s
Lemma [16] it follows from (2) that fX (x) ? exp ?1/2xT (0, T ?1 (I p ?C))x , where T = diag[?i2 ].
Therefore, ? = T ?1 (I p ?C) and hence (I p ?C) should be PD.
However, since L = I p ?D?1/2 AD?1/2 is PSD, we can set C = D?1/2 AD?1/2 ?? I for any ? > 0. In
other words, (I p ?C) = L + ? I p , which implies that L? ? L + ? I p = T ?, and hence L? ?1 = ?T ?1 .
Taking limit as ? ? 0, it follows that L and L + correspond to ? and ?, respectively.
The second part follows directly from the above connection between L? ?1 and ?. In particular,
suppose, without loss of generality, that ?i2 = 1. Then, it is easily seen that the principal components
of X are given by eigenfunctions of L? ?1 , which are in turn equal to the eigenfunctions of L? with
the ordering of the eigenvalues reversed. However, since eigenfunctions of L + ? I p and L are
equal, the principal components of X are obtained from eigenfunctions of L .
3

X1

?1

X2

?2

X3

Figure 1: Left: A simple subnetwork of interest, marked with the dotted circle. Right: Illustration
of the Neumann random walk, the dotted curve indicates the boundary of the subnetwork.
Remark 2. An alternative justification for the above result, for general probability distributions
defined on graphs, can be given by assuming that the graph represents ?similarities? among random
variables and using an optimal embedding of graph G in a lower dimensional Euclidean space1 .
In the case of one dimensional embedding, the goal is to find an embedding v = (v1 , . . . , v p )T that
preserves the distances among the nodes of the graph. The objective function of the embedding
problem is then given by Q = ?i, j (vi ? v j )2 Ai j , or alternatively Q = 2vT (D ? A)v [17]. Thus, the
optimal embedding is found by solving argminvT Dv=1 vT (D ? A)v. Setting u = D1/2 v, this is solved
by finding the eigenvector corresponding to the smallest eigenvalue of L .
Lemma 1 provides an efficient dimension reduction framework that summarizes the information in
the entire network into few feature vectors. Although the resulting dimension reduction method
can be used efficiently in classification (as in [12]), the eigenfunctions of G do not provide any
information about significance of arbitrary subnetworks, and therefore cannot be used to analyze
the changes in subnetworks. In the next section, we introduce a restricted version of Laplacian
eigenmaps, and discuss the problem of analysis of subnetworks.

3

Analysis of Subnetworks and PCR on Graph (GPCR)

In [5], the authors argue that to analyze the effect of subnetworks, the test statistic needs to represent
the pure effect of the subnetwork, without being influenced by external nodes, and propose an
inference procedure based on mixed linear models to achieve this goal. However, in order to achieve
dimension reduction, we need a method that only incorporates local information at the level of each
subnetwork, and possibly its neighbors (see the left panel of Figure 1).
Using the connection of the Laplace operator in Reimannian manifolds to heat flow (see e.g. [17]),
the problem of analysis of arbitrary subnetworks can be reformulated as a heat equation with boundary conditions. It then follows that in order to assess the ?effect? of each subnetwork, the appropriate
boundary conditions should block the flow of heat at the boundary of the set. This corresponds to
insulating the boundary, also known as the Neumann boundary condition. For the general heat
equation ?(v, x), this boundary condition is given by ?? ?v (x) = 0 at each boundary point x, where v is
the normal direction orthogonal to the tangent hyperplane at x.
The eigenvalues of subgraphs with boundary conditions are studied in [13]. In particular, let S
be any (connected) subnetwork of G , and denote by ? S the boundary of S in G . The Neumann
boundary condition states that for every x ? ? S, ?y:{x,y}?? S ( f (x) ? f (y)) = 0.
The Neumann eigenfunctions of S are then the optimizers of the restricted Rayleigh quotient
?S,i = inf sup

f g?Ci?1

?{t,u}?S?? S ( f (t) ? f (u))2
2
?t?S ( f (t) ? g(t)) dt

where Ci?1 is the projection to the space of previous eigenfunctions.
1 For

unweighted graphs, this justification was given by [17], using the unnormlized Laplacian matrix.

4

In [13], a connection between the Neumann boundary conditions and a reflected random walk on the
graph is established, and it is shown that the Neumann eigenvectors can be alternatively calculated
from the eigenvectors of the transition probability matrix of this reflected random walk, also known
as the Neumann random walk (see [13] for additional details). Here, we generalize this idea to
weighted adjacency matrices.
Let P? and P denote the transition probability matrix of the reflected random walk, and the original
random walk defined on G , respectively. Noting that P = D?1 A, we can extend the results in [13]
as follows. For the general case of weighted graphs, define the transition probability matrix of the
reflected random walk by
?
j ? i, i, j ? S
? Pi j
A A
Pi j + dik d 0k j j ? k ? i, k ?
/S
P?i j =
(3)
i k
?
0
o.w.
where dk0 = ?i?k,i?S Aki denotes the degree of the node k in S. Then, the Neumann eigenvalues are
?
given by ?i = 1 ? ?i , where ?i is the ith eigenvalue of P.
Remark 3. The connection with the Neumann random walk also sheds light into the effect of the
proposed boundary condition on the joint probability distribution of the random variables on the
graph. To illustrate this, consider the simple graph in the right panel of Figure 1. For the moment,
suppose that the random variables X1 , X2 , X3 are Gaussian, and the edges from X1 and X2 to X3 are
directed. As discussed in [5], the joint probability distribution of the random variables on the graph
is then given by linear structural equation models:
X1
X2
X3

= ?1
= ?2
= ?1 X1 + ?1 X2

?

Y = ??,

?=

1
0
?1

0
1
?2

0
0
1

!

Then, the conditional probability distribution of X1 and X2 given X3 , is Gaussian, with the inverse
covariance matrix given by


1 + ?12 ?1 ?2
(4)
?1 ?2 1 + ?22
A comparison between (3) and (4) then reveals that the proposed Neumann random walk corresponds to conditioning on the boundary variables, if the edges going from the set S to its boundary
are directed. The reflected random walk, for the original problem, therefore corresponds to first
setting all the influences from other nodes in the graph to nodes in the set S to zero (resulting in
directed edges) and then conditioning on the boundary variables. Therefore, the proposed method
offers a compromise compared to the full model of [5], based on local information at the level of
each subnetwork.
3.1

Group-Penalized PCR on Graph

Using the Neumann eigenvectors of subnetworks, we now define a principal component regression
on graphs, which can be used to analyze the significance of subnetworks. Let N j denote the |S j | ?
m j matrix of the m j smallest Neumann eigenfunctions for subgraph S j . Also, let X ( j) be the n ? |S j |
matrix of observations for the j-th subnetwork. An m j -dimensional projection of the original data
matrix X ( j) is then given by X? ( j) = X ( j) N j . Different methods can be used in order to determine
the number of eigenfunctions m j for each subnetwork. A simple procedure determines a predefined
threshold for the proportion of variance explained by each eigenfunction. These proportions can be
determined by considering the reciprocal of Neumann eigenvalues (ignoring the 0-eigenvalue). To
simplify the presentation, here we assume m j = m, ? j.
5

The significance of subnetwork S j is a function of the combined effect of all the nodes, captured
by the transformed data matrix X? ( j) . This can be evaluated by forming a multivariate ANOVA
(MANOVA) model. Formally, let y be the mn ? 1 vector of observations obtained by stacking all
the transformed data matrices X? ( j) . Also, let X be the mn ? Jmr design matrix corresponding to the
experimental settings, where r is the number of parameters used to model experimental conditions,
and ? be the vector of regression coefficients. For simplicity, here we focus on the case of a twoclass inference problem (e.g. treatment vs. control). Extensions to more general experimental
settings follow naturally and are discussed in Section 5.
To evaluate the combined effect of each subnetwork, we impose a group penalty on the coefficient
of the regression of y on the design matrix X . In particular, using the group lasso penalty [18], we
estimate the significance of the subnetwork by solving the following optimization problem2
(
)
J

argmin n?1 ky ? ? X ( j) ? ( j) k22 + ?
?

j=1

J

? k? ( j) k2

(5)

j=1

where J is the total number of subnetworks considered and X ( j) and ? ( j) denote the columns of
X , and entries of ? corresponding to the subnetwork j, respectively.
In equation (5), ? is the tuning parameter and is usually determined by performing k-fold cross validation or evaluation on independent data sets. However, since the goal of our analysis is to determine
the significance of subnetworks, ? should be determined so that the probability of false positives is
controlled at a given significance level ?. Here we adapt the approach in [20] and determine the
optimal value of ? so that the family-wise error rate (FWER) in repeated sampling with replacement
(bootstrap) is controlled at the level ?. Specifically, let qi? be the total number of subnetworks considered significant based on the value of ? in the ith bootstrap sample. Let ? be the threshold for
( j)
selection of variables as significant. In other words, if Pi is the probability of selecting the coefficients corresponding to subnetwork j in the ith bootstrap sample, the subnetwork j is considered
p
( j)
significant if max? Pi ? ?. Using this method, we select ? such that qi? = (2? ? 1)? p.3
The following result shows that the proposed methodology correctly selects the significant subnetworks, while controlling FWER at level ?. We begin by introducing some additional notations and
assumptions. We assume the columns of design matrix X are normalized so that n?1 Xi T Xi = 1,
Throughout this paper, we consider the case where the total number of nodes in the graph p, and the
number of design parameters r are allowed to diverge (the p  n setting). In addition, let s be the
total number of non-zero elements in the true regression vector ? .
Theorem 4. Suppose that m, n ? 1 and there exists ? ? 1 and t ? s ? 1 such that n?1 X T Xi j ?
(7?t)?1 for all i 6= j. Also suppose that for j 6= k, the transformed random variables
X? ( j) and X? (k)
p
are independent. If the tuning parameter ? is selected such that such that q? = (2? ? 1)?rp,
(i) there exists ? = ? (n, p) > 0 such that ? ? 0 as n ? ? and with probability at least 1 ? ? the
significant subnetworks are correctly selected with high probability,
(ii) the family-wise error rate is controlled at the level ?.
Outline of the Proof. First note that the MANOVA model presented above can be reformulated as a
multi-task
learning problem [21]. Upon establishing the fact that for the proposed tuning parameter
p
? ? log p/(nm3/2 ), it follows from the results in [22] that for each bootstrap sample, there exists
? = ?(n) > 0 such that with probability at least 1 ? (rp)?? the significant subnetworks are correctly
selected. Thus if ? ? 1?(rp)?? , the coefficients for significant subnetworks are included in the final
2 The

problem in (5) can be solved using the R-package grplasso [19].
details for this method are given in [20], but are excluded here due to space limitations.

3 Additional

6

?
model with hight probability. In particular, it can be shown that ? = ?{ B(1 ? (rp)?? ? ?)/2},
where B is the number of bootstrap samples and ? is the cumulative normal distribution. This
proves the first claim.
Next, note that the normality assumption, and the fact that the eigenfunctions within each sub( j)
network are orthogonal, imply that for each j, X?i , i = 1, . . . , m are independent. Moreover, the
assumption of independence of X? ( j) and X? (k) for j 6= k implies that the values of y are independent
realizations of i.i.d standard normal random variables. On the other hand, the KarushKuhnTucker
conditions for the optimization problem in (5) imply that ? ( j) 6= 0 iff (nm)(?1) h(y ? X ? ), X ( j) i =
sgn (?? ( j) )?, where hx, yi denotes their inner product. It is hence clear that 1[? ( j) 6=0] are exchangeable.
Combining this with the first part of the theorem, the claim follows from Theorem 1 of [20].
Remark 5. The main assumption of Theorem 4 is the independence of the variables in different subnetworks. Although this is not satisfied in general problems, it may be satisfied by the conditioning
argument of Remark 3. It is possible to further relax this assumption using an argument similar to
Theorem 2 of [20], but we do not pursue this here.

4

Experiments

We illustrate the performance of the proposed method using simulated data motivated by biological
applications, as well as a real data application based on gene expression analysis. In the simulation,
we generate a small network of 80 nodes (genes), with 8 subnetworks. The random variables (expression levels of genes) are generated according to a normal distribution with mean ?. Under the
null hypothesis, ?null = 1 and the association weight ? for all edges of the network is set to 0.2. The
setting of parameters under the alternative hypothesis are given in Table 1, where ?alt = 3. These
settings are illustrated in the left panel of Figure 2. Table 1 also includes the estimated powers of
the tests for subnetworks based on 200 simulations with n = 50 observations. It can be seen that the
proposed GPCR method offers improvements over GSEA [1], especially in case of subnetworks 3
and 6. However, it results in a less accurate inference compared to NetGSA [5].
In [5], the pathways involved in Galactose utilization in yeast were analyzed based on the data from
[23], and the performances of the NetGSA and GSEA methods were compared. The interactions
among genes, along with significance of individual genes (based on single gene analysis) are given
in the right panel of Figure 2, and the results of significance analysis based on NetGSA, GSEA
and the proposed GPCR are given in Table 2. As in the simulated example, the results of this
analysis indicate that GPCR results in improved efficiency over GSEA, while failing to detect the
significance of some of the pathways detected by NetGSA.

5

Conclusion

We proposed a principal component regression method for graphs, called GPCR, using Laplacian
eigenmaps with Neumann boundary conditions. The proposed method offers a systematic approach

Table 1: Parameter settings under the alternative and estimated powers for the simulation study.
Subnet
1
2
3
4

Parameter Setting
% ?alt
?
0.05
0.2
0.20
0.2
0.50
0.2
0.80
0.2

Estimated Powers
NetGSA
GPCR
GSEA
0.02
0.08
0.01
0.03
0.21
0.02
1.00
0.65
0.27
1.00
0.81
0.90

Subnet
5
6
7
8

7

Parameter Setting
% ?alt
?
0.05
0.6
0.20
0.6
0.50
0.6
0.80
0.6

Estimated Powers
NetGSA
GPCR
GSEA
0.94
0.41
0.12
1.00
0.61
0.15
1.00
0.99
0.97
1.00
0.99
1.00

Figure 2: Left: Setting of the simulation parameters under the alternative hypothesis. Right: Network of yeast genes involved in Galactose utilization.
for dimension reduction in networks, with a priori defined subnetworks of interest. It can also incorporate both weighted and unweighted adjacency matrices and can be easily extended to analyzing
complex experimental conditions through the framework of linear models. This method can also be
used in longitudinal and time-course studies.
Our simulation studies, and the real data example indicate that the proposed GPCR method offers
significant improvements over the methods of gene set enrichment analysis (GSEA). However, it
does not achieve optimal powers in comparison to NetGSA. This difference in power may be attributable to the mechanism of incorporating the network information in the two methods: while
NetGSA incorporates the full network information, GPCR only account for local network information, at the level of each subnetwork, and restricts the interactions with the rest of the network based
on the Neumann boundary condition. However, the most computationally involved step in NetGSA requires O(p3 ) operation, whereas the computational cost of GPCR is O(m3 ). It is clear that
since m  p in most applications, GPCR could result in significant improvement in terms of computational time and memory requirements for analysis of high dimensional networks. In addition,
NetGSA requires that r < n, whilst the dimension reduction and the penalization of the proposed
GPCR removes the need for any such restriction and facilitates the analysis of complex experiments
in the settings with small sample sizes.
Acknowledgments
Funding for this work was provided by NIH grants 1RC1CA145444-0110 and 5R01LM010138-02.

Table 2: Significance of pathways in Galactose utilization.
PATHWAY
rProtein Synthesis
Glycolytic Enzymes
RNA Processing
Fatty Acid Oxidation
O2 Stress
Mating, Cell Cycle
Vesicular Transport
Amino Acid Synthesis

Size
28
16
75
7
13
58
19
30

NetGSA
X

GPCR

X

X

GSEA

PATHWAY
Sugar Transport
Glycogen Metabolism
Stress
Metal Uptake
Respiration
Gluconeogenesis
Galactose Utilization

8

Size
2
12
12
4
9
7
12

NetGSA

GPCR

X

X

GSEA

X
X

X

X

References
[1] A. Subramanian, P. Tamayo, V.K. Mootha, S. Mukherjee, B.L. Ebert, M.A. Gillette, A. Paulovich, S.L.
Pomeroy, T.R. Golub, E.S. Lander, et al. Gene set enrichment analysis: A knowledge-based approach
for interpreting genome-wide expression profiles. Proceedings of the National Academy of Sciences,
102(43):15545?15550, 2005.
[2] B. Efron and R. Tibshirani. On testing the significance of sets of genes. Annals of Applied Statistics,
1(1):107?129, 2007.
[3] T. Ideker, O. Ozier, B. Schwikowski, and A.F. Siegel. Discovering regulatory and signalling circuits in
molecular interaction networks. Bioinformatics, 18(1):S233?S240, 2002.
[4] Zhi Wei and Li Hongzhe. A markov random field model for network-based analysis of genomic data.
Bioinformatics, 2007.
[5] A. Shojaie and G. Michailidis. Analysis of gene sets based on the underlying regulatory network. Journal
of Computational Biology, 16(3):407?426, 2009.
[6] A. Shojaie and G. Michailidis. Network enrichment analysis in complex experiments. Statisitcal Applications in Genetics and Molecular Biology, 9(1), Article 22, 2010.
[7] J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Transactions on pattern analysis
and machine intelligence, 22(8):888?905, 2000.
[8] M. Saerens, F. Fouss, L. Yen, and P. Dupont. The principal components analysis of a graph, and its
relationships to spectral clustering. Machine Learning: ECML 2004, pages 371?383, 2004.
[9] A.Y. Ng, M.I. Jordan, and Y. Weiss. On spectral clustering: Analysis and an algorithm. Advances in
neural information processing systems, 2:849?856, 2002.
[10] F. Fouss, A. Pirotte, J.M. Renders, and M. Saerens. A novel way of computing dissimilarities between
nodes of a graph, with application to collaborative filtering and subspace projection of the graph nodes.
In European Conference on Machine Learning Proceedings, ECML, 2004.
[11] C. Li and H. Li. Variable Selection and Regression Analysis for Graph-Structured Covariates with an
Application to Genomics. Annals of Applied Statistics, in press, 2010.
[12] F. Rapaport, A. Zinovyev, M. Dutreix, E. Barillot, and J.P. Vert. Classification of microarray data using
gene networks. BMC bioinformatics, 8(1):35, 2007.
[13] F.R.K. Chung. Spectral graph theory. American Mathematical Society, 1997.
[14] S.L. Lauritzen. Graphical models. Oxford Univ Press, 1996.
[15] H. Rue and L. Held. Gaussian Markov random fields: theory and applications. Chapman & Hall, 2005.
[16] J. Besag. Spatial interaction and the statistical analysis of lattice systems. Journal of the Royal Statistical
Society. Series B (Methodological), 36(2):192?236, 1974.
[17] M. Belkin and P. Niyogi. Laplacian eigenmaps and spectral techniques for embedding and clustering.
Advances in neural information processing systems, 1:585?592, 2002.
[18] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. Journal of
Royal Statistical Society. Series B Statistical Methodology, 68(1):49, 2006.
[19] L. Meier, S. Van de Geer, and P. Buhlmann. The group lasso for logistic regression. Journal of Royal
Statistical Society. Series B Statistical Methodology, 70(1):53, 2008.
[20] N. Meinshausen and P. B?uhlmann. Stability selection. Preprint, arXiv, 809, 2009.
[21] A. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-task feature learning. Machine Learning,
73(3):243?272, 2008.
[22] K. Lounici, M. Pontil, A.B. Tsybakov, and S. van de Geer. Taking Advantage of Sparsity in Multi-Task
Learning. Preprint, arXiv, 903, 2009.
[23] T. Ideker, V. Thorsson, J.A. Ranish, R. Christmas, J. Buhler, J.K. Eng, R. Bumgarner, D.R. Goodlett,
R. Aebersold, and L. Hood. Integrated genomic and proteomic analyses of a systematically perturbed
metabolic network. Science, 292(5518):929, 2001.

9


----------------------------------------------------------------

title: 4878-understanding-dropout.pdf

Understanding Dropout

Peter Sadowski
Department of Computer Science
University of California, Irvine
Irvine, CA 92697
pjsadows@ics.uci.edu

Pierre Baldi
Department of Computer Science
University of California, Irvine
Irvine, CA 92697
pfbaldi@uci.edu

Abstract
Dropout is a relatively new algorithm for training neural networks which relies
on stochastically ?dropping out? neurons during training in order to avoid the
co-adaptation of feature detectors. We introduce a general formalism for studying dropout on either units or connections, with arbitrary probability values, and
use it to analyze the averaging and regularizing properties of dropout in both linear and non-linear networks. For deep neural networks, the averaging properties
of dropout are characterized by three recursive equations, including the approximation of expectations by normalized weighted geometric means. We provide
estimates and bounds for these approximations and corroborate the results with
simulations. Among other results, we also show how dropout performs stochastic
gradient descent on a regularized error function.

1

Introduction

Dropout is an algorithm for training neural networks that was described at NIPS 2012 [7]. In its
most simple form, during training, at each example presentation, feature detectors are deleted with
probability q = 1 ? p = 0.5 and the remaining weights are trained by backpropagation. All weights
are shared across all example presentations. During prediction, the weights are divided by two.
The main motivation behind the algorithm is to prevent the co-adaptation of feature detectors, or
overfitting, by forcing neurons to be robust and rely on population behavior, rather than on the
activity of other specific units. In [7], dropout is reported to achieve state-of-the-art performance on
several benchmark datasets. It is also noted that for a single logistic unit dropout performs a kind of
?geometric averaging? over the ensemble of possible subnetworks, and conjectured that something
similar may occur also in multilayer networks leading to the view that dropout may be an economical
approximation to training and using a very large ensemble of networks.
In spite of the impressive results that have been reported, little is known about dropout from a
theoretical standpoint, in particular about its averaging, regularization, and convergence properties.
Likewise little is known about the importance of using q = 0.5, whether different values of q can
be used including different values for different layers or different units, and whether dropout can be
applied to the connections rather than the units. Here we address these questions.

2

Dropout in Linear Networks

It is instructive to first look at some of the properties of dropout in linear networks, since these can
be studied exactly in the most general setting of a multilayer feedforward network described by an
underlying acyclic graph. The activity in unit i of layer h can be expressed as:
Sih (I) =

XX
l<h

hl l
wij
Sj

j

1

with Sj0 = Ij

(1)

where the variables w denote the weights and I the input vector. Dropout applied to the units can be
expressed in the form
Sih =

XX
l<h

hl l l
wij
?j Sj

with

Sj0 = Ij

(2)

j

where ?jl is a gating 0-1 Bernoulli variable, with P (?jl = 1) = plj . Throughout this paper we assume
that the variables ?jl are independent of each other, independent of the weights, and independent of
the activity of the units. Similarly, dropout applied to the connections leads to the random variables
Sih =

XX
l<h

hl hl l
?ij
wij Sj

with

Sj0 = Ij

(3)

j

For brevity in the rest of this paper, we focus exclusively on dropout applied to the units, but all the
results remain true for the case of dropout applied to the connections with minor adjustments.
For a fixed input vector, the expectation of the activity of all the units, taken over all possible realizations of the gating variables hence all possible subnetworks, is given by:
E(Sih ) =

XX
l<h

hl l
wij
pj E(Sjl )

for h > 0

(4)

j

with E(Sj0 ) = Ij in the input layer. In short, the ensemble average can easily be computed by
hl
hl l
feedforward propagation in the original network, simply replacing the weights wij
by wij
pj .

3
3.1

Dropout in Neural Networks
Dropout in Shallow Neural Networks

Pn
Consider first a single logistic unit with n inputs O = ?(S) = 1/(1 + ce??S ) and S = 1 wj Ij .
To achieve the greatest level of generality, we assume that the unit produces different outputs
OP
1 , . . . , Om , corresponding to different sums S1 . . . , Sm with different probabilities P1 , . . . , Pm
( Pm = 1). In the most relevant case, these outputs and these sums are associated with the
m = 2n possible subnetworks of the unit. The probabilities P1 , . . . , Pm could be generated, for
instance, by using Bernoulli gating variables, although this isP
not necessary for this derivation. It is
useful to define the following four quantities: the mean E =
Pi Oi ; the mean of the complements
P
Q
E0 =
Pi (1 ? Oi ) = 1 ? E; the weighted geometric
mean
(W GM ) G = i OiPi ; and the
Q
weighted geometric mean of the complements G0 = i (1 ? Oi )Pi . We also define the normalized
weighted geometric mean N W GM = G/(G + G0 ). We can now prove the key averaging theorem
for logistic functions:
1
= ?(E(S))
1 + ce??E(S)

(5)

1
1
Q
Q
=
(1?Oi )Pi
(1??(Si ))Pi
1 + Q Pi
1 + Q ?(S )Pi

(6)

N W GM (O1 , . . . , Om ) =
To prove this result, we write
N W GM (O1 , . . . , Om ) =

Oi

i

??x

The logistic function satisfies the identity [1 ? ?(x)]/?(x) = ce

and thus

1
1
P
Q ??S P =
N W GM (O1 , . . . , Om ) =
= ?(E(S))
(7)
i
i
??
Pi Si
1 + [ce
]
1 + ce
Thus in the case of Bernoulli gating variables, we can compute the N WP
GM over all possible
n
dropout configurations by simple forward propagation by: N W GM = ?( 1 wj pj Ij ). A similar
result is true also for normalized exponential transfer functions. Finally, one can also show that
the only class of functions f that satisfy N W GM (f ) = f (E) are the constant functions and the
logistic functions [1].
2

3.2

Dropout in Deep Neural Networks

We can now deal with the most interesting case of deep feedforward networks of sigmoidal units 1 ,
described by a set of equations of the form
Oih = ?(Sih ) = ?(

XX

hl l
wij
Oj )

with

Oj0 = Ij

(8)

j

l<h

where Oih is the output of unit i in layer h. Dropout on the units can be described by
Oih = ?(Sih ) = ?(

XX
l<h

hl l l
wij
?j Oj )

with Oj0 = Ij

(9)

j

using the Bernoulli selector variables ?jl . For each sigmoidal unit
N W GM (Oih ) = Q

h P (N )
N (Oi )
Q
h
P
(N
)
+ N (1 ?
N (Oi )

Q

Oih )P (N )

(10)

where N ranges over all possible subnetworks. Assume for now that the N W GM provides a
good approximation to the expectation (this point will be analyzed in the next section). Then the
averaging properties of dropout are described by the following three recursive equations. First the
approximation of means by NWGMs:
E(Oih ) ? N W GM (Oih )

(11)

Second, using the result of the previous section, the propagation of expectation symbols:


N W GM (Oih ) = ?ih E(Sih )

(12)

And third, using the linearity of the expectation with respect to sums, and to products of independent
random variables:
E(Sih ) =

XX
l<h

hl l
wij
pj E(Ojl )

(13)

j

Equations 11, 12, and 13 are the fundamental equations explaining the averaging properties of the
dropout procedure. The only approximation is of course Equation 11 which is analyzed in the next
section. If the network contains linear units, then Equation 11 is not necessary for those units and
their average can be computed exactly. In the case of regression with linear units in the top layers,
this allows one to shave off one layer of approximations. The same is true in binary classification
by requiring the output layer to compute directly the N W GM of the ensemble rather than the
expectation. It can be shown that for any error function that is convex up (?), the error of the mean,
weighted geometric mean, and normalized weighted geometric mean of an ensemble is always less
than the expected error of the models [1].
Equation 11 is exact if and only if the numbers Oih are identical over all possible subnetworks N .
h
Thus it is useful to
 measure
 the consistency C(Oi , I) of neuron i in layer h for input I by using
the variance V ar Oih (I) taken over all subnetworks N and their distribution when the input I is
fixed. The larger the variance is, the less consistent the neuron is, and the worse we can expect
the approximation in Equation 11 to be. Note that for a random variable O in [0,1] the variance
cannot exceed 1/4 anyway. This is because V ar(O) = E(O2 ) ? (E(O))2 ? E(O) ? (E(O))2 =
E(O)(1 ? E(O)) ? 1/4. This measure can also be averaged over a training set or a test set.
1
Given the results of the previous sections, the network can also include linear units or normalized exponential units.

3

4

The Dropout Approximation

Given a set of numbers O1 , . . . , Om between 0 and 1, with probabilities P1 , . . . , PM (corresponding
to the outputs of a sigmoidal neuron for a fixed input and different subnetworks), we are primarily
interested in the approximation of E by N W GM . The N W GM provides a good approximation
because we show below that to a first order of approximation: E ? N W GM and E ? G. Furthermore, there are formulae in the literature for bounding the error E ? G in terms of the consistency
(e.g. the Cartwright and Field inequality [6]). However, one can suspect that the N W GM provides
even a better approximation to E than the geometric mean. For instance, if the numbers Oi satisfy
0 < Oi ? 0.5 (consistently low), then
G
E
G
? 0 and therefore G ?
?E
(14)
G0
E
G + G0
This is proven by applying Jensen?s inequality to the function ln x ? ln(1 ? x) for x ? (0, 0.5]. It is
also known as the Ky Fan inequality [2, 8, 9].
To get even better results, one must consider a second order approximation. For this, we write
Oi = 0.5 + i with 0 ? |i | ? 0.5. Thus we have E(O) = 0.5 + E() and V ar(O) = V ar().
Using a Taylor expansion:
?
?
?  
X
X pi (pi ? 1)
X
1 Y X pi
1
G=
(2i )n = ?1 +
pi 2i +
(2i )2 +
4pi pj i j + R3 (i )?
2 i n=0 n
2
2
i
i
i<j
(15)
where R3 (i ) is the remainder and
R3 (i ) =

 
pi
(2i )3
3 (1 + ui )3?pi

(16)

where |ui | ? 2|i |. Expanding the product gives
X
X
1 X
1
G= +
pi i +(
i )2 ?
pi 2i +R3 () = +E()?V ar()+R3 () = E(O)?V ar(O)+R3 ()
2 i
2
i
(17)
By symmetry, we have
G0 =

Y
(1 ? Oi )pi = 1 ? E(O) ? V ar(O) + R3 ()

(18)

i

where R3 () is the higher order remainder. Neglecting the remainder and writing E = E(O) and
V = V ar(O) we have
G
E?V
G0
1?E?V
?
and
?
(19)
0
0
G+G
1 ? 2V
G+G
1 ? 2V
Thus, to a second order, the differences between the mean and the geometric mean and the normalized geometric means satisfy
E?G?V

and E ?

G
V (1 ? 2E)
?
0
G+G
1 ? 2V

(20)

and
G0
V (1 ? 2E)
?
(21)
0
G+G
1 ? 2V
Finally it is easy to check that the factor (1 ? 2E)/(1 ? 2V ) is always less or equal to 1. In addition
we always have V ? E(1 ? E), with equality achieved only for 0-1 Bernoulli variables. Thus
1 ? E ? G0 ? V

and

(1 ? E) ?

4

V |1 ? 2E|
E(1 ? E)|1 ? 2E|
G
|?
?
? 2E(1 ? E)|1 ? 2E|
(22)
G + G0
1 ? 2V
1 ? 2V
The first inequality is optimal in the sense that it is attained in the case of a Bernoulli variable
with expectation E and, intuitively, the second inequality shows that the approximation error is
always small, regardless of whether E is close to 0, 0.5, or 1. In short, the NWGM provides a
very good approximation to E, better than the geometric mean G. The property is always true to
a second order of approximation and it is exact when the activities are consistently low, or when
N W GM ? E, since the latter implies G ? N W GM ? E. Several additional properties of the
dropout approximation, including the extension to rectified linear units and other transfer functions,
are studied in [1].
|E ?

5

Dropout Dynamics

Dropout performs gradient descent on-line with respect to both the training examples and the ensemble of all possible subnetworks. As such, and with the appropriately decreasing learning rates,
it is almost surely convergent like other forms of stochastic gradient descent [11, 4, 5]. To further
understand the properties of dropout, it is again instructive to look at the properties of the gradient
in the linear case.
5.1

Single Linear Unit

In the case of a single linear unit, consider the two error functions EEN S and ED associated with
the ensemble of all possible subnetworks and the network with dropout. For a single input I, these
are defined by:

EEN S

n
X
1
1
2
= (t ? OEN S ) = (t ?
pi wi Ii )2
2
2
i=1

(23)

n
X
1
1
(t ? OD )2 = (t ?
?i wi Ii )2
2
2
i=1

(24)

ED =

We use a single training input I for notational simplicity, otherwise the errors of each training
example can be combined additively. The learning gradient is given by
?EEN S
?OEN S
= ?(t ? OEN S )
= ?(t ? OEN S )pi Ii
?wi
?wi
X
?ED
?OD
= ?(t ? OD )
= ?(t ? OD )?i Ii = ?t?i Ii + wi ?i2 Ii2 +
wj ?i ?j Ii Ij
?wi
?wi

(25)

(26)

j6=i

The dropout gradient is a random variable and we can take its expectation. A short calculation yields

E

?ED
?wi


=

?EEN S
?EEN S
+ wi pi (1 ? pi )Ii2
+ wi Ii2 V ar(?i )
?wi
?wi

(27)

Thus, remarkably, in this case the expectation of the gradient with dropout is the gradient of the
regularized ensemble error
n

E = EEN S +

1X 2 2
w I V ar(?i )
2 i=1 i i

(28)

The regularization term is the usual weight decay or Gaussian prior term based on the square of the
weights to prevent overfitting. Dropout provides immediately the magnitude of the regularization
term which is adaptively scaled by the inputs and by the variance of the dropout variables. Note that
pi = 0.5 is the value that provides the highest level of regularization.
5

5.2

Single Sigmoidal Unit

The previous result generalizes to a sigmoidal unit O = ?(S) = 1/(1 + ce??S ) trained to minimize
the relative entropy error E = ?(t log O + (1 ? t) log(1 ? O)). In this case,
?ED
?S
= ??(t ? O)
= ??(t ? O)?i Ii
(29)
?wi
?wi
The terms O and Ii are not independent but using a Taylor expansion with the N W GM approximation gives

?EEN S
?ED
?
+ ?? 0 (SEN S )wi Ii2 V ar(?i )
(30)
E
?wi
?wi
P
with SEN S = j wj pj Ij . Thus, as in the linear case, the expectation of the dropout gradient is approximately the gradient of the ensemble network regularized by weight decay terms with the proper
adaptive coefficients. A similar analysis, can be carried also for a set of normalized exponential
units and for deeper networks [1].


5.3

Learning Phases and Sparse Coding

During dropout learning, we can expect three learning phases: (1) At the beginning of learning, when
the weights are typically small and random, the total input to each unit is close to 0 for all the units
and the consistency is high: the output of the units remains roughly constant across subnetworks
(and equal to 0.5 with c = 1). (2) As learning progresses, activities tend to move towards 0 or 1
and the consistency decreases, i.e. for a given input the variance of the units across subnetworks
increases. (3) As the stochastic gradient learning procedure converges, the consistency of the units
converges to a stable value.
Finally, for simplicity, assume that dropout
is applied only in layer h where the units have an output
P
hl l l
of the form Oih = ?(Sih ) and Sih = l<h wij
?j Oj . For a fixed input, Ojl is a constant since dropout
is not applied to layer l. Thus
V ar(Sih ) =

X

hl 2
(wij
) (Ojl )2 plj (1 ? plj )

(31)

l<h

under the usual assumption that the selector variables ?jl are independent of each other. Thus
V ar(Sih ) depends on three factors. Everything else being equal, it is reduced by: (1) Small weights
which goes together with the regularizing effect of dropout; (2) Small activities, which shows that
dropout is not symmetric with respect to small or large activities. Overall, dropout tends to favor
small activities and thus sparse coding; and (3) Small (close to 0) or large (close to 1) values of the
dropout probabilities plj . Thus values plj = 0.5 maximize the regularization effect but may also lead
to slower convergence to the consistent state. Additional results and simulations are given in [1].

6

Simulation Results

We use Monte Carlo simulation to partially investigate the approximation framework embodied by
the three fundamental dropout equations 11, 12, and 13, the accuracy of the second-order approximation and bounds in Equations 20 and 22, and the dynamics of dropout learning. We experiment
with an MNIST classifier of four hidden layers (784-1200-1200-1200-1200-10) that replicates the
results in [7] using the Pylearn2 and Theano software libraries[12, 3]. The network is trained with
a dropout probability of 0.8 in the input, and 0.5 in the four hidden layers. For fixed weights and
a fixed input, 10,000 Monte Carlo simulations are used to estimate the distribution of activity O
in each neuron. Let O? be the activation under the deterministic setting with the weights scaled
appropriately.
The left column of Figure 1 confirms empirically that the second-order approximation in Equation
20 and the bound in Equation 22 are accurate. The right column of Figure 1 shows the difference between the true ensemble average E(O) and the prediction-time neuron activity O? . This difference
grows very slowly in the higher layers, and only for active neurons.
6

Figure 1: Left: The difference E(O) ? N W GM (O), it?s second-order approximation in Equation
20, and the bound from Equation 22, plotted for four hidden layers and a typical fixed input. Right:
The difference between the true ensemble average E(O) and the final neuron prediction O? .
Next, we examine the neuron consistency during dropout training. Figure 2a shows the three phases
of learning for a typical neuron. In Figure 2b, we observe that the consistency does not decline in
higher layers of the network.
One clue into how this happens is the distribution of neuron activity. As noted in [10] and section 5
above, dropout training results in sparse activity in the hidden layers (Figure 3). This increases the
consistency of neurons in the next layer.

7

(a) The three phases of learning. For a particular input, a typical active neuron (red) starts out
with low variance, experiences a large increase in
variance during learning, and eventually settles to
some steady constant value. In contrast, a typical
inactive neuron (blue) quickly learns to stay silent.
Shown are the mean with 5% and 95% percentiles.

(b) Consistency does not noticeably decline in the upper layers. Shown here are the mean Std(O) for active
neurons (0.1 < O after training) in each layer, along
with the 5% and 95% percentiles.

Figure 2

Figure 3: In every hidden layer of a dropout trained network, the distribution of neuron activations
O? is sparse and not symmetric. These histograms were totalled over a set of 100 random inputs.

8

References
[1] P. Baldi and P. Sadowski. The Dropout Learning Algorithm. Artificial Intelligence, 2014. In
press.
[2] E. F. Beckenbach and R. Bellman. Inequalities. Springer-Verlag Berlin, 1965.
[3] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins, J. Turian,
D. Warde-Farley, and Y. Bengio. Theano: a CPU and GPU math expression compiler. In
Proceedings of the Python for Scientific Computing Conference (SciPy), Austin, TX, June
2010. Oral Presentation.
[4] L. Bottou. Online algorithms and stochastic approximations. In D. Saad, editor, Online Learning and Neural Networks. Cambridge University Press, Cambridge, UK, 1998.
[5] L. Bottou. Stochastic learning. In O. Bousquet and U. von Luxburg, editors, Advanced Lectures
on Machine Learning, Lecture Notes in Artificial Intelligence, LNAI 3176, pages 146?168.
Springer Verlag, Berlin, 2004.
[6] D. Cartwright and M. Field. A refinement of the arithmetic mean-geometric mean inequality.
Proceedings of the American Mathematical Society, pages 36?38, 1978.
[7] G. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. http://arxiv.org/abs/1207.0580,
2012.
[8] E. Neuman and J. S?andor. On the Ky Fan inequality and related inequalities i. MATHEMATICAL INEQUALITIES AND APPLICATIONS, 5:49?56, 2002.
[9] E. Neuman and J. Sandor. On the Ky Fan inequality and related inequalities ii. Bulletin of the
Australian Mathematical Society, 72(1):87?108, 2005.
[10] S. Nitish. Improving Neural Networks with Dropout. PhD thesis, University of Toronto,
Toronto, Canada, 2013.
[11] H. Robbins and D. Siegmund. A convergence theorem for non negative almost supermartingales and some applications. Optimizing methods in statistics, pages 233?257, 1971.
[12] D. Warde-Farley, I. Goodfellow, P. Lamblin, G. Desjardins, F. Bastien, and Y. Bengio.
pylearn2. 2011. http://deeplearning.net/software/pylearn2.

9


----------------------------------------------------------------

title: 5059-compete-to-compute.pdf

Compete to Compute

Rupesh Kumar Srivastava, Jonathan Masci, Sohrob Kazerounian,
Faustino Gomez, J?rgen Schmidhuber
IDSIA, USI-SUPSI
Manno?Lugano, Switzerland
{rupesh, jonathan, sohrob, tino, juergen}@idsia.ch

Abstract
Local competition among neighboring neurons is common in biological neural networks (NNs). In this paper, we apply the concept to gradient-based,
backprop-trained artificial multilayer NNs. NNs with competing linear
units tend to outperform those with non-competing nonlinear units, and
avoid catastrophic forgetting when training sets change over time.

1

Introduction

Although it is often useful for machine learning methods to consider how nature has arrived
at a particular solution, it is perhaps more instructive to first understand the functional
role of such biological constraints. Indeed, artificial neural networks, which now represent
the state-of-the-art in many pattern recognition tasks, not only resemble the brain in a
superficial sense, but also draw on many of its computational and functional properties.
One of the long-studied properties of biological neural circuits which has yet to fully impact
the machine learning community is the nature of local competition. That is, a common
finding across brain regions is that neurons exhibit on-center, off-surround organization
[1, 2, 3], and this organization has been argued to give rise to a number of interesting
properties across networks of neurons, such as winner-take-all dynamics, automatic gain
control, and noise suppression [4].
In this paper, we propose a biologically inspired mechanism for artificial neural networks
that is based on local competition, and ultimately relies on local winner-take-all (LWTA)
behavior. We demonstrate the benefit of LWTA across a number of different networks and
pattern recognition tasks by showing that LWTA not only enables performance comparable
to the state-of-the-art, but moreover, helps to prevent catastrophic forgetting [5, 6] common
to artificial neural networks when they are first trained on a particular task, then abruptly
trained on a new task. This property is desirable in continual learning wherein learning
regimes are not clearly delineated [7]. Our experiments also show evidence that a type of
modularity emerges in LWTA networks trained in a supervised setting, such that different
modules (subnetworks) respond to different inputs. This is beneficial when learning from
multimodal data distributions as compared to learning a monolithic model.
In the following, we first discuss some of the relevant neuroscience background motivating
local competition, then show how we incorporate it into artificial neural networks, and
how LWTA, as implemented here, compares to alternative methods. We then show how
LWTA networks perform on a variety of tasks, and how it helps buffer against catastrophic
forgetting.

2

Neuroscience Background

Competitive interactions between neurons and neural circuits have long played an important
role in biological models of brain processes. This is largely due to early studies showing that
1

many cortical [3] and sub-cortical (e.g., hippocampal [1] and cerebellar [2]) regions of the
brain exhibit a recurrent on-center, off-surround anatomy, where cells provide excitatory
feedback to nearby cells, while scattering inhibitory signals over a broader range. Biological
modeling has since tried to uncover the functional properties of this sort of organization,
and its role in the behavioral success of animals.
The earliest models to describe the emergence of winner-take-all (WTA) behavior from local
competition were based on Grossberg?s shunting short-term memory equations [4], which
showed that a center-surround structure not only enables WTA dynamics, but also contrast
enhancement, and normalization. Analysis of their dynamics showed that networks with
slower-than-linear signal functions uniformize input patterns; linear signal functions preserve
and normalize input patterns; and faster-than-linear signal functions enable WTA dynamics.
Sigmoidal signal functions which contain slower-than-linear, linear, and faster-than-linear
regions enable the supression of noise in input patterns, while contrast-enhancing, normalizing and storing the relevant portions of an input pattern (a form of soft WTA). The
functional properties of competitive interactions have been further studied to show, among
other things, the effects of distance-dependent kernels [8], inhibitory time lags [8, 9], development of self-organizing maps [10, 11, 12], and the role of WTA networks in attention [13].
Biological models have also been extended to show how competitive interactions in spiking
neural networks give rise to (soft) WTA dynamics [14], as well as how they may be efficiently
constructed in VLSI [15, 16].
Although competitive interactions, and WTA dynamics have been studied extensively in the
biological literature, it is only more recently that they have been considered from computational or machine learning perspectives. For example, Maas [17, 18] showed that feedforward
neural networks with WTA dynamics as the only non-linearity are as computationally powerful as networks with threshold or sigmoidal gates; and, networks employing only soft
WTA competition are universal function approximators. Moreover, these results hold, even
when the network weights are strictly positive?a finding which has ramifications for our
understanding of biological neural circuits, as well as the development of neural networks
for pattern recognition. The large body of evidence supporting the advantages of locally
competitive interactions makes it noteworthy that this simple mechanism has not provoked
more study by the machine learning community. Nonetheless, networks employing local
competition have existed since the late 80s [21], and, along with [22], serve as a primary
inspiration for the present work. More recently, maxout networks [19] have leveraged locally
competitive interactions in combination with a technique known as dropout [20] to obtain
the best results on certain benchmark problems.

3

Networks with local winner-take-all blocks

This section describes the general network architecture with locally competing neurons.
The network consists of B blocks which are organized into layers (Figure 1). Each block,
bi , i = 1..B, contains n computational units (neurons), and produces an output vector yi ,
determined by the local interactions between the individual neuron activations in the block:
yij = g(h1i , h2i ..., hni ),

(1)

where g(?) is the competition/interaction function, encoding the effect of local interactions
in each block, and hji , j = 1..n, is the activation of the j-th neuron in block i computed by:
T
hi = f (wij
x),

(2)

where x is the input vector from neurons in the previous layer, wij is the weight vector of
neuron j in block i, and f (?) is a (generally non-linear) activation function. The output
activations y are passed as inputs to the next layer. In this paper we use the winner-take-all
interaction function, inspired by studies in computational neuroscience. In particular, we
use the hard winner-take-all function:
 j
hi if hji ? hki , ?k = 1..n
yij =
0 otherwise.
In the case of multiple winners, ties are broken by index precedence. In order to investigate the capabilities of the hard winner-take-all interaction function in isolation, f (x) = x
2

Figure 1: A Local Winner-Take-All (LWTA) network with blocks of size two showing the
winning neuron in each block (shaded) for a given input example. Activations flow forward
only through the winning neurons, errors are backpropagated through the active neurons.
Greyed out connections do not propagate activations. The active neurons form a subnetwork
of the full network which changes depending on the inputs.

(identity) is used for the activation function in equation (2). The difference between this
Local Winner Take All (LWTA) network and a standard multilayer perceptron is that no
non-linear activation functions are used, and during the forward propagation of inputs, local
competition between the neurons in each block turns off the activation of all neurons except
the one with the highest activation. During training the error signal is only backpropagated
through the winning neurons.
In a LWTA layer, there are as many neurons as there are blocks active at any one time for
a given input pattern1 . We denote a layer with blocks of size n as LWTA-n. For each input
pattern presented to a network, only a subgraph of the full network is active, e.g. the highlighted neurons and synapses in figure 1. Training on a dataset consists of simultaneously
training an exponential number of models that share parameters, as well as learning which
model should be active for each pattern. Unlike networks with sigmoidal units, where all of
the free parameters need to be set properly for all input patterns, only a subset is used for
any given input, so that patterns coming from very different sub-distributions can potentially be modelled more efficiently through specialization. This modular property is similar
to that of networks with rectified linear units (ReLU) which have recently been shown to
be very good at several learning tasks (links with ReLU are discussed in section 4.3).

4
4.1

Comparison with related methods
Max-pooling

Neural networks with max-pooling layers [23] have been found to be very useful, especially
for image classification tasks where they have achieved state-of-the-art performance [24, 25].
These layers are usually used in convolutional neural networks to subsample the representation obtained after convolving the input with a learned filter, by dividing the representation
into pools and selecting the maximum in each one. Max-pooling lowers the computational
burden by reducing the number of connections in subsequent convolutional layers, and adds
translational/rotational invariance.
1
However, there is always the possibility that the winning neuron in a block has an activation
of exactly zero, so that the block has no output.

3

0.5

0.5

0

0.8

0.8

before

after

0.8
0.8

before

after

(a) max-pooling

(b) LWTA

Figure 2: Max-pooling vs. LWTA. (a) In max-pooling, each group of neurons in a layer
has a single set of output weights that transmits the winning unit?s activation (0.8 in this
case) to the next layer, i.e. the layer activations are subsampled. (b) In an LWTA block,
there is no subsampling. The activations flow into subsequent units via a different set of
connections depending on the winning unit.

At first glance, the max-pooling seems very similar to a WTA operation, however, the two
differ substantially: there is no downsampling in a WTA operation and thus the number of
features is not reduced, instead the representation is "sparsified" (see figure 2).
4.2

Dropout

Dropout [20] can be interpreted as a model-averaging technique that jointly trains several
models sharing subsets of parameters and input dimensions, or as data augmentation when
applied to the input layer [19, 20]. This is achieved by probabilistically omitting (?dropping?) units from a network for each example during training, so that those neurons do not
participate in forward/backward propagation. Consider, hypothetically, training an LWTA
network with blocks of size two, and selecting the winner in each block at random. This
is similar to training a neural network with a dropout probability of 0.5. Nonetheless, the
two are fundamentally different. Dropout is a regularization technique while in LWTA the
interaction between neurons in a block replaces the per-neuron non-linear activation.
Dropout is believed to improve generalization performance since it forces the units to learn
independent features, without relying on other units being active. During testing, when
propagating an input through the network, all units in a layer trained with dropout are
used with their output weights suitably scaled. In an LWTA network, no output scaling is
required. A fraction of the units will be inactive for each input pattern depending on their
total inputs. Viewed this way, WTA is restrictive in that only a fraction of the parameters
are utilized for each input pattern. However, we hypothesize that the freedom to use different
subsets of parameters for different inputs allows the architecture to learn from multimodal
data distributions more accurately.
4.3

Rectified Linear units

Rectified Linear Units (ReLU) are simply linear neurons that clamp negative activations to
zero (f (x) = x if x > 0, f (x) = 0 otherwise). ReLU networks were shown to be useful for
Restricted Boltzmann Machines [26], outperformed sigmoidal activation functions in deep
neural networks [27], and have been used to obtain the best results on several benchmark
problems across multiple domains [24, 28].
Consider an LWTA block with two neurons compared to two ReLU neurons, where x1 and
x2 are the weighted sum of the inputs to each neuron. Table 1 shows the outputs y1 and
y2 in all combinations of positive and negative x1 and x2 , for ReLU and LWTA neurons.
For both ReLU and LWTA neurons, x1 and x2 are passed through as output in half of the
possible cases. The difference is that in LWTA both neurons are never active or inactive at
the same time, and the activations and errors flow through exactly one neuron in the block.
For ReLU neurons, being inactive (saturation) is a potential drawback since neurons that
4

Table 1: Comparison of rectified linear activation and LWTA-2.
x1

x2

Positive
Positive
Negative

Positive
Negative
Negative

Positive
Negative
Negative

Positive
Positive
Negative

ReLU neurons
y1
y2
x1 > x2
x1
x2
x1
0
0
0
x2 > x1
x1
x2
0
x2
0
0

LWTA neurons
y1
y2
x1
x1
x1

0
0
0

0
0
0

x2
x2
x2

do not get activated will not get trained, leading to wasted capacity. However, previous
work suggests that there is no negative impact on optimization, leading to the hypothesis
that such hard saturation helps in credit assignment, and, as long as errors flow through
certain paths, optimization is not affected adversely [27]. Continued research along these
lines validates this hypothesis [29], but it is expected that it is possible to train ReLU
networks better.
While many of the above arguments for and against ReLU networks apply to LWTA networks, there is a notable difference. During training of an LWTA network, inactive neurons
can become active due to training of the other neurons in the same block. This suggests
that LWTA nets may be less sensitive to weight initialization, and a greater portion of the
network?s capacity may be utilized.

5

Experiments

In the following experiments, LWTA networks were tested on various supervised learning
datasets, demonstrating their ability to learn useful internal representations without utilizing
any other non-linearities. In order to clearly assess the utility of local competition, no special
strategies such as augmenting data with transformations, noise or dropout were used. We
also did not encourage sparse representations in the hidden layers by adding activation
penalties to the objective function, a common technique also for ReLU units. Thus, our
objective is to evaluate the value of using LWTA rather than achieving the absolute best
testing scores. Blocks of size two are used in all the experiments.2
All networks were trained using stochastic gradient descent with mini-batches, learning rate
lt and momentum mt at epoch t given by

?0 ?t if ?t > ?min
?t =
?
otherwise
 t min
m
+
(1 ? Tt )mf if t < T
i
T
mt =
pf
if t ? T
where ? is the learning rate annealing factor, ?min is the lower learning rate limit, and
momentum is scaled from mi to mf over T epochs after which it remains constant at
mf . L2 weight decay was used for the convolutional network (section 5.2), and max-norm
normalization for other experiments. This setup is similar to that of [20].
5.1

Permutation Invariant MNIST

The MNIST handwritten digit recognition task consists of 70,000 28x28 images (60,000
training, 10,000 test) of the 10 digits centered by their center of mass [33]. In the permutation
invariant setting of this task, we attempted to classify the digits without utilizing the 2D
structure of the images, e.g. every digit is a vector of pixels. The last 10,000 examples in the
training set were used for hyperparameter tuning. The model with the best hyperparameter
setting was trained until convergence on the full training set. Mini-batches of size 20 were
2

To speed up our experiments, the Gnumpy [30] and CUDAMat [31] libraries were used.

5

Table 2: Test set errors on the permutation invariant MNIST dataset for methods without
data augmentation or unsupervised pre-training
Activation
Sigmoid [32]
ReLU [27]
ReLU + dropout in hidden layers [20]
LWTA-2

Test Error
1.60%
1.43%
1.30%
1.28%

Table 3: Test set errors on MNIST dataset for convolutional architectures with no data
augmentation. Results marked with an asterisk use layer-wise unsupervised feature learning
to pre-train the network and global fine tuning.
Architecture
2-layer CNN + 2 layer MLP [34] *
2-layer ReLU CNN + 2 layer LWTA-2
3-layer ReLU CNN [35]
2-layer CNN + 2 layer MLP [36] *
3-layer ReLU CNN + stochastic pooling [33]
3-layer maxout + dropout [19]

Test Error
0.60%
0.57%
0.55%
0.53%
0.47%
0.45%

used, the pixel values were rescaled to [0, 1] (no further preprocessing). The best model
obtained, which gave a test set error of 1.28%, consisted of three LWTA layers of 500
blocks followed by a 10-way softmax layer. To our knowledge, this is the best reported
error, without utilizing implicit/explicit model averaging, for this setting which does not use
deformations/noise to enhance the dataset or unsupervised pretraining. Table 2 compares
our results with other methods which do not use unsupervised pre-training. The performance
of LWTA is comparable to that of a ReLU network with dropout in the hidden layers. Using
dropout in input layers as well, lower error rates of 1.1% using ReLU [20] and 0.94% using
maxout [19] have been obtained.
5.2

Convolutional Network on MNIST

For this experiment, a convolutional network (CNN) was used consisting of 7 ? 7 filters in
the first layer followed by a second layer of 6 ? 6, with 16 and 32 maps respectively, and
ReLU activation. Every convolutional layer is followed by a 2 ? 2 max-pooling operation.
We then use two LWTA-2 layers each with 64 blocks and finally a 10-way softmax output
layer. A weight decay of 0.05 was found to be beneficial to improve generalization. The
results are summarized in Table 3 along with other state-of-the-art approaches which do not
use data augmentation (for details of convolutional architectures, see [33]).
5.3

Amazon Sentiment Analysis

LWTA networks were tested on the Amazon sentiment analysis dataset [37] since ReLU units
have been shown to perform well in this domain [27, 38]. We used the balanced subset of the
dataset consisting of reviews of four categories of products: Books, DVDs, Electronics and
Kitchen appliances. The task is to classify the reviews as positive or negative. The dataset
consists of 1000 positive and 1000 negative reviews in each category. The text of each review
was converted into a binary feature vector encoding the presence or absence of unigrams
and bigrams. Following [27], the 5000 most frequent vocabulary entries were retained as
features for classification. We then divided the data into 10 equal balanced folds, and
tested our network with cross-validation, reporting the mean test error over all folds. ReLU
activation was used on this dataset in the context of unsupervised learning with denoising
autoencoders to obtain sparse feature representations which were used for classification. We
trained an LWTA-2 network with three layers of 500 blocks each in a supervised setting to
directly classify each review as positive or negative using a 2-way softmax output layer. We
obtained mean accuracies of Books: 80%, DVDs: 81.05%, Electronics: 84.45% and Kitchen:
85.8%, giving a mean accuracy of 82.82%, compared to 78.95% reported in [27] for denoising
autoencoders using ReLU and unsupervised pre-training to find a good initialization.
6

Table 4: LWTA networks outperform sigmoid and ReLU activation in remembering dataset
P1 after training on dataset P2.
Testing error on P1
After training on P1
After training on P2

6

LWTA
1.55 ? 0.20%
6.12 ? 3.39%

Sigmoid
1.38 ? 0.06%
57.84 ? 1.13%

ReLU
1.30 ? 0.13%
16.63 ? 6.07%

Implicit long term memory

This section examines the effect of the LWTA architecture on catastrophic forgetting. That
is, does the fact that the network implements multiple models allow it to retain information
about dataset A, even after being trained on a different dataset B? To test for this implicit
long term memory, the MNIST training and test sets were each divided into two parts, P1
containing only digits {0, 1, 2, 3, 4}, and P2 consisting of the remaining digits {5, 6, 7, 8, 9}.
Three different network architectures were compared: (1) three LWTA layers each with 500
blocks of size 2, (2) three layers each with 1000 sigmoidal neurons, and (3) three layers each
of 1000 ReLU neurons. All networks have a 5-way softmax output layer representing the
probability of an example belonging to each of the five classes. All networks were initialized
with the same parameters, and trained with a fixed learning rate and momentum.
Each network was first trained to reach a 0.03 log-likelihood error on the P1 training set.
This value was chosen heuristically to produce low test set errors in reasonable time for
all three network types. The weights for the output layer (corresponding to the softmax
classifier) were then stored, and the network was trained further, starting with new initial
random output layer weights, to reach the same log-likelihood value on P2. Finally, the
output layer weights saved from P1 were restored, and the network was evaluated on the
P1 test set. The experiment was repeated for 10 different initializations.
Table 4 shows that the LWTA network remembers what was learned from P1 much better
than sigmoid and ReLU networks, though it is notable that the sigmoid network performs
much worse than both LWTA and ReLU. While the test error values depend on the learning
rate and momentum used, LWTA networks tended to remember better than the ReLU
network by about a factor of two in most cases, and sigmoid networks always performed
much worse. Although standard network architectures are known to suffer from catastrophic
forgetting, we not only show here, for the first time, that ReLU networks are actually quite
good in this regard, and moreover, that they are outperformed by LWTA. We expect this
behavior to manifest itself in competitive models in general, and to become more pronounced
with increasingly complex datasets. The neurons encoding specific features in one dataset
are not affected much during training on another dataset, whereas neurons encoding common
features can be reused. Thus, LWTA may be a step forward towards models that do not
forget easily.

7

Analysis of subnetworks

A network with a single LWTA-m of N blocks consists of mN subnetworks which can be
selected and trained for individual examples while training over a dataset. After training,
we expect the subnetworks consisting of active neurons for examples from the same class to
have more neurons in common compared to subnetworks being activated for different classes.
In the case of relatively simple datasets like MNIST, it is possible to examine the number
of common neurons between mean subnetworks which are used for each class. To do this,
which neurons were active in the layer for each example in a subset of 10,000 examples were
recorded. For each class, the subnetwork consisting of neurons active for at least 90% of the
examples was designated the representative mean subnetwork, which was then compared to
all other class subnetworks by counting the number of neurons in common.
Figure 3a shows the fraction of neurons in common between the mean subnetworks of each
pair of digits. Digits that are morphologically similar such as ?3? and ?8? have subnetworks
with more neurons in common than the subnetworks for digits ?1? and ?2? or ?1? and ?5?
which are intuitively less similar. To verify that this subnetwork specialization is a result
of training, we looked at the fraction of common neurons between all pairs of digits for the
7

untrained
trained

0.4

0.7
0.6
0.5
0.4

0.3

0.3

0.2
0

1

2

3

4 5 6
Digits

7

8

9

0.2
0

10

20

30

40

50

Fraction of neurons in common

Digits

0
1
2
3
4
5
6
7
8
9

0.1

MNIST digit pairs
(b)

(a)

Figure 3: (a) Each entry in the matrix denotes the fraction of neurons that a pair of MNIST
digits has in common, on average, in the subnetworks that are most active for each of the
two digit classes. (b) The fraction of neurons in common in the subnetworks of each of the
55 possible digit pairs, before and after training.
same 10000 examples both before and after training (Figure 3b). Clearly, the subnetworks
were much more similar prior to training, and the full network has learned to partition its
parameters to reflect the structure of the data.

8

Conclusion and future research directions

Our LWTA networks automatically self-modularize into multiple parameter-sharing subnetworks responding to different input representations. Without significant degradation of
state-of-the-art results on digit recognition and sentiment analysis, LWTA networks also
avoid catastrophic forgetting, thus retaining useful representations of one set of inputs even
after being trained to classify another. This has implications for continual learning agents
that should not forget representations of parts of their environment when being exposed to
other parts. We hope to explore many promising applications of these ideas in the future.
Acknowledgments
This research was funded by EU projects WAY (FP7-ICT-288551), NeuralDynamics (FP7ICT-270247), and NASCENCE (FP7-ICT-317662); additional funding from ArcelorMittal.

References
[1] Per Anderson, Gary N. Gross, Terje L?mo, and Ola Sveen. Participation of inhibitory and
excitatory interneurones in the control of hippocampal cortical output. In Mary A.B. Brazier,
editor, The Interneuron, volume 11. University of California Press, Los Angeles, 1969.
[2] John Carew Eccles, Masao Ito, and J?nos Szent?gothai. The cerebellum as a neuronal machine.
Springer-Verlag New York, 1967.
[3] Costas Stefanis. Interneuronal mechanisms in the cortex. In Mary A.B. Brazier, editor, The
Interneuron, volume 11. University of California Press, Los Angeles, 1969.
[4] Stephen Grossberg. Contour enhancement, short-term memory, and constancies in reverberating neural networks. Studies in Applied Mathematics, 52:213?257, 1973.
[5] Michael McCloskey and Neal J. Cohen. Catastrophic interference in connectionist networks:
The sequential learning problem. The Psychology of Learning and Motivation, 24:109?164,
1989.
[6] Gail A. Carpenter and Stephen Grossberg. The art of adaptive pattern recognition by a
self-organising neural network. Computer, 21(3):77?88, 1988.
[7] Mark B. Ring. Continual Learning in Reinforcement Environments. PhD thesis, Department
of Computer Sciences, The University of Texas at Austin, Austin, Texas 78712, August 1994.
[8] Samuel A. Ellias and Stephen Grossberg. Pattern formation, contrast control, and oscillations
in the short term memory of shunting on-center off-surround networks. Bio. Cybernetics, 1975.
[9] Brad Ermentrout. Complex dynamics in winner-take-all neural nets with slow inhibition.
Neural Networks, 5(1):415?431, 1992.

8

[10] Christoph von der Malsburg. Self-organization of orientation sensitive cells in the striate cortex.
Kybernetik, 14(2):85?100, December 1973.
[11] Teuvo Kohonen. Self-organized formation of topologically correct feature maps. Biological
cybernetics, 43(1):59?69, 1982.
[12] Risto Mikkulainen, James A. Bednar, Yoonsuck Choe, and Joseph Sirosh. Computational maps
in the visual cortex. Springer Science+ Business Media, 2005.
[13] Dale K. Lee, Laurent Itti, Christof Koch, and Jochen Braun. Attention activates winner-takeall competition among visual filters. Nature Neuroscience, 2(4):375?81, April 1999.
[14] Matthias Oster and Shih-Chii Liu. Spiking inputs to a winner-take-all network. In Proceedings
of NIPS, volume 18. MIT; 1998, 2006.
[15] John P. Lazzaro, Sylvie Ryckebusch, Misha Anne Mahowald, and Caver A. Mead. Winnertake-all networks of O(n) complexity. Technical report, 1988.
[16] Giacomo Indiveri. Modeling selective attention using a neuromorphic analog VLSI device.
Neural Computation, 12(12):2857?2880, 2000.
[17] Wolfgang Maass. Neural computation with winner-take-all as the only nonlinear operation. In
Proceedings of NIPS, volume 12, 1999.
[18] Wolfgang Maass. On the computational power of winner-take-all. Neural Computation,
12:2519?2535, 2000.
[19] Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio.
Maxout networks. In Proceedings of the ICML, 2013.
[20] Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R.
Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors,
2012. arXiv:1207.0580.
[21] Juergen Schmidhuber. A local learning algorithm for dynamic feedforward and recurrent
networks. Connection Science, 1(4):403?412, 1989.
[22] Rupesh K. Srivastava, Bas R. Steunebrink, and Juergen Schmidhuber. First experiments with
powerplay. Neural Networks, 2013.
[23] Maximillian Riesenhuber and Tomaso Poggio. Hierarchical models of object recognition in
cortex. Nature Neuroscience, 2(11), 1999.
[24] Alex Krizhevsky, Ilya Sutskever, and Goeffrey E. Hinton. Imagenet classification with deep
convolutional neural networks. In Proceedings of NIPS, pages 1?9, 2012.
[25] Dan Ciresan, Ueli Meier, and J?rgen Schmidhuber. Multi-column deep neural networks for
image classification. Proceeedings of the CVPR, 2012.
[26] Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the ICML, number 3, 2010.
[27] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier networks. In AISTATS, volume 15, pages 315?323, 2011.
[28] George E. Dahl, Tara N. Sainath, and Geoffrey E. Hinton. Improving Deep Neural Networks
for LVCSR using Rectified Linear Units and Dropout. In Proceedings of ICASSP, 2013.
[29] Andrew L. Maas, Awni Y. Hannun, and Andrew Y. Ng. Rectifier nonlinearities improve neural
network acoustic models. In Proceedings of the ICML, 2013.
[30] Tijmen Tieleman. Gnumpy: an easy way to use GPU boards in Python. Department of
Computer Science, University of Toronto, 2010.
[31] Volodymyr Mnih. CUDAMat: a CUDA-based matrix class for Python. Department of Computer Science, University of Toronto, Tech. Rep. UTML TR, 4, 2009.
[32] Patrice Y. Simard, Dave Steinkraus, and John C. Platt. Best practices for convolutional
neural networks applied to visual document analysis. In International Conference on Document
Analysis and Recognition (ICDAR), 2003.
[33] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning
applied to document recognition. Proceedings of the IEEE, 1998.
[34] Marc?Aurelio Ranzato, Christopher Poultney, Sumit Chopra, and Yann LeCun. Efficient learning of sparse representations with an energy-based model. In Proceedings of NIPS, 2007.
[35] Matthew D. Zeiler and Rob Fergus. Stochastic pooling for regularization of deep convolutional
neural networks. In Proceedings of the ICLR, 2013.
[36] Kevin Jarrett, Koray Kavukcuoglu, Marc?Aurelio Ranzato, and Yann LeCun. What is the best
multi-stage architecture for object recognition? In Proc. of the ICCV, pages 2146?2153, 2009.
[37] John Blitzer, Mark Dredze, and Fernando Pereira. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classification. Annual Meeting-ACL, 2007.
[38] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Domain adaptation for large-scale sentiment classification: A deep learning approach. In Proceedings of the ICML, number 1, 2011.

9


----------------------------------------------------------------

title: 298-language-induction-by-phase-transition-in-dynamical-recognizers.pdf

Language Induction by Phase Transition
in Dynamical Recognizers

Jordan B. Pollack
Laboratory for AI Research
The Ohio State University
Columbus,OH 43210
pollack@cis.ohio-state.edu

Abstract
A higher order recurrent neural network architecture learns to recognize and
generate languages after being "trained" on categorized exemplars. Studying
these networks from the perspective of dynamical systems yields two
interesting discoveries: First, a longitudinal examination of the learning
process illustrates a new form of mechanical inference: Induction by phase
transition. A small weight adjustment causes a "bifurcation" in the limit
behavior of the network. This phase transition corresponds to the onset of the
network's capacity for generalizing to arbitrary-length strings. Second, a
study of the automata resulting from the acquisition of previously published
languages indicates that while the architecture is NOT guaranteed to find a
minimal finite automata consistent with the given exemplars, which is an
NP-Hard problem, the architecture does appear capable of generating nonregular languages by exploiting fractal and chaotic dynamics. I end the paper
with a hypothesis relating linguistic generative capacity to the behavioral
regimes of non-linear dynamical systems.

1 Introduction
I expose a recurrent high-order back-propagation network to both positive and negative
examples of boolean strings, and report that although the network does not find the
minimal-description finite state automata for the languages (which is NP-Hard (Angluin,
1978?, it does induction in a novel and interesting fashion, and searches through a
hypothesis space which, theoretically, is not constrained to machines of finite state. These
results are of import to many related neural models currently under development, e.g.
(Elman, 1990; Giles et aI., 1990; Servan-Schreiber et al., 1989), and relates ultimately to
the question of how linguistic capacity can arise in nature.
Although the transitions among states in a finite-state automata are usually thought of as
being fully specified by a table, a transition function can also be specified as a
mathematical function of the current state and the input. It is known from (McCulloch &
Pitts, 1943) that even the most elementary modeling assumptions yield finite-state

619

620

Pollack
control. and it is worth reiterating that any network with the capacity to compute arbitrary
boolean functions (say. as logical sums of products) lapedes farber how nets 1. white
homik .1. can be used recurrently to implement arbitrary finite state machines.
From a different point of view. a recurrent network with a state evolving across k units
can be considered a k-dimensional discrete-time continuous-space dynamical tystem.
with a precise initial condition. Zk(O). and a state space in Z. a subspace of R . The
governing function. F. is parameterized by a set of weights. W. and merely computes the
next state from the current state and input. Yj(t). a finite sequence of patterns
representing tokens from some alphabet 1::

Zk(t+ 1) =FW(Zk(t).YjCt?
If we view one of the dimensions of this system. say Za. as an "acceptance" dimension.
we can define the language accepted by such a Dynamical Recognizer as all strings of
input tokens evolved from the precise initial state for which the accepting dimension of
the state is above a certain threshold. In network terms. one output unit would be
subjected to a threshold test after processing a sequence of input patterns.

The first question to ask is how can such a dynamical system be constructed. or taught, to
accept a particular language? The weights in the network. individually. do not correspond
directly to graph transitions or to phrase structure rules. The second question to ask is
what sort of generative power can be achieved by such systems?

2 The Model
To begin to answer the question of learning. I now present and elaborate upon my earlier
work on Cascaded Networks (pollack. 1987). which were used in a recurrent fashion to
learn parity. depth-limited parenthesis balancing, and to map between word sequences
and proposition representations (pollack. 1990a). A Cascaded Network is a wellcontrolled higher-order connectionist architecture to which the back-propagation
technique of weight adjustment (Rumelhart et al.. 1986) can be applied. Basically. it
consists of two subnetworks: The function network is a standard feed-forward network;
with or without hidden layers. However. the weights are dynamically computed by the
linear context network. whose outputs are mapped in a 1: 1 fashion to the weights of the
function net. Thus the input pattern to the context network is used to "multiplex" the the
function computed, which can result in simpler learning tasks.
When the outputs of the function network are used as inputs to context network. a system
can be built which learns to produce specific outputs for variable-length sequences of
inputs. Because of the multiplicative connections, each input is, in effect, processed by a
different function. Given an initial context. Zk(O). and a sequence of inputs.
Yj(t). t= 1. .. n. the network computes a sequence of state vectors, Zi(t). t= 1. .. n by
dynamically changing the set of weights. Wij(t). Without hidden units the forward pass
computation is:

Wij(t) =

L Wijk zk(t-1)
k

Zi(t) =

geL Wij(t) Yj(t?
j

Language Induction by Phase 'll'ansition in Dynamical Recognizers
where g is the usual sigmoid function used in back-propagation system.
In previous work, I assumed that a teacher could supply a consistent and generalizable
desired-state for each member of a large set of strings, which was a significant
overconstraint. In learning a two-state machine like parity, this did not matter, as the I-bit
state fully determines the output However, for the case of a higher-dimensional system,
we know what the final output of a system should be, but we don't care what its state
should be along the way.
Jordan (1986) showed how recurrent back-propagation networks could be trained with
"don't care" conditions. If there is no specific preference for the value of an output unit
for a particular training example, simply consider the error term for that unit to be O.
This will work, as long as that same unit receives feedback from other examples. When
the don't-cares line up, the weights to those units will never change. My solution to this
problem involves a backspace, unrolling the loop only once: After propagating the errors
determined on only a subset of the weights from the "acceptance" unit Za:
aE
= (za(n) aza).()
n

da ) za(n) (1- za(n? Yj(n)

aE

The error on the remainder of the weights (aaE ,i
w"k
.
.
')
from the penuIOrnate
Orne step:

_a_E_=LL aE
azk(n-l)

a

j

~ a ) is calculated

using values

aE

aWajk awa/n)

aE
aE
aWij(n-l) - aZi(n-l) Yj(n-l)
aE
aE
-- zk(n-2)
aWijk
aWij(n-l)

This is done, in batch (epoch) style, for a set of examples of varying lengths.

3 Induction as Phase Transition
In initial studies of learning the simple regular language of odd parity, I expected the
recognizer to merely implement "exclusive or" with a feedback link. It turns out that this
is not quite enough. Because termination of back-propagation is usually defined as a 20%
error (e.g. logical "I" is above 0.8) recurrent use of this logic tends to a limit point. In
other words, mere separation of the exemplars is no guarantee that the network can
recognize parity in the limit. Nevertheless, this is indeed possible as illustrated by
illustrated below. In order to test the limit behavior of a recognizer, we can observe its
response to a very long "characteristic string". For odd parity, the string 1* requires an
alternation of responses.
A small cascaded network composed of a 1-2 function net and a 2-6 context net

621

622

Pollack
(requiring 18 weights) was was trained on odd parity of a small set of strings up to length
5. At each epoch, the weights in the network were saved in a file. Subsequently, each
configuration was tested in its response to the first 25 characteristic strings. In figure I,
each vertical column, corresponding to an epoch, contains 25 points between 0 and 1.
Initially, all strings longer than length 1 are not distinguished. From cycle 60-80, the
network is improving at separating finite strings. At cycle 85, the network undergoes a
"bifurcation," where the small change in weights of a single epoch leads to a phase
transition from a limit point to a limit cycle. 1 This phase transition is so "adaptive" to the
classification task that the network rapidly exploits iL

~
........
.
........
?.. ??.:iII
:.... . . -",'?:?:??sa
,,?.':-J

..,,~....
~iilU hli!iIi!ili 3__

.'

0.8

........".-.....

0.6

.

.. -:

,',' .,'

?

?

~

.e'!'

'. '''::1

? -,

:t-:-j~
.:.......
.....":..
'.............._.~:

".

......~:...
........

..~

~'

~_.

.."r

-.

-.::::.

- ????~

I" .-.'....~

,...

:::::~.....

..:.:~.::

. .... .

" . . . . .' . - .

0.2

'o,

50

.

--~

~>-~

~_-~

~ --

."'--.

".

Figure 1:

_--....
w ""'pe.? --- 4
1_?

'-.':::~.!

..-:",.'..:

0.4

..
Wi#- _ _ _ _ _ ,

;a.

100

150

-="""~

=

200

A bifurcation diagram showing the response of the parity-learner to the first
25 characteristic strings over 200 epochs of training.

I wish to stress that this is a new and very interesting form of mechanical induction, and
reveals that with the proper perspective, non-linear connectionist networks are capable of
much more complex behavior than hill-climbing. Before the phase transition, the
machine is in principle not capable of performing the serial parity task; after the phase
transition it is. The similarity of learning through a "flash of insight" to biological change
through a "punctuated" evolution is much more than coincidence.

4 BenChmarking Results
Tomita (1982) performed elegant experiments in inducing finite automata from positive
and negative evidence using hillclim bing in the space of 9-state automata. Each case was
defined by two sets of boolean strings, accepted by and rejected by the regular languages
1 For the simple low dimensional dynamical systems usually studied, the "knob" or cootrol parameter for
such a bifurcation diagram is a scalar variable; here the control parameter is the entire 32-0 vcc:tor of
weights in the network, and bade-propagation turns the knobl

Language Induction by Phase ltansition in Dynamical Recognizers
listed below.
1
2
3
4
5
6
7

1*
(10)*
no odd zero strings after odd 1 strings
no triples of zeros
pairwise, an even sum of 01 's and lO's.
number of 1's - number ofO's = 3n
0*1*0*1*

Rather than inventing my own training data, or sampling these languages for a wellformed training set I ran all 7 Tomita training environments as given, on a sequential
cascaded network of a I-input 4-output function network (with bias, 8 weights to set) and
a 3-input 8-output context network with bias, using a learning rate was of 0.3 and a
momentum to 0.7. Termination was when all accepted strings returned output bits above
0.8 and rejected strings below 0.2.
Of Tomita's 7 cases, all but cases #2 and #6 converged without a problem in several
hundred epochs. Case 2 would not converge, and kept treating a negative case as correct
because of the difficulty for my architecture to induce a "trap" state; I had to modify the
training set (by added reject strings 110 and 11010) in order to overcome this problem?
Case 6 took several restarts and thousands of cycles to converge, cause unknown. The
complete experimental data is available in a longer report (pollack, 1990b).
Because the states are "in a box" of low dimension,3 we can view these machines
graphically to gain some understanding of how the state space is being arranged. Based
upon some intitial studies of parity, my initial hypothesis was that a set of clusters would
be found, organized in some geometric fashion: i.e. an em bedding of a finite state
machine into a finite dimensional geometry such that each token'S transitions would
correspond to a simple transformation of space. Graphs of the states visited by all
possible inputs up to length 10, for the 7 Tomita test cases are shown in figure 2. Each
figure contains 2048 points, but often they overlap.
The images (a) and (d) are what were expected, clumps of points which closely map to
states of equivalent FSA's. Images (b) and (e) have limit "ravine's" which can each be
considered states as well.

5 Discussion
However, the state spaces, (c), (f), and (g) of the dynamical recognizers for Tomita cases
3,6, and 7, are interesting, because, theoretically, they are infinite state machines, where
the states are not arbitrary or random, requiring an infinite table of transitions, but are
constrained in a powerful way by mathematical principle. In other words, the complexity
is in the dynamics, not in the specifications (weights).
In thinking about such a principle, consider other systems in which extreme observed
complexity emerges from algorithmic simplicity plus computational power. It is
2 It can be argued that other FSA inducing methods get around this problem by presupposing rather than
learning trap states.
] One reason I have succeeded in such low dimensional induction is because my architecture is a Mealy,
rather than Moore Machine (Lee Giles, Personal Communication)

623

624

Pollack

A

B

c

D

E

F

G

Figure 2: Images of the state-spaces
for the 7 benchmark cases. Each
image contains 2048 points
corresponding to the states of all
boolean strings up to length 10.

Language Induction by Phase 1ransition in Dynamical Recognizers
interesting to note that by eliminating the sigmoid and commuting the Yj and Zk terms,
the forward equation for higher order recurrent networks with is identical to the generator
of an Iterated Function System (IFS) (Bamsley et al., 1985). Thus, my figures of statespaces, which emerge from the projection of
into Z, are of the same class of
mathematical object as Barnsley's fractal attractors (e.g. the widely reproduced fern).
Using the method of (Grassberger & Procaccia, 1983), the correlation dimension of the
attractor in Figure 2(g) was found to be about 1.4.

:r.

The link between work in complex dynamical systems and neural networks is wellestablished both on the neurobiological level (Skarda & Freeman, 1987) and on the
mathematical level (Derrida & Meir, 1988; Huberman & Hogg, 1987; Kurten, 1987;
Smolensky, 1986). This paper expands a theme from an earlier proposal to link them at
the "cognitive" level (pollack, 1989).
There is an interesting formal question, which has been brought out in the work of
(Wolfram, 1984) and others on the universality of cellular automata, and more recently in
the work of (Crutchfield & Young, 1989) on the descriptive complexity of bifurcating
systems: What is the relationship between complex dynamics (of neural systems) and
traditional measures of computational complexity? From this work and other supporting
evidence, I venture the following hypothesis:

:r.

~:roo, is an Attractor,
The state-space limit of a dynamical recognizer, as
which is cut by a threshold (or similar decision) function. The complexity of
the language recognized is regular if the cut falls between disjoint limit
points or cycles, context-free if it cuts a "self-similar" (recursive) region, and
context-sensitive if it cuts a "chaotic" (pseudo-random) region.
Acknowledgements
This research has been partially supported by the Office of Naval Research under
grant NOOO 14-89-J-1200.
References
Angluin, D. (1978). On the complexity of minimum inference of regular sets.
Information and Control. 39,337-350.
Bamsley, M. F., Ervin, V., Hardin, D. & Lancaster, J. (1985). Solution of an
inverse problem for fractals and other sets. Proceedings of the National Academy
of Science. 83.
Crutchfield, 1. P & Young, K. (1989). Computation at the Onset of Chaos. In W.
Zurek, (Ed.), Complexity. Entropy and the Physics of INformation. Reading, MA:
Addison-Wesley.
Derrida, B. & Meir, R. (1988). Chaotic behavior of a layered neural network.

Phys. Rev. A. 38.
Elman, J. L. (1990). Finding Structure in Time. Cognitive Science. 14, 179-212.
Giles, C. L., Sun, G. Z., Chen, H. H., Lee, Y. C. & Chen, D. (1990). Higher Order
Recurrent Networks and Grammatical Inference. In D. Touretzky, (Ed.),
Advances in Neural Information Processing Systems. Los Gatos, CA: Morgan
Kaufman.

625

626

Pollack
Grassberger. P. & Procaccia. I. (1983). Measuring the Strangeness of Strange
Attractors. Physica. 9D. 189-208.
Huberman. B. A. & Hogg. T. (1987). Phase Transitions in Artificial Intelligence
Systems. Artificial Intelligence. 33. 155-172.
Jordan. M. I. (1986). Serial Order: A Parallel Distributed Processing Approach.
ICS report 8608. La Jolla: Institute for Cognitive Science. UCSD.
Kurten. K. E. (1987). Phase transitions in quasirandom neural networks. In
Institute of Electrical and Electronics Engineers First International Conference on
Neural Networks. San Diego. 11-197-20.
McCulloch. w. S. & Pitts. W. (1943). A logical calculus of the ideas immanent in
nervous activity. Bulletin of Mathematical Biophysics. 5. 115-133.
POllack. J. B. (1987). Cascaded Back Propagation on Dynamic Connectionist
Networks. In Proceedings of the Ninth Conference of the Cognitive Science
Society. Seattle. 391-404.
Pollack, J. B. (1989). Implications of Recursive Distributed Representations. In
D. Touretzky. (Ed.). Advances in Neural Information Processing Systems. Los
Gatos. CA: Morgan Kaufman.
Pollack. J. B. (1990). Recursive Distributed Representation. Artificial
Intelligence. 46, 77-105.
Pollack. J. B. (1990). The Induction of Dynamical Recognizers. Tech Report 90lP-Automata. Columbus. OH 43210: LAIR. Ohio State University.
Rumelhart. D. E .? Hinton. G. & Williams. R. (1986). Learning Internal
Representations through Error Propagation. In D. E. Rumelhart. 1. L. McClelland
& the PDP research Group. (Eds.). Parallel Distributed Processing: Experiments in
the Microstructure of Cognition. Vol. 1. Cambridge: MIT Press.
Servan-Schreiber. D .? Cleeremans. A. & McClelland. J. L (1989). Encoding
Sequential Structure in Simple Recurrent Networks. In D. Touretzky. (Ed.).
Advances in Neural Information Processing Systems. Los Gatos. CA: Morgan
Kaufman.
Skarda. C. A. & Freeman. W. J. (1987). How brains make chaos. Brain &
Behavioral Science.lO.
Smolensky. P. (1986). Information Processing in DynamiCal Systems:
Foundations of Harmony Theory. In D. E. Rumelhart. J. L. McClelland & the PDP
research GrouP. (Eds.). Parallel Distributed Processing: Experiments in the
Microstructure of Cognition. Vol. 1. Cambridge: MIT Press.
Tomita. M. (1982). Dynamic construction of finite-state automata from examples
using hill-climbing. In Proceedings of the Fourth Annual Cognitive Science
Conference. Ann Arbor. MI. 105-108.
Wolfram. S. (1984). Universality and Complexity in Cellular Automata. Physica.
lOD.1-35.


----------------------------------------------------------------

title: 162-mapping-classifier-systems-into-neural-networks.pdf

49

Mapping Classifier Systems
Into Neural Networks
Lawrence Davis
BBN Laboratories
BBN Systems and Technologies Corporation
10 Moulton Street
Cambridge, MA 02238
January 16, 1989
Abstract
Classifier systems are machine learning systems incotporating a genetic algorithm as the learning mechanism. Although they respond to inputs that neural
networks can respond to, their internal structure, representation fonnalisms, and
learning mechanisms differ marlcedly from those employed by neural network researchers in the same sorts of domains. As a result, one might conclude that these
two types of machine learning fonnalisms are intrinsically different. This is one
of two papers that, taken together, prove instead that classifier systems and neural
networks are equivalent. In this paper, half of the equivalence is demonstrated
through the description of a transfonnation procedure that will map classifier
systems into neural networks that are isomotphic in behavior. Several alterations
on the commonly-used paradigms employed by neural networlc researchers are
required in order to make the transfonnation worlc. These alterations are noted
and their appropriateness is discussed. The paper concludes with a discussion of
the practical import of these results, and with comments on their extensibility.

1

Introd uction

Classifier systems are machine learning systems that have been developed since the
1970s by 10hn Holland and, more recently, by other members of the genetic algorithm
research community as well l . Classifier systems are varieties of genetic algorithms
- algorithms for optimization and learning. Genetic algorithms employ techniques
inspired by the process of biological evolution in order to "evolve" better and better
IThis paper has benefited from discussions with Wayne Mesard, Rich Sutton, Ron Williams, Stewart
Wilson, Craig Shaefer, David Montana, Gil Syswerda and other members of BARGAIN, the Boston Area
Research Group in Genetic Algorithms and Inductive Networks.

50

Davis

individuals that are taken to be solutions to problems such as optimizing a function,
traversing a maze, etc. (For an explanation of genetic algorithms, the reader is
referred to [Goldberg 1989].) Classifier systems receive messages from an external
source as inputs and organize themselves using a genetic algorithm so that they will
"learn" to produce responses for internal use and for interaction with the external
source.
This paper is one of two papers exploring the question of the fonnal relationship
between classifier systems and neural networks. As normally employed, the two sorts
of algorithms are probably distinct, although a procedure for translating the operation
of neural networks into isomorphic classifier systems is given in [Belew and Gherrity
1988]. The technique Belew and Gherrity use does not include the conversion of the
neural network learning procedure into the classifier system framework, and it appears
that the technique will not support such a conversion. Thus, one might conjecture that
the two sorts of machine learning systems employ learning techniques that cannot be
reconciled, although if there were a subsumption relationship, Belew and Gherrity's
result suggests that the set of classifier systems might be a superset of the set of
neural networks.
The reverse conclusion is suggested by consideration of the inputs that each sort
of learning algorithm processes. When viewed as "black boxes", both mechanisms
for learning receive inputs, carry out self-modifying procedures, and produce outputs.
The class of inputs that are traditionally processed by classifier systems - the class
of bit strings of a fixed length - is a subset of the class of inputs that have been
traditionally processed by neural networks. Thus, it appears that classifier systems
operate on a subset of the inputs that neural networks can process, when viewed as
mechanisms that can modify their behavior.
In fact, both these impressions are correct. One can translate classifier systems
into neural networks, preserving their learning behavior, and one can translate neural
networks into classifier systems, again preserving learning behavior. In order to do
so, however, some specializations of each sort of algorithm must be made. This
paper deals with the translation from classifier systems to neural networks and with
those specializations of neural networks that are required in order for the translation
to take place. The reverse translation uses quite different techniques, and is treated
in [Davis 1989].
The following sections contain a description of classifier systems, a description of
the transformation operator, discussions of the extensibility of the proof, comments
on some issues raised in the course of the proof, and conclusions.

2 Classifier Systems
A classifier system operates in the context of an environment that sends messages to
the system and provides it with reinforcement based on the behavior it displays. A
classifier system has two components - a message list and a population of rule-like
entities called classifiers. Each message on the message list is composed of bits, and

Mapping Classifier Systems Into Neural Networks

each has a pointer to its source (messages may be generated by the environment or
by a classifier.) Each classifier in the population of classifiers has three components:
a match string made up of the characters 0,1, and # (for "don't care"); a message
made up of the characters 0 and 1; and a strength. The top-level description of a
classifier system is that it contains a population of production rules that attempt to
match some condition on the message list (thus "classifying" some input) and post
their message to the message list, thus potentially affecting the envirorunent or other
classifiers. Reinforcement from the environment is used by the classifier system to
modify the strengths of its classifiers. Periodically, a genetic algorithm is invoked
to create new classifiers, which replace certain members of the classifier set. (For
an explanation of classifier systems, their potential as machine learning systems, and
their formal properties, the reader is referred to [Holland et al 1986].)
Let us specify these processing stages more precisely. A classifier system operates
by cycling through a fixed list of procedures. In order, these procedures are:
Message List Processing. 1. Clear the message list. 2. Post the envirorunental
messages to the message list. 3. Post messages to the message list from classifiers
in the post set of the previous cycle. 4. Implement envirorunental reinforcement by
analyzing the messages on the message list and altering the strength of classifiers in
the post set of the previous cycle.
Form the Bid Set. 1. Determine which classifiers match a message in the
message list. A classifier matches a message if each bit in its match field matches its
corresponding message bit. A 0 matches a 0, a 1 matches a I, and a # matches either
bit. The set of all matching classifiers forms the current bid set. 2. Implement bid
taxes by subtracting a portion of the strength of each classifier c in the bid set. Add
the strength taken from c to the strength of the classifier or classifiers that posted
messages matched by c in the prior step.
Form the Post Set. 1. If the bid set is larger than the maximum post set size,
choose classifiers stochastically to post from the bid set, weighting them in proportion
to the magnitude of their bid taxes. The set of classifiers chosen is the post set.
Reproduction Reproduction generally does not occur on every cycle. When it
does occur, these steps are carried out: 1. Create n children from parents. Use
crossover and/or mutation, chOOSing parents stochastically but favoring the strongest
ones. (Crossover and mutation are two of the operators used in genetic algorithms.)
2. Set the strength of each child to equal the average of the strength of that child's
parents. (Note: this is one of many ways to set the strength of a new classifier.
The transformation will work in analogous ways for each of them.) 3. Remove n
members of the classifier population and add the n new children to the classifier
population.

3

Mapping Classifiers Into Classifier Networks

The mapping operator that I shall describe maps each classifier into a classifier
network. Each classifier network has links to environmental input units, links to

51

52

Davis

other classifier networks, and match, post, and message units. The weights on the
links leading to a match node and leaving a post node are related to the fields in
the match and message lists in the classifier. An additional link is added to provide
a bias term for the match node. (Note: it is assumed here that the environment
posts at most one message per cycle. Modifications to the transfonnation operator to
accommodate multiple environmental messages are described in the final comments
of this paper.)
Given a classifier system CS with n classifiers, each matching and sending messages of length m, we can construct an isomorphic neural network composed of n
classifier networks in the following way. For each classifier c in CS, we construct its
corresponding classifier network, composed of n match nodes, I post node, and m
message nodes. One match node (the environmental match node) has links to inputs
from the environment. Each of the other match nodes is linked to the message and
post node of another classifier network. The reader is referred to Figure 2 for an
example of such a transformation.
Each match node in a classifier network has m + 1 incoming links. The weights
on the first m links are derived by applying the following transformation to the m
elements of c's match field: 0 is associated with weight -1, 1 is associated with
weight 1, and # is associated with weight O. The weight . of the final link is set to
m + 1 - l, where l is the number of links with weight = 1. Thus, a classifier with
match field (1 0 # 0 1) would have an associated network with weights on the links
leading to its match nodes of 1, -1, 0, -I, 1, and 4. A classifier with match field (1
0#) would have weights of 1, -I, 0, and 3.
The weights on the links to each message node in the classifier network are set
to equal the corresponding element of the classifier's message field. Thus, if the
message field of the classifier were (0 1 0), the weights on the links leading to the
three message nodes in the corresponding classifier network would be 0, I, and O.
The weights on all other links in the classifier network are set to 1.
Each node in a classifier network uses a threshold function to determine its activation level. Match nodes have thresholds = m + .9. All other nodes have thresholds
=.9. If a node's threshold is exceeded, the node's activation level is set to 1. If not,
it is set to O.
Each classifier network has an associated quantity called strength that may be
altered when the network is run, during the processing cycle described below.
A cycle of processing of a classifier system CS maps onto the following cycle of
processing in a set of classifier networks:
Message List Processing. 1. Compute the activation level of each message
node in CS. 2. If the environment supplies reinforcement on this cycle, divide that
reinforcement by the number of post nodes that are currently active, plus 1 if the
environment posted a message on the preceding cycle, and add the quotient to the
strength of each active post node's classifier network. 3. If there is a message on this
cycle from the environment, map it onto the first m environment nodes so that each
node associated with a 0 is off and each node associated with a 1 is on. Tum the final
environmental node on. If there is no environmental message, turn all environmental

Mapping Classifier Systems Into Neural Networks

nodes off.
Form the Bid Set. 1. Compute the activation level of each match node in
each classifier network. 2. Compute the activation level of each bid node in each
classifier network (the set of classifier networks with an active bid node is the bid
set). 3. Subtract a fixed proportion of the strength of each classifier network cn in
the bid set. Add this amount to the strength of those networks connected to an active
match node in cn. (Strength given to the environment passes out of the system.)
Form the Post Set. 1. If the bid set is larger than the maximum post set size,
choose networks stochastically to post from the bid set, weighting them in proportion
to the magnitude of their bid taxes. The set of networks chosen is the post set. (This
might be viewed as a stochastic n-winners-take-all procedure).
Reproduction. If this is a cycle on which reproduction would occur in the
classifier system, carry out its analog in the neural network in the following way.
1. Create n children from parents. Use crossover and/or mutation, choosing parents
stochastically but favoring the strongest ones. The ternary alphabet composed of -I,
I, and 0 is used instead of the classifier alphabet of 0, 1, and #. After each operator
is applied, the final member of the match list is set to m + 1 - l. 2. Write over the
weights on the match links and the message links of n classifier networks to match
the weights in the children. Choose networks to be re-weighted stochastically, so that
the weakest ones are most likely to be chosen. Set the strength of each re-weighted
classifier network to be the average of the strengths of its parents.
It is simple to show that a classifier network match node will match a message
in just those cases in which its associated classifier matched a message. There are
three cases to consider. If the original match character was a #, then it matched any
message bit. The corresponding link weight is set to 0, so the state of the node it
comes from will not affect the activation of the match node it goes to. If the original
match character was a 1, then its message bit had to be a 1 for the message to be
matched. The corresponding link weight is set to 1, and we see by inspection of the
weight on the final link, the match node threshold, and the fact that no other type
of link has a positive weight, that every link with weight I must be connected to an
active node for the match node to be activated. Finally, the link weight corresponding
to a 0 is set to -1. If any of these links is connected to a node that is active, then the
effect is that of turning off a node connected to a link with weight 1, and we have
just seen that this will cause the match node to be inactive.
Given this correspondence in matching behavior, one can verify that a set of
classifier networks associated with a classifier system has the following properties:
During each cycle of processing of the classifier system, a classifier is in the bid set
in just those cases in which its associated networlc has an active bid node. Assuming
that both systems use the same randomizing technique, initialized in the same way,
the classifier is in the post set in just those cases when the network is in the post
set. Finally, the parents that are chosen for reproduction are the transform as of those
chosen in the classifier system, and the children produced are the transformations of
the classifier system parents. The two systems are isomorphic in operation, assuming
that they use the same random number generator.

53

54

Davis

CLASSIFIER NETWORK 1
strength = 49.3

CLASSIFIER NETWORK 2
strength

= 21.95

MESSAGE
NODES
TH = .9

POST
NODES
TH =.9

MATCH
NODES
TH = 3 .9

2
ENVIRONMENT
INPUT
NODES

Figure 1: Result of mapping a classifier system
witH two classifiers into a neural network .
Classifier 1 has match field (0 1 #), message field (1 1 0),
and strength 49 .3. Classifier 2 has match field (1 1 #),
message field (0 1 1), and strength 21.95.

Mapping Classifier Systems Into Neural Networks

4 Concluding Comments
The transfonnation procedure described above will map a classifier system into a
neural network that operates in the same way. There are several points raised by the
techniques used to accomplish the mapping. In closing, let us consider four of them.
First, there is some excess complexity in the classifier networks as they are shown
here. In fact, one could eliminate all non-environmental match nodes and their
links, since one can determine whenever a classifier network is reweigh ted whether it
matches the message of each other classifier network in the system. If so, one could
introduce a link directly from the post node of the other classifier networlc to the post
node of the new networlc. The match nodes to the environment are necessary, as
long as one cannot predict what messages the environment will post. Message nodes
are necessary as long as messages must be sent out to the environment. If not, they
and their incoming links could be eliminated as well. These simplifications have not
been introduced here because the extensions discussed next require the complexity
of the current architecture.
Second, on the genetic algorithm side, the classifier system considered here is an
extremely simple one. There are many extensions and refinements that have been
used by classifier system researchers. I believe that such refinements can be handled
by expanded mapping procedures and by modifications of the architecture of the
classifier networks. To give an indication of the way such modifications would go,
let us consider two sample cases. The first is the case of an environment that may
produce multiple messages on each cycle. To handle multiple messages, an additional
link must be added to each environmental match node with weight set to the match
node's threshold. This link will latch the match node. An additional match node
with links to the environment nodes must be added, and a latched counting node
must be attached to it. Given these two architectural modifications, the cycle is
modified as follows: During the message matching cycle, a series of subcycles is
carried out, one for each message posted by the environment. In each subcycle, an
environmental message is input and each environmental match node computes its
activation. The environmental match nodes are latched., so that each will be active
if it matched any environmental message. The count nodes will record how many
were matched by each classifier network. When bid strength'is paid from a classifier
network to the posters of messages that it matched, the divisor is the number of
environmental messages matched as recorded by the count node, plus the number
of other messages matched. Finally, when new weights are written onto a classifier
network's links, they are written onto the match node connected to the count node
as well. A second sort of complication is that of pass-through bits - bits that
are passed from a message that is matched to the message that is posted. This
sort of mechanism can be implemented in an obvious fashion by complicating the
structure of the classifier networlc. Similar complications are produced by considering
multiple-message matching, negation, messages to effectors, and so forth. It is an
open question whether all such cases can be handled by modifying the architecture
and the mapping operator, but I have not yet found one that cannot be so handled.

55

56

Davis

Third, the classifier networks do not use the sigmoid activation functions that support hill-c~bing techniques such as back-propagation. Further, they are recurrent
networks rather than strict feed-forwanl networks. Thus, one might wonder whether
the fact that one can carry out such transformations should affect the behavior of
researchers in the field. This point is one that is taken up at greater length in the
companion paper. My conclusion there is that several of the techniques imported into
the neural network domain by the mapping appear to improve the performance of neural networks. These include tracking strength in order to guide the learning process,
using genetic operators to modify the network makeup. and using population-level
measurements in order to determine what aspects of a network to use in reproduction.
The reader is referred to [Montana and Davis 1989] for an example of the benefits
to be gained by employing these techniques.
Finally, one might wonder what the import of this proof is intended to be. In
my view, this proof and the companion proof suggest some exciting ways in which
one can hybridize the learning techniques of each field. One such approach and its
successful application to a real-world problem is characterized in [Montana and Davis
1989].

References
[1] Belew, Richard K. and Michael Gherrity, "Back Propagation for the Classifier

System", in preparation.
[2] Davis, Lawrence, "Mapping Neural Networks into Classifier Systems", submitted to the 1989 International Conference on Genetic Algorithms.
[3] Goldberg, David E. Genetic Algorithms in Search, Optimization, and Machine
Learning, Addison Wesley 1989.
[4] Holland, John H, Keith J. Holyoak, Richard E. Nisbett, and Paul R. Thagard,
Induction, MIT Press, 1986.

[5] Montana, David J. and Lawrence Davis, "Training Feedforward Neural Networks Using Genetic Algorithms", submitted to the 1989 International Joint
Conference on Artificial Intelligence.


----------------------------------------------------------------

title: 3714-a-game-theoretic-approach-to-hypergraph-clustering.pdf

A Game-Theoretic Approach to
Hypergraph Clustering

Samuel Rota Bul`o
Marcello Pelillo
University of Venice, Italy
{srotabul,pelillo}@dsi.unive.it

Abstract
Hypergraph clustering refers to the process of extracting maximally coherent
groups from a set of objects using high-order (rather than pairwise) similarities.
Traditional approaches to this problem are based on the idea of partitioning the
input data into a user-defined number of classes, thereby obtaining the clusters as
a by-product of the partitioning process. In this paper, we provide a radically different perspective to the problem. In contrast to the classical approach, we attempt
to provide a meaningful formalization of the very notion of a cluster and we show
that game theory offers an attractive and unexplored perspective that serves well
our purpose. Specifically, we show that the hypergraph clustering problem can
be naturally cast into a non-cooperative multi-player ?clustering game?, whereby
the notion of a cluster is equivalent to a classical game-theoretic equilibrium concept. From the computational viewpoint, we show that the problem of finding the
equilibria of our clustering game is equivalent to locally optimizing a polynomial
function over the standard simplex, and we provide a discrete-time dynamics to
perform this optimization. Experiments are presented which show the superiority
of our approach over state-of-the-art hypergraph clustering techniques.

1 Introduction
Clustering is the problem of organizing a set of objects into groups, or clusters, in a way as to have
similar objects grouped together and dissimilar ones assigned to different groups, according to some
similarity measure. Unfortunately, there is no universally accepted formal definition of the notion
of a cluster, but it is generally agreed that, informally, a cluster should correspond to a set of objects
satisfying two conditions: an internal coherency condition, which asks that the objects belonging to
the cluster have high mutual similarities, and an external incoherency condition, which states that
the overall cluster internal coherency decreases by adding to it any external object.
Objects similarities are typically expressed as pairwise relations, but in some applications higherorder relations are more appropriate, and approximating them in terms of pairwise interactions can
lead to substantial loss of information. Consider for instance the problem of clustering a given set of
d-dimensional Euclidean points into lines. As every pair of data points trivially defines a line, there
does not exist a meaningful pairwise measure of similarity for this problem. However, it makes
perfect sense to define similarity measures over triplets of points that indicate how close they are
to being collinear. Clearly, this example can be generalized to any problem of model-based point
pattern clustering, where the deviation of a set of points from the model provides a measure of their
dissimilarity. The problem of clustering objects using high-order similarities is usually referred to
as the hypergraph clustering problem.
In the machine learning community, there has been increasing interest around this problem. Zien
and co-authors [24] propose two approaches called ?clique expansion? and ?star expansion?, respectively. Both approaches transform the similarity hypergraph into an edge-weighted graph, whose
edge-weights are a function of the hypergraph?s original weights. This way they are able to tackle
1

the problem with standard pairwise clustering algorithms. Bolla [6] defines a Laplacian matrix for
an unweighted hypergraph and establishes a link between the spectral properties of this matrix and
the hypergraph?s minimum cut. Rodr`?guez [16] achieves similar results by transforming the hypergraph into a graph according to ?clique expansion? and shows a relationship between the spectral
properties of a Laplacian of the resulting matrix and the cost of minimum partitions of the hypergraph. Zhou and co-authors [23] generalize their earlier work on regularization on graphs and
define a hypergraph normalized cut criterion for a k-partition of the vertices, which can be achieved
by finding the second smallest eigenvector of a normalized Laplacian. This approach generalizes
the well-known ?Normalized cut? pairwise clustering algorithm [19]. Finally, in [2] we find another
work based on the idea of applying a spectral graph partitioning algorithm on an edge-weighted
graph, which approximates the original (edge-weighted) hypergraph. It is worth noting that the approaches mentioned above are devised for dealing with higher-order relations, but can all be reduced
to standard pairwise clustering approaches [1]. A different formulation is introduced in [18], where
the clustering problem with higher-order (super-symmetric) similarities is cast into a nonnegative
factorization of the closest hyper-stochastic version of the input affinity tensor.
All the afore-mentioned approaches to hypergraph clustering are partition-based. Indeed, clusters
are not modeled and sought directly, but they are obtained as a by-product of the partition of the input
data into a fixed number of classes. This renders these approaches vulnerable to applications where
the number of classes is not known in advance, or where data is affected by clutter elements which
do not belong to any cluster (as in figure/ground separation problems). Additionally, by partitioning,
clusters are necessarily disjoint sets, although it is in many cases natural to have overlapping clusters,
e.g., two intersecting lines have the point in the intersection belonging to both lines.
In this paper, following [14, 20] we offer a radically different perspective to the hypergraph clustering problem. Instead of insisting on the idea of determining a partition of the input data, and hence
obtaining the clusters as a by-product of the partitioning process, we reverse the terms of the problem and attempt instead to derive a rigorous formulation of the very notion of a cluster. This allows
one, in principle, to deal with more general problems where clusters may overlap and/or outliers
may get unassigned. We found that game theory offers a very elegant and general mathematical
framework that serves well our purposes. The basic idea behind our approach is that the hypergraph
clustering problem can be considered as a multi-player non-cooperative ?clustering game?. Within
this context, the notion of a cluster turns out to be equivalent to a classical equilibrium concept from
(evolutionary) game theory, as the latter reflects both the internal and external cluster conditions
alluded to before. We also show that there exists a correspondence between these equilibria and
the local solutions of a polynomial, linearly-constrained, optimization problem, and provide an algorithm for finding them. Experiments on two standard hypergraph clustering problems show the
superiority of the proposed approach over state-of-the-art hypergraph clustering techniques.

2 Basic notions from evolutionary game theory
Evolutionary game theory studies models of strategic interactions (called games) among large
numbers of anonymous agents. A game can be formalized as a triplet ? = (P, S, ?), where
P = {1, . . . , k} is the set of players involved in the game, S = {1, . . . , n} is the set of pure
strategies (in the terminology of game-theory) available to each player and ? : S k ? R is the payoff
function, which assigns a payoff to each strategy profile, i.e., the (ordered) set of pure strategies
played by the individuals. The payoff function ? is assumed to be invariant to permutations of the
strategy profile. It is worth noting that in general games, each player may have its own set of strategies and own payoff function. For a comprehensive introduction to evolutionary game theory we
refer to [22].
By undertaking an evolutionary setting we assume to have a large population of non-rational agents,
which are randomly matched to play a game ? = (P, S, ?). Agents are considered non-rational, because each of them initially chooses a strategy from S, which will be always played when selected
for the game. An agent, who selected strategy i ? S, is called i-strategist. Evolution in the population takes place, because we assume that there exists a selection mechanism, which, by analogy with
a Darwinian process, spreads the fittest strategies in the population to the detriment of the weakest
one, which will in turn be driven to extinction. We will see later in this work a formalization of such
a selection mechanism.
2

The state of the population at a given time t can be represented as a n-dimensional vector x(t),
where xi (t) represents the fraction of i-strategists in the population at time t. The set of all possible
states describing a population is given by
(
)
X
n
?= x?R :
xi = 1 and xi ? 0 for all i ? S ,
i?S

which is called standard simplex. In the sequel we will drop the time reference from the population
state, where not necessary. Moreover, we denote with ?(x) the support of x ? ?, i.e., the set of
strategies still alive in population x ? ?: ?(x) = {i ? S : xi > 0}.
If y(i) ? ? is the probability distribution identifying which strategy the ith player will adopt if
drawn to play the game ?, then the average payoff obtained by the agents can be computed as
k
Y
X
.
(1)
ys(j)
?(s1 , . . . , sk )
u(y(1) , . . . , y(k) ) =
j
(s1 ,...,sk )?S k

j=1

Note that (1) is invariant to any permutation of the input probability vectors.

Assuming that the agents are randomly and independently drawn from a population x ? ? to play
the game ?, the population average payoff is given by u(xk ), where xk is a shortcut for x, . . . , x
repeated k times. Furthermore, the average payoff that an i-strategist obtains in a population x ? ?
is given by u(ei , xk?1 ), where ei ? ? is a vector with xi = 1 and zero elsewhere.
An important notion in game theory is that of equilibrium [22]. A population x ? ? is in equilibrium
when the distribution of strategies will not change anymore, which intuitively happens when every
individual in the population obtains the same average payoff and no strategy can thus prevail on the
other ones. Formally, x ? ? is a Nash equilibrium if
u(ei , xk?1 ) ? u(xk ) ,
for all i ? S .
(2)
In other words, every agent in the population performs at most as well as the population average
payoff. Due to the multi-linearity of u, a consequence of (2) is that
u(ei , xk?1 ) = u(xk ) ,
for all i ? ?(x) ,
(3)
i.e., all the agents that survived the evolution obtain the same average payoff, which coincides with
the population average payoff.
A key concept pertaining to evolutionary game theory is that of an evolutionary stable strategy
[7, 22]. Such a strategy is robust to evolutionary pressure in an exact sense. Assume that in a
population x ? ?, a small share ? of mutant agents appears, whose distribution of strategies is
y ? ?. The resulting postentry population is given by w? = (1 ? ?)x + ?y. Biological intuition
suggests that evolutionary forces select against mutant individuals if and only if the average payoff
of a mutant agent in the postentry population is lower than that of an individual from the original
population, i.e.,
u(y, w?k?1 ) < u(x, w?k?1 ) .
(4)
A population x ? ? is evolutionary stable (or an ESS) if inequality (4) holds for any distribution of
mutant agents y ? ? \ {x}, granted the population share of mutants ? is sufficiently small (see, [22]
for pairwise contests and [7] for n-wise contests).
An alternative, but equivalent, characterization of ESSs involves a leveled notion of evolutionary
stable strategies [7]. We say that x ? ? is an ESS of level j against y ? ?, if there exists j ?
{0, . . . , k ? 1} such that both conditions
u(yj+1 , xk?j?1 ) <
i+1

k?i?1

u(yj , xk?j ) ,
i

k?i

(5)

u(y , x
) = u(y , x ) , for all 0 ? i < j ,
(6)
are satisfied. Clearly, x ? ? is an ESS if it satisfies a condition of this form for every y ? ? \ {x}.
It is straightforward to see that any ESS is a Nash equilibrium [22, 7]. An ESS, which satisfies
conditions (6) with j never more than J, will be called an ESS of level J. Note that for the generic
case most of the preceding conditions will be superfluous, i.e., only ESSs of level 0 or 1 are required
[7]. Hence, in the sequel, we will consider only ESSs of level 1. It is not difficult to verify that any
ESS (of level 1) x ? ? satisfies
u(w?k ) < u(xk ) ,
(7)
for all y ? ? \ {x} and small enough values of ?.
3

3 The hypergraph clustering game
The hypergraph clustering problem can be described by an edge-weighted hypergraph. Formally,
an edge-weighted hypergraph is a triplet H = (V, E, s), where V = {1, . . . , n} is a finite set
of vertices, E ? P(V ) \ {?} is the set of (hyper-)edges (here, P(V ) is the power set of V ) and
s : E ? R is a weight function which associates a real value with each edge. Note that negative
weights are allowed too. Although hypergraphs may have edges of varying cardinality, we will focus
on a particular class of hypergraphs, called k-graphs, whose edges have all fixed cardinality k ? 2.
In this paper, we cast the hypergraph clustering problem into a game, called (hypergraph) clustering
game, which will be played in an evolutionary setting. Clusters are then derived from the analysis of the ESSs of the clustering game. Specifically, given a k-graph H = (V, E, s) modeling a
hypergraph clustering problem, where V = {1, . . . , n} is the set of objects to cluster and s is the
similarity function over the set of objects in E, we can build a game involving k players, each of
them having the same set of (pure) strategies, namely the set of objects to cluster V . Under this
setting, a population x ? ? of agents playing a clustering game represents in fact a cluster, where
xi is the probability for object i to be part of it. Indeed, any cluster can be modeled as a probability
distribution over the set of objects to cluster. The payoff function of the clustering game is defined
in a way as to favour the evolution of agents supporting highly coherent objects. Intuitively, this
is accomplished by rewarding the k players in proportion to the similarity that the k played objects
have. Hence, assuming (v1 , . . . , vk ) ? V k to be the tuple of objects selected by k players, the payoff
function can be simply defined as
1
s ({v1 , . . . , vk }) if {v1 , . . . , vk } ? E ,
?(v1 , . . . , vk ) = k!
(8)
0
else ,
where the term 1/k! has been introduced for technical reasons.
Given a population x ? ? playing the clustering game, we have that the average population payoff
u(xk ) measures the cluster?s internal coherency as the average similarity of the objects forming the
cluster, whereas the average payoff u(ei , xk?1 ) of an agent supporting object i ? V in population
x, measures the average similarity of object i with respect to the cluster.
An ESS of a clustering game incorporates the properties of internal coherency and external incoherency of a cluster:
internal coherency: since ESSs are Nash equilibria, from (3), it follows that every object contributing to the cluster, i.e., every object in ?(x), has the same average similarity with respect to
the cluster, which in turn corresponds to the cluster?s overall average similarity. Hence, the
cluster is internally coherent;
external incoherency: from (2), every object external to the cluster, i.e., every object in V \ ?(x),
has an average similarity which does not exceed the cluster?s overall average similarity.
There may still be cases where the average similarity of an external object is the same as
that of an internal object, mining the cluster?s external incoherency. However, since x is
an ESS, from (7) we see that whenever we try to extend a cluster with small shares of
external objects, the cluster?s overall average similarity drops. This guarantees the external
incoherency property of a cluster to be also satisfied.
Finally, it is worth noting that this theory generalizes the dominant-sets clustering framework which
has recently been introduced in [14]. Indeed, ESSs of pairwise clustering games, i.e. clustering
games defined on graphs, correspond to the dominant-set clusters [20, 17]. This is an additional
evidence that ESSs are meaningful notions of cluster.

4 Evolution towards a cluster
In this section we will show that the ESSs of a clustering game are in one-to-one correspondence
with (strict) local solution of a non-linear optimization program. In order to find ESSs, we will also
provide a dynamics due to Baum and Eagon, which generalizes the replicator dynamics [22].
Let H = (V, E, s) be a hypergraph clustering problem and ? = (P, V, ?) be the corresponding
clustering game. Consider the following non-linear optimization problem:
X
Y
xi , subject to x ? ? .
(9)
maximize f (x) =
s(e)
e?E

i?e

4

It is simple to see that any first-order Karush-Kuhn-Tucker (KKT) point x ? ? of program (9) [13]
is a Nash equilibrium of ?. Indeed, by the KKT conditions there exist ?i ? 0, i ? S, and ? ? R
such that for all i ? S,
?f (x)i + ?i ? ? =

1
u(ei , xk?1 ) + ?i ? ? = 0
k

and

?i xi = 0 ,

where ? is the gradient operator. From this it follows straightforwardly that u(ei , xk?1 ) ? u(xk )
for all i ? S. Moreover, it turns out that any strict local maximizer x ? ? of (9) is an ESS of ?.
Indeed, by definition, a strict local maximizer of this program satisfies u(zk ) = f (z) < f (x) =
u(xk ), for any z ? ? \ {x} close enough to x, which is in turn equivalent to (7) for sufficiently
small values of ?.
The problem of extracting ESSs of our hypergraph clustering game can thus be cast into the problem
of finding strict local solutions of (9). We will address this optimization task using a result due to
Baum and Eagon [3], who introduced a class of nonlinear transformations in probability domain.
Theorem 1 (Baum-Eagon). Let P (x) be a homogeneous polynomial in the variables xi with nonnegative coefficients, and let x ? ?. Define the mapping z = M(x) as follows:
n
.X
xj ?j P (x),
zi = xi ?i P (x)

i = 1, . . . , n.

(10)

j=1

Then P (M(x)) > P (x), unless M(x) = x. In other words M is a growth transformation for the
polynomial P .
The Baum-Eagon inequality provides an effective iterative means for maximizing polynomial functions in probability domains, and in fact it has served as the basis for various statistical estimation
techniques developed within the theory of probabilistic functions of Markov chains [4]. It was also
employed for the solution of relaxation labelling processes [15].
Since f (x) is a homogeneous polynomial in the variables xi , we can use the transformation of
Theorem 1 in order to find a local solution x ? ? of (9), which in turn provides us with an ESS of the
hypergraph clustering game. By taking the support of x, we have a cluster under our framework. The
complexity of finding a cluster is thus O(?|E|), where |E| is the number of edges of the hypergraph
describing the clustering problem and ? is the average number of iteration needed to converge. Note
that ? never exceeded 100 in our experiments.
In order to obtain the clustering, in principle, we have to find the ESSs of the clustering game.
This is a non-trivial, although still feasible, task [21], which we leave as a future extension of this
work. By now, we adopt a naive peeling-off strategy for our cluster extraction procedure. Namely,
we iteratively find a cluster and remove it from the set of objects, and we repeat this process on
the remaining objects until a desired number of clusters have been extracted. The set of extracted
ESSs with this procedure does not technically correspond to the ESSs of the original game, but to
ESSs of sub-games of it. The cost of this approximation is that we unfortunately loose (by now) the
possibility of having overlapping clusters.

5 Experiments
In this section we present two types of experiments. The first one addresses the problem of line
clustering, while the second one addresses the problem of illuminant-invariant face clustering. We
tested our approach against Clique Averaging algorithm (C AVERAGE), since it was the best performing approach in [2] on the same type of experiments. Specifically, C AVERAGE outperformed
Clique Expansion [10] combined with Normalized cuts, Gibson?s Algorithm under sum and product
model [9], kHMeTiS [11] and Cascading RANSAC [2]. We also compare against Super-symmetric
Non-negative Tensor Factorization (S NTF) [18], because it is the only approach, other than ours,
which does not approximate the hypergraph to a graph.
Since both C AVERAGE and S NTF, as opposed to our method, require the number of classes K to be
specified, we run them with values of K ? {K ? ? 1, K ? , K ? + 1} among which the optimal one
(K ? ) is present. This allows us to verify the robustness of the approaches under wrong values of K,
which may occur in general as the optimal number of clusters is not known in advance.
5

1.2
HoCluGame
Cav. K=2
Cav. K=3
Cav. K=4
Sntf K=2
Sntf K=3
Sntf K=4

1.5

1
1

0.8
F-measure

0.5

0

?0.5

0.6

0.4

?1

0.2
?1.5
?1.5

?1

?0.5

0

0.5

1

1.5

2

0
0

0.01 0.02

(a) Example of three lines with ? = 0.04.

?

0.08

(b) Three lines.

1.2

1.2
HoCluGame
Cav. K=3
Cav. K=4
Cav. K=5
Sntf K=3
Sntf K=4
Sntf K=5

1

HoCluGame
Cav. K=4
Cav. K=5
Cav. K=6
Sntf K=4
Sntf K=5
Snft K=6

1

0.8
F-measure

0.8
F-measure

0.04

0.6

0.6

0.4

0.4

0.2

0.2

0

0
0

0.01 0.02

0.04

?

0.08

0

(c) Four lines.

0.01 0.02

0.04

?

0.08

(d) Five lines.

Figure 1: Results on clustering 3, 4 and 5 lines perturbed with increasing levels of Gaussian noise
(? = 0, 0.01, 0.02, 0.04, 0.08).
We executed the experiments on a AMD Sempron 3Ghz computer with 1Gb RAM. Moreover, we
evaluated the quality of a clustering by computing the average F-measure of each cluster in the
ground-truth with the most compatible one in the obtained solution (according to a one-to-one correspondence).
5.1 Line clustering
We consider the problem of clustering lines in spaces of dimension greater than two, i.e., given a
set of points in Rd , the task is to find sets of collinear points. Pairwise measures of similarity are
useless and at least three points are needed. The dissimilarity measure on triplets of points is given
by their mean distance to the best fitting line. If d(i, j, k) is the dissimilarity of points {i, j, k}, the
similarity function is given by s({i, j, k}) = exp(?d(i, j, k)2 /? 2 ), where ? is a scaling parameter,
which has been optimally selected for all the approaches according to a small test set.
We conducted two experiments, in order to assess the robustness of the approaches to both local
and global noise. Local noise refers to a Gaussian perturbation applied to the points of a line, while
global noise consists of random outlier points.
A first experiment consists in clustering 3, 4 and 5 lines generated in the 5-dimensional space
[?2, 2]5 . Each line consists of 20 points, which have been perturbed according to 5 increasing
levels of Gaussian noise, namely ? = 0, 0.01, 0.02, 0.04, 0.08. With this setting there are no outliers
and every point should be assigned to a line (e.g., see Figure 1(a)). Figure 1(b) shows the results
obtained with three lines. We reported, for each noise level, the mean and the standard deviation
of the average F-measures obtained by the algorithms on 30 randomly generated instances. Note
that, if the optimal K is used, C AVERAGE and S NTF perform well and the influence of local noise
is minimal. This behavior intuitively makes sense under moderate perturbations, because if the approaches correctly partitioned the data without noise, it is unlikely that the result will change by
slightly perturbing them. Our approach however achieves good performances as well, although we
can notice that with the highest noise level, the performance slightly drops. This is due to the fact
that points deviating too much from the overall cluster average collinearity will be excluded as they
undermine the cluster?s internal coherency. Hence, some perturbed points will be considered outliers. Nevertheless, it is worth noting that by underestimating the optimal number of classes both
C AVERAGE and S NTF exhibit a drastic performance drop, whereas the influence of overestimations
6

1.2
HoCluGame
Cav. K=2
Cav. K=3
Cav. K=4
Sntf K=2
Sntf K=3
Sntf K=4

1.5

1
1

0.5

F-measure

0.8

0

?0.5

0.6

0.4
?1
First line
Second line

?1.5

0.2

Outliers
?2
?2

?1.5

?1

?0.5

0

0.5

1

1.5

2

0
0

10

(a) Example of two lines with 40 outliers.

?

40

(b) Two lines.

1.2

1.2
HoCluGame
Cav. K=2
Cav. K=3
Cav. K=4
Sntf K=2
Sntf K=3
Sntf K=4

1

HoCluGame
Cav. K=3
Cav. K=4
Cav. K=5
Sntf K=3
Sntf K=4
Sntf K=5

1

0.8
F-measure

0.8
F-measure

20

0.6

0.6

0.4

0.4

0.2

0.2

0

0
0

10

20

?

40

0

(c) Three lines.

10

20

?

40

(d) Four lines.

Figure 2: Results on clustering 2, 3 and 4 lines with an increasing number of outliers (0, 10, 20, 40).
has a lower impact on the two partition-based algorithms. By increasing the number of lines involved
in the experiment from three to four (Figure 1(c)) and to five (Figure 1(d)) the scenario remains almost the same for our approach and S NTF, while we can notice a slight decrease of C AVERAGE?s
performance.
The second experiment consists in clustering 2, 3 and 4 slightly perturbed lines (with fixed local
noise ? = 0.01) generated in the 5-dimensional space [?2, 2]5 . Again, each line consists of 20
points. This time however we added also global noise, i.e., 0, 10, 20 and 40 random points as outliers
(e.g., see Figure 2(a)). Figure 2(b) shows the results obtained with two lines. Here, the supremacy
of our approach over partition-based ones is clear. Indeed, our method is not influenced by outliers
and therefore it performs almost perfectly, whereas C AVERAGE and S NTF perform well only without
outliers and with the optimal K. It is interesting to notice that, as outliers are introduced, C AVERAGE
and S NTF perform better with K > 2. Indeed, the only way to get rid of outliers is to group them in
additional clusters. However, since outliers are not mutually similar and intuitively they do not form
a cluster, we have that the performance of C AVERAGE and S NTF drop drastically as the number of
outliers increases. Finally, by increasing the number of lines from two to three (Figure 2(c)) and
to four (Figure 2(d)), the performance of C AVERAGE and S NTF get worse, while our approach still
achieves good results.
5.2 Illuminant-invariant face clustering
In [5] it has been shown that images of a Lambertian object illuminated by a point light source lie in
a three dimensional subspace. According to this result, if we assume that four images of a face form
the columns of a matrix then d = s24 /(s21 + ? ? ? + s24 ) provides us with a measure of dissimilarity,
where si is the ith singular value of this matrix [2]. We use this dissimilarity measure for the face
clustering problem and we consider as dataset the Yale Face Database B and its extended version
[8, 12]. In total we have faces of 38 individuals, each under 64 different illumination conditions. We
compared our approach against C AVERAGE and S NTF on subsets of this face dataset. Specifically,
we considered cases where we have faces from 4 and 5 random individuals (10 faces per individual),
and with and without outliers. The case with outliers consists in 10 additional faces each from a
different individual. For each of those combinations, we created 10 random subsets. Similarly to the
case of line clustering, we run C AVERAGE and S NTF with values of K ? {K ? ? 1, K ? , K ? + 1},
where K ? is the optimal one.
7

n. of classes:
n. of outliers:
C AVERAGE K=3
C AVERAGE K=4
C AVERAGE K=5
C AVERAGE K=6
S NTF K=3
S NTF K=4
S NTF K=5
S NTF K=6
HoCluGame

4
0
0.63?0.11
0.96?0.06
0.91?0.06
0.62?0.12
0.87?0.07
0.82?0.09
0.95?0.03

5
10
0.59?0.07
0.84?0.07
0.79?0.05
0.58?0.10
0.81?0.08
0.76?0.09
0.94?0.02

0
0.56?0.14
0.85?0.12
0.84?0.09
0.61?0.13
0.86?0.12
0.85?0.08
0.95?0.05

10
0.58?0.07
0.83?0.06
0.82?0.06
0.59?0.09
0.80?0.07
0.79?0.11
0.94?0.02

Table 1: Experiments on illuminant-invariant face clustering.
In Table 1 we report the average F-measures (mean and standard deviation) obtained by the three
approaches. The results are consistent with those obtained in the case of line clustering with the
exception of S NTF, which performs worse than the other approaches on this real-world application.
C AVERAGE and our algorithm perform comparably well when clustering 4 individuals without outliers. However, our approach turns out to be more robust in every other tested case, i.e., when the
number of classes increases and when outliers are introduced. Indeed, C AVERAGE?s performance
decreases, while our approach yields the same good results.
In both the experiments of line and face clustering the execution times of our approach were higher
than those of C AVERAGE, but considerably lower than S NTF. The main reason why C AVERAGE
run faster is that our approach and S NTF work directly on the hypergraph without resorting to pairwise relations, which is indeed what C AVERAGE does. Further, we mention that our code was not
optimized to improve speed and all the approaches were run without any sampling policy.

6 Discussion
In this paper, we offered a game-theoretic perspective to the hypergraph clustering problem. Within
our framework the clustering problem is viewed as a multi-player non-cooperative game, and classical equilibrium notions from evolutionary game theory turn out to provide a natural formalization
of the notion of a cluster. We showed that the problem of finding these equilibria (clusters) is equivalent to solving a polynomial optimization problem with linear constraints, which we solve using an
algorithm based on the Baum-Eagon inequality. An advantage of our approach over traditional techniques is the independence from the number of clusters, which is indeed an intrinsic characteristic
of the input data, and the robustness against outliers, which is especially useful when solving figureground-like grouping problems. We also mention, as a potential positive feature of the proposed
approach, the possibility of finding overlapping clusters (e.g., along the lines presented in [21]), although in this paper we have not explicitly dealt with this problem. The experimental results show
the superiority of our approach with respect to the state of the art in terms of quality of solution. We
are currently studying alternatives to the plain Baum-Eagon dynamics in order to improve efficiency.
Acknowledgments. We acknowledge financial support from the FET programme within EU FP7,
under the SIMBAD project (contract 213250). We also thank Sameer Agarwal and Ron Zass for
providing us with the code of their algorithms.

References
[1] S. Agarwal, K. Branson, and S. Belongie. Higher order learning with graphs. In Int. Conf. on
Mach. Learning, volume 148, pages 17?24, 2006.
[2] S. Agarwal, J. Lim, L. Zelnik-Manor, P. Perona, D. Kriegman, and S. Belongie. Beyond
pairwise clustering. In IEEE Conf. Computer Vision and Patt. Recogn., volume 2, pages 838?
845, 2005.
[3] L. E. Baum and J. A. Eagon. An inequality with applications to statistical estimation for
probabilistic functions of Markov processes and to a model for ecology. Bull. Amer. Math.
Soc., 73:360?363, 1967.
8

[4] L. E. Baum, T. Petrie, G. Soules, and N. Weiss. A maximization technique occurring in the
statistical analysis of probabilistic functions of Markov chains. Ann. Math. Statistics, 41:164?
171, 1970.
[5] P. Belhumeur and D. Kriegman. What is the set of images of an object under all possible
lighting conditions. Int. J. Comput. Vision, 28(3):245?260, 1998.
[6] M. Bolla. Spectral, euclidean representations and clusterings of hypergraphs. Discr. Math.,
117:19?39, 1993.
[7] M. Broom., C. Cannings, and G. T. Vickers. Multi-player matrix games. Bull. Math. Biology,
59(5):931?952, 1997.
[8] A. S. Georghiades., P. N. Belhumeur, and D. J. Kriegman. From few to many: illumination
cone models for face recognition under variable lighting and pose. IEEE Trans. Pattern Anal.
Machine Intell., 23(6):643?660, 2001.
[9] D. Gibson, J. M. Kleinberg, and P. Raghavan. VLDB, chapter Clustering categoral data: An
approach based on dynamical systems., pages 311?322. Morgan Kaufmann Publishers Inc.,
1998.
[10] T. Hu and K. Moerder. Multiterminal flows in hypergraphs. In T. Hu and E. S. Kuh, editors,
VLSI circuit layout: theory and design, pages 87?93. 1985.
[11] G. Karypis and V. Kumar. Multilevel k-way hypergraph partitioning. VLSI Design, 11(3):285?
300, 2000.
[12] K. C. Lee, J. Ho, and D. Kriegman. Acquiring linear subspaces for face recognition under
variable lighting. IEEE Trans. Pattern Anal. Machine Intell., 27(5):684?698, 2005.
[13] D. G. Luenberger. Linear and nonlinear programming. Addison Wesley, 1984.
[14] M. Pavan and M. Pelillo. Dominant sets and pairwise clustering. IEEE Trans. Pattern Anal.
Machine Intell., 29(1):167?172, 2007.
[15] M. Pelillo. The dynamics of nonlinear relaxation labeling processes. J. Math. Imag. and Vision,
7(4):309?323, 1997.
[16] J. Rodr`?guez. On the Laplacian spectrum and walk-regular hypergraphs. Linear and Multilinear Algebra, 51:285?297, 2003.
[17] S. Rota Bul`o. A game-theoretic framework for similarity-based data clustering. PhD thesis,
University of Venice, 2009.
[18] A. Shashua, R. Zass, and T. Hazan. Multi-way clustering using super-symmetric non-negative
tensor factorization. In Europ. Conf. on Comp. Vision, volume 3954, pages 595?608, 2006.
[19] J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Trans. Pattern Anal.
Machine Intell., 22:888?905, 2000.
[20] A. Torsello, S. Rota Bul`o, and M. Pelillo. Grouping with asymmetric affinities: a gametheoretic perspective. In IEEE Conf. Computer Vision and Patt. Recogn., pages 292?299,
2006.
[21] A. Torsello, S. Rota Bul`o, and M. Pelillo. Beyond partitions: allowing overlapping groups in
pairwise clustering. In Int. Conf. Patt. Recogn., 2008.
[22] J. W. Weibull. Evolutionary game theory. Cambridge University Press, 1995.
[23] D. Zhou, J. Huang, and B. Sch?olkopf. Learning with hypergraphs: clustering, classification,
embedding. In Adv. in Neur. Inf. Processing Systems, volume 19, pages 1601?1608, 2006.
[24] J. Y. Zien, M. D. F. Schlag, and P. K. Chan. Multilevel spectral hypergraph partitioning
with arbitrary vertex sizes. IEEE Trans. on Comp.-Aided Design of Integr. Circ. and Systems,
18:1389?1399, 1999.

9


----------------------------------------------------------------

