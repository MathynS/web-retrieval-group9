query sentence: weakest link in subnetworks
---------------------------------------------------------------------
title: 3491-measures-of-clustering-quality-a-working-set-of-axioms-for-clustering.pdf

measur cluster qualiti work set axiom cluster margareta ackerman shai ben-david school comput scienc univers waterloo canada abstract aim toward develop general cluster theori discuss abstract axiomat cluster respect follow work kleinberg show imposs result axiomat argu imposs result inher featur cluster rather larg extent artifact specif formal use oppos previous work focus cluster function propos address cluster qualiti measur object axiomat show principl like formul kleinberg axiom readili express latter framework without lead inconsist clustering-qu measur cqm function given data set partit cluster return non-neg real number repres strong conclus cluster analyz clustering-qu measur look like introduc set requir axiom measur axiom captur principl express kleinberg axiom retain consist propos sever natur cluster qualiti measur satisfi propos axiom addit analyz comput complex evalu qualiti given cluster show propos cqms comput polynomi time introduct high influenti paper kleinberg advoc develop theori cluster independ particular algorithm object function generat data model step direct kleinberg set set axiom aim defin cluster function kleinberg suggest three axiom sound plausibl show seem natur axiom lead contradict exist function satisfi three requir kleinberg result often interpret state imposs defin cluster even develop general theori cluster disagre view paper show imposs result larg extent due specif formal use kleinberg rather inher featur cluster rather attempt defin cluster function demonstr fail attempt turn attent close relat issu evalu qualiti given data cluster paper develop formal consist axiomat latter notion turn clustering-qu framework richer flexibl cluster function particular allow postul axiom captur featur kleinberg axiom aim express without lead contradict clustering-qu measur function map pair form dataset cluster order set say set non-neg real number valu reflect good cogent cluster measur qualiti cluster interest vehicl axiomat cluster need measur qualiti given data cluster aris natur mani cluster issu aim cluster uncov meaning group data howev arbitrari partit given data set reflect structur upon obtain cluster usual via algorithm user need determin whether cluster suffici meaning reli upon data mine analysi practic applic clustering-qu measur cqms aim answer need quantifi good specif cluster clustering-qu measur may also use help cluster model-select compar differ cluster data set compar result given cluster paradigm differ choic cluster paramet number cluster pose problem find clustering-qu measur first attempt may invok loss object function use cluster algorithm k-mean k-median clustering-qu measur howev measur shortcom purpos hand name measur usual scale-invari use compar qualiti cluster obtain differ algorithm aim minim differ cluster cost k-mean differ valu see section detail cluster qualiti previous discuss appli statist literatur varieti techniqu evalu cluster valid propos mani method extern criteria discuss base assum predetermin data generat model answer quest general theori cluster work concern qualiti measur regardless specif generat model exampl see intern criteria survey formul theoret basi clustering-qu evalu propos set requir axiom clustering-qu measur demonstr relev consist axiom show sever natur notion satisfi requir particular introduc quality-measur reflect under intuit center-bas linkage-bas cluster notion satisfi axiom given data cluster valu cluster comput polynomi time paper outlin begin present kleinberg axiom cluster function discuss failur section show axiom translat axiom pertain cluster qualiti measur prove result set axiom consist section discuss desir properti axiomat propos accord revis set axiom next section present sever clustering-qu measur claim satisfi axiom final section show qualiti cluster comput polynomi time respect propos clustering-qu measur definit notat let domain set usual finit function distanc function xi xi xi xj otherwis note requir triangl inequ k-cluster k-partit c2 ck ci cj ci cluster k-cluster cluster trivial cluster contain one point consist one cluster cluster write whenev cluster cluster otherwis cluster function domain set function take distanc function output cluster clustering-qu measur cqm function given cluster distanc function return non-neg real number well satisfi addit requir work explor question requir kleinberg axiom kleinberg propos follow three axiom cluster function axiom intend captur mean cluster determin function domain set endow distanc function worthi consid cluster function kleinberg show set inconsist exist function satisfi three axiom first two axiom requir invari cluster defin chang input distanc function function scale invari scale invari requir output cluster function invari uniform scale input function scale-invari everi distanc function posit defin set everi pair domain point function consist consist requir within-clust distanc decreas between-clust distanc increas output cluster function chang formal given cluster distanc function d0 c-consist variant d0 d0 function consist whenev d0 -consist variant function rich rich requir modifi distanc function partit under data set obtain function rich partit exist distanc function c. theorem kleinberg exist cluster function simultan satisfi scale invari consist rich discuss intuit behind axiom rather clear let us consid exampl consist requir seem reason pull closer point cluster push apart point differ cluster confid given cluster rise howev intuit readili formul term cluster qualiti name chang decreas qualiti cluster formul cluster function say actual requir chang under distanc function creat new contend best-clust data exampl consid figur illustr good 6-cluster right hand-sid show consist chang 6-cluster notic result data 3-cluster reason better origin 6-cluster one may argu qualiti origin 6-cluster decreas result distanc chang qualiti 3-cluster improv beyond 6-cluster illustr signific weak consist axiom cluster function implicit requir origin cluster remain best cluster follow consist chang heart kleinberg imposs result shall see relax extra requir axiom longer unsatisfi axiom clustering-qu measur section chang primit defin axiom cluster function clustering-qu measur reformul three axiom term cqms figur consist chang 6-cluster show revis formul consist also satisfi number natur cluster qualiti measur addit extend set axiom ad anoth axiom clustering-qu measur requir rule measur count cqms clustering-qu measur analogu kleinberg axiom translat scale invari axiom cqm terminolog straightforward definit scale invari qualiti measur satisfi scale invari everi cluster everi posit translat consist axiom place result cqm formul inde weaker origin axiom function clear captur intuit consist chang hurt qualiti given partit allow possibl result chang partit improv others1 definit consist qualiti measur satisfi consist everi cluster whenev d0 consist variant d0 definit rich qualiti measur satisfi rich non-trivi cluster exist distanc function argmax c theorem consist scale invari rich clustering-qu measur form consist set requir proof show scale-invari consist rich form consist set axiom present cluster qualiti measur satisfi three axiom measur captur qualiti intuit center-bas cluster section introduc qualiti measur captur goal type cluster cqm satisfi three axiom point data set consid ratio distanc point closest center distanc point second closest center intuit smaller ratio better cluster point confid cluster membership use averag ratio qualiti measur definit relat point margin k-relat point margin k-rmx x cx x cx0 cx closest center cx second closest center follow formal assum larger valu indic better cluster qualiti qualiti measur smaller valu indic better cluster qualiti case revers direct inequ consist use argmin instead argmax rich set repres set cluster consist exact one point cluster c. definit repres set set repres set cluster c2 ck ci definit relat margin relat margin cluster rmx min repres set avgx x\k k-rmx smaller valu relat margin indic better cluster qualiti lemma relat margin scale-invari proof let cluster let d0 distanc function d0 d0 point d0 note also scale chang center select relat margin therefor rmx d0 rmx lemma relat margin consist proof let cluster distanc function let d0 consist variant d0 d0 therefor rmx d0 rmx lemma relat margin rich proof given non-trivi cluster data set consid distanc function argmin c follow scale-invari consist rich consist axiom sound complet axiom set axiom cluster satisfi usual when set axiom propos semant notion class object say cluster function aim sound complet sound mean everi element describ class satisfi axiom particular sound impli consist axiom complet mean everi properti share object class impli axiom intuit ignor logic subtleti set axiom complet class object element outsid class fail least one axiom context major difficulti exist semant definit cluster wish use axiom as definit cluster function then mean sound complet settl less while clear definit cluster exampl function consid cluster function come exampl partit clear worthi call cluster replac sound requir axiom satisfi exampl common cluster function relax sound want partit function clear cluster fail least one axiom relax complet respect axiom bad fail relax version sound axiom natur cluster function fail satisfi impli kleinberg demonstr pair axiom satisfi natur cluster while three togeth never hold argu scale invari consist rich sound class cqms howev make complet set axiom even relax sens function consid reason cluster qualiti measur yet satisfi three axiom one type non-clustering-funct function make cluster membership decis base ident domain point exampl function return relat margin data set whenev specif pair data point belong cluster twice relat margin data set otherwis we overcom problem introduc new axiom isomorph invari axiom resembl permut invari object function axiom puzicha model requir cluster indiffer individu ident cluster element axiom clustering-qu measur correspond kleinberg axiom definit cluster isomorph two cluster domain isomorph denot exist distance-preserv isomorph definit isomorph invari qualiti measur isomorph invari cluster c theorem set axiom consist isomorph invari scale invari consist rich cqm formul consist set axiom proof note relat margin qualiti measur satisfi four axiom as mention discuss satisfactori axiom system notion one need requir just consist worthi label axiom requir we propos satisfi reason notion cqm cours sinc we defin what cqms reason we turn formal statement what we howev demonstr varieti natur cqms satisfi axiom done next section exampl cluster qualiti measur survey valid measur milligan discuss exampl qualiti measur satisfi axiom name scale-invari consist rich perturb invari we verifi best perform intern criteria examin in satisfi axiom in section we introduc two novel qcms measur reflect under intuit linkage-bas cluster measur center-bas cluster in addit satisfi axiom given cluster measur comput in polynomi time weakest link in linkage-bas cluster whenev pair point share cluster connect via tight chain point in cluster weakest link qualiti measur focus longest link in chain definit weakest link point c-w lx min ci max x cluster ci cluster in c. weakest link maxim valu c-w lx pair point belong cluster divid shortest between-clust distanc definit weakest link weakest link cluster maxx c c-w lx minx6 c rang valu weakest link addit margin in section we introduc relat margin qualiti measur center-bas cluster we introduc anoth qualiti measur center-bas cluster instead look ratio addit margin evalu differ definit addit point margin k-addit point margin k-amx cx0 cx cx closest center cx0 second closest center addit margin cluster averag addit point margin divid averag within-clust distanc normal necessari scale invari definit addit margin addit margin center-bas cluster x x k-amx amx min repres set x c unlik relat margin addit margin give higher valu better cluster comput complex clustering-qu measur use import abl quick comput qualiti cluster use measur qualiti cluster use measur present in this paper comput in polynomi time in term number point in data set use relat addit margin take oper comput cluster qualiti data set exponenti in set center given relat margin comput in o nk oper addit margin comput in oper weakest link cluster comput in oper variant qualiti measur given clustering-qu measur we construct new qualiti measur differ characterist appli qualiti measur subset cluster it suffic consid qualiti measur defin cluster consist cluster given measur we creat new qualiti measur exampl mmin min measur worst qualiti pair cluster in c. altern we defin mmax mavg evalu best averag qualiti pair cluster in c. nice featur these variat satisfi four axiom clustering-qu measur then mmin mmax mavg general defin for cluster arbitrari number cluster we defin qualiti measur as function larger cluster for exampl mmax subset mani variat appli exist clustering-qu measur subset cluster satisfi axiom clustering-qu measur whenev origin qualiti measur satisfi axiom depend number cluster clustering-qu measur discuss in this paper independ number cluster enabl comparison cluster differ number cluster in this section we discuss altern type cluster qualiti evalu depend number cluster qualiti measur aris natur common loss function object function drive cluster algorithm as k-mean k-median these common loss function fail satisfi two axiom scale-invari rich one easili overcom depend scale normal as we show result normal loss function make a differ type clustering-qu measur measur we previous discuss due depend on number cluster a natur remedi failur scale invari normal a loss function divid it the varianc the data altern the loss the 1-cluster the data definit l-normal the l-normal a cluster l-normal c l call call denot the 1-cluster common loss function even normal usual a bias toward either refin toward coars cluster assign lower cost higher qualiti refin respect coars cluster this prevent use as a meaning tool for compar the qualiti cluster differ number cluster we formal this featur common cluster loss function the notion refin prefer definit refin coarsen for a pair cluster the domain we say a refin equival a coarsen for everi cluster ci ci a union cluster definit refinement/coarsen prefer a measur refinement-pref for everi cluster if it a non-trivi refin then there exist such a refin for c coarsening-pref measur defin analog note refin prefer coarsen prefer measur fail satisfi the rich axiom it seem there a divid between two type evalu for cluster satisfi rich satisfi either refin coarsen prefer evalu the qualiti a cluster use a refin coarsen prefer measur it essenti fix the number cluster sinc the correct number cluster often unknown measur independ the number cluster appli in a more general set conclus we investig the possibl provid a general axiomat basi for cluster start point the imposs result kleinberg we argu a natur way overcom these negat conclus chang the primit use to formul the axiom cluster function to cluster qualiti measur cqms we demonstr the merit the latter framework provid a set axiom for cqms captur the essenc all kleinberg axiom while maintain consist we propos sever cqms satisfi our propos set axiom we hope that this work our demonstr a way to overcom the imposs result stimul research toward a general theori of cluster
----------------------------------------------------------------

title: 4659-topic-partitioned-multinetwork-embeddings.pdf

topic-partit multinetwork embed peter krafft csail mit pkrafft mit.edu juston moor bruce desmarai hanna wallach depart comput scienc depart polit scienc univers massachusett amherst jmoor wallach cs.umass.edu desmarai polsci.umass.edu abstract introduc new bayesian admixtur model intend exploratori analysi communic network specif discoveri visual topic-specif subnetwork email data set model produc principl visual email network visual precis mathemat interpret term model relationship observ data valid model assumpt demonstr model achiev better link predict perform three state-of-the-art network model exhibit topic coher compar latent dirichlet alloc showcas model abil discov visual topic-specif communic pattern use new email data set new hanov counti email network provid extens analysi communic pattern lead us recommend model exploratori analysi email network similarly-structur communic data final advoc principl visual primari object develop new network model introduct structur organiz communic network critic collabor problem solv although seldom possibl research direct observ complet organiz communic network email data set provid one mean least partial observ reason result especi light rich textual detail exist infrastructur widespread usag email data set hold potenti answer mani import scientif practic question within organiz social scienc question may answer studi structur email network whole nuanc question answer finer level granular specif studi topic-specif subnetwork exampl break communic duplic communic particular topic may indic need form organiz restructur order facilit studi kind question present new bayesian admixtur model intend discov summar topic-specif communic subnetwork email data set number probabilist model incorpor network text data although model specif email network mccallum al author recipi topic model intend network document web page link academ paper citat contrast email network natur view network actor exchang document actor associ node document associ edg word email network defin multinetwork may multipl edg one per email pair actor perhap import much recent work model network text focus task work done univers massachusett amherst figur model partit observ email network left topic-specif subnetwork right associ author recipi edg observ network singl topic predict link detect communiti instead take complementari approach focus exploratori analysi specif goal discov visual topic-specif subnetwork rather take two-stag approach subnetwork discov use one model visual use anoth present singl probabilist model partit observ email network topic-specif subnetwork simultan produc visual represent subnetwork network model visual undertaken separ result visual may direct reflect model relationship observ data rather visual provid view model data seen len visual algorithm associ assumpt conclus drawn visual bias artifact visual algorithm produc principl visual network visual precis interpret term associ network model relationship observ data remain open challeng statist network model address open challeng primari object develop new model order discov visual topic-specif subnetwork model must associ author recipi edg observ email network topic shown figur model draw upon idea latent dirichlet alloc lda identifi set corpus-wid topic communic well subset topic best describ observ email model network structur use approach similar hoff al latent space model lsm facilit visual given observ network lsm associ actor network point k-dimension euclidean space pair actor smaller distanc point like interact interact probabl collect known communic pattern direct visual 3-dimension space via locat actor-specif point model extend idea associ k-dimension euclidean space topic observ author recipi edg explicit associ topic via k-dimension topic-specif communic pattern next section present mathemat detail new model outlin correspond infer algorithm introduc new email data set new hanov counti nhc email network although model intend exploratori analysi test model assumpt via three valid task section show model achiev better link predict perform three state-of-the-art network model also demonstr model capabl infer topic coher infer use lda togeth experi indic model appropri model network structur model structur compromis topic qualiti final valid experi show synthet data generat use model possess similar network statist nhc email network section showcas model abil discov visual topic-specif communic pattern use nhc network give extens analysi communic pattern demonstr provid access visual emailbas collabor possess precis meaning interpret within mathemat framework model find lead us recommend model exploratori analysi email network similarly-structur communic data final advoc principl visual primari object develop new network model topic-partit multinetwork embed section present new probabilist generat model associ infer algorithm communic network concret frame discuss model term email data although general applic similarly-structur communic data generat process graphic model provid supplementari materi singl email index repres set token wn compris text email integ indic ident email author set binari variabl yr indic whether actor network recipi email simplic assum author send email yr given real-world email data set model permit infer topic express text email set topic-specif k-dimension embed point k-dimension euclidean space actor network partit full communic network set topic-specif subnetwork lda topic character discret distribut word type probabl vector symmetr dirichlet prior concentr paramet place captur relationship topic express email email recipi topic also associ communic pattern matrix probabl given email topic author actor element par probabl actor includ actor recipi email inspir lsm communic pattern repres implicit via set point k-dimension euclidean par pra ksa sr space sa scalar bias term sa i if represent enabl topic-specif communic pattern visual 3-dimension space via locat point associ actor worth note dimens k-dimension space inher mean isol point sa convey inform howev distanc two point precis meaning interpret generat process specif recipi email associ topic like actor near email author euclidean space correspond topic email index discret distribut topic symmetr dirichlet prior concentr paramet place token wn associ topic assign zn zn wn zn model includ distribut author generat process condit upon ident email-specif binari variabl yr indic recipi email thus presenc absenc email-specif edg author actor consequ may multipl edg one per email pair actor defin multinetwork entir set actor assum complet multinetwork compris topic-specif subnetwork word yr associ topic therefor topic-specif communic pattern yr bern par natur way associ yr topic would draw topic assign manner analog generat zn howev outlin blei jordan approach result undesir scenario one subset topic associ token anoth disjoint subset associ edg addit model annot data possess exchang structur tend exhibit poor general better approach advoc blei jordan draw topic assign yr empir distribut topic defin definit set topic associ edg therefor subset topic associ token one way simul generat process associ yr posit max therefor topic assign zn position2 draw posit assign xr max yr indirect procedur ensur yr bern par xr zn desir function logist function function l2 norm email contain text convey inform frequenc communic author recipi result omit email instead augment one singl dummi topic assign z1 associ token w1 infer real-world data author token recipi observ unobserv dirichlet multinomi conjugaci allow margin typic valu remain unobserv variabl sampl joint posterior distribut use markov chain mont carlo method section outlin metropolis-within-gibb sampl algorithm oper sequenti resampl valu latent variabl sa bt zn xr condit posterior sinc zn discret random variabl new valu may sampl direct use wn w\d n z\d n yr n\d n r xr n\d n pa otherwis subscript denot quantiti exclud data posit email count total number token assign topic type belong email new valu discret random variabl xr may sampl direct use zn z\d n yr pa new valu continu random variabl sa sampl direct condit posterior may instead obtain use metropoli hast algorithm non-inform prior sa sa condit posterior sa ar ar count pd count defin similar likewis improp noninform prior condit posterior ar ar r r data due varieti factor involv person privaci concern ownership content email servic provid academ research rare access organiz email data exampl enron data set arguabl wide studi email data set releas court order public record altern sourc organiz email data public record data set wide avail continu updat yet remain relat untap academ communiti therefor introduc analyz new public record email data set relev research organiz social scienc well machin learn research data set consist email manag depart constitut execut arm govern counti level new hanov counti north carolina semi-autonom local govern counti manag act execut individu depart synonym individu depart agenc instanc u.s. feder govern therefor email data set offer view communic pattern manag new hanov counti analys also serv case studi model inter-ag communic u.s. feder govern administr function evalu one if argument evalu true evalu zero otherwis model erosheva baselin mmsb baselin lsm number topic model erosheva baselin mmsb baselin lsm number topic model erosheva baselin lda averag topic coher model erosheva baselin lda averag topic coher averag f1 score averag f1 score number topic number topic figur averag link predict perform nhc email network enron data set mmsb lsm report result obtain use best-perform hyperparamet valu averag topic coher score nhc email network enron data set new hanov counti nhc email network compris complet inbox outbox depart manag month februari total email author manag email sent manag via field exclud email sent manag experi use email actor verifi model applic beyond nhc email network also perform two valid experi use enron email data set data set treat uniqu enron email address actor use email activ actor determin total number email sent receiv email sent least one activ actor via field discard avoid duplic email retain email sent mail sent sent item folder step result total email involv actor data set preprocess concaten text subject line messag bodi remov stop word url quot text possibl signatur experi model primarili intend exploratori analysi tool organiz communic network section use nhc email network showcas model abil discov visual topic-specif communic subnetwork first howev test under model assumpt via three quantit valid task recommend schrodt link predict order gaug model predict perform evalu abil predict recipi test email either nhc email network enron data set condit text email ident author test email binari variabl indic recipi email yr treat unobserv typic valu variabl sampl joint posterior distribut compar true valu yield f1 score form test split data set random select email probabl data set averag f1 score five random test split compar model perform two baselin three exist network model therebi situat within exist literatur given test email author actor simplest baselin na veli predict actor includ actor recipi email probabl equal number non-test email sent actor actor divid total number non-test email sent actor second baselin variant model topic-specif communic pattern repres explicit via probabl drawn symmetr beta prior concentr paramet compar model variant enabl us valid assumpt topic-specif communic pattern inde accur repres set point one per actor k-dimension euclidean space also compar model perform three exist network model variant erosheva al model analyz scientif public lsm mixed-membership stochast blockmodel mmsb erosheva al model view variant model topic assign yr drawn instead empir distribut topic defin like second baselin topic-specif communic pattern repres explicit via probabl drawn symmetr beta prior concentr paramet howev unlik baselin one repres use prob abil par pr lsm view network-on variant model text model as result topic singl communic pattern pattern repres implicit via set a actor-specif point k-dimension euclidean space final mmsb a widely-us model mixed-membership communiti discoveri network model variant typic valu yr a sampl joint posterior distribut use appropriately-modifi version metropolis-within-gibb algorithm section experi ran algorithm iter iter defin propos distribut a gaussian distribut center valu iter covari matrix max i therebi result larger covari earlier iter beta binomi conjugaci allow element margin second baselin erosheva al model mmsb typic valu sampl use a modifi version chang gibb sampl algorithm ran algorithm iter model involv topic we set concentr paramet nhc network for enron data set for data set we set concentr paramet we vari number topic order facilit visual we use 2-dimension euclidean space for model for lsm howev we vari dimension euclidean space we report result obtain use best-perform dimension for second baselin erosheva al model we set concentr paramet for mmsb we perform a grid search hyperparamet valu number block as lsm report result obtain use best-perform values.5 f1 score averag five random test split data set shown figur although model intend for exploratori analysi it achiev better link predict perform model furthermor fact model outperform second baselin erosheva al model valid assumpt topic-specif communic pattern inde accur repres a set a actor-specif point 2-dimension euclidean space topic coher evalu unsupervis topic model topic coher metric often use as a proxi for subject evalu semant coher order demonstr incorpor network data impair model abil model text we compar coher topic infer use model coher topic infer use lda second baselin erosheva al model for model we vari number topic drew five sampl joint posterior distribut unobserv random variabl model we evalu topic result sampl use mimno al coher metric topic coher averag five sampl shown figur model achiev coher compar lda result combin result section demonstr model achiev state-of-the-art predict perform produc coher topic posterior predict check we use posterior predict check assess extent model a good fit for nhc email network specif we defin four network statist four discrep function summar meaning aspect nhc network general graph transit dyad intens distribut vertex degre distribut geodes distanc distribution.6 we generat synthet network posterior predict distribut impli these valu obtain slice sampl typic valu for concentr paramet lda they consist concentr paramet valu use previous work these valu correspond a prior block membership a prior diagon entri blockmodel a prior off-diagon entri block these statist defin supplementari materi transit simul quantil degre simul quantil frequenc observ quantil actor sort observ degre observ quantil figur four posterior predict check model use nhc email network topic a histogram graph transit synthet network graph transit nhc email network indic a vertic line a quantil quantil plot compar distribut dyadic intens synthet network observ network a box plot indic sampl degre manag synthet network manag sort highest lowest observ degre observ degre indic a line a quantil quantil plot compar observ synthet geodes distanc distribut model nhc network we appli discrep function synthet network yield four distribut valu four network statist if model a good fit for nhc network these distribut center around valu correspond discrep function when comput use observ nhc network as shown figur model generat synthet network dyad intens vertex degre geodes distanc distribut similar nhc network distribut synthet graph transit valu center around observ graph transit observ transit suffici far tail distribut warrant reparameter model exploratori analysi order demonstr model novel abil discov visual topic-specif communic pattern we perform exploratori analysi four pattern infer nhc email network use our model these pattern visual in figur pattern repres implicit via a singl set a point in 2-dimension euclidean space drawn joint posterior distribut recipi email associ topic like actor near email author in euclidean space correspond topic we select pattern in figur as highlight type insight obtain use our model although mani structur properti may interest we focus modular assort for each topic-specif communic pattern we examin whether activ disconnect compon in topic euclidean space high modular presenc compon indic group actor engag in within between-group communic topic we also use a combin node proxim node color determin whether communic depart belong divis in new hanov counti govern organiz chart depart within differ divis assort in figur we show one topic exhibit strong modular littl assort public signag topic one topic exhibit strong assort littl modular broadcast messag topic one topic exhibit both strong assort strong modular meet schedul topic public relat topic includ communic news agenc most domin a cluster involv mani depart final meet schedul topic display hierarch structur two assist counti manag locat center group correspond divis exploratori analysi communic pattern a power tool for understand organiz communic network for exampl examin assort reveal whether actual communic pattern resembl offici organiz structur similar if a communic pattern exhibit modular each disconnect compon may benefit organiz effort facilit inter-compon communic final structur properti assort modular may also yield scientif practic insight depend on organiz need public signag broadcast messag chang sign sign process ordin fw fyi bulletin summari week legisl pm cc hl am sf ps ds eg eg lb am ds fn ps pi tx bg lb fs hr ms cm rd am am em pg it ce em rd pg ca cc sf tx fs ss ev it ms pm rm bg el ev cm ca fn ys pi ss rm ys vs hl hr vs el ce public relat meet schedul citi breakdown inform give meet march board agenda week el fs sf ca ps rm rd eg rm hr ms pi ds fs ce ev cm el pg rd tx cc lb ce em am hr bg fn hl pm ss lb hl em am it pg bg ca ms ps cm eg ss it tx cc am ys am vs ds pi fn ys pm vs sf assist counti manag budget cooper extens counti attorney counti commission counti manag develop servic elect emerg manag engin environment manag financ fire servic health human resourc inform technolog librari museum park garden plan inspect pretrial releas screen properti manag regist deed risk manag sheriff social servic tax veteran servic youth empower servic am bg ce ca cc cm ds el em eg ev fn fs hl hr it lb ms pg pi ps pm rd rm sf ss tx vs ys ev figur four topic-specif communic pattern infer nhc email network each pattern label a human-select name for correspond topic along with topic probabl word in order decreas probabl size each manag acronym in topic pattern given da maxa da da the degre actor a in subnetwork indic often manag communic topic manag acronym color accord respect divis in the new hanov counti organiz chart the acronym appear twice in plot there two assist counti manag conclus we introduc a new bayesian admixtur model for the discoveri visual topic-specif communic subnetwork although our model intend for exploratori analysi the valid experi describ in section demonstr our model achiev stateof-the-art predict perform while exhibit topic coher compar lda showcas our model abil discov visual topic-specif communic pattern we introduc a new data set the nhc email network analyz four pattern infer this data set use our model via this analysi abl to examin the extent to actual communic pattern resembl offici organiz structur identifi group manag engag in within between-group communic certain topic togeth these predict exploratori analys lead us to recommend our model for exploratori analysi email network similarly-structur communic data final our model capabl produc principl visual email network visual precis mathemat interpret in term this model relationship to the observ data we advoc for principl visual as a primari object in the develop new network model acknowledg this work support in part the center for intellig inform retriev in part by the nsf grfp grant ani opinion find conclus recommend express in this materi the author necessarili reflect of the sponsor
----------------------------------------------------------------

title: 964-an-input-output-hmm-architecture.pdf

input output hmm architectur yoshua bengio dept informatiqu recherch operationnell universit de montreal qc bengioyoiro.umontreal.ca paolo frasconi dipartimento di sistemi informatica universita di firenz itali paoloomcculloch.ing.unifi.it abstract introduc recurr architectur modular structur formul train procedur base em algorithm result model similar hidden markov model support recurr network process style allow exploit supervis learn paradigm use maximum likelihood estim introduct learn problem involv sequenti structur data effect dealt static model feedforward network recurr network allow model complex dynam system store retriev contextu inform flexibl way up present time research effort supervis learn recurr network almost exclus focus error minim gradient descent method although effect learn short term memori practic difficulti report train recurr neural network perform task tempor conting present input/output sequenc span long interv bengio mozer previous work altern train algorithm bengio could suggest root problem lie essenti discret natur process store inform indefinit amount time thus potenti solut propag backward time target discret state space rather differenti error inform extend previous work bengio frasconi paper propos statist approach target propag base em algorithm consid parametr dynam system discret state introduc modular architectur subnetwork associ discret state architectur interpret statist model train em general em gem algorithm dempster consid intern state trajectori miss data way learn decoupl also bell lab holmdel yoshua bengio paolo frasconi tempor credit assign subproblem static learn subproblem consist fit paramet next-stat output map defin estim trajectori order iter tune paramet em gem algorithm system propag forward backward discret distribut state result procedur similar baum-welch algorithm use train standard hidden markov model hmms levinson hmms howev adjust paramet use unsupervis learn wherea use em supervis fashion furthermor model present could call input/output hmm iohmm use learn map input sequenc output sequenc unlik standard hmms learn output sequenc distribut model also seen recurr version mixtur expert architectur jacob relat model alreadi propos cacciator nowlan experi artifici task bengio frasconi shown em recurr learn deal long term depend effect backpropa~ time altern algorithm howev model use bengio frasconi limit represent capabl map input sequenc final discret state present paper describ extend architectur allow fulli exploit input output portion data requir supervis learn paradigm way general sequenc process task product classif predict dealt propos architectur consid discret state dynam system base follow state space f x descript yt ut ut input vector time yt output vector xt i discret state equat defin general meali finit state machin input output may take continu valu paper consid probabilist version dynam current input current state distribut use estim state distribut output distribut next time step admiss state transit specifi direct graph whose vertic correspond model state set successor state sj system defin equat model recurr architectur depict figur architectur compos set state etwork set output network oj one state output network uniqu associ one state network share input ut state network task predict next state distribut base current input given xt-l similar output network oj predict output system given current state input subnetwork assum static defin mean smooth map nj ut oj 9j 1jj vector adjust paramet connect weight rang function may constrain order account under transit graph output pij state subnetwork nj time associ one successor state thus last layer mani unit cardin sj conveni notat suppos pij defin impos condit pij belong softmax function use last layer pij j illesj ea sj aij intermedi variabl thought input output hmm architectur cu nt input eiyt xt-l current pectod output given pilat input mquenc current atilt dlatrtbutton ct pl i ul lull xt xt+l yt-l yt yt+l xt-l xt+l yt i i ut-l hmm i ut iohmm ut+l figur propos iohmm architectur bottom bayesian network express condit depend iohmm top bayesian network standard hmm activ output unit subnetwork way pij tij vector repres intern state model comput linear combin output state network gate previous comput intern state t-iipj ipj output network compet predict global output system 1jt 1jt jt1jjt 1jjt output subnetwork oj level need specifi intern architectur state output subnetwork depend task design may decid whether includ hidden layer activ rule use hidden unit connectionist architectur also interpret probabl model let us assum multinomi distribut state variabl xt let us consid main variabl tempor recurr initi vector posit number sum interpret vector initi state probabl general obtain relat p xt i denot ui subsequ input time inclus equat follow probabilist interpret un p xt lui p xt i xt-i ut p xt-1 lui-i j=l subnetwork comput transit probabl condit input sequenc ut xt i xt-l pij neural network train minim output squar error output 1jt architectur interpret expect posit paramet probabl distribut output yt howev addit condit input ut expect also condit state xt yoshua bengio paolo frasconi e yt i xt ut actual form output densiti denot y yt chosen accord task exampl multinomi distribut suitabl sequenc classif symbol mutual exclus output instead gaussian distribut adequ produc continu output first case use softmax function output subnetwork oj second case use linear output unit subnetwork oj order reduc amount comput introduc independ model among variabl involv probabilist interpret architectur shall use bayesian network character probabilist depend among variabl specif suppos direct acycl graph depict bottom figur 1b bayesian network depend one evid consequ model associ variabl independ model previous state current input relev determin next-stat one-step memori properti analogu markov assumpt hidden markov model fact bayesian network hmms obtain simpli remov ut node arc top figur ib i yi supervis learn algorithm learn algorithm propos architectur deriv maximum likelihood principl train data set pair input output sequenc length p let denot vector paramet obtain collect paramet jj iji architectur likelihood function given p yip p i uip p p=l output valu use target may also specifi intermitt exampl sequenc classif task one may interest output yt end sequenc modif likelihood account intermitt target straightforward accord maximum likelihood principl optim paramet obtain maxim order appli em case begin note state variabl xt observ knowledg model 's state trajectori would allow one decompos tempor learn problem 2n static learn subproblem inde xt known probabl would either would possibl train subnetwork separ without take account tempor depend observ allow link em learn target propag approach discuss introduct note use viterbi-lik approxim consid like path would inde 2n static learn problem at epoch order deriv learn equat let us defin complet data uip p yip p xip p p p correspond complet data l -likelihood l iogp yip p zlp p i p=l sinc lc depend hidden state variabl maxim direct mle optim solv introduc auxiliari function iter follow two step estim comput maxim updat paramet arg max j input output hmm architectur expect express tp og p yt i xt ut hij tlog pij i hij eizitzj t-l uf yf denot zit indic variabl xt otherwis hat hij mean variabl comput use old paramet order comput hij introduc forward probabl qit p yl xt ud backward probabl f3it p yf i xt updat follow un f3it fy yt l1it lt pti ut+df3l t+l qit fy yt l1it lt pa ut qt f3it qj t-l pij qit iter em algorithm requir maxim first consid simplifi case input quantiz belong finit alphabet subnetwork behav like lookup tabl address input symbol interpret paramet wi p xt i xt-l o simplic restrict analysi classif task suppos target specifi desir final state sequenc furthermor output subnetwork use particular applic algorithm case obtain reestim formula wijk ij esj wt ut=k general howev subnetwork hidden sigmoid unit use softmax function constrain output sum one maximum found analyt case resort gem algorithm simpli produc increas exampl gradient ascent case deriv respect paramet easili comput follow let ojlt generic weight state subnetwork equat l l l hij t_l_8 pij pij t partial deriv comput use backpropag similar denot t'jik generic weight output subnetwork oi t~logfi yy l1it also comput use backpropag intuit paramet updat estim step em provid target output 2n subnetwork time although gem algorithm also guarante find local maximum likelihood converg may signific slower compar em sever experi notic converg acceler stochast gradient ascent yoshua bengio paolo frasconi comparison appear natur find similar recurr architectur describ far standard hmms levinson architectur propos paper differ from standard hmms two respect comput style learn iohmm sequenc process similar recurr network input sequenc synchron transform output sequenc comput style real-tim predict output avail input sequenc process this architectur thus allow one implement three fundament sequenc process task product predict classif final transit probabl in standard hmms fix state form homogen markov chain in iohmm transit probabl condit input thus depend time result in inhomogen markov chain consequ dynam system specifi transit probabl fix adapt in time depend input sequenc fundament differ in learn procedur while interest capabl model sequenti phenomena major weak standard hmms poor discrimin power due unsupervis learn an approach found use improv discrimin in hmms base maximum mutual inform mmi train point supervis learn discrimin learn criteria like mmi actual strict relat bridl although paramet adjust procedur defin base mle use desir output in respons input result in discrimin supervis learn final it worth mention number hybrid approach propos integr connectionist approach hmm frame\'vork exampl in bengio observ use hmm generat feedforward neural network in bourlard welleken feedforward network use estim state probabl condit acoust sequenc common featur algorithm one propos in this paper neural network use extract tempor local inform wherea markovian system integr long-term constraint yf uf also establish link iohmm adapt mixtur expert jacob recent cacciator nowlan propos recurr extens me architectur call mixtur control in gate network feedback connect thus allow take tempor context account iohmm architectur interpret special case mc architectur in set state subnetwork play role gate network modular structur second order connect regular grammar infer in this section describ an applic our architectur problem grammat infer in this task learner present set label string request infer set rule defin formal languag it consid prototyp complex languag process problem howev even in simplest case regular grammar task prove np-complet angluin smith report experiment result set regular grammar introduc tomita afterward use research measur accuraci infer method base recurr network gile pollack watrous kuhn use scalar output supervis final output yt model bernoulli variabl fy yt l-yt yt string reject yt it accept in tbis applic appli an input output hmm architectur tabl summari experiment result seven tomita 's grammar grammar size fsa min converg averag accuraci worst best w k best extern input output network this correspond model moor finit state machin given absenc prior knowledg plausibl state path use an ergod transit graph fulli connect .in experi we measur converg general perform use differ size recurr architectur for set we ran trial differ seed for initi weight we consid trial success train network abl correct label all train string model size chosen use cross-valid criterion base perform random generat string length for comparison in tabl we also report for each grammar number state minim recogn fsa tomita we test train network corpus binari string length final result summar in tabl column converg report fraction trial succeed separ train set next three column report averag order statist worst best trial fraction correct classifi string measur success trial for each grammar result refer model size select cross-valid general alway perfect grammar for each grammar the best trial also attain perfect general these result compar favor obtain second-ord network train gradient descent use the learn set propos tomita for comparison in the last column tabl we reproduc the result report watrous kuhn in the best five trial in the success trial the model learn an actual fsa behavior transit probabl asymptot converg either this render trivial the extract the correspond fsa inde for grammar we found the train network behav exact like the minim recogn fsa potenti train problem the presenc local maxima in the likelihood function for exampl the number converg trial for grammar quit small the difficulti discov the optim solut might becom a serious restrict for task involv a larg number state in experi bengio frasconi we notic restrict the connect the transit graph signific help remov problem converg cours this approach effect exploit if prior knowledg the state space avail for exampl applic hmms speech recognit alway reli structur topolog conclus there still a number open question in particular the effect the model task involv larg larg state space need care evalu in bengio frasconi we show learn long term depend in these model becom difficult as we increas the connect the state yoshua bengio paolo frasconi transit graph howev transit probabl iohmm chang at each deal better with this problem long-term depend standard hmms anoth interest aspect investig the capabl the model success perform task of sequenc product predict for exampl interest task could also approach relat time seri model motor control learn
----------------------------------------------------------------

title: 2428-clustering-with-the-connectivity-kernel.pdf

cluster connect kernel bernd fischer volker roth joachim m. buhmann institut comput scienc swiss feder institut technolog zurich zurich switzerland bernd.fisch volker.roth jbuhmann inf.ethz.ch abstract cluster aim extract hidden structur dataset problem find compact cluster wide studi literatur extract arbitrarili form elong structur consid much harder problem paper present novel cluster algorithm tackl problem two step procedur first data transform way elong structur becom compact one second step new object cluster optim compactness-bas criterion advantag method relat approach threefold robust properti compactness-bas criteria natur transfer problem extract elong structur lead model high robust outlier object transform distanc induc mercer kernel allow us formul polynomi approxim scheme general phard cluster problem iii new method contain free kernel paramet contrast method like spectral cluster mean-shift cluster introduct cluster group data import topic machin learn pattern recognit research among various possibl group principl method tri find compact cluster gain particular import presum promin method kind k-mean cluster vectori data despit power model capabl compactness-bas cluster method most fail find elong structur fast singl linkag algorithm often use algorithm search elong structur known sensit outlier dataset mean shift cluster anoth method class capabl extract elong cluster mode under probabl distribut one singl maximum furthermor suitabl kernel bandwidth paramet preselect spectral cluster show good perform mani case algorithm analyz special input instanc complet analysi algorithm still miss concern preselect suitabl kernel width spectral cluster suffer similar problem mean shift cluster paper present altern method cluster elong structur apart number cluster complet parameter-fre group principl build work path-bas cluster slight modif origin prob lem show defin path distanc induc kernel matrix fulfil mercer condit comput path-bas distanc compactness-bas pairwis cluster principl use partit data general p-hard pairwis cluster problem approxim algorithm known present polynomi time approxim scheme ptas special case path-bas distanc mercer properti distanc allow us emb data dimension vector space even non-metr input graph vector space pairwis cluster reduc minim k-mean cost function dimens latter problem howev exist ptas addit theoret result also present effici practic algorithm resort 2-approxim algorithm base kernel pca experi suggest kernel pca effect reduc nois data preserv coars cluster structur method compar spectral cluster mean shift cluster select artifici dataset addit perform demonstr usp handwritten digit dataset cluster connect main idea cluster criterion transform elong structur compact one preprocess step given transform data infer cluster solut optim compact base criterion advantag circumv problem direct find connect elong region data span tree approach follow span tree algorithm extrem sensit outlier two-step procedur may benefit statist robust certain compact base method concern general case dataset given vector space character pairwis dissimilar pairwis cluster model shown robust outlier dataset may thus natur choic formul second step search partit vector minim pairwis cluster cost function pk pc ci j cj dij denot number cluster ci denot number object cluster dij pairwis effect dissimilar object comput preprocess step idea preprocess step defin distanc object consid certain path total object set natur formal path problem repres object graph consid connect graph d0 vertic object symmetr nonneg edg weight d0ij edg origin dissimilar let us denot pij path vertex vertex order make object similar connect bridg object defin path pij effect dissimilar dpij connect maximum weight path weakest link path total dissimilar vertic defin minimum path-specif effect dissimilar dpij dij min max p pij figur illustr definit effect dissimilar object cluster pairwis effect dissimilar small fig two object belong two differ cluster howev path contain least one larg dissimilar result effect dissimilar larg fig note singl outlier fig affect basic structur path-bas distanc problem dij dij dij figur effect dissimilar object belong high-dens region ij small differ region dij larger region connect bridg occur point densiti along bridg two cluster high densiti backbon cluster see case howev point belong bridg hard consid outlier reader notic singl linkag algorithm poss robust properti sinc separ three distant outlier object exampl remain data detect domin structur summar model formal path-bas cluster problem input symmetr matrix d0ij nonneg pairwis dissimilar object zero diagon element question find cluster minim pc matrix repres effect dissimilar deriv eq connect kernel section show effect dissimilar induc mercer kernel weight graph g. mercer properti allow us deriv sever approxim result p-hard pairwis cluster problem section definit metric call ultra-metr satisfi condit dij max dik dkj distinct theorem dissimilar defin induc ultra-metr g. proof check axiom metric distanc measur plus restrict triangl inequ dij max dik dkj dij sinc weight nonneg dij dji sinc consid symmetr weight iii dii follow immedi definit proof restrict triangl inequ follow contradict suppos exist tripl dij max dik dkj situat howev contradict definit dij case exist path weakest link shorter dij equat impli dij must smaller equal max dik dkj definit metric embedd if exist set vector rp pair kxi dij proof follow lemma given lemma everi ultra-metr embedd consid realize embed introduc notion central matrix let matrix let n1 en en n-vector one ident matrix defin central qp q follow lemma proof see character embed lemma given metric embedd iff qdq negat semi definit combin lemmata yield follow theorem theorem distanc matrix defin set theorem matrix dc qdq gram matrix mercer kernel contain dot product set vector squar euclidean distanc kxi dij proof sinc ultra-metr embedd lemma negat semi definit lemma thus dc posit semi definit posit semi definit matrix defin gram matrix mercer kernel sinc scij dotproduct two vector squaredeuclidean distanc defin kxi scii scjj 2scij dcii dcjj 2dcij definit central distanc seen easili one term name origin distanc cancel dcii dcjj 2dcij dij approxim result pairwis cluster known p-hard knowledg polynomi time approxim algorithm known general case pairwis cluster special case data transform effect dissimilar howev present polynomi time approxim scheme polynomi time approxim scheme let us first consid comput effect dissimilar d. despit fact path-bas distanc minimum path whole distanc matrix comput polynomi time lemma path-bas dissimilar matrix defin equat comput run time log proof comput connect kernel matrix extent kruskal minimum span tree algorithm start cluster contain one singl object iter step two cluster ci cj merg minim cost dij minp ci q cj d0pq d0pq edg weight input graph link dij give effect dissimilar object ci object cj proof one consid case dij effect dissimilar ci cj exist path cluster ck object path smaller weight impli exist anoth pair cluster smaller merg cost run time log span tree algorithm complet input graph addit fill element matrix d. let us discuss cluster step recal first problem k-mean cluster given vector rp task partit vector way squar euclidean distanc cluster centroid minim object function k-mean given pk km ci j cj minim k-mean object function squar euclidean distanc p-hard if dimens vector grow lemma exist polynomi time approxim scheme ptas km arbitrari dimens fix k. proof ostrovski rabani present ptas k-mean use approxim lemma abl proof exist ptas pairwis data cluster use distanc defin theorem distanc defin exist ptas pc proof lemma dissimilar matrix comput polynomi time theorem find vector rp dij squar euclidean distanc howev algebra ident pc km lemma exist ptas km thus pc 2-approxim kernel pca exist ptas interest theoret approxim result automat follow ptas use construct way deriv practic algorithm take practic viewpoint consid anoth weaker approxim result howev effici algorithm design easili fact defin connect kernel matrix use kernel pca reduc data dimens vector project first principl compon diagon center kernel matrix lead orthogon matrix vn contain eigenvector diagon matrix diag contain correspond eigenvalu diagon assum now eigenvalu descend pp order data project first eigenvector x0i vji theorem embed path-bas distanc rk kernel pca enumer possibl voronoi partit yield o nk algorithm approxim path-bas cluster within constant factor proof solut k-mean cost function induc voronoi partit dataset if dimens data kept fix number differ voronoi partit o nkp enumer o nkp+1 time if embed dimens chosen k-mean rk 2-approxim algorithm k-mean combin result arriv 2-approxim algorithm run time o nk heurist without approxim guarante run time 2-approxim algorithm may still larg mani applic therefor refer two heurist optim method without approxim guarante instead enumer possibl voronoi partit one simpli partit data fast classic kmean algorithm one sweep assign object nearest centroid while keep object assign fix centroid reloc accord new assign sinc run time grow linear data dimens use first emb data dimens lead us function optim solut even worst case within factor two desir solut know approxim result reduc space k-mean heurist appli hope exist local minima low-dimension subspac second heurist one appli ward method agglom optim k-mean object function.1 start cluster contain one object step two cluster minim k-mean object function merg ward method produc cluster hierarchi applic method see figur experi first compar method classic singl linkag algorithm artifici data consist three noisi spiral see figur main concern experi robust nois data figur show dendrogram produc singl linkag leav tree object figur better visual tree structur bar diagram tree show label three cluster shown ward method optim heurist due equival km special case properti carri k-mean figur comparison cluster method mean shift cluster spectral cluster connect kernel cluster color imag http //www.inf.ethz.ch/ befische/nips03 figur hierarch cluster solut exampl singl linkag ward method connect kernel appli embed object dimens ward method kernel pca embed dimens solut drawn fig height inner node depict merg cost two subtre level hierarchi one cluster solut it obvious main part spiral arm found object drawn right side separ rest cluster respect object outlier separ in highest hierarch level algorithm conclud small singl linkag tendenc separ singl outlier object data way connect kernel we transform origin dyadic data dimension vectori data show compar result for connect kernel we appli ward method embed vector figur show cluster hierarchi for ward method in full space dimens oppos singl linkag result main structur spiral arm success found in hierarchi correspond three cluster solut below three cluster lever tree appear noisi it also notic cost three cluster solut much larger cost four cluster solut indic three cluster solut form distinct separ hierarch level figur demonstr distinct separ level found after appli kernel pca embed object low-dimension space dimens ward method then appli embed object one see coars struc ture tree preserv while cost cluster solut for shrunken toward zero we conclud pca effect de-nois hierarch tree lead robust agglom algorithm now we compar result recent publish cluster techniqu design extract elong structur mean shift cluster comput trajectori vector toward gradient under probabl densiti probabl distribut estim densiti estim kernel gaussian kernel trajectori start each point in featur space converg local maxima probabl distribut mean shift cluster applic finit dimension vector space it implicit involv densiti estim potenti shortcom mean-shift cluster follow if mode distribut multipl local maxima in spiral arm exampl there exist kernel bandwidth success separ data accord under structur in figur best result for mean shift cluster drawn for smaller valu spiral arm further subdivid addit cluster for larger bandwidth valu result becom similar compactness-bas criteria like k-mean spectral method becom quit popular in last year usual laplacian matrix base gaussian kernel comput by way pca data embed in low dimension space k-mean algorithm embed data then give result partit it also propos project data unit sphere appli k-mean spectral cluster gaussian kernel known abl separ nest circl we observ it sever problem extract noisi spiral arm see in spectral cluster kernel width free paramet select correct if larg spectral cluster becom similar standard k-mean fail extract elong structur if hand small the algorithm becom increas sensit outlier in the sens it the tendenc separ singl outlier object approach cluster the connect kernel howev could success extract the three spiral arm as seen in figur the reader notic method requir the user preselect kernel paramet figur exampl the usp dataset train exampl digit embed in two dimens ground truth label k-mean label cluster connect kernel in a last experi we show the advantag our method compar a parameter-fre compact criterion k-mean the problem cluster digit from the usp digit dataset figur show the cluster result our method use the connect kernel the digit gray-valu imag the usp dataset interpret as vector project the two lead principl compon in figur the ground truth solut drawn figur show the partit by direct appli k-mean cluster figur show the result produc by our method compar the ground truth solut path-bas cluster succeed in extract the elong structur result in a small error mislabel digit the compactness-bas kmean method the hand produc clear suboptim cluster with error rate conclus in this paper we present a cluster approach base path-bas distanc in the input graph in a first step elong structur transform compact one in the second step partit by the compactness-bas pairwis cluster method we show the transform distanc induc a mercer kernel in turn allow us deriv a polynomi time approxim scheme for the general p-hard pairwis cluster problem moreov mercer properti render it possibl emb the data low-dimension subspac by kernel pca these embed form the basi for effici 2-approxim algorithm also for de-nois the data robustifi fast agglom optim heurist compar to relat method like singl linkag mean shift cluster spectral cluster our method shown to success overcom the problem sensit to outlier object while capabl extract nest elong structur our method involv free kernel paramet we consid to a particular advantag mean shift spectral cluster
----------------------------------------------------------------

title: 3957-penalized-principal-component-regression-on-graphs-for-analysis-of-subnetworks.pdf

penal princip compon regress graph analysi subnetwork georg michailidi depart statist eec univers michigan ann arbor mi gmichail umich.edu ali shojai depart statist univers michigan ann arbor mi shojai umich.edu abstract network model wide use captur interact among compon complex system social biolog understand behavior often necessari analyz function relat compon system correspond to subsystem therefor analysi subnetwork may provid addit insight behavior system evid individu compon we propos novel approach incorpor avail network inform analysi arbitrari subnetwork propos method offer effici dimens reduct strategi use laplacian eigenmap neumann boundari condit provid flexibl infer framework analysi subnetwork base group-pen princip compon regress model graph asymptot properti propos infer method well choic tune paramet control fals posit rate discuss high dimension set perform propos methodolog illustr use simul real data exampl biolog introduct simultan analysi group system compon similar function subsystem recent receiv consider attent problem particular interest high dimension biolog applic chang individu compon may reveal under biolog phenomenon wherea combin effect function relat compon could improv effici interpret result this idea motiv the method gene set enrich analysi gsea along number relat method the main premis this method assess the signific set rather individu compon gene interact among preserv effici infer method develop a differ class model
----------------------------------------------------------------

title: 4878-understanding-dropout.pdf

understand dropout peter sadowski depart comput scienc univers california irvin irvin ca pjsadow ics.uci.edu pierr baldi depart comput scienc univers california irvin irvin ca pfbaldi uci.edu abstract dropout relat new algorithm train neural network reli stochast drop neuron train order avoid co-adapt featur detector introduc general formal studi dropout either unit connect arbitrari probabl valu use analyz averag regular properti dropout linear non-linear network deep neural network averag properti dropout character three recurs equat includ approxim expect normal weight geometr mean provid estim bound approxim corrobor result simul among result also show dropout perform stochast gradient descent regular error function introduct dropout algorithm train neural network describ nip simpl form train exampl present featur detector delet probabl remain weight train backpropag weight share across exampl present predict weight divid two main motiv behind algorithm prevent co-adapt featur detector overfit forc neuron robust reli popul behavior rather activ specif unit dropout report achiev state-of-the-art perform sever benchmark dataset also note singl logist unit dropout perform kind geometr averag ensembl possibl subnetwork conjectur someth similar may occur also multilay network lead view dropout may econom approxim train use larg ensembl network spite impress result report littl known dropout theoret standpoint particular averag regular converg properti likewis littl known import use whether differ valu use includ differ valu differ layer differ unit whether dropout appli connect rather unit address question dropout linear network instruct first look properti dropout linear network sinc studi exact general set multilay feedforward network describ under acycl graph activ unit layer express sih xx l h hl wij sj sj0 ij variabl denot weight input vector dropout appli unit express form sih xx l h hl wij sj sj0 ij jl gate bernoulli variabl plj throughout paper assum variabl jl independ independ weight independ activ unit similar dropout appli connect lead random variabl sih xx l h hl hl wij sj sj0 ij breviti rest paper focus exclus dropout appli unit result remain true case dropout appli connect minor adjust fix input vector expect activ unit taken possibl realize gate variabl henc possibl subnetwork given e sih xx l h hl wij pj e sjl e sj0 ij input layer short ensembl averag easili comput hl hl feedforward propag origin network simpli replac weight wij wij pj dropout neural network dropout shallow neural network pn consid first singl logist unit input ce wj ij achiev greatest level general assum unit produc differ output op om correspond differ sum s1 sm differ probabl p1 pm pm relev case output sum associ 2n possibl subnetwork unit probabl p1 pm could generat instanc use bernoulli gate variabl although isp necessari deriv use defin follow four quantiti mean pi oi mean complement e0 pi oi weight geometr mean gm oipi weight geometr mean complement g0 oi pi also defin normal weight geometr mean gm g0 prove key averag theorem logist function pi pi pi gm om prove result write gm om oi logist function satisfi ident ce thus gm om pi si ce ce thus case bernoulli gate variabl comput wp gm possibl dropout configur simpl forward propag gm wj pj ij similar result true also normal exponenti transfer function final one also show class function satisfi gm constant function logist function dropout deep neural network deal interest case deep feedforward network sigmoid unit describ set equat form oih sih xx hl wij oj oj0 ij l h oih output unit layer dropout unit describ oih sih xx l h hl wij oj oj0 ij use bernoulli selector variabl jl sigmoid unit gm oih oi oi oih rang possibl subnetwork assum gm provid good approxim expect point analyz next section averag properti dropout describ follow three recurs equat first approxim mean nwgms e oih gm oih second use result previous section propag expect symbol gm oih ih e sih third use linear expect respect sum product independ random variabl e sih xx l h hl wij pj e ojl equat fundament equat explain averag properti dropout procedur approxim cours equat analyz next section network contain linear unit equat necessari unit averag comput exact case regress linear unit top layer allow one shave one layer approxim true binari classif requir output layer comput direct gm ensembl rather expect shown error function convex error mean weight geometr mean normal weight geometr mean ensembl alway less expect error model equat exact number oih ident possibl subnetwork thus use measur consist c oi neuron layer input use varianc ar oih taken subnetwork distribut input fix larger varianc less consist neuron wors expect approxim equat note random variabl varianc exceed anyway ar o measur also averag train set test set given result previous section network also includ linear unit normal exponenti unit dropout approxim given set number o1 om probabl p1 pm correspond output sigmoid neuron fix input differ subnetwork primarili interest approxim gm gm provid good approxim show first order approxim gm g. furthermor formula literatur bound error term consist cartwright field inequ howev one suspect gm provid even better approxim geometr mean instanc if number oi satisfi oi consist low then therefor g0 g0 proven appli jensen inequ function also known ky fan inequ get even better result one must consid second order approxim write oi thus ar o use taylor expans pi pi pi pi 4pi pj r3 i j r3 remaind r3 pi ui ui expand product give pi pi symmetri g0 oi pi ar o r3 r3 higher order remaind neglect remaind write ar o e v g0 g+g 2v g+g 2v thus second order differ mean geometr mean normal geometr mean satisfi e g v g+g 2v g0 g+g 2v final easi check factor 2v alway less equal addit we alway equal achiev bernoulli variabl thus g0 g0 2v 2v first inequ optim sens attain case bernoulli variabl expect intuit second inequ show approxim error alway small regardless whether close short nwgm provid good approxim better geometr mean g. properti alway true second order approxim it exact activ consist low gm sinc latter impli gm e. sever addit properti dropout approxim includ extens rectifi linear unit transfer function studi dropout dynam dropout perform gradient descent on-lin respect train exampl ensembl possibl subnetwork appropri decreas learn rate it almost sure converg like form stochast gradient descent understand properti dropout it instruct look properti gradient linear case singl linear unit case singl linear unit consid two error function een ed associ ensembl possibl subnetwork network dropout singl input i defin een oen pi ii od ii ed we use singl train input i notat simplic otherwis error train exampl combin addit learn gradient given een oen oen oen pi ii ed od od od ii ii ii2 wj ii ij dropout gradient random variabl we take expect short calcul yield ed een een pi pi ii2 ar i thus remark this case expect gradient dropout gradient regular ensembl error een 1x i ar i regular term usual weight decay gaussian prior term base squar weight prevent overfit dropout provid immedi magnitud regular term adapt scale input varianc dropout variabl note pi valu provid highest level regular singl sigmoid unit previous result general sigmoid unit ce s train minim relat entropi error log log o this case ed ii term ii independ use taylor expans gm approxim give een ed sen ii2 ar i sen wj pj ij thus as linear case expect dropout gradient approxim gradient ensembl network regular weight decay term proper adapt coeffici similar analysi carri also set normal exponenti unit deeper network learn phase spars code dropout learn we expect three learn phase at begin learn weight typic small random total input unit close for all unit consist high output unit remain rough constant across subnetwork equal as learn progress activ tend move toward consist decreas for given input varianc unit across subnetwork increas as stochast gradient learn procedur converg consist unit converg stabl valu final for simplic assum dropout appli layer unit output hl form oih sih sih l h wij oj for fix input ojl constant sinc dropout appli layer thus ar sih hl wij ojl plj plj l h usual assumpt selector variabl jl independ thus ar sih depend three factor everyth els equal it reduc small weight goe togeth regular effect dropout small activ show dropout symmetr respect small larg activ overal dropout tend favor small activ thus spars code small close larg close valu dropout probabl plj thus valu plj maxim regular effect may also lead slower converg consist state addit result simul given simul result we use mont carlo simul to partial investig approxim framework embodi by three fundament dropout equat accuraci second-ord approxim bound equat dynam dropout learn we experi mnist classifi four hidden layer replic result use pylearn2 theano softwar librari network train dropout probabl input four hidden layer for fix weight fix input mont carlo simul use to estim distribut activ neuron let activ determinist set weight scale appropri left column figur confirm empir second-ord approxim equat bound equat accur right column figur show differ the true ensembl averag the prediction-tim neuron activ this differ grow slowli in the higher layer for activ neuron figur left the differ gm it s second-ord approxim in equat the bound equat plot for four hidden layer typic fix input right the differ the true ensembl averag the final neuron predict next we examin the neuron consist dure dropout train figur 2a show the three phase learn for typic neuron in figur we observ the consist declin in higher layer the network one clue this happen the distribut neuron activ as note in section dropout train result in spars activ in the hidden layer figur this increas the consist neuron in the next layer the three phase learn for particular input typic activ neuron red start low varianc experi larg increas in varianc dure learn eventu settl to steadi constant valu in contrast a typic inact neuron blue quick learn to stay silent shown the mean percentil consist notic declin in the upper layer shown here the mean std o for activ neuron train in layer along the percentil figur figur in everi hidden layer a dropout train network the distribut neuron activ spars and symmetr these histogram total a set random input
----------------------------------------------------------------

title: 5059-compete-to-compute.pdf

compet comput rupesh kumar srivastava jonathan masci sohrob kazerounian faustino gomez j rgen schmidhub idsia usi-supsi manno lugano switzerland rupesh jonathan sohrob tino juergen idsia.ch abstract local competit among neighbor neuron common biolog neural network paper appli concept gradient-bas backprop-train artifici multilay nns nns compet linear unit tend outperform non-compet nonlinear unit avoid catastroph forget train set chang time introduct although often use machin learn method consid natur arriv particular solut perhap instruct first understand function role biolog constraint inde artifici neural network repres state-of-the-art mani pattern recognit task resembl brain superfici sens also draw mani comput function properti one long-studi properti biolog neural circuit yet fulli impact machin learn communiti natur local competit common find across brain region neuron exhibit on-cent off-surround organ organ argu give rise number interest properti across network neuron winner-take-al dynam automat gain control nois suppress paper propos biolog inspir mechan artifici neural network base local competit ultim reli local winner-take-al lwta behavior demonstr benefit lwta across number differ network pattern recognit task show lwta enabl perform compar state-of-the-art moreov help prevent catastroph forget common artifici neural network first train particular task abrupt train new task properti desir continu learn wherein learn regim clear delin experi also show evid type modular emerg lwta network train supervis set differ modul subnetwork respond differ input benefici learn multimod data distribut compar learn monolith model follow first discuss relev neurosci background motiv local competit show incorpor artifici neural network lwta implement compar altern method show lwta network perform varieti task help buffer catastroph forget neurosci background competit interact neuron neural circuit long play import role biolog model brain process larg due earli studi show mani cortic sub-cort hippocamp cerebellar region brain exhibit recurr on-cent off-surround anatomi cell provid excitatori feedback nearbi cell scatter inhibitori signal broader rang biolog model sinc tri uncov function properti sort organ role behavior success anim earliest model describ emerg winner-take-al wta behavior local competit base grossberg shunt short-term memori equat show center-surround structur enabl wta dynam also contrast enhanc normal analysi dynam show network slower-than-linear signal function uniform input pattern linear signal function preserv normal input pattern faster-than-linear signal function enabl wta dynam sigmoid signal function contain slower-than-linear linear faster-than-linear region enabl supress nois input pattern contrast-enhanc normal store relev portion input pattern form soft wta function properti competit interact studi show among thing effect distance-depend kernel inhibitori time lag develop self-organ map role wta network attent biolog model also extend show competit interact spike neural network give rise soft wta dynam well may effici construct vlsi although competit interact wta dynam studi extens biolog literatur recent consid comput machin learn perspect exampl maa show feedforward neural network wta dynam non-linear comput power network threshold sigmoid gate network employ soft wta competit univers function approxim moreov result hold even network weight strict posit find ramif understand biolog neural circuit well develop neural network pattern recognit larg bodi evid support advantag local competit interact make noteworthi simpl mechan provok studi machin learn communiti nonetheless network employ local competit exist sinc late along serv primari inspir present work recent maxout network leverag local competit interact combin techniqu known dropout obtain best result certain benchmark problem network local winner-take-al block section describ general network architectur local compet neuron network consist block organ layer figur block bi contain comput unit neuron produc output vector determin local interact individu neuron activ block yij g h1i h2i hni competition/interact function encod effect local interact block hji activ j-th neuron block comput hi wij input vector neuron previous layer wij weight vector neuron block general non-linear activ function output activ pass input next layer paper use winner-take-al interact function inspir studi comput neurosci particular use hard winner-take-al function hi hji hki yij otherwis case multipl winner tie broken index preced order investig capabl hard winner-take-al interact function isol figur local winner-take-al lwta network block size two show win neuron block shade given input exampl activ flow forward win neuron error backpropag activ neuron grey connect propag activ activ neuron form subnetwork full network chang depend input ident use activ function equat differ local winner take lwta network standard multilay perceptron non-linear activ function use forward propag input local competit neuron block turn activ neuron except one highest activ train error signal backpropag win neuron lwta layer mani neuron block activ one time given input pattern1 denot layer block size lwta-n input pattern present network subgraph full network activ highlight neuron synaps figur train dataset consist simultan train exponenti number model share paramet well learn model activ pattern unlik network sigmoid unit free paramet need set proper input pattern subset use given input pattern come differ sub-distribut potenti model effici special modular properti similar network rectifi linear unit relu recent shown good sever learn task link relu discuss section comparison relat method max-pool neural network max-pool layer found use especi imag classif task achiev state-of-the-art perform layer usual use convolut neural network subsampl represent obtain convolv input learn filter divid represent pool select maximum one max-pool lower comput burden reduc number connect subsequ convolut layer add translational/rot invari howev alway possibl win neuron block activ exact zero block output max-pool lwta figur max-pool lwta max-pool group neuron layer singl set output weight transmit win unit activ case next layer layer activ subsampl lwta block subsampl activ flow subsequ unit via differ set connect depend win unit first glanc max-pool seem similar wta oper howev two differ substanti downsampl wta oper thus number featur reduc instead represent sparsifi figur dropout dropout interpret model-averag techniqu joint train sever model share subset paramet input dimens data augment appli input layer achiev probabilist omit drop unit network exampl train neuron particip forward/backward propag consid hypothet train lwta network block size two select winner block random similar train neural network dropout probabl nonetheless two fundament differ dropout regular techniqu lwta interact neuron block replac per-neuron non-linear activ dropout believ improv general perform sinc forc unit learn independ featur without reli unit activ dure test propag input network unit layer train dropout use output weight suitabl scale lwta network output scale requir fraction unit inact input pattern depend total input view way wta restrict fraction paramet util input pattern howev hypothes freedom use differ subset paramet differ input allow architectur learn multimod data distribut accur rectifi linear unit rectifi linear unit relu simpli linear neuron clamp negat activ zero otherwis relu network shown use restrict boltzmann machin outperform sigmoid activ function deep neural network use obtain best result sever benchmark problem across multipl domain consid lwta block two neuron compar two relu neuron weight sum input neuron tabl show output y1 y2 combin posit negat relu lwta neuron relu lwta neuron pass output half possibl case differ lwta neuron never activ inact time activ error flow exact one neuron block relu neuron inact satur potenti drawback sinc neuron tabl comparison rectifi linear activ lwta-2 posit posit negat posit negat negat posit negat negat posit posit negat relu neuron y1 y2 lwta neuron y1 y2 get activ get train lead wast capac howev previous work suggest negat impact optim lead hypothesi hard satur help credit assign long error flow certain path optim affect advers continu research along line valid hypothesi expect possibl train relu network better mani argument relu network appli lwta network notabl differ dure train lwta network inact neuron becom activ due train neuron block suggest lwta net may less sensit weight initi greater portion network capac may util experi follow experi lwta network test various supervis learn dataset demonstr abil learn use intern represent without util non-linear order clear assess util local competit special strategi augment data transform nois dropout use also encourag spars represent hidden layer ad activ penalti object function common techniqu also relu unit thus object evalu valu use lwta rather achiev absolut best test score block size two use experiments.2 network train use stochast gradient descent mini-batch learn rate lt momentum mt epoch given min otherwis min tt mf mt pf learn rate anneal factor min lower learn rate limit momentum scale mi mf epoch remain constant mf weight decay use convolut network section max-norm normal experi setup similar permut invari mnist mnist handwritten digit recognit task consist imag train test digit center center mass permut invari set task we attempt classifi digit without util 2d structur imag everi digit vector pixel last exampl train set use hyperparamet tune model best hyperparamet set train converg full train set mini-batch size speed experi gnumpi cudamat librari use tabl test set error permut invari mnist dataset method without data augment unsupervis pre-train activ sigmoid relu relu dropout hidden layer lwta-2 test error tabl test set error mnist dataset convolut architectur data augment result mark asterisk use layer-wis unsupervis featur learn pre-train network global fine tune architectur 2-layer cnn layer mlp 2-layer relu cnn layer lwta-2 3-layer relu cnn 2-layer cnn layer mlp 3-layer relu cnn stochast pool 3-layer maxout dropout test error use pixel valu rescal preprocess best model obtain gave test set error consist three lwta layer block follow softmax layer knowledg best report error without util implicit/explicit model averag this set use deformations/nois enhanc dataset unsupervis pretrain tabl compar result method use unsupervis pre-train perform lwta compar relu network dropout hidden layer use dropout input layer well lower error rate use relu use maxout obtain convolut network mnist this experi convolut network cnn use consist filter first layer follow second layer map respect relu activ everi convolut layer follow max-pool oper we use two lwta-2 layer block final softmax output layer weight decay found benefici improv general result summar tabl along state-of-the-art approach use data augment detail convolut architectur see amazon sentiment analysi lwta network test amazon sentiment analysi dataset sinc relu unit shown perform well this domain we use balanc subset dataset consist review four categori product book dvds electron kitchen applianc task classifi review posit negat dataset consist posit negat review categori text review convert binari featur vector encod presenc absenc unigram bigram follow frequent vocabulari entri retain featur classif we divid data equal balanc fold test our network cross-valid report mean test error fold relu activ use this dataset context unsupervis learn denois autoencod obtain spars featur represent use classif we train lwta-2 network three layer block supervis set direct classifi review posit negat use 2-way softmax output layer we obtain mean accuraci book dvds electron kitchen give mean accuraci compar report denois autoencod use relu unsupervis pre-train find good initi tabl lwta network outperform sigmoid relu activ rememb dataset p1 train dataset p2 test error p1 train p1 train p2 lwta sigmoid relu implicit long term memori this section examin effect lwta architectur catastroph forget fact network implement multipl model allow retain inform dataset even train differ dataset test this implicit long term memori mnist train test set divid two part p1 contain digit p2 consist remain digit three differ network architectur compar three lwta layer block size three layer sigmoid neuron three layer relu neuron network 5-way softmax output layer repres probabl exampl belong five class all network initi paramet train fix learn rate momentum network first train reach log-likelihood error p1 train set this valu chosen heurist produc low test set error reason time all three network type weight output layer correspond softmax classifi store network train start new initi random output layer weight reach log-likelihood valu p2 final output layer weight save p1 restor network evalu p1 test set experi repeat differ initi tabl show lwta network rememb learn p1 much better sigmoid relu network though notabl sigmoid network perform much wors lwta relu while test error valu depend learn rate momentum use lwta network tend rememb better relu network factor two case sigmoid network alway perform much wors although standard network architectur known suffer catastroph forget we show first time relu network actual quit good this regard moreov outperform lwta we expect this behavior manifest competit model general becom pronounc increas complex dataset neuron encod specif featur in one dataset affect much dure train anoth dataset wherea neuron encod common featur reus thus lwta may step forward toward model forget easili analysi subnetwork network a singl lwta-m block consist mn subnetwork select train individu exampl while train a dataset train we expect subnetwork consist activ neuron exampl class neuron in common compar subnetwork activ differ class in case relat simpl dataset like mnist possibl examin number common neuron mean subnetwork use class this neuron activ in the layer exampl in a subset exampl record for class the subnetwork consist neuron activ for least the exampl design the repres mean subnetwork compar all class subnetwork count the number neuron in common figur 3a show the fraction neuron in common the mean subnetwork pair digit digit morpholog similar subnetwork neuron in common the subnetwork for digit intuit less similar verifi this subnetwork special a result train we look at the fraction common neuron all pair digit for the untrain train digit fraction neuron in common digit mnist digit pair figur each entri in the matrix denot the fraction neuron a pair mnist digit in common averag in the subnetwork that activ for each the two digit class the fraction neuron in common in the subnetwork each the possibl digit pair after train exampl after train figur clear the subnetwork much more similar prior train the full network learn partit paramet to reflect the structur the data conclus futur research direct our lwta network automat self-modular multipl parameter-shar subnetwork respond to differ input represent without signific degrad state-of-the-art result digit recognit sentiment analysi lwta network also avoid catastroph forget thus retain use represent one set input even after train to classifi anoth this implic for continu learn agent that forget represent part environ expos to part we hope to explor mani promis applic these idea in the futur acknowledg this research fund eu project way neuraldynam nascenc addit fund arcelormitt
----------------------------------------------------------------

title: 298-language-induction-by-phase-transition-in-dynamical-recognizers.pdf

languag induct phase transit dynam recogn jordan b. pollack laboratori ai research ohio state univers columbus oh pollack cis.ohio-state.edu abstract higher order recurr neural network architectur learn recogn generat languag train categor exemplar studi network perspect dynam system yield two interest discoveri first longitudin examin learn process illustr new form mechan infer induct phase transit small weight adjust caus bifurc limit behavior network phase transit correspond onset network 's capac general arbitrary-length string second studi automata result acquisit previous publish languag indic architectur guarante find minim finit automata consist given exemplar np-hard problem architectur appear capabl generat nonregular languag exploit fractal chaotic dynam i end paper hypothesi relat linguist generat capac behavior regim non-linear dynam system introduct i expos recurr high-ord back-propag network posit negat exampl boolean string report although network find minimal-descript finit state automata languag np-hard angluin induct novel interest fashion search hypothesi space theoret constrain machin finit state result import mani relat neural model current develop elman gile servan-schreib relat ultim question linguist capac aris natur although transit among state finite-st automata usual thought fulli specifi tabl transit function also specifi mathemat function current state input known mcculloch pitt even elementari model assumpt yield finite-st pollack control worth reiter network capac comput arbitrari boolean function say logic sum product laped farber net white homik use recurr implement arbitrari finit state machin differ point view recurr network state evolv across unit consid k-dimension discrete-tim continuous-spac dynam tystem precis initi condit state space z. subspac govern function f. parameter set weight w. mere comput next state current state input finit sequenc pattern repres token alphabet zk fw zk .yjct view one dimens system say za accept dimens defin languag accept dynam recogn string input token evolv precis initi state accept dimens state certain threshold network term one output unit would subject threshold test process sequenc input pattern first question ask dynam system construct taught accept particular languag weight network individu correspond direct graph transit phrase structur rule second question ask sort generat power achiev system model begin answer question learn i present elabor upon earlier work cascad network pollack use recurr fashion learn pariti depth-limit parenthesi balanc map word sequenc proposit represent pollack cascad network wellcontrol higher-ord connectionist architectur back-propag techniqu weight adjust rumelhart appli basic consist two subnetwork function network standard feed-forward network without hidden layer howev weight dynam comput linear context network whose output map fashion weight function net thus input pattern context network use multiplex function comput result simpler learn task output function network use input context network system built learn produc specif output variable-length sequenc input multipl connect input effect process differ function given initi context sequenc input network comput sequenc state vector dynam chang set weight wij without hidden unit forward pass comput wij wijk zi gel wij yj languag induct phase ll'ansit dynam recogn usual sigmoid function use back-propag system previous work i assum teacher could suppli consist generaliz desired-st member larg set string signific overconstraint learn two-stat machin like pariti matter i-bit state fulli determin output howev case higher-dimension system know final output system n't care state along way jordan show recurr back-propag network could train n't care condit specif prefer valu output unit particular train exampl simpli consid error term unit work long unit receiv feedback exampl when don't-car line weight unit never chang solut problem involv backspac unrol loop propag error determin subset weight accept unit za ae da za n za n yj n ae error remaind weight aae w k penuiorn orn step ae azk n-l calcul use valu ae awajk awa/n ae ae awij n-l azi n-l yj n-l ae ae awijk awij n-l done batch epoch style set exampl vari length induct phase transit initi studi learn simpl regular languag odd pariti i expect recogn mere implement exclus feedback link turn quit enough termin back-propag usual defin error logic recurr use logic tend limit point word mere separ exemplar guarante network recogn pariti limit nevertheless inde possibl illustr illustr order test limit behavior recogn observ respons long characterist string odd pariti string requir altern respons small cascad network compos function net context net pollack requir weight train odd pariti small set string length epoch weight network save file subsequ configur test respons first characterist string figur i vertic column correspond epoch contain point initi string longer length distinguish cycl network improv separ finit string at cycl network undergo bifurc small chang weight singl epoch lead phase transit limit point limit cycl phase transit adapt classif task network rapid exploit il iilu hli iii ili i figur bifurc diagram show respons parity-learn first characterist string epoch train i wish stress this new interest form mechan induct reveal proper perspect non-linear connectionist network capabl much complex behavior hill-climb befor phase transit machin principl capabl perform serial pariti task phase transit similar learn flash insight biolog chang punctuat evolut much coincid benchmark result tomita perform eleg experi induc finit automata posit negat evid use hillclim bing space 9-state automata case defin two set boolean string accept reject regular languag simpl low dimension dynam system usual studi knob cootrol paramet bifurc diagram scalar variabl control paramet entir vcc tor weight network bade-propag turn knobl languag induct phase ltansit dynam recogn list odd zero string after odd string tripl zero pairwis even sum lo number number ofo 's 3n rather invent train data sampl languag wellform train set i ran tomita train environ given sequenti cascad network i-input 4-output function network bias weight set 3-input 8-output context network bias use learn rate momentum termin when accept string return output bit reject string tomita 's case case converg without problem sever hundr epoch case would converg kept treat negat case correct difficulti architectur induc trap state i modifi train set ad reject string order overcom this problem case took sever restart thousand cycl converg caus unknown complet experiment data avail longer report pollack becaus state box low dimension,3 view these machin graphic gain understand state space arrang base upon intiti studi pariti initi hypothesi set cluster would found organ geometr fashion em bed finit state machin finit dimension geometri token 's transit would correspond simpl transform space graph state visit possibl input length tomita test case shown figur each figur contain point often overlap imag expect clump point close map state equival fsa 's imag limit ravin 's each consid state well discuss howev state space dynam recogn tomita case interest becaus theoret infinit state machin state arbitrari random requir infinit tabl transit constrain power way mathemat principl word complex dynam not in specif weight in think principl consid system in extrem observ complex emerg algorithm simplic plus comput power it it argu fsa induc method get around this problem presuppos rather learn trap state one reason i succeed in low dimension induct becaus architectur meali rather moor machin lee gile person communic pollack a figur imag state-spac for benchmark case each imag contain point correspond state boolean string length languag induct phase 1ransit in dynam recogn interest note elimin sigmoid commut yj zk term forward equat for higher order recurr network ident generat iter function system if bamsley thus my figur statespac emerg from project class mathemat object barnsley 's fractal attractor wide reproduc fern use method grassberg procaccia correl dimens attractor in figur found link work in complex dynam system neural network wellestablish the neurobiolog level skarda freeman the mathemat level derrida meir huberman hogg kurten smolenski this paper expand a theme from earlier propos to link at the cognit level pollack there interest formal question brought in the work wolfram other the univers cellular automata recent in the work crutchfield young the descript complex bifurc system what the relationship complex dynam neural system tradit measur comput complex from this work support evid i ventur the follow hypothesi attractor the state-spac limit a dynam recogn cut a threshold similar decis function the complex the languag recogn regular the cut fall disjoint limit point cycl context-fre if it cut a self-similar recurs region context-sensit if it cut a chaotic pseudo-random region acknowledg this research partial support the offic of naval research grant nooo
----------------------------------------------------------------

title: 162-mapping-classifier-systems-into-neural-networks.pdf

map classifi system neural network lawrenc davi bbn laboratori bbn system technolog corpor moulton street cambridg ma januari abstract classifi system machin learn system incotpor genet algorithm learn mechan although respond input neural network respond intern structur represent fonnal learn mechan differ marlc employ neural network research sort domain result one might conclud two type machin learn fonnal intrins differ one two paper taken togeth prove instead classifi system neural network equival paper half equival demonstr descript transfonn procedur map classifi system neural network isomotph behavior sever alter commonly-us paradigm employ neural networlc research requir order make transfonn worlc alter note appropri discuss paper conclud discuss practic import result comment extens introd uction classifi system machin learn system develop sinc holland recent member genet algorithm research communiti well classifi system varieti genet algorithm algorithm optim learn genet algorithm employ techniqu inspir process biolog evolut order evolv better better ithi paper benefit discuss wayn mesard rich sutton ron william stewart wilson craig shaefer david montana gil syswerda member bargain boston area research group genet algorithm induct network davi individu taken solut problem optim function travers maze etc explan genet algorithm reader refer goldberg classifi system receiv messag extern sourc input organ use genet algorithm learn produc respons intern use interact extern sourc paper one two paper explor question fonnal relationship classifi system neural network normal employ two sort algorithm probabl distinct although procedur translat oper neural network isomorph classifi system given belew gherriti techniqu belew gherriti use includ convers neural network learn procedur classifi system framework appear techniqu support convers thus one might conjectur two sort machin learn system employ learn techniqu reconcil although subsumpt relationship belew gherriti result suggest set classifi system might superset set neural network revers conclus suggest consider input sort learn algorithm process view black box mechan learn receiv input carri self-modifi procedur produc output class input tradit process classifi system class bit string fix length subset class input tradit process neural network thus appear classifi system oper subset input neural network process view mechan modifi behavior fact impress correct one translat classifi system neural network preserv learn behavior one translat neural network classifi system preserv learn behavior order howev special sort algorithm must made paper deal translat classifi system neural network special neural network requir order translat take place revers translat use quit differ techniqu treat davi follow section contain descript classifi system descript transform oper discuss extens proof comment issu rais cours proof conclus classifi system classifi system oper context environ send messag system provid reinforc base behavior display classifi system two compon messag list popul rule-lik entiti call classifi messag messag list compos bit map classifi system neural network pointer sourc messag may generat environ classifi classifi popul classifi three compon match string made charact n't care messag made charact strength top-level descript classifi system contain popul product rule attempt match condit messag list thus classifi input post messag messag list thus potenti affect envirorun classifi reinforc environ use classifi system modifi strength classifi period genet algorithm invok creat new classifi replac certain member classifi set explan classifi system potenti machin learn system formal properti reader refer holland al let us specifi process stage precis classifi system oper cycl fix list procedur order procedur messag list process clear messag list post envirorunent messag messag list post messag messag list classifi post set previous cycl implement envirorunent reinforc analyz messag messag list alter strength classifi post set previous cycl form bid set determin classifi match messag messag list classifi match messag bit match field match correspond messag bit match match i match either bit set match classifi form current bid set implement bid tax subtract portion strength classifi bid set add strength taken strength classifi classifi post messag match prior step form post set bid set larger maximum post set size choos classifi stochast post bid set weight proport magnitud bid tax set classifi chosen post set reproduct reproduct general occur everi cycl occur step carri creat children parent use crossov and/or mutat choos parent stochast favor strongest one crossov mutat two oper use genet algorithm set strength child equal averag strength child parent note one mani way set strength new classifi transform work analog way remov member classifi popul add new children classifi popul map classifi classifi network map oper i shall describ map classifi classifi network classifi network link environment input unit link davi classifi network match post messag unit weight link lead match node leav post node relat field match messag list classifi addit link ad provid bias term match node note assum environ post one messag per cycl modif transfonn oper accommod multipl environment messag describ final comment paper given classifi system cs classifi match send messag length construct isomorph neural network compos classifi network follow way classifi cs construct correspond classifi network compos match node i post node messag node one match node environment match node link input environ match node link messag post node anoth classifi network reader refer figur exampl transform match node classifi network incom link weight first link deriv appli follow transform element c 's match field associ weight associ weight associ weight weight final link set number link weight thus classifi match field would associ network weight link lead match node classifi match field would weight weight link messag node classifi network set equal correspond element classifi 's messag field thus messag field classifi weight link lead three messag node correspond classifi network would i weight link classifi network set node classifi network use threshold function determin activ level match node threshold node threshold node 's threshold exceed node 's activ level set if set classifi network associ quantiti call strength may alter network run process cycl describ cycl process classifi system cs map onto follow cycl process set classifi network messag list process comput activ level messag node cs if environ suppli reinforc cycl divid reinforc number post node current activ plus if environ post messag preced cycl add quotient strength activ post node 's classifi network if messag cycl environ map onto first environ node node associ node associ tum final environment node if environment messag turn environment map classifi system into neural network node form bid set comput activ level match node classifi network comput activ level bid node classifi network set classifi network activ bid node bid set subtract fix proport strength classifi network cn bid set add amount strength network connect activ match node cn strength given environ pass system form post set if bid set larger maximum post set size choos network stochast post bid set weight proport magnitud bid tax set network chosen post set might view stochast n-winners-take-al procedur reproduct if cycl reproduct would occur classifi system carri analog neural network follow way creat children parent use crossov and/or mutat choos parent stochast favor strongest one ternari alphabet compos i use instead classifi alphabet after oper appli final member match list set write weight match link messag link classifi network match weight children choos network re-weight stochast weakest one like chosen set strength re-weight classifi network averag strength parent simpl show classifi network match node match messag case associ classifi match messag three case consid if origin match charact match messag bit correspond link weight set state node come affect activ match node goe if origin match charact messag bit messag match correspond link weight set see inspect weight final link match node threshold fact type link posit weight everi link weight i must connect activ node match node activ final link weight correspond set if link connect node activ effect turn node connect link weight seen this caus match node inact given this correspond match behavior one verifi set classifi network associ classifi system follow properti dure cycl process classifi system classifi bid set case associ networlc activ bid node assum system use random techniqu initi way classifi post set case when network post set final parent chosen for reproduct transform chosen classifi system children produc transform classifi system parent two system isomorph oper assum use random number generat davi classifi network strength classifi network strength messag node th post node th match node th environ input node figur result map classifi system two classifi into neural network classifi match field messag field strength classifi match field messag field strength map classifi system into neural network conclud comment transfonn procedur describ map classifi system into neural network oper way sever point rais techniqu use accomplish map close let us consid four first there excess complex classifi network shown fact one could elimin non-environment match node link sinc one determin whenev classifi network reweigh ted whether match messag classifi network system if one could introduc link direct post node classifi networlc post node new networlc match node environ necessari long one predict messag environ post messag node necessari long messag must sent environ if incom link could elimin well simplif introduc extens discuss next requir complex current architectur second genet algorithm side classifi system consid extrem simpl one there mani extens refin use classifi system research i believ refin handl expand map procedur modif architectur classifi network give indic way modif would go let us consid two sampl case first case environ may produc multipl messag cycl handl multipl messag addit link must ad each environment match node weight set match node 's threshold this link latch match node addit match node with link environ node must ad a latch count node must attach given these two architectur modif cycl modifi follow dure messag match cycl a seri subcycl carri one for each messag post environ each subcycl environment messag input each environment match node comput activ environment match node latch each activ if match environment messag count node record mani match each classifi network when bid strength'i paid a classifi network poster messag it match the divisor the number environment messag match record the count node plus the number messag match final when new weight written onto a classifi network 's link written onto the match node connect the count node as well a second sort complic pass-through bit bit pass a messag match the messag post this sort mechan implement obvious fashion complic the structur the classifi networlc similar complic produc consid multiple-messag match negat messag effector forth it an open question whether all case handl modifi the architectur the map oper i yet found one handl davi third the classifi network use the sigmoid activ function support hill-c~b techniqu as back-propag further recurr network rather strict feed-forwanl network thus one might wonder whether the fact one carri transform affect the behavior research in the field this point one taken greater length in the companion paper conclus there sever the techniqu import into the neural network domain the map appear improv the perform neural network these includ track strength in order guid the learn process use genet oper modifi the network makeup use population-level measur in order determin aspect a network use in reproduct the reader refer montana davi for an exampl the benefit gain employ these techniqu final one might wonder the import this proof intend to in my view this proof the companion proof suggest excit way in one hybrid the learn techniqu each field one approach success applic to a real-world problem character in montana davi
----------------------------------------------------------------

title: 3714-a-game-theoretic-approach-to-hypergraph-clustering.pdf

game-theoret approach hypergraph cluster samuel rota bul`o marcello pelillo univers venic itali srotabul pelillo dsi.unive.it abstract hypergraph cluster refer process extract maxim coher group set object use high-ord rather pairwis similar tradit approach problem base idea partit input data user-defin number class therebi obtain cluster by-product partit process paper provid radic differ perspect problem contrast classic approach attempt provid meaning formal notion cluster show game theori offer attract unexplor perspect serv well purpos specif show hypergraph cluster problem natur cast non-coop multi-play cluster game wherebi notion cluster equival classic game-theoret equilibrium concept comput viewpoint show problem find equilibria cluster game equival local optim polynomi function standard simplex provid discrete-tim dynam perform optim experi present show superior approach state-of-the-art hypergraph cluster techniqu introduct cluster problem organ set object group cluster way similar object group togeth dissimilar one assign differ group accord similar measur unfortun univers accept formal definit notion cluster general agre inform cluster correspond set object satisfi two condit intern coher condit ask object belong cluster high mutual similar extern incoher condit state overal cluster intern coher decreas ad extern object object similar typic express pairwis relat applic higherord relat appropri approxim term pairwis interact lead substanti loss inform consid instanc problem cluster given set d-dimension euclidean point line everi pair data point trivial defin line exist meaning pairwis measur similar problem howev make perfect sens defin similar measur triplet point indic close collinear clear exampl general problem model-bas point pattern cluster deviat set point model provid measur dissimilar problem cluster object use high-ord similar usual refer hypergraph cluster problem machin learn communiti increas interest around problem zien co-author propos two approach call cliqu expans star expans respect approach transform similar hypergraph edge-weight graph whose edge-weight function hypergraph origin weight way abl tackl problem standard pairwis cluster algorithm bolla defin laplacian matrix unweight hypergraph establish link spectral properti matrix hypergraph minimum cut rodr` guez achiev similar result transform hypergraph graph accord cliqu expans show relationship spectral properti laplacian result matrix cost minimum partit hypergraph zhou co-author general earlier work regular graph defin hypergraph normal cut criterion k-partit vertic achiev find second smallest eigenvector normal laplacian approach general well-known normal cut pairwis cluster algorithm final find anoth work base idea appli spectral graph partit algorithm edge-weight graph approxim origin edge-weight hypergraph worth note approach mention devis deal higher-ord relat reduc standard pairwis cluster approach differ formul introduc cluster problem higher-ord super-symmetr similar cast nonneg factor closest hyper-stochast version input affin tensor afore-ment approach hypergraph cluster partition-bas inde cluster model sought direct obtain by-product partit input data fix number class render approach vulner applic number class known advanc data affect clutter element belong cluster figure/ground separ problem addit partit cluster necessarili disjoint set although it mani case natur overlap cluster two intersect line point intersect belong line this paper follow offer radic differ perspect hypergraph cluster problem instead insist idea determin partit input data henc obtain cluster by-product partit process revers term problem attempt instead deriv rigor formul notion cluster this allow one principl deal general problem cluster may overlap and/or outlier may get unassign found game theori offer eleg general mathemat framework serv well purpos basic idea behind approach hypergraph cluster problem consid multi-play non-coop cluster game within this context notion cluster turn equival classic equilibrium concept evolutionari game theori latter reflect both intern extern cluster condit allud also show exist correspond equilibria local solut polynomi linearly-constrain optim problem provid algorithm find experi two standard hypergraph cluster problem show superior propos approach state-of-the-art hypergraph cluster techniqu basic notion evolutionari game theori evolutionari game theori studi model strateg interact call game among larg number anonym agent game formal triplet set player involv game set pure strategi terminolog game-theori avail player payoff function assign payoff strategi profil order set pure strategi play individu payoff function assum invari permut strategi profil it worth note general game player may set strategi payoff function comprehens introduct evolutionari game theori refer undertak evolutionari set assum larg popul non-rat agent random match play game agent consid non-rat initi choos strategi from alway play select game an agent select strategi call i-strategist evolut popul take place we assum exist select mechan analog darwinian process spread fittest strategi in popul detriment weakest one in turn driven extinct we see later in this work formal select mechan state popul a given time repres as a n-dimension vector repres the fraction i-strategist in the popul time the set possibl state describ a popul given by x r for all s call standard simplex in the sequel we drop the time
----------------------------------------------------------------

