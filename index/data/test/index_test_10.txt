query sentence: Noninfinitesimal algorithm
<<------------------------------------------------------------------------------------------------------------------------------------------->>
title: 1271-online-learning-from-finite-training-sets-an-analytical-case-study.pdf

Online learning from finite training sets
An analytical case study
Peter Sollich
Department of Physics
University of Edinburgh
Edinburgh EH9 U.K.
P.SollichOed.ac.uk
David Barber
Neural Computing Research Group
Department of Applied Mathematics
Aston University
Birmingham B4 U.K.
D.BarberOaston.ac.uk
Abstract
We analyse online learning from finite training sets at noninfinitesimal learning rates TJ. By an extension of statistical mechanics methods we obtain exact results for the time-dependent
generalization error of a linear network with a large number of
weights N. We find for example that for small training sets of
size larger learning rates can be used without compromising asymptotic generalization performance or convergence speed
Encouragingly for optimal settings of TJ and less importantly
weight decay at given final learning time the generalization performance of online learning is essentially as good as that of offline
learning
INTRODUCTION
The analysis of online gradient descent learning which is one of the most common
approaches to supervised learning found in the neural networks community has
recently been the focus of much attention The characteristic feature of online
learning is that the weights of a network student are updated each time a new
training example is presented such that the error on this example is reduced In
offline learning on the other hand the total error on all examples in the training
set is accumulated before a gradient descent weight update is made Online and
Royal Society Dorothy Hodgkin Research Fellow
Supported by EPSRC grant Novel Developments in Learning Theory
for Neural Networks
Online Leamingfrom Finite Training Sets An Analytical Case Study
offline learning are equivalent only in the limiting case where the learning rate
see The main quantity of interest is normally the evolution of
the generalization error How well does the student approximate the input-output
mapping teacher underlying the training examples after a given number of weight
updates
Most analytical treatments of online learning assume either that the size of the
training set is infinite or that the learning rate is vanishingly small Both of
these restrictions are undesirable In practice most training sets are finite and noninfinitesimal values of are needed to ensure that the learning process converges
after a reasonable number of updates General results have been derived for the
difference between online and offline learning to first order in which apply to
training sets of any size see These results however do not directly
address the question of generalization performance The most explicit analysis of
the time evolution of the generalization error for finite training sets was provided by
Krogh and Hertz for a scenario very similar to the one we consider below Their
offline calculation will serve as a baseline for our work For finite
progress has been made in particular for so-called soft committee machine network
architectures but only for the case of infinite training sets
Our aim in this paper is to analyse a simple model system in order to assess how the
combination of non-infinitesimal learning rates and finite training sets containing
a examples per weight affects online learning In particular we will consider
the dependence of the asymptotic generalization error on and a the effect of
finite a on both the critical learning rate and the learning rate yielding optimal
convergence speed and optimal values of and weight decay A. We also compare
the performance of online and offline learning and discuss the extent to which infinite
training set analyses are applicable for finite a
MODEL AND OUTLINE OF CALCULATION
We consider online training of a linear student network with input-output relation
Here is an N-dimensional vector of real-valued inputs the single real output
and the wei~t vector of the network denotes the transpose of a vector and
the factor is introduced for convenience Whenever a training example
is presented to the network its weight vector is updated along the gradient of the
squared error on this example
where is the learning rate We are interested in online learning from finite training sets where for each update an example is randomly chosen from a given set
xll,yll),j.l ofp training examples The case of cyclical presentation of
examples is left for future study If example J.l is chosen for update the weight
vector is changed to
Here we have also included a weight decay We will normally parameterize the
strength of the weight decay in terms of A where a is the number
P. Sollich and D. Barber
of examples per weight which plays the same role as the weight decay commonly
used in offline learning For simplicity all student weights are assumed to be
initially zero Wn=o
The main quantity of interest is the evolution of the generalization error of the
student We assume that the training examples are generated by a linear teacher
yJJ W. JJ IVN+e where JJ is zero mean additive noise of variance The
teacher weight vector is taken to be normalized to for simplicity and the
input vectors are assumed to be sampled randomly from an isotropic distribution
over the hypersphere N. The generalization error defined as the average of
the squared error between student and teacher outputs for random inputs is then
where
Vn
Wn
W?.
In order to make the scenario analytically tractable we focus on the limit
of a large number of input components and weights taken at constant number of
examples per weight a piN and updates per weight learning time niN In
this limit the generalization error fg(t becomes self-averaging and can be calculated
by averaging both over the random selection of examples from a given training set
and over all training sets Our results can be straightforwardly extended to the case
of percept ron teachers with a nonlinear transfer function as in
The usual statistical mechanical approach to the online learning problem expresses
the generalization error in terms of order parameters like wJw whose
self-averaging time evolution is determined from appropriately averaged update
equations This method works because for infinite training sets the average order parameter updates can again be expressed in terms of the order parameters
alone For finite training sets on the other hand the updates involve new order
parameters such as Rl wJ where A is the correlation matrix of the
training inputs A lx JJ(xJJ)T Their time evolution is in turn determined
by order parameters involving higher powers of A yielding an infinite hierarchy
of order parameters We solve this problem by considering instead order parameter generating junctions such as a generalized form of the generalization error
2~vJexp(hA)vn This allows powers of A to be obtained by differentiation with respect to reSUlting in a closed system of partial differential equations
for and wJ exp(hA)w
The resulting equations and details of their solution will be given in a future publication The final solution is most easily expressed in terms of the Laplace transform
of the generalization error
fg(Z
a
fdt
fg(t)e-z(f//a)t fdz
2f
The functions fi can be expressed in closed form in terms of a
and A and of course The Laplace transform yields directly the asymptotic
value of the generalization error foo fg(t limz--+o zig{z which can be
calculated analytically For finite learning times fg(t is obtained by numerical
inversion of the Laplace transform
RESULTS AND DISCUSSION
We now discuss the consequences of our main result focusing first on the asymptotic generalization error foo then the convergence speed for large learning times
Online Learningfrom Finite Training Sets An Analytical Case Study
a=O.s
a=i
Figure Asymptotic generalization error vs and A. a as shown
and finally the behaviour at small For numerical evaluations we generally take
corresponding to a sizable noise-to-signal ratio of JQ.I
The asymptotic generalization error is shown in as a function of and A
for a
We observe that it is minimal for A and as expected
from corresponding resul ts for offline learning We also read off that for fixed A
is an increasing function of The larger the more the weight updates tend
to overshoot the minimum of the total offline training error This causes a
diffusive motion of the weights around their average asymptotic values which
increases In the absence of weight decay A and for a however
is independent of In this case the training data can be fitted perfectly every
term in the total sum-of-squares training error is then zero and online learning does
not lead to weight diffusion because all individual updates vanish In general the
relative increase due to nonzero depends significantly on a
For
and a
for example this increase is smaller than for all A at
and for a it is at most This means that in cases where training
data is limited can be chosen fairly large in order to optimize learning
speed without seriously affecting the asymptotic generalization error In the large
a limit on the other hand one finds The relative
increase over the value at a therefore grows linearly with a already for a
increases of around can occur for
also shows that diverges as approaches a critical learning rate As
the overshoot of the weight update steps becomes so large that the weights
eventually diverge From the Laplace transform one finds that is determined
by it is a function of a and A only As shown in
increases with A. This is reasonable as the weight decay reduces the length of the
weight vector at each update counteracting potential weight divergences In the
small and large a limit one has A and respectively
For constant A therefore decreases with a
We now turn to the large behaviour of the generalization error For small
the most slowly decaying contribution mode to varies as exp its
The optimal value of the unscaledweight decay decreases with a as ja because
for large training sets there is less need to counteract noise in the training data by using
a large weight decay
2Conversely for constant increases with a from to For large a
the weight decay is applied more often between repeat presentations of a training example
that would otherwise cause the weights to diverge
P. Sollich and D. Barber
va
decay constant
a scaling linearly with 71 the size of the weight
updates as expected For small a the condition ct for fg(t to have
reached its asymptotic value foo is and scales with tla which is
the number of times each training example has been used For large a on the other
hand the condition becomes The size of the training set drops out since
convergence occurs before repetitions of training examples become significant
For larger 71 the picture changes due to a new slow mode arising from the denominator of Interestingly this mode exists only for 71 above a finite threshold
71min a For finite a it could therefore not have been predicted
from a small 71 expansion of Its decay constant Cslow decreases to zero as
71 and crosses that of the normal mode at For 71
the slow mode therefore determines the convergence speed for large and fastest
convergence is obtained for 71 However it may still be advantageous to use
lower values of 71 in order to lower the asymptotic generalization error below
values of 71 would deteriorate both convergence speed and asymptotic performance Fig shows the dependence of and on a For
not too large has a maximum at a where while decaying as
for larger a This is because for a the total training error surface is very anisotropic around its minimum in weight space The steepest
directions determine and convergence along them would be fastest for 71
as in the isotropic case However the overall convergence speed is determined by
the shallow directions which require maximal 71 for fastest convergence
Consider now the small behaviour of illustrates the dependence of
fg(t on comparison with simulation results for clearly confirms our
calculations and demonstrates that finite effects are not significant even for such
fairly small N. For a we see that nonzero 71 acts as effective update
noise eliminating the minimum in fg(t which corresponds to over-training foo
is also seen to be essentially independent of 71 as predicted for the small value of
chosen For a 3b clearly shows the increase of foo with 71 It
also illustrates how convergence first speeds up as 71 is increased from zero and then
slows down again as is approached
Above we discussed optimal settings of 71 and for minimal asymptotic generalization error foo shows what happens if we minimize fg(t instead for
a given final learning time corresponding to a fixed amount of computational
effort for training the network As increases the optimal 71 decreases towards
zero as required by the tradeoff between asymptotic performance and convergence
in
2a
a
Figure Definitions of71min and and their dependence on a as shown
Online Learning rom Finite Training Sets An Analytical Case Study
a
a
Figure fg vs for different TJ. Simulations for are shown by symbols
standard errors less than symbol sizes a as shown The learning
rate TJ increases from below at large over the range 75
Figure Optimal TJ and A vs given final learning time and resulting
Solid/dashed lines a a bold/thin lines online/offline learning
Dotted lines in Fits of form TJ bIn to optimal TJ for online learning
speed Minimizing const exp Cl TJC2 C3 exp C4TJt leads to
TJopt bIn with some constants a Although derived for small TJ
this functional form dotted lines in also provides a good description down
to fairly small where TJopt becomes large The optimal weight decay A increases
with towards the limiting value However optimizing A is much less important than choosing the right TJ Minimizing for fixed A yields almost the same
generalization error as optimizing both TJ and A we omit detailed results here It
is encouraging to see from 4c that after as few as updates per weight
with optimal TJ the generalization error is almost indistinguishable from its optimal
value for this also holds if A is kept fixed Optimization of the learning
rate should therefore be worthwhile in most practical scenarios
In we also compare the performance of online learning to that of offline
learning calculated from the appropriate discrete time version of again with
ne might have expected the opposite effect of having larger at low in order to
contain potential divergences from the larger optimal learning rates tJ However smaller
tends to make the asymptotic value foo less sensitive to large values of tJ as we saw
above and we conclude that this effect dominates
4Por fixed where fg(t has an over-training minimum Pig. the asymptotic behaviour of tJopt changes to tJopt without the In factor corresponding to a
fixed effective learning time tJt required to reach this minimum
P. SalJich and D. Barber
optimized values of TJ and A for given The performance loss from using online
instead of offline learning is seen to be negligible This may seem surprising given
the effective noise on weight updates implied by online learning in particular for
small However comparing the respective optimal learning rates we see
that online learning makes up for this deficiency by allowing larger values of TJ to
be used for large a for example TJc(offline TJc(online
Finally we compare our finite a results with those for the limiting case a
Good agreement exists for any learning time if the asymptotic generalization error
is dominated by the contribution from the nonzero learning rate TJ as is
the case for a In practice however one wants TJ to be small enough to make
only a negligible contribution to in this regime the a results are
essentially useless
CONCLUSIONS
The main theoretical contribution of this paper is the extension of the statistical
mechanics method of order parameter dynamics to the dynamics of order parameter
generating functions The results that we have obtained for a simple linear model
system are also of practical relevance For example the calculated dependence on
TJ of the asymptotic generalization error and the convergence speed shows that
in general sizable values of TJ can be used for training sets of limited size
while for larger a it is important to keep learning rates small We also found a
simple functional form for the dependence of the optimal TJ on a given final learning
time This could be used for example to estimate the optimal TJ for large from
test runs with only a small number of weight updates Finally we found that for
optimized TJ online learning performs essentially as well as offline learning whether
or not the weight decay A is optimized as well This is encouraging since online
learning effectively induces noisy weight updates This allows it to cope better than
offline learning with the problem of local training error minima in realistic neural
networks Online learning has the further advantage that the critical learning rates
are not significantly lowered by input distributions with nonzero mean whereas for
offline learning they are significantly reduced In the future we hope to extend
our approach to dynamic t-dependent optimization of TJ although performance
improvements over optimal fixed TJ may be small and to more complicated network architectures in which the crucial question of local minima can be addressed

<<----------------------------------------------------------------------------------------------------------------------->>

title: 1390-on-line-learning-from-finite-training-sets-in-nonlinear-networks.pdf

Online learning from finite training sets
in nonlinear networks
David Barber
Peter Sollich
Department of Physics
University of Edinburgh
Edinburgh ERg U.K.
Department of Applied Mathematics
Aston University
Birmingham B4 U.K.
P.Sollich~ed.ac.uk
D.Barber~aston ac.uk
Abstract
Online learning is one of the most common forms of neural network training We present an analysis of online learning from finite
training sets for non-linear networks namely soft-committee machines advancing the theory to more realistic learning scenarios
Dynamical equations are derived for an appropriate set of order
parameters these are exact in the limiting case of either linear
networks or infinite training sets Preliminary comparisons with
simulations suggest that the theory captures some effects of finite
training sets but may not yet account correctly for the presence of
local minima
INTRODUCTION
The analysis of online gradient descent learning as one of the most common forms
of supervised learning has recently stimulated a great deal of interest In
online learning the weights of a network student are updated immediately after
presentation of each training example input-output pair in order to reduce the
error that the network makes on that example One of the primary goals of online
learning analysis is to track the resulting evolution of the generalization error the
error that the student network makes on a novel test example after a given number
of example presentations In order to specify the learning problem the training
outputs are assumed to be generated by a teacher network of known architecture
Previous studies of online learning have often imposed somewhat restrictive and
Royal Society Dorothy Hodgkin Research Fellow
tSupported by EPSRC grant Novel Developments in Learning Theory for
Neural Networks
P. SolIich and D. Barber
unrealistic assumptions about the learning framework These restrictions are either
that the size of the training set is infinite or that the learning rate is small[l
Finite training sets present a significant analytical difficulty as successive weight
updates are correlated giving rise to highly non-trivial generalization dynamics
For linear networks the difficulties encountered with finite training sets and noninfinitesimal learning rates can be overcome by extending the standard set of descriptive order parameters to include the effects of weight update correlations[7
In the present work we extend our analysis to nonlinear networks The particular
model we choose to study is the soft-committee machine which is capable of representing a rich variety of input-output mappings Its online learning dynamics has
been studied comprehensively for infinite training sets[l In order to carry out
our analysis we adapt tools originally developed in the statistical mechanics literature which have found application for example in the study of Hopfield network
dynamics[2
MODEL AND OUTLINE OF CALCULATION
For an N-dimensional input vector the output of the soft committee machine is
given by
where the nonlinear activation function g(hl erf(hz/V2 acts on the activations
hi wtxl.JFi the factor is for convenience only This is a neural network
with hidden units input to hidden weight vectors WI and all hidden to
output weights set to
In online learning the student weights are adapted on a sequence of presented examples to better approximate the teacher mapping The training examples are drawn
with replacement from a finite set This set remains fixed
piN
during training Its size relative to the input dimension is denoted by a
We take the input vectors as samples from an dimensional Gaussian distribution with zero mean and unit variance The training outputs are assumed to
be generated by a teacher soft committee machine with hidden weight vectors
with additive Gaussian noise corrupting its activations and output
The discrepancy between the teacher and student on a particular training example drawn from the training set is given by the squared difference of their
corresponding outputs
yr g(km eor
where the student and teacher activations are respectively
em
and
and
output respectively
J;wtx
km
eo are noise variables corrupting the teacher activations and
Given a training example the student weights are updated by a gradient
descent step with learning rate
JNx8h
On-line Learning from Finite Training Sets in Nonlinear Networks
The generalization error is defined to be the average error that the student makes on
a test example selected at random and uncorrelated with the training set which
we write as
Although one could in principle model the student weight dynamics directly this
will typically involve too many parameters and we seek a more compact representation for the evolution of the generalization error It is straightforward to show that
the generalization error depends not on a detailed description of all the network
weights but only on the overlap parameters Qll WI and Rim
In the case of infinite it is possible to obtain a closed set of equations
governing the overlap parameters For finite training sets however this is
no longer possible due to the correlations between successive weight updates[7
In order to overcome this difficulty we use a technique developed originally to study
statistical physics systems Initially consider the dynamics of a general vector of
order parameters denoted by which are functions of the network weights If
the weight updates are described by a transition probability T(w then an
approximate update equation for is
IfdW T(w
Intuitively the integral in the above equation expresses the average change of
caused by a weight update starting from given initial weights Since
our aim is to develop a closed set of equations for the order parameter dynamics we
need to remove the dependency on the initial weights The only information we
have regarding is contained in the chosen order parameters and we therefore
average the result over the subshell of all which correspond to these values of
the order parameters This is expressed as the 8-function constraint in equation(4
It is clear that if the integral in depends on only through then the
average is unnecessary and the resulting dynamical equations are exact This is in
fact the case for and the standard order parameters mentioned
above[5 If this cannot be achieved one should choose a set of order parameters to
obtain approximate equations which are as close as possible to the exact solution
The motivation for our choice of order parameters is based on the linear perceptron
case where in addition to the standard parameters and the overlaps projected
onto eigenspaces of the training input correlation matrix A xl are
required We therefore split the eigenvalues of A into equal blocks
containing Ir eigenvalues each ordering the eigenvalues such that they
increase with We then define projectors p'Y onto the corresponding eigenspaces
and take as order parameters
R1m
Tp'Y
wm
UI.'Y
Nt W,Tp'Yb
where the are linear combinations of the noise variables and training inputs
Here we assume that the system size is large enough that the mean values of the
parameters alone describe the dynamics sufficiently well self-averaging holds
2The order parameters actually used in our calculation for the linear perceptron[7 are
Laplace transforms of these projected order parameters
P. Sollich and D. Barber
As
these order parameters become functionals of a continuous variable3
The updates for the order parameters due to the weight updates can be
found by taking the scalar products of with either projected student or teacher
weights as appropriate This then introduces the following activation components
k'Y
VNi
ff(w Tp'"Yx
so that the student and teacher activations are hi and km
respectively For the linear perceptron the chosen order parameters form a complete
set the dynamical equations close without need for the average in
For the nonlinear case we now sketch the calculation of the order parameter update
equations Taken together the integral over Wi sum of discrete terms in
our case one for each training example and the subshell average in define
an average over the activations their components and the noise variables
These variables turn out to be Gaussian distributed with zero mean and
therefore only their covariances need to be worked out One finds that these are in
fact given by the naive training set averages For example
where we have used A with the eigenvalue of A in the
eigenspace this is well defined for for details of the eigenvalue
spectrum The correlations of the activations and noise variables explicitly appearing in the error in are calculated similarly to give
Rim
where the final equation defines the noise variances The are projected overlaps between teacher weight vectors We will assume that
the teacher weights and training inputs are uncorrelated so that is independent of The required covariances of the component activations are
kinh
hi
a
a
a
ls
hJkm
a
a
mm
a
a
3Note that the limit is taken after the thermodynamic limit N. This
ensures that the number of order parameters is always negligible compared to otherwise
self-averaging would break down
On-line Learning from Finite Training Sets in Nonlinear Networks
I I
I
OOOOOOOOC
L..
I
NNNoaa oa
aaaoaaaaaaaaaaaaaaaac
Figure fg vs for student and teacher with one hidden unit
a from above learning rate I Noise of equal variance was added to
both activations and output Simulations
for are shown by circles standard errors are of the order of the symbol
size The bottom dashed lines show the infinite training set result for comparison
was used for calculating the theoretical predictions the curved marked
in with and a shows that this is large enough to be effectively in
the limit
Using equation and the definitions we can now write down the dynamical
equations replacing the number of updates by the continuous variable
in the limit
OtRim
OtU?s
OtQIz
Oh I
Oh a-y Oh,Eoh
a
where the averages are over zero mean Gaussian variables with covariances
Using the explicit form of the error we have
oh,E Lg(km
I
em
eo
which together with the equations completes the description of the dynamics
The Gaussian averages in can be straightforwardly evaluated in a manner
similar to the infinite training set and we omit the rather cumbersome
explicit form of the resulting equations
We note that in contrast to the infinite training set case the student activations
hI and the noise variables and
are now correlated through equation
Intuitively this is reasonable as the weights become correlated during training
with the examples in the training set In calculating the generalization error on the
other hand such correlations are absent and one has the same result as for infinite
training sets The dynamical equations together with constitute our
main result They are exact for the limits of either a linear network
so that ex or a and can be integrated numerically in a straightforward
way In principle the limit should be taken but as shown below relatively
small values of can be taken in practice
es
RESULTS AND DISCUSSION
We now discuss the main consequences of our result comparing the resulting
predictions for the generalization dynamics to the infinite training set theory
P. Sollich and D. Barber
02
ooooooooooooooooooo
1W
Figure VS for two hidden units Left a with a
shown by dashed line for comparison no noise Right a no noise bottom
and noise on teacher activations and outputs of variance Simulations for
are shown by small circles standard errors are less than the symbol size
Learning rate fJ throughout
and to simulations Throughout the teacher overlap matrix is set to
orthogonal teacher weight vectors of length
Tij
c5ij
In figure(l we study the accuracy of our method as a function of the training
set size for a nonlinear network with one hidden unit at two different noise levels
The learning rate was set to fJ for both and For small activation
and output noise figure(la there is good agreement with the simulations for a down to a below which the theory begins to underestimate
the generalization error compared to simulations Our finite a theory however
is still considerably more accurate than the infinite a predictions For larger noise
figure(lb our theory provides a reasonable quantitative estimate of the
generalization dynamics for a Below this value there is significant disagreement although the qualitative behaviour of the dynamics is predicted quite well
including the overfitting phenomenon beyond The infinite a theory in this
case is qualitatively incorrect
In the two hidden unit case figure(2 our theory captures the initial evolution of
very well but diverges significantly from the simulations at larger nevertheless it provides a considerable improvement on the infinite a theory One reason for
the discrepancy at large is that the theory predicts that different student hidden
units will always specialize to individual teacher hidden units for whatever
the value of a This leads to a decay of from a plateau value at intermediate times
In the simulations on the other hand this specialization symmetry breaking
appears to be inhibited or at least delayed until very large This can happen even
for zero noise and a where the training data should should contain enough
information to force student and teacher weights to be equal asymptotically The
reason for this is not clear to us and deserves further study Our initial investigations however suggest that symmetry breaking may be strongly delayed due to the
presence of saddle points in the training error surface with very shallow unstable
directions
When our theory fails which of its assumptions are violated It is conceivable
that multiple local minima in the training error surface could cause self-averaging
to break down however we have found no evidence for this see figure(3a On
the other hand the simulation results in figure(3b clearly show that the implicit
assumption of Gaussian student activations as discussed before eq can be
violated
On-line Learning from Finite Training Sets in Nonlinear Networks
Variance over training histories
Figure Variance of fg(t vs input dimension for student and teacher
with two hidden units a fJ and zero noise The bottom
curve shows the variance due to different random choices of training examples from
a fixed training set training history the top curve also includes the variance due
to different training sets Both are compatible with the liN decay expected if selfaveraging holds dotted line Distribution over training set of the activation
hI of the first hidden unit of the student Histogram from simulations for
all other parameter values as in
In summary the main theoretical contribution of this paper is the extension of online
learning analysis for finite training sets to nonlinear networks Our approximate
theory does not require the use of replicas and yields ordinary first order differential
equations for the time evolution of a set of order parameters Its central implicit
assumption and its Achilles heel is that the student activations are Gaussian
distributed In comparison with simulations we have found that it is more accurate
than the infinite training set analysis at predicting the generalization dynamics for
finite training sets both qualitatively and also quantitatively for small learning
times Future work will have to show whether the theory can be extended to cope
with non-Gaussian student activations without incurring the technical difficulties
of dynamical replica theory and whether this will help to capture the effects of
local minima and more generally rough training error surfaces
Acknowledgments We would like to thank Ansgar West for helpful discussions

<<----------------------------------------------------------------------------------------------------------------------->>

title: 4135-a-theory-of-multiclass-boosting.pdf

A Theory of Multiclass Boosting
Indraneel Mukherjee
Robert E. Schapire
Princeton University Department of Computer Science Princeton NJ
imukherj,schapire}@cs.princeton.edu
Abstract
Boosting combines weak classifiers to form highly accurate predictors Although
the case of binary classification is well understood in the multiclass setting the
correct requirements on the weak classifier or the notion of the most efficient
boosting algorithms are missing In this paper we create a broad and general
framework within which we make precise and identify the optimal requirements
on the weak-classifier as well as design the most effective in a certain sense
boosting algorithms that assume such requirements
Introduction
Boosting refers to a general technique of combining rules of thumb or weak classifiers to form
highly accurate combined classifiers Minimal demands are placed on the weak classifiers so that a
variety of learning algorithms also called weak-learners can be employed to discover these simple
rules making the algorithm widely applicable The theory of boosting is well-developed for the case
of binary classification In particular the exact requirements on the weak classifiers in this setting
are known any algorithm that predicts better than random on any distribution over the training set
is said to satisfy the weak learning assumption Further boosting algorithms that minimize loss as
efficiently as possible have been designed Specifically it is known that the Boost-by-majority
algorithm is optimal in a certain sense and that AdaBoost is a practical approximation
Such an understanding would be desirable in the multiclass setting as well since many natural classification problems involve more than two labels recognizing a digit from its image natural
language processing tasks such as part-of-speech tagging and object recognition in vision However for such multiclass problems a complete theoretical understanding of boosting is lacking In
particular we do not know the correct way to define the requirements on the weak classifiers nor
has the notion of optimal boosting been explored in the multiclass setting
Straightforward extensions of the binary weak-learning condition to multiclass do not work Requiring less error than random guessing on every distribution as in the binary case turns out to be too
weak for boosting to be possible when there are more than two labels On the other hand requiring
more than accuracy even when the number of labels is much larger than two is too stringent
and simple weak classifiers like decision stumps fail to meet this criterion even though they often
can be combined to produce highly accurate classifiers The most common approaches so far
have relied on reductions to binary classification but it is hardly clear that the weak-learning
conditions implicitly assumed by such reductions are the most appropriate
The purpose of a weak-learning condition is to clarify the goal of the weak-learner thus aiding in
its design while providing a specific minimal guarantee on performance that can be exploited by a
boosting algorithm These considerations may significantly impact learning and generalization because knowing the correct weak-learning conditions might allow the use of simpler weak classifiers
which in turn can help prevent overfitting Furthermore boosting algorithms that more efficiently
and effectively minimize training error may prevent underfitting which can also be important
In this paper we create a broad and general framework for studying multiclass boosting that formalizes the interaction between the boosting algorithm and the weak-learner Unlike much but not all
of the previous work on multiclass boosting we focus specifically on the most natural and perhaps
weakest case in which the weak classifiers are genuine classifiers in the sense of predicting a single
multiclass label for each instance Our new framework allows us to express a range of weak-learning
conditions both new ones and most of the ones that had previously been assumed often only implicitly Within this formalism we can also now finally make precise what is meant by correct
weak-learning conditions that are neither too weak nor too strong
We focus particularly on a family of novel weak-learning conditions that have an especially appealing form like the binary conditions they require performance that is only slightly better than
random guessing though with respect to performance measures that are more general than ordinary
classification error We introduce a whole family of such conditions since there are many ways of
randomly guessing on more than two labels a key difference between the binary and multiclass settings Although these conditions impose seemingly mild demands on the weak-learner we show that
each one of them is powerful enough to guarantee boostability meaning that some combination of
the weak classifiers has high accuracy And while no individual member of the family is necessary
for boostability we also show that the entire family taken together is necessary in the sense that for
every boostable learning problem there exists one member of the family that is satisfied Thus we
have identified a family of conditions which as a whole is necessary and sufficient for multiclass
boosting Moreover we can combine the entire family into a single weak-learning condition that is
necessary and sufficient by taking a kind of union or logical OR of all the members This combined
condition can also be expressed in our framework
With this understanding we are able to characterize previously studied weak-learning conditions In
particular the condition implicitly used by AdaBoost.MH which is based on a one-against-all
reduction to binary turns out to be strictly stronger than necessary for boostability This also applies
to AdaBoost.M1 the most direct generalization of AdaBoost to multiclass whose conditions
can be shown to be equivalent to those of AdaBoost.MH in our setting On the other hand the
condition implicit to Zhu al.?s SAMME algorithm is too weak in the sense that even when the
condition is satisfied no boosting algorithm can guarantee to drive down the training error Finally
the condition implicit to AdaBoost.MR also called AdaBoost.M2 turns out to be exactly
necessary and sufficient for boostability
Employing proper weak-learning conditions is important but we also need boosting algorithms that
can exploit these conditions to effectively drive down error For a given weak-learning condition
the boosting algorithm that drives down training error most efficiently in our framework can be
understood as the optimal strategy for playing a certain two-player game These games are nontrivial to analyze However using the powerful machinery of drifting games we are able to
compute the optimal strategy for the games arising out of each weak-learning condition in the family
described above These optimal strategies have a natural interpretation in terms of random walks a
phenomenon that has been observed in other settings
Our focus in this paper is only on minimizing training error which for the algorithms we derive
provably decreases exponentially fast with the number of rounds of boosting Such results can be
used in turn to derive bounds on the generalization error using standard techniques that have been
applied to other boosting algorithms We omit these due to lack of space
The game-theoretic strategies are non-adaptive in that they presume prior knowledge about the edge
that is how much better than random are the weak classifiers Algorithms that are adaptive such as
AdaBoost are much more practical because they do not require such prior information We show
therefore how to derive an adaptive boosting algorithm by modifying one of the game-theoretic
strategies
We present experiments aimed at testing the efficacy of the new methods when working with a very
weak weak-learner to check that the conditions we have identified are indeed weaker than others that
had previously been used We find that our new adaptive strategy achieves low test error compared
to other multiclass boosting algorithms which usually heavily underfit This validates the potential
practical benefit of a better theoretical understanding of multiclass boosting
Previous work The first boosting algorithms were given by Schapire and Freund followed
by their AdaBoost algorithm Multiclass boosting techniques include AdaBoost.M1 and AdaBoost.M2 as well as AdaBoost.MH and AdaBoost.MR Other approaches include
There are also more general approaches that can be applied to boosting including Two
game-theoretic perspectives have been applied to boosting The first one views the weak
learning condition as a minimax game while drifting games were designed to analyze the
most efficient boosting algorithms These games have been further analyzed in the multiclass and
continuous time setting in
Framework
We introduce some notation Unless otherwise stated matrices will be denoted by bold capital letters
like and vectors by bold small letters like Entries of a matrix and vector will be denoted as
or while will denote the ith row of a matrix Inner product of two vectors
is denoted by hu vi The Frobenius inner product of two matrices Tr(MM0 will be denoted by
M0 The indicator function is denoted by The distribution over the set will be
denoted by
In multiclass classification we want to predict the labels of examples lying in some set Each
example has a unique label in the set where We are provided a training
set of labeled examples y1 xm ym
Boosting combines several mildly powerful predictors called weak classifiers to form a highly
accurate combined classifier and has been previously applied for multiclass classification In this
paper we only allow weak classifier that predict a single class for each example This is appealing
since the combined classifier has the same form although it differs from what has been used in much
previous work
We adopt a game-theoretic view of boosting A game is played between two players Booster and
Weak-Learner for a fixed number of rounds With binary labels Booster outputs a distribution
in each round and Weak-Learner returns a weak classifier achieving more than accuracy on
that distribution The multiclass game is an extension of the binary game In particular in each
round Booster creates a cost-matrix Ct Rm?k specifying to Weak-Learner that the cost
of classifying example as is The cost-matrix may not be arbitrary but should conform
to certain restrictions as discussed below Weak-Learner returns some weakP
classifier ht
from a fixed space ht so that the cost incurred is Ct 1ht Ct ht
is small enough according to some conditions discussed below Here by 1h we mean the
matrix whose j)-th entry is Booster computes a weight for the current weak
classifier based on how much cost was incurred in this round
At the end Booster predicts according to the weighted plurality vote of the classifiers returned in
each round
ht
argmax fT where fT
By carefully choosing the cost matrices in each round Booster aims to minimize the training error
of the final classifer even when Weak-Learner is adversarial The restrictions on cost-matrices
created by Booster and the maximum cost Weak-Learner can suffer in each round together define
the weak-learning condition being used For binary labels the traditional weak-learning condition
states for any non-negative weights
on the training set the error of the weak
classfier returned is at most Here parametrizes the condition There are many
ways to translate this condition into our language The one with fewest restrictions on the costmatrices requires labeling correctly should be less costly than labeling incorrectly
y?i while
on the
returned
weak classifier
requires less cost than predicting
the restriction
randomly h(xi y?i By the correspondence
y?i we may verify the two conditions are the same
We will rewrite this condition after making some simplifying assumptions Henceforth without
loss of generality we assume that the true label is always Let bin consist of matrices
which satisfy Further let Ubin
be the matrix whose each row is
Then Weak-Learner searching
space satisfies the binary weak-learning
condition if bin 1h Ubin
There are two main benefits to this refor
mulation With linear homogeneous constraints the mathematics is simplified as will be apparent
later More importantly by varying the restrictions bin on the cost vectors and the matrix Ubin we
can generate a vast variety of weak-learning conditions for the multiclass setting as we now
show
Let Rm?k and matrix Rm?k which we call the baseline we say a weak classifier space
satisfies the condition if
B(i)i
In the variable matrix specifies how costly each misclassification is while the baseline
specifies a weight for each misclassification The condition therefore states that a weak classifier should not exceed the average cost when weighted according to baseline B. This large class
of weak-learning conditions captures many previously used conditions such as the ones used by
AdaBoost.M1 AdaBoost.MH and AdaBoost.MR below as well as novel conditions introduced in the next section
By studying this vast class of weak-learning conditions we hope to find the one that will serve the
main purpose of the boosting game finding a convex combination of weak classifiers that has zero
training error For this to be possible at the minimum the weak classifiers should be sufficiently rich
for such a perfect combination to exist Formally a collection of weak classifiers is eligible for
boosting or simply boostable
if there exists a distribution on this space that linearly separates the
data h?H h(xi The weak-learning condition plays two
roles It rejects spaces that are not boostable and provides an algorithmic means of searching for the
right combination Ideally the second factor will not cause the weak-learning condition to impose
additional restrictions on the weak classifiers in that case the weak-learning condition is merely a
reformulation of being boostable that is more appropriate for deriving an algorithm In general it
could be too strong certain boostable spaces will fail to satisfy the conditions Or it could be too
weak non-boostable spaces might satisfy such a condition Booster strategies relying on either
of these conditions will fail to drive down error the former due to underfitting and the latter due
to overfitting In the next section we will describe conditions captured by our framework that avoid
being too weak or too strong
Necessary and sufficient weak-learning conditions
The binary weak-learning condition has an appealing form for any distribution over the examples
the weak classifier needs to achieve error not greater than that of a random player who guesses
the correct answer with probability Further this is the weakest condition under which
boosting is possible as follows from a game-theoretic perspective Multiclass weak-learning
conditions with similar properties are missing in the literature In this section we show how our
framework captures such conditions
In the multiclass setting we model a random player as a baseline predictor Rm?k whose rows
are distributions over the labels The prediction on example is a sample from
We only consider the space of edge-over-random baselines B?eor Rm?k who have a faint
clue about the correct answer More precisely any baseline B?eor in this space is more likely
to predict the correct label than an incorrect one on every example
with equality holding for some
When the space B?eor consists of the unique player Ubin
and the binary weak-learning
bin
bin
condition is given by The new conditions generalize this to In particular define
eor to be the multiclass extension of bin any cost-matrix in eor should
put the least cost on the
correct label the rows of the cost-matrices should come from the set Rk
Then for every baseline B?eor we introduce the condition eor which we call an edgeover-random weak-learning condition Since is the expected cost of the edge-over-random
baseline on matrix the constraints imposed by the new condition essentially require better
than random performance
We now present the central results of this section The seemingly mild edge-over-random conditions
guarantee eligibility meaning weak classifiers that satisfy any one such condition can be combined
to form a highly accurate combined classifier
Theorem Sufficiency If a weak classifier space satisfies a weak-learning condition eor
for some B?eor then is boostable
The proof involves the Von-Neumann Minimax theorem and is in the spirit of the ones in On
the other hand the family of such conditions taken as a whole is necessary for boostability in the
sense that every eligible space of weak classifiers satisfies some edge-over-random condition
Theorem Relaxed necessity For every boostable weak classifier space there exists a
and B?eor such that satisfies the weak-learning condition eor B).
The proof shows existence through non-constructive averaging arguments Theorem states that
any boostable weak classifier space will satisfy some condition in our family
but it does not help
us choose the right condition Experiments in Section suggest eor is effective with very
simple weak-learners compared to popular boosting algorithms Here B?eor is the edge-overrandom baseline closest to uniform it has weight on incorrect labels and
on the correct label However there are theoretical examples showing each condition in our family
is too strong supplement
A perhaps extreme way of weakening the condition is by requiring the performance on a cost matrix
to be competitive not with a fixed baseline B?eor but with the worst of them
eor 1h maxeor B.
Condition states that during the course of the same boosting game Weak-Learner may choose
to beat any edge-over-random baseline B?eor possibly a different one for every round and every
cost-matrix This may superficially seem much too weak On the contrary this condition turns out
to be equivalent to boostability In other words according to our criterion it is neither too weak nor
too strong as a weak-learning condition However unlike the edge-over-random conditions it also
turns out to be more difficult to work with algorithmically
Furthermore this condition can be shown to be equivalent to the one used by AdaBoost.MR
This is perhaps remarkable since the latter is based on the apparently completely unrelated all-pairs
MR
consists of
multiclass to binary reduction the MR condition is given by MR BMR
where
cost-matrices that put non-negative costs on incorrect labels and whose rows sum up to zero while
m?k
BMR
is the matrix that has on the first column and on all other columns(supplement
Further the MR condition and hence can be shown to be neither too weak nor too strong
Theorem A weak classifier space satisfies AdaBoost.MR?s weak-learning condition
MR BMR
if and only if it satisfies Moreover this condition is equivalent to being boostable
Next we illustrate the strengths of our random-over-edge weak-learning conditions through concrete
comparisons with previous algorithms
Comparison with SAMME The SAMME algorithm of requires the weak classifiers to
achieve less error than uniform random guessing for multiple labels in our language their weaklearning condition is As is well-known this condition is
not sufficient for boosting to be possible In particular consider the dataset with
and a weak classifier space consisting of h1 h2 which always predict respectively Since neither classifier distinguishes between a we cannot achieve perfect accuracy by
combining them in any way Yet due to the constraints on the cost-matrix one of h1 h2 will always
manage non-positive cost while random always suffers positive cost On the other hand our weaklearning condition allows the Booster to choose far richer cost matrices In particular when the
cost matrix is eor both classifiers in the above
example suffer more loss than the random player and fail to satisfy our condition
Comparison with AdaBoost.MH AdaBoost.MH is a popular multiclass boosting algorithm that is
based on the one-against-all reduction[19 However we show that its implicit demands on the weak
classifier space is too strong We construct a classifier space that satisfies the condition eor
in our family but cannot satisfy AdaBoost.MH?s weak-learning condition
Consider a space that has for every element subset of the examples a classifier
that predicts correctly on exactly those elements The expected loss of a randomly chosen classifier
from this space is the same as that of the random player Hence satisfies this weak-learning
condition On the other hand it can be shown supplement that AdaBoost.MH?s weak-learning
MH
condition is the pair MH BMH
has non-(positive)negative entries on in)correct labels
where
and where each row of the matrix BMH
is
the
vector A
quick calculation shows that for any and MH with in the first column and zeroes
elsewhere 1h BMH
This is positive when so that fails to satisfy
AdaBoost.MH?s condition
Algorithms
In this section we devise algorithms by analyzing the boosting games that employ our edge-overrandom weak-learning conditions We compute the optimum Booster strategy against a completely
adversarial Weak-Learner which here is permitted to choose weak classifiers without restriction
the entire space Hall of all possible functions mapping examples to labels By modeling WeakLearner adversarially we make absolutely no assumptions on the algorithm it might use Hence
error guarantees enjoyed in this situation will be universally applicable Our algorithms are derived
from the very general drifting games framework for solving boosting games in turn inspired
by Freund?s Boost-by-majority algorithm which we review next
The OS Algorithm Fix the number of rounds and an edge-over-random weak-learning condition
B). For simplicity of presentation we fix the weights in each round With fT defined as
in the optimum Booster payoff can be written as
min
max
min
max
L(fT fT fT
C1
h1 Hall
C1
CT
hT Hall
CT
Here the function Rk is error but we can also consider other loss functions such as
exponential loss hinge loss etc that upper-bound error and are proper is increasing in
the weight of the correct label and decreasing in the weights of the incorrect labels
Directly analyzing the optimal payoff is hard However Schapire observed that the payoffs
can be very well approximated by certain potential functions Indeed for any Rk define the
potential function
by the following recurrence
min
max
El?p
el El?p hb ci
c?Rk
where el Rk is the unit-vector whose lth coordinate is and the remaining coordinates zero
These potential functions compute an estimate
st of whether an example will be misclassified
based on its current state st consisting of counts of votes received so far on various classes st
t0 ht and the number of rounds remaining Using these functions Schapire
proposed a Booster strategy aka the OS strategy which in round constructs a cost matrix
whose each row achieves the minimum of the right hand side of with replaced by
replaced by and replaced by current state st The following theorem provides a guarantee
for the loss suffered by the OS algorithm and also shows that it is the game-theoretically optimum
strategy when the number of examples is large
Theorem Extension of results in Suppose the weak-learning condition is given by If
Pm
Booster employs the OS algorithm then the average potential of the states
never increases in any round In particular loss suffered after rounds of play is at most
Pm
Further for any when the loss function satisfies some mild conditions and no Booster strategy can achieve loss less than the above bound in
rounds
Computing the potentials In order to implement the OS strategy using our weak-learning conditions we only need to compute the potential
for distributions Fortunately
these potentials have a very simple solution in terms of the homogeneous random-walk Rtb the
random position of a particle after time steps that starts at location Rk and in each step moves
in direction el with probability
Theorem If is proper and satisfies then
Rtb Furthermore the vector achieving the minimum in the right hand side of is
given by
el
Theorem implies the OS strategy chooses the following cost matrix in round
st el where st is the state of example in round Therefore everything boils
down to computing the potentials which is made possible by Theorem There is no simple closed
form solution for the non-convex loss maxi>1 si However using Theorem we can write the potential explicitly and then compute it using dynamic programming
in time This yields very tight bounds
To obtain a more efficient procedure and one that we will soon show can be made adaptive we next
focus on the exponential loss associated with AdaBoost that does have a closed form solution
Lemma If s1 exp(?k sk s1 where each is positive then
Pk
sl
the solution in Theorem evaluates to
where al bl
al
bl b1
The proof by induction is straightforward In particular when the condition is eor and
Pk
the relevant potential is e?(sl where
The cost-matrix output by the OS algorithm can be
simplified by rescaling or adding the same number to each coordinate of a cost vector without
affecting the constraints it imposes on a weak classifier to the following form
e?(sl
if
Pk
e?(sj if
With such
Pm a choice Theorem and the form of the potential guarantee that the average loss
L(st of the states st changes by a factor of at most every round Hence
the final loss is at most
Variable edges So far we have required Weak-Learner to beat random by at least a fixed amount
in each round of the boosting game In reality the edge over random is larger initially
and gets smaller as the OS algorithm creates harder cost matrices Therefore requiring a fixed
edge is either unduly pessimistic or overly optimistic If the fixed edge is too small not enough
progress is made in the initial rounds and if the edge is too large Weak-Learner fails to meet the
weak-learning condition in latter rounds We attempt to fix this via two approaches prescribing a
decaying sequence of edges or being completely flexible aka adaptive with respect to
the edges returned by the weak-learner In either case we only use the edge-over-random condition
eor but with varying values of
Fixed sequence of edges With a prescribed sequence of edges the weak-learning condition eor U?t in each round is different We allow the weights to be arbitrary but they
must be fixed in advance All the results for uniform and weights hold in this case as well
Pm Pk
In particular by the arguments leading to if we want to minimize e{ft
where ft is as defined in then the following strategy is optimal in round output the cost matrix
if
eft?1
Pk
eft?1 if
Pm Pk
This will ensure that the expression e{ft changes by a factor of at most
QT
in each round Hence the final loss will be at most
Adaptive In the adaptive setting we depart from the game-theoretic framework in that WeakLearner is no longer adversarial Further we are no longer guaranteed to receive a certain sequence
of edges Since the choice of cost-matrix in does not depend on the edges we could fix an
arbitrary set of weights in advance follow the same algorithm as before and enjoy the same bound
QT
The trouble with this is is not less than unless is small compared to
To ensure progress the weight must be chosen adaptively as a function of Since we do not
know what edge we will receive we choose the cost matrix as before but anticipating infinitesimally
small edge in the spirit of and with some rescaling
eft?1
if
Pk
lim
if
eft?1
if
Pk
eft?1 if
pendigits
satimage
poker
letter
forest
connect4
pendigits
poker
satimage
letter
forest
connect4
Figure Figure plots the final test-errors of M1(black dashed MH(blue dotted and New method(red
solid against the maximum tree-sizes allowed as weak classifiers Figure plots how fast the test-errors of
these algorithms drop with rounds when the maximum tree-size allowed is
Since Weak-Learner cooperates we expect the edge of the returned classifier ht on the supplied
cost-matrix to be more than just infinitesimal In that case by continuity there are noninfinitesimal choices of the weight such that the edge achieved by ht on the cost-matrix C?t
remains large enough to ensure In fact with any choice of we
get
supplement Tuning to
results in
loss
and hence error after
This algorithm is adaptive and ensures
that the
QT
PT
rounds is at most exp
Experiments
We report preliminary experimental results on six varying multiclass UCI datasets
The first set of experiments were aimed at determining
overall performance of our new algorithm We compared
MH
a standard implementation M1 of AdaBoost.M1 with
M1
New Method
as weak learner and the Boostexter implementation MH
of AdaBoost.MH using stumps with the adaptive
algorithm described in Section which we call New
method using a naive greedy tree-searching algorithm
Greedy for weak-learner The size of trees was chosen
to be of the same order as the tree sizes used by M1. Test
errors after rounds of boosting are plotted in Figure
The performance is comparable with M1 and far better
than MH understandably since stumps are far weaker than
trees even though our weak-learner is very naive com Figure This is a plot of the final test-errors
of standard implementations of MH and
pared to
connect4
forest
letter
pendigits
poker
satimage
New method after rounds of boosting
We next investigated how each algorithm performs with
less powerful weak-classifiers namely decision trees whose size has been sharply limited to various
pre-specified limits Figure shows test-error plotted as a function of tree size As predicted by
our theory our algorithm succeeds in boosting the accuracy even when the tree size is too small
to meet the stronger weak learning assumptions of the other algorithms The differences in performance are particularly strong when using the smallest tree sizes
More insight is provided by plots in Figure of the rate of convergence of test error with rounds
when the tree size allowed is very small Both M1 and MH drive down the error for a few rounds
But since boosting keeps creating harder cost-matrices very soon the small-tree learning algorithms
are no longer able to meet the excessive requirements of M1 and MH. However our algorithm makes
more reasonable demands that are easily met by the weak learner

<<----------------------------------------------------------------------------------------------------------------------->>

title: 4745-a-scalable-cur-matrix-decomposition-algorithm-lower-time-complexity-and-tighter-bound.pdf

A Scalable CUR Matrix Decomposition Algorithm
Lower Time Complexity and Tighter Bound
Shusen Wang and Zhihua Zhang
College of Computer Science Technology
Zhejiang University
Hangzhou China
wss,zhzhang}@zju.edu.cn
Abstract
The CUR matrix decomposition is an important extension of Nystr?om approximation to a general matrix It approximates any data matrix in terms of a small number of its columns and rows In this paper we propose a novel randomized CUR
algorithm with an expected relative-error bound The proposed algorithm has the
advantages over the existing relative-error CUR algorithms that it possesses tighter
theoretical bound and lower time complexity and that it can avoid maintaining the
whole data matrix in main memory Finally experiments on several real-world
datasets demonstrate significant improvement over the existing relative-error algorithms
Introduction
Large-scale matrices emerging from stocks genomes web documents web images and videos everyday bring new challenges in modern data analysis Most efforts have been focused on manipulating understanding and interpreting large-scale data matrices In many cases matrix factorization
methods are employed to construct compressed and informative representations to facilitate computation and interpretation A principled approach is the truncated singular value decomposition
SVD which finds the best low-rank approximation of a data matrix Applications of SVD such as
eigenface and latent semantic analysis have been illustrated to be very successful
However the basis vectors resulting from SVD have little concrete meaning which makes it very
difficult for us to understand and interpret the data in question
An example in has well
shown this viewpoint that is the vector 2)height the sum of the
significant uncorrelated features from a dataset of people?s features is not particularly informative
The authors of have also claimed it would be interesting to try to find basis vectors for all
experiment vectors using actual experiment vectors and not artificial bases that offer little insight
Therefore it is of great interest to represent a data matrix in terms of a small number of actual
columns and/or actual rows of the matrix
The CUR matrix decomposition provides such techniques and it has been shown to be very useful
in high dimensional data analysis Given a matrix A the CUR technique selects a subset of
columns of A to construct a matrix and a subset of rows of A to construct a matrix and
CUR best approximates A. The typical CUR algorithms
computes a matrix such that A
work in a two-stage manner Stage is a standard column selection procedure and Stage
does row selection from A and simultaneously Thus Stage is more complicated than Stage
The CUR matrix decomposition problem is widely studied in the literature 13
18 19 Perhaps the most widely known work on the CUR problem is in which the authors
devised a randomized CUR algorithm called the subspace sampling algorithm Particularly the
algorithm has relative-error ratio with high probability
Unfortunately all the existing CUR algorithms require a large number of columns and rows to be
chosen For example for an matrix A and a target rank min{m the state-ofthe-art CUR algorithm the subspace sampling algorithm in requires exactly O(k
rows or log2 rows in expectation to achieve relative-error ratio Moreover
the computational cost of this algorithm is at least the cost of the truncated SVD of A that is
O(min{mn2 nm2 The algorithms are therefore impractical for large-scale matrices
In this paper we develop a CUR algorithm which beats the state-of-the-art algorithm in both theory
and experiments In particular we show in Theorem a novel randomized CUR algorithm with
lower time complexity and tighter theoretical bound in comparison with the state-of-the-art CUR
algorithm in
The rest of this paper is organized as follows Section introduces several existing column selection
algorithms and the state-of-the-art CUR algorithm Section describes and analyzes our novel
CUR algorithm Section empirically compares our proposed algorithm with the state-of-the-art
algorithm
Notations
For a matrix A aij Rm?n let be its i-th row and aj be its j-th column Let
be the Frobenius norm and be the spectral
i,j aij be the norm
i,j aij
norm Moreover let Im denote an identity matrix and 0mn denotes an zero matrix
UA,k VA,k
UA,k VA,k
be the
Let A UA A VA
uA,i vA,i
SVD of A where rank(A and UA,k and VA,k correspond to the top singular values
We denote Ak UA,k VA,k
Furthermore let A
be the Moore-Penrose
inverse of A
Related Work
Section introduces several relative-error column selection algorithms related to this work Section describes the state-of-the-art CUR algorithm in Section discusses the connection
between the column selection problem and the CUR problem
Relative-Error Column Selection Algorithms
Given a matrix A Rm?n column selection is a problem of selecting columns of A to construct
Rm?c to minimize A CC A?F Since there are nc possible choices of constructing
so selecting the best subset is a hard problem In recent years many polynomial-time approximate
algorithms have been proposed among which we are particularly interested in the algorithms with
relative-error bounds that is with columns selected from A there is a constant such that
A CC A?F Ak
We call the relative-error ratio We now present some recent results related to this work
We first introduce a recently developed deterministic algorithm called the dual set sparsification
proposed in We show their results in Lemma Furthermore this algorithm is a building
block of some more powerful algorithms Lemma and our novel CUR algorithm also relies
on this algorithm We attach the algorithm in Appendix A.
Lemma Column Selection via Dual Set Sparsification Algorithm Given a matrix A Rm?n
of rank and a target rank there exists a deterministic algorithm to select columns
of A and form a matrix Rm?c such that
Ak
A CC A
A
Although some partial SVD algorithms such as Krylov subspace methods require only O(mnk time
they are all numerical unstable See for more discussions
Moreover the matrix can be computed in TVA,k O(mn+nck where TVA,k is the time needed
to compute the top right singular vectors of A.
There are also a variety of randomized column selection algorithms achieving relative-error bounds
in the literature
An randomized algorithm in selects only 2k
columns to achieve the expected
relative-error ratio The algorithm is based on the approximate SVD via random projection the dual set sparsification algorithm and the adaptive sampling algorithm Here we
present the main results of this algorithm in Lemma Our proposed CUR algorithm is motivated
by and relies on this algorithm
Lemma Near-Optimal Column Selection Algorithm Given a matrix A Rm?n of rank a
target rank and there exists a randomized algorithm to select at most
2k
columns of A to form a matrix Rm?c such that
E2 A CC A?F E?A CC Ak
where the expectations are taken C. Furthermore the matrix can be computed in O((mnk
nk
The Subspace Sampling CUR Algorithm
Drineas proposed a two-stage randomized CUR algorithm which has a relative-error
bound Given a matrix A Rm?n and a target rank in the first stage the algorithm
chooses exactly O(k log columns log log in expectation of
A to construct Rm?c in the second stage it chooses exactly log rows
log log in expectation of A and simultaneously to construct and U. With
probability at least the relative-error ratio is The computational cost is dominated by
the truncated SVD of A and C.
Though the algorithm is optimal with high probability it requires too many rows get chosen at
least log2 rows in expectation In this paper we seek to devise an algorithm with
mild requirement on column and row numbers
Connection between Column Selection and CUR Matrix Decomposition
The CUR problem has a close connection with the column selection problem As aforementioned
the first stage of existing CUR algorithms is simply a column selection procedure However the
second stage is more complicated If the second stage is na??vely solved by a column selection
algorithm on AT then the error ratio will be at least
For a relative-error CUR algorithm the first stage seeks to bound a construction error ratio of
A?F
AR R?F
given C. Actually the first
A?Ak while the section stage seeks to bound
A?F
stage is a special case of the second stage where Ak Given a matrix A if an algorithm solv
AR R?F
ing the second stage results in a bound A?CC
then this algorithm also solves the
A?F
column selection problem for A with an relative-error ratio Thus the second stage of CUR is a
generalization of the column selection problem
Main Results
In this section we introduce our proposed CUR algorithm We call it the fast CUR algorithm because
it has lower time complexity compared with SVD. We describe it in Algorithm and give a theoretical analysis in Theorem Theorem relies on Lemma and Theorem and Theorem relies on
Theorem Theorem is a generalization of Theorem and Theorem is a generalization
of Theorem
Algorithm The Fast CUR Algorithm
Input a real matrix A Rm?n target rank target column number 2k
target
row number 2c
Stage select columns of A to construct Rm?c
kV
Compute approximate truncated SVD via random projection such that Ak
kV
V1 columns of
kT
Construct U1 columns of A
Compute s1 Dual Set Spectral-Frobenius Sparsification Algorithm V1
Construct C1 ADiag(s1 and then delete the all-zero columns
Residual matrix A C1 A
Compute sampling probabilities pi di
Sampling c2 columns from A with probability pn to construct C2
Stage select rows of A to construct Rr?n
kV
V2 columns of
Tk
Construct U2 columns of A
Compute s2 Dual Set Spectral-Frobenius Sparsification Algorithm V2
Construct R1 Diag(s2 and then delete the all-zero rows
Residual matrix A R1 Compute qj
Sampling r2 rows from A with probability qm to construct R2
return C2 RT2 and AR
Adaptive Sampling
The relative-error adaptive sampling algorithm is established in Theorem The algorithm
is based on the following idea after selecting a proportion of columns from A to form C1 by
an arbitrary algorithm the algorithms randomly samples additional c2 columns according to the
residual A C1 A. Boutsidis used the adaptive sampling algorithm to decrease the
residual of the dual set sparsification algorithm and obtained an relative-error bound Here
we prove a new bound for the adaptive sampling algorithm Interestingly this new bound is a
generalization of the original one in Theorem In other words Theorem of is a direct
corollary of our following theorem in which Ak is set
Theorem The Adaptive Sampling Algorithm Given a matrix A Rm?n and a matrix
Rm?c such that rank(C rank(CC A we let R1 Rr1 consist of r1
rows of A and define the residual A R1 Additionally for we define
pi
We further sample r2 rows from A in each trial of which the i-th row is chosen with probability
pi Let R2 Rr2 contains the r2 sampled rows and let RT2 Then
the following inequality holds
E?A CC AR A CC A R1
r2
where the expectation is taken R2
The Fast CUR Algorithm
Based on the dual set sparsification algorithm of of Lemma and the adaptive sampling algorithm
of Theorem we develop a randomized algorithm to solve the second stage of CUR problem We
present the results of the algorithm in Theorem Theorem of is a special case of the following
theorem where Ak
Theorem The Fast Row Selection Algorithm Given a matrix A Rm?n and a matrix
Rm?c such that rank(C rank(CC A and a target rank the
r?n
proposed randomized algorithm selects
such
rows of A to construct
that
E?A CC AR A CC Ak
where the expectation is taken R. Furthermore the matrix can be computed in O((mnk
mk time
Based on Lemma and Theorem here we present the main theorem for the fast CUR algorithm
Table A summary of the datasets
Dataset
Type
size
Source
Redrocknatural
http://www.agarwala.org/efficient gdc
Arcene
biology
http://archive.ics.uci.edu/ml/datasets/Arcene
Dexter bag of words 2600http://archive.ics.uci.edu/ml/datasets/Dexter
Theorem The Fast CUR Algorithm Given a matrix A Rm?n and a positive integer
min{m the fast CUR algorithm described in Algorithm randomly selects 2k
columns of A to construct Rm?c with the near-optimal column selection algorithm of Lemma
r?n
and then selects 2c
with the fast row selection
rows of A to construct
algorithm of Theorem Then we have
E?A CUR?F E?A AR Ak
Moreover the algorithm runs in time n)k mk nk
Since min{m by the assumptions so the time complexity of the fast CUR algorithm
is lower than that of the SVD of A. This is the main reason why we call it the fast CUR algorithm
Another advantage of this algorithm is avoiding loading the whole data matrix A into main
memory None of three steps the randomized SVD the dual set sparsification algorithm and the
adaptive sampling algorithm requires loading the whole of A into memory The most memoryexpensive operation throughout the fast CUR Algorithm is computing the Moore-Penrose inverse
of and which requires maintaining an matrix or an matrix in memory In
comparison the subspace sampling algorithm requires loading the whole matrix into memory to
compute its truncated SVD.
Empirical Comparisons
In this section we provide empirical comparisons among the relative-error CUR algorithms on several datasets We report the relative-error ratio and the running time of each algorithm on each data
set The relative-error ratio is defined by
A CUR?F
Relative-error ratio
A Ak
where is a specified target rank
We conduct experiments on three datasets including natural image biology data and bags of words
Table briefly summarizes some information of the datasets Redrock is a large size natural image
Arcene and Dexter are both from the UCI datasets Arcene is a biology dataset with
instances and attributes Dexter is a bag of words dataset with a 20000-vocabulary and
documents Each dataset is actually represented as a data matrix upon which we apply the CUR
algorithms
We implement all the algorithms in MATLAB We conduct experiments on a workstation
with Intel Xeon CPUs memory and Ubuntu system According to the
analysis in and this paper and should be integers far less than and For each data
set and each algorithm we set or and where ranges in each set of
experiments We repeat each set of experiments for times and report the average and the standard
deviation of the error ratios The results are depicted in Figures
The results show that the fast CUR algorithm has much lower relative-error ratio than the subspace
sampling algorithm The experimental results well match our theoretical analyses in Section As
for the running time the fast CUR algorithm is more efficient when and are small When and
become large the fast CUR algorithm becomes less efficient This is because the time complexity
of the fast CUR algorithm is linear in and large and imply small However the purpose
of CUR is to select a small number of columns and rows from the data matrix that is and
So we are not interested in the cases where and are large compared with and say
and
Running Time
Running Time
Subspace Sampling Exactly
Subspace Sampling Expected
Fast CUR
Subspace Sampling Exactly
Subspace Sampling Expected
Fast CUR
Time
Time
Time
Running Time
18 22 24 26 28 32 34 36
Construction Error Frobenius Norm
18 22 24
Subspace Sampling Exactly
Subspace Sampling Expected
Fast CUR
18
18 22 24 26 28 32 34 36
Subspace Sampling Exactly
Subspace Sampling Expected
Fast CUR
Construction Error Frobenius Norm
Relative Error Ratio
Subspace Sampling Exactly
Subspace Sampling Expected
Fast CUR
Relative Error Ratio
Construction Error Frobenius Norm
Relative Error Ratio
Subspace Sampling Exactly
Subspace Sampling Expected
Fast CUR
18 22 24
18
and and and
Figure Empirical results on the Redrock data set
Running Time
Running Time
18
Time
Time
Time
Running Time
Subspace Sampling Exactly
Subspace Sampling Expected
Fast CUR
18 22 24 26 28
Construction Error Frobenius Norm
18
Subspace Sampling Exactly
Subspace Sampling Expected
Fast CUR
Construction Error Frobenius Norm
Construction Error Frobenius Norm
Subspace Sampling Exactly
Subspace Sampling Expected
Fast CUR
Relative Error Ratio
Subspace Sampling Exactly
Subspace Sampling Expected
Fast CUR
Subspace Sampling Exactly
Subspace Sampling Expected
Fast CUR
Relative Error Ratio
Relative Error Ratio
Subspace Sampling Exactly
Subspace Sampling Expected
Fast CUR
18 22 24 26 28
18
and and and
Figure Empirical results on the Arcene data set
Running Time
Running Time
Running Time
Subspace Sampling Exactly
Subspace Sampling Expected
Fast CUR
Subspace Sampling Exactly
Subspace Sampling Expected
Fast CUR
Subspace Sampling Exactly
Subspace Sampling Expected
Fast CUR
Time
Time
Time
18 22 24 26 28
18 22 24
18 22 24 26 28
18
18 22 24
Subspace Sampling Exactly
Subspace Sampling Expected
Fast CUR
Relative Error Ratio
Relative Error Ratio
Subspace Sampling Exactly
Subspace Sampling Expected
Fast CUR
Construction Error Frobenius Norm
Subspace Sampling Exactly
Subspace Sampling Expected
Fast CUR
Construction Error Frobenius Norm
Construction Error Frobenius Norm
Relative Error Ratio
18
and and and
Figure Empirical results on the Dexter data set
Conclusions
In this paper we have proposed a novel randomized algorithm for the CUR matrix decomposition
problem This algorithm is faster more scalable and more accurate than the state-of-the-art algorithm the subspace sampling algorithm Our algorithm requires only
columns and rows to achieve relative-error ratio To achieve the same
relative-error bound the subspace sampling algorithm requires log columns and
log rows selected from the original matrix Our algorithm also beats the subspace
sampling algorithm in time-complexity Our algorithm costs n)k
mk nk time which is lower than O(min{mn2 m2 of the subspace sampling algorithm when is small Moreover our algorithm enjoys another advantage of avoiding loading the
whole data matrix into main memory which also makes our algorithm more scalable Finally the
empirical comparisons have also demonstrated the effectiveness and efficiency of our algorithm
A The Dual Set Sparsification Algorithm
For the sake of completeness we attach the dual set sparsification algorithm here and describe
some implementation details The dual set sparsification algorithms are deterministic algorithms
established in The fast CUR algorithm calls the dual set spectral-Frobenius sparsification algorithm Lemma in both stages We show this algorithm in Algorithm and its bounds in
Lemma
Lemma Dual Set Spectral-Frobenius Sparsification Let Rl
l?n
contains the columns of an arbitrary matrix
RT Let vn
be a decompositions of the identity
I
Given
an
integer
with
Algorithm deterministically computes a set of weights si at most of which
are non-zero such that
and
tr
si xTi
si vi vi
Algorithm Deterministic Dual Set Spectral-Frobenius Sparsification Algorithm
Input
vi with
vi vi Ik
Initialize s0 A0
Compute for and then compute
k/r
for to do
Compute the eigenvalue decomposition of A
Find an index in and compute a weight such that
vjT A
vj
vj
vjT A
A A
where
A
rk
Update the j-th component of and A
end for
k/r
return
sr
A A tvj vjT
The weights si can be computed deterministically in O(rnk nl time
Here we would like to mention the implementation of Algorithm which is not described in detailed
by In each iteration the algorithm performs once eigenvalue decomposition A W?WT
is guaranteed to be positive semi-definite in each iteration Since
A Ik WDiag WT
we can efficiently compute based on the eigenvalue decomposition of A With
the eigenvalues at hand A can also be computed directly
Acknowledgments
This work has been supported in part by the Natural Science Foundations of China
the Google visiting faculty program and the Scholarship Award for Excellent Doctoral Student
granted by Ministry of Education

<<----------------------------------------------------------------------------------------------------------------------->>

title: 5074-low-rank-matrix-reconstruction-and-clustering-via-approximate-message-passing.pdf

Low-rank matrix reconstruction and clustering via
approximate message passing
Ryosuke Matsushita
NTT DATA Mathematical Systems Inc.
1F Shinanomachi Rengakan 35
Shinanomachi Shinjuku-ku Tokyo
Japan
matsur8@gmail.com
Toshiyuki Tanaka
Department of Systems Science
Graduate School of Informatics Kyoto University
Yoshida Hon-machi Sakyo-ku Kyoto-shi
Japan
tt@i.kyoto-u.ac.jp
Abstract
We study the problem of reconstructing low-rank matrices from their noisy observations We formulate the problem in the Bayesian framework which allows
us to exploit structural properties of matrices in addition to low-rankedness such
as sparsity We propose an efficient approximate message passing algorithm derived from the belief propagation algorithm to perform the Bayesian inference for
matrix reconstruction We have also successfully applied the proposed algorithm
to a clustering problem by reformulating it as a low-rank matrix reconstruction
problem with an additional structural property Numerical experiments show that
the proposed algorithm outperforms Lloyd?s K-means algorithm
Introduction
Low-rankedness of matrices has frequently been exploited when one reconstructs a matrix from its
noisy observations In such problems there are often demands to incorporate additional structural
properties of matrices in addition to the low-rankedness In this paper we consider the case where
a matrix A0 Rm?N to be reconstructed is factored as A0 U0 U0 Rm?r V0 RN
and where one knows structural properties of the factors U0 and V0 a priori Sparseness
and non-negativity of the factors are popular examples of such structural properties
Since the properties of the factors to be exploited vary according to the problem it is desirable
that a reconstruction method has enough flexibility to incorporate a wide variety of properties The
Bayesian approach achieves such flexibility by allowing us to select prior distributions of U0 and V0
reflecting a priori knowledge on the structural properties The Bayesian approach however often
involves computationally expensive processes such as high-dimensional integrations thereby requiring approximate inference methods in practical implementations Monte Carlo sampling methods
and variational Bayes methods have been proposed for low-rank matrix reconstruction to meet this
requirement
We present in this paper an approximate message passing AMP based algorithm for Bayesian lowrank matrix reconstruction Developed in the context of compressed sensing the AMP algorithm reconstructs sparse vectors from their linear measurements with low computational cost and achieves
a certain theoretical limit AMP algorithms can also be used for approximating Bayesian inference with a large class of prior distributions of signal vectors and noise distributions These
successes of AMP algorithms motivate the use of the same idea for low-rank matrix reconstruction
The IterFac algorithm for the rank-one case has been derived as an AMP algorithm An AMP
algorithm for the general-rank case is proposed in which however can only treat estimation of
posterior means We extend their algorithm so that one can deal with other estimations such as the
maximum a posteriori MAP estimation It is the first contribution of this paper
As the second contribution we apply the derived AMP algorithm to K-means type clustering to
obtain a novel efficient clustering algorithm It is based on the observation that our formulation
of the low-rank matrix reconstruction problem includes the clustering problem as a special case
Although the idea of applying low-rank matrix reconstruction to clustering is not new our
proposed algorithm is to our knowledge the first that directly deals with the constraint that each
datum should be assigned to exactly one cluster in the framework of low-rank matrix reconstruction
We present results of numerical experiments which show that the proposed algorithm outperforms
Lloyd?s K-means algorithm when data are high-dimensional
Recently AMP algorithms for dictionary learning and blind calibration and for matrix reconstruction with a generalized observation model were proposed Although our work has some
similarities to these studies it differs in that we fix the rank rather than the ratio r/m when taking
the limit in the derivation of the algorithm Another difference is that our formulation
explained in the next section does not assume statistical independence among the components of
each row of U0 and V0 A detailed comparison among these algorithms remains to be made
Problem setting
Low-rank matrix reconstruction
We consider the following problem setting A matrix A0 Rm?N to be estimated is defined
by two matrices U0 Rm?r and V0 RN as
A0 U0 where Rr We consider the case where Observations of A0
are corrupted by additive noise Rm?N whose components Wi,j are Gaussian random
variables following Here is a noise variance parameter and denotes the
Gaussian distribution with mean a and variance The factor in the noise variance is introduced
to allow a proper scaling in the limit where and go to infinity in the same order which is
employed in deriving the algorithm An observed matrix A Rm?N is given by A A0
Reconstructing A0 and V0 from A is the problem considered in this paper
We take the Bayesian approach to address this problem in which one requires prior distributions
of variables to be estimated as well as conditional distributions relating observations with variables
to be estimated These distributions need not be the true ones because in some cases they are not
available so that one has to assume them arbitrarily and in some other cases one expects advantages
by assuming them in some specific manner in view of computational efficiencies In this paper we
suppose that one uses the true conditional distribution
V0
A U0
mN exp
where denotes the Frobenius norm Meanwhile we suppose that the assumed prior distributions of U0 and V0 denoted by p?U and p?V respectively may be different from the true distributions
pU and pV respectively
We restrict p?U and p?V to distributions of the form p?U p?u
and p?V p?v respectively which allows us to construct computationally efficient
algorithms When p?U and p?V the posterior distribution of given A is
A p?U
pV
exp
Prior probability density functions p?u and p?v can be improper that is they can integrate to
infinity as long as the posterior is proper We also consider cases where the assumed rank
may be different from the true rank We thus suppose that estimates and are of size
and respectively
We consider two problems appearing in the Bayesian approach The first problem which we call
the marginalization problem is to calculate the marginal posterior distributions given A
p?i,j ui vj
duk
dvl
These are used to calculate
the posterior mean E[U and the
marginal MAP estimates
MMAP
uMMAP
arg
max
v|A)dv
and
arg
max
p?i,j v|A)du Because
i,j
calculation of p?i,j ui vj typically involves high-dimensional integrations requiring high computational cost approximation methods are needed
The second problem which we call the MAP problem is to calculate the MAP estimate
arg maxU,V It is formulated as the following optimization problem
min MAP
U,V
where MAP is the negative logarithm of
MAP
A
log p?u ui
log p?v vj
Because A is a non-convex function of it is generally hard to find the global
optimal solutions of and therefore approximation methods are needed in this problem as well
Clustering as low-rank matrix reconstruction
A clustering problem can be formulated as a problem of low-rank matrix reconstruction Suppose that er where el is the vector whose lth component
is and the others are When V0 and U0 are fixed aj follows one of the Gaussian distributions
where
is the lth column of U0 We regard that each Gaussian
being the center of cluster and representing the cluster
distribution defines a cluster
assignment of the datum aj One can then perform clustering on the dataset aN by reconstructing U0 and V0 from A aN under the structural constraint that every row of V0
should belong to where is an assumed number of clusters
Let us consider maximum likelihood estimation arg maxU,V or equivalently MAP esti?r
mation with the improper uniform prior distributions p?u and p?v
The corresponding MAP problem is
min
A
subject to vj
When satisfies the constraints the objective function A
aj
I(vj el is the sum of squared distances each of which is between a datum and the center of
the cluster that the datum is assigned to The optimization problem its objective function and
clustering based on it are called in this paper the K-means problem the K-means loss function and
the K-means clustering respectively
One can also use the marginal MAP estimation for clustering If U0 and V0 follow p?U and p?V respectively the marginal MAP estimation is optimal in the sense that it maximizes the expectation of
accuracy with respect to Here accuracy is defined as the fraction of correctly assigned data
among all data We call the clustering using approximate marginal MAP estimation the maximum
accuracy clustering even when incorrect prior distributions are used
Previous work
Existing methods for approximately solving the marginalization problem and the MAP problem
are divided into stochastic methods such as Markov-Chain Monte-Carlo methods and deterministic
ones A popular deterministic method is to use the variational Bayesian formalism The variational
Bayes matrix factorization approximates the posterior distribution as the product
VB
of two functions pVB
and pV which are determined so that the Kullback-Leibler
VB
VB
divergence from pU pV to is minimized Global minimization of the KL divergence is difficult except for some special cases so that an iterative method to obtain a local
minimum is usually adopted Applying the variational Bayes matrix factorization to the MAP problem one obtains the iterated conditional modes ICM algorithm which alternates minimization of
MAP over for fixed and minimization over for fixed
The representative algorithm to solve the K-means problem approximately is Lloyd?s K-means algorithm Lloyd?s K-means algorithm is regarded as the ICM algorithm It alternates minimization
of the K-means loss function over for fixed and minimization over for fixed iteratively
Algorithm Lloyd?s K-means algorithm
ntl
I(vjt el
tl
ljt+1 arg
min
tl
aj
aj I(vjt el
ntl
vjt+1 elt+1
Throughout this paper we represent an algorithm by a set of equations as in the above This representation means that the algorithm begins with a set of initial values and repeats the update of the
variables using the equations presented until it satisfies some stopping criteria Lloyd?s K-means
algorithm begins with a set of initial assignments er?}N This algorithm easily gets
stuck in local minima and its performance heavily depends on the initial values of the algorithm
Some methods for initialization to obtain a better local minimum are proposed
Maximum accuracy clustering can be solved approximately by using the variational Bayes matrix
factorization since it gives an approximation to the marginal posterior distribution of vj given A.
Proposed algorithm
Approximate message passing algorithm for low-rank matrix reconstruction
We first discuss the general idea of the AMP algorithm and advantages of the AMP algorithm compared with the variational Bayes matrix factorization The AMP algorithm is derived by approximating the belief propagation message passing algorithm in a way thought to be asymptotically exact for
large-scale problems with appropriate randomness Fixed points of the belief propagation message
passing algorithm correspond to local minima of the KL divergence between a kind of trial function
and the posterior distribution Therefore the belief propagation message passing algorithm can
be regarded as an iterative algorithm based on an approximation of the posterior distribution which
is called the Bethe approximation The Bethe approximation can reflect dependence of random variables dependence between and in in our problem to some extent Therefore one
can intuitively expect that performance of the AMP algorithm is better than that of the variational
Bayes matrix factorization which treats and as if they were independent in
An important property of the AMP algorithm aside from its efficiency and effectiveness is that
one can predict performance of the algorithm accurately for large-scale problems by using a set of
equations called the state evolution Analysis with the state evolution also shows that required
iteration numbers are even when the problem size is large Although we can present the state
evolution for the algorithm proposed in this paper and give a proof of its validity like we do
not discuss the state evolution here due to the limited space available
We introduce a one-parameter extension of the posterior distribution to treat the marginalization problem and the MAP problem in a unified manner It is defined as follows
exp
A p?U
pV
which is proportional to where is the parameter When
is reduced to In the limit the distribution concentrates on the
maxima of An algorithm for the marginalization problem on is particularized to the algorithms for the marginalization problem and for the MAP problem for the original
posterior distribution by letting and respectively The AMP algorithm
for the marginalization problem on is derived in a way similar to that described in
as detailed in the Supplementary Material
In the derived algorithm the values of variables But btu,m Rm??r Bvt
btv,N RN tu tv utm Rm??r
vN
RN S1t Sm
and T1t TNt are calculated iteratively where the superscript represents iteration numbers Variables with a negative
iteration number are defined as The algorithm is as follows
Algorithm
But
AV
Tj
tu
Tj
uti btu,i tu p?u
Sit G(btu,i tu p?u
Si tv
A
Si
Bvt
vjt+1 btv,j tv p?v
Tjt+1 G(btv,j tv p?v
Algorithm is almost symmetric in and Equations and update quantities
related to the estimates of U0 and V0 respectively The algorithm requires an initial value and
begins with Tj0 The functions Rr and
which have a Rr as a parameter are defined by
where is the normalized of defined by
exp log
One can see that is the mean of the distribution and that is its
covariance matrix scaled by The function need not be differentiable everywhere
Algorithm works if is differentiable at for which one needs to calculate in
running the algorithm
We assume in the rest of this section the convergence of Algorithm although the convergence is
not guaranteed in general Let Bu Bv
be the converged values
Si Tj and
of the respective variables First consider running Algorithm with The marginal posterior
distribution is then approximated as
vj
p?i,j ui vj q?(ui
v,j
u,i
Since
of
and
respectively the
and vj are the means
u,i
v,j
posterior mean E[U A)dU dV is approximated as
E[U
and vjMMAP are approximated as
The marginal MAP estimates uMMAP
vjMMAP arg max
v,j
uMMAP
arg max
u,i
Taking the limit in Algorithm yields an algorithm for the MAP problem In this case
the functions and are replaced with
arg min log
One may calculate from the Hessian of log at denoted by
via the identity
This identity follows from the implicit function theorem
under some additional assumptions and helps in the case where the explicit form of is
not available The MAP estimate is approximated by
Properties of the algorithm
Algorithm has several plausible properties First it has a low computational cost The computational cost per iteration is O(mN which is linear in the number of components of the matrix
A. Calculation of and is performed O(N times per iteration The constant
factor depends on and Calculation of for generally involves an r?-dimensional numerical integration although they are not needed in cases where an analytic expression of the integral
is available and cases where the variables take only discrete values Calculation of involves
minimization over an r?-dimensional vector When log is a convex function and is positive
semidefinite this minimization problem is convex and can be solved at relatively low cost
Second Algorithm has a form similar to that of an algorithm based on the variational Bayesian
matrix factorization In fact if the last terms on the right-hand sides of the four equations in
and are removed the resulting algorithm is the same as an algorithm based on the variational
Bayesian matrix factorization proposed in and in particular the same as the ICM algorithm when
Note however that only treats the case where the priors p?u and p?v are multivariate
Gaussian distributions Note that additional computational cost for these extra terms is O(m
which is insignificant compared with the cost of the whole algorithm which is O(mN
Third when one deals with the MAP problem the value of MAP may increase in iterations of Algorithm The following proposition however guarantees optimality of the output of
Algorithm in a certain sense if it has converged
Proposition Let Sm
TN be a fixed point of the AMP algorithm
for the MAP problem and suppose that Si and Tj are positive semidefinite Then
is a global minimum of MAP and is a global minimum of MAP
The proof is in the Supplementary Material The key to the proof is the following reformulation
MAP
Tjt
arg min
tr
If Tjt is positive semidefinite the second term of the minimand is the negative squared pseudometric between and which is interpreted as a penalty on nearness to the temporal estimate
Positive semidefiniteness of Sit and Tjt holds in almost all cases In fact we only have
to assume since is a scaled covariance matrix of
which is positive semidefinite It follows from Proposition that any fixed point of the
AMP algorithm is also a fixed point of the ICM algorithm It has two implications Execution
of the ICM algorithm initialized with the converged values of the AMP algorithm does not improve
MAP The AMP algorithm has not more fixed points than the ICM algorithm The
second implication may help the AMP algorithm avoid getting stuck in bad local minima
Clustering via AMP algorithm
One can use the AMP algorithm for the MAP problem to perform the K-means clustering by letting
p?u and p?v el Noting that p?v is piecewise constant with
respect to and hence p?v is almost everywhere we obtain the following algorithm
Algorithm AMP algorithm for the K-means clustering
AV tu
But
Bvt
A tv
tv btv,j
vjt+1 arg
min
But
It is initialized with an assignment er?}N Algorithm is rewritten as follows
ntl
ljt+1 arg
I(vjt el
tl
aj I(vjt el
ntl
2m
tl I(vjt el
aj
nl
nl
min
vjt+1 elt+1
The parameter appearing in
algorithm does not exist in the
K-means clustering problem In
the
fact appears because A2ij Sit was estimated by Sit in deriving Algorithm
which can be justified for large-sized problems In practice we propose using A
as a temporary estimate of at tth iteration While the AMP algorithm for the Kmeans clustering updates the value of in the same way as Lloyd?s K-means algorithm it performs
assignments of data to clusters in a different way In the AMP algorithm in addition to distances
from data to centers of clusters the assignment at present is taken into consideration in two ways
A datum is less likely to be assigned to the cluster that it is assigned to at present Data are
more likely to be assigned to a cluster whose size at present is smaller The former can intuitively be
understood by observing that if vjt el one should take account of the fact that the cluster center
tl is biased toward aj The term 2m(ntl I(vjt el in corrects this bias which as it
should be is inversely proportional to the cluster size
The AMP algorithm for maximum accuracy clustering is obtained by letting and p?v be
a discrete distribution on After the algorithm converges arg maxv vj
gives the final cluster assignment of the jth datum and gives the estimate of the cluster centers
Numerical experiments
We conducted numerical experiments on both artificial and real data sets to evaluate performance
of the proposed algorithms for clustering In the experiment on artificial data sets we set
were generated according to the
and and let Cluster centers
multivariate Gaussian distribution I). Cluster assignments were generated
according to the uniform distribution on er For fixed and we generated
problem instances and solved them with five algorithms Lloyd?s K-means algorithm K-means
the AMP algorithm for the K-means clustering AMP-KM the variational Bayes matrix factorization for maximum accuracy clustering VBMF-MA the AMP algorithm for maximum accuracy
clustering AMP-MA and the K-means The K-means updates the variables in the same
way as Lloyd?s K-means algorithm with an initial value chosen in a sophisticated manner For the
other algorithms initial values vj0 were randomly generated from the same distribution as We used the true prior distributions of and for maximum accuracy clustering
We ran Lloyd?s K-means algorithm and the K-means until no change was observed We ran the
AMP algorithm for the K-means clustering until either or is satisfied
This is because we observed oscillations of assignments of a small number of data For the other
two algorithms we terminated the iteration when and
were met or the number of iterations exceeded We then evaluated
the following performance measures for the obtained solution
N1
where a
Normalized K-means loss aj a
aj
Accuracy maxP I(P vj where the maximization is taken over all
r-by-r permutation matrices We used the Hungarian algorithm to solve this maximization problem efficiently
Number of iterations needed to converge
We calculated the averages and the standard deviations of these performance measures over
instances We conducted the above experiments for various values of
Figure shows the results The AMP algorithm for the K-means clustering achieves the smallest Kmeans loss among the five algorithms while the Lloyd?s K-means algorithm and K-means show
large K-means losses for We emphasize that all the three algorithms are aimed to minimize
the same K-means loss and the differences lie in the algorithms for minimization The AMP algorithm for maximum accuracy clustering achieves the highest accuracy among the five algorithms It
also shows fast convergence In particular the convergence speed of the AMP algorithm for maximum accuracy clustering is comparable to that of the AMP algorithm for the K-means clustering
when the two algorithms show similar accuracy This is in contrast to the common observation that the variational Bayes method often shows slower convergence than the ICM algorithm
K-means
AMP-KM
VBMF-MA
AMP-MA
K-means
K-means
AMP-KM
VBMF-MA
AMP-MA
K-means
Accuracy
Normalized K-means loss
18
18
K-means
AMP-KM
VBMF-MA
AMP-MA
K-means
Number of iterations
Accuracy
AMP-KM
VBMF-MA
AMP-MA
18
Iteration number
Figure Performance for different Normalized K-means loss Accuracy
Number of iterations needed to converge Dynamics for Average accuracy at each
iteration is shown Error bars represent standard deviations
K-means
AMP-KM
K-means
AMP-KM
Accuracy
Normalized K-means loss
Number of trials
Number of trials
Figure Performance measures in real-data experiments Normalized K-means loss Accuracy The results for the trials are shown in the descending order of performance for AMP-KM
The worst two results for AMP-KM are out of the range
In the experiment on real data we used the ORL Database of Faces which contains images
of human faces ten different images of each of distinct subjects Each image consists of
92 pixels whose value ranges from to We divided images into
clusters with the K-means and the AMP algorithm for the K-means clustering We adopted the
initialization method of the K-means also for the AMP algorithm because random initialization
often yielded empty clusters and almost all data were assigned to only one cluster The parameter
was estimated in the way proposed in Subsection We ran trials with different initial values
and Figure summarizes the results
The AMP algorithm for the K-means clustering outperformed the standard K-means algorithm
in 48 out of the trials in terms of the K-means loss and in 47 trials in terms of the accuracy
The AMP algorithm yielded just one cluster with all data assigned to it in two trials The attained
minimum value of K-means loss is with the K-means and with the AMP algorithm
The accuracies at these trials are with the K-means and with the AMP algorithm The
average number of iterations was with the K-means and with the AMP algorithm These
results demonstrate efficiency of the proposed algorithm on real data

<<----------------------------------------------------------------------------------------------------------------------->>

title: 3812-streaming-k-means-approximation.pdf

Streaming k-means approximation
Nir Ailon
Google Research
nailon@google.com
Ragesh Jaiswal
Columbia University
rjaiswal@gmail.com
Claire Monteleoni
Columbia University
cmontel@ccls.columbia.edu
Abstract
We provide a clustering algorithm that approximately optimizes the k-means objective in the one-pass streaming setting We make no assumptions about the
data and our algorithm is very light-weight in terms of memory and computation This setting is applicable to unsupervised learning on massive data sets or
resource-constrained devices The two main ingredients of our theoretical work
are a derivation of an extremely simple pseudo-approximation batch algorithm
for k-means based on the recent k-means in which the algorithm is allowed
to output more than centers and a streaming clustering algorithm in which batch
clustering algorithms are performed on small inputs fitting in memory and combined in a hierarchical manner Empirical evaluations on real and simulated data
reveal the practical utility of our method
Introduction
As commercial social and scientific data sources continue to grow at an unprecedented rate it is
increasingly important that algorithms to process and analyze this data operate in online or one-pass
streaming settings The goal is to design light-weight algorithms that make only one pass over the
data Clustering techniques are widely used in machine learning applications as a way to summarize
large quantities of high-dimensional data by partitioning them into clusters that are useful for
the specific application The problem with many heuristics designed to implement some notion of
clustering is that their outputs can be hard to evaluate Approximation guarantees with respect to
some reasonable objective are therefore useful The k-means objective is a simple intuitive and
widely-cited clustering objective for data in Euclidean space However although many clustering
algorithms have been designed with the k-means objective in mind very few have approximation
guarantees with respect to this objective
In this work we give a one-pass streaming algorithm for the k-means problem We are not aware
of previous approximation guarantees with respect to the k-means objective that have been shown
for simple clustering algorithms that operate in either online or streaming settings We extend work
of Arthur and Vassilvitskii to provide a bi-criterion approximation algorithm for k-means
in the batch setting They define a seeding procedure which chooses a subset of points from a
batch of points and they show that this subset gives an expected O(log k))-approximation to the kmeans objective This seeding procedure is followed by Lloyd?s algorithm1 which works very well
in practice with the seeding The combined algorithm is called k-means and is an O(log k))approximation algorithm in expectation.2 We modify k-means to obtain a new algorithm kmeans which chooses a subset of O(k log points and we show that the chosen subset of
Department of Computer Science Research supported by DARPA award
Center for Computational Learning Systems
Lloyd?s algorithm is popularly known as the k-means algorithm
Since the approximation guarantee is proven based on the seeding procedure alone for the purposes of this
exposition we denote the seeding procedure as k-means
points gives a constant approximation to the k-means objective Apart from giving us a bi-criterion
approximation algorithm our modified seeding procedure is very simple to analyze
defines a divide-and-conquer strategy to combine multiple bi-criterion approximation
algorithms for the k-medoid problem to yield a one-pass streaming approximation algorithm for
k-median We extend their analysis to the k-means problem and then use k-means and k-means
in the divide-and-conquer strategy yielding an extremely efficient single pass streaming algorithm
with an log k))-approximation guarantee where log log is the number of input
points in the stream and is the amount of work memory available to the algorithm Empirical
evaluations on simulated and real data demonstrate the practical utility of our techniques
Related work
There is much literature on both clustering algorithms
ORSS06 and streaming algorithms
There has also been work on combining these settings designing clustering algorithms that operate in the streaming setting Our work is inspired by
that of Arthur and Vassilvitskii and Guha which we mentioned above
and will discuss in further detail k-means the seeding procedure in had previously been
analyzed by under special assumptions on the input data
In order to be useful in machine learning applications we are concerned with designing algorithms
that are extremely light-weight and practical k-means is efficient very simple and performs
well in practice There do exist constant approximations to the k-means objective in the nonstreaming setting such as a local search technique due to A number of works
CMTS02 give constant approximation algorithms for the related k-median problem in which the objective is to minimize the sum of distances of the points to
their nearest centers rather than the square of the distances as in k-means and the centers must be
a subset of the input points It is popularly believed that most of these algorithms can be extended to
work for the k-means problem without too much degredation of the approximation however there
is no formal evidence for this yet Moreover the running times of most of these algorithms depend
worse than linearly on the parameters and which makes these algorithms less useful in practice As future work we propose analyzing variants of these algorithms in our streaming clustering
algorithm with the goal of yielding a streaming clustering algorithm with a constant approximation
to the k-means objective
Finally it is important to make a distinction from some lines of clustering research which involve
assumptions on the data to be clustered Common assumptions include data and
data that admits a clustering with well separated means in ORSS06 Recent
work assumes a target clustering for the specific application and data set that is close
to any constant approximation of the clustering objective In contrast we prove approximation
guarantees with respect to the optimal k-means clustering with no assumptions on the input data.5
As in our probabilistic guarantees are only with respect to randomness in the algorithm
Preliminaries
The k-means clustering problem is defined as follows Given points Rd and a weight
function
the goal is to find a2subset such that the following quantity is
minimized x?X where denotes the distance of to the nearest
point in C. When the subset is clear from the context we denote this distance by Also
for two points denotes the distance between and The subset is alternatively
called a clustering of and is called the potential function corresponding to the clustering We
will use the term center to refer to any C.
For a comprehensive survey of streaming results and literature refer to
In recent independent work Aggarwal Deshpande and Kannan extend the seeding procedure of
k-means to obtain a constant factor approximation algorithm which outputs centers They use similar
techniques to ours but reduce the number of centers by using a stronger concentration property
It may be interesting future work to analyze our algorithm in special cases such as well-separated clusters
For the unweighted case we can assume that for all
Definition Competitive ratio b-approximation Given an algorithm for the k-means problems let be the potential of the clustering returned by on some input set which is implicit
and let COP denote the potential of the optimal clustering COP Then the competitive ratio is
defined to be the worst case ratio The algorithm is said to be b-approximation algorithm
if
COP
OP
The previous definition might be too strong for an approximation algorithm for some purposes For
example the clustering algorithm performs poorly when it is constrained to output centers but it
might become competitive when it is allowed to output more centers
Definition b)-approximation We call an algorithm b)-approximation for the kmeans problem if it outputs a clustering with ak centers with potential such that in
OP
the worst case Where a
Note that for simplicity we measure the memory in terms of the words which essentially means that
we assume a point in Rd can be stored in space
k-means The advantages of careful and liberal seeding
The k-means algorithm is an expected log k)-approximation algorithm In this section we
extend the ideas in to get an O(log O(1))-approximation algorithm Here is the kmeans algorithm
Choose an initial center c1 uniformly at random from
Repeat times
Choose the next center ci selecting ci with probability D(xD(x
x?X
here denotes the distances to the subset of points chosen in the previous rounds
Algorithm k-means
In the original definition of k-means in the above algorithm is followed by Lloyd?s
algorithm The above algorithm is used as a seeding step for Lloyd?s algorithm which is known
to give the best results in practice On the other hand the theoretical guarantee of the k-means
comes from analyzing this seeding step and not Lloyd?s algorithm So for our analysis we focus on
this seeding step The running time of the algorithm is O(nkd
In the above algorithm denotes the set of given points and for any point denotes the
distance of this point from the nearest center among the centers chosen in the previous rounds To
get an O(log O(1))-approximation algorithm we make a simple change to the above algorithm
We first set up the tools for analysis These are the basic lemmas from We will need the
following definition first
Definition Potential a set Given a clustering
its potential with respect to some set A
is denoted by and is defined as x?A where is the distance of the
point from the nearest point in C.
Lemma Lemma Let A be an arbitrary cluster in COP and let be the clustering
with just one center chosen uniformly at random from A. Then Exp[?C COP
Corollary Let A be an arbitrary cluster in COP and let be the clustering with just one
center which is chosen uniformly at random from A. Then Pr[?C 8?COP
Proof The proof follows from Markov?s inequality
Lemma Lemma Let A be an arbitrary cluster in COP and let be an arbitrary
clustering If we add a random center to from A chosen with D2 weighting to get then
Exp[?C COP
Corollary Let A be an arbitrary cluster in COP and let be an arbitrary clustering If
we add a random center to from A chosen with D2 weighting to get then Pr[?C
32 COP
We will use k-means and the above two lemmas to obtain a O(log O(1))-approximation
algorithm for the k-means problem Consider the following algorithm
Choose log centers independently and uniformly at random from
Repeat times
Choose log centers independently and with probability D(xD(x
x?X
here denotes the distances to the subset of points chosen in the previous rounds
Algorithm k-means
Note that the algorithm is almost the same as the k-means algorithm except that in each round
of choosing centers we pick O(log centers rather than a single center The running time of the
above algorithm is clearly O(ndk log
Let A Ak denote the set of clusters in the optimal clustering COP Let denote the
clustering after ith round of choosing centers Let Aic denote the subset of clusters A such that
A Aic 32 COP
We call this subset of clusters the covered clusters Let Aiu A\Aic be the subset of uncovered
clusters The following simple lemma shows that with constant probability step of k-means
picks a center such that at least one of the clusters gets covered or in other words Let us
call this event E.
Lemma Pr[E
Proof The proof easily follows from Corollary
Let Xci A?Aic A and let Xui Xci Now after the ith round either Xci Xui
or otherwise In the former case using Corollary we show that the probability of covering an
uncovered cluster in the round is large In the latter case we will show that the current set
of centers is already competitive with constant approximation ratio Let us start with the latter case
Lemma If event occurs and for any Xci Xui then
Proof We get the main result using the following sequence of inequalities Xci
Xui Xci Xci 32 COP Xci 64 COP using the definition of Xci
Lemma If for any Xci Xui then
Ac
Proof Note that in the round the probability that a center is chosen from a cluster
Aic is
at least Ci
Conditioned on this event with probability at least any of the
Xc
centers chosen in round satisfies 32 COP for some uncovered cluster
A Aiu This means that with probability at least any of the chosen centers in round
satisfies 32 COP for some uncovered cluster A Aiu This further implies that
with probability at least at least one of the chosen centers in round satisfies
32 COP for some uncovered cluster A Aiu
We use the above two lemmas to prove our main theorem
Theorem k-means is a O(log O(1))-approximation algorithm
Proof From Lemma we know that event Aic occurs Given this suppose for
any after the ith round Xc Xu Then from Lemma we have
If no such exist then from Lemma we get that the probability that there exists a
cluster A A such that A is not covered even after rounds(i.e end of the algorithm is at most
So with probability at least the algorithm covers all the clusters in A.
In this case from Lemma we have 32 COP
We have shown that k-means is a randomized algorithm for clustering which with probability at
least gives a clustering with competitive ratio 64
A single pass streaming algorithm for k-means
In this section we will provide a single pass streaming algorithm The basic ingredients for the algorithm is a divide and conquer strategy defined by which uses bi-criterion approximation algorithms in the batch setting We will use k-means which is a O(log k))-approximation
algorithm and k-means which is a O(log O(1))-approximation algorithm to construct a single
pass streaming O(log k)-approximation algorithm for k-means problem In the next subsection we
develop some of the tools needed for the above
A streaming a,b)-approximation for k-means
We will show that a simple streaming divide-and-conquer scheme analyzed by with
respect to the k-medoid objective can be used to approximate the k-means objective First we
present the scheme due to where in this case we use k-means-approximating algorithms as input
Inputs Point set Rd Let
Number of desired clusters
A an b)-approximation algorithm to the k-means objective
A an approximation algorithm to the k-means objective
Divide into groups S1 S2
For each
Run A on Si to get ak centers Ti ti2
Denote the induced clusters of Si as Si1 Si2
Sw T1 T2 with weights w(tij Sij
Run A on Sw to get a centers
Return
Algorithm Streaming divide-and-conquer clustering
First note that when every batch Si has size nk this algorithm takes one pass and O(a nk
memory Now we will give an approximation guarantee
Theorem The algorithm above outputs a clustering that is an 2b 1))approximation to the k-means objective
The a approximation of the desired number of centers follows directly from the approximation
property of A with respect to the number of centers since A is the last algorithm to be run It
remains to show the approximation of the k-means objective The proof which appears in the
Appendix involves extending the analysis of to the case of the k-means objective
Using the exposition in Dasgupta?s lecture notes of the proof due to our
extension is straightforward and differs in the following ways from the k-medoid analysis
The k-means objective involves squared distance as opposed to k-medoid in which the
distance is not squared so the triangle inequality cannot be invoked directly We replace it
with an application of the triangle inequality followed by everywhere
it occurs introducing several factors of
Cluster centers are chosen from Rd for the k-means problem so in various parts of the
proof we save an approximation a factor of from the k-medoid problem in which cluster
centers must be chosen from the input data
Using k-means and k-means in the divide-and-conquer strategy
In the previous subsection we saw how a b)-approximation algorithm A and an approximation algorithm A can be used to get a single pass 2b 1))-approximation
streaming algorithm We now have two randomized algorithms k-means which with probability
at least is a log 64)-approximation algorithm and k-means which is a O(log k))approximation algorithm the approximation factor being in expectation We can now use these
two algorithms in the divide-and-conquer strategy to obtain a single pass streaming algorithm
We use the following as algorithms as A and A in the divide-and-conquer strategy
A Run k-means on the data log times independently and pick the clustering
with the smallest cost
Run k-means
Weighted versus non-weighted Note that k-means and k-means are approximation algorithms
for the non-weighted case for all points On the other hand in the divide-andconquer strategy we need the algorithm A to work for the weighted case where the weights are
integers Note that both k-means and k-means can be easily generalized for the weighted case
when the weights are integers Both algorithms compute probabilities based on the cost with respect
to the current clustering This cost can be computed by taking into account the weights For the
analysis we can assume points with multiplicities equal to the integer weight of the point The
memory required remains logarithmic in the input size including the storing the weights
Analysis With probability at least log n1 algorithm A is a log 64)approximation algorithm Moreover the space requirement remains logarithmic
in the input size In
step of
Algorithm we run A on batches of data Since each batch is of size nk the number of
batches is the probability
that A is a log 64)-approximation algorithm for all of these
batches is at least
Conditioned on this event the divide-and-conquer strategy
gives a O(log k)-approximation algorithm The memory required is O(log(k nk times the
logarithm of the input size Moreover the algorithm has running time O(dnk log log
Improved memory-approximation tradeoffs
We saw in the last section how to obtain a single-pass cbb approximation for k-means using
first an b)-approximation on input blocks and then an approximation on the union of the
output center
sets where is some global constant The optimal memory required for this scheme
was O(a This immediately implies a tradeoff between the memory requirements growing
like the number of centers outputted which is a and the approximation to the potential which
is cbb with respect to the optimal solution using centers A more subtle tradeoff is possible by a
recursive application of the technique in multiple levels Indeed the b)-approximation could be
broken up in turn into two levels and so on This idea was used in Here we make a
more precise account of the tradeoff between the different parameters
Assume we have subroutines for performing bi approximation for k-means in batch mode for
we will choose a1 ar b1 br later We will hold buffers B1 Br as
work areas where the size of buffer Bi is Mi In the topmost level we will divide the input into
equal blocks of size M1 and run our b1 approximation algorithm on each block Buffer B1
will be repeatedly reused for this task and after each application of the approximation algorithm
the outputted set of at most ka1 centers will be added to B2 When B2 is filled we will run
the b2 approximation algorithm on the data and add the ka2 outputted centers to B3 This
will continue until buffer Br fills and the ar br approximation algorithm outputs the final ar
centers Let ti denote the number of times the i?th level algorithm is executed Clearly we have
ti kai for For the last stage we have tr which means that
Mr Mr and generally ti Mr r?i But
M1
we must also have t1 implying
a1 In order to minimize the total memory
Mi under the last constraint using standard arguments in multivariate analysis we must have
M1 Mr or in other words Mi nk a1
for all
The resulting one-pass algorithm will have an approximation guarantee of ar b1 br using
a straightforward extension of the result in the previous section and memory requirement of at most
rn1/r
Assume now that we are in the realistic setting in which the available memory is of fixed size
We will choose below and for each we choose to either run k-means
or the repeated k-means algorithm A in the previous subsection bi O(log
or log for each For we choose k-means ar br O(log we
are interested in outputting exactly centers as the final solution Let denote the number of
We assume all quotients are integers for simplicity of the proof but note that fractional blocks would arise
in practice
35
Cost in units of
Batch Lloyds
Divide and Conquer with km and
Divide and Conquer with
45
Cost in units of
Cost in units of
Batch Lloyds
Divide and Conquer with km and
Divide and Conquer with
Batch Lloyds
Online Lloyds
Divide and Conquer with km and
Divide and Conquer with
Figure Cost Mixtures of gaussians simulation Cloud data Spam data
indexes such that bi log By the above discussion the memory is
used optimally if rn1/r log k)q/r in which case the final approximation guarantee will be
log k)r?q for some global We concentrate on the case growing polynomially in
say for some In this case the memory optimality constraint implies for
large enough regardless of the choice of This implies that the final approximation guarantee
is best if in other words we choose the repeated k-means for levels and
k-means for level Summarizing we get
Theorem If there is access to memory of size for some fixed then for sufficiently
large the best application of the multi-level scheme described above is obtained by running
log log levels and choosing the repeated k-means for all but the last level in which
k-means is chosen The resulting algorithm is a randomized one-pass streaming approximation
to k-means with an approximation ratio of
log for some global The running
time of the algorithm is O(dnk log log
We should compare the above multi-level streaming algorithm with the state-of-art terms of
memory approximation tradeoff streaming algorithm for the k-median problem Charikar
Callaghan and Panigrahy give a one-pass streaming algorithm for the k-median problem
which gives a constant factor approximation and uses O(k?poly log(n memory The main problem
with this algorithm from a practical point of view is that the average processing time per item is
large It is proportional to the amount of memory used which is poly-logarithmic in This might
be undesirable in practical scenarios where we need to process a data item quickly when it arrives In
contrast the average per item processing time using the divide-and-conquer-strategy is constant and
furthermore the algorithm can be pipelined data items can be temporarily stored in a memory
buffer and quickly processed before the the next memory buffer is filled So even if can
be extended to the k-means setting streaming algorithms based on the divide-and-conquer-strategy
would be more interesting from a practical point of view
Experiments
Datasets In our discussion denotes the number of points in the data denotes the dimension
and denotes the number of clusters Our first evaluation detailed in Tables and Figure
compares our algorithms on the following data norm25 is synthetic data generated in the following manner we choose random vertices from a dimensional hypercube of side length
We then add gaussian random points with variance around each of these points.8 So for this
data and The optimum cost for is The UCI Cloud
dataset consists of cloud cover data Here and The UCI Spambase
dataset is data for an e-mail spam detection task Here and 58
To compare against a baseline method known to be used in practice we used Lloyd?s algorithm
commonly referred to as the k-means algorithm Standard Lloyd?s algorithm operates in the batch
setting which is an easier problem than the one-pass streaming setting so we ran experiments with
this algorithm to form a baseline We also compare to an online version of Lloyd?s algorithm
however the performance is worse than the batch version and our methods for all problems so we
Testing clustering algorithms on this simulation distribution was inspired by
BL
OL
BL
OL
BL
OL
BL
OL
BL
OL
BL
OL
Table Columns have the clustering cost and columns have time in sec norm25 dataset
Cloud dataset Spambase dataset
Memory
levels
Cost
Time
Memory
levels
Cost
Time
26
Memory
levels
Cost
Time
34
Table Multi-level hierarchy evaluation Cloud dataset A subset of norm25 dataset
Spambase dataset The memory size decreases as the number of
levels of the hierarchy increases levels means running batch k-means on the data
do not include it in our plots for the real data sets.9 Tables shows average k-means cost over
random restarts for the randomized algorithms all but Online Lloyd?s for these algorithms
BL Batch Lloyd?s initialized with random centers in the input data and run to convergence.10
OL Online Lloyd?s
The simple 1-stage divide and conquer algorithm of Section
The simple 1-stage divide and conquer algorithm of Section The sub-algorithms
used are A run k-means log times and pick best clustering and A is k-means In our
context k-means and k-means are only the seeding step not followed by Lloyd?s algorithm
In all problems our streaming methods achieve much lower cost than Online Lloyd?s for all settings
of and lower cost than Batch Lloyd?s for most settings of including the correct in
The gains with respect to batch are noteworthy since the batch problem is less constrained
than the one-pass streaming problem The performance of and is comparable
Table shows an evaluation of the one-pass multi-level hierarchical algorithm of Section on the
different datasets simulating different memory restrictions Although our worst-case theoretical results imply an exponential clustering cost as a function of the number of levels our results show a far
more optimistic outcome in which adding levels and limiting memory actually improves the outcome We conjecture that our data contains enough information for clustering even on chunks that fit
in small buffers and therefore the results may reflect the benefit of the hierarchical implementation
Acknowledgements We thank Sanjoy Dasgupta for suggesting the study of approximation algorithms for k-means in the streaming setting for excellent lecture notes and for helpful discussions
Despite the poor performance we observed this algorithm is apparently used in practice see
We measured convergence by change in cost less than

<<----------------------------------------------------------------------------------------------------------------------->>

title: 1076-learning-sparse-perceptrons.pdf

Learning Sparse Perceptrons
Jeffrey C. Jackson
Mathematics Computer Science Dept
Duquesne University
Forbes Ave
Pittsburgh PA
jackson@mathcs.duq.edu
Mark W. Craven
Computer Sciences Dept
University of Wisconsin-Madison
West Dayton St.
Madison WI
craven@cs.wisc.edu
Abstract
We introduce a new algorithm designed to learn sparse perceptrons over input representations which include high-order features
Our algorithm which is based on a hypothesis-boosting method
is able to PAC-learn a relatively natural class of target concepts
Moreover the algorithm appears to work well in practice on a set
of three problem domains the algorithm produces classifiers that
utilize small numbers of features yet exhibit good generalization
performance Perhaps most importantly our algorithm generates
concept descriptions that are easy for humans to understand
Introd uction
Multi-layer perceptron MLP learning is a powerful method for tasks such as concept classification However in many applications such as those that may involve
scientific discovery it is crucial to be able to explain predictions Multi-layer perceptrons are limited in this regard since their representations are notoriously difficult
for humans to understand We present an approach to learning understandable
yet accurate classifiers Specifically our algorithm constructs sparse perceptrons
single-layer perceptrons that have relatively few non-zero weights Our algorithm for learning sparse perceptrons is based on a new hypothesis boosting algorithm Freund Schapire Although our algorithm was initially developed
from a learning-theoretic point of view and retains certain theoretical guarantees it
PAC-learns the class of sparse perceptrons it also works well in practice Our experiments in a number of real-world domains indicate that our algorithm produces
perceptrons that are relatively comprehensible and that exhibit generalization performance comparable to that of backprop-trained MLP's Rumelhart
and better than decision trees learned using Quinlan
Learning Sparse Perceptrons
We contend that sparse perceptrons unlike MLP's are comprehensible because they
have relatively few parameters and each parameter describes a simple linear
relationship As evidence that sparse perceptrons are comprehensible consider that
such linear functions are commonly used to express domain knowledge in fields such
as medicine Spackman and molecular biology Stormo
Sparse Perceptrons
A perceptron is a weighted threshold over the set of input features and over higherorder features consisting of functions operating on only a limited number of the
input features Informally a sparse perceptron is any perceptron that has relatively
few non-zero weights For our later theoretical results we will need a more precise
definition of sparseness which we develop now Consider a Boolean function I
Let Ck be the set of all conjunctions of at most of the inputs
to I. includes the conjunction of inputs which we take as the identically
function All of the functions in map to and every conjunction in
occurs in both a positive sense represents true and a negated sense
represents true Then the function I is a k-perceptron if there is some integer
such that where for all hi Ck and sign(y is undefined
if and is y/lyl otherwise Note that while we have not explicitly shown any
weights in our definition of a k-perceptron I integer weights are implicitly present
in that we allow a particular hi Ck to appear more than once in the sum defining
I. In fact it is often convenient to think of a k-perceptron as a simple linear
discriminant function with integer weights defined over a feature space with O(nk
features one feature for each element of Ck
We call a given collection of conjunctions hi a k-perceptron representation of
the corresponding function I and we call the size of the representation We define
the size of a given k-perceptron function I as the minimal size of any k-perceptron
representation of I. An s-sparse k-perceptron is a k-perceptron I such that the size
of I is at most We denote by PI the set of Boolean functions over which
can be represented as k-perceptrons and we define Pk Un Pi:. The subclass of
s-sparse k-perceptrons is denoted by We are also interested in the class
of k-perceptrons with real-valued weights at most of which are non-zero
The Learning Algorithm
In this section we develop our learning algorithm and prove certain performance
guarantees Our algorithm is based on a recent hypothesis boosting algorithm
that we describe after reviewing some basic learning-theory terminology
PAC Learning and Hypothesis Boosting
Following Valiant we say that a function class such as Pk for fixed
is strongly PAC-learnable if there is an algorithm A and a polynomial function
PI such that for any positive and any I the target junction and any
probability distribution over the domain of I with probability at least algorithm A(EX(f produces a function the hypothesis such that
Pr[PrD[/(x I hex The outermost probability is over the random choices
made by the EX oracle and any random choices made by A. Here EX(f denotes
an oracle that when queried chooses a vector of input values with probability
and returns the pair to A. The learning algorithm A must run in time
PI where is the length of the input vector to I and is the size of
J. C. JACKSON M. W. CRAVEN
AdaBoost
Input training set of examples of function
is Y)-approximate
Algorithm
weak learning algorithm WL that
In(m
for all xES l/m
for to do
for all XES Di(X
invoke WL on and distribution Di producing weak hypothesis hi
L:l=l
L:z Di(X
for all XES if then
enddo
Output sign
Figure The AdaBoost algorithm
the algorithm is charged one unit of time for each call to EX. We sometimes
call the function output by A an approximator strong approximator to
with respect to D. If is PAC-learnable by an algorithm A that outputs only
hypotheses in class then we say that is PAC-learnable by If is PAClearnable for where'P2 is a polynomial function then is
weakly PA C-learnable and the output hypothesis in this case is called a weak
approximator
Our algorithm for finding sparse perceptrons is as indicated earlier based on the
notion of hypothesis boosting The specific boosting algorithm we use Figure
is a version of the recent AdaBoost algorithm Freund Schapire In the
next section we apply AdaBoost to boost a weak learning algorithm for into
a strong learner for AdaBoost is given a set of examples of a function
and a weak learning algorithm WL which takes
for a given must be bounded by an inverse polynomial in nand Adaf300st
runs for stages At each stage it creates a probability distribution
Di over the training set and invokes WL to find a weak hypothesis hi with respect
to Di note that an example oracle EX(j Di can be simulated given Di and S).
At the end of the stages a final hypothesis is output this is just a weighted
threshold over the weak hypotheses hi I T}. If the weak learner succeeds
in producing a Y)-approximator at each stage then AdaBoost's final hypothesis
is guaranteed to be consistent with the training set Freund Schapire
PAC-Learning Sparse k-Perceptrons
We now show that sparse k-perceptrons are PAC learnable by real-weighted kperceptrons having relatively few nonzero weights Specifically ignoring log factors
is learnable by for any constant We first show that given a training
set for any we can efficiently find a consistent This consistency algorithm is the basis of the algorithm we later apply to empirical learning
problems We then show how to turn the consistency algorithm into a PAC learning
algorithm Our proof is implicit in somewhat more general work by Freund
although he did not actually present a learning algorithm for this class or analyze
Learning Sparse Perceptrons
the sample size needed to ensure f-approximation as we do Following Freund we
begin our development with the following lemma Goldmann
Lemma Goldmann Hastad Razhorov For I and
any set 01 functions with the same domain and range il I can be represented as
sign(L::=l where hi then lor any probability distribution
over there is some hi such that PrD[f(x
If we specialize this lemma by taking Ck recall that Ck is the set of conjunctions of at most input features of then this implies that for any I and
any probability distribution over the input features of I there is some hi Ck
that weakly approximates I with respect to D. Therefore given a training set
and distribution that has nonzero weight only on instances in the following
simple algorithm is a weak learning algorithm for Pk exhaustively test each of the
O(nk possible conjunctions of at most features until we find a conjunction that
approximates I with respect to we can efficiently compute the approximation of a conjunction hi by summing the values of over those inputs where hi
a
and I agree Any such conjunction can be returned as the weak hypothesis The
above lemma proves that if I is a k-perceptron then this exhaustive search must
succeed at finding such a hypothesis Therefore given a training set of examples
of any s-sparse k-perceptron I AdaBoost run with the above weak learner will after stages produce a hypothesis consistent with the training set Because
each stage adds one weak hypothesis to the output hypothesis the final hypothesis
will be a real-weighted k-perceptron with at most nonzero weights
We can convert this consistency algorithm to a PAC learning algorithm as follows
First given a finite set of functions it is straightforward to show the following
see Haussler
Lemma Let be a finite set ollunctions over a domain For any function
lover any probability distribution over and any positive and given a
set ofm examples drawn consecutively from EX(f where
In IFI then Pr[3h I Ix where
the outer probability is over the random choices made by
The consistency algorithm above finds a consistent hypothesis in where
Also based on a result of Bruck it can be shown that In IP~I
kr log Therefore ignoring log factors a randomly-generated training set
of size O(kS4 If is sufficient to guarantee that with high probability our algorithm
will produce an f-approximator for any s-sparse k-perceptron target In other words
the following is a PAC algorithm for compute sufficiently large but polynomial
in the PAC parameters draw examples from EX(f to create a training
set and run the consistency algorithm on this training set
So far we have shown that sparse k-perceptrons are learnable by sparse perceptron
hypotheses with potentially polynomially-many more weights In practice of
course we expect that many real-world classification tasks cannot be performed
exactly by sparse perceptrons In fact it can be shown that for certain reasonable
definitions of noisy sparse perceptrons loosely functions that are approximated
reasonably well by sparse perceptrons the class of noisy sparse k-perceptrons is
still PAC-learnable This claim is based on results of Aslam and Decatur
who present a noise-tolerant boosting algorithm In fact several different boosting
algorithms could be used to learn Pk,s Freund We have chosen to use
AdaBoost because it seems to offer significant practical advantages particularly in
terms of efficiency Also our empirical results to date indicate that our algorithm
J. C. JACKSON M. W. CRAVEN
works very well on difficult presumably noisy real-world problems However
one potential advantage of basing the algorithm on one of these earlier boosters
instead of AdaBoost is that the algorithm would then produce a perceptron with
integer weights while still maintaining the sparseness guarantee of the AdaBoostbased algorithm
Practical Considerations
We turn now to the practical details of our algorithm which is based on the consistency algorithm above First it should be noted that the theory developed above
works over discrete input domains Boolean or nominal-valued features Thus in
this paper we consider only tasks with discrete input features Also because the
algorithm uses exhaustive search over all conjunctions of size learning time depends exponentially on the choice of In this study we to use throughout
since this choice results in reasonable learning times
Another implementation concern involves deciding when the learning algorithm
should terminate The consistency algorithm uses the size of the target function
in calculating the number of boosting stages Of course such size information is
not available in real-world applications and in fact the target function may not be
exactly representable as a sparse perceptron In practice we use cross validation
to determine an appropriate termination point To facilitate comprehensibility we
also limit the number of boosting stages to at most the number of weights that
would occur in an ordinary perceptron for the task For similar reasons we also
modify the criteria used to select the weak hypothesis at each stage so that simple
features are preferred over conjunctive features In particular given distribution
at some stage for each hi Ck we compute a correlation We
then mUltiply each high-order feature's correlation by The hi with the largest
resulting correlation serves as the weak hypothesis for stage
Empirical Evaluation
In our experiments we are interested in assessing both the generalization ability
and the complexity of the hypotheses produced by our algorithm We compare our
algorithm to ordinary perceptrons trained using backpropagation Rumelhart
multi-layer perceptrons trained using backpropagation and decision trees
induced using the system Quinlan We use in our experiments as
a representative of symbolic learning algorithms Symbolic algorithms are widely
believed to learn hypotheses that are more comprehensible than neural networks
Additionally to test the hypothesis that the performance of our algorithm can be
explained solely by its use of second-order features we train ordinary perceptrons
using feature sets that include all pairwise conjunctions as well as the ordinary
features To test the hypothesis that the performance of our algorithm can be
explained by its use of relatively few weights we consider ordinary perceptrons
which have been pruned using a variant of the Optimal Brain Damage OBD
algorithm Le Cun In our version of OBD we train a perceptron until
the stopping criteria are met prune the weight with the smallest salience and then
iterate the process We use a validation set to decide when to stop pruning weights
For each training set we use cross-validation to select the number of hidden units
or for the MLP's and the pruning confidence level for the
trees We use a validation set to decide when to stop training for the MLP's
We evaluate our algorithm using three real-world domains the voting data set from
the UC-Irvine database a promoter data set which is a more complex superset of
Learning Sparse Perceptrons
domain
voting
promoter
coding
boosting
Ta ble 11est set accuracy
perceptrons
multi-layer ordinary 2nd-order
Table Hypothesis complexity weights
perceptrons
domain
boosting multi-layer ordinary 2nd-order
voting
promoters
41
protein coding
52
pruned
pruned
59
37
UC-Irvine one and a data set in which the task is to recognize protein-coding
regions in DNA Craven Shavlik We remove the physician-fee-freeze
feature from the voting data set to make the problem more difficult We conduct
our experiments using a lO-fold cross validation methodology except for in the
protein-coding domain Because of certain domain-specific characteristics of this
data set we use 4-fold cross-validation for our experiments with it
Table reports test-set accuracy for each method on all three domains We measure the statistical significance of accuracy differences using a paired two-tailed
t-test The symbol marks results in cases where another algorithm is less accurate than our boosting algorithm at the level of significance No other
algorithm is significantly better than our boosting method in any of the domains
From these results we conclude that our algorithm exhibits good generalization
performance on number of interesting real-world problems and the generalization performance of our algorithm is not explained solely by its use of second-order
features nor is it solely explained by the sparseness of the perceptrons it produces
An interesting open question is whether perceptrons trained with both pruning and
second-order features are able to match the accuracy of our algorithm we plan to
investigate this question in future work
Table reports the average number of weights for all of the perceptrons For all
three problems our algorithm produces perceptrons with fewer weights than the
MLP's the ordinary perceptrons and the perceptrons with second-order features
The sizes of the OBD-pruned perceptrons and those produced by our algorithm
are comparable for all three domains Recall however that for all three tasks
the perceptrons learned by our algorithm had significantly better generalization
performance than their similar-sized OBD-pruned counterparts We contend that
the sizes of the perceptrons produced by our algorithm are within the bounds of
what humans can readily understand In the biological literature for example linear
discriminant functions are frequently used to communicate domain knowledge about
sequences of interest These functions frequently involve more weights than the
perceptrons produced by our algorithm We conclude therefore that our algorithm
produces hypotheses that are not only accurate but also comprehensible
We believe that the results on the protein-coding domain are especially interesting
The input representation for this problem consists of nominal features representing consecutive bases in a DNA sequence In the regions of DNA that encode
proteins the positive examples in our task non-overlapping triplets of consecu
J. C. JACKSON M. W. eRA VEN
tive bases represent meaningful words called codons In previous work Craven
Shavlik it has been found that a feature set that explicitly represents
codons results in better generalization than a representation of just bases However we used the bases representation in our experiments in order to investigate the
ability of our algorithm to select the right second-order features Interestingly
nearly all of the second-order features included in our sparse perceptrons represent
conjunctions of bases that are in the same codon This result suggests that our
algorithm is especially good at selecting relevant features from large feature sets
Future Work
Our present algorithm has a number of limitations which we plan to address Two
areas of current research are generalizing the algorithm for application to problems
with real-valued features and developing methods for automatically suggesting highorder features to be included in our algorithm's feature set
Acknowledgements
Mark Craven was partially supported by ONR grant Jeff Jackson
was partially supported by NSF grant

<<----------------------------------------------------------------------------------------------------------------------->>

title: 1194-an-apobayesian-relative-of-winnow.pdf

An Apobayesian Relative of Winnow
Nick Littlestone
NEC Research Institute
Independence Way
Princeton NJ
Chris Mesterharm
NEC Research Institute
Independence Way
Princeton NJ
Abstract
We study a mistake-driven variant of an on-line Bayesian learning algorithm similar to one studied by Cesa-Bianchi Helmbold
and Panizza This variant only updates its state learns
on trials in which it makes a mistake The algorithm makes binary
classifications using a linear-threshold classifier and runs in time linear in the number of attributes seen by the learner We have been
able to show theoretically and in simulations that this algorithm
performs well under assumptions quite different from those embodied in the prior of the original Bayesian algorithm It can handle
situations that we do not know how to handle in linear time with
Bayesian algorithms We expect our techniques to be useful in
deriving and analyzing other apobayesian algorithms
Introduction
We consider two styles of on-line learning In both cases learning proceeds in a
sequence of trials In each trial a learner observes an instance to be classified
makes a prediction of its classification and then observes a label that gives the
correct classification One style of on-line learning that we consider is Bayesian
The learner uses probabilistic assumptions about the world embodied in a prior
over some model class and data observed in past trials to construct a probabilistic
model embodied in a posterior distribution over the model class The learner uses
this model to make a prediction in the current trial When the learner is told the
correct classification of the instance the learner uses this information to update the
model generating a new posterior to be used in the next trial
In the other style of learning that we consider the attention is on the correctness
of the predictions rather than on the model of the world The internal state of the
An Apobayesian Relative o!Winnow
learner is only changed when the learner makes a mistake when the prediction fails
to match the label We call such an algorithm mistake-driven Such algorithms are
often called conservative in the computational learning theory literature There is a
simple way to derive a mistake-driven algorithm from anyon-line learning algorithm
we restrict our attention in this paper to deterministic algorithms The derived
algorithm is just like the original algorithm except that before every trial it makes
a record of its entire state and after every trial in which its prediction is correct
it resets its state to match the recorded state entirely forgetting the intervening
trial Typically this is actually implemented not by making such a record but by
merely omitting the step that updates the state For example if some algorithm
keeps track of the number of trials it has seen then the mistake-driven version of
this algorithm will end up keeping track of the number of mistakes it has made
Whether the original or mistake-driven algorithm will do better depends on the task
and on how the algorithms are evaluated
We will start with a Bayesian learning algorithm that we call SBSB and use this
procedure to derive a mistake-driven variant SASB Note that the variant cannot
be expected to be a Bayesian learning algorithm at least in the ordinary sense
since a Bayesian algorithm would make a prediction that minimizes the Bayes risk
based on all the available data and the mistake-driven variant has forgotten quite
a bit We call such algorithms apobayesian learning algorithms This name is
intended to suggest that they are derived from Bayesian learning algorithms but
are not themselves Bayesian Our algorithm SASB is very close to an algorithm
of We study its application to different tasks than they do analyzing its
performance when it is applied to linearly separable data as described below
In this paper instances will be chosen from the instance space l}n for some
Thus instances are composed of boolean attributes We consider only two
category classifications tasks with predictions and labels chosen from I
We obtain a bound on the number of mistakes SASB makes that is comparable to
bounds for various Winnow family algorithms given in As for those
algorithms the bound holds under the assumption that the points labeled are
linearly separable from the points labeled and the bound depends on the size of
the gap between the two classes See Section for a definition of The mistake
bound for SASB is or log While this bound has an extra factor of log not
present in the bounds for the Winnow algorithms SASB has the advantage of not
needing any parameters The Winnow family algorithms have parameters and the
algorithms mistake bounds depend on setting the parameters to values that depend
on Often the value of will not be known by the learner We expect the
techniques used to obtain this bound to be useful in analyzing other apobayesian
learning algorithms
A number of authors have done related research regarding worst-case on-line
loss bounds including Simulation experiments involving a
Bayesian algorithm and a mistake-driven variant are described in That
paper provides useful background for this paper Note that our present analysis
techniques do not apply to the apobayesian algorithm studied there The closest of
the original Winnow family algorithms to SASB appears to be the Weighted MaJority algorithm which was analyzed for a case similar to that considered
in this paper in One should get a roughly correct impression of SASB if
N. Littlestone and C. Mesterharm
one thinks of it as a version of the Weighted Majority algorithm that learns its
parameters
In the next section we describe the Bayesian algorithm that we start with In
Section we discuss its mistake-driven apobayesian variant Section mentions
some simulation experiments using these algorithms and Section is the conclusion
A Bayesian Learning Algorithm
To describe the Bayesian learning algorithm we must specify a family of distributions over and a prior over this family of distributions We parameterize
the distributions with parameters 8n chosen from The
parameter 8n gives the probability that the label is and the parameter 8i gives
the probability that the ith attribute matches the label Note that the probability
that the ith attribute is given that the label is equals the probability that the
ith attribute is given that the label is We speak of this linkage between the
probabilities for the two classes as a symmetry condition With this linkage the
observation of a point from either class will affect the posterior distribution for both
classes It is perhaps more typical to choose priors that allow the two classes to be
treated separately so that the posterior for each class giving the probability of elements of conditioned on the label depends only on the prior and on observations
from that class The symmetry condition that we impose appears to be important
to the success of our analysis of the apobayesian variant of this algorithm Though
we impose this condition to derive the algorithm it turns out that the apobayesian
variant can actually handle tasks where this condition is not satisfied
We choose a prior on that gives probability to the set of all elements
8n for which at most one of 81 does not equal
The prior is uniform on this set Note that for any in this set only a single attribute has a probability other than of matching the label and thus only a single
attribute is relevant Concentrating on this set turns out to lead to an apobayesian
algorithm that can in fact handle more than one relevant attribute and that performs particularly well when only a small fraction of the attributes are relevant
This prior is related to to the familiar Naive Bayes model which also assumes
that the attributes are conditionally independent given the labels However in the
typical Naive Bayes model there is no restriction to a single relevant attribute and
the symmetry condition linking the two classes is not imposed
Our prior leads to the following algorithm The name SBSB stands for Symmetric
Bayesian Algorithm with Singly-variant prior for Bernoulli distribution
Algorithm SBSB Algorithm SBSB maintains counts Si of the number of times
each attribute matches the label a count of the number of times the label is
and a count of the number of trials
Initialization
Prediction
Si
t-O
tt-O
Predict given instance if and only if
XiCSi+l)+Clixi)(t-Si+1
i=l
Update
for
i=l
and for each if
then
Si
Si
An Apobayesian Relative of Winnow
An Apobayesian Algorithm
We construct an apobayesian algorithm by converting algorithm SBSB into a
mistake-driven algorithm using the standard conversion given in the introduction
We call the resulting learning algorithm SASBj we have replaced Bayesian with
Apobayesian in the acronym
In the previous section we made assumptions made about the generation of the
instances and labels that led to SBSB and thence to SASB These assumptions
have served their purpose and we now abandon them In analyzing the apobayesian
algorithm we do not assume that the instances and labels are generated by some
stochastic process Instead we assume that the instance-label pairs in all of the
trials are linearly-separable that is that there exist some WI Wn and such
that for every instance-label pair we have WiXi when and
WiXi when We actually make a somewhat stronger assumption
given in the following theorem which gives our bound for the apobayesian algorithm
Theorem Suppose that Yi and Ii for and that Yi
I Suppose that bo bi and let bi bo Suppose that algorithm
SASB is run on a sequence of trials such that the instance and label in each
trial satisfy YiXi Ii bo if and YiXi Ii bi if
Then the number of mistakes made by SASB will be bounded by
log
We have space to say only a little about how the derivation of this bound proceeds
Details are given in
In analyzing SASB we work with an abstract description of the associated algorithm
SBSB This algorithm starts with a prior on as described above We represent
this with a density Po. Then after each trial it calculates a new posterior density
Pt(O t-d8kP(X'YI~k where Pt is the density after trial and ylO is the
pt-d
conditional probability ofthe instance and label observed in trial given Thus
we can think of the algorithm as maintaining a current distribution on that is
initially the prior SASB is similar but it leaves the current distribution unchanged
when a mistake is not made For there to exist a finite mistake bound there must
exist some possible choice for the current distribution for which SASB would make
perfect predictions should it ever arrive at that distribution We call any such
distribution leading to perfect predictions a possible target distribution It turns out
that the separability condition given in Theorem guarantees that a suitable target
distribution exists The analysis proceeds by showing that for an appropriate choice
of a target density the relative entropy of the current distribution with respect to
the target distribution log(p Pt decreases by at least some amount
whenever a mistake is made Since the relative entropy is never negative the
number of mistakes is bounded by the initial relative entropy divided by R. This
form of analysis is very similar to the analysis of the various members of the Winnow
family in
The same technique can be applied to other apobayesian algorithms The abstract
update of Pt given above is quite general The success of the analysis depends on
conditions on Po and ylO that we do not have space here to discuss
N. LittLestone and C. Mesterharm
Optimal
SBSB
SASB
SASB voting
Optimal
SBSB
SASB
SASB voting
Trials
Trials
Figure Comparison of SASB with SBSB
Simulation Experiments
The bound of the previous section was for perfectly linearly-separable data We
have also done some simulation experiments exploring the performance of SASB on
non-separable data and comparing it with SBSB and with various other mistakedriven algorithms A sample comparison of SASB with SBSB is shown in Figure
In each experimental run we generated trials with the instances and labels
chosen randomly according to a distribution specified by Ok
Ok+l where 01 On+l are interpreted as specified in Section
is the number of attributes and and are as specified at the top of each
plot The line labeled optimal shows the performance obtained by an optimal
predictor that knows the distribution used to generate the data ahead of time and
thus does not need to do any learning The lines labeled SBSB and SASB show
the performance of the corresponding learning algorithms The lines labeled SASB
voting show the performance of SASB with the addition of a voting procedure
described in This procedure improves the asymptotic mistake rate of the
algorithms Each line on the graph is the average of runs Each line plots the
cumulative number of mistakes made by the algorithm from the beginning of the run
as a function of the number of trials
In the left hand plot there is only relevant attribute This is exactly the case that
SBSB is intended for and it does better than SASB In right hand plot there are
relevant attributes SBSB appears unable to take advantage of the extra information
present in the extra relevant attributes but SASB successfully does
Comparison of SASB and previous Winnow family algorithms is still in progress
and we defer presenting details until a clearer picture has been obtained SASB and
the Weighted Majority algorithm often perform similarly in simulations Typically
as one would expect the Weighted Majority algorithm does somewhat better than
An Apobayesian Relative of Winnow
SASB when its parameters are chosen optimally for the particular learning task and
worse for bad choices of parameters
Conclusion
Our mistake bounds and simulations suggest that SASB may be a useful alternative
to the existing algorithms in the Winnow family Based on the analysis style and the
bounds SASB should perhaps itself be considered a Winnow family algorithm Further experiments are in progress comparing SASB with Winnow family algorithms
run with a variety of parameter settings
Perhaps of even greater interest is the potential application of our analytic techniques
to a variety of other apobayesian algorithms though as we have observed earlier
the techniques do not appear to apply to all such algorithms We have already
obtained some preliminary results regarding an interpretation of the Perceptron
algorithm as an apobayesian algorithm We are interested in looking for entirely
new algorithms that can be derived in this way and also in better understanding
the scope of applicability of our techniques All of the analyses that we have looked
at depend on symmetry conditions relating the probabilities for the two classes It
would be of interest to see what can be said when such symmetry conditions do not
hold In simulation experiments a mistake-driven variant of the standard
Naive Bayes algorithm often does very well despite the absence of such symmetry
in the prior that it is based on
Our simulation experiments and also the analysis of the related algorithm Winnow
suggest that SASB can be expected to handle some instance-label pairs inside
of the separating gap or on the wrong side especially if they are not too far on the
wrong side In particular it appears to be able to handle data generated according
to the distributions on which SBSB is based which do not in general yield perfectly
separable data
It is of interest to compare the capabilities of the original Bayesian algorithm with
the derived apobayesian algorithm When the data is stochastically generated in a
manner consistent with the assumptions behind the original algorithm the original
Bayesian algorithm can be expected to do better see for example Figure On
the other hand the apobayesian algorithm can handle data beyond the capabilities of the original Bayesian algorithm For example in the case we consider the
apobayesian algorithm can take advantage of the presence of more than one relevant
attribute even though the prior behind the original Bayesian algorithm assumes a
single relevant attribute Furthermore as for all of the Winnow family algorithms
the mistake bound for the apobayesian algorithm does not depend on details of the
behavior of the irrelevant attributes including redundant attributes
Instead of using the apobayesian variant one might try to construct a Bayesian
learning algorithm for a prior that reflects the actual dependencies among the attributes and the labels However it may not be clear what the appropriate prior is
It may be particularly unclear how to model the behavior of the irrelevant attributes Furthermore such a Bayesian algorithm may end up being computationally
expensive For example attempting to keep track of correlations among all pairs
of attributes may lead to an algorithm that needs time and space quadratic in the
number of attributes On the other hand if we start with a Bayesian algorithm that
N. Littlestone and C. Mesterharm
uses time and space linear in the number of attributes we can obtain an apobayesian
algorithm that still uses linear time and space but that can handle situations beyond
the capabilities of the original Bayesian algorithm
Acknowledgments
This paper has benefited from discussions with Adam Grove

<<----------------------------------------------------------------------------------------------------------------------->>

title: 2434-semi-definite-programming-by-perceptron-learning.pdf

Semidefinite Programming
by Perceptron Learning
Ralf Herbrich
Thore Graepel
Microsoft Research Ltd Cambridge UK
thoreg,rherb}@microsoft.com
Andriy Kharechko
John Shawe-Taylor
Royal Holloway University of London UK
ak03r,jst}@ecs.soton.ac.uk
Abstract
We present a modified version of the perceptron learning algorithm
PLA which solves semidefinite programs SDPs in polynomial
time The algorithm is based on the following three observations
Semidefinite programs are linear programs with infinitely many
linear constraints every linear program can be solved by a
sequence of constraint satisfaction problems with linear constraints
iii in general the perceptron learning algorithm solves a constraint
satisfaction problem with linear constraints in finitely many updates
Combining the PLA with a probabilistic rescaling algorithm which
on average increases the size of the feasable region results in a probabilistic algorithm for solving SDPs that runs in polynomial time
We present preliminary results which demonstrate that the algorithm works but is not competitive with state-of-the-art interior
point methods
Introduction
Semidefinite programming SDP is one of the most active research areas in optimisation Its appeal derives from important applications in combinatorial optimisation
and control theory from the recent development of efficient algorithms for solving
SDP problems and the depth and elegance of the underlying optimisation theory
which covers linear quadratic and second-order cone programming as special cases
Recently semidefinite programming has been discovered as a useful toolkit in machine
learning with applications ranging from pattern separation via ellipsoids to kernel
matrix optimisation and transformation invariant learning
Methods for solving SDPs have mostly been developed in an analogy to linear programming Generalised simplex-like algorithms were developed for SDPs but to
the best of our knowledge are currently merely of theoretical interest The ellipsoid
method works by searching for a feasible point via repeatedly halving an ellipsoid
that encloses the affine space of constraint matrices such that the centre of the ellipsoid is a feasible point However this method shows poor performance in practice
as the running time usually attains its worst-case bound A third set of methods
for solving SDPs are interior point methods These methods minimise a linear
function on convex sets provided the sets are endowed with self-concordant barrier
functions Since such a barrier function is known for SDPs interior point methods
are currently the most efficient method for solving SDPs in practice
Considering the great generality of semidefinite programming and the complexity of
state-of-the-art solution methods it is quite surprising that the forty year old simple
perceptron learning algorithm can be modified so as to solve SDPs In this
paper we present a combination of the perceptron learning algorithm PLA with a
rescaling algorithm originally developed for LPs that is able to solve semidefinite
programs in polynomial time We start with a short introduction into semidefinite
programming and the perceptron learning algorithm in Section In Section we
present our main algorithm together with some performance guarantees whose proofs
we only sketch due to space restrictions While our numerical results presented in
Section are very preliminary they do give insights into the workings of the algorithm
and demonstrate that machine learning may have something to offer to the field of
convex optimisation
For the rest of the paper we denote matrices and vectors by bold face upper and
lower case letters A and We shall use kxk to denote the unit length
vector in the direction of The notation A is used to denote Ax for all
that is A is positive semidefinite
Learning and Convex Optimisation
Semidefinite Programming
In semidefinite programming a linear objective function is minimised over the image
of an affine transformation of the cone of semidefinite matrices expressed by linear
matrix inequalities
minimise
x?R
c0
subject to
F0
Fi
where Rn and Fi Rm?m for all The following proposition shows
that semidefinite programs are a direct generalisation of linear programs
Proposition Every semidefinite program is a linear program with infinitely many
linear constraints
Proof Obviously the objective function in is linear in For any Rm define
the vector au F1 u0 Fn Then the constraints in can be written as
Rm
u0
Rm
This is a linear constraint in for all
au F0
of which there are infinitely many
Since the objective function is linear in we can solve an SDP by a sequence of
semidefinite constraint satisfaction problems CSPs introducing the additional constraint c0 c0 and varying c0 R. Moreover we have the following proposition
Proposition Any SDP can be solved by a sequence of homogenised semidefinite
CSPs of the following form
find
subject to
Gi
Algorithm Perceptron Learning Algorithm
Require A possibly infinite set A of vectors a Rn
Set and
while there exists a A such that x0t a do
a
end while
return
Proof In order to make F0 and c0 dependent on the optimisation variables we
introduce an auxiliary variable the solution to the original problem is given
by
Moreover we can repose the two linear constraints c0 and
as an LMI using the fact that a block-diagonal matrix is positive semi)definite
if and only if every block is positive semi)definite Thus the following matrices are
sufficient
F0
Fi
ci
G0 c0
Gi
Given an upper and a lower bound on the objective function repeated bisection can
be used to determine the solution in O(log steps to accuracy
In order to simplify notation we will assume that and whenever
we speak about a semidefinite CSP for an SDP in variables with Fi Rm?m
Perceptron Learning Algorithm
The perceptron learning algorithm PLA is an online procedure which finds a
linear separation of a set of points from the origin Algorithm In machine
learning this algorithm is usually applied to two sets and of points labelled
and by multiplying every data vector by its class label1 the resulting vector
often referred to as the weight vector in perceptron learning is then read as the
normal of a hyperplane which separates the sets and
A remarkable property of the perceptron learning algorithm is that the total number
of updates is independent of the cardinality of A but can be upper bounded simply
in terms of the following quantity
maxn maxn min a0
x?R
x?R
a?A
This quantity is known as the normalised margin of A in the machine learning
community or as the radius of the feasible region in the optimisation community
It quantifies the radius of the largest ball that can be fitted in the convex region
enclosed by all a A the so-called feasible set Then the perceptron convergence
theorem states that
For the purpose of this paper we observe that Algorithm solves a linear CSP where
the linear constraints are given by the vectors a A. Moreover by the last argument
we have the following proposition
Proposition If the feasible set has a positive radius then the perceptron learning
algorithm solves a linear CSP in finitely many steps
It is worth mentioning that in the last few decades a series of modified PLAs A
have been developed for a good overview which mainly aim at guaranteeing
Note that sometimes the update equation is given using the unnormalised vector a
Algorithm Rescaling algorithm
Require A maximal number of steps and a parameter
Set uniformly at random in kzk
for do
Pun smallest EV of
Find au such that
if does not exists then
Set Gi Gi return
end if
au au
end for
return unsolved
not only feasibility of the solution but also a lower bound on These
guarantees usually come at the price of a slightly larger mistake bound which we
shall denote by that is
Semidefinite Programming by Perceptron Learning
If we combine Propositions and together with Equation we obtain a perceptron algorithm that sequentially solves SDPs However there remain two problems
How do we find a vector a A such that a
How can we make the running time of this algorithm polynomial in the
description length of the data?2
In order to address the first problem we notice that A in Algorithm is not explicitly
given but is defined by virtue of
A Gn au G1 u0 Gn Rm
Hence finding a vector au A such that au is equivalent to identifying a
vector Rm such that
u0 Gi u0
One possible way of finding such a vector and consequently au for the current
solution in Algorithm is to calculate the eigenvector corresponding to the smallest
eigenvalue of if this eigenvalue is positive the algorithm stops and outputs
Note however that computationally easier procedures can be applied to find a
suitable Rm also Section
The second problem requires us to improve the dependency of the runtime from
to To this end we employ a probabilistic rescaling algorithm
Algorithm which was originally developed for LPs The purpose of this algorithm is to enlarge the feasible region terms of A Gn by a constant
factor on average which would imply a decrease in the number of updates of the
perceptron algorithm exponential in the number of calls to this rescaling algorithm
This is achieved by running Algorithm If the algorithm does not return unsolved
the rescaling procedure on the Gi has the effect that au changes into au au
for every Rm In order to be able to reconstruct the solution to the original
problem whenever we rescale the Gi we need to remember the vector used for
rescaling In Figure we have shown the effect of rescaling for three linear con2
Note that polynomial runtime is only guaranteed if A Gn is bounded by
a polynomial function of the description length of the data
Figure Illustration of the rescaling procedure Shown is the feasible region and
one feasible point before left and after left rescaling with the feasible point
straints in R3 The main idea of Algorithm is to find a vector that is close to
the current feasible region and hence leads to an increase in its radius when used for
rescaling The following property holds for Algorithm
Theorem Assume Algorithm did not return unsolved Let
be the
radius of the feasible set before rescaling and be the radius of the feasible set after
rescaling and assume that 4n
Then
with probability at most 34
4n
with probability at least
The probabilistic nature of the theorem stems from the fact that the rescaling can
only be shown to increase the size of the feasible region if the random initial value
already points sufficiently closely to the feasible region A consequence of this theorem is that on average the radius increases by Algorithm
combines rescaling and perceptron learning which results in a probabilistic polynomial runtime algorithm3 which alternates between calls to Algorithm and This
algorithm may return infeasible in two cases either Ti many calls to Algorithm
have returned unsolved or many calls of Algorithm together with rescaling have
not returned a solution Each of these two conditions can either happen because of
an unlucky draw of in Algorithm or because A Gn is too small
Following the argument in one can show that for min the total
probability of returning infeasible despite A Gn min cannot exceed
exp
Experimental Results
The experiments reported in this section fall into two parts Our initial aim was
to demonstrate that the method works in practice and to assess its efficacy on a
Note that we assume that the optimisation problem in line of Algorithm can be
solved in polynomial time with algorithms such as Newton-Raphson
Algorithm Positive Definite Perceptron Algorithm
Require G1 Gn Rm?m and maximal number of iteration
Set In
for do
Call Algorithm for at most A 4n
many updates
if Algorithm converged then return Bx
ln(?i
Set and Ti
34
for Ti do
Call Algorithm with and
if Algorithm returns then In yy goto the outer for-loop
end for
return infeasible
end for
return infeasible
benchmark example from graph bisection
These experiments would also indicate how competitive the baseline method is when
compared to other solvers The algorithm was implemented in MATLAB and all of
the experiments were run on machines The time taken can be compared
with a standard method SDPT3 partially implemented in but running under
MATLAB
We considered benchmark problems arising from semidefinite relaxations to the
MAXCUT problems of weighted graphs which is posed as finding a maximum weight
bisection of a graph The benchmark MAXCUT problems have the following relaxed
SDP form
subject to diag(C1 diag
minimise
x?Rn
F0
Fi
where Rn?n is the adjacency matrix of the graph with vertices
The benchmark used was provided by SDPLIB For this problem
and it is known that the optimal value of the objective function equals
The baseline method used the bisection approach to identify the critical
value of the objective referred to throughout this section as c0
Figure left shows a plot of the time per iteration against the value of c0 for the
first four iterations of the bisection method As can be seen from the plots the time
taken by the algorithm for each iteration is quite long with the time of the fourth
iteration being around seconds The initial value of for c0 was found
without an objective constraint and converged within secs The bisection then
started with the lower infeasible value of and the upper value of Iteration
was run with c0 but the feasible solution had an objective value of This
was found in just secs The second iteration used a value of c0 slightly
above the optimum of The third iteration was infeasible but since it was quite
far from the optimum the algorithm was able to deduce this fact quite quickly The
final iteration was also infeasible but much closer to the optimal value The running
time suffered correspondingly taking hours If we were to continue the next
iteration would also be infeasible but closer to the optimum and so would take even
longer
The first experiment demonstrated several things First that the method does indeed work as predicted secondly that the running times are very far from being
Time sec
Optimal value
Optimal value
Value of objective function
Value of objective function
Iterations
Figure Left Four iterations of the bisection method showing time taken per iteration outer for loop in Algorithm against the value of the objective constraint
Right Decay of the attained objective function value while iterating through Algorithm with a non-zero threshold of
competitive SDPT3 takes under seconds to solve this problem and thirdly that
the running times increase as the value of c0 approaches the optimum with those
iterations that must prove infeasibility being more costly than those that find a solution
The final observation prompted our first adaptation of the base algorithm Rather
than perform the search using the bisection method we implemented a non-zero
threshold on the objective constraint the while-statement in Algorithm The
value of this threshold is denoted following the notation introduced in
Using a value of ensured that when a feasible solution is found its objective
value is significantly below that of the objective constraint c0 Figure right
shows the values of c0 as a function of the outer for-loops iterations the algorithm
eventually approached its estimate of the optimal value at This is within
of the optimum though of course iterations could have been continued Despite
the clear convergence using this approach the running time to an accurate estimate
of the solution is still prohibitive because overall the algorithm took approximately
hours of CPU time to find its solution
A profile of the execution however revealed that up to of the execution time is
spent in the eigenvalue decomposition to identify Observe that we do not need a
minimal eigenvector to perform an update simply a vector satisfying
u0 G(x)u
Cholesky decomposition will either return satisfying or it will converge indicating that is psd and Algorithm has converged
Conclusions
Semidefinite programming has interesting applications in machine learning In turn
we have shown how a simple learning algorithm can be modified to solve higher
order convex optimisation problems such as semidefinite programs Although the
experimental results given here suggest the approach is far from computationally
competitive the insights gained may lead to effective algorithms in concrete applications in the same way that for example SMO is a competitive algorithm for solving
quadratic programming problems arising from support vector machines While the
optimisation setting leads to the somewhat artificial and inefficient bisection method
the positive definite perceptron algorithm excels at solving positive definite CSPs
as found in problems of transformation invariant pattern recognition as solved
by Semidefinite Programming Machines In future work it will be of interest to
consider the combined primal-dual problem at a predefined level of granularity so
as to avoid the necessity of bisection search
Acknowledgments We would like to thank J. Kandola J. Dunagan and A. Ambroladze for interesting discussions This work was supported by EPSRC under grant
number and by Microsoft Research Cambridge

<<----------------------------------------------------------------------------------------------------------------------->>

title: 5438-optimal-regret-minimization-in-posted-price-auctions-with-strategic-buyers.pdf

Optimal Regret Minimization in Posted-Price
Auctions with Strategic Buyers
Mehryar Mohri
Courant Institute and Google Research
Mercer Street
New York NY
Medina
Andres Munoz
Courant Institute
Mercer Street
New York NY
mohri@cims.nyu.edu
munoz@cims.nyu.edu
Abstract
We study revenue optimization learning algorithms for posted-price auctions with
strategic buyers We analyze a very broad family of monotone regret minimization
algorithms for this problem which includes the previously best known algorithm
and show
that no algorithm in that family admits a strategic regret more favorable
than We then introduce a new algorithm that achieves a strategic regret
differing from the lower bound only by a factor in O(log an exponential improvement upon the previous best algorithm Our new algorithm admits a natural
analysis and simpler proofs and the ideas behind its design are general We also
report the results of empirical evaluations comparing our algorithm with the previous state of the art and show a consistent exponential improvement in several
different scenarios
Introduction
Auctions have long been an active area of research in Economics and Game Theory Vickrey
Milgrom and Weber Ostrovsky and Schwarz In the past decade however the advent
of online advertisement has prompted a more algorithmic study of auctions including the design of
learning algorithms for revenue maximization for generalized second-price auctions or second-price
auctions with reserve Cesa-Bianchi Mohri and Mu?noz Medina He
These studies have been largely motivated by the widespread use of AdExchanges and the vast
amount of historical data thereby collected AdExchanges are advertisement selling platforms using second-price auctions with reserve price to allocate advertisement space Thus far the learning
algorithms proposed for revenue maximization in these auctions critically rely on the assumption
that the bids that is the outcomes of auctions are drawn according to some unknown distribution However this assumption may not hold in practice In particular with the knowledge that a
revenue optimization algorithm is being used an advertiser could seek to mislead the publisher by
under-bidding In fact consistent empirical evidence of strategic behavior by advertisers has been
found by Edelman and Ostrovsky This motivates the analysis presented in this paper of the
interactions between sellers and strategic buyers that is buyers that may act non-truthfully with the
goal of maximizing their surplus
The scenario we consider is that of posted-price auctions which albeit simpler than other mechanisms in fact matches a common situation in AdExchanges where many auctions admit a single
bidder In this setting second-price auctions with reserve are equivalent to posted-price auctions a
seller sets a reserve price for a good and the buyer decides whether or not to accept it that is to bid
higher than the reserve price In order to capture the buyer?s strategic behavior we will analyze an
online scenario at each time a price pt is offered by the seller and the buyer must decide to either
accept it or leave it This scenario can be modeled as a two-player repeated non-zero sum game with
incomplete information where the seller?s objective is to maximize his revenue while the advertiser
seeks to maximize her surplus as described in more detail in Section
The literature on non-zero sum games is very rich Nachbar Morris but much of
the work in that area has focused on characterizing different types of equilibria which is not directly
relevant to the algorithmic questions arising here Furthermore the problem we consider admits a
particular structure that can be exploited to design efficient revenue optimization algorithms
From the seller?s perspective this game can also be viewed as a bandit problem Kuleshov and Precup Robbins since only the revenue reward for the prices offered is accessible to
the seller Kleinberg and Leighton precisely studied this continuous bandit setting under the
assumption of an oblivious buyer that is one that does not exploit the seller?s behavior more precisely the authors assume that at each round the seller interacts with a different buyer The authors
presented a tight regret bound of log log for the scenario of a buyer holding a fixed valuation
and a regret bound of O(T when facing an adversarial buyer by using an elegant reduction to a
discrete bandit problem However as argued by Amin when dealing with a strategic
buyer the usual definition of regret is no longer meaningful Indeed consider the following example let the valuation of the buyer be given by and assume that an algorithm with sublinear
regret such as Exp3 Auer or UCB Auer is used for rounds by the
seller A possible strategy for the buyer knowing the seller?s algorithm would be to accept prices
only if they are smaller than some small value certain that the seller would eventually learn to offer
only prices less than If the buyer would considerably boost her surplus while in theory
the seller would have not incurred a large regret since in hindsight the best fixed strategy would
have been to offer price for all rounds This however is clearly not optimal for the seller The
stronger notion of policy regret introduced by Arora has been shown to be the appropriate one for the analysis of bandit problems with adaptive adversaries However for the example
just described a sublinear policy regret can be similarly achieved Thus this notion of regret is also
not the pertinent one for the study of our scenario
We will adopt instead the definition of strategic-regret which was introduced by Amin
precisely for the study of this problem This notion of regret also matches the concept of learning
loss introduced by Agrawal when facing an oblivious adversary Using this definition Amin
presented both upper and lower bounds for the regret of a seller facing a strategic
buyer and showed that the buyer?s surplus must be discounted over time in order to be able to
achieve sublinear regret?(see Section However the gap between the upper and lower bounds
they presented is in In the following we analyze a very broad family of monotone regret
minimization algorithms for this problem Section which includes the algorithm of Amin
and show that no algorithm in that family admits a strategic regret more favorable than
Next we introduce a nearly-optimal algorithm that achieves a strategic regret differing
from the lower bound at most by a factor in O(log Section This represents an exponential
improvement upon the existing best algorithm for this setting Our new algorithm admits a natural
analysis and simpler proofs A key idea behind its design is a method deterring the buyer from lying
that is rejecting prices below her valuation
Setup
We consider the following game played by a buyer and a seller A good such as an advertisement
space is repeatedly offered for sale by the seller to the buyer over rounds The buyer holds a
private valuation for that good At each round a price pt is offered by the
seller and a decision at is made by the buyer at takes value when the buyer accepts
to buy at that price otherwise We will say that a buyer lies whenever at while pt
At the beginning of the game the algorithm A used by the seller to set prices is announced to the
buyer Thus the buyer plays strategically against this algorithm The knowledge of A is a standard
assumption in mechanism design and also matches the practice in AdExchanges
For any define the discounted surplus of the buyer as follows
Sur(A
at pt
The value of the discount factor indicates the strength of the preference of the buyer for current
surpluses versus future ones The performance of a seller?s algorithm is measured by the notion of
strategic-regret Amin defined as follows
Reg(A
at pt
The buyer?s objective is to maximize his discounted surplus while the seller seeks to minimize his
regret Note that in view of the discounting factor the buyer is not fully adversarial The problem
consists of designing algorithms achieving sublinear strategic regret that is a regret in o(T
The motivation behind the definition of strategic-regret is straightforward a seller with access to
the buyer?s valuation can set a fixed price for the good close to this value The buyer having no
control on the prices offered has no option but to accept this price in order to optimize his utility
The revenue per round of the seller is therefore Since there is no scenario where higher revenue
can be achieved this is a natural setting to compare the performance of our algorithm
To gain more intuition about the problem let us examine some of the complications arising when
dealing with a strategic buyer Suppose the seller attempts to learn the buyer?s valuation by performing a binary search This would be a natural algorithm when facing a truthful buyer However
in view of the buyer?s knowledge of the algorithm for it is in her best interest to lie on the
initial rounds thereby quickly in fact exponentially decreasing the price offered by the seller The
seller would then incur an regret A binary search approach is therefore too aggressive Indeed an untruthful buyer can manipulate the seller into offering prices less than by lying about
her value even just once This discussion suggests following a more conservative approach In the
next section we discuss a natural family of conservative algorithms for this problem
Monotone algorithms
The following conservative pricing strategy was introduced by Amin Let p1
and If price pt is rejected at round the lower price pt is offered at the next
round If at any time price pt is accepted then this price is offered for all the remaining rounds We
will denote this algorithm by monotone The motivation behind its design is clear for a suitable
choice of the seller can slowly decrease the prices offered thereby pressing the buyer to reject
many prices
which is not convenient for her before obtaining a favorable price The authors present
an regret bound for this algorithm with
A more careful analysis shows
that this bound can be further tightened to when the discount factor is known to
the seller
Despite its sublinear regret the monotone algorithm remains sub-optimal for certain choices of
Indeed consider a scenario with For this setting the buyer would no longer have an
incentive to lie thus an algorithm such as binary search would achieve logarithmic
regret while the
regret achieved by the monotone algorithm is only guaranteed to be in
One may argue that the monotone algorithm is too specific since it admits a single parameter
and that perhaps a more complex algorithm with the same monotonic idea could achieve a more
favorable regret Let us therefore analyze a generic monotone algorithm Am defined by Algorithm
Definition For any buyer?s valuation define the acceptance time as the
first time a price offered by the seller using algorithm Am is accepted
Proposition For any decreasing sequence of prices pt there exists a truthful buyer with
valuation v0 such that algorithm Am suffers regret of at least
Reg(Am v0
T.
Proof By definition of the regret
We can
we have Reg(Am
consider two cases for some v0
and for every
In the former case we have Reg(Am v0 v0 which implies the statement of the
proposition Thus we can assume the latter condition
Algorithm Family of monotone algorithms
Algorithm Definition of Ar
the root of
while Offered prices less than do
Offer price pn
if Accepted then
else
Offer price pn for rounds
end if
end while
Let p1 and pt for
pt
Offer price
while Buyer rejects and do
pt
Offer price
end while
while do
Offer price
end while
Let be uniformly distributed over In view of Lemma Appendix we have
The right-hand side is minimized for
Plugging in this value yields
E[Reg(Am
which
implies
the
existence
of
with
Reg(A
We have thus shown that any monotone algorithm Am suffers a regret of at least even when
facing a truthful buyer A tighter lower bound can be given under a mild condition on the prices
offered
Definition A sequence pt is said to be convex if it verifies pt for
An instance of a convex sequence is given by the prices offered by the monotone algorithm A
seller offering prices forming a decreasing convex sequence seeks to control the number of lies of
the buyer by slowly reducing prices The following proposition gives a lower bound on the regret of
any algorithm in this family
Proposition Let pt be a decreasing convex sequence of prices There exists a valuation
v0
for the buyer such that the regret of the monotone algorithm defined by these prices is
where
The full proof of this proposition is given in Appendix The proposition shows that when the
discount factor is known the monotone algorithm is in fact asymptotically optimal in its class
The results just presented suggest that the dependency on cannot be improved by any monotone
algorithm In some sense this family of algorithms is too conservative Thus to achieve a more
favorable regret guarantee an entirely different algorithmic idea must be introduced In the next
section we describe a new algorithm that achieves a substantially more advantageous strategic regret
by combining the fast convergence properties of a binary search-type algorithm a truthful setting
with a method penalizing untruthful behaviors of the buyer
A nearly optimal algorithm
Let A be an algorithm for revenue optimization used against a truthful buyer Denote by the
tree associated to A after rounds That is is a full tree of height with nodes
labeled with the prices pn offered by A. The right and left children of are denoted by and
respectively The price offered when pn is accepted by the buyer is the label of while the
price offered by A if pn is rejected is the label of Finally we will denote the left and right
subtrees rooted at node by and respectively Figure depicts the tree generated by an
algorithm proposed by Kleinberg and Leighton which we will describe later
Figure Tree associated to the algorithm proposed in Kleinberg and Leighton Modified
tree with
Since the buyer holds a fixed valuation we will consider algorithms that increase prices only after a
price is accepted and decrease it only after a rejection This is formalized in the following definition
Definition An algorithm A is said to be consistent if maxn0 pn0 pn minn0 pn0
for any node
For any consistent algorithm A we define a modified algorithm Ar parametrized by an integer
designed to face strategic buyers Algorithm Ar offers the same prices as A but it is defined
with the following modification when a price is rejected by the buyer the seller offers the same
price for rounds The pseudocode of Ar is given in Algorithm The motivation behind the
modified algorithm is given by the following simple observation a strategic buyer will lie only if
she is certain that rejecting a price will boost her surplus in the future By forcing the buyer to reject
a price for several rounds the seller ensures that the future discounted surplus will be negligible
thereby coercing the buyer to be truthful
We proceed to formally analyze algorithm Ar In particular we will quantify the effect of the
parameter on the choice of the buyer?s strategy To do so a measure of the spread of the prices
offered by Ar is needed
Definition For any node define the right increment of as nr pr(n pn Similarly
define its left increment to be nl maxn0 pn pn0
The prices offered by Ar define a path in For each node in this path we can define time
to be the number of rounds needed for this node to be reached by Ar Note that since may
be greater than the path chosen by Ar might not necessarily reach the leaves of Finally
let be the function representing the surplus obtained by the buyer when playing an
optimal strategy against Ar after node is reached
Lemma The function satisfies the following recursive relation
max pn
Proof Define a weighted tree of nodes reachable by algorithm Ar We assign
weights to the edges in the following way if an edge on is of the form its weight
is set to be pn otherwise it is set to It is easy to see that the function evaluates
the weight of the longest path from node to the leafs of It thus follows from elementary
graph algorithms that equation holds
The previous lemma immediately gives us necessary conditions for a buyer to reject a price
Proposition For any reachable node if price pn is rejected by the buyer then the following
inequality holds
pn
Proof A direct implication of Lemma is that price pn will be rejected by the buyer if and only if
pn
However by definition the buyer?s surplus obtained by following any path in is bounded
above by In particular this is true for the path which rejects pr(n and accepts every price
PT
afterwards The surplus of this path is given by pbt where
pt
are the prices the seller would offer if price pr(n were rejected Furthermore since algorithm Ar is
consistent we must have pbt pr(n pn nr Therefore can be bounded as follows
pn nr
pn nr
We proceed to upper bound Since pn p0n nl for all n0 pn0 pn nl
and
pn nl
pn nl
t=t
Combining inequalities and we conclude that
pn
pn nr
pn nl
nl nr nl
pn
Rearranging the terms in the above inequality yields the desired result
pn
Let us consider the following instantiation of algorithm A introduced in Kleinberg and Leighton
The algorithm keeps track of a feasible interval initialized to and an increment
parameter initialized to The algorithm works in phases Within each phase it offers prices
a a until a price is rejected If price a is rejected then a new phase starts with
the feasible interval set to a a and the increment parameter set to This process
continues until a at which point the last phase starts and price a is offered for the
remaining rounds It is not hard to see that the number of phases needed by the algorithm is less
than dlog2 log2 A more surprising fact is that this algorithm has been shown to achieve regret
O(log log when the seller faces a truthful buyer We will show that the modification Ar of this
algorithm admits a particularly favorable regret bound We will call this algorithm PFSr penalized
fast search algorithm
Proposition For any value of and any the regret of algorithm PFSr admits
the following upper bound
Reg(PFSr vr log2
Note that for and the upper bound coincides with that of Kleinberg and Leighton
Proof Algorithm PFSr can accumulate regret in two ways the price offered pn is rejected in which
case the regret is or the price is accepted and its regret is pn
Let dlog2 log2 be the number of phases run by algorithm PFSr Since at most
different prices are rejected by the buyer one rejection per phase and each price must be rejected
for rounds the cumulative regret of all rejections is upper bounded by vKr
The second type of regret can also be bounded straightforwardly For any phase let and bi
denote the corresponding search parameter and feasible interval respectively If bi
regret accrued in the case where the buyer accepts a price in this interval is bounded
by bi
If on the other hand bi then it readily follows that pn bi for all prices pn
offered in phase Therefore the regret obtained in acceptance rounds is bounded by
Ni bi
bi Ni
where Ni
denotes the number of prices offered during the i-th round
Finally notice that in view of the algorithm?s definition every bi corresponds to a rejected price
Thus by Proposition there exist nodes ni not necessarily distinct such that pni bi and
bi pni
ni
It is immediate that nr and nl for any node thus we can write
bi Ni
T.
The last inequality holds since at most prices are offered by our algorithm Combining the bounds
for both regret types yields the result
When an upper bound on the discount factor is known to the seller he can leverage this information
and optimize upper bound with respect to the parameter
Theorem Let and argminr?1
For any
if the regret of PFSr satisfies
Reg(PFSr log cT v)(log2 log2
where log
The proof of this theorem is fairly technical and is deferred to the Appendix The theorem helps
us define conditions under which logarithmic regret can be achieved Indeed if log
log1 using the inequality e?x valid for all we obtain
log2
log T.
log
It then follows from Theorem that
Reg(PFSr log log cT v)(log2 log2 log T.
Let us compare the regret bound given by Theorem with the one given by Amin The
above discussion shows that for certain values of an exponentially better regret can be achieved
by our algorithm It can be argued that the knowledge of an upper bound on
is required whereas
this is not needed for the monotone algorithm However if the regret bound
on monotone is super-linear and therefore uninformative
Thus in order to properly compare
both algorithms we may
assume
that
in
which
case by Theorem the regret
of our algorithm is log whereas only linear regret
can
be
guaranteed by the monotone
algorithm Even under the more favorable bound of for any and
the monotone algorithm will achieve regret O(T while a strictly better regret
O(T log log log is attained by ours
Lower bound
The following lower bounds have been derived in previous work
Theorem Amin Let be fixed For any algorithm A there exists a valuation
for the buyer such that Reg(A
This theorem is in fact given for the stochastic setting where the buyer?s valuation is a random
variable taken from some fixed distribution D. However the proof of the theorem selects to be a
point mass therefore reducing the scenario to a fixed priced setting
Theorem Kleinberg and Leighton Given any algorithm A to be played against a
truthful buyer there exists a value such that Reg(A log log for some universal
constant C.
95 75
PFS
mon
Regret
Regret
75
PFS
mon
PFS
mon
Number of rounds log-scale
PFS
mon
Number of rounds log-scale
Regret
Regret
85 75
Number of rounds log-scale
Number of rounds log-scale
Figure Comparison of the monotone algorithm and PFSr for different choices of and The regret of
each algorithm is plotted as a function of the number rounds when is not known to the algorithms first two
figures and when its value is made accessible to the algorithms last two figures
Combining these results leads immediately to the following
Corollary Given
any algorithm there exists a buyer?s valuation such that
Reg(A max
log log for a universal constant C.
We now compare the upper bounds given in the previous section with the bound of Corollary For
we have Reg(PFSr log log log On the other hand for we may
choose in which case by Proposition Reg(PFSr O(log log Thus the upper and
lower bounds match up to an O(log factor
Empirical results
In this section we present the result of simulations comparing the monotone algorithm and our
algorithm PFSr The experiments were carried out as follows given a buyer?s valuation a discrete
set of false valuations vb were selected out of the set 06 Both algorithms were run
against a buyer making the seller believe her valuation is vb instead of The value of vb achieving
the best utility for the buyer was chosen and the regret for both algorithms is reported in Figure
We considered two sets of experiments First the value of parameter was left unknown to both
algorithms and the value of was set to log(T This choice is motivated by the discussion following
Theorem since for large values of we can expect to achieve logarithmic regret The first two
plots from left to right in Figure depict these results The apparent stationarity in the regret of
PFSr is just a consequence of the scale of the plots as the regret is in fact growing as log(T For
the second set of experiments we allowed access to the parameter to both algorithms The value
of was chosen optimally
based on the resultsp
of Theorem
and the parameter of monotone
was set to to ensure regret in It is worth noting that even though
our algorithm was designed under the assumption of some knowledge about the value of the
experimental results show that an exponentially better performance over the monotone algorithm
is still attainable and in fact the performances of the optimized and unoptimized versions of our
algorithm are comparable A more comprehensive series of experiments is presented in Appendix
Conclusion
We presented a detailed analysis of revenue optimization algorithms against strategic buyers In
doing so we reduced the gap between upper and lower bounds on strategic regret to a logarithmic
factor Furthermore the algorithm we presented is simple to analyze and reduces to the truthful
scenario in the limit of an important property that previous algorithms did not admit We
believe that our analysis helps gain a deeper understanding of this problem and that it can serve as a
tool for studying more complex scenarios such as that of strategic behavior in repeated second-price
auctions VCG auctions and general market strategies
Acknowledgments
We thank Kareem Amin Afshin Rostamizadeh and Umar Syed for several discussions about the
topic of this paper This work was partly funded by the NSF award

<<----------------------------------------------------------------------------------------------------------------------->>

