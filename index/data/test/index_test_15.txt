query sentence: Storing covariance by the associative long-term potentiation and depression of synaptic strengths in the hippocampus
<<------------------------------------------------------------------------------------------------------------------------------------------->>
title: 100-storing-covariance-by-the-associative-long-term-potentiation-and-depression-of-synaptic-strengths-in-the-hippocampus.pdf

STORING COVARIANCE BY THE ASSOCIATIVE
LONG?TERM POTENTIATION AND DEPRESSION
OF SYNAPTIC STRENGTHS IN THE HIPPOCAMPUS
Patric K. Stanton and Terrence J. Sejnowski
Department of Biophysics
Johns Hopkins University
Baltimore MD
ABSTRACT
In modeling studies or memory based on neural networks both the selective
enhancement and depression or synaptic strengths are required ror effident storage
or inrormation Sejnowski Kohonen Bienenstock aI
Sejnowski and Tesauro We have tested this assumption in the hippocampus
a cortical structure or the brain that is involved in long-term memory A brier
high-frequency activation or excitatory synapses in the hippocampus produces an
increase in synaptic strength known as long-term potentiation or LTP BUss and
Lomo that can last ror many days LTP is known to be Hebbian since it
requires the simultaneous release or neurotransmitter from presynaptic terminals
coupled with postsynaptic depolarization Kelso al Malinow and Miller
Gustatrson al However a mechanism ror the persistent reduction or
synaptic strength that could balance LTP has not yet been demonstrated We studied the associative interactions between separate inputs onto the same dendritic
trees or hippocampal pyramidal cells or field CAl and round that a low-frequency
input which by itselr does not persistently change synaptic strength can either
increase associative LTP or decrease in strength associative long-term depression
or LTD depending upon whether it is positively or negatively correlated in time
with a second high-frequency bursting input LTP or synaptic strength is Hebbian
and LTD is anti-Hebbian since it is elicited by pairing presynaptic firing with postsynaptic hyperpolarization sufficient to block postsynaptic activity Thus associative LTP and associative LTO are capable or storing inrormation contained in the
covariance between separate converging hippocampal inputs
Present address Dep~ents of NeW'Oscience and Neurology Albert Einstein College
of Medicine Pelham Parkway South Bronx NY USA.
tPresent address Computational Neurobiology Laboratory The Salk Institute P.O. Box
San Diego CA USA.
Storing Covariance by Synaptic Strengths in the Hippocampus
INTRODUCTION
Associative LTP can be produced in some hippocampal neuroos when lowfrequency Weak and high-frequency Strong inputs to the same cells are simultaneously activated Levy and Steward Levy and Steward Barrionuevo and
Brown When stimulated alone a weak input does not have a long-lasting effect
on synaptic strength however when paired with stimulation of a separate strong input
sufficient to produce homo synaptic LTP of that pathway the weak pathway is associatively potentiated Neural network modeling studies have predicted that in addition to
this Hebbian form of plasticity synaptic strength should be weakened when weak and
strong inputs are anti-correlated Sejnowski Kohonen Bienenstock al
Sejnowski and Tesauro Evidence for heterosynaptic depression in the hippocampus has been found for inputs that are inactive Levy and Steward Lynch
al or weakly active Levy and Steward during the stimulation of a strong
input but this depression did not depend on any pattern of weak input activity and was
not typically as long-lasting as LTP.
Therefore we searched for conditions under which stimulation of a hippocampal
pathway rather than its inactivity could produce either long-term depression or potentiation of synaptic strengths depending on the pattern of stimulation The stimulus paradigm that we used illustrated in I is based on the finding that bursts of stimuli at
Hz are optimal in eliciting LTP in the hippocampus Larson and Lynch A highfrequency burst S'IRONG stimulus was applied to Schaffer collateral axons and a lowfrequency WEAK stimulus given to a separate subicular input coming from the opposite side of the recording site but terminating on dendrites of the same population of CAl
pyramidal neurons Due to the rhythmic nature of the strong input bursts each weak
input shock could be either superimposed on the middle of each burst of the strong input
IN PHASE or placed symmetrically between bursts OUT OF PHASE
RESULTS
Extracellular evoked field potentials were recorded from the apical dendritic and
somatic layers of CAl pyramidal cells The weak stimulus train was first applied alone
and did not itself induce long-lasting changes The strong site was then stimulated alone
which elicited homosynaptic LTP of the strong pathway but did not significantly alter
amplitude of responses to the weak input When weak and strong inputs were activated
IN PHASE there was an associative LTP of the weak input synapses as shown in
Both the synaptic excitatory post-synaptic potential
and population action potential Pike were
significantly enhanced for at least min up to min following stimulation
In contrast when weak and strong inputs were applied OUT OF PHASE they elicited an associative long-term depression LTO of the weak input synapses as shown in
There was a marked reduction in the population spike
with smaller decreases in the Note that the stimulus patterns applied to each input were identical in these two experiments and only the relative
Stanton and Sejnowski
phase of the weak and strong stimuli was altered With these stimulus patterns synaptic
strength could be repeatedly enhanced and depressed in a single slice as illustrated in Fig
As a control experiment to determine whether information concerning covariance
between the inputs was actually a determinant of plasticity we combined the in phase
and out of phase conditions giving both the weak input shocks superimposed on the
bursts plus those between the bursts for a net frequency of Hz. This pattern which
resulted in zero covariance between weak and strong inputs produced no net change in
weak input synaptic strength measmed by extracellular evoked potentials Thus the assoa
A.SSOCIA.TIVE STIMULUS PA.RA.DIGMS
POSJTIVE.LY CORKELA TED IN PHASE
SI1IONG,NJO\IT
u.Jj1l
NEGATIVELY CORRELATED our OF PHASE
W[AKIN'lTf
STIONG
I
Figure Hippocampal slice preparation and stimulus paradigms a The in vitro hippocampal slice showing recording sites in CAl pyramidal cell somatic stratum pyramidale and dendritic stratum radiatum layers and stimulus sites activating Schaffer collateral STRONG and commissural WEAK afferents Hippocampal slices Jlm
thick were incubated in an interface slice chamber at C. Extracellular M!l
resistance 2M NaCI filled and intracellular 2M K-acetate filled recording electrodes and bipolar glass-insulated platinum wire stimulating electrodes Jlm
tip diameter were prepared by standard methods Mody al Stimulus paradigms used Strong input stimuli STRONG INPUT were four trains of Hz bursts
Each burst had stimuli and the interburst interval was msec Each train lasted
seconds for a total of stimuli Weak input stimuli WEAK INPUT were four trains of
shocks at Hz frequency each train lasting for seconds When these inputs were IN
PHASE the weak single shocks were superimposed on the middle of each burst of the
strong input When the weak input was OUT OF PHASE the single shocks were placed
symmetrically between the bursts
Storing Covariance by Synaptic Strengths in the Hippocampus
ciative LTP and LTD mechanisms appear to be balanced in a manner ideal for the
storage of temporal covariance relations
The simultaneous depolarization of the postsynaptic membrane and activation of
glutamate receptors of the N-methyl-D-aspartate NMDA subtype appears to be necessary for LTP induction Collingridge Harris al Wigstrom and Gustaffson The SJ?read of current from strong to weak synapses in the dendritic tree
ASSOCIATIVE
LON(;.TE
I'OTENTIATION
LONG-TE
DE,/tESSION
ASSOCIATIVE
I
I
I
I
I
I
Figure mustration of associative long-term potentiation LTP and associative longterm depression LTD using extracellular recordings a Associative LTP of evoked
excitatory postsynaptic potentials and population action potential responses in
the weak inpuL Test responses are shown before Pre and min after post application of weak stimuli in phase with the coactive strong input Associative LTD of
evoked and population spike responses in the weak input Test responses are
shown before Pre and min after post application of weak stimuli out of phase with
the coactive strong input Time course of the changes in population spike amplitude
observed at each input for a typical experiment Test responses from the strong input
open circles show that the high-frequency bursts pulses/l00 Hz msec interburst
interval as in elicited synapse-specific LTP independent of other input activity
Test responses from the weak input filled circles show that stimulation of the weak
pathway out of phase with the strong one produced associative LTD Assoc LTD of this
input Associative LTP Assoc LTP of the same pathway was then elicited following in
phase stimulation Amplitude and duration of associative LTD or LTP could be increased
by stimulating input pathways with more trains of shocks
Stanton and Sejnowski
coupled with release of glutamate from the weak inputs could account for the ability of
the strong pathway to associatively potentiate a weak one Kelso al Malinow
and Miller Gustaffson al Consistent with this hypothesis we find that
the NMDA receptor antagonist 2-amino-S-phosphonovaleric acid APS blocks
induction of associative LTP in CAl pyramidal neurons data not shown In contrast the application of APS to the bathing solution at this same concentration had no
significant effect on associative LTD data not shown Thus the induction of LTD
seems to involve cellular mechanisms different from associative LTP.
The conditions necessary for LTD induction were explored in another series of
experiments using intracellular recordings from CAl pyramidal neurons made using
standard techniques Mody al Induction of associative LTP Fig WEAK
S+W IN PHASE produced an increase in amplitude of the single cell evoked and
a lowered action potential threshold in the weak pathway as reported previously Barrionuevo and Brown Conversely the induction of associative LTD
WEAK S+W OUT OF PHASE was accompanied by a long-lasting reduction of
amplitude and reduced ability to elicit action potential firing As in control extracellular
experiments the weak input alone produced no long-lasting alterations in intracellular
or firing properties while the strong input alone yielded specific increases of
the strong pathway without altering elicited by weak input stimulation
PRE
min POST
S+W OUT OF PHASE
min POST
S+W IN PHASE
Figure Demonstration of associative LTP and LTD using intracellular recordings from
a CAl pyramidal neuron Intracellular prior to repetitive stimulation
min after out of phase stimulation OUT OF PHASE and min after subsequent in phase stimuli IN PHASE The strong input Schaffer collateral side
lower traces exhibited LTP of the evoked independent of weak input activity
Out of phase stimulation of the weak Subicular side upper traces pathway produced a
marked persistent reduction in amplitude In the same cell subsequent in phase
stimuli resulted in associative LTP of the weak input that reversed the LTD and enhanced
amplitude of the past the original baseline RMP 62 mY RN MO
Storing Covariance by Synaptic Strengths in the Hippocampus
A weak stimulus that is out of phase with a strong one anives when the postsynaptic neuron is hyperpolarized as a consequence of inhibitory postsynaptic potentials and
afterhyperpolarization from mechanisms intrinsic to pyramidal neurons This suggests
that postsynaptic hyperpolarization coupled with presynaptic activation may trigger L'ID
To test this hypothesis we injected current with intracellular microelectrodes to hyperpolarize or depolarize the cell while stimulating a synaptic input Pairing the injection of
depolarizing current with the weak input led to LTP of those synapses STIM
a
PRE
IDPOST
S'I1M DEPOL
COI'ITROL
Jj
I
W.c:ULVllj
PRE
lOlIIin POST
STlM HYPERPOL
Figure Pairing of postsynaptic hyperpolarization with stimulation of synapses on CAl
hippocampal pyramidal neurons produces L'ID specific to the activated pathway while
pairing of postsynaptic depolarization with synaptic stimulation produces synapsespecific LTP. a Intracellular evoked are shown at stimulated STIM and
unstimulated CONTROL pathway synapses before Pre and min after post pairing a mY depolarization constant current nA with Hz synaptic stimulation
The stimulated pathway exhibited associative LTP of the while the control
unstimulated input showed no change in synaptic strength RMP 65 mY RN 35
Mfl Intracellular are shown evoked at stimulated and control pathway
synapses before Pre and min after post pairing a mV hyperpolarization constant current nA with Hz synaptic stimulation The input STIM activated during
the hyperpolarization showed associative LTD of synaptic evoked while
synaptic strength of the silent input CONTROL was unaltered RMP mV RN
Stanton and Sejnowski
while a control input inactive during the stimulation did not change
CONTROL as reported previously Kelso al Malinow and Miller Gustaffson al Conversely prolonged hyperpolarizing current injection paired with
the same low-frequency stimuli led to induction of LTD in the stimulated pathway
STIM but not in the unstimulated pathway CONTROL The
application of either depolarizing current hyperpolarizing current or the weak Hz
synaptic stimulation alone did not induce long-term alterations in synaptic strengths
Thus hyperpolarization and simultaneous presynaptic activity supply sufficient conditions for the induction of LTD in CAl pyramidal neurons
CONCLUSIONS
These experiments identify a novel fono of anti-Hebbian synaptic plasticity in the
hippocampus and confirm predictions made from modeling studies of information storage
in neural networks Unlike previous reports of synaptic depression in the hippocampus
the plasticity is associative long-lasting and is produced when presynaptic activity
occurs while the postsynaptic membrane is hyperpolarized In combination with Hebbian
mechanisms also present at hippocampal synapses associative LTP and associative LTD
may allow neurons in the hippocampus to compute and store covariance between inputs
Sejnowski Stanton and Sejnowski These finding make temporal as
well as spatial context an important feature of memory mechanisms in the hippocampus
Elsewhere in the brain the receptive field properties of cells in cat visual cortex
can be altered by visual experience paired with iontophoretic excitation or depression of
cellular activity Fregnac al Greuel al In particular the chronic hyperpolarization of neurons in visual cortex coupled with presynaptic transmitter release leads
to a long-teno depression of the active but not inactive inputs from the lateral geniculate
nucleus Reiter and Stryker Thus both Hebbian and anti-Hebbian mechanisms
found in the hippocampus seem to also be present in other brain areas and covariance of
firing patterns between converging inputs a likely key to understanding higher cognitive
function
This research was supported by grants from the National Science Foundation and
the Office of Naval research to TJS. We thank Drs. Charles Stevens and Richard Morris
for discussions about related experiments

<<----------------------------------------------------------------------------------------------------------------------->>

title: 4871-correlations-strike-back-again-the-case-of-associative-memory-retrieval.pdf

Correlations strike back again the case of
associative memory retrieval
Cristina Savin1
cs664@cam.ac.uk
Peter Dayan2
dayan@gatsby.ucl.ac.uk
M?at?e Lengyel1
m.lengyel@eng.cam.ac.uk
Computational Biological Learning Lab Dept Engineering University of Cambridge UK
Gatsby Computational Neuroscience Unit University College London UK
Abstract
It has long been recognised that statistical dependencies in neuronal activity need
to be taken into account when decoding stimuli encoded in a neural population
Less studied though equally pernicious is the need to take account of dependencies between synaptic weights when decoding patterns previously encoded in an
auto-associative memory We show that activity-dependent learning generically
produces such correlations and failing to take them into account in the dynamics
of memory retrieval leads to catastrophically poor recall We derive optimal network dynamics for recall in the face of synaptic correlations caused by a range of
synaptic plasticity rules These dynamics involve well-studied circuit motifs such
as forms of feedback inhibition and experimentally observed dendritic nonlinearities We therefore show how addressing the problem of synaptic correlations leads
to a novel functional account of key biophysical features of the neural substrate
Introduction
Auto-associative memories have a venerable history in computational neuroscience However it is
only rather recently that the statistical revolution in the wider field has provided theoretical traction
for this problem The idea is to see memory storage as a form of lossy compression information
on the item being stored is mapped into a set of synaptic changes with the neural dynamics during
retrieval representing a biological analog of a corresponding decompression algorithm This implies
there should be a tight and indeed testable link between the learning rule used for encoding and the
neural dynamics used for retrieval
One issue that has been either ignored or trivialized in these treatments of recall is correlations
among the synapses beyond the perfect anti-)correlations emerging between reciprocal
synapses with precisely anti-)symmetric learning rules There is ample experimental data for
the existence of such correlations for example in rat visual cortex synaptic connections tend to
cluster together in the form of overrepresented patterns or motifs with reciprocal connections being
much more common than expected by chance and the strengths of the connections to and from
each neuron being correlated The study of neural coding has indicated that it is essential to
treat correlations in neural activity appropriately in order to extract stimulus information well
Similarly it becomes pressing to examine the nature of correlations among synaptic weights in
auto-associative memories the consequences for retrieval of ignoring them and methods by which
they might be accommodated
Here we consider several well-known learning rules from simple additive ones to bounded synapses
with metaplasticity and show that with a few significant exceptions they induce correlations between synapses that share a pre or a post-synaptic partner To assess the importance of these dependencies for recall we adopt the strategy of comparing the performance of decoders which either
do or do not take them into account showing that they do indeed have an important effect on
efficient retrieval Finally we show that approximately optimal retrieval involves particular forms
of nonlinear interactions between different neuronal inputs as observed experimentally
General problem formulation
We consider a network of binary neurons that enjoy all-to-all connectivity.1 As is conventional
and indeed plausibly underpinned by neuromodulatory interactions we assume that network
dynamics do not play a role during storage with stimuli being imposed as patterns of activity on the
neurons and that learning does not occur during retrieval
To isolate the effects of different plasticity rules on synaptic correlations from other sources of
correlations we assume that the patterns of activity inducing the synaptic changes have no particular
structure their distribution factorizes For further simplicity we take these activity patterns to
be binary with pattern density a prior over patterns defined as
Pstore
Pstore
Pstore
During recall the network is presented with a cue
which is a noisy or partial version of one
of the originally stored patterns Network dynamics should complete this partial pattern using the
information in the weights and the cue We start by considering arbitrary dynamics later we
impose the critical constraint for biological realisability that they be strictly local the activity of
neuron should depend exclusively on inputs through incoming synapses
Since information storage by synaptic plasticity is lossy recall is inherently a probabilistic inference
problem requiring estimation of the posterior over patterns given the information
in the weights and the recall cue
Pstore Pnoise
This formulation has formed the foundation of recent work on constructing efficient autoassociative
recall dynamics for a range of different learning rules In this paper we focus on the last term
which expresses the probability of obtaining as the synaptic weight matrix when is
stored along with random patterns sampled from the prior Critically this is where
we diverge from previous analyses that assumed this distribution was factorised or only trivially
correlated due to reciprocal synapses being precisely anti-)symmetric In contrast we
explicitly study the emergence and effects of non-trivial correlations in the synaptic weight matrixdistribtion because almost all synaptic plasticity rules induce statistical dependencies between the
synaptic weights of each neuron
The inference problem expressed by can be translated into neural dynamics in several ways
dynamics could be deterministic attractor-like converging to the most likely pattern MAP
estimate of the distribution of or to a mean-field approximate solution alternatively the
dynamics could be stochastic with the activity over time representing samples from the posterior
and hence implicitly capturing the uncertainty associated with the answer We consider the latter
Since we estimate performance by average errors the optimal response is the mean of the posterior
which can be estimated by integrating the activity of the network during retrieval
We start by analysing the class of additive learning rules to get a sense for the effect of correlations on retrieval Later we focus on multi-state synapses for which learning rules are described
by transition probabilities between the states These have been used to capture a variety of
important biological constraints such as bounds on synaptic strengths and metaplasticity the
fact that synaptic changes induced by a certain activity pattern depend on the history of activity at
the synapse The two classes of learning rule are radically different so if synaptic correlations
matter during retrieval in both cases then the conclusion likely applies in general
Complete connectivity simplifies the computation of the parameters for the optimal dynamics for cascadelike learning rules considered in the following but is not necessary for the theory
covariance rule
simple Hebb rule
cortical data Song
error
corr
error
corr
a
control
exact considering correlations
simple ignoring correlations
control
Figure Memory recall as inference and additive learning rules a Top Synaptic weights
arise by storing the target pattern together with other patterns During
is a noisy version of the target pattern The task of recall is to infer given
recall the cue
and
by marginalising out Bottom The activity of neuron across the stored patterns is
a source of shared variability between synapses connecting it to neurons and Covariance
rule patterns of synaptic correlations and recall performance for retrieval dynamics ignoring or
considering synaptic correlations Same for the simple Hebbian learning rule The
control is an optimal decoder that ignores W.
Additive learning rules
Local additive learning rules assume that synaptic changes induced by different activity patterns
combine additively such that storing a sequence of patterns from Pstore results in weights
Wij with function describing the change in synaptic strength induced
by presynaptic activity and postsynaptic activity We consider a generalized Hebbian form for
this function with This class includes for example the covariance
rule classically used in Hopfield networks or simple Hebbian learning
As synaptic changes are deterministic the only source of uncertainty in the distribution
is the identity of the other stored patterns To estimate this let us first consider the distribution of
the weights after storing one random pattern from Pstore The mean and covariance of the
weight change induced by this event can be computed
Pstore
Pstore dx
Since the rule is additive and the patterns are independent the mean and covariance scale linearly
with the number of intervening patterns Hence the distribution over possible weight values at
recall given that pattern is stored along with other random patterns has mean
and covariance CW C. Most importantly because the rule is
additive in the limit of many stored patterns and in practice even for modest values of the
distribution approaches a multivariate Gaussian that is characterized completely by these
two quantities moreover its covariance is independent of
For retrieval dynamics based on Gibbs sampling the key quantity is the log-odds ratio
P(xi
Ii log
P(xi
for neuron which could be represented by the total current entering the unit This would translate
into a probability of firing given by the sigmoid activation function Ii e?Ii
The total current entering a neuron is a sum of two terms one term from the external input of the
form c1
c2 with constants c1 and c2 determined by parameters and and one term
from the recurrent input of the form
Iirec
For notational convenience we use a column-vector form of the matrix of weight changes and the
weight matrix marked by subscript
where and is the vector of activities obtained from in which
the activity of neuron is set to or respectively
It is easy to see that for the covariance rule synapses sharing
a single pre or post-synaptic partner happen to be uncorrelated Moreover as for any
anti-)symmetric additive learning rule reciprocal connections are perfectly correlated Wij Wji
The non-degenerate part of the covariance matrix in this case becomes diagonal and the total
current in optimal retrieval reduces to simple linear dynamics
Ii
Wij
recurrent input
2f
2f
Wij
feedback inhibition
homeostatic term
constant
where
is the variance of a synaptic weight resulting from storing a single pattern This term
includes a contribution from recurrent excitatory input dynamic feedback inhibition proportional
to the total population activity and a homeostatic term that reduces neuronal excitability as function
of the net strength of its synapses proxy for average current the neuron expects to receive
Reassuringly the optimal decoder for the covariance rule recovers a form for the input current that is
closely related to classic Hopfield-like dynamics with external field feedback inhibition
is needed only when the stored patterns are not balanced for the balanced case the
homeostatic term can be integrated in the recurrent current by rewriting neural activities as spins
In sum for the covariance rule synapses are fortuitously uncorrelated except for symmetric pairs
which are perfectly correlated and thus simple classical linear recall dynamics suffice
The covariance rule is however the exception rather than the rule For example for simple Hebbian
learning synapses sharing a pre or post-synaptic partner are correlated
and so the covariance matrix is no longer diagonal Interestingly the final expression of the
recurrent current to a neuron remains strictly local because of additivity and symmetry and very
similar to but feedback inhibition becomes a non-linear function of the total activity in the
network In this case synaptic correlations have a dramatic effect using the optimal non-linear
dynamics ensures high performance but trying to retrieve information using a decoder that assumes
synaptic independence and thus uses linear dynamics yields extremely poor performance which
is even worse than the obvious control of relying only on the information in the recall cue and the
prior over patterns
For the generalized Hebbian case with the optimal decoder becomes even more complex with the total current including additional terms accounting for pairwise
correlations between any two synapses that have neuron as a pre or post-synaptic partner
Hence retrieval is no longer strictly local3 and a biological implementation will require approximating the contribution of non-local terms as a function of locally available information as we discuss
in detail for palimpsest learning below
Palimpsest learning rules
Though additive learning rules are attractive for their analytical tractability they ignore several important aspects of synaptic plasticity they assume that synapses can grow without bound We
investigate the effects of bounded weights by considering another class of learning rules which assumes synaptic efficacies can only take binary values with stochastic transitions between the two
underpinned by paired cascades of latent internal states These learning rules though
very simple capture an important aspect of memory the fact that memory is leaky and information
about the past is overwritten by newly stored items usually referred to as the palimpsest property
Additionally such rules can account for experimentally observed synaptic metaplasticity
For additive learning rules the current to neuron always depends only on synapses local to a neuron but
these can also include outgoing synapses of which the weight W?i should not influence its dynamics We refer
to such dynamics as semi-local For other learning rules the optimal current to neuron may depend on all
connections in the network including Wjk with non-local dynamics
R1
post
R2
R3
pre
cortex data Song
correlated synapses
error
correlation coefficient
a
pseudostorage
exact approx
simple
dynamics
corr-dependent
dynamics
Figure Palimpsest learning a The cascade model Colored circles are latent states that
belong to two different synaptic weights arrows are state transitions blue depression red
potentiation Different variants of mapping pre and post-synaptic activations to depression
and potentiation R1?postsynaptically gated R2?presynaptically gated R3?XOR rule Correlation structure induced by these learning rules Retrieval performance for each rule
Learning rule
Learning is stochastic and local with changes in the state of a synapse Vij being determined only by
the activation of the pre and post-synaptic neurons and In general one could define separate
transition matrices for each activity pattern M(xi describing the probability of a synaptic state
transitioning between any two states Vij to Vij0 following an activity pattern For simplicity
we define only two such matrices for potentiation and depression respectively and then
map different activity patterns to these events In particular we assume Fusi?s cascade model
and three possible mappings 2b a postsynaptically gated learning rule where changes
occur only when the postsynaptic neuron is active with co-activation of pre and post neuron leading to potentiation and to depression otherwise5 a presynaptically gated learning rule typically
assumed when analysing cascades[20 and an XOR-like learning rule which assumes potentiation occurs whenever the pre and post synaptic activity levels are the same with depression
otherwise The last rule proposed by Ref. 22 was specifically designed to eliminate correlations
between synapses and can be viewed as a version of the classic covariance rule fashioned for binary
synapses
Estimating the mean and covariance of synaptic weights
At the level of a single synapse the presentation of a sequence of uncorrelated patterns from
Pstore corresponds to a Markov random walk
defined by a transition matrix which averages over possible neural activity patterns Pstore Pstore M(xi The
distribution over synaptic states steps after the initial encoding can be calculated by starting from
the stationary distribution of the weights assuming a large number of other patterns have previously been stored formally this is the eigenvector of corresponding to eigenvalue then
storing the pattern and finally other patterns from the prior
M(xi
lV
with the distribution over states given as a column vector
P(Vij l|xi
where is the depth of the cascade Lastly the distribution over weights P(Wij can be
derived as MV where MV is a deterministic map from states to observed
weights
As in the additive case the states of synapses sharing a pre or post synaptic partner will be correlated Figs The degree of correlations for different synaptic configurations can be estimated
by generalising the above procedure to computing the joint distribution of the states of pairs of
synapses which we represent as a matrix E.g. for a pair of synapses sharing a postsynaptic
partner Figs and element is uv P(Vpost,pre1 Vpost,pre2 Hence the
presentation of an activity pattern xpre1 xpre2 xpost induces changes in the corresponding pair of
Other models serial could be used as well without qualitatively affecting the results
One could argue that this is the most biologically relevant as plasticity is often NMDA-receptor dependent
and hence it requires postsynaptic depolarisation for any effect to occur
incoming synapses to neuron post as M(xpost xpre1 M(xpost xpre2 where
is the stationary distribution corresponding to storing an infinite number of triplets from the pattern
distribution
Replacing with which is now a function of the triplet xpre1 xpre2 xpost and the multiplication by with the slightly more complicated operator above we can estimate the evolution of
the joint distribution over synaptic states in a manner very similar to
M(x
Pstore M(x
where M(x
Pstore M(xi Also as above the final joint distribution over states
can be mapped into a joint distribution over synaptic weights as MV MT
This
approach can be naturally extended to all other correlated pairs of synapses
The structure of correlations for different synaptic pairs varies significantly as a function of the
learning rule with the overall degree of correlations depending on a range of factors
Correlations tend to decrease with cascade depth and pattern sparsity The first two variants of the
learning rule considered are not symmetric and so induce different patterns of correlations than the
additive rules above The XOR rule is similar to the covariance rule but the reciprocal connections
are no longer perfectly correlated due to metaplasticity which means that it is no longer possible
to factorize Hence assuming independence at decoding seems bound to introduce errors
Approximately optimal retrieval when synapses are independent
If we ignore synaptic correlations the evidence from the weights factorizes
i,j P(Wij and so the exact dynamics would be semi-local We can further approximate
the contribution of the outgoing weights by its mean which recovers the same simple dynamics
derived for the additive case
P(xi
Ii log
c1
Wij c2
Wij c3
c4 x?i c5
P(xi
The parameters depend on the prior over the noise model the parameters of the learning rule
and Again the optimal decoder is similar to previously derived attractor dynamics in particular
for stochastic binary synapses with presynaptically gated learning the optimal dynamics require
dynamic inhibition only for sparse patterns and no homeostatic term as used in
To validate these dynamics we remove synaptic correlations by a pseudo-storage procedure in which
synapses are allowed to evolve independently according to transition matrix rather than changing
as actual intermediate patterns are stored The dynamics work well in this case as expected
blue bars However when storing actual patterns drawn from the prior performance becomes extremely poor and often worse than the control gray bars Moreover performance worsens
as the network size increases not shown Hence ignoring correlations is highly detrimental for this
class of learning rules too
Approximately optimal retrieval when synapses are correlated
To accommodate synaptic correlations we approximate with a maximum entropy distribution with the same marginals and covariance structure ignoring the higher order moments.6
Specifically we assume the evidence from the weights has the functional form
1X
exp
kij Wij
J(ij)(kl Wij Wkl
ij
ijkl
We use the TAP mean-field method to find parameters and and the partition function
for each possible activity pattern given the mean and covariance for the synaptic weights matrix
computed above7
This is just a generalisation of the simple dynamics which assume a first order max entropy model moreover the resulting weight distribution is a binary analog of the multivariate normal used in the additive case
allowing the two to be directly compared
Here we ask whether it is possible to accommodate correlations in appropriate neural dynamics at all
ignoring the issue of how the optimal values for the parameters of the network dynamics would come about
a
no corr
corr
number of coactive inputs
number of coactive inputs
normalized EPSP
TIP
MIDDLE
postsynaptic current
postsynaptic current
BASE
number of inputs
Figure Implications for neural dynamics a parameters for Iirec linear modulation by
network activity nb nonlinear modulation of pairwise term by network activity middle
panel in other parameters have
linear dependences on nb Total current as
Pfunction of
number of coactivated inputs Wij lines different levels of neural excitability Wij line
widths scale with frequency of occurrence in a sample run Same for R2. Nonlinear integration
in dendrites reproduced from cf curves in
Exact retrieval dynamics based on but not respecting locality constraints work substantially
better in the presence of synaptic correlations for all rules yellow bars It is important to
note that for the XOR rule which was supposed to be the closest analog to the covariance rule and
hence afford simple recall dynamics error rates stay above control suggesting that it is actually
a case in which even dependencies beyond 2nd-order correlation would need to be considered
As in the additive case exact recall dynamics are biologically implausible as the total current to
the neuron depends on the full weight matrix It is possible to approximate the dynamics using
strictly local information by replacing the nonlocal term by its mean which however
is no longer a
constant but rather a linear function of the total activity in the network nb Under
this approximation the current from recurrent connections corresponding to the evidence from the
weights becomes
1X
J4
x)Wij Wik
Iirec log
ij
ij
jk
where is the index of the neuron to be updated and activity vector has the to-be-updated
neuron?s activity set to or respectively and all other components given by the
current network
state The functions kij
kij kij J(ij)(kl
J(ij)(kl
and log log depend on the local activity at the indexed synapses
modulated by the number of active neurons in the network nb This approximation is again consistent with our previous analysis in the absence of synaptic correlations the complex dynamics
recover the simple case presented before Importantly this approximation also does about as well as
exact dynamics red bars
For post-synaptically gated learning comparing the parameters of the dynamics in the case of independent versus correlated synapses reveals a modest modulation of the recurrent input by
the total activity More importantly the net current to the postsynaptic
neuron depends non-linearly
formally quadratically on the number of co-active inputs nW Wij which
is reminiscent of experimentally observed dendritic non-linearities Conversely for
the presynaptically gated learning rule approximately optimal dynamics predict a non-monotonic
modulation of activity by lateral inhibition but linear neural integration Lastly
retrieval based on the XOR rule has the same form as the simple dynamics derived for the factorized
case However the total current has to be rescaled to compensate for the correlations introduced
by reciprocal connections
The difference between the two rules emerges exclusively because of the constraint of strict locality of the
approximation since the exact form of the dynamics is essentially the same for the two
additive
cascade
RULE
covariance
simple Hebbian
generalized Hebbian
presyn gated
postsyn gated
XOR
EXACT DYNAMICS
strictly local linear
strictly local nonlinear
semi-local nonlinear
nonlocal nonlinear
nonlocal nonlinear
beyond correlations
NEURAL IMPLEMENTATION
linear feedback inh homeostasis
nonlinear feedback inh
nonlinear feedback inh
nonlinear feedback inh linear dendritic integr
linear feedback inh non-linear dendritic integr
Table Results summary circuit adaptations against correlations for different learning rules
Discussion
Statistical dependencies between synaptic efficacies are a natural consequence of activity dependent
synaptic plasticity and yet their implications for network function have been unexplored Here in
the context of an auto-associative memory network we investigated the patterns of synaptic correlations induced by several well-known learning rules and their consequent effects on retrieval We
showed that most rules considered do indeed induce synaptic correlations and that failing to take
them into account greatly damages recall One fortuitous exception is the covariance rule for which
there are no synaptic correlations This might explain why the bulk of classical treatments of autoassociative memories using the covariance rule could achieve satisfying capacity levels despite
overlooking the issue of synaptic correlations 24
In general taking correlations into account optimally during recall requires dynamics in which there
are non-local interactions between neurons However we derived approximations that perform well
and are biologically realisable without such non-locality Table Examples include the modulation of neural responses by the total activity of the population which could be mediated by feedback
inhibition and specific dendritic nonlinearities In particular for the post-synaptically gated learning rule which may be viewed as an abstract model of hippocampal NMDA receptor-dependent
plasticity our model predicts a form of non-linear mapping of recurrent inputs into postsynaptic
currents which is similar to experimentally observed dendritic integration in cortical pyramidal cells
In general the tight coupling between the synaptic plasticity used for encoding manifested
in patterns of synaptic correlations and circuit dynamics offers an important route for experimental
validation
None of the rules governing synaptic plasticity that we considered perfectly reproduced the pattern
of correlations in and indeed exactly which rule applies in what region of the brain under which
neuromodulatory influences is unclear Furthermore results in concern the neocortex rather
than the hippocampus which is a more common target for models of auto-associative memory
Nonetheless our analysis has shown that synaptic correlations matter for a range of very different
learning rules that span the spectrum of empirical observations
Another strategy to handle the negative effects of synaptic correlations is to weaken or eliminate
them For instance in the palimpsest synaptic model the deeper the cascade the weaker the
correlations and so metaplasticity may have the beneficial effect of making recall easier Another
popular idea is to use very sparse patterns although this reduces the information content of
each one More speculatively one might imagine a process of off-line synaptic pruning or recoding
in which strong correlations are removed or the weights adjusted so that simple recall methods will
work
Here we focused on second-order correlations However for plasticity rules such as XOR we
showed that this does not suffice Rather higher-order correlations would need to be considered
and thus presumably higher-order interactions between neurons approximated Finally we know
from work on neural coding of sensory stimuli that there are regimes in which correlations either
help or hurt the informational quality of the code assuming that decoding takes them into account
Given our results it becomes important to look at the relative quality of different plasticity rules
assuming realizable decoding it is not clear whether rules that strive to eliminate correlations will
be bested by ones that do not
Acknowledgments This work was supported by the Wellcome Trust the Gatsby Charitable Foundation and the European Union Seventh Framework Programme
under grant agreement no BrainScaleS

<<----------------------------------------------------------------------------------------------------------------------->>

title: 2859-learning-in-silicon-timing-is-everything.pdf

Learning in Silicon Timing is Everything
John V. Arthur and Kwabena Boahen
Department of Bioengineering
University of Pennsylvania
Philadelphia PA
jarthur boahen}@seas.upenn.edu
Abstract
We describe a neuromorphic chip that uses binary synapses with spike
timing-dependent plasticity STDP to learn stimulated patterns of activity and to compensate for variability in excitability Specifically STDP
preferentially potentiates turns on synapses that project from excitable
neurons which spike early to lethargic neurons which spike late The
additional excitatory synaptic current makes lethargic neurons spike earlier thereby causing neurons that belong to the same pattern to spike in
synchrony Once learned an entire pattern can be recalled by stimulating
a subset
Variability in Neural Systems
Evidence suggests precise spike timing is important in neural coding specifically in the
hippocampus The hippocampus uses timing in the spike activity of place cells addition
to rate to encode location in space Place cells employ a phase code the timing at
which a neuron spikes relative to the phase of the inhibitory theta rhythm conveys
information As an animal approaches a place cell?s preferred location the place cell not
only increases its spike rate but also spikes at earlier phases in the theta cycle
To implement a phase code the theta rhythm is thought to prevent spiking until the input
synaptic current exceeds the sum of the neuron threshold and the decreasing inhibition on
the downward phase of the cycle However even with identical inputs and common
theta inhibition neurons do not spike in synchrony Variability in excitability spreads the
activity in phase Lethargic neurons such as those with high thresholds spike late in the
theta cycle since their input exceeds the sum of the neuron threshold and theta inhibition
only after the theta inhibition has had time to decrease Conversely excitable neurons
such as those with low thresholds spike early in the theta cycle Consequently variability
in excitability translates into variability in timing
We hypothesize that the hippocampus achieves its precise spike timing about
through plasticity enhanced phase-coding The source of hippocampal timing precision in the presence of variability and noise remains unexplained Synaptic plasticity can
compensate for variability in excitability if it increases excitatory synaptic input to neurons
in inverse proportion to their excitabilities Recasting this in a phase-coding framework we
desire a learning rule that increases excitatory synaptic input to neurons directly related to
their phases Neurons that lag require additional synaptic input whereas neurons that lead
A
Figure STDP Chip A The chip has a array of microcircuits one microcircuit
includes four principal neurons each with STDP circuits The STDP Chip is embedded in a circuit board including DACs a CPLD a RAM chip and a USB chip which
communicates with a PC.
require none The spike timing-dependent plasticity STDP observed in the hippocampus
satisfies this requirement It requires repeated pre-before-post spike pairings within a
time window to potentiate and repeated post-before-pre pairings to depress a synapse
Here we validate our hypothesis with a model implemented in silicon where variability is
as ubiquitous as it is in biology Section presents our silicon system including the
STDP Chip Section describes and characterizes the STDP circuit Section demonstrates that PEP compensates for variability and provides evidence that STDP is the compensation mechanism Section explores a desirable consequence of PEP unconventional
associative pattern recall Section discusses the implications of the PEP model including
its benefits and applications in the engineering of neuromorphic systems and in the study
of neurobiology
Silicon System
We have designed submitted and tested a silicon implementation of PEP. The STDP Chip
was fabricated through MOSIS in a CMOS process with just under
transistors in just over of area It has a 32 by 32 array of excitatory principal neurons commingled with a by array of inhibitory interneurons that are not used here
Figure Each principal neuron has STDP synapses The address-event representation AER is used to transmit spikes off chip and to receive afferent and recurrent spike
input
To configure the STDP Chip as a recurrent network we embedded it in a circuit board Figure The board has five primary components a CPLD complex programmable logic
device the STDP Chip a RAM chip a USB interface chip and DACs digital-to-analog
converters The central component in the system is the CPLD The CPLD handles AER
traffic mediates communication between devices and implements recurrent connections
by accessing a lookup table stored in the RAM chip The USB interface chip provides
a bidirectional link with a PC. The DACs control the analog biases in the system including the leak current which the PC varies in real-time to create the global inhibitory theta
rhythm
The principal neuron consists of a refractory period and calcium-dependent potassium circuit a synapse circuit and a soma circuit Figure RCK and the synapse are
ISOMA
Soma
Synapse
STDP
Presyn
Spike
PE
LPF
A
Presyn
Spike
Raster
AH
Spike probability
RCK
Postsyn
Spike
Time(s
Figure Principal neuron A A simplified schematic is shown including the synapse
refractory and calcium-dependent potassium channel soma and axon-hillock
circuits plus their constituent elements the pulse extender and the low-pass filter
Spikes dots from 81 principal neurons are temporally dispersed when excited
by poisson-like inputs and inhibited by the common theta rhythm solid line
The histogram includes spikes from five theta cycles
composed of two reusable blocks the low-pass filter LPF and the pulse extender
The soma is a modified version of the LPF which receives additional input from an axonhillock circuit
RCK is inhibitory to the neuron It consists of a PE which models calcium influx during
a spike and a LPF which models calcium buffering When AH fires a spike a packet of
charge is dumped onto a capacitor in the PE. The PE?s output activates until the charge
decays away which takes a few milliseconds Also while the PE is active charge accumulates on the LPF?s capacitor lowering the LPF?s output voltage Once the PE deactivates this charge leaks away as well but this takes tens of milliseconds because the leak is
smaller The PE?s and the LPF?s inhibitory effects on the soma are both described below
in terms of the sum ISHUNT of the currents their output voltages produce in pMOS transistors whose sources are at Vdd Figure Note that in the absence of spikes these
currents decay exponentially with a time-constant determined by their respective leaks
The synapse circuit is excitatory to the neuron It is composed of a PE which represents
the neurotransmitter released into the synaptic cleft and a LPF which represents the bound
neurotransmitter The synapse circuit is similar to RCK in structure but differs in function
It is activated not by the principal neuron itself but by the STDP circuits directly by
afferent spikes that bypass these circuits fixed synapses The synapse?s effect on the
soma is also described below in terms of the current ISYN its output voltage produces in a
pMOS transistor whose source is at Vdd.
The soma circuit is a leaky integrator It receives excitation from the synapse circuit and
shunting inhibition from RCK and has a leak current as well Its temporal behavior is
described by
dISOMA
ISYN I0
ISOMA
dt
ISHUNT
where ISOMA is the current the capacitor?s voltage produces in a pMOS transistor whose
source is at Vdd Figure ISHUNT is the sum of the leak refractory and calciumdependent potassium currents These currents also determine the time constant
Ut
ISHUNT where I0 and are transistor parameters and Ut is the thermal voltage
STDP circuit
LTP
SRAM
Presynaptic spike
A
LTD
Inverse number of pairings
Integrator
Decay
Postsynaptic spike
Potentiation
Depression
Presynaptic spike
Postsynaptic spike
Spike timing pre post
Figure STDP circuit design and characterization A The circuit is composed of three
subcircuits decay integrator and SRAM The circuit potentiates when the presynaptic
spike precedes the postsynaptic spike and depresses when the postsynaptic spike precedes
the presynaptic spike
The soma circuit is connected to an AH the locus of spike generation The AH consists
of model voltage-dependent sodium and potassium channel populations modified from
by Kai Hynna It initiates the AER signaling process required to send a spike off chip
To characterize principal neuron variability we excited 81 neurons with poisson-like
spike trains Figure We made these spike trains poisson-like by starting with a regular
spike train and dropping spikes randomly with probability of Thus spikes
were delivered to neurons that won the coin toss in synchrony every However neurons
did not lock onto the input synchrony due to filtering by the synaptic time constant
Figure They also received a common inhibitory input at the theta frequency
via their leak current Each neuron was prevented from firing more than one spike in a theta
cycle by its model calcium-dependent potassium channel population
The principal neurons spike times were variable To quantify the spike variability we used
timing precision which we define as twice the standard deviation of spike times accumulated from five theta cycles With an input rate of the timing precision was
STDP Circuit
The STDP circuit related to for which the STDP Chip is named is the most
abundant with copies on the chip This circuit is built from three subcircuits
decay integrator and SRAM Figure The decay and integrator are used to implement
potentiation and depression in a symmetric fashion The SRAM holds the current binary
state of the synapse either potentiated or depressed
For potentiation the decay remembers the last presynaptic spike Its capacitor is charged
when that spike occurs and discharges linearly thereafter A postsynaptic spike samples the
charge remaining on the capacitor passes it through an exponential function and dumps
the resultant charge into the integrator This charge decays linearly thereafter At the time
of the postsynaptic spike the SRAM a cross-coupled inverter pair reads the voltage on the
integrator?s capacitor If it exceeds a threshold the SRAM switches state from depressed
to potentiated LTD goes high and LTP goes low The depression side of the STDP
circuit is exactly symmetric except that it responds to postsynaptic activation followed by
presynaptic activation and switches the SRAM?s state from potentiated to depressed LTP
goes high and LTD goes low When the SRAM is in the potentiated state the presynaptic
After STDP
83
92
Timing precision(ms
Before STDP
75
Before STDP
After STDP
70
90
Input rate(Hz
58
67
text
A
Time(s
Time(s
Figure Plasticity enhanced phase-coding A Spike rasters of 81 neurons by cluster
display synchrony over a two-fold range of input rates after STDP The degree of enhancement is quantified by timing precision Each neuron center box sends synapses to
dark gray and receives synapses from light gray twenty-one randomly chosen neighbors
up to five nodes away black indicates both connections
spike activates the principal neuron?s synapse otherwise the spike has no effect
We characterized the STDP circuit by activating a plastic synapse and a fixed synapse
which elicits a spike at different relative times We repeated this pairing at We
counted the number of pairings required to potentiate depress the synapse Based
on this count we calculated the efficacy of each pairing as the inverse number of pairings required Figure For example if twenty pairings were required to potentiate the
synapse the efficacy of that pre-before-post time-interval was one twentieth The efficacy
of both potentiation and depression are fit by exponentials with time constants of
and respectively This behavior is similar to that observed in the hippocampus
potentiation has a shorter time constant and higher maximum efficacy than depression
Recurrent Network
We carried out an experiment designed to test the STDP circuit?s ability to compensate for
variability in spike timing through PEP. Each neuron received recurrent connections from
randomly selected neurons within an by neighborhood centered on itself
Figure Conversely it made recurrent connections to randomly chosen neurons within
the same neighborhood These connections were mediated by STDP circuits initialized to
the depressed state We chose a by cluster of neurons and delivered spikes at a mean
rate of to to each one dropping spikes with a probability of to from a
regular train and provided common theta inhibition as before
We compared the variability in spike timing after five seconds of learning with the initial
distribution Phase coding was enhanced after STDP Figure Before STDP spike
timing among neurons was highly variable except for the very highest input rate After
STDP variability was virtually eliminated except for the very lowest input rate Initially
the variability characterized by timing precision was inversely related to the input rate
decreasing from 34 to After five seconds of STDP variability decreased and was
largely independent of input rate remaining below
Potentiated synapses
A
Synaptic state
after STDP
Spiking order
Figure Compensating for variability A Some synapses dots become potentiated light
while others remain depressed dark after STDP The number of potentiated synapses
neurons make pluses and receive circles is negatively and positively
correlated to their rank in the spiking order respectively
Comparing the number of potentiated synapses each neuron made or received with its excitability confirmed the PEP hypothesis leading neurons provide additional synaptic
current to lagging neurons via potentiated recurrent synapses In this experiment to eliminate variability due to noise as opposed to excitability we provided a 17 by 17 cluster
of neurons with a regular excitatory input Theta inhibition was present as before
and all synapses were initialized to the depressed state After seconds of STDP a large
fraction of the synapses were potentiated Figure When the number of potentiated
synapses each neuron made or received was plotted versus its rank in spiking order Figure
a clear correlation emerged or respectively As expected neurons that
spiked early made more and received fewer potentiated synapses In contrast neurons that
spiked late made fewer and received more potentiated synapses
Pattern Completion
After STDP we found that the network could recall an entire pattern given a subset thus
the same mechanisms that compensated for variability and noise could also compensate
for lack of information We chose a by cluster of neurons as our pattern and delivered
a poisson-like spike train with mean rate of to each one as in the first experiment
Theta inhibition was present as before and all synapses were initialized to the depressed
state Before STDP we stimulated a subset of the pattern and only neurons in that subset
spiked Figure After five seconds of STDP we stimulated the same subset again This
time they recruited spikes from other neurons in the pattern completing it Figure
Upon varying the fraction of the pattern presented we found that the fraction recalled
increased faster than the fraction presented We selected subsets of the original pattern
randomly varying the fraction of neurons chosen from to ten trials for each We
classified neurons as active if they spiked in the two second period over which we recorded
Thus we characterized PEP?s pattern-recall performance as a function of the probability
that the pattern in question?s neurons are activated Figure At a fraction of presented nearly all of the neurons in the pattern are consistently activated showing robust pattern completion We fitted the recall performance with a sigmoid that reached
recall fraction with an input fraction of No spurious neurons were activated during any trials
Rate(Hz
Rate(Hz
A
Network activity
after STDP
Network activity
before STDP
Fraction of pattern actived
Fraction of pattern stimulated
Figure Associative recall A Before STDP half of the neurons in a pattern are stimulated
only they are activated After STDP half of the neurons in a pattern are stimulated and
all are activated The fraction of the pattern activated grows faster than the fraction
stimulated
Discussion
Our results demonstrate that PEP successfully compensates for graded variations in our silicon recurrent network using binary on?off synapses contrast with where weights
are graded While our chip results are encouraging variability was not eliminated in every
case In the case of the lowest input we see virtually no change Figure We
suspect the timing remains imprecise because with such low input neurons do not spike
every theta cycle and consequently provide fewer opportunities for the STDP synapses to
potentiate This shortfall illustrates the system?s limits it can only compensate for variability within certain bounds and only for activity appropriate to the PEP model
As expected STDP is the mechanism responsible for PEP. STDP potentiated recurrent
synapses from leading neurons to lagging neurons reducing the disparity among the diverse population of neurons Even though the STDP circuits are themselves variable with
different efficacies and time constants when using timing the sign of the weight-change
is always correct data not shown For this reason we chose STDP over other more
physiological implementations of plasticity such as membrane-voltage-dependent plasticity MVDP which has the capability to learn with graded voltage signals such as those
found in active dendrites providing more computational power
Previously we investigated a MVDP circuit which modeled a voltage-dependent NMDAreceptor-gated synapse It potentiated when the calcium current analog exceeded a
threshold which was designed to occur only during a dendritic action potential This circuit
produced behavior similar to STDP implying it could be used in PEP. However it was
sensitive to variability in the NMDA and potentiation thresholds causing a fraction of the
population to potentiate anytime the synapse received an input and another fraction to never
potentiate rendering both subpopulations useless Therefore the simpler less biophysical
STDP circuit won out over the MVDP circuit In our system timing is everything
Associative storage and recall naturally emerge in the PEP network when synapses between
neurons coactivated by a pattern are potentiated These synapses allow neurons to recruit
their peers when a subset of the pattern is presented thereby completing the pattern However this form of pattern storage and completion differs from Hopfield?s attractor model
Rather than forming symmetric recurrent neuronal circuits our recurrent network
forms asymmetric circuits in which neurons make connections exclusively to less excitable
neurons in the pattern In both the poisson-like and regular cases Figures only
about six percent of potentiated connections were reciprocated as expected by chance We
plan to investigate the storage capacity of this asymmetric form of associative memory
Our system lends itself to modeling brain regions that use precise spike timing such as
the hippocampus We plan to extend the work presented to store and recall sequences of
patterns as the hippocampus is hypothesized to do Place cells that represent different
locations spike at different phases of the theta cycle in relation to the distance to their preferred locations This sequential spiking will allow us to link patterns representing different
locations in the order those locations are visited thereby realizing episodic memory
We propose PEP as a candidate neural mechanism for information coding and storage in the
hippocampal system Observations from the CA1 region of the hippocampus suggest that
basal dendrites which primarily receive excitation from recurrent connections support
submillisecond timing precision consistent with PEP We have shown in a silicon
model PEP?s ability to exploit such fast recurrent connections to sharpen timing precision
as well as to associatively store and recall patterns
Acknowledgments
We thank Joe Lin for assistance with chip generation The Office of Naval Research funded
this work Award No.

<<----------------------------------------------------------------------------------------------------------------------->>

title: 1972-self-regulation-mechanism-of-temporally-asymmetric-hebbian-plasticity.pdf

Self-regulation Mechanism of Temporally
Asymmetric Hebbian Plasticity
Narihisa Matsumoto
Graduate School of Science and Engineering
Saitama University
RIKEN Brain Science Institute
Saitama Japan
xmatumo@brain.riken.go.jp
Masato Okada
RIKEN Brain Science Institute
Saitama Japan
okada@brain.riken.go.jp
Abstract
Recent biological experimental findings have shown that the synaptic plasticity depends on the relative timing of the pre and postsynaptic spikes which determines whether Long Term Potentiation
LTP occurs or Long Term Depression LTD does The synaptic
plasticity has been called Temporally Asymmetric Hebbian plasticity Many authors have numerically shown that spatiotemporal patterns can be stored in neural networks However the
mathematical mechanism for storage of the spatio-temporal patterns is still unknown especially the effects of LTD. In this paper
we employ a simple neural network model and show that interference of LTP and LTD disappears in a sparse coding scheme
On the other hand it is known that the covariance learning is indispensable for storing sparse patterns We also show that TAH
qualitatively has the same effect as the covariance learning when
spatio-temporal patterns are embedded in the network
Introduction
Recent biological experimental findings have indicated that the synaptic plasticity
depends on the relative timing of the pre and post synaptic spikes which determines whether Long Term Potentiation LTP occurs or Long Term Depression
LTD does LTP occurs when a presynaptic firing precedes a postsynaptic
one by no more than about In contrast LTD occurs when a presynaptic
firing follows a postsynaptic one A rapid transition occurs between LTP and LTD
within a time difference of a few ms Such a learning rule is called Temporally
Asymmetric Hebbian learning or Spike Timing Dependent synaptic
Plasticity STDP Many authors have numerically shown that spatio-temporal
patterns can be stored in neural networks Song discussed
the variablity of spike generation about the network consisting of spiking neurons
using TAH They found that the condition that the area of LTD was slightly
larger than that of LTP was indispensable of the stability Namely the balance of
LTP and LTD is crucial Yoshioka also discussed the associative memory network
consisting of spiking neurons using TAH He found that the area of LTP was
needed to be equal to that of LTD for stable retrieval Munro and Hernandez numerically showed that a network can retrieve spatio-temporal patterns even in a
noisy environment owing to LTD However they did not discuss the reason why
TAH was effective in terms of the storage and retrieval of the spatio-temporal patterns Since TAH has not only the effect of LTP but that of LTD the interference
of LTP and LTD may prevent retrieval of the patterns To investigate this unknown
mathematical mechanism for retrieval we employ an associative memory network
consisting of binary neurons To simplify the dynamics of internal potential enables
us to analyze the details of the retrieval process We use a learning rule that is
the similar formulation in the previous works We show the mechanism that the
spatio-temporal patterns can be retrieved in this network
There are many works concerned with associative memory networks that store
spatio-temporal patterns by the covariance learning Many biological findings imply that sparse coding schemes may be used in the brain It is wellknown that the covariance learning is indispensable when the sparse patterns are
embedded in a network as attractors The information on the firing rate
for the stored patterns is not indispensable for TAH although it is indispensable
for the covariance learning We theoretically show that TAH qualitatively has the
same effect as the covariance learning when the spatio-temporal patterns are embedded in the network This means that the difference in spike times induces LTP
or LTD and the effect of the firing rate information can be canceled out by this
spike time difference We conclude that this is the reason why TAH doesn?t require
the information on the firing rate for the stored patterns
Model
We investigate a network consisting of binary neurons that are connected mutually In this paper we consider the case of We use a neuronal model with
binary state We also use discrete time steps and the following synchronous
updating rule
ui
Jij
where is the state of the i-th neuron at time ui its internal potential
and a uniform threshold If the i-th neuron fires at time its state is
otherwise The specific value of the threshold is discussed later Jij is
the synaptic weight from the j-th neuron to the i-th neuron Each element of
the memory pattern
is generated independently by
Prob[?i Prob[?i
The expectation of is
and thus can be considered as the mean firing
rate of the memory pattern The memory pattern is sparse when and
this coding scheme is called sparse coding The synaptic weight Jij follows the
synaptic plasticity that depends on the difference in spike times between the i-th
post and j-th neurons The difference determines whether LTP occurs or
LTD does Such a learning rule is called Temporally Asymmetric Hebbian learning
or Spike Timing Dependent synaptic Plasticity This biological
LTP
LTP
Jij
Change in EPSP amplitude
experimental finding indicates that LTP or LTD is induced when the difference in
the pre and post-synaptic spike times falls within about Figure
We define that one time step in equations corresponds to in Figure
and a time duration within is ignored Figure Figure shows
that LTP occurs when the j-th neuron fires one time step before the i-th neuron
does and that LTD occurs when the j-th neuron fires one time step
after the i-th neuron does The previous work indicates the blance
of LTP and LTD is significant Therefore we define that the area of LTP is the
LTD
LTD
tpre tpost
tj ti
Figure Temporally Asymmetric Hebbian plasticity The result of biological
finding and the learning rule in our model LTP occurs when the j-th
neuron fires one time step before the i-th one On the contrary LTD occurs when
the j-th neuron fires one time step after the i-th one Synaptic weight Jij is followed
by this rule
same as that of LTD and that the amplitude of LTP is also the same as that of
LTD. On the basis of these definitions we employ the following learning rule
Jij
The number of memory patterns is where is defined as the loading
rate There is a critical value of loading rate If the loading rate is larger than
the pattern sequence becomes unstable is called the storage capacity
The previous works have shown that the learning method of equation can store
spatio-temporal patterns that is pattern sequences We show that memory
patterns are retrieved periodically like In other
words is retrieved at at and at
Here we discuss the value of threshold It is well-known that the threshold
value should be controlled according to the progress of the retrieval process timedependently One candidate algorithm for controlling the threshold value
is to maintain the mean firing rate of the network at that of memory pattern as
follows
It is known that the obtained threshold value is nearly optimal since it approximately gives a maximal storage capacity value
Theory
Many neural network models that store and retrieve sequential patterns by TAH
have been discussed by many authors They have numerically shown that
TAH is effective for storing pattern sequences For example Munro and Hernandez
showed that their model could retrieve a stored pattern sequence even in a noisy
environment However the previous works have not mentioned the reason why
TAH is effective Exploring such a mechanism is the main purpose of our paper
Here we discuss the mechanism that the network learned by TAH can store and
retrieve sequential patterns Before providing details of the retrieval process we
discuss a simple situation where the number of memory patterns is very small
relative to the number of neurons Let the state at time be the
same as the t-th memory pattern Then the internal potential of
the equation is given by
ui
ui depends on two independent random variables and according to the
equation The first term of the equation is a signal term for the recall of
the pattern which is designed to be retrieved at time and the second term
can interfere in retrieval of According to the equation ui takes a
value of or means that the interference of LTD exists If the
threshold is set between and isn?t influenced by the interference
of When and the interference does influence the
retrieval of We consider the probability distribution of the internal potential
ui to examine how the interference of LTD influences the retrieval of The
probability of and is that of and is
that of and is and that of and is
Then the probability distribution of is given by this equation
Prob(ui
Since the threshold is set between and the state is with
probability and with The overlap between the state x(t
and the memory pattern is given by
f)xi
In a sparse limit the probability of and approaches
This means that the interference of LTD disappears in a sparse limit and the model
can retrieve the next pattern Then the overlap approaches
Next we discuss whether the information on the firing rate is indispensable for
TAH or not To investigate this we consider the case that the number of memory
patterns is extensively large O(N Using the equation the internal
potential ui of the i-th neuron at time is represented as
ui mt zi
zi
zi is called the cross-talk noise which represents contributions from non-target
patterns excluding and prevents the target pattern from being retrieved
This disappeared in the finite loading case
It is well-known that the covariance learning is indispensable when the sparse patterns are embedded in a network as attractors Under sparse coding schemes
unless the covariance learning is employed the cross-talk noise does diverge in the
large limit Consequently the patterns can not be stored The information on
the firing rate for the stored patterns is not indispensable for TAH although it is
indispensable for the covariance learning We use the method of the statistical
neurodynamics to examine whether the variance of cross-talk noise diverges or not If a pattern sequence can be stored the cross-talk noise is obeyed
by a Gaussian distribution with mean and time-dependent variance Otherwise diverges Since is changing over time it is necessary to control
a threshold at an appropriate value at each time step According to the
statistical neurodynamics we obtain the recursive equations for the overlap mt
between the network state and the target pattern and the variance
The details of the derivation will be shown elsewhere Here we show the recursive
equations for mt and
2f
mt
a
2f 2f
2f 2f
a a
erf(y
exp Ca
These equations reveal that the variance of cross-talk noise does not diverge
as long as a pattern sequence can be retrieved This result means that TAH qualitatively has the same effect as the covariance learning
Next we discuss the mechanism that the variance of cross-talk noise does not diverge Let us consider the equation Synaptic weight Jij from j-th neuron to
i-th neuron is also derived as follows
Jij
This equation implies that TAH has the information on the firing rate of the memory
patterns when spatio-temporal patterns are embedded in a network Therefore
the variance of cross-talk noise doesn?t diverge and this is another factor for the
network learned by TAH to store and retrieve a pattern sequence We conclude
that the difference in spike times induces LTP or LTD and the effect of the firing
rate information can be canceled out by this spike times difference
Results
We investigate the property of our model and examine the following two conditions
a fixed threshold and a time-dependent threshold using the statistical neurodynamics and computer simulations
overlap solid activity/f dashed
Figure shows
how the overlap mt and the mean firing rate of the network
depend on the loading rate when the mean firing rate of
the memory pattern is and the threshold is where the storage
capacity is maximum with respect to the threshold The stored pattern sequence
can be retrieved when the initial overlap m1 is greater than the critical value
mC The lower line indicates how the critical initial overlap depends on the
loading rate In other words the lower line represents the basin of attraction
for the retrieved sequence The upper line denotes a steady value of overlap mt
when the pattern sequence is retrieved mt is obtained by setting the initial
state to the first memory pattern In this case the storage capacity is
The dashed line shows a steady value of the normalized mean firing rate
of network for the pattern sequence The data points and error bars indicate
the results of the computer simulations with neurons The former
indicates mean values and the latter does variances in trials Since the results
loading rate
Figure The critical overlap the lower line and the
overlap at the stationary state the upper line The
dashed line shows the mean firing rate of the network
divided firing rate which is The threshold is
and the number of neurons is The data points and
error bars show the means and variances respectively in
trials of computer simulations The storage capacity
is
of the computer simulations coincide with those of the statistical neurodynamics
hereafter we show the results only of the statistical neurodynamics
overlap solid activity/f dashed
Next we examine the threshold control scheme in the equation where the
threshold is controlled to maintain the mean firing rate of the network at
PN
in equation is equal to the mean firing rate because N1
under the condition Thus the threshold is adjusted to
satisfy the following equation
2f 2f
Figure shows the overlap mt as a function of loading rate with The
storage capacity is The basin of attraction becomes larger than that
of the fixed threshold condition Figure Thus the network becomes
robust against noise This means that even if the initial state is different from
the first memory pattern that is the state includes a lot of noise the pattern
sequence can be retrieved
loading rate
Figure The critical overlap the lower line and the
overlap at the stationary state the upper line when
the threshold is changing over time to maintain mean
firing rate of the network at The dashed line shows
the mean firing rates of the network divided firing rate
which is The basin of attraction become larger than
that of the fixed threshold condition Figure
Finally we discuss how the storage capacity depends on the firing rate of the
memory pattern It is known that the storage capacity diverges as log
in a
sparse limit Therefore we investigate the asymptotic property
of the storage capacity in a sparse limit Figure shows how the storage capacity
depends on the firing rate where the threshold is controlled to maintain the network
activity at symbol The storage capacity diverges as log
in a sparse limit
Figure The storage capacity as a function of in
the case of maintaining activity at symbol Ths
storage capacity diverges as log
in a sparse limit
Discussion
Using a simple neural network model we have discussed the mechanism that TAH
enables the network to store and retrieve a pattern sequence First we showed that
the interference of LTP and LTD disappeared in a sparse coding scheme This is
a factor to enable the network to store and retrieve a pattern sequence Next we
showed the mechanism that TAH qualitatively had the same effect as the covariance
learning by analyzing the stability of the stored pattern sequence and the retrieval
process by means of the statistical neurodynamics Consequently the variance of
cross-talk noise didn?t diverge and this is another factor for the network learned by
TAH to store and retrieve a pattern sequence We conclude that the difference in
spike times induces LTP or LTD and the effect of the firing rate information can
be canceled out by this spike times difference We investigated the property of our
model To improve the retrieval property of the basin of attraction we introduced
a threshold control algorithm where a threshold value was adjusted to maintain the
mean firing rate of the network at that of a memory pattern As a result we found
that this scheme enlarged the basin of attraction and that the network became
robust against noise We also found that the loading rate diverged as log
in a
sparse limit
Here we compare the storage capacity of our model with that of the model using
the covariance learning Figure The dynamical equations of the model using the
covariance learning is derived by Kitano and Aoyagi We calculate the storage
capacity COV
from their dynamical equations and compare these of our model
TCAH by the ratio of TCAH COV
The threshold control method is the same as
in this paper As decreases the ratio of storage capacities approaches The
contribution of LTD reduces the storage capacity of our model to half Therefore
in terms of the storage capacity the covariance learning is better than TAH. But as
we discussed previously the information of the firing rate is indispensable in TAH.
In biological systems to get the information of the firing rate is difficult
COV
TAH
Figure The comparison of the storage capacity of
our model with that of the model using the covariance
learning As decreases the ratio of storage capacity
approaches
log10f

<<----------------------------------------------------------------------------------------------------------------------->>

title: 4872-a-memory-frontier-for-complex-synapses.pdf

A memory frontier for complex synapses
Subhaneil Lahiri and Surya Ganguli
Department of Applied Physics Stanford University Stanford CA
sulahiri@stanford.edu sganguli@stanford.edu
Abstract
An incredible gulf separates theoretical models of synapses often described solely
by a single scalar value denoting the size of a postsynaptic potential from the
immense complexity of molecular signaling pathways underlying real synapses
To understand the functional contribution of such molecular complexity to learning and memory it is essential to expand our theoretical conception of a synapse
from a single scalar to an entire dynamical system with many internal molecular
functional states Moreover theoretical considerations alone demand such an expansion network models with scalar synapses assuming finite numbers of distinguishable synaptic strengths have strikingly limited memory capacity This raises
the fundamental question how does synaptic complexity give rise to memory To
address this we develop new mathematical theorems elucidating the relationship
between the structural organization and memory properties of complex synapses
that are themselves molecular networks Moreover in proving such theorems we
uncover a framework based on first passage time theory to impose an order on
the internal states of complex synaptic models thereby simplifying the relationship between synaptic structure and function
Introduction
It is widely thought that our very ability to remember the past over long time scales depends crucially
on our ability to modify synapses in our brain in an experience dependent manner Classical models
of synaptic plasticity model synaptic efficacy as an analog scalar value denoting the size of a postsynaptic potential injected into one neuron from another Theoretical work has shown that such
models have a reasonable extensive memory capacity in which the number of long term associations
that can be stored by a neuron is proportional its number of afferent synapses However
recent experimental work has shown that many synapses are more digital than analog they cannot
robustly assume an infinite continuum of analog values but rather can only take on a finite number
of distinguishable strengths a number than can be as small as two though see This
one simple modification leads to a catastrophe in memory capacity classical models with digital
synapses when operating in a palimpset mode in which the ongoing storage of new memories can
overwrite previous memories have a memory capacity proportional to the logarithm of the number
of synapses Intuitively when synapses are digital the storage of a new memory can flip
a population of synaptic switches thereby rapidly erasing previous memories stored in the same
synaptic population This result indicates that the dominant theoretical basis for the storage of long
term memories in modifiable synaptic switches is flawed
Recent work has suggested that a way out of this logarithmic catastrophe is to expand our
theoretical conception of a synapse from a single scalar value to an entire stochastic dynamical system in its own right This conceptual expansion is further necessitated by the experimental reality
that synapses contain within them immensely complex molecular signaling pathways with many internal molecular functional states see 13 While externally synaptic efficacy could be
digital candidate patterns of electrical activity leading to potentiation or depression could yield transitions between these internal molecular states without necessarily inducing an associated change in
synaptic efficacy This form of synaptic change known as metaplasticity can allow the
probability of synaptic potentiation or depression to acquire a rich dependence on the history of
prior changes in efficacy thereby potentially improving memory capacity
Theoretical studies of complex metaplastic synapses have focused on analyzing the memory performance of a limited number of very specific molecular dynamical systems characterized by a
number of internal states in which potentiation and depression each induce a specific set of allowable transitions between states see Figure below While these models can vastly outperform
simple binary synaptic switches these analyses leave open several deep and important questions
For example how does the structure of a synaptic dynamical system determine its memory performance What are the fundamental limits of memory performance over the space of all possible
synaptic dynamical systems What is the structural organization of synaptic dynamical systems that
achieve these limits Moreover from an experimental perspective it is unlikely that all synapses
can be described by a single canonical synaptic model just like the case of neurons there is an
incredible diversity of molecular networks underlying synapses both across species and across brain
regions within a single organism In order to elucidate the functional contribution of this diverse molecular complexity to learning and memory it is essential to move beyond the analysis of
specific models and instead develop a general theory of learning and memory for complex synapses
Moreover such a general theory of complex synapses could aid in development of novel artificial
memory storage devices
Here we initiate such a general theory by proving upper bounds on the memory curve associated with
any synaptic dynamical system within the well established ideal observer framework of
Along the way we develop principles based on first passage time theory to order the structure of
synaptic dynamical systems and relate this structure to memory performance We summarize our
main results in the discussion section
Overall framework synaptic models and their memory curves
In this section we describe the class of models of synaptic plasticity that we are studying and how
we quantify their memory performance In the subsequent sections we will find upper bounds on
this performance
We use a well established formalism for the study of learning and memory with complex synapses
In this approach electrical patterns of activity corresponding to candidate potentiating and depressing plasticity events occur randomly and independently at all synapses at a
Poisson rate These events reflect possible synaptic changes due to either spontaneous network
activity or the storage of new memories We let pot and dep denote the fraction of these events that
are candidate potentiating or depressing events respectively Furthermore we assume our synaptic
model has internal molecular functional states and that a candidate potentiating depotentiating event induces a stochastic transition in the internal state described by an discrete time
Markov transition matrix Mpot Mdep In this framework the states of different synapses will be
independent and the entire synaptic population can be fully described by the probability distribution
across these states which we will indicate with the row-vector Thus the i?th component of
denotes the fraction of the synaptic population in state Furthermore each state has its own
synaptic weight which we take in the worst case scenario to be restricted to two values After
shifting and scaling these two values we can assume they are without loss of generality
We also employ an ideal observer approach to the memory readout where the synaptic weights
are read directly This provides an upper bound on the quality of any readout using neural activity
For any single memory stored at time we assume there will be an ideal pattern of synaptic
weights across a population of synapses the element vector
ideal that is at all synapses
that experience a candidate potentiation event and at all synapses that experience a candidate
depression event at the time of memory storage We assume that any pattern of synaptic weights
close to
ideal is sufficient to recall the memory However the actual pattern of synaptic weights at
some later time will change to
due to further modifications from the storage of subsequent
memories We can use the overlap between these
ideal
as a measure of the quality of the
memory As the system will return to its steady state distribution which will be uncorrelated
Cascade model
Serial model
SNR
Cascade
Serial
Time
Figure Models of complex synapses The cascade model of showing transitions between
states of high/low synaptic weight red/blue circles due to potentiation/depression solid red/dashed
blue arrows The serial model of The memory curves of these two models showing
the decay of the signal-to-noise ratio to be defined in as subsequent memories are stored
with the memory stored at The probability distribution of the quantity
ideal
can be
used as a null model for comparison
The extent to which the memory has been stored is described by a signal-to-noise ratio SNR
hw
ideal w(t)i
hw
ideal
SNR(t
Var(w
ideal
The noise in the denominator is essentially There is a correction when potentiation and depression are imbalanced but this will not affect the upper bounds that we will discuss below and
will be ignored in the subsequent formulae
A simple average memory curve can be derived as follows All of the preceding plasticity events
prior to will put the population of synapses in its steady-state distribution The memory we are tracking at will change the internal state distribution to Mpot Mdep
in those synapses that experience a candidate potentiation depression event As the potentiating/depressing nature of the subsequent memories is independent of
ideal we can average over all
sequences resulting in the evolution of the probability distribution
dp(t
rp(t)WF
where WF pot Mpot dep Mdep I.
dt
Here WF is a continuous time transition matrix that models the process of forgetting the memory
stored at time due to random candidate potentiation/depression events occurring at each
synapse due to the storage of subsequent memories Its stationary distribution is
This results in the following SNR
SNR(t 2f pot dep Mpot Mdep ertW
A detailed derivation of this formula can be found in the supplementary material We will frequently
refer to this function as the memory curve It can be thought of as the excess fraction of synapses
relative to equilibrium that maintain their ideal synaptic strength at time as dictated by the stored
memory at time
Much of the previous work on these types of complex synaptic models has focused on understanding
the memory curves of specific models or choices of Mpot/dep Two examples of these models are
shown in Figure We see that they have different memory properties The serial model performs
relatively well at one particular timescale but it performs poorly at other times The cascade model
does not perform quite as well at that time but it maintains its performance over a wider range of
timescales
In this work rather than analyzing specific models we take a different approach in order to obtain
a more general theory We consider the entire space of these models and find upper bounds on the
memory capacity of any of them The space of models with a fixed number of internal states is
parameterized by the pair of discrete time stochastic transition matrices Mpot and Mdep in
addition to pot/dep The parameters must satisfy the following constraints
Mpot/dep
ij
Mpot/dep
ij
pot/dep
pot dep
WF
and pot/dep follow automatically from the other constraints
The upper bounds on Mpot/dep
ij
The critical question is what do these constraints imply about the space of achievable memory
curves in To answer this question especially for limits on achievable memory at finite times it
will be useful to employ the eigenmode decomposition
WF
qa ua va va ub ab WF ua qa ua va WF qa va
a
Here qa are the negative of the eigenvalues of the forgetting process WF ua are the right column
eigenvectors and va are the left row eigenvectors This decomposition allows us to write the
memory curve as a sum of exponentials
SNR(t
Ia e?rt/?a
a
where Ia pot dep Mpot Mdep ua va and a We can then ask the question
what are the constraints on these quantities namely eigenmode initial SNR?s Ia and time constants
a implied by the constraints in We will derive some of these constraints in the next section
Upper bounds on achievable memory capacity
In the previous section in we have described an analytic expression for a memory curve as a
function of the structure of a synaptic dynamical system described by the pair of stochastic transition
matrices Mpot/dep Since the performance measure for memory is an entire memory curve and not
just a single number there is no universal scalar notion of optimal memory in the space of synaptic
dynamical systems Instead there are tradeoffs between storing proximal and distal memories often
attempts to increase memory at late early times by changing Mpot/dep incurs a performance loss
in memory at early late times in specific models considered so far Thus our end goal
achieved in is to derive an envelope memory curve in the SNR-time plane or a curve that forms
an upper-bound on the entire memory curve of any model In order to achieve this goal in this
section we must first derive upper bounds over the space of all possible synaptic models on two
different scalar functions of the memory curve its initial SNR and the area under the memory curve
In the process of upper-bounding the area we will develop an essential framework to organize the
structure of synaptic dynamical systems based on first passage time theory
Bounding initial SNR
We now give an upper bound on the initial SNR
2f pot dep Mpot Mdep
over all possible models and also find the class of models that saturate this bound A useful quantity
is the equilibrium probability flux between two disjoint sets of states A and
XX
AB
rp
Wij
i?A j?B
The initial SNR is closely related to the flux from the states with to those with wj
supplementary material
This inequality becomes an equality if potentiation never decreases the synaptic weight and depression never increases it which should be a property of any sensible model
To maximize this flux potentiation from a weak state must be guaranteed to end in a strong state
and depression must do the reverse An example of such a model is shown in Figure These
models have a property known as lumpability for the discrete time version and
for continuous time They are completely equivalent have the same memory curve as
a two state model with transition probabilities equal to as shown in Figure
Figure Synaptic models that maximize initial SNR. For potentiation all transitions starting
from a weak state lead to a strong state and the probabilities for all transitions leaving a given weak
state sum to Depression is similar to potentiation but with strong and weak interchanged
The equivalent two state model with transition probabilities under potentiation and depression
equal to one
This two state model has the equilibrium distribution dep pot and its flux is given by
rf pot dep This is maximized when pot dep leading to the upper bound
We note that while this model has high initial SNR it also has very fast memory decay with a
timescale 1r As the synapse is very plastic the initial memory is encoded very easily but
the subsequent memories also overwrite it rapidly This is one example of the tradeoff between
optimizing memory at early versus late times
Imposing order on internal states through first passage times
Our goal of understanding the relationship between structure and function in the space of all possible
synaptic models is complicated by the fact that this space contains many different possible network
topologies encoded in the nonzero matrix elements of Mpot/dep To systematically analyze this
entire space we develop an important organizing principle using the theory of first passage times
in the stochastic process of forgetting described by WF The mean first passage time matrix Tij
is defined as the average time it takes to reach state for the first time starting from state The
diagonal elements are defined to be zero
A remarkable theorem we will exploit is that the quantity
Tij
known as Kemeny?s constant is independent of the starting state Intuitively
states that the average time it takes to reach any state weighted by its equilibrium probability is
independent of the starting state implying a hidden constancy inherent in any stochastic process
In the context of complex synapses we can define the partial sums
Tij
Tij
These can be thought of as the average time it takes to reach the strong/weak states respectively
Using these definitions we can then impose an order on the states by arranging them in order of
decreasing or increasing Because is independent of the two orderings are
the same In this order which depends sensitively on the structure of Mpot/dep states later to the
right in figures below can be considered to be more potentiated than states earlier to the left in
figures below despite the fact that they have the same synaptic efficacy In essence in this order a
state is considered to be more potentiated if the average time it takes to reach all the strong efficacy
states is shorter We will see that synaptic models that optimize various measures of memory have
an exceedingly simple structure when and only when their states are arranged in this order.1
Note that we do not need to worry about the order of the changing during the optimization necessary
conditions for a maximum only require that there is no infinitesimal perturbation that increases the area Therefore we need only consider an infinitesimal neighborhood of the model in which the order will not change
Figure Perturbations that increase the area Perturbations that increase elements of Mpot
above the diagonal and decrease the corresponding elements of Mdep It can no longer be used
when Mdep is lower triangular depression must move synapses to more depressed states
Perturbations that decrease elements of Mpot below the diagonal and increase the corresponding
elements of Mdep It can no longer be used when Mpot is upper triangular potentiation must
move synapses to more potentiated states Perturbation that decreases shortcut transitions
and increases the bypassed direct transitions It can no longer be used when there are only nearestneighbor direct transitions
Bounding area
Now consider the area under the memory curve
A
dt SNR(t
We will find an upper bound on this quantity as well as the model that saturates this bound
First passage time theory introduced in the previous section becomes useful because the area has a
simple expression in terms of quantities introduced in supplementary material
pot
dep
A pot dep
ij
ij
ij
pot dep
dep
Mpot
ij Mij
ij
With the states in the order described above we can find perturbations of Mpot/dep that will always
increase the area whilst leaving the equilibrium distribution unchanged Some of these perturbations are shown in Figure see supplementary material for details For example in Figure
for two states on the left and on the right with being more potentiated than
dep
we have proven that increasing Mpot
ij and decreasing Mij leads to an increase in area The only
thing that can prevent these perturbations from increasing the area is when they require the decrease
of a matrix element that has already been set to This determines the topology non-zero transition
probabilities of the model with maximal area It is of the form shown in Figure 4(c),with potentiation moving one step to the right and depression moving one step to the left Any other topology
would allow some class of perturbations in Figure to further increase the area
As these perturbations do not change the equilibrium distribution this means that the area of any
model is bounded by that of a linear chain with the same equilibrium distribution The area of
a linear chain model can be expressed directly in terms of its equilibrium state distribution
yielding the following upper bound on the area of any model with the same supplementary
material
jp
jp
A
pk
where we chose wk sgn[k jp
We can then maximize this by pushing all of the equilibrium distribution symmetrically to the two end states This can be done by reducing the transition
probabilities out of these states as in Figure This makes it very difficult to exit these states
once they have been entered The resulting area is
A
This analytical result is similar to a numerical result found in under a slightly different information theoretic measure of memory performance
The sticky end states result in very slow decay of memory but they also make it difficult to encode
the memory in the first place since a small fraction of synapses are able to change synaptic efficacy
during the storage of a new memory Thus models that maximize area optimize memory at late
times at the expense of early times
Memory curve envelope
Now we will look at the implications of the upper bounds found in the previous section for the SNR
at finite times As argued in the memory curve can be written in the form
SNR(t
Ia e?rt/?a
a
The upper bounds on the initial SNR and the area imply the following constraints on the
parameters Ia a
Ia
Ia a
a
a
We are not claiming that these are a complete set of constraints not every set Ia a that satisfies
these inequalities will actually be achievable by a synaptic model However any set that violates
either inequality will definitely not be achievable
Now we can pick some fixed time t0 and maximize the SNR at that time wrt the parameters
Ia a subject to the constraints above This always results in a single nonzero Ia in essence
optimizing memory at a single time requires a single exponential The resulting optimal memory
curve along with the achieved memory at the chosen time depends on t0 as follows
t0
SNR(t e?rt/(M
SNR(t0 e?rt0
t0
SNR(t
SNR(t0
rt0
ert0
Both the initial SNR bound and the area bound are saturated at early times At late times only
the area bound is saturated The function SNR(t0 the green curve in Figure above forms a
memory curve envelope with late-time power-law decay
No synaptic model can have an
SNR that is greater than this at any time We can use this to find an upper bound on the memory
lifetime by finding the point at which the envelope crosses
er
where we assume Intriguingly both the lifetime and memory envelope expand linearly
with the number of internal states and increase as the square root of the number of synapses
This leaves the question of whether this bound is achievable At any time can we find a model
whose memory curve touches the envelope The red curves in Figure show the closest we
have come to the envelope with actual models by repeated numerical optimization of SNR(t0 over
Mpot/dep with random initialization and by hand designed models
We see that at early but not late times there is a gap between the upper bound that we can prove
and what we can achieve with actual models There may be other models we haven?t found that
could beat the ones we have and come closer to our proven envelope However we suspect that the
area constraint is not the bottleneck for optimizing memory at times less than
We believe
there is some other constraint that prevents models from approaching the envelope and currently are
exploring several mathematical conjectures for the precise form of this constraint in order to obtain
a potentially tighter envelope Nevertheless we have proven rigorously that no model?s memory
curve can ever exceed this envelope and that it is at least tight for late times longer than
where models of the form in Figure 4(c)can come close to the envelope
Discussion
We have initiated the development of a general theory of learning and memory with complex
synapses allowing for an exploration of the entire space of complex synaptic models rather than
SNR
envelope
numerical search
hand designed
Area bound active
Initial SNR bound active
Time
Figure The memory curve envelope for An upper bound on the SNR
at any time is shown in green The red dashed curve shows the result of numerical optimization of
synaptic models with random initialization The solid red curve shows the highest SNR we have
found with hand designed models At early times these models are of the form shown in with
different numbers of states and all transition probabilities equal to At late times they are of the
form shown in with different values of The model shown in also saturates the area bound
in the limit
analyzing individual models one at a time In doing so we have obtained several new mathematical results delineating the functional limits of memory achievable by synaptic complexity and the
structural characterization of synaptic dynamical systems that achieve these limits In particular
operating within the ideal observer framework of we have shown that for a population
of synapses with internal states the initial SNR of any synaptic model cannot exceed
and any model that achieves this bound is equivalent to a binary synapse the area under the
memory curve of any model cannot exceed that of a linear chain model with the same
equilibrium
distribution both the area and memory lifetime of any model cannot exceed and the
model that achieves this limit has a linear chain topology with only nearest neighbor transitions
we have derived an envelope memory curve in the SNR-time plane that cannot be exceeded by the
memory curve of any model and models that approach this envelope for times greater
than
are linear chain models and this late-time envelope is a power-law proportional to
indicating that synaptic complexity can strongly enhance the limits of achievable memory
This theoretical study opens up several avenues for further inquiry In particular the tightness of our
envelope for early times less than
remains an open question and we are currently pursuing
several conjectures We have also derived memory constrained envelopes by asking in the space
of models that achieve a given SNR at a given time what is the maximal SNR achievable at other
times If these two times are beyond a threshold separation optimal constrained models require
two exponentials It would be interesting to systematically analyze the space of models that achieve
good memory at multiple times and understand their structural organization and how they give rise
to multiple exponentials leading to power law memory decays
Finally it would be interesting to design physiological experiments in order to perform optimal
systems identification of potential Markovian dynamical systems hiding within biological synapses
given measurements of pre and post-synaptic spike trains along with changes in post-synaptic potentials Then given our theory we could match this measured synaptic model to optimal models to
understand for which timescales of memory if any biological synaptic dynamics may be tuned
In summary we hope that a deeper theoretical understanding of the functional role of synaptic
complexity initiated here will help advance our understanding of the neurobiology of learning and
memory aid in the design of engineered memory circuits and lead to new mathematical theorems
about stochastic processes
Acknowledgements
We thank Sloan Genenetech Burroughs-Wellcome and Swartz foundations for support We thank
Larry Abbott Marcus Benna Stefano Fusi Jascha Sohl-Dickstein and David Sussillo for useful
discussions

<<----------------------------------------------------------------------------------------------------------------------->>

title: 353-self-organization-of-hebbian-synapses-in-hippocampal-neurons.pdf

Self-organization of Hebbian Synapses
in Hippocampal Neurons
Thomas H. Brown,t Zachary F. Mainen,t Anthony M. Zador,t and Brenda J. Claiborne
Department of Psychology
Division of Life Sciences
Yale University
University of Texas
New Haven cr
San Antonio TX
ABSTRACT
We are exploring the significance of biological complexity for neuronal
computation Here we demonstrate that Hebbian synapses in realistically-modeled hippocampal pyramidal cells may give rise to two novel
forms of self-organization in response to structured synaptic input First
on the basis of the electrotonic relationships between synaptic contacts
a cell may become tuned to a small subset of its input space Second the
same mechanisms may produce clusters of potentiated synapses across
the space of the dendrites The latter type of self-organization may be
functionally significant in the presence of nonlinear dendritic conductances
INTRODUCTION
Long-term potentiation LTP is an experimentally observed form of synaptic plasticity
that has been interpreted as an instance of a Hebbian modification Kelso al
Brown al The induction ofLTP requires synchronous presynaptic activity and
postsynaptic depolarization Kelso al We have previously developed a detailed
biophysical model of the LTP observed at synapses onto hippocampal region CAl pyrami
39
Brown Mainen Zador and Claiborne
Figure Two-dimensional projection of a reconstructed hippocampal CAl pyramidal cell
dal neurons Zador al The synapses at which this form of LTP occurs are distributed across an extensive dendritic arbor During synaptic stimulation the
membrane voltage at each synapse is different In this way a biological neuron differs
from the processing elements typically used in neural network models where the postsynaptic activity can be represented by a single state variable We have developed an electrotonic model based on an anatomically reconstructed neuron We have used this model to
explore how the spatial distribution of inputs and the temporal relationships of their activation affect synaptic potentiation
THE NEURONAL MODEL
Standard compartmental modeling techniques were used to represent the electrical structure of hippocampal CAl pyramidal cells
MORPHOLOGY AND ELECTRICAL PARAMETERS
Morphometric data were obtained from three-dimensional reconstructions Brown
of hippocampal neurons A correction factor was applied to the membrane
area based on an estimate for spine density of The original measurements divided
a single neuron into cylinders with an average length of For simulation
purposes this structure was collapsed into compartments preserving the connectivity pattern and changes in process diameter Electrical constants were Rm 70 ID-cm
em JlF'lcrrll Ri n-cm Spruston Johnston The membrane was electrically passive Synaptic currents were modeled as the sum of fast AMPA and slow NMDA
conductances on the head of a two-compartment spine Zador The AMPA
conductance was represented by an alpha function Jack with time constant of
msec Brown and Johnston The NMDA conductance was represented by a
more complicated function with two time constants and a voltage dependence due to voltage-sensitive channel blocking by ions Zador Brown
The initial peak conductances gAMPA and gNMDA were set to and nS respectively
Self-organization of Hebbian Synapses in Hippocampal Neurons
SIMULATION AND SYNAPTIC MODIFICATION
Simulations were run on a Sun workstation using a customized version of NEURON
a simulator developed by Michael Hines Hines Prior to a simulation patterns of
synapses were selected at random from a pool of synapses distributed unifonnly over
the apical and basal dendrites Simulations were divided into trials of msec At the
beginning of each trial a particular pattern of synapses was activated synchronously
stimuli at intervals of msec The sequential presentation of all selected patterns
constituted an epoch An entire simulation consisted of presentation epochs Over the
course of each trial membrane potential was computed at each location in the dendritic
tree and these voltages were used to compute weight changes according to the
Hebbian algorithm described below After each trial the actual peak AMPA conductances
gAMPA hereafter denoted were scaled by the sigmoidal function
gmax
where detennines the steepness of the sigmoid and gfM was set to nS
The rule for synaptic modification was based on a biophysical interpretation Kairiss aI
Brown aI of a generalized bilinear fonn of Hebbian algorithm Brown
where a and are functionals.l is a constant represents postsynaptic activity and
a represents presynaptic activity This equation specifies an interactive fonn of synaptic
ehhancement combined with three noninteractive forms of synaptic depression all of
which have possible neurobiological analogs Brown aI The interactive tenn was
derived from a biophysical model of LTP induction in a spine Zador A simplified version of this model was used to compute the concentration of Ca bound calmodulin It has been suggested that CaM-C84 may trigger protein kinases
responsible for LTP induction In general was a nonlinear function of subsynaptic voltage Zador al
The biophysical mechanisms underlying synaptic depression are less well understood The
constant represents a passive decay process and was generally set to zero The functional
represents heterosynaptic depression based on postsynaptic activity In these simulations was proportional the amount of depolarization of the subsynaptic membrane from
resting potential The functional represents homosynaptic depression based
on presynaptic activity Were was proportional to the AMPA conductance which can
be considered a measure of exclusively presynaptic activity because it is insensitive to
postsynaptic voltage The three activity-dependent tenns were integrated over the period
of the trial in order to obtain a measure of weight change Reinterpreting a and Yas constants the equation is thus
ial
a CamCa
V.rYII
YgAMPA dt
41
42
Brown Mainen Zador and Claiborne
tOO
m.sec
tOO
m.sec
epochs
Figure Interactions among Hebbian synapses produce differing global effects winning and
losing patterns on the basis of the spatial distribution of synapses The PSP always measured
at the soma due to two different patterns of synapses are plotted as a function of the presentation
epoch Initially pattern solid line evoked a slightly greater PSP than pattern dotted line inset top right Mter epochs these responses were reversed thePSP due to pattern was depressed while the PSP due to pattern was potentiated inset top left
RESULTS
Analysis of the simulations revealed self-organization in the form of differential modification of synaptic strengths Mainen Two aspects of the self-organization phenomena were distinguished In some simulations a form of pattern selection was observed
in which clear winners and losers emerged In other simulations the average synaptic
efficacy remained about the same but spatial heterogeneities~lustering~f synaptic
strength developed Different measures were used to assess these phenomena
PATTERNSELECTION
The change in the peak postsynaptic potential recorded at the soma SP provided one useful measure of pattern selection In many simulations pattern selection resulted in a
marked potentiation of the PSP due to some patterns and a depression of the PSP due to
others The PSP can be regarded as an indirect measure of the functional consequence of
self-organization In the simulation illustrated in patterns of synapses produced
an average PSP of mV before learning After learning responses ranged from to
of this amount Underlying.pattern selection was a ch8!!ge in the average peak synaptic conductance for the patterng8YIIO).1 The initial value of g8YII was same for all patterns and its final value was bounded by eq In many simulations g8YII approached the
upper bound for some patterns and the lower bound for other patterns In this way
the neuron became selectively tuned to a subset of its original set of inputs The specificity
Self-organization of Hebbian Synapses in Hippocampal Neurons
epochs
Figure The mean synaptic conductance gSy"of two patterns is plotted as a function of the presentation epoch Both patterns began with iaenucal total synaptic strength synapses with gs,r
Synaptic conductances were constrained to the range nS Mter twenty epochs
gSY of pattern solid line approached the minimum ofO.OnS while gsy of pattern dotted line
approached the maximum of nS
of this tuning was dependent on the parameter values of the neuronal model learning rule
and stimulus set
CLUSTER FORMAnON
Heterogeneity in the spatial distribution of strengthened and weakened synapses was often
observed After learning spatial clusters of synapses with similar conductances formed
These spatial heterogeneities can be illustrated in several ways In one convenient method
Brown synapses are represented as colored points superimposed on a rendition of the neuronal morphology as illustrated in By COlor-coding gsyn for each
synapse in a pattern correlations in synaptic strength across dendritic space are immediately apparent In a second method better suited to the monochrome graphics available in the
present text the evolution of the variance of gsyn is plotted as a function of time
In the simulation illustrated here the increase in variance was due to the formation of a single relatively large cluster of strengthened synapses Within other parameter regimes multiple clusters of smaller size were formed
DISCUSSION
The important differences between synaptic modifications in the biophysically-modeled
neuron and those in simple processing elements arise from voltage gradients present in the
realistic model Brown Kairiss In standard processing elements
Although SJ and the somatic PSP were generally correlated the relationship between the two is
not linear as was often evident in simulations compare initial trials in Figs and
43
44
Brown Mainen Zador and Claiborne
til
epochs
Figure Synaptic heterogeneity is indicated by increases in the variance of the set of synaptic
conductances for each pattern The variances of the peak synaptic conductances of patterns
are plotted as ajy)lction of the epoch The variance of all patterns approached the theoretical
maximum of In this parameter regime the variance was due to the potentiation of a single
large cluster of synapes combined with the depression of other synapses
a single state variable represents postsynaptic activity In contrast the critical subsynaptic
voltages which represent postsynaptic activity in the neuron are correlated but are not strictly equal The structure and electrical properties of the cell interact with its synaptic input
to detennine the precise spatiotemporal pattern of membrane voltage Thus the voltage at
any synapse depends strongly on its electrotonic relationships to other active synapses The
way in which this local depolarization affects the nature of self-organization depends on the
specific mechanisms of the synaptic modification rule We have modeled a pair of opposing voltage-dependent mechanisms An interactive potentiation mechanism the functional
ex promotes cooperativity between spatially proximal synapses with temporally correlated
activity A heterosynaptic depression mechanism the functional which is independent
of presynaptic activity promotes competition among spatially proximal synapses
Through mechanisms such as these the specific electrotonic structure of a neuron predetennines a complex set of interactions between any given spatial distribution of synaptic
inputs We have shown that these higher-order interactions can give rise to self-organization with at least two interesting effects
SPARSE REPRESENTATION
The phenomenon of pattern selection demonstrates how Hebbian self-organization may
naturally tune neurons to respond to a subset of their input space This tuning mechanism
might allow a large field of neurons to develop a sparse coding of the activity in a set of
input fibers since each neuron would respond to a particular small portion of the input
space Sparse coding may be advantageous to associative learning and other types of neural
computation Kanerva
Self-organization of Hebbian Synapses in Hippocampal Neurons
CLUSTERING AND NONLINEAR COMPUTATION
The fonnation of clusters of strengthened synapses illustrates a property of Hebbian selforganization whose functional significance might only be appreciated in the presence of
nonlinear voltage-dependent dendritic conductances We have examined the self-organization process in an electrically passive neuron Under these conditions the presence of
clustering within patterns has little effect on the observed output In fact it is known that
hippocampal cells of the type modeled possess a variety of spatially heterogeneous nonlinear dendritic conductances Jones The computational role of such nonlinearities is just beginning to be explored It is possible that interactions between synaptic
clustering and nonlinear membrane patches may significantly affect both the perfonnance
of dendritic computations and the process of self-organization itself
Acknowledgments
This research was supported by grants from the Office of Naval Research the Defense Advanced Research Projects Agency and the Air Force Office of Scientific Research

<<----------------------------------------------------------------------------------------------------------------------->>

title: 4364-two-is-better-than-one-distinct-roles-for-familiarity-and-recollection-in-retrieving-palimpsest-memories.pdf

Two is better than one distinct roles for familiarity
and recollection in retrieving palimpsest memories
Cristina Savin1
cs664@cam.ac.uk
Peter Dayan2
dayan@gatsby.ucl.ac.uk
M?at?e Lengyel1
m.lengyel@eng.cam.ac.uk
Computational Biological Learning Lab Dept of Engineering University of Cambridge UK
Gatsby Computational Neuroscience Unit University College London UK
Abstract
Storing a new pattern in a palimpsest memory system comes at the cost of interfering with the memory traces of previously stored items Knowing the age of
a pattern thus becomes critical for recalling it faithfully This implies that there
should be a tight coupling between estimates of age as a form of familiarity and
the neural dynamics of recollection something which current theories omit Using a normative model of autoassociative memory we show that a dual memory
system consisting of two interacting modules for familiarity and recollection has
best performance for both recollection and recognition This finding provides a
new window onto actively contentious psychological and neural aspects of recognition memory
Introduction
Episodic memory such as that in the hippocampus acts like a palimpsest each new entity to be
stored is overlaid on top of its predecessors and in turn is submerged by its successors This implies
both anterograde interference existing memories hinder the processing of new ones and retrograde
interference new memories overwrite information about old ones Both pose important challenges
for the storage and retrieval of information in neural circuits Some aspects of these challenges have
been addressed in two theoretical frameworks one focusing on anterograde interference through the
interaction of novelty and storage the other on retrograde interference in individual synapses
However neither fully considered the critical issue of retrieval from palimpsests this is our focus
First made the critical observation that autoassociative memories only work if normal recall
dynamics are suppressed on presentation of new patterns that need to be stored Otherwise rather
than memorizing the new pattern the memory associated with the existing pattern that most closely
matches the new input will be strengthened This suggests that it is critical to have a mechanism for
assessing pattern novelty or conversely familiarity a function that is often ascribed to neocortical
areas surrounding the hippocampus
Second considered the palimpsest problem of overwriting information in synapses whose efficacies have limited dynamic ranges They pointed out that this can be at least partially addressed
through allowing multiple internal states for instance forming a cascade for each observable synaptic efficacy level However although provide an attractive formalism for analyzing and optimizing synaptic storage a retrieval mechanism associated with this storage is missing
a
potentiation
depression
Figure a The cascade model Internal states of a synapse circles can express one of two different
efficacies columns Transitions between states are stochastic and can either be potentiating
or depressing depending on pre and postsynaptic activities Probabilities of transitions between
states expressing the same efficacy and between states expressing different efficacies decrease
geometrically with cascade depth Generative model for the autoassociative memory task The
is a noisy version of one of the stored patterns Upon storing pattern synaptic states
recall cue
changed from V0 sampled from the stationary distribution of synaptic dynamics to V1 Recall
occurs after the presentation of intervening patterns when synapses are in states Vt with
are observed at recall
corresponding synaptic efficacies Wt Only Wt and
Although these pieces of work might seem completely unrelated we show here that they are closely
linked via retrieval The critical fact about recall from memory in general is to know how the information should appear at the time of retrieval In the case of a palimpsest the trace of a memory
in the synaptic efficacies depends critically on the age of the memory its relative familiarity
This suggests a central role for novelty familiarity signals during recollection Indeed we show
retrieval is substantially worse when familiarity is not explicitly represented than when it is
Dual system models for recognition memory are the topic of a heated debate Our results
could provide a computational rationale for them showing that separating a perirhinal-like network
involved in familiarity from a hippocampal-like network can be beneficial even when the only
task is recollection We also show that the task of recognition can also be best accomplished by
combining the outputs of both networks as suggested experimentally
Storage in a palimpsest memory
We consider the task of autoassociative recall of binary patterns from a palimpsest memory Specifically the neural circuit consists of binary neurons that enjoy all-to-all connectivity During
storage network activity is clamped to the presented pattern inducing changes in the synapses
internal states and corresponding observed binary efficacies
At recall we seek to retrieve a pattern that was originally stored given a noisy cue
and the
current weight matrix W. This weight matrix is assumed to result from storing on top of the
stationary distribution of the synaptic efficacies coming from the large number of patterns that had
been previously stored and then subsequently storing a sequence of other intervening patterns
with the same statistics on top of
In more detail a pattern to be stored has density and is drawn from the distribution
Pstore
Pstore
The recall cue is a noisy version of the original pattern modeled using a binary symmetric channel
Pnoise
Pnoise
Pnoise r)xi
x?i
where defines the level of input noise
rxi
The recall time is assumed to come from a geometric distribution with mean
Precall
The synaptic learning rule is local and stochastic with the probability of an event actually leading
to state changes determined by the current state of the synapse Vij and the activity at the pre and
post-synaptic neurons and Hence learning is specified through a set of transition matrices
with P(Vij0 l0 Vij For convenience we adopted the cascade model which assumes that the probability of potentiation and depression decays
with cascade depth as a geometric progression qi with qn to compensate for
boundary effects The transition between metastates is given by
with the correction
factors
and ensuring that different metastates are equally occupied for different pattern sparseness values Furthermore we assume synaptic changes occur only when the
postsynaptic neuron is active leading to potentiation if the presynaptic neuron is also active and to
depression otherwise The specific form of the learning rule could influence the memory span of the
network but we expect it not to change the results below qualitatively
The evolution of the distribution over synaptic states after encoding can be described by a Markov
process with a transition matrix given as the averageP
change in synaptic states expected after
storing an arbitrary pattern from the prior Pstore Pstore Pstore
Additionally we define the column vectors and for the distribution of the
synaptic states and observable efficacies respectively when one of the patterns stored was
such that lW P(Wij l|xi and lV P(Vij l|xi Given these
definitions we can express the final distribution over synaptic states as
Precall
M(xi
where we start from the stationary distribution the eigenvector of for eigenvalue encode
pattern and then additional patterns from the same distribution The corresponding
weight distribution is where is a 2n matrix defining the
deterministic mapping from synaptic states to observable efficacies
The fact that the recency of the pattern to be recalled appears in equation implies that pattern age
will strongly influence information retrieval In the following we consider two possible solutions
to this problem We first show the limitations of recall dynamics that involve a single monolithic
module which averages over We then prove the benefits of a dual system with two qualitatively
different modules one of which explicitly represents an estimate of pattern age
A single module recollection system
Optimal retrieval dynamics
Since information storage by synaptic plasticity is lossy the recollection task described above is a
probabilistic inference problem Essentially neural dynamics should represent aspects of the
posterior over stored patterns
that expresses the probability of any pattern being the
and the synaptic efficacies W.
correct response for the recall query given a noisy recall cue
In more detail the posterior over possible stored patterns can be computed as
Pstore Pnoise
where
we assume that evidence from the weights factorizes over synapses
ij
ij
This assumption is never exactly true in practice as synapses that share a pre or post synaptic partner are
bound to be correlated Here we assume the intervening patterns cause independent weight changes and ignore
the effects of such correlations
Previous Bayesian recall dynamics derivations assumed learning rules for which the contribution of
each pattern to the final weight were the same irrespective of the order of pattern presentation
By contrast the Markov chain behaviour of our synaptic learning rule forces us to explicitly consider
pattern age Furthermore as pattern age is unknown at recall we need to integrate over all possible
values This integral which is technically a sum for discrete can be computed analytically
using the eigenvalue decomposition of the transition matrix M. Alternatively if the value of is
known during recall the prior is replaced by a delta function Precall
There are several possible ways of representing the posterior in through neural dynamics without reifying For consistency we assume neural states to be binary with network activity at each
step representing a sample from the posterior An advantage of this approach is that the full
posterior is represented in the network dynamics such that higher decision modules can not only
extract the best pattern for the mean squared error cost function considered here this would be
the mean of the posterior but also estimate the uncertainty of this solution Nevertheless other
representations for example representing the parameters of a mean-field approximation to the true
posterior would also be possible and similarly informative about uncertainty
In particular we use Gibbs sampling as it allows for neurally plausible recall dynamics This
results in asynchronous updates in which the activity of a neuron changes stochastically as a
function of its input cue
the activity of all other neurons x\i and neighbouring synapses
and Specifically the Gibbs sampler results in a sigmoid transfer function with the total current
to the neuron given by the log-odds ratio
Iirec
log
P(xi
Iirec,in Iirec,out a
P(xi
in/out
defining the evidence from the incoming and outgoing synapses of neuron
with the terms Irec
and the constants a and determined by the prior over patterns and the noise model.2 The terms
describing the contribution from recurrent interactions have a similar shape
in
in
in
Iirec,in
cin
Wij c2 Wij c3 c4
Iirec,out
out
out
out
cout
Wji c2 Wji c3 c4
in/out
The parameters ck
uniquely determined by the learning rule and the priors for and rescale
the contribution of the evidence from the weights as a function of pattern age supplementary
text Furthermore these constants translate into a unique signal giving a sort of sufficient statistic for the expected memory strength NoteP
that the optimal dynamics include two homeostatic
processes corresponding to global inhibition and neuronal excitability regulation Wij
that stabilize network activity during recall
Limitations
Beside the effects of assuming a factorized weight distribution the neural dynamics derived above
should be the best we can do given the available data recall cue and synaptic weights How
well does the network fare in practice
Performance is as expected when pattern age is assumed known as the available information from
the weights decreases so does performance finally converging to control levels defined by the retrieval performance of a network without plastic recurrent connections when inference uses only
the recall cue and the prior over stored patterns green When is unknown performance
also deteriorates with increasing pattern age however this time beneath control levels blue
Intuitively one can see that relying on the prior over is similar to assuming fixed to a value close
out
Real neurons can only receive information from their presynaptic partners so cannot estimate Irec
We
therefore ran simulations without this term in the dynamics and found that although it did decrease recall performance this decrease was similar to that obtained by randomly pruning half of the connections in the network
and keeping this term in the dynamics not shown This indicated that performance is mostly determined by
the number of available synapses used for inference and not so much by the direction of those synapses Hence
in the following we use both terms and leave the systematic study of connectivity for future work
known
unknown
control
control
error
error
a
known
single module
dual system
Gibbs tempered
transitions
Figure a Recall performance for a single module memory system Average recollection error
comparison for the single and dual memory system Black lines mark control performance when
ignoring the information from the synaptic weights
to the mean of this prior When the pattern that was actually presented is older than this estimate the
resulting memory signal is weaker than expected suggesting that the initial pattern was very sparse
since a pair of inactive elements does not induce any synaptic changes according to our learning
rule However less reasonable is the fact that averaging over the prior distribution of recall times
performance is worse than this control
One possible reason for this failure is that the sampling procedure used for inference might not work
in certain cases Since Gibbs samplers are known to mix poorly when the shape of the posterior is
complex with strong correlations as in frustrated Ising models perhaps our neural dynamics are
unable to sample the desired distribution effectively We confirmed this hypothesis by implementing
a more sophisticated sampling procedure using tempered transitions details in supplementary
text Indeed with tempered transitions performance becomes significantly better than control even
for the cases where Gibbs sampling fails Unfortunately there has yet to be a convincing
suggestion as to how tempering dynamics in fact any other sampling algorithm that works well
with correlated posteriors can be represented neurally since for example they require a global
acceptance decision to be taken at the end of each temperature cycle
It is worth noting that with more complex synaptic dynamics deeper cascades simple Gibbs
sampling works reasonably well data not shown probably because the posterior is smoother and
hence easier to sample
A dual memory system
An alternative to implicitly marginalizing over the age of the pattern throughout the inference process is to estimate it at the same time as performing recollection This suggests the use of dual
modules that together estimate the joint posterior
with sampling proceeding in a
loop the familiarity module generates a sample from the posterior over the age of the currently esti and the recollection module uses this estimated age to compute a new
mated pattern
sample from the distribution over possible stored patterns given the age
The module that computes familiarity can also be seen as a palimpsest with each pattern overlaying
and being overlaid by its predecessors and successors Formally it needs to compute the probability
as the system continues to implement a Gibbs sampler with as an additional dimenP(t|x
sion As a separate module the neural network estimating familiarity cannot however access the
weights of the recollection module A biologically plausible approximation is to assume that
the familiarity module uses a separate set of weights which we call Wfam Also it is clear from
conditioned on thus the conditioning on
can be dropped when
1b that is independent of
computing the posterior over that is external input need only feed directly into the recollection
but not the familiarity module
In particular we assume a feedforward network structure in the familiarity module with each neuron
receiving the output of the recollection module as inputs through synapses Wfam These synaptic
cue
familiarity
recollection
activation
neuron index
familiarity signal
a
Figure a An overview of the dual memory system The familiarity network has a feedforward
structure with the activity of individual neurons estimating the probability of the true pattern age
being a certain value see example in inset The estimated pattern age translates into a familiarity
signal which scales the contribution of the recurrent inputs in the network dynamics Dependence
of the familiarity signal on the estimated pattern age
weights change according to the same cascade rule used for recollection.3 For simplicity we assume
that the familiarity neurons are always activated during encoding so that synapses can change state
either by potentiation or depression with every storage event
Concretely the familiarity module consists of Nfam neurons each corresponding to a certain pattern
age in the range 1?Nfam the last unit codes for Nfam This forms a localist code for familiarity
The total input to a neuron is given by the log-posterior Iifam log P(t Wfam which
translates into a simple linear activation function
fam
fam
fam
fam
Iifam
cfam
cfam
log log(Z
Wij Wij
in/out
before albeit different for each neuron
where the constants cfam
k,i are similar to parameters
because of their tuning to different values of and is the unknown partition function
As mentioned above we treat the activity of the familiarity module as a sample from the posterior
over age This representation requires lateral competition between different units such that only one
can become active at each step Dynamics of this sort can be implemented using a softmax operator
Ii
P(xfam
Pe eIj thus rendering the evaluation of the partition function unnecessary and
are a common feature of a range of neural models
Critically this familiarity module is not just a convenient theoretical construct associated with retrieval First as we mentioned before the assessment of novelty actually plays a key part in memory
storage in making the decision as to whether a pattern that is presented is novel and so should
be stored or familiar and so should have its details be recalled This venerable suggestion has
played a central part in the understanding of structure-function relationships in the hippocampus
The graded familiarity module that we have suggested is an obvious extension of this idea the use
for retrieval is new Second it is in general accord with substantial data on the role of perirhinal cortex and the activity of neurons in this structure Recency neurons would be associated with small
values of novelty neurons with large or effectively infinite values of although perirhinal
cortex appears to adopt a population coding strategy for age rather than just one-of-n
The recollection module has the same dynamics as before with constants ci computed assuming
fixed to the output of the familiarity module Thus we predict that familiarity multiplicatively modulates recurrent interactions in the recollection module during recall Since there is a deterministic
mapping between and this modulatory factor it can be computed using a linear unit pooling the outputs of all the neurons in the familiarity module with weights given by the corresponding
values for cfam
There is nothing to say that the learning rule that optimizes the recollection network?s ability to recall
patterns should be equally appropriate for assessing familiarity Hence the familiarity module could have their
own learning rule optimized for its specific task
novel
familiar
hits
familiarity estimated
a
fam
rec
90
both
recollection average entropy
false alarms
fam
rec
Figure a Decision boundaries for the recognition module Corresponding ROC curve
Performance comparison when the decision layer uses signals from the familiarity module the recollection module or both Same comparison when data is restricted to recent stimuli Note that
difference between fam and rec became significant compared to
In order to compare single and dual module systems fairly the computational resources employed by
each should be the same We therefore reduced the overall connectivity in the dual system such that
the two have the same total number of synapses Moreover since elements of Wfam are correlated
the effective number of connections is in fact somewhat lower in the dual system Regardless the
dual memory system performs significantly better than the single module system
Recognition memory
We have so far considered familiarity merely as an instrument for effective recollection However
there are many practical and experimental tasks in which it is sufficient to make a binary decision
about whether a pattern is novel or familiar rather than recalling it in all its gory detail It is these
tasks that have been used to elucidate the role of perirhinal cortex in recognition memory
In the dual module system information about recognition is available from both the familiarity
module patterns judged to have young ages are recognized and the recollection module patterns
recalled with higher certainty are recognized We therefore construct an additional decision module
which takes the outputs of the familiarity and recollection modules and maps them into a binary
behavioral response familiar novel
Specifically we use the average of the entropies associated with the activities of neurons in the
recollection module and the mean estimate of from the familiarity module Since the palimpsest
property implicitly assumes that all patterns have been presented at some point we define a pattern to
be familiar if its age is less than a fixed threshold tth We train the decision module using a Gaussian
process classifier4 which yields as outcome the probability of a hit P(familiar|t shown
in The shape of the resulting discriminator that it is not parallel to either axis suggests that
the output of both modules is needed for successful recognition as suggested experimentally
The fact that a classifier trained using only one of the two dimensions cannot match the recognition
performance of that using both confirms this observation
Moreover the ROC curve produced by the classifier plotting hit rates against false alarms as relative
losses are varied has a similar shape to those obtained for human behavioral data it has a so-called
curvi-linear character because of the apparent intersect at a finite hit probability for false alarm
rate Lastly as recognition is known to rely more on familiarity for relatively recent
patterns we estimate recognition performance for recent patterns which we define as having
age t2th To determine the contribution of each module in recognition outcomes in this case we
estimate performance of classifiers trained on single input dimensions for this test data Consistent
with experimental data our analysis reveals that the familiarity signal gives a more reliable estimate
of novelty compared to the recollection output for relatively recent items
The specific classifier was chosen as it allows for an easy estimation of the ROC curves Future work
should explore analytical decision rules
Conclusions and discussion
Knowing the age of a pattern is critical for retrieval from palimpsest memories a consideration
that has so far eluded theoretical inquiry We showed that a memory system could either treat this
information implicitly by marginalizing over all possible ages or it could estimate age explicitly as
a form of familiarity In principle both solutions should have similar performance given the same
resources In practice however a system involving dual modules is significantly better
In our model the posterior over possible stored patterns was represented in neural activities via
samples We showed that a complex biologically-questionable sampling procedure would be necessary for the implicit single module system Instead a dual memory system with two functionally
distinct but closely interacting modules yielded the best performance both for efficient recollection
and for recognition Importantly though Gibbs sampling and tempered transitions provide a useful framework for understanding the performance differences between different memory systems
the presented results are not restricted to a sampling-based implementation Since age and identity
are tightly correlated a mean field solution that use factorized distributions shows very similar behavior supplementary text Similarly the specific details of the familiarity module are
not critical for these effects which should be apparent for any alternative implementation correctly
estimating pattern age
Representing pattern age explicitly essentially amounts to implementing an auxiliary variable for
sampling the space of possible patterns more efficiently Such auxiliary variable methods are
widely used to increase sampling efficiency when other simpler methods fail Moreover since
in our case specifically modulates the correlated components of the posterior it can be seen as a
temperature parameter and so we can understand the advantages brought about by the dual system
as due to implementing a form of simulated tempering a class of methods known to help mixing
in strongly correlated posteriors
Our proposal provides a powerful new window onto the contentious debate about the neural mechanisms of recognition and recall The rationale for our familiarity network was improving recollection however the form of the network was motivated by the substantial experimental data on
recognition and indeed standard models of perirhinal cortex activity These for instance also
rely on some form of inhibition to mediate interactions between different familiarity neurons Nevertheless our model is the first to link the computational function of familiarity networks to recall
it is distinct also in that it considers palimpsest synapses as previous models use purely additive
learning rules Although we only considered pattern age as the basis of familiarity here the
principle of the interaction between familiarity and recollection remains the same in an extended
setting when familiarity characterizes the expected strength of the memory trace more completely
including the effects of retention interval number of repetitions and spacing between repetitions
Future work with the extended model should allow us to address familiarity novelty and recency
neurons in the perirhinal cortex and indeed provide a foundation for new thinking about this region
In our model familiarity interacts with recollection by multiplicatively divisively modulating the
contribution of recurrent inputs in the recollection module Neurally this effect could be mediated
by shunting inhibition via specific classes of hippocampal interneurons which target the dendritic
segment corresponding to recurrent connections thus rescaling the relative contribution of external versus recurrent inputs Whether pathways reaching CA3 from perirhinal cortex through
entorhinal cortex preserve a sufficient amount of input specificity of feed-forward inhibition is unknown
Our theory predicts important systems-level aspects of memory from synaptic-level constraints In
particular by optimizing our dual system solely for memory recall we also predicted non-trivial
ROC curves for recognition that are in at least broad qualitative agreement with experiments Future
work will be needed to explore whether the ROC curves in our model show similar dissociations in
response to specific lesions of the two modules to those found in recent experiments and the
relation to other recognition memory models
Acknowledgements
This work was supported by the Wellcome Trust ML and the Gatsby Charitable Foundation

<<----------------------------------------------------------------------------------------------------------------------->>

title: 1703-distributed-synchrony-of-spiking-neurons-in-a-hebbian-cell-assembly.pdf

Distributed Synchrony of Spiking Neurons
in a Hebbian Cell Assembly
David Horn Nir Levy
School of Physics and Astronomy
Raymond and Beverly Sackler Faculty of Exact Sciences
Tel Aviv University Tel Aviv Israel
horn~neuron.tau.ac.il
nirlevy~post.tau.ac.il
Isaac Meilijson Eytan Ruppin
School of Mathematical Sciences
Raymond and Beverly Sackler Faculty of Exact Sciences
Tel Aviv University Tel Aviv Israel
isaco~math.tau.ac.il
ruppin~math.tau.ac.il
Abstract
We investigate the behavior of a Hebbian cell assembly of spiking
neurons formed via a temporal synaptic learning curve This learning function is based on recent experimental findings It includes
potentiation for short time delays between pre and post-synaptic
neuronal spiking and depression for spiking events occuring in the
reverse order The coupling between the dynamics of the synaptic
learning and of the neuronal activation leads to interesting results
We find that the cell assembly can fire asynchronously but may
also function in complete synchrony or in distributed synchrony
The latter implies spontaneous division of the Hebbian cell assembly into groups of cells that fire in a cyclic manner We invetigate
the behavior of distributed synchrony both by simulations and by
analytic calculations of the resulting synaptic distributions
Introduction
The Hebbian paradigm that serves as the basis for models of associative memory is
often conceived as the statement that a group of excitatory neurons the Hebbian
cell assembly that are coupled synaptically to one another fire together when a
subset of the group is being excited by an external input Yet the details of the
temporal spiking patterns of neurons in such an assembly are still ill understood
Theoretically it seems quite obvious that there are two general types of behavior
synchronous neuronal firing and asynchrony where no temporal order exists in the
assembly and the different neurons fire randomly but with the same overall rate
Further subclassifications were recently suggested by BruneI Experimentally this question is far from being settled because evidence for the associative
D. Hom N. Levy Meilijson and E. Ruppin
memory paradigm is quite scarce On one hand one possible realization of associative memories in the brain was demonstrated by Miyashita in the inferotemporal cortex This area was recently reinvestigated by Yakovlev who
compared their experimental results with a model of asynchronized spiking neurons
On the other hand there exists experimental evidence Abeles for temporal
activity patterns in the frontal cortex that Abeles called synfire-chains Could they
correspond to an alternative type of synchronous realization of a memory attractor
To answer these questions and study the possible realizations of attractors in
cortical-like networks we investigate the temporal structure of an attractor assuming
the existence of a synaptic learning curve that is continuously applied to the memory system This learning curve is motivated by the experimental observations of
Markram Zhang that synaptic potentiation or depression
occurs within a critical time window in which both pre and post-synaptic neurons
have to fire If the pre-synaptic neuron fires first within or so potentiation
will take place Depression is the rule for the reverse order
The regulatory effects of such a synaptic learning curve on the synapses of
a single neuron that is subjected to external inputs were investigated by
Abbott and Song and by Kempter We investigate here the
effect of such a rule within an assembly of neurons that are all excited by the
same external input throughout a training period and are allowed to influence one
another through their resulting sustained activity
The Model
We study a network composed of excitatory and NJ inhibitory integrate-and-fire
neurons Each neuron in the network is described by its subthreshold membrane
potential Vi{t obeying
Vi{t
Vi{t
Tn
where Tn is the neuronal integration time constant A spike is generated when Vi{t
reaches the threshold Vrest fJ upon which a refractory period of TRP is set on and
the membrane potential is reset to Vreset where Vrest Vreset Vrest fJ Ii{t is
the sum of recurrent and external synaptic current inputs The net synaptic input
charging the membrane of excitatory neuron at time is
I
Td
Ji~J
tj
Td
summing over the different synapses of NE excitatory neurons and of
NJ inhibitory neurons with postsynaptic efficacies and Ji~J respectively The sum over represents a sum on different spikes arriving at
synapse at times Td tj where is the emission time of
the l-th spike from the excitatory inhibitory neuron and Td is the synap
tic delay Iext the external current is assumed to be random and independent at
each neuron and each time step drawn from a Poisson distribution with mean Aext
Analogously the synaptic input to the inhibitory neuron at time tis
We assume full connectivity among the excitatory neurons but only partial connectivity between all other three types of possible connnections with connection
Distributed Synchrony of Spiking Neurons in a Hebbian Cell Assembly
probabilities denoted by eEl and I. In the following we will report simulation results in which the synaptic delays Td were assigned to each synapse or pair of
neurons randomly chosen from some finite set of values Our analytic calculation
will be done for one fixed value of this delay parameter
The synaptic efficacies between excitatory neurons are assumed to be potentiated
or depressed according to the firing patterns of the pre and post-synaptic neurons
In addition we allow for a uniform synaptic decay Thus each excitatory synapse
obeys
where the synaptic decay constant Ts is assumed to be very large compared to the
membrane time constant Tn. J/JE(t are constrained to vary in the range Jma
The change in synaptic efficacy is defined by Fij as
Fij(t
where Kp and KD are the potentiation and depression branches of the kernel function
cO exp
plotted in Figure Following Zhang we distinguish between the situation where the postsynaptic spike at appears after or before the presynaptic
spike at using the asymmetric kernel that captures the essence of their experimental observations
I
I
Figure The kernel function whose left part Kp leads to potentiation of the
synapse and whose right branch KD causes synaptic depression
Distributed Synchrony of a Hebbian Assembly
We have run our system with synaptic delays chosen randomly to be either or
and temporal parameters Tn chosen as for excitatory neurons and
for inhibitory ones Turning external input currents off after a while we obtained
sustained firing activities in the range of Hz. We have found in addition to
synchronous and asynchronous realizations of this attractor a mode of distributed
synchrony A characteristic example of a long cycle is shown in Figure The
excitatory neurons split into groups such that each group fires at the same frequency
and at a fixed phase difference from any other group The J/JE synaptic efficacies
D. Horn Levy Meilijson and E. Ruppin
tI
I I
I
I
I
I I
I I
I
I
I
I
I
I
I
JO
Figure Distributed synchronized firing mode The firing patterns of six cell assemblies of excitatory neurons are displayed vs time These six groups of
neurons formed in a self-organized manner for a kernel function with equal potentiation and depression The delays were chosen randomly from three values or
and the system is monitored every
are initiated as small random values The learning process leads to the self-organized
synaptic matrix displayed in Figure The block form of this matrix represents
the ordered couplings that are responsible for the fact that each coherent group of
neurons feeds the activity of groups that follow it The self-organized groups form
spontaneously When the synapses are affected by some external noise as can come
about from Hebbian learning in which these neurons are being coupled with other
pools of neurons the groups will change and regroup as seen in Figure and
Figure A synaptic matrix for distributed synchrony The synaptic matrix
between the excitatory neurons of our system is displayed in a grey-level code
with black meaning zero efficacy and white standing for the synaptic upper-bound
The matrix that exists during the distributed synchronous mode of Figure
Its basis is ordered such that neurons that fire together are grouped together
Using the same basis as in a new synaptic matrix is shown one that is formed
after stopping the sustained activity of Figure introducing noise in the synaptic
matrix and reinstituting the original memory training The same matrix as
is shown in a new basis that exhibits connections that lead to a new and different
realization of distributed synchrony
A stable distributed synchrony cycle can be simply understood for the case of a
single synaptic delay setting the basic step or phase difference of the cycle When
several delay parameters exist a situation that probably more accurately represents
the a-function character of synaptic transmission in cortical networks distributed
Distributed Synchrony of Spiking Neurons in a Hebbian Cell Assembly
synchrony may still be obtained as is evident from Figure After some time
the cycle may destabilize and regrouping may occur by itself without external
interference The likelihood of this scenario is increased because different synaptic
connections that have different delays can interfere with one another Nonetheless
over time scales of the type shown in Figure grouping is stable
Analysis of a Cycle
In this section we analyze the dynamics of the network when it is in a stable state
of distributed synchrony We assume that groups of neurons are formed and
calculate the stationary distribution of JffE(t In this state the firing pattern of
every two neurons in the network can be characterized by their frequency and
by their relative phase We assume that is a random normal variable with
mean J.Lo and standard deviation Thus can be rewritten as the following
stochastic differential equation
dJi~E(t
J.LFij(t
dt+O'Fij(t)dW(t
where Fij is represented here by a drift term J.LFij and a diffusion
term O'Fij which are its mean and standard deviation describes a Wiener
process Note that both J.LFij and O'Fij are calculated for a specific distribution
of and are functions of J.Lo and
The stochastic process that satisfies will satisfy the Fokker-Plank equation
for the probability distribution of JIfE
JPlE f(JPlE f(JPlE
8JEE2
8f(JPlE
8t
8JPlE
1J
T'J
tJ
ij
with reflecting boundary conditions imposed by the synaptic bounds and Jmax
Since we are interested in the stable state of the process we solve the stationary
equation The resulting density function is
2J.LFij EE 1J
EE
f(Jij O'}ij exp O'}ij
ij
Ts
ij
where
enables us to calculate the stationary distribution of the synaptic efficacies
between the presynaptic neuron and the post-synaptic neuron given their frequency and the parameters J.Lo and An example of a solution for a 3-cycle is
shown in Figure In this case all neurons fire with frequency and J.Lt5
takes one of the values Td.
Simulation results of a 3-cycle in a network of excitatory and inhibitory integrateand-fire neurons described in Section are given in Figure As can be seen the
results obtained from the analysis match those observed in the simulation
Discussion
The interesting experimental observations of synaptic learning curves
Markram Zhang have led us to study their implications for the firing patterns of a Hebbian cell assembly We find that in addition
D. Horn N. Levy Meilijson and E. Ruppin
70
so
01
04
EE
Figure Results of the analysis for a6 2ms and Td The
synaptic matrix Each of the nine blocks symbolizes a group of connections between
neurons that have a common phase-lag The mean of Ji~E was calculated for
each cell by and its value is given by the gray scale tone The distribution
of synaptic values between all excitatory neurons
JEE
Figure Simulation results for a network of and NJ integrateand-fire neurons when the network is in a stable state Tn for both
excitatory and inhibitory neurons The average frequency of the neurons is Hz.
The excitatory synaptic matrix Histogram of the synaptic efficacies
to the expected synchronous and asynchronous modes an interesting behavior
of distributed synchrony can emerge This is the phenomenon that we have
investigated both by simulations and by analytic evaluation
Distributed synchrony is a mode in which the Hebbian cell assembly breaks into an
n-cycle This cycle is formed by instantaneous symmetry breaking hence specific
classification of neurons into one of the groups depends on initial conditions noise
etc Thus the different groups of a single cycle do not have a semantic invariant
meaning of their own It seems perhaps premature to try and identify these cycles
with synfire chains Abeles that show recurrence of firing patterns of groups
of neurons with periods of hundreds of ms Note however that if we make such an
identification it is a different explanation from the model of Herrmann
which realizes the synfire chain by combining sets of preexisting patterns into a cycle
The simulations in Figures and were carried out with a learning curve that
possessed equal potentiation and depression branches was completely antisymmetric in its argument In that case no synaptic decay was allowed Figure
on the other hand had stronger potentiation than depression and a finite synaptic
Distributed Synchrony ofSpiking Neurons in a Hebbian Cell Assembly
decay time was assumed Other conditions in these nets were different too yet
both had a window of parameters where distributed synchrony showed up Using
the analytic approach of section we can derive the probability distribution of
synaptic values once a definite cyclic pattern of distributed synchrony is formed
An analytic solution of the combined dynamics of both the synapses and the spiking
neurons is still an open challenge Hence we have to rely on the simulations to prove
that distributed synchrony is a natural spatiotemporal behavior that follows from
combined neuronal dynamics and synaptic learning as outlined in section To the
extent that both types of dynamics reflect correctly the dynamics of cortical neural
networks we may expect distributed synchrony to be a mode in which neuronal
attractors are being realized
The mode of distrbuted synchrony is of special significance to the field of neural computation since it forms a bridge between the feedback and feed-forward paradigms
Note that whereas the attractor that is formed by the Hebbian cell assembly is of
global feedback nature one may regard all neurons of the assembly as being
connected to other neurons within the same assembly the emerging structure of
distributed synchrony shows that it breaks down into groups These groups are
connected to one another in a self-organized feed-forward manner thus forming the
cyclic behavior we have observed

<<----------------------------------------------------------------------------------------------------------------------->>

title: 972-a-model-of-the-hippocampus-combining-self-organization-and-associative-memory-function.pdf

A model of the hippocampus combining selforganization and associative memory function
Michael E. Hasselmo Eric Schnell
Joshua Berke and Edi Barkai
Dept of Psychology Harvard University
33 Kirkland Cambridge MA
hasselmo@katla.harvard.edu
Abstract
A model of the hippocampus is presented which forms rapid self-organized representations of input arriving via the perforant path performs
recall of previous associations in region and performs comparison
of this recall with afferent input in region CA This comparison drives
feedback regulation of cholinergic modulation to set appropriate
dynamics for learning of new representations in region CA3 and CA
The network responds to novel patterns with increased cholinergic modulation allowing storage of new self-organized representations but
responds to familiar patterns with a decrease in acetylcholine allowing
recall based on previous representations This requires selectivity of the
cholinergic suppression of synaptic transmission in stratum radiatum of
regions CA3 and CAl which has been demonstrated experimentally
INTRODUCTION
A number of models of hippocampal function have been developed Burgess
Myers and Gluck Touretzky but remarkably few simulations have
addressed hippocampal function within the constraints provided by physiological and anatomical data Theories of the function of specific subregions of the hippocampal formation often do not address physiological mechanisms for changing dynamics between
learning of novel stimuli and recall of familiar stimuli For example the afferent input to
the hippocampus has been proposed to form orthogonal representations of entorhinal
activity Marr McNaughton and Morris Eichenbaum and Buckingham
but simulations have not addressed the problem of when these representations
78
Michael E. Hasselmo Eric Schnell Joshua Berke Edi Barkai
should remain stable and when they should be altered In addition models of autoassociative memory function in region CA3 Marr McNaughton and Morris
Levy Eichenbaum and Buckingham and heteroassociative memory function
at the Schaffer collaterals projecting from region CA3 to CAl Levy McNaughton
require very different activation dynamics during learning versus recall
Acetylcholine may set appropriate dynamics for storing new information in the cortex
Hasselmo Hasselmo Hasselmo and Bower Acetylcholine has been shown to selectively suppress synaptic transmission at intrinsic but
not afferent fiber synapses Hasselmo and Bower to suppress the neuronal adaptation of cortical pyramidal cells Hasselmo Barkai and Hasselmo and
to enhance long-term potentiation of synaptic potentials Hasselmo Models
show that suppression of synaptic transmission during learning prevents recall of previously stored information from interfering with the storage of new information Hasselmo
Hasselmo while cholinergic enhancement of synaptic
modification enhances the rate of learning Hasselmo
Feedback regulation of cholinergic modulation may set the appropriate level of cholinergic modulation dependent upon the novelty or familiarity of a particular input pattern
We have explored possible mechanisms for the feedback regulation of cholinergic modulation in simulations of region CAl Hasselmo and Schnell and region CA3. Here
we show that self-regulated learning and recall of self-organized representations can be
obtained in a network simulation of the hippocampal formation This model utilizes selective cholinergic suppression of synaptic transmission in stratum radiatum of region
which has been demonstrated in brain slice preparations of the hippocampus
METHODS
SIMPLIFIED REPRESENTA nON OF HIPPOCAMPAL NEURONS
In place of the sigmoid input-output functions used in many models this model uses a
simple representation in which the output of a neuron is not explicitly constrained but the
total network activity is regulated by feedback from inhibitory interneurons and adaptation due to intracellular calcium concentration Separate variables represent pyramidal
cell membrane potential a intracellular calcium concentration and the membrane potential of inhibitory interneurons
l1a Ai
Wijg(aj Hikg(h
I1c?I Vg(a
I
I1hk
Qc
IWkjg(aj-eo)-l1hk IHk/g(h/-e
where A afferent input
passive decay of membrane potential Il strength of cal
A Model of Hippocampus
79
cium-dependent potassium current proportional to intracellular calcium Wij excitatory
recurrent synapses longitudinal association path tenninating in stratum radiatum gO is a
threshold linear function proportional to the amount by which membrane potential
exceeds an output threshold or threshold for calcium current Oc strength of voltagedependent calcium current diffusion constant of calcium Wki excitatory synapses
inhibitory interneurons Hilc inhibitory synapses from interneurons to pyramidal cells
inhibitory synapses between interneurons This representation gives neurons adaptation characteristics similar to those observed with intracellular recording Barkai and Hasselmo including a prominent afterhyperpolarization potential Figure
An
N~JJL
lO
Figure Comparison of pyramidal cell model with experimental data
In Figure I A shows the membrane potential of a modeled pyramidal cell in response to
simulated current injection Output of this model is a continuous variable proportional to
how much membrane potential exceeds threshold This is analogous to the reciprocal of
interspike interval in real neuronal recordings Note that the model displays adaptation
during current injection and afterhyperpolarization afterwards due to the calcium-dependent potassium current shows the intracellularly recorded membrane potential in a pirifonn cortex pyramidal cell demonstrating adaptation of firing frequency due to
activation of calcium-dependent potassium current The firing rate falls off in a manner
similar to the smooth decrease in firing rate in the simplified representation shows an
intracellular recording illustrating long-tenn afterhyperpolarization caused by calcium
influx induced by spiking of the neuron during current injection
NETWORK CONNECTIVITY
A schematic representation of the network simulation of the hippocampal fonnation is
shown in Figure The anatomy of the hippocampal fonnation is summarized on the left
in A and the function of these different subregions in the model is shown on the right in
B. Each of the subregions in the model contained a population of excitatory neurons with
a single inhibitory interneuron mediating feedback inhibition and keeping excitatory
activity bounded Thus the local activation dynamics in each region follow the equations
presented above The connectivity of the network is further summarized in Figure in the
Results section A learning rule of the Hebbian type was utilized at all synaptic connections with the exception of the mossy fibers from the dentate gyrus to region and the
connections to and from the medial septum Self-organization of perforant path synapses
was obtained through decay of synapses with only pre or post-synaptic activity and
growth of synapses with combined activity Associative memory function at synapses
Michael E. Hasse/mo Eric Schnell Joshua Berke Edi Barkai
arising from region CA3 was obtained through synaptic modification during cholinergic
suppression of synaptic transmission
Entorhinal cortex
Self-organized
representation
Comparison
Feedback regulation of
cholinergic modulation
Regulation of
learning dynamics
Figure Schematic representation of hippocampal circuitry
and the corresponding function of connections in the model
CHOLINERGIC MODULAnON
The total output from region CAl determined the level of cholinergic modulation within
both region CA3 and CAl with increased output causing decreased modulation This is
consistent with experimental evidence suggesting that activity in region CAl and region
CA3 can inhibit activity in the medial septum and thereby downregulate cholinergic modulation This effect was obtained in the model by excitatory connections from region CAl
to an inhibitory interneuron in the medial septum which suppressed the activity of a cholinergic neuron providing modulation to the full network When levels of cholinergic
modulation were high there was strong suppression of synaptic transmission at the excitatory recurrent synapses in CA3 and the Schaffer collaterals projecting from region CA3 to
CAL This prevented the spread of activity due to previous learning from interfering with
self-organization When levels of cholinergic modulation were decreased the strength of
synaptic transmission was increased allowing associative recall to dominate Cholinergic
modulation also increased the rate of synaptic modification and depolarized neurons
TESTS OF SELF-REGULATED LEARNING AND RECALL
Simulations of the full hippocampal network evaluated the response to the sequential presentation of a series of highly overlapping activity patterns in the entorhinal cortex Recall
was tested with interspersed presentation of degraded versions of previously presented
activity patterns For effective recall the pattern of activity in entorhinal cortex layer IV
evoked by degraded patterns matched the pattern evoked by the full learned version of
these patterns The function of the full network is illustrated in Figure In simulations
A Model of Hippocampus
81
focused on region activity patterns were induced sequentially in region representing afferent input from the entorhinal cortex Different levels of external activation of
the cholinergic neuron resulted in different levels of learning of new overlapping patterns
These results are illustrated in Figure
BRAIN SLICE EXPERIMENTS
The effects in the simulations of region CA3 depended upon the cholinergic suppression
of synaptic transmission in stratum radiatum of this region The cholinergic suppression of
glutamatergic synaptic transmission in region CA3 was tested in brain slice preparations
by analysis of the influence of the cholinergic agonist carbachol on the size of field potentials elicited by stimulation of stratum radiatum These experiments used techniques similar to previously published work in region CAl Hasselmo and Schnell
RESULTS
In the full hippocampal simulation input of an unfamiliar pattern to entorhinal cortex
layer resulted in high levels of acetylcholine This allowed rapid self-organization of
the perforant path input to the dentate gyrus and region CAl. Cholinergic suppression of
synaptic transmission in region CAl prevented recall from interfering with self-organization Instead recurrent collaterals in region CA3 stored an autoassociative representation
of the input from the dentate gyrus to region and connections from CA3 to CA
stored associations between the pattern of activity in CA3 and the associated self-organized representation in region CAl.
identity
self-org matrix
auto
assoc
Self-org
iden~ity
hetero.9 hetero8
matrix
assoc
assoc
I I If
I I I
I I
ld
I I
I I I
I I I I
3d
4d
ld
2d
n,n
I I I
I
I
I
I
1I
I
I
I
I I Itt
I.Ll
I
I
I
I I
U.
I I
I
I I
I I I
I I
I I
I
I J1
III I
I
I
I
I I
I
I
I III I I
W.
Jl
lU
Neuron
Figure Activity in each subregion of the full network simulation of the hippocampal
formation during presentation of a sequence of activity patterns in entorhinal cortex
82
Michael E. Hasselmo Eric Schnell Joshua Berke Edi Barkai
In Figure width of the lines represents the activity of each neuron at a particular time
step As seen here the network forms a self-organized representation of each new pattern
consisting of active neurons in the dentate gyrus and region CAL At the same time an
association is formed between the self-organized representation in region CAl and the
same afferent input pattern presented to entorhinal cortex layer IV. Four overlapping patterns are presented sequentially each of which results in learning of a separate selforganized representation in the dentate gyrus and region CAl. with an association formed
between this representation and the full input pattern in entorhinal cortex
The recall characteristics of the network are apparent when degraded versions of the afferent input patterns are presented in the sequence This degraded afferent input
weakly activates the same representations previously formed in the dentate gyrus Recurrent excitation in region CA3 enhances this activity giving robust recall of the full version
of this pattern This activity then reaches CA where it causes strong activation if it
matches the pattern of afferent input from the entorhinal cortex Strong activation in
region CAl decreases cholinergic modulation preventing formation of a new representation and allowing recall to dominate Strong activation of the representation stored in
region CAl then activates the full representation of the pattern in entorhinal cortex layer
IV. Thus the network can accurately recall each of many highly overlapping patterns
The effect of cholinergic modulation on the level of learning or recall can be seen more
clearly in a simulation of auto-associative memory function in region CA3 as shown in
Figure Each box shows the response of the network to sequential presentation of full
and degraded versions of two highly overlapping input patterns The width of the black
traces represents the activity of each of CA3 pyramidal cells during each simulation
step In the top row level of cholinergic modulation ACh is plotted In A. external activation of the cholinergic neuron is absent so there is no cholinergic suppression of synaptic transmission In this case the first pattern is learned and recalled properly but
subsequent presentation of a second overlapping pattern results only in recall of the previously learned pattern In B. with greater cholinergic suppression recall is suppressed sufficiently to allow learning of a combination of the two input patterns Finally in C. strong
cholinergic suppression prevents recall allowing learning of the new overlapping pattern
to dominate over the previously stored pattern
A
Stored
patterns
ACh
Inhib
ACh input
ACh input
ACh input
I
Figure Increased cholinergic suppression of synaptic transmission in region CA3
causes greater learning of new aspects of afferent input patterns
A Model of Hippocampus
83
Extracellular recording in brain slice preparations of hippocampal region CA3 have demonstrated that perfusion of the cholinergic agonist carbachol strongly suppresses synaptic
potentials recorded in stratum radiatum as shown in Figure In contrast suppression of
synaptic transmission at the afferent fiber synapses arising from entorhinal cortex is much
weaker At a concentration of carbachol suppressed synaptic potentials in stratum
radiatum on average by Synaptic potentials elicited in stratum lacunosum
were more weakly suppressed with an average suppression
Control
Carbachol
Wash
Figure Cholinergic suppression of synaptic transmission in stratum radiatum of CA3.
DISCUSSION
In this model of the hippocampus self-organization at perforant path synapses forms compressed representations of specific patterns of cortical activity associated with events in
the environment Feedback regulation of cholinergic modulation sets appropriate dynamics for learning in response to novel stimuli allowing predominance of self-organization
and appropriate dynamics for recall in response to familiar stimuli allowing predominance of associative memory function This combination of self-organization and associative memory function may also occur in neocortical structures The selective cholinergic
suppression of feedback and intrinsic synapses has been proposed to allow self-organization of feedforward synapses while feedback synapses mediate storage of associations
between higher level representations and activity in primary cortical areas Hasselmo
This previous proposal could provide a physiological justification for a similar
mechanism utilized in recent models Dayan Detailed modeling of cholinergic effects in the hippocampus provides a theoretical framework for linking the considerable behavioral evidence for a role of acetylcholine in memory function Hagan and
Morris to the neurophysiological evidence for the effects of acetylcholine within
cortical structures Hasselmo and Bower Hasselmo
Acknowledgements
This work supported by a pilot grant from the Massachusetts Alzheimer's Disease
Research Center and by an NIMH FIRST award

<<----------------------------------------------------------------------------------------------------------------------->>

title: 3917-spike-timing-dependent-plasticity-as-dynamic-filter.pdf

Spike timing-dependent plasticity as dynamic filter
Joscha T. Schmiedt Christian Albers and Klaus Pawelzik
Institute for Theoretical Physics
University of Bremen
Bremen Germany
schmiedt@uni-bremen.de calbers pawelzik}@neuro.uni-bremen.de
Abstract
When stimulated with complex action potential sequences synapses exhibit spike
timing-dependent plasticity STDP with modulated pre and postsynaptic contributions to long-term synaptic modifications In order to investigate the functional
consequences of these contribution dynamics we propose a minimal model
formulated in terms of differential equations We find that our model reproduces
data from to recent experimental studies with a small number of biophysically interpretable parameters The model allows to investigate the susceptibility of STDP
to arbitrary time courses of pre and postsynaptic activities its nonlinear filter
properties We demonstrate this for the simple example of small periodic modulations of pre and postsynaptic firing rates for which our model can be solved
It predicts synaptic strengthening for synchronous rate modulations Modifications are dominant in the theta frequency range a result which underlines the
well known relevance of theta activities in hippocampus and cortex for learning
We also find emphasis of specific baseline spike rates and suppression for high
background rates The latter suggests a mechanism of network activity regulation
inherent in STDP Furthermore our novel formulation provides a general framework for investigating the joint dynamics of neuronal activity and the CD of STDP
in both spike-based as well as rate-based neuronal network models
Introduction
During the past decade the effects of exact spike timing on the change of synaptic connectivity have
been studied extensively In vitro studies have shown that the induction of long-term potentiation
LTP requires the presynaptic input to a cell to precede the postsynaptic output and vice versa
for long-term depression LTD This phenomenon has been termed spike timingdependent plasticity STDP and emphasizes the importance of a causal order in neuronal signaling
Thereby it extends pure Hebbian learning which requires only the coincidence of pre and postsynaptic activity Consequently experiments have shown an asymmetric exponential dependence on
the timing of spike pairs and a molecular mechanism mostly dependent on the influx of
for reviews Further when induced with more complex spike trains synaptic modification
shows nonlinearities indicating the influence of short-term plasticity
Theoretical approaches to STDP cover studies using the asymmetric pair-based STDP window as
a lookup table more biophysical models based on synaptic and neuronal variables and sophisticated kinetic models for a review see Recently the experimentally observed influence of the
postsynaptic membrane potential has also been taken into account
Our approach is based on differential Hebbian learning which generates asymmetric
timing windows similar to STDP depending on the shape of the back-propagating action
Postal correspondence should be addressed to Universit?at Bremen Fachbereich Institut f?ur Theoretische
Physik Abt. Neurophysik Postfach Bremen Germany
potential We extend it with a mechanism for activating learning by an increase in postsynaptic
activity because both the induction of LTP and LTD require to exceed a threshold
Moreover we include a mechanism for adaptive suppression on both synaptic sides similar to the
model in Finally we for simplicity assume that both the presynaptic and the postsynaptic
side function as low-pass filters a spike leaves a fast increasing and exponentially decaying trace
Together we propose a set of differential equations which captures the contribution dynamics
of pre and postsynaptic activities to STDP thereby describing synaptic plasticity as a filter
Our framework reproduces experimental findings from two recent in vitro studies in the visual cortex and the hippocampus in most details Furthermore it proves to be particularly suitable for the
analysis of the susceptibility of STDP to pre and postsynaptic rate modulations This is demonstrated by an analysis of synaptic changes depending on oscillatory modulations of baseline firing
rates
Formulation of the model
We use a variant of the classical differential Hebbian learning assuming a change of synaptic connectivity which is dependent on the presynaptic activity trace ypre and the temporal derivative of
the postsynaptic activity trace ypost
cw ypre post
cw denotes a constant learning rate An illustration of this learning rule for pairs of spikes is given
in Figure For simplicity we assume these activity traces to be abstract low-pass filtered versions
of neuronal activity in the presynaptic and postsynaptic cells the concentration of or
the amount of bound glutamate
ypre
pre
ypost
post upost xpost
post
pre upre xpre
The dynamics of the y?s are characterized by their respective time constants pre and post The
contribution of each spike is regulated by a suppressing attenuation factor pre and postsynaptically On the postsynaptical side an additional activation factor enables the synapse to learn
The dynamics of and are discussed below represents neuronal activity which can be either a
time-continuous firing rate or spike trains given by series of pulses
xpre post
tipre post
which allows analytical investigations of the properties of our model Note that formally has
then to be taken as x(t An illustrating overview over the different parts of the model with
sample trajectories is shown in Figure
We define the relative change of synaptic connectivity after after a period from Equation as
cw
ypre post dt
The dependence on the initial synaptic strength as observed in shall not be discussed
here but can easily be achieved by making the learning rate cw in Equation w-dependent Here
is chosen to be
Ignoring attenuation and activation a single pair of spikes at temporal distance analytically yields
the typical STDP window Figure 2A and
cw
cw
pre
pre post
pre
t/?post
pre post
for
for
A
Modulation
Factors
Activity
Activity Traces
Contributions
Differential Hebbian
Learning
Low-pass
SYNAPSE
POST
Low-pass
PRE
dt
Example for spike pairs
pre
ypre
ypost
ypost
ypre post dt
ypre post dt
post
Time
Figure Schematic illustration of differential Hebbian learning with contribution dynamics A
Pre and postsynaptic activity second column is modulated attenuated with activated with
first column and filtered third column before it contributes to differential Hebbian learning
fourth column Spike pair example for differential Hebbian learning Left a presynaptic spike
trace ypre preceding a postsynaptic spike trace ypost dotted line yields a synaptic strengthening
due to the initially positive postsynaptic contribution post solid line which is always stronger
than the following negative part Right for the reverse timing the positive presynaptic contribution
is only multiplied with the negative postsynaptic trace right Areas contributing to learning are
shaded
The importance of adaptive suppressing mechanisms for synaptic plasticity has experimentally been
shown by Froemke and colleagues Therefore we down-regulate the contribution of the
spikes to the activity traces in Equation and with an attenuation factor on both pre and
postsynaptic sides
upre cpre upre xpre
rec
pre
rec upost cpost upost u0 xpost
post
pre
post
This should be understood as an abstract representation of for instance the depletion of transmitters
in the presynaptic bouton or the frequency-dependent spike attenuation in dendritic spines
respectively These recover with their time constants rec and are bound between u0 and
post
For the presynaptic side we assume in the following upre
so we abbreviate u0 u0 The
constants cpre post denote the impact a spike has on the relaxed synapse
In several experiments it has been shown that a single spike is not sufficient to induce synaptic
modification Therefore we introduce a spike-induced postsynaptic activation factor
cact xpost z0
which enhances the contribution of a postsynaptic spike to the postsynaptic trace by the removal
of the block from postsynaptic NMDA receptors The nonlinear positive feedback
is introduced to describe strong enhancing effects as for instance autocatalytic mechanisms which
have been suggested to play a role in learning on several time-scales The activation
decays hyperbolically to a lower bound z0 and the contribution of a spike is weighted with the
constant cact
Comparison to experiments
In order to evaluate our model we implemented experimental stimulation protocols from in vitro
studies on synapses of the visual cortex and the hippocampus of rats In both studies
simple pairs of spikes and more complex spike trains were artificially elicited in the presynaptic and
the postsynaptic cell and the induced change of synaptic connectivity was recorded
Froemke and colleagues focused on the effects of spike bursts on synaptic modification in the
visual cortex In addition to the classical STDP pairing protocol a presynaptic spike preceding
or following a postsynaptic spike after a specific time four other experimental protocols
Figure 2B to were performed bursts with five spikes of a certain frequency on both
synaptic sides where the postsynaptic side follows the presynaptic side presynaptic Hz
bursts with spikes following one postsynaptic spike post-n-pre presynaptic Hz bursts
with different numbers of spikes followed by one postsynaptic spike n-pre-post and a post-pre
pair with varying number of following postsynaptic spikes post-pre-n-post
Experiment
Model
A
pre
post
LTP
LTD
Presynaptic spikes
Frequency
Presynaptic spikes
Postsynaptic spikes
Figure Differential Hebbian learning with CD reproduces synaptic modification induced with
STDP spike patterns in visual cortex Data taken from personal communication A experimental fit and model prediction with Equation of pair-based STDP dependence of synaptic
modifications on the frequency of bursts with presynaptic spikes following postsynaptic spikes
by ms and synaptic modification induced by post-n-pre n-pre-post and post-pre-n-post
Hz spike trains
A
pre
post
Experiment
Model
Interspike interval
Interspike interval
Interspike interval
Figure Differential Hebbian learning with CD reproduces synaptic modification induced with
STDP spike patterns in hippocampus Data taken from as reported in A experimental
fit and model prediction with Equation of pair-based STDP quadruplet protocol and
post-pre-post and pre-post-pre triplet protocol for different interspike intervals
Table Parameters and evaluation results for the data sets from visual cortex and hippocampus
normalized mean-square error ratio of correctly predicted signs of synaptic modification
cpre
cpost
cact
rec
pre
rec
post
u0
z0
Visual cortex
Hippocampus
In the hippocampal study of Wang synaptic modification induced by triplets pre-post-pre
and post-pre-post and quadruplets pre-post-post-pre and post-pre-pre-post of spikes was measured
while the respective interspike intervals were varied Figure 3B to D).
As a first step we took the time constants from the experimentally measured pair-based STDP windows as our low-pass filter time constants Equation They remained constant for each data
set pre ms and post ms for pre ms and post ms for
taken from since not present in the study Next we chose the learning rate cw in Equation
to fit the synaptic change for the pairing protocol cw for the visual cortex data
cw for the hippocampal data set The remaining parameters were estimated manually within
biologically plausible ranges and are shown in Table The model was then applied to the more
complex stimulation protocols by solving the differential equations semi-analytically separately
for every spike and the following interspike interval As measure for the prediction error of our
model we used the normalized mean-square error
wiexp wimod
where wiexp and wimod are the experimentally measured and the predicted modifications of synaptic strength in the ith experiment is the number of data points 18 for the visual cortex data
set for the hippocampal data set is the standard error of the mean of the experimental
data Additionally we counted the number of correctly predicted signs of synaptic modification
induced depression or potentiation The prediction error for both data sets is shown in Table
Phase shift
Cortex
Hippocampus
Modulation frequency
Figure Synaptic change depending on frequency and phase shift of pre and postsynaptic
rate modulations for different baseline rates The color codes are identical within each column
and in arbitrary units Note the strong suppression with increasing baseline rate for cortical synapses
which is due to strong attenuation effects of pre and postsynaptic contributions It is weaker for
hippocampal synapses because we found the postsynaptic attenuation to be bounded
Phase frequency and baseline rate dependence of STDP with contribution
dynamics
As shown in the previous section our model can reproduce the experimental findings of synaptic
weight changes in response to spike sequences surprisingly well and yields better fits than former
studies The proposed framework however is not restricted to spike sequences but allows to investigate synaptic changes depending on arbitrary pre and postsynaptic activities For
instance it could be used for investigations of the plasticity effects in simulations with inhomogeneous Poisson processes Taking to be firing rates of Poissonian spike trains our account of
STDP represents a useful approximation for the expected changes of synaptic strength depending
on the time courses of xpre and xpost compare Therefore our model can serve also as
building block in rate based network models for investigation of the joint dynamics of neuronal
activities and synaptic weights
Here we demonstrate the benefit of our approach for determining the filter properties of STDP
subject to CD we use the equations together with the parameters from the experiments for
determining the dependency of weight changes on frequency relative phase and baseline rates
of modulated pre and postsynaptic firing rates While for substantial modulations of firing rates
the nonlinearities are difficult to be treated analytically for small periodical modulations around a
baseline rate the corresponding synaptic changes can be calculated analytically This is done by
considering
xpre cos(2?f and
xpost cos(2?f
which for small allows linearization of all equations from which one obtains
pre post where is the period of the respective oscillations Neglect6
ing transients this finally yields the expected weight changes per unit time Though lengthy the
calculations are straightforward and presented in the supplementary material We here show only
the exact result for the case of constant and
pre post post pre pre post
post pre
sin
arctan
pre
pre post
post
The analytical results for the case with CD are shown graphically in Figure using the parameters
from cortex and hippocampus respectively Tab. These plots contain the main findings
rate modulations in the theta frequency range lead to strongest synaptic changes
also for phase-zero synchronous rate modulations weight changes are positive in hippocampus
maximal weight change magnitudes occur at baseline rates around Hz and for high baseline
rates weight changes become suppressed for the hippocampus for the visual
cortex Numerical simulations with finite rate modulations were found to confirm these analytical
predictions surprisingly well Also for the nonlinear regime and Poissionian spike trains deviations
remained moderate
Discussion
STDP has been proposed to represent a fundamental mechanism underlying learning and many
models explored its computational role examples are 26 In contrast research targeting
the computational roles of dynamical phenomena inherent in STDP are in the beginning
Here we here formulated a minimal yet biologically plausible model including the dynamics of how
neuronal activity contributes to STDP We found that our model reproduces the synaptic changes in
response to spike sequences in experiments in cortex and hippocampus with high accuracy
Using the corresponding parameters our model predicts weight changes depending on temporal
structures in the pre and postsynaptic activities including spike sequences and varying firing rates
When applied to pre and postsynaptic rate modulations our approach quantifies synaptic changes
depending on frequency and phase shifts between pre and postsynaptic activities A rigorous perturbation analysis of our model reveals that the dynamical filter properties of STDP make weight
changes sensitively dependent on combinations of specific features of pre and postsynaptic signals
In particular our analysis indicates that both cortical as well as hippocampal STDP is most susceptible for modulations in the theta frequency range It predicts the dependency of synaptic changes
on pre and postsynaptic phase relations of rate modulations These results are in line with experimental results on the relation of theta rhythms and learning For instance in hippocampus it is well
established that theta oscillations are relevant for learning for a recent paper see Furthermore
spike activities in hippocampus exhibit specific phase relations with the theta rhythm for a review
see Also it has been found that during learning cortex and hippocampus tend to synchronize
with particular phase relations that depend on the novelty of the item to be learned The results
presented here underline these findings and make testable predictions for the corresponding synaptic
changes
Also we find potentiation for zero phase differences and strong attenuation of weight changes at
large baseline rates which is particularly strong for cortical synapses This finding suggests a mechanism for restricting weight changes with high activity levels and that STDP is de facto switched off
when large firing rates are required for the execution of a function as opposed to learning phases
during the latter baseline rates should be rather low which is particularly relevant in cortex While
for cortical synapses our analysis predicts that very low baseline activities are contributing most to
weight changes in hippocampus synaptic modifications peak at baseline firing rates around Hz
which suggests that can control learning
Our study suggests that the filter properties of STDP originating from the dynamics of pre and
postsynaptic activity contributions are in fact exploited for learning in the brain In particular shifts
in baseline rates as well as the frequency and the respective phases of pre and postsynaptic rate
modulations induced by theta oscillations could be tuned to match the values that make STDP most
susceptible for synaptic modifications A fascinating possibility thereby is that these features could
be used to control the learning rate which would represent a novel mechanism in addition to other
control signals as neuromodulators

<<----------------------------------------------------------------------------------------------------------------------->>

