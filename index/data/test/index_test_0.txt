query sentence: duplicated documents detection
<<------------------------------------------------------------------------------------------------------------------------------------------->>
title: 2639-a-probabilistic-model-for-online-document-clustering-with-application-to-novelty-detection.pdf

A Probabilistic Model for Online Document
Clustering with Application to Novelty Detection
Jian Zhang
School of Computer Science
Cargenie Mellon University
Pittsburgh PA
jian.zhang@cs.cmu.edu
Zoubin Ghahramani
Gatsby Computational Neuroscience Unit
University College London
London WC1N UK
zoubin@gatsby.ucl.ac.uk
Yiming Yang
School of Computer Science
Cargenie Mellon University
Pittsburgh PA
yiming@cs.cmu.edu
Abstract
In this paper we propose a probabilistic model for online document clustering We use non-parametric Dirichlet process prior to model the growing number of clusters and use a prior of general English language
model as the base distribution to handle the generation of novel clusters
Furthermore cluster uncertainty is modeled with a Bayesian Dirichletmultinomial distribution We use empirical Bayes method to estimate
hyperparameters based on a historical dataset Our probabilistic model
is applied to the novelty detection task in Topic Detection and Tracking
TDT and compared with existing approaches in the literature
Introduction
The task of online document clustering is to group documents into clusters as long as
they arrive in a temporal sequence Generally speaking it is difficult for several reasons
First it is unsupervised learning and the learning has to be done in an online fashion
which imposes constraints on both strategy and efficiency Second similar to other learning
problems in text we have to deal with a high-dimensional space with tens of thousands of
features And finally the number of clusters can be as large as thousands in newswire data
The objective of novelty detection is to identify the novel objects from a sequence of data
where novel is usually defined as dissimilar to previous seen instances Here we are interested in novelty detection in the text domain where we want to identify the earliest report
of every new event in a sequence of news stories Applying online document clustering
to the novelty detection task is straightforward by assigning the first seed of every cluster
as novel and all its remaining ones as non-novel The most obvious application of novelty
detection is that by detecting novel events systems can automatically alert people when
new events happen for example
In this paper we apply Dirichlet process prior to model the growing number of clusters and
propose to use a General English language model as a basis of newly generated clusters
In particular the new clusters will be generated according to the prior and a background
General English model and each document cluster is modeled using a Bayesian Dirichletmultinomial language model The Bayesian inference can be easily carried out due to
conjugacy and model hyperparameters are estimated using a historical dataset by the empirical Bayes method We evaluate our online clustering algorithm as well as its variants
on the novelty detection task in TDT which has been regarded as the hardest task in that
literature
The rest of this paper is organized as follows We first introduce our probabilistic model
in Section and in Section we give detailed information on how to estimate model
hyperparameters We describe the experiments in Section and related work in Section
We conclude and discuss future work in Section
A Probabilistic Model for Online Document Clustering
In this section we will describe the generative probabilistic model for online document
clustering We use n2 nV to represent a document vector where each
element nv denotes the term frequency of the th corresponding word in the document
and is the total size of the vocabulary
Dirichlet-Multinomial Model
The multinomial distribution has been one of the most frequently used language models for
modeling documents in information retrieval It assumes that given the set of parameters
a document is generated with the following probability
PV
nv
QVv=1
nv
From the formula we can see the so-called naive assumption words are assumed to be independent of each other Given a collection of documents generated from the same model
the parameter can be estimated with Maximum Likelihood Estimation
In a Bayesian approach we would like to put a Dirichlet prior over the parameter
such that the probability
of generating a document is obtained by integrating over
the parameter space This integration can be easily written
down due to the conjugacy between Dirichlet and multinomial distributions The key difference between the Bayesian approach and the MLE is that the former uses a distribution
to model the uncertainty of the parameter while the latter gives only a point estimation
Online Document Clustering with Dirichlet Process Mixture Model
In our system documents are grouped into clusters in an online fashion Each cluster is
modeled with a multinomial distribution whose parameter follows a Dirichlet prior First
a cluster is chosen based on a Dirichlet process prior can be either a new or existing
cluster and then a document is drawn from that cluster
We use Dirichlet Process to model the prior distribution of and our hierarchical
model is as follows
ci ci
iid
DP G0
where ci is the cluster indicator variable is the multinomial parameter for each document and ci is the unique for the cluster ci is a random distribution generated from
the Dirichlet process DP G0 which has a precision parameter and a base distribution G0 Here our base distribution G0 is a Dirichlet distribution
PV
with which reflects our expected knowledge about G. Intuitively our G0
distribution can be treated as the prior over general English word frequencies which has
been used in information retrieval literature to model general English documents
The exact cluster-document generation process can be described as follows
Let be the current document under processing the ith document in the input
sequence and C1 C2 Cm are already generated clusters
Draw a cluster ci based on the following Dirichlet process prior
Pmj
p(ci Cj
Cj
p(ci
Pm
Cj
Pm
where Cj stands for the cardinality of cluster with Cj and
with certain probability a new cluster will be generated
Draw the document from the cluster ci
Model Updating
Our models for each cluster need to be updated based on incoming documents We can
write down the probability that the current document is generated by any cluster as
p(xi Cj Cj p(xi
where Cj Cj is the posterior distribution of parameters of the th cluster
and we use to represent the prior distribution of the parameters of the new cluster for convenience Although the dimensionality of
is high in our case closed-form solution can be obtained under our Dirichletmultinomial assumption Once the conditional probabilities p(xi Cj are computed the
probabilities p(Cj can be easily calculated using the Bayes rule
p(Cj p(xi Cj
p(Cj
p(Cj p(xi Cj
where the prior probability of each cluster is calculated using equation
Now there are several choices we can consider on how to update the cluster models The
first choice which is correct but obviously intractable is to fork children of the
current system where the th child is updated with document assigned to cluster while
the final system is a probabilistic combination of those children with the corresponding
probabilities p(Cj The second choice is to make a hard decision by assigning the
current document to the cluster with the maximum probability
p(Cj p(xi Cj
ci arg max p(Cj
Cj
p(Cj p(xi Cj
For we use to denote the th element in the vector to denote the parameter vector that
generates the ith document and to denote the parameter vector for the th cluster
The third choice is to use a soft probabilistic updating which is similar in spirit to the
Assumed Density Filtering ADF in the literature That is each cluster is updated by
exponentiating the likelihood function with probabilities
p(Cj
Cj
p(xi
Cj
However we have to specially deal with the new cluster since we cannot afford both timewise and space-wise to generate a new cluster for each incoming document Instead we
will update all existing clusters as above and new cluster will be generated only if
We will use HD and PD hard decision and probabilistic decision to denote the
last two candidates in our experiments
Learning Model Parameters
In the above probabilistic model there are still several hyperparameters not specified
namely the and in the base distribution G0 and the precision parameter in the DP G0 Since we can obtain a partially labeled historical
dataset we now discuss how to estimate those parameters respectively
We will mainly use the empirical Bayes method to estimate those parameters instead
of taking a full Bayesian approach since it is easier to compute and generally reliable
when the number of data points is relatively large compared to the number of parameters
Because the are iid from the random distribution by integrating out the we get
G0
j<i
where the distribution is a mixture of continuous and discrete distributions and the
denotes the probability measure giving point mass to
Now suppose we have a historical dataset which contains labeled clusters
with the th cluster Hk xk,mk having mk documents
The joint probability of of all documents can be obtained as
G0
j<i
where is the total number of documents By integrating over the unknown parameter
we can get
p(xi
p(xi
G0
j<i
Empirical Bayes method can be applied to equation to estimate the model parameters
by maximization3 In the following we discuss how to estimate parameters individually in
detail
Although documents are grouped into clusters in the historical dataset we cannot make directly
use of those labels due to the fact that clusters in the test dataset are different from those in the
historical dataset
Since only a subset of documents are labeled in the historical dataset the maximization is
only taken over the union of the labeled clusters
Estimating
Our hyperparameter vector contains number of parameters for the base distribution
which can be treated as the expected distribution of the prior of the cluster parameter
Although contains number of actual parameters in our case we can still use
the empirical Bayes to do a reliable point estimation since the amount of data we have to
represent general English is large our historical dataset there are around documents
around English words in total and highly informative about We use the
smoothed estimation n1 n2 nV where nt x?H nt
PV
is the total number of times that term happened in the collection and should
be normalized to Furthermore the pseudo-count one is added to alleviate the out-ofvocabulary problem
Estimating
Though is just a scalar parameter it has the effect to control the uncertainty of the prior
knowledge about how clusters are related to the general English model with the parameter
We can see that controls how far each new cluster can deviate from the general English
model It can be estimated as follows
arg max
p(Hk arg max
p(Hk
can be numerically computed by solving the following equation
n(H
where the digamma function is defined as
dx
n(H
Alternatively we can choose by evaluating over the historical dataset This is applicable
though computationally expensive since it is only a scalar parameter and we can precompute its possible range based on equation
Estimating
The precision parameter of the DP is also very important for the model which controls
how far the random distribution can deviate from the baseline model G0 In our case it is
also the prior belief about how quickly new clusters will be generated in the sequence Similarly we can use equation to estimate since items related to can be factored out as
Suppose we have a labeled subset y1 y2 xM yM
of training data where is if is a novel document or otherwise Here we describe
two possible choices
The simplest way is to assume that is a fixed constant during the process and it
arg max
can be computed as
i?H here denotes the subset of
indices of labeled documents in the whole sequence
The assumption that is fixed maybe restrictive in reality especially considering
the fact that it reflects the generation rate of new clusters More generally we
The mean and variance of a Dirichlet distribution
are and Var[?v
can assume that is some function of variable In particular we assume
a/i ci where a and are non-negative numbers This formulation is a
generalization of the above case where the term allows a much faster decrease
at the beginning and is the asymptotic rate of events happening as
Again the parameters a and are estimated by MLE over the training dataset
a
arg maxa,b,c>0 i?H
a/i+b+ci+i
Experiments
We apply the above online clustering model to the novelty detection task in Topic Detection
and Tracking TDT has been a research community since its pilot study which
is a research initiative that aims at techniques to automatically process news documents in
terms of events There are several tasks defined in TDT and among them Novelty Detection
First Story Detection or New Event Detection has been regarded as the hardest task
in this area The objective of the novelty detection task is to detect the earliest report
for each event as soon as that report arrives in the temporal sequence of news stories
Dataset
We use the TDT2 corpus as our historical dataset for estimating parameters and use the
TDT3 corpus to evaluate our model Notice that we have a subset of documents in the
historical dataset for which events labels are given The TDT2 corpus used for
novelty detection task consists of documents among them documents are
labeled in 96 clusters Stopwords are removed and words are stemmed and after that there
are on average words per document The total number of features unique words is
around
Evaluation Measure
In our experiments we use the standard TDT evaluation measure to evaluate our results
The performance is characterized in terms of the probability of two types of errors miss
and false alarm PM iss and PF A These two error probabilities are then combined into a
single detection cost Cdet by assigning costs to miss and false alarm errors
Cdet CM iss PM iss Ptarget CF A PF A Pnon?target
where
CM iss and CF A are the costs of a miss and a false alarm respectively
PM iss and PF A are the conditional probabilities of a miss and a false alarm respectively and
Ptarget and Pnon?target is the priori target probabilities Ptarget
Pnon?target
It is the following normalized cost that is actually used in evaluating various TDT systems
Cdet norm
Cdet
min(CM iss Ptarget CF A Pnon?target
where the denominator is the minimum of two trivial systems Besides two types of evaluations are used in TDT namely macro-averaged topic-weighted and micro-averaged
Strictly speaking we only used the subsets of TDT2 and TDT3 that is designated for the novelty
detection task
story-weighted evaluations In macro-averaged evaluation the cost is computed for every
event and then the average is taken In micro-averaged evaluation the cost is averaged over
all documents decisions generated by the system thus large event will have bigger impact
on the overall performance Note that macro-averaged evaluation is used as the primary
evaluation measure in TDT. In addition to the binary decision novel or non-novel each
system is required to generated a confidence score for each test document The higher the
score is the more likely the document is novel Here we mainly use the minimum cost to
evaluate systems by varying the threshold which is independent of the threshold setting
Methods
One simple but effective method is the GAC-INCR clustering method with cosine
similarity metric and TFIDF term weighting which has remained to be the top performing
system in TDT official evaluations For this method the novelty confidence
score we used is one minus the similarity score between the current cluster and its nearest
neighbor cluster s(xi maxj<i sim(ci cj where ci and cj are the clusters that
and are assigned to respectively and the similarity is taken to be the cosine similarity
between two cluster vectors where the ltc TFIDF term weighting scheme is used to scale
each dimension of the vector Our second method is to train a logistic regression model
which combines multiple features generated by the GAC-INCR method Those features
not only include the similarity score used by the first method but also include the size of
its nearest cluster the time difference between the current cluster and the nearest cluster
etc We call this method Logistic Regression where we use the posterior probability
p(novelty|xi as the confidence score Finally for our online clustering algorithm we
choose the quantity s(xi log as the output confidence score
Experimental Results
Our results for three methods are listed in Table where both macro-averaged and microaveraged minimum normalized costs are reported The GAC-INCR method performs
very well so does the logistic regression method For our DP results we observed that
using the optimized will get results not listed in the table that are around worse
than using the obtained through validation which might be due to the flatness of the
optimal function value as well as the sample bias of the clusters in the historical dataset
Another observation is that the probabilistic decision does not actually improve the hard
decision performance especially for the var option Generally speaking our DP methods
are comparable to the other two methods especially in terms of topic-weighted measure
Table Results for novelty detection on TDT3 corpus
Method
GAC-INCR
Logistic Regression
DP with ix HD
DP with var HD
DP with ix PD
DP with var PD
Topic-weighted Cost
COST Miss FA
Story-weighted Cost
COST Miss FA
In TDT official evaluation there is also the DET curve which is similar in spirit to the ROC curve
that can reflects how the performance changes as the threshold varies We will report those results in
a longer version of this paper
It is known that the cluster labeling process of LDC is biased toward topics that will be covered
in multiple languages instead of one single language
Related Work
Zaragoza applied a Bayesian Dirichlet-multinomial model to the ad hoc information retrieval task and showed that it is comparable to other smoothed language models
Blei used Chinese Restaurant Processes to model topic hierachies for a collection of documents West discussed the sampling techniques for base distribution
parameters in the Dirichlet process mixture model
Conclusions and Future Work
In this paper we used a hierarchical probabilistic model for online document clustering
We modeled the generation of new clusters with a Dirichlet process mixture model where
the base distribution can be treated as the prior of general English model and the precision
parameter is closely related to the generation rate of new clusters Model parameters are
estimated with empirical Bayes and validation over the historical dataset Our model is
evaluated on the TDT novelty detection task and results show that our method is promising
In future work we would like to investigate other ways of estimating parameters and use
sampling methods to revisit previous cluster assignments We would also like to apply our
model to the retrospective detection task in TDT where systems do not need to make decisions online Though its simplicity the unigram multinomial model has its well-known
limitation which is the naive assumption about word independence We also plan to explore richer but still tractable language models in this framework Meanwhile we would
like to combine this model with the topic-conditioned framework as well as incorporate hierarchical mixture model so that novelty detection will be conditioned on some topic
which will be modeled by either supervised or semi-supervised learning techniques

<<----------------------------------------------------------------------------------------------------------------------->>

title: 4571-online-l1-dictionary-learning-with-application-to-novel-document-detection.pdf

Online 1-Dictionary Learning with Application to
Novel Document Detection
Huahua Wang
University of Minnesota
huwang@cs.umn.edu
Shiva Prasad Kasiviswanathan
General Electric Global Research
kasivisw@gmail.com
Arindam Banerjee
University of Minnesota
banerjee@cs.umn.edu
Prem Melville
IBM T.J. Watson Research Center
pmelvil@us.ibm.com
Abstract
Given their pervasive use social media such as Twitter have become a leading
source of breaking news A key task in the automated identification of such news
is the detection of novel documents from a voluminous stream of text documents
in a scalable manner Motivated by this challenge we introduce the problem of
online dictionary learning where unlike traditional dictionary learning which
uses squared loss the penalty is used for measuring the reconstruction error
We present an efficient online algorithm for this problem based on alternating
directions method of multipliers and establish a sublinear regret bound for this
algorithm Empirical results on news-stream and Twitter data shows that this
online dictionary learning algorithm for novel document detection gives more
than an order of magnitude speedup over the previously known batch algorithm
without any significant loss in quality of results
Introduction
The high volume and velocity of social media such as blogs and Twitter have propelled them to
the forefront as sources of breaking news On Twitter it is possible to find the latest updates on
diverse topics from natural disasters to celebrity deaths and identifying such emerging topics has
many practical applications such as in marketing disease control and national security The
key challenge in automatic detection of breaking news is being able to detect novel documents in
a stream of text where a document is considered novel if it is unlike documents seen in the past
Recently this has been made possible by dictionary learning which has emerged as a powerful data
representation framework In dictionary learning each data point is represented as a sparse linear
combination Ax of dictionary atoms where A is the dictionary and is a sparse vector A
dictionary learning approach can be easily converted into a novel document detection method let A
be a dictionary representing all documents till time for a new data document arriving at time
if one does not find a sparse combination of the dictionary atoms and the best reconstruction
Ax yields a large loss then clearly is not well represented by the dictionary A and is hence novel
compared to documents in the past At the end of timestep the dictionary is updated to represent
all the documents till time
Kasiviswanathan presented such a batch dictionary learning approach for detecting novel
documents/topics They used an penalty on the reconstruction error instead of squared loss com
Part of this wok was done while the author was a postdoc at the IBM T.J. Watson Research Center
H. Wang and A. Banerjee was supported in part by NSF CAREER grant NSF grants and NASA grant NNX12AQ39A
monly used in the dictionary learning literature as the penalty has been found to be more effective
for text analysis Section They also showed this approach outperforms other techniques such
as a nearest-neighbor approach popular in the related area of First Story Detection We build
upon this work by proposing an efficient algorithm for online dictionary learning with penalty
Our online dictionary learning algorithm is based on the online alternating directions method which
was recently proposed by Wang and Banerjee to solve online composite optimization problems
with additional linear equality constraints Traditional online convex optimization methods such
as require explicit computation of the subgradient making them computationally
expensive to be applied in our high volume text setting whereas in our algorithm the subgradients
are computed implicitly The algorithm has simple closed form updates for all steps yielding a fast
and scalable algorithm for updating the dictionary Under suitable assumptions
to cope with the
non-convexity of the dictionary learning problem we establish an regret bound for the objective matching the regret bounds of existing methods Using this online algorithm
for dictionary learning we obtain an online algorithm for novel document detection which we
empirically validate on traditional news-streams as well as streaming data from Twitter Experimental results show a substantial speedup over the batch dictionary learning based approach of
Kasiviswanathan without a loss of performance in detecting novel documents
Related Work Online convex optimization is an area of active research and for a detailed survey
on the literature we refer the reader to Online dictionary learning was recently introduced
by Mairal who showed that it provides a scalable approach for handling large dynamic
datasets They considered an penalty and showed that their online algorithm converges to the
minimum objective value in the stochastic case with distributional assumptions on the data
However the ideas proposed in do not translate to the penalty The problem of novel document/topics detection was also addressed by a recent work of Saha where they proposed a
non-negative matrix factorization based approach for capturing evolving and novel topics However
their algorithm operates over a sliding time window does not have online regret guarantees and
works only for penalty
Preliminaries
Notation Vectors
are always column vectors
and2are denoted by boldface letters For a matrix
its norm kZk1 i,j zij and kZk2F ij zij
For arbitrary real matrices the standard inner
product is defined as hY Zi Tr(Y Z). We use max to denote the largest eigenvalue of
Z. For a scalar let sign(r if if and if Define
soft(r sign(r max{|r The operators sign and soft are extended to a matrix by
applying it to every entry in the matrix denotes a matrix of all zeros of size and the
subscript is omitted when the dimension of the represented matrix is clear from the context
Dictionary Learning Background Dictionary learning is the problem of estimating a collection
of basis vectors over which a given data collection can be accurately reconstructed often with sparse
encodings It falls into a general category of techniques known as matrix factorization Classic dictionary learning techniques for sparse representation and

<<----------------------------------------------------------------------------------------------------------------------->>

title: 368-a-four-neuron-circuit-accounts-for-change-sensitive-inhibition-in-salamander-retina.pdf

A four neuron circuit accounts for change sensitive
inhibition in salamander retina
Jeffrey L. Teeters
Lawrence Livennore Lab
PO Box
Livennore CA
Frank H. Eeckman
Lawrence Livennore Lab
PO Box
Livennore CA
Frank S. Werblin
UC-Berkeley
Room LSA
Berkeley CA
Abstract
In salamander retina the response of On-Off ganglion cells to a central
flash is reduced by movement in the receptive field surround Through
computer simulation of a model which takes into account their
anatomical and physiological properties we show that interactions
between four neuron types two bipolar and two amacrine may be
responsible for the generation and lateral conductance of this change
sensitive inhibition The model shows that the four neuron circuit can
account for previously observed movement sensitive reductions in
ganglion cell sensitivity and allows visualization and prediction of the
spatio-temporal pattern of activity in change sensitive retinal cells
INTRODUCTION
In the salamander retina the response of transient On-Off ganglion cells to a central
flash is reduced by movement in the receptive field surround Werblin Werblin
Copenhagen as illustrated in Fig This phenomenon requires the detection of
change in the surround and the lateral transmission of this change sensitive inhibition to
the ganglion cell dendrites Wunk Werblin showed that all ganglion cells
receive change-sensitive inhibition and Barnes Werblin implicated a changesensitive amacrine cell with widely distributed processes The change-sensitivity of these
amacrine cells has been traced in part to a truncation of synaptic release from the bipolar
tenninals that presumably drive them Maguire The transient response of
these amacrine cells mediated by voltage gated currents Barnes Werblin Eliasof
also contributes to this change sensitivity
These and other experiments suggest that interactions between four neuron types underlie
both the change detection and the lateral transmission of inhibition Werblin
Maguire To test this hypothesis and make predictions that could be
compared with later experiments we have constructed a computational model of the four
neuron circuit and incorporated it into an overall model of the retina This model allows
us to simulate the effect of inhibition generated by the four neuron circuit on ganglion
cells
Stimulus
I
Windmill with
central test spot
Ganglion Cell Response
Stationary
windmill
Spinning
windmill
second
I
Resting level
NormaJ
Figure Change-Sensitive Inhibition Data is from Werblin
IMPLEMENTING THE HYPOTHETICAL CIRCUIT
The proposed change-sensitive circuit Werblin Maguire al is
reproduced in Figure This is meant to describe a very local region of the retina where
the receptive fields of the two bipolar cells are spatially overlapping When a visual
target enters this receptive field the bipolar cells are both depolarized The sustained
bipolar cell activates the narrow field amacrine cell that in tum feeds back to the synaptic
terminal of the transient bipolar cell to truncate transmitter release after a brief
msec delay Because the signal reaching the wide field amacrine cell is truncated after
about msec the wide field amacrine cell will receive excitation when the target enters
the recepti ve field but will not continue to respond in the presence of the target
The spatial profiles of synaptic input and output for the cell types involved in the model
are summarized in Figure The bipolar and narrow field amacrine cell sensitivities
extend over a region corresponding roughly to their dendritic spread The wide field
amacrine cell appears to receive input over a local region near the cell body but delivers
its inhibitory output over a much wider region corresponding the the full extent
mm of its processes
Figure shows the electrical circuit model for each cell type and illustrates the
interactions between cells that are implemented in the model In Figure boxes contain
the circuit for each cell and arrows between them represent synaptic interactions thought
NarrowJietd amacrine
To
Ganglion
Figure Circuitry to be Analyzed
cells
Bipolar
Narrow
Inpul and oulpull
fleld Input and output
amacrine
Wide field
Inpull
Distance from cell center Ilm
Figure Spatial Profiles of Input Sensitivity and Output Transmission
to occur as determined through experiments in which a neurotransmitter is puffed onto
bipolar dendrites Bipolar cells are modeled using two compartments corresponding to
the cell body and axon terminal as suggested in Maguire at Amacrine cells are
modeled using only one compartment as in Eliasof at
Each compartment has a voltage Vbs Vbst Vbtt Van. Vaw The cell body for the
sustained and transient bipolar are assumed to be the same Batteries in the figure
correspond to excitatory Ena or inhibitory reversal potentials Ek Eel
Resistors represent ionic conductances Circles and arrows through resisters indicate
transmitter dependent conductances which are controlled by the voltage of a presynaptic or
same cell Functions relating voltages to conductances are mostly linear with a threshold
More details are given in Teeters at
Neurotransmitter Input
Wide field
Fi~ure
Details of Circuitry
A Four Neuron Circuit Accounts for Change Sensitive Inhibition
TESTING THE COMPUTATIONAL MODEL
Computer simulation was used to tune model parameters and test whether the single cell
properties and proposed interactions between cells shown in Figure are consistent with
the responses recorded from the neurons during applications of a neurotransmitter puff
Results are shown in Figure Voltage clamp experiments electrically clamp the cell
membrane potential to a constant voltage and determine the current required to maintain
the voltage over time Downward traces indicate that current is flowing into the cell
upward traces indicate outward current For simplicity scales are not shown but in all
cases the magnitude of the simulated response is close to that of the observed response
The simulated and observed responses voltage clamps of the wide field amacrine shown in
the fourth row vary because there is a sustained outward current observed experimentally
that is not apparent in the simulations This shows that the model is not perfect and is
something that needs further investigation
This difference between the model and observed response does not prevent the
hypothesized function of the circuit from being simulated This is shown on the bottom
row where both the observed and simulated voltage responses from the wide field amacrine
are transient
SIMULATING INHIBITION TO GANGLION CELLS
Figure illustrates that we have to a large degree succeeded in combining the
characteristics of single cells into a model which can explain many of the observed
properties thought to be due to the interaction between these cells in a local region
Experiment
Observed response
Simulated Response
Neurotransm Itter
Puff Input
Voltage clamp of
bipOlar cell body
Voltage clamp of
narrow field amacrine
Wide field amacrine
Voltage clamp
Voltage clamp with
pIcrotoxin block
Voltage response
Figure Example Puff Simulations
Teeters Eeckman and Werblin
The next step in our analysis is to investigate how this circuit influences the response of
ganglion cells To do this requires simulating the input to the bipolar dendrites and
simulating the ganglion cells which receive the transient inhibition generated by the wide
field amacrine This amounts to a integrated model of an entire patch of retina including
receptors horizontal cells the four neuron circuit discussed earlier and ganglion cells
The manner in which we accomplish this is illustrated in Figure
The left side of figure shows the model elements Receptors and horizontal cells are
modeled as low pass filters with different time constants and different spatial inputs The
ganglion cell model receives a transient excitatory input generated phenomenologically by
a thresholded high pass filter from the transient bipolar Inhibitory input to the ganglion
cell is implemented as coming from the transient wide field amacrine cells described
previously For simplicity voltage gated currents and spiking are not implemented in the
ganglion cell model and only the off bipolar pathways are simulated
The right hand of Figure illustrates how the model is implemented spatially The
circuit for each cell type is duplicated across the retina patch in a matrix fonnat The
known spatial properties of each cell such as the spatial range of transmitter sensitivity
and release are incorporated into the model Details are given in Teeters
SIMULATING INHIBITION TO GANGLION CELLS
To test if the model can account for the observed reduction in ganglion cell response
during movement in the receptive field surround we simulated the experiment depicted in
Figure mainly the flashing of a central light during the presence of a stationary and
spinning windmill The results are shown in Figure
Model Elements
Receptor
Spatial Implementation
Horizontal Cell
Threshold
High-pass
filter
ang Ion
el
t:t-rEcIT
On-Off Ganglion cells
Figure Integrated Retinal Model
A Four Neuron Circuit Accounts for Change Sensitive Inhibition
Rather than displaying a single curve representing the response of a single unit over time
Figure shows the simultaneous pattern of activity in an array of neurons spatially
distributed across the retina patch at an instant in time just after a central light spot is
turned The neuron responses are the transient bipolar terminal the wide field
amacrine neurotransmitter release and the ganglion cell voltage response On the left
column is shown the response to a flashing spot when the windmill is stationary On the
right is shown the response to the same flashing spot but with a spinning windmill
When the windmill is stationary the transient bipolar terminal responds only to the
center flash Responses to the windmill vanes are suppressed by the narrow field
amacrine cell causing the appearance of four regions of hyperpolarizing responses around
the center The wide field amacrine responds to the central test flash and releases
transmitter as shown in the second row The array of ganglion cells responds to both the
excitatory input generated by the spot at the bipolar terminals and the inhibitory input
generated by the wide field amacrines Because the wide field inhibition has not yet taken
effect at this point in time the ganglion cells respond well to the flashing spot
When the windmill is spinning as is shown on the right hand column the transient
bipolar terminals generate a response to the leading edge of the windmill vanes The wide
field amacrine cells receive excitatory input from the transient bipolar terminal responses
to the vane and consequently release inhibitory neurotransmitter over a wide area as
shown in in the right column Because inhibition is being continuously generated by the
spinning windmill the response of the ganglion cells across the retinal patch has a large
Stationary Windmill
Spinning windmill
Transient Bipolar Terminal
Wide field
Ganglion cell
Ganglion Cell Inhibition Caused By Spinning Windmill
bowl shaped area of hyperpolarization which reduces the ganglion cell response of the
cells to the central test flash This is seen by the fact that the height of depolarization in
the centrally located ganglion cells is much smaller under conditions of a spinning
windmill than if the windmill is stationary This is consistent with the results found
experimentally which are illustrated in Figure Experimental data not yet attained but
which are predicted by the model simulations illustrated in Figure are the spatial
patterns of activity generated in the bipolar amacrine and ganglion cells in response to
the different stimuli
SUMMARY
Using computer simulation of a neurophysiologically based model we demonstrate that
the experimental data describing properties of four neurons in the inner retina are
compatible with the hypothesis that these neurons are involved in the detection of change
and the feedforward of change-sensitive inhibition to ganglion cells First we build a
computational model of the hypothesized four neuron circuit and determine that the
proposed interactions between them are sufficient to reproduce many of the observed
network properties in response to a puff of neurotransmitter Next we integrate this
model into a full retina model to simulate their influence on ganglion cell responses
The model verifies the consistency of presently available data and allows formation of
predictions of neural activity are subject to refutation or verification by new experiments
We are currently recording the spatio-temporal response of ganglion cells to moving
stimuli so that direct comparisons to these model predictions can be made

<<----------------------------------------------------------------------------------------------------------------------->>

title: 600-a-neural-network-that-learns-to-interpret-myocardial-planar-thallium-scintigrams.pdf

A Neural Network that Learns to Interpret
Myocardial Planar Thallium Scintigrams
Charles Rosenberg Ph.D
Jacob Erel M.D.
Department of Computer Science
Hebrew University
Jerusalem Israel
Department of Cardiology
Sapir Medical Center
Meir General Hospital
Kfar Saba Israel
Henri Atlan PhD.
Department of Biophysics and Nuclear Medicine
Hadassah Medical Center
Jerusalem Israel
Abstract
The planar thallium-201 myocardial perfusion scintigram is a widely used
diagnostic technique for detecting and estimating the risk of coronary
artery disease Neural networks learned to interpret thallium scintigrams as determined by individual expert ratings Standard error backpropagation was compared to standard LMS and LMS combined with
one layer of RBF units Using the leave-one-out method generalization was tested on all cases Training time was determined automatically from cross-validation perfonnance Best perfonnance was attained
by the RBF/LMS network with three hidden units per view and compares
favorably with human experts
Introduction
Coronary artery disease CAD is one of the leading causes of death in the Western World
The planar thallium-201 is considered to be a reliable diagnostic tool in the detection of
Current address Geriatrics Research Educational and Clinical Center VA Medical Center Salt
Lake City Utah
Rosenberg Erel and Atlan
CAD. Thallium is a radioactive isotope that distributes in mammalian tissues after intervenous administration and is imaged by a gamma camera The resulting scintigram is visually
interpreted by the physician for the presence or absence of defects areas with relatively
lower perfusion levels In myocardial applications thallium is used to measure myocardial
ischemia and to differentiate between viable and non-viable infarcted heart muscle pohost and Henzlova
Diagnosis of CAD is based on the comparison of two sets of images one set acquired
immediately after a standard effort test BRUCE protocol and the second following a
delay period of four hours During this delay the thallium redistributes in the heart muscle
and spontaneously decays Defects caused by scar tissue are relatively unchanged over
the delay period fixed defect while those caused by ischemia are partially or completely
filled-in reversible defect Beller Datz
Image interpretation is difficult for a number of reasons the inherent variability in biological systems which makes each case essentially unique the vast amount of irrelevant and
noisy information in an image and the context-dependency of the interpretation on data
from many other tests and clinical history Interpretation can also be significantly affected
by attentional shifts perceptual abilities and mental state Franken Jr. and Berbaum
Cuar6n
While networks have found considerable application in ECG processing Artis
and clinical decision-making Baxt Baxt they have thus far found
limited application in the field of nuclear medicine Non-cardiac imaging applications include the grading of breast carcinomas Dawson and the discrimination of normal Alzheimer's PET scans Kippenhan Of the studies dealing specifically
with cardiac imaging neural networks have been applied to several problems in cardiology
including the identification of stenosis Porenta Cios Cios
Cianflone Fujita These studies encouraged us to explore
the use of neural networks in the interpretation of cardiac scintigraphy
Methods
We trained one network consisting of a layer of gaussian RBF units in an unsupervised fashion to discover features in circumferential profiles in planar thallium scintigraphy Then a
second network was trained in a supervised way to map these features to physician's visual
interpretations of those images using the delta rule Widrow and Hoff This architecture was previously found to compare favorably to other network learning algorithms
2-layer backpropagation and single-layer networks on this task Rosenberg
Erel
In our experiments all of the input vectors representing single views were first normalized
to unit length IIfll The activation value of a gaussian unit OJ is then given by
netj
where is an index to a gaussian unit and is an input unit index The width of the gaussian
A Neural Network that Learns to Interpret Myocardial Planar Thallium Scintigrams
R~gion.1
Output
IL.
Scores
IL.
IL.
Ii.
Ii.
IL.
III I
Ill
Severe
Moderate
Mild
Normal
RBF
Input
ANT
LAO 45
LAT
VIEWS
Figure The network architecture The first layer Input encoded the three circumferential profiles representing the three views anterior left lateral oblique and
left lateral The second layer consisted of radial basis function RBF units the third
layer semi-linear units trained in a supervised fashion The outputs of the network corresponded to the visual scores as given by the expert observer An additional unit per view
encoded the scaling factor of the input patterns lost as a result of input normalization
given by was fixed at for all units
The gaussian units were trained using a competitive learning rule which moves the center
of the unit closest to the current input pattern Omax the winner closer to the input
pattern2
tui,winner
Wi,winner
Data Acquisition and Selection
Scintigraphic images were acquired for each of three views anterior left lateral
oblique LAO and left lateral LAT for each patient case Acquisition was performed
twice once immediately following a standard effort test and once following a delay period
of four hours Each image was pre-processed to produce a circumferential profile Garcia
Francisco in which maximum pixel counts within each of
contiguous segmental regions are plotted as a function of angle Garcia Preprocessing involved positioning of the region of interest interpolative background
subtraction smoothing and rotational alignment to the heart's apex Garcia
1We have considered applying the learning rule to the unit widths as well as the RBF weights
however we have not as yet pursued this possibility
2Following Rumelhart and Zipser Rumelhart and Zipser the other units were also pulled
towards the input vector although to a much smaller extent than the winner We used a ratio of to
3The profiles were generated using the Elscint CTL software package for planar quantitative
thallium-20l based on the Cedars-Sinai technique Garcia Maddahi Areeda
Rosenberg Ere and Atlan
Lesion
mild
moderate
severe
Total
single
17
multiple
43
Total
28
Table Distribution of Abnormal Cases as Scored by the Expert Observer Defects occurring in any combination of two or more regions even the proximal and distal subregions
of a single area were treated as one multiple defect The severity level of multiple lesions
was based on the most severe lesion present
Cases were pre-selected based on the following criteria Beller
Insufficient exercise Cases in which the heart rate was less than were
eliminated as this level of stress is generally deemed insufficient to accurately
distinguish normal from abnormal conditions
Positional abnormalities In a few cases the region of interest was not positioned or aligned correctly by the technician
Increased lung uptake Typically in cases of multi-vessel disease a significant
proportion of the perfusion occurs in the lungs as well as in the heart making it
more difficult to determine the condition of the heart due to the partially overlapping positions of the heart and lungs
Breast artifacts
Cases were selected at random between August and March Approximately a
third of the cases were eliminated due to insufficient heart rate due to breast artifacts
due to lung uptake and due to positional abnormalities A set of one hundred
usable cases remained
Visual Interpretation
Each case was visually scored by a single expert observer for each of nine anatomical regions generally accepted as those that best relate to the coronary circulation Septal proximal and distal Anterior proximal and distal Apex Inferior proximal and distal and
Posterior-Lateral proximal and distal Scoring for each region was from normal to
severe indicating the level of the observed perfusion deficit
Intra-observer variability was examined by having the observer re-interpret 17 of the cases
a second time The observer was unable to remember the cases from the first reading and
could not refer to the previous scores
Exact matches were obtained on of the regions only of the total regions
were labeled as a defect mild moderate or severe on one occasion and not on the other
All differences when they occurred were of a single rating level4
4In contrast measured inter-observer variability was much higher A set of 13 cases was individ
A Neural Network that Learns to Interpret Myocardial Planar Thallium Scintigrams
The Network Model
The input units of the network were divided into groups of units each each group
representing the circumferential profile for a single view A set of RBF units were assigned
to each input group Then a second layer of weights was trained using the delta rule to
reproduce the target visual scores assigned by the expert observer The categorical visual
scores were translated to numerical values to make the data suitable for network learning
normal mild defect moderate defect and severe defect
In order to make efficient use of the available data we actually trained identical networks each network was trained on a subset of 99 of the cases and tested on the remaining one This procedure sometimes referred to as the leave-one-out or jack-knife
method enabled us to determine the generalization performance for each case This procedure was followed for both the RBF and the delta rule training Training of a single
network took only a few minutes of Sun computer time
Results
Because of the larger numbers of confusions between normal and mild regions in both the
inter and intra-observer scores disease was defined as moderate or severe defects The
threshold value dividing the output values of the network into these two sets was varied
from to in step increments The number of agreements between the expert observer
and the network were computed for each threshold value The resulting scores accumulated
over all threshold values were plotted as a Receiver Operating Characteristic ROC curve
Best performance percent correct was achieved with a threshold value of which
yielded an overall accuracy of regions on the stress data However this
value of the threshold heavily favored specificity over sensitivity due to the preponderance
of normal regions in the data Using the decision threshold which maximized the sum
of sensitivity and specificity accuracy dropped to but sensitivity
improved to and specificity was
Distinguishing Fixed Reversible Defects
In order to take into account the delayed distribution as well as the stress set of images the
network was essentially duplicated one network processed the stress data and the other
ually interpreted by expert observers in a previous experiment Rosenberg Percent
agreement exact matches between the observers was Of the 63 mis-matches or
about of the regions were of levels of severity There were no differences of levels of severity
Approximately two-thirds of the disagreements were between normal and mild regions These results
indicate that the single observer data employed in the present study are more reliable than the mixed
consensus and individual scores used previously
5Details of network learning were as follows Each of the networks was initialized and trained
in the same way RBF-to-output unit weights were initialized to small random values between and
Input-to-RBF unit weights were first randomized and then normalized so that the weight vectors
to each RBF unit were of unit length Unsupervised competitive training of the RBF units continued
for epochs or complete sweeps through the set of 99 cases epochs with a learning rate
of followed by epochs at without momentum Supervised training using a learning
rate of and momentum was terminated based on cross-validation testing after epochs
Further training led to over-training and poorer generalization
Rosenberg Erel and Atlan
the redistribution data For details see Erel
The combined network exhibited only a limited ability to distinguish between scar and
ischemia Performance on scar detection was good sens spec
but the sensitivity of the network on ischemia detection was only
This result may be explained at least in part by the much smaller number of ischemic regions included in the data set as compared with scars versus
Conclusions and Future Directions
We suspect that our major limitation is in defect sampling In order that a statistical system
networks or otherwise generalize well to new cases the data used in training must be
representative of the full population of data likely to be sampled This is unlikely to happen
when the number of positive cases is on the order of as was the case with ischemia
since each possible defect location plus all the possible combinations of locations must be
included
A variant ofbackpropagation called competitive backpropagation has recently been developed which is claimed to generalize appropriately in the presence of multiple defects Cho
and Reggia Weights in this network are constrained to take on positive values
so that diagnoses made by the system add constructively In a standard backpropagation
network multiple diseases can cancel each other out due to complex interactions of both
positive and negative connection strengths We are currently planning to investigate the
application of this learning algorithm to the problem of ischemia detection
Other improvements and extensions include
Elicit confidence ratings Expert visual interpretations could be augmented by
degree of confidence ratings Highly ambiguous cases could be reduced in importance or eliminated The ratings could also be used as additional targets for
the network6 cases indicated by the network with low levels of confidence would
require closer inspection by a physician Initial results are promising in this regard
Provide additional information We have not yet incorporated clinical history
gender and examination EKG. Clinical history has been found to have a profound
impact on interpretation of radiographs Doubilet and Herman The inclusion of these variables should allow the network to approximate more closely a
complete diagnosis and boost the utility of the network in the clinical setting
Add constraints Currently we do not utilize the angles that relate the three views
It may be possible to build these angles in as constraints and thereby cut down on
the number of free network parameters
Expand application Besides planar thallium our approach may also be applied
to non-planar imaging technologies such as SPECT and other nuclear agents or
stress-inducing modalities such as dipyridamole Preliminary results are promising in this regard
6See fesauro and Sejnowski for a related idea
A Neural Network that Learns to Interpret Myocardial Planar Thallium Scintigrams
Acknowledgements
The authors wish to thank Mr. Haim Karger for technical assistance and the Departments
of Computer Science and Psychology at the Hebrew University for computational support
We would also like to thank Drs. David Shechter Moshe Bocher Roland Chisin and the
staff of the Department of Medical Biophysics and Nuclear Medicine for their help both
large and small and two anonymous reviewers Terry Sejnowski suggested our use of RBF
units

<<----------------------------------------------------------------------------------------------------------------------->>

title: 4518-factoring-nonnegative-matrices-with-linear-programs.pdf

Factoring nonnegative matrices with linear programs
Victor Bittorf
bittorf@cs.wisc.edu
Benjamin Recht
brecht@cs.wisc.edu
Computer Sciences
University of Wisconsin
Christopher R?e
chrisre@cs.wisc.edu
Joel A. Tropp
Computing and Mathematical Sciences
California Institute of Technology
tropp@cms.caltech.edu
Abstract
This paper describes a new approach based on linear programming for computing nonnegative matrix factorizations NMFs The key idea is a data-driven
model for the factorization where the most salient features in the data are used to
express the remaining features More precisely given a data matrix the algorithm identifies a matrix that satisfies CX and some linear constraints
The constraints are chosen to ensure that the matrix selects features these features can then be used to find a low-rank NMF of A theoretical analysis
demonstrates that this approach has guarantees similar to those of the recent NMF
algorithm of Arora In contrast with this earlier work the proposed
method extends to more general noise models and leads to efficient scalable algorithms Experiments with synthetic and real datasets provide evidence that the
new approach is also superior in practice An optimized implementation can
factor a multigigabyte matrix in a matter of minutes
Introduction
Nonnegative matrix factorization NMF is a popular approach for selecting features in data
Many machine-learning and data-mining software packages including Matlab and
Oracle Data Mining now include heuristic computational methods for NMF. Nevertheless we
still have limited theoretical understanding of when these heuristics are correct
The difficulty in developing rigorous methods for NMF stems from the fact that the problem is
computationally challenging Indeed Vavasis has shown that NMF is NP-Hard see for
further worst-case hardness results As a consequence we must instate additional assumptions on
the data if we hope to compute nonnegative matrix factorizations in practice
In this spirit Arora Ge Kannan and Moitra AGKM have exhibited a polynomial-time algorithm
for NMF that is provably correct?provided that the data is drawn from an appropriate model based
on ideas from The AGKM result describes one circumstance where we can be sure that NMF
algorithms are capable of producing meaningful answers This work has the potential to make an
impact in machine learning because proper feature selection is an important preprocessing step for
many other techniques Even so the actual impact is damped by the fact that the AGKM algorithm
is too computationally expensive for large-scale problems and is not tolerant to departures from the
modeling assumptions Thus for NMF there remains a gap between the theoretical exercise and the
actual practice of machine learning
The present work presents a scalable robust algorithm that can successfully solve the NMF problem
under appropriate hypotheses Our first contribution is a new formulation of the nonnegative feature
selection problem that only requires the solution of a single linear program Second we provide
a theoretical analysis of this algorithm This argument shows that our method succeeds under the
same modeling assumptions as the AGKM algorithm with an additional margin constraint that is
common in machine learning We prove that if there exists a unique well-defined model then we
can recover this model accurately our error bound improves substantially on the error bound for
the AGKM algorithm in the high SNR regime One may argue that NMF only makes sense
is well posed when a unique solution exists and so we believe our result has independent interest
Furthermore our algorithm can be adapted for a wide class of noise models
In addition to these theoretical contributions our work also includes a major algorithmic and experimental component Our formulation of NMF allows us to exploit methods from operations research
and database systems to design solvers that scale to extremely large datasets We develop an efficient
stochastic gradient descent SGD algorithm that is at least two orders of magnitude faster than the
approach of AGKM when both are implemented in Matlab We describe a parallel implementation
of our SGD algorithm that can robustly factor matrices with features and examples in a few
minutes on a multicore workstation
Our formulation of NMF uses a data-driven modeling approach to simplify the factorization problem More precisely we search for a small collection of rows from the data matrix that can be
used to express the other rows This type of approach appears in a number of other factorization
problems including rank-revealing QR interpolative decomposition subspace clustering dictionary learning and others Our computational techniques can be adapted to
address large-scale instances of these problems as well
Separable Nonnegative Matrix Factorizations and Hott Topics
Notation For a matrix and indices and we write Mi for the ith row of and M?j for the
jth column of We write Mij for the entry
Let be a nonnegative data matrix with columns indexing examples and rows indexing
features Exact NMF seeks a factorization where the feature matrix is where
the weight matrix is and both factors are nonnegative Typically min{f
Unless stated otherwise we assume that each row of the data matrix is normalized so it sums to
one Under this hypothesis we may also assume that each row of and of also sums to one
It is notoriously difficult to solve the NMF problem Vavasis showed that it is NP-complete to decide
whether a matrix admits a rank-r nonnegative factorization AGKM proved that an exact NMF
algorithm can be used to solve 3-SAT in subexponential time
The literature contains some mathematical analysis of NMF that can be used to motivate algorithmic
development Thomas developed a necessary and sufficient condition for the existence of a
rank-r NMF. More recently Donoho and Stodden obtained a related sufficient condition for
uniqueness AGKM exhibited an algorithm that can produce a nonnegative matrix factorization
under a weaker sufficient condition To state their results we need a definition
Definition A set of vectors vr Rd is simplicial if no vector vi lies in the convex
hull of vj The set of vectors is robust simplicial if for each the distance from vi
to the convex hull of vj is at least Figure illustrates these concepts
These ideas support the uniqueness results of Donoho and Stodden and the AGKM algorithm Indeed we can find an NMF of efficiently if contains a set of rows that is simplicial and whose
convex hull contains the remaining rows
Definition An NMF is called separable if the rows of are simplicial and there is
a permutation matrix such that
Ir
Algorithm AGKM Approximably Separable
Nonnegative Matrix Factorization
Initialize
Compute the matrix with Dij
d1
kXi Xj k1
for do
Find the set Nk of rows that are at least
away from Xk
Compute the distance of Xk from
conv({Xj Nk
if add to the set R.
end for
Cluster the rows in as follows and are
in the same cluster if Djk
Choose one element from each cluster to
yield
arg minZ?Rf kX ZW
d1
d2
Figure Numbered circles are hott topics Their
convex hull orange contains the other topics small
circles so the data admits a separable NMF. The arrow d1 marks the distance from hott topic to the
convex hull of the other two hott topics definitions of
d2 and d3 are similar The hott topics are robustly
simplicial when each di
To compute a separable factorization of we must first identify a simplicial set of rows from
Afterward we compute weights that express the remaining rows as convex combinations of this
distinguished set We call the simplicial rows hott and the corresponding features hott topics
This model allows us to express all the features for a particular instance if we know the values of
the instance at the simplicial rows This assumption can be justified in a variety of applications For
example in text knowledge of a few keywords may be sufficient to reconstruct counts of the other
words in a document In vision localized features can be used to predict gestures In audio data a
few bins of the spectrogram may allow us to reconstruct the remaining bins
While a nonnegative matrix one encounters in practice might not admit a separable factorization it
may be well-approximated by a nonnnegative matrix with separable factorization AGKM derived an
algorithm for nonnegative matrix factorization of a matrix that is well-approximated by a separable
factorization To state their result we introduce a norm on matrices
max
Theorem AGKM Let and be nonnegative constants satisfying
Let be
a nonnegative data matrix Assume where is a nonnegative matrix whose rows
have unit norm where is a rank-r separable factorization in which the rows of
are robust simplicial and where Then Algorithm finds a rank-r nonnegative
that satisfies the error bound
factorization
In particular the AGKM algorithm computes the factorization exactly when Although
this method is guaranteed to run in polynomial time it has many undesirable features First the
algorithm requires a priori knowledge of the parameters and It may be possible to calculate
but we can only estimate if we know which rows are hott Second the algorithm computes
all distances between rows at a cost of O(f Third for every row in the matrix we must
determine its distance to the convex hull of the rows that lie at a sufficient distance this step requires
us to solve a linear program for each row of the matrix at a cost of Finally this method is
intimately linked to the choice of the error norm It is not obvious how to adapt the algorithm
for other noise models We present a new approach based on linear programming that overcomes
these drawbacks
Main Theoretical Results NMF by Linear Programming
This paper shows that we can factor an approximately separable nonnegative matrix by solving a
linear program A major advantage of this formulation is that it scales to very large data sets
d3
Algorithm Separable Nonnegative Matrix Factorization by Linear Programming
Require An nonnegative matrix with a rank-r separable NMF.
Ensure An matrix and matrix with and
Find the unique to minimize pT diag(C where is any vector with distinct values
Let I Cii and set YI and C?I
Here is the key observation Suppose that is any nonnegative matrix that admits a rank-r
separable factorization If we pad with zeros to form an matrix we have
Ir
CY
We call the matrix factorization localizing Note that any factorization localizing matrix is an
element of the polyhedral set
CY Tr(C Cjj Cij Cjj
Thus to find an exact NMF of it suffices to find a feasible element of whose
diagonal is integral This task can be accomplished by linear programming Once we have such
a we construct by extracting the rows of that correspond to the indices where Cii
We construct the feature matrix by extracting the nonzero columns of C. This approach is
summarized in Algorithm In turn we can prove the following result
Theorem Suppose is a nonnegative matrix with a rank-r separable factorization
Then Algorithm constructs a rank-r nonnegative matrix factorization of
As the theorem suggests we can isolate the rows of that yield a simplicial factorization by solving
a single linear program The factor can be found by extracting columns of C.
Robustness to Noise
Suppose we observe a nonnegative matrix whose rows sum to one Assume that
where is a nonnegative matrix whose rows sum to one which has a rank-r separable factorization
such that the rows of are robust simplicial and where Define the
polyhedral set
kCX Tr(C Cjj Cij Cjj
The set consists of matrices that approximately locate a factorization of We can prove
the following result
Theorem Suppose that satisfies the assumptions stated in the previous paragraph Furthermore assume that for every row that is not hott we have the margin constraint
kYj
d0
for all hott rows Then we can find a nonnegative factorization satisfying
Furthermore this factorization correctly identifies the hott topics
provided that min{?d
appearing in the separable factorization of
Algorithm requires the solution of two linear programs The first minimizes a cost vector over
Afterward the matrix can be found by setting
This lets us find
arg min
Our robustness result requires a margin-type constraint assuming that the original configuration
consists either of duplicate hott topics or topics that are reasonably far away from the hott topics On
the other hand under such a margin constraint we can construct a considerably better approximation
that guaranteed by the AGKM algorithm Moreover unlike AGKM our algorithm does not need to
know the parameter
Algorithm Approximably Separable Nonnegative Matrix Factorization by Linear Programming
Require An nonnegative matrix that satisfies the hypotheses of Theorem
Ensure An matrix and matrix with and kX
Find that minimizes pT diag where is any vector with distinct values
Let I Cii and set XI
Set arg minZ?Rf kX ZW
The proofs of Theorems and can be found in the version of this paper The main idea
is to show that we can only represent a hott topic efficiently using the hott topic itself Some earlier
versions of this paper contained incomplete arguments which we have remedied For a signifcantly
stronger robustness analysis of Algorithm see the recent paper
Having established these theoretical guarantees it now remains to develop an algorithm to solve
the LP. Off-the-shelf LP solvers may suffice for moderate-size problems but for large-scale matrix
factorization problems their running time is prohibitive as we show in Section In Section we
turn to describe how to solve Algorithm efficiently for large data sets
Related Work
Localizing factorizations via column or row subset selection is a popular alternative to direct factorization methods such as the SVD. Interpolative decomposition such as Rank-Revealing QR
and CUR have favorable efficiency properties as compared to factorizations such as SVD that
are not based on exemplars Factorization localization has been used in subspace clustering and has
been shown to be robust to outliers
In recent work on dictionary learning Esser and Elhamifar have proposed a factorization
localization solution to nonnegative matrix factorization using group sparsity techniques
Esser prove asymptotic exact recovery in a restricted noise model but this result requires
preprocessing to remove duplicate or near-duplicate rows Elhamifar shows exact representative
recovery in the noiseless setting assuming no hott topics are duplicated Our work here improves
upon this work in several aspects enabling finite sample error bounds the elimination of any need
to preprocess the data and algorithmic implementations that scale to very large data sets
Incremental Gradient Algorithms for NMF
The rudiments of our fast implementation rely on two standard optimization techniques dual decomposition and incremental gradient descent Both techniques are described in depth in Chapters
and of Bertsekas and Tstisklis
We aim to minimize pT diag(C subject to To proceed form the Lagrangian
diag(C
kXi k1
with multipliers and Note that we do not dualize out all of the constraints The remaining
ones appear in the constraint set diag(C and Cij Cjj for all
Dual subgradient ascent solves this problem by alternating between minimizing the Lagrangian over
the constraint set and then taking a subgradient step with respect to the dual variables
kXi k1 and s(Tr(C
where is the minimizer of the Lagrangian over The update of makes very little difference
in the solution quality so we typically only update
We minimize the Lagrangian using projected incremental gradient descent Note that we can rewrite
the Lagrangian as
1T
wj kXjk CX]jk k1 pj Cjj
j?supp(X?k
Algorithm OTTOPIXX Approximate Separable NMF by Incremental Gradient Descent
Require An nonnegative matrix Primal and dual stepsizes sp and sd
Ensure An matrix and matrix with and kX
Pick a cost with distinct entries
Initialize
for Nepochs do
for do
Choose uniformly at random from
sp sign(X?k CX?k
sp diag
end for
Project onto
sd
end for
Let I Cii and set XI
Set arg minZ?Rf kX ZW
Here supp(x is the set indexing the entries where is nonzero and is the number of nonzeros
in row divided by The incremental gradient method chooses one of the summands at random
and follows its subgradient We then project the iterate onto the constraint set The projection
onto can be performed in the time required to sort the individual columns of plus a linear-time
operation The full procedure is described in the extended version of this paper In the case
where we expect a unique solution we can drop the constraint Cij Cjj resulting in a simple
clipping procedure set all negative items to zero and set any diagonal entry exceeding one to one
In practice we perform a tradeoff Since the constraint Cij Cjj is used solely for symmetry
breaking we have found empirically that we only need to project onto every iterations or so
This incremental iteration is repeated times in a phase called an epoch After each epoch we
update the dual variables and quit after we believe we have identified the large elements of the
diagonal of C. Just as before once we have identified the hott rows we can form by selecting
these rows of We can find just as before by solving Note that this minimization can
also be computed by incremental subgradient descent The full procedure called OTTOPIXX is
described in Algorithm
Sparsity and Computational Enhancements for Large Scale
For small-scale problems OTTOPIXX can be implemented in a few lines of Matlab code But for
the very large data sets studied in Section we take advantage of natural parallelism and a host
of low-level optimizations that are also enabled by our formulation As in any numerical program
memory layout and cache behavior can be critical factors for performance We use standard techniques in-memory clustering to increase prefetching opportunities padded data structures for better
cache alignment and compiler directives to allow the Intel compiler to apply vectorization
Note that the incremental gradient step step in Algorithm only modifies the entries of where
X?k is nonzero Thus we can parallelize the algorithm with respect to updating either the rows
or the columns of C. We store in large contiguous blocks of memory to encourage hardware
prefetching In contrast we choose a dense representation of our localizing matrix this choice
trades space for runtime performance
Each worker thread is assigned a number of rows of so that all rows fit in the shared L3 cache
Then each worker thread repeatedly scans while marking updates to multiple rows of C. We
repeat this process until all rows of are scanned similar to the classical block-nested loop join in
relational databases
Experiments
Except for the speedup curves all of the experiments were run on an identical configuration a dual
Xeon cores each machine with of RAM. The kernel is Linux
hott
hott fast
hott
AGKM
hott
hott fast
AGKM
hott
hott fast
hott
AGKM
hott
hott fast
AGKM
Pr(RMSE RMSEmin
Pr(RMSE RMSEmin
hott
hott fast
AGKM
Pr(error errormin
Pr(time timemin
Pr(error errormin
Pr(error errormin
hott
hott fast
AGKM
Figure Performance profiles for synthetic data 1)-norm error for sized instances and
all instances is the performance profile for running time on all instances RMSE performance profiles
for the small scale and medium scale experiments 1)-norm error for the In the noisy
examples even epochs of OTTOPIXX is sufficient to obtain competitive reconstruction error
In small-scale synthetic experiments we compared OTTOPIXX to the AGKM algorithm and the
linear programming formulation of Algorithm implemented in Matlab Both AGKM and Algorithm were run using CVX coupled to the SDPT3 solver We ran OTTOPIXX for
epochs with primal stepsize and dual stepsize Once the hott topics were identified we fit
using two cleaning epochs of incremental gradient descent for all three algorithms
To generate our instances we sampled hott topics uniformly from the unit simplex in Rn These
topics were duplicated times We generated the remaining r(d rows to be random convex
combinations of the hott topics with the combinations selected uniformly at random We then
added noise with 1)-norm error bounded by
Recall that AGKM algorithm is only
guaranteed to work for We ran with
and Each experiment was repeated times
Because we ran over experiments with different parameter settings it is convenient to use
the performance profiles to compare the performance of the different algorithms Let be the
set of experiments and A denote the set of different algorithms we are comparing Let Qa be
the value of some performance metric of the experiment for algorithm a A. Then the
performance profile at for a particular algorithm is the fraction of the experiments where the value
of Qa lies within a factor of of the minimal value of minb?A Qb That is
Pa
Qa mina0 A Qa0
In a performance profile the higher a curve corresponding to an algorithm the more often it outperforms the other algorithms This gives a convenient way to contrast algorithms visually
Our performance profiles are shown in Figure The first two figures correspond to experiments
with and The third figure is for the synthetic experiments with all other values
of and In terms of 1)-norm error the linear programming solver typically achieves the
lowest error However using SDPT3 it is prohibitively slow to factor larger matrices On the other
hand OTTOPIXX achieves better noise performance than the AGKM algorithm in much less time
Moreover the AGKM algorithm must be fed the values of and in order to run OTTOPIXX does
not require this information and still achieves about the same error performance
We also display a graph for running only four epochs hott This algorithm is by far the fastest
algorithm but does not achieve as optimal a noise performance For very high levels of noise
however it achieves a lower reconstruction error than the AGKM algorithm whose performance
data set
jumbo
clueweb
RCV1
features
documents
nonzeros
size
time
Table Description of the large data sets Time is to find hott topics on the core machines
jumbo
clueweb
class error
RMSE
speedup
threads
number of topics
number of topics
Figure left The speedup over a serial implementation for OTTOPIXX on the jumbo and clueweb data
sets Note the superlinear speedup for up to threads middle The RMSE for the clueweb data set right
The test error on RCV1 CCAT class versus the number of hott topics The horizontal line indicates the test
error achieved using all of the features
degrades once approaches or exceeds Figure We also provide performance profiles for
the root-mean-square error of the nonnegative matrix factorizations Figure and The
performance is qualitatively similar to that for the
We also coded OTTOPIXX in using the design principles described in Section and ran on
three large data sets We generated a large synthetic example jumbo as above with We
generated a co-occurrence matrix of people and places from the ClueWeb09 Dataset normalized
by TFIDF We also used OTTOPIXX to select features from the RCV1 data set to recognize the
class CCAT The statistics for these data sets can be found in Table
In Figure left we plot the speed-up over a serial implementation In contrast to other parallel
methods that exhibit memory contention we see superlinear speed-ups for up to threads
due to hardware prefetching and cache effects All three of our large data sets can be trained in
minutes showing that we can scale OTTOPIXX on both synthetic and real data Our algorithm is
able to correctly identify the hott topics on the jumbo set For clueweb we plot the RMSE Figure
middle This curve rolls off quickly for the first few hundred topics demonstrating that our algorithm may be useful for dimensionality reduction in Natural Language Processing applications For
RCV1 we trained an SVM on the set of features extracted by OTTOPIXX and plot the misclassification error versus the number of topics in Figure right With hott topics we achieve
misclassification error as compared to with the entire set of features
Discussion
This paper provides an algorithmic and theoretical framework for analyzing and deploying any factorization problem that can be posed as a linear convex factorization localizing program Future
work should investigate the applicability of OTTOPIXX to other factorization localizing algorithms
such as subspace clustering and should revisit earlier theoretical bounds on such prior art
Acknowledgments
The authors would like to thank Sanjeev Arora Michael Ferris Rong Ge Nicolas Gillis Ankur
Moitra and Stephen Wright for helpful suggestions BR is generously supported by ONR award
NSF award and a Sloan Research Fellowship CR is generously
supported by NSF CAREER award under ONR award and gifts or
research awards from American Family Insurance Google Greenplum and Oracle JAT is generously supported by ONR award AFOSR award and a Sloan
Research Fellowship

<<----------------------------------------------------------------------------------------------------------------------->>

title: 3254-object-recognition-by-scene-alignment.pdf

Object Recognition by Scene Alignment
Bryan C. Russell Antonio Torralba Ce Liu Rob Fergus William T. Freeman
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
Cambrige MA USA
brussell,torralba,celiu,fergus,billf}@csail.mit.edu
Abstract
Current object recognition systems can only recognize a limited number of object
categories scaling up to many categories is the next challenge We seek to build
a system to recognize and localize many different object categories in complex
scenes We achieve this through a simple approach by matching the input image in an appropriate representation to images in a large training set of labeled
images Due to regularities in object identities across similar scenes the retrieved
matches provide hypotheses for object identities and locations We build a probabilistic model to transfer the labels from the retrieval set to the input image We
demonstrate the effectiveness of this approach and study algorithm component
contributions using held-out test sets from the LabelMe database
Introduction
The recognition of objects in a scene often consists of matching representations of image regions
to an object model while rejecting background regions Recent examples of this approach include
aligning pictorial cues shape correspondence and modeling the constellation of parts
Other models exploiting knowledge of the scene context in which the objects reside have proven
successful in boosting object recognition performance These methods model the
relationship between scenes and objects and allow information transfer across the two
Here we exploit scene context using a different approach we formulate the object detection problem as one of aligning elements of the entire scene to a large database of labeled images The
background instead of being treated as a set of outliers is used to guide the detection process Our
approach relies on the observation that when we have a large enough database of labeled images we
can find with high probability some images in the database that are very close to the query image
in appearance scene contents and spatial arrangement Since the images in the database
are partially labeled we can transfer the knowledge of the labeling to the query image Figure
illustrates this idea With these assumptions the problem of object detection in scenes becomes a
problem of aligning scenes The main issues are Can we find a big enough dataset to span the
required large number of scene configurations Given an input image how do we find a set of
images that aligns well with the query image How do we transfer the knowledge about objects
contained in the labels
The LabelMe dataset is well-suited for this task having a large number of images and labels
spanning hundreds of object categories Recent studies using non-parametric methods for computer
vision and graphics show that when a large number of images are available simple indexing
techniques can be used to retrieve images with object arrangements similar to those of a query image
The core part of our system is the transfer of labels from the images that best match the query image
We assume that there are commonalities amongst the labeled objects in the retrieved images and we
cluster them to form candidate scenes These scene clusters give hints as to what objects are depicted
screen
desk
mousepad
keyboard
Input image
Images with similar scene
configuration
mouse
Output image with object
labels transferred
Figure
Overview of our system Given an input image we search for images having a similar scene
configuration in a large labeled database The knowledge contained in the object labels for the best matching
images is then transfered onto the input image to detect objects Additional information such as depth-ordering
relationships between the objects can also be transferred
Figure Retrieval set images Each of the two rows depicts an input image on the left and images from
the LabelMe dataset that best match the input image using the gist feature and L1 distance the images
are sorted by their distances in raster order Notice that the retrieved images generally belong to similar scene
categories Also the images contain mostly the same object categories with the larger objects often matching
in spatial location within the image Many of the retrieved images share similar geometric perspective
in the query image and their likely location We describe a relatively simple generative model for
determining which scene cluster best matches the query image and use this to detect objects
The remaining sections are organized as follows In Section we describe our representation for
scenes and objects We formulate a model that integrates the information in the object labels with
object detectors in Section In Section we extend this model to allow clustering of the retrieved
images based on the object labels We show experimental results of our system output in Section
and conclude in Section
Matching Scenes and Objects with the Gist Feature
We describe the gist feature which is a low dimensional representation of an image region
and has been shown to achieve good performance for the scene recognition task when applied to an
entire image To construct the gist feature an image region is passed through a Gabor filter bank
comprising scales and orientations The image region is divided into a non-overlapping grid
and the output energy of each filter is averaged within each grid cell The resulting representation
is a dimensional vector Note that the gist feature preserves spatial structure
information and is similar to applying the SIFT descriptor to the image region
We consider the task of retrieving a set of images which we refer to as the retrieval set that closely
matches the scene contents and geometrical layout of an input image Figure shows retrieval sets
for two typical input images using the gist feature We show the top closest matching images
from the LabelMe database based on the L1-norm distance which is robust to outliers Notice that
the gist feature retrieves images that match the scene type of the input image Furthermore many
of the objects depicted in the input image appear in the retrieval set with the larger objects residing
in approximately the same spatial location relative to the image Also the retrieval set has many
images that share a similar geometric perspective Of course not every retrieved image matches
well and we account for outliers in Section
Utilizing Retrieval Set
Images for Object Detection
screen
SVM local appearance
We evaluate the ability of the retrieval
set to predict the presence of objects in
the input image For this we found a
retrieval set of images and formed
a normalized histogram the histogram
entries sum to one of the object categories that were labeled We compute
performance for object categories with
at least training examples and that
appear in at least test images We
compute the area under the ROC curve
for each object category As a comparison we evaluate the performance
of an SVM applied to gist features by
using the maximal score over a set of
bounding boxes extracted from the image The area under ROC performance
of the retrieval set versus the SVM is
shown in Figure as a scatter plot with
each point corresponding to a tested object category As a guide a diagonal
line is displayed those points that reside above the diagonal indicate better
SVM performance and vice versa Notice that the retrieval set predicts well
the objects present in the input image
and outperforms the detectors based on
local appearance information the SVM
for most object classes
sidewalk
road
mouse
head
keyboard
phone
mousepad
table bookshelf
lampspeaker motorbike
pole cup
cabinet
mug
blindbottle
paper book
car
chair
streetlight
plant
tree
person
window
door
sky
Retrieval set
Figure Evaluation of the goodness of the retrieval set by
how well it predicts which objects are present in the input image We build a simple classifier based on object counts in the
retrieval set as provided by their associated LabelMe object labels We compare this to detection based on local appearance
alone using an SVM applied to bounding boxes in the input image the maximal score is used The area under the ROC curve
is computed for many object categories for the two classifiers
Performance is shown as a scatter plot where each point represents an object category Notice that the retrieval set predicts
well object presence and in a majority cases outperforms the
SVM output which is based only on local appearance
In Section we observed that the set of labels corresponding to images that best match an input
image predict well the contents of the input image In this section we will describe a model that
integrates local appearance with object presence and spatial likelihood information given by the
object labels belonging to the retrieval set
We wish to model the relationship between object categories their spatial location within an
image and their appearance For a set of images each having Mi object proposals over
object categories we assume a joint model that factorizes as follows
Mi
p(oi,j hi,j p(xi,j oi,j hi,j p(gi,j oi,j hi,j
hi,j
We assume that the joint model factorizes as a product of three terms p(oi,j hi,j the
likelihood of which object categories will appear in the image p(xi,j oi,j hi,j
the likely spatial locations of observing object category in the image and iii p(gi,j oi,j hi,j
the appearance likelihood of object category We let hi,j indicate whether object
category oi,j is actually present in location xi,j hi,j indicates absence Figure depicts the
above as a graphical model We use plate notation where the variable nodes inside a plate are
duplicated based on the counts depicted in the top-left corner of the plate
We instantiate the model as follows The spatial location of objects are parameterized as bounding
boxes xi,j cxi,j cyi,j cw
i,j ci,j where ci,j ci,j is the centroid and ci,j ci,j is the width and
height bounding boxes are extracted from object labels by tightly cropping the polygonal annotation Each component of xi,j is normalized with respect to the image to lie in We assume
are multinomial parameters and are Gaussian means and covariances over the
bounding box parameters Finally we assume gi,j is the output of a trained SVM applied to a gist
feature g?i,j We let parameterize the logistic function exp(??m,l gi,j
The parameters are learned offline by first
training SVMs for each object class on the set
of all labeled examples of object class and a
Mi
set of distractors We then fit logistic functions
to the positive and negative examples of each
i,j
i,j
class We learn the parameters and
online using the object labels corresponding to
gi,j
xi,j
the retrieval set These are learned by sim
ply counting the object class occurrences and
fitting Gaussians to the bounding boxes corre Figure Graphical model that integrates information about which objects are likely to be present in the
sponding to the object labels
image their appearance and their likely spatial lo
For the input image we wish to infer the latent cation The parameters for object appearance are
variables hi,j corresponding to a dense sam learned offline using positive and negative examples for
pling of all possible bounding box locations each object class The parameters for object presence
xi,j and object classes oi,j using the learned likelihood and spatial location are learned online
parameters and For this we from the retrieval set For all possible bounding boxes
compute the postierior distribution p(hi,j in the input image we wish to infer which indicates
m|oi,j xi,j gi,j which is whether an object is present or absent
proportional to the product of the three learned distributions for
The procedure outlined here allows for significant computational savings over naive application of
an object detector Without finding similar images that match the input scene configuration we
would need to apply an object detector densely across the entire image for all object categories In
contrast our model can constrain which object categories to look for and where More precisely
we only need to consider object categories with relatively high probability in the scene model and
bounding boxes within the range of the likely search locations These can be decided based on
thresholds Also note that the conditional independences implied by the graphical model allows us
to fit the parameters from the retrieval set and train the object detectors separately
Note that for tractability we assume Dirichlet and Normal-Inverse-Wishart conjugate prior distributions over and with hyperparemters and expected mean pseudocounts on the scale of the spatial observations degrees of freedom and sample covariance
Furthermore we assume a Bernoulli prior distribution over hi,j parameterized by We
hand-tuned the remaining parameters in the model For hi,j we assume the noninformative
distributions oi,j nif and each component of xi,j nif
Clustering Retrieval Set Images for Robustness to Mismatches
While many images in the retrieval set match the input image scene configuration and contents
there are also outliers Typically most of the labeled objects in the outlier images are not present
in the input image or in the set of correctly matched retrieval images In this section we describe
a process to organize the retrieval set images into consistent clusters based on the co-occurrence of
the object labels within the images The clusters will typically correspond to different scene types
and/or viewpoints The task is to then automatically choose the cluster of retrieval set images that
will best assist us in detecting objects in the input image
We augment the model of Section by assigning each image to a latent cluster si The cluster assignments are distributed according to the mixing weights We depict the model in Figure
Intuitively the model finds clusters using the object labels oi,j and their spatial location xi,j within
the retrieved set of images To automatically infer the number of clusters we use a Dirichlet Process
prior on the mixing weights Stick where Stick is the stick-breaking process of Grif4
si
Cluster counts
Input image
Mi
oi,j
Counts
hi,j
gi,j
xi,j
Cluster
Cluster
Cluster
Clusters
Cluster
Cluster
ch all
in
do
pi flo
ct or
ca tabre
bi le
la
bomp
ok
sc
ke rdeen
yb
oask
bo ousrd
okchae
sh ir
peflo lf
porsoor
st
er
pe
rs
be bon
ds ag
dde
fu ish
rn fo
ga itu ot
re
glden
heass
ad
plree
an
sk
gr flolocy
ee
ne er
be lanry
brries
us
sidindca
buew ow
ildalk
roing
pe sad
de ky
do str ree
or ian
ay
Figure Graphical model for clustering retrieval set images using their object labels We extend the
model of Figure to allow each image to be assigned to a latent cluster si which is drawn from mixing weights
We use a Dirichlet process prior to automatically infer the number of clusters We illustrate the clustering
process for the retrieval set corresponding to the input image in Histogram of the number of images
assigned to the five clusters with highest likelihood Montages of retrieval set images assigned to each
cluster along with their object labels colors show spatial extent shown in The likelihood of an object
category being present in a given cluster the top nine most likely objects are listed Spatial likelihoods for
the objects listed in Note that the montage cells are sorted in raster order
fiths Engen and McCloskey with concentration parameter In the Chinese restaurant
analogy the different clusters correspond to tables and the parameters for object presence and
spatial location are the dishes served at a given table An image along with its object labels
corresponds to a single customer that is seated at a table
We illustrate the clustering process for a retrieval set belonging to the input image in Figure
The five clusters with highest likelihood are visualized in the columns of Figure Figure
shows montages of retrieval images with highest likelihood that were assigned to each cluster The
total number of retrieval images that were assigned to each cluster are shown as a histogram in
Figure The number of images assigned to each cluster is proportional to the cluster mixing
weights Figure depicts the object labels that were provided for the images in Figure
with the colors showing the spatial extent of the object labels Notice that the images and labels
belonging to each cluster share approximately the same object categories and geometrical configuration Also the cluster that best matches the input image tends to have the highest number of
retrieval images assigned to it Figure shows the likelihood of objects that appear in the cluster
the nine objects with highest likelihood are shown This corresponds to in the model Figure
depicts the spatial distribution of the object centroid within the cluster The montage of nine cells
correspond to the nine objects listed in Figure sorted in raster order The spatial distributions
illustrate Notice that typically at least one cluster predicts well the objects contained in the input
image in addition to their location via the object likelihoods and spatial distributions
To learn and we use a Rao-Blackwellized Gibbs sampler to draw samples from the posterior
distribution over si given the object labels belonging to the set of retrieved images We ran the
Gibbs sampler for iterations Empirically we observed relatively fast convergence to a stable
solution Note that improved performance may be achieved with variational inference for Dirichlet
Processes We manually tuned all hyperparameters using a validation set of images with
concentration parameter and spatial location parameters and
across all bounding box parameters with the exception of for the horizontal
centroid location which reflects less certainty a priori about the horizontal location of objects We
used a symmetric Dirichlet hyperparameter with across all object categories
For final object detection we use the learned parameters and to infer hi,j Since si and hi,j
are latent random variables for the input image we perform hard EM by marginalizing over hi,j to
infer the best cluster s?i We then in turn fix s?i and infer hi,j as outlined in Section
Experimental Results
In this section we show qualitative and quantitative results for our model We use a subset of the
LabelMe dataset for our experiments discarding spurrious and nonlabeled images The dataset is
split into training and test sets The training set has images and annotations The
test set has images and annotations The test set comprises images of street scenes and
indoor office scenes To avoid overfitting we used street scene images that were photographed in
a different city from the images in the training set To overcome the diverse object labels provided
by users of LabelMe we used WordNet to resolve synonyms For object detection we extracted
bounding boxes per image For the final detection results we used non-maximal suppression
Example object detections from our system are shown in Figure Notice that our system
can find many different objects embedded in different scene type configurations When mistakes
are made the proposed object location typically makes sense within the scene In Figure we
compare against a baseline object detector using only appearance information and trained with a
linear kernel SVM. Thresholds for both detectors were set to yield a false positive rate per image
for each object category false positives per window Notice that our system produces
more detections and rejects objects that do not belong to the scene In Figure we show typical
failures of the system which usually occurs when the retrieval set is not correct or an input image is
outside of the training set
In Figure we show quantitative results for object detection for a number of object categories
We show ROC curves plotted on log-log axes for the local appearance detector the detector from
Section without clustering and the full system with clustering We scored detections using the
PASCAL VOC criteria where the outputs are sorted from most confident to least and the
ratio of intersection area to union area is computed between an output bounding box and groundtruth bounding box If the ratio exceeds then the output is deemed correct and the ground-truth
label is removed While this scoring criteria is good for some objects other objects are not well
represented by bounding boxes buildings and sky
Notice that the detectors that take into account context typically outperforms the detector using local
appearance only Also clustering does as well and in some cases outperforms no clustering Finally
the overall system sometimes performs worse for indoor scenes This is due to poor retrieval set
matching which causes a poor context model to be learned
Conclusion
We presented a framework for object detection in scenes based on transferring knowledge about
objects from a large labeled image database We have shown that a relatively simple parametric
sky
wall
wall
screen
sky
tree
building tree
road
road
car
car
car
car
keyboard
road
sky
keyboard
building
sky
wall
sky
building
sidewalk
sky
window
car
car
chair table
car
keyboard
road
tabletable
keyboard keyboard
keyboard
chair
keyboard
road
screen
building
table
table road
keyboard
window
window
person
person
person
car
wall
road
car
car
car
sidewalk
car
road
car
sky
sky
window screen screen
building building
chair
road
keyboard
Figure Input images Object detections from our system combining scene alignment with local
detection Object detections using appearance information only with an SVM. Notice that our system
detects more objects and rejects out-of-context objects More outputs from our system Notice that many
different object categories are detected across different scenes Failure cases for our system These often
occur when the retrieval set is incorrect
model trained on images loosely matching the spatial configuration of the input image is capable
of accurately inferring which objects are depicted in the input image along with their location We
showed that we can successfully detect a wide range of objects depicted in a variety of scene types
Acknowledgments
This work was supported by the National Science Foundation Grant No. the National
Geospatial-Intelligence Agency and the Office of Naval Research MURI
Grant

<<----------------------------------------------------------------------------------------------------------------------->>

title: 2478-multiple-instance-learning-via-disjunctive-programming-boosting.pdf

Multiple Instance Learning via
Disjunctive Programming Boosting
Stuart Andrews
Department of Computer Science
Brown University Providence RI
stu@cs.brown.edu
Thomas Hofmann
Department of Computer Science
Brown University Providence RI
th@cs.brown.edu
Abstract
Learning from ambiguous training data is highly relevant in many
applications We present a new learning algorithm for classification
problems where labels are associated with sets of pattern instead
of individual patterns This encompasses multiple instance learning as a special case Our approach is based on a generalization
of linear programming boosting and uses results from disjunctive
programming to generate successively stronger linear relaxations of
a discrete non-convex problem
Introduction
In many applications of machine learning it is inherently difficult or prohibitively
expensive to generate large amounts of labeled training data However it is often
considerably less challenging to provide weakly labeled data where labels or annotations are associated with sets of patterns or bags instead of individual patterns
These bags reflect a fundamental ambiguity about the correspondence of
patterns and
the associated label which can be expressed logically as a disjunction
of the form x?X is an example of class In plain English each labeled bag
contains at least one pattern but possibly more belonging to this class but the
identities of these patterns are unknown
A special case of particular relevance is known as multiple instance learning
In MIL labels are binary and the ambiguity is asymmetric in the sense that
bags with negative labels are always of size one Hence the label uncertainty is
restricted to members of positive bags There are many interesting problems where
training data of this kind arises quite naturally including drug activity prediction
content-based image indexing and text categorization The ambiguity
typically arises because of polymorphisms allowing multiple representations a
molecule which can be in different conformations or because of a part/whole am
biguity annotations may be associated with images or documents where they
should be attached to objects in an image or passages in a document Notice also
that there are two intertwined objectives the goal may be to learn a pattern-level
classifier from ambiguous training examples but sometimes one may be primarily
interested in classifying new bags without necessarily resolving the ambiguity for
individual patterns
A number of algorithms have been developed for MIL including special purpose
algorithms using axis-parallel rectangular hypotheses diverse density
neural networks and kernel methods In two versions of a maximummargin learning architecture for solving the multiple instance learning problem have
been presented Because of the combinatorial nature of the problem a simple
optimization heuristic was used in to learn discriminant functions In this paper
we take a more principled approach by carefully analyzing the nature of the resulting
optimization problem and by deriving a sequence of successively stronger relaxations
that can be used to compute lower and upper bounds on the objective Since it
turns out that exploiting sparseness is a crucial aspect we have focused on a linear
programming formulation by generalizing the LPBoost algorithm we call
the resulting method Disjunctive Programming Boosting DPBoost
Linear Programming Boosting
LPBoost is a linear programming approach to boosting which
aims at learning
ensemble classifiers of the form sgn with hk where
hk are the so-called base classifiers weak hypotheses
or features and are combination weights The ensemble margin of a labeled
example is defined as yF
Given a set of labeled training examples y1 xm ym LPBoost formulates the supervised learning problem using the 1-norm soft margin objective
min
Here controls the tradeoff between the Hinge loss and the L1 regularization
term Notice that this formulation remains meaningful even if all training examples
are just negative or just positive
Following the dual program of can be written as
max
ui
ui hk
ui
It is useful to take a closer look at the KKT complementary conditions
ui and
ui hk
Since the optimal values of the slack variables are implicitly determined by as
the first set of conditions states that ui whenever
Since ui can be interpreted as the misclassification cost this implies
that only instances with tight margin constraints may have non-vanishing
associated
Pm
costs The second set of conditions ensures that if ui hk
which states that
a weak hypothesis hk is never included in the ensemble if its
weighted score ui hk is strictly below the maximum score of So a typical
LPBoost solution may be sparse in two ways Only a small number of weak
hypothesis with may contribute to the ensemble and the solution may
only depend on a subset of the training data those instances with ui
LPBoost exploits the sparseness of the ensemble by incrementally selecting columns
from the simplex tableau and optimizing the smaller tableau This amounts to
finding in each round a hypothesis hk for which the constraint in is violated
adding it to the ensemble and re-optimizing the tableau with the selected columns
As a column selection heuristic the authors of propose to use the
magnitude of
the violation pick the weak hypothesis hk with maximal score ui hk
Disjunctive Programming Boosting
In order to deal with pattern ambiguity we employ the disjunctive programming
framework In the spirit of transductive large margin methods we
propose to estimate the parameters of the discriminant function in a way that
achieves a large margin for at least one of the patterns in each bag Applying this
principle we can compile the training data into a set of disjunctive constraints on
To that extend let us define the following polyhedra
Hi
hk
Then we can formulate the following disjunctive program
min
Hi
x?Xi
Notice that if then the constraint imposed by is highly non-convex
since it is defined via a union of halfspaces However for trivial bags with
the resulting constraints are the same as in Since we will handle these two
cases quite differently in the sequel let us introduce index sets I
and Xj
A suitable way to define a relaxation to this non-convex optimization problem is
to replace the disjunctive set in by its convex hull As shown in a
whole hierarchy of such relaxations can be built using the fundamental fact that
cl-conv(A cl-conv(B cl-conv(A where cl-conv(A denotes the closure of
the convex hull of the limiting points of A. This means a tighter convex relaxation
is obtained if we intersect as many sets as possible before taking their convex hull
Since repeated intersections of disjunctive sets with more than one element each
leads to an combinatorial blow-up in the number of constraints we propose to intersect every ambiguous disjunctive constraint with every non-ambiguous constraint
as well as with Q. This is also called a parallel reduction step It results in the
following convex relaxation of the constraints in
Hi
cl-conv
Hj
i?I
x?Xi
j?J
where we have abused the notation slightly and identified Xj for bags with
one pattern The rationale in using this relaxation is that the resulting convex
optimization problem is tractable and may provide a reasonably accurate approximation to the original disjunctive program which can be further strengthened by
using it in combination with branch-and-bound search
There is a lift-and-project representation of the convex hulls in one
can characterize the feasible set as a projection of a higher dimensional polyhedron
which can be explicitly characterized
Proposition Assume a set of non-empty
linear constraints Hi Ai
is given Then cl-conv Hi if and only if there exist and
such that
zj
Aj bj
Proof
Let us pause here briefly and recapitulate what we have achieved so far We have
derived a LP relaxation of the original disjunctive program for boosting with ambiguity This relaxation was obtained by a linearization of the original non-convex
constraints Furthermore we have demonstrated how this relaxation can be improved using parallel reduction steps
Applying this linearization to every convex hull in individually notice that
one needs to introduce duplicates of the parameters and slack
variables
for every In addition to the constraints kx ix jx ix and x?Xi ix
the relevant constraint set for ambiguous bag for I of the resulting LP can
be written as
kx hk ix ix
yj
kx hk jx ix
I
x?Xi
kx
jx
x?Xi
The first margin constraint in is the one associated with the specific pattern
while the second set of margin constraints in stems from the parallel
reduction performed with unambiguous bags One can calculate the dual LP of
the above relaxation the derivation of which can be found in the appendix The
resulting program has a more complicated bound structure on the u-variables and
the following crucial constraints involving the data
uxi hk
yj uxj hk ik
ik
j?J
i?I
However the size of the resulting problem is significant As a result of linearization
and parallel reductions the number of parameters in the primal LP is now O(q
where denote the number of patterns in ambiguous and unambiguous
bags compared to O(n of the standard LPBoost The number of constraints
variables in the dual has also been inflated significantly from to
where is the number of ambiguous bags
In order to maintain the spirit of LPBoost in dealing efficiently with a large-scale
linear program we propose to maintain the column selection scheme of selecting
one or more kx in every round Notice that the column
selection can not proceed
independently because of the equality constraints x?Xi kx for all in
particular kx implies so that kz for at least some for each
I. We hence propose to simultaneously add all columns I
involving the same weak hypothesis and to prune those back after each boosting
round in order to exploit the expected sparseness of the solution In order to select
a feature hk we compute the following score
max uxi hk
yj uxj hk
j?J
Notice that due to the block structure of the tableau working with a reduced set of
columns also eliminates a large number of inequalities rows However the large
set of inequalities for the parallel reductions is still prohibitive
In order to address this problem we propose to perform incremental row selection
in an outer loop Once we have converged to a column basis for the current relaxed
LP we add a subset of rows corresponding to the most useful parallel reductions
One can use the magnitude of the margin violation as a heuristic to perform this
row selection Hence we propose to use the following score
ix yj
kx hk where I
This means that for current values of the duplicated ensemble weights kx one
selects the parallel reduction margin constraint associated with ambiguous pattern
and unambiguous pattern that is violated most strongly
Although the margin constraints imposed by unambiguous training instances
yj are redundant after we performed the parallel reduction step in
we add them to the problem because this will give us a better starting point with
respect to the row selection process and may lead to a sparser solution We hence
add the following constraints to the primal
yj
hk
which will introduce additional dual variables uj J. Notice that in the worst
case where all inequalities imposed by ambiguous training instances are vacuous
this will make sure that one recovers the standard LPBoost formulation on the
unambiguous examples One can then think of the row generation process as a way
of deriving useful information from ambiguous examples This information takes
the form of linear inequalities in the high dimensional representation of the convex
hull and will sequentially reduce the version space the set of feasible pairs
Algorithm DPBoost Algorithm
initialize I ux
I uj
uj
uxi
repeat
repeat
column selection select hk with maximal
hk
I
solve LP
until max
row selection select a set of pairs with maximal
uxj
solve LP
until max
90
70
90
70
90
70
Figure Left Normalized intensity plot used to generate synthetic data sets
Right Performance relative to the degree of label ambiguity Mean and standard
deviation of the pattern-level classification accuracy plotted versus for perfectknowledge solid perfect-selector dotted DPboost dashed and naive dashdot algorithms The three plots correspond to data sets of size
Experiments
We generated a set of synthetic weakly labeled data sets to evaluate DPboost on a
small scale These were multiple-instance data sets where the label uncertainty was
asymmetric the only ambiguous bags were positive More specifically we
generated instances sampled uniformly at random from the white
and black regions of Figure leaving the intermediate gray
area as a separating margin The degree of ambiguity was controlled by generating
ambiguous bags of size Poisson having only one positive and negative
patterns To control data set size we generated a pre-specified number of ambiguous
bags and the same number of singleton unambiguous bags
As a proof of concept benchmark we compared the classification perfomance of
DPboost with two other LPboost variants perfect-knowledge perfect-selector and
naive algorithms All variants use LPboost as their base algorithm and have slightly
different preprocessing steps to accomodate the MIL data sets The first corresponds
to the supervised LPboost algorithm the true pattern-level labels are used
Since this algorithm does not have to deal with ambiguity it will perform better
than DPboost The second uses the true pattern-level labels to prune the negative
examples from ambiguous bags and solves the smaller supervised problem with
LPboost as above This algorithm provides an interesting benchmark since its
performance is the best we can hope for from DPboost At the other extreme the
third variant assumes the ambiguous pattern labels are equal to their respective
bag labels For all algorithms we used thresholded RBF-like features
Figure shows the discriminant boundary black line learned by each of the four
algorithms for a data set generated with and having ambiguous bags
no ambig 71 no total The ambiguous patterns are
marked by unambiguous ones and the background is shaded to indicate
the value of the ensemble clamped to It is clear from the shading that
the ensemble has a small number of active features for DPboost perfect-selector
and perfect-knowledge algorithms For each classifier we report the pattern-level
classification accuracy for a uniform grid of points The sparsity of the dual
variables was also verified less than percent of the dual variables and reductions
were active
We ran 5-fold cross-validation on the synthetic data sets for and for
data sets having Figure right side shows the mean pattern-level
classification accuracy with error bars showing one standard deviation as a function
Figure Discriminant boundaries learned by naive accuracy DPboost
perfect-selector and perfect-knowledge algorithms
of the parameter
Conclusion
We have presented a new learning algorithm for classification problems where labels
are associated with sets of pattern instead of individual patterns Using synthetic
data the expected behaviour of the algorithm has been demonstrated Our current
implementation could not handle large data sets and so improvements followed by
a large-scale validation and comparison to other algorithms using benchmark MIL
data sets will follow
Acknowledgments
David Musicant for making his CPLEX MEX interface available online Also to
Ioannis Tsochantaridis and Keith Hall for useful discussion and advice This work
was sponsored by an NSF-ITR grant award number

<<----------------------------------------------------------------------------------------------------------------------->>

title: 3278-spatial-latent-dirichlet-allocation.pdf

Spatial Latent Dirichlet Allocation
Xiaogang Wang and Eric Grimson
Computer Science and Artificial Intelligence Lab
Massachusetts Institute of Technology Cambridge MA USA
xgwang@csail.mit.edu welg@csail.mit.edu
Abstract
In recent years the language model Latent Dirichlet Allocation which
clusters co-occurring words into topics has been widely applied in the computer
vision field However many of these applications have difficulty with modeling
the spatial and temporal structure among visual words since LDA assumes that a
document is a bag-of-words It is also critical to properly design words and
documents when using a language model to solve vision problems In this paper we propose a topic model Spatial Latent Dirichlet Allocation SLDA which
better encodes spatial structures among visual words that are essential for solving
many vision problems The spatial information is not encoded in the values of
visual words but in the design of documents Instead of knowing the partition of
words into documents a priori the word-document assignment becomes a random
hidden variable in SLDA There is a generative procedure where knowledge of
spatial structure can be flexibly added as a prior grouping visual words which are
close in space into the same document We use SLDA to discover objects from a
collection of images and show it achieves better performance than LDA.
Introduction
Latent Dirichlet Allocation LDA is a language model which clusters co-occurring words into
topics In recent years LDA has been widely used to solve computer vision problems For example
LDA was used to discover objects from a collection of images and to classify images into
different scene categories employed LDA to classify human actions In visual surveillance
LDA was used to model atomic activities and interactions in a crowded scene In these applications LDA clustered low-level visual words which were image patches spatial and temporal
interest points or moving pixels into topics with semantic meanings which corresponded to objects
parts of objects human actions or atomic activities utilizing their co-occurrence information
Even with these promising achievements however directly borrowing a language model to solve
vision problems has some difficulties First LDA assumes that a document is a bag of words
such that spatial and temporal structures among visual words which are meaningless in a language
model but important in many computer vision problems are ignored Second users need to define
the meaning of documents in vision problems The design of documents often implies some
assumptions on vision problems For example in order to cluster image patches which are treated
as words into classes of objects researchers treated images as documents This assumes that
if two types of patches are from the same object class they often appear in the same images This
assumption is reasonable but not strong enough As an example shown in Figure even though
sky is far from vehicles if they often exist in the same images in some data set they would be
clustered into the same topic by LDA. Furthermore since in this image most of the patches are sky
and building a patch on a vehicle is likely to be labeled as building or sky as well These problems
could be solved if the document of a patch such as the yellow patch in Figure only includes other
Figure There will be some problems text if the whole image is treated as one document
when using LDA to discover classes of objects
patches falling within its neighborhood marked by the red dashed window in Figure instead of
the whole image So a better assumption is that if two types of image patches are from the same
object class they are not only often in the same images but also close in space We expect to utilize
spatial information in a flexible way when designing documents for solving vision problems
In this paper we propose a Spatial Latent Dirichlet Allocation SLDA model which encodes the
spatial structure among visual words It clusters visual words an eye patch and a nose patch
which often occur in the same images and are close in space into one topic face This is
a more proper assumption for solving many vision problems when images often contain several
objects It is also easy for SLDA to model activities and human actions by encoding temporal
information However the spatial or temporal information is not encoded in the values of visual
words but in the design of documents LDA and its extensions such as the author-topic model
the dynamic topic model and the correlated topic model all assume that the partition
of words into documents is known a priori A key difference of SLDA is that the word-document
assignment becomes a hidden random variable There is a generative procedure to assign words to
documents When visual words are close in space or time they have a high probability to be grouped
into the same document Some approaches such as could also capture some spatial
structures among visual words assumed that the spatial distribution of an object class could
be modeled as Gaussian and the number of objects in the image was known Both and first
roughly segmented images using graph cuts and added spatial constraint using these segments
modeled the spatial dependency among image patches as Markov random fields
As an example application we use the SLDA model to discover objects from a collection of images
As shown in Figure there are different classes of objects such as cows cars faces grasses
sky bicycles etc in the image set And an image usually contains several objects of different
classes The goal is to segment objects from images and at the same time to label these segments
as different object classes in an unsupervised way It integrates object segmentation and recognition
In our approach images are divided into local patches A local descriptor is computed for each
image patch and quantized into a visual word Using topic models the visual words are clustered
into topics which correspond to object classes Thus an image patch can be labeled as one of the
object classes Our work is related to which used LDA to cluster image patches As shown in
Figure SLDA achieves much better performance than LDA. We will compare more results of
LDA and SLDA in the experimental section
Computation of Visual Words
To obtain the local descriptors images are convolved with the filter bank proposed in which is
a combination of Gaussians Laplacian of Gaussians and first order derivatives of Gaussians
and was shown to have good performance for object categorization Instead of only computing
visual words at interest points as in we divide an image into local patches on a grid and densely
sample a local descriptor for each patch A codebook of size is created by clustering all the
local descriptors in the image set using K-means Each local patch is quantized into a visual word
according to the codebook In the next step these visual words image patches will be further
clustered into classes of objects We will compare two clustering methods LDA and SLDA
Figure Given a collection of images as shown in the first row which are selected from the MSRC
image dataset the goal is to segment images into objects and cluster these objects into different
classes The second row uses manual segmentation and labeling as ground truth The third row is
the LDA result and the fourth row is the SLDA result Under the same labeling approach image
patches marked in the same color are in one object cluster but the meaning of colors changes across
different labeling methods
LDA
When LDA is used to solve our problem we treat local patches of images as words and the whole
image as a document The graphical model of LDA is shown in Figure There are documents images in the corpus Each document has Nj words image patches wji is the observed
value of word in document All the words in the corpus will be clustered into topics classes
of objects Each topic is modeled as a multinomial distribution over the codebook and are
Dirichlet prior hyperparameters and zji are hidden variables to be inferred The generative
process of LDA is
For a topic a multinomial parameter is sampled from Dirichlet prior
For a document a multinomial parameter over the topics is sampled from Dirichlet
prior Dir
For a word in document a topic label zji is sampled from discrete distribution zji
Discrete
The value wji of word in document is sampled from the discrete distribution of topic
zji wji Discrete zji
zji can be sampled through a Gibbs sampling procedure which integrates out
p(zji
where
ji
ji
ji wji
wji
ji
and
ji
ji
is the number of words in the corpus with value assigned to topic excluding word
in document and ji is the number of words in document assigned to topic excluding
word in document Eq is the product of two ratios the probability of word wji under topic
and the probability of topic in document So LDA clusters the visual words often co-occurring
in the same images into one object class
As shown by some examples in Figure more results in the experimental section there are
two problems in using LDA for object segmentation and recognition The segmentation result is
Figure Graphical model of LDA and SLDA See text for details
noisy since spatial information is not considered Although LDA assumes that one image contains
multiple topics from experimental results we observe that the patches in the same image are likely
to have the same labels Since the whole image is treated as one document if one object class
car in Figure is dominant in the image the second ratio in Eq will lead to a large bias towards
the car class and thus the patches of street are also likely to be labeled as car This problem could
be solved if a local patch only considers its neighboring patches as being in the same document
SLDA
We assume that if visual words are from the same class of objects they not only often co-occur in the
same images but also are close in space So we try to group image patches which are close in space
into the same documents One straightforward way is to divide the image into regions as shown in
Figure Each region is treated as a document instead of the whole image However since these
regions are not overlapped some patches such as A red patch and cyan patch in Figure
even though very close in space are assigned to different documents In Figure patch A on
the cow is likely to be labeled as grass since most other patches in its document are grass To solve
this problem we may put many overlapped regions each of which is a document on the images as
shown in Figure If a patch is inside a region it could belong to that document Any two
patches whose distance is smaller than the region size could belong to the same document if the
regions are placed densely enough We use the word could because each local patch is covered
by several regions so we have to decide to which document it belongs Different from the LDA
model in which the word-document relationship is known a priori we need a generative procedure
assigning words to documents If two patches are closer in space they have a higher probability
to be assigned to the same document since there are more regions covering both of them Actually
we can go even further As shown in Figure each document can be represented by a point
marked by magenta circle in the image assuming its region covers the whole image If an image
patch is close to a document it has a high probability to be assigned to that document
The graphical model is shown in Figure In SLDA there are documents and words in the
corpus A hidden variable di indicates which document word is assigned to For each document
there is a hyperparameter cdj gjd xdj yjd known a priori gjd is the index of the image where
document is placed and xdj yjd is the location of the document For a word in addition to the
observed word value its location and image index gi are also observed and stored in
variable ci gi The generative procedure of SLDA is
For a topic a multinomial parameter is sampled from Dirichlet prior
Figure There are several ways to add spatial information among image patches when designing
documents Divide the image into regions without overlapping Each region marked by a
dashed window corresponds to a document Image patches inside the region are assigned to the
corresponding document densely put overlapped regions over images One image patch is
covered by multiple regions Each document is associated with a point marked in magenta
color These points are densely placed over the image If a image patch is close to a document it
has a high probability to be assigned to that document
For a document a multinomial parameter over the topics is sampled from Dirichlet
prior Dir
For a word image patch a random variable di is sampled from prior p(di indicating
to which document word is assigned We choose p(di as a uniform prior
The image index and location of word is sampled from distribution p(ci cddi We may
choose this as a Gaussian kernel
xddi yddi
gdd gi exp
p((gi gdi xdi ydi
p(ci cddi if the word and the document are not in the same image
The topic label zi of word is sampled from the discrete distribution of document di
zi Discrete di
The value of word is sampled from the discrete distribution of topic zi
Discrete zi
Gibbs Sampling
zi and di can be sampled through a Gibbs sampling procedure integrating out
the conditional distribution of zi given di is the same as in LDA.
p(zi di
iw
ik
where
and
ik
In SLDA
is the number of words in the corpus with value assigned to topic excluding word
and
is the number of words in document assigned to topic excluding word This is
easy to understand since if the word-document assignment is fixed SLDA is the same as LDA.
In addition we also need to sample di from the conditional distribution given zi
di zi ci cdj
di ci cdj
zi di
zi
di
zi
is obtained by integrating out
p(zj ji
di
nk
nk
We choose di as a uniform prior and ci cdj as a Gaussian kernel Thus the conditional distribution of di is
di j|zi z?i d?i ci cdj0
gjd gi
xdj yjd
n?i,k
Word is likely to be assigned to document if they are in the same image close in space and word
has the same topic label as other words in document In real applications we only care about the
distribution of zi while dj can be marginalized by simply ignoring its samples From Eq and
we observed that a word tends to have the same topic label as other words in its document and words
closer in space are more likely to be assigned to the same documents So essentially under SLDA a
word tends to be labeled as the same topic as other words close to it This satisfies our assumption
that visual words from the same object class are closer in space
Since we densely place many documents over one image during Gibbs sampling some documents
are only assigned a few words and the distributions cannot be well estimated To solve this problem
we replicate each image patch to get many particles These particles have the same word value and
location but can be assigned to different documents and have different labels Thus each document
will have enough samples of words to estimate the distributions
Discussion
SLDA is a flexible model intended to encode spatial structure among image patches and design
documents If there is only one document placed over one image SLDA simply reduces to LDA.
If p(ci cdj is an uniform distribution inside a local region SLDA implements the scheme described
in Figure If these local regions are not overlapped it is the case of Figure There are
also other possible ways to add spatial information by choosing different spatial priors p(ci cdj In
SLDA the spatial information is used when designing documents However the object class model
simply a multinomial distribution over the codebook has no spatial structure So the objects of
a class could be in any shape and anywhere in the images as long as they smoothly distribute in
space By simply adding a time stamp to ci and cdj it is easy for SLDA to encode temporal structure
among visual words So SLDA also can be applied to human action and activity analysis
Experiments
We test LDA and SLDA on the MSRC image dataset with images Our codebook size is
and the topic number is In Figure we show some examples of results using LDA and
SLDA Colors are used indicate different topics The results of LDA are noisy and within one image
most of the patches are labeled as one topic SLDA achieves much better results than LDA. The
results are smoother and objects are well segmented The detection rate and false alarm rate of four
classes cows cars faces and bicycles are shown in Table They are counted in pixels We use the
manual segmentation and labeling in as ground truth
The two models are also tested on a tiger video sequence with frames We treat all the frames
in the sequence as an image collection and ignore their temporal order Figure shows their results
on two sampled frames Please see the result of the whole video sequence from our website
Using LDA usually there are one or two dominant topics distributed like noise in a frame Topics
change as the video background changes LDA cannot segment out any objects SLDA clusters
image patches into tigers rock water and grass If we choose the topic of tiger as shown in the last
row of Figure all the tigers in the video can be segmented out
Conclusion
We propose a novel Spatial Latent Dirichlet Allocation model which clusters co-occurring and spatially neighboring visual words into the same topic Instead of knowing word-document assignment
a priori SLDA has a generative procedure partitioning visual words which are close in space into
the same documents It is also easy to extend SLDA to including temporal information
Figure Discovering objects from a video sequence The first column shows two frames in the
video sequence In the second column we label the patches in the two frames as different topics
using LDA. The thrid column plots the topic labels using SLDA The red color indicates the topic
of tigers In the fourth column we segment tigers out by choosing the topic marked in red
Table Detection(D rate and False Alarm rate of LDA and SLDA on the MSRC data set
LDA(D
SLDA(D
LDA(FA
SLDA(FA
cows
cars
faces
bicycles
Acknowledgement
The authors wish to acknowledge DSO National Laboratory of Singapore for partially supporting
this research

<<----------------------------------------------------------------------------------------------------------------------->>

title: 3063-detecting-humans-via-their-pose.pdf

Detecting Humans via Their Pose
Alessandro Bissacco
Computer Science Department
University of California Los Angeles
Los Angeles CA
bissacco@cs.ucla.edu
Ming-Hsuan Yang
Honda Research Institute
California Street
Mountain View CA
mhyang@ieee.org
Stefano Soatto
Computer Science Department
University of California Los Angeles
Los Angeles CA
soatto@cs.ucla.edu
Abstract
We consider the problem of detecting humans and classifying their pose from a
single image Specifically our goal is to devise a statistical model that simultaneously answers two questions is there a human in the image and if so what
is a low-dimensional representation of her pose We investigate models that can
be learned in an unsupervised manner on unlabeled images of human poses and
provide information that can be used to match the pose of a new image to the ones
present in the training set Starting from a set of descriptors recently proposed for
human detection we apply the Latent Dirichlet Allocation framework to model
the statistics of these features and use the resulting model to answer the above
questions We show how our model can efficiently describe the space of images
of humans with their pose by providing an effective representation of poses for
tasks such as classification and matching while performing remarkably well in
human/non human decision problems thus enabling its use for human detection
We validate the model with extensive quantitative experiments and comparisons
with other approaches on human detection and pose matching
Introduction
Human detection and localization from a single image is an active area of research that has witnessed
a surge of interest in recent years 18 Simply put given an image we want to devise an
automatic procedure that locates the regions that contain human bodies in arbitrary pose This is
hard because of the wide variability that images of humans exhibit Given that it is impractical
to explicitly model nuisance factors such as clothing lighting conditions viewpoint body pose
partial and/or self-occlusions one can learn a descriptive model of human/non human statistics
The problem then reduces to a binary classification task for which we can directly apply general
statistical learning techniques Consequently the main focus of research on human detection so far
has been on deriving a suitable representation 18 one that is most insensitive to typical
appearance variations so that it provides good features to a standard classifier
Recently local descriptors based on histograms of gradient orientations such as have proven
to be particularly successful for human detection tasks The main idea is to use distributions of
gradient orientations in order to be insensitve to color brightness and contrast changes and to some
extent local deformations However to account for more macroscopic variations due for example
to changes in pose a more complex statistical model is warranted We show how a special class of
hierarchical Bayesian processes can be used as generative models for these features and applied to
the problem of detection and pose classification
This work can be interpreted as an attempt to bridge the gap between the two related problems of
human detection and pose estimation in the literature In human detection since a simple yes/no
answer is required there is no need to introduce a complex model with latent variables associated
to physical quantities In pose estimation on the other hand the goal is to infer these quantities and
therefore a full generative model is a natural approach Between these extremes lies our approach
We estimate a probabilistic model with a set of latent variables which do not necessarily admit a
direct interpretation in terms of configurations of objects in the image However these quantities are
instrumental to both human detection and the pose classification problem
The main difficulty is in the representation of the pose information Humans are highly articulated
objects with many degrees of freedom which makes defining pose classes a remarkably difficult
problem Even with manual labeling how does one judge the distance between two poses or cluster
them In such situations we believe that the only avenue is an unsupervised method We propose an approach which allows for unsupervised clustering of images of humans and provides a
low dimensional representation encoding essential information on their pose The chief difference
with standard clustering or dimensionality reduction techniques is that we derive a full probabilistic
framework which provides principled ways to combine and compare different models as required
for tasks such as human detection pose classification and matching
Context and Motivation
The literature on human detection and pose estimation is too broad for us to review here So we focus
on the case of a single image neglecting scenarios where temporal information or a background
model are available and effective algorithms based on silhouettes or motion patterns
can be applied
Detecting humans and estimating poses from single images is a fundamental problem with a range
of sensible applications such as image retrieval and understanding It makes sense to tackle this
problem as we know humans are capable of telling the locations and poses of people from the visual
information contained in photographs The question is how to represent such information and the
answer we give constitutes the main novelty of this work
Numerous representation schemes have been exploited for human detection Haar wavelets
edges gradient orientations gradients and second derivatives and regions from
image segmentation With these representations algorithms have been applied for the detection
process such as template matching support vector machine Adaboost and grouping
to name a few Most approaches to pose estimation are based on body part detectors using
either edge shape color and texture cues or learned from training data The optimal
configuration of the part assembly is then computed using dynamic programming as first introduced
in or by performing inference on a generative probabilistic model using either Data Driven
Markov Chain Monte Carlo Belief Propagation or its non-Gaussian extensions
These works focus on only one of the two problems either detection or pose estimation Our approach is different in that our goal is to extract more information than a simple yes/no answer while
at the same time not reaching the full level of detail of determining the precise location of all body
parts Thus we want to simultaneously perform detection and pose classification and we want to
do it in an unsupervised manner In this aspect our work is related to the constellation models of
Weber although we do not have an explicit decomposition of the object in parts
We start from the representation based on gradient histograms recently applied to human detection with excellent results and derive a probabilistic model for it We show that with this model
one can successfully detect humans and classify their poses The statistical tools used in this work
Latent Dirichlet Allocation LDA and related algorithms have been introduced in the
text analysis context and recently applied to the problem of recognition of object and action classes
22 Contrary to most approaches all but where the image is treated as a bag of
features and all spatial information is lost we encode the location and orientation of edges in the
basic elements words so that this essential information is explicitly represented by the model
A Probabilistic Model for Gradient Orientations
We first describe the features that we use as the basic representations of images and then propose a
probabilistic model with its application to the feature generation process
Histogram of Oriented Gradients
Local descriptors based on gradient orientations are one of the most successful representations for
image-based detection and matching as was firstly demonstrated by Lowe in Among the various approaches within this class the best performer for humans appears to be This descriptor is
obtained by computing weighted histograms of gradient orientations over a grid of spatial neighborhoods cells which are then grouped in overlapping regions blocks and normalized for brightness
and contrast changes
Assume that we are given a patch of 64 pixels we divide the patch into cells of
pixels and for each cell a gradient orientation histogram is computed The histogram represents
a quantization in bins of gradient orientations in the range Each pixel contributes
to the neighboring bins both in orientation and space by an amount proportional to the gradient
magnitude and linearly decreasing with the distance from the bin center These cells are grouped in
blocks and the contribution of each pixel is also weighted by a Gaussian kernel with
centered in the block Finally the vectors of cell histograms within one block are normalized in
norm
The final descriptor is a collection of histograms from overlapping
blocks each cell shared by blocks
The main characteristic of such a representation is robustness to local deformations illumination
changes and to a limited extent viewpoint and pose changes due to coarsening of the histograms
In order to handle the larger variations typical of human body images we need to complement this
representation with a model We propose a probabilistic model that can accurately describe the
generation process of these features
Latent Dirichlet Allocation
Latent Dirichlet Allocation LDA is a hierachical model for sparse discrete mixture distributions where the basic elements words are sampled from a mixture of component distributions and
each component defines a discrete distribuition over the set of words
We are given a collection of documents where words the basic units of our data take values in
a dictionary of unique elements A document w1 w2 wW
PW
is a collection of word counts wj wj The standard LDA model does not include the
distribution of so it can be omitted in what follows The corpus w1 w2 wM is a
collection of documents
The LDA model introduces a set of latent variables called topics Each word in the document
is assumed to be generated by one of the topics Under this model the generative process for each
document in the corpus is as follows
Choose Dirichlet
For each word in the dictionary choose a word count wj p(wj
where the word counts wj are drawn from a discrete distribution conditioned on the topic proportions p(wj Recently several variants to this model have been developed notably
the Multinomial PCA where the discrete distributions are replaced by multinomials and the
Gamma-Poisson process where the number of words from each component are independent
Gamma samples and p(wj is Poisson The hyperparameter RK
represents the prior on
the topic distribution RK
are the parameters of the
are the topic proportions and
word distributions conditioned on topics In the context of this work words correspond to oriented
gradients and documents as well as corpus correspond to images and a set of images respectively
The topic derived by the LDA model is the pose of interest in this work Here we can safely assume
that the topic distributions are deterministic parameters later for the purpose of inference we will
treat them as random variables and assign them a Dirichlet prior Dirichlet where
denotes the k-th column of
Then the likelihood of a document is
p(wn
where documents are represented as a continuous mixture distribution The advantage over standard
mixture of discrete distributions is that we allow each document to be generated by more than
one topic
A Bayesian Model for Gradient Orientation Histograms
Now we can show how the described two-level Bayesian process finds a natural application in modeling the spatial distribution of gradient orientations Here we consider the histogram of oriented
gradients as the basic feature from which we build our generative model but let us point out
that the framework we introduce is more general and can be applied to any descriptor based on histograms1 In this histogram descriptor we have that each bin represents the intensity of the gradient
at a particular location defined by a range of orientations and a local neighborhood cell Thus the
bin height denotes the strength and number of the edges in the cell
The first thing to notice in deriving a generative models for this class of features is that since they
represent a weighted histogram they have non-negative elements Thus a proper generative model
for these descriptors imposes non-negativity constraints As we will see in the experiments a linear approach such as Non-negative Matrix Factorization leads to extremely poor performance
probably due to the high curvature of the space On the opposite end representing the nonlinearity
of the space with a set of samples by Vector Quantization is feasible only using a large number of
samples which is against our goal of deriving an economical representation of the pose
We propose using the Latent Dirichlet Allocation model to represent the statistics of the gradient
orientation features In order to do so we need to quantize feature values While not investigated
in the original paper quantization is common practice for similar histogram-based descriptors
such as We tested the effect of quantization on the performance of the human detector based
on Histogram of Oriented Gradient descriptors and linear Support Vector Machines described in
As evident in Figure with or more discrete levels we practically obtain the same performance
as with the original continuous descriptors Thus in what follows we can safely assume that the
basic features are collections of small integers the histogram bin counts wj
Thus if we quantize histogram bins and assign a unique word to each bin we obtain a representation for which we can directly apply the LDA framework Analogous to document analysis an
orientation histogram computed on an image patch is a document represented as a bag of words
w1 wW where the word counts wj are the bin heights We assume that such a histogram
is generated by a mixture of basic components topics where each topic induces a discrete distribution on bins representing a typical configuration of edges common to a class of elements
in the dataset By summing the contributions from each topic we obtain the total count wj for each
bin distributed according to p(wj
The main property of such feature formation process desirable for our applications is the fact that
topics combine additively That is the same bin may have contributions from multiple topics and
this models the fact that the bin height is the count of edges in a neighborhood which may include
parts generated by different components Finally let us point out that by assigning a unique word
to each bin we model spatial information encoded in the word identity whereas most previous
approaches using similar probabilistic models for object class recognition did not exploit
this kind of information
Probabilistic Detection and Pose Estimation
The first application of our approach is human detection Notice that our main goal is to develop a
model to represent the statistics of images for human pose classification We use the human detection
problem as a convenient testbed for validating the goodness of our representation since for this
application large labelled datasets and efficient algorithms are available By no means we intend
to compete with state-of-the-art discriminative approaches for human detection alone which are
optimized to represent the decision boundary and thus are supposed to perform better than generative
approaches in binary classification tasks However if the generative model is good at capturing the
statistics of human images we expect it to perform well also in discriminating humans from the
background
In human detection given a set of positive and negative examples and a previously unseen image
Inew we are asked to choose between two hypotheses either it contains a human or it is a background image The first step is to compute the gradient histogram representation for the test
and training images Then we learn a model for humans and background images and use a threshold
Notice that due to the particular normalization procedure applied the histogram features we consider here
do not have unit norm fact they are zero on uniform regions
on the likelihood ratio2 for detection
w(Inew Human
w(Inew Background
For the the LDA and related models the likelihoods are computed as in
where are model parameters and can be learned from data In practice we can assume
is known and compute an estimate of from the training corpus In doing so we can choose
from two main inference algorithms mean field or variational inference and Gibbs sampling
Mean field algorithms provide a lower bound on the likelihood while Gibbs sampling gives
statistics based on a sequential sampling scheme As shown in Figure in our experiments Gibbs
sampling exhibited superior performance over mean field in terms of classification accuracy We
have experimented with two variations a direct method and Rao-Blackwellised sampling
for details Both methods gave similar performance here we report the results obtained using the
direct method whose main iteration is as follows
For each document wi,W
First sample and then sample vj Multinomial(?j wi,j
For each topic
Sample Dirichlet v.k
In pose classification we start from a set of unlabeled training examples of human poses and learn
the topic distribution This defines a probabilistic mapping to the topic variables which can be
seen as an economical representation encoding essential information of the pose That is from a
image Inew we estimate the topic proportions
Inew as
new p(?|w(Inew
Pose information can be recovered by matching the new image Inew to an image I in the training set For matching ideally we would like to compute the matching score as Sopt Inew
w(Inew the posterior probability of the test image Inew given the training image I and the model However this would be computationally expensive as for each pair I Inew
it requires computing an expectation of the form thus we opted for a suboptimal solution For
each training document I in the learning step we compute the posterior topic proportions
as
in Then the matching score between Inew and I is given by the dot product between the two
and
new
vectors
new
Inew
The computation of this score requires only a dot product between low dimensional unit vectors
so our approach represent an efficient method for matching and clustering poses in large datasets
Experiments
We first tested the efficacy of our model for the human detection task We used the dataset provided
by consisting of 64 images of pedestrians in various configurations and images
of outdoor scenes not containing humans We collected negative examples by random sampling
patches from each of the first non-human images These together with positive examples
and their left-right reflections constituted our first training set We used the learned model to classify
remaining positive and on patches randomly extracted from the residual background
images
We first computed the histograms of oriented gradients from the image patches following the procedure outlined in Section These feature are quantized so that they can be represented by our
discrete stochastic model
We tested the effect of different quantization levels on the performances of the boosted SVM classifier a initial training on the provided dataset is followed by a boosting round where the trained
classifier is applied to the background images to find false positive these hard examples are then
added to for a second training of the classifier As Figure shows the effect of quantization is significant only if we use less than bits Therefore we chose to discretize the features to quantization
levels
Ideally we would like to use the posterior ratio Human|Inew Background|Inew However
notice that is equal to if we assume equal priors Human Background
Given the number of topics and the prior hyperparameters we learned topic distributions
using either Gibbs sampling or Mean Field We tested both Gamma
and topic proportions
and Dirichlet distributions for topic priors obtaining best results with the multinomial model
with scalar priors a in these experiments a and
The number of topics is an important parameter that should be carefully chosen based on considerations on modeling power and complexity With a higher number of topics we can more accurately
fit the data which can be measured by the increase in the likelihood of the training set This does
not come for free we have a larger number of parameters and an increased computational cost for
learning Eventually an excessive topic number causes overfitting which can be measured as the
likelihood in the test dataset decreases For the INRIA data experimental evaluations suggested that
a good tradeoff is obtained with 24
We learned two models one for positive and one for negative examples For learning we run the
Gibbs sampling algorithm described in Section for a total number of samples per document
including samples to compute the likelihoods We also trained the model using the Mean
Field approximation but as we can see in Figures and the results using Gibbs sampling are
better For details on the implementation we refer to We then obtain a detector by computing
the likelihood ratio and comparing it with a threshold
In Figure we show the performances of our detector on the INRIA dataset where for the sake of
comparison with other approaches boosting is not performed We show the results for
Linear SVM classifier Trained as described using the SVMLight software package
Vector Quantization Positive and negative models learned as collections of clusters
using the K-Means algorithm Then the decision rule is Nearest Neighbor that is whether
the closest cluster belongs to positive or negative model
Non-negative Matrix Factorization Feature vectors are collected in a matrix and the factorization that minimizes with nonnegative is computed using the multiplicative update algorithm of Using an analogy with the LDA model the columns
of contain the topic distributions while the columns of represent the component
weights A classifier is obtained as the difference of the residuals of the feature projections
on the positive and negative models
From the plot we see how the results of our approach are comparable with the performance of the
Linear SVM while being far superior to the other generative approaches We would like to stress
that a sole comparison on detection performance with state-of-the discriminative classifiers would
be inappropriate since our model targets pose classification which is harder than binary detection
A fair comparison should divide the dataset in classes and compare our model with a multiclass
classifier But then we would face the difficult problem of how to label human poses
For the experiments on pose classification and matching we used the CMU Mobo dataset It
consists of sequences of subjects performing different motion patterns each sequence taken from
different views In the experiments we used 22 sequences of fast walking motion picking the first
frames from each sequence
In the first experiment we trained the model with all the views and set the number of topics equal
to the number of views As expected each topic distribution represents a view and by
assigning every image I to the topic with highest proportion arg maxk we correctly
associated all the images from the same view to the same topic
To obtain a more challenging setup we restricted to a single view and tested the classification performance of our approach in matching poses We learned a model with topics from training
sequences and used the remaining for testing In Figure we show sample topics distributions
from this model In Figure for each test sequence we display a sample frame and the associated top ten matches from the training data according to the score We can see how the pose is
matched against change of appearance and motion style specifically a test subject pose is matched
to similar poses of different subjects in the training set This shows how the topic representation
factors out most of the appearance variations and retains only essential information on the pose
In order to give a quantitative evaluation of the pose matching performance and compare with other
approaches we labeled the dataset by mapping the set of walking poses to the interval We
manually assigned to the frames at the beginning of the double support phase when the swinging
Effect of Histogram Quantization on Human Detection
Detector Performance Comparison
miss rate
miss rate
Continous
32 Levels
Levels
Levels
Levels
false positives per window FPPW
NMF
VQ
LDA Gibbs
LDA MF
Linear SVM
false positives per window FPPW
Figure Human detection results Left Effect on human detection performances of quantizing the histogram of oriented gradient descriptor for a boosted linear SVM classifier based on these features Here
we show false positive false negative curves on log scale We can see that for quantization levels or
more the differences are negligible thus validating our discrete approach Right Performances of five detectors using HOG features trained without boosting and tested on the INRIA dataset LDA detectors learned by
Gibbs Sampling and Mean Field Vector Quantization Non-negative Matrix Factorization all with 24
components/codewords and Linear SVM. We can see how the Gibbs LDA outperform by far the other unsupervised clustering techniques and scores comparably with the Linear SVM which is specifically optimized
for the simpler binary classification problem
Figure Topics distributions and clusters We show sample topics out of from the LDA model trained
on the single view Mobo sequences For each topic we show images in rows The first column shows the
distribution of local orientations associated with topic top visualization of the orientations and bottom
average gradient intensities for each cell The right columns show the top ten images in the dataset with
highest topic proportion shown below each image We can see that topics are tightly related to pose classes
foot touches the ground and to the frames where the legs are approximately parallel We labeled
the remaining frames automatically using linear interpolation between keyframes The average interval between keyframes is frames this motivates our choice of the number of topics
For each test frame we computed the pose error as the difference between the associated pose value
and the average pose of the best top matches in the training dataset We obtained an average error
of corresponding to frames In Figure we show the average pose error per test sequence
obtained with our approach compared with Vector Quantization where the pose is obtained as average of labels associated with the closest clusters and Non-negative Matrix Factorization where as in
LDA similarity of poses is computed as dot product of the component weights In all the models we
set equal number of components/clusters to We can see that our approach performs best in
all testing sequences In Figure we also show the average pose error when matching test frames to
a single train sequence Although the different appearance affects the matching performance overall the results shows how our approach can be successfully applied to automatically match poses of
different subjects
Conclusions
We introduce a novel approach to human detection pose classification and matching from a single
image Starting from a representation robust to a limited range of variations in the appearance of
humans in images we derive a generative probabilistic model which allows for automatic discovery
of pose information The model can successfully perform detection and provides a low dimensional
representation of the pose It automatically clusters the images using representative distributions and
allows for an efficient approach to pose matching Our experiments show that our approach matches
or exceeds the state of the art in human detection pose classification and matching
Figure Pose matching examples On the left one sample frame from test sequences on the right the top
matches in the training set based on the similarity score reported below the image We can see how
our approach allows to match poses even despite large changes in appearance and the same pose is correctly
matched across different subjects
Average Pose Error
LDA Gibbs
LDA MF
NMF
VQ
Figure Pose matching error Left Average pose error in matching test sequences to the training set for
our model both Gibbs and Mean Field learning Non-Negative Matrix Factorization and Vector Quantization
We see how our model trained with Gibbs sampling model clearly outperforms the other approaches Right
Average pose error in matching test and training sequence pairs with our approach where each row is a test
sequence and each column a training sequence The highest error corresponds to about frames while the
mean error is and amounts to approximately frames
Acknowledgments
This work was conducted while the first author was an intern at Honda Research Institute in
Work at UCLA was supported by AFOSR and ONR

<<----------------------------------------------------------------------------------------------------------------------->>

title: 802-constructive-learning-using-internal-representation-conflicts.pdf

Constructive Learning Using Internal
Representation Conflicts
Laurens R. Leerink and Marwan A. abri
Systems Engineering Design Automation Laboratory
Department of Electrical Engineering
The University of Sydney
Sydney NSW Australia
Abstract
We present an algorithm for the training of feedforward and recurrent neural networks It detects internal representation conflicts
and uses these conflicts in a constructive manner to add new neurons to the network The advantages are twofold starting with
a small network neurons are only allocated when required by
detecting and resolving internal conflicts at an early stage learning
time is reduced Empirical results on two real-world problems substantiate the faster learning speed when applied to the training
of a recurrent network on a well researched sequence recognition
task the Reber grammar training times are significantly less than
previously reported
Introduction
Selecting the optimal network architecture for a specific application is a nontrivial
task and several algorithms have been proposed to automate this process The
first class of network adaptation algorithms start out with a redundant architecture
and proceed by pruning away seemingly unimportant weights Sietsma and Dow
Le Cun aI A second class of algorithms starts off with a sparse
architecture and grows the network to the complexity required by the problem
Several algorithms have been proposed for growing feedforward networks The
upstart algorithm of Frean and the cascade-correlation algorithm of Fahlman
are examples of this approach
Leerink and Jabri
The cascade correlation algorithm has also been extended to recurrent networks
Fahlman and has been shown to produce good results The recurrent
cascade-correlation RCC algorithm adds a fully connected layer to the network
after every step in the process attempting to correlate the output of the additional
layer with the error In contrast our proposed algorithm uses the statistical properties of the weight adjustments produced during batch learning to add additional
units
The RCC algorithm will be used as a baseline against which the performance of
our method will be compared In a recent paper Chen al presented an
algorithm which adds one recurrent neuron with small weights every epochs
However no significant improvement in training speed was reported over training
the corresponding fixed size network and the algorithm will not be further analyzed
To the authors knowledge little work besides the two mentioned papers have applied
constructive algorithms to recurrent networks
In the majority of our empirical studies we have used partially recurrent neural
networks and in this paper we will focus our attention on such networks The motivation for the development of this algorithm partly stemmed from the long training
times experienced with the problems of phoneme and word recognition from continuous speech However the algorithm is directly applicable to feedforward networks
The same criteria and method used to add recurrent neurons to a recurrent network
can be used for adding neurons to any hidden layer of a feed-forward network
Architecture
In a standard feedforward network the outputs only depend on the current inputs
the network architecture and the weights in the network However because of the
temporal nature of several applications in particular speech recognition it might
be necessary for the network to have a short term memory
Partially recurrent networks often referred to as Jordan or Elman
networks are well suited to these problems The architecture examined in this
paper is based on the work done by Robinson and Fallside who have applied
their recurrent error propagation network to continuous speech recognition
A common feature of all partially recurrent networks is that there is a special set
of neurons called context units which receive feedback signals from a previous time
step Let the values of the context units at time be represented by During
normal operation the input vector at time are applied to the input nodes and
during the feedforward calculation values are produced at both the output nodes
O(t and the context units C(t The values of the context units are then
copied back to the input layer for use as input in the following time step
Several training algorithms exist for training partially recurrent neural networks
but for tasks with large training sets the back-propagation through time Werbos
is often used This method is computationally efficient and does not use
any approximations in following the gradient For an application where the time
information is spread over T. input patterns the algorithm simply duplicates the
network times which results in a feedforward network that can be trained by a
variation of the standard backpropagation algorithm
Constructive Learning Using Internal Representation Conflicts
The Algorithm
For partially recurrent networks consisting of input output and context neurons
the following assertions can be made
The role of the context units in the network is to extract and store all
relevant prior information from the sequence pertaining to the classification
problem
For weights entering context units the weight update values accumulated
during batch learning will eventually determine what context information
is stored in the unit the sum of the weight update values is larger than the
initial random weights
We assume that initially the number of context units in the network is
insufficient to implement this extraction and storage of information we
start training with a small network Then at different moments in time
during the recognition of long temporal sequences a context unit could be
required to preserve several different contexts
These conflicts are manifested as distinct peaks in the distribution of the
weight update values during the epoch
All but the last fact follows directly from the network architecture and requires no
further elaboration The peaks in the distribution of the weight update values are a
result of the training algorithm attempting to adjust the value of the context units in
order to provide a context value that will resolve short-term memory requirements
After the algorithm had been developed it was discovered that this aspect of the
weight update values had been used in the past by Wynne-Jones and in
the Meiosis Networks of Hanson The method of Wynne-Jones in
particular is very closely related in this case principal component analysis of the
weight updates and the Hessian matrix is used to detect oscillating nodes in fully
trained feed-forward networks This aspect of backpropagation training is fully
discussed in Wynne-Jones to which reader is referred for further details
The above assertions lead to the proposed training algorithm which states that if
there are distinct maxima in the distribution of weight update values of the weights
entering a context unit then this is an indication that the batch learning algorithm
requires this context unit for the storage of more than one context
If this conflict can be resolved the network can effectively store all the contexts
required leading to a reduction in training time and potentially an increase III
performance
The training algorithm is given below the mode of the distribution is defined as
the number of distinct maxima
For all context units
Set modality ot the distribution ot weight update values
It then
Add new context units to the network which are identical
terms ot weighted inputs to the current context unit
Leerink and Jabri
Adjust each of these context units including the
original by the weight update value determined by each
maxima the average value of the mode
Adjust all weights leaving these context units so that the
addition of the new units do not affect any subsequent layers
division by N). This ensures that the network retains all
previously acquired knowledge
The main problem in the implementation of the above algorithm is the automatic
detection of significant maxima in the distribution of weight updates A standard
statistical approach for the determination of the modality the number of maxima
of a distribution of noisy data is to fit a curve of a certain predetermined order to
the data The maxima and minima are then found by setting the derivative to
zero This method was found to be unsuitable mainly because after curve fitting it
was difficult to determine the significance of the detected peaks
It was decided that only instances of bi-modality and tri-modality were to be iden
tified each corresponding to the addition of one or two context units The following
heuristic was constructed
Calculate the mean and standard deviation of the weight update values
Obtain the maximum value in the distribution
If there are any peaks larger than of the maxima outside one standard
deviation of the mean regard this as significant
This heuristic provided adequate identification of the modalities The distribution
was divided into three areas using the mean the standard deviation as boundaries
Depending on the number of maxima detected the average within each area is used
to adjust the weights
Discussion
According to our algorithm it follows that if at least one weight entering a context
unit has a multi-modal distribution then that context unit is duplicated In the
case where multi-modality is detected in more than one weight context units were
added according to the highest modality
Although this algorithm increases the computational load during training the standard deviation of the weight updates rapidly decreases as the network converges
The narrowing of the distribution makes it more difficult to determine the modality In practice it was only found useful to apply the algorithm during the initial
training epochs typically during the first
During simulations in which strong multi-modalities were detected in certain nodes
frequently the multi-modalities would persist in the newly created nodes In this
Constructive Learning Using Internal Representation Conflicts
manner a strong bi-modality would cause one node to split into two the two nodes
to grow to four etc This behaviour was prevented by disabling the splitting of
a node for a variable number of epochs after a multi-modality had been detected
Disabling this behaviour for two epochs provided good results
Simulation Results
The algorithm was evaluated empirically on two different tasks
Phoneme recognition from continuous multi-speaker speech usmg the
TIMIT Garofolo acoustic-phonetic database
Sequence Recognition Learning a finite-state grammar from examples of
valid sequences
For the phoneme recognition task the algorithm decreased training times by a factor
of to depending on the size of the network and the size of the training set
The sequence recognition task has been studied by other researchers in the past notably Fahlman Fahlman compared the performance of the recurrent cascade
correlation RCC network with that of previous results by Cleeremans al
who used an Elman network It was concluded that the RCC algorithm
provides the same or better performance than the Elman network with less training
cycles on a smaller training set Our simulations have shown that the recurrent
error propagation network of Robinson and Fallside when trained with our
constructive algorithm and a learning rate adaptation heuristic can provide the
same performance as the RCC architecture in fewer training epochs using a
training set of the same size The resulting network has the same number of weights
as the minimum size RCC network which correctly solves this problem
Constructive algorithms are often criticized in terms of efficiency Is the increase in learning speed due to the algorithm or just the additional degrees of
freedom resulting from the added neuron and associated weights To address this
question several simulations were conducted on the speech recognition task comparing the performance and learning time of a network with fixed context units
to that of a network with small number of context units and growing a network
with a maximum of context units Results indicate that the constructive algorithm consistently trains faster even though both networks often have the same
final performance
Summary
In this paper the statistical properties of the weight update values obtained during
the training of a simple recurrent network using back-propagation through time
have been examined An algorithm has been presented for using these properties to
detect internal representation conflicts during training and to use this information
to add recurrent units to the network Simulation results show that the algorithm
decreases training time compared to networks which have a fixed number of context
units The algorithm has not been applied to feedforward networks but can III
principle be added to all training algorithms that operate in batch mode
Leerink and Jabri

<<----------------------------------------------------------------------------------------------------------------------->>

